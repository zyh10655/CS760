{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing Yelp Dataset\n",
    "\n",
    "Author(s): Brian Lin, Yuqi Jiao (Anthony)\n",
    "\n",
    "This notebook is for obtaining reduced dataset with 100,000 records and 35 features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from datetime import time, datetime, date\n",
    "import json\n",
    "import sqlite3\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CWD = Path.cwd()\n",
    "ROOT = CWD.parent\n",
    "# path to processed data dir\n",
    "PROC_PATH = CWD/\"processed_data\"\n",
    "# path to experiment-ready data\n",
    "READY_DATA_DIR = CWD.parent/\"ready_data\"\n",
    "DB_PATH = ROOT/\"database/YelpData.db\"\n",
    "\n",
    "RANDOM_SEED = 760 # for reproducibility"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract additional 4 features from database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = sqlite3.connect(DB_PATH)\n",
    "joinStr = '''\n",
    "SELECT\n",
    "    r.id AS r_id,\n",
    "    u.review_count AS u_review_count,\n",
    "    u.compliment_hot AS u_comp_hot,\n",
    "    u.compliment_more AS u_comp_more,\n",
    "    u.compliment_profile AS u_comp_profile,\n",
    "    u.compliment_cute AS u_comp_cute,\n",
    "    u.compliment_list AS u_comp_list,\n",
    "    u.compliment_note AS u_comp_note,\n",
    "    u.compliment_plain AS u_comp_plain,\n",
    "    u.compliment_cool AS u_comp_cool,\n",
    "    u.compliment_funny AS u_comp_funny,\n",
    "    u.compliment_writer AS u_comp_writer,\n",
    "    u.compliment_photos AS u_comp_photos,\n",
    "    u.elite AS u_elite,\n",
    "    b.hours AS b_hours,\n",
    "    u.fans AS u_fans,\n",
    "    u.average_stars as u_avg_stars,\n",
    "    u.useful as u_give_useful\n",
    "FROM review AS r\n",
    "LEFT JOIN business AS b\n",
    "ON r.business_id=b.business_id\n",
    "LEFT JOIN user AS u\n",
    "ON r.user_id=u.user_id\n",
    "WHERE b.is_open<>0 AND r.useful<>0\n",
    "'''\n",
    "# removes closed businesses and reviews with no useful votes\n",
    "data = pd.read_sql(joinStr, conn, parse_dates=['r_date', 'u_yelping_since'])\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obtain user's average compliments per review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data[data.u_review_count != 0] # exclude zero review count records\n",
    "data[\"u_comp_avg\"] = data.u_comp_hot + data.u_comp_more + data.u_comp_profile + data.u_comp_cute + data.u_comp_list + data.u_comp_note + data.u_comp_plain + data.u_comp_cool + data.u_comp_funny + data.u_comp_writer + data.u_comp_photos\n",
    "data.u_comp_avg = data.u_comp_avg / data.u_review_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obtain number of years user was elite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# error in data. \"2020\" has been split into \"20,20\" throughout. Rectify this.\n",
    "# matches to any split 2020, except when it is the only year or 2020 is the last year.\n",
    "data.u_elite = data.u_elite.str.replace(pat=\"20,20,\", repl=\"2020,\", regex=True)\n",
    "# matches when 2020 is the only year or is the last year.\n",
    "data.u_elite = data.u_elite.str.replace(pat=\"20,20$\", repl=\"2020\", regex=True)\n",
    "\n",
    "# if row is None then this returns NaN.\n",
    "# if no commas but not None (e.g single year only), then returns 0\n",
    "u_n_elite_yrs = data.u_elite.str.count(\",\")\n",
    "u_n_elite_yrs[(u_n_elite_yrs != 0) & ~u_n_elite_yrs.isna()] += 1\n",
    "# no commas mean single year\n",
    "u_n_elite_yrs[u_n_elite_yrs == 0] = 1\n",
    "# na means no years\n",
    "u_n_elite_yrs[u_n_elite_yrs.isna()] = 0\n",
    "# convert from float to int\n",
    "u_n_elite_yrs = u_n_elite_yrs.astype(int)\n",
    "data[\"u_n_elite_yrs\"] = u_n_elite_yrs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obtain number of days open per week"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_days_wk(hours_str):\n",
    "    if hours_str:\n",
    "        return len(json.loads(hours_str).keys())\n",
    "    return np.nan\n",
    "\n",
    "data[\"b_days_open_wk\"] = data.b_hours.apply(count_days_wk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obtain number of hours open per week"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test = '''{\"Monday\": \"8:30-17:30\", \"Tuesday\": \"17:0-2:0\", \"Sunday\": \"0:0-0:0\"}'''\n",
    "def count_hours_wk(hours_str):\n",
    "    if hours_str:\n",
    "        js = json.loads(hours_str)\n",
    "        total_hrs = 0\n",
    "        for day_hrs in js.values():\n",
    "            # opening/ending times are formated as either x:0 or x:30, where x is [0,23]\n",
    "            begin, end = day_hrs.split(\"-\")\n",
    "            # 0:0-0:0 indicates 24hrs\n",
    "            if begin == end: total_hrs += 24\n",
    "            else:\n",
    "                # convert to time object\n",
    "                b_hr, b_m = begin.split(\":\")\n",
    "                e_hr, e_m = end.split(\":\")\n",
    "                begin = time(hour=int(b_hr), minute=int(b_m))\n",
    "                end= time(hour=int(e_hr), minute=int(e_m))\n",
    "\n",
    "                # times that cross midnight gets -1 day, but correct number of hours\n",
    "                diff = datetime.combine(date.min, end) - datetime.combine(date.min, begin)\n",
    "                total_hrs += diff.seconds / 3600\n",
    "\n",
    "        return total_hrs\n",
    "    return np.nan\n",
    "\n",
    "data[\"b_hours_open_wk\"] = data.b_hours.apply(count_hours_wk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load in other data files for joining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_df = pd.read_parquet(PROC_PATH/\"joined.parquet.snappy\")\n",
    "is_english = pd.read_parquet(PROC_PATH/\"joined_data_lang_detected.parquet\", columns=[\"r_id\", \"is_english\"])\n",
    "is_english = is_english.astype({\"r_id\": int})\n",
    "linguistic = pd.read_parquet(PROC_PATH/\"joined_linguistic_extra.parquet.snappy\")\n",
    "new_nlp = pd.read_parquet(PROC_PATH/\"newnlp.parquet.snappy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exclude invalid data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exclude non-english (values that = 1)\n",
    "to_exclude = set(is_english.r_id[(is_english.is_english != 0)])\n",
    "# negative useful votes\n",
    "to_exclude = to_exclude.union(set(main_df.r_id[main_df.r_useful < 1]))\n",
    "# reviews older than user\n",
    "to_exclude = to_exclude.union(\n",
    "    set(main_df.r_id[main_df.r_date <= main_df.u_yelping_since]))\n",
    "\n",
    "print(f\"excluding: {len(to_exclude)} records\")\n",
    "main_df = main_df[~main_df.r_id.isin(to_exclude)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Join data files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join in sentiment, subjectivity, readability. join on r_id\n",
    "main_df = main_df.merge(linguistic, on=\"r_id\", validate=\"1:1\")\n",
    "print(f\"correct n rows: {main_df.shape[0] + len(to_exclude) == linguistic.shape[0]}\")\n",
    "\n",
    "# Join in other NLP numeric features\n",
    "new_nlp = new_nlp.drop(columns=[\"r_useful\", \"r_text\"]) # remove redundant\n",
    "main_df = main_df.merge(new_nlp, on=\"r_id\", validate=\"1:1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove unnecessary columns\n",
    "data = data[[\"r_id\", \"u_comp_avg\", \"u_n_elite_yrs\", \"b_days_open_wk\", \"b_hours_open_wk\", \"u_fans\", \"u_avg_stars\", \"u_give_useful\"]]\n",
    "main_df = main_df.merge(data, on=\"r_id\", validate=\"1:1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate elapsed month since creating account, relative to review post date\n",
    "u_month_age = (main_df.r_date - main_df.u_yelping_since) / np.timedelta64(1, 'M')\n",
    "\n",
    "main_df[\"u_month_age\"] = u_month_age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reduce dataset size\n",
    "main_df = main_df.sample(n=100000, random_state=RANDOM_SEED, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove unneeded cols\n",
    "text_df = main_df[[\"r_id\", \"r_useful\", \"r_text\"]]\n",
    "main_df = main_df.drop(columns=[\"b_id\", \"r_funny\", \"r_cool\", \"u_id\", \"r_date\", \"r_text\", \"u_yelping_since\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Order features by category. r_id first, r_useful last.\n",
    "col_order = [\"r_id\", \"r_stars\", \"r_stars_square\", \"r_length\",\n",
    "    \"u_friends_count\", \"u_review_count\", \"u_month_age\", \"u_comp_avg\", \"u_n_elite_yrs\", \"u_fans\", \"u_avg_stars\", \"u_give_useful\",\n",
    "    \"b_stars\", \"b_review_count\", \"b_days_open_wk\", \"b_hours_open_wk\",\n",
    "    \"r_sen\", \"r_sub\", \"r_rea\",\n",
    "    'r_word_cnt', 'r_character_cnt', 'r_sent_cnt', 'r_unique_word_cnt',\n",
    "    'r_stopword_cnt', 'r_avg_wordlength', 'r_avg_sentlength', 'r_unique/words',\n",
    "    'r_stopwords/words','r_digit_cnt', 'r_noun_cnt', 'r_Adj_cnt', 'r_Adv_cnt',\n",
    "    'r_capital_word_cnt', 'r_quoted_word_cnt', 'r_hashtag_cnt', 'r_exclam_cnt',\n",
    "    \"r_useful\"]\n",
    "main_df = main_df[col_order]\n",
    "main_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract and save dataset files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split df into train and remainder. Shuffles before split\n",
    "train_main_df, rem_main_df, train_text_df, rem_text_df = train_test_split(\n",
    "    main_df, text_df, train_size=0.8, random_state=RANDOM_SEED)\n",
    "\n",
    "# split remainder into val and test. Shuffles before split\n",
    "val_main_df, test_main_df, val_text_df, test_text_df = train_test_split(\n",
    "    rem_main_df, rem_text_df, train_size=0.5, random_state=RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# validate created splits proportions. should be about 0.8, 0.1, 0.1\n",
    "print(len(train_main_df) / len(main_df), len(val_main_df) / len(main_df), len(test_main_df) / len(main_df))\n",
    "print(len(train_text_df) / len(main_df), len(val_text_df) / len(main_df), len(test_text_df) / len(main_df))\n",
    "# check records align in main df and text\n",
    "print(np.all(train_main_df.index == train_text_df.index)) # want: TRUE\n",
    "print(np.all(test_main_df.index == test_text_df.index)) # want: TRUE\n",
    "print(np.all(val_main_df.index == val_text_df.index)) # want: TRUE\n",
    "# check uniqueness of records\n",
    "print(len(np.intersect1d(train_main_df.index, test_main_df.index)) == 0) # want: TRUE\n",
    "print(len(np.intersect1d(train_main_df.index, val_main_df.index)) == 0) # want: TRUE\n",
    "print(len(np.intersect1d(val_main_df.index, test_main_df.index)) == 0) # want: TRUE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save output\n",
    "train_main_df.to_parquet(READY_DATA_DIR/\"100K35F_train_main.parquet.snappy\", index=False)\n",
    "train_text_df.to_parquet(READY_DATA_DIR/\"100K35F_train_text.parquet.snappy\", index=False)\n",
    "\n",
    "val_main_df.to_parquet(READY_DATA_DIR/\"100K35F_val_main.parquet.snappy\", index=False)\n",
    "val_text_df.to_parquet(READY_DATA_DIR/\"100K35F_val_text.parquet.snappy\", index=False)\n",
    "\n",
    "test_main_df.to_parquet(READY_DATA_DIR/\"100K35F_test_main.parquet.snappy\", index=False)\n",
    "test_text_df.to_parquet(READY_DATA_DIR/\"100K35F_test_text.parquet.snappy\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_main_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_main_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_main_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('CS760': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "12ede14e1ffbe31a015ff23db9d83bed6e28cdb2c11038b651bf1634c6ce7b9c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
