{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "J5uIHs3qKGJX",
        "outputId": "eccea316-b8a2-4c5f-ebf9-9a6fdce87ad1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting optuna==0.14.0\n",
            "  Downloading optuna-0.14.0.tar.gz (91 kB)\n",
            "\u001b[K     |████████████████████████████████| 91 kB 4.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: sqlalchemy>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from optuna==0.14.0) (1.4.41)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from optuna==0.14.0) (1.21.6)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from optuna==0.14.0) (1.7.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from optuna==0.14.0) (1.15.0)\n",
            "Collecting cliff\n",
            "  Downloading cliff-3.10.1-py3-none-any.whl (81 kB)\n",
            "\u001b[K     |████████████████████████████████| 81 kB 8.1 MB/s \n",
            "\u001b[?25hCollecting colorlog\n",
            "  Downloading colorlog-6.7.0-py2.py3-none-any.whl (11 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from optuna==0.14.0) (1.3.5)\n",
            "Collecting alembic\n",
            "  Downloading alembic-1.8.1-py3-none-any.whl (209 kB)\n",
            "\u001b[K     |████████████████████████████████| 209 kB 56.3 MB/s \n",
            "\u001b[?25hCollecting typing\n",
            "  Downloading typing-3.7.4.3.tar.gz (78 kB)\n",
            "\u001b[K     |████████████████████████████████| 78 kB 6.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from sqlalchemy>=1.1.0->optuna==0.14.0) (4.12.0)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.7/dist-packages (from sqlalchemy>=1.1.0->optuna==0.14.0) (1.1.3)\n",
            "Collecting Mako\n",
            "  Downloading Mako-1.2.3-py3-none-any.whl (78 kB)\n",
            "\u001b[K     |████████████████████████████████| 78 kB 7.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: importlib-resources in /usr/local/lib/python3.7/dist-packages (from alembic->optuna==0.14.0) (5.9.0)\n",
            "Requirement already satisfied: PyYAML>=3.12 in /usr/local/lib/python3.7/dist-packages (from cliff->optuna==0.14.0) (6.0)\n",
            "Collecting pbr!=2.1.0,>=2.0.0\n",
            "  Downloading pbr-5.10.0-py2.py3-none-any.whl (112 kB)\n",
            "\u001b[K     |████████████████████████████████| 112 kB 88.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: PrettyTable>=0.7.2 in /usr/local/lib/python3.7/dist-packages (from cliff->optuna==0.14.0) (3.4.1)\n",
            "Collecting cmd2>=1.0.0\n",
            "  Downloading cmd2-2.4.2-py3-none-any.whl (147 kB)\n",
            "\u001b[K     |████████████████████████████████| 147 kB 73.7 MB/s \n",
            "\u001b[?25hCollecting autopage>=0.4.0\n",
            "  Downloading autopage-0.5.1-py3-none-any.whl (29 kB)\n",
            "Collecting stevedore>=2.0.1\n",
            "  Downloading stevedore-3.5.0-py3-none-any.whl (49 kB)\n",
            "\u001b[K     |████████████████████████████████| 49 kB 6.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pyparsing>=2.1.0 in /usr/local/lib/python3.7/dist-packages (from cliff->optuna==0.14.0) (3.0.9)\n",
            "Requirement already satisfied: wcwidth>=0.1.7 in /usr/local/lib/python3.7/dist-packages (from cmd2>=1.0.0->cliff->optuna==0.14.0) (0.2.5)\n",
            "Collecting pyperclip>=1.6\n",
            "  Downloading pyperclip-1.8.2.tar.gz (20 kB)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from cmd2>=1.0.0->cliff->optuna==0.14.0) (4.1.1)\n",
            "Requirement already satisfied: attrs>=16.3.0 in /usr/local/lib/python3.7/dist-packages (from cmd2>=1.0.0->cliff->optuna==0.14.0) (22.1.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->sqlalchemy>=1.1.0->optuna==0.14.0) (3.8.1)\n",
            "Requirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.7/dist-packages (from Mako->alembic->optuna==0.14.0) (2.0.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->optuna==0.14.0) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas->optuna==0.14.0) (2022.2.1)\n",
            "Building wheels for collected packages: optuna, pyperclip, typing\n",
            "  Building wheel for optuna (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for optuna: filename=optuna-0.14.0-py3-none-any.whl size=125709 sha256=ae640358ce76701a2141f5b9de4987be1e9ea0a1261dbd8302223d1d149a320f\n",
            "  Stored in directory: /root/.cache/pip/wheels/c1/41/64/03b183676c5d5e978de160cab6268d5b4fb095dff63f720e01\n",
            "  Building wheel for pyperclip (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyperclip: filename=pyperclip-1.8.2-py3-none-any.whl size=11137 sha256=fa9fda3cbab42bb84571ea4f77d3ebf82dc68b35cd13cd60426b458377b3594a\n",
            "  Stored in directory: /root/.cache/pip/wheels/9f/18/84/8f69f8b08169c7bae2dde6bd7daf0c19fca8c8e500ee620a28\n",
            "  Building wheel for typing (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for typing: filename=typing-3.7.4.3-py3-none-any.whl size=26325 sha256=b28adfb0b47624d33f484f9b92fef98c22f31aae0b8251e6cdf0b5759e3234d5\n",
            "  Stored in directory: /root/.cache/pip/wheels/35/f3/15/01aa6571f0a72ee6ae7b827c1491c37a1f72d686fd22b43b0e\n",
            "Successfully built optuna pyperclip typing\n",
            "Installing collected packages: pyperclip, pbr, stevedore, Mako, cmd2, autopage, typing, colorlog, cliff, alembic, optuna\n",
            "Successfully installed Mako-1.2.3 alembic-1.8.1 autopage-0.5.1 cliff-3.10.1 cmd2-2.4.2 colorlog-6.7.0 optuna-0.14.0 pbr-5.10.0 pyperclip-1.8.2 stevedore-3.5.0 typing-3.7.4.3\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "typing"
                ]
              }
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting optkeras==0.0.7\n",
            "  Downloading optkeras-0.0.7-py3-none-any.whl (6.9 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from optkeras==0.0.7) (1.21.6)\n",
            "Requirement already satisfied: keras in /usr/local/lib/python3.7/dist-packages (from optkeras==0.0.7) (2.8.0)\n",
            "Requirement already satisfied: optuna>=0.9.0 in /usr/local/lib/python3.7/dist-packages (from optkeras==0.0.7) (0.14.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from optuna>=0.9.0->optkeras==0.0.7) (1.7.3)\n",
            "Requirement already satisfied: cliff in /usr/local/lib/python3.7/dist-packages (from optuna>=0.9.0->optkeras==0.0.7) (3.10.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from optuna>=0.9.0->optkeras==0.0.7) (1.3.5)\n",
            "Requirement already satisfied: colorlog in /usr/local/lib/python3.7/dist-packages (from optuna>=0.9.0->optkeras==0.0.7) (6.7.0)\n",
            "Requirement already satisfied: sqlalchemy>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from optuna>=0.9.0->optkeras==0.0.7) (1.4.41)\n",
            "Requirement already satisfied: alembic in /usr/local/lib/python3.7/dist-packages (from optuna>=0.9.0->optkeras==0.0.7) (1.8.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from optuna>=0.9.0->optkeras==0.0.7) (1.15.0)\n",
            "Requirement already satisfied: typing in /usr/local/lib/python3.7/dist-packages (from optuna>=0.9.0->optkeras==0.0.7) (3.7.4.3)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.7/dist-packages (from sqlalchemy>=1.1.0->optuna>=0.9.0->optkeras==0.0.7) (1.1.3)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from sqlalchemy>=1.1.0->optuna>=0.9.0->optkeras==0.0.7) (4.12.0)\n",
            "Requirement already satisfied: Mako in /usr/local/lib/python3.7/dist-packages (from alembic->optuna>=0.9.0->optkeras==0.0.7) (1.2.3)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.7/dist-packages (from alembic->optuna>=0.9.0->optkeras==0.0.7) (5.9.0)\n",
            "Requirement already satisfied: PyYAML>=3.12 in /usr/local/lib/python3.7/dist-packages (from cliff->optuna>=0.9.0->optkeras==0.0.7) (6.0)\n",
            "Requirement already satisfied: pyparsing>=2.1.0 in /usr/local/lib/python3.7/dist-packages (from cliff->optuna>=0.9.0->optkeras==0.0.7) (3.0.9)\n",
            "Requirement already satisfied: autopage>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from cliff->optuna>=0.9.0->optkeras==0.0.7) (0.5.1)\n",
            "Requirement already satisfied: PrettyTable>=0.7.2 in /usr/local/lib/python3.7/dist-packages (from cliff->optuna>=0.9.0->optkeras==0.0.7) (3.4.1)\n",
            "Requirement already satisfied: cmd2>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from cliff->optuna>=0.9.0->optkeras==0.0.7) (2.4.2)\n",
            "Requirement already satisfied: pbr!=2.1.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from cliff->optuna>=0.9.0->optkeras==0.0.7) (5.10.0)\n",
            "Requirement already satisfied: stevedore>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from cliff->optuna>=0.9.0->optkeras==0.0.7) (3.5.0)\n",
            "Requirement already satisfied: pyperclip>=1.6 in /usr/local/lib/python3.7/dist-packages (from cmd2>=1.0.0->cliff->optuna>=0.9.0->optkeras==0.0.7) (1.8.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from cmd2>=1.0.0->cliff->optuna>=0.9.0->optkeras==0.0.7) (4.1.1)\n",
            "Requirement already satisfied: wcwidth>=0.1.7 in /usr/local/lib/python3.7/dist-packages (from cmd2>=1.0.0->cliff->optuna>=0.9.0->optkeras==0.0.7) (0.2.5)\n",
            "Requirement already satisfied: attrs>=16.3.0 in /usr/local/lib/python3.7/dist-packages (from cmd2>=1.0.0->cliff->optuna>=0.9.0->optkeras==0.0.7) (22.1.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->sqlalchemy>=1.1.0->optuna>=0.9.0->optkeras==0.0.7) (3.8.1)\n",
            "Requirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.7/dist-packages (from Mako->alembic->optuna>=0.9.0->optkeras==0.0.7) (2.0.1)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas->optuna>=0.9.0->optkeras==0.0.7) (2022.2.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->optuna>=0.9.0->optkeras==0.0.7) (2.8.2)\n",
            "Installing collected packages: optkeras\n",
            "Successfully installed optkeras-0.0.7\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting fastparquet\n",
            "  Downloading fastparquet-0.8.1-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.5 MB 5.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: fsspec in /usr/local/lib/python3.7/dist-packages (from fastparquet) (2022.8.2)\n",
            "Collecting cramjam>=2.3.0\n",
            "  Downloading cramjam-2.5.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.whl (1.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.6 MB 73.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pandas>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from fastparquet) (1.3.5)\n",
            "Requirement already satisfied: numpy>=1.18 in /usr/local/lib/python3.7/dist-packages (from fastparquet) (1.21.6)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=1.1.0->fastparquet) (2022.2.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=1.1.0->fastparquet) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas>=1.1.0->fastparquet) (1.15.0)\n",
            "Installing collected packages: cramjam, fastparquet\n",
            "Successfully installed cramjam-2.5.0 fastparquet-0.8.1\n"
          ]
        }
      ],
      "source": [
        "!pip3 install optuna==0.14.0\n",
        "!pip3 install optkeras==0.0.7\n",
        "!pip install fastparquet"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from keras.datasets import mnist\n",
        "from keras.models import Sequential,Model\n",
        "from keras.layers import Activation, Flatten, Dense, Conv2D, Conv1D,Input\n",
        "from keras.layers import MaxPooling1D, Dropout, BatchNormalization\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.optimizers import SGD, Adagrad, RMSprop, Adam, Adadelta, Adamax, Nadam\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.impute import SimpleImputer\n",
        "from keras.layers.core import *\n",
        "from keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "import keras.backend as K\n",
        "import optuna\n",
        "import math"
      ],
      "metadata": {
        "id": "8o4dbzGuLONQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import keras\n",
        "print('Keras', keras.__version__)\n",
        "\n",
        "import tensorflow as tf\n",
        "print('TensorFlow', tf.__version__)\n",
        "\n",
        "# import Optuna and OptKeras after Keras\n",
        "import optuna \n",
        "print('Optuna', optuna.__version__)\n",
        "\n",
        "from optkeras.optkeras import OptKeras\n",
        "import optkeras\n",
        "print('OptKeras', optkeras.__version__)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EwpYWLIqLRIA",
        "outputId": "7d975e75-ea1d-4ab0-e164-a8e8dd039243"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Keras 2.8.0\n",
            "TensorFlow 2.8.2\n",
            "Optuna 0.14.0\n",
            "OptKeras 0.0.7\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_df = pd.read_parquet(\"/content/drive/MyDrive/New Data/18features/18_train_main.parquet.snappy\",engine='fastparquet')\n",
        "test_df = pd.read_parquet(\"/content/drive/MyDrive/New Data/18features/18_test_main.parquet.snappy\",engine='fastparquet')"
      ],
      "metadata": {
        "id": "MPqujRB7LSOH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prepare the cross-validation procedure\n",
        "kfold = KFold(n_splits=3, shuffle=True)\n",
        "\n",
        "training_withaim=train_df.drop(labels=\"r_id\", axis=1)\n",
        "testing_withaim=test_df.drop(labels=\"r_id\", axis=1)\n",
        "\n",
        "#Check the NaN in data and drop them\n",
        "imp_train=SimpleImputer(missing_values=np.NaN)\n",
        "training=pd.DataFrame(imp_train.fit_transform(training_withaim))\n",
        "\n",
        "imp_test=SimpleImputer(missing_values=np.NaN)\n",
        "testing=pd.DataFrame(imp_test.fit_transform(testing_withaim))\n",
        "print(training)\n",
        "# There aren't any nan data in the dataframe \n",
        "training.isnull().values.sum()\n",
        "testing.isnull().values.sum()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "esfqiU3EMyUr",
        "outputId": "6de15013-c6d1-4969-c704-0c1722fed1de"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "         0     1      2      3       4           5         6    7     8   \\\n",
            "0       5.0  25.0   80.0  173.0   129.0  108.203988  0.178295  3.0   6.0   \n",
            "1       4.0  16.0  171.0  128.0   330.0   58.652416  0.909091  7.0  21.0   \n",
            "2       3.0   9.0  114.0  277.0  1064.0   64.378573  1.753759  2.0  57.0   \n",
            "3       5.0  25.0   34.0   35.0    59.0  102.077493  0.067797  0.0   0.0   \n",
            "4       1.0   1.0  261.0  137.0     5.0   44.994772  0.000000  0.0   0.0   \n",
            "...     ...   ...    ...    ...     ...         ...       ...  ...   ...   \n",
            "399995  5.0  25.0  191.0    1.0    16.0    0.000110  0.000000  0.0   0.0   \n",
            "399996  3.0   9.0  434.0  164.0   471.0   70.677502  0.116773  4.0  22.0   \n",
            "399997  5.0  25.0  131.0  211.0     7.0   23.186602  0.142857  0.0   1.0   \n",
            "399998  5.0  25.0   49.0    1.0    16.0    6.598250  0.062500  0.0   0.0   \n",
            "399999  1.0   1.0   25.0    1.0    58.0   27.243015  0.034483  0.0   0.0   \n",
            "\n",
            "          9       10   11      12   13     14        15        16     17    18  \n",
            "0       4.50   118.0  5.0    10.0  6.0   60.0  0.279954  0.433241  66.44   2.0  \n",
            "1       3.84   891.0  4.0   711.0  6.0   48.0  0.272727  0.474242  72.66   3.0  \n",
            "2       4.21  9948.0  3.5   154.0  6.0   33.0  0.146667  0.294583  68.13  18.0  \n",
            "3       4.30    48.0  4.0   109.0  7.0   71.0  0.383333  0.541667  76.93   1.0  \n",
            "4       3.40     5.0  4.0    18.0  5.0   42.5  0.039118  0.300063  88.97   3.0  \n",
            "...      ...     ...  ...     ...  ...    ...       ...       ...    ...   ...  \n",
            "399995  4.72     9.0  4.0    30.0  6.0   60.0 -0.130221  0.420113  69.11   3.0  \n",
            "399996  3.62   716.0  4.5   454.0  6.0   64.0  0.241433  0.523121  77.16   1.0  \n",
            "399997  2.93     3.0  4.0   380.0  7.0   72.0  0.084722  0.475076  82.24   1.0  \n",
            "399998  5.00     3.0  4.0  1984.0  7.0   60.0  0.340000  0.460000  78.45   1.0  \n",
            "399999  3.45    40.0  2.0    30.0  7.0  112.0 -0.500000  0.566667  88.74   1.0  \n",
            "\n",
            "[400000 rows x 19 columns]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "training = training.iloc[: , 0:18]\n",
        "testing = testing.iloc[: , 0:18]\n",
        "training"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "Te12QBIeR-3t",
        "outputId": "fe27125c-f515-4c3e-c0f0-84c73a6646f3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "         0     1      2      3       4           5         6    7     8   \\\n",
              "0       5.0  25.0   80.0  173.0   129.0  108.203988  0.178295  3.0   6.0   \n",
              "1       4.0  16.0  171.0  128.0   330.0   58.652416  0.909091  7.0  21.0   \n",
              "2       3.0   9.0  114.0  277.0  1064.0   64.378573  1.753759  2.0  57.0   \n",
              "3       5.0  25.0   34.0   35.0    59.0  102.077493  0.067797  0.0   0.0   \n",
              "4       1.0   1.0  261.0  137.0     5.0   44.994772  0.000000  0.0   0.0   \n",
              "...     ...   ...    ...    ...     ...         ...       ...  ...   ...   \n",
              "399995  5.0  25.0  191.0    1.0    16.0    0.000110  0.000000  0.0   0.0   \n",
              "399996  3.0   9.0  434.0  164.0   471.0   70.677502  0.116773  4.0  22.0   \n",
              "399997  5.0  25.0  131.0  211.0     7.0   23.186602  0.142857  0.0   1.0   \n",
              "399998  5.0  25.0   49.0    1.0    16.0    6.598250  0.062500  0.0   0.0   \n",
              "399999  1.0   1.0   25.0    1.0    58.0   27.243015  0.034483  0.0   0.0   \n",
              "\n",
              "          9       10   11      12   13     14        15        16     17  \n",
              "0       4.50   118.0  5.0    10.0  6.0   60.0  0.279954  0.433241  66.44  \n",
              "1       3.84   891.0  4.0   711.0  6.0   48.0  0.272727  0.474242  72.66  \n",
              "2       4.21  9948.0  3.5   154.0  6.0   33.0  0.146667  0.294583  68.13  \n",
              "3       4.30    48.0  4.0   109.0  7.0   71.0  0.383333  0.541667  76.93  \n",
              "4       3.40     5.0  4.0    18.0  5.0   42.5  0.039118  0.300063  88.97  \n",
              "...      ...     ...  ...     ...  ...    ...       ...       ...    ...  \n",
              "399995  4.72     9.0  4.0    30.0  6.0   60.0 -0.130221  0.420113  69.11  \n",
              "399996  3.62   716.0  4.5   454.0  6.0   64.0  0.241433  0.523121  77.16  \n",
              "399997  2.93     3.0  4.0   380.0  7.0   72.0  0.084722  0.475076  82.24  \n",
              "399998  5.00     3.0  4.0  1984.0  7.0   60.0  0.340000  0.460000  78.45  \n",
              "399999  3.45    40.0  2.0    30.0  7.0  112.0 -0.500000  0.566667  88.74  \n",
              "\n",
              "[400000 rows x 18 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-ee3263ce-e4c1-4d73-91d5-b15f8b878a0e\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>5.0</td>\n",
              "      <td>25.0</td>\n",
              "      <td>80.0</td>\n",
              "      <td>173.0</td>\n",
              "      <td>129.0</td>\n",
              "      <td>108.203988</td>\n",
              "      <td>0.178295</td>\n",
              "      <td>3.0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>4.50</td>\n",
              "      <td>118.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>10.0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>60.0</td>\n",
              "      <td>0.279954</td>\n",
              "      <td>0.433241</td>\n",
              "      <td>66.44</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>4.0</td>\n",
              "      <td>16.0</td>\n",
              "      <td>171.0</td>\n",
              "      <td>128.0</td>\n",
              "      <td>330.0</td>\n",
              "      <td>58.652416</td>\n",
              "      <td>0.909091</td>\n",
              "      <td>7.0</td>\n",
              "      <td>21.0</td>\n",
              "      <td>3.84</td>\n",
              "      <td>891.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>711.0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>48.0</td>\n",
              "      <td>0.272727</td>\n",
              "      <td>0.474242</td>\n",
              "      <td>72.66</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3.0</td>\n",
              "      <td>9.0</td>\n",
              "      <td>114.0</td>\n",
              "      <td>277.0</td>\n",
              "      <td>1064.0</td>\n",
              "      <td>64.378573</td>\n",
              "      <td>1.753759</td>\n",
              "      <td>2.0</td>\n",
              "      <td>57.0</td>\n",
              "      <td>4.21</td>\n",
              "      <td>9948.0</td>\n",
              "      <td>3.5</td>\n",
              "      <td>154.0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>33.0</td>\n",
              "      <td>0.146667</td>\n",
              "      <td>0.294583</td>\n",
              "      <td>68.13</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>5.0</td>\n",
              "      <td>25.0</td>\n",
              "      <td>34.0</td>\n",
              "      <td>35.0</td>\n",
              "      <td>59.0</td>\n",
              "      <td>102.077493</td>\n",
              "      <td>0.067797</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>4.30</td>\n",
              "      <td>48.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>109.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>71.0</td>\n",
              "      <td>0.383333</td>\n",
              "      <td>0.541667</td>\n",
              "      <td>76.93</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>261.0</td>\n",
              "      <td>137.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>44.994772</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>3.40</td>\n",
              "      <td>5.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>18.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>42.5</td>\n",
              "      <td>0.039118</td>\n",
              "      <td>0.300063</td>\n",
              "      <td>88.97</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>399995</th>\n",
              "      <td>5.0</td>\n",
              "      <td>25.0</td>\n",
              "      <td>191.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>16.0</td>\n",
              "      <td>0.000110</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>4.72</td>\n",
              "      <td>9.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>30.0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>60.0</td>\n",
              "      <td>-0.130221</td>\n",
              "      <td>0.420113</td>\n",
              "      <td>69.11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>399996</th>\n",
              "      <td>3.0</td>\n",
              "      <td>9.0</td>\n",
              "      <td>434.0</td>\n",
              "      <td>164.0</td>\n",
              "      <td>471.0</td>\n",
              "      <td>70.677502</td>\n",
              "      <td>0.116773</td>\n",
              "      <td>4.0</td>\n",
              "      <td>22.0</td>\n",
              "      <td>3.62</td>\n",
              "      <td>716.0</td>\n",
              "      <td>4.5</td>\n",
              "      <td>454.0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>64.0</td>\n",
              "      <td>0.241433</td>\n",
              "      <td>0.523121</td>\n",
              "      <td>77.16</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>399997</th>\n",
              "      <td>5.0</td>\n",
              "      <td>25.0</td>\n",
              "      <td>131.0</td>\n",
              "      <td>211.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>23.186602</td>\n",
              "      <td>0.142857</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>2.93</td>\n",
              "      <td>3.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>380.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>72.0</td>\n",
              "      <td>0.084722</td>\n",
              "      <td>0.475076</td>\n",
              "      <td>82.24</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>399998</th>\n",
              "      <td>5.0</td>\n",
              "      <td>25.0</td>\n",
              "      <td>49.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>16.0</td>\n",
              "      <td>6.598250</td>\n",
              "      <td>0.062500</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>5.00</td>\n",
              "      <td>3.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>1984.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>60.0</td>\n",
              "      <td>0.340000</td>\n",
              "      <td>0.460000</td>\n",
              "      <td>78.45</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>399999</th>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>25.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>58.0</td>\n",
              "      <td>27.243015</td>\n",
              "      <td>0.034483</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>3.45</td>\n",
              "      <td>40.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>30.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>112.0</td>\n",
              "      <td>-0.500000</td>\n",
              "      <td>0.566667</td>\n",
              "      <td>88.74</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>400000 rows × 18 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-ee3263ce-e4c1-4d73-91d5-b15f8b878a0e')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-ee3263ce-e4c1-4d73-91d5-b15f8b878a0e button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-ee3263ce-e4c1-4d73-91d5-b15f8b878a0e');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "scaler = StandardScaler()\n",
        "\n",
        "# scale skewed and target features\n",
        "std_train_df = train_df.copy(deep=True)\n",
        "std_train_df = scaler.fit_transform(training)\n",
        "std_test_df = test_df.copy(deep=True)\n",
        "std_test_df = scaler.transform(testing)\n",
        "#std_test_df[['r_stars','r_stars_square','r_length', 'u_friends_count', 'u_review_count', 'u_month_age', 'b_stars','b_review_count','r_rea']] = scaler.transform(test_df[['r_stars','r_stars_square','r_length', 'u_friends_count', 'u_review_count', 'u_month_age', 'b_stars','b_review_count','r_rea']])\n",
        "\n",
        "print(std_train_df[0])\n",
        "std_train_df = pd.DataFrame(std_train_df)\n",
        "std_test_df = pd.DataFrame(std_test_df)\n",
        "std_train_df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 476
        },
        "id": "qo216mLySLVU",
        "outputId": "a2f5cf12-434c-4e36-cc4b-d0335dba4381"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ 0.90216754  0.99645674 -0.4633628  -0.02815708 -0.12709163  1.889782\n",
            " -0.11838612  0.3622633  -0.14541076  0.94854255 -0.15259145  1.58161918\n",
            " -0.48965899 -0.58778166 -0.41108489  0.50629158 -0.08015604 -1.15116974]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "              0         1         2         3         4         5         6   \\\n",
              "0       0.902168  0.996457 -0.463363 -0.028157 -0.127092  1.889782 -0.118386   \n",
              "1       0.252164  0.060934  0.330297 -0.108118  0.288456  0.458212  0.116132   \n",
              "2      -0.397840 -0.666695 -0.166830  0.156643  1.805931  0.623643  0.387193   \n",
              "3       0.902168  0.996457 -0.864554 -0.273372 -0.271810  1.712784 -0.153846   \n",
              "4      -1.697847 -1.498271  1.115236 -0.092126 -0.383450  0.063635 -0.175602   \n",
              "...          ...       ...       ...       ...       ...       ...       ...   \n",
              "399995  0.902168  0.996457  0.504728 -0.333787 -0.360708 -1.236284 -0.175602   \n",
              "399996 -0.397840 -0.666695  2.624063 -0.044149  0.579960  0.805623 -0.138129   \n",
              "399997  0.902168  0.996457 -0.018564  0.039366 -0.379315 -0.566414 -0.129758   \n",
              "399998  0.902168  0.996457 -0.733731 -0.333787 -0.360708 -1.045660 -0.155546   \n",
              "399999 -1.697847 -1.498271 -0.943048 -0.333787 -0.273877 -0.449222 -0.164536   \n",
              "\n",
              "              7         8         9         10        11        12        13  \\\n",
              "0       0.362263 -0.145411  0.948543 -0.152591  1.581619 -0.489659 -0.587782   \n",
              "1       1.662180 -0.029172  0.144179  0.016366  0.321906  0.559060 -0.587782   \n",
              "2       0.037284  0.249799  0.595110  1.995984 -0.307951 -0.274230 -0.587782   \n",
              "3      -0.612674 -0.191906  0.704796 -0.167892  0.321906 -0.341552  0.617819   \n",
              "4      -0.612674 -0.191906 -0.392064 -0.177290  0.321906 -0.477691 -1.793382   \n",
              "...          ...       ...       ...       ...       ...       ...       ...   \n",
              "399995 -0.612674 -0.191906  1.216664 -0.176416  0.321906 -0.459738 -0.587782   \n",
              "399996  0.687243 -0.021423 -0.123943 -0.021885  0.951762  0.174580 -0.587782   \n",
              "399997 -0.612674 -0.184157 -0.964869 -0.177727  0.321906  0.063873  0.617819   \n",
              "399998 -0.612674 -0.191906  1.557909 -0.177727  0.321906  2.463511  0.617819   \n",
              "399999 -0.612674 -0.191906 -0.331127 -0.169640 -2.197521 -0.459738  0.617819   \n",
              "\n",
              "              14        15        16        17  \n",
              "0      -0.411085  0.506292 -0.080156 -1.151170  \n",
              "1      -0.797300  0.468778  0.199067 -0.623535  \n",
              "2      -1.280069 -0.185575 -1.024437 -1.007809  \n",
              "3      -0.057054  1.042912  0.658240 -0.261316  \n",
              "4      -0.974315 -0.743841 -0.987117  0.760023  \n",
              "...          ...       ...       ...       ...  \n",
              "399995 -0.411085 -1.622847 -0.169560 -0.924677  \n",
              "399996 -0.282347  0.306337  0.531939 -0.241805  \n",
              "399997 -0.024870 -0.507120  0.204746  0.189125  \n",
              "399998 -0.411085  0.817979  0.102077 -0.132376  \n",
              "399999  1.262513 -3.542297  0.828494  0.740512  \n",
              "\n",
              "[400000 rows x 18 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-74137810-4718-47cf-9543-fc93ec386a30\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.902168</td>\n",
              "      <td>0.996457</td>\n",
              "      <td>-0.463363</td>\n",
              "      <td>-0.028157</td>\n",
              "      <td>-0.127092</td>\n",
              "      <td>1.889782</td>\n",
              "      <td>-0.118386</td>\n",
              "      <td>0.362263</td>\n",
              "      <td>-0.145411</td>\n",
              "      <td>0.948543</td>\n",
              "      <td>-0.152591</td>\n",
              "      <td>1.581619</td>\n",
              "      <td>-0.489659</td>\n",
              "      <td>-0.587782</td>\n",
              "      <td>-0.411085</td>\n",
              "      <td>0.506292</td>\n",
              "      <td>-0.080156</td>\n",
              "      <td>-1.151170</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.252164</td>\n",
              "      <td>0.060934</td>\n",
              "      <td>0.330297</td>\n",
              "      <td>-0.108118</td>\n",
              "      <td>0.288456</td>\n",
              "      <td>0.458212</td>\n",
              "      <td>0.116132</td>\n",
              "      <td>1.662180</td>\n",
              "      <td>-0.029172</td>\n",
              "      <td>0.144179</td>\n",
              "      <td>0.016366</td>\n",
              "      <td>0.321906</td>\n",
              "      <td>0.559060</td>\n",
              "      <td>-0.587782</td>\n",
              "      <td>-0.797300</td>\n",
              "      <td>0.468778</td>\n",
              "      <td>0.199067</td>\n",
              "      <td>-0.623535</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>-0.397840</td>\n",
              "      <td>-0.666695</td>\n",
              "      <td>-0.166830</td>\n",
              "      <td>0.156643</td>\n",
              "      <td>1.805931</td>\n",
              "      <td>0.623643</td>\n",
              "      <td>0.387193</td>\n",
              "      <td>0.037284</td>\n",
              "      <td>0.249799</td>\n",
              "      <td>0.595110</td>\n",
              "      <td>1.995984</td>\n",
              "      <td>-0.307951</td>\n",
              "      <td>-0.274230</td>\n",
              "      <td>-0.587782</td>\n",
              "      <td>-1.280069</td>\n",
              "      <td>-0.185575</td>\n",
              "      <td>-1.024437</td>\n",
              "      <td>-1.007809</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.902168</td>\n",
              "      <td>0.996457</td>\n",
              "      <td>-0.864554</td>\n",
              "      <td>-0.273372</td>\n",
              "      <td>-0.271810</td>\n",
              "      <td>1.712784</td>\n",
              "      <td>-0.153846</td>\n",
              "      <td>-0.612674</td>\n",
              "      <td>-0.191906</td>\n",
              "      <td>0.704796</td>\n",
              "      <td>-0.167892</td>\n",
              "      <td>0.321906</td>\n",
              "      <td>-0.341552</td>\n",
              "      <td>0.617819</td>\n",
              "      <td>-0.057054</td>\n",
              "      <td>1.042912</td>\n",
              "      <td>0.658240</td>\n",
              "      <td>-0.261316</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>-1.697847</td>\n",
              "      <td>-1.498271</td>\n",
              "      <td>1.115236</td>\n",
              "      <td>-0.092126</td>\n",
              "      <td>-0.383450</td>\n",
              "      <td>0.063635</td>\n",
              "      <td>-0.175602</td>\n",
              "      <td>-0.612674</td>\n",
              "      <td>-0.191906</td>\n",
              "      <td>-0.392064</td>\n",
              "      <td>-0.177290</td>\n",
              "      <td>0.321906</td>\n",
              "      <td>-0.477691</td>\n",
              "      <td>-1.793382</td>\n",
              "      <td>-0.974315</td>\n",
              "      <td>-0.743841</td>\n",
              "      <td>-0.987117</td>\n",
              "      <td>0.760023</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>399995</th>\n",
              "      <td>0.902168</td>\n",
              "      <td>0.996457</td>\n",
              "      <td>0.504728</td>\n",
              "      <td>-0.333787</td>\n",
              "      <td>-0.360708</td>\n",
              "      <td>-1.236284</td>\n",
              "      <td>-0.175602</td>\n",
              "      <td>-0.612674</td>\n",
              "      <td>-0.191906</td>\n",
              "      <td>1.216664</td>\n",
              "      <td>-0.176416</td>\n",
              "      <td>0.321906</td>\n",
              "      <td>-0.459738</td>\n",
              "      <td>-0.587782</td>\n",
              "      <td>-0.411085</td>\n",
              "      <td>-1.622847</td>\n",
              "      <td>-0.169560</td>\n",
              "      <td>-0.924677</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>399996</th>\n",
              "      <td>-0.397840</td>\n",
              "      <td>-0.666695</td>\n",
              "      <td>2.624063</td>\n",
              "      <td>-0.044149</td>\n",
              "      <td>0.579960</td>\n",
              "      <td>0.805623</td>\n",
              "      <td>-0.138129</td>\n",
              "      <td>0.687243</td>\n",
              "      <td>-0.021423</td>\n",
              "      <td>-0.123943</td>\n",
              "      <td>-0.021885</td>\n",
              "      <td>0.951762</td>\n",
              "      <td>0.174580</td>\n",
              "      <td>-0.587782</td>\n",
              "      <td>-0.282347</td>\n",
              "      <td>0.306337</td>\n",
              "      <td>0.531939</td>\n",
              "      <td>-0.241805</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>399997</th>\n",
              "      <td>0.902168</td>\n",
              "      <td>0.996457</td>\n",
              "      <td>-0.018564</td>\n",
              "      <td>0.039366</td>\n",
              "      <td>-0.379315</td>\n",
              "      <td>-0.566414</td>\n",
              "      <td>-0.129758</td>\n",
              "      <td>-0.612674</td>\n",
              "      <td>-0.184157</td>\n",
              "      <td>-0.964869</td>\n",
              "      <td>-0.177727</td>\n",
              "      <td>0.321906</td>\n",
              "      <td>0.063873</td>\n",
              "      <td>0.617819</td>\n",
              "      <td>-0.024870</td>\n",
              "      <td>-0.507120</td>\n",
              "      <td>0.204746</td>\n",
              "      <td>0.189125</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>399998</th>\n",
              "      <td>0.902168</td>\n",
              "      <td>0.996457</td>\n",
              "      <td>-0.733731</td>\n",
              "      <td>-0.333787</td>\n",
              "      <td>-0.360708</td>\n",
              "      <td>-1.045660</td>\n",
              "      <td>-0.155546</td>\n",
              "      <td>-0.612674</td>\n",
              "      <td>-0.191906</td>\n",
              "      <td>1.557909</td>\n",
              "      <td>-0.177727</td>\n",
              "      <td>0.321906</td>\n",
              "      <td>2.463511</td>\n",
              "      <td>0.617819</td>\n",
              "      <td>-0.411085</td>\n",
              "      <td>0.817979</td>\n",
              "      <td>0.102077</td>\n",
              "      <td>-0.132376</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>399999</th>\n",
              "      <td>-1.697847</td>\n",
              "      <td>-1.498271</td>\n",
              "      <td>-0.943048</td>\n",
              "      <td>-0.333787</td>\n",
              "      <td>-0.273877</td>\n",
              "      <td>-0.449222</td>\n",
              "      <td>-0.164536</td>\n",
              "      <td>-0.612674</td>\n",
              "      <td>-0.191906</td>\n",
              "      <td>-0.331127</td>\n",
              "      <td>-0.169640</td>\n",
              "      <td>-2.197521</td>\n",
              "      <td>-0.459738</td>\n",
              "      <td>0.617819</td>\n",
              "      <td>1.262513</td>\n",
              "      <td>-3.542297</td>\n",
              "      <td>0.828494</td>\n",
              "      <td>0.740512</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>400000 rows × 18 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-74137810-4718-47cf-9543-fc93ec386a30')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-74137810-4718-47cf-9543-fc93ec386a30 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-74137810-4718-47cf-9543-fc93ec386a30');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "training = std_train_df.iloc[: , 0:18]\n",
        "testing = std_test_df.iloc[: , 0:18]\n",
        "labelsForTrain=training_withaim.iloc[: , -1]\n",
        "labelsForTest=testing_withaim.iloc[: , -1]\n",
        "input_shape = training.shape"
      ],
      "metadata": {
        "id": "5rblUnW1VXFc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QvKzHGkaWDzX",
        "outputId": "8000d27b-e9e6-4e2f-f321-09cbb228cb30"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(400000, 18)"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def create_model(activation, num_hidden_layer, num_hidden_unit):\n",
        "  inputs = Input(shape=(training.shape[1],))\n",
        "  model = inputs\n",
        "  for i in range(1,num_hidden_layer):\n",
        "    model = Dense(num_hidden_unit, activation=activation,)(model)\n",
        "        \n",
        "        \n",
        "  model = Dense(1,)(model)\n",
        "  model = Model(inputs, model)\n",
        "\n",
        "  return model"
      ],
      "metadata": {
        "id": "p3kN7ae7O-Pt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_model_cnn(activation, num_hidden_layer, num_hidden_unit, kernel_size, filter):\n",
        "  model_cnn = Sequential()\n",
        "  model_cnn.add(Conv1D(filter, kernel_size, activation=activation, input_shape=(18,1)))\n",
        "  model_cnn.add(MaxPooling1D())\n",
        "  model_cnn.add(Flatten())\n",
        "  for i in range(1,num_hidden_layer):\n",
        "    model_cnn.add(Dense(num_hidden_unit, activation=activation,))\n",
        "\n",
        "  model_cnn.add(Dense(1))\n",
        "\n",
        "  return model_cnn"
      ],
      "metadata": {
        "id": "H1iEIoI9WcY8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def objective_cnn(trial):\n",
        "  K.clear_session()\n",
        "    \n",
        "  activation = trial.suggest_categorical('activation',['relu','tanh','linear'])\n",
        "  optimizer = trial.suggest_categorical('optimizer',['adam','rmsprop','adagrad', 'sgd'])\n",
        "    \n",
        "  num_hidden_layer = trial.suggest_int('num_hidden_layer',1,5)\n",
        "  num_hidden_unit = trial.suggest_int('num_hidden_unit',10,100)\n",
        "  kernel_size = trial.suggest_int('kernel_size',1,5)\n",
        "  filter = trial.suggest_int('filter',1,100)\n",
        "    \n",
        "  learning_rate = trial.suggest_loguniform('learning_rate', 0.00001,0.1)\n",
        "  if optimizer == 'adam':\n",
        "    optimizer = Adam(learning_rate=learning_rate)\n",
        "  elif optimizer == 'adagrad':\n",
        "    optimizer = Adagrad(learning_rate=learning_rate)\n",
        "  elif optimizer =='rmsprop':\n",
        "    optimizer = RMSprop(learning_rate=learning_rate)\n",
        "  elif optimizer =='sgd':\n",
        "    optimizer = SGD(learning_rate=learning_rate)\n",
        "    \n",
        "  num_folds = 3\n",
        "  kfold=KFold(n_splits=3,shuffle=True)\n",
        "  fold_no=1\n",
        "  loss_per_fold = []\n",
        "  es = EarlyStopping(monitor='mse', patience=50)\n",
        "  model = create_model_cnn(activation, num_hidden_layer, num_hidden_unit, kernel_size, filter)\n",
        "  model_list_cnn.append(model)\n",
        "  model.compile(loss='mse',optimizer=optimizer,metrics=['mse','mae'])\n",
        "  for train,test in kfold.split(training,labelsForTrain):\n",
        "\n",
        "    print('------------------------------------------------------------------------')\n",
        "    print(f'Training for fold {fold_no} ...')\n",
        "  \n",
        "    # Fit data to model\n",
        "    history = model.fit(training, labelsForTrain,\n",
        "                batch_size=20,\n",
        "                epochs=10,\n",
        "                verbose=2,\n",
        "                callbacks=[es])\n",
        "    scores=model.evaluate(testing,labelsForTest,verbose=0)\n",
        "    print(f'Score for fold {fold_no}: {model.metrics_names[0]} of {scores[0]}')\n",
        "    loss_per_fold.append(scores[0])\n",
        "    # Increase fold number\n",
        "    fold_no = fold_no + 1  \n",
        "  \n",
        "  history_list_cnn.append(history)\n",
        "    \n",
        "  mse = np.array(history.history['mse'])\n",
        "    \n",
        "  return mse[-1]"
      ],
      "metadata": {
        "id": "VDfPgkNyWfoW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def objective(trial):\n",
        "  K.clear_session()\n",
        "    \n",
        "  activation = trial.suggest_categorical('activation',['relu','tanh','linear'])\n",
        "  optimizer = trial.suggest_categorical('optimizer',['adam','rmsprop','adagrad', 'sgd'])\n",
        "    \n",
        "  num_hidden_layer = trial.suggest_int('num_hidden_layer',1,3)\n",
        "  #get more features per layer, add num of hidden unit if have time\n",
        "  num_hidden_unit = trial.suggest_int('num_hidden_unit',100,2000)\n",
        "    \n",
        "  learning_rate = trial.suggest_loguniform('learning_rate', 0.00001,0.1)\n",
        "  if optimizer == 'adam':\n",
        "    optimizer = Adam(learning_rate=learning_rate)\n",
        "  elif optimizer == 'adagrad':\n",
        "    optimizer = Adagrad(learning_rate=learning_rate)\n",
        "  elif optimizer =='rmsprop':\n",
        "    optimizer = RMSprop(learning_rate=learning_rate)\n",
        "  elif optimizer =='sgd':\n",
        "    optimizer = SGD(learning_rate=learning_rate)\n",
        "    \n",
        "  num_folds = 3\n",
        "  kfold=KFold(n_splits=3,shuffle=True)\n",
        "  fold_no=1\n",
        "  loss_per_fold = []\n",
        "  es = EarlyStopping(monitor='mse', patience=50)\n",
        "  model = create_model(activation, num_hidden_layer, num_hidden_unit)\n",
        "  model_list.append(model)\n",
        "  model.compile(loss='mse',optimizer=optimizer,metrics=['mse','mae'])\n",
        "  for train,test in kfold.split(training,labelsForTrain):\n",
        "\n",
        "    print('------------------------------------------------------------------------')\n",
        "    print(f'Training for fold {fold_no} ...')\n",
        "  \n",
        "    # Fit data to model\n",
        "    history = model.fit(training, labelsForTrain,\n",
        "                batch_size=20,\n",
        "                epochs=10,\n",
        "                verbose=2,\n",
        "                callbacks=[es])\n",
        "    scores=model.evaluate(testing,labelsForTest,verbose=0)\n",
        "    print(f'Score for fold {fold_no}: {model.metrics_names[0]} of {scores[0]}')\n",
        "    loss_per_fold.append(scores[0])\n",
        "    # Increase fold number\n",
        "    fold_no = fold_no + 1  \n",
        "  \n",
        "  history_list.append(history)\n",
        "    \n",
        "  mse = np.array(history.history['mse'])\n",
        "    \n",
        "  return mse[-1]"
      ],
      "metadata": {
        "id": "bDu0tWUvWgyF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_list_cnn=[]\n",
        "history_list_cnn=[]\n",
        "study_name = 'CNN_study'\n",
        "study = optuna.create_study(study_name=study_name, load_if_exists=True)\n",
        "study.optimize(objective_cnn, n_trials=50, )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hPSkjO23Wj5p",
        "outputId": "424c26a2-1e32-498c-fc9d-df41e3633f46"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 39s - loss: 18.1846 - mse: 18.1846 - mae: 1.6748 - 39s/epoch - 2ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 39s - loss: 18.1968 - mse: 18.1968 - mae: 1.6916 - 39s/epoch - 2ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 41s - loss: 17.9370 - mse: 17.9370 - mae: 1.6810 - 41s/epoch - 2ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 38s - loss: 17.7152 - mse: 17.7152 - mae: 1.6856 - 38s/epoch - 2ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 38s - loss: 17.9484 - mse: 17.9484 - mae: 1.6666 - 38s/epoch - 2ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 38s - loss: 17.6137 - mse: 17.6137 - mae: 1.6608 - 38s/epoch - 2ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 37s - loss: 17.6848 - mse: 17.6848 - mae: 1.6780 - 37s/epoch - 2ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 37s - loss: 17.8561 - mse: 17.8561 - mae: 1.6794 - 37s/epoch - 2ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 36s - loss: 18.0299 - mse: 18.0299 - mae: 1.7310 - 36s/epoch - 2ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 37s - loss: 18.4054 - mse: 18.4054 - mae: 1.7309 - 37s/epoch - 2ms/step\n",
            "Score for fold 1: loss of 25.36672592163086\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 37s - loss: 17.9831 - mse: 17.9831 - mae: 1.7217 - 37s/epoch - 2ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 37s - loss: 18.0957 - mse: 18.0957 - mae: 1.7268 - 37s/epoch - 2ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 37s - loss: 18.3610 - mse: 18.3610 - mae: 1.7569 - 37s/epoch - 2ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 37s - loss: 18.4121 - mse: 18.4121 - mae: 1.7804 - 37s/epoch - 2ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 37s - loss: 19.2244 - mse: 19.2244 - mae: 1.8138 - 37s/epoch - 2ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 37s - loss: 18.7338 - mse: 18.7338 - mae: 1.7806 - 37s/epoch - 2ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 36s - loss: 19.0049 - mse: 19.0049 - mae: 1.8174 - 36s/epoch - 2ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 36s - loss: 18.8103 - mse: 18.8103 - mae: 1.7884 - 36s/epoch - 2ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 36s - loss: 18.8508 - mse: 18.8508 - mae: 1.7836 - 36s/epoch - 2ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 36s - loss: 18.9176 - mse: 18.9176 - mae: 1.8104 - 36s/epoch - 2ms/step\n",
            "Score for fold 2: loss of 13.956217765808105\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 36s - loss: 19.4610 - mse: 19.4610 - mae: 1.8388 - 36s/epoch - 2ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 36s - loss: 20.3769 - mse: 20.3769 - mae: 1.8884 - 36s/epoch - 2ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 36s - loss: 19.1540 - mse: 19.1540 - mae: 1.8033 - 36s/epoch - 2ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 36s - loss: 20.1172 - mse: 20.1172 - mae: 1.8763 - 36s/epoch - 2ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 36s - loss: 19.1629 - mse: 19.1629 - mae: 1.8361 - 36s/epoch - 2ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 36s - loss: 20.0430 - mse: 20.0430 - mae: 1.8656 - 36s/epoch - 2ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 36s - loss: 19.4236 - mse: 19.4236 - mae: 1.8391 - 36s/epoch - 2ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 36s - loss: 19.7096 - mse: 19.7096 - mae: 1.8486 - 36s/epoch - 2ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 35s - loss: 19.1369 - mse: 19.1369 - mae: 1.8329 - 35s/epoch - 2ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 36s - loss: 19.5105 - mse: 19.5105 - mae: 1.8471 - 36s/epoch - 2ms/step\n",
            "Score for fold 3: loss of 15.548145294189453\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-09-30 03:23:48,408]\u001b[0m Finished trial#0 resulted in value: 19.510541915893555. Current best value is 19.510541915893555 with parameters: {'activation': 'tanh', 'optimizer': 'adam', 'num_hidden_layer': 5, 'num_hidden_unit': 35, 'kernel_size': 5, 'filter': 60, 'learning_rate': 0.004538346443656373}.\u001b[0m\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 29s - loss: 18.1093 - mse: 18.1093 - mae: 1.6580 - 29s/epoch - 1ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 28s - loss: 17.7154 - mse: 17.7154 - mae: 1.6366 - 28s/epoch - 1ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 29s - loss: 17.5829 - mse: 17.5829 - mae: 1.6334 - 29s/epoch - 1ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 29s - loss: 17.8670 - mse: 17.8670 - mae: 1.6332 - 29s/epoch - 1ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 28s - loss: 17.5546 - mse: 17.5546 - mae: 1.6355 - 28s/epoch - 1ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 29s - loss: 17.6939 - mse: 17.6939 - mae: 1.6335 - 29s/epoch - 1ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 29s - loss: 17.4680 - mse: 17.4680 - mae: 1.6288 - 29s/epoch - 1ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 28s - loss: 17.9172 - mse: 17.9172 - mae: 1.6347 - 28s/epoch - 1ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 28s - loss: 17.5001 - mse: 17.5001 - mae: 1.6333 - 28s/epoch - 1ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 28s - loss: 19.1402 - mse: 19.1402 - mae: 1.6378 - 28s/epoch - 1ms/step\n",
            "Score for fold 1: loss of 23.236820220947266\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 27s - loss: 17.5051 - mse: 17.5051 - mae: 1.6307 - 27s/epoch - 1ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 27s - loss: 17.5873 - mse: 17.5873 - mae: 1.6360 - 27s/epoch - 1ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 28s - loss: 17.8733 - mse: 17.8733 - mae: 1.6361 - 28s/epoch - 1ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 28s - loss: 17.5823 - mse: 17.5823 - mae: 1.6358 - 28s/epoch - 1ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 28s - loss: 17.5613 - mse: 17.5613 - mae: 1.6348 - 28s/epoch - 1ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 29s - loss: 17.6490 - mse: 17.6490 - mae: 1.6332 - 29s/epoch - 1ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 29s - loss: 17.6356 - mse: 17.6356 - mae: 1.6337 - 29s/epoch - 1ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 27s - loss: 17.8235 - mse: 17.8235 - mae: 1.6352 - 27s/epoch - 1ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 28s - loss: 17.6647 - mse: 17.6647 - mae: 1.6376 - 28s/epoch - 1ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 27s - loss: 17.6882 - mse: 17.6882 - mae: 1.6375 - 27s/epoch - 1ms/step\n",
            "Score for fold 2: loss of 12.43328857421875\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 27s - loss: 17.9548 - mse: 17.9548 - mae: 1.6309 - 27s/epoch - 1ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 28s - loss: 17.6919 - mse: 17.6919 - mae: 1.6348 - 28s/epoch - 1ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 28s - loss: 17.6847 - mse: 17.6847 - mae: 1.6351 - 28s/epoch - 1ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 28s - loss: 17.4210 - mse: 17.4210 - mae: 1.6325 - 28s/epoch - 1ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 28s - loss: 17.5600 - mse: 17.5600 - mae: 1.6371 - 28s/epoch - 1ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 28s - loss: 17.6582 - mse: 17.6582 - mae: 1.6342 - 28s/epoch - 1ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 27s - loss: 17.5296 - mse: 17.5296 - mae: 1.6333 - 27s/epoch - 1ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 28s - loss: 17.6474 - mse: 17.6474 - mae: 1.6364 - 28s/epoch - 1ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 28s - loss: 17.6299 - mse: 17.6299 - mae: 1.6362 - 28s/epoch - 1ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 28s - loss: 17.7411 - mse: 17.7411 - mae: 1.6394 - 28s/epoch - 1ms/step\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-09-30 03:37:56,692]\u001b[0m Finished trial#1 resulted in value: 17.741100311279297. Current best value is 17.741100311279297 with parameters: {'activation': 'linear', 'optimizer': 'rmsprop', 'num_hidden_layer': 5, 'num_hidden_unit': 96, 'kernel_size': 2, 'filter': 14, 'learning_rate': 0.004102079349565866}.\u001b[0m\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Score for fold 3: loss of 12.804410934448242\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 29s - loss: 19.9669 - mse: 19.9669 - mae: 1.7517 - 29s/epoch - 1ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 28s - loss: 18.9033 - mse: 18.9033 - mae: 1.6926 - 28s/epoch - 1ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 29s - loss: 18.6682 - mse: 18.6682 - mae: 1.6692 - 29s/epoch - 1ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 29s - loss: 18.5256 - mse: 18.5256 - mae: 1.6599 - 29s/epoch - 1ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 30s - loss: 18.4173 - mse: 18.4173 - mae: 1.6531 - 30s/epoch - 1ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 30s - loss: 18.3251 - mse: 18.3251 - mae: 1.6463 - 30s/epoch - 2ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 30s - loss: 18.2454 - mse: 18.2454 - mae: 1.6404 - 30s/epoch - 1ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 30s - loss: 18.1761 - mse: 18.1761 - mae: 1.6351 - 30s/epoch - 2ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 31s - loss: 18.1150 - mse: 18.1150 - mae: 1.6352 - 31s/epoch - 2ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 30s - loss: 18.0596 - mse: 18.0596 - mae: 1.6330 - 30s/epoch - 2ms/step\n",
            "Score for fold 1: loss of 23.6226749420166\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 29s - loss: 18.0095 - mse: 18.0095 - mae: 1.6297 - 29s/epoch - 1ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 30s - loss: 17.9636 - mse: 17.9636 - mae: 1.6276 - 30s/epoch - 1ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 29s - loss: 17.9210 - mse: 17.9210 - mae: 1.6283 - 29s/epoch - 1ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 30s - loss: 17.8814 - mse: 17.8814 - mae: 1.6246 - 30s/epoch - 2ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 30s - loss: 17.8441 - mse: 17.8441 - mae: 1.6247 - 30s/epoch - 2ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 29s - loss: 17.8091 - mse: 17.8091 - mae: 1.6238 - 29s/epoch - 1ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 29s - loss: 17.7759 - mse: 17.7759 - mae: 1.6224 - 29s/epoch - 1ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 28s - loss: 17.7443 - mse: 17.7443 - mae: 1.6218 - 28s/epoch - 1ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 27s - loss: 17.7137 - mse: 17.7137 - mae: 1.6187 - 27s/epoch - 1ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 28s - loss: 17.6838 - mse: 17.6838 - mae: 1.6198 - 28s/epoch - 1ms/step\n",
            "Score for fold 2: loss of 13.336681365966797\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 28s - loss: 17.6554 - mse: 17.6554 - mae: 1.6189 - 28s/epoch - 1ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 28s - loss: 17.6279 - mse: 17.6279 - mae: 1.6191 - 28s/epoch - 1ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 28s - loss: 17.6014 - mse: 17.6014 - mae: 1.6179 - 28s/epoch - 1ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 28s - loss: 17.5759 - mse: 17.5759 - mae: 1.6162 - 28s/epoch - 1ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 28s - loss: 17.5514 - mse: 17.5514 - mae: 1.6154 - 28s/epoch - 1ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 27s - loss: 17.5276 - mse: 17.5276 - mae: 1.6161 - 27s/epoch - 1ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 28s - loss: 17.5050 - mse: 17.5050 - mae: 1.6160 - 28s/epoch - 1ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 28s - loss: 17.4838 - mse: 17.4838 - mae: 1.6122 - 28s/epoch - 1ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 28s - loss: 17.4637 - mse: 17.4637 - mae: 1.6136 - 28s/epoch - 1ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 27s - loss: 17.4449 - mse: 17.4449 - mae: 1.6136 - 27s/epoch - 1ms/step\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-09-30 03:52:31,628]\u001b[0m Finished trial#2 resulted in value: 17.444852828979492. Current best value is 17.444852828979492 with parameters: {'activation': 'relu', 'optimizer': 'adagrad', 'num_hidden_layer': 5, 'num_hidden_unit': 20, 'kernel_size': 1, 'filter': 54, 'learning_rate': 0.0002886793263424461}.\u001b[0m\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Score for fold 3: loss of 12.959259033203125\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 43s - loss: 23.7443 - mse: 23.7443 - mae: 1.8900 - 43s/epoch - 2ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 43s - loss: 20.4917 - mse: 20.4917 - mae: 1.6693 - 43s/epoch - 2ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 42s - loss: 19.9146 - mse: 19.9146 - mae: 1.7378 - 42s/epoch - 2ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 41s - loss: 19.6980 - mse: 19.6980 - mae: 1.7564 - 41s/epoch - 2ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 40s - loss: 19.5529 - mse: 19.5529 - mae: 1.7537 - 40s/epoch - 2ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 41s - loss: 19.4369 - mse: 19.4369 - mae: 1.7455 - 41s/epoch - 2ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 41s - loss: 19.3402 - mse: 19.3402 - mae: 1.7386 - 41s/epoch - 2ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 41s - loss: 19.2574 - mse: 19.2574 - mae: 1.7308 - 41s/epoch - 2ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 41s - loss: 19.1855 - mse: 19.1855 - mae: 1.7239 - 41s/epoch - 2ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 40s - loss: 19.1214 - mse: 19.1214 - mae: 1.7167 - 40s/epoch - 2ms/step\n",
            "Score for fold 1: loss of 23.847829818725586\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 40s - loss: 19.0642 - mse: 19.0642 - mae: 1.7084 - 40s/epoch - 2ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 41s - loss: 19.0131 - mse: 19.0131 - mae: 1.7042 - 41s/epoch - 2ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 42s - loss: 18.9671 - mse: 18.9671 - mae: 1.6989 - 42s/epoch - 2ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 42s - loss: 18.9249 - mse: 18.9249 - mae: 1.6948 - 42s/epoch - 2ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 42s - loss: 18.8862 - mse: 18.8862 - mae: 1.6906 - 42s/epoch - 2ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 41s - loss: 18.8504 - mse: 18.8504 - mae: 1.6865 - 41s/epoch - 2ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 41s - loss: 18.8169 - mse: 18.8169 - mae: 1.6825 - 41s/epoch - 2ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 42s - loss: 18.7856 - mse: 18.7856 - mae: 1.6798 - 42s/epoch - 2ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 42s - loss: 18.7561 - mse: 18.7561 - mae: 1.6764 - 42s/epoch - 2ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 42s - loss: 18.7283 - mse: 18.7283 - mae: 1.6738 - 42s/epoch - 2ms/step\n",
            "Score for fold 2: loss of 14.452120780944824\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 42s - loss: 18.7020 - mse: 18.7020 - mae: 1.6708 - 42s/epoch - 2ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 43s - loss: 18.6772 - mse: 18.6772 - mae: 1.6683 - 43s/epoch - 2ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 42s - loss: 18.6535 - mse: 18.6535 - mae: 1.6664 - 42s/epoch - 2ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 42s - loss: 18.6309 - mse: 18.6309 - mae: 1.6650 - 42s/epoch - 2ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 41s - loss: 18.6094 - mse: 18.6094 - mae: 1.6615 - 41s/epoch - 2ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 41s - loss: 18.5886 - mse: 18.5886 - mae: 1.6600 - 41s/epoch - 2ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 41s - loss: 18.5686 - mse: 18.5686 - mae: 1.6583 - 41s/epoch - 2ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 41s - loss: 18.5491 - mse: 18.5491 - mae: 1.6563 - 41s/epoch - 2ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 41s - loss: 18.5303 - mse: 18.5303 - mae: 1.6558 - 41s/epoch - 2ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 41s - loss: 18.5120 - mse: 18.5120 - mae: 1.6534 - 41s/epoch - 2ms/step\n",
            "Score for fold 3: loss of 14.056706428527832\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-09-30 04:13:28,413]\u001b[0m Finished trial#3 resulted in value: 18.51199722290039. Current best value is 17.444852828979492 with parameters: {'activation': 'relu', 'optimizer': 'adagrad', 'num_hidden_layer': 5, 'num_hidden_unit': 20, 'kernel_size': 1, 'filter': 54, 'learning_rate': 0.0002886793263424461}.\u001b[0m\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 33s - loss: nan - mse: nan - mae: nan - 33s/epoch - 2ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 33s - loss: nan - mse: nan - mae: nan - 33s/epoch - 2ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 33s - loss: nan - mse: nan - mae: nan - 33s/epoch - 2ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 34s - loss: nan - mse: nan - mae: nan - 34s/epoch - 2ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 32s - loss: nan - mse: nan - mae: nan - 32s/epoch - 2ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 34s - loss: nan - mse: nan - mae: nan - 34s/epoch - 2ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 33s - loss: nan - mse: nan - mae: nan - 33s/epoch - 2ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 33s - loss: nan - mse: nan - mae: nan - 33s/epoch - 2ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 33s - loss: nan - mse: nan - mae: nan - 33s/epoch - 2ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 33s - loss: nan - mse: nan - mae: nan - 33s/epoch - 2ms/step\n",
            "Score for fold 1: loss of 24.599163055419922\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 33s - loss: nan - mse: nan - mae: nan - 33s/epoch - 2ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 32s - loss: nan - mse: nan - mae: nan - 32s/epoch - 2ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 32s - loss: nan - mse: nan - mae: nan - 32s/epoch - 2ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 33s - loss: nan - mse: nan - mae: nan - 33s/epoch - 2ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 33s - loss: nan - mse: nan - mae: nan - 33s/epoch - 2ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 32s - loss: nan - mse: nan - mae: nan - 32s/epoch - 2ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 33s - loss: nan - mse: nan - mae: nan - 33s/epoch - 2ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 32s - loss: nan - mse: nan - mae: nan - 32s/epoch - 2ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 33s - loss: nan - mse: nan - mae: nan - 33s/epoch - 2ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 32s - loss: nan - mse: nan - mae: nan - 32s/epoch - 2ms/step\n",
            "Score for fold 2: loss of nan\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 32s - loss: nan - mse: nan - mae: nan - 32s/epoch - 2ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 33s - loss: nan - mse: nan - mae: nan - 33s/epoch - 2ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 32s - loss: nan - mse: nan - mae: nan - 32s/epoch - 2ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 33s - loss: nan - mse: nan - mae: nan - 33s/epoch - 2ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 34s - loss: nan - mse: nan - mae: nan - 34s/epoch - 2ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 33s - loss: nan - mse: nan - mae: nan - 33s/epoch - 2ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 32s - loss: nan - mse: nan - mae: nan - 32s/epoch - 2ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 33s - loss: nan - mse: nan - mae: nan - 33s/epoch - 2ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 33s - loss: nan - mse: nan - mae: nan - 33s/epoch - 2ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 32s - loss: nan - mse: nan - mae: nan - 32s/epoch - 2ms/step\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[33m[W 2022-09-30 04:30:01,080]\u001b[0m Setting status of trial#4 as TrialState.FAIL because the objective function returned nan.\u001b[0m\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Score for fold 3: loss of nan\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 34s - loss: 23707.2598 - mse: 23707.2598 - mae: 9.1901 - 34s/epoch - 2ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 34s - loss: 517241.6250 - mse: 517241.6250 - mae: 36.9493 - 34s/epoch - 2ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 34s - loss: 23368.9668 - mse: 23368.9668 - mae: 9.1042 - 34s/epoch - 2ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 34s - loss: 45231.0273 - mse: 45231.0273 - mae: 16.0267 - 34s/epoch - 2ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 34s - loss: 104860.2109 - mse: 104860.2109 - mae: 23.4389 - 34s/epoch - 2ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 34s - loss: 2743.0671 - mse: 2743.0671 - mae: 5.4888 - 34s/epoch - 2ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 34s - loss: 15259.8154 - mse: 15259.8154 - mae: 10.2614 - 34s/epoch - 2ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 34s - loss: 6728.3394 - mse: 6728.3394 - mae: 6.8161 - 34s/epoch - 2ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 34s - loss: 67151.9688 - mse: 67151.9688 - mae: 18.6490 - 34s/epoch - 2ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 34s - loss: 64645.1719 - mse: 64645.1719 - mae: 13.5229 - 34s/epoch - 2ms/step\n",
            "Score for fold 1: loss of 23.041975021362305\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 34s - loss: 679685.3125 - mse: 679685.3125 - mae: 53.4278 - 34s/epoch - 2ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 34s - loss: 116943.2969 - mse: 116943.2969 - mae: 23.8667 - 34s/epoch - 2ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 34s - loss: 11077.7080 - mse: 11077.7080 - mae: 9.6813 - 34s/epoch - 2ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 34s - loss: 13953.3096 - mse: 13953.3086 - mae: 8.9985 - 34s/epoch - 2ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 34s - loss: 251352.0469 - mse: 251352.0469 - mae: 33.5864 - 34s/epoch - 2ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 34s - loss: 158339.1406 - mse: 158339.1406 - mae: 31.3378 - 34s/epoch - 2ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 34s - loss: 25365.8887 - mse: 25365.8887 - mae: 15.6258 - 34s/epoch - 2ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 34s - loss: 21701.0664 - mse: 21701.0664 - mae: 11.5714 - 34s/epoch - 2ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 34s - loss: 13733.4775 - mse: 13733.4775 - mae: 8.5167 - 34s/epoch - 2ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 34s - loss: 14803.8105 - mse: 14803.8105 - mae: 10.0549 - 34s/epoch - 2ms/step\n",
            "Score for fold 2: loss of 20.017545700073242\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 34s - loss: 8352.1279 - mse: 8352.1279 - mae: 6.7632 - 34s/epoch - 2ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 34s - loss: 29268.4551 - mse: 29268.4551 - mae: 13.7519 - 34s/epoch - 2ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 35s - loss: 54813.3438 - mse: 54813.3438 - mae: 15.1766 - 35s/epoch - 2ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 34s - loss: 12058.0771 - mse: 12058.0771 - mae: 8.8205 - 34s/epoch - 2ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 34s - loss: 275611.7812 - mse: 275611.7812 - mae: 36.5977 - 34s/epoch - 2ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 34s - loss: 58948.2031 - mse: 58948.2031 - mae: 17.3195 - 34s/epoch - 2ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 34s - loss: 44595.0820 - mse: 44595.0820 - mae: 16.6128 - 34s/epoch - 2ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 34s - loss: 25842.8027 - mse: 25842.8027 - mae: 12.1169 - 34s/epoch - 2ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 34s - loss: 5365.7930 - mse: 5365.7930 - mae: 6.5479 - 34s/epoch - 2ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 34s - loss: 49720.6562 - mse: 49720.6562 - mae: 16.6892 - 34s/epoch - 2ms/step\n",
            "Score for fold 3: loss of 14.110597610473633\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-09-30 04:47:12,020]\u001b[0m Finished trial#5 resulted in value: 49720.65625. Current best value is 17.444852828979492 with parameters: {'activation': 'relu', 'optimizer': 'adagrad', 'num_hidden_layer': 5, 'num_hidden_unit': 20, 'kernel_size': 1, 'filter': 54, 'learning_rate': 0.0002886793263424461}.\u001b[0m\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 23s - loss: 18.3157 - mse: 18.3157 - mae: 1.6158 - 23s/epoch - 1ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 23s - loss: 17.4108 - mse: 17.4108 - mae: 1.5803 - 23s/epoch - 1ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 23s - loss: 17.1841 - mse: 17.1841 - mae: 1.5723 - 23s/epoch - 1ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 23s - loss: 17.0497 - mse: 17.0497 - mae: 1.5652 - 23s/epoch - 1ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 23s - loss: 16.9525 - mse: 16.9525 - mae: 1.5585 - 23s/epoch - 1ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 23s - loss: 16.8597 - mse: 16.8597 - mae: 1.5518 - 23s/epoch - 1ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 23s - loss: 16.7863 - mse: 16.7863 - mae: 1.5463 - 23s/epoch - 1ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 22s - loss: 16.7303 - mse: 16.7303 - mae: 1.5427 - 22s/epoch - 1ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 28s - loss: 16.6722 - mse: 16.6722 - mae: 1.5397 - 28s/epoch - 1ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 27s - loss: 16.6527 - mse: 16.6527 - mae: 1.5378 - 27s/epoch - 1ms/step\n",
            "Score for fold 1: loss of 24.213401794433594\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 25s - loss: 16.6170 - mse: 16.6170 - mae: 1.5363 - 25s/epoch - 1ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 25s - loss: 16.5841 - mse: 16.5841 - mae: 1.5343 - 25s/epoch - 1ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 24s - loss: 16.5510 - mse: 16.5510 - mae: 1.5339 - 24s/epoch - 1ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 23s - loss: 16.5344 - mse: 16.5344 - mae: 1.5322 - 23s/epoch - 1ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 22s - loss: 16.5040 - mse: 16.5040 - mae: 1.5318 - 22s/epoch - 1ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 22s - loss: 16.4861 - mse: 16.4861 - mae: 1.5307 - 22s/epoch - 1ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 22s - loss: 16.4586 - mse: 16.4586 - mae: 1.5297 - 22s/epoch - 1ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 22s - loss: 16.4304 - mse: 16.4304 - mae: 1.5297 - 22s/epoch - 1ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 23s - loss: 16.4119 - mse: 16.4119 - mae: 1.5289 - 23s/epoch - 1ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 22s - loss: 16.3936 - mse: 16.3936 - mae: 1.5272 - 22s/epoch - 1ms/step\n",
            "Score for fold 2: loss of 12.318285942077637\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 23s - loss: 16.3707 - mse: 16.3707 - mae: 1.5266 - 23s/epoch - 1ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 22s - loss: 16.3601 - mse: 16.3601 - mae: 1.5260 - 22s/epoch - 1ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 22s - loss: 16.3442 - mse: 16.3442 - mae: 1.5269 - 22s/epoch - 1ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 23s - loss: 16.3420 - mse: 16.3420 - mae: 1.5259 - 23s/epoch - 1ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 23s - loss: 16.3348 - mse: 16.3348 - mae: 1.5256 - 23s/epoch - 1ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 23s - loss: 16.3379 - mse: 16.3379 - mae: 1.5257 - 23s/epoch - 1ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 23s - loss: 16.3362 - mse: 16.3362 - mae: 1.5259 - 23s/epoch - 1ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 23s - loss: 16.3249 - mse: 16.3249 - mae: 1.5272 - 23s/epoch - 1ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 22s - loss: 16.3197 - mse: 16.3197 - mae: 1.5270 - 22s/epoch - 1ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 22s - loss: 16.2988 - mse: 16.2988 - mae: 1.5275 - 22s/epoch - 1ms/step\n",
            "Score for fold 3: loss of 12.039624214172363\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-09-30 04:58:56,594]\u001b[0m Finished trial#6 resulted in value: 16.298831939697266. Current best value is 16.298831939697266 with parameters: {'activation': 'tanh', 'optimizer': 'rmsprop', 'num_hidden_layer': 1, 'num_hidden_unit': 85, 'kernel_size': 3, 'filter': 41, 'learning_rate': 0.0001157368746299737}.\u001b[0m\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 24s - loss: 17.0585 - mse: 17.0585 - mae: 1.5826 - 24s/epoch - 1ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 24s - loss: 16.7975 - mse: 16.7975 - mae: 1.5697 - 24s/epoch - 1ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 24s - loss: 16.6752 - mse: 16.6752 - mae: 1.5677 - 24s/epoch - 1ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 24s - loss: 16.6036 - mse: 16.6036 - mae: 1.5660 - 24s/epoch - 1ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 25s - loss: 16.5738 - mse: 16.5738 - mae: 1.5645 - 25s/epoch - 1ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 24s - loss: 16.5447 - mse: 16.5447 - mae: 1.5647 - 24s/epoch - 1ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 24s - loss: 16.5153 - mse: 16.5153 - mae: 1.5656 - 24s/epoch - 1ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 24s - loss: 16.4330 - mse: 16.4330 - mae: 1.5666 - 24s/epoch - 1ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 24s - loss: 16.5386 - mse: 16.5386 - mae: 1.5662 - 24s/epoch - 1ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 25s - loss: 16.4521 - mse: 16.4521 - mae: 1.5675 - 25s/epoch - 1ms/step\n",
            "Score for fold 1: loss of 24.230140686035156\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 25s - loss: 16.4256 - mse: 16.4256 - mae: 1.5692 - 25s/epoch - 1ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 24s - loss: 16.4841 - mse: 16.4841 - mae: 1.5688 - 24s/epoch - 1ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 25s - loss: 16.3987 - mse: 16.3987 - mae: 1.5701 - 25s/epoch - 1ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 25s - loss: 16.3929 - mse: 16.3929 - mae: 1.5711 - 25s/epoch - 1ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 25s - loss: 16.3548 - mse: 16.3548 - mae: 1.5720 - 25s/epoch - 1ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 24s - loss: 16.4276 - mse: 16.4276 - mae: 1.5731 - 24s/epoch - 1ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 25s - loss: 16.3903 - mse: 16.3903 - mae: 1.5722 - 25s/epoch - 1ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 25s - loss: 16.3709 - mse: 16.3709 - mae: 1.5726 - 25s/epoch - 1ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 25s - loss: 16.3290 - mse: 16.3290 - mae: 1.5718 - 25s/epoch - 1ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 26s - loss: 16.4411 - mse: 16.4411 - mae: 1.5725 - 26s/epoch - 1ms/step\n",
            "Score for fold 2: loss of 11.810317039489746\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 26s - loss: 16.2764 - mse: 16.2764 - mae: 1.5731 - 26s/epoch - 1ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 27s - loss: 16.3162 - mse: 16.3162 - mae: 1.5742 - 27s/epoch - 1ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 27s - loss: 16.3398 - mse: 16.3398 - mae: 1.5744 - 27s/epoch - 1ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 27s - loss: 16.4030 - mse: 16.4030 - mae: 1.5751 - 27s/epoch - 1ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 27s - loss: 16.3134 - mse: 16.3134 - mae: 1.5753 - 27s/epoch - 1ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 27s - loss: 16.4523 - mse: 16.4523 - mae: 1.5787 - 27s/epoch - 1ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 27s - loss: 16.3257 - mse: 16.3257 - mae: 1.5782 - 27s/epoch - 1ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 27s - loss: 16.3350 - mse: 16.3350 - mae: 1.5798 - 27s/epoch - 1ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 27s - loss: 16.3046 - mse: 16.3046 - mae: 1.5804 - 27s/epoch - 1ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 26s - loss: 16.3594 - mse: 16.3594 - mae: 1.5802 - 26s/epoch - 1ms/step\n",
            "Score for fold 3: loss of 11.786415100097656\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-09-30 05:11:43,261]\u001b[0m Finished trial#7 resulted in value: 16.359373092651367. Current best value is 16.298831939697266 with parameters: {'activation': 'tanh', 'optimizer': 'rmsprop', 'num_hidden_layer': 1, 'num_hidden_unit': 85, 'kernel_size': 3, 'filter': 41, 'learning_rate': 0.0001157368746299737}.\u001b[0m\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 29s - loss: 17.0364 - mse: 17.0364 - mae: 1.5495 - 29s/epoch - 1ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 29s - loss: 16.4805 - mse: 16.4805 - mae: 1.5398 - 29s/epoch - 1ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 29s - loss: 16.3505 - mse: 16.3505 - mae: 1.5391 - 29s/epoch - 1ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 29s - loss: 16.1564 - mse: 16.1564 - mae: 1.5389 - 29s/epoch - 1ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 29s - loss: 16.1408 - mse: 16.1408 - mae: 1.5421 - 29s/epoch - 1ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 30s - loss: 16.1540 - mse: 16.1540 - mae: 1.5427 - 30s/epoch - 1ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 29s - loss: 16.1802 - mse: 16.1802 - mae: 1.5449 - 29s/epoch - 1ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 29s - loss: 16.1449 - mse: 16.1449 - mae: 1.5412 - 29s/epoch - 1ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 29s - loss: 16.1657 - mse: 16.1657 - mae: 1.5412 - 29s/epoch - 1ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 28s - loss: 16.1810 - mse: 16.1810 - mae: 1.5488 - 28s/epoch - 1ms/step\n",
            "Score for fold 1: loss of 22.873083114624023\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 29s - loss: 16.0885 - mse: 16.0885 - mae: 1.5514 - 29s/epoch - 1ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 29s - loss: 16.1368 - mse: 16.1368 - mae: 1.5487 - 29s/epoch - 1ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 29s - loss: 16.0383 - mse: 16.0383 - mae: 1.5459 - 29s/epoch - 1ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 29s - loss: 16.0410 - mse: 16.0410 - mae: 1.5449 - 29s/epoch - 1ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 29s - loss: 16.0543 - mse: 16.0543 - mae: 1.5439 - 29s/epoch - 1ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 29s - loss: 16.0436 - mse: 16.0436 - mae: 1.5438 - 29s/epoch - 1ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 28s - loss: 16.0657 - mse: 16.0657 - mae: 1.5441 - 28s/epoch - 1ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 29s - loss: 16.1055 - mse: 16.1055 - mae: 1.5487 - 29s/epoch - 1ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 28s - loss: 15.9861 - mse: 15.9861 - mae: 1.5452 - 28s/epoch - 1ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 28s - loss: 16.0693 - mse: 16.0693 - mae: 1.5463 - 28s/epoch - 1ms/step\n",
            "Score for fold 2: loss of 12.002470016479492\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 29s - loss: 16.0270 - mse: 16.0270 - mae: 1.5476 - 29s/epoch - 1ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 29s - loss: 16.0362 - mse: 16.0362 - mae: 1.5469 - 29s/epoch - 1ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 29s - loss: 15.9667 - mse: 15.9667 - mae: 1.5429 - 29s/epoch - 1ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 28s - loss: 15.9989 - mse: 15.9989 - mae: 1.5435 - 28s/epoch - 1ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 28s - loss: 16.0303 - mse: 16.0303 - mae: 1.5421 - 28s/epoch - 1ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 28s - loss: 15.9954 - mse: 15.9954 - mae: 1.5457 - 28s/epoch - 1ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 29s - loss: 16.0417 - mse: 16.0417 - mae: 1.5491 - 29s/epoch - 1ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 28s - loss: 15.9918 - mse: 15.9918 - mae: 1.5499 - 28s/epoch - 1ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 30s - loss: 16.0536 - mse: 16.0536 - mae: 1.5475 - 30s/epoch - 1ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 28s - loss: 15.9916 - mse: 15.9916 - mae: 1.5480 - 28s/epoch - 1ms/step\n",
            "Score for fold 3: loss of 11.647722244262695\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-09-30 05:26:16,284]\u001b[0m Finished trial#8 resulted in value: 15.991597175598145. Current best value is 15.991597175598145 with parameters: {'activation': 'tanh', 'optimizer': 'rmsprop', 'num_hidden_layer': 2, 'num_hidden_unit': 20, 'kernel_size': 5, 'filter': 73, 'learning_rate': 0.001434913997681979}.\u001b[0m\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 28s - loss: 19.3515 - mse: 19.3515 - mae: 1.6764 - 28s/epoch - 1ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 27s - loss: 18.3548 - mse: 18.3548 - mae: 1.6263 - 27s/epoch - 1ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 27s - loss: 17.7974 - mse: 17.7974 - mae: 1.5938 - 27s/epoch - 1ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 28s - loss: 17.4453 - mse: 17.4453 - mae: 1.5771 - 28s/epoch - 1ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 27s - loss: 17.2150 - mse: 17.2150 - mae: 1.5682 - 27s/epoch - 1ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 27s - loss: 17.0253 - mse: 17.0253 - mae: 1.5578 - 27s/epoch - 1ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 27s - loss: 16.8476 - mse: 16.8476 - mae: 1.5498 - 27s/epoch - 1ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 26s - loss: 16.6901 - mse: 16.6901 - mae: 1.5404 - 26s/epoch - 1ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 26s - loss: 16.5577 - mse: 16.5577 - mae: 1.5344 - 26s/epoch - 1ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 26s - loss: 16.4522 - mse: 16.4522 - mae: 1.5297 - 26s/epoch - 1ms/step\n",
            "Score for fold 1: loss of 22.717004776000977\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 27s - loss: 16.3598 - mse: 16.3598 - mae: 1.5263 - 27s/epoch - 1ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 27s - loss: 16.2823 - mse: 16.2823 - mae: 1.5226 - 27s/epoch - 1ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 26s - loss: 16.2084 - mse: 16.2084 - mae: 1.5197 - 26s/epoch - 1ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 26s - loss: 16.1420 - mse: 16.1420 - mae: 1.5171 - 26s/epoch - 1ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 27s - loss: 16.0786 - mse: 16.0786 - mae: 1.5149 - 27s/epoch - 1ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 26s - loss: 16.0271 - mse: 16.0271 - mae: 1.5123 - 26s/epoch - 1ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 26s - loss: 15.9753 - mse: 15.9753 - mae: 1.5108 - 26s/epoch - 1ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 26s - loss: 15.9312 - mse: 15.9312 - mae: 1.5082 - 26s/epoch - 1ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 26s - loss: 15.8870 - mse: 15.8870 - mae: 1.5072 - 26s/epoch - 1ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 27s - loss: 15.8481 - mse: 15.8481 - mae: 1.5048 - 27s/epoch - 1ms/step\n",
            "Score for fold 2: loss of 12.177204132080078\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 27s - loss: 15.8067 - mse: 15.8067 - mae: 1.5032 - 27s/epoch - 1ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 26s - loss: 15.7734 - mse: 15.7734 - mae: 1.5022 - 26s/epoch - 1ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 26s - loss: 15.7394 - mse: 15.7394 - mae: 1.5003 - 26s/epoch - 1ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 26s - loss: 15.7085 - mse: 15.7085 - mae: 1.4993 - 26s/epoch - 1ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 26s - loss: 15.6738 - mse: 15.6738 - mae: 1.4972 - 26s/epoch - 1ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 26s - loss: 15.6450 - mse: 15.6450 - mae: 1.4966 - 26s/epoch - 1ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 26s - loss: 15.6179 - mse: 15.6179 - mae: 1.4950 - 26s/epoch - 1ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 26s - loss: 15.5871 - mse: 15.5871 - mae: 1.4943 - 26s/epoch - 1ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 27s - loss: 15.5661 - mse: 15.5661 - mae: 1.4932 - 27s/epoch - 1ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 26s - loss: 15.5349 - mse: 15.5349 - mae: 1.4918 - 26s/epoch - 1ms/step\n",
            "Score for fold 3: loss of 11.68696403503418\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-09-30 05:39:42,596]\u001b[0m Finished trial#9 resulted in value: 15.534948348999023. Current best value is 15.534948348999023 with parameters: {'activation': 'tanh', 'optimizer': 'sgd', 'num_hidden_layer': 2, 'num_hidden_unit': 26, 'kernel_size': 2, 'filter': 58, 'learning_rate': 6.003867710257596e-05}.\u001b[0m\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 30s - loss: 17.7915 - mse: 17.7915 - mae: 1.6364 - 30s/epoch - 2ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 29s - loss: 17.0702 - mse: 17.0702 - mae: 1.6129 - 29s/epoch - 1ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 29s - loss: 16.9995 - mse: 16.9995 - mae: 1.6059 - 29s/epoch - 1ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 29s - loss: 16.9677 - mse: 16.9677 - mae: 1.6034 - 29s/epoch - 1ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 29s - loss: 16.9542 - mse: 16.9542 - mae: 1.6041 - 29s/epoch - 1ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 29s - loss: 16.8006 - mse: 16.8006 - mae: 1.5956 - 29s/epoch - 1ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 28s - loss: 16.8637 - mse: 16.8637 - mae: 1.5998 - 28s/epoch - 1ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 29s - loss: 16.8230 - mse: 16.8230 - mae: 1.5987 - 29s/epoch - 1ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 29s - loss: 16.7055 - mse: 16.7055 - mae: 1.5913 - 29s/epoch - 1ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 29s - loss: 16.6818 - mse: 16.6818 - mae: 1.5925 - 29s/epoch - 1ms/step\n",
            "Score for fold 1: loss of 24.460508346557617\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 29s - loss: 16.7965 - mse: 16.7965 - mae: 1.5917 - 29s/epoch - 1ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 29s - loss: 16.6404 - mse: 16.6404 - mae: 1.5855 - 29s/epoch - 1ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 29s - loss: 16.6089 - mse: 16.6089 - mae: 1.5880 - 29s/epoch - 1ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 28s - loss: 16.6708 - mse: 16.6708 - mae: 1.5862 - 28s/epoch - 1ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 28s - loss: 16.6018 - mse: 16.6018 - mae: 1.5887 - 28s/epoch - 1ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 29s - loss: 16.6495 - mse: 16.6495 - mae: 1.5834 - 29s/epoch - 1ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 28s - loss: 16.6945 - mse: 16.6945 - mae: 1.5874 - 28s/epoch - 1ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 28s - loss: 16.6242 - mse: 16.6242 - mae: 1.5859 - 28s/epoch - 1ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 28s - loss: 16.5863 - mse: 16.5863 - mae: 1.5849 - 28s/epoch - 1ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 29s - loss: 16.6057 - mse: 16.6057 - mae: 1.5847 - 29s/epoch - 1ms/step\n",
            "Score for fold 2: loss of 12.32502269744873\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 28s - loss: 16.6021 - mse: 16.6021 - mae: 1.5831 - 28s/epoch - 1ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 28s - loss: 16.4975 - mse: 16.4975 - mae: 1.5811 - 28s/epoch - 1ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 28s - loss: 16.4671 - mse: 16.4671 - mae: 1.5811 - 28s/epoch - 1ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 28s - loss: 16.5536 - mse: 16.5536 - mae: 1.5804 - 28s/epoch - 1ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 29s - loss: 16.5514 - mse: 16.5514 - mae: 1.5799 - 29s/epoch - 1ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 29s - loss: 16.7024 - mse: 16.7024 - mae: 1.5813 - 29s/epoch - 1ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 29s - loss: 16.4904 - mse: 16.4904 - mae: 1.5813 - 29s/epoch - 1ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 29s - loss: 16.5300 - mse: 16.5300 - mae: 1.5788 - 29s/epoch - 1ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 29s - loss: 16.5505 - mse: 16.5505 - mae: 1.5793 - 29s/epoch - 1ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 29s - loss: 16.4493 - mse: 16.4493 - mae: 1.5788 - 29s/epoch - 1ms/step\n",
            "Score for fold 3: loss of 11.873122215270996\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-09-30 05:54:13,753]\u001b[0m Finished trial#10 resulted in value: 16.449277877807617. Current best value is 15.534948348999023 with parameters: {'activation': 'tanh', 'optimizer': 'sgd', 'num_hidden_layer': 2, 'num_hidden_unit': 26, 'kernel_size': 2, 'filter': 58, 'learning_rate': 6.003867710257596e-05}.\u001b[0m\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 38s - loss: 19.7809 - mse: 19.7809 - mae: 1.6961 - 38s/epoch - 2ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 39s - loss: 18.6779 - mse: 18.6779 - mae: 1.6476 - 39s/epoch - 2ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 38s - loss: 18.1797 - mse: 18.1797 - mae: 1.6188 - 38s/epoch - 2ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 39s - loss: 17.7807 - mse: 17.7807 - mae: 1.5959 - 39s/epoch - 2ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 38s - loss: 17.5060 - mse: 17.5060 - mae: 1.5836 - 38s/epoch - 2ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 38s - loss: 17.3249 - mse: 17.3249 - mae: 1.5773 - 38s/epoch - 2ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 39s - loss: 17.1985 - mse: 17.1985 - mae: 1.5732 - 39s/epoch - 2ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 39s - loss: 17.0918 - mse: 17.0918 - mae: 1.5689 - 39s/epoch - 2ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 38s - loss: 16.9937 - mse: 16.9937 - mae: 1.5656 - 38s/epoch - 2ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 38s - loss: 16.8982 - mse: 16.8982 - mae: 1.5611 - 38s/epoch - 2ms/step\n",
            "Score for fold 1: loss of 23.323932647705078\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 39s - loss: 16.8052 - mse: 16.8052 - mae: 1.5558 - 39s/epoch - 2ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 37s - loss: 16.7148 - mse: 16.7148 - mae: 1.5522 - 37s/epoch - 2ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 38s - loss: 16.6248 - mse: 16.6248 - mae: 1.5477 - 38s/epoch - 2ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 39s - loss: 16.5421 - mse: 16.5421 - mae: 1.5436 - 39s/epoch - 2ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 39s - loss: 16.4595 - mse: 16.4595 - mae: 1.5390 - 39s/epoch - 2ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 38s - loss: 16.3889 - mse: 16.3889 - mae: 1.5357 - 38s/epoch - 2ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 39s - loss: 16.3211 - mse: 16.3211 - mae: 1.5320 - 39s/epoch - 2ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 40s - loss: 16.2601 - mse: 16.2601 - mae: 1.5294 - 40s/epoch - 2ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 39s - loss: 16.2082 - mse: 16.2082 - mae: 1.5267 - 39s/epoch - 2ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 39s - loss: 16.1559 - mse: 16.1559 - mae: 1.5237 - 39s/epoch - 2ms/step\n",
            "Score for fold 2: loss of 12.586248397827148\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 39s - loss: 16.1094 - mse: 16.1094 - mae: 1.5231 - 39s/epoch - 2ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 39s - loss: 16.0679 - mse: 16.0679 - mae: 1.5207 - 39s/epoch - 2ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 40s - loss: 16.0299 - mse: 16.0299 - mae: 1.5196 - 40s/epoch - 2ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 40s - loss: 15.9920 - mse: 15.9920 - mae: 1.5182 - 40s/epoch - 2ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 40s - loss: 15.9614 - mse: 15.9614 - mae: 1.5162 - 40s/epoch - 2ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 39s - loss: 15.9279 - mse: 15.9279 - mae: 1.5156 - 39s/epoch - 2ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 40s - loss: 15.9005 - mse: 15.9005 - mae: 1.5138 - 40s/epoch - 2ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 38s - loss: 15.8750 - mse: 15.8750 - mae: 1.5125 - 38s/epoch - 2ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 38s - loss: 15.8462 - mse: 15.8462 - mae: 1.5116 - 38s/epoch - 2ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 39s - loss: 15.8180 - mse: 15.8180 - mae: 1.5101 - 39s/epoch - 2ms/step\n",
            "Score for fold 3: loss of 12.196752548217773\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-09-30 06:13:50,928]\u001b[0m Finished trial#11 resulted in value: 15.81804084777832. Current best value is 15.534948348999023 with parameters: {'activation': 'tanh', 'optimizer': 'sgd', 'num_hidden_layer': 2, 'num_hidden_unit': 26, 'kernel_size': 2, 'filter': 58, 'learning_rate': 6.003867710257596e-05}.\u001b[0m\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 39s - loss: 20.3577 - mse: 20.3577 - mae: 1.7133 - 39s/epoch - 2ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 38s - loss: 19.0689 - mse: 19.0689 - mae: 1.6661 - 38s/epoch - 2ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 38s - loss: 18.7348 - mse: 18.7348 - mae: 1.6474 - 38s/epoch - 2ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 39s - loss: 18.4385 - mse: 18.4385 - mae: 1.6312 - 39s/epoch - 2ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 38s - loss: 18.1620 - mse: 18.1620 - mae: 1.6151 - 38s/epoch - 2ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 39s - loss: 17.9214 - mse: 17.9214 - mae: 1.6016 - 39s/epoch - 2ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 45s - loss: 17.7248 - mse: 17.7248 - mae: 1.5915 - 45s/epoch - 2ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 47s - loss: 17.5676 - mse: 17.5676 - mae: 1.5846 - 47s/epoch - 2ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 47s - loss: 17.4426 - mse: 17.4426 - mae: 1.5789 - 47s/epoch - 2ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 45s - loss: 17.3425 - mse: 17.3425 - mae: 1.5749 - 45s/epoch - 2ms/step\n",
            "Score for fold 1: loss of 23.607194900512695\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 40s - loss: 17.2598 - mse: 17.2598 - mae: 1.5728 - 40s/epoch - 2ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 40s - loss: 17.1883 - mse: 17.1883 - mae: 1.5708 - 40s/epoch - 2ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 40s - loss: 17.1254 - mse: 17.1254 - mae: 1.5682 - 40s/epoch - 2ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 38s - loss: 17.0652 - mse: 17.0652 - mae: 1.5666 - 38s/epoch - 2ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 37s - loss: 17.0083 - mse: 17.0083 - mae: 1.5637 - 37s/epoch - 2ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 38s - loss: 16.9518 - mse: 16.9518 - mae: 1.5603 - 38s/epoch - 2ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 39s - loss: 16.8992 - mse: 16.8992 - mae: 1.5597 - 39s/epoch - 2ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 38s - loss: 16.8441 - mse: 16.8441 - mae: 1.5562 - 38s/epoch - 2ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 39s - loss: 16.7913 - mse: 16.7913 - mae: 1.5534 - 39s/epoch - 2ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 39s - loss: 16.7373 - mse: 16.7373 - mae: 1.5502 - 39s/epoch - 2ms/step\n",
            "Score for fold 2: loss of 12.989178657531738\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 39s - loss: 16.6835 - mse: 16.6835 - mae: 1.5475 - 39s/epoch - 2ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 39s - loss: 16.6321 - mse: 16.6321 - mae: 1.5443 - 39s/epoch - 2ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 39s - loss: 16.5828 - mse: 16.5828 - mae: 1.5417 - 39s/epoch - 2ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 39s - loss: 16.5348 - mse: 16.5348 - mae: 1.5387 - 39s/epoch - 2ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 39s - loss: 16.4910 - mse: 16.4910 - mae: 1.5371 - 39s/epoch - 2ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 38s - loss: 16.4469 - mse: 16.4469 - mae: 1.5354 - 38s/epoch - 2ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 39s - loss: 16.4070 - mse: 16.4070 - mae: 1.5330 - 39s/epoch - 2ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 39s - loss: 16.3688 - mse: 16.3688 - mae: 1.5309 - 39s/epoch - 2ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 39s - loss: 16.3318 - mse: 16.3318 - mae: 1.5297 - 39s/epoch - 2ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 38s - loss: 16.2990 - mse: 16.2990 - mae: 1.5282 - 38s/epoch - 2ms/step\n",
            "Score for fold 3: loss of 12.471705436706543\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-09-30 06:33:53,423]\u001b[0m Finished trial#12 resulted in value: 16.29897689819336. Current best value is 15.534948348999023 with parameters: {'activation': 'tanh', 'optimizer': 'sgd', 'num_hidden_layer': 2, 'num_hidden_unit': 26, 'kernel_size': 2, 'filter': 58, 'learning_rate': 6.003867710257596e-05}.\u001b[0m\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 26s - loss: 18.9340 - mse: 18.9340 - mae: 1.6582 - 26s/epoch - 1ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 26s - loss: 17.5290 - mse: 17.5290 - mae: 1.5821 - 26s/epoch - 1ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 26s - loss: 17.0493 - mse: 17.0493 - mae: 1.5673 - 26s/epoch - 1ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 25s - loss: 16.7581 - mse: 16.7581 - mae: 1.5552 - 25s/epoch - 1ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 25s - loss: 16.5120 - mse: 16.5120 - mae: 1.5421 - 25s/epoch - 1ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 25s - loss: 16.3148 - mse: 16.3148 - mae: 1.5323 - 25s/epoch - 1ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 25s - loss: 16.1531 - mse: 16.1531 - mae: 1.5246 - 25s/epoch - 1ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 25s - loss: 16.0307 - mse: 16.0307 - mae: 1.5181 - 25s/epoch - 1ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 25s - loss: 15.9227 - mse: 15.9227 - mae: 1.5145 - 25s/epoch - 1ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 25s - loss: 15.8298 - mse: 15.8298 - mae: 1.5108 - 25s/epoch - 1ms/step\n",
            "Score for fold 1: loss of 24.428470611572266\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 25s - loss: 15.7581 - mse: 15.7581 - mae: 1.5085 - 25s/epoch - 1ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 25s - loss: 15.6932 - mse: 15.6932 - mae: 1.5042 - 25s/epoch - 1ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 24s - loss: 15.6295 - mse: 15.6295 - mae: 1.5015 - 24s/epoch - 1ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 25s - loss: 15.5851 - mse: 15.5851 - mae: 1.5000 - 25s/epoch - 1ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 25s - loss: 15.5208 - mse: 15.5208 - mae: 1.4971 - 25s/epoch - 1ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 25s - loss: 15.4873 - mse: 15.4873 - mae: 1.4963 - 25s/epoch - 1ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 24s - loss: 15.4392 - mse: 15.4392 - mae: 1.4937 - 24s/epoch - 1ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 26s - loss: 15.4043 - mse: 15.4043 - mae: 1.4940 - 26s/epoch - 1ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 25s - loss: 15.3563 - mse: 15.3563 - mae: 1.4908 - 25s/epoch - 1ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 25s - loss: 15.3323 - mse: 15.3323 - mae: 1.4910 - 25s/epoch - 1ms/step\n",
            "Score for fold 2: loss of 11.61575984954834\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 25s - loss: 15.2951 - mse: 15.2951 - mae: 1.4889 - 25s/epoch - 1ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 26s - loss: 15.2562 - mse: 15.2562 - mae: 1.4876 - 26s/epoch - 1ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 25s - loss: 15.2248 - mse: 15.2248 - mae: 1.4861 - 25s/epoch - 1ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 26s - loss: 15.2033 - mse: 15.2033 - mae: 1.4860 - 26s/epoch - 1ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 25s - loss: 15.1781 - mse: 15.1781 - mae: 1.4845 - 25s/epoch - 1ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 25s - loss: 15.1445 - mse: 15.1445 - mae: 1.4830 - 25s/epoch - 1ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 25s - loss: 15.1179 - mse: 15.1179 - mae: 1.4825 - 25s/epoch - 1ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 25s - loss: 15.0851 - mse: 15.0851 - mae: 1.4812 - 25s/epoch - 1ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 25s - loss: 15.0649 - mse: 15.0649 - mae: 1.4804 - 25s/epoch - 1ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 25s - loss: 15.0352 - mse: 15.0352 - mae: 1.4802 - 25s/epoch - 1ms/step\n",
            "Score for fold 3: loss of 11.134897232055664\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-09-30 06:46:39,099]\u001b[0m Finished trial#13 resulted in value: 15.035204887390137. Current best value is 15.035204887390137 with parameters: {'activation': 'tanh', 'optimizer': 'sgd', 'num_hidden_layer': 3, 'num_hidden_unit': 63, 'kernel_size': 2, 'filter': 25, 'learning_rate': 7.817684817378859e-05}.\u001b[0m\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 25s - loss: 18.5825 - mse: 18.5825 - mae: 1.6607 - 25s/epoch - 1ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 25s - loss: 17.4218 - mse: 17.4218 - mae: 1.6015 - 25s/epoch - 1ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 24s - loss: 17.0064 - mse: 17.0064 - mae: 1.5844 - 24s/epoch - 1ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 23s - loss: 16.7567 - mse: 16.7567 - mae: 1.5735 - 23s/epoch - 1ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 24s - loss: 16.5538 - mse: 16.5538 - mae: 1.5616 - 24s/epoch - 1ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 24s - loss: 16.3722 - mse: 16.3722 - mae: 1.5508 - 24s/epoch - 1ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 23s - loss: 16.2318 - mse: 16.2318 - mae: 1.5426 - 23s/epoch - 1ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 24s - loss: 16.1050 - mse: 16.1050 - mae: 1.5358 - 24s/epoch - 1ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 23s - loss: 16.0368 - mse: 16.0368 - mae: 1.5312 - 23s/epoch - 1ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 23s - loss: 15.9369 - mse: 15.9369 - mae: 1.5254 - 23s/epoch - 1ms/step\n",
            "Score for fold 1: loss of 22.99694061279297\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 24s - loss: 15.8515 - mse: 15.8515 - mae: 1.5214 - 24s/epoch - 1ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 23s - loss: 15.7972 - mse: 15.7972 - mae: 1.5181 - 23s/epoch - 1ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 23s - loss: 15.7598 - mse: 15.7598 - mae: 1.5162 - 23s/epoch - 1ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 24s - loss: 15.6858 - mse: 15.6858 - mae: 1.5112 - 24s/epoch - 1ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 24s - loss: 15.6401 - mse: 15.6401 - mae: 1.5080 - 24s/epoch - 1ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 24s - loss: 15.6454 - mse: 15.6454 - mae: 1.5089 - 24s/epoch - 1ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 23s - loss: 15.5428 - mse: 15.5428 - mae: 1.5051 - 23s/epoch - 1ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 24s - loss: 15.5578 - mse: 15.5578 - mae: 1.5057 - 24s/epoch - 1ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 24s - loss: 15.5150 - mse: 15.5150 - mae: 1.5012 - 24s/epoch - 1ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 24s - loss: 15.4892 - mse: 15.4892 - mae: 1.5017 - 24s/epoch - 1ms/step\n",
            "Score for fold 2: loss of 11.664942741394043\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 24s - loss: 15.4822 - mse: 15.4822 - mae: 1.5005 - 24s/epoch - 1ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 24s - loss: 15.4424 - mse: 15.4424 - mae: 1.4984 - 24s/epoch - 1ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 23s - loss: 15.4127 - mse: 15.4127 - mae: 1.4984 - 23s/epoch - 1ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 23s - loss: 15.4020 - mse: 15.4020 - mae: 1.4975 - 23s/epoch - 1ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 24s - loss: 15.3822 - mse: 15.3822 - mae: 1.4977 - 24s/epoch - 1ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 23s - loss: 15.3400 - mse: 15.3400 - mae: 1.4934 - 23s/epoch - 1ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 23s - loss: 15.3115 - mse: 15.3115 - mae: 1.4932 - 23s/epoch - 1ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 24s - loss: 15.2982 - mse: 15.2982 - mae: 1.4932 - 24s/epoch - 1ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 23s - loss: 15.2724 - mse: 15.2724 - mae: 1.4909 - 23s/epoch - 1ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 24s - loss: 15.2926 - mse: 15.2926 - mae: 1.4923 - 24s/epoch - 1ms/step\n",
            "Score for fold 3: loss of 11.227965354919434\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-09-30 06:58:37,946]\u001b[0m Finished trial#14 resulted in value: 15.292609214782715. Current best value is 15.035204887390137 with parameters: {'activation': 'tanh', 'optimizer': 'sgd', 'num_hidden_layer': 3, 'num_hidden_unit': 63, 'kernel_size': 2, 'filter': 25, 'learning_rate': 7.817684817378859e-05}.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 24s - loss: 18.4859 - mse: 18.4859 - mae: 1.6496 - 24s/epoch - 1ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 23s - loss: 17.2808 - mse: 17.2808 - mae: 1.5984 - 23s/epoch - 1ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 23s - loss: 16.9052 - mse: 16.9052 - mae: 1.5837 - 23s/epoch - 1ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 22s - loss: 16.6583 - mse: 16.6583 - mae: 1.5691 - 22s/epoch - 1ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 23s - loss: 16.4657 - mse: 16.4657 - mae: 1.5566 - 23s/epoch - 1ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 23s - loss: 16.2790 - mse: 16.2790 - mae: 1.5444 - 23s/epoch - 1ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 22s - loss: 16.1505 - mse: 16.1505 - mae: 1.5378 - 22s/epoch - 1ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 23s - loss: 16.0135 - mse: 16.0135 - mae: 1.5297 - 23s/epoch - 1ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 24s - loss: 15.9601 - mse: 15.9601 - mae: 1.5267 - 24s/epoch - 1ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 24s - loss: 15.8236 - mse: 15.8236 - mae: 1.5195 - 24s/epoch - 1ms/step\n",
            "Score for fold 1: loss of 24.304744720458984\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 23s - loss: 15.7689 - mse: 15.7689 - mae: 1.5164 - 23s/epoch - 1ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 24s - loss: 15.7706 - mse: 15.7706 - mae: 1.5169 - 24s/epoch - 1ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 23s - loss: 15.7095 - mse: 15.7095 - mae: 1.5120 - 23s/epoch - 1ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 23s - loss: 15.6648 - mse: 15.6648 - mae: 1.5104 - 23s/epoch - 1ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 26s - loss: 15.6094 - mse: 15.6094 - mae: 1.5084 - 26s/epoch - 1ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 22s - loss: 15.5940 - mse: 15.5940 - mae: 1.5087 - 22s/epoch - 1ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 23s - loss: 15.5621 - mse: 15.5621 - mae: 1.5052 - 23s/epoch - 1ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 23s - loss: 15.5734 - mse: 15.5734 - mae: 1.5068 - 23s/epoch - 1ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 24s - loss: 15.4507 - mse: 15.4507 - mae: 1.5011 - 24s/epoch - 1ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 25s - loss: 15.4658 - mse: 15.4658 - mae: 1.4999 - 25s/epoch - 1ms/step\n",
            "Score for fold 2: loss of 11.622879981994629\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 25s - loss: 15.4635 - mse: 15.4635 - mae: 1.5013 - 25s/epoch - 1ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 24s - loss: 15.4155 - mse: 15.4155 - mae: 1.4976 - 24s/epoch - 1ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 23s - loss: 15.3624 - mse: 15.3624 - mae: 1.4960 - 23s/epoch - 1ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 24s - loss: 15.3542 - mse: 15.3542 - mae: 1.4955 - 24s/epoch - 1ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 24s - loss: 15.3317 - mse: 15.3317 - mae: 1.4938 - 24s/epoch - 1ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 24s - loss: 15.3268 - mse: 15.3268 - mae: 1.4939 - 24s/epoch - 1ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 24s - loss: 15.3022 - mse: 15.3022 - mae: 1.4921 - 24s/epoch - 1ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 24s - loss: 15.2496 - mse: 15.2496 - mae: 1.4917 - 24s/epoch - 1ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 25s - loss: 15.2610 - mse: 15.2610 - mae: 1.4919 - 25s/epoch - 1ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 25s - loss: 15.2149 - mse: 15.2149 - mae: 1.4892 - 25s/epoch - 1ms/step\n",
            "Score for fold 3: loss of 11.266279220581055\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-09-30 07:10:38,315]\u001b[0m Finished trial#15 resulted in value: 15.214879035949707. Current best value is 15.035204887390137 with parameters: {'activation': 'tanh', 'optimizer': 'sgd', 'num_hidden_layer': 3, 'num_hidden_unit': 63, 'kernel_size': 2, 'filter': 25, 'learning_rate': 7.817684817378859e-05}.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 26s - loss: 17.9957 - mse: 17.9957 - mae: 1.6324 - 26s/epoch - 1ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 26s - loss: 16.5966 - mse: 16.5966 - mae: 1.5554 - 26s/epoch - 1ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 26s - loss: 16.3339 - mse: 16.3339 - mae: 1.5434 - 26s/epoch - 1ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 26s - loss: 16.0693 - mse: 16.0693 - mae: 1.5323 - 26s/epoch - 1ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 26s - loss: 15.9214 - mse: 15.9214 - mae: 1.5255 - 26s/epoch - 1ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 26s - loss: 15.8024 - mse: 15.8024 - mae: 1.5219 - 26s/epoch - 1ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 26s - loss: 15.8588 - mse: 15.8588 - mae: 1.5232 - 26s/epoch - 1ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 26s - loss: 15.7107 - mse: 15.7107 - mae: 1.5171 - 26s/epoch - 1ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 25s - loss: 15.7488 - mse: 15.7488 - mae: 1.5223 - 25s/epoch - 1ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 25s - loss: 15.5851 - mse: 15.5850 - mae: 1.5124 - 25s/epoch - 1ms/step\n",
            "Score for fold 1: loss of 24.514427185058594\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 25s - loss: 15.5789 - mse: 15.5789 - mae: 1.5134 - 25s/epoch - 1ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 24s - loss: 15.5197 - mse: 15.5197 - mae: 1.5114 - 24s/epoch - 1ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 25s - loss: 15.5949 - mse: 15.5949 - mae: 1.5148 - 25s/epoch - 1ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 24s - loss: 15.6506 - mse: 15.6506 - mae: 1.5192 - 24s/epoch - 1ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 24s - loss: 15.3946 - mse: 15.3946 - mae: 1.5083 - 24s/epoch - 1ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 24s - loss: 15.7740 - mse: 15.7740 - mae: 1.5273 - 24s/epoch - 1ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 25s - loss: 15.5137 - mse: 15.5137 - mae: 1.5170 - 25s/epoch - 1ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 24s - loss: 15.4706 - mse: 15.4706 - mae: 1.5126 - 24s/epoch - 1ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 24s - loss: 15.3133 - mse: 15.3133 - mae: 1.5073 - 24s/epoch - 1ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 25s - loss: 15.2558 - mse: 15.2558 - mae: 1.5061 - 25s/epoch - 1ms/step\n",
            "Score for fold 2: loss of 11.426512718200684\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 24s - loss: 15.3624 - mse: 15.3624 - mae: 1.5135 - 24s/epoch - 1ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 24s - loss: 15.4794 - mse: 15.4794 - mae: 1.5161 - 24s/epoch - 1ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 24s - loss: 15.3847 - mse: 15.3847 - mae: 1.5141 - 24s/epoch - 1ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 24s - loss: 15.2237 - mse: 15.2237 - mae: 1.5060 - 24s/epoch - 1ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 24s - loss: 15.1833 - mse: 15.1833 - mae: 1.5056 - 24s/epoch - 1ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 24s - loss: 15.1387 - mse: 15.1387 - mae: 1.5026 - 24s/epoch - 1ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 24s - loss: 15.0824 - mse: 15.0824 - mae: 1.5020 - 24s/epoch - 1ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 24s - loss: 14.9940 - mse: 14.9940 - mae: 1.4984 - 24s/epoch - 1ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 24s - loss: 14.9226 - mse: 14.9226 - mae: 1.4971 - 24s/epoch - 1ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 25s - loss: 14.9072 - mse: 14.9072 - mae: 1.4962 - 25s/epoch - 1ms/step\n",
            "Score for fold 3: loss of 12.217475891113281\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-09-30 07:23:10,683]\u001b[0m Finished trial#16 resulted in value: 14.90717601776123. Current best value is 14.90717601776123 with parameters: {'activation': 'tanh', 'optimizer': 'sgd', 'num_hidden_layer': 4, 'num_hidden_unit': 56, 'kernel_size': 3, 'filter': 7, 'learning_rate': 0.00032117094663704654}.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 24s - loss: 20.4651 - mse: 20.4651 - mae: 1.7468 - 24s/epoch - 1ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 24s - loss: 21.3690 - mse: 21.3690 - mae: 1.9071 - 24s/epoch - 1ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 24s - loss: 21.3690 - mse: 21.3690 - mae: 1.9073 - 24s/epoch - 1ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 24s - loss: 21.3673 - mse: 21.3673 - mae: 1.9077 - 24s/epoch - 1ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 23s - loss: 21.3677 - mse: 21.3677 - mae: 1.9070 - 23s/epoch - 1ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 23s - loss: 21.3678 - mse: 21.3678 - mae: 1.9083 - 23s/epoch - 1ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 23s - loss: 21.3677 - mse: 21.3677 - mae: 1.9079 - 23s/epoch - 1ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 22s - loss: 21.3671 - mse: 21.3671 - mae: 1.9079 - 22s/epoch - 1ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 23s - loss: 21.3670 - mse: 21.3670 - mae: 1.9074 - 23s/epoch - 1ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 23s - loss: 21.3668 - mse: 21.3668 - mae: 1.9085 - 23s/epoch - 1ms/step\n",
            "Score for fold 1: loss of 23.658903121948242\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 23s - loss: 21.3673 - mse: 21.3673 - mae: 1.9083 - 23s/epoch - 1ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 23s - loss: 21.3672 - mse: 21.3672 - mae: 1.9075 - 23s/epoch - 1ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 23s - loss: 21.3667 - mse: 21.3667 - mae: 1.9084 - 23s/epoch - 1ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 23s - loss: 21.3667 - mse: 21.3667 - mae: 1.9078 - 23s/epoch - 1ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 23s - loss: 21.3666 - mse: 21.3666 - mae: 1.9083 - 23s/epoch - 1ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 23s - loss: 21.3668 - mse: 21.3668 - mae: 1.9086 - 23s/epoch - 1ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 24s - loss: 21.3668 - mse: 21.3668 - mae: 1.9075 - 24s/epoch - 1ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 23s - loss: 21.3670 - mse: 21.3670 - mae: 1.9077 - 23s/epoch - 1ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 23s - loss: 21.3667 - mse: 21.3667 - mae: 1.9084 - 23s/epoch - 1ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 23s - loss: 21.3670 - mse: 21.3670 - mae: 1.9081 - 23s/epoch - 1ms/step\n",
            "Score for fold 2: loss of 16.962770462036133\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 23s - loss: 21.3662 - mse: 21.3662 - mae: 1.9084 - 23s/epoch - 1ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 23s - loss: 21.3666 - mse: 21.3666 - mae: 1.9081 - 23s/epoch - 1ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 23s - loss: 21.3663 - mse: 21.3663 - mae: 1.9068 - 23s/epoch - 1ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 23s - loss: 21.3663 - mse: 21.3663 - mae: 1.9091 - 23s/epoch - 1ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 23s - loss: 21.3666 - mse: 21.3666 - mae: 1.9079 - 23s/epoch - 1ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 23s - loss: 21.3662 - mse: 21.3662 - mae: 1.9081 - 23s/epoch - 1ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 23s - loss: 21.3662 - mse: 21.3662 - mae: 1.9083 - 23s/epoch - 1ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 23s - loss: 21.3668 - mse: 21.3668 - mae: 1.9082 - 23s/epoch - 1ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 23s - loss: 21.3671 - mse: 21.3671 - mae: 1.9084 - 23s/epoch - 1ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 23s - loss: 21.3662 - mse: 21.3662 - mae: 1.9070 - 23s/epoch - 1ms/step\n",
            "Score for fold 3: loss of 16.96268653869629\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-09-30 07:34:54,683]\u001b[0m Finished trial#17 resulted in value: 21.366195678710938. Current best value is 14.90717601776123 with parameters: {'activation': 'tanh', 'optimizer': 'sgd', 'num_hidden_layer': 4, 'num_hidden_unit': 56, 'kernel_size': 3, 'filter': 7, 'learning_rate': 0.00032117094663704654}.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 23s - loss: 19.1286 - mse: 19.1286 - mae: 1.7212 - 23s/epoch - 1ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 23s - loss: 18.1999 - mse: 18.1999 - mae: 1.6923 - 23s/epoch - 1ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 24s - loss: 17.6877 - mse: 17.6877 - mae: 1.6638 - 24s/epoch - 1ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 24s - loss: 17.5739 - mse: 17.5739 - mae: 1.6536 - 24s/epoch - 1ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 24s - loss: 17.5238 - mse: 17.5238 - mae: 1.6487 - 24s/epoch - 1ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 24s - loss: 17.5231 - mse: 17.5231 - mae: 1.6450 - 24s/epoch - 1ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 23s - loss: 17.5333 - mse: 17.5333 - mae: 1.6475 - 23s/epoch - 1ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 23s - loss: 17.5085 - mse: 17.5085 - mae: 1.6446 - 23s/epoch - 1ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 24s - loss: 17.4864 - mse: 17.4864 - mae: 1.6391 - 24s/epoch - 1ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 23s - loss: 17.3750 - mse: 17.3750 - mae: 1.6341 - 23s/epoch - 1ms/step\n",
            "Score for fold 1: loss of 23.228532791137695\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 24s - loss: 17.3600 - mse: 17.3600 - mae: 1.6351 - 24s/epoch - 1ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 25s - loss: 17.4989 - mse: 17.4989 - mae: 1.6414 - 25s/epoch - 1ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 24s - loss: 17.2873 - mse: 17.2873 - mae: 1.6261 - 24s/epoch - 1ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 23s - loss: 17.2357 - mse: 17.2357 - mae: 1.6242 - 23s/epoch - 1ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 23s - loss: 17.3051 - mse: 17.3051 - mae: 1.6231 - 23s/epoch - 1ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 23s - loss: 17.2768 - mse: 17.2768 - mae: 1.6240 - 23s/epoch - 1ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 23s - loss: 17.1755 - mse: 17.1755 - mae: 1.6178 - 23s/epoch - 1ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 23s - loss: 17.2292 - mse: 17.2292 - mae: 1.6188 - 23s/epoch - 1ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 23s - loss: 17.1713 - mse: 17.1713 - mae: 1.6166 - 23s/epoch - 1ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 23s - loss: 17.1343 - mse: 17.1343 - mae: 1.6137 - 23s/epoch - 1ms/step\n",
            "Score for fold 2: loss of 16.96833038330078\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 23s - loss: 17.1543 - mse: 17.1543 - mae: 1.6157 - 23s/epoch - 1ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 23s - loss: 17.0582 - mse: 17.0582 - mae: 1.6107 - 23s/epoch - 1ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 23s - loss: 19.7287 - mse: 19.7287 - mae: 1.7734 - 23s/epoch - 1ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 23s - loss: 18.7086 - mse: 18.7086 - mae: 1.6653 - 23s/epoch - 1ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 23s - loss: 18.3086 - mse: 18.3086 - mae: 1.6303 - 23s/epoch - 1ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 23s - loss: 18.1970 - mse: 18.1970 - mae: 1.6200 - 23s/epoch - 1ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 22s - loss: 18.1616 - mse: 18.1616 - mae: 1.6172 - 22s/epoch - 1ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 22s - loss: 18.1279 - mse: 18.1279 - mae: 1.6149 - 22s/epoch - 1ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 23s - loss: 18.0998 - mse: 18.0998 - mae: 1.6128 - 23s/epoch - 1ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 23s - loss: 18.0739 - mse: 18.0739 - mae: 1.6114 - 23s/epoch - 1ms/step\n",
            "Score for fold 3: loss of 12.847992897033691\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-09-30 07:46:40,207]\u001b[0m Finished trial#18 resulted in value: 18.073877334594727. Current best value is 14.90717601776123 with parameters: {'activation': 'tanh', 'optimizer': 'sgd', 'num_hidden_layer': 4, 'num_hidden_unit': 56, 'kernel_size': 3, 'filter': 7, 'learning_rate': 0.00032117094663704654}.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 26s - loss: 17.2723 - mse: 17.2723 - mae: 1.6121 - 26s/epoch - 1ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 27s - loss: 17.2858 - mse: 17.2858 - mae: 1.6226 - 27s/epoch - 1ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 27s - loss: 17.2320 - mse: 17.2320 - mae: 1.6198 - 27s/epoch - 1ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 27s - loss: 17.6176 - mse: 17.6176 - mae: 1.6495 - 27s/epoch - 1ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 27s - loss: 17.8808 - mse: 17.8808 - mae: 1.6624 - 27s/epoch - 1ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 27s - loss: 18.2012 - mse: 18.2012 - mae: 1.6845 - 27s/epoch - 1ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 28s - loss: 17.9880 - mse: 17.9880 - mae: 1.6657 - 28s/epoch - 1ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 27s - loss: 18.0873 - mse: 18.0873 - mae: 1.6697 - 27s/epoch - 1ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 26s - loss: 17.7011 - mse: 17.7011 - mae: 1.6580 - 26s/epoch - 1ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 26s - loss: 17.9849 - mse: 17.9849 - mae: 1.6842 - 26s/epoch - 1ms/step\n",
            "Score for fold 1: loss of 23.341449737548828\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 26s - loss: 18.2117 - mse: 18.2117 - mae: 1.6896 - 26s/epoch - 1ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 27s - loss: 18.2564 - mse: 18.2564 - mae: 1.6952 - 27s/epoch - 1ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 27s - loss: 18.5084 - mse: 18.5084 - mae: 1.6869 - 27s/epoch - 1ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 27s - loss: 18.3492 - mse: 18.3492 - mae: 1.6758 - 27s/epoch - 1ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 27s - loss: 18.0775 - mse: 18.0775 - mae: 1.6621 - 27s/epoch - 1ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 27s - loss: 17.9309 - mse: 17.9309 - mae: 1.6668 - 27s/epoch - 1ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 28s - loss: 18.2399 - mse: 18.2399 - mae: 1.6721 - 28s/epoch - 1ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 28s - loss: 17.9632 - mse: 17.9632 - mae: 1.6627 - 28s/epoch - 1ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 29s - loss: 18.2578 - mse: 18.2578 - mae: 1.6907 - 29s/epoch - 1ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 28s - loss: 18.5107 - mse: 18.5107 - mae: 1.7038 - 28s/epoch - 1ms/step\n",
            "Score for fold 2: loss of 14.534157752990723\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 28s - loss: 18.3222 - mse: 18.3222 - mae: 1.6875 - 28s/epoch - 1ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 28s - loss: 18.2584 - mse: 18.2584 - mae: 1.6785 - 28s/epoch - 1ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 27s - loss: 18.0351 - mse: 18.0351 - mae: 1.6653 - 27s/epoch - 1ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 27s - loss: 17.8182 - mse: 17.8182 - mae: 1.6585 - 27s/epoch - 1ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 27s - loss: 17.7594 - mse: 17.7594 - mae: 1.6456 - 27s/epoch - 1ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 27s - loss: 18.0387 - mse: 18.0387 - mae: 1.6532 - 27s/epoch - 1ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 27s - loss: 17.8955 - mse: 17.8955 - mae: 1.6571 - 27s/epoch - 1ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 27s - loss: 17.8702 - mse: 17.8702 - mae: 1.6805 - 27s/epoch - 1ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 26s - loss: 18.2616 - mse: 18.2616 - mae: 1.6819 - 26s/epoch - 1ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 27s - loss: 18.1049 - mse: 18.1049 - mae: 1.6744 - 27s/epoch - 1ms/step\n",
            "Score for fold 3: loss of 13.918411254882812\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-09-30 08:00:23,056]\u001b[0m Finished trial#19 resulted in value: 18.10491180419922. Current best value is 14.90717601776123 with parameters: {'activation': 'tanh', 'optimizer': 'sgd', 'num_hidden_layer': 4, 'num_hidden_unit': 56, 'kernel_size': 3, 'filter': 7, 'learning_rate': 0.00032117094663704654}.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 31s - loss: 21.9259 - mse: 21.9259 - mae: 2.0343 - 31s/epoch - 2ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 29s - loss: 22.7539 - mse: 22.7539 - mae: 2.0892 - 29s/epoch - 1ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 29s - loss: 22.7162 - mse: 22.7162 - mae: 2.0828 - 29s/epoch - 1ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 30s - loss: 22.7689 - mse: 22.7689 - mae: 2.0886 - 30s/epoch - 1ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 31s - loss: 22.7527 - mse: 22.7527 - mae: 2.0881 - 31s/epoch - 2ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 32s - loss: 22.7875 - mse: 22.7875 - mae: 2.0949 - 32s/epoch - 2ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 36s - loss: 22.7570 - mse: 22.7570 - mae: 2.0866 - 36s/epoch - 2ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 35s - loss: 22.7657 - mse: 22.7657 - mae: 2.0908 - 35s/epoch - 2ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 35s - loss: 22.7649 - mse: 22.7649 - mae: 2.0904 - 35s/epoch - 2ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 32s - loss: 22.7414 - mse: 22.7414 - mae: 2.0819 - 32s/epoch - 2ms/step\n",
            "Score for fold 1: loss of 25.245424270629883\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 35s - loss: 22.7338 - mse: 22.7338 - mae: 2.0872 - 35s/epoch - 2ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 36s - loss: 22.7544 - mse: 22.7544 - mae: 2.0905 - 36s/epoch - 2ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 34s - loss: 22.7553 - mse: 22.7553 - mae: 2.0871 - 34s/epoch - 2ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 34s - loss: 22.7215 - mse: 22.7215 - mae: 2.0858 - 34s/epoch - 2ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 34s - loss: 22.8028 - mse: 22.8028 - mae: 2.0927 - 34s/epoch - 2ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 33s - loss: 22.8410 - mse: 22.8410 - mae: 2.1007 - 33s/epoch - 2ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 34s - loss: 22.7967 - mse: 22.7967 - mae: 2.0915 - 34s/epoch - 2ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 33s - loss: 22.7539 - mse: 22.7539 - mae: 2.0880 - 33s/epoch - 2ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 34s - loss: 22.7390 - mse: 22.7390 - mae: 2.0871 - 34s/epoch - 2ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 33s - loss: 22.7903 - mse: 22.7903 - mae: 2.0906 - 33s/epoch - 2ms/step\n",
            "Score for fold 2: loss of 19.510173797607422\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 32s - loss: 22.8294 - mse: 22.8294 - mae: 2.0933 - 32s/epoch - 2ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 32s - loss: 22.7590 - mse: 22.7590 - mae: 2.0912 - 32s/epoch - 2ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 32s - loss: 22.7879 - mse: 22.7879 - mae: 2.0947 - 32s/epoch - 2ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 32s - loss: 22.7964 - mse: 22.7964 - mae: 2.0882 - 32s/epoch - 2ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 31s - loss: 22.7659 - mse: 22.7659 - mae: 2.0876 - 31s/epoch - 2ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 31s - loss: 22.7724 - mse: 22.7724 - mae: 2.0860 - 31s/epoch - 2ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 31s - loss: 22.7834 - mse: 22.7834 - mae: 2.0907 - 31s/epoch - 2ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 31s - loss: 22.7498 - mse: 22.7498 - mae: 2.0888 - 31s/epoch - 2ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 32s - loss: 22.7515 - mse: 22.7515 - mae: 2.0918 - 32s/epoch - 2ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 32s - loss: 22.8025 - mse: 22.8025 - mae: 2.0987 - 32s/epoch - 2ms/step\n",
            "Score for fold 3: loss of 18.424545288085938\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-09-30 08:16:49,933]\u001b[0m Finished trial#20 resulted in value: 22.802457809448242. Current best value is 14.90717601776123 with parameters: {'activation': 'tanh', 'optimizer': 'sgd', 'num_hidden_layer': 4, 'num_hidden_unit': 56, 'kernel_size': 3, 'filter': 7, 'learning_rate': 0.00032117094663704654}.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 30s - loss: 24.0753 - mse: 24.0753 - mae: 1.9406 - 30s/epoch - 1ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 29s - loss: 21.5672 - mse: 21.5672 - mae: 1.6193 - 29s/epoch - 1ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 29s - loss: 20.6043 - mse: 20.6043 - mae: 1.6127 - 29s/epoch - 1ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 28s - loss: 20.1593 - mse: 20.1593 - mae: 1.6422 - 28s/epoch - 1ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 28s - loss: 19.9310 - mse: 19.9310 - mae: 1.6712 - 28s/epoch - 1ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 28s - loss: 19.7966 - mse: 19.7966 - mae: 1.6941 - 28s/epoch - 1ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 29s - loss: 19.7072 - mse: 19.7072 - mae: 1.7109 - 29s/epoch - 1ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 29s - loss: 19.6412 - mse: 19.6412 - mae: 1.7219 - 29s/epoch - 1ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 29s - loss: 19.5887 - mse: 19.5887 - mae: 1.7286 - 29s/epoch - 1ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 32s - loss: 19.5444 - mse: 19.5444 - mae: 1.7335 - 32s/epoch - 2ms/step\n",
            "Score for fold 1: loss of 22.842863082885742\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 28s - loss: 19.5058 - mse: 19.5058 - mae: 1.7354 - 28s/epoch - 1ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 28s - loss: 19.4714 - mse: 19.4714 - mae: 1.7373 - 28s/epoch - 1ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 28s - loss: 19.4401 - mse: 19.4401 - mae: 1.7381 - 28s/epoch - 1ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 28s - loss: 19.4112 - mse: 19.4112 - mae: 1.7382 - 28s/epoch - 1ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 28s - loss: 19.3841 - mse: 19.3841 - mae: 1.7377 - 28s/epoch - 1ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 28s - loss: 19.3588 - mse: 19.3588 - mae: 1.7371 - 28s/epoch - 1ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 28s - loss: 19.3346 - mse: 19.3346 - mae: 1.7361 - 28s/epoch - 1ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 27s - loss: 19.3119 - mse: 19.3119 - mae: 1.7350 - 27s/epoch - 1ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 27s - loss: 19.2901 - mse: 19.2901 - mae: 1.7334 - 27s/epoch - 1ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 27s - loss: 19.2693 - mse: 19.2693 - mae: 1.7318 - 27s/epoch - 1ms/step\n",
            "Score for fold 2: loss of 14.939772605895996\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 27s - loss: 19.2493 - mse: 19.2493 - mae: 1.7299 - 27s/epoch - 1ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 27s - loss: 19.2300 - mse: 19.2300 - mae: 1.7286 - 27s/epoch - 1ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 28s - loss: 19.2114 - mse: 19.2114 - mae: 1.7272 - 28s/epoch - 1ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 28s - loss: 19.1933 - mse: 19.1933 - mae: 1.7248 - 28s/epoch - 1ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 27s - loss: 19.1757 - mse: 19.1757 - mae: 1.7231 - 27s/epoch - 1ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 27s - loss: 19.1586 - mse: 19.1586 - mae: 1.7210 - 27s/epoch - 1ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 27s - loss: 19.1419 - mse: 19.1419 - mae: 1.7197 - 27s/epoch - 1ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 27s - loss: 19.1257 - mse: 19.1257 - mae: 1.7186 - 27s/epoch - 1ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 27s - loss: 19.1095 - mse: 19.1095 - mae: 1.7158 - 27s/epoch - 1ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 27s - loss: 19.0935 - mse: 19.0935 - mae: 1.7138 - 27s/epoch - 1ms/step\n",
            "Score for fold 3: loss of 14.666855812072754\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-09-30 08:31:00,437]\u001b[0m Finished trial#21 resulted in value: 19.093542098999023. Current best value is 14.90717601776123 with parameters: {'activation': 'tanh', 'optimizer': 'sgd', 'num_hidden_layer': 4, 'num_hidden_unit': 56, 'kernel_size': 3, 'filter': 7, 'learning_rate': 0.00032117094663704654}.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 26s - loss: 18.4951 - mse: 18.4951 - mae: 1.6517 - 26s/epoch - 1ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 26s - loss: 17.3205 - mse: 17.3205 - mae: 1.6034 - 26s/epoch - 1ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 25s - loss: 16.9346 - mse: 16.9346 - mae: 1.5869 - 25s/epoch - 1ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 24s - loss: 16.6710 - mse: 16.6710 - mae: 1.5689 - 24s/epoch - 1ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 24s - loss: 16.4661 - mse: 16.4661 - mae: 1.5554 - 24s/epoch - 1ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 24s - loss: 16.3074 - mse: 16.3074 - mae: 1.5445 - 24s/epoch - 1ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 24s - loss: 16.2051 - mse: 16.2051 - mae: 1.5406 - 24s/epoch - 1ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 24s - loss: 16.0592 - mse: 16.0592 - mae: 1.5321 - 24s/epoch - 1ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 24s - loss: 15.9624 - mse: 15.9624 - mae: 1.5275 - 24s/epoch - 1ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 24s - loss: 15.9076 - mse: 15.9076 - mae: 1.5251 - 24s/epoch - 1ms/step\n",
            "Score for fold 1: loss of 23.089365005493164\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 24s - loss: 15.8721 - mse: 15.8721 - mae: 1.5203 - 24s/epoch - 1ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 23s - loss: 15.8022 - mse: 15.8022 - mae: 1.5180 - 23s/epoch - 1ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 23s - loss: 15.7516 - mse: 15.7516 - mae: 1.5153 - 23s/epoch - 1ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 23s - loss: 15.6816 - mse: 15.6816 - mae: 1.5136 - 23s/epoch - 1ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 24s - loss: 15.6845 - mse: 15.6845 - mae: 1.5114 - 24s/epoch - 1ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 24s - loss: 15.6827 - mse: 15.6827 - mae: 1.5117 - 24s/epoch - 1ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 24s - loss: 15.6284 - mse: 15.6284 - mae: 1.5082 - 24s/epoch - 1ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 24s - loss: 15.6351 - mse: 15.6351 - mae: 1.5097 - 24s/epoch - 1ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 24s - loss: 15.5913 - mse: 15.5913 - mae: 1.5067 - 24s/epoch - 1ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 24s - loss: 15.5531 - mse: 15.5531 - mae: 1.5046 - 24s/epoch - 1ms/step\n",
            "Score for fold 2: loss of 11.761058807373047\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 26s - loss: 15.5287 - mse: 15.5287 - mae: 1.5046 - 26s/epoch - 1ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 26s - loss: 15.4841 - mse: 15.4841 - mae: 1.5020 - 26s/epoch - 1ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 25s - loss: 15.4677 - mse: 15.4677 - mae: 1.5031 - 25s/epoch - 1ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 25s - loss: 15.4255 - mse: 15.4255 - mae: 1.4994 - 25s/epoch - 1ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 25s - loss: 15.4543 - mse: 15.4543 - mae: 1.5002 - 25s/epoch - 1ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 24s - loss: 15.4501 - mse: 15.4501 - mae: 1.5001 - 24s/epoch - 1ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 24s - loss: 15.3645 - mse: 15.3645 - mae: 1.4968 - 24s/epoch - 1ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 24s - loss: 15.3861 - mse: 15.3861 - mae: 1.4978 - 24s/epoch - 1ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 23s - loss: 15.3494 - mse: 15.3494 - mae: 1.4945 - 23s/epoch - 1ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 24s - loss: 15.3049 - mse: 15.3049 - mae: 1.4941 - 24s/epoch - 1ms/step\n",
            "Score for fold 3: loss of 11.267330169677734\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-09-30 08:43:18,771]\u001b[0m Finished trial#22 resulted in value: 15.304940223693848. Current best value is 14.90717601776123 with parameters: {'activation': 'tanh', 'optimizer': 'sgd', 'num_hidden_layer': 4, 'num_hidden_unit': 56, 'kernel_size': 3, 'filter': 7, 'learning_rate': 0.00032117094663704654}.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 23s - loss: 18.3339 - mse: 18.3339 - mae: 1.6419 - 23s/epoch - 1ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 23s - loss: 16.9640 - mse: 16.9640 - mae: 1.5735 - 23s/epoch - 1ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 23s - loss: 16.5853 - mse: 16.5853 - mae: 1.5564 - 23s/epoch - 1ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 23s - loss: 16.3854 - mse: 16.3854 - mae: 1.5464 - 23s/epoch - 1ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 23s - loss: 16.2134 - mse: 16.2134 - mae: 1.5394 - 23s/epoch - 1ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 24s - loss: 16.1072 - mse: 16.1072 - mae: 1.5362 - 24s/epoch - 1ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 23s - loss: 16.0382 - mse: 16.0382 - mae: 1.5311 - 23s/epoch - 1ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 24s - loss: 15.9221 - mse: 15.9221 - mae: 1.5263 - 24s/epoch - 1ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 24s - loss: 15.8530 - mse: 15.8530 - mae: 1.5239 - 24s/epoch - 1ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 23s - loss: 15.7835 - mse: 15.7835 - mae: 1.5204 - 23s/epoch - 1ms/step\n",
            "Score for fold 1: loss of 25.74936294555664\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 24s - loss: 15.7727 - mse: 15.7727 - mae: 1.5199 - 24s/epoch - 1ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 24s - loss: 15.7335 - mse: 15.7335 - mae: 1.5178 - 24s/epoch - 1ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 23s - loss: 15.6574 - mse: 15.6574 - mae: 1.5136 - 23s/epoch - 1ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 23s - loss: 15.6237 - mse: 15.6237 - mae: 1.5129 - 23s/epoch - 1ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 23s - loss: 15.5473 - mse: 15.5473 - mae: 1.5106 - 23s/epoch - 1ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 23s - loss: 15.6609 - mse: 15.6609 - mae: 1.5144 - 23s/epoch - 1ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 24s - loss: 15.6612 - mse: 15.6612 - mae: 1.5147 - 24s/epoch - 1ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 23s - loss: 15.6391 - mse: 15.6391 - mae: 1.5185 - 23s/epoch - 1ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 23s - loss: 15.4675 - mse: 15.4675 - mae: 1.5073 - 23s/epoch - 1ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 24s - loss: 15.4697 - mse: 15.4697 - mae: 1.5079 - 24s/epoch - 1ms/step\n",
            "Score for fold 2: loss of 11.555278778076172\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 24s - loss: 15.4041 - mse: 15.4041 - mae: 1.5054 - 24s/epoch - 1ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 23s - loss: 15.4473 - mse: 15.4473 - mae: 1.5073 - 23s/epoch - 1ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 23s - loss: 15.5092 - mse: 15.5092 - mae: 1.5119 - 23s/epoch - 1ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 23s - loss: 15.3742 - mse: 15.3742 - mae: 1.5046 - 23s/epoch - 1ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 23s - loss: 15.7003 - mse: 15.7003 - mae: 1.5170 - 23s/epoch - 1ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 22s - loss: 15.5507 - mse: 15.5507 - mae: 1.5135 - 22s/epoch - 1ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 23s - loss: 15.4998 - mse: 15.4998 - mae: 1.5100 - 23s/epoch - 1ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 23s - loss: 15.3914 - mse: 15.3914 - mae: 1.5065 - 23s/epoch - 1ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 24s - loss: 15.3941 - mse: 15.3941 - mae: 1.5056 - 24s/epoch - 1ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 24s - loss: 15.2958 - mse: 15.2958 - mae: 1.5031 - 24s/epoch - 1ms/step\n",
            "Score for fold 3: loss of 11.615846633911133\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-09-30 08:55:07,820]\u001b[0m Finished trial#23 resulted in value: 15.295763969421387. Current best value is 14.90717601776123 with parameters: {'activation': 'tanh', 'optimizer': 'sgd', 'num_hidden_layer': 4, 'num_hidden_unit': 56, 'kernel_size': 3, 'filter': 7, 'learning_rate': 0.00032117094663704654}.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 24s - loss: 17.4862 - mse: 17.4862 - mae: 1.6140 - 24s/epoch - 1ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 24s - loss: 16.3872 - mse: 16.3872 - mae: 1.5589 - 24s/epoch - 1ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 25s - loss: 15.9201 - mse: 15.9201 - mae: 1.5301 - 25s/epoch - 1ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 25s - loss: 15.6822 - mse: 15.6822 - mae: 1.5169 - 25s/epoch - 1ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 25s - loss: 15.6454 - mse: 15.6454 - mae: 1.5175 - 25s/epoch - 1ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 25s - loss: 15.5268 - mse: 15.5268 - mae: 1.5133 - 25s/epoch - 1ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 26s - loss: 15.5290 - mse: 15.5290 - mae: 1.5137 - 26s/epoch - 1ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 25s - loss: 15.3970 - mse: 15.3970 - mae: 1.5086 - 25s/epoch - 1ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 24s - loss: 15.4360 - mse: 15.4360 - mae: 1.5118 - 24s/epoch - 1ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 24s - loss: 15.4917 - mse: 15.4917 - mae: 1.5156 - 24s/epoch - 1ms/step\n",
            "Score for fold 1: loss of 24.828332901000977\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 41s - loss: 15.2782 - mse: 15.2782 - mae: 1.5065 - 41s/epoch - 2ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 40s - loss: 15.2748 - mse: 15.2748 - mae: 1.5035 - 40s/epoch - 2ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 41s - loss: 15.2156 - mse: 15.2156 - mae: 1.5023 - 41s/epoch - 2ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 40s - loss: 15.2232 - mse: 15.2232 - mae: 1.5015 - 40s/epoch - 2ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 40s - loss: 15.1354 - mse: 15.1354 - mae: 1.4987 - 40s/epoch - 2ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 39s - loss: 15.0855 - mse: 15.0855 - mae: 1.4975 - 39s/epoch - 2ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 38s - loss: 15.1101 - mse: 15.1101 - mae: 1.5023 - 38s/epoch - 2ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 38s - loss: 14.9751 - mse: 14.9751 - mae: 1.4938 - 38s/epoch - 2ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 37s - loss: 14.9219 - mse: 14.9219 - mae: 1.4919 - 37s/epoch - 2ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 36s - loss: 14.9266 - mse: 14.9266 - mae: 1.4917 - 36s/epoch - 2ms/step\n",
            "Score for fold 2: loss of 11.07700252532959\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 25s - loss: 14.9011 - mse: 14.9011 - mae: 1.4901 - 25s/epoch - 1ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 26s - loss: 14.9153 - mse: 14.9153 - mae: 1.4904 - 26s/epoch - 1ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 25s - loss: 14.9276 - mse: 14.9276 - mae: 1.4908 - 25s/epoch - 1ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 24s - loss: 14.9588 - mse: 14.9588 - mae: 1.4961 - 24s/epoch - 1ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 25s - loss: 14.8525 - mse: 14.8525 - mae: 1.4904 - 25s/epoch - 1ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 25s - loss: 14.7951 - mse: 14.7951 - mae: 1.4874 - 25s/epoch - 1ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 29s - loss: 14.8741 - mse: 14.8741 - mae: 1.4924 - 29s/epoch - 1ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 37s - loss: 14.8177 - mse: 14.8177 - mae: 1.4894 - 37s/epoch - 2ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 36s - loss: 14.7735 - mse: 14.7735 - mae: 1.4866 - 36s/epoch - 2ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 35s - loss: 14.7075 - mse: 14.7075 - mae: 1.4855 - 35s/epoch - 2ms/step\n",
            "Score for fold 3: loss of 10.629579544067383\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-09-30 09:10:44,031]\u001b[0m Finished trial#24 resulted in value: 14.707450866699219. Current best value is 14.707450866699219 with parameters: {'activation': 'tanh', 'optimizer': 'sgd', 'num_hidden_layer': 2, 'num_hidden_unit': 67, 'kernel_size': 2, 'filter': 22, 'learning_rate': 0.000689843638273629}.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 29s - loss: 17.2209 - mse: 17.2209 - mae: 1.5951 - 29s/epoch - 1ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 38s - loss: 16.0866 - mse: 16.0866 - mae: 1.5348 - 38s/epoch - 2ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 38s - loss: 15.8045 - mse: 15.8045 - mae: 1.5223 - 38s/epoch - 2ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 37s - loss: 15.5589 - mse: 15.5589 - mae: 1.5122 - 37s/epoch - 2ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 36s - loss: 15.4475 - mse: 15.4475 - mae: 1.5080 - 36s/epoch - 2ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 35s - loss: 15.3481 - mse: 15.3481 - mae: 1.5044 - 35s/epoch - 2ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 34s - loss: 15.2527 - mse: 15.2527 - mae: 1.5011 - 34s/epoch - 2ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 33s - loss: 15.2027 - mse: 15.2027 - mae: 1.4994 - 33s/epoch - 2ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 33s - loss: 15.1356 - mse: 15.1356 - mae: 1.4976 - 33s/epoch - 2ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 32s - loss: 15.1554 - mse: 15.1554 - mae: 1.4977 - 32s/epoch - 2ms/step\n",
            "Score for fold 1: loss of 24.713560104370117\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 32s - loss: 15.1084 - mse: 15.1084 - mae: 1.4962 - 32s/epoch - 2ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 33s - loss: 15.0449 - mse: 15.0449 - mae: 1.4943 - 33s/epoch - 2ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 31s - loss: 14.9552 - mse: 14.9552 - mae: 1.4920 - 31s/epoch - 2ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 30s - loss: 14.9944 - mse: 14.9944 - mae: 1.4944 - 30s/epoch - 1ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 30s - loss: 14.9149 - mse: 14.9149 - mae: 1.4899 - 30s/epoch - 1ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 29s - loss: 14.9275 - mse: 14.9275 - mae: 1.4902 - 29s/epoch - 1ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 28s - loss: 14.9355 - mse: 14.9355 - mae: 1.4934 - 28s/epoch - 1ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 28s - loss: 14.7735 - mse: 14.7735 - mae: 1.4874 - 28s/epoch - 1ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 28s - loss: 14.7540 - mse: 14.7540 - mae: 1.4884 - 28s/epoch - 1ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 28s - loss: 14.8798 - mse: 14.8798 - mae: 1.4919 - 28s/epoch - 1ms/step\n",
            "Score for fold 2: loss of 10.956591606140137\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 28s - loss: 14.8213 - mse: 14.8213 - mae: 1.4901 - 28s/epoch - 1ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 28s - loss: 14.7213 - mse: 14.7213 - mae: 1.4850 - 28s/epoch - 1ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 28s - loss: 14.7245 - mse: 14.7245 - mae: 1.4832 - 28s/epoch - 1ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 28s - loss: 14.6961 - mse: 14.6961 - mae: 1.4821 - 28s/epoch - 1ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 28s - loss: 14.6924 - mse: 14.6924 - mae: 1.4878 - 28s/epoch - 1ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 28s - loss: 14.5993 - mse: 14.5993 - mae: 1.4847 - 28s/epoch - 1ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 28s - loss: 14.6023 - mse: 14.6023 - mae: 1.4880 - 28s/epoch - 1ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 27s - loss: 14.5102 - mse: 14.5102 - mae: 1.4806 - 27s/epoch - 1ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 28s - loss: 14.6205 - mse: 14.6205 - mae: 1.4860 - 28s/epoch - 1ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 28s - loss: 14.5853 - mse: 14.5853 - mae: 1.4871 - 28s/epoch - 1ms/step\n",
            "Score for fold 3: loss of 10.906414031982422\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-09-30 09:26:15,847]\u001b[0m Finished trial#25 resulted in value: 14.5852689743042. Current best value is 14.5852689743042 with parameters: {'activation': 'tanh', 'optimizer': 'sgd', 'num_hidden_layer': 3, 'num_hidden_unit': 69, 'kernel_size': 2, 'filter': 38, 'learning_rate': 0.0006622826526133496}.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 24s - loss: 18.2413 - mse: 18.2413 - mae: 1.6802 - 24s/epoch - 1ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 23s - loss: 17.5125 - mse: 17.5125 - mae: 1.6473 - 23s/epoch - 1ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 23s - loss: 17.1226 - mse: 17.1226 - mae: 1.6161 - 23s/epoch - 1ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 23s - loss: 16.9800 - mse: 16.9800 - mae: 1.6084 - 23s/epoch - 1ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 23s - loss: 16.9858 - mse: 16.9858 - mae: 1.6094 - 23s/epoch - 1ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 23s - loss: 17.0000 - mse: 17.0000 - mae: 1.6058 - 23s/epoch - 1ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 23s - loss: 16.9800 - mse: 16.9800 - mae: 1.6064 - 23s/epoch - 1ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 23s - loss: 17.0102 - mse: 17.0102 - mae: 1.6114 - 23s/epoch - 1ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 22s - loss: 17.0058 - mse: 17.0058 - mae: 1.6053 - 22s/epoch - 1ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 22s - loss: 16.9487 - mse: 16.9487 - mae: 1.6050 - 22s/epoch - 1ms/step\n",
            "Score for fold 1: loss of 23.693187713623047\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 22s - loss: 17.1398 - mse: 17.1398 - mae: 1.6075 - 22s/epoch - 1ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 23s - loss: 17.0308 - mse: 17.0308 - mae: 1.6097 - 23s/epoch - 1ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 23s - loss: 17.0434 - mse: 17.0434 - mae: 1.6108 - 23s/epoch - 1ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 22s - loss: 17.0712 - mse: 17.0712 - mae: 1.6103 - 22s/epoch - 1ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 22s - loss: 17.0609 - mse: 17.0609 - mae: 1.6144 - 22s/epoch - 1ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 22s - loss: 17.0224 - mse: 17.0224 - mae: 1.6100 - 22s/epoch - 1ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 22s - loss: 17.0519 - mse: 17.0519 - mae: 1.6188 - 22s/epoch - 1ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 22s - loss: 17.0140 - mse: 17.0140 - mae: 1.6093 - 22s/epoch - 1ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 23s - loss: 17.0536 - mse: 17.0536 - mae: 1.6096 - 23s/epoch - 1ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 23s - loss: 17.0321 - mse: 17.0321 - mae: 1.6117 - 23s/epoch - 1ms/step\n",
            "Score for fold 2: loss of 12.574555397033691\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 23s - loss: 17.0769 - mse: 17.0769 - mae: 1.6108 - 23s/epoch - 1ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 23s - loss: 17.0397 - mse: 17.0397 - mae: 1.6200 - 23s/epoch - 1ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 23s - loss: 16.9586 - mse: 16.9586 - mae: 1.6120 - 23s/epoch - 1ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 23s - loss: 16.9682 - mse: 16.9682 - mae: 1.6104 - 23s/epoch - 1ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 23s - loss: 17.0497 - mse: 17.0497 - mae: 1.6110 - 23s/epoch - 1ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 23s - loss: 17.0328 - mse: 17.0328 - mae: 1.6136 - 23s/epoch - 1ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 23s - loss: 17.0352 - mse: 17.0352 - mae: 1.6151 - 23s/epoch - 1ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 23s - loss: 17.0146 - mse: 17.0146 - mae: 1.6130 - 23s/epoch - 1ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 23s - loss: 17.3558 - mse: 17.3558 - mae: 1.6188 - 23s/epoch - 1ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 22s - loss: 17.2645 - mse: 17.2645 - mae: 1.6226 - 22s/epoch - 1ms/step\n",
            "Score for fold 3: loss of 12.729856491088867\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-09-30 09:37:46,367]\u001b[0m Finished trial#26 resulted in value: 17.264522552490234. Current best value is 14.5852689743042 with parameters: {'activation': 'tanh', 'optimizer': 'sgd', 'num_hidden_layer': 3, 'num_hidden_unit': 69, 'kernel_size': 2, 'filter': 38, 'learning_rate': 0.0006622826526133496}.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 26s - loss: 17.4227 - mse: 17.4227 - mae: 1.6101 - 26s/epoch - 1ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 26s - loss: 16.2957 - mse: 16.2957 - mae: 1.5484 - 26s/epoch - 1ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 26s - loss: 15.8913 - mse: 15.8913 - mae: 1.5244 - 26s/epoch - 1ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 25s - loss: 15.6758 - mse: 15.6758 - mae: 1.5165 - 25s/epoch - 1ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 25s - loss: 15.5645 - mse: 15.5645 - mae: 1.5122 - 25s/epoch - 1ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 25s - loss: 15.4437 - mse: 15.4437 - mae: 1.5059 - 25s/epoch - 1ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 26s - loss: 15.4185 - mse: 15.4185 - mae: 1.5055 - 26s/epoch - 1ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 25s - loss: 15.3427 - mse: 15.3427 - mae: 1.5043 - 25s/epoch - 1ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 26s - loss: 15.3576 - mse: 15.3576 - mae: 1.5052 - 26s/epoch - 1ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 26s - loss: 15.2173 - mse: 15.2173 - mae: 1.4995 - 26s/epoch - 1ms/step\n",
            "Score for fold 1: loss of 25.589588165283203\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 25s - loss: 15.1490 - mse: 15.1490 - mae: 1.4955 - 25s/epoch - 1ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 25s - loss: 15.2216 - mse: 15.2216 - mae: 1.4990 - 25s/epoch - 1ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 25s - loss: 15.1064 - mse: 15.1064 - mae: 1.4976 - 25s/epoch - 1ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 26s - loss: 15.0655 - mse: 15.0655 - mae: 1.4941 - 26s/epoch - 1ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 25s - loss: 15.1128 - mse: 15.1128 - mae: 1.5014 - 25s/epoch - 1ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 26s - loss: 15.0482 - mse: 15.0482 - mae: 1.4945 - 26s/epoch - 1ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 25s - loss: 14.9880 - mse: 14.9880 - mae: 1.4918 - 25s/epoch - 1ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 25s - loss: 15.0002 - mse: 15.0002 - mae: 1.4918 - 25s/epoch - 1ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 25s - loss: 14.8954 - mse: 14.8954 - mae: 1.4859 - 25s/epoch - 1ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 26s - loss: 14.8725 - mse: 14.8725 - mae: 1.4880 - 26s/epoch - 1ms/step\n",
            "Score for fold 2: loss of 11.406647682189941\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 26s - loss: 14.8487 - mse: 14.8487 - mae: 1.4861 - 26s/epoch - 1ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 26s - loss: 14.7925 - mse: 14.7925 - mae: 1.4816 - 26s/epoch - 1ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 26s - loss: 14.8627 - mse: 14.8627 - mae: 1.4902 - 26s/epoch - 1ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 25s - loss: 14.7878 - mse: 14.7878 - mae: 1.4854 - 25s/epoch - 1ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 26s - loss: 14.7501 - mse: 14.7501 - mae: 1.4820 - 26s/epoch - 1ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 26s - loss: 14.6577 - mse: 14.6577 - mae: 1.4799 - 26s/epoch - 1ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 25s - loss: 14.5891 - mse: 14.5891 - mae: 1.4761 - 25s/epoch - 1ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 26s - loss: 14.5987 - mse: 14.5987 - mae: 1.4759 - 26s/epoch - 1ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 25s - loss: 14.6036 - mse: 14.6036 - mae: 1.4802 - 25s/epoch - 1ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 25s - loss: 14.5887 - mse: 14.5887 - mae: 1.4825 - 25s/epoch - 1ms/step\n",
            "Score for fold 3: loss of 10.738475799560547\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-09-30 09:50:42,123]\u001b[0m Finished trial#27 resulted in value: 14.588662147521973. Current best value is 14.5852689743042 with parameters: {'activation': 'tanh', 'optimizer': 'sgd', 'num_hidden_layer': 3, 'num_hidden_unit': 69, 'kernel_size': 2, 'filter': 38, 'learning_rate': 0.0006622826526133496}.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 27s - loss: 17.3219 - mse: 17.3219 - mae: 1.6102 - 27s/epoch - 1ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 27s - loss: 16.3053 - mse: 16.3053 - mae: 1.5631 - 27s/epoch - 1ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 27s - loss: 15.8850 - mse: 15.8850 - mae: 1.5318 - 27s/epoch - 1ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 26s - loss: 15.6603 - mse: 15.6603 - mae: 1.5196 - 26s/epoch - 1ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 26s - loss: 15.4977 - mse: 15.4977 - mae: 1.5121 - 26s/epoch - 1ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 26s - loss: 15.3728 - mse: 15.3728 - mae: 1.5072 - 26s/epoch - 1ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 27s - loss: 15.3232 - mse: 15.3232 - mae: 1.5070 - 27s/epoch - 1ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 27s - loss: 15.1937 - mse: 15.1937 - mae: 1.4981 - 27s/epoch - 1ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 27s - loss: 15.1408 - mse: 15.1408 - mae: 1.4970 - 27s/epoch - 1ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 27s - loss: 15.0908 - mse: 15.0908 - mae: 1.4950 - 27s/epoch - 1ms/step\n",
            "Score for fold 1: loss of 24.705875396728516\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 27s - loss: 15.1082 - mse: 15.1082 - mae: 1.4959 - 27s/epoch - 1ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 27s - loss: 15.0870 - mse: 15.0870 - mae: 1.4955 - 27s/epoch - 1ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 26s - loss: 15.0201 - mse: 15.0201 - mae: 1.4916 - 26s/epoch - 1ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 26s - loss: 15.0082 - mse: 15.0082 - mae: 1.4919 - 26s/epoch - 1ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 27s - loss: 14.9354 - mse: 14.9354 - mae: 1.4887 - 27s/epoch - 1ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 27s - loss: 14.8407 - mse: 14.8407 - mae: 1.4845 - 27s/epoch - 1ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 27s - loss: 14.8879 - mse: 14.8879 - mae: 1.4878 - 27s/epoch - 1ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 26s - loss: 14.7929 - mse: 14.7929 - mae: 1.4839 - 26s/epoch - 1ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 27s - loss: 14.7548 - mse: 14.7548 - mae: 1.4836 - 27s/epoch - 1ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 26s - loss: 14.6879 - mse: 14.6879 - mae: 1.4790 - 26s/epoch - 1ms/step\n",
            "Score for fold 2: loss of 10.923906326293945\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 27s - loss: 14.6595 - mse: 14.6595 - mae: 1.4768 - 27s/epoch - 1ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 28s - loss: 14.7226 - mse: 14.7226 - mae: 1.4817 - 28s/epoch - 1ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 27s - loss: 14.6682 - mse: 14.6682 - mae: 1.4786 - 27s/epoch - 1ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 27s - loss: 14.5956 - mse: 14.5956 - mae: 1.4759 - 27s/epoch - 1ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 28s - loss: 14.5703 - mse: 14.5703 - mae: 1.4735 - 28s/epoch - 1ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 27s - loss: 14.5071 - mse: 14.5071 - mae: 1.4720 - 27s/epoch - 1ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 28s - loss: 14.4620 - mse: 14.4620 - mae: 1.4697 - 28s/epoch - 1ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 29s - loss: 14.4430 - mse: 14.4430 - mae: 1.4720 - 29s/epoch - 1ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 28s - loss: 14.3812 - mse: 14.3812 - mae: 1.4680 - 28s/epoch - 1ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 29s - loss: 14.3818 - mse: 14.3818 - mae: 1.4662 - 29s/epoch - 1ms/step\n",
            "Score for fold 3: loss of 10.813464164733887\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-09-30 10:04:21,667]\u001b[0m Finished trial#28 resulted in value: 14.38178539276123. Current best value is 14.38178539276123 with parameters: {'activation': 'tanh', 'optimizer': 'sgd', 'num_hidden_layer': 2, 'num_hidden_unit': 81, 'kernel_size': 2, 'filter': 40, 'learning_rate': 0.000693576191437307}.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 25s - loss: 18.2375 - mse: 18.2375 - mae: 1.6973 - 25s/epoch - 1ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 24s - loss: 17.5338 - mse: 17.5338 - mae: 1.6615 - 24s/epoch - 1ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 26s - loss: 17.1127 - mse: 17.1127 - mae: 1.6189 - 26s/epoch - 1ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 26s - loss: 17.1439 - mse: 17.1440 - mae: 1.6223 - 26s/epoch - 1ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 25s - loss: 17.1919 - mse: 17.1919 - mae: 1.6336 - 25s/epoch - 1ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 25s - loss: 17.2281 - mse: 17.2281 - mae: 1.6305 - 25s/epoch - 1ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 24s - loss: 17.0748 - mse: 17.0748 - mae: 1.6254 - 24s/epoch - 1ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 23s - loss: 17.1153 - mse: 17.1153 - mae: 1.6256 - 23s/epoch - 1ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 23s - loss: 17.3180 - mse: 17.3180 - mae: 1.6332 - 23s/epoch - 1ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 23s - loss: 17.2387 - mse: 17.2387 - mae: 1.6316 - 23s/epoch - 1ms/step\n",
            "Score for fold 1: loss of 24.712106704711914\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 23s - loss: 17.1490 - mse: 17.1490 - mae: 1.6266 - 23s/epoch - 1ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 22s - loss: 17.1210 - mse: 17.1210 - mae: 1.6241 - 22s/epoch - 1ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 23s - loss: 17.2793 - mse: 17.2793 - mae: 1.6274 - 23s/epoch - 1ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 23s - loss: 17.1366 - mse: 17.1366 - mae: 1.6246 - 23s/epoch - 1ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 23s - loss: 17.1170 - mse: 17.1170 - mae: 1.6236 - 23s/epoch - 1ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 24s - loss: 17.1761 - mse: 17.1761 - mae: 1.6204 - 24s/epoch - 1ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 23s - loss: 17.2885 - mse: 17.2885 - mae: 1.6305 - 23s/epoch - 1ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 24s - loss: 17.5446 - mse: 17.5446 - mae: 1.6729 - 24s/epoch - 1ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 23s - loss: 17.4173 - mse: 17.4173 - mae: 1.6358 - 23s/epoch - 1ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 23s - loss: 17.2962 - mse: 17.2962 - mae: 1.6325 - 23s/epoch - 1ms/step\n",
            "Score for fold 2: loss of 12.757411003112793\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 23s - loss: 17.4296 - mse: 17.4296 - mae: 1.6316 - 23s/epoch - 1ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 24s - loss: 17.3918 - mse: 17.3918 - mae: 1.6312 - 24s/epoch - 1ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 24s - loss: 17.4282 - mse: 17.4282 - mae: 1.6497 - 24s/epoch - 1ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 24s - loss: 17.2483 - mse: 17.2483 - mae: 1.6265 - 24s/epoch - 1ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 23s - loss: 17.2138 - mse: 17.2138 - mae: 1.6273 - 23s/epoch - 1ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 23s - loss: 17.1582 - mse: 17.1582 - mae: 1.6280 - 23s/epoch - 1ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 23s - loss: 17.2202 - mse: 17.2202 - mae: 1.6261 - 23s/epoch - 1ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 22s - loss: 17.2053 - mse: 17.2053 - mae: 1.6270 - 22s/epoch - 1ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 23s - loss: 17.1699 - mse: 17.1699 - mae: 1.6243 - 23s/epoch - 1ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 22s - loss: 17.1833 - mse: 17.1833 - mae: 1.6295 - 22s/epoch - 1ms/step\n",
            "Score for fold 3: loss of 13.215439796447754\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-09-30 10:16:19,293]\u001b[0m Finished trial#29 resulted in value: 17.18332290649414. Current best value is 14.38178539276123 with parameters: {'activation': 'tanh', 'optimizer': 'sgd', 'num_hidden_layer': 2, 'num_hidden_unit': 81, 'kernel_size': 2, 'filter': 40, 'learning_rate': 0.000693576191437307}.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 36s - loss: 18.6714 - mse: 18.6714 - mae: 1.7494 - 36s/epoch - 2ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 35s - loss: 18.1829 - mse: 18.1829 - mae: 1.7270 - 35s/epoch - 2ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 36s - loss: 18.2936 - mse: 18.2936 - mae: 1.7399 - 36s/epoch - 2ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 36s - loss: 18.4457 - mse: 18.4457 - mae: 1.7455 - 36s/epoch - 2ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 37s - loss: 18.0440 - mse: 18.0440 - mae: 1.7484 - 37s/epoch - 2ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 36s - loss: 18.0634 - mse: 18.0634 - mae: 1.7351 - 36s/epoch - 2ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 37s - loss: 18.6488 - mse: 18.6488 - mae: 1.7806 - 37s/epoch - 2ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 37s - loss: 19.1109 - mse: 19.1109 - mae: 1.8289 - 37s/epoch - 2ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 37s - loss: 18.8874 - mse: 18.8874 - mae: 1.8308 - 37s/epoch - 2ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 36s - loss: 18.7304 - mse: 18.7304 - mae: 1.8024 - 36s/epoch - 2ms/step\n",
            "Score for fold 1: loss of 23.807310104370117\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 36s - loss: 19.0607 - mse: 19.0607 - mae: 1.8195 - 36s/epoch - 2ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 36s - loss: 18.9505 - mse: 18.9505 - mae: 1.8420 - 36s/epoch - 2ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 36s - loss: 18.5671 - mse: 18.5671 - mae: 1.7982 - 36s/epoch - 2ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 36s - loss: 18.1132 - mse: 18.1132 - mae: 1.7603 - 36s/epoch - 2ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 36s - loss: 18.6675 - mse: 18.6675 - mae: 1.8232 - 36s/epoch - 2ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 36s - loss: 18.5647 - mse: 18.5647 - mae: 1.8059 - 36s/epoch - 2ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 36s - loss: 18.2275 - mse: 18.2275 - mae: 1.7727 - 36s/epoch - 2ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 36s - loss: 18.4924 - mse: 18.4924 - mae: 1.7901 - 36s/epoch - 2ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 36s - loss: 18.9327 - mse: 18.9327 - mae: 1.8494 - 36s/epoch - 2ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 36s - loss: 18.2856 - mse: 18.2856 - mae: 1.7734 - 36s/epoch - 2ms/step\n",
            "Score for fold 2: loss of 14.312089920043945\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 36s - loss: 17.9450 - mse: 17.9450 - mae: 1.7588 - 36s/epoch - 2ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 36s - loss: 18.0112 - mse: 18.0112 - mae: 1.7550 - 36s/epoch - 2ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 36s - loss: 18.7983 - mse: 18.7983 - mae: 1.8129 - 36s/epoch - 2ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 36s - loss: 18.1798 - mse: 18.1798 - mae: 1.7681 - 36s/epoch - 2ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 37s - loss: 18.4410 - mse: 18.4410 - mae: 1.8069 - 37s/epoch - 2ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 36s - loss: 18.2737 - mse: 18.2737 - mae: 1.8008 - 36s/epoch - 2ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 36s - loss: 18.7559 - mse: 18.7559 - mae: 1.8459 - 36s/epoch - 2ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 36s - loss: 18.8842 - mse: 18.8842 - mae: 1.8289 - 36s/epoch - 2ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 36s - loss: 18.3446 - mse: 18.3446 - mae: 1.8138 - 36s/epoch - 2ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 36s - loss: 18.4844 - mse: 18.4844 - mae: 1.8087 - 36s/epoch - 2ms/step\n",
            "Score for fold 3: loss of 13.367654800415039\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-09-30 10:34:35,262]\u001b[0m Finished trial#30 resulted in value: 18.48440933227539. Current best value is 14.38178539276123 with parameters: {'activation': 'tanh', 'optimizer': 'sgd', 'num_hidden_layer': 2, 'num_hidden_unit': 81, 'kernel_size': 2, 'filter': 40, 'learning_rate': 0.000693576191437307}.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 29s - loss: 17.8741 - mse: 17.8742 - mae: 1.5884 - 29s/epoch - 1ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 29s - loss: 17.3386 - mse: 17.3386 - mae: 1.5585 - 29s/epoch - 1ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 29s - loss: 17.1320 - mse: 17.1320 - mae: 1.5466 - 29s/epoch - 1ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 29s - loss: 16.9958 - mse: 16.9958 - mae: 1.5399 - 29s/epoch - 1ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 28s - loss: 16.8942 - mse: 16.8942 - mae: 1.5350 - 28s/epoch - 1ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 29s - loss: 16.8165 - mse: 16.8165 - mae: 1.5306 - 29s/epoch - 1ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 29s - loss: 16.7507 - mse: 16.7507 - mae: 1.5282 - 29s/epoch - 1ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 29s - loss: 16.6960 - mse: 16.6960 - mae: 1.5263 - 29s/epoch - 1ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 29s - loss: 16.6508 - mse: 16.6508 - mae: 1.5240 - 29s/epoch - 1ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 29s - loss: 16.6094 - mse: 16.6094 - mae: 1.5225 - 29s/epoch - 1ms/step\n",
            "Score for fold 1: loss of 23.89369773864746\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 28s - loss: 16.5720 - mse: 16.5720 - mae: 1.5211 - 28s/epoch - 1ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 29s - loss: 16.5411 - mse: 16.5411 - mae: 1.5199 - 29s/epoch - 1ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 28s - loss: 16.5094 - mse: 16.5094 - mae: 1.5186 - 28s/epoch - 1ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 28s - loss: 16.4835 - mse: 16.4835 - mae: 1.5174 - 28s/epoch - 1ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 29s - loss: 16.4584 - mse: 16.4584 - mae: 1.5161 - 29s/epoch - 1ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 28s - loss: 16.4357 - mse: 16.4357 - mae: 1.5162 - 28s/epoch - 1ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 27s - loss: 16.4118 - mse: 16.4118 - mae: 1.5149 - 27s/epoch - 1ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 28s - loss: 16.3936 - mse: 16.3936 - mae: 1.5142 - 28s/epoch - 1ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 28s - loss: 16.3752 - mse: 16.3752 - mae: 1.5134 - 28s/epoch - 1ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 28s - loss: 16.3559 - mse: 16.3559 - mae: 1.5127 - 28s/epoch - 1ms/step\n",
            "Score for fold 2: loss of 12.36133098602295\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 28s - loss: 16.3405 - mse: 16.3405 - mae: 1.5124 - 28s/epoch - 1ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 28s - loss: 16.3231 - mse: 16.3231 - mae: 1.5114 - 28s/epoch - 1ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 28s - loss: 16.3089 - mse: 16.3089 - mae: 1.5107 - 28s/epoch - 1ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 28s - loss: 16.2937 - mse: 16.2937 - mae: 1.5102 - 28s/epoch - 1ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 28s - loss: 16.2802 - mse: 16.2802 - mae: 1.5099 - 28s/epoch - 1ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 28s - loss: 16.2662 - mse: 16.2662 - mae: 1.5092 - 28s/epoch - 1ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 28s - loss: 16.2512 - mse: 16.2512 - mae: 1.5098 - 28s/epoch - 1ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 29s - loss: 16.2414 - mse: 16.2414 - mae: 1.5081 - 29s/epoch - 1ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 29s - loss: 16.2280 - mse: 16.2280 - mae: 1.5079 - 29s/epoch - 1ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 28s - loss: 16.2167 - mse: 16.2167 - mae: 1.5075 - 28s/epoch - 1ms/step\n",
            "Score for fold 3: loss of 12.154995918273926\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-09-30 10:49:00,136]\u001b[0m Finished trial#31 resulted in value: 16.216733932495117. Current best value is 14.38178539276123 with parameters: {'activation': 'tanh', 'optimizer': 'sgd', 'num_hidden_layer': 2, 'num_hidden_unit': 81, 'kernel_size': 2, 'filter': 40, 'learning_rate': 0.000693576191437307}.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 26s - loss: 17.4289 - mse: 17.4289 - mae: 1.6132 - 26s/epoch - 1ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 25s - loss: 16.3243 - mse: 16.3243 - mae: 1.5547 - 25s/epoch - 1ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 26s - loss: 15.8948 - mse: 15.8948 - mae: 1.5270 - 26s/epoch - 1ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 26s - loss: 15.6761 - mse: 15.6761 - mae: 1.5188 - 26s/epoch - 1ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 26s - loss: 15.5558 - mse: 15.5558 - mae: 1.5121 - 26s/epoch - 1ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 26s - loss: 15.4486 - mse: 15.4486 - mae: 1.5073 - 26s/epoch - 1ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 26s - loss: 15.3801 - mse: 15.3801 - mae: 1.5053 - 26s/epoch - 1ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 26s - loss: 15.3342 - mse: 15.3342 - mae: 1.5038 - 26s/epoch - 1ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 26s - loss: 15.2060 - mse: 15.2060 - mae: 1.4990 - 26s/epoch - 1ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 25s - loss: 15.2443 - mse: 15.2443 - mae: 1.5018 - 25s/epoch - 1ms/step\n",
            "Score for fold 1: loss of 25.521242141723633\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 26s - loss: 15.1931 - mse: 15.1931 - mae: 1.4976 - 26s/epoch - 1ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 26s - loss: 15.1701 - mse: 15.1701 - mae: 1.5017 - 26s/epoch - 1ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 26s - loss: 15.0660 - mse: 15.0660 - mae: 1.4949 - 26s/epoch - 1ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 25s - loss: 15.0232 - mse: 15.0232 - mae: 1.4922 - 25s/epoch - 1ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 25s - loss: 14.9853 - mse: 14.9853 - mae: 1.4909 - 25s/epoch - 1ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 25s - loss: 14.9684 - mse: 14.9684 - mae: 1.4907 - 25s/epoch - 1ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 24s - loss: 14.9407 - mse: 14.9407 - mae: 1.4886 - 24s/epoch - 1ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 25s - loss: 14.8953 - mse: 14.8953 - mae: 1.4872 - 25s/epoch - 1ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 25s - loss: 14.9960 - mse: 14.9960 - mae: 1.4988 - 25s/epoch - 1ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 25s - loss: 14.9628 - mse: 14.9628 - mae: 1.4906 - 25s/epoch - 1ms/step\n",
            "Score for fold 2: loss of 10.929169654846191\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 25s - loss: 14.8482 - mse: 14.8482 - mae: 1.4857 - 25s/epoch - 1ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 25s - loss: 14.7640 - mse: 14.7640 - mae: 1.4836 - 25s/epoch - 1ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 25s - loss: 14.7247 - mse: 14.7247 - mae: 1.4810 - 25s/epoch - 1ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 26s - loss: 14.7860 - mse: 14.7860 - mae: 1.4927 - 26s/epoch - 1ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 25s - loss: 14.7909 - mse: 14.7909 - mae: 1.4896 - 25s/epoch - 1ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 26s - loss: 14.6401 - mse: 14.6401 - mae: 1.4813 - 26s/epoch - 1ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 26s - loss: 14.6424 - mse: 14.6424 - mae: 1.4831 - 26s/epoch - 1ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 26s - loss: 14.6194 - mse: 14.6194 - mae: 1.4806 - 26s/epoch - 1ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 26s - loss: 14.6040 - mse: 14.6040 - mae: 1.4835 - 26s/epoch - 1ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 25s - loss: 14.6514 - mse: 14.6514 - mae: 1.4884 - 25s/epoch - 1ms/step\n",
            "Score for fold 3: loss of 10.656362533569336\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-09-30 11:01:56,945]\u001b[0m Finished trial#32 resulted in value: 14.651439666748047. Current best value is 14.38178539276123 with parameters: {'activation': 'tanh', 'optimizer': 'sgd', 'num_hidden_layer': 2, 'num_hidden_unit': 81, 'kernel_size': 2, 'filter': 40, 'learning_rate': 0.000693576191437307}.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 26s - loss: 17.3727 - mse: 17.3727 - mae: 1.6116 - 26s/epoch - 1ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 25s - loss: 16.3254 - mse: 16.3254 - mae: 1.5579 - 25s/epoch - 1ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 26s - loss: 15.8984 - mse: 15.8984 - mae: 1.5308 - 26s/epoch - 1ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 26s - loss: 15.7130 - mse: 15.7130 - mae: 1.5205 - 26s/epoch - 1ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 26s - loss: 15.5670 - mse: 15.5670 - mae: 1.5136 - 26s/epoch - 1ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 26s - loss: 15.4736 - mse: 15.4736 - mae: 1.5090 - 26s/epoch - 1ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 26s - loss: 15.3797 - mse: 15.3797 - mae: 1.5051 - 26s/epoch - 1ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 27s - loss: 15.2705 - mse: 15.2705 - mae: 1.5021 - 27s/epoch - 1ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 27s - loss: 15.2475 - mse: 15.2475 - mae: 1.5001 - 27s/epoch - 1ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 28s - loss: 15.1847 - mse: 15.1847 - mae: 1.4987 - 28s/epoch - 1ms/step\n",
            "Score for fold 1: loss of 24.790664672851562\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 27s - loss: 15.1611 - mse: 15.1611 - mae: 1.4973 - 27s/epoch - 1ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 27s - loss: 15.0826 - mse: 15.0826 - mae: 1.4932 - 27s/epoch - 1ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 28s - loss: 15.0666 - mse: 15.0666 - mae: 1.4911 - 28s/epoch - 1ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 27s - loss: 14.9791 - mse: 14.9791 - mae: 1.4907 - 27s/epoch - 1ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 26s - loss: 14.9764 - mse: 14.9764 - mae: 1.4896 - 26s/epoch - 1ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 27s - loss: 15.0417 - mse: 15.0417 - mae: 1.4935 - 27s/epoch - 1ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 27s - loss: 14.9667 - mse: 14.9667 - mae: 1.4918 - 27s/epoch - 1ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 28s - loss: 14.9279 - mse: 14.9279 - mae: 1.4882 - 28s/epoch - 1ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 27s - loss: 14.9202 - mse: 14.9202 - mae: 1.4869 - 27s/epoch - 1ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 26s - loss: 14.8881 - mse: 14.8881 - mae: 1.4898 - 26s/epoch - 1ms/step\n",
            "Score for fold 2: loss of 10.867837905883789\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 25s - loss: 14.8676 - mse: 14.8676 - mae: 1.4915 - 25s/epoch - 1ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 25s - loss: 14.8397 - mse: 14.8397 - mae: 1.4861 - 25s/epoch - 1ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 25s - loss: 14.7889 - mse: 14.7889 - mae: 1.4834 - 25s/epoch - 1ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 26s - loss: 14.8116 - mse: 14.8116 - mae: 1.4849 - 26s/epoch - 1ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 26s - loss: 14.7264 - mse: 14.7264 - mae: 1.4816 - 26s/epoch - 1ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 25s - loss: 14.6559 - mse: 14.6559 - mae: 1.4797 - 25s/epoch - 1ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 25s - loss: 14.6474 - mse: 14.6474 - mae: 1.4785 - 25s/epoch - 1ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 25s - loss: 14.5914 - mse: 14.5914 - mae: 1.4767 - 25s/epoch - 1ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 25s - loss: 14.6250 - mse: 14.6250 - mae: 1.4770 - 25s/epoch - 1ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 26s - loss: 14.5643 - mse: 14.5643 - mae: 1.4755 - 26s/epoch - 1ms/step\n",
            "Score for fold 3: loss of 10.685906410217285\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-09-30 11:15:14,475]\u001b[0m Finished trial#33 resulted in value: 14.564294815063477. Current best value is 14.38178539276123 with parameters: {'activation': 'tanh', 'optimizer': 'sgd', 'num_hidden_layer': 2, 'num_hidden_unit': 81, 'kernel_size': 2, 'filter': 40, 'learning_rate': 0.000693576191437307}.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 28s - loss: 19.4287 - mse: 19.4287 - mae: 1.8157 - 28s/epoch - 1ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 27s - loss: 20.7127 - mse: 20.7127 - mae: 2.0242 - 27s/epoch - 1ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 28s - loss: 21.0122 - mse: 21.0122 - mae: 2.0499 - 28s/epoch - 1ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 28s - loss: 21.9686 - mse: 21.9686 - mae: 2.1305 - 28s/epoch - 1ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 28s - loss: 21.6944 - mse: 21.6944 - mae: 2.0984 - 28s/epoch - 1ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 28s - loss: 21.6259 - mse: 21.6259 - mae: 2.1077 - 28s/epoch - 1ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 27s - loss: 22.5719 - mse: 22.5719 - mae: 2.1794 - 27s/epoch - 1ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 27s - loss: 22.0212 - mse: 22.0212 - mae: 2.1253 - 27s/epoch - 1ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 28s - loss: 21.7642 - mse: 21.7642 - mae: 2.1101 - 28s/epoch - 1ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 29s - loss: 21.8854 - mse: 21.8854 - mae: 2.1116 - 29s/epoch - 1ms/step\n",
            "Score for fold 1: loss of 23.38573455810547\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 29s - loss: 22.1270 - mse: 22.1270 - mae: 2.1342 - 29s/epoch - 1ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 28s - loss: 21.6489 - mse: 21.6489 - mae: 2.0829 - 28s/epoch - 1ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 28s - loss: 21.6540 - mse: 21.6540 - mae: 2.1003 - 28s/epoch - 1ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 28s - loss: 22.0144 - mse: 22.0144 - mae: 2.1382 - 28s/epoch - 1ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 28s - loss: 21.6765 - mse: 21.6765 - mae: 2.1023 - 28s/epoch - 1ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 28s - loss: 21.3509 - mse: 21.3509 - mae: 2.0894 - 28s/epoch - 1ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 28s - loss: 21.5568 - mse: 21.5568 - mae: 2.1055 - 28s/epoch - 1ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 28s - loss: 22.0770 - mse: 22.0770 - mae: 2.1402 - 28s/epoch - 1ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 28s - loss: 22.3907 - mse: 22.3907 - mae: 2.1672 - 28s/epoch - 1ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 28s - loss: 21.9626 - mse: 21.9626 - mae: 2.1164 - 28s/epoch - 1ms/step\n",
            "Score for fold 2: loss of 15.396944999694824\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 28s - loss: 22.0889 - mse: 22.0889 - mae: 2.1336 - 28s/epoch - 1ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 27s - loss: 21.7981 - mse: 21.7981 - mae: 2.1079 - 27s/epoch - 1ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 28s - loss: 21.7765 - mse: 21.7765 - mae: 2.1240 - 28s/epoch - 1ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 27s - loss: 21.5990 - mse: 21.5990 - mae: 2.0911 - 27s/epoch - 1ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 28s - loss: 21.5044 - mse: 21.5044 - mae: 2.0837 - 28s/epoch - 1ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 27s - loss: 21.7298 - mse: 21.7298 - mae: 2.0977 - 27s/epoch - 1ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 27s - loss: 21.5005 - mse: 21.5005 - mae: 2.0756 - 27s/epoch - 1ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 28s - loss: 21.2704 - mse: 21.2704 - mae: 2.0450 - 28s/epoch - 1ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 27s - loss: 21.3830 - mse: 21.3830 - mae: 2.0670 - 27s/epoch - 1ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 27s - loss: 21.8753 - mse: 21.8753 - mae: 2.1119 - 27s/epoch - 1ms/step\n",
            "Score for fold 3: loss of 14.114496231079102\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-09-30 11:29:16,406]\u001b[0m Finished trial#34 resulted in value: 21.875328063964844. Current best value is 14.38178539276123 with parameters: {'activation': 'tanh', 'optimizer': 'sgd', 'num_hidden_layer': 2, 'num_hidden_unit': 81, 'kernel_size': 2, 'filter': 40, 'learning_rate': 0.000693576191437307}.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 23s - loss: nan - mse: nan - mae: nan - 23s/epoch - 1ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 23s - loss: nan - mse: nan - mae: nan - 23s/epoch - 1ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 23s - loss: nan - mse: nan - mae: nan - 23s/epoch - 1ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 23s - loss: nan - mse: nan - mae: nan - 23s/epoch - 1ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 23s - loss: nan - mse: nan - mae: nan - 23s/epoch - 1ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 22s - loss: nan - mse: nan - mae: nan - 22s/epoch - 1ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 22s - loss: nan - mse: nan - mae: nan - 22s/epoch - 1ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 23s - loss: nan - mse: nan - mae: nan - 23s/epoch - 1ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 23s - loss: nan - mse: nan - mae: nan - 23s/epoch - 1ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 23s - loss: nan - mse: nan - mae: nan - 23s/epoch - 1ms/step\n",
            "Score for fold 1: loss of 24.36428451538086\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 23s - loss: nan - mse: nan - mae: nan - 23s/epoch - 1ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 23s - loss: nan - mse: nan - mae: nan - 23s/epoch - 1ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 23s - loss: nan - mse: nan - mae: nan - 23s/epoch - 1ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 23s - loss: nan - mse: nan - mae: nan - 23s/epoch - 1ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 23s - loss: nan - mse: nan - mae: nan - 23s/epoch - 1ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 23s - loss: nan - mse: nan - mae: nan - 23s/epoch - 1ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 22s - loss: nan - mse: nan - mae: nan - 22s/epoch - 1ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 22s - loss: nan - mse: nan - mae: nan - 22s/epoch - 1ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 22s - loss: nan - mse: nan - mae: nan - 22s/epoch - 1ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 22s - loss: nan - mse: nan - mae: nan - 22s/epoch - 1ms/step\n",
            "Score for fold 2: loss of nan\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 23s - loss: nan - mse: nan - mae: nan - 23s/epoch - 1ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 23s - loss: nan - mse: nan - mae: nan - 23s/epoch - 1ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 22s - loss: nan - mse: nan - mae: nan - 22s/epoch - 1ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 23s - loss: nan - mse: nan - mae: nan - 23s/epoch - 1ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 22s - loss: nan - mse: nan - mae: nan - 22s/epoch - 1ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 22s - loss: nan - mse: nan - mae: nan - 22s/epoch - 1ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 23s - loss: nan - mse: nan - mae: nan - 23s/epoch - 1ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 23s - loss: nan - mse: nan - mae: nan - 23s/epoch - 1ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 22s - loss: nan - mse: nan - mae: nan - 22s/epoch - 1ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 22s - loss: nan - mse: nan - mae: nan - 22s/epoch - 1ms/step\n",
            "Score for fold 3: loss of nan\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[33m[W 2022-09-30 11:40:47,370]\u001b[0m Setting status of trial#35 as TrialState.FAIL because the objective function returned nan.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 23s - loss: 18.8119 - mse: 18.8119 - mae: 1.7163 - 23s/epoch - 1ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 22s - loss: 19.2819 - mse: 19.2819 - mae: 1.7424 - 22s/epoch - 1ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 23s - loss: 19.2442 - mse: 19.2442 - mae: 1.7466 - 23s/epoch - 1ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 23s - loss: 20.3114 - mse: 20.3114 - mae: 1.7636 - 23s/epoch - 1ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 23s - loss: 19.1407 - mse: 19.1407 - mae: 1.7329 - 23s/epoch - 1ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 22s - loss: 19.0839 - mse: 19.0839 - mae: 1.7266 - 22s/epoch - 1ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 22s - loss: nan - mse: nan - mae: nan - 22s/epoch - 1ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 23s - loss: nan - mse: nan - mae: nan - 23s/epoch - 1ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 23s - loss: nan - mse: nan - mae: nan - 23s/epoch - 1ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 23s - loss: nan - mse: nan - mae: nan - 23s/epoch - 1ms/step\n",
            "Score for fold 1: loss of 24.130361557006836\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 22s - loss: nan - mse: nan - mae: nan - 22s/epoch - 1ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 23s - loss: nan - mse: nan - mae: nan - 23s/epoch - 1ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 22s - loss: nan - mse: nan - mae: nan - 22s/epoch - 1ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 22s - loss: nan - mse: nan - mae: nan - 22s/epoch - 1ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 22s - loss: nan - mse: nan - mae: nan - 22s/epoch - 1ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 22s - loss: nan - mse: nan - mae: nan - 22s/epoch - 1ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 22s - loss: nan - mse: nan - mae: nan - 22s/epoch - 1ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 22s - loss: nan - mse: nan - mae: nan - 22s/epoch - 1ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 22s - loss: nan - mse: nan - mae: nan - 22s/epoch - 1ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 22s - loss: nan - mse: nan - mae: nan - 22s/epoch - 1ms/step\n",
            "Score for fold 2: loss of nan\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 22s - loss: nan - mse: nan - mae: nan - 22s/epoch - 1ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 22s - loss: nan - mse: nan - mae: nan - 22s/epoch - 1ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 22s - loss: nan - mse: nan - mae: nan - 22s/epoch - 1ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 22s - loss: nan - mse: nan - mae: nan - 22s/epoch - 1ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 22s - loss: nan - mse: nan - mae: nan - 22s/epoch - 1ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 22s - loss: nan - mse: nan - mae: nan - 22s/epoch - 1ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 22s - loss: nan - mse: nan - mae: nan - 22s/epoch - 1ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 22s - loss: nan - mse: nan - mae: nan - 22s/epoch - 1ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 23s - loss: nan - mse: nan - mae: nan - 23s/epoch - 1ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 22s - loss: nan - mse: nan - mae: nan - 22s/epoch - 1ms/step\n",
            "Score for fold 3: loss of nan\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[33m[W 2022-09-30 11:52:05,130]\u001b[0m Setting status of trial#36 as TrialState.FAIL because the objective function returned nan.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 23s - loss: nan - mse: nan - mae: nan - 23s/epoch - 1ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 23s - loss: nan - mse: nan - mae: nan - 23s/epoch - 1ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 24s - loss: nan - mse: nan - mae: nan - 24s/epoch - 1ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 23s - loss: nan - mse: nan - mae: nan - 23s/epoch - 1ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 24s - loss: nan - mse: nan - mae: nan - 24s/epoch - 1ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 24s - loss: nan - mse: nan - mae: nan - 24s/epoch - 1ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 24s - loss: nan - mse: nan - mae: nan - 24s/epoch - 1ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 23s - loss: nan - mse: nan - mae: nan - 23s/epoch - 1ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 23s - loss: nan - mse: nan - mae: nan - 23s/epoch - 1ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 23s - loss: nan - mse: nan - mae: nan - 23s/epoch - 1ms/step\n",
            "Score for fold 1: loss of 24.373947143554688\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 23s - loss: nan - mse: nan - mae: nan - 23s/epoch - 1ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 23s - loss: nan - mse: nan - mae: nan - 23s/epoch - 1ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 23s - loss: nan - mse: nan - mae: nan - 23s/epoch - 1ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 23s - loss: nan - mse: nan - mae: nan - 23s/epoch - 1ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 23s - loss: nan - mse: nan - mae: nan - 23s/epoch - 1ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 24s - loss: nan - mse: nan - mae: nan - 24s/epoch - 1ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 23s - loss: nan - mse: nan - mae: nan - 23s/epoch - 1ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 24s - loss: nan - mse: nan - mae: nan - 24s/epoch - 1ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 23s - loss: nan - mse: nan - mae: nan - 23s/epoch - 1ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 23s - loss: nan - mse: nan - mae: nan - 23s/epoch - 1ms/step\n",
            "Score for fold 2: loss of nan\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 24s - loss: nan - mse: nan - mae: nan - 24s/epoch - 1ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 23s - loss: nan - mse: nan - mae: nan - 23s/epoch - 1ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 23s - loss: nan - mse: nan - mae: nan - 23s/epoch - 1ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 23s - loss: nan - mse: nan - mae: nan - 23s/epoch - 1ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 24s - loss: nan - mse: nan - mae: nan - 24s/epoch - 1ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 24s - loss: nan - mse: nan - mae: nan - 24s/epoch - 1ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 24s - loss: nan - mse: nan - mae: nan - 24s/epoch - 1ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 25s - loss: nan - mse: nan - mae: nan - 25s/epoch - 1ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 24s - loss: nan - mse: nan - mae: nan - 24s/epoch - 1ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 25s - loss: nan - mse: nan - mae: nan - 25s/epoch - 1ms/step\n",
            "Score for fold 3: loss of nan\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[33m[W 2022-09-30 12:04:00,466]\u001b[0m Setting status of trial#37 as TrialState.FAIL because the objective function returned nan.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 31s - loss: nan - mse: nan - mae: nan - 31s/epoch - 2ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 33s - loss: nan - mse: nan - mae: nan - 33s/epoch - 2ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 30s - loss: nan - mse: nan - mae: nan - 30s/epoch - 2ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 31s - loss: nan - mse: nan - mae: nan - 31s/epoch - 2ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 25s - loss: nan - mse: nan - mae: nan - 25s/epoch - 1ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 24s - loss: nan - mse: nan - mae: nan - 24s/epoch - 1ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 25s - loss: nan - mse: nan - mae: nan - 25s/epoch - 1ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 24s - loss: nan - mse: nan - mae: nan - 24s/epoch - 1ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 24s - loss: nan - mse: nan - mae: nan - 24s/epoch - 1ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 23s - loss: nan - mse: nan - mae: nan - 23s/epoch - 1ms/step\n",
            "Score for fold 1: loss of 25.607328414916992\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 25s - loss: nan - mse: nan - mae: nan - 25s/epoch - 1ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 26s - loss: nan - mse: nan - mae: nan - 26s/epoch - 1ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 26s - loss: nan - mse: nan - mae: nan - 26s/epoch - 1ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 25s - loss: nan - mse: nan - mae: nan - 25s/epoch - 1ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 25s - loss: nan - mse: nan - mae: nan - 25s/epoch - 1ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 24s - loss: nan - mse: nan - mae: nan - 24s/epoch - 1ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 25s - loss: nan - mse: nan - mae: nan - 25s/epoch - 1ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 25s - loss: nan - mse: nan - mae: nan - 25s/epoch - 1ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 24s - loss: nan - mse: nan - mae: nan - 24s/epoch - 1ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 24s - loss: nan - mse: nan - mae: nan - 24s/epoch - 1ms/step\n",
            "Score for fold 2: loss of nan\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 24s - loss: nan - mse: nan - mae: nan - 24s/epoch - 1ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 24s - loss: nan - mse: nan - mae: nan - 24s/epoch - 1ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 24s - loss: nan - mse: nan - mae: nan - 24s/epoch - 1ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 24s - loss: nan - mse: nan - mae: nan - 24s/epoch - 1ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 24s - loss: nan - mse: nan - mae: nan - 24s/epoch - 1ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 24s - loss: nan - mse: nan - mae: nan - 24s/epoch - 1ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 24s - loss: nan - mse: nan - mae: nan - 24s/epoch - 1ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 24s - loss: nan - mse: nan - mae: nan - 24s/epoch - 1ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 24s - loss: nan - mse: nan - mae: nan - 24s/epoch - 1ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 24s - loss: nan - mse: nan - mae: nan - 24s/epoch - 1ms/step\n",
            "Score for fold 3: loss of nan\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[33m[W 2022-09-30 12:16:51,249]\u001b[0m Setting status of trial#38 as TrialState.FAIL because the objective function returned nan.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 25s - loss: 17.6529 - mse: 17.6529 - mae: 1.6414 - 25s/epoch - 1ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 25s - loss: 17.3671 - mse: 17.3671 - mae: 1.6377 - 25s/epoch - 1ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 25s - loss: 17.9401 - mse: 17.9401 - mae: 1.6741 - 25s/epoch - 1ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 27s - loss: 17.5022 - mse: 17.5022 - mae: 1.6574 - 27s/epoch - 1ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 25s - loss: 17.2625 - mse: 17.2625 - mae: 1.6417 - 25s/epoch - 1ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 25s - loss: 17.2728 - mse: 17.2728 - mae: 1.6393 - 25s/epoch - 1ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 25s - loss: 17.4930 - mse: 17.4930 - mae: 1.6501 - 25s/epoch - 1ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 25s - loss: 17.5367 - mse: 17.5367 - mae: 1.6565 - 25s/epoch - 1ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 25s - loss: 17.2752 - mse: 17.2752 - mae: 1.6358 - 25s/epoch - 1ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 24s - loss: 17.5365 - mse: 17.5365 - mae: 1.6538 - 24s/epoch - 1ms/step\n",
            "Score for fold 1: loss of 24.12310028076172\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 25s - loss: 17.6912 - mse: 17.6912 - mae: 1.6677 - 25s/epoch - 1ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 25s - loss: 17.5937 - mse: 17.5937 - mae: 1.6556 - 25s/epoch - 1ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 25s - loss: 17.4331 - mse: 17.4331 - mae: 1.6597 - 25s/epoch - 1ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 24s - loss: 17.3064 - mse: 17.3064 - mae: 1.6573 - 24s/epoch - 1ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 25s - loss: 17.3031 - mse: 17.3031 - mae: 1.6485 - 25s/epoch - 1ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 25s - loss: 17.0831 - mse: 17.0831 - mae: 1.6398 - 25s/epoch - 1ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 25s - loss: 17.5807 - mse: 17.5807 - mae: 1.6560 - 25s/epoch - 1ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 24s - loss: 17.5141 - mse: 17.5141 - mae: 1.6583 - 24s/epoch - 1ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 24s - loss: 17.4474 - mse: 17.4474 - mae: 1.6650 - 24s/epoch - 1ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 24s - loss: 17.6610 - mse: 17.6610 - mae: 1.6650 - 24s/epoch - 1ms/step\n",
            "Score for fold 2: loss of 12.791524887084961\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 24s - loss: 17.3917 - mse: 17.3917 - mae: 1.6489 - 24s/epoch - 1ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 24s - loss: 17.4922 - mse: 17.4922 - mae: 1.6519 - 24s/epoch - 1ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 24s - loss: 17.2977 - mse: 17.2977 - mae: 1.6526 - 24s/epoch - 1ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 24s - loss: 17.0564 - mse: 17.0564 - mae: 1.6429 - 24s/epoch - 1ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 23s - loss: 17.1420 - mse: 17.1420 - mae: 1.6354 - 23s/epoch - 1ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 24s - loss: 17.1975 - mse: 17.1975 - mae: 1.6412 - 24s/epoch - 1ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 24s - loss: 17.6639 - mse: 17.6639 - mae: 1.6670 - 24s/epoch - 1ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 25s - loss: 17.3422 - mse: 17.3422 - mae: 1.6522 - 25s/epoch - 1ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 24s - loss: 17.4836 - mse: 17.4836 - mae: 1.6549 - 24s/epoch - 1ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 24s - loss: 17.5149 - mse: 17.5149 - mae: 1.6643 - 24s/epoch - 1ms/step\n",
            "Score for fold 3: loss of 12.886427879333496\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-09-30 12:29:17,727]\u001b[0m Finished trial#39 resulted in value: 17.514904022216797. Current best value is 14.38178539276123 with parameters: {'activation': 'tanh', 'optimizer': 'sgd', 'num_hidden_layer': 2, 'num_hidden_unit': 81, 'kernel_size': 2, 'filter': 40, 'learning_rate': 0.000693576191437307}.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 41s - loss: 17.2446 - mse: 17.2446 - mae: 1.5998 - 41s/epoch - 2ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 43s - loss: 16.6507 - mse: 16.6507 - mae: 1.5776 - 43s/epoch - 2ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 41s - loss: 16.4684 - mse: 16.4684 - mae: 1.5728 - 41s/epoch - 2ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 42s - loss: 16.4912 - mse: 16.4912 - mae: 1.5682 - 42s/epoch - 2ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 41s - loss: 16.3788 - mse: 16.3788 - mae: 1.5656 - 41s/epoch - 2ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 41s - loss: 16.3208 - mse: 16.3208 - mae: 1.5631 - 41s/epoch - 2ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 40s - loss: 16.2788 - mse: 16.2788 - mae: 1.5616 - 40s/epoch - 2ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 40s - loss: 16.1532 - mse: 16.1532 - mae: 1.5592 - 40s/epoch - 2ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 40s - loss: 16.1081 - mse: 16.1081 - mae: 1.5599 - 40s/epoch - 2ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 41s - loss: 16.2602 - mse: 16.2602 - mae: 1.5578 - 41s/epoch - 2ms/step\n",
            "Score for fold 1: loss of 23.293851852416992\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 40s - loss: 16.0798 - mse: 16.0798 - mae: 1.5545 - 40s/epoch - 2ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 40s - loss: 16.0059 - mse: 16.0059 - mae: 1.5526 - 40s/epoch - 2ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 40s - loss: 16.0080 - mse: 16.0080 - mae: 1.5535 - 40s/epoch - 2ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 40s - loss: 16.0356 - mse: 16.0356 - mae: 1.5442 - 40s/epoch - 2ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 40s - loss: 16.0288 - mse: 16.0288 - mae: 1.5419 - 40s/epoch - 2ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 44s - loss: 15.9194 - mse: 15.9194 - mae: 1.5416 - 44s/epoch - 2ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 44s - loss: 15.9041 - mse: 15.9041 - mae: 1.5384 - 44s/epoch - 2ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 42s - loss: 16.0584 - mse: 16.0584 - mae: 1.5375 - 42s/epoch - 2ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 42s - loss: 16.0089 - mse: 16.0089 - mae: 1.5394 - 42s/epoch - 2ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 42s - loss: 15.9363 - mse: 15.9363 - mae: 1.5381 - 42s/epoch - 2ms/step\n",
            "Score for fold 2: loss of 12.104045867919922\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 42s - loss: 15.9202 - mse: 15.9202 - mae: 1.5375 - 42s/epoch - 2ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 41s - loss: 15.8742 - mse: 15.8742 - mae: 1.5388 - 41s/epoch - 2ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 43s - loss: 15.8350 - mse: 15.8350 - mae: 1.5364 - 43s/epoch - 2ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 42s - loss: 15.8912 - mse: 15.8912 - mae: 1.5362 - 42s/epoch - 2ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 42s - loss: 15.8117 - mse: 15.8117 - mae: 1.5350 - 42s/epoch - 2ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 40s - loss: 15.8635 - mse: 15.8635 - mae: 1.5351 - 40s/epoch - 2ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 41s - loss: 15.8960 - mse: 15.8960 - mae: 1.5353 - 41s/epoch - 2ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 41s - loss: 15.8397 - mse: 15.8397 - mae: 1.5340 - 41s/epoch - 2ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 41s - loss: 15.8286 - mse: 15.8286 - mae: 1.5359 - 41s/epoch - 2ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 41s - loss: 16.0104 - mse: 16.0104 - mae: 1.5350 - 41s/epoch - 2ms/step\n",
            "Score for fold 3: loss of 11.552197456359863\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-09-30 12:50:04,933]\u001b[0m Finished trial#40 resulted in value: 16.0103816986084. Current best value is 14.38178539276123 with parameters: {'activation': 'tanh', 'optimizer': 'sgd', 'num_hidden_layer': 2, 'num_hidden_unit': 81, 'kernel_size': 2, 'filter': 40, 'learning_rate': 0.000693576191437307}.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 30s - loss: 17.9462 - mse: 17.9462 - mae: 1.6294 - 30s/epoch - 2ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 30s - loss: 16.8324 - mse: 16.8324 - mae: 1.5757 - 30s/epoch - 1ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 30s - loss: 16.4106 - mse: 16.4106 - mae: 1.5548 - 30s/epoch - 2ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 30s - loss: 16.1334 - mse: 16.1334 - mae: 1.5420 - 30s/epoch - 2ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 31s - loss: 15.9644 - mse: 15.9644 - mae: 1.5342 - 31s/epoch - 2ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 31s - loss: 15.8045 - mse: 15.8045 - mae: 1.5243 - 31s/epoch - 2ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 30s - loss: 15.6811 - mse: 15.6811 - mae: 1.5174 - 30s/epoch - 2ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 30s - loss: 15.5723 - mse: 15.5723 - mae: 1.5107 - 30s/epoch - 1ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 31s - loss: 15.4797 - mse: 15.4797 - mae: 1.5053 - 31s/epoch - 2ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 31s - loss: 15.4043 - mse: 15.4043 - mae: 1.5006 - 31s/epoch - 2ms/step\n",
            "Score for fold 1: loss of 23.92235565185547\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 31s - loss: 15.3573 - mse: 15.3573 - mae: 1.4988 - 31s/epoch - 2ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 31s - loss: 15.2937 - mse: 15.2937 - mae: 1.4959 - 31s/epoch - 2ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 31s - loss: 15.2162 - mse: 15.2162 - mae: 1.4934 - 31s/epoch - 2ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 31s - loss: 15.1886 - mse: 15.1886 - mae: 1.4904 - 31s/epoch - 2ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 30s - loss: 15.1430 - mse: 15.1430 - mae: 1.4888 - 30s/epoch - 2ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 30s - loss: 15.0949 - mse: 15.0949 - mae: 1.4872 - 30s/epoch - 1ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 30s - loss: 15.0990 - mse: 15.0990 - mae: 1.4876 - 30s/epoch - 1ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 30s - loss: 15.0393 - mse: 15.0393 - mae: 1.4850 - 30s/epoch - 1ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 30s - loss: 15.0195 - mse: 15.0195 - mae: 1.4836 - 30s/epoch - 1ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 30s - loss: 14.9682 - mse: 14.9682 - mae: 1.4823 - 30s/epoch - 2ms/step\n",
            "Score for fold 2: loss of 11.217016220092773\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 29s - loss: 14.9393 - mse: 14.9393 - mae: 1.4801 - 29s/epoch - 1ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 29s - loss: 14.9001 - mse: 14.9001 - mae: 1.4795 - 29s/epoch - 1ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 30s - loss: 14.8818 - mse: 14.8818 - mae: 1.4787 - 30s/epoch - 1ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 29s - loss: 14.8463 - mse: 14.8463 - mae: 1.4768 - 29s/epoch - 1ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 29s - loss: 14.8065 - mse: 14.8065 - mae: 1.4761 - 29s/epoch - 1ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 29s - loss: 14.8380 - mse: 14.8380 - mae: 1.4767 - 29s/epoch - 1ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 29s - loss: 14.7634 - mse: 14.7634 - mae: 1.4743 - 29s/epoch - 1ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 29s - loss: 14.7296 - mse: 14.7296 - mae: 1.4742 - 29s/epoch - 1ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 29s - loss: 14.7079 - mse: 14.7079 - mae: 1.4724 - 29s/epoch - 1ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 29s - loss: 14.6843 - mse: 14.6843 - mae: 1.4723 - 29s/epoch - 1ms/step\n",
            "Score for fold 3: loss of 10.68216609954834\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-09-30 13:05:17,414]\u001b[0m Finished trial#41 resulted in value: 14.684320449829102. Current best value is 14.38178539276123 with parameters: {'activation': 'tanh', 'optimizer': 'sgd', 'num_hidden_layer': 2, 'num_hidden_unit': 81, 'kernel_size': 2, 'filter': 40, 'learning_rate': 0.000693576191437307}.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 28s - loss: 18.3279 - mse: 18.3279 - mae: 1.6373 - 28s/epoch - 1ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 27s - loss: 17.7134 - mse: 17.7134 - mae: 1.6021 - 27s/epoch - 1ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 27s - loss: 17.3230 - mse: 17.3230 - mae: 1.5938 - 27s/epoch - 1ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 27s - loss: 17.1677 - mse: 17.1677 - mae: 1.5919 - 27s/epoch - 1ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 27s - loss: 17.0725 - mse: 17.0725 - mae: 1.5895 - 27s/epoch - 1ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 27s - loss: 17.0157 - mse: 17.0157 - mae: 1.5899 - 27s/epoch - 1ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 27s - loss: 16.9602 - mse: 16.9602 - mae: 1.5889 - 27s/epoch - 1ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 28s - loss: 16.9216 - mse: 16.9216 - mae: 1.5879 - 28s/epoch - 1ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 28s - loss: 16.8837 - mse: 16.8837 - mae: 1.5880 - 28s/epoch - 1ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 28s - loss: 16.8626 - mse: 16.8626 - mae: 1.5888 - 28s/epoch - 1ms/step\n",
            "Score for fold 1: loss of 23.221689224243164\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 27s - loss: 16.8213 - mse: 16.8213 - mae: 1.5890 - 27s/epoch - 1ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 27s - loss: 16.8087 - mse: 16.8087 - mae: 1.5866 - 27s/epoch - 1ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 27s - loss: 16.8173 - mse: 16.8173 - mae: 1.5880 - 27s/epoch - 1ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 27s - loss: 16.8346 - mse: 16.8346 - mae: 1.5887 - 27s/epoch - 1ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 27s - loss: 16.8155 - mse: 16.8155 - mae: 1.5896 - 27s/epoch - 1ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 28s - loss: 16.8204 - mse: 16.8204 - mae: 1.5899 - 28s/epoch - 1ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 27s - loss: 16.8249 - mse: 16.8249 - mae: 1.5905 - 27s/epoch - 1ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 27s - loss: 16.8361 - mse: 16.8361 - mae: 1.5904 - 27s/epoch - 1ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 27s - loss: 16.8102 - mse: 16.8102 - mae: 1.5913 - 27s/epoch - 1ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 27s - loss: 16.7813 - mse: 16.7813 - mae: 1.5925 - 27s/epoch - 1ms/step\n",
            "Score for fold 2: loss of 12.423508644104004\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 27s - loss: 16.7687 - mse: 16.7687 - mae: 1.5926 - 27s/epoch - 1ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 27s - loss: 16.7901 - mse: 16.7901 - mae: 1.5928 - 27s/epoch - 1ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 27s - loss: 16.7640 - mse: 16.7640 - mae: 1.5913 - 27s/epoch - 1ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 28s - loss: 16.7645 - mse: 16.7645 - mae: 1.5920 - 28s/epoch - 1ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 28s - loss: 16.7569 - mse: 16.7569 - mae: 1.5915 - 28s/epoch - 1ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 31s - loss: 16.7459 - mse: 16.7459 - mae: 1.5922 - 31s/epoch - 2ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 27s - loss: 16.7868 - mse: 16.7868 - mae: 1.5923 - 27s/epoch - 1ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 28s - loss: 16.7662 - mse: 16.7662 - mae: 1.5922 - 28s/epoch - 1ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 28s - loss: 16.7605 - mse: 16.7605 - mae: 1.5950 - 28s/epoch - 1ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 27s - loss: 16.7589 - mse: 16.7589 - mae: 1.5941 - 27s/epoch - 1ms/step\n",
            "Score for fold 3: loss of 12.424995422363281\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-09-30 13:19:14,698]\u001b[0m Finished trial#42 resulted in value: 16.75894546508789. Current best value is 14.38178539276123 with parameters: {'activation': 'tanh', 'optimizer': 'sgd', 'num_hidden_layer': 2, 'num_hidden_unit': 81, 'kernel_size': 2, 'filter': 40, 'learning_rate': 0.000693576191437307}.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 31s - loss: 17.8704 - mse: 17.8704 - mae: 1.6315 - 31s/epoch - 2ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 31s - loss: 17.2433 - mse: 17.2433 - mae: 1.6064 - 31s/epoch - 2ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 31s - loss: 17.0856 - mse: 17.0856 - mae: 1.6032 - 31s/epoch - 2ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 31s - loss: 17.0127 - mse: 17.0127 - mae: 1.6034 - 31s/epoch - 2ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 32s - loss: 16.9733 - mse: 16.9733 - mae: 1.6030 - 32s/epoch - 2ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 30s - loss: 16.9460 - mse: 16.9460 - mae: 1.6031 - 30s/epoch - 2ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 28s - loss: 16.9258 - mse: 16.9258 - mae: 1.6031 - 28s/epoch - 1ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 28s - loss: 16.9101 - mse: 16.9101 - mae: 1.6036 - 28s/epoch - 1ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 28s - loss: 16.8980 - mse: 16.8980 - mae: 1.6029 - 28s/epoch - 1ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 29s - loss: 16.8878 - mse: 16.8878 - mae: 1.6047 - 29s/epoch - 1ms/step\n",
            "Score for fold 1: loss of 23.444196701049805\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 28s - loss: 16.8791 - mse: 16.8791 - mae: 1.6040 - 28s/epoch - 1ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 28s - loss: 16.8712 - mse: 16.8712 - mae: 1.6032 - 28s/epoch - 1ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 27s - loss: 16.8643 - mse: 16.8643 - mae: 1.6026 - 27s/epoch - 1ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 27s - loss: 16.8582 - mse: 16.8582 - mae: 1.6035 - 27s/epoch - 1ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 28s - loss: 16.8526 - mse: 16.8526 - mae: 1.6025 - 28s/epoch - 1ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 27s - loss: 16.8483 - mse: 16.8483 - mae: 1.6027 - 27s/epoch - 1ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 27s - loss: 16.8431 - mse: 16.8431 - mae: 1.6020 - 27s/epoch - 1ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 27s - loss: 16.8383 - mse: 16.8383 - mae: 1.6022 - 27s/epoch - 1ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 27s - loss: 16.8339 - mse: 16.8339 - mae: 1.6020 - 27s/epoch - 1ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 28s - loss: 16.8305 - mse: 16.8305 - mae: 1.6020 - 28s/epoch - 1ms/step\n",
            "Score for fold 2: loss of 12.24199104309082\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 27s - loss: 16.8271 - mse: 16.8271 - mae: 1.6013 - 27s/epoch - 1ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 28s - loss: 16.8234 - mse: 16.8234 - mae: 1.6017 - 28s/epoch - 1ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 27s - loss: 16.8202 - mse: 16.8202 - mae: 1.6009 - 27s/epoch - 1ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 27s - loss: 16.8170 - mse: 16.8170 - mae: 1.6001 - 27s/epoch - 1ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 28s - loss: 16.8143 - mse: 16.8143 - mae: 1.6017 - 28s/epoch - 1ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 27s - loss: 16.8117 - mse: 16.8117 - mae: 1.6007 - 27s/epoch - 1ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 28s - loss: 16.8084 - mse: 16.8084 - mae: 1.5988 - 28s/epoch - 1ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 27s - loss: 16.8065 - mse: 16.8065 - mae: 1.6015 - 27s/epoch - 1ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 28s - loss: 16.8034 - mse: 16.8034 - mae: 1.6013 - 28s/epoch - 1ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 28s - loss: 16.8014 - mse: 16.8014 - mae: 1.5991 - 28s/epoch - 1ms/step\n",
            "Score for fold 3: loss of 12.207408905029297\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-09-30 13:33:35,318]\u001b[0m Finished trial#43 resulted in value: 16.801410675048828. Current best value is 14.38178539276123 with parameters: {'activation': 'tanh', 'optimizer': 'sgd', 'num_hidden_layer': 2, 'num_hidden_unit': 81, 'kernel_size': 2, 'filter': 40, 'learning_rate': 0.000693576191437307}.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 29s - loss: 16.9491 - mse: 16.9491 - mae: 1.5822 - 29s/epoch - 1ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 28s - loss: 16.8484 - mse: 16.8484 - mae: 1.5826 - 28s/epoch - 1ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 28s - loss: 17.2963 - mse: 17.2963 - mae: 1.6010 - 28s/epoch - 1ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 28s - loss: 17.7343 - mse: 17.7343 - mae: 1.6168 - 28s/epoch - 1ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 29s - loss: 17.8177 - mse: 17.8177 - mae: 1.6282 - 29s/epoch - 1ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 28s - loss: 17.6641 - mse: 17.6642 - mae: 1.6370 - 28s/epoch - 1ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 28s - loss: 17.6396 - mse: 17.6396 - mae: 1.6398 - 28s/epoch - 1ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 29s - loss: 17.6573 - mse: 17.6573 - mae: 1.6428 - 29s/epoch - 1ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 28s - loss: 17.7231 - mse: 17.7231 - mae: 1.6471 - 28s/epoch - 1ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 28s - loss: 17.8088 - mse: 17.8088 - mae: 1.6500 - 28s/epoch - 1ms/step\n",
            "Score for fold 1: loss of 26.394392013549805\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 28s - loss: 17.7284 - mse: 17.7284 - mae: 1.6437 - 28s/epoch - 1ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 29s - loss: 17.7989 - mse: 17.7989 - mae: 1.6419 - 29s/epoch - 1ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 28s - loss: 17.8847 - mse: 17.8847 - mae: 1.6436 - 28s/epoch - 1ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 28s - loss: 17.9272 - mse: 17.9272 - mae: 1.6430 - 28s/epoch - 1ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 28s - loss: 18.0520 - mse: 18.0520 - mae: 1.6392 - 28s/epoch - 1ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 28s - loss: 17.9998 - mse: 17.9998 - mae: 1.6351 - 28s/epoch - 1ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 27s - loss: 18.1303 - mse: 18.1303 - mae: 1.6354 - 27s/epoch - 1ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 28s - loss: 18.0131 - mse: 18.0131 - mae: 1.6385 - 28s/epoch - 1ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 28s - loss: 18.1751 - mse: 18.1751 - mae: 1.6423 - 28s/epoch - 1ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 27s - loss: 18.1254 - mse: 18.1254 - mae: 1.6374 - 27s/epoch - 1ms/step\n",
            "Score for fold 2: loss of 13.717713356018066\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 28s - loss: 18.4183 - mse: 18.4183 - mae: 1.6297 - 28s/epoch - 1ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 28s - loss: 18.1455 - mse: 18.1455 - mae: 1.6290 - 28s/epoch - 1ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 27s - loss: 17.7860 - mse: 17.7860 - mae: 1.6294 - 27s/epoch - 1ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 27s - loss: 18.5610 - mse: 18.5610 - mae: 1.6394 - 27s/epoch - 1ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 28s - loss: 18.7188 - mse: 18.7188 - mae: 1.6436 - 28s/epoch - 1ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 27s - loss: 18.5460 - mse: 18.5460 - mae: 1.6454 - 27s/epoch - 1ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 27s - loss: 20.7476 - mse: 20.7476 - mae: 1.6514 - 27s/epoch - 1ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 28s - loss: 19.6748 - mse: 19.6748 - mae: 1.6573 - 28s/epoch - 1ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 27s - loss: 18.8849 - mse: 18.8849 - mae: 1.6633 - 27s/epoch - 1ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 27s - loss: 18.4983 - mse: 18.4983 - mae: 1.6393 - 27s/epoch - 1ms/step\n",
            "Score for fold 3: loss of 13.230597496032715\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-09-30 13:47:38,475]\u001b[0m Finished trial#44 resulted in value: 18.49834442138672. Current best value is 14.38178539276123 with parameters: {'activation': 'tanh', 'optimizer': 'sgd', 'num_hidden_layer': 2, 'num_hidden_unit': 81, 'kernel_size': 2, 'filter': 40, 'learning_rate': 0.000693576191437307}.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 21s - loss: 18.0377 - mse: 18.0377 - mae: 1.6768 - 21s/epoch - 1ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 20s - loss: 17.3926 - mse: 17.3926 - mae: 1.6411 - 20s/epoch - 1ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 21s - loss: 17.2502 - mse: 17.2502 - mae: 1.6288 - 21s/epoch - 1ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 21s - loss: 17.2661 - mse: 17.2661 - mae: 1.6295 - 21s/epoch - 1ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 21s - loss: 17.3371 - mse: 17.3371 - mae: 1.6328 - 21s/epoch - 1ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 20s - loss: 17.5206 - mse: 17.5206 - mae: 1.6387 - 20s/epoch - 1ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 21s - loss: 17.6886 - mse: 17.6886 - mae: 1.6642 - 21s/epoch - 1ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 20s - loss: 17.6323 - mse: 17.6323 - mae: 1.6478 - 20s/epoch - 1ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 20s - loss: 17.5850 - mse: 17.5850 - mae: 1.6501 - 20s/epoch - 1ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 21s - loss: 17.6211 - mse: 17.6211 - mae: 1.6485 - 21s/epoch - 1ms/step\n",
            "Score for fold 1: loss of 21.84690284729004\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 20s - loss: 17.5888 - mse: 17.5888 - mae: 1.6464 - 20s/epoch - 999us/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 21s - loss: 17.5287 - mse: 17.5287 - mae: 1.6463 - 21s/epoch - 1ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 21s - loss: 17.4330 - mse: 17.4330 - mae: 1.6430 - 21s/epoch - 1ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 21s - loss: 17.5955 - mse: 17.5955 - mae: 1.6455 - 21s/epoch - 1ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 20s - loss: 17.6663 - mse: 17.6663 - mae: 1.6506 - 20s/epoch - 1ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 20s - loss: 17.6503 - mse: 17.6503 - mae: 1.6526 - 20s/epoch - 1ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 20s - loss: 17.6041 - mse: 17.6041 - mae: 1.6543 - 20s/epoch - 992us/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 21s - loss: 17.5224 - mse: 17.5224 - mae: 1.6514 - 21s/epoch - 1ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 21s - loss: 17.4287 - mse: 17.4287 - mae: 1.6513 - 21s/epoch - 1ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 20s - loss: 17.8388 - mse: 17.8388 - mae: 1.6935 - 20s/epoch - 1ms/step\n",
            "Score for fold 2: loss of 13.104246139526367\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 20s - loss: 17.5947 - mse: 17.5947 - mae: 1.6493 - 20s/epoch - 1ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 20s - loss: 17.4477 - mse: 17.4477 - mae: 1.6489 - 20s/epoch - 1ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 20s - loss: 17.8472 - mse: 17.8472 - mae: 1.6591 - 20s/epoch - 1ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 20s - loss: 17.7959 - mse: 17.7959 - mae: 1.6600 - 20s/epoch - 1ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 20s - loss: 17.6347 - mse: 17.6347 - mae: 1.6500 - 20s/epoch - 978us/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 20s - loss: 17.5068 - mse: 17.5068 - mae: 1.6552 - 20s/epoch - 998us/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 20s - loss: 17.4376 - mse: 17.4376 - mae: 1.6529 - 20s/epoch - 1ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 20s - loss: 17.3848 - mse: 17.3848 - mae: 1.6580 - 20s/epoch - 986us/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 20s - loss: 18.2322 - mse: 18.2322 - mae: 1.7303 - 20s/epoch - 1ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 20s - loss: 18.2681 - mse: 18.2681 - mae: 1.6933 - 20s/epoch - 999us/step\n",
            "Score for fold 3: loss of 13.160038948059082\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-09-30 13:57:57,737]\u001b[0m Finished trial#45 resulted in value: 18.268056869506836. Current best value is 14.38178539276123 with parameters: {'activation': 'tanh', 'optimizer': 'sgd', 'num_hidden_layer': 2, 'num_hidden_unit': 81, 'kernel_size': 2, 'filter': 40, 'learning_rate': 0.000693576191437307}.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 27s - loss: 17.3636 - mse: 17.3636 - mae: 1.6061 - 27s/epoch - 1ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 28s - loss: 16.3302 - mse: 16.3302 - mae: 1.5561 - 28s/epoch - 1ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 28s - loss: 15.8979 - mse: 15.8979 - mae: 1.5327 - 28s/epoch - 1ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 26s - loss: 15.6989 - mse: 15.6989 - mae: 1.5204 - 26s/epoch - 1ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 25s - loss: 15.5506 - mse: 15.5506 - mae: 1.5132 - 25s/epoch - 1ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 25s - loss: 15.4306 - mse: 15.4306 - mae: 1.5054 - 25s/epoch - 1ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 25s - loss: 15.3349 - mse: 15.3349 - mae: 1.5045 - 25s/epoch - 1ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 25s - loss: 15.2772 - mse: 15.2772 - mae: 1.5017 - 25s/epoch - 1ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 25s - loss: 15.1546 - mse: 15.1546 - mae: 1.4968 - 25s/epoch - 1ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 25s - loss: 15.1052 - mse: 15.1052 - mae: 1.4943 - 25s/epoch - 1ms/step\n",
            "Score for fold 1: loss of 23.97454071044922\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 25s - loss: 15.0702 - mse: 15.0702 - mae: 1.4931 - 25s/epoch - 1ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 25s - loss: 15.0451 - mse: 15.0451 - mae: 1.4946 - 25s/epoch - 1ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 25s - loss: 14.9917 - mse: 14.9917 - mae: 1.4880 - 25s/epoch - 1ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 25s - loss: 14.9437 - mse: 14.9437 - mae: 1.4877 - 25s/epoch - 1ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 25s - loss: 14.9254 - mse: 14.9254 - mae: 1.4879 - 25s/epoch - 1ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 25s - loss: 14.8723 - mse: 14.8723 - mae: 1.4848 - 25s/epoch - 1ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 25s - loss: 14.8339 - mse: 14.8339 - mae: 1.4825 - 25s/epoch - 1ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 25s - loss: 14.7896 - mse: 14.7896 - mae: 1.4808 - 25s/epoch - 1ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 26s - loss: 14.7536 - mse: 14.7536 - mae: 1.4801 - 26s/epoch - 1ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 25s - loss: 14.7379 - mse: 14.7379 - mae: 1.4776 - 25s/epoch - 1ms/step\n",
            "Score for fold 2: loss of 10.86792278289795\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 26s - loss: 14.6904 - mse: 14.6904 - mae: 1.4771 - 26s/epoch - 1ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 26s - loss: 14.6228 - mse: 14.6228 - mae: 1.4753 - 26s/epoch - 1ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 25s - loss: 14.6264 - mse: 14.6264 - mae: 1.4785 - 25s/epoch - 1ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 25s - loss: 14.6050 - mse: 14.6050 - mae: 1.4755 - 25s/epoch - 1ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 25s - loss: 14.6591 - mse: 14.6591 - mae: 1.4936 - 25s/epoch - 1ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 25s - loss: 14.5745 - mse: 14.5745 - mae: 1.4761 - 25s/epoch - 1ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 25s - loss: 14.5083 - mse: 14.5083 - mae: 1.4723 - 25s/epoch - 1ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 25s - loss: 14.5504 - mse: 14.5504 - mae: 1.4795 - 25s/epoch - 1ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 25s - loss: 14.4783 - mse: 14.4783 - mae: 1.4717 - 25s/epoch - 1ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 26s - loss: 14.3950 - mse: 14.3950 - mae: 1.4675 - 26s/epoch - 1ms/step\n",
            "Score for fold 3: loss of 10.54679012298584\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-09-30 14:10:51,226]\u001b[0m Finished trial#46 resulted in value: 14.395009994506836. Current best value is 14.38178539276123 with parameters: {'activation': 'tanh', 'optimizer': 'sgd', 'num_hidden_layer': 2, 'num_hidden_unit': 81, 'kernel_size': 2, 'filter': 40, 'learning_rate': 0.000693576191437307}.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 25s - loss: 17.1156 - mse: 17.1156 - mae: 1.6059 - 25s/epoch - 1ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 24s - loss: 16.1366 - mse: 16.1366 - mae: 1.5490 - 24s/epoch - 1ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 24s - loss: 15.9060 - mse: 15.9060 - mae: 1.5370 - 24s/epoch - 1ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 24s - loss: 15.7938 - mse: 15.7938 - mae: 1.5371 - 24s/epoch - 1ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 24s - loss: 15.6450 - mse: 15.6450 - mae: 1.5292 - 24s/epoch - 1ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 24s - loss: 15.6468 - mse: 15.6468 - mae: 1.5248 - 24s/epoch - 1ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 23s - loss: 15.7050 - mse: 15.7050 - mae: 1.5294 - 23s/epoch - 1ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 23s - loss: 15.6968 - mse: 15.6968 - mae: 1.5313 - 23s/epoch - 1ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 25s - loss: 15.5835 - mse: 15.5835 - mae: 1.5255 - 25s/epoch - 1ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 24s - loss: 15.5085 - mse: 15.5085 - mae: 1.5202 - 24s/epoch - 1ms/step\n",
            "Score for fold 1: loss of 24.659317016601562\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 23s - loss: 15.4539 - mse: 15.4539 - mae: 1.5204 - 23s/epoch - 1ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 25s - loss: 15.3187 - mse: 15.3187 - mae: 1.5151 - 25s/epoch - 1ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 24s - loss: 15.2608 - mse: 15.2608 - mae: 1.5171 - 24s/epoch - 1ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 23s - loss: 15.2519 - mse: 15.2519 - mae: 1.5125 - 23s/epoch - 1ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 23s - loss: 15.1706 - mse: 15.1706 - mae: 1.5107 - 23s/epoch - 1ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 25s - loss: 15.2134 - mse: 15.2134 - mae: 1.5148 - 25s/epoch - 1ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 23s - loss: 15.4229 - mse: 15.4229 - mae: 1.5238 - 23s/epoch - 1ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 24s - loss: 15.2016 - mse: 15.2016 - mae: 1.5144 - 24s/epoch - 1ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 23s - loss: 15.0894 - mse: 15.0894 - mae: 1.5094 - 23s/epoch - 1ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 23s - loss: 15.0622 - mse: 15.0622 - mae: 1.5065 - 23s/epoch - 1ms/step\n",
            "Score for fold 2: loss of 11.136448860168457\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 22s - loss: 15.1068 - mse: 15.1068 - mae: 1.5092 - 22s/epoch - 1ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 23s - loss: 15.0038 - mse: 15.0038 - mae: 1.5039 - 23s/epoch - 1ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 23s - loss: 14.9902 - mse: 14.9902 - mae: 1.5031 - 23s/epoch - 1ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 23s - loss: 15.1487 - mse: 15.1487 - mae: 1.5213 - 23s/epoch - 1ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 23s - loss: 15.0016 - mse: 15.0016 - mae: 1.5084 - 23s/epoch - 1ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 23s - loss: 15.0176 - mse: 15.0176 - mae: 1.5111 - 23s/epoch - 1ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 23s - loss: 15.0091 - mse: 15.0091 - mae: 1.5066 - 23s/epoch - 1ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 23s - loss: 14.9175 - mse: 14.9175 - mae: 1.5052 - 23s/epoch - 1ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 23s - loss: 14.9871 - mse: 14.9871 - mae: 1.5165 - 23s/epoch - 1ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 23s - loss: 14.8921 - mse: 14.8921 - mae: 1.5083 - 23s/epoch - 1ms/step\n",
            "Score for fold 3: loss of 11.155153274536133\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-09-30 14:22:44,173]\u001b[0m Finished trial#47 resulted in value: 14.892133712768555. Current best value is 14.38178539276123 with parameters: {'activation': 'tanh', 'optimizer': 'sgd', 'num_hidden_layer': 2, 'num_hidden_unit': 81, 'kernel_size': 2, 'filter': 40, 'learning_rate': 0.000693576191437307}.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 26s - loss: 17.5871 - mse: 17.5871 - mae: 1.6167 - 26s/epoch - 1ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 26s - loss: 16.5663 - mse: 16.5663 - mae: 1.5715 - 26s/epoch - 1ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 26s - loss: 16.1452 - mse: 16.1452 - mae: 1.5472 - 26s/epoch - 1ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 26s - loss: 15.8713 - mse: 15.8713 - mae: 1.5282 - 26s/epoch - 1ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 26s - loss: 15.7001 - mse: 15.7001 - mae: 1.5195 - 26s/epoch - 1ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 27s - loss: 15.5796 - mse: 15.5796 - mae: 1.5129 - 27s/epoch - 1ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 26s - loss: 15.4305 - mse: 15.4305 - mae: 1.5063 - 26s/epoch - 1ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 26s - loss: 15.3667 - mse: 15.3667 - mae: 1.5023 - 26s/epoch - 1ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 27s - loss: 15.3119 - mse: 15.3119 - mae: 1.5006 - 27s/epoch - 1ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 27s - loss: 15.2601 - mse: 15.2601 - mae: 1.4985 - 27s/epoch - 1ms/step\n",
            "Score for fold 1: loss of 23.429912567138672\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 27s - loss: 15.2055 - mse: 15.2055 - mae: 1.4966 - 27s/epoch - 1ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 27s - loss: 15.1517 - mse: 15.1517 - mae: 1.4935 - 27s/epoch - 1ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 26s - loss: 15.0481 - mse: 15.0481 - mae: 1.4904 - 26s/epoch - 1ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 26s - loss: 15.0190 - mse: 15.0190 - mae: 1.4889 - 26s/epoch - 1ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 30s - loss: 14.9658 - mse: 14.9658 - mae: 1.4868 - 30s/epoch - 1ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 66s - loss: 14.9360 - mse: 14.9360 - mae: 1.4856 - 66s/epoch - 3ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 66s - loss: 14.9095 - mse: 14.9095 - mae: 1.4848 - 66s/epoch - 3ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 65s - loss: 14.8519 - mse: 14.8519 - mae: 1.4811 - 65s/epoch - 3ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 65s - loss: 14.8293 - mse: 14.8293 - mae: 1.4803 - 65s/epoch - 3ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 59s - loss: 14.7878 - mse: 14.7878 - mae: 1.4793 - 59s/epoch - 3ms/step\n",
            "Score for fold 2: loss of 10.90858268737793\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 52s - loss: 14.7649 - mse: 14.7649 - mae: 1.4787 - 52s/epoch - 3ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 57s - loss: 14.7179 - mse: 14.7179 - mae: 1.4759 - 57s/epoch - 3ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 57s - loss: 14.6903 - mse: 14.6903 - mae: 1.4747 - 57s/epoch - 3ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 56s - loss: 14.6750 - mse: 14.6750 - mae: 1.4754 - 56s/epoch - 3ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 55s - loss: 14.6306 - mse: 14.6306 - mae: 1.4729 - 55s/epoch - 3ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 55s - loss: 14.6003 - mse: 14.6003 - mae: 1.4713 - 55s/epoch - 3ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 55s - loss: 14.6317 - mse: 14.6317 - mae: 1.4746 - 55s/epoch - 3ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 54s - loss: 14.5580 - mse: 14.5580 - mae: 1.4715 - 54s/epoch - 3ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 53s - loss: 14.5223 - mse: 14.5223 - mae: 1.4695 - 53s/epoch - 3ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 48s - loss: 14.4846 - mse: 14.4846 - mae: 1.4676 - 48s/epoch - 2ms/step\n",
            "Score for fold 3: loss of 10.97659969329834\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-09-30 14:43:55,882]\u001b[0m Finished trial#48 resulted in value: 14.484635353088379. Current best value is 14.38178539276123 with parameters: {'activation': 'tanh', 'optimizer': 'sgd', 'num_hidden_layer': 2, 'num_hidden_unit': 81, 'kernel_size': 2, 'filter': 40, 'learning_rate': 0.000693576191437307}.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 27s - loss: 18.0008 - mse: 18.0008 - mae: 1.6323 - 27s/epoch - 1ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 27s - loss: 16.9701 - mse: 16.9701 - mae: 1.5841 - 27s/epoch - 1ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 28s - loss: 16.5931 - mse: 16.5931 - mae: 1.5699 - 28s/epoch - 1ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 27s - loss: 16.2968 - mse: 16.2968 - mae: 1.5553 - 27s/epoch - 1ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 26s - loss: 16.0707 - mse: 16.0707 - mae: 1.5410 - 26s/epoch - 1ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 27s - loss: 15.8990 - mse: 15.8990 - mae: 1.5293 - 27s/epoch - 1ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 26s - loss: 15.7580 - mse: 15.7580 - mae: 1.5211 - 26s/epoch - 1ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 31s - loss: 15.6356 - mse: 15.6356 - mae: 1.5131 - 31s/epoch - 2ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 32s - loss: 15.5570 - mse: 15.5570 - mae: 1.5076 - 32s/epoch - 2ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 31s - loss: 15.4548 - mse: 15.4548 - mae: 1.5021 - 31s/epoch - 2ms/step\n",
            "Score for fold 1: loss of 23.360483169555664\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 28s - loss: 15.3917 - mse: 15.3917 - mae: 1.4979 - 28s/epoch - 1ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 30s - loss: 15.3263 - mse: 15.3263 - mae: 1.4949 - 30s/epoch - 1ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 28s - loss: 15.2813 - mse: 15.2813 - mae: 1.4932 - 28s/epoch - 1ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 27s - loss: 15.2240 - mse: 15.2240 - mae: 1.4900 - 27s/epoch - 1ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 32s - loss: 15.1549 - mse: 15.1549 - mae: 1.4879 - 32s/epoch - 2ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 49s - loss: 15.1147 - mse: 15.1147 - mae: 1.4863 - 49s/epoch - 2ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 48s - loss: 15.0718 - mse: 15.0718 - mae: 1.4847 - 48s/epoch - 2ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 48s - loss: 15.0456 - mse: 15.0456 - mae: 1.4836 - 48s/epoch - 2ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 47s - loss: 14.9912 - mse: 14.9912 - mae: 1.4820 - 47s/epoch - 2ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 44s - loss: 14.9618 - mse: 14.9618 - mae: 1.4817 - 44s/epoch - 2ms/step\n",
            "Score for fold 2: loss of 11.380929946899414\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 34s - loss: 14.9284 - mse: 14.9284 - mae: 1.4798 - 34s/epoch - 2ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 38s - loss: 14.9026 - mse: 14.9026 - mae: 1.4799 - 38s/epoch - 2ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 38s - loss: 14.8597 - mse: 14.8597 - mae: 1.4778 - 38s/epoch - 2ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 37s - loss: 14.8450 - mse: 14.8450 - mae: 1.4768 - 37s/epoch - 2ms/step\n",
            "Epoch 5/10\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#{'activation': 'tanh', 'optimizer': 'sgd', 'num_hidden_layer': 2, 'num_hidden_unit': 81, 'kernel_size': 2, 'filter': 40, 'learning_rate': 0.000693576191437307}\n",
        "optimizer = SGD(learning_rate=0.000693576191437307)\n",
        "\n",
        "num_folds = 3\n",
        "kfold=KFold(n_splits=3,shuffle=True)\n",
        "fold_no=1\n",
        "loss_per_fold = []\n",
        "model = create_model_cnn(activation='tanh', num_hidden_layer=2, num_hidden_unit=81, kernel_size=2, filter=40)\n",
        "model.summary()\n",
        "from keras.utils.vis_utils import plot_model\n",
        "plot_model(model, to_file='model_plot.png', show_shapes=True, show_layer_names=True)\n",
        "model.compile(loss='mse',optimizer=optimizer,metrics=['mse','mae'])\n",
        "for train,test in kfold.split(training,labelsForTrain):\n",
        "  scores=model.evaluate(testing,labelsForTest,verbose=0)\n",
        "\n",
        "  print('------------------------------------------------------------------------')\n",
        "  print(f'Training for fold {fold_no} ...')\n",
        "  \n",
        "  # Fit data to model\n",
        "  history = model.fit(training, labelsForTrain,\n",
        "                batch_size=20,\n",
        "                epochs=20,\n",
        "                verbose=2)\n",
        "    \n",
        "  print(f'Score for fold {fold_no}: {model.metrics_names[0]} of {scores[0]}')\n",
        "  loss_per_fold.append(scores[0])\n",
        "  # Increase fold number\n",
        "  fold_no = fold_no + 1    "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xEJ-3GH-Zqdq",
        "outputId": "f4fef15c-bdd4-48c8-bc35-9f66431bbe78"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv1d_1 (Conv1D)           (None, 17, 40)            120       \n",
            "                                                                 \n",
            " max_pooling1d_1 (MaxPooling  (None, 8, 40)            0         \n",
            " 1D)                                                             \n",
            "                                                                 \n",
            " flatten_1 (Flatten)         (None, 320)               0         \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 81)                26001     \n",
            "                                                                 \n",
            " dense_3 (Dense)             (None, 1)                 82        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 26,203\n",
            "Trainable params: 26,203\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/20\n",
            "20000/20000 - 26s - loss: 17.2971 - mse: 17.2971 - mae: 1.6127 - 26s/epoch - 1ms/step\n",
            "Epoch 2/20\n",
            "20000/20000 - 26s - loss: 16.2696 - mse: 16.2696 - mae: 1.5584 - 26s/epoch - 1ms/step\n",
            "Epoch 3/20\n",
            "20000/20000 - 25s - loss: 15.8710 - mse: 15.8710 - mae: 1.5314 - 25s/epoch - 1ms/step\n",
            "Epoch 4/20\n",
            "20000/20000 - 25s - loss: 15.6570 - mse: 15.6570 - mae: 1.5199 - 25s/epoch - 1ms/step\n",
            "Epoch 5/20\n",
            "20000/20000 - 25s - loss: 15.4993 - mse: 15.4993 - mae: 1.5131 - 25s/epoch - 1ms/step\n",
            "Epoch 6/20\n",
            "20000/20000 - 25s - loss: 15.4390 - mse: 15.4390 - mae: 1.5088 - 25s/epoch - 1ms/step\n",
            "Epoch 7/20\n",
            "20000/20000 - 25s - loss: 15.3349 - mse: 15.3349 - mae: 1.5055 - 25s/epoch - 1ms/step\n",
            "Epoch 8/20\n",
            "20000/20000 - 25s - loss: 15.2470 - mse: 15.2470 - mae: 1.5010 - 25s/epoch - 1ms/step\n",
            "Epoch 9/20\n",
            "20000/20000 - 25s - loss: 15.1692 - mse: 15.1692 - mae: 1.4996 - 25s/epoch - 1ms/step\n",
            "Epoch 10/20\n",
            "20000/20000 - 25s - loss: 15.1160 - mse: 15.1160 - mae: 1.4968 - 25s/epoch - 1ms/step\n",
            "Epoch 11/20\n",
            "20000/20000 - 25s - loss: 15.0399 - mse: 15.0399 - mae: 1.4938 - 25s/epoch - 1ms/step\n",
            "Epoch 12/20\n",
            "20000/20000 - 25s - loss: 15.0006 - mse: 15.0006 - mae: 1.4924 - 25s/epoch - 1ms/step\n",
            "Epoch 13/20\n",
            "20000/20000 - 25s - loss: 14.9715 - mse: 14.9715 - mae: 1.4909 - 25s/epoch - 1ms/step\n",
            "Epoch 14/20\n",
            "20000/20000 - 24s - loss: 14.9218 - mse: 14.9218 - mae: 1.4915 - 24s/epoch - 1ms/step\n",
            "Epoch 15/20\n",
            "20000/20000 - 24s - loss: 14.9357 - mse: 14.9357 - mae: 1.4924 - 24s/epoch - 1ms/step\n",
            "Epoch 16/20\n",
            "20000/20000 - 25s - loss: 14.8799 - mse: 14.8799 - mae: 1.4896 - 25s/epoch - 1ms/step\n",
            "Epoch 17/20\n",
            "20000/20000 - 24s - loss: 14.8531 - mse: 14.8531 - mae: 1.4992 - 24s/epoch - 1ms/step\n",
            "Epoch 18/20\n",
            "20000/20000 - 24s - loss: 14.8006 - mse: 14.8006 - mae: 1.4881 - 24s/epoch - 1ms/step\n",
            "Epoch 19/20\n",
            "20000/20000 - 24s - loss: 14.8052 - mse: 14.8052 - mae: 1.4849 - 24s/epoch - 1ms/step\n",
            "Epoch 20/20\n",
            "20000/20000 - 24s - loss: 14.7864 - mse: 14.7864 - mae: 1.4849 - 24s/epoch - 1ms/step\n",
            "Score for fold 1: loss of 24.219444274902344\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/20\n",
            "20000/20000 - 25s - loss: 14.7672 - mse: 14.7672 - mae: 1.4847 - 25s/epoch - 1ms/step\n",
            "Epoch 2/20\n",
            "20000/20000 - 25s - loss: 14.7043 - mse: 14.7043 - mae: 1.4816 - 25s/epoch - 1ms/step\n",
            "Epoch 3/20\n",
            "20000/20000 - 24s - loss: 14.6537 - mse: 14.6537 - mae: 1.4812 - 24s/epoch - 1ms/step\n",
            "Epoch 4/20\n",
            "20000/20000 - 25s - loss: 14.6511 - mse: 14.6511 - mae: 1.4807 - 25s/epoch - 1ms/step\n",
            "Epoch 5/20\n",
            "20000/20000 - 24s - loss: 14.5717 - mse: 14.5717 - mae: 1.4784 - 24s/epoch - 1ms/step\n",
            "Epoch 6/20\n",
            "20000/20000 - 25s - loss: 14.5461 - mse: 14.5461 - mae: 1.4765 - 25s/epoch - 1ms/step\n",
            "Epoch 7/20\n",
            "20000/20000 - 25s - loss: 14.4608 - mse: 14.4608 - mae: 1.4745 - 25s/epoch - 1ms/step\n",
            "Epoch 8/20\n",
            "20000/20000 - 25s - loss: 14.4869 - mse: 14.4869 - mae: 1.4738 - 25s/epoch - 1ms/step\n",
            "Epoch 9/20\n",
            "20000/20000 - 25s - loss: 14.4208 - mse: 14.4208 - mae: 1.4736 - 25s/epoch - 1ms/step\n",
            "Epoch 10/20\n",
            "20000/20000 - 25s - loss: 14.5488 - mse: 14.5488 - mae: 1.4896 - 25s/epoch - 1ms/step\n",
            "Epoch 11/20\n",
            "20000/20000 - 25s - loss: 14.4390 - mse: 14.4390 - mae: 1.4769 - 25s/epoch - 1ms/step\n",
            "Epoch 12/20\n",
            "20000/20000 - 25s - loss: 14.3832 - mse: 14.3832 - mae: 1.4724 - 25s/epoch - 1ms/step\n",
            "Epoch 13/20\n",
            "20000/20000 - 25s - loss: 14.4130 - mse: 14.4130 - mae: 1.4831 - 25s/epoch - 1ms/step\n",
            "Epoch 14/20\n",
            "20000/20000 - 25s - loss: 14.3474 - mse: 14.3474 - mae: 1.4785 - 25s/epoch - 1ms/step\n",
            "Epoch 15/20\n",
            "20000/20000 - 25s - loss: 14.4150 - mse: 14.4150 - mae: 1.4812 - 25s/epoch - 1ms/step\n",
            "Epoch 16/20\n",
            "20000/20000 - 25s - loss: 14.3391 - mse: 14.3391 - mae: 1.4755 - 25s/epoch - 1ms/step\n",
            "Epoch 17/20\n",
            "20000/20000 - 24s - loss: 14.3736 - mse: 14.3736 - mae: 1.4791 - 24s/epoch - 1ms/step\n",
            "Epoch 18/20\n",
            "20000/20000 - 25s - loss: 14.3436 - mse: 14.3436 - mae: 1.4822 - 25s/epoch - 1ms/step\n",
            "Epoch 19/20\n",
            "20000/20000 - 25s - loss: 14.2991 - mse: 14.2991 - mae: 1.4797 - 25s/epoch - 1ms/step\n",
            "Epoch 20/20\n",
            "20000/20000 - 25s - loss: 14.2781 - mse: 14.2781 - mae: 1.4752 - 25s/epoch - 1ms/step\n",
            "Score for fold 2: loss of 11.015496253967285\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/20\n",
            "20000/20000 - 25s - loss: 14.1821 - mse: 14.1821 - mae: 1.4685 - 25s/epoch - 1ms/step\n",
            "Epoch 2/20\n",
            "20000/20000 - 25s - loss: 14.1719 - mse: 14.1719 - mae: 1.4699 - 25s/epoch - 1ms/step\n",
            "Epoch 3/20\n",
            "20000/20000 - 25s - loss: 14.1838 - mse: 14.1838 - mae: 1.4685 - 25s/epoch - 1ms/step\n",
            "Epoch 4/20\n",
            "20000/20000 - 25s - loss: 14.1523 - mse: 14.1523 - mae: 1.4657 - 25s/epoch - 1ms/step\n",
            "Epoch 5/20\n",
            "20000/20000 - 25s - loss: 14.1143 - mse: 14.1143 - mae: 1.4666 - 25s/epoch - 1ms/step\n",
            "Epoch 6/20\n",
            "20000/20000 - 26s - loss: 14.0762 - mse: 14.0762 - mae: 1.4646 - 26s/epoch - 1ms/step\n",
            "Epoch 7/20\n",
            "20000/20000 - 26s - loss: 14.0453 - mse: 14.0453 - mae: 1.4639 - 26s/epoch - 1ms/step\n",
            "Epoch 8/20\n",
            "20000/20000 - 25s - loss: 14.0397 - mse: 14.0397 - mae: 1.4600 - 25s/epoch - 1ms/step\n",
            "Epoch 9/20\n",
            "20000/20000 - 25s - loss: 13.9944 - mse: 13.9944 - mae: 1.4609 - 25s/epoch - 1ms/step\n",
            "Epoch 10/20\n",
            "20000/20000 - 24s - loss: 13.9512 - mse: 13.9512 - mae: 1.4584 - 24s/epoch - 1ms/step\n",
            "Epoch 11/20\n",
            "20000/20000 - 24s - loss: 13.9744 - mse: 13.9744 - mae: 1.4598 - 24s/epoch - 1ms/step\n",
            "Epoch 12/20\n",
            "20000/20000 - 25s - loss: 13.9064 - mse: 13.9064 - mae: 1.4547 - 25s/epoch - 1ms/step\n",
            "Epoch 13/20\n",
            "20000/20000 - 24s - loss: 13.9160 - mse: 13.9160 - mae: 1.4572 - 24s/epoch - 1ms/step\n",
            "Epoch 14/20\n",
            "20000/20000 - 25s - loss: 13.8819 - mse: 13.8819 - mae: 1.4551 - 25s/epoch - 1ms/step\n",
            "Epoch 15/20\n",
            "20000/20000 - 25s - loss: 13.8736 - mse: 13.8736 - mae: 1.4549 - 25s/epoch - 1ms/step\n",
            "Epoch 16/20\n",
            "20000/20000 - 24s - loss: 13.8448 - mse: 13.8448 - mae: 1.4555 - 24s/epoch - 1ms/step\n",
            "Epoch 17/20\n",
            "20000/20000 - 24s - loss: 13.9372 - mse: 13.9372 - mae: 1.4679 - 24s/epoch - 1ms/step\n",
            "Epoch 18/20\n",
            "20000/20000 - 25s - loss: 13.8507 - mse: 13.8507 - mae: 1.4561 - 25s/epoch - 1ms/step\n",
            "Epoch 19/20\n",
            "20000/20000 - 24s - loss: 13.8282 - mse: 13.8282 - mae: 1.4562 - 24s/epoch - 1ms/step\n",
            "Epoch 20/20\n",
            "20000/20000 - 25s - loss: 13.7823 - mse: 13.7823 - mae: 1.4517 - 25s/epoch - 1ms/step\n",
            "Score for fold 3: loss of 10.295652389526367\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Mean mse for train is 14.289 (3.780) best is 13.844(3.72), mean mae for train is 1.469\n",
        "results = model_cnn_best.evaluate(testing, labelsForTest, batch_size=20)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KkfAxdyxqoei",
        "outputId": "b8a17b82-1744-4c70-f537-55a078d07c4c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5000/5000 [==============================] - 8s 2ms/step - loss: 13.6385 - mse: 13.6385 - mae: 2.0673\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_list=[]\n",
        "history_list=[]\n",
        "study_name = 'NN_study'\n",
        "study = optuna.create_study(study_name=study_name, load_if_exists=True)\n",
        "#15.1\n",
        "study.optimize(objective, n_trials=50, )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1qnvHMyp9TQ8",
        "outputId": "acded2ce-cc1c-4a97-a804-c1dd8d19fd00"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 19s - loss: 17.8997 - mse: 17.8997 - mae: 1.6168 - 19s/epoch - 973us/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 19s - loss: 16.6445 - mse: 16.6445 - mae: 1.5685 - 19s/epoch - 965us/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 20s - loss: 16.5071 - mse: 16.5071 - mae: 1.5603 - 20s/epoch - 1ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 20s - loss: 16.4072 - mse: 16.4072 - mae: 1.5568 - 20s/epoch - 998us/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 20s - loss: 16.3235 - mse: 16.3235 - mae: 1.5529 - 20s/epoch - 1ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 20s - loss: 16.2645 - mse: 16.2645 - mae: 1.5478 - 20s/epoch - 981us/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 19s - loss: 16.2027 - mse: 16.2027 - mae: 1.5455 - 19s/epoch - 935us/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 19s - loss: 16.1479 - mse: 16.1479 - mae: 1.5423 - 19s/epoch - 938us/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 19s - loss: 16.1032 - mse: 16.1032 - mae: 1.5392 - 19s/epoch - 950us/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 18s - loss: 16.0596 - mse: 16.0596 - mae: 1.5371 - 18s/epoch - 912us/step\n",
            "Score for fold 1: loss of 24.185527801513672\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 19s - loss: 16.0173 - mse: 16.0173 - mae: 1.5346 - 19s/epoch - 964us/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 19s - loss: 15.9837 - mse: 15.9837 - mae: 1.5323 - 19s/epoch - 970us/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 18s - loss: 15.9449 - mse: 15.9449 - mae: 1.5312 - 18s/epoch - 923us/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 19s - loss: 15.9031 - mse: 15.9031 - mae: 1.5276 - 19s/epoch - 966us/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 20s - loss: 15.8774 - mse: 15.8774 - mae: 1.5266 - 20s/epoch - 985us/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 19s - loss: 15.8549 - mse: 15.8549 - mae: 1.5247 - 19s/epoch - 947us/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 19s - loss: 15.8211 - mse: 15.8211 - mae: 1.5230 - 19s/epoch - 931us/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 18s - loss: 15.7971 - mse: 15.7971 - mae: 1.5219 - 18s/epoch - 922us/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 19s - loss: 15.7710 - mse: 15.7710 - mae: 1.5186 - 19s/epoch - 962us/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 19s - loss: 15.7465 - mse: 15.7465 - mae: 1.5182 - 19s/epoch - 942us/step\n",
            "Score for fold 2: loss of 11.761616706848145\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 19s - loss: 15.7190 - mse: 15.7190 - mae: 1.5162 - 19s/epoch - 957us/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 19s - loss: 15.7046 - mse: 15.7046 - mae: 1.5146 - 19s/epoch - 955us/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 19s - loss: 15.6809 - mse: 15.6809 - mae: 1.5140 - 19s/epoch - 948us/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 20s - loss: 15.6632 - mse: 15.6632 - mae: 1.5118 - 20s/epoch - 982us/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 19s - loss: 15.6451 - mse: 15.6451 - mae: 1.5097 - 19s/epoch - 934us/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 19s - loss: 15.6082 - mse: 15.6082 - mae: 1.5086 - 19s/epoch - 934us/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 20s - loss: 15.5970 - mse: 15.5970 - mae: 1.5076 - 20s/epoch - 1ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 19s - loss: 15.5684 - mse: 15.5684 - mae: 1.5056 - 19s/epoch - 971us/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 19s - loss: 15.5439 - mse: 15.5439 - mae: 1.5055 - 19s/epoch - 965us/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 19s - loss: 15.5424 - mse: 15.5424 - mae: 1.5030 - 19s/epoch - 971us/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-10-04 22:34:35,071]\u001b[0m Finished trial#0 resulted in value: 15.542407035827637. Current best value is 15.542407035827637 with parameters: {'activation': 'tanh', 'optimizer': 'sgd', 'num_hidden_layer': 2, 'num_hidden_unit': 924, 'learning_rate': 4.981928169085122e-05}.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Score for fold 3: loss of 11.505845069885254\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 51s - loss: 17.1896 - mse: 17.1896 - mae: 1.6037 - 51s/epoch - 3ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 52s - loss: 17.0186 - mse: 17.0186 - mae: 1.6033 - 52s/epoch - 3ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 52s - loss: 17.0303 - mse: 17.0303 - mae: 1.6005 - 52s/epoch - 3ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 52s - loss: 17.0077 - mse: 17.0077 - mae: 1.6003 - 52s/epoch - 3ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 51s - loss: 16.9907 - mse: 16.9907 - mae: 1.6025 - 51s/epoch - 3ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 52s - loss: 16.9520 - mse: 16.9520 - mae: 1.5993 - 52s/epoch - 3ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 53s - loss: 16.9810 - mse: 16.9810 - mae: 1.6004 - 53s/epoch - 3ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 53s - loss: 16.9848 - mse: 16.9848 - mae: 1.6009 - 53s/epoch - 3ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 52s - loss: 16.9615 - mse: 16.9615 - mae: 1.6022 - 52s/epoch - 3ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 55s - loss: 16.9497 - mse: 16.9497 - mae: 1.5951 - 55s/epoch - 3ms/step\n",
            "Score for fold 1: loss of 24.228544235229492\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 57s - loss: 16.9732 - mse: 16.9732 - mae: 1.5998 - 57s/epoch - 3ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 54s - loss: 16.9509 - mse: 16.9509 - mae: 1.6033 - 54s/epoch - 3ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 80s - loss: 16.9748 - mse: 16.9748 - mae: 1.5995 - 80s/epoch - 4ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 63s - loss: 16.9706 - mse: 16.9706 - mae: 1.5994 - 63s/epoch - 3ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 54s - loss: 16.9674 - mse: 16.9674 - mae: 1.6004 - 54s/epoch - 3ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 55s - loss: 16.9686 - mse: 16.9686 - mae: 1.5986 - 55s/epoch - 3ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 54s - loss: 16.9636 - mse: 16.9636 - mae: 1.6013 - 54s/epoch - 3ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 55s - loss: 16.9681 - mse: 16.9681 - mae: 1.5995 - 55s/epoch - 3ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 56s - loss: 16.9606 - mse: 16.9606 - mae: 1.5994 - 56s/epoch - 3ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 55s - loss: 16.9560 - mse: 16.9560 - mae: 1.6006 - 55s/epoch - 3ms/step\n",
            "Score for fold 2: loss of 12.307723999023438\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 58s - loss: 16.9634 - mse: 16.9634 - mae: 1.5988 - 58s/epoch - 3ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 59s - loss: 16.9637 - mse: 16.9637 - mae: 1.6002 - 59s/epoch - 3ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 57s - loss: 16.9602 - mse: 16.9602 - mae: 1.5996 - 57s/epoch - 3ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 55s - loss: 16.9585 - mse: 16.9585 - mae: 1.5993 - 55s/epoch - 3ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 60s - loss: 16.9586 - mse: 16.9586 - mae: 1.5996 - 60s/epoch - 3ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 59s - loss: 16.9597 - mse: 16.9597 - mae: 1.6008 - 59s/epoch - 3ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 58s - loss: 16.9601 - mse: 16.9601 - mae: 1.6002 - 58s/epoch - 3ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 57s - loss: 16.9591 - mse: 16.9591 - mae: 1.5995 - 57s/epoch - 3ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 55s - loss: 16.9569 - mse: 16.9569 - mae: 1.5986 - 55s/epoch - 3ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 55s - loss: 16.9442 - mse: 16.9442 - mae: 1.6021 - 55s/epoch - 3ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-10-04 23:02:46,398]\u001b[0m Finished trial#1 resulted in value: 16.94422721862793. Current best value is 15.542407035827637 with parameters: {'activation': 'tanh', 'optimizer': 'sgd', 'num_hidden_layer': 2, 'num_hidden_unit': 924, 'learning_rate': 4.981928169085122e-05}.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Score for fold 3: loss of 12.302616119384766\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 62s - loss: nan - mse: nan - mae: nan - 62s/epoch - 3ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 62s - loss: nan - mse: nan - mae: nan - 62s/epoch - 3ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 62s - loss: nan - mse: nan - mae: nan - 62s/epoch - 3ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 58s - loss: nan - mse: nan - mae: nan - 58s/epoch - 3ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 58s - loss: nan - mse: nan - mae: nan - 58s/epoch - 3ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 57s - loss: nan - mse: nan - mae: nan - 57s/epoch - 3ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 57s - loss: nan - mse: nan - mae: nan - 57s/epoch - 3ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 57s - loss: nan - mse: nan - mae: nan - 57s/epoch - 3ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 57s - loss: nan - mse: nan - mae: nan - 57s/epoch - 3ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 57s - loss: nan - mse: nan - mae: nan - 57s/epoch - 3ms/step\n",
            "Score for fold 1: loss of 23.872737884521484\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 56s - loss: nan - mse: nan - mae: nan - 56s/epoch - 3ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 60s - loss: nan - mse: nan - mae: nan - 60s/epoch - 3ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 58s - loss: nan - mse: nan - mae: nan - 58s/epoch - 3ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 57s - loss: nan - mse: nan - mae: nan - 57s/epoch - 3ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 57s - loss: nan - mse: nan - mae: nan - 57s/epoch - 3ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 58s - loss: nan - mse: nan - mae: nan - 58s/epoch - 3ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 57s - loss: nan - mse: nan - mae: nan - 57s/epoch - 3ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 57s - loss: nan - mse: nan - mae: nan - 57s/epoch - 3ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 57s - loss: nan - mse: nan - mae: nan - 57s/epoch - 3ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 57s - loss: nan - mse: nan - mae: nan - 57s/epoch - 3ms/step\n",
            "Score for fold 2: loss of nan\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 58s - loss: nan - mse: nan - mae: nan - 58s/epoch - 3ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 58s - loss: nan - mse: nan - mae: nan - 58s/epoch - 3ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 58s - loss: nan - mse: nan - mae: nan - 58s/epoch - 3ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 59s - loss: nan - mse: nan - mae: nan - 59s/epoch - 3ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 61s - loss: nan - mse: nan - mae: nan - 61s/epoch - 3ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 60s - loss: nan - mse: nan - mae: nan - 60s/epoch - 3ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 61s - loss: nan - mse: nan - mae: nan - 61s/epoch - 3ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 59s - loss: nan - mse: nan - mae: nan - 59s/epoch - 3ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 59s - loss: nan - mse: nan - mae: nan - 59s/epoch - 3ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 59s - loss: nan - mse: nan - mae: nan - 59s/epoch - 3ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[33m[W 2022-10-04 23:32:12,832]\u001b[0m Setting status of trial#2 as TrialState.FAIL because the objective function returned nan.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Score for fold 3: loss of nan\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 30s - loss: 21.0988 - mse: 21.0988 - mae: 1.8777 - 30s/epoch - 1ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 30s - loss: 21.4143 - mse: 21.4143 - mae: 1.9077 - 30s/epoch - 1ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 29s - loss: 21.3729 - mse: 21.3729 - mae: 1.9083 - 29s/epoch - 1ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 30s - loss: 21.3724 - mse: 21.3724 - mae: 1.9076 - 30s/epoch - 1ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 30s - loss: 21.3723 - mse: 21.3723 - mae: 1.9080 - 30s/epoch - 2ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 29s - loss: 21.3727 - mse: 21.3727 - mae: 1.9079 - 29s/epoch - 1ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 29s - loss: 21.3730 - mse: 21.3730 - mae: 1.9081 - 29s/epoch - 1ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 29s - loss: 21.3719 - mse: 21.3719 - mae: 1.9082 - 29s/epoch - 1ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 28s - loss: 21.3729 - mse: 21.3729 - mae: 1.9080 - 28s/epoch - 1ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 28s - loss: 21.3747 - mse: 21.3747 - mae: 1.9077 - 28s/epoch - 1ms/step\n",
            "Score for fold 1: loss of 23.11947250366211\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 28s - loss: 21.3726 - mse: 21.3726 - mae: 1.9080 - 28s/epoch - 1ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 29s - loss: 21.3734 - mse: 21.3734 - mae: 1.9084 - 29s/epoch - 1ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 29s - loss: 21.3729 - mse: 21.3729 - mae: 1.9079 - 29s/epoch - 1ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 28s - loss: 21.3730 - mse: 21.3730 - mae: 1.9080 - 28s/epoch - 1ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 28s - loss: 21.3739 - mse: 21.3739 - mae: 1.9078 - 28s/epoch - 1ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 28s - loss: 21.3729 - mse: 21.3729 - mae: 1.9081 - 28s/epoch - 1ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 28s - loss: 21.3734 - mse: 21.3734 - mae: 1.9082 - 28s/epoch - 1ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 28s - loss: 21.3736 - mse: 21.3736 - mae: 1.9080 - 28s/epoch - 1ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 28s - loss: 21.3726 - mse: 21.3726 - mae: 1.9079 - 28s/epoch - 1ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 28s - loss: 21.3720 - mse: 21.3719 - mae: 1.9076 - 28s/epoch - 1ms/step\n",
            "Score for fold 2: loss of 16.962703704833984\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 28s - loss: 21.3723 - mse: 21.3723 - mae: 1.9081 - 28s/epoch - 1ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 28s - loss: 21.3734 - mse: 21.3734 - mae: 1.9082 - 28s/epoch - 1ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 28s - loss: 21.3718 - mse: 21.3718 - mae: 1.9083 - 28s/epoch - 1ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 28s - loss: 21.3727 - mse: 21.3727 - mae: 1.9078 - 28s/epoch - 1ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 29s - loss: 21.3737 - mse: 21.3737 - mae: 1.9080 - 29s/epoch - 1ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 28s - loss: 21.3725 - mse: 21.3725 - mae: 1.9083 - 28s/epoch - 1ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 28s - loss: 21.3735 - mse: 21.3735 - mae: 1.9076 - 28s/epoch - 1ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 28s - loss: 21.3734 - mse: 21.3734 - mae: 1.9083 - 28s/epoch - 1ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 28s - loss: 21.3732 - mse: 21.3732 - mae: 1.9083 - 28s/epoch - 1ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 28s - loss: 21.3739 - mse: 21.3739 - mae: 1.9081 - 28s/epoch - 1ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-10-04 23:46:41,329]\u001b[0m Finished trial#3 resulted in value: 21.37392234802246. Current best value is 15.542407035827637 with parameters: {'activation': 'tanh', 'optimizer': 'sgd', 'num_hidden_layer': 2, 'num_hidden_unit': 924, 'learning_rate': 4.981928169085122e-05}.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Score for fold 3: loss of 16.980010986328125\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 26s - loss: 16.4508 - mse: 16.4508 - mae: 1.5372 - 26s/epoch - 1ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 26s - loss: 15.6298 - mse: 15.6298 - mae: 1.4981 - 26s/epoch - 1ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 26s - loss: 15.3396 - mse: 15.3396 - mae: 1.4864 - 26s/epoch - 1ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 27s - loss: 15.1696 - mse: 15.1696 - mae: 1.4773 - 27s/epoch - 1ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 26s - loss: 15.0361 - mse: 15.0361 - mae: 1.4706 - 26s/epoch - 1ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 26s - loss: 14.8260 - mse: 14.8260 - mae: 1.4648 - 26s/epoch - 1ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 26s - loss: 14.7587 - mse: 14.7587 - mae: 1.4624 - 26s/epoch - 1ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 26s - loss: 14.6984 - mse: 14.6984 - mae: 1.4585 - 26s/epoch - 1ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 26s - loss: 14.5658 - mse: 14.5658 - mae: 1.4550 - 26s/epoch - 1ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 26s - loss: 14.5143 - mse: 14.5143 - mae: 1.4536 - 26s/epoch - 1ms/step\n",
            "Score for fold 1: loss of 26.95563316345215\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 26s - loss: 14.4909 - mse: 14.4909 - mae: 1.4504 - 26s/epoch - 1ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 26s - loss: 14.3818 - mse: 14.3818 - mae: 1.4492 - 26s/epoch - 1ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 26s - loss: 14.3207 - mse: 14.3207 - mae: 1.4459 - 26s/epoch - 1ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 26s - loss: 14.2028 - mse: 14.2028 - mae: 1.4450 - 26s/epoch - 1ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 26s - loss: 14.1817 - mse: 14.1817 - mae: 1.4427 - 26s/epoch - 1ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 26s - loss: 14.0772 - mse: 14.0772 - mae: 1.4418 - 26s/epoch - 1ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 26s - loss: 14.0271 - mse: 14.0271 - mae: 1.4390 - 26s/epoch - 1ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 27s - loss: 13.9950 - mse: 13.9950 - mae: 1.4394 - 27s/epoch - 1ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 27s - loss: 13.8706 - mse: 13.8706 - mae: 1.4361 - 27s/epoch - 1ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 27s - loss: 13.7782 - mse: 13.7782 - mae: 1.4351 - 27s/epoch - 1ms/step\n",
            "Score for fold 2: loss of 10.550527572631836\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 27s - loss: 13.6955 - mse: 13.6955 - mae: 1.4369 - 27s/epoch - 1ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 27s - loss: 13.7343 - mse: 13.7343 - mae: 1.4331 - 27s/epoch - 1ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 27s - loss: 13.5561 - mse: 13.5561 - mae: 1.4296 - 27s/epoch - 1ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 27s - loss: 13.5119 - mse: 13.5119 - mae: 1.4318 - 27s/epoch - 1ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 27s - loss: 13.4354 - mse: 13.4354 - mae: 1.4306 - 27s/epoch - 1ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 27s - loss: 13.3683 - mse: 13.3683 - mae: 1.4301 - 27s/epoch - 1ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 27s - loss: 13.3356 - mse: 13.3356 - mae: 1.4275 - 27s/epoch - 1ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 27s - loss: 13.2237 - mse: 13.2237 - mae: 1.4268 - 27s/epoch - 1ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 27s - loss: 12.9427 - mse: 12.9427 - mae: 1.4284 - 27s/epoch - 1ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 27s - loss: 13.0203 - mse: 13.0203 - mae: 1.4268 - 27s/epoch - 1ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-10-05 00:00:04,946]\u001b[0m Finished trial#4 resulted in value: 13.020280838012695. Current best value is 13.020280838012695 with parameters: {'activation': 'relu', 'optimizer': 'adam', 'num_hidden_layer': 3, 'num_hidden_unit': 159, 'learning_rate': 0.00025128296799451465}.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Score for fold 3: loss of 10.140913963317871\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 21s - loss: 18.3057 - mse: 18.3057 - mae: 1.6625 - 21s/epoch - 1ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 21s - loss: 17.0424 - mse: 17.0424 - mae: 1.5952 - 21s/epoch - 1ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 21s - loss: 17.0139 - mse: 17.0139 - mae: 1.5933 - 21s/epoch - 1ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 21s - loss: 16.9809 - mse: 16.9809 - mae: 1.5937 - 21s/epoch - 1ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 22s - loss: 16.9948 - mse: 16.9948 - mae: 1.5940 - 22s/epoch - 1ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 22s - loss: 16.9699 - mse: 16.9699 - mae: 1.5937 - 22s/epoch - 1ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 22s - loss: 16.9766 - mse: 16.9766 - mae: 1.5958 - 22s/epoch - 1ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 23s - loss: 16.9711 - mse: 16.9711 - mae: 1.5973 - 23s/epoch - 1ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 23s - loss: 16.9716 - mse: 16.9716 - mae: 1.5963 - 23s/epoch - 1ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 23s - loss: 16.9613 - mse: 16.9613 - mae: 1.5969 - 23s/epoch - 1ms/step\n",
            "Score for fold 1: loss of 23.38985252380371\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 23s - loss: 16.9776 - mse: 16.9776 - mae: 1.5949 - 23s/epoch - 1ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 22s - loss: 16.9602 - mse: 16.9602 - mae: 1.5951 - 22s/epoch - 1ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 22s - loss: 16.9890 - mse: 16.9890 - mae: 1.5949 - 22s/epoch - 1ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 22s - loss: 16.9961 - mse: 16.9961 - mae: 1.5941 - 22s/epoch - 1ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 22s - loss: 16.9867 - mse: 16.9867 - mae: 1.5945 - 22s/epoch - 1ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 22s - loss: 16.9899 - mse: 16.9899 - mae: 1.5945 - 22s/epoch - 1ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 22s - loss: 16.9755 - mse: 16.9755 - mae: 1.5962 - 22s/epoch - 1ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 22s - loss: 16.9797 - mse: 16.9797 - mae: 1.5949 - 22s/epoch - 1ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 21s - loss: 16.9792 - mse: 16.9792 - mae: 1.5956 - 21s/epoch - 1ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 21s - loss: 16.9639 - mse: 16.9639 - mae: 1.5960 - 21s/epoch - 1ms/step\n",
            "Score for fold 2: loss of 12.358043670654297\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 22s - loss: 16.9747 - mse: 16.9747 - mae: 1.5973 - 22s/epoch - 1ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 23s - loss: 16.9822 - mse: 16.9822 - mae: 1.5958 - 23s/epoch - 1ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 22s - loss: 16.9770 - mse: 16.9770 - mae: 1.5944 - 22s/epoch - 1ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 23s - loss: 16.9837 - mse: 16.9837 - mae: 1.5964 - 23s/epoch - 1ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 24s - loss: 16.9816 - mse: 16.9816 - mae: 1.5952 - 24s/epoch - 1ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 23s - loss: 16.9673 - mse: 16.9673 - mae: 1.5964 - 23s/epoch - 1ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 23s - loss: 16.9452 - mse: 16.9452 - mae: 1.5969 - 23s/epoch - 1ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 24s - loss: 16.9838 - mse: 16.9838 - mae: 1.5943 - 24s/epoch - 1ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 23s - loss: 16.9722 - mse: 16.9722 - mae: 1.5964 - 23s/epoch - 1ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 22s - loss: 16.9853 - mse: 16.9853 - mae: 1.5950 - 22s/epoch - 1ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-10-05 00:11:19,068]\u001b[0m Finished trial#5 resulted in value: 16.98528289794922. Current best value is 13.020280838012695 with parameters: {'activation': 'relu', 'optimizer': 'adam', 'num_hidden_layer': 3, 'num_hidden_unit': 159, 'learning_rate': 0.00025128296799451465}.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Score for fold 3: loss of 12.365083694458008\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 25s - loss: 17.0174 - mse: 17.0174 - mae: 1.6836 - 25s/epoch - 1ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 24s - loss: 16.6137 - mse: 16.6137 - mae: 1.6798 - 24s/epoch - 1ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 23s - loss: 16.4555 - mse: 16.4555 - mae: 1.6729 - 23s/epoch - 1ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 23s - loss: 16.3631 - mse: 16.3631 - mae: 1.6821 - 23s/epoch - 1ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 23s - loss: 16.2587 - mse: 16.2587 - mae: 1.6772 - 23s/epoch - 1ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 23s - loss: 16.1901 - mse: 16.1901 - mae: 1.6760 - 23s/epoch - 1ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 23s - loss: 16.0814 - mse: 16.0814 - mae: 1.6763 - 23s/epoch - 1ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 24s - loss: 16.0522 - mse: 16.0522 - mae: 1.6776 - 24s/epoch - 1ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 26s - loss: 16.0344 - mse: 16.0344 - mae: 1.6788 - 26s/epoch - 1ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 27s - loss: 15.9386 - mse: 15.9386 - mae: 1.6728 - 27s/epoch - 1ms/step\n",
            "Score for fold 1: loss of 24.05752944946289\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 27s - loss: 15.9210 - mse: 15.9210 - mae: 1.6771 - 27s/epoch - 1ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 26s - loss: 15.8140 - mse: 15.8140 - mae: 1.6656 - 26s/epoch - 1ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 26s - loss: 15.7784 - mse: 15.7784 - mae: 1.6708 - 26s/epoch - 1ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 26s - loss: 15.7558 - mse: 15.7558 - mae: 1.6677 - 26s/epoch - 1ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 26s - loss: 15.7577 - mse: 15.7577 - mae: 1.6694 - 26s/epoch - 1ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 26s - loss: 15.7500 - mse: 15.7500 - mae: 1.6706 - 26s/epoch - 1ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 26s - loss: 15.7706 - mse: 15.7706 - mae: 1.6716 - 26s/epoch - 1ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 25s - loss: 15.6852 - mse: 15.6852 - mae: 1.6742 - 25s/epoch - 1ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 25s - loss: 15.6570 - mse: 15.6570 - mae: 1.6718 - 25s/epoch - 1ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 26s - loss: 15.6757 - mse: 15.6757 - mae: 1.6765 - 26s/epoch - 1ms/step\n",
            "Score for fold 2: loss of 11.53688907623291\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 26s - loss: 15.5392 - mse: 15.5392 - mae: 1.6699 - 26s/epoch - 1ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 25s - loss: 15.5720 - mse: 15.5720 - mae: 1.6673 - 25s/epoch - 1ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 25s - loss: 15.5632 - mse: 15.5632 - mae: 1.6720 - 25s/epoch - 1ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 26s - loss: 15.5540 - mse: 15.5540 - mae: 1.6694 - 26s/epoch - 1ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 29s - loss: 15.4398 - mse: 15.4398 - mae: 1.6705 - 29s/epoch - 1ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 27s - loss: 15.4979 - mse: 15.4979 - mae: 1.6672 - 27s/epoch - 1ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 26s - loss: 15.4781 - mse: 15.4782 - mae: 1.6715 - 26s/epoch - 1ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 26s - loss: 15.3830 - mse: 15.3830 - mae: 1.6610 - 26s/epoch - 1ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 26s - loss: 15.3794 - mse: 15.3794 - mae: 1.6676 - 26s/epoch - 1ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 26s - loss: 15.3701 - mse: 15.3701 - mae: 1.6682 - 26s/epoch - 1ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-10-05 00:24:10,486]\u001b[0m Finished trial#6 resulted in value: 15.37014102935791. Current best value is 13.020280838012695 with parameters: {'activation': 'relu', 'optimizer': 'adam', 'num_hidden_layer': 3, 'num_hidden_unit': 159, 'learning_rate': 0.00025128296799451465}.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Score for fold 3: loss of 11.702485084533691\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 17s - loss: 27.6561 - mse: 27.6561 - mae: 2.8132 - 17s/epoch - 870us/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 18s - loss: 27.4719 - mse: 27.4719 - mae: 2.7916 - 18s/epoch - 880us/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 17s - loss: 27.3570 - mse: 27.3570 - mae: 2.7780 - 17s/epoch - 854us/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 16s - loss: 27.2651 - mse: 27.2651 - mae: 2.7670 - 16s/epoch - 780us/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 16s - loss: 27.1867 - mse: 27.1867 - mae: 2.7575 - 16s/epoch - 818us/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 16s - loss: 27.1173 - mse: 27.1173 - mae: 2.7491 - 16s/epoch - 820us/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 16s - loss: 27.0542 - mse: 27.0542 - mae: 2.7415 - 16s/epoch - 797us/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 16s - loss: 26.9966 - mse: 26.9966 - mae: 2.7344 - 16s/epoch - 791us/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 16s - loss: 26.9430 - mse: 26.9430 - mae: 2.7279 - 16s/epoch - 802us/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 16s - loss: 26.8929 - mse: 26.8929 - mae: 2.7217 - 16s/epoch - 803us/step\n",
            "Score for fold 1: loss of 23.130714416503906\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 16s - loss: 26.8456 - mse: 26.8456 - mae: 2.7159 - 16s/epoch - 815us/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 16s - loss: 26.8008 - mse: 26.8008 - mae: 2.7104 - 16s/epoch - 784us/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 15s - loss: 26.7581 - mse: 26.7581 - mae: 2.7051 - 15s/epoch - 769us/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 16s - loss: 26.7174 - mse: 26.7174 - mae: 2.7000 - 16s/epoch - 795us/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 16s - loss: 26.6784 - mse: 26.6784 - mae: 2.6952 - 16s/epoch - 823us/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 17s - loss: 26.6408 - mse: 26.6408 - mae: 2.6905 - 17s/epoch - 830us/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 17s - loss: 26.6045 - mse: 26.6045 - mae: 2.6860 - 17s/epoch - 870us/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 17s - loss: 26.5694 - mse: 26.5694 - mae: 2.6816 - 17s/epoch - 865us/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 17s - loss: 26.5358 - mse: 26.5358 - mae: 2.6774 - 17s/epoch - 849us/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 17s - loss: 26.5029 - mse: 26.5029 - mae: 2.6732 - 17s/epoch - 827us/step\n",
            "Score for fold 2: loss of 22.097604751586914\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 17s - loss: 26.4710 - mse: 26.4710 - mae: 2.6692 - 17s/epoch - 833us/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 18s - loss: 26.4402 - mse: 26.4402 - mae: 2.6653 - 18s/epoch - 924us/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 18s - loss: 26.4100 - mse: 26.4100 - mae: 2.6615 - 18s/epoch - 918us/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 18s - loss: 26.3807 - mse: 26.3807 - mae: 2.6578 - 18s/epoch - 918us/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 19s - loss: 26.3521 - mse: 26.3521 - mae: 2.6542 - 19s/epoch - 938us/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 19s - loss: 26.3242 - mse: 26.3242 - mae: 2.6506 - 19s/epoch - 942us/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 18s - loss: 26.2968 - mse: 26.2968 - mae: 2.6472 - 18s/epoch - 915us/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 18s - loss: 26.2701 - mse: 26.2701 - mae: 2.6438 - 18s/epoch - 893us/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 18s - loss: 26.2440 - mse: 26.2440 - mae: 2.6404 - 18s/epoch - 884us/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 18s - loss: 26.2184 - mse: 26.2184 - mae: 2.6372 - 18s/epoch - 891us/step\n",
            "Score for fold 3: loss of 21.715545654296875\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-10-05 00:32:48,926]\u001b[0m Finished trial#7 resulted in value: 26.218360900878906. Current best value is 13.020280838012695 with parameters: {'activation': 'relu', 'optimizer': 'adam', 'num_hidden_layer': 3, 'num_hidden_unit': 159, 'learning_rate': 0.00025128296799451465}.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 60s - loss: 28.5533 - mse: 28.5533 - mae: 1.9275 - 60s/epoch - 3ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 60s - loss: 21.4026 - mse: 21.4026 - mae: 1.9099 - 60s/epoch - 3ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 57s - loss: 21.4014 - mse: 21.4014 - mae: 1.9099 - 57s/epoch - 3ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 56s - loss: 21.4048 - mse: 21.4048 - mae: 1.9094 - 56s/epoch - 3ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 54s - loss: 21.4018 - mse: 21.4018 - mae: 1.9102 - 54s/epoch - 3ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 53s - loss: 21.4006 - mse: 21.4006 - mae: 1.9096 - 53s/epoch - 3ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 51s - loss: 21.4013 - mse: 21.4013 - mae: 1.9095 - 51s/epoch - 3ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 53s - loss: 21.4028 - mse: 21.4028 - mae: 1.9094 - 53s/epoch - 3ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 53s - loss: 21.4036 - mse: 21.4036 - mae: 1.9102 - 53s/epoch - 3ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 53s - loss: 21.4006 - mse: 21.4006 - mae: 1.9090 - 53s/epoch - 3ms/step\n",
            "Score for fold 1: loss of 25.627925872802734\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 52s - loss: 21.3999 - mse: 21.3999 - mae: 1.9088 - 52s/epoch - 3ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 53s - loss: 21.4024 - mse: 21.4024 - mae: 1.9096 - 53s/epoch - 3ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 50s - loss: 21.3991 - mse: 21.3991 - mae: 1.9093 - 50s/epoch - 3ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 48s - loss: 21.4006 - mse: 21.4006 - mae: 1.9100 - 48s/epoch - 2ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 49s - loss: 21.3999 - mse: 21.3999 - mae: 1.9098 - 49s/epoch - 2ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 49s - loss: 21.3983 - mse: 21.3983 - mae: 1.9096 - 49s/epoch - 2ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 51s - loss: 21.4030 - mse: 21.4030 - mae: 1.9097 - 51s/epoch - 3ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 53s - loss: 21.4035 - mse: 21.4035 - mae: 1.9095 - 53s/epoch - 3ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 52s - loss: 21.4018 - mse: 21.4018 - mae: 1.9104 - 52s/epoch - 3ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 51s - loss: 21.4030 - mse: 21.4030 - mae: 1.9095 - 51s/epoch - 3ms/step\n",
            "Score for fold 2: loss of 16.974515914916992\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 51s - loss: 21.4040 - mse: 21.4040 - mae: 1.9099 - 51s/epoch - 3ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 54s - loss: 21.4011 - mse: 21.4011 - mae: 1.9096 - 54s/epoch - 3ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 54s - loss: 21.4024 - mse: 21.4024 - mae: 1.9095 - 54s/epoch - 3ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 54s - loss: 21.4020 - mse: 21.4020 - mae: 1.9097 - 54s/epoch - 3ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 52s - loss: 21.3999 - mse: 21.3999 - mae: 1.9094 - 52s/epoch - 3ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 51s - loss: 21.4008 - mse: 21.4008 - mae: 1.9091 - 51s/epoch - 3ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 51s - loss: 21.4045 - mse: 21.4045 - mae: 1.9098 - 51s/epoch - 3ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 51s - loss: 21.4023 - mse: 21.4023 - mae: 1.9095 - 51s/epoch - 3ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 51s - loss: 21.4002 - mse: 21.4002 - mae: 1.9102 - 51s/epoch - 3ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 50s - loss: 21.4018 - mse: 21.4018 - mae: 1.9098 - 50s/epoch - 3ms/step\n",
            "Score for fold 3: loss of 16.96730613708496\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-10-05 00:59:19,218]\u001b[0m Finished trial#8 resulted in value: 21.401775360107422. Current best value is 13.020280838012695 with parameters: {'activation': 'relu', 'optimizer': 'adam', 'num_hidden_layer': 3, 'num_hidden_unit': 159, 'learning_rate': 0.00025128296799451465}.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 24s - loss: 22.1278 - mse: 22.1278 - mae: 2.4152 - 24s/epoch - 1ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 24s - loss: 21.9905 - mse: 21.9905 - mae: 2.4081 - 24s/epoch - 1ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 24s - loss: 21.8465 - mse: 21.8465 - mae: 2.3960 - 24s/epoch - 1ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 24s - loss: 21.6643 - mse: 21.6643 - mae: 2.3972 - 24s/epoch - 1ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 24s - loss: 21.5576 - mse: 21.5576 - mae: 2.3853 - 24s/epoch - 1ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 25s - loss: 21.3313 - mse: 21.3313 - mae: 2.3592 - 25s/epoch - 1ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 24s - loss: 21.6235 - mse: 21.6235 - mae: 2.3905 - 24s/epoch - 1ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 24s - loss: 21.4873 - mse: 21.4873 - mae: 2.3840 - 24s/epoch - 1ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 24s - loss: 21.4963 - mse: 21.4963 - mae: 2.3995 - 24s/epoch - 1ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 24s - loss: 21.2966 - mse: 21.2966 - mae: 2.3783 - 24s/epoch - 1ms/step\n",
            "Score for fold 1: loss of 24.096323013305664\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 25s - loss: 21.4498 - mse: 21.4498 - mae: 2.3849 - 25s/epoch - 1ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 25s - loss: 21.4817 - mse: 21.4817 - mae: 2.3782 - 25s/epoch - 1ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 25s - loss: 21.5067 - mse: 21.5067 - mae: 2.3936 - 25s/epoch - 1ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 25s - loss: 21.3148 - mse: 21.3148 - mae: 2.3737 - 25s/epoch - 1ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 25s - loss: 21.4399 - mse: 21.4399 - mae: 2.3796 - 25s/epoch - 1ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 25s - loss: 21.4057 - mse: 21.4057 - mae: 2.3948 - 25s/epoch - 1ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 25s - loss: 21.3376 - mse: 21.3376 - mae: 2.3699 - 25s/epoch - 1ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 26s - loss: 21.2986 - mse: 21.2986 - mae: 2.3723 - 26s/epoch - 1ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 26s - loss: 21.3507 - mse: 21.3507 - mae: 2.3753 - 26s/epoch - 1ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 25s - loss: 21.0131 - mse: 21.0131 - mae: 2.3571 - 25s/epoch - 1ms/step\n",
            "Score for fold 2: loss of 14.800067901611328\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 25s - loss: 21.1371 - mse: 21.1371 - mae: 2.3633 - 25s/epoch - 1ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 25s - loss: 21.0507 - mse: 21.0507 - mae: 2.3527 - 25s/epoch - 1ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 25s - loss: 21.0120 - mse: 21.0120 - mae: 2.3458 - 25s/epoch - 1ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 25s - loss: 20.9950 - mse: 20.9950 - mae: 2.3438 - 25s/epoch - 1ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 25s - loss: 21.1569 - mse: 21.1569 - mae: 2.3638 - 25s/epoch - 1ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 25s - loss: 21.0253 - mse: 21.0253 - mae: 2.3528 - 25s/epoch - 1ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 26s - loss: 20.8485 - mse: 20.8485 - mae: 2.3361 - 26s/epoch - 1ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 26s - loss: 20.6736 - mse: 20.6736 - mae: 2.3236 - 26s/epoch - 1ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 26s - loss: 20.6336 - mse: 20.6336 - mae: 2.3151 - 26s/epoch - 1ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 26s - loss: 20.8287 - mse: 20.8287 - mae: 2.3235 - 26s/epoch - 1ms/step\n",
            "Score for fold 3: loss of 14.759678840637207\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-10-05 01:11:54,883]\u001b[0m Finished trial#9 resulted in value: 20.828689575195312. Current best value is 13.020280838012695 with parameters: {'activation': 'relu', 'optimizer': 'adam', 'num_hidden_layer': 3, 'num_hidden_unit': 159, 'learning_rate': 0.00025128296799451465}.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 79s - loss: 19.8920 - mse: 19.8920 - mae: 1.6593 - 79s/epoch - 4ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 79s - loss: 18.8291 - mse: 18.8291 - mae: 1.6623 - 79s/epoch - 4ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 80s - loss: 18.5642 - mse: 18.5642 - mae: 1.6472 - 80s/epoch - 4ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 79s - loss: 18.3984 - mse: 18.3984 - mae: 1.6309 - 79s/epoch - 4ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 71s - loss: 18.2782 - mse: 18.2782 - mae: 1.6249 - 71s/epoch - 4ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 71s - loss: 18.1826 - mse: 18.1826 - mae: 1.6200 - 71s/epoch - 4ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 68s - loss: 18.1033 - mse: 18.1033 - mae: 1.6157 - 68s/epoch - 3ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 70s - loss: 18.0352 - mse: 18.0352 - mae: 1.6116 - 70s/epoch - 3ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 71s - loss: 17.9752 - mse: 17.9752 - mae: 1.6087 - 71s/epoch - 4ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 72s - loss: 17.9217 - mse: 17.9217 - mae: 1.6055 - 72s/epoch - 4ms/step\n",
            "Score for fold 1: loss of 22.60212516784668\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 70s - loss: 17.8737 - mse: 17.8737 - mae: 1.6048 - 70s/epoch - 3ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 70s - loss: 17.8301 - mse: 17.8301 - mae: 1.6008 - 70s/epoch - 4ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 70s - loss: 17.7905 - mse: 17.7905 - mae: 1.5985 - 70s/epoch - 3ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 70s - loss: 17.7540 - mse: 17.7540 - mae: 1.5979 - 70s/epoch - 4ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 79s - loss: 17.7201 - mse: 17.7201 - mae: 1.5978 - 79s/epoch - 4ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 79s - loss: 17.6885 - mse: 17.6885 - mae: 1.5955 - 79s/epoch - 4ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 81s - loss: 17.6588 - mse: 17.6588 - mae: 1.5926 - 81s/epoch - 4ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 78s - loss: 17.6309 - mse: 17.6309 - mae: 1.5939 - 78s/epoch - 4ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 76s - loss: 17.6045 - mse: 17.6045 - mae: 1.5918 - 76s/epoch - 4ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 78s - loss: 17.5795 - mse: 17.5795 - mae: 1.5911 - 78s/epoch - 4ms/step\n",
            "Score for fold 2: loss of 13.193120956420898\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 80s - loss: 17.5558 - mse: 17.5558 - mae: 1.5891 - 80s/epoch - 4ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 79s - loss: 17.5332 - mse: 17.5332 - mae: 1.5874 - 79s/epoch - 4ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 81s - loss: 17.5117 - mse: 17.5117 - mae: 1.5868 - 81s/epoch - 4ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 78s - loss: 17.4910 - mse: 17.4910 - mae: 1.5858 - 78s/epoch - 4ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 76s - loss: 17.4713 - mse: 17.4713 - mae: 1.5854 - 76s/epoch - 4ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 80s - loss: 17.4523 - mse: 17.4523 - mae: 1.5854 - 80s/epoch - 4ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 79s - loss: 17.4342 - mse: 17.4342 - mae: 1.5831 - 79s/epoch - 4ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 79s - loss: 17.4167 - mse: 17.4167 - mae: 1.5831 - 79s/epoch - 4ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 76s - loss: 17.3998 - mse: 17.3998 - mae: 1.5821 - 76s/epoch - 4ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 77s - loss: 17.3836 - mse: 17.3836 - mae: 1.5807 - 77s/epoch - 4ms/step\n",
            "Score for fold 3: loss of 12.871527671813965\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-10-05 01:50:06,496]\u001b[0m Finished trial#10 resulted in value: 17.383586883544922. Current best value is 13.020280838012695 with parameters: {'activation': 'relu', 'optimizer': 'adam', 'num_hidden_layer': 3, 'num_hidden_unit': 159, 'learning_rate': 0.00025128296799451465}.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 17s - loss: 19.2690 - mse: 19.2690 - mae: 1.6827 - 17s/epoch - 831us/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 16s - loss: 17.3381 - mse: 17.3381 - mae: 1.5869 - 16s/epoch - 797us/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 16s - loss: 17.2060 - mse: 17.2060 - mae: 1.5754 - 16s/epoch - 807us/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 16s - loss: 17.2899 - mse: 17.2899 - mae: 1.5714 - 16s/epoch - 817us/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 17s - loss: 17.2788 - mse: 17.2788 - mae: 1.5684 - 17s/epoch - 854us/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 17s - loss: 17.3144 - mse: 17.3144 - mae: 1.5671 - 17s/epoch - 837us/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 17s - loss: 17.3093 - mse: 17.3093 - mae: 1.5654 - 17s/epoch - 834us/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 16s - loss: 17.3245 - mse: 17.3245 - mae: 1.5655 - 16s/epoch - 799us/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 16s - loss: 17.3081 - mse: 17.3081 - mae: 1.5642 - 16s/epoch - 805us/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 17s - loss: 17.3231 - mse: 17.3231 - mae: 1.5649 - 17s/epoch - 835us/step\n",
            "Score for fold 1: loss of 24.690004348754883\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 16s - loss: 17.3057 - mse: 17.3057 - mae: 1.5644 - 16s/epoch - 802us/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 17s - loss: 17.3477 - mse: 17.3477 - mae: 1.5647 - 17s/epoch - 846us/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 17s - loss: 17.3437 - mse: 17.3437 - mae: 1.5641 - 17s/epoch - 850us/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 17s - loss: 17.3631 - mse: 17.3631 - mae: 1.5640 - 17s/epoch - 834us/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 17s - loss: 17.3486 - mse: 17.3486 - mae: 1.5639 - 17s/epoch - 826us/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 16s - loss: 17.3480 - mse: 17.3480 - mae: 1.5631 - 16s/epoch - 820us/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 17s - loss: 17.3223 - mse: 17.3223 - mae: 1.5646 - 17s/epoch - 827us/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 16s - loss: 17.3361 - mse: 17.3361 - mae: 1.5653 - 16s/epoch - 812us/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 17s - loss: 17.3310 - mse: 17.3310 - mae: 1.5646 - 17s/epoch - 837us/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 17s - loss: 17.3667 - mse: 17.3667 - mae: 1.5642 - 17s/epoch - 866us/step\n",
            "Score for fold 2: loss of 12.882192611694336\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 16s - loss: 17.3587 - mse: 17.3587 - mae: 1.5644 - 16s/epoch - 810us/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 17s - loss: 17.3461 - mse: 17.3461 - mae: 1.5638 - 17s/epoch - 847us/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 17s - loss: 17.3363 - mse: 17.3363 - mae: 1.5650 - 17s/epoch - 827us/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 16s - loss: 17.3703 - mse: 17.3703 - mae: 1.5644 - 16s/epoch - 804us/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 16s - loss: 17.3314 - mse: 17.3314 - mae: 1.5639 - 16s/epoch - 810us/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 16s - loss: 17.3486 - mse: 17.3486 - mae: 1.5645 - 16s/epoch - 814us/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 16s - loss: 17.3340 - mse: 17.3340 - mae: 1.5646 - 16s/epoch - 820us/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 17s - loss: 17.3480 - mse: 17.3480 - mae: 1.5648 - 17s/epoch - 850us/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 16s - loss: 17.3827 - mse: 17.3827 - mae: 1.5638 - 16s/epoch - 822us/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 16s - loss: 17.3681 - mse: 17.3681 - mae: 1.5643 - 16s/epoch - 804us/step\n",
            "Score for fold 3: loss of 12.969449996948242\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-10-05 01:58:29,448]\u001b[0m Finished trial#11 resulted in value: 17.36809539794922. Current best value is 13.020280838012695 with parameters: {'activation': 'relu', 'optimizer': 'adam', 'num_hidden_layer': 3, 'num_hidden_unit': 159, 'learning_rate': 0.00025128296799451465}.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 30s - loss: 18.6249 - mse: 18.6249 - mae: 1.8704 - 30s/epoch - 1ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 29s - loss: 18.0071 - mse: 18.0071 - mae: 1.8449 - 29s/epoch - 1ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 29s - loss: 17.8479 - mse: 17.8479 - mae: 1.8352 - 29s/epoch - 1ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 30s - loss: 17.6922 - mse: 17.6922 - mae: 1.8330 - 30s/epoch - 1ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 30s - loss: 17.7110 - mse: 17.7110 - mae: 1.8395 - 30s/epoch - 1ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 30s - loss: 17.5969 - mse: 17.5969 - mae: 1.8451 - 30s/epoch - 1ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 29s - loss: 17.4546 - mse: 17.4546 - mae: 1.8201 - 29s/epoch - 1ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 30s - loss: 17.2028 - mse: 17.2028 - mae: 1.8271 - 30s/epoch - 1ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 30s - loss: 17.2018 - mse: 17.2018 - mae: 1.8174 - 30s/epoch - 1ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 30s - loss: 17.2830 - mse: 17.2830 - mae: 1.8329 - 30s/epoch - 1ms/step\n",
            "Score for fold 1: loss of 23.851869583129883\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 29s - loss: 17.2946 - mse: 17.2946 - mae: 1.8381 - 29s/epoch - 1ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 30s - loss: 17.3355 - mse: 17.3355 - mae: 1.8316 - 30s/epoch - 1ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 30s - loss: 17.1986 - mse: 17.1986 - mae: 1.8251 - 30s/epoch - 1ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 30s - loss: 17.0942 - mse: 17.0942 - mae: 1.8320 - 30s/epoch - 1ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 30s - loss: 17.0953 - mse: 17.0953 - mae: 1.8326 - 30s/epoch - 1ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 30s - loss: 17.0363 - mse: 17.0363 - mae: 1.8226 - 30s/epoch - 1ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 30s - loss: 16.9785 - mse: 16.9785 - mae: 1.8295 - 30s/epoch - 2ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 30s - loss: 17.0255 - mse: 17.0255 - mae: 1.8356 - 30s/epoch - 1ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 30s - loss: 16.9568 - mse: 16.9568 - mae: 1.8287 - 30s/epoch - 2ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 31s - loss: 17.0209 - mse: 17.0209 - mae: 1.8426 - 31s/epoch - 2ms/step\n",
            "Score for fold 2: loss of 13.405728340148926\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 30s - loss: 16.9939 - mse: 16.9939 - mae: 1.8307 - 30s/epoch - 1ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 30s - loss: 16.8938 - mse: 16.8938 - mae: 1.8440 - 30s/epoch - 1ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 30s - loss: 16.9165 - mse: 16.9165 - mae: 1.8346 - 30s/epoch - 1ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 30s - loss: 16.9968 - mse: 16.9968 - mae: 1.8436 - 30s/epoch - 1ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 30s - loss: 16.7918 - mse: 16.7918 - mae: 1.8387 - 30s/epoch - 1ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 30s - loss: 16.8721 - mse: 16.8721 - mae: 1.8392 - 30s/epoch - 2ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 31s - loss: 16.9199 - mse: 16.9199 - mae: 1.8415 - 31s/epoch - 2ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 30s - loss: 16.8376 - mse: 16.8376 - mae: 1.8424 - 30s/epoch - 2ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 31s - loss: 16.7954 - mse: 16.7954 - mae: 1.8424 - 31s/epoch - 2ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 31s - loss: 16.8080 - mse: 16.8080 - mae: 1.8439 - 31s/epoch - 2ms/step\n",
            "Score for fold 3: loss of 12.217122077941895\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-10-05 02:13:38,592]\u001b[0m Finished trial#12 resulted in value: 16.808002471923828. Current best value is 13.020280838012695 with parameters: {'activation': 'relu', 'optimizer': 'adam', 'num_hidden_layer': 3, 'num_hidden_unit': 159, 'learning_rate': 0.00025128296799451465}.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 16s - loss: 18.3245 - mse: 18.3245 - mae: 1.6310 - 16s/epoch - 809us/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 16s - loss: 18.2405 - mse: 18.2405 - mae: 1.6408 - 16s/epoch - 783us/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 16s - loss: 20.1530 - mse: 20.1530 - mae: 1.6514 - 16s/epoch - 790us/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 16s - loss: 18.5558 - mse: 18.5558 - mae: 1.6478 - 16s/epoch - 806us/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 16s - loss: 18.4103 - mse: 18.4103 - mae: 1.6367 - 16s/epoch - 806us/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 16s - loss: 18.0432 - mse: 18.0432 - mae: 1.6362 - 16s/epoch - 797us/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 16s - loss: 19.2238 - mse: 19.2238 - mae: 1.6550 - 16s/epoch - 793us/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 16s - loss: 20.0298 - mse: 20.0298 - mae: 1.6618 - 16s/epoch - 790us/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 15s - loss: 17.4750 - mse: 17.4750 - mae: 1.6235 - 15s/epoch - 764us/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 15s - loss: 19.6005 - mse: 19.6005 - mae: 1.6628 - 15s/epoch - 773us/step\n",
            "Score for fold 1: loss of 27.034992218017578\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 16s - loss: 19.1771 - mse: 19.1771 - mae: 1.6337 - 16s/epoch - 781us/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 15s - loss: 18.3329 - mse: 18.3329 - mae: 1.6504 - 15s/epoch - 741us/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 15s - loss: 18.0197 - mse: 18.0197 - mae: 1.6382 - 15s/epoch - 764us/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 15s - loss: 17.5679 - mse: 17.5679 - mae: 1.6223 - 15s/epoch - 764us/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 15s - loss: 17.7837 - mse: 17.7837 - mae: 1.6282 - 15s/epoch - 755us/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 16s - loss: 17.6699 - mse: 17.6699 - mae: 1.6228 - 16s/epoch - 786us/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 15s - loss: 19.0914 - mse: 19.0914 - mae: 1.6425 - 15s/epoch - 764us/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 15s - loss: 18.7333 - mse: 18.7333 - mae: 1.6514 - 15s/epoch - 759us/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 16s - loss: 19.6832 - mse: 19.6832 - mae: 1.6582 - 16s/epoch - 819us/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 15s - loss: 18.9601 - mse: 18.9601 - mae: 1.6400 - 15s/epoch - 754us/step\n",
            "Score for fold 2: loss of 13.698552131652832\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 15s - loss: 17.4235 - mse: 17.4235 - mae: 1.6218 - 15s/epoch - 755us/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 16s - loss: 17.7381 - mse: 17.7381 - mae: 1.6362 - 16s/epoch - 778us/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 16s - loss: 17.3947 - mse: 17.3947 - mae: 1.6259 - 16s/epoch - 782us/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 15s - loss: 18.2558 - mse: 18.2558 - mae: 1.6341 - 15s/epoch - 749us/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 15s - loss: 18.4334 - mse: 18.4334 - mae: 1.6457 - 15s/epoch - 754us/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 15s - loss: 18.6223 - mse: 18.6223 - mae: 1.6572 - 15s/epoch - 750us/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 15s - loss: 19.4959 - mse: 19.4959 - mae: 1.6402 - 15s/epoch - 773us/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 15s - loss: 19.0060 - mse: 19.0060 - mae: 1.6364 - 15s/epoch - 736us/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 15s - loss: 18.1505 - mse: 18.1505 - mae: 1.6523 - 15s/epoch - 749us/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 15s - loss: 18.6655 - mse: 18.6655 - mae: 1.6463 - 15s/epoch - 731us/step\n",
            "Score for fold 3: loss of 12.508822441101074\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-10-05 02:21:29,698]\u001b[0m Finished trial#13 resulted in value: 18.665470123291016. Current best value is 13.020280838012695 with parameters: {'activation': 'relu', 'optimizer': 'adam', 'num_hidden_layer': 3, 'num_hidden_unit': 159, 'learning_rate': 0.00025128296799451465}.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 43s - loss: 16.3972 - mse: 16.3972 - mae: 1.5774 - 43s/epoch - 2ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 43s - loss: 15.8311 - mse: 15.8311 - mae: 1.5450 - 43s/epoch - 2ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 43s - loss: 15.5759 - mse: 15.5759 - mae: 1.5294 - 43s/epoch - 2ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 43s - loss: 15.4040 - mse: 15.4040 - mae: 1.5202 - 43s/epoch - 2ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 43s - loss: 15.2952 - mse: 15.2952 - mae: 1.5090 - 43s/epoch - 2ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 43s - loss: 15.1419 - mse: 15.1419 - mae: 1.5032 - 43s/epoch - 2ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 42s - loss: 15.0210 - mse: 15.0210 - mae: 1.4983 - 42s/epoch - 2ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 43s - loss: 14.8592 - mse: 14.8592 - mae: 1.4930 - 43s/epoch - 2ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 42s - loss: 14.7874 - mse: 14.7874 - mae: 1.4887 - 42s/epoch - 2ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 43s - loss: 14.6479 - mse: 14.6479 - mae: 1.4855 - 43s/epoch - 2ms/step\n",
            "Score for fold 1: loss of 24.101884841918945\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 44s - loss: 14.5405 - mse: 14.5405 - mae: 1.4819 - 44s/epoch - 2ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 45s - loss: 14.4200 - mse: 14.4200 - mae: 1.4818 - 45s/epoch - 2ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 44s - loss: 14.4058 - mse: 14.4058 - mae: 1.4808 - 44s/epoch - 2ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 43s - loss: 14.3101 - mse: 14.3101 - mae: 1.4798 - 43s/epoch - 2ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 43s - loss: 14.2467 - mse: 14.2467 - mae: 1.4811 - 43s/epoch - 2ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 43s - loss: 14.1582 - mse: 14.1582 - mae: 1.4762 - 43s/epoch - 2ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 43s - loss: 14.0669 - mse: 14.0669 - mae: 1.4764 - 43s/epoch - 2ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 44s - loss: 14.0402 - mse: 14.0402 - mae: 1.4740 - 44s/epoch - 2ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 43s - loss: 13.9239 - mse: 13.9239 - mae: 1.4762 - 43s/epoch - 2ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 43s - loss: 13.8925 - mse: 13.8925 - mae: 1.4724 - 43s/epoch - 2ms/step\n",
            "Score for fold 2: loss of 10.62740421295166\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 43s - loss: 13.7641 - mse: 13.7641 - mae: 1.4745 - 43s/epoch - 2ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 43s - loss: 13.8187 - mse: 13.8187 - mae: 1.4747 - 43s/epoch - 2ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 43s - loss: 13.7594 - mse: 13.7594 - mae: 1.4712 - 43s/epoch - 2ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 42s - loss: 13.6047 - mse: 13.6047 - mae: 1.4713 - 42s/epoch - 2ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 43s - loss: 13.5622 - mse: 13.5622 - mae: 1.4716 - 43s/epoch - 2ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 42s - loss: 13.5084 - mse: 13.5084 - mae: 1.4718 - 42s/epoch - 2ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 42s - loss: 13.5127 - mse: 13.5127 - mae: 1.4717 - 42s/epoch - 2ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 43s - loss: 13.4030 - mse: 13.4030 - mae: 1.4695 - 43s/epoch - 2ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 44s - loss: 13.4755 - mse: 13.4755 - mae: 1.4692 - 44s/epoch - 2ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 43s - loss: 13.2901 - mse: 13.2901 - mae: 1.4698 - 43s/epoch - 2ms/step\n",
            "Score for fold 3: loss of 10.093722343444824\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-10-05 02:43:13,047]\u001b[0m Finished trial#14 resulted in value: 13.290119171142578. Current best value is 13.020280838012695 with parameters: {'activation': 'relu', 'optimizer': 'adam', 'num_hidden_layer': 3, 'num_hidden_unit': 159, 'learning_rate': 0.00025128296799451465}.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 22s - loss: 16.6986 - mse: 16.6986 - mae: 1.5265 - 22s/epoch - 1ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 22s - loss: 15.9876 - mse: 15.9876 - mae: 1.5048 - 22s/epoch - 1ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 22s - loss: 15.7645 - mse: 15.7645 - mae: 1.4992 - 22s/epoch - 1ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 23s - loss: 15.6086 - mse: 15.6086 - mae: 1.4976 - 23s/epoch - 1ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 22s - loss: 15.5266 - mse: 15.5266 - mae: 1.4942 - 22s/epoch - 1ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 22s - loss: 15.4728 - mse: 15.4728 - mae: 1.4926 - 22s/epoch - 1ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 23s - loss: 15.4717 - mse: 15.4717 - mae: 1.4900 - 23s/epoch - 1ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 22s - loss: 15.4063 - mse: 15.4063 - mae: 1.4899 - 22s/epoch - 1ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 23s - loss: 15.3659 - mse: 15.3659 - mae: 1.4864 - 23s/epoch - 1ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 23s - loss: 15.3062 - mse: 15.3062 - mae: 1.4848 - 23s/epoch - 1ms/step\n",
            "Score for fold 1: loss of 24.843650817871094\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 21s - loss: 15.2800 - mse: 15.2800 - mae: 1.4824 - 21s/epoch - 1ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 22s - loss: 15.2531 - mse: 15.2531 - mae: 1.4800 - 22s/epoch - 1ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 22s - loss: 15.2557 - mse: 15.2557 - mae: 1.4808 - 22s/epoch - 1ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 22s - loss: 15.2040 - mse: 15.2040 - mae: 1.4799 - 22s/epoch - 1ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 21s - loss: 15.1849 - mse: 15.1849 - mae: 1.4785 - 21s/epoch - 1ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 22s - loss: 15.1754 - mse: 15.1754 - mae: 1.4784 - 22s/epoch - 1ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 23s - loss: 15.1685 - mse: 15.1685 - mae: 1.4780 - 23s/epoch - 1ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 23s - loss: 15.1583 - mse: 15.1583 - mae: 1.4775 - 23s/epoch - 1ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 23s - loss: 15.1851 - mse: 15.1851 - mae: 1.4778 - 23s/epoch - 1ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 22s - loss: 15.1786 - mse: 15.1786 - mae: 1.4789 - 22s/epoch - 1ms/step\n",
            "Score for fold 2: loss of 11.261674880981445\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 22s - loss: 15.1712 - mse: 15.1712 - mae: 1.4786 - 22s/epoch - 1ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 22s - loss: 15.1728 - mse: 15.1728 - mae: 1.4803 - 22s/epoch - 1ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 23s - loss: 15.1759 - mse: 15.1759 - mae: 1.4825 - 23s/epoch - 1ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 22s - loss: 15.1774 - mse: 15.1774 - mae: 1.4823 - 22s/epoch - 1ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 23s - loss: 15.1698 - mse: 15.1698 - mae: 1.4829 - 23s/epoch - 1ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 23s - loss: 15.1242 - mse: 15.1242 - mae: 1.4821 - 23s/epoch - 1ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 21s - loss: 15.1373 - mse: 15.1373 - mae: 1.4848 - 21s/epoch - 1ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 23s - loss: 15.1235 - mse: 15.1235 - mae: 1.4841 - 23s/epoch - 1ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 23s - loss: 15.1313 - mse: 15.1313 - mae: 1.4846 - 23s/epoch - 1ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 23s - loss: 15.1165 - mse: 15.1165 - mae: 1.4824 - 23s/epoch - 1ms/step\n",
            "Score for fold 3: loss of 11.142592430114746\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-10-05 02:54:31,186]\u001b[0m Finished trial#15 resulted in value: 15.116490364074707. Current best value is 13.020280838012695 with parameters: {'activation': 'relu', 'optimizer': 'adam', 'num_hidden_layer': 3, 'num_hidden_unit': 159, 'learning_rate': 0.00025128296799451465}.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 45s - loss: 17.4889 - mse: 17.4889 - mae: 1.6246 - 45s/epoch - 2ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 46s - loss: 17.2277 - mse: 17.2277 - mae: 1.6109 - 46s/epoch - 2ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 46s - loss: 17.2189 - mse: 17.2189 - mae: 1.6079 - 46s/epoch - 2ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 46s - loss: 17.1251 - mse: 17.1251 - mae: 1.6039 - 46s/epoch - 2ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 47s - loss: 17.1014 - mse: 17.1014 - mae: 1.6049 - 47s/epoch - 2ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 46s - loss: 17.0666 - mse: 17.0666 - mae: 1.6039 - 46s/epoch - 2ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 47s - loss: 17.1253 - mse: 17.1253 - mae: 1.6018 - 47s/epoch - 2ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 48s - loss: 17.0701 - mse: 17.0701 - mae: 1.6029 - 48s/epoch - 2ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 51s - loss: 17.0956 - mse: 17.0956 - mae: 1.5981 - 51s/epoch - 3ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 47s - loss: 17.0940 - mse: 17.0940 - mae: 1.6015 - 47s/epoch - 2ms/step\n",
            "Score for fold 1: loss of 24.2609920501709\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 47s - loss: 17.1155 - mse: 17.1155 - mae: 1.6038 - 47s/epoch - 2ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 47s - loss: 17.0737 - mse: 17.0737 - mae: 1.6004 - 47s/epoch - 2ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 46s - loss: 17.0525 - mse: 17.0525 - mae: 1.5960 - 46s/epoch - 2ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 47s - loss: 17.0483 - mse: 17.0483 - mae: 1.6025 - 47s/epoch - 2ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 47s - loss: 17.0809 - mse: 17.0809 - mae: 1.6010 - 47s/epoch - 2ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 46s - loss: 17.0566 - mse: 17.0566 - mae: 1.5987 - 46s/epoch - 2ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 46s - loss: 17.0606 - mse: 17.0606 - mae: 1.5990 - 46s/epoch - 2ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 46s - loss: 17.0918 - mse: 17.0918 - mae: 1.6023 - 46s/epoch - 2ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 46s - loss: 17.0487 - mse: 17.0487 - mae: 1.6005 - 46s/epoch - 2ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 46s - loss: 17.0712 - mse: 17.0712 - mae: 1.6014 - 46s/epoch - 2ms/step\n",
            "Score for fold 2: loss of 12.30615234375\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 47s - loss: 16.9980 - mse: 16.9980 - mae: 1.6018 - 47s/epoch - 2ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 46s - loss: 17.0585 - mse: 17.0585 - mae: 1.6000 - 46s/epoch - 2ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 46s - loss: 16.9600 - mse: 16.9600 - mae: 1.5995 - 46s/epoch - 2ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 46s - loss: 17.0832 - mse: 17.0832 - mae: 1.5986 - 46s/epoch - 2ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 47s - loss: 17.0377 - mse: 17.0377 - mae: 1.5998 - 47s/epoch - 2ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 47s - loss: 17.0765 - mse: 17.0765 - mae: 1.5992 - 47s/epoch - 2ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 47s - loss: 17.1075 - mse: 17.1075 - mae: 1.5977 - 47s/epoch - 2ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 46s - loss: 17.0650 - mse: 17.0650 - mae: 1.5980 - 46s/epoch - 2ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 47s - loss: 17.0954 - mse: 17.0954 - mae: 1.5987 - 47s/epoch - 2ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 46s - loss: 17.0414 - mse: 17.0414 - mae: 1.5991 - 46s/epoch - 2ms/step\n",
            "Score for fold 3: loss of 12.309948921203613\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-10-05 03:18:02,656]\u001b[0m Finished trial#16 resulted in value: 17.04140281677246. Current best value is 13.020280838012695 with parameters: {'activation': 'relu', 'optimizer': 'adam', 'num_hidden_layer': 3, 'num_hidden_unit': 159, 'learning_rate': 0.00025128296799451465}.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 39s - loss: 16.4217 - mse: 16.4217 - mae: 1.5345 - 39s/epoch - 2ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 39s - loss: 15.6966 - mse: 15.6966 - mae: 1.5010 - 39s/epoch - 2ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 40s - loss: 15.3616 - mse: 15.3616 - mae: 1.4864 - 40s/epoch - 2ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 40s - loss: 15.1012 - mse: 15.1012 - mae: 1.4748 - 40s/epoch - 2ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 39s - loss: 15.0500 - mse: 15.0500 - mae: 1.4696 - 39s/epoch - 2ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 39s - loss: 14.9176 - mse: 14.9176 - mae: 1.4655 - 39s/epoch - 2ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 39s - loss: 14.8546 - mse: 14.8546 - mae: 1.4644 - 39s/epoch - 2ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 39s - loss: 14.7858 - mse: 14.7858 - mae: 1.4605 - 39s/epoch - 2ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 43s - loss: 14.6641 - mse: 14.6641 - mae: 1.4598 - 43s/epoch - 2ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 49s - loss: 14.6097 - mse: 14.6097 - mae: 1.4571 - 49s/epoch - 2ms/step\n",
            "Score for fold 1: loss of 24.10710334777832\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 50s - loss: 14.5640 - mse: 14.5640 - mae: 1.4544 - 50s/epoch - 3ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 43s - loss: 14.4172 - mse: 14.4172 - mae: 1.4527 - 43s/epoch - 2ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 42s - loss: 14.3745 - mse: 14.3745 - mae: 1.4515 - 42s/epoch - 2ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 41s - loss: 14.2694 - mse: 14.2694 - mae: 1.4500 - 41s/epoch - 2ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 40s - loss: 14.1562 - mse: 14.1562 - mae: 1.4521 - 40s/epoch - 2ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 40s - loss: 14.1499 - mse: 14.1499 - mae: 1.4504 - 40s/epoch - 2ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 40s - loss: 13.9742 - mse: 13.9742 - mae: 1.4487 - 40s/epoch - 2ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 42s - loss: 13.8345 - mse: 13.8345 - mae: 1.4477 - 42s/epoch - 2ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 42s - loss: 13.8962 - mse: 13.8962 - mae: 1.4463 - 42s/epoch - 2ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 43s - loss: 13.8946 - mse: 13.8946 - mae: 1.4465 - 43s/epoch - 2ms/step\n",
            "Score for fold 2: loss of 10.379559516906738\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 49s - loss: 13.4297 - mse: 13.4297 - mae: 1.4463 - 49s/epoch - 2ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 49s - loss: 13.4785 - mse: 13.4785 - mae: 1.4475 - 49s/epoch - 2ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 45s - loss: 13.3606 - mse: 13.3606 - mae: 1.4443 - 45s/epoch - 2ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 44s - loss: 13.5295 - mse: 13.5295 - mae: 1.4461 - 44s/epoch - 2ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 46s - loss: 13.2250 - mse: 13.2250 - mae: 1.4446 - 46s/epoch - 2ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 45s - loss: 12.9987 - mse: 12.9987 - mae: 1.4440 - 45s/epoch - 2ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 42s - loss: 13.5156 - mse: 13.5156 - mae: 1.4443 - 42s/epoch - 2ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 45s - loss: 12.8254 - mse: 12.8254 - mae: 1.4439 - 45s/epoch - 2ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 45s - loss: 13.1567 - mse: 13.1567 - mae: 1.4408 - 45s/epoch - 2ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 43s - loss: 13.0025 - mse: 13.0025 - mae: 1.4433 - 43s/epoch - 2ms/step\n",
            "Score for fold 3: loss of 10.423528671264648\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-10-05 03:39:35,690]\u001b[0m Finished trial#17 resulted in value: 13.002501487731934. Current best value is 13.002501487731934 with parameters: {'activation': 'relu', 'optimizer': 'adam', 'num_hidden_layer': 3, 'num_hidden_unit': 355, 'learning_rate': 0.00142911022839336}.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 52s - loss: 18.2051 - mse: 18.2051 - mae: 1.5868 - 52s/epoch - 3ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 54s - loss: 17.3409 - mse: 17.3409 - mae: 1.5499 - 54s/epoch - 3ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 56s - loss: 1712.2772 - mse: 1712.2771 - mae: 2.0619 - 56s/epoch - 3ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 55s - loss: nan - mse: nan - mae: nan - 55s/epoch - 3ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 54s - loss: nan - mse: nan - mae: nan - 54s/epoch - 3ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 54s - loss: nan - mse: nan - mae: nan - 54s/epoch - 3ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 53s - loss: nan - mse: nan - mae: nan - 53s/epoch - 3ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 56s - loss: nan - mse: nan - mae: nan - 56s/epoch - 3ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 51s - loss: nan - mse: nan - mae: nan - 51s/epoch - 3ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 51s - loss: nan - mse: nan - mae: nan - 51s/epoch - 3ms/step\n",
            "Score for fold 1: loss of 24.301929473876953\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 50s - loss: nan - mse: nan - mae: nan - 50s/epoch - 2ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 49s - loss: nan - mse: nan - mae: nan - 49s/epoch - 2ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 49s - loss: nan - mse: nan - mae: nan - 49s/epoch - 2ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 49s - loss: nan - mse: nan - mae: nan - 49s/epoch - 2ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 49s - loss: nan - mse: nan - mae: nan - 49s/epoch - 2ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 49s - loss: nan - mse: nan - mae: nan - 49s/epoch - 2ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 49s - loss: nan - mse: nan - mae: nan - 49s/epoch - 2ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 49s - loss: nan - mse: nan - mae: nan - 49s/epoch - 2ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 49s - loss: nan - mse: nan - mae: nan - 49s/epoch - 2ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 48s - loss: nan - mse: nan - mae: nan - 48s/epoch - 2ms/step\n",
            "Score for fold 2: loss of nan\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 49s - loss: nan - mse: nan - mae: nan - 49s/epoch - 2ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 49s - loss: nan - mse: nan - mae: nan - 49s/epoch - 2ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 49s - loss: nan - mse: nan - mae: nan - 49s/epoch - 2ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 49s - loss: nan - mse: nan - mae: nan - 49s/epoch - 2ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 51s - loss: nan - mse: nan - mae: nan - 51s/epoch - 3ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 52s - loss: nan - mse: nan - mae: nan - 52s/epoch - 3ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 52s - loss: nan - mse: nan - mae: nan - 52s/epoch - 3ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 51s - loss: nan - mse: nan - mae: nan - 51s/epoch - 3ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 52s - loss: nan - mse: nan - mae: nan - 52s/epoch - 3ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 51s - loss: nan - mse: nan - mae: nan - 51s/epoch - 3ms/step\n",
            "Score for fold 3: loss of nan\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[33m[W 2022-10-05 04:05:21,072]\u001b[0m Setting status of trial#18 as TrialState.FAIL because the objective function returned nan.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 73s - loss: 16.3736 - mse: 16.3736 - mae: 1.5136 - 73s/epoch - 4ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 72s - loss: 15.8489 - mse: 15.8489 - mae: 1.4974 - 72s/epoch - 4ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 72s - loss: 15.9416 - mse: 15.9416 - mae: 1.4927 - 72s/epoch - 4ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 73s - loss: 15.6357 - mse: 15.6357 - mae: 1.4804 - 73s/epoch - 4ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 72s - loss: 15.3100 - mse: 15.3100 - mae: 1.4782 - 72s/epoch - 4ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 72s - loss: 15.3666 - mse: 15.3666 - mae: 1.4793 - 72s/epoch - 4ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 72s - loss: 15.1975 - mse: 15.1975 - mae: 1.4808 - 72s/epoch - 4ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 75s - loss: 15.1603 - mse: 15.1603 - mae: 1.4798 - 75s/epoch - 4ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 75s - loss: 15.1384 - mse: 15.1384 - mae: 1.4786 - 75s/epoch - 4ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 75s - loss: 15.1244 - mse: 15.1244 - mae: 1.4777 - 75s/epoch - 4ms/step\n",
            "Score for fold 1: loss of 23.8621883392334\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 68s - loss: 15.2186 - mse: 15.2186 - mae: 1.4804 - 68s/epoch - 3ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 65s - loss: 15.1440 - mse: 15.1440 - mae: 1.4800 - 65s/epoch - 3ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 67s - loss: 15.1866 - mse: 15.1866 - mae: 1.4786 - 67s/epoch - 3ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 66s - loss: 15.1233 - mse: 15.1233 - mae: 1.4801 - 66s/epoch - 3ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 64s - loss: 15.1768 - mse: 15.1768 - mae: 1.4815 - 64s/epoch - 3ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 64s - loss: 15.1529 - mse: 15.1529 - mae: 1.4814 - 64s/epoch - 3ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 64s - loss: 15.1500 - mse: 15.1500 - mae: 1.4825 - 64s/epoch - 3ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 64s - loss: 15.2395 - mse: 15.2395 - mae: 1.4814 - 64s/epoch - 3ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 64s - loss: 15.2830 - mse: 15.2830 - mae: 1.4801 - 64s/epoch - 3ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 65s - loss: 15.2111 - mse: 15.2111 - mae: 1.4798 - 65s/epoch - 3ms/step\n",
            "Score for fold 2: loss of 10.72707748413086\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 65s - loss: 15.3586 - mse: 15.3586 - mae: 1.4807 - 65s/epoch - 3ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 67s - loss: 15.2009 - mse: 15.2009 - mae: 1.4781 - 67s/epoch - 3ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 68s - loss: 15.1369 - mse: 15.1369 - mae: 1.4786 - 68s/epoch - 3ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 68s - loss: 15.1743 - mse: 15.1743 - mae: 1.4773 - 68s/epoch - 3ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 68s - loss: 15.2731 - mse: 15.2731 - mae: 1.4783 - 68s/epoch - 3ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 68s - loss: 15.1480 - mse: 15.1480 - mae: 1.4783 - 68s/epoch - 3ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 66s - loss: 15.1637 - mse: 15.1637 - mae: 1.4800 - 66s/epoch - 3ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 67s - loss: 15.1064 - mse: 15.1064 - mae: 1.4804 - 67s/epoch - 3ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 75s - loss: 15.2216 - mse: 15.2216 - mae: 1.4818 - 75s/epoch - 4ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 67s - loss: 15.0568 - mse: 15.0568 - mae: 1.4809 - 67s/epoch - 3ms/step\n",
            "Score for fold 3: loss of 10.789663314819336\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-10-05 04:39:58,370]\u001b[0m Finished trial#19 resulted in value: 15.056763648986816. Current best value is 13.002501487731934 with parameters: {'activation': 'relu', 'optimizer': 'adam', 'num_hidden_layer': 3, 'num_hidden_unit': 355, 'learning_rate': 0.00142911022839336}.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 42s - loss: 17.8323 - mse: 17.8323 - mae: 1.6042 - 42s/epoch - 2ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 43s - loss: 16.4878 - mse: 16.4878 - mae: 1.5347 - 43s/epoch - 2ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 44s - loss: 16.1652 - mse: 16.1652 - mae: 1.5178 - 44s/epoch - 2ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 43s - loss: 15.9891 - mse: 15.9891 - mae: 1.5107 - 43s/epoch - 2ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 45s - loss: 15.8942 - mse: 15.8942 - mae: 1.5031 - 45s/epoch - 2ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 43s - loss: 15.7959 - mse: 15.7959 - mae: 1.4992 - 43s/epoch - 2ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 43s - loss: 15.7178 - mse: 15.7178 - mae: 1.4962 - 43s/epoch - 2ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 43s - loss: 15.6783 - mse: 15.6783 - mae: 1.4926 - 43s/epoch - 2ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 42s - loss: 15.6303 - mse: 15.6303 - mae: 1.4903 - 42s/epoch - 2ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 42s - loss: 15.5704 - mse: 15.5704 - mae: 1.4883 - 42s/epoch - 2ms/step\n",
            "Score for fold 1: loss of 25.629419326782227\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 42s - loss: 15.5186 - mse: 15.5186 - mae: 1.4876 - 42s/epoch - 2ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 42s - loss: 15.4813 - mse: 15.4813 - mae: 1.4853 - 42s/epoch - 2ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 42s - loss: 15.4297 - mse: 15.4297 - mae: 1.4843 - 42s/epoch - 2ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 39s - loss: 15.4037 - mse: 15.4037 - mae: 1.4818 - 39s/epoch - 2ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 37s - loss: 15.3393 - mse: 15.3393 - mae: 1.4816 - 37s/epoch - 2ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 37s - loss: 15.3348 - mse: 15.3348 - mae: 1.4809 - 37s/epoch - 2ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 37s - loss: 15.2952 - mse: 15.2952 - mae: 1.4789 - 37s/epoch - 2ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 37s - loss: 15.2352 - mse: 15.2352 - mae: 1.4779 - 37s/epoch - 2ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 37s - loss: 15.2250 - mse: 15.2250 - mae: 1.4764 - 37s/epoch - 2ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 37s - loss: 15.1784 - mse: 15.1784 - mae: 1.4760 - 37s/epoch - 2ms/step\n",
            "Score for fold 2: loss of 11.148052215576172\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 38s - loss: 15.1568 - mse: 15.1568 - mae: 1.4740 - 38s/epoch - 2ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 37s - loss: 15.1187 - mse: 15.1187 - mae: 1.4756 - 37s/epoch - 2ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 37s - loss: 15.0912 - mse: 15.0912 - mae: 1.4730 - 37s/epoch - 2ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 37s - loss: 15.0798 - mse: 15.0798 - mae: 1.4732 - 37s/epoch - 2ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 37s - loss: 15.0296 - mse: 15.0296 - mae: 1.4727 - 37s/epoch - 2ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 38s - loss: 15.0210 - mse: 15.0210 - mae: 1.4703 - 38s/epoch - 2ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 39s - loss: 14.9864 - mse: 14.9864 - mae: 1.4708 - 39s/epoch - 2ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 39s - loss: 14.9710 - mse: 14.9710 - mae: 1.4695 - 39s/epoch - 2ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 39s - loss: 14.9463 - mse: 14.9463 - mae: 1.4686 - 39s/epoch - 2ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 39s - loss: 14.9149 - mse: 14.9149 - mae: 1.4680 - 39s/epoch - 2ms/step\n",
            "Score for fold 3: loss of 10.878283500671387\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-10-05 05:00:07,668]\u001b[0m Finished trial#20 resulted in value: 14.914850234985352. Current best value is 13.002501487731934 with parameters: {'activation': 'relu', 'optimizer': 'adam', 'num_hidden_layer': 3, 'num_hidden_unit': 355, 'learning_rate': 0.00142911022839336}.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 22s - loss: nan - mse: nan - mae: nan - 22s/epoch - 1ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 23s - loss: nan - mse: nan - mae: nan - 23s/epoch - 1ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 23s - loss: nan - mse: nan - mae: nan - 23s/epoch - 1ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 23s - loss: nan - mse: nan - mae: nan - 23s/epoch - 1ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 22s - loss: nan - mse: nan - mae: nan - 22s/epoch - 1ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 22s - loss: nan - mse: nan - mae: nan - 22s/epoch - 1ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 22s - loss: nan - mse: nan - mae: nan - 22s/epoch - 1ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 22s - loss: nan - mse: nan - mae: nan - 22s/epoch - 1ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 23s - loss: nan - mse: nan - mae: nan - 23s/epoch - 1ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 22s - loss: nan - mse: nan - mae: nan - 22s/epoch - 1ms/step\n",
            "Score for fold 1: loss of 23.20631217956543\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 22s - loss: nan - mse: nan - mae: nan - 22s/epoch - 1ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 22s - loss: nan - mse: nan - mae: nan - 22s/epoch - 1ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 22s - loss: nan - mse: nan - mae: nan - 22s/epoch - 1ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 22s - loss: nan - mse: nan - mae: nan - 22s/epoch - 1ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 23s - loss: nan - mse: nan - mae: nan - 23s/epoch - 1ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 23s - loss: nan - mse: nan - mae: nan - 23s/epoch - 1ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 23s - loss: nan - mse: nan - mae: nan - 23s/epoch - 1ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 23s - loss: nan - mse: nan - mae: nan - 23s/epoch - 1ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 23s - loss: nan - mse: nan - mae: nan - 23s/epoch - 1ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 24s - loss: nan - mse: nan - mae: nan - 24s/epoch - 1ms/step\n",
            "Score for fold 2: loss of nan\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 23s - loss: nan - mse: nan - mae: nan - 23s/epoch - 1ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 22s - loss: nan - mse: nan - mae: nan - 22s/epoch - 1ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 23s - loss: nan - mse: nan - mae: nan - 23s/epoch - 1ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 22s - loss: nan - mse: nan - mae: nan - 22s/epoch - 1ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 23s - loss: nan - mse: nan - mae: nan - 23s/epoch - 1ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 22s - loss: nan - mse: nan - mae: nan - 22s/epoch - 1ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 23s - loss: nan - mse: nan - mae: nan - 23s/epoch - 1ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 23s - loss: nan - mse: nan - mae: nan - 23s/epoch - 1ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 22s - loss: nan - mse: nan - mae: nan - 22s/epoch - 1ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 22s - loss: nan - mse: nan - mae: nan - 22s/epoch - 1ms/step\n",
            "Score for fold 3: loss of nan\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[33m[W 2022-10-05 05:11:34,776]\u001b[0m Setting status of trial#21 as TrialState.FAIL because the objective function returned nan.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 18s - loss: 16.9554 - mse: 16.9554 - mae: 1.5714 - 18s/epoch - 907us/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 18s - loss: 16.7004 - mse: 16.7004 - mae: 1.5548 - 18s/epoch - 921us/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 18s - loss: 16.9530 - mse: 16.9530 - mae: 1.5699 - 18s/epoch - 898us/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 18s - loss: 17.4208 - mse: 17.4208 - mae: 1.6006 - 18s/epoch - 912us/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 18s - loss: 16.8868 - mse: 16.8868 - mae: 1.5634 - 18s/epoch - 886us/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 18s - loss: 16.6032 - mse: 16.6032 - mae: 1.5530 - 18s/epoch - 918us/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 19s - loss: 16.3549 - mse: 16.3549 - mae: 1.5413 - 19s/epoch - 935us/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 18s - loss: 16.7102 - mse: 16.7102 - mae: 1.5621 - 18s/epoch - 898us/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 18s - loss: 16.6474 - mse: 16.6474 - mae: 1.5601 - 18s/epoch - 916us/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 19s - loss: 17.1582 - mse: 17.1582 - mae: 1.5872 - 19s/epoch - 931us/step\n",
            "Score for fold 1: loss of 24.361263275146484\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 19s - loss: 16.7006 - mse: 16.7006 - mae: 1.5444 - 19s/epoch - 942us/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 19s - loss: 16.5683 - mse: 16.5683 - mae: 1.5555 - 19s/epoch - 946us/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 20s - loss: 16.5095 - mse: 16.5095 - mae: 1.5502 - 20s/epoch - 987us/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 20s - loss: 16.3387 - mse: 16.3387 - mae: 1.5418 - 20s/epoch - 988us/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 19s - loss: 17.2717 - mse: 17.2717 - mae: 1.5831 - 19s/epoch - 962us/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 19s - loss: 17.7525 - mse: 17.7525 - mae: 1.5619 - 19s/epoch - 942us/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 19s - loss: 17.6658 - mse: 17.6658 - mae: 1.6153 - 19s/epoch - 960us/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 19s - loss: 16.5195 - mse: 16.5195 - mae: 1.5557 - 19s/epoch - 946us/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 19s - loss: 16.1586 - mse: 16.1586 - mae: 1.5435 - 19s/epoch - 941us/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 19s - loss: 16.7894 - mse: 16.7894 - mae: 1.5673 - 19s/epoch - 950us/step\n",
            "Score for fold 2: loss of 11.991439819335938\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 19s - loss: 16.0401 - mse: 16.0401 - mae: 1.5509 - 19s/epoch - 958us/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 19s - loss: 16.0245 - mse: 16.0245 - mae: 1.5283 - 19s/epoch - 956us/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 19s - loss: 16.1784 - mse: 16.1784 - mae: 1.5339 - 19s/epoch - 961us/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 19s - loss: 16.3990 - mse: 16.3990 - mae: 1.5561 - 19s/epoch - 946us/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 19s - loss: 16.4059 - mse: 16.4059 - mae: 1.5447 - 19s/epoch - 948us/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 19s - loss: 16.2824 - mse: 16.2824 - mae: 1.5350 - 19s/epoch - 934us/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 18s - loss: 16.1808 - mse: 16.1808 - mae: 1.5469 - 18s/epoch - 907us/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 18s - loss: 17.7465 - mse: 17.7465 - mae: 1.6167 - 18s/epoch - 915us/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 19s - loss: 16.3926 - mse: 16.3926 - mae: 1.5387 - 19s/epoch - 925us/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 19s - loss: 15.9720 - mse: 15.9720 - mae: 1.5457 - 19s/epoch - 927us/step\n",
            "Score for fold 3: loss of 12.174610137939453\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-10-05 05:21:04,759]\u001b[0m Finished trial#22 resulted in value: 15.972029685974121. Current best value is 13.002501487731934 with parameters: {'activation': 'relu', 'optimizer': 'adam', 'num_hidden_layer': 3, 'num_hidden_unit': 355, 'learning_rate': 0.00142911022839336}.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 101s - loss: 18.9366 - mse: 18.9366 - mae: 1.7107 - 101s/epoch - 5ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 107s - loss: 19.0720 - mse: 19.0720 - mae: 1.6686 - 107s/epoch - 5ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 107s - loss: 18.1869 - mse: 18.1869 - mae: 1.6639 - 107s/epoch - 5ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 104s - loss: 19.3677 - mse: 19.3677 - mae: 1.6526 - 104s/epoch - 5ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 99s - loss: 17.2155 - mse: 17.2155 - mae: 1.6442 - 99s/epoch - 5ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 101s - loss: 17.1338 - mse: 17.1338 - mae: 1.6294 - 101s/epoch - 5ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 96s - loss: 16.9588 - mse: 16.9588 - mae: 1.6271 - 96s/epoch - 5ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 81s - loss: 16.8322 - mse: 16.8322 - mae: 1.6198 - 81s/epoch - 4ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 80s - loss: 16.7124 - mse: 16.7124 - mae: 1.6149 - 80s/epoch - 4ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 80s - loss: 18.1044 - mse: 18.1044 - mae: 1.6447 - 80s/epoch - 4ms/step\n",
            "Score for fold 1: loss of 23.38610076904297\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 79s - loss: 16.8983 - mse: 16.8983 - mae: 1.6295 - 79s/epoch - 4ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 80s - loss: 16.7911 - mse: 16.7911 - mae: 1.6249 - 80s/epoch - 4ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 80s - loss: 16.7910 - mse: 16.7910 - mae: 1.6221 - 80s/epoch - 4ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 79s - loss: 17.0595 - mse: 17.0595 - mae: 1.6229 - 79s/epoch - 4ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 80s - loss: 16.7034 - mse: 16.7034 - mae: 1.6236 - 80s/epoch - 4ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 80s - loss: 16.7268 - mse: 16.7268 - mae: 1.6228 - 80s/epoch - 4ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 81s - loss: 18.2198 - mse: 18.2198 - mae: 1.6193 - 81s/epoch - 4ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 82s - loss: 17.5005 - mse: 17.5005 - mae: 1.6350 - 82s/epoch - 4ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 84s - loss: 26.7429 - mse: 26.7429 - mae: 1.7393 - 84s/epoch - 4ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 82s - loss: 16.7201 - mse: 16.7201 - mae: 1.6467 - 82s/epoch - 4ms/step\n",
            "Score for fold 2: loss of 12.286290168762207\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 82s - loss: 16.5357 - mse: 16.5357 - mae: 1.6290 - 82s/epoch - 4ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 84s - loss: 17.7170 - mse: 17.7170 - mae: 1.6431 - 84s/epoch - 4ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 85s - loss: 17.7962 - mse: 17.7962 - mae: 1.6207 - 85s/epoch - 4ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 104s - loss: 16.4758 - mse: 16.4758 - mae: 1.6264 - 104s/epoch - 5ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 114s - loss: 16.4375 - mse: 16.4375 - mae: 1.6263 - 114s/epoch - 6ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 110s - loss: 23.4071 - mse: 23.4071 - mae: 1.6696 - 110s/epoch - 5ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 113s - loss: 16.4353 - mse: 16.4353 - mae: 1.6295 - 113s/epoch - 6ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 106s - loss: 16.3333 - mse: 16.3333 - mae: 1.6196 - 106s/epoch - 5ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 104s - loss: 16.3985 - mse: 16.3985 - mae: 1.6240 - 104s/epoch - 5ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 98s - loss: 20.6452 - mse: 20.6452 - mae: 1.6353 - 98s/epoch - 5ms/step\n",
            "Score for fold 3: loss of 12.90695858001709\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-10-05 06:07:25,780]\u001b[0m Finished trial#23 resulted in value: 20.645156860351562. Current best value is 13.002501487731934 with parameters: {'activation': 'relu', 'optimizer': 'adam', 'num_hidden_layer': 3, 'num_hidden_unit': 355, 'learning_rate': 0.00142911022839336}.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 41s - loss: 16.3277 - mse: 16.3277 - mae: 1.5278 - 41s/epoch - 2ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 41s - loss: 15.4368 - mse: 15.4368 - mae: 1.4907 - 41s/epoch - 2ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 47s - loss: 15.1590 - mse: 15.1590 - mae: 1.4767 - 47s/epoch - 2ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 47s - loss: 15.0105 - mse: 15.0105 - mae: 1.4681 - 47s/epoch - 2ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 47s - loss: 14.8638 - mse: 14.8638 - mae: 1.4618 - 47s/epoch - 2ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 49s - loss: 14.7825 - mse: 14.7825 - mae: 1.4575 - 49s/epoch - 2ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 47s - loss: 14.7037 - mse: 14.7037 - mae: 1.4540 - 47s/epoch - 2ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 47s - loss: 14.5003 - mse: 14.5003 - mae: 1.4498 - 47s/epoch - 2ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 46s - loss: 14.4599 - mse: 14.4599 - mae: 1.4477 - 46s/epoch - 2ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 46s - loss: 14.3579 - mse: 14.3579 - mae: 1.4440 - 46s/epoch - 2ms/step\n",
            "Score for fold 1: loss of 24.58268928527832\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 60s - loss: 14.2913 - mse: 14.2913 - mae: 1.4418 - 60s/epoch - 3ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 68s - loss: 14.2207 - mse: 14.2207 - mae: 1.4403 - 68s/epoch - 3ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 67s - loss: 14.0824 - mse: 14.0824 - mae: 1.4378 - 67s/epoch - 3ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 68s - loss: 14.0583 - mse: 14.0583 - mae: 1.4349 - 68s/epoch - 3ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 64s - loss: 13.9006 - mse: 13.9006 - mae: 1.4328 - 64s/epoch - 3ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 65s - loss: 13.7468 - mse: 13.7468 - mae: 1.4316 - 65s/epoch - 3ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 67s - loss: 13.6901 - mse: 13.6901 - mae: 1.4310 - 67s/epoch - 3ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 69s - loss: 13.6669 - mse: 13.6669 - mae: 1.4276 - 69s/epoch - 3ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 67s - loss: 13.4119 - mse: 13.4119 - mae: 1.4283 - 67s/epoch - 3ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 67s - loss: 13.4603 - mse: 13.4603 - mae: 1.4243 - 67s/epoch - 3ms/step\n",
            "Score for fold 2: loss of 10.330498695373535\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 55s - loss: 13.0936 - mse: 13.0936 - mae: 1.4228 - 55s/epoch - 3ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 51s - loss: 13.1737 - mse: 13.1737 - mae: 1.4214 - 51s/epoch - 3ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 50s - loss: 13.0257 - mse: 13.0257 - mae: 1.4190 - 50s/epoch - 3ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 48s - loss: 12.7672 - mse: 12.7672 - mae: 1.4185 - 48s/epoch - 2ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 48s - loss: 12.7176 - mse: 12.7176 - mae: 1.4168 - 48s/epoch - 2ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 49s - loss: 12.5087 - mse: 12.5087 - mae: 1.4151 - 49s/epoch - 2ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 50s - loss: 12.4420 - mse: 12.4420 - mae: 1.4145 - 50s/epoch - 3ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 51s - loss: 12.2000 - mse: 12.2000 - mae: 1.4136 - 51s/epoch - 3ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 50s - loss: 12.0586 - mse: 12.0586 - mae: 1.4121 - 50s/epoch - 2ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 47s - loss: 11.7852 - mse: 11.7852 - mae: 1.4120 - 47s/epoch - 2ms/step\n",
            "Score for fold 3: loss of 10.553601264953613\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-10-05 06:34:37,426]\u001b[0m Finished trial#24 resulted in value: 11.785216331481934. Current best value is 11.785216331481934 with parameters: {'activation': 'relu', 'optimizer': 'adam', 'num_hidden_layer': 3, 'num_hidden_unit': 472, 'learning_rate': 0.00025103018239764653}.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 42s - loss: 16.2981 - mse: 16.2981 - mae: 1.5331 - 42s/epoch - 2ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 43s - loss: 15.7581 - mse: 15.7581 - mae: 1.5024 - 43s/epoch - 2ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 41s - loss: 15.4578 - mse: 15.4578 - mae: 1.4908 - 41s/epoch - 2ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 44s - loss: 15.1759 - mse: 15.1759 - mae: 1.4809 - 44s/epoch - 2ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 45s - loss: 15.0376 - mse: 15.0376 - mae: 1.4749 - 45s/epoch - 2ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 45s - loss: 14.9236 - mse: 14.9236 - mae: 1.4718 - 45s/epoch - 2ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 43s - loss: 14.7935 - mse: 14.7935 - mae: 1.4656 - 43s/epoch - 2ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 44s - loss: 14.7068 - mse: 14.7068 - mae: 1.4628 - 44s/epoch - 2ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 44s - loss: 14.6642 - mse: 14.6642 - mae: 1.4605 - 44s/epoch - 2ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 44s - loss: 14.5929 - mse: 14.5929 - mae: 1.4566 - 44s/epoch - 2ms/step\n",
            "Score for fold 1: loss of 24.524490356445312\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 41s - loss: 14.5010 - mse: 14.5010 - mae: 1.4552 - 41s/epoch - 2ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 41s - loss: 14.4574 - mse: 14.4574 - mae: 1.4526 - 41s/epoch - 2ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 39s - loss: 14.3768 - mse: 14.3768 - mae: 1.4511 - 39s/epoch - 2ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 41s - loss: 14.3646 - mse: 14.3646 - mae: 1.4488 - 41s/epoch - 2ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 41s - loss: 14.2596 - mse: 14.2596 - mae: 1.4463 - 41s/epoch - 2ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 42s - loss: 14.1924 - mse: 14.1924 - mae: 1.4444 - 42s/epoch - 2ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 40s - loss: 14.1203 - mse: 14.1203 - mae: 1.4429 - 40s/epoch - 2ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 41s - loss: 14.0810 - mse: 14.0810 - mae: 1.4421 - 41s/epoch - 2ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 42s - loss: 13.9781 - mse: 13.9781 - mae: 1.4397 - 42s/epoch - 2ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 41s - loss: 13.9212 - mse: 13.9212 - mae: 1.4366 - 41s/epoch - 2ms/step\n",
            "Score for fold 2: loss of 10.493861198425293\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 41s - loss: 13.8567 - mse: 13.8567 - mae: 1.4372 - 41s/epoch - 2ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 41s - loss: 13.7724 - mse: 13.7724 - mae: 1.4364 - 41s/epoch - 2ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 44s - loss: 13.7419 - mse: 13.7419 - mae: 1.4343 - 44s/epoch - 2ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 46s - loss: 13.5730 - mse: 13.5730 - mae: 1.4329 - 46s/epoch - 2ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 43s - loss: 13.5490 - mse: 13.5490 - mae: 1.4320 - 43s/epoch - 2ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 45s - loss: 13.5183 - mse: 13.5183 - mae: 1.4308 - 45s/epoch - 2ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 45s - loss: 13.4023 - mse: 13.4023 - mae: 1.4310 - 45s/epoch - 2ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 45s - loss: 13.3060 - mse: 13.3060 - mae: 1.4288 - 45s/epoch - 2ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 48s - loss: 13.2849 - mse: 13.2849 - mae: 1.4280 - 48s/epoch - 2ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 48s - loss: 13.1994 - mse: 13.1994 - mae: 1.4261 - 48s/epoch - 2ms/step\n",
            "Score for fold 3: loss of 10.389739036560059\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-10-05 06:56:16,942]\u001b[0m Finished trial#25 resulted in value: 13.199433326721191. Current best value is 11.785216331481934 with parameters: {'activation': 'relu', 'optimizer': 'adam', 'num_hidden_layer': 3, 'num_hidden_unit': 472, 'learning_rate': 0.00025103018239764653}.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 59s - loss: 16.3145 - mse: 16.3145 - mae: 1.5287 - 59s/epoch - 3ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 55s - loss: 15.5994 - mse: 15.5994 - mae: 1.4878 - 55s/epoch - 3ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 51s - loss: 15.2292 - mse: 15.2292 - mae: 1.4757 - 51s/epoch - 3ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 51s - loss: 15.0644 - mse: 15.0644 - mae: 1.4679 - 51s/epoch - 3ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 58s - loss: 14.9324 - mse: 14.9324 - mae: 1.4709 - 58s/epoch - 3ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 59s - loss: 14.8751 - mse: 14.8751 - mae: 1.4710 - 59s/epoch - 3ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 58s - loss: 14.7178 - mse: 14.7178 - mae: 1.4644 - 58s/epoch - 3ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 56s - loss: 14.6525 - mse: 14.6525 - mae: 1.4605 - 56s/epoch - 3ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 54s - loss: 14.5848 - mse: 14.5848 - mae: 1.4594 - 54s/epoch - 3ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 53s - loss: 14.5036 - mse: 14.5036 - mae: 1.4547 - 53s/epoch - 3ms/step\n",
            "Score for fold 1: loss of 23.96470069885254\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 54s - loss: 14.4236 - mse: 14.4236 - mae: 1.4521 - 54s/epoch - 3ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 51s - loss: 14.3031 - mse: 14.3031 - mae: 1.4489 - 51s/epoch - 3ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 55s - loss: 14.2166 - mse: 14.2166 - mae: 1.4441 - 55s/epoch - 3ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 57s - loss: 14.1775 - mse: 14.1775 - mae: 1.4421 - 57s/epoch - 3ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 57s - loss: 14.0526 - mse: 14.0525 - mae: 1.4375 - 57s/epoch - 3ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 56s - loss: 13.9507 - mse: 13.9507 - mae: 1.4377 - 56s/epoch - 3ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 54s - loss: 13.9583 - mse: 13.9583 - mae: 1.4339 - 54s/epoch - 3ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 58s - loss: 13.7880 - mse: 13.7880 - mae: 1.4313 - 58s/epoch - 3ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 58s - loss: 13.7261 - mse: 13.7261 - mae: 1.4297 - 58s/epoch - 3ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 61s - loss: 13.6197 - mse: 13.6197 - mae: 1.4294 - 61s/epoch - 3ms/step\n",
            "Score for fold 2: loss of 10.672884941101074\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 58s - loss: 13.7075 - mse: 13.7075 - mae: 1.4274 - 58s/epoch - 3ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 56s - loss: 13.4635 - mse: 13.4635 - mae: 1.4255 - 56s/epoch - 3ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 58s - loss: 13.2240 - mse: 13.2240 - mae: 1.4259 - 58s/epoch - 3ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 57s - loss: 13.2177 - mse: 13.2177 - mae: 1.4253 - 57s/epoch - 3ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 53s - loss: 13.1948 - mse: 13.1948 - mae: 1.4227 - 53s/epoch - 3ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 49s - loss: 12.8976 - mse: 12.8976 - mae: 1.4241 - 49s/epoch - 2ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 49s - loss: 12.7683 - mse: 12.7683 - mae: 1.4219 - 49s/epoch - 2ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 48s - loss: 12.8253 - mse: 12.8253 - mae: 1.4198 - 48s/epoch - 2ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 49s - loss: 12.4655 - mse: 12.4655 - mae: 1.4184 - 49s/epoch - 2ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 50s - loss: 12.2832 - mse: 12.2832 - mae: 1.4213 - 50s/epoch - 3ms/step\n",
            "Score for fold 3: loss of 10.247172355651855\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-10-05 07:23:52,838]\u001b[0m Finished trial#26 resulted in value: 12.283205032348633. Current best value is 11.785216331481934 with parameters: {'activation': 'relu', 'optimizer': 'adam', 'num_hidden_layer': 3, 'num_hidden_unit': 472, 'learning_rate': 0.00025103018239764653}.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 52s - loss: 16.6740 - mse: 16.6740 - mae: 1.5624 - 52s/epoch - 3ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 50s - loss: 16.0964 - mse: 16.0964 - mae: 1.5341 - 50s/epoch - 2ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 49s - loss: 15.8107 - mse: 15.8107 - mae: 1.5277 - 49s/epoch - 2ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 50s - loss: 15.5277 - mse: 15.5277 - mae: 1.5197 - 50s/epoch - 2ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 50s - loss: 15.5422 - mse: 15.5422 - mae: 1.5179 - 50s/epoch - 3ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 50s - loss: 15.4358 - mse: 15.4358 - mae: 1.5117 - 50s/epoch - 2ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 49s - loss: 15.3146 - mse: 15.3146 - mae: 1.5095 - 49s/epoch - 2ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 49s - loss: 15.3270 - mse: 15.3270 - mae: 1.5076 - 49s/epoch - 2ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 49s - loss: 15.2406 - mse: 15.2406 - mae: 1.5044 - 49s/epoch - 2ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 52s - loss: 15.2447 - mse: 15.2447 - mae: 1.5052 - 52s/epoch - 3ms/step\n",
            "Score for fold 1: loss of 24.80477523803711\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 54s - loss: 15.0110 - mse: 15.0110 - mae: 1.5023 - 54s/epoch - 3ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 52s - loss: 14.9870 - mse: 14.9870 - mae: 1.4985 - 52s/epoch - 3ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 51s - loss: 14.8976 - mse: 14.8976 - mae: 1.4981 - 51s/epoch - 3ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 51s - loss: 14.8922 - mse: 14.8922 - mae: 1.5003 - 51s/epoch - 3ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 52s - loss: 14.8553 - mse: 14.8553 - mae: 1.4984 - 52s/epoch - 3ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 52s - loss: 14.8679 - mse: 14.8679 - mae: 1.4972 - 52s/epoch - 3ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 53s - loss: 14.8418 - mse: 14.8418 - mae: 1.4955 - 53s/epoch - 3ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 53s - loss: 14.7808 - mse: 14.7808 - mae: 1.4953 - 53s/epoch - 3ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 55s - loss: 14.6598 - mse: 14.6598 - mae: 1.4949 - 55s/epoch - 3ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 52s - loss: 14.6924 - mse: 14.6924 - mae: 1.4967 - 52s/epoch - 3ms/step\n",
            "Score for fold 2: loss of 11.436291694641113\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 52s - loss: 14.6983 - mse: 14.6983 - mae: 1.4940 - 52s/epoch - 3ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 52s - loss: 14.4897 - mse: 14.4897 - mae: 1.4926 - 52s/epoch - 3ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 53s - loss: 14.4568 - mse: 14.4568 - mae: 1.4903 - 53s/epoch - 3ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 53s - loss: 14.6369 - mse: 14.6369 - mae: 1.4925 - 53s/epoch - 3ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 52s - loss: 14.3594 - mse: 14.3594 - mae: 1.4924 - 52s/epoch - 3ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 51s - loss: 14.4092 - mse: 14.4092 - mae: 1.4925 - 51s/epoch - 3ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 51s - loss: 14.3282 - mse: 14.3282 - mae: 1.5002 - 51s/epoch - 3ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 51s - loss: 14.2561 - mse: 14.2561 - mae: 1.4922 - 51s/epoch - 3ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 50s - loss: 14.1188 - mse: 14.1188 - mae: 1.4903 - 50s/epoch - 3ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 49s - loss: 14.1728 - mse: 14.1728 - mae: 1.4895 - 49s/epoch - 2ms/step\n",
            "Score for fold 3: loss of 11.23771858215332\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-10-05 07:49:46,133]\u001b[0m Finished trial#27 resulted in value: 14.172849655151367. Current best value is 11.785216331481934 with parameters: {'activation': 'relu', 'optimizer': 'adam', 'num_hidden_layer': 3, 'num_hidden_unit': 472, 'learning_rate': 0.00025103018239764653}.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 63s - loss: 16.5378 - mse: 16.5378 - mae: 1.5326 - 63s/epoch - 3ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 58s - loss: 15.4308 - mse: 15.4308 - mae: 1.4917 - 58s/epoch - 3ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 55s - loss: 15.2759 - mse: 15.2759 - mae: 1.4795 - 55s/epoch - 3ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 57s - loss: 15.1135 - mse: 15.1135 - mae: 1.4797 - 57s/epoch - 3ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 58s - loss: 14.9969 - mse: 14.9969 - mae: 1.4784 - 58s/epoch - 3ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 58s - loss: 14.8755 - mse: 14.8755 - mae: 1.4704 - 58s/epoch - 3ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 58s - loss: 14.7898 - mse: 14.7898 - mae: 1.4658 - 58s/epoch - 3ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 59s - loss: 14.7992 - mse: 14.7992 - mae: 1.4596 - 59s/epoch - 3ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 59s - loss: 14.5779 - mse: 14.5779 - mae: 1.4541 - 59s/epoch - 3ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 59s - loss: 14.5020 - mse: 14.5020 - mae: 1.4503 - 59s/epoch - 3ms/step\n",
            "Score for fold 1: loss of 23.071025848388672\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 58s - loss: 14.4014 - mse: 14.4014 - mae: 1.4474 - 58s/epoch - 3ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 56s - loss: 14.3051 - mse: 14.3051 - mae: 1.4448 - 56s/epoch - 3ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 56s - loss: 14.2527 - mse: 14.2527 - mae: 1.4450 - 56s/epoch - 3ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 60s - loss: 14.2448 - mse: 14.2448 - mae: 1.4418 - 60s/epoch - 3ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 61s - loss: 14.0786 - mse: 14.0786 - mae: 1.4420 - 61s/epoch - 3ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 57s - loss: 13.9183 - mse: 13.9183 - mae: 1.4397 - 57s/epoch - 3ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 58s - loss: 13.8238 - mse: 13.8238 - mae: 1.4363 - 58s/epoch - 3ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 57s - loss: 13.6219 - mse: 13.6219 - mae: 1.4361 - 57s/epoch - 3ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 56s - loss: 13.7878 - mse: 13.7878 - mae: 1.4348 - 56s/epoch - 3ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 57s - loss: 13.4125 - mse: 13.4125 - mae: 1.4349 - 57s/epoch - 3ms/step\n",
            "Score for fold 2: loss of 10.631502151489258\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 58s - loss: 13.4812 - mse: 13.4812 - mae: 1.4333 - 58s/epoch - 3ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 64s - loss: 13.1851 - mse: 13.1851 - mae: 1.4323 - 64s/epoch - 3ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 60s - loss: 13.1355 - mse: 13.1355 - mae: 1.4333 - 60s/epoch - 3ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 62s - loss: 13.0028 - mse: 13.0028 - mae: 1.4314 - 62s/epoch - 3ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 62s - loss: 12.6798 - mse: 12.6798 - mae: 1.4301 - 62s/epoch - 3ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 61s - loss: 12.9643 - mse: 12.9643 - mae: 1.4294 - 61s/epoch - 3ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 61s - loss: 12.5397 - mse: 12.5397 - mae: 1.4296 - 61s/epoch - 3ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 59s - loss: 12.3611 - mse: 12.3611 - mae: 1.4291 - 59s/epoch - 3ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 57s - loss: 12.3901 - mse: 12.3901 - mae: 1.4276 - 57s/epoch - 3ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 58s - loss: 12.0839 - mse: 12.0839 - mae: 1.4253 - 58s/epoch - 3ms/step\n",
            "Score for fold 3: loss of 10.44257640838623\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-10-05 08:19:21,916]\u001b[0m Finished trial#28 resulted in value: 12.083877563476562. Current best value is 11.785216331481934 with parameters: {'activation': 'relu', 'optimizer': 'adam', 'num_hidden_layer': 3, 'num_hidden_unit': 472, 'learning_rate': 0.00025103018239764653}.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 27s - loss: 16.3338 - mse: 16.3338 - mae: 1.5332 - 27s/epoch - 1ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 28s - loss: 15.7147 - mse: 15.7147 - mae: 1.5079 - 28s/epoch - 1ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 28s - loss: 15.4956 - mse: 15.4956 - mae: 1.5019 - 28s/epoch - 1ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 28s - loss: 15.2652 - mse: 15.2652 - mae: 1.4975 - 28s/epoch - 1ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 27s - loss: 15.2361 - mse: 15.2361 - mae: 1.4935 - 27s/epoch - 1ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 28s - loss: 15.1039 - mse: 15.1039 - mae: 1.4889 - 28s/epoch - 1ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 27s - loss: 15.0538 - mse: 15.0538 - mae: 1.4861 - 27s/epoch - 1ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 27s - loss: 15.0041 - mse: 15.0041 - mae: 1.4834 - 27s/epoch - 1ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 27s - loss: 14.9559 - mse: 14.9559 - mae: 1.4795 - 27s/epoch - 1ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 28s - loss: 14.8668 - mse: 14.8668 - mae: 1.4801 - 28s/epoch - 1ms/step\n",
            "Score for fold 1: loss of 24.462032318115234\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 28s - loss: 14.8307 - mse: 14.8307 - mae: 1.4765 - 28s/epoch - 1ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 28s - loss: 14.7980 - mse: 14.7980 - mae: 1.4763 - 28s/epoch - 1ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 28s - loss: 14.8634 - mse: 14.8634 - mae: 1.4752 - 28s/epoch - 1ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 28s - loss: 14.6808 - mse: 14.6808 - mae: 1.4737 - 28s/epoch - 1ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 28s - loss: 14.7072 - mse: 14.7072 - mae: 1.4751 - 28s/epoch - 1ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 28s - loss: 14.6749 - mse: 14.6749 - mae: 1.4723 - 28s/epoch - 1ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 28s - loss: 14.6062 - mse: 14.6062 - mae: 1.4708 - 28s/epoch - 1ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 28s - loss: 14.5527 - mse: 14.5527 - mae: 1.4692 - 28s/epoch - 1ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 29s - loss: 14.5695 - mse: 14.5695 - mae: 1.4702 - 29s/epoch - 1ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 28s - loss: 14.5639 - mse: 14.5639 - mae: 1.4706 - 28s/epoch - 1ms/step\n",
            "Score for fold 2: loss of 10.623114585876465\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 28s - loss: 14.4859 - mse: 14.4859 - mae: 1.4711 - 28s/epoch - 1ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 29s - loss: 14.4240 - mse: 14.4240 - mae: 1.4667 - 29s/epoch - 1ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 29s - loss: 14.4740 - mse: 14.4740 - mae: 1.4666 - 29s/epoch - 1ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 28s - loss: 14.4246 - mse: 14.4246 - mae: 1.4687 - 28s/epoch - 1ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 28s - loss: 14.3694 - mse: 14.3694 - mae: 1.4658 - 28s/epoch - 1ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 28s - loss: 14.3474 - mse: 14.3474 - mae: 1.4654 - 28s/epoch - 1ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 28s - loss: 14.3834 - mse: 14.3834 - mae: 1.4674 - 28s/epoch - 1ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 28s - loss: 14.3225 - mse: 14.3225 - mae: 1.4665 - 28s/epoch - 1ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 28s - loss: 14.2909 - mse: 14.2909 - mae: 1.4640 - 28s/epoch - 1ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 28s - loss: 14.2947 - mse: 14.2947 - mae: 1.4641 - 28s/epoch - 1ms/step\n",
            "Score for fold 3: loss of 10.604828834533691\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-10-05 08:33:30,627]\u001b[0m Finished trial#29 resulted in value: 14.294655799865723. Current best value is 11.785216331481934 with parameters: {'activation': 'relu', 'optimizer': 'adam', 'num_hidden_layer': 3, 'num_hidden_unit': 472, 'learning_rate': 0.00025103018239764653}.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 59s - loss: 16.2851 - mse: 16.2851 - mae: 1.5275 - 59s/epoch - 3ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 60s - loss: 15.5803 - mse: 15.5803 - mae: 1.4915 - 60s/epoch - 3ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 58s - loss: 15.3175 - mse: 15.3175 - mae: 1.4813 - 58s/epoch - 3ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 64s - loss: 15.0002 - mse: 15.0002 - mae: 1.4694 - 64s/epoch - 3ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 67s - loss: 14.9744 - mse: 14.9744 - mae: 1.4619 - 67s/epoch - 3ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 63s - loss: 14.8599 - mse: 14.8599 - mae: 1.4609 - 63s/epoch - 3ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 60s - loss: 14.7641 - mse: 14.7641 - mae: 1.4635 - 60s/epoch - 3ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 62s - loss: 14.6639 - mse: 14.6639 - mae: 1.4603 - 62s/epoch - 3ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 61s - loss: 14.5287 - mse: 14.5287 - mae: 1.4571 - 61s/epoch - 3ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 58s - loss: 14.4513 - mse: 14.4513 - mae: 1.4521 - 58s/epoch - 3ms/step\n",
            "Score for fold 1: loss of 24.41258430480957\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 57s - loss: 14.3328 - mse: 14.3328 - mae: 1.4516 - 57s/epoch - 3ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 61s - loss: 14.2928 - mse: 14.2928 - mae: 1.4498 - 61s/epoch - 3ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 59s - loss: 14.2710 - mse: 14.2710 - mae: 1.4473 - 59s/epoch - 3ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 61s - loss: 14.1081 - mse: 14.1081 - mae: 1.4436 - 61s/epoch - 3ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 58s - loss: 14.1270 - mse: 14.1270 - mae: 1.4407 - 58s/epoch - 3ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 57s - loss: 13.9486 - mse: 13.9486 - mae: 1.4384 - 57s/epoch - 3ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 56s - loss: 13.8771 - mse: 13.8771 - mae: 1.4357 - 56s/epoch - 3ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 59s - loss: 13.7689 - mse: 13.7689 - mae: 1.4325 - 59s/epoch - 3ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 59s - loss: 13.7444 - mse: 13.7444 - mae: 1.4310 - 59s/epoch - 3ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 58s - loss: 13.4774 - mse: 13.4774 - mae: 1.4280 - 58s/epoch - 3ms/step\n",
            "Score for fold 2: loss of 10.43887996673584\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 60s - loss: 13.2957 - mse: 13.2957 - mae: 1.4272 - 60s/epoch - 3ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 59s - loss: 13.3527 - mse: 13.3527 - mae: 1.4269 - 59s/epoch - 3ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 57s - loss: 13.2933 - mse: 13.2933 - mae: 1.4237 - 57s/epoch - 3ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 58s - loss: 12.9431 - mse: 12.9431 - mae: 1.4234 - 58s/epoch - 3ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 57s - loss: 13.1710 - mse: 13.1710 - mae: 1.4225 - 57s/epoch - 3ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 56s - loss: 12.7814 - mse: 12.7814 - mae: 1.4207 - 56s/epoch - 3ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 57s - loss: 12.6063 - mse: 12.6063 - mae: 1.4194 - 57s/epoch - 3ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 56s - loss: 12.5492 - mse: 12.5492 - mae: 1.4181 - 56s/epoch - 3ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 57s - loss: 12.4199 - mse: 12.4199 - mae: 1.4176 - 57s/epoch - 3ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 57s - loss: 12.3171 - mse: 12.3171 - mae: 1.4195 - 57s/epoch - 3ms/step\n",
            "Score for fold 3: loss of 10.571925163269043\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-10-05 09:03:15,908]\u001b[0m Finished trial#30 resulted in value: 12.317089080810547. Current best value is 11.785216331481934 with parameters: {'activation': 'relu', 'optimizer': 'adam', 'num_hidden_layer': 3, 'num_hidden_unit': 472, 'learning_rate': 0.00025103018239764653}.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 68s - loss: 17.2187 - mse: 17.2187 - mae: 1.6091 - 68s/epoch - 3ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 70s - loss: 17.0818 - mse: 17.0818 - mae: 1.6025 - 70s/epoch - 3ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 68s - loss: 17.0281 - mse: 17.0281 - mae: 1.6019 - 68s/epoch - 3ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 68s - loss: 17.0146 - mse: 17.0146 - mae: 1.5980 - 68s/epoch - 3ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 68s - loss: 17.0084 - mse: 17.0084 - mae: 1.6051 - 68s/epoch - 3ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 68s - loss: 16.9996 - mse: 16.9996 - mae: 1.5980 - 68s/epoch - 3ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 70s - loss: 17.0005 - mse: 17.0005 - mae: 1.6010 - 70s/epoch - 3ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 74s - loss: 16.9976 - mse: 16.9976 - mae: 1.5995 - 74s/epoch - 4ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 74s - loss: 16.9815 - mse: 16.9815 - mae: 1.6028 - 74s/epoch - 4ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 73s - loss: 16.9930 - mse: 16.9930 - mae: 1.5995 - 73s/epoch - 4ms/step\n",
            "Score for fold 1: loss of 24.050987243652344\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 73s - loss: 16.9868 - mse: 16.9868 - mae: 1.6008 - 73s/epoch - 4ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 74s - loss: 16.9781 - mse: 16.9781 - mae: 1.6001 - 74s/epoch - 4ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 74s - loss: 16.9760 - mse: 16.9760 - mae: 1.6011 - 74s/epoch - 4ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 74s - loss: 16.9741 - mse: 16.9741 - mae: 1.6007 - 74s/epoch - 4ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 77s - loss: 16.9635 - mse: 16.9635 - mae: 1.6012 - 77s/epoch - 4ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 76s - loss: 16.9699 - mse: 16.9699 - mae: 1.5966 - 76s/epoch - 4ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 74s - loss: 16.9740 - mse: 16.9740 - mae: 1.6004 - 74s/epoch - 4ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 77s - loss: 16.9683 - mse: 16.9683 - mae: 1.6016 - 77s/epoch - 4ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 78s - loss: 16.9697 - mse: 16.9697 - mae: 1.5985 - 78s/epoch - 4ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 76s - loss: 16.9623 - mse: 16.9623 - mae: 1.6013 - 76s/epoch - 4ms/step\n",
            "Score for fold 2: loss of 12.317099571228027\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 79s - loss: 16.9650 - mse: 16.9650 - mae: 1.5987 - 79s/epoch - 4ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 76s - loss: 16.9638 - mse: 16.9638 - mae: 1.6005 - 76s/epoch - 4ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 72s - loss: 16.9418 - mse: 16.9418 - mae: 1.6003 - 72s/epoch - 4ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 73s - loss: 16.9793 - mse: 16.9793 - mae: 1.5997 - 73s/epoch - 4ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 74s - loss: 16.9678 - mse: 16.9678 - mae: 1.6008 - 74s/epoch - 4ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 76s - loss: 16.9686 - mse: 16.9686 - mae: 1.6011 - 76s/epoch - 4ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 73s - loss: 16.9670 - mse: 16.9670 - mae: 1.5990 - 73s/epoch - 4ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 77s - loss: 16.9680 - mse: 16.9680 - mae: 1.6012 - 77s/epoch - 4ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 81s - loss: 16.9627 - mse: 16.9627 - mae: 1.5988 - 81s/epoch - 4ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 87s - loss: 16.9643 - mse: 16.9643 - mae: 1.6009 - 87s/epoch - 4ms/step\n",
            "Score for fold 3: loss of 12.340326309204102\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-10-05 09:40:35,236]\u001b[0m Finished trial#31 resulted in value: 16.964250564575195. Current best value is 11.785216331481934 with parameters: {'activation': 'relu', 'optimizer': 'adam', 'num_hidden_layer': 3, 'num_hidden_unit': 472, 'learning_rate': 0.00025103018239764653}.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 21s - loss: 17.7096 - mse: 17.7096 - mae: 1.5959 - 21s/epoch - 1ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 21s - loss: 16.9015 - mse: 16.9015 - mae: 1.5738 - 21s/epoch - 1ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 22s - loss: 16.5634 - mse: 16.5634 - mae: 1.5559 - 22s/epoch - 1ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 22s - loss: 16.4057 - mse: 16.4057 - mae: 1.5485 - 22s/epoch - 1ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 21s - loss: 16.3255 - mse: 16.3255 - mae: 1.5388 - 21s/epoch - 1ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 21s - loss: 16.2306 - mse: 16.2306 - mae: 1.5295 - 21s/epoch - 1ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 21s - loss: 16.2538 - mse: 16.2538 - mae: 1.5243 - 21s/epoch - 1ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 21s - loss: 16.0149 - mse: 16.0149 - mae: 1.5179 - 21s/epoch - 1ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 21s - loss: 16.0612 - mse: 16.0612 - mae: 1.5147 - 21s/epoch - 1ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 21s - loss: 16.0445 - mse: 16.0445 - mae: 1.5105 - 21s/epoch - 1ms/step\n",
            "Score for fold 1: loss of 22.444887161254883\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 21s - loss: 16.0117 - mse: 16.0117 - mae: 1.5094 - 21s/epoch - 1ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 21s - loss: 15.8581 - mse: 15.8581 - mae: 1.5048 - 21s/epoch - 1ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 21s - loss: 15.8760 - mse: 15.8760 - mae: 1.5019 - 21s/epoch - 1ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 21s - loss: 15.8468 - mse: 15.8468 - mae: 1.5002 - 21s/epoch - 1ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 21s - loss: 15.7854 - mse: 15.7854 - mae: 1.4983 - 21s/epoch - 1ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 21s - loss: 15.7524 - mse: 15.7524 - mae: 1.4967 - 21s/epoch - 1ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 21s - loss: 15.7357 - mse: 15.7357 - mae: 1.4951 - 21s/epoch - 1ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 21s - loss: 15.8304 - mse: 15.8304 - mae: 1.4945 - 21s/epoch - 1ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 21s - loss: 15.5810 - mse: 15.5810 - mae: 1.4923 - 21s/epoch - 1ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 21s - loss: 15.6069 - mse: 15.6069 - mae: 1.4920 - 21s/epoch - 1ms/step\n",
            "Score for fold 2: loss of 11.360392570495605\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 21s - loss: 15.5739 - mse: 15.5739 - mae: 1.4897 - 21s/epoch - 1ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 21s - loss: 15.6299 - mse: 15.6299 - mae: 1.4913 - 21s/epoch - 1ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 21s - loss: 15.6745 - mse: 15.6745 - mae: 1.4888 - 21s/epoch - 1ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 21s - loss: 15.5289 - mse: 15.5289 - mae: 1.4882 - 21s/epoch - 1ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 21s - loss: 15.5278 - mse: 15.5278 - mae: 1.4876 - 21s/epoch - 1ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 22s - loss: 15.5297 - mse: 15.5297 - mae: 1.4879 - 22s/epoch - 1ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 22s - loss: 15.4753 - mse: 15.4753 - mae: 1.4854 - 22s/epoch - 1ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 22s - loss: 15.5288 - mse: 15.5288 - mae: 1.4866 - 22s/epoch - 1ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 22s - loss: 15.4918 - mse: 15.4918 - mae: 1.4849 - 22s/epoch - 1ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 21s - loss: 15.4778 - mse: 15.4778 - mae: 1.4848 - 21s/epoch - 1ms/step\n",
            "Score for fold 3: loss of 11.209601402282715\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-10-05 09:51:25,413]\u001b[0m Finished trial#32 resulted in value: 15.477829933166504. Current best value is 11.785216331481934 with parameters: {'activation': 'relu', 'optimizer': 'adam', 'num_hidden_layer': 3, 'num_hidden_unit': 472, 'learning_rate': 0.00025103018239764653}.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 25s - loss: 18.0971 - mse: 18.0971 - mae: 1.5918 - 25s/epoch - 1ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 25s - loss: 16.9012 - mse: 16.9012 - mae: 1.5286 - 25s/epoch - 1ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 24s - loss: 16.6847 - mse: 16.6847 - mae: 1.5119 - 24s/epoch - 1ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 25s - loss: 16.5211 - mse: 16.5211 - mae: 1.5033 - 25s/epoch - 1ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 24s - loss: 16.3264 - mse: 16.3264 - mae: 1.4985 - 24s/epoch - 1ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 25s - loss: 16.2360 - mse: 16.2360 - mae: 1.4951 - 25s/epoch - 1ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 24s - loss: 16.1463 - mse: 16.1463 - mae: 1.4916 - 24s/epoch - 1ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 24s - loss: 16.0596 - mse: 16.0596 - mae: 1.4885 - 24s/epoch - 1ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 25s - loss: 16.0059 - mse: 16.0059 - mae: 1.4862 - 25s/epoch - 1ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 25s - loss: 15.9539 - mse: 15.9539 - mae: 1.4838 - 25s/epoch - 1ms/step\n",
            "Score for fold 1: loss of 23.569194793701172\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 24s - loss: 15.9183 - mse: 15.9183 - mae: 1.4818 - 24s/epoch - 1ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 25s - loss: 15.8750 - mse: 15.8750 - mae: 1.4798 - 25s/epoch - 1ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 24s - loss: 15.8483 - mse: 15.8483 - mae: 1.4787 - 24s/epoch - 1ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 24s - loss: 15.8309 - mse: 15.8309 - mae: 1.4771 - 24s/epoch - 1ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 24s - loss: 15.8071 - mse: 15.8071 - mae: 1.4757 - 24s/epoch - 1ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 25s - loss: 15.7918 - mse: 15.7918 - mae: 1.4753 - 25s/epoch - 1ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 25s - loss: 15.7604 - mse: 15.7604 - mae: 1.4737 - 25s/epoch - 1ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 24s - loss: 15.7440 - mse: 15.7440 - mae: 1.4735 - 24s/epoch - 1ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 24s - loss: 15.7391 - mse: 15.7391 - mae: 1.4718 - 24s/epoch - 1ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 24s - loss: 15.7144 - mse: 15.7144 - mae: 1.4712 - 24s/epoch - 1ms/step\n",
            "Score for fold 2: loss of 11.354543685913086\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 24s - loss: 15.7133 - mse: 15.7133 - mae: 1.4708 - 24s/epoch - 1ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 25s - loss: 15.7038 - mse: 15.7038 - mae: 1.4700 - 25s/epoch - 1ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 25s - loss: 15.6993 - mse: 15.6993 - mae: 1.4692 - 25s/epoch - 1ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 26s - loss: 15.6831 - mse: 15.6831 - mae: 1.4689 - 26s/epoch - 1ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 26s - loss: 15.6916 - mse: 15.6916 - mae: 1.4683 - 26s/epoch - 1ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 26s - loss: 15.6899 - mse: 15.6899 - mae: 1.4684 - 26s/epoch - 1ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 25s - loss: 15.6921 - mse: 15.6921 - mae: 1.4675 - 25s/epoch - 1ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 25s - loss: 15.6994 - mse: 15.6994 - mae: 1.4675 - 25s/epoch - 1ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 25s - loss: 15.6857 - mse: 15.6857 - mae: 1.4667 - 25s/epoch - 1ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 25s - loss: 15.6911 - mse: 15.6911 - mae: 1.4668 - 25s/epoch - 1ms/step\n",
            "Score for fold 3: loss of 11.185789108276367\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-10-05 10:03:54,010]\u001b[0m Finished trial#33 resulted in value: 15.691132545471191. Current best value is 11.785216331481934 with parameters: {'activation': 'relu', 'optimizer': 'adam', 'num_hidden_layer': 3, 'num_hidden_unit': 472, 'learning_rate': 0.00025103018239764653}.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 52s - loss: 16.3622 - mse: 16.3622 - mae: 1.5271 - 52s/epoch - 3ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 56s - loss: 15.5761 - mse: 15.5761 - mae: 1.4919 - 56s/epoch - 3ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 57s - loss: 15.2039 - mse: 15.2039 - mae: 1.4780 - 57s/epoch - 3ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 54s - loss: 15.1576 - mse: 15.1576 - mae: 1.4690 - 54s/epoch - 3ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 53s - loss: 14.9324 - mse: 14.9324 - mae: 1.4635 - 53s/epoch - 3ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 54s - loss: 14.8077 - mse: 14.8077 - mae: 1.4600 - 54s/epoch - 3ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 54s - loss: 14.7052 - mse: 14.7052 - mae: 1.4558 - 54s/epoch - 3ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 56s - loss: 14.6142 - mse: 14.6142 - mae: 1.4532 - 56s/epoch - 3ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 58s - loss: 14.5242 - mse: 14.5242 - mae: 1.4484 - 58s/epoch - 3ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 56s - loss: 14.4408 - mse: 14.4408 - mae: 1.4451 - 56s/epoch - 3ms/step\n",
            "Score for fold 1: loss of 22.593591690063477\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 55s - loss: 14.3220 - mse: 14.3220 - mae: 1.4444 - 55s/epoch - 3ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 53s - loss: 14.3381 - mse: 14.3381 - mae: 1.4447 - 53s/epoch - 3ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 53s - loss: 14.1952 - mse: 14.1952 - mae: 1.4413 - 53s/epoch - 3ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 56s - loss: 14.0743 - mse: 14.0743 - mae: 1.4406 - 56s/epoch - 3ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 52s - loss: 14.0533 - mse: 14.0533 - mae: 1.4392 - 52s/epoch - 3ms/step\n",
            "Epoch 6/10\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "num_folds = 3\n",
        "\n",
        "optimizer = Adam(learning_rate=0.0003135813689247155)\n",
        "\n",
        "kfold=KFold(n_splits=3,shuffle=True)\n",
        "fold_no=1\n",
        "loss_per_fold = []\n",
        "#{'activation': 'relu', 'optimizer': 'adam', 'num_hidden_layer': 5, 'num_hidden_unit': 60, 'learning_rate': 0.0003135813689247155}\n",
        "model_nn_best = create_model(activation='relu', num_hidden_layer=5, num_hidden_unit=60)\n",
        "model_nn_best.summary()\n",
        "model_nn_best.compile(loss='mse',optimizer=optimizer,metrics=['mse','mae'])\n",
        "for train,test in kfold.split(training,labelsForTrain):\n",
        "  scores=model_nn_best.evaluate(testing,labelsForTest,verbose=0)\n",
        "\n",
        "  print('------------------------------------------------------------------------')\n",
        "  print(f'Training for fold {fold_no} ...')\n",
        "  \n",
        "  # Fit data to model\n",
        "  history = model_nn_best.fit(training, labelsForTrain,\n",
        "              batch_size=20,\n",
        "              #The result from epoch 20 and epoch 200 don't make too much differenct\n",
        "              epochs=100,\n",
        "              verbose=2,)\n",
        "    \n",
        "  print(f'Score for fold {fold_no}: {model_nn_best.metrics_names[0]} of {scores[0]}')\n",
        "  loss_per_fold.append(scores[0])\n",
        "  # Increase fold number\n",
        "  fold_no = fold_no + 1  "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i9yP0G4CIydk",
        "outputId": "1c41593d-9d79-49aa-d995-3907b544e4ad"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_1 (InputLayer)        [(None, 18)]              0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 60)                1140      \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 60)                3660      \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 60)                3660      \n",
            "                                                                 \n",
            " dense_3 (Dense)             (None, 60)                3660      \n",
            "                                                                 \n",
            " dense_4 (Dense)             (None, 1)                 61        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 12,181\n",
            "Trainable params: 12,181\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "20000/20000 - 25s - loss: 16.4224 - mse: 16.4224 - mae: 1.5396 - 25s/epoch - 1ms/step\n",
            "Epoch 2/100\n",
            "20000/20000 - 25s - loss: 15.6687 - mse: 15.6687 - mae: 1.5012 - 25s/epoch - 1ms/step\n",
            "Epoch 3/100\n",
            "20000/20000 - 25s - loss: 15.4015 - mse: 15.4015 - mae: 1.4901 - 25s/epoch - 1ms/step\n",
            "Epoch 4/100\n",
            "20000/20000 - 25s - loss: 15.2443 - mse: 15.2443 - mae: 1.4808 - 25s/epoch - 1ms/step\n",
            "Epoch 5/100\n",
            "20000/20000 - 24s - loss: 15.1252 - mse: 15.1252 - mae: 1.4755 - 24s/epoch - 1ms/step\n",
            "Epoch 6/100\n",
            "20000/20000 - 24s - loss: 14.9577 - mse: 14.9577 - mae: 1.4714 - 24s/epoch - 1ms/step\n",
            "Epoch 7/100\n",
            "20000/20000 - 25s - loss: 14.8216 - mse: 14.8216 - mae: 1.4673 - 25s/epoch - 1ms/step\n",
            "Epoch 8/100\n",
            "20000/20000 - 25s - loss: 14.8014 - mse: 14.8014 - mae: 1.4648 - 25s/epoch - 1ms/step\n",
            "Epoch 9/100\n",
            "20000/20000 - 25s - loss: 14.6206 - mse: 14.6206 - mae: 1.4606 - 25s/epoch - 1ms/step\n",
            "Epoch 10/100\n",
            "20000/20000 - 25s - loss: 14.6016 - mse: 14.6016 - mae: 1.4586 - 25s/epoch - 1ms/step\n",
            "Epoch 11/100\n",
            "20000/20000 - 25s - loss: 14.4987 - mse: 14.4987 - mae: 1.4551 - 25s/epoch - 1ms/step\n",
            "Epoch 12/100\n",
            "20000/20000 - 25s - loss: 14.4179 - mse: 14.4179 - mae: 1.4534 - 25s/epoch - 1ms/step\n",
            "Epoch 13/100\n",
            "20000/20000 - 25s - loss: 14.2848 - mse: 14.2848 - mae: 1.4509 - 25s/epoch - 1ms/step\n",
            "Epoch 14/100\n",
            "20000/20000 - 25s - loss: 14.2252 - mse: 14.2252 - mae: 1.4507 - 25s/epoch - 1ms/step\n",
            "Epoch 15/100\n",
            "20000/20000 - 24s - loss: 14.0406 - mse: 14.0406 - mae: 1.4465 - 24s/epoch - 1ms/step\n",
            "Epoch 16/100\n",
            "20000/20000 - 25s - loss: 13.9909 - mse: 13.9909 - mae: 1.4448 - 25s/epoch - 1ms/step\n",
            "Epoch 17/100\n",
            "20000/20000 - 25s - loss: 13.9385 - mse: 13.9385 - mae: 1.4427 - 25s/epoch - 1ms/step\n",
            "Epoch 18/100\n",
            "20000/20000 - 25s - loss: 13.8290 - mse: 13.8290 - mae: 1.4428 - 25s/epoch - 1ms/step\n",
            "Epoch 19/100\n",
            "20000/20000 - 25s - loss: 13.5022 - mse: 13.5022 - mae: 1.4417 - 25s/epoch - 1ms/step\n",
            "Epoch 20/100\n",
            "20000/20000 - 25s - loss: 13.4691 - mse: 13.4691 - mae: 1.4424 - 25s/epoch - 1ms/step\n",
            "Epoch 21/100\n",
            "20000/20000 - 25s - loss: 13.1611 - mse: 13.1611 - mae: 1.4392 - 25s/epoch - 1ms/step\n",
            "Epoch 22/100\n",
            "20000/20000 - 25s - loss: 12.7602 - mse: 12.7602 - mae: 1.4383 - 25s/epoch - 1ms/step\n",
            "Epoch 23/100\n",
            "20000/20000 - 25s - loss: 12.3913 - mse: 12.3913 - mae: 1.4390 - 25s/epoch - 1ms/step\n",
            "Epoch 24/100\n",
            "20000/20000 - 25s - loss: 11.9640 - mse: 11.9640 - mae: 1.4383 - 25s/epoch - 1ms/step\n",
            "Epoch 25/100\n",
            "20000/20000 - 25s - loss: 12.2147 - mse: 12.2147 - mae: 1.4382 - 25s/epoch - 1ms/step\n",
            "Epoch 26/100\n",
            "20000/20000 - 25s - loss: 11.2800 - mse: 11.2800 - mae: 1.4369 - 25s/epoch - 1ms/step\n",
            "Epoch 27/100\n",
            "20000/20000 - 24s - loss: 10.8285 - mse: 10.8285 - mae: 1.4339 - 24s/epoch - 1ms/step\n",
            "Epoch 28/100\n",
            "20000/20000 - 25s - loss: 10.3204 - mse: 10.3204 - mae: 1.4308 - 25s/epoch - 1ms/step\n",
            "Epoch 29/100\n",
            "20000/20000 - 24s - loss: 9.2845 - mse: 9.2845 - mae: 1.4299 - 24s/epoch - 1ms/step\n",
            "Epoch 30/100\n",
            "20000/20000 - 24s - loss: 9.8392 - mse: 9.8392 - mae: 1.4320 - 24s/epoch - 1ms/step\n",
            "Epoch 31/100\n",
            "20000/20000 - 24s - loss: 9.5977 - mse: 9.5977 - mae: 1.4288 - 24s/epoch - 1ms/step\n",
            "Epoch 32/100\n",
            "20000/20000 - 25s - loss: 9.6752 - mse: 9.6752 - mae: 1.4267 - 25s/epoch - 1ms/step\n",
            "Epoch 33/100\n",
            "20000/20000 - 25s - loss: 9.6480 - mse: 9.6480 - mae: 1.4228 - 25s/epoch - 1ms/step\n",
            "Epoch 34/100\n",
            "20000/20000 - 25s - loss: 9.5273 - mse: 9.5273 - mae: 1.4217 - 25s/epoch - 1ms/step\n",
            "Epoch 35/100\n",
            "20000/20000 - 25s - loss: 9.1746 - mse: 9.1746 - mae: 1.4229 - 25s/epoch - 1ms/step\n",
            "Epoch 36/100\n",
            "20000/20000 - 25s - loss: 9.1810 - mse: 9.1810 - mae: 1.4220 - 25s/epoch - 1ms/step\n",
            "Epoch 37/100\n",
            "20000/20000 - 25s - loss: 9.2365 - mse: 9.2365 - mae: 1.4211 - 25s/epoch - 1ms/step\n",
            "Epoch 38/100\n",
            "20000/20000 - 25s - loss: 9.6168 - mse: 9.6168 - mae: 1.4241 - 25s/epoch - 1ms/step\n",
            "Epoch 39/100\n",
            "20000/20000 - 25s - loss: 9.1326 - mse: 9.1326 - mae: 1.4245 - 25s/epoch - 1ms/step\n",
            "Epoch 40/100\n",
            "20000/20000 - 24s - loss: 9.3211 - mse: 9.3211 - mae: 1.4233 - 24s/epoch - 1ms/step\n",
            "Epoch 41/100\n",
            "20000/20000 - 25s - loss: 8.9769 - mse: 8.9769 - mae: 1.4218 - 25s/epoch - 1ms/step\n",
            "Epoch 42/100\n",
            "20000/20000 - 24s - loss: 8.7730 - mse: 8.7730 - mae: 1.4209 - 24s/epoch - 1ms/step\n",
            "Epoch 43/100\n",
            "20000/20000 - 24s - loss: 8.8003 - mse: 8.8003 - mae: 1.4170 - 24s/epoch - 1ms/step\n",
            "Epoch 44/100\n",
            "20000/20000 - 24s - loss: 8.8042 - mse: 8.8042 - mae: 1.4209 - 24s/epoch - 1ms/step\n",
            "Epoch 45/100\n",
            "20000/20000 - 24s - loss: 9.0939 - mse: 9.0939 - mae: 1.4215 - 24s/epoch - 1ms/step\n",
            "Epoch 46/100\n",
            "20000/20000 - 25s - loss: 9.4976 - mse: 9.4976 - mae: 1.4162 - 25s/epoch - 1ms/step\n",
            "Epoch 47/100\n",
            "20000/20000 - 24s - loss: 9.7111 - mse: 9.7111 - mae: 1.4196 - 24s/epoch - 1ms/step\n",
            "Epoch 48/100\n",
            "20000/20000 - 24s - loss: 8.6138 - mse: 8.6138 - mae: 1.4137 - 24s/epoch - 1ms/step\n",
            "Epoch 49/100\n",
            "20000/20000 - 25s - loss: 9.0846 - mse: 9.0846 - mae: 1.4153 - 25s/epoch - 1ms/step\n",
            "Epoch 50/100\n",
            "20000/20000 - 24s - loss: 9.6797 - mse: 9.6797 - mae: 1.4182 - 24s/epoch - 1ms/step\n",
            "Epoch 51/100\n",
            "20000/20000 - 25s - loss: 8.8768 - mse: 8.8768 - mae: 1.4174 - 25s/epoch - 1ms/step\n",
            "Epoch 52/100\n",
            "20000/20000 - 24s - loss: 8.7806 - mse: 8.7806 - mae: 1.4182 - 24s/epoch - 1ms/step\n",
            "Epoch 53/100\n",
            "20000/20000 - 24s - loss: 9.0075 - mse: 9.0075 - mae: 1.4143 - 24s/epoch - 1ms/step\n",
            "Epoch 54/100\n",
            "20000/20000 - 24s - loss: 8.9671 - mse: 8.9671 - mae: 1.4174 - 24s/epoch - 1ms/step\n",
            "Epoch 55/100\n",
            "20000/20000 - 24s - loss: 9.1049 - mse: 9.1049 - mae: 1.4164 - 24s/epoch - 1ms/step\n",
            "Epoch 56/100\n",
            "20000/20000 - 25s - loss: 8.7738 - mse: 8.7738 - mae: 1.4176 - 25s/epoch - 1ms/step\n",
            "Epoch 57/100\n",
            "20000/20000 - 25s - loss: 8.6564 - mse: 8.6564 - mae: 1.4157 - 25s/epoch - 1ms/step\n",
            "Epoch 58/100\n",
            "20000/20000 - 25s - loss: 9.5911 - mse: 9.5911 - mae: 1.4147 - 25s/epoch - 1ms/step\n",
            "Epoch 59/100\n",
            "20000/20000 - 25s - loss: 9.1032 - mse: 9.1032 - mae: 1.4156 - 25s/epoch - 1ms/step\n",
            "Epoch 60/100\n",
            "20000/20000 - 25s - loss: 8.5082 - mse: 8.5082 - mae: 1.4135 - 25s/epoch - 1ms/step\n",
            "Epoch 61/100\n",
            "20000/20000 - 24s - loss: 8.8021 - mse: 8.8021 - mae: 1.4136 - 24s/epoch - 1ms/step\n",
            "Epoch 62/100\n",
            "20000/20000 - 24s - loss: 8.7506 - mse: 8.7506 - mae: 1.4105 - 24s/epoch - 1ms/step\n",
            "Epoch 63/100\n",
            "20000/20000 - 25s - loss: 8.3141 - mse: 8.3141 - mae: 1.4097 - 25s/epoch - 1ms/step\n",
            "Epoch 64/100\n",
            "20000/20000 - 25s - loss: 8.7370 - mse: 8.7370 - mae: 1.4133 - 25s/epoch - 1ms/step\n",
            "Epoch 65/100\n",
            "20000/20000 - 24s - loss: 8.9051 - mse: 8.9051 - mae: 1.4099 - 24s/epoch - 1ms/step\n",
            "Epoch 66/100\n",
            "20000/20000 - 25s - loss: 8.2988 - mse: 8.2988 - mae: 1.4087 - 25s/epoch - 1ms/step\n",
            "Epoch 67/100\n",
            "20000/20000 - 25s - loss: 9.2619 - mse: 9.2619 - mae: 1.4101 - 25s/epoch - 1ms/step\n",
            "Epoch 68/100\n",
            "20000/20000 - 24s - loss: 8.5702 - mse: 8.5702 - mae: 1.4069 - 24s/epoch - 1ms/step\n",
            "Epoch 69/100\n",
            "20000/20000 - 24s - loss: 8.6192 - mse: 8.6192 - mae: 1.4088 - 24s/epoch - 1ms/step\n",
            "Epoch 70/100\n",
            "20000/20000 - 24s - loss: 8.3751 - mse: 8.3751 - mae: 1.4089 - 24s/epoch - 1ms/step\n",
            "Epoch 71/100\n",
            "20000/20000 - 24s - loss: 8.3027 - mse: 8.3027 - mae: 1.4091 - 24s/epoch - 1ms/step\n",
            "Epoch 72/100\n",
            "20000/20000 - 24s - loss: 8.5016 - mse: 8.5016 - mae: 1.4087 - 24s/epoch - 1ms/step\n",
            "Epoch 73/100\n",
            "20000/20000 - 25s - loss: 10.1371 - mse: 10.1371 - mae: 1.4079 - 25s/epoch - 1ms/step\n",
            "Epoch 74/100\n",
            "20000/20000 - 25s - loss: 8.3555 - mse: 8.3555 - mae: 1.4035 - 25s/epoch - 1ms/step\n",
            "Epoch 75/100\n",
            "20000/20000 - 24s - loss: 8.5317 - mse: 8.5317 - mae: 1.4041 - 24s/epoch - 1ms/step\n",
            "Epoch 76/100\n",
            "20000/20000 - 25s - loss: 8.5604 - mse: 8.5604 - mae: 1.4057 - 25s/epoch - 1ms/step\n",
            "Epoch 77/100\n",
            "20000/20000 - 24s - loss: 8.3732 - mse: 8.3732 - mae: 1.4058 - 24s/epoch - 1ms/step\n",
            "Epoch 78/100\n",
            "20000/20000 - 24s - loss: 8.4365 - mse: 8.4365 - mae: 1.4074 - 24s/epoch - 1ms/step\n",
            "Epoch 79/100\n",
            "20000/20000 - 24s - loss: 8.0969 - mse: 8.0969 - mae: 1.4045 - 24s/epoch - 1ms/step\n",
            "Epoch 80/100\n",
            "20000/20000 - 24s - loss: 8.9877 - mse: 8.9877 - mae: 1.4032 - 24s/epoch - 1ms/step\n",
            "Epoch 81/100\n",
            "20000/20000 - 25s - loss: 8.5059 - mse: 8.5059 - mae: 1.4005 - 25s/epoch - 1ms/step\n",
            "Epoch 82/100\n",
            "20000/20000 - 24s - loss: 8.1990 - mse: 8.1990 - mae: 1.3992 - 24s/epoch - 1ms/step\n",
            "Epoch 83/100\n",
            "20000/20000 - 25s - loss: 7.9634 - mse: 7.9634 - mae: 1.4006 - 25s/epoch - 1ms/step\n",
            "Epoch 84/100\n",
            "20000/20000 - 25s - loss: 9.3399 - mse: 9.3399 - mae: 1.4001 - 25s/epoch - 1ms/step\n",
            "Epoch 85/100\n",
            "20000/20000 - 24s - loss: 8.5055 - mse: 8.5055 - mae: 1.4031 - 24s/epoch - 1ms/step\n",
            "Epoch 86/100\n",
            "20000/20000 - 24s - loss: 8.4826 - mse: 8.4826 - mae: 1.4008 - 24s/epoch - 1ms/step\n",
            "Epoch 87/100\n",
            "20000/20000 - 24s - loss: 7.7847 - mse: 7.7847 - mae: 1.3999 - 24s/epoch - 1ms/step\n",
            "Epoch 88/100\n",
            "20000/20000 - 24s - loss: 7.8778 - mse: 7.8778 - mae: 1.3973 - 24s/epoch - 1ms/step\n",
            "Epoch 89/100\n",
            "20000/20000 - 25s - loss: 8.4053 - mse: 8.4053 - mae: 1.4023 - 25s/epoch - 1ms/step\n",
            "Epoch 90/100\n",
            "20000/20000 - 24s - loss: 7.8807 - mse: 7.8807 - mae: 1.3998 - 24s/epoch - 1ms/step\n",
            "Epoch 91/100\n",
            "20000/20000 - 24s - loss: 8.6021 - mse: 8.6021 - mae: 1.3983 - 24s/epoch - 1ms/step\n",
            "Epoch 92/100\n",
            "20000/20000 - 25s - loss: 8.4070 - mse: 8.4070 - mae: 1.3962 - 25s/epoch - 1ms/step\n",
            "Epoch 93/100\n",
            "20000/20000 - 25s - loss: 8.7613 - mse: 8.7613 - mae: 1.3947 - 25s/epoch - 1ms/step\n",
            "Epoch 94/100\n",
            "20000/20000 - 24s - loss: 8.8108 - mse: 8.8108 - mae: 1.3977 - 24s/epoch - 1ms/step\n",
            "Epoch 95/100\n",
            "20000/20000 - 24s - loss: 8.0352 - mse: 8.0352 - mae: 1.3957 - 24s/epoch - 1ms/step\n",
            "Epoch 96/100\n",
            "20000/20000 - 25s - loss: 8.6022 - mse: 8.6022 - mae: 1.3953 - 25s/epoch - 1ms/step\n",
            "Epoch 97/100\n",
            "20000/20000 - 25s - loss: 7.8507 - mse: 7.8507 - mae: 1.3989 - 25s/epoch - 1ms/step\n",
            "Epoch 98/100\n",
            "20000/20000 - 24s - loss: 7.6350 - mse: 7.6350 - mae: 1.3939 - 24s/epoch - 1ms/step\n",
            "Epoch 99/100\n",
            "20000/20000 - 24s - loss: 9.4739 - mse: 9.4739 - mae: 1.3938 - 24s/epoch - 1ms/step\n",
            "Epoch 100/100\n",
            "20000/20000 - 24s - loss: 7.8999 - mse: 7.8999 - mae: 1.3940 - 24s/epoch - 1ms/step\n",
            "Score for fold 1: loss of 22.6796817779541\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "20000/20000 - 25s - loss: 7.6113 - mse: 7.6113 - mae: 1.3937 - 25s/epoch - 1ms/step\n",
            "Epoch 2/100\n",
            "20000/20000 - 25s - loss: 7.8341 - mse: 7.8341 - mae: 1.3949 - 25s/epoch - 1ms/step\n",
            "Epoch 3/100\n",
            "20000/20000 - 25s - loss: 7.6999 - mse: 7.6999 - mae: 1.3901 - 25s/epoch - 1ms/step\n",
            "Epoch 4/100\n",
            "20000/20000 - 24s - loss: 7.7059 - mse: 7.7059 - mae: 1.3876 - 24s/epoch - 1ms/step\n",
            "Epoch 5/100\n",
            "20000/20000 - 24s - loss: 7.6588 - mse: 7.6588 - mae: 1.3880 - 24s/epoch - 1ms/step\n",
            "Epoch 6/100\n",
            "20000/20000 - 25s - loss: 7.9495 - mse: 7.9495 - mae: 1.3981 - 25s/epoch - 1ms/step\n",
            "Epoch 7/100\n",
            "20000/20000 - 25s - loss: 7.9412 - mse: 7.9412 - mae: 1.3900 - 25s/epoch - 1ms/step\n",
            "Epoch 8/100\n",
            "20000/20000 - 25s - loss: 8.4159 - mse: 8.4159 - mae: 1.3905 - 25s/epoch - 1ms/step\n",
            "Epoch 9/100\n",
            "20000/20000 - 25s - loss: 7.8080 - mse: 7.8080 - mae: 1.3860 - 25s/epoch - 1ms/step\n",
            "Epoch 10/100\n",
            "20000/20000 - 25s - loss: 7.6637 - mse: 7.6637 - mae: 1.3902 - 25s/epoch - 1ms/step\n",
            "Epoch 11/100\n",
            "20000/20000 - 24s - loss: 8.5750 - mse: 8.5750 - mae: 1.3887 - 24s/epoch - 1ms/step\n",
            "Epoch 12/100\n",
            "20000/20000 - 25s - loss: 7.8431 - mse: 7.8431 - mae: 1.3887 - 25s/epoch - 1ms/step\n",
            "Epoch 13/100\n",
            "20000/20000 - 24s - loss: 7.6397 - mse: 7.6397 - mae: 1.3902 - 24s/epoch - 1ms/step\n",
            "Epoch 14/100\n",
            "20000/20000 - 25s - loss: 7.7340 - mse: 7.7340 - mae: 1.3899 - 25s/epoch - 1ms/step\n",
            "Epoch 15/100\n",
            "20000/20000 - 25s - loss: 7.3977 - mse: 7.3977 - mae: 1.3863 - 25s/epoch - 1ms/step\n",
            "Epoch 16/100\n",
            "20000/20000 - 25s - loss: 8.0542 - mse: 8.0542 - mae: 1.3914 - 25s/epoch - 1ms/step\n",
            "Epoch 17/100\n",
            "20000/20000 - 25s - loss: 8.0625 - mse: 8.0625 - mae: 1.3902 - 25s/epoch - 1ms/step\n",
            "Epoch 18/100\n",
            "20000/20000 - 25s - loss: 8.0226 - mse: 8.0226 - mae: 1.3862 - 25s/epoch - 1ms/step\n",
            "Epoch 19/100\n",
            "20000/20000 - 25s - loss: 7.8997 - mse: 7.8997 - mae: 1.3900 - 25s/epoch - 1ms/step\n",
            "Epoch 20/100\n",
            "20000/20000 - 25s - loss: 7.6492 - mse: 7.6492 - mae: 1.3858 - 25s/epoch - 1ms/step\n",
            "Epoch 21/100\n",
            "20000/20000 - 25s - loss: 7.8120 - mse: 7.8120 - mae: 1.3871 - 25s/epoch - 1ms/step\n",
            "Epoch 22/100\n",
            "20000/20000 - 25s - loss: 7.7165 - mse: 7.7165 - mae: 1.3895 - 25s/epoch - 1ms/step\n",
            "Epoch 23/100\n",
            "20000/20000 - 25s - loss: 8.0268 - mse: 8.0268 - mae: 1.3862 - 25s/epoch - 1ms/step\n",
            "Epoch 24/100\n",
            "20000/20000 - 25s - loss: 7.4032 - mse: 7.4032 - mae: 1.3864 - 25s/epoch - 1ms/step\n",
            "Epoch 25/100\n",
            "20000/20000 - 25s - loss: 8.5004 - mse: 8.5004 - mae: 1.3827 - 25s/epoch - 1ms/step\n",
            "Epoch 26/100\n",
            "20000/20000 - 25s - loss: 7.5712 - mse: 7.5712 - mae: 1.3872 - 25s/epoch - 1ms/step\n",
            "Epoch 27/100\n",
            "20000/20000 - 25s - loss: 7.5035 - mse: 7.5035 - mae: 1.3851 - 25s/epoch - 1ms/step\n",
            "Epoch 28/100\n",
            "20000/20000 - 25s - loss: 7.6028 - mse: 7.6028 - mae: 1.3859 - 25s/epoch - 1ms/step\n",
            "Epoch 29/100\n",
            "20000/20000 - 25s - loss: 8.0319 - mse: 8.0319 - mae: 1.3870 - 25s/epoch - 1ms/step\n",
            "Epoch 30/100\n",
            "20000/20000 - 25s - loss: 7.3242 - mse: 7.3242 - mae: 1.3829 - 25s/epoch - 1ms/step\n",
            "Epoch 31/100\n",
            "20000/20000 - 25s - loss: 7.0874 - mse: 7.0874 - mae: 1.3797 - 25s/epoch - 1ms/step\n",
            "Epoch 32/100\n",
            "20000/20000 - 25s - loss: 7.6543 - mse: 7.6543 - mae: 1.3856 - 25s/epoch - 1ms/step\n",
            "Epoch 33/100\n",
            "20000/20000 - 25s - loss: 8.1188 - mse: 8.1188 - mae: 1.3846 - 25s/epoch - 1ms/step\n",
            "Epoch 34/100\n",
            "20000/20000 - 25s - loss: 7.7243 - mse: 7.7243 - mae: 1.3902 - 25s/epoch - 1ms/step\n",
            "Epoch 35/100\n",
            "20000/20000 - 25s - loss: 7.5963 - mse: 7.5963 - mae: 1.3824 - 25s/epoch - 1ms/step\n",
            "Epoch 36/100\n",
            "20000/20000 - 25s - loss: 7.6981 - mse: 7.6981 - mae: 1.3837 - 25s/epoch - 1ms/step\n",
            "Epoch 37/100\n",
            "20000/20000 - 25s - loss: 7.1800 - mse: 7.1800 - mae: 1.3790 - 25s/epoch - 1ms/step\n",
            "Epoch 38/100\n",
            "20000/20000 - 25s - loss: 7.3933 - mse: 7.3933 - mae: 1.3778 - 25s/epoch - 1ms/step\n",
            "Epoch 39/100\n",
            "20000/20000 - 25s - loss: 7.2019 - mse: 7.2019 - mae: 1.3784 - 25s/epoch - 1ms/step\n",
            "Epoch 40/100\n",
            "20000/20000 - 25s - loss: 7.2580 - mse: 7.2580 - mae: 1.3798 - 25s/epoch - 1ms/step\n",
            "Epoch 41/100\n",
            "20000/20000 - 25s - loss: 7.8844 - mse: 7.8844 - mae: 1.3802 - 25s/epoch - 1ms/step\n",
            "Epoch 42/100\n",
            "20000/20000 - 25s - loss: 7.2977 - mse: 7.2977 - mae: 1.3783 - 25s/epoch - 1ms/step\n",
            "Epoch 43/100\n",
            "20000/20000 - 24s - loss: 7.3834 - mse: 7.3834 - mae: 1.3771 - 24s/epoch - 1ms/step\n",
            "Epoch 44/100\n",
            "20000/20000 - 25s - loss: 7.4510 - mse: 7.4510 - mae: 1.3771 - 25s/epoch - 1ms/step\n",
            "Epoch 45/100\n",
            "20000/20000 - 24s - loss: 7.1657 - mse: 7.1657 - mae: 1.3793 - 24s/epoch - 1ms/step\n",
            "Epoch 46/100\n",
            "20000/20000 - 24s - loss: 8.7841 - mse: 8.7841 - mae: 1.3844 - 24s/epoch - 1ms/step\n",
            "Epoch 47/100\n",
            "20000/20000 - 24s - loss: 7.1954 - mse: 7.1954 - mae: 1.3786 - 24s/epoch - 1ms/step\n",
            "Epoch 48/100\n",
            "20000/20000 - 25s - loss: 7.4157 - mse: 7.4157 - mae: 1.3781 - 25s/epoch - 1ms/step\n",
            "Epoch 49/100\n",
            "20000/20000 - 25s - loss: 7.9623 - mse: 7.9623 - mae: 1.3794 - 25s/epoch - 1ms/step\n",
            "Epoch 50/100\n",
            "20000/20000 - 25s - loss: 7.1422 - mse: 7.1422 - mae: 1.3761 - 25s/epoch - 1ms/step\n",
            "Epoch 51/100\n",
            "20000/20000 - 25s - loss: 8.2801 - mse: 8.2801 - mae: 1.3815 - 25s/epoch - 1ms/step\n",
            "Epoch 52/100\n",
            "20000/20000 - 25s - loss: 7.0457 - mse: 7.0457 - mae: 1.3756 - 25s/epoch - 1ms/step\n",
            "Epoch 53/100\n",
            "20000/20000 - 25s - loss: 7.3187 - mse: 7.3187 - mae: 1.3780 - 25s/epoch - 1ms/step\n",
            "Epoch 54/100\n",
            "20000/20000 - 25s - loss: 8.5911 - mse: 8.5911 - mae: 1.3819 - 25s/epoch - 1ms/step\n",
            "Epoch 55/100\n",
            "20000/20000 - 25s - loss: 7.1594 - mse: 7.1594 - mae: 1.3764 - 25s/epoch - 1ms/step\n",
            "Epoch 56/100\n",
            "20000/20000 - 25s - loss: 8.3280 - mse: 8.3280 - mae: 1.3793 - 25s/epoch - 1ms/step\n",
            "Epoch 57/100\n",
            "20000/20000 - 25s - loss: 7.1515 - mse: 7.1515 - mae: 1.3764 - 25s/epoch - 1ms/step\n",
            "Epoch 58/100\n",
            "20000/20000 - 25s - loss: 7.5561 - mse: 7.5561 - mae: 1.3780 - 25s/epoch - 1ms/step\n",
            "Epoch 59/100\n",
            "20000/20000 - 25s - loss: 7.2575 - mse: 7.2575 - mae: 1.3791 - 25s/epoch - 1ms/step\n",
            "Epoch 60/100\n",
            "20000/20000 - 25s - loss: 7.3094 - mse: 7.3094 - mae: 1.3753 - 25s/epoch - 1ms/step\n",
            "Epoch 61/100\n",
            "20000/20000 - 24s - loss: 7.3965 - mse: 7.3965 - mae: 1.3788 - 24s/epoch - 1ms/step\n",
            "Epoch 62/100\n",
            "20000/20000 - 24s - loss: 6.9336 - mse: 6.9336 - mae: 1.3742 - 24s/epoch - 1ms/step\n",
            "Epoch 63/100\n",
            "20000/20000 - 25s - loss: 7.2168 - mse: 7.2168 - mae: 1.3734 - 25s/epoch - 1ms/step\n",
            "Epoch 64/100\n",
            "20000/20000 - 25s - loss: 7.8131 - mse: 7.8131 - mae: 1.3762 - 25s/epoch - 1ms/step\n",
            "Epoch 65/100\n",
            "20000/20000 - 25s - loss: 7.3937 - mse: 7.3937 - mae: 1.3742 - 25s/epoch - 1ms/step\n",
            "Epoch 66/100\n",
            "20000/20000 - 24s - loss: 7.3961 - mse: 7.3961 - mae: 1.3759 - 24s/epoch - 1ms/step\n",
            "Epoch 67/100\n",
            "20000/20000 - 24s - loss: 7.4968 - mse: 7.4968 - mae: 1.3816 - 24s/epoch - 1ms/step\n",
            "Epoch 68/100\n",
            "20000/20000 - 25s - loss: 7.0788 - mse: 7.0788 - mae: 1.3745 - 25s/epoch - 1ms/step\n",
            "Epoch 69/100\n",
            "20000/20000 - 25s - loss: 8.9591 - mse: 8.9591 - mae: 1.3844 - 25s/epoch - 1ms/step\n",
            "Epoch 70/100\n",
            "20000/20000 - 24s - loss: 6.9752 - mse: 6.9752 - mae: 1.3745 - 24s/epoch - 1ms/step\n",
            "Epoch 71/100\n",
            "20000/20000 - 25s - loss: 7.8533 - mse: 7.8533 - mae: 1.3756 - 25s/epoch - 1ms/step\n",
            "Epoch 72/100\n",
            "20000/20000 - 25s - loss: 7.4225 - mse: 7.4225 - mae: 1.3758 - 25s/epoch - 1ms/step\n",
            "Epoch 73/100\n",
            "20000/20000 - 25s - loss: 7.0840 - mse: 7.0840 - mae: 1.3760 - 25s/epoch - 1ms/step\n",
            "Epoch 74/100\n",
            "20000/20000 - 25s - loss: 7.3797 - mse: 7.3797 - mae: 1.3737 - 25s/epoch - 1ms/step\n",
            "Epoch 75/100\n",
            "20000/20000 - 25s - loss: 7.6059 - mse: 7.6059 - mae: 1.3751 - 25s/epoch - 1ms/step\n",
            "Epoch 76/100\n",
            "20000/20000 - 25s - loss: 7.5733 - mse: 7.5733 - mae: 1.3767 - 25s/epoch - 1ms/step\n",
            "Epoch 77/100\n",
            "20000/20000 - 25s - loss: 7.5032 - mse: 7.5032 - mae: 1.3773 - 25s/epoch - 1ms/step\n",
            "Epoch 78/100\n",
            "20000/20000 - 25s - loss: 6.9903 - mse: 6.9903 - mae: 1.3745 - 25s/epoch - 1ms/step\n",
            "Epoch 79/100\n",
            "20000/20000 - 25s - loss: 8.7273 - mse: 8.7273 - mae: 1.3732 - 25s/epoch - 1ms/step\n",
            "Epoch 80/100\n",
            "20000/20000 - 25s - loss: 7.1057 - mse: 7.1057 - mae: 1.3729 - 25s/epoch - 1ms/step\n",
            "Epoch 81/100\n",
            "20000/20000 - 25s - loss: 7.6557 - mse: 7.6557 - mae: 1.3773 - 25s/epoch - 1ms/step\n",
            "Epoch 82/100\n",
            "20000/20000 - 25s - loss: 7.1916 - mse: 7.1916 - mae: 1.3749 - 25s/epoch - 1ms/step\n",
            "Epoch 83/100\n",
            "20000/20000 - 25s - loss: 7.3600 - mse: 7.3600 - mae: 1.3736 - 25s/epoch - 1ms/step\n",
            "Epoch 84/100\n",
            "20000/20000 - 25s - loss: 7.0858 - mse: 7.0858 - mae: 1.3755 - 25s/epoch - 1ms/step\n",
            "Epoch 85/100\n",
            "20000/20000 - 25s - loss: 6.9865 - mse: 6.9865 - mae: 1.3727 - 25s/epoch - 1ms/step\n",
            "Epoch 86/100\n",
            "20000/20000 - 25s - loss: 6.9720 - mse: 6.9720 - mae: 1.3709 - 25s/epoch - 1ms/step\n",
            "Epoch 87/100\n",
            "20000/20000 - 25s - loss: 8.5387 - mse: 8.5387 - mae: 1.3721 - 25s/epoch - 1ms/step\n",
            "Epoch 88/100\n",
            "20000/20000 - 25s - loss: 7.2197 - mse: 7.2197 - mae: 1.3741 - 25s/epoch - 1ms/step\n",
            "Epoch 89/100\n",
            "20000/20000 - 25s - loss: 7.0491 - mse: 7.0491 - mae: 1.3737 - 25s/epoch - 1ms/step\n",
            "Epoch 90/100\n",
            "20000/20000 - 25s - loss: 7.0805 - mse: 7.0805 - mae: 1.3706 - 25s/epoch - 1ms/step\n",
            "Epoch 91/100\n",
            "20000/20000 - 25s - loss: 7.4201 - mse: 7.4201 - mae: 1.3721 - 25s/epoch - 1ms/step\n",
            "Epoch 92/100\n",
            "20000/20000 - 25s - loss: 6.8725 - mse: 6.8725 - mae: 1.3729 - 25s/epoch - 1ms/step\n",
            "Epoch 93/100\n",
            "20000/20000 - 25s - loss: 6.9478 - mse: 6.9478 - mae: 1.3703 - 25s/epoch - 1ms/step\n",
            "Epoch 94/100\n",
            "20000/20000 - 25s - loss: 7.3883 - mse: 7.3883 - mae: 1.3747 - 25s/epoch - 1ms/step\n",
            "Epoch 95/100\n",
            "20000/20000 - 25s - loss: 7.0761 - mse: 7.0761 - mae: 1.3733 - 25s/epoch - 1ms/step\n",
            "Epoch 96/100\n",
            "20000/20000 - 25s - loss: 7.2328 - mse: 7.2328 - mae: 1.3698 - 25s/epoch - 1ms/step\n",
            "Epoch 97/100\n",
            "20000/20000 - 25s - loss: 7.2374 - mse: 7.2374 - mae: 1.3726 - 25s/epoch - 1ms/step\n",
            "Epoch 98/100\n",
            "20000/20000 - 25s - loss: 7.1505 - mse: 7.1505 - mae: 1.3727 - 25s/epoch - 1ms/step\n",
            "Epoch 99/100\n",
            "20000/20000 - 25s - loss: 7.6869 - mse: 7.6869 - mae: 1.3725 - 25s/epoch - 1ms/step\n",
            "Epoch 100/100\n",
            "20000/20000 - 25s - loss: 6.9855 - mse: 6.9855 - mae: 1.3701 - 25s/epoch - 1ms/step\n",
            "Score for fold 2: loss of 10.89925479888916\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "20000/20000 - 25s - loss: 7.2247 - mse: 7.2247 - mae: 1.3707 - 25s/epoch - 1ms/step\n",
            "Epoch 2/100\n",
            "20000/20000 - 25s - loss: 6.9739 - mse: 6.9739 - mae: 1.3694 - 25s/epoch - 1ms/step\n",
            "Epoch 3/100\n",
            "20000/20000 - 25s - loss: 7.2247 - mse: 7.2247 - mae: 1.3707 - 25s/epoch - 1ms/step\n",
            "Epoch 4/100\n",
            "20000/20000 - 25s - loss: 6.8330 - mse: 6.8330 - mae: 1.3712 - 25s/epoch - 1ms/step\n",
            "Epoch 5/100\n",
            "20000/20000 - 25s - loss: 7.1842 - mse: 7.1842 - mae: 1.3708 - 25s/epoch - 1ms/step\n",
            "Epoch 6/100\n",
            "20000/20000 - 25s - loss: 6.9641 - mse: 6.9641 - mae: 1.3727 - 25s/epoch - 1ms/step\n",
            "Epoch 7/100\n",
            "20000/20000 - 25s - loss: 6.7643 - mse: 6.7643 - mae: 1.3677 - 25s/epoch - 1ms/step\n",
            "Epoch 8/100\n",
            "20000/20000 - 25s - loss: 7.1530 - mse: 7.1530 - mae: 1.3705 - 25s/epoch - 1ms/step\n",
            "Epoch 9/100\n",
            "20000/20000 - 25s - loss: 6.9390 - mse: 6.9390 - mae: 1.3712 - 25s/epoch - 1ms/step\n",
            "Epoch 10/100\n",
            "20000/20000 - 25s - loss: 7.1749 - mse: 7.1749 - mae: 1.3701 - 25s/epoch - 1ms/step\n",
            "Epoch 11/100\n",
            "20000/20000 - 25s - loss: 7.5219 - mse: 7.5219 - mae: 1.3720 - 25s/epoch - 1ms/step\n",
            "Epoch 12/100\n",
            "20000/20000 - 25s - loss: 7.1226 - mse: 7.1226 - mae: 1.3670 - 25s/epoch - 1ms/step\n",
            "Epoch 13/100\n",
            "20000/20000 - 25s - loss: 7.4602 - mse: 7.4602 - mae: 1.3728 - 25s/epoch - 1ms/step\n",
            "Epoch 14/100\n",
            "20000/20000 - 26s - loss: 6.8597 - mse: 6.8597 - mae: 1.3718 - 26s/epoch - 1ms/step\n",
            "Epoch 15/100\n",
            "20000/20000 - 25s - loss: 7.0077 - mse: 7.0077 - mae: 1.3691 - 25s/epoch - 1ms/step\n",
            "Epoch 16/100\n",
            "20000/20000 - 25s - loss: 7.0619 - mse: 7.0619 - mae: 1.3673 - 25s/epoch - 1ms/step\n",
            "Epoch 17/100\n",
            "20000/20000 - 25s - loss: 7.2266 - mse: 7.2266 - mae: 1.3714 - 25s/epoch - 1ms/step\n",
            "Epoch 18/100\n",
            "20000/20000 - 25s - loss: 6.8272 - mse: 6.8272 - mae: 1.3668 - 25s/epoch - 1ms/step\n",
            "Epoch 19/100\n",
            "20000/20000 - 25s - loss: 7.2566 - mse: 7.2566 - mae: 1.3711 - 25s/epoch - 1ms/step\n",
            "Epoch 20/100\n",
            "20000/20000 - 25s - loss: 6.8832 - mse: 6.8832 - mae: 1.3675 - 25s/epoch - 1ms/step\n",
            "Epoch 21/100\n",
            "20000/20000 - 25s - loss: 6.6847 - mse: 6.6847 - mae: 1.3678 - 25s/epoch - 1ms/step\n",
            "Epoch 22/100\n",
            "20000/20000 - 26s - loss: 7.0728 - mse: 7.0728 - mae: 1.3671 - 26s/epoch - 1ms/step\n",
            "Epoch 23/100\n",
            "20000/20000 - 26s - loss: 7.2298 - mse: 7.2298 - mae: 1.3679 - 26s/epoch - 1ms/step\n",
            "Epoch 24/100\n",
            "20000/20000 - 25s - loss: 6.6646 - mse: 6.6646 - mae: 1.3667 - 25s/epoch - 1ms/step\n",
            "Epoch 25/100\n",
            "20000/20000 - 25s - loss: 7.1959 - mse: 7.1959 - mae: 1.3664 - 25s/epoch - 1ms/step\n",
            "Epoch 26/100\n",
            "20000/20000 - 25s - loss: 6.7998 - mse: 6.7998 - mae: 1.3696 - 25s/epoch - 1ms/step\n",
            "Epoch 27/100\n",
            "20000/20000 - 25s - loss: 7.2994 - mse: 7.2994 - mae: 1.3699 - 25s/epoch - 1ms/step\n",
            "Epoch 28/100\n",
            "20000/20000 - 25s - loss: 6.7767 - mse: 6.7767 - mae: 1.3682 - 25s/epoch - 1ms/step\n",
            "Epoch 29/100\n",
            "20000/20000 - 25s - loss: 6.7639 - mse: 6.7639 - mae: 1.3662 - 25s/epoch - 1ms/step\n",
            "Epoch 30/100\n",
            "20000/20000 - 25s - loss: 6.7501 - mse: 6.7501 - mae: 1.3666 - 25s/epoch - 1ms/step\n",
            "Epoch 31/100\n",
            "20000/20000 - 25s - loss: 6.7231 - mse: 6.7231 - mae: 1.3653 - 25s/epoch - 1ms/step\n",
            "Epoch 32/100\n",
            "20000/20000 - 25s - loss: 7.0023 - mse: 7.0023 - mae: 1.3689 - 25s/epoch - 1ms/step\n",
            "Epoch 33/100\n",
            "20000/20000 - 25s - loss: 7.3311 - mse: 7.3311 - mae: 1.3684 - 25s/epoch - 1ms/step\n",
            "Epoch 34/100\n",
            "20000/20000 - 25s - loss: 6.7948 - mse: 6.7948 - mae: 1.3633 - 25s/epoch - 1ms/step\n",
            "Epoch 35/100\n",
            "20000/20000 - 25s - loss: 7.0732 - mse: 7.0732 - mae: 1.3668 - 25s/epoch - 1ms/step\n",
            "Epoch 36/100\n",
            "20000/20000 - 25s - loss: 6.9154 - mse: 6.9154 - mae: 1.3667 - 25s/epoch - 1ms/step\n",
            "Epoch 37/100\n",
            "20000/20000 - 25s - loss: 8.1430 - mse: 8.1430 - mae: 1.3672 - 25s/epoch - 1ms/step\n",
            "Epoch 38/100\n",
            "20000/20000 - 25s - loss: 7.1981 - mse: 7.1981 - mae: 1.3666 - 25s/epoch - 1ms/step\n",
            "Epoch 39/100\n",
            "20000/20000 - 26s - loss: 7.1042 - mse: 7.1042 - mae: 1.3700 - 26s/epoch - 1ms/step\n",
            "Epoch 40/100\n",
            "20000/20000 - 25s - loss: 6.5448 - mse: 6.5448 - mae: 1.3636 - 25s/epoch - 1ms/step\n",
            "Epoch 41/100\n",
            "20000/20000 - 25s - loss: 7.3744 - mse: 7.3744 - mae: 1.3651 - 25s/epoch - 1ms/step\n",
            "Epoch 42/100\n",
            "20000/20000 - 25s - loss: 6.8256 - mse: 6.8256 - mae: 1.3645 - 25s/epoch - 1ms/step\n",
            "Epoch 43/100\n",
            "20000/20000 - 25s - loss: 6.7296 - mse: 6.7296 - mae: 1.3650 - 25s/epoch - 1ms/step\n",
            "Epoch 44/100\n",
            "20000/20000 - 25s - loss: 6.8165 - mse: 6.8165 - mae: 1.3650 - 25s/epoch - 1ms/step\n",
            "Epoch 45/100\n",
            "20000/20000 - 25s - loss: 6.9374 - mse: 6.9374 - mae: 1.3623 - 25s/epoch - 1ms/step\n",
            "Epoch 46/100\n",
            "20000/20000 - 25s - loss: 6.9933 - mse: 6.9933 - mae: 1.3640 - 25s/epoch - 1ms/step\n",
            "Epoch 47/100\n",
            "20000/20000 - 25s - loss: 7.2547 - mse: 7.2547 - mae: 1.3646 - 25s/epoch - 1ms/step\n",
            "Epoch 48/100\n",
            "20000/20000 - 25s - loss: 6.6669 - mse: 6.6669 - mae: 1.3637 - 25s/epoch - 1ms/step\n",
            "Epoch 49/100\n",
            "20000/20000 - 25s - loss: 6.6629 - mse: 6.6629 - mae: 1.3647 - 25s/epoch - 1ms/step\n",
            "Epoch 50/100\n",
            "20000/20000 - 25s - loss: 6.7442 - mse: 6.7442 - mae: 1.3612 - 25s/epoch - 1ms/step\n",
            "Epoch 51/100\n",
            "20000/20000 - 25s - loss: 6.6072 - mse: 6.6072 - mae: 1.3602 - 25s/epoch - 1ms/step\n",
            "Epoch 52/100\n",
            "20000/20000 - 25s - loss: 7.2876 - mse: 7.2876 - mae: 1.3652 - 25s/epoch - 1ms/step\n",
            "Epoch 53/100\n",
            "20000/20000 - 25s - loss: 6.7385 - mse: 6.7385 - mae: 1.3670 - 25s/epoch - 1ms/step\n",
            "Epoch 54/100\n",
            "20000/20000 - 25s - loss: 6.8105 - mse: 6.8105 - mae: 1.3657 - 25s/epoch - 1ms/step\n",
            "Epoch 55/100\n",
            "20000/20000 - 25s - loss: 6.7767 - mse: 6.7767 - mae: 1.3670 - 25s/epoch - 1ms/step\n",
            "Epoch 56/100\n",
            "20000/20000 - 25s - loss: 7.1180 - mse: 7.1180 - mae: 1.3666 - 25s/epoch - 1ms/step\n",
            "Epoch 57/100\n",
            "20000/20000 - 25s - loss: 6.7071 - mse: 6.7071 - mae: 1.3621 - 25s/epoch - 1ms/step\n",
            "Epoch 58/100\n",
            "20000/20000 - 25s - loss: 6.8340 - mse: 6.8340 - mae: 1.3679 - 25s/epoch - 1ms/step\n",
            "Epoch 59/100\n",
            "20000/20000 - 25s - loss: 6.8778 - mse: 6.8778 - mae: 1.3599 - 25s/epoch - 1ms/step\n",
            "Epoch 60/100\n",
            "20000/20000 - 25s - loss: 6.5059 - mse: 6.5059 - mae: 1.3613 - 25s/epoch - 1ms/step\n",
            "Epoch 61/100\n",
            "20000/20000 - 25s - loss: 6.7182 - mse: 6.7182 - mae: 1.3637 - 25s/epoch - 1ms/step\n",
            "Epoch 62/100\n",
            "20000/20000 - 25s - loss: 6.7349 - mse: 6.7349 - mae: 1.3623 - 25s/epoch - 1ms/step\n",
            "Epoch 63/100\n",
            "20000/20000 - 25s - loss: 6.6685 - mse: 6.6685 - mae: 1.3635 - 25s/epoch - 1ms/step\n",
            "Epoch 64/100\n",
            "20000/20000 - 25s - loss: 6.8012 - mse: 6.8012 - mae: 1.3623 - 25s/epoch - 1ms/step\n",
            "Epoch 65/100\n",
            "20000/20000 - 25s - loss: 6.8249 - mse: 6.8249 - mae: 1.3602 - 25s/epoch - 1ms/step\n",
            "Epoch 66/100\n",
            "20000/20000 - 25s - loss: 6.5381 - mse: 6.5381 - mae: 1.3595 - 25s/epoch - 1ms/step\n",
            "Epoch 67/100\n",
            "20000/20000 - 25s - loss: 6.7016 - mse: 6.7016 - mae: 1.3602 - 25s/epoch - 1ms/step\n",
            "Epoch 68/100\n",
            "20000/20000 - 25s - loss: 6.7820 - mse: 6.7820 - mae: 1.3643 - 25s/epoch - 1ms/step\n",
            "Epoch 69/100\n",
            "20000/20000 - 25s - loss: 6.6308 - mse: 6.6308 - mae: 1.3603 - 25s/epoch - 1ms/step\n",
            "Epoch 70/100\n",
            "20000/20000 - 25s - loss: 6.6574 - mse: 6.6574 - mae: 1.3595 - 25s/epoch - 1ms/step\n",
            "Epoch 71/100\n",
            "20000/20000 - 25s - loss: 6.5545 - mse: 6.5545 - mae: 1.3600 - 25s/epoch - 1ms/step\n",
            "Epoch 72/100\n",
            "20000/20000 - 25s - loss: 6.9778 - mse: 6.9778 - mae: 1.3607 - 25s/epoch - 1ms/step\n",
            "Epoch 73/100\n",
            "20000/20000 - 25s - loss: 6.8518 - mse: 6.8518 - mae: 1.3621 - 25s/epoch - 1ms/step\n",
            "Epoch 74/100\n",
            "20000/20000 - 25s - loss: 6.8542 - mse: 6.8542 - mae: 1.3614 - 25s/epoch - 1ms/step\n",
            "Epoch 75/100\n",
            "20000/20000 - 25s - loss: 6.6114 - mse: 6.6114 - mae: 1.3603 - 25s/epoch - 1ms/step\n",
            "Epoch 76/100\n",
            "20000/20000 - 25s - loss: 7.8395 - mse: 7.8395 - mae: 1.3608 - 25s/epoch - 1ms/step\n",
            "Epoch 77/100\n",
            "20000/20000 - 25s - loss: 6.8699 - mse: 6.8699 - mae: 1.3639 - 25s/epoch - 1ms/step\n",
            "Epoch 78/100\n",
            "20000/20000 - 25s - loss: 7.0046 - mse: 7.0046 - mae: 1.3629 - 25s/epoch - 1ms/step\n",
            "Epoch 79/100\n",
            "20000/20000 - 25s - loss: 6.5149 - mse: 6.5149 - mae: 1.3601 - 25s/epoch - 1ms/step\n",
            "Epoch 80/100\n",
            "20000/20000 - 25s - loss: 6.7433 - mse: 6.7433 - mae: 1.3599 - 25s/epoch - 1ms/step\n",
            "Epoch 81/100\n",
            "20000/20000 - 25s - loss: 6.7345 - mse: 6.7345 - mae: 1.3602 - 25s/epoch - 1ms/step\n",
            "Epoch 82/100\n",
            "20000/20000 - 25s - loss: 6.5183 - mse: 6.5183 - mae: 1.3563 - 25s/epoch - 1ms/step\n",
            "Epoch 83/100\n",
            "20000/20000 - 25s - loss: 6.5259 - mse: 6.5259 - mae: 1.3605 - 25s/epoch - 1ms/step\n",
            "Epoch 84/100\n",
            "20000/20000 - 25s - loss: 6.5769 - mse: 6.5769 - mae: 1.3595 - 25s/epoch - 1ms/step\n",
            "Epoch 85/100\n",
            "20000/20000 - 25s - loss: 6.5155 - mse: 6.5155 - mae: 1.3624 - 25s/epoch - 1ms/step\n",
            "Epoch 86/100\n",
            "20000/20000 - 25s - loss: 7.0414 - mse: 7.0414 - mae: 1.3606 - 25s/epoch - 1ms/step\n",
            "Epoch 87/100\n",
            "20000/20000 - 25s - loss: 6.4356 - mse: 6.4356 - mae: 1.3569 - 25s/epoch - 1ms/step\n",
            "Epoch 88/100\n",
            "20000/20000 - 25s - loss: 6.6331 - mse: 6.6331 - mae: 1.3595 - 25s/epoch - 1ms/step\n",
            "Epoch 89/100\n",
            "20000/20000 - 25s - loss: 6.7730 - mse: 6.7730 - mae: 1.3568 - 25s/epoch - 1ms/step\n",
            "Epoch 90/100\n",
            "20000/20000 - 25s - loss: 6.4960 - mse: 6.4960 - mae: 1.3547 - 25s/epoch - 1ms/step\n",
            "Epoch 91/100\n",
            "20000/20000 - 25s - loss: 6.6741 - mse: 6.6741 - mae: 1.3570 - 25s/epoch - 1ms/step\n",
            "Epoch 92/100\n",
            "20000/20000 - 25s - loss: 6.7631 - mse: 6.7631 - mae: 1.3562 - 25s/epoch - 1ms/step\n",
            "Epoch 93/100\n",
            "20000/20000 - 25s - loss: 6.5691 - mse: 6.5691 - mae: 1.3574 - 25s/epoch - 1ms/step\n",
            "Epoch 94/100\n",
            "20000/20000 - 25s - loss: 6.7395 - mse: 6.7395 - mae: 1.3579 - 25s/epoch - 1ms/step\n",
            "Epoch 95/100\n",
            "20000/20000 - 25s - loss: 6.6368 - mse: 6.6368 - mae: 1.3561 - 25s/epoch - 1ms/step\n",
            "Epoch 96/100\n",
            "20000/20000 - 25s - loss: 6.4519 - mse: 6.4519 - mae: 1.3545 - 25s/epoch - 1ms/step\n",
            "Epoch 97/100\n",
            "20000/20000 - 25s - loss: 6.5499 - mse: 6.5499 - mae: 1.3582 - 25s/epoch - 1ms/step\n",
            "Epoch 98/100\n",
            "20000/20000 - 25s - loss: 6.5586 - mse: 6.5586 - mae: 1.3571 - 25s/epoch - 1ms/step\n",
            "Epoch 99/100\n",
            "20000/20000 - 25s - loss: 7.3563 - mse: 7.3563 - mae: 1.3594 - 25s/epoch - 1ms/step\n",
            "Epoch 100/100\n",
            "20000/20000 - 25s - loss: 6.7530 - mse: 6.7530 - mae: 1.3591 - 25s/epoch - 1ms/step\n",
            "Score for fold 3: loss of 9.945383071899414\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "results = model_nn_best.evaluate(testing, labelsForTest, batch_size=10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QTlrRj1zLccs",
        "outputId": "dbc012b2-2b27-4cc7-fc8c-f026e2de3aeb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10000/10000 [==============================] - 13s 1ms/step - loss: 10.6730 - mse: 10.6730 - mae: 1.4169\n"
          ]
        }
      ]
    }
  ]
}