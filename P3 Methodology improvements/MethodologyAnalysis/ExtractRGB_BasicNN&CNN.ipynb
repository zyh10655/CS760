{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "tE6apRjqvK5L",
        "outputId": "3508eed9-98f4-47c0-98da-045211f80927"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting optuna==0.14.0\n",
            "  Downloading optuna-0.14.0.tar.gz (91 kB)\n",
            "\u001b[K     |████████████████████████████████| 91 kB 4.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: sqlalchemy>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from optuna==0.14.0) (1.4.41)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from optuna==0.14.0) (1.21.6)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from optuna==0.14.0) (1.7.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from optuna==0.14.0) (1.15.0)\n",
            "Collecting cliff\n",
            "  Downloading cliff-3.10.1-py3-none-any.whl (81 kB)\n",
            "\u001b[K     |████████████████████████████████| 81 kB 7.9 MB/s \n",
            "\u001b[?25hCollecting colorlog\n",
            "  Downloading colorlog-6.7.0-py2.py3-none-any.whl (11 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from optuna==0.14.0) (1.3.5)\n",
            "Collecting alembic\n",
            "  Downloading alembic-1.8.1-py3-none-any.whl (209 kB)\n",
            "\u001b[K     |████████████████████████████████| 209 kB 47.0 MB/s \n",
            "\u001b[?25hCollecting typing\n",
            "  Downloading typing-3.7.4.3.tar.gz (78 kB)\n",
            "\u001b[K     |████████████████████████████████| 78 kB 7.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from sqlalchemy>=1.1.0->optuna==0.14.0) (4.12.0)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.7/dist-packages (from sqlalchemy>=1.1.0->optuna==0.14.0) (1.1.3)\n",
            "Collecting Mako\n",
            "  Downloading Mako-1.2.3-py3-none-any.whl (78 kB)\n",
            "\u001b[K     |████████████████████████████████| 78 kB 7.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: importlib-resources in /usr/local/lib/python3.7/dist-packages (from alembic->optuna==0.14.0) (5.9.0)\n",
            "Requirement already satisfied: PrettyTable>=0.7.2 in /usr/local/lib/python3.7/dist-packages (from cliff->optuna==0.14.0) (3.4.1)\n",
            "Collecting pbr!=2.1.0,>=2.0.0\n",
            "  Downloading pbr-5.10.0-py2.py3-none-any.whl (112 kB)\n",
            "\u001b[K     |████████████████████████████████| 112 kB 86.8 MB/s \n",
            "\u001b[?25hCollecting cmd2>=1.0.0\n",
            "  Downloading cmd2-2.4.2-py3-none-any.whl (147 kB)\n",
            "\u001b[K     |████████████████████████████████| 147 kB 63.5 MB/s \n",
            "\u001b[?25hCollecting stevedore>=2.0.1\n",
            "  Downloading stevedore-3.5.0-py3-none-any.whl (49 kB)\n",
            "\u001b[K     |████████████████████████████████| 49 kB 5.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pyparsing>=2.1.0 in /usr/local/lib/python3.7/dist-packages (from cliff->optuna==0.14.0) (3.0.9)\n",
            "Collecting autopage>=0.4.0\n",
            "  Downloading autopage-0.5.1-py3-none-any.whl (29 kB)\n",
            "Requirement already satisfied: PyYAML>=3.12 in /usr/local/lib/python3.7/dist-packages (from cliff->optuna==0.14.0) (6.0)\n",
            "Collecting pyperclip>=1.6\n",
            "  Downloading pyperclip-1.8.2.tar.gz (20 kB)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from cmd2>=1.0.0->cliff->optuna==0.14.0) (4.1.1)\n",
            "Requirement already satisfied: wcwidth>=0.1.7 in /usr/local/lib/python3.7/dist-packages (from cmd2>=1.0.0->cliff->optuna==0.14.0) (0.2.5)\n",
            "Requirement already satisfied: attrs>=16.3.0 in /usr/local/lib/python3.7/dist-packages (from cmd2>=1.0.0->cliff->optuna==0.14.0) (22.1.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->sqlalchemy>=1.1.0->optuna==0.14.0) (3.8.1)\n",
            "Requirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.7/dist-packages (from Mako->alembic->optuna==0.14.0) (2.0.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->optuna==0.14.0) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas->optuna==0.14.0) (2022.2.1)\n",
            "Building wheels for collected packages: optuna, pyperclip, typing\n",
            "  Building wheel for optuna (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for optuna: filename=optuna-0.14.0-py3-none-any.whl size=125709 sha256=83d3b5ff91bdbf1eb9ef5a72580f38d80574890171d565ec402fb39445c77219\n",
            "  Stored in directory: /root/.cache/pip/wheels/c1/41/64/03b183676c5d5e978de160cab6268d5b4fb095dff63f720e01\n",
            "  Building wheel for pyperclip (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyperclip: filename=pyperclip-1.8.2-py3-none-any.whl size=11137 sha256=bd2cf62c5ca3b6a42900b29264bff4c258e657024607754ee0aab0a12b362331\n",
            "  Stored in directory: /root/.cache/pip/wheels/9f/18/84/8f69f8b08169c7bae2dde6bd7daf0c19fca8c8e500ee620a28\n",
            "  Building wheel for typing (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for typing: filename=typing-3.7.4.3-py3-none-any.whl size=26325 sha256=f58279722d99b25fb6c779ad7281da98688ff6e4680b43b005238c60d0dcd135\n",
            "  Stored in directory: /root/.cache/pip/wheels/35/f3/15/01aa6571f0a72ee6ae7b827c1491c37a1f72d686fd22b43b0e\n",
            "Successfully built optuna pyperclip typing\n",
            "Installing collected packages: pyperclip, pbr, stevedore, Mako, cmd2, autopage, typing, colorlog, cliff, alembic, optuna\n",
            "Successfully installed Mako-1.2.3 alembic-1.8.1 autopage-0.5.1 cliff-3.10.1 cmd2-2.4.2 colorlog-6.7.0 optuna-0.14.0 pbr-5.10.0 pyperclip-1.8.2 stevedore-3.5.0 typing-3.7.4.3\n"
          ]
        },
        {
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "typing"
                ]
              }
            }
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting optkeras==0.0.7\n",
            "  Downloading optkeras-0.0.7-py3-none-any.whl (6.9 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from optkeras==0.0.7) (1.21.6)\n",
            "Requirement already satisfied: optuna>=0.9.0 in /usr/local/lib/python3.7/dist-packages (from optkeras==0.0.7) (0.14.0)\n",
            "Requirement already satisfied: keras in /usr/local/lib/python3.7/dist-packages (from optkeras==0.0.7) (2.8.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from optuna>=0.9.0->optkeras==0.0.7) (1.3.5)\n",
            "Requirement already satisfied: colorlog in /usr/local/lib/python3.7/dist-packages (from optuna>=0.9.0->optkeras==0.0.7) (6.7.0)\n",
            "Requirement already satisfied: sqlalchemy>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from optuna>=0.9.0->optkeras==0.0.7) (1.4.41)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from optuna>=0.9.0->optkeras==0.0.7) (1.7.3)\n",
            "Requirement already satisfied: alembic in /usr/local/lib/python3.7/dist-packages (from optuna>=0.9.0->optkeras==0.0.7) (1.8.1)\n",
            "Requirement already satisfied: typing in /usr/local/lib/python3.7/dist-packages (from optuna>=0.9.0->optkeras==0.0.7) (3.7.4.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from optuna>=0.9.0->optkeras==0.0.7) (1.15.0)\n",
            "Requirement already satisfied: cliff in /usr/local/lib/python3.7/dist-packages (from optuna>=0.9.0->optkeras==0.0.7) (3.10.1)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.7/dist-packages (from sqlalchemy>=1.1.0->optuna>=0.9.0->optkeras==0.0.7) (1.1.3)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from sqlalchemy>=1.1.0->optuna>=0.9.0->optkeras==0.0.7) (4.12.0)\n",
            "Requirement already satisfied: Mako in /usr/local/lib/python3.7/dist-packages (from alembic->optuna>=0.9.0->optkeras==0.0.7) (1.2.3)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.7/dist-packages (from alembic->optuna>=0.9.0->optkeras==0.0.7) (5.9.0)\n",
            "Requirement already satisfied: cmd2>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from cliff->optuna>=0.9.0->optkeras==0.0.7) (2.4.2)\n",
            "Requirement already satisfied: autopage>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from cliff->optuna>=0.9.0->optkeras==0.0.7) (0.5.1)\n",
            "Requirement already satisfied: pbr!=2.1.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from cliff->optuna>=0.9.0->optkeras==0.0.7) (5.10.0)\n",
            "Requirement already satisfied: PyYAML>=3.12 in /usr/local/lib/python3.7/dist-packages (from cliff->optuna>=0.9.0->optkeras==0.0.7) (6.0)\n",
            "Requirement already satisfied: PrettyTable>=0.7.2 in /usr/local/lib/python3.7/dist-packages (from cliff->optuna>=0.9.0->optkeras==0.0.7) (3.4.1)\n",
            "Requirement already satisfied: pyparsing>=2.1.0 in /usr/local/lib/python3.7/dist-packages (from cliff->optuna>=0.9.0->optkeras==0.0.7) (3.0.9)\n",
            "Requirement already satisfied: stevedore>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from cliff->optuna>=0.9.0->optkeras==0.0.7) (3.5.0)\n",
            "Requirement already satisfied: wcwidth>=0.1.7 in /usr/local/lib/python3.7/dist-packages (from cmd2>=1.0.0->cliff->optuna>=0.9.0->optkeras==0.0.7) (0.2.5)\n",
            "Requirement already satisfied: pyperclip>=1.6 in /usr/local/lib/python3.7/dist-packages (from cmd2>=1.0.0->cliff->optuna>=0.9.0->optkeras==0.0.7) (1.8.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from cmd2>=1.0.0->cliff->optuna>=0.9.0->optkeras==0.0.7) (4.1.1)\n",
            "Requirement already satisfied: attrs>=16.3.0 in /usr/local/lib/python3.7/dist-packages (from cmd2>=1.0.0->cliff->optuna>=0.9.0->optkeras==0.0.7) (22.1.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->sqlalchemy>=1.1.0->optuna>=0.9.0->optkeras==0.0.7) (3.8.1)\n",
            "Requirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.7/dist-packages (from Mako->alembic->optuna>=0.9.0->optkeras==0.0.7) (2.0.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->optuna>=0.9.0->optkeras==0.0.7) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas->optuna>=0.9.0->optkeras==0.0.7) (2022.2.1)\n",
            "Installing collected packages: optkeras\n",
            "Successfully installed optkeras-0.0.7\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting fastparquet\n",
            "  Downloading fastparquet-0.8.1-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.5 MB 5.2 MB/s \n",
            "\u001b[?25hCollecting cramjam>=2.3.0\n",
            "  Downloading cramjam-2.5.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.whl (1.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.6 MB 78.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pandas>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from fastparquet) (1.3.5)\n",
            "Requirement already satisfied: numpy>=1.18 in /usr/local/lib/python3.7/dist-packages (from fastparquet) (1.21.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.7/dist-packages (from fastparquet) (2022.8.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=1.1.0->fastparquet) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=1.1.0->fastparquet) (2022.2.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas>=1.1.0->fastparquet) (1.15.0)\n",
            "Installing collected packages: cramjam, fastparquet\n",
            "Successfully installed cramjam-2.5.0 fastparquet-0.8.1\n"
          ]
        }
      ],
      "source": [
        "!pip3 install optuna==0.14.0\n",
        "!pip3 install optkeras==0.0.7\n",
        "!pip install fastparquet"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZMyiobnqhI3o"
      },
      "source": [
        "# New part"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "szDiNFwDhM3O"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import math\n",
        "from keras.datasets import mnist\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Activation, Flatten, Dense, Conv2D, Conv1D\n",
        "from keras.layers import MaxPooling1D, Dropout, BatchNormalization\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.optimizers import SGD, Adagrad, RMSprop, Adam, Adadelta, Adamax, Nadam\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.model_selection import cross_val_score\n",
        "import keras.backend as K"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tDmClStahORU",
        "outputId": "d3c87ef3-ad8f-4694-e093-fa2d3cf4bd31"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Keras 2.8.0\n",
            "TensorFlow 2.8.2\n",
            "Optuna 0.14.0\n",
            "OptKeras 0.0.7\n"
          ]
        }
      ],
      "source": [
        "import keras\n",
        "print('Keras', keras.__version__)\n",
        "\n",
        "import tensorflow as tf\n",
        "print('TensorFlow', tf.__version__)\n",
        "\n",
        "# import Optuna and OptKeras after Keras\n",
        "import optuna \n",
        "print('Optuna', optuna.__version__)\n",
        "\n",
        "from optkeras.optkeras import OptKeras\n",
        "import optkeras\n",
        "print('OptKeras', optkeras.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ySCh6csLhoSs"
      },
      "outputs": [],
      "source": [
        "#train_df = pd.read_parquet(\"/content/drive/MyDrive/New Data/train_main.parquet.snappy\",engine='fastparquet')\n",
        "#test_df = pd.read_parquet(\"/content/drive/MyDrive/New Data/val_main.parquet.snappy\",engine='fastparquet')\n",
        "#val_df = pd.read_parquet(\"/content/drive/MyDrive/New Data/test_main.parquet.snappy\",engine='fastparquet')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 165
        },
        "id": "wpr47Zpnz2lN",
        "outputId": "8a08eef6-c38a-483c-fc4e-40c760d8cc24"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-ea99a0224e67>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'train_df' is not defined"
          ]
        }
      ],
      "source": [
        "train_df.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wthpsBwyTGkX"
      },
      "source": [
        "##New Part for data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LxqbUHqhTayz"
      },
      "outputs": [],
      "source": [
        "train_df = pd.read_parquet(\"/content/drive/MyDrive/New Data/sub_data/train_main.parquet.snappy\",engine='fastparquet')\n",
        "test_df = pd.read_parquet(\"/content/drive/MyDrive/New Data/sub_data/test_main.parquet.snappy\",engine='fastparquet')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "0zuFGFtNRbrq",
        "outputId": "0d39fbff-e380-47b4-ef5a-1ec3e6071bed"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "          r_id  r_stars  r_stars_square  r_length  u_friends_count  \\\n",
              "0      4195320        4              16        86              163   \n",
              "1      4406379        5              25        28               18   \n",
              "2      1172072        5              25        76               69   \n",
              "3      1949778        1               1       186               23   \n",
              "4      4377517        5              25        57                1   \n",
              "...        ...      ...             ...       ...              ...   \n",
              "99995  5373252        5              25        25              288   \n",
              "99996  4500278        5              25        76               21   \n",
              "99997  4981139        5              25        65              236   \n",
              "99998  3032513        1               1        33                2   \n",
              "99999  4839479        3               9        65                1   \n",
              "\n",
              "       u_review_count  u_month_age  b_stars  b_review_count     r_sen  \\\n",
              "0                  40    27.298941      4.0             445  0.117031   \n",
              "1                 115     0.001011      4.0             636 -0.140000   \n",
              "2                   1     0.000002      2.5              15  0.256723   \n",
              "3                   6    23.288288      3.5             185 -0.209252   \n",
              "4                  25    48.728723      4.5             227  0.295040   \n",
              "...               ...          ...      ...             ...       ...   \n",
              "99995               2    24.026242      3.5             117  0.775000   \n",
              "99996              59    44.444338      4.0             532  0.325664   \n",
              "99997             143     4.868817      4.0            7568  0.097821   \n",
              "99998              28     0.002720      3.0              80  0.300000   \n",
              "99999               5     0.760154      4.0              18  0.317217   \n",
              "\n",
              "          r_sub   r_rea  r_useful  \n",
              "0      0.361875   86.10         1  \n",
              "1      0.133333   96.48         3  \n",
              "2      0.355398   69.07         1  \n",
              "3      0.378994   82.54         1  \n",
              "4      0.454762   91.11         1  \n",
              "...         ...     ...       ...  \n",
              "99995  0.662500   64.04         1  \n",
              "99996  0.561008   71.48         1  \n",
              "99997  0.502692   83.66         2  \n",
              "99998  0.306250  100.04         3  \n",
              "99999  0.397006   60.35         2  \n",
              "\n",
              "[100000 rows x 13 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-62fb6e11-8a6d-4589-99d7-a229787f2a49\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>r_id</th>\n",
              "      <th>r_stars</th>\n",
              "      <th>r_stars_square</th>\n",
              "      <th>r_length</th>\n",
              "      <th>u_friends_count</th>\n",
              "      <th>u_review_count</th>\n",
              "      <th>u_month_age</th>\n",
              "      <th>b_stars</th>\n",
              "      <th>b_review_count</th>\n",
              "      <th>r_sen</th>\n",
              "      <th>r_sub</th>\n",
              "      <th>r_rea</th>\n",
              "      <th>r_useful</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>4195320</td>\n",
              "      <td>4</td>\n",
              "      <td>16</td>\n",
              "      <td>86</td>\n",
              "      <td>163</td>\n",
              "      <td>40</td>\n",
              "      <td>27.298941</td>\n",
              "      <td>4.0</td>\n",
              "      <td>445</td>\n",
              "      <td>0.117031</td>\n",
              "      <td>0.361875</td>\n",
              "      <td>86.10</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>4406379</td>\n",
              "      <td>5</td>\n",
              "      <td>25</td>\n",
              "      <td>28</td>\n",
              "      <td>18</td>\n",
              "      <td>115</td>\n",
              "      <td>0.001011</td>\n",
              "      <td>4.0</td>\n",
              "      <td>636</td>\n",
              "      <td>-0.140000</td>\n",
              "      <td>0.133333</td>\n",
              "      <td>96.48</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1172072</td>\n",
              "      <td>5</td>\n",
              "      <td>25</td>\n",
              "      <td>76</td>\n",
              "      <td>69</td>\n",
              "      <td>1</td>\n",
              "      <td>0.000002</td>\n",
              "      <td>2.5</td>\n",
              "      <td>15</td>\n",
              "      <td>0.256723</td>\n",
              "      <td>0.355398</td>\n",
              "      <td>69.07</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1949778</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>186</td>\n",
              "      <td>23</td>\n",
              "      <td>6</td>\n",
              "      <td>23.288288</td>\n",
              "      <td>3.5</td>\n",
              "      <td>185</td>\n",
              "      <td>-0.209252</td>\n",
              "      <td>0.378994</td>\n",
              "      <td>82.54</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4377517</td>\n",
              "      <td>5</td>\n",
              "      <td>25</td>\n",
              "      <td>57</td>\n",
              "      <td>1</td>\n",
              "      <td>25</td>\n",
              "      <td>48.728723</td>\n",
              "      <td>4.5</td>\n",
              "      <td>227</td>\n",
              "      <td>0.295040</td>\n",
              "      <td>0.454762</td>\n",
              "      <td>91.11</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>99995</th>\n",
              "      <td>5373252</td>\n",
              "      <td>5</td>\n",
              "      <td>25</td>\n",
              "      <td>25</td>\n",
              "      <td>288</td>\n",
              "      <td>2</td>\n",
              "      <td>24.026242</td>\n",
              "      <td>3.5</td>\n",
              "      <td>117</td>\n",
              "      <td>0.775000</td>\n",
              "      <td>0.662500</td>\n",
              "      <td>64.04</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>99996</th>\n",
              "      <td>4500278</td>\n",
              "      <td>5</td>\n",
              "      <td>25</td>\n",
              "      <td>76</td>\n",
              "      <td>21</td>\n",
              "      <td>59</td>\n",
              "      <td>44.444338</td>\n",
              "      <td>4.0</td>\n",
              "      <td>532</td>\n",
              "      <td>0.325664</td>\n",
              "      <td>0.561008</td>\n",
              "      <td>71.48</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>99997</th>\n",
              "      <td>4981139</td>\n",
              "      <td>5</td>\n",
              "      <td>25</td>\n",
              "      <td>65</td>\n",
              "      <td>236</td>\n",
              "      <td>143</td>\n",
              "      <td>4.868817</td>\n",
              "      <td>4.0</td>\n",
              "      <td>7568</td>\n",
              "      <td>0.097821</td>\n",
              "      <td>0.502692</td>\n",
              "      <td>83.66</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>99998</th>\n",
              "      <td>3032513</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>33</td>\n",
              "      <td>2</td>\n",
              "      <td>28</td>\n",
              "      <td>0.002720</td>\n",
              "      <td>3.0</td>\n",
              "      <td>80</td>\n",
              "      <td>0.300000</td>\n",
              "      <td>0.306250</td>\n",
              "      <td>100.04</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>99999</th>\n",
              "      <td>4839479</td>\n",
              "      <td>3</td>\n",
              "      <td>9</td>\n",
              "      <td>65</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>0.760154</td>\n",
              "      <td>4.0</td>\n",
              "      <td>18</td>\n",
              "      <td>0.317217</td>\n",
              "      <td>0.397006</td>\n",
              "      <td>60.35</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>100000 rows × 13 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-62fb6e11-8a6d-4589-99d7-a229787f2a49')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-62fb6e11-8a6d-4589-99d7-a229787f2a49 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-62fb6e11-8a6d-4589-99d7-a229787f2a49');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tlQt5Ca7T2tU"
      },
      "outputs": [],
      "source": [
        "# prepare the cross-validation procedure\n",
        "kfold = KFold(n_splits=3, shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QgHax3O8ztLY"
      },
      "outputs": [],
      "source": [
        "training=train_df.drop(labels=\"r_id\", axis=1)\n",
        "testing=test_df.drop(labels=\"r_id\", axis=1)\n",
        "#validing=val_df.drop(labels=\"r_id\", axis=1)\n",
        "\n",
        "training = training.iloc[: , 0:11]\n",
        "testing = testing.iloc[: , 0:11]\n",
        "#validing = validing.iloc[: , 0:11]\n",
        "\n",
        "#labelsForTrain=train_df.iloc[: , -1]\n",
        "#labelsForValid=test_df.iloc[: , -1]\n",
        "#labelsForTest=val_df.iloc[: , -1]\n",
        "#input_shape = training.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q8nOUgdK0Xr9"
      },
      "outputs": [],
      "source": [
        "#scaler = StandardScaler()\n",
        "\n",
        "#std_train_df2=scaler.fit_transform(training)\n",
        "#std_test_df2=scaler.transform(testing)\n",
        "#std_val_df2=scaler.transform(validing)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "q5iof-AGhtYz",
        "outputId": "eabc444c-c915-49a1-abbb-b720d189447a"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-77ff6f87-9b0c-4215-8411-d7287bde70d0\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>r_stars</th>\n",
              "      <th>r_stars_square</th>\n",
              "      <th>r_length</th>\n",
              "      <th>u_friends_count</th>\n",
              "      <th>u_review_count</th>\n",
              "      <th>u_month_age</th>\n",
              "      <th>b_stars</th>\n",
              "      <th>b_review_count</th>\n",
              "      <th>r_sen</th>\n",
              "      <th>r_sub</th>\n",
              "      <th>r_rea</th>\n",
              "      <th>r_useful</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>-1.694060</td>\n",
              "      <td>-1.494530</td>\n",
              "      <td>-0.297754</td>\n",
              "      <td>-0.076615</td>\n",
              "      <td>0.335185</td>\n",
              "      <td>-1.101172</td>\n",
              "      <td>-0.308494</td>\n",
              "      <td>-0.472926</td>\n",
              "      <td>-0.015873</td>\n",
              "      <td>0.529586</td>\n",
              "      <td>0.546201</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.904508</td>\n",
              "      <td>0.998977</td>\n",
              "      <td>1.233089</td>\n",
              "      <td>-0.329431</td>\n",
              "      <td>-0.373350</td>\n",
              "      <td>-0.300852</td>\n",
              "      <td>0.322022</td>\n",
              "      <td>0.958189</td>\n",
              "      <td>0.320463</td>\n",
              "      <td>0.549522</td>\n",
              "      <td>0.969516</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>-1.694060</td>\n",
              "      <td>-1.494530</td>\n",
              "      <td>0.798190</td>\n",
              "      <td>0.176200</td>\n",
              "      <td>-0.379321</td>\n",
              "      <td>-0.069431</td>\n",
              "      <td>-0.308494</td>\n",
              "      <td>-0.052545</td>\n",
              "      <td>-0.090963</td>\n",
              "      <td>0.533279</td>\n",
              "      <td>-0.170954</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.904508</td>\n",
              "      <td>0.998977</td>\n",
              "      <td>0.572043</td>\n",
              "      <td>-0.012104</td>\n",
              "      <td>-0.245973</td>\n",
              "      <td>-1.223231</td>\n",
              "      <td>0.322022</td>\n",
              "      <td>0.173580</td>\n",
              "      <td>0.217308</td>\n",
              "      <td>0.556899</td>\n",
              "      <td>0.192674</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.904508</td>\n",
              "      <td>0.998977</td>\n",
              "      <td>-0.854423</td>\n",
              "      <td>-0.329431</td>\n",
              "      <td>-0.377331</td>\n",
              "      <td>-0.107964</td>\n",
              "      <td>-2.200042</td>\n",
              "      <td>-0.497208</td>\n",
              "      <td>0.408333</td>\n",
              "      <td>0.608333</td>\n",
              "      <td>-0.053417</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>399995</th>\n",
              "      <td>-1.044418</td>\n",
              "      <td>-1.182842</td>\n",
              "      <td>7.713075</td>\n",
              "      <td>-0.285842</td>\n",
              "      <td>-0.315632</td>\n",
              "      <td>0.268719</td>\n",
              "      <td>-0.939010</td>\n",
              "      <td>-0.465338</td>\n",
              "      <td>-0.025891</td>\n",
              "      <td>0.270243</td>\n",
              "      <td>1.025529</td>\n",
              "      <td>10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>399996</th>\n",
              "      <td>-1.694060</td>\n",
              "      <td>-1.494530</td>\n",
              "      <td>-0.341243</td>\n",
              "      <td>-0.179485</td>\n",
              "      <td>-0.377331</td>\n",
              "      <td>-0.135109</td>\n",
              "      <td>0.952538</td>\n",
              "      <td>-0.459267</td>\n",
              "      <td>0.046314</td>\n",
              "      <td>0.397489</td>\n",
              "      <td>0.099013</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>399997</th>\n",
              "      <td>-1.044418</td>\n",
              "      <td>-1.182842</td>\n",
              "      <td>-0.428223</td>\n",
              "      <td>-0.315482</td>\n",
              "      <td>-0.216119</td>\n",
              "      <td>-0.683035</td>\n",
              "      <td>-1.569526</td>\n",
              "      <td>-0.460785</td>\n",
              "      <td>0.231790</td>\n",
              "      <td>0.673650</td>\n",
              "      <td>-0.509789</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>399998</th>\n",
              "      <td>-1.694060</td>\n",
              "      <td>-1.494530</td>\n",
              "      <td>0.084957</td>\n",
              "      <td>-0.329431</td>\n",
              "      <td>-0.379321</td>\n",
              "      <td>-0.425276</td>\n",
              "      <td>-1.569526</td>\n",
              "      <td>-0.298400</td>\n",
              "      <td>0.029268</td>\n",
              "      <td>0.306944</td>\n",
              "      <td>1.904296</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>399999</th>\n",
              "      <td>-1.694060</td>\n",
              "      <td>-1.494530</td>\n",
              "      <td>0.128447</td>\n",
              "      <td>-0.329431</td>\n",
              "      <td>-0.375340</td>\n",
              "      <td>-1.237572</td>\n",
              "      <td>-2.830558</td>\n",
              "      <td>-0.453197</td>\n",
              "      <td>0.007870</td>\n",
              "      <td>0.375938</td>\n",
              "      <td>0.615070</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>400000 rows × 12 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-77ff6f87-9b0c-4215-8411-d7287bde70d0')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-77ff6f87-9b0c-4215-8411-d7287bde70d0 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-77ff6f87-9b0c-4215-8411-d7287bde70d0');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "         r_stars  r_stars_square  r_length  u_friends_count  u_review_count  \\\n",
              "0      -1.694060       -1.494530 -0.297754        -0.076615        0.335185   \n",
              "1       0.904508        0.998977  1.233089        -0.329431       -0.373350   \n",
              "2      -1.694060       -1.494530  0.798190         0.176200       -0.379321   \n",
              "3       0.904508        0.998977  0.572043        -0.012104       -0.245973   \n",
              "4       0.904508        0.998977 -0.854423        -0.329431       -0.377331   \n",
              "...          ...             ...       ...              ...             ...   \n",
              "399995 -1.044418       -1.182842  7.713075        -0.285842       -0.315632   \n",
              "399996 -1.694060       -1.494530 -0.341243        -0.179485       -0.377331   \n",
              "399997 -1.044418       -1.182842 -0.428223        -0.315482       -0.216119   \n",
              "399998 -1.694060       -1.494530  0.084957        -0.329431       -0.379321   \n",
              "399999 -1.694060       -1.494530  0.128447        -0.329431       -0.375340   \n",
              "\n",
              "        u_month_age   b_stars  b_review_count     r_sen     r_sub     r_rea  \\\n",
              "0         -1.101172 -0.308494       -0.472926 -0.015873  0.529586  0.546201   \n",
              "1         -0.300852  0.322022        0.958189  0.320463  0.549522  0.969516   \n",
              "2         -0.069431 -0.308494       -0.052545 -0.090963  0.533279 -0.170954   \n",
              "3         -1.223231  0.322022        0.173580  0.217308  0.556899  0.192674   \n",
              "4         -0.107964 -2.200042       -0.497208  0.408333  0.608333 -0.053417   \n",
              "...             ...       ...             ...       ...       ...       ...   \n",
              "399995     0.268719 -0.939010       -0.465338 -0.025891  0.270243  1.025529   \n",
              "399996    -0.135109  0.952538       -0.459267  0.046314  0.397489  0.099013   \n",
              "399997    -0.683035 -1.569526       -0.460785  0.231790  0.673650 -0.509789   \n",
              "399998    -0.425276 -1.569526       -0.298400  0.029268  0.306944  1.904296   \n",
              "399999    -1.237572 -2.830558       -0.453197  0.007870  0.375938  0.615070   \n",
              "\n",
              "        r_useful  \n",
              "0              3  \n",
              "1              1  \n",
              "2              1  \n",
              "3              1  \n",
              "4              2  \n",
              "...          ...  \n",
              "399995        10  \n",
              "399996         1  \n",
              "399997         1  \n",
              "399998         1  \n",
              "399999         1  \n",
              "\n",
              "[400000 rows x 12 columns]"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "scaler = StandardScaler()\n",
        "\n",
        "# scale skewed and target features\n",
        "std_train_df = train_df.copy(deep=True)\n",
        "std_train_df[['r_stars','r_stars_square','r_length', 'u_friends_count', 'u_review_count', 'u_month_age', 'b_stars','b_review_count','r_rea']] = scaler.fit_transform(train_df[['r_stars','r_stars_square','r_length', 'u_friends_count', 'u_review_count', 'u_month_age', 'b_stars','b_review_count','r_rea']])\n",
        "std_test_df = test_df.copy(deep=True)\n",
        "std_test_df[['r_stars','r_stars_square','r_length', 'u_friends_count', 'u_review_count', 'u_month_age', 'b_stars','b_review_count','r_rea']] = scaler.transform(test_df[['r_stars','r_stars_square','r_length', 'u_friends_count', 'u_review_count', 'u_month_age', 'b_stars','b_review_count','r_rea']])\n",
        "#std_val_df = val_df.copy(deep=True)\n",
        "#std_val_df[['r_stars','r_stars_square','r_length', 'u_friends_count', 'u_review_count', 'u_month_age', 'b_stars','b_review_count','r_rea']] = scaler.transform(val_df[['r_stars','r_stars_square','r_length', 'u_friends_count', 'u_review_count', 'u_month_age', 'b_stars','b_review_count','r_rea']])\n",
        "\n",
        "std_train_df=std_train_df.drop(labels=\"r_id\", axis=1)\n",
        "std_test_df=std_test_df.drop(labels=\"r_id\", axis=1)\n",
        "#std_val_df=std_val_df.drop(labels=\"r_id\", axis=1)\n",
        "\n",
        "#std_train_df=scaler.fit_transform(std_train_df)\n",
        "#std_test_df=scaler.transform(std_test_df)\n",
        "#std_val_df=scaler.transform(std_val_df)\n",
        "\n",
        "std_train_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jxAu6yX7mHUX",
        "outputId": "ab102493-38f2-42be-dad3-ab3207df7287"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(400000, 12)"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "std_train_df.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PdFnHUOLjxy7"
      },
      "outputs": [],
      "source": [
        "training = std_train_df.iloc[: , 0:11]\n",
        "testing = std_test_df.iloc[: , 0:11]\n",
        "#validing = std_val_df.iloc[: , 0:11]\n",
        "labelsForTrain=std_train_df.iloc[: , -1]\n",
        "#labelsForValid=std_val_df.iloc[: , -1]\n",
        "labelsForTest=std_test_df.iloc[: , -1]\n",
        "input_shape = training.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KvNXnVaATPN-",
        "outputId": "0313d555-08dc-4e24-f575-52162262e027"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(400000, 11)"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "training.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8mq1FzisZ7Dq"
      },
      "source": [
        "# Another test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xhJmENUMZ8SQ"
      },
      "outputs": [],
      "source": [
        "from keras.layers import Input\n",
        "from keras.layers.core import *\n",
        "from tensorflow.keras.optimizers import Adam, Adagrad, RMSprop, SGD\n",
        "from keras.models import Model\n",
        "from keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "import keras.backend as K\n",
        "import optuna\n",
        "import math"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "86PZeFURZakh"
      },
      "outputs": [],
      "source": [
        "def create_model(activation, num_hidden_layer, num_hidden_unit):\n",
        "  inputs = Input(shape=(training.shape[1],))\n",
        "  model = inputs\n",
        "  for i in range(1,num_hidden_layer):\n",
        "    model = Dense(num_hidden_unit, activation=activation,)(model)\n",
        "        \n",
        "        \n",
        "  model = Dense(1,)(model)\n",
        "  model = Model(inputs, model)\n",
        "\n",
        "  return model\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dXhj8IZGu1-Q"
      },
      "outputs": [],
      "source": [
        "#def create_model_cnn(activation, num_hidden_layer, num_hidden_unit, kernel_size, filter):\n",
        "#  inputs = Input(shape=(training.shape[1],))\n",
        "#  model = inputs\n",
        "#  model = Sequential()\n",
        "#  for j in range(1):\n",
        "#    model = Conv1D(filter, kernel_size, activation=activation, input_shape=(11,1))\n",
        "#    model = Flatten()\n",
        "#    for i in range(1,num_hidden_layer):\n",
        "#      model = Dense(num_hidden_unit, activation=activation,)(model)\n",
        "#  model = Dense(1, activation='relu')(model)\n",
        "#  model = Model(inputs, model)\n",
        "\n",
        "#  return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RSd27Yt-2rFw"
      },
      "outputs": [],
      "source": [
        "def create_model_cnn(activation, num_hidden_layer, num_hidden_unit, kernel_size, filter):\n",
        "  model_cnn = Sequential()\n",
        "  model_cnn.add(Conv1D(filter, kernel_size, activation=activation, input_shape=(11,1)))\n",
        "  model_cnn.add(MaxPooling1D())\n",
        "  model_cnn.add(Flatten())\n",
        "  for i in range(1,num_hidden_layer):\n",
        "    model_cnn.add(Dense(num_hidden_unit, activation=activation,))\n",
        "\n",
        "  model_cnn.add(Dense(1))\n",
        "\n",
        "  return model_cnn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6JOb2VDAvIcT"
      },
      "outputs": [],
      "source": [
        "def objective_cnn(trial):\n",
        "  K.clear_session()\n",
        "    \n",
        "  activation = trial.suggest_categorical('activation',['relu','tanh','linear'])\n",
        "  optimizer = trial.suggest_categorical('optimizer',['adam','rmsprop','adagrad', 'sgd'])\n",
        "    \n",
        "  num_hidden_layer = trial.suggest_int('num_hidden_layer',1,5)\n",
        "  num_hidden_unit = trial.suggest_int('num_hidden_unit',10,100)\n",
        "  kernel_size = trial.suggest_int('kernel_size',1,5)\n",
        "  filter = trial.suggest_int('filter',1,100)\n",
        "    \n",
        "  learning_rate = trial.suggest_loguniform('learning_rate', 0.00001,0.1)\n",
        "  if optimizer == 'adam':\n",
        "    optimizer = Adam(learning_rate=learning_rate)\n",
        "  elif optimizer == 'adagrad':\n",
        "    optimizer = Adagrad(learning_rate=learning_rate)\n",
        "  elif optimizer =='rmsprop':\n",
        "    optimizer = RMSprop(learning_rate=learning_rate)\n",
        "  elif optimizer =='sgd':\n",
        "    optimizer = SGD(learning_rate=learning_rate)\n",
        "    \n",
        "  num_folds = 3\n",
        "  kfold=KFold(n_splits=3,shuffle=True)\n",
        "  fold_no=1\n",
        "  loss_per_fold = []\n",
        "  es = EarlyStopping(monitor='mse', patience=50)\n",
        "  model = create_model_cnn(activation, num_hidden_layer, num_hidden_unit, kernel_size, filter)\n",
        "  model_list_cnn.append(model)\n",
        "  model.compile(loss='mse',optimizer=optimizer,metrics=['mse','mae'])\n",
        "  for train,test in kfold.split(training,labelsForTrain):\n",
        "    scores=model.evaluate(testing,labelsForTest,verbose=0)\n",
        "\n",
        "    print('------------------------------------------------------------------------')\n",
        "    print(f'Training for fold {fold_no} ...')\n",
        "  \n",
        "    # Fit data to model\n",
        "    history = model.fit(training, labelsForTrain,\n",
        "                batch_size=20,\n",
        "                epochs=10,\n",
        "                verbose=2,\n",
        "                callbacks=[es])\n",
        "    \n",
        "    print(f'Score for fold {fold_no}: {model.metrics_names[0]} of {scores[0]}')\n",
        "    loss_per_fold.append(scores[0])\n",
        "    # Increase fold number\n",
        "    fold_no = fold_no + 1  \n",
        "  \n",
        "  history_list_cnn.append(history)\n",
        "    \n",
        "  mse = np.array(history.history['mse'])\n",
        "    \n",
        "  return mse[-1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FmIRzoaSZf44"
      },
      "outputs": [],
      "source": [
        "def objective(trial):\n",
        "  K.clear_session()\n",
        "    \n",
        "  activation = trial.suggest_categorical('activation',['relu','tanh','linear'])\n",
        "  optimizer = trial.suggest_categorical('optimizer',['adam','rmsprop','adagrad', 'sgd'])\n",
        "    \n",
        "  num_hidden_layer = trial.suggest_int('num_hidden_layer',1,3)\n",
        "  num_hidden_unit = trial.suggest_int('num_hidden_unit',10,500)\n",
        "    \n",
        "  learning_rate = trial.suggest_loguniform('learning_rate', 0.00001,0.1)\n",
        "  if optimizer == 'adam':\n",
        "    optimizer = Adam(learning_rate=learning_rate)\n",
        "  elif optimizer == 'adagrad':\n",
        "    optimizer = Adagrad(learning_rate=learning_rate)\n",
        "  elif optimizer =='rmsprop':\n",
        "    optimizer = RMSprop(learning_rate=learning_rate)\n",
        "  elif optimizer =='sgd':\n",
        "    optimizer = SGD(learning_rate=learning_rate)\n",
        "    \n",
        "  num_folds = 3\n",
        "  kfold=KFold(n_splits=3,shuffle=True)\n",
        "  fold_no=1\n",
        "  loss_per_fold = []\n",
        "  es = EarlyStopping(monitor='mse', patience=50)\n",
        "  model = create_model(activation, num_hidden_layer, num_hidden_unit)\n",
        "  model_list.append(model)\n",
        "  model.compile(loss='mse',optimizer=optimizer,metrics=['mse','mae'])\n",
        "  for train,test in kfold.split(training,labelsForTrain):\n",
        "    scores=model.evaluate(testing,labelsForTest,verbose=0)\n",
        "\n",
        "    print('------------------------------------------------------------------------')\n",
        "    print(f'Training for fold {fold_no} ...')\n",
        "  \n",
        "    # Fit data to model\n",
        "    history = model.fit(training, labelsForTrain,\n",
        "                batch_size=20,\n",
        "                epochs=10,\n",
        "                verbose=2,\n",
        "                callbacks=[es])\n",
        "    \n",
        "    print(f'Score for fold {fold_no}: {model.metrics_names[0]} of {scores[0]}')\n",
        "    loss_per_fold.append(scores[0])\n",
        "    # Increase fold number\n",
        "    fold_no = fold_no + 1  \n",
        "  \n",
        "  history_list.append(history)\n",
        "    \n",
        "  mse = np.array(history.history['mse'])\n",
        "    \n",
        "  return mse[-1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NoNXzvs0viW3",
        "outputId": "0135e568-0188-47d9-ae34-4a512a09088a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 31s - loss: 1005.5142 - mse: 1005.5142 - mae: 7.8811 - 31s/epoch - 2ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 30s - loss: 1597.7765 - mse: 1597.7765 - mae: 9.2947 - 30s/epoch - 1ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 30s - loss: 2279.2959 - mse: 2279.2959 - mae: 10.1906 - 30s/epoch - 2ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 32s - loss: 1745.0280 - mse: 1745.0280 - mae: 9.8582 - 32s/epoch - 2ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 31s - loss: 1636.7992 - mse: 1636.7992 - mae: 9.5566 - 31s/epoch - 2ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 30s - loss: 2039.6914 - mse: 2039.6914 - mae: 10.0259 - 30s/epoch - 2ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 29s - loss: 2213.4902 - mse: 2213.4902 - mae: 9.6500 - 29s/epoch - 1ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 28s - loss: 1950.7869 - mse: 1950.7869 - mae: 10.0176 - 28s/epoch - 1ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 28s - loss: 2025.6382 - mse: 2025.6382 - mae: 9.9798 - 28s/epoch - 1ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 29s - loss: 2225.2217 - mse: 2225.2217 - mae: 10.3990 - 29s/epoch - 1ms/step\n",
            "Score for fold 1: loss of 29.56524085998535\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 28s - loss: 1775.6365 - mse: 1775.6365 - mae: 9.6528 - 28s/epoch - 1ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 29s - loss: 2080.5276 - mse: 2080.5276 - mae: 9.8835 - 29s/epoch - 1ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 28s - loss: 2500.8523 - mse: 2500.8523 - mae: 10.2347 - 28s/epoch - 1ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 29s - loss: 2191.0955 - mse: 2191.0955 - mae: 10.0196 - 29s/epoch - 1ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 29s - loss: 2485.1426 - mse: 2485.1426 - mae: 10.3540 - 29s/epoch - 1ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 29s - loss: 1597.3207 - mse: 1597.3207 - mae: 9.7454 - 29s/epoch - 1ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 28s - loss: 1739.4192 - mse: 1739.4192 - mae: 9.5021 - 28s/epoch - 1ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 29s - loss: 2270.3862 - mse: 2270.3862 - mae: 10.5126 - 29s/epoch - 1ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 29s - loss: 2269.9363 - mse: 2269.9363 - mae: 10.2778 - 29s/epoch - 1ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 29s - loss: 2160.0374 - mse: 2160.0374 - mae: 10.2007 - 29s/epoch - 1ms/step\n",
            "Score for fold 2: loss of 28.279115676879883\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 29s - loss: 1973.5537 - mse: 1973.5537 - mae: 10.1837 - 29s/epoch - 1ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 29s - loss: 2469.9470 - mse: 2469.9470 - mae: 10.0596 - 29s/epoch - 1ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 29s - loss: 1785.5845 - mse: 1785.5845 - mae: 9.8333 - 29s/epoch - 1ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 29s - loss: 2449.8276 - mse: 2449.8276 - mae: 10.0769 - 29s/epoch - 1ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 28s - loss: 1652.6462 - mse: 1652.6462 - mae: 9.8204 - 28s/epoch - 1ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 29s - loss: 1865.4723 - mse: 1865.4723 - mae: 9.9695 - 29s/epoch - 1ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 29s - loss: 1727.6580 - mse: 1727.6580 - mae: 9.7626 - 29s/epoch - 1ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 29s - loss: 1515.9337 - mse: 1515.9337 - mae: 9.5271 - 29s/epoch - 1ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 29s - loss: 1963.0820 - mse: 1963.0820 - mae: 10.0651 - 29s/epoch - 1ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 29s - loss: 2390.0183 - mse: 2390.0183 - mae: 9.9826 - 29s/epoch - 1ms/step\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-09-28 21:22:10,524]\u001b[0m Finished trial#0 resulted in value: 2390.018310546875. Current best value is 2390.018310546875 with parameters: {'activation': 'linear', 'optimizer': 'rmsprop', 'num_hidden_layer': 4, 'num_hidden_unit': 79, 'kernel_size': 5, 'filter': 99, 'learning_rate': 0.025805989874184886}.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Score for fold 3: loss of 36.11547088623047\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 23s - loss: 14.6577 - mse: 14.6577 - mae: 1.6834 - 23s/epoch - 1ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 22s - loss: 14.6124 - mse: 14.6124 - mae: 1.6809 - 22s/epoch - 1ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 22s - loss: 14.6114 - mse: 14.6114 - mae: 1.6818 - 22s/epoch - 1ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 21s - loss: 14.6150 - mse: 14.6150 - mae: 1.6818 - 21s/epoch - 1ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 22s - loss: 14.6059 - mse: 14.6059 - mae: 1.6823 - 22s/epoch - 1ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 22s - loss: 14.6023 - mse: 14.6023 - mae: 1.6829 - 22s/epoch - 1ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 22s - loss: 14.6001 - mse: 14.6001 - mae: 1.6830 - 22s/epoch - 1ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 22s - loss: 14.6009 - mse: 14.6009 - mae: 1.6826 - 22s/epoch - 1ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 22s - loss: 14.6004 - mse: 14.6004 - mae: 1.6832 - 22s/epoch - 1ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 21s - loss: 14.6032 - mse: 14.6032 - mae: 1.6827 - 21s/epoch - 1ms/step\n",
            "Score for fold 1: loss of 30.17433738708496\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 21s - loss: 14.5946 - mse: 14.5946 - mae: 1.6839 - 21s/epoch - 1ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 21s - loss: 14.6010 - mse: 14.6010 - mae: 1.6824 - 21s/epoch - 1ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 21s - loss: 14.5968 - mse: 14.5968 - mae: 1.6833 - 21s/epoch - 1ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 21s - loss: 14.5974 - mse: 14.5974 - mae: 1.6830 - 21s/epoch - 1ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 21s - loss: 14.5994 - mse: 14.5994 - mae: 1.6823 - 21s/epoch - 1ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 21s - loss: 14.5984 - mse: 14.5984 - mae: 1.6837 - 21s/epoch - 1ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 21s - loss: 14.5963 - mse: 14.5963 - mae: 1.6830 - 21s/epoch - 1ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 21s - loss: 14.6016 - mse: 14.6016 - mae: 1.6827 - 21s/epoch - 1ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 21s - loss: 14.5989 - mse: 14.5989 - mae: 1.6827 - 21s/epoch - 1ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 21s - loss: 14.5915 - mse: 14.5915 - mae: 1.6836 - 21s/epoch - 1ms/step\n",
            "Score for fold 2: loss of 19.556621551513672\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 21s - loss: 14.6005 - mse: 14.6005 - mae: 1.6830 - 21s/epoch - 1ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 21s - loss: 14.5989 - mse: 14.5989 - mae: 1.6823 - 21s/epoch - 1ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 22s - loss: 14.5957 - mse: 14.5957 - mae: 1.6823 - 22s/epoch - 1ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 21s - loss: 14.5971 - mse: 14.5971 - mae: 1.6829 - 21s/epoch - 1ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 21s - loss: 14.5949 - mse: 14.5949 - mae: 1.6827 - 21s/epoch - 1ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 21s - loss: 14.5949 - mse: 14.5949 - mae: 1.6835 - 21s/epoch - 1ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 21s - loss: 14.5905 - mse: 14.5905 - mae: 1.6830 - 21s/epoch - 1ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 21s - loss: 14.5954 - mse: 14.5954 - mae: 1.6832 - 21s/epoch - 1ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 21s - loss: 14.5916 - mse: 14.5916 - mae: 1.6833 - 21s/epoch - 1ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 21s - loss: 14.5938 - mse: 14.5938 - mae: 1.6828 - 21s/epoch - 1ms/step\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-09-28 21:32:58,263]\u001b[0m Finished trial#1 resulted in value: 14.593823432922363. Current best value is 14.593823432922363 with parameters: {'activation': 'linear', 'optimizer': 'sgd', 'num_hidden_layer': 3, 'num_hidden_unit': 64, 'kernel_size': 1, 'filter': 31, 'learning_rate': 0.0019453864051127093}.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Score for fold 3: loss of 19.58966827392578\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 28s - loss: 14.4640 - mse: 14.4640 - mae: 1.6519 - 28s/epoch - 1ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 28s - loss: 14.3155 - mse: 14.3155 - mae: 1.6486 - 28s/epoch - 1ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 28s - loss: 14.3014 - mse: 14.3015 - mae: 1.6482 - 28s/epoch - 1ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 28s - loss: 14.2870 - mse: 14.2870 - mae: 1.6476 - 28s/epoch - 1ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 28s - loss: 14.2828 - mse: 14.2828 - mae: 1.6472 - 28s/epoch - 1ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 29s - loss: 14.2842 - mse: 14.2842 - mae: 1.6468 - 29s/epoch - 1ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 28s - loss: 14.2805 - mse: 14.2805 - mae: 1.6464 - 28s/epoch - 1ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 28s - loss: 14.2769 - mse: 14.2769 - mae: 1.6463 - 28s/epoch - 1ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 28s - loss: 14.2762 - mse: 14.2762 - mae: 1.6462 - 28s/epoch - 1ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 28s - loss: 14.2788 - mse: 14.2788 - mae: 1.6462 - 28s/epoch - 1ms/step\n",
            "Score for fold 1: loss of 29.130847930908203\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 28s - loss: 14.2740 - mse: 14.2740 - mae: 1.6459 - 28s/epoch - 1ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 28s - loss: 14.2708 - mse: 14.2708 - mae: 1.6461 - 28s/epoch - 1ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 28s - loss: 14.2706 - mse: 14.2706 - mae: 1.6454 - 28s/epoch - 1ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 28s - loss: 14.2674 - mse: 14.2674 - mae: 1.6453 - 28s/epoch - 1ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 27s - loss: 14.2620 - mse: 14.2620 - mae: 1.6450 - 27s/epoch - 1ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 28s - loss: 14.2664 - mse: 14.2664 - mae: 1.6455 - 28s/epoch - 1ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 28s - loss: 14.2627 - mse: 14.2627 - mae: 1.6454 - 28s/epoch - 1ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 28s - loss: 14.2657 - mse: 14.2657 - mae: 1.6444 - 28s/epoch - 1ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 28s - loss: 14.2602 - mse: 14.2602 - mae: 1.6450 - 28s/epoch - 1ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 28s - loss: 14.2646 - mse: 14.2646 - mae: 1.6449 - 28s/epoch - 1ms/step\n",
            "Score for fold 2: loss of 19.316585540771484\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 28s - loss: 14.2588 - mse: 14.2588 - mae: 1.6449 - 28s/epoch - 1ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 28s - loss: 14.2602 - mse: 14.2602 - mae: 1.6449 - 28s/epoch - 1ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 28s - loss: 14.2565 - mse: 14.2565 - mae: 1.6449 - 28s/epoch - 1ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 28s - loss: 14.2590 - mse: 14.2590 - mae: 1.6434 - 28s/epoch - 1ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 28s - loss: 14.2581 - mse: 14.2581 - mae: 1.6445 - 28s/epoch - 1ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 28s - loss: 14.2572 - mse: 14.2572 - mae: 1.6442 - 28s/epoch - 1ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 28s - loss: 14.2559 - mse: 14.2559 - mae: 1.6442 - 28s/epoch - 1ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 28s - loss: 14.2544 - mse: 14.2544 - mae: 1.6442 - 28s/epoch - 1ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 28s - loss: 14.2549 - mse: 14.2549 - mae: 1.6436 - 28s/epoch - 1ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 28s - loss: 14.2513 - mse: 14.2513 - mae: 1.6439 - 28s/epoch - 1ms/step\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-09-28 21:47:01,316]\u001b[0m Finished trial#2 resulted in value: 14.251280784606934. Current best value is 14.251280784606934 with parameters: {'activation': 'linear', 'optimizer': 'adam', 'num_hidden_layer': 2, 'num_hidden_unit': 40, 'kernel_size': 2, 'filter': 69, 'learning_rate': 0.00014783991244109973}.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Score for fold 3: loss of 19.255849838256836\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 21s - loss: 14.6288 - mse: 14.6288 - mae: 1.6748 - 21s/epoch - 1ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 20s - loss: 14.5455 - mse: 14.5455 - mae: 1.6752 - 20s/epoch - 1ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 19s - loss: 14.5420 - mse: 14.5420 - mae: 1.6742 - 19s/epoch - 963us/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 20s - loss: 14.5372 - mse: 14.5372 - mae: 1.6753 - 20s/epoch - 977us/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 20s - loss: 14.5372 - mse: 14.5372 - mae: 1.6739 - 20s/epoch - 984us/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 20s - loss: 14.5333 - mse: 14.5333 - mae: 1.6751 - 20s/epoch - 990us/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 19s - loss: 14.5357 - mse: 14.5357 - mae: 1.6735 - 19s/epoch - 971us/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 20s - loss: 14.5337 - mse: 14.5337 - mae: 1.6742 - 20s/epoch - 989us/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 20s - loss: 14.5340 - mse: 14.5340 - mae: 1.6732 - 20s/epoch - 981us/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 19s - loss: 14.5318 - mse: 14.5318 - mae: 1.6741 - 19s/epoch - 975us/step\n",
            "Score for fold 1: loss of 28.75012969970703\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 19s - loss: 14.5304 - mse: 14.5304 - mae: 1.6742 - 19s/epoch - 968us/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 20s - loss: 14.5315 - mse: 14.5315 - mae: 1.6739 - 20s/epoch - 976us/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 19s - loss: 14.5303 - mse: 14.5303 - mae: 1.6740 - 19s/epoch - 969us/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 19s - loss: 14.5278 - mse: 14.5278 - mae: 1.6737 - 19s/epoch - 955us/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 19s - loss: 14.5279 - mse: 14.5279 - mae: 1.6736 - 19s/epoch - 953us/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 19s - loss: 14.5296 - mse: 14.5296 - mae: 1.6735 - 19s/epoch - 967us/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 20s - loss: 14.5265 - mse: 14.5265 - mae: 1.6734 - 20s/epoch - 989us/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 20s - loss: 14.5268 - mse: 14.5268 - mae: 1.6736 - 20s/epoch - 978us/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 20s - loss: 14.5231 - mse: 14.5231 - mae: 1.6751 - 20s/epoch - 979us/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 20s - loss: 14.5289 - mse: 14.5289 - mae: 1.6728 - 20s/epoch - 977us/step\n",
            "Score for fold 2: loss of 19.54649543762207\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 19s - loss: 14.5250 - mse: 14.5250 - mae: 1.6737 - 19s/epoch - 975us/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 20s - loss: 14.5212 - mse: 14.5212 - mae: 1.6733 - 20s/epoch - 1ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 19s - loss: 14.5257 - mse: 14.5257 - mae: 1.6730 - 19s/epoch - 963us/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 20s - loss: 14.5228 - mse: 14.5228 - mae: 1.6733 - 20s/epoch - 981us/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 20s - loss: 14.5216 - mse: 14.5216 - mae: 1.6744 - 20s/epoch - 977us/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 19s - loss: 14.5243 - mse: 14.5243 - mae: 1.6734 - 19s/epoch - 960us/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 19s - loss: 14.5264 - mse: 14.5264 - mae: 1.6734 - 19s/epoch - 971us/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 20s - loss: 14.5222 - mse: 14.5222 - mae: 1.6741 - 20s/epoch - 1ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 20s - loss: 14.5234 - mse: 14.5234 - mae: 1.6730 - 20s/epoch - 979us/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 20s - loss: 14.5228 - mse: 14.5228 - mae: 1.6737 - 20s/epoch - 979us/step\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-09-28 21:56:56,499]\u001b[0m Finished trial#3 resulted in value: 14.522780418395996. Current best value is 14.251280784606934 with parameters: {'activation': 'linear', 'optimizer': 'adam', 'num_hidden_layer': 2, 'num_hidden_unit': 40, 'kernel_size': 2, 'filter': 69, 'learning_rate': 0.00014783991244109973}.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Score for fold 3: loss of 19.55617904663086\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 21s - loss: 14.9040 - mse: 14.9040 - mae: 1.6641 - 21s/epoch - 1ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 20s - loss: 14.2668 - mse: 14.2668 - mae: 1.6307 - 20s/epoch - 994us/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 20s - loss: 14.1248 - mse: 14.1248 - mae: 1.6230 - 20s/epoch - 1ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 20s - loss: 14.0464 - mse: 14.0464 - mae: 1.6197 - 20s/epoch - 982us/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 20s - loss: 13.9987 - mse: 13.9987 - mae: 1.6182 - 20s/epoch - 1ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 20s - loss: 13.9679 - mse: 13.9679 - mae: 1.6163 - 20s/epoch - 1ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 20s - loss: 13.9424 - mse: 13.9424 - mae: 1.6160 - 20s/epoch - 991us/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 20s - loss: 13.9237 - mse: 13.9237 - mae: 1.6150 - 20s/epoch - 1ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 20s - loss: 13.9081 - mse: 13.9081 - mae: 1.6132 - 20s/epoch - 1ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 20s - loss: 13.8928 - mse: 13.8928 - mae: 1.6137 - 20s/epoch - 1ms/step\n",
            "Score for fold 1: loss of 28.599645614624023\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 22s - loss: 13.8790 - mse: 13.8790 - mae: 1.6123 - 22s/epoch - 1ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 22s - loss: 13.8717 - mse: 13.8717 - mae: 1.6124 - 22s/epoch - 1ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 21s - loss: 13.8563 - mse: 13.8563 - mae: 1.6115 - 21s/epoch - 1ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 20s - loss: 13.8402 - mse: 13.8402 - mae: 1.6114 - 20s/epoch - 1ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 20s - loss: 13.8425 - mse: 13.8425 - mae: 1.6108 - 20s/epoch - 1ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 20s - loss: 13.8345 - mse: 13.8345 - mae: 1.6107 - 20s/epoch - 987us/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 20s - loss: 13.8292 - mse: 13.8292 - mae: 1.6101 - 20s/epoch - 992us/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 20s - loss: 13.8205 - mse: 13.8205 - mae: 1.6100 - 20s/epoch - 981us/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 20s - loss: 13.8141 - mse: 13.8141 - mae: 1.6095 - 20s/epoch - 979us/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 20s - loss: 13.8063 - mse: 13.8063 - mae: 1.6097 - 20s/epoch - 989us/step\n",
            "Score for fold 2: loss of 18.775590896606445\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 20s - loss: 13.7993 - mse: 13.7993 - mae: 1.6086 - 20s/epoch - 1ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 21s - loss: 13.7972 - mse: 13.7972 - mae: 1.6089 - 21s/epoch - 1ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 20s - loss: 13.7881 - mse: 13.7881 - mae: 1.6088 - 20s/epoch - 1ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 20s - loss: 13.7829 - mse: 13.7829 - mae: 1.6087 - 20s/epoch - 1ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 20s - loss: 13.7822 - mse: 13.7822 - mae: 1.6081 - 20s/epoch - 1ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 21s - loss: 13.7713 - mse: 13.7713 - mae: 1.6077 - 21s/epoch - 1ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 20s - loss: 13.7705 - mse: 13.7705 - mae: 1.6081 - 20s/epoch - 1ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 21s - loss: 13.7614 - mse: 13.7614 - mae: 1.6075 - 21s/epoch - 1ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 20s - loss: 13.7530 - mse: 13.7530 - mae: 1.6076 - 20s/epoch - 1ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 20s - loss: 13.7499 - mse: 13.7499 - mae: 1.6073 - 20s/epoch - 1ms/step\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-09-28 22:07:12,408]\u001b[0m Finished trial#4 resulted in value: 13.749914169311523. Current best value is 13.749914169311523 with parameters: {'activation': 'relu', 'optimizer': 'sgd', 'num_hidden_layer': 2, 'num_hidden_unit': 20, 'kernel_size': 5, 'filter': 32, 'learning_rate': 8.175260211450466e-05}.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Score for fold 3: loss of 18.72627830505371\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 26s - loss: 14.4836 - mse: 14.4836 - mae: 1.6817 - 26s/epoch - 1ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 25s - loss: 14.3905 - mse: 14.3905 - mae: 1.6736 - 25s/epoch - 1ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 25s - loss: 14.3922 - mse: 14.3922 - mae: 1.6752 - 25s/epoch - 1ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 25s - loss: 14.4412 - mse: 14.4412 - mae: 1.6863 - 25s/epoch - 1ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 26s - loss: 14.5672 - mse: 14.5672 - mae: 1.7019 - 26s/epoch - 1ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 26s - loss: 14.4620 - mse: 14.4620 - mae: 1.6892 - 26s/epoch - 1ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 27s - loss: 14.5061 - mse: 14.5061 - mae: 1.6899 - 27s/epoch - 1ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 29s - loss: 14.5367 - mse: 14.5367 - mae: 1.6907 - 29s/epoch - 1ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 28s - loss: 14.5647 - mse: 14.5647 - mae: 1.7009 - 28s/epoch - 1ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 28s - loss: 14.6123 - mse: 14.6123 - mae: 1.7046 - 28s/epoch - 1ms/step\n",
            "Score for fold 1: loss of 30.729597091674805\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 26s - loss: 15.0688 - mse: 15.0688 - mae: 1.7520 - 26s/epoch - 1ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 26s - loss: 15.1992 - mse: 15.1992 - mae: 1.7759 - 26s/epoch - 1ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 26s - loss: 15.0484 - mse: 15.0484 - mae: 1.7511 - 26s/epoch - 1ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 26s - loss: 15.0123 - mse: 15.0123 - mae: 1.7447 - 26s/epoch - 1ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 26s - loss: 15.0249 - mse: 15.0249 - mae: 1.7493 - 26s/epoch - 1ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 26s - loss: 14.9132 - mse: 14.9132 - mae: 1.7361 - 26s/epoch - 1ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 26s - loss: 15.0631 - mse: 15.0631 - mae: 1.7550 - 26s/epoch - 1ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 26s - loss: 14.9035 - mse: 14.9035 - mae: 1.7520 - 26s/epoch - 1ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 26s - loss: 15.0265 - mse: 15.0265 - mae: 1.7545 - 26s/epoch - 1ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 26s - loss: 15.5452 - mse: 15.5452 - mae: 1.8112 - 26s/epoch - 1ms/step\n",
            "Score for fold 2: loss of 19.455839157104492\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 26s - loss: 15.4200 - mse: 15.4200 - mae: 1.7950 - 26s/epoch - 1ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 27s - loss: 15.4449 - mse: 15.4449 - mae: 1.8004 - 27s/epoch - 1ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 27s - loss: 15.5073 - mse: 15.5073 - mae: 1.8114 - 27s/epoch - 1ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 26s - loss: 15.5077 - mse: 15.5077 - mae: 1.8114 - 26s/epoch - 1ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 26s - loss: 15.4723 - mse: 15.4723 - mae: 1.8080 - 26s/epoch - 1ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 26s - loss: 15.4691 - mse: 15.4691 - mae: 1.8057 - 26s/epoch - 1ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 26s - loss: 15.5415 - mse: 15.5415 - mae: 1.8116 - 26s/epoch - 1ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 26s - loss: 15.4437 - mse: 15.4437 - mae: 1.8020 - 26s/epoch - 1ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 26s - loss: 15.4463 - mse: 15.4463 - mae: 1.8018 - 26s/epoch - 1ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 26s - loss: 15.2393 - mse: 15.2393 - mae: 1.7861 - 26s/epoch - 1ms/step\n",
            "Score for fold 3: loss of 20.20859718322754\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-09-28 22:20:31,494]\u001b[0m Finished trial#5 resulted in value: 15.239266395568848. Current best value is 13.749914169311523 with parameters: {'activation': 'relu', 'optimizer': 'sgd', 'num_hidden_layer': 2, 'num_hidden_unit': 20, 'kernel_size': 5, 'filter': 32, 'learning_rate': 8.175260211450466e-05}.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 23s - loss: 14.4870 - mse: 14.4870 - mae: 1.6554 - 23s/epoch - 1ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 23s - loss: 14.2850 - mse: 14.2850 - mae: 1.6447 - 23s/epoch - 1ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 23s - loss: 14.2095 - mse: 14.2095 - mae: 1.6382 - 23s/epoch - 1ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 23s - loss: 14.1949 - mse: 14.1949 - mae: 1.6383 - 23s/epoch - 1ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 23s - loss: 14.1536 - mse: 14.1536 - mae: 1.6387 - 23s/epoch - 1ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 23s - loss: 14.1453 - mse: 14.1453 - mae: 1.6390 - 23s/epoch - 1ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 23s - loss: 14.1375 - mse: 14.1375 - mae: 1.6424 - 23s/epoch - 1ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 23s - loss: 14.1806 - mse: 14.1806 - mae: 1.6464 - 23s/epoch - 1ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 22s - loss: 14.0958 - mse: 14.0958 - mae: 1.6350 - 22s/epoch - 1ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 23s - loss: 14.1330 - mse: 14.1330 - mae: 1.6471 - 23s/epoch - 1ms/step\n",
            "Score for fold 1: loss of 29.097660064697266\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 23s - loss: 14.1298 - mse: 14.1298 - mae: 1.6422 - 23s/epoch - 1ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 23s - loss: 14.0823 - mse: 14.0823 - mae: 1.6382 - 23s/epoch - 1ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 23s - loss: 14.1034 - mse: 14.1034 - mae: 1.6424 - 23s/epoch - 1ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 23s - loss: 14.0363 - mse: 14.0363 - mae: 1.6390 - 23s/epoch - 1ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 22s - loss: 14.0647 - mse: 14.0647 - mae: 1.6373 - 22s/epoch - 1ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 23s - loss: 14.0423 - mse: 14.0423 - mae: 1.6358 - 23s/epoch - 1ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 22s - loss: 14.0287 - mse: 14.0287 - mae: 1.6301 - 22s/epoch - 1ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 23s - loss: 14.6989 - mse: 14.6989 - mae: 1.6817 - 23s/epoch - 1ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 22s - loss: 14.8252 - mse: 14.8252 - mae: 1.6952 - 22s/epoch - 1ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 22s - loss: 14.7780 - mse: 14.7780 - mae: 1.6880 - 22s/epoch - 1ms/step\n",
            "Score for fold 2: loss of 18.996797561645508\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 23s - loss: 14.7823 - mse: 14.7823 - mae: 1.6888 - 23s/epoch - 1ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 23s - loss: 14.7653 - mse: 14.7653 - mae: 1.6868 - 23s/epoch - 1ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 23s - loss: 14.7613 - mse: 14.7613 - mae: 1.6844 - 23s/epoch - 1ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 22s - loss: 14.7696 - mse: 14.7696 - mae: 1.6859 - 22s/epoch - 1ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 23s - loss: 14.7500 - mse: 14.7500 - mae: 1.6847 - 23s/epoch - 1ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 22s - loss: 14.7478 - mse: 14.7478 - mae: 1.6857 - 22s/epoch - 1ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 23s - loss: 14.7436 - mse: 14.7436 - mae: 1.6835 - 23s/epoch - 1ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 23s - loss: 14.7415 - mse: 14.7415 - mae: 1.6845 - 23s/epoch - 1ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 23s - loss: 14.7445 - mse: 14.7445 - mae: 1.6848 - 23s/epoch - 1ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 22s - loss: 14.7422 - mse: 14.7422 - mae: 1.6846 - 22s/epoch - 1ms/step\n",
            "Score for fold 3: loss of 19.803607940673828\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-09-28 22:32:02,107]\u001b[0m Finished trial#6 resulted in value: 14.74223518371582. Current best value is 13.749914169311523 with parameters: {'activation': 'relu', 'optimizer': 'sgd', 'num_hidden_layer': 2, 'num_hidden_unit': 20, 'kernel_size': 5, 'filter': 32, 'learning_rate': 8.175260211450466e-05}.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 25s - loss: 15.2562 - mse: 15.2562 - mae: 1.6652 - 25s/epoch - 1ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 24s - loss: 14.4145 - mse: 14.4145 - mae: 1.6236 - 24s/epoch - 1ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 25s - loss: 14.1945 - mse: 14.1945 - mae: 1.6152 - 25s/epoch - 1ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 24s - loss: 14.0779 - mse: 14.0779 - mae: 1.6115 - 24s/epoch - 1ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 24s - loss: 14.0072 - mse: 14.0072 - mae: 1.6102 - 24s/epoch - 1ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 24s - loss: 13.9564 - mse: 13.9564 - mae: 1.6089 - 24s/epoch - 1ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 24s - loss: 13.9224 - mse: 13.9224 - mae: 1.6071 - 24s/epoch - 1ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 24s - loss: 13.8908 - mse: 13.8908 - mae: 1.6063 - 24s/epoch - 1ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 24s - loss: 13.8633 - mse: 13.8633 - mae: 1.6059 - 24s/epoch - 1ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 24s - loss: 13.8311 - mse: 13.8311 - mae: 1.6041 - 24s/epoch - 1ms/step\n",
            "Score for fold 1: loss of 28.138612747192383\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 24s - loss: 13.8152 - mse: 13.8152 - mae: 1.6038 - 24s/epoch - 1ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 24s - loss: 13.7959 - mse: 13.7959 - mae: 1.6024 - 24s/epoch - 1ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 24s - loss: 13.7800 - mse: 13.7800 - mae: 1.6020 - 24s/epoch - 1ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 25s - loss: 13.7626 - mse: 13.7626 - mae: 1.6019 - 25s/epoch - 1ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 24s - loss: 13.7478 - mse: 13.7478 - mae: 1.6013 - 24s/epoch - 1ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 24s - loss: 13.7346 - mse: 13.7346 - mae: 1.6003 - 24s/epoch - 1ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 24s - loss: 13.7212 - mse: 13.7212 - mae: 1.5999 - 24s/epoch - 1ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 24s - loss: 13.7076 - mse: 13.7076 - mae: 1.5992 - 24s/epoch - 1ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 24s - loss: 13.6959 - mse: 13.6959 - mae: 1.5988 - 24s/epoch - 1ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 24s - loss: 13.6841 - mse: 13.6841 - mae: 1.5986 - 24s/epoch - 1ms/step\n",
            "Score for fold 2: loss of 18.85688591003418\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 25s - loss: 13.6738 - mse: 13.6738 - mae: 1.5977 - 25s/epoch - 1ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 24s - loss: 13.6691 - mse: 13.6691 - mae: 1.5979 - 24s/epoch - 1ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 24s - loss: 13.6553 - mse: 13.6553 - mae: 1.5973 - 24s/epoch - 1ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 24s - loss: 13.6457 - mse: 13.6457 - mae: 1.5966 - 24s/epoch - 1ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 24s - loss: 13.6197 - mse: 13.6197 - mae: 1.5962 - 24s/epoch - 1ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 24s - loss: 13.6288 - mse: 13.6288 - mae: 1.5968 - 24s/epoch - 1ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 24s - loss: 13.6118 - mse: 13.6118 - mae: 1.5956 - 24s/epoch - 1ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 24s - loss: 13.6037 - mse: 13.6037 - mae: 1.5955 - 24s/epoch - 1ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 24s - loss: 13.5965 - mse: 13.5965 - mae: 1.5948 - 24s/epoch - 1ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 24s - loss: 13.5935 - mse: 13.5935 - mae: 1.5952 - 24s/epoch - 1ms/step\n",
            "Score for fold 3: loss of 18.65546989440918\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-09-28 22:44:16,695]\u001b[0m Finished trial#7 resulted in value: 13.593522071838379. Current best value is 13.593522071838379 with parameters: {'activation': 'tanh', 'optimizer': 'sgd', 'num_hidden_layer': 5, 'num_hidden_unit': 17, 'kernel_size': 2, 'filter': 42, 'learning_rate': 9.013283825341552e-05}.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 25s - loss: 15.4365 - mse: 15.4365 - mae: 1.6814 - 25s/epoch - 1ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 25s - loss: 14.8551 - mse: 14.8551 - mae: 1.6448 - 25s/epoch - 1ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 24s - loss: 14.6224 - mse: 14.6224 - mae: 1.6272 - 24s/epoch - 1ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 24s - loss: 14.4481 - mse: 14.4481 - mae: 1.6169 - 24s/epoch - 1ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 25s - loss: 14.3536 - mse: 14.3536 - mae: 1.6141 - 25s/epoch - 1ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 25s - loss: 14.3076 - mse: 14.3076 - mae: 1.6139 - 25s/epoch - 1ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 25s - loss: 14.2795 - mse: 14.2795 - mae: 1.6133 - 25s/epoch - 1ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 24s - loss: 14.2616 - mse: 14.2616 - mae: 1.6132 - 24s/epoch - 1ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 25s - loss: 14.2472 - mse: 14.2472 - mae: 1.6131 - 25s/epoch - 1ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 25s - loss: 14.2421 - mse: 14.2421 - mae: 1.6129 - 25s/epoch - 1ms/step\n",
            "Score for fold 1: loss of 29.338804244995117\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 25s - loss: 14.2372 - mse: 14.2372 - mae: 1.6132 - 25s/epoch - 1ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 25s - loss: 14.2314 - mse: 14.2314 - mae: 1.6132 - 25s/epoch - 1ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 25s - loss: 14.2295 - mse: 14.2295 - mae: 1.6130 - 25s/epoch - 1ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 25s - loss: 14.2258 - mse: 14.2258 - mae: 1.6131 - 25s/epoch - 1ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 25s - loss: 14.2169 - mse: 14.2169 - mae: 1.6129 - 25s/epoch - 1ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 25s - loss: 14.2203 - mse: 14.2203 - mae: 1.6132 - 25s/epoch - 1ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 25s - loss: 14.2117 - mse: 14.2117 - mae: 1.6138 - 25s/epoch - 1ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 26s - loss: 14.2134 - mse: 14.2134 - mae: 1.6142 - 26s/epoch - 1ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 25s - loss: 14.2141 - mse: 14.2141 - mae: 1.6135 - 25s/epoch - 1ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 24s - loss: 14.2201 - mse: 14.2201 - mae: 1.6139 - 24s/epoch - 1ms/step\n",
            "Score for fold 2: loss of 19.292753219604492\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 25s - loss: 14.2181 - mse: 14.2181 - mae: 1.6137 - 25s/epoch - 1ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 26s - loss: 14.2214 - mse: 14.2214 - mae: 1.6136 - 26s/epoch - 1ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 25s - loss: 14.2260 - mse: 14.2260 - mae: 1.6143 - 25s/epoch - 1ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 25s - loss: 14.2250 - mse: 14.2250 - mae: 1.6147 - 25s/epoch - 1ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 25s - loss: 14.2265 - mse: 14.2265 - mae: 1.6140 - 25s/epoch - 1ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 25s - loss: 14.2248 - mse: 14.2248 - mae: 1.6145 - 25s/epoch - 1ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 25s - loss: 14.2233 - mse: 14.2233 - mae: 1.6147 - 25s/epoch - 1ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 25s - loss: 14.2209 - mse: 14.2209 - mae: 1.6140 - 25s/epoch - 1ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 25s - loss: 14.2212 - mse: 14.2212 - mae: 1.6142 - 25s/epoch - 1ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 25s - loss: 14.2252 - mse: 14.2252 - mae: 1.6145 - 25s/epoch - 1ms/step\n",
            "Score for fold 3: loss of 19.251733779907227\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-09-28 22:56:51,675]\u001b[0m Finished trial#8 resulted in value: 14.225178718566895. Current best value is 13.593522071838379 with parameters: {'activation': 'tanh', 'optimizer': 'sgd', 'num_hidden_layer': 5, 'num_hidden_unit': 17, 'kernel_size': 2, 'filter': 42, 'learning_rate': 9.013283825341552e-05}.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 27s - loss: 20.0677 - mse: 20.0677 - mae: 1.9583 - 27s/epoch - 1ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 26s - loss: 17.3046 - mse: 17.3046 - mae: 1.6144 - 26s/epoch - 1ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 26s - loss: 16.3362 - mse: 16.3362 - mae: 1.6344 - 26s/epoch - 1ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 27s - loss: 16.0050 - mse: 16.0050 - mae: 1.6768 - 27s/epoch - 1ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 27s - loss: 15.8602 - mse: 15.8602 - mae: 1.7046 - 27s/epoch - 1ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 27s - loss: 15.7746 - mse: 15.7746 - mae: 1.7166 - 27s/epoch - 1ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 27s - loss: 15.7118 - mse: 15.7118 - mae: 1.7220 - 27s/epoch - 1ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 26s - loss: 15.6606 - mse: 15.6606 - mae: 1.7231 - 26s/epoch - 1ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 26s - loss: 15.6168 - mse: 15.6168 - mae: 1.7217 - 26s/epoch - 1ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 26s - loss: 15.5784 - mse: 15.5784 - mae: 1.7196 - 26s/epoch - 1ms/step\n",
            "Score for fold 1: loss of 28.760482788085938\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 26s - loss: 15.5442 - mse: 15.5442 - mae: 1.7167 - 26s/epoch - 1ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 26s - loss: 15.5130 - mse: 15.5130 - mae: 1.7140 - 26s/epoch - 1ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 26s - loss: 15.4846 - mse: 15.4846 - mae: 1.7105 - 26s/epoch - 1ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 25s - loss: 15.4582 - mse: 15.4582 - mae: 1.7083 - 25s/epoch - 1ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 26s - loss: 15.4337 - mse: 15.4337 - mae: 1.7055 - 26s/epoch - 1ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 26s - loss: 15.4108 - mse: 15.4108 - mae: 1.7028 - 26s/epoch - 1ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 26s - loss: 15.3893 - mse: 15.3893 - mae: 1.7002 - 26s/epoch - 1ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 26s - loss: 15.3690 - mse: 15.3690 - mae: 1.6977 - 26s/epoch - 1ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 26s - loss: 15.3499 - mse: 15.3499 - mae: 1.6952 - 26s/epoch - 1ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 26s - loss: 15.3319 - mse: 15.3319 - mae: 1.6930 - 26s/epoch - 1ms/step\n",
            "Score for fold 2: loss of 20.67195701599121\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 26s - loss: 15.3148 - mse: 15.3148 - mae: 1.6910 - 26s/epoch - 1ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 26s - loss: 15.2987 - mse: 15.2987 - mae: 1.6892 - 26s/epoch - 1ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 26s - loss: 15.2833 - mse: 15.2833 - mae: 1.6874 - 26s/epoch - 1ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 26s - loss: 15.2686 - mse: 15.2686 - mae: 1.6857 - 26s/epoch - 1ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 26s - loss: 15.2546 - mse: 15.2546 - mae: 1.6837 - 26s/epoch - 1ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 26s - loss: 15.2410 - mse: 15.2410 - mae: 1.6823 - 26s/epoch - 1ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 26s - loss: 15.2277 - mse: 15.2277 - mae: 1.6807 - 26s/epoch - 1ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 26s - loss: 15.2151 - mse: 15.2151 - mae: 1.6793 - 26s/epoch - 1ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 26s - loss: 15.2027 - mse: 15.2027 - mae: 1.6780 - 26s/epoch - 1ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 26s - loss: 15.1905 - mse: 15.1905 - mae: 1.6766 - 26s/epoch - 1ms/step\n",
            "Score for fold 3: loss of 20.405487060546875\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-09-28 23:10:02,336]\u001b[0m Finished trial#9 resulted in value: 15.190500259399414. Current best value is 13.593522071838379 with parameters: {'activation': 'tanh', 'optimizer': 'sgd', 'num_hidden_layer': 5, 'num_hidden_unit': 17, 'kernel_size': 2, 'filter': 42, 'learning_rate': 9.013283825341552e-05}.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 23s - loss: 23.6951 - mse: 23.6951 - mae: 2.6513 - 23s/epoch - 1ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 23s - loss: 23.4378 - mse: 23.4378 - mae: 2.6006 - 23s/epoch - 1ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 31s - loss: 23.2749 - mse: 23.2749 - mae: 2.5679 - 31s/epoch - 2ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 35s - loss: 23.1452 - mse: 23.1452 - mae: 2.5416 - 35s/epoch - 2ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 34s - loss: 23.0340 - mse: 23.0340 - mae: 2.5188 - 34s/epoch - 2ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 34s - loss: 22.9357 - mse: 22.9357 - mae: 2.4985 - 34s/epoch - 2ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 33s - loss: 22.8465 - mse: 22.8465 - mae: 2.4799 - 33s/epoch - 2ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 32s - loss: 22.7646 - mse: 22.7646 - mae: 2.4627 - 32s/epoch - 2ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 31s - loss: 22.6887 - mse: 22.6887 - mae: 2.4466 - 31s/epoch - 2ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 31s - loss: 22.6177 - mse: 22.6177 - mae: 2.4315 - 31s/epoch - 2ms/step\n",
            "Score for fold 1: loss of 29.464204788208008\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 27s - loss: 22.5512 - mse: 22.5512 - mae: 2.4173 - 27s/epoch - 1ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 28s - loss: 22.4884 - mse: 22.4884 - mae: 2.4037 - 28s/epoch - 1ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 26s - loss: 22.4289 - mse: 22.4289 - mae: 2.3908 - 26s/epoch - 1ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 26s - loss: 22.3725 - mse: 22.3725 - mae: 2.3785 - 26s/epoch - 1ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 26s - loss: 22.3185 - mse: 22.3185 - mae: 2.3667 - 26s/epoch - 1ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 25s - loss: 22.2671 - mse: 22.2671 - mae: 2.3554 - 25s/epoch - 1ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 23s - loss: 22.2177 - mse: 22.2177 - mae: 2.3445 - 23s/epoch - 1ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 23s - loss: 22.1705 - mse: 22.1705 - mae: 2.3341 - 23s/epoch - 1ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 23s - loss: 22.1249 - mse: 22.1249 - mae: 2.3239 - 23s/epoch - 1ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 22s - loss: 22.0811 - mse: 22.0811 - mae: 2.3142 - 22s/epoch - 1ms/step\n",
            "Score for fold 2: loss of 28.012645721435547\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 22s - loss: 22.0391 - mse: 22.0391 - mae: 2.3047 - 22s/epoch - 1ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 23s - loss: 21.9983 - mse: 21.9983 - mae: 2.2955 - 23s/epoch - 1ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 22s - loss: 21.9588 - mse: 21.9588 - mae: 2.2866 - 22s/epoch - 1ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 23s - loss: 21.9208 - mse: 21.9208 - mae: 2.2780 - 23s/epoch - 1ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 22s - loss: 21.8837 - mse: 21.8837 - mae: 2.2695 - 22s/epoch - 1ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 22s - loss: 21.8479 - mse: 21.8479 - mae: 2.2614 - 22s/epoch - 1ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 22s - loss: 21.8131 - mse: 21.8131 - mae: 2.2534 - 22s/epoch - 1ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 22s - loss: 21.7791 - mse: 21.7791 - mae: 2.2456 - 22s/epoch - 1ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 22s - loss: 21.7462 - mse: 21.7462 - mae: 2.2380 - 22s/epoch - 1ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 22s - loss: 21.7141 - mse: 21.7141 - mae: 2.2306 - 22s/epoch - 1ms/step\n",
            "Score for fold 3: loss of 27.481718063354492\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-09-28 23:23:09,089]\u001b[0m Finished trial#10 resulted in value: 21.714059829711914. Current best value is 13.593522071838379 with parameters: {'activation': 'tanh', 'optimizer': 'sgd', 'num_hidden_layer': 5, 'num_hidden_unit': 17, 'kernel_size': 2, 'filter': 42, 'learning_rate': 9.013283825341552e-05}.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 21s - loss: 14.8068 - mse: 14.8068 - mae: 1.6631 - 21s/epoch - 1ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 21s - loss: 14.3534 - mse: 14.3534 - mae: 1.6386 - 21s/epoch - 1ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 22s - loss: 14.2399 - mse: 14.2399 - mae: 1.6361 - 22s/epoch - 1ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 21s - loss: 14.1713 - mse: 14.1713 - mae: 1.6325 - 21s/epoch - 1ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 21s - loss: 14.1230 - mse: 14.1230 - mae: 1.6302 - 21s/epoch - 1ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 21s - loss: 14.0868 - mse: 14.0868 - mae: 1.6268 - 21s/epoch - 1ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 21s - loss: 14.0544 - mse: 14.0544 - mae: 1.6246 - 21s/epoch - 1ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 22s - loss: 14.0260 - mse: 14.0260 - mae: 1.6226 - 22s/epoch - 1ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 21s - loss: 14.0065 - mse: 14.0065 - mae: 1.6215 - 21s/epoch - 1ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 21s - loss: 13.9800 - mse: 13.9800 - mae: 1.6192 - 21s/epoch - 1ms/step\n",
            "Score for fold 1: loss of 30.378522872924805\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 21s - loss: 13.9624 - mse: 13.9624 - mae: 1.6194 - 21s/epoch - 1ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 21s - loss: 13.9471 - mse: 13.9471 - mae: 1.6170 - 21s/epoch - 1ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 21s - loss: 13.9325 - mse: 13.9325 - mae: 1.6175 - 21s/epoch - 1ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 20s - loss: 13.9232 - mse: 13.9232 - mae: 1.6163 - 20s/epoch - 1ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 21s - loss: 13.9151 - mse: 13.9151 - mae: 1.6165 - 21s/epoch - 1ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 20s - loss: 13.9059 - mse: 13.9059 - mae: 1.6160 - 20s/epoch - 1ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 20s - loss: 13.8974 - mse: 13.8974 - mae: 1.6150 - 20s/epoch - 1ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 21s - loss: 13.8941 - mse: 13.8941 - mae: 1.6152 - 21s/epoch - 1ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 21s - loss: 13.8863 - mse: 13.8863 - mae: 1.6152 - 21s/epoch - 1ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 20s - loss: 13.8780 - mse: 13.8780 - mae: 1.6139 - 20s/epoch - 1ms/step\n",
            "Score for fold 2: loss of 18.92411994934082\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 20s - loss: 13.8745 - mse: 13.8745 - mae: 1.6145 - 20s/epoch - 1ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 21s - loss: 13.8667 - mse: 13.8667 - mae: 1.6131 - 21s/epoch - 1ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 20s - loss: 13.8642 - mse: 13.8642 - mae: 1.6137 - 20s/epoch - 1ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 21s - loss: 13.8558 - mse: 13.8558 - mae: 1.6130 - 21s/epoch - 1ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 21s - loss: 13.8578 - mse: 13.8578 - mae: 1.6128 - 21s/epoch - 1ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 20s - loss: 13.8513 - mse: 13.8513 - mae: 1.6123 - 20s/epoch - 1ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 21s - loss: 13.8478 - mse: 13.8478 - mae: 1.6128 - 21s/epoch - 1ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 21s - loss: 13.8420 - mse: 13.8420 - mae: 1.6132 - 21s/epoch - 1ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 20s - loss: 13.8436 - mse: 13.8436 - mae: 1.6116 - 20s/epoch - 1ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 20s - loss: 13.8412 - mse: 13.8412 - mae: 1.6116 - 20s/epoch - 1ms/step\n",
            "Score for fold 3: loss of 18.819318771362305\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-09-28 23:33:41,133]\u001b[0m Finished trial#11 resulted in value: 13.841216087341309. Current best value is 13.593522071838379 with parameters: {'activation': 'tanh', 'optimizer': 'sgd', 'num_hidden_layer': 5, 'num_hidden_unit': 17, 'kernel_size': 2, 'filter': 42, 'learning_rate': 9.013283825341552e-05}.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 23s - loss: 14.9405 - mse: 14.9405 - mae: 1.6707 - 23s/epoch - 1ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 24s - loss: 14.4429 - mse: 14.4429 - mae: 1.6443 - 24s/epoch - 1ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 24s - loss: 14.2720 - mse: 14.2720 - mae: 1.6337 - 24s/epoch - 1ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 23s - loss: 14.1463 - mse: 14.1463 - mae: 1.6261 - 23s/epoch - 1ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 22s - loss: 14.0610 - mse: 14.0610 - mae: 1.6201 - 22s/epoch - 1ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 23s - loss: 14.0035 - mse: 14.0035 - mae: 1.6155 - 23s/epoch - 1ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 24s - loss: 13.9618 - mse: 13.9618 - mae: 1.6138 - 24s/epoch - 1ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 23s - loss: 13.9294 - mse: 13.9294 - mae: 1.6112 - 23s/epoch - 1ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 22s - loss: 13.9023 - mse: 13.9023 - mae: 1.6102 - 22s/epoch - 1ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 23s - loss: 13.8810 - mse: 13.8810 - mae: 1.6088 - 23s/epoch - 1ms/step\n",
            "Score for fold 1: loss of 28.74309730529785\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 23s - loss: 13.8700 - mse: 13.8700 - mae: 1.6085 - 23s/epoch - 1ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 23s - loss: 13.8569 - mse: 13.8569 - mae: 1.6070 - 23s/epoch - 1ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 23s - loss: 13.8418 - mse: 13.8418 - mae: 1.6066 - 23s/epoch - 1ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 23s - loss: 13.8313 - mse: 13.8313 - mae: 1.6061 - 23s/epoch - 1ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 22s - loss: 13.8244 - mse: 13.8244 - mae: 1.6048 - 22s/epoch - 1ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 23s - loss: 13.8124 - mse: 13.8124 - mae: 1.6049 - 23s/epoch - 1ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 22s - loss: 13.8034 - mse: 13.8034 - mae: 1.6038 - 22s/epoch - 1ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 22s - loss: 13.7940 - mse: 13.7940 - mae: 1.6037 - 22s/epoch - 1ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 23s - loss: 13.7838 - mse: 13.7838 - mae: 1.6024 - 23s/epoch - 1ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 22s - loss: 13.7761 - mse: 13.7761 - mae: 1.6026 - 22s/epoch - 1ms/step\n",
            "Score for fold 2: loss of 18.85659408569336\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 23s - loss: 13.7695 - mse: 13.7695 - mae: 1.6023 - 23s/epoch - 1ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 23s - loss: 13.7542 - mse: 13.7542 - mae: 1.6017 - 23s/epoch - 1ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 22s - loss: 13.7508 - mse: 13.7508 - mae: 1.6015 - 22s/epoch - 1ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 23s - loss: 13.7433 - mse: 13.7433 - mae: 1.6012 - 23s/epoch - 1ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 23s - loss: 13.7338 - mse: 13.7338 - mae: 1.6009 - 23s/epoch - 1ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 22s - loss: 13.7306 - mse: 13.7306 - mae: 1.6008 - 22s/epoch - 1ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 23s - loss: 13.7188 - mse: 13.7188 - mae: 1.6002 - 23s/epoch - 1ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 23s - loss: 13.7172 - mse: 13.7172 - mae: 1.6004 - 23s/epoch - 1ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 23s - loss: 13.7079 - mse: 13.7079 - mae: 1.5996 - 23s/epoch - 1ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 23s - loss: 13.7020 - mse: 13.7020 - mae: 1.5995 - 23s/epoch - 1ms/step\n",
            "Score for fold 3: loss of 18.75115203857422\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-09-28 23:45:12,835]\u001b[0m Finished trial#12 resulted in value: 13.70200252532959. Current best value is 13.593522071838379 with parameters: {'activation': 'tanh', 'optimizer': 'sgd', 'num_hidden_layer': 5, 'num_hidden_unit': 17, 'kernel_size': 2, 'filter': 42, 'learning_rate': 9.013283825341552e-05}.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 32s - loss: 14.3559 - mse: 14.3559 - mae: 1.6473 - 32s/epoch - 2ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 31s - loss: 13.9850 - mse: 13.9850 - mae: 1.6226 - 31s/epoch - 2ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 32s - loss: 13.9002 - mse: 13.9002 - mae: 1.6143 - 32s/epoch - 2ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 31s - loss: 13.8428 - mse: 13.8428 - mae: 1.6115 - 31s/epoch - 2ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 32s - loss: 13.8109 - mse: 13.8109 - mae: 1.6101 - 32s/epoch - 2ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 33s - loss: 13.7832 - mse: 13.7832 - mae: 1.6080 - 33s/epoch - 2ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 32s - loss: 13.7315 - mse: 13.7315 - mae: 1.6070 - 32s/epoch - 2ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 31s - loss: 13.6953 - mse: 13.6953 - mae: 1.6050 - 31s/epoch - 2ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 31s - loss: 13.6383 - mse: 13.6383 - mae: 1.6025 - 31s/epoch - 2ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 31s - loss: 13.6138 - mse: 13.6138 - mae: 1.6031 - 31s/epoch - 2ms/step\n",
            "Score for fold 1: loss of 29.17911720275879\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 31s - loss: 13.5821 - mse: 13.5821 - mae: 1.6021 - 31s/epoch - 2ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 31s - loss: 13.5479 - mse: 13.5479 - mae: 1.6006 - 31s/epoch - 2ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 31s - loss: 13.5385 - mse: 13.5385 - mae: 1.6003 - 31s/epoch - 2ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 31s - loss: 13.4739 - mse: 13.4739 - mae: 1.5974 - 31s/epoch - 2ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 31s - loss: 13.4589 - mse: 13.4589 - mae: 1.5968 - 31s/epoch - 2ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 31s - loss: 13.4414 - mse: 13.4414 - mae: 1.5968 - 31s/epoch - 2ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 31s - loss: 13.3987 - mse: 13.3987 - mae: 1.5950 - 31s/epoch - 2ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 32s - loss: 13.3617 - mse: 13.3617 - mae: 1.5942 - 32s/epoch - 2ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 32s - loss: 13.3653 - mse: 13.3653 - mae: 1.5945 - 32s/epoch - 2ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 32s - loss: 13.2793 - mse: 13.2793 - mae: 1.5921 - 32s/epoch - 2ms/step\n",
            "Score for fold 2: loss of 18.483312606811523\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 31s - loss: 13.3267 - mse: 13.3267 - mae: 1.5935 - 31s/epoch - 2ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 31s - loss: 13.2671 - mse: 13.2671 - mae: 1.5919 - 31s/epoch - 2ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 31s - loss: 13.2662 - mse: 13.2662 - mae: 1.5930 - 31s/epoch - 2ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 31s - loss: 13.2219 - mse: 13.2219 - mae: 1.5905 - 31s/epoch - 2ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 31s - loss: 13.1462 - mse: 13.1462 - mae: 1.5892 - 31s/epoch - 2ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 31s - loss: 13.1666 - mse: 13.1666 - mae: 1.5909 - 31s/epoch - 2ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 31s - loss: 13.1546 - mse: 13.1546 - mae: 1.5898 - 31s/epoch - 2ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 32s - loss: 13.2339 - mse: 13.2339 - mae: 1.5919 - 32s/epoch - 2ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 31s - loss: 13.2412 - mse: 13.2412 - mae: 1.5924 - 31s/epoch - 2ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 31s - loss: 13.2340 - mse: 13.2340 - mae: 1.5929 - 31s/epoch - 2ms/step\n",
            "Score for fold 3: loss of 18.288719177246094\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-09-29 00:01:03,787]\u001b[0m Finished trial#13 resulted in value: 13.234004974365234. Current best value is 13.234004974365234 with parameters: {'activation': 'tanh', 'optimizer': 'sgd', 'num_hidden_layer': 5, 'num_hidden_unit': 99, 'kernel_size': 2, 'filter': 59, 'learning_rate': 0.0004389202772246041}.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 33s - loss: 14.3070 - mse: 14.3070 - mae: 1.6446 - 33s/epoch - 2ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 33s - loss: 13.9775 - mse: 13.9775 - mae: 1.6232 - 33s/epoch - 2ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 33s - loss: 13.8890 - mse: 13.8890 - mae: 1.6157 - 33s/epoch - 2ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 33s - loss: 13.8397 - mse: 13.8397 - mae: 1.6136 - 33s/epoch - 2ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 34s - loss: 13.7818 - mse: 13.7818 - mae: 1.6117 - 34s/epoch - 2ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 33s - loss: 13.7440 - mse: 13.7440 - mae: 1.6095 - 33s/epoch - 2ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 33s - loss: 13.6911 - mse: 13.6911 - mae: 1.6079 - 33s/epoch - 2ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 57s - loss: 13.6537 - mse: 13.6537 - mae: 1.6068 - 57s/epoch - 3ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 48s - loss: 13.6576 - mse: 13.6576 - mae: 1.6049 - 48s/epoch - 2ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 56s - loss: 13.5583 - mse: 13.5583 - mae: 1.6022 - 56s/epoch - 3ms/step\n",
            "Score for fold 1: loss of 29.311227798461914\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 77s - loss: 13.5525 - mse: 13.5525 - mae: 1.6025 - 77s/epoch - 4ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 78s - loss: 13.4979 - mse: 13.4979 - mae: 1.6008 - 78s/epoch - 4ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 78s - loss: 13.4874 - mse: 13.4874 - mae: 1.6007 - 78s/epoch - 4ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 77s - loss: 13.4578 - mse: 13.4578 - mae: 1.5989 - 77s/epoch - 4ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 86s - loss: 13.3802 - mse: 13.3802 - mae: 1.5966 - 86s/epoch - 4ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 77s - loss: 13.3867 - mse: 13.3867 - mae: 1.5979 - 77s/epoch - 4ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 63s - loss: 13.3410 - mse: 13.3410 - mae: 1.5963 - 63s/epoch - 3ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 58s - loss: 13.3666 - mse: 13.3666 - mae: 1.5968 - 58s/epoch - 3ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 56s - loss: 13.3273 - mse: 13.3273 - mae: 1.5966 - 56s/epoch - 3ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 55s - loss: 13.2699 - mse: 13.2699 - mae: 1.5946 - 55s/epoch - 3ms/step\n",
            "Score for fold 2: loss of 18.48609161376953\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 52s - loss: 13.2056 - mse: 13.2056 - mae: 1.5927 - 52s/epoch - 3ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 53s - loss: 13.2671 - mse: 13.2671 - mae: 1.5941 - 53s/epoch - 3ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 52s - loss: 13.1181 - mse: 13.1181 - mae: 1.5903 - 52s/epoch - 3ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 51s - loss: 13.2125 - mse: 13.2125 - mae: 1.5933 - 51s/epoch - 3ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 51s - loss: 13.1492 - mse: 13.1492 - mae: 1.5906 - 51s/epoch - 3ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 50s - loss: 12.9975 - mse: 12.9975 - mae: 1.5875 - 50s/epoch - 2ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 50s - loss: 13.0561 - mse: 13.0561 - mae: 1.5898 - 50s/epoch - 2ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 50s - loss: 13.0013 - mse: 13.0013 - mae: 1.5882 - 50s/epoch - 3ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 49s - loss: 13.0258 - mse: 13.0258 - mae: 1.5895 - 49s/epoch - 2ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 48s - loss: 13.0616 - mse: 13.0616 - mae: 1.5891 - 48s/epoch - 2ms/step\n",
            "Score for fold 3: loss of 18.2840518951416\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-09-29 00:28:03,018]\u001b[0m Finished trial#14 resulted in value: 13.061644554138184. Current best value is 13.061644554138184 with parameters: {'activation': 'tanh', 'optimizer': 'sgd', 'num_hidden_layer': 5, 'num_hidden_unit': 99, 'kernel_size': 2, 'filter': 67, 'learning_rate': 0.00047798418441004315}.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 44s - loss: 14.2569 - mse: 14.2569 - mae: 1.6408 - 44s/epoch - 2ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 43s - loss: 13.9723 - mse: 13.9723 - mae: 1.6219 - 43s/epoch - 2ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 42s - loss: 13.8954 - mse: 13.8954 - mae: 1.6160 - 42s/epoch - 2ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 42s - loss: 13.8423 - mse: 13.8423 - mae: 1.6127 - 42s/epoch - 2ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 42s - loss: 13.7932 - mse: 13.7932 - mae: 1.6091 - 42s/epoch - 2ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 40s - loss: 13.7481 - mse: 13.7481 - mae: 1.6086 - 40s/epoch - 2ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 40s - loss: 13.7090 - mse: 13.7090 - mae: 1.6062 - 40s/epoch - 2ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 40s - loss: 13.6249 - mse: 13.6249 - mae: 1.6046 - 40s/epoch - 2ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 40s - loss: 13.6031 - mse: 13.6031 - mae: 1.6041 - 40s/epoch - 2ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 40s - loss: 13.5446 - mse: 13.5446 - mae: 1.6029 - 40s/epoch - 2ms/step\n",
            "Score for fold 1: loss of 29.443635940551758\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 39s - loss: 13.5321 - mse: 13.5321 - mae: 1.6024 - 39s/epoch - 2ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 40s - loss: 13.4968 - mse: 13.4968 - mae: 1.6010 - 40s/epoch - 2ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 40s - loss: 13.5104 - mse: 13.5104 - mae: 1.6015 - 40s/epoch - 2ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 40s - loss: 13.4681 - mse: 13.4681 - mae: 1.6002 - 40s/epoch - 2ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 39s - loss: 13.4114 - mse: 13.4114 - mae: 1.5973 - 39s/epoch - 2ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 39s - loss: 13.3530 - mse: 13.3530 - mae: 1.5957 - 39s/epoch - 2ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 40s - loss: 13.3360 - mse: 13.3360 - mae: 1.5956 - 40s/epoch - 2ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 40s - loss: 13.2521 - mse: 13.2521 - mae: 1.5931 - 40s/epoch - 2ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 40s - loss: 13.2968 - mse: 13.2968 - mae: 1.5943 - 40s/epoch - 2ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 40s - loss: 13.2443 - mse: 13.2443 - mae: 1.5937 - 40s/epoch - 2ms/step\n",
            "Score for fold 2: loss of 18.533079147338867\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 39s - loss: 13.1794 - mse: 13.1794 - mae: 1.5919 - 39s/epoch - 2ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 39s - loss: 13.2085 - mse: 13.2085 - mae: 1.5930 - 39s/epoch - 2ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 39s - loss: 13.1342 - mse: 13.1342 - mae: 1.5907 - 39s/epoch - 2ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 39s - loss: 13.1387 - mse: 13.1387 - mae: 1.5908 - 39s/epoch - 2ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 39s - loss: 13.1256 - mse: 13.1256 - mae: 1.5924 - 39s/epoch - 2ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 39s - loss: 13.0596 - mse: 13.0596 - mae: 1.5900 - 39s/epoch - 2ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 39s - loss: 13.0442 - mse: 13.0442 - mae: 1.5890 - 39s/epoch - 2ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 39s - loss: 12.9941 - mse: 12.9941 - mae: 1.5883 - 39s/epoch - 2ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 40s - loss: 12.9281 - mse: 12.9281 - mae: 1.5860 - 40s/epoch - 2ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 39s - loss: 12.9443 - mse: 12.9443 - mae: 1.5887 - 39s/epoch - 2ms/step\n",
            "Score for fold 3: loss of 18.324403762817383\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-09-29 00:48:16,090]\u001b[0m Finished trial#15 resulted in value: 12.944266319274902. Current best value is 12.944266319274902 with parameters: {'activation': 'tanh', 'optimizer': 'sgd', 'num_hidden_layer': 5, 'num_hidden_unit': 97, 'kernel_size': 2, 'filter': 75, 'learning_rate': 0.0005541538399852009}.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 39s - loss: 15.6331 - mse: 15.6331 - mae: 1.8337 - 39s/epoch - 2ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 39s - loss: 15.6887 - mse: 15.6887 - mae: 1.8481 - 39s/epoch - 2ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 39s - loss: 15.7060 - mse: 15.7060 - mae: 1.8468 - 39s/epoch - 2ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 38s - loss: 16.0653 - mse: 16.0653 - mae: 1.8797 - 38s/epoch - 2ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 38s - loss: 15.8339 - mse: 15.8339 - mae: 1.8648 - 38s/epoch - 2ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 38s - loss: 15.8221 - mse: 15.8221 - mae: 1.8633 - 38s/epoch - 2ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 39s - loss: 15.8216 - mse: 15.8216 - mae: 1.8631 - 39s/epoch - 2ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 38s - loss: 15.8856 - mse: 15.8856 - mae: 1.8621 - 38s/epoch - 2ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 38s - loss: 15.8757 - mse: 15.8757 - mae: 1.8651 - 38s/epoch - 2ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 38s - loss: 15.7784 - mse: 15.7784 - mae: 1.8560 - 38s/epoch - 2ms/step\n",
            "Score for fold 1: loss of 29.780170440673828\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 38s - loss: 15.8293 - mse: 15.8293 - mae: 1.8658 - 38s/epoch - 2ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 39s - loss: 15.8775 - mse: 15.8775 - mae: 1.8596 - 39s/epoch - 2ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 38s - loss: 15.8169 - mse: 15.8169 - mae: 1.8627 - 38s/epoch - 2ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 39s - loss: 15.9549 - mse: 15.9549 - mae: 1.8747 - 39s/epoch - 2ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 39s - loss: 15.8316 - mse: 15.8316 - mae: 1.8612 - 39s/epoch - 2ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 39s - loss: 15.8103 - mse: 15.8103 - mae: 1.8606 - 39s/epoch - 2ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 39s - loss: 15.9970 - mse: 15.9970 - mae: 1.8776 - 39s/epoch - 2ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 38s - loss: 15.9687 - mse: 15.9687 - mae: 1.8750 - 38s/epoch - 2ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 39s - loss: 15.8907 - mse: 15.8907 - mae: 1.8657 - 39s/epoch - 2ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 39s - loss: 16.1683 - mse: 16.1683 - mae: 1.8895 - 39s/epoch - 2ms/step\n",
            "Score for fold 2: loss of 20.125452041625977\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 38s - loss: 15.9261 - mse: 15.9261 - mae: 1.8700 - 38s/epoch - 2ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 38s - loss: 15.8378 - mse: 15.8378 - mae: 1.8592 - 38s/epoch - 2ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 39s - loss: 15.9146 - mse: 15.9146 - mae: 1.8729 - 39s/epoch - 2ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 39s - loss: 15.8417 - mse: 15.8417 - mae: 1.8667 - 39s/epoch - 2ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 39s - loss: 16.0228 - mse: 16.0228 - mae: 1.8797 - 39s/epoch - 2ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 38s - loss: 15.8304 - mse: 15.8304 - mae: 1.8674 - 38s/epoch - 2ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 38s - loss: 15.9678 - mse: 15.9678 - mae: 1.8783 - 38s/epoch - 2ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 39s - loss: 16.1414 - mse: 16.1414 - mae: 1.8872 - 39s/epoch - 2ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 40s - loss: 16.0125 - mse: 16.0125 - mae: 1.8807 - 40s/epoch - 2ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 40s - loss: 15.9377 - mse: 15.9377 - mae: 1.8734 - 40s/epoch - 2ms/step\n",
            "Score for fold 3: loss of 23.716140747070312\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-09-29 01:07:46,995]\u001b[0m Finished trial#16 resulted in value: 15.937726020812988. Current best value is 12.944266319274902 with parameters: {'activation': 'tanh', 'optimizer': 'sgd', 'num_hidden_layer': 5, 'num_hidden_unit': 97, 'kernel_size': 2, 'filter': 75, 'learning_rate': 0.0005541538399852009}.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 42s - loss: 14.1858 - mse: 14.1858 - mae: 1.6346 - 42s/epoch - 2ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 43s - loss: 13.9449 - mse: 13.9449 - mae: 1.6207 - 43s/epoch - 2ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 44s - loss: 13.9214 - mse: 13.9214 - mae: 1.6153 - 44s/epoch - 2ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 45s - loss: 13.7728 - mse: 13.7728 - mae: 1.6106 - 45s/epoch - 2ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 45s - loss: 13.7027 - mse: 13.7027 - mae: 1.6080 - 45s/epoch - 2ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 45s - loss: 13.5962 - mse: 13.5962 - mae: 1.6034 - 45s/epoch - 2ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 45s - loss: 13.5564 - mse: 13.5564 - mae: 1.6006 - 45s/epoch - 2ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 44s - loss: 13.5289 - mse: 13.5289 - mae: 1.6017 - 44s/epoch - 2ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 44s - loss: 13.4460 - mse: 13.4460 - mae: 1.5979 - 44s/epoch - 2ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 43s - loss: 13.4200 - mse: 13.4200 - mae: 1.5966 - 43s/epoch - 2ms/step\n",
            "Score for fold 1: loss of 29.423370361328125\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 44s - loss: 13.3565 - mse: 13.3565 - mae: 1.5957 - 44s/epoch - 2ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 45s - loss: 13.3361 - mse: 13.3361 - mae: 1.5938 - 45s/epoch - 2ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 45s - loss: 13.2831 - mse: 13.2831 - mae: 1.5919 - 45s/epoch - 2ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 45s - loss: 13.2079 - mse: 13.2079 - mae: 1.5905 - 45s/epoch - 2ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 44s - loss: 13.1855 - mse: 13.1855 - mae: 1.5884 - 44s/epoch - 2ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 44s - loss: 13.2008 - mse: 13.2008 - mae: 1.5893 - 44s/epoch - 2ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 44s - loss: 13.1463 - mse: 13.1463 - mae: 1.5868 - 44s/epoch - 2ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 44s - loss: 13.0969 - mse: 13.0969 - mae: 1.5864 - 44s/epoch - 2ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 44s - loss: 13.0863 - mse: 13.0863 - mae: 1.5873 - 44s/epoch - 2ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 44s - loss: 13.0467 - mse: 13.0467 - mae: 1.5867 - 44s/epoch - 2ms/step\n",
            "Score for fold 2: loss of 18.364648818969727\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 43s - loss: 12.9978 - mse: 12.9978 - mae: 1.5851 - 43s/epoch - 2ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 44s - loss: 12.9874 - mse: 12.9874 - mae: 1.5836 - 44s/epoch - 2ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 44s - loss: 12.9085 - mse: 12.9085 - mae: 1.5831 - 44s/epoch - 2ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 44s - loss: 12.9702 - mse: 12.9702 - mae: 1.5843 - 44s/epoch - 2ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 44s - loss: 12.9179 - mse: 12.9179 - mae: 1.5829 - 44s/epoch - 2ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 44s - loss: 12.8658 - mse: 12.8658 - mae: 1.5811 - 44s/epoch - 2ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 44s - loss: 12.8721 - mse: 12.8721 - mae: 1.5824 - 44s/epoch - 2ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 44s - loss: 12.7678 - mse: 12.7678 - mae: 1.5806 - 44s/epoch - 2ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 44s - loss: 12.6864 - mse: 12.6864 - mae: 1.5776 - 44s/epoch - 2ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 44s - loss: 12.6886 - mse: 12.6886 - mae: 1.5793 - 44s/epoch - 2ms/step\n",
            "Score for fold 3: loss of 18.669490814208984\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-09-29 01:29:58,492]\u001b[0m Finished trial#17 resulted in value: 12.688554763793945. Current best value is 12.688554763793945 with parameters: {'activation': 'tanh', 'optimizer': 'adam', 'num_hidden_layer': 5, 'num_hidden_unit': 85, 'kernel_size': 4, 'filter': 77, 'learning_rate': 0.00041922751339262246}.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 41s - loss: 14.3507 - mse: 14.3507 - mae: 1.6565 - 41s/epoch - 2ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 40s - loss: 14.1595 - mse: 14.1595 - mae: 1.6435 - 40s/epoch - 2ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 40s - loss: 14.1111 - mse: 14.1111 - mae: 1.6403 - 40s/epoch - 2ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 41s - loss: 14.1441 - mse: 14.1441 - mae: 1.6445 - 41s/epoch - 2ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 42s - loss: 14.1380 - mse: 14.1380 - mae: 1.6459 - 42s/epoch - 2ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 42s - loss: 14.0964 - mse: 14.0964 - mae: 1.6423 - 42s/epoch - 2ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 42s - loss: 14.0599 - mse: 14.0599 - mae: 1.6410 - 42s/epoch - 2ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 42s - loss: 14.0526 - mse: 14.0526 - mae: 1.6426 - 42s/epoch - 2ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 42s - loss: 14.0826 - mse: 14.0826 - mae: 1.6430 - 42s/epoch - 2ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 41s - loss: 14.0394 - mse: 14.0394 - mae: 1.6383 - 41s/epoch - 2ms/step\n",
            "Score for fold 1: loss of 28.644968032836914\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 40s - loss: 14.0192 - mse: 14.0192 - mae: 1.6378 - 40s/epoch - 2ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 41s - loss: 13.9647 - mse: 13.9647 - mae: 1.6391 - 41s/epoch - 2ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 41s - loss: 13.9929 - mse: 13.9929 - mae: 1.6392 - 41s/epoch - 2ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 41s - loss: 13.9951 - mse: 13.9951 - mae: 1.6428 - 41s/epoch - 2ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 41s - loss: 14.0838 - mse: 14.0838 - mae: 1.6480 - 41s/epoch - 2ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 41s - loss: 14.0741 - mse: 14.0741 - mae: 1.6476 - 41s/epoch - 2ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 41s - loss: 14.1300 - mse: 14.1300 - mae: 1.6521 - 41s/epoch - 2ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 41s - loss: 14.0948 - mse: 14.0948 - mae: 1.6507 - 41s/epoch - 2ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 41s - loss: 14.1493 - mse: 14.1493 - mae: 1.6509 - 41s/epoch - 2ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 42s - loss: 14.1703 - mse: 14.1703 - mae: 1.6547 - 42s/epoch - 2ms/step\n",
            "Score for fold 2: loss of 19.437538146972656\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 40s - loss: 14.1761 - mse: 14.1761 - mae: 1.6561 - 40s/epoch - 2ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 41s - loss: 14.1670 - mse: 14.1670 - mae: 1.6535 - 41s/epoch - 2ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 41s - loss: 14.1575 - mse: 14.1575 - mae: 1.6526 - 41s/epoch - 2ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 41s - loss: 14.1605 - mse: 14.1605 - mae: 1.6554 - 41s/epoch - 2ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 41s - loss: 14.1569 - mse: 14.1569 - mae: 1.6541 - 41s/epoch - 2ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 41s - loss: 14.3376 - mse: 14.3376 - mae: 1.6636 - 41s/epoch - 2ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 41s - loss: 14.2937 - mse: 14.2937 - mae: 1.6619 - 41s/epoch - 2ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 42s - loss: 14.3443 - mse: 14.3443 - mae: 1.6673 - 42s/epoch - 2ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 41s - loss: 14.3806 - mse: 14.3806 - mae: 1.6769 - 41s/epoch - 2ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 41s - loss: 14.3039 - mse: 14.3039 - mae: 1.6664 - 41s/epoch - 2ms/step\n",
            "Score for fold 3: loss of 19.04244613647461\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-09-29 01:50:45,161]\u001b[0m Finished trial#18 resulted in value: 14.303902626037598. Current best value is 12.688554763793945 with parameters: {'activation': 'tanh', 'optimizer': 'adam', 'num_hidden_layer': 5, 'num_hidden_unit': 85, 'kernel_size': 4, 'filter': 77, 'learning_rate': 0.00041922751339262246}.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 44s - loss: 15.0382 - mse: 15.0382 - mae: 1.7319 - 44s/epoch - 2ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 44s - loss: 15.1812 - mse: 15.1812 - mae: 1.7605 - 44s/epoch - 2ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 44s - loss: 15.6411 - mse: 15.6411 - mae: 1.8150 - 44s/epoch - 2ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 44s - loss: 16.8097 - mse: 16.8097 - mae: 1.8965 - 44s/epoch - 2ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 45s - loss: 16.8720 - mse: 16.8720 - mae: 1.9015 - 45s/epoch - 2ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 45s - loss: 16.8640 - mse: 16.8640 - mae: 1.9001 - 45s/epoch - 2ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 44s - loss: 16.8733 - mse: 16.8733 - mae: 1.9013 - 44s/epoch - 2ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 78s - loss: 16.8663 - mse: 16.8663 - mae: 1.9011 - 78s/epoch - 4ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 103s - loss: 16.8680 - mse: 16.8680 - mae: 1.9002 - 103s/epoch - 5ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 95s - loss: 16.8638 - mse: 16.8638 - mae: 1.8999 - 95s/epoch - 5ms/step\n",
            "Score for fold 1: loss of 29.321744918823242\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 83s - loss: 16.8641 - mse: 16.8641 - mae: 1.9004 - 83s/epoch - 4ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 80s - loss: 16.8697 - mse: 16.8697 - mae: 1.9009 - 80s/epoch - 4ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 79s - loss: 16.8622 - mse: 16.8622 - mae: 1.9005 - 79s/epoch - 4ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 73s - loss: 16.8743 - mse: 16.8743 - mae: 1.9013 - 73s/epoch - 4ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 64s - loss: 16.8665 - mse: 16.8665 - mae: 1.9008 - 64s/epoch - 3ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 72s - loss: 16.8670 - mse: 16.8670 - mae: 1.9003 - 72s/epoch - 4ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 72s - loss: 16.8665 - mse: 16.8665 - mae: 1.9006 - 72s/epoch - 4ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 71s - loss: 16.8686 - mse: 16.8686 - mae: 1.9001 - 71s/epoch - 4ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 70s - loss: 16.8734 - mse: 16.8734 - mae: 1.9006 - 70s/epoch - 3ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 70s - loss: 16.8727 - mse: 16.8727 - mae: 1.9002 - 70s/epoch - 3ms/step\n",
            "Score for fold 2: loss of 22.084365844726562\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 63s - loss: 16.8782 - mse: 16.8782 - mae: 1.9016 - 63s/epoch - 3ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 64s - loss: 16.8714 - mse: 16.8714 - mae: 1.9004 - 64s/epoch - 3ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 64s - loss: 16.8748 - mse: 16.8748 - mae: 1.9014 - 64s/epoch - 3ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 54s - loss: 16.8679 - mse: 16.8679 - mae: 1.9013 - 54s/epoch - 3ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 45s - loss: 16.8643 - mse: 16.8643 - mae: 1.8999 - 45s/epoch - 2ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 45s - loss: 16.8695 - mse: 16.8695 - mae: 1.9001 - 45s/epoch - 2ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 44s - loss: 16.8689 - mse: 16.8689 - mae: 1.9009 - 44s/epoch - 2ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 45s - loss: 16.8706 - mse: 16.8706 - mae: 1.9007 - 45s/epoch - 2ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 45s - loss: 16.8730 - mse: 16.8730 - mae: 1.9016 - 45s/epoch - 2ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 44s - loss: 16.8795 - mse: 16.8795 - mae: 1.9006 - 44s/epoch - 2ms/step\n",
            "Score for fold 3: loss of 21.955860137939453\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-09-29 02:21:31,875]\u001b[0m Finished trial#19 resulted in value: 16.879541397094727. Current best value is 12.688554763793945 with parameters: {'activation': 'tanh', 'optimizer': 'adam', 'num_hidden_layer': 5, 'num_hidden_unit': 85, 'kernel_size': 4, 'filter': 77, 'learning_rate': 0.00041922751339262246}.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 38s - loss: 18.0390 - mse: 18.0390 - mae: 2.0801 - 38s/epoch - 2ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 38s - loss: 18.1099 - mse: 18.1099 - mae: 2.0898 - 38s/epoch - 2ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 39s - loss: 18.1908 - mse: 18.1908 - mae: 2.1003 - 39s/epoch - 2ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 39s - loss: 18.1362 - mse: 18.1362 - mae: 2.0904 - 39s/epoch - 2ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 40s - loss: 18.1297 - mse: 18.1297 - mae: 2.0930 - 40s/epoch - 2ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 40s - loss: 18.1480 - mse: 18.1480 - mae: 2.0916 - 40s/epoch - 2ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 40s - loss: 18.1555 - mse: 18.1555 - mae: 2.0943 - 40s/epoch - 2ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 40s - loss: 18.2070 - mse: 18.2070 - mae: 2.1028 - 40s/epoch - 2ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 39s - loss: 18.1779 - mse: 18.1779 - mae: 2.0943 - 39s/epoch - 2ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 39s - loss: 18.1690 - mse: 18.1690 - mae: 2.1003 - 39s/epoch - 2ms/step\n",
            "Score for fold 1: loss of 29.4488468170166\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 39s - loss: 18.1787 - mse: 18.1787 - mae: 2.0986 - 39s/epoch - 2ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 40s - loss: 18.1809 - mse: 18.1809 - mae: 2.0975 - 40s/epoch - 2ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 40s - loss: 18.1997 - mse: 18.1997 - mae: 2.1010 - 40s/epoch - 2ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 40s - loss: 18.0939 - mse: 18.0939 - mae: 2.0834 - 40s/epoch - 2ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 40s - loss: 18.1556 - mse: 18.1556 - mae: 2.0959 - 40s/epoch - 2ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 40s - loss: 18.1640 - mse: 18.1640 - mae: 2.0971 - 40s/epoch - 2ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 40s - loss: 18.1114 - mse: 18.1114 - mae: 2.0856 - 40s/epoch - 2ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 40s - loss: 18.1618 - mse: 18.1618 - mae: 2.0907 - 40s/epoch - 2ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 40s - loss: 18.1043 - mse: 18.1043 - mae: 2.0881 - 40s/epoch - 2ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 40s - loss: 18.1569 - mse: 18.1569 - mae: 2.0920 - 40s/epoch - 2ms/step\n",
            "Score for fold 2: loss of 22.242210388183594\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 39s - loss: 18.1673 - mse: 18.1673 - mae: 2.0980 - 39s/epoch - 2ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 40s - loss: 18.1339 - mse: 18.1339 - mae: 2.0937 - 40s/epoch - 2ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 39s - loss: 18.1688 - mse: 18.1688 - mae: 2.1008 - 39s/epoch - 2ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 40s - loss: 18.1491 - mse: 18.1491 - mae: 2.0929 - 40s/epoch - 2ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 39s - loss: 18.1457 - mse: 18.1457 - mae: 2.0923 - 39s/epoch - 2ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 39s - loss: 18.0993 - mse: 18.0993 - mae: 2.0832 - 39s/epoch - 2ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 40s - loss: 18.1723 - mse: 18.1723 - mae: 2.0913 - 40s/epoch - 2ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 40s - loss: 18.1877 - mse: 18.1877 - mae: 2.0972 - 40s/epoch - 2ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 40s - loss: 18.1674 - mse: 18.1674 - mae: 2.0944 - 40s/epoch - 2ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 40s - loss: 18.1298 - mse: 18.1298 - mae: 2.0904 - 40s/epoch - 2ms/step\n",
            "Score for fold 3: loss of 22.007129669189453\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-09-29 02:41:25,110]\u001b[0m Finished trial#20 resulted in value: 18.129837036132812. Current best value is 12.688554763793945 with parameters: {'activation': 'tanh', 'optimizer': 'adam', 'num_hidden_layer': 5, 'num_hidden_unit': 85, 'kernel_size': 4, 'filter': 77, 'learning_rate': 0.00041922751339262246}.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 36s - loss: 14.7832 - mse: 14.7832 - mae: 1.6616 - 36s/epoch - 2ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 37s - loss: 14.5155 - mse: 14.5155 - mae: 1.6444 - 37s/epoch - 2ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 38s - loss: 14.4332 - mse: 14.4332 - mae: 1.6400 - 38s/epoch - 2ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 37s - loss: 14.3826 - mse: 14.3826 - mae: 1.6355 - 37s/epoch - 2ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 38s - loss: 14.3440 - mse: 14.3440 - mae: 1.6327 - 38s/epoch - 2ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 37s - loss: 14.3132 - mse: 14.3132 - mae: 1.6306 - 37s/epoch - 2ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 38s - loss: 14.2870 - mse: 14.2870 - mae: 1.6280 - 38s/epoch - 2ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 37s - loss: 14.2648 - mse: 14.2648 - mae: 1.6270 - 37s/epoch - 2ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 37s - loss: 14.2446 - mse: 14.2446 - mae: 1.6255 - 37s/epoch - 2ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 38s - loss: 14.2276 - mse: 14.2276 - mae: 1.6239 - 38s/epoch - 2ms/step\n",
            "Score for fold 1: loss of 29.233766555786133\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 37s - loss: 14.2120 - mse: 14.2120 - mae: 1.6226 - 37s/epoch - 2ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 38s - loss: 14.1981 - mse: 14.1981 - mae: 1.6220 - 38s/epoch - 2ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 38s - loss: 14.1858 - mse: 14.1858 - mae: 1.6218 - 38s/epoch - 2ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 38s - loss: 14.1740 - mse: 14.1740 - mae: 1.6211 - 38s/epoch - 2ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 38s - loss: 14.1642 - mse: 14.1642 - mae: 1.6202 - 38s/epoch - 2ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 38s - loss: 14.1546 - mse: 14.1546 - mae: 1.6196 - 38s/epoch - 2ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 37s - loss: 14.1462 - mse: 14.1462 - mae: 1.6190 - 37s/epoch - 2ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 38s - loss: 14.1381 - mse: 14.1381 - mae: 1.6185 - 38s/epoch - 2ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 38s - loss: 14.1309 - mse: 14.1309 - mae: 1.6186 - 38s/epoch - 2ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 38s - loss: 14.1242 - mse: 14.1242 - mae: 1.6179 - 38s/epoch - 2ms/step\n",
            "Score for fold 2: loss of 19.265972137451172\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 37s - loss: 14.1177 - mse: 14.1177 - mae: 1.6176 - 37s/epoch - 2ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 37s - loss: 14.1114 - mse: 14.1114 - mae: 1.6172 - 37s/epoch - 2ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 37s - loss: 14.1058 - mse: 14.1058 - mae: 1.6172 - 37s/epoch - 2ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 38s - loss: 14.1006 - mse: 14.1006 - mae: 1.6173 - 38s/epoch - 2ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 38s - loss: 14.0956 - mse: 14.0956 - mae: 1.6164 - 38s/epoch - 2ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 38s - loss: 14.0909 - mse: 14.0909 - mae: 1.6165 - 38s/epoch - 2ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 38s - loss: 14.0863 - mse: 14.0863 - mae: 1.6161 - 38s/epoch - 2ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 37s - loss: 14.0822 - mse: 14.0822 - mae: 1.6159 - 37s/epoch - 2ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 37s - loss: 14.0781 - mse: 14.0781 - mae: 1.6158 - 37s/epoch - 2ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 37s - loss: 14.0740 - mse: 14.0740 - mae: 1.6160 - 37s/epoch - 2ms/step\n",
            "Score for fold 3: loss of 19.146686553955078\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-09-29 03:00:20,141]\u001b[0m Finished trial#21 resulted in value: 14.074000358581543. Current best value is 12.688554763793945 with parameters: {'activation': 'tanh', 'optimizer': 'adam', 'num_hidden_layer': 5, 'num_hidden_unit': 85, 'kernel_size': 4, 'filter': 77, 'learning_rate': 0.00041922751339262246}.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 41s - loss: 14.2984 - mse: 14.2984 - mae: 1.6503 - 41s/epoch - 2ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 43s - loss: 14.1378 - mse: 14.1378 - mae: 1.6350 - 43s/epoch - 2ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 43s - loss: 14.0658 - mse: 14.0658 - mae: 1.6335 - 43s/epoch - 2ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 43s - loss: 14.0525 - mse: 14.0525 - mae: 1.6332 - 43s/epoch - 2ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 43s - loss: 14.0667 - mse: 14.0667 - mae: 1.6341 - 43s/epoch - 2ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 43s - loss: 13.9772 - mse: 13.9772 - mae: 1.6362 - 43s/epoch - 2ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 43s - loss: 13.9575 - mse: 13.9575 - mae: 1.6329 - 43s/epoch - 2ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 43s - loss: 13.8951 - mse: 13.8951 - mae: 1.6282 - 43s/epoch - 2ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 43s - loss: 13.8867 - mse: 13.8867 - mae: 1.6279 - 43s/epoch - 2ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 47s - loss: 13.8878 - mse: 13.8878 - mae: 1.6293 - 47s/epoch - 2ms/step\n",
            "Score for fold 1: loss of 29.64447021484375\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 53s - loss: 13.9306 - mse: 13.9306 - mae: 1.6312 - 53s/epoch - 3ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 65s - loss: 13.8655 - mse: 13.8655 - mae: 1.6266 - 65s/epoch - 3ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 63s - loss: 13.8925 - mse: 13.8925 - mae: 1.6261 - 63s/epoch - 3ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 63s - loss: 13.8681 - mse: 13.8681 - mae: 1.6273 - 63s/epoch - 3ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 63s - loss: 13.8974 - mse: 13.8974 - mae: 1.6260 - 63s/epoch - 3ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 62s - loss: 13.8730 - mse: 13.8730 - mae: 1.6273 - 62s/epoch - 3ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 63s - loss: 13.8711 - mse: 13.8711 - mae: 1.6283 - 63s/epoch - 3ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 62s - loss: 13.7865 - mse: 13.7865 - mae: 1.6249 - 62s/epoch - 3ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 61s - loss: 13.7925 - mse: 13.7925 - mae: 1.6242 - 61s/epoch - 3ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 58s - loss: 13.7905 - mse: 13.7905 - mae: 1.6248 - 58s/epoch - 3ms/step\n",
            "Score for fold 2: loss of 19.045087814331055\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 48s - loss: 13.8142 - mse: 13.8142 - mae: 1.6244 - 48s/epoch - 2ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 53s - loss: 13.7844 - mse: 13.7844 - mae: 1.6241 - 53s/epoch - 3ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 52s - loss: 13.7947 - mse: 13.7947 - mae: 1.6245 - 52s/epoch - 3ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 51s - loss: 13.8665 - mse: 13.8665 - mae: 1.6285 - 51s/epoch - 3ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 51s - loss: 13.8285 - mse: 13.8285 - mae: 1.6268 - 51s/epoch - 3ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 50s - loss: 13.8357 - mse: 13.8357 - mae: 1.6271 - 50s/epoch - 2ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 48s - loss: 13.8357 - mse: 13.8357 - mae: 1.6238 - 48s/epoch - 2ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 48s - loss: 13.7830 - mse: 13.7830 - mae: 1.6249 - 48s/epoch - 2ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 47s - loss: 13.8495 - mse: 13.8495 - mae: 1.6282 - 47s/epoch - 2ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 47s - loss: 13.8631 - mse: 13.8631 - mae: 1.6305 - 47s/epoch - 2ms/step\n",
            "Score for fold 3: loss of 18.8209171295166\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-09-29 03:26:10,148]\u001b[0m Finished trial#22 resulted in value: 13.863064765930176. Current best value is 12.688554763793945 with parameters: {'activation': 'tanh', 'optimizer': 'adam', 'num_hidden_layer': 5, 'num_hidden_unit': 85, 'kernel_size': 4, 'filter': 77, 'learning_rate': 0.00041922751339262246}.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 37s - loss: 14.3908 - mse: 14.3908 - mae: 1.6457 - 37s/epoch - 2ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 34s - loss: 14.0101 - mse: 14.0101 - mae: 1.6233 - 34s/epoch - 2ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 36s - loss: 13.9191 - mse: 13.9191 - mae: 1.6163 - 36s/epoch - 2ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 36s - loss: 13.8564 - mse: 13.8564 - mae: 1.6118 - 36s/epoch - 2ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 36s - loss: 13.8178 - mse: 13.8178 - mae: 1.6093 - 36s/epoch - 2ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 36s - loss: 13.7858 - mse: 13.7858 - mae: 1.6084 - 36s/epoch - 2ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 35s - loss: 13.7435 - mse: 13.7435 - mae: 1.6063 - 35s/epoch - 2ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 38s - loss: 13.7241 - mse: 13.7241 - mae: 1.6050 - 38s/epoch - 2ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 40s - loss: 13.6702 - mse: 13.6702 - mae: 1.6028 - 40s/epoch - 2ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 38s - loss: 13.6476 - mse: 13.6476 - mae: 1.6017 - 38s/epoch - 2ms/step\n",
            "Score for fold 1: loss of 29.344667434692383\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 37s - loss: 13.6218 - mse: 13.6218 - mae: 1.6013 - 37s/epoch - 2ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 36s - loss: 13.5668 - mse: 13.5668 - mae: 1.5997 - 36s/epoch - 2ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 36s - loss: 13.5536 - mse: 13.5536 - mae: 1.5990 - 36s/epoch - 2ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 36s - loss: 13.5206 - mse: 13.5206 - mae: 1.5980 - 36s/epoch - 2ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 36s - loss: 13.4687 - mse: 13.4687 - mae: 1.5970 - 36s/epoch - 2ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 36s - loss: 13.4717 - mse: 13.4717 - mae: 1.5968 - 36s/epoch - 2ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 35s - loss: 13.4359 - mse: 13.4359 - mae: 1.5954 - 35s/epoch - 2ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 36s - loss: 13.4047 - mse: 13.4047 - mae: 1.5945 - 36s/epoch - 2ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 36s - loss: 13.3754 - mse: 13.3754 - mae: 1.5933 - 36s/epoch - 2ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 36s - loss: 13.3838 - mse: 13.3838 - mae: 1.5939 - 36s/epoch - 2ms/step\n",
            "Score for fold 2: loss of 18.644081115722656\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 36s - loss: 13.3392 - mse: 13.3392 - mae: 1.5922 - 36s/epoch - 2ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 36s - loss: 13.3188 - mse: 13.3188 - mae: 1.5916 - 36s/epoch - 2ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 36s - loss: 13.2862 - mse: 13.2862 - mae: 1.5914 - 36s/epoch - 2ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 36s - loss: 13.2642 - mse: 13.2642 - mae: 1.5917 - 36s/epoch - 2ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 37s - loss: 13.2601 - mse: 13.2601 - mae: 1.5903 - 37s/epoch - 2ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 36s - loss: 13.2238 - mse: 13.2238 - mae: 1.5890 - 36s/epoch - 2ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 37s - loss: 13.2087 - mse: 13.2087 - mae: 1.5888 - 37s/epoch - 2ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 37s - loss: 13.1797 - mse: 13.1797 - mae: 1.5883 - 37s/epoch - 2ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 36s - loss: 13.1060 - mse: 13.1060 - mae: 1.5861 - 36s/epoch - 2ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 36s - loss: 13.1613 - mse: 13.1613 - mae: 1.5871 - 36s/epoch - 2ms/step\n",
            "Score for fold 3: loss of 18.30083465576172\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-09-29 03:44:29,343]\u001b[0m Finished trial#23 resulted in value: 13.161332130432129. Current best value is 12.688554763793945 with parameters: {'activation': 'tanh', 'optimizer': 'adam', 'num_hidden_layer': 5, 'num_hidden_unit': 85, 'kernel_size': 4, 'filter': 77, 'learning_rate': 0.00041922751339262246}.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 29s - loss: 14.2239 - mse: 14.2239 - mae: 1.6412 - 29s/epoch - 1ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 30s - loss: 13.9649 - mse: 13.9649 - mae: 1.6232 - 30s/epoch - 1ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 29s - loss: 13.8692 - mse: 13.8692 - mae: 1.6174 - 29s/epoch - 1ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 30s - loss: 13.7909 - mse: 13.7909 - mae: 1.6142 - 30s/epoch - 1ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 30s - loss: 13.7428 - mse: 13.7428 - mae: 1.6133 - 30s/epoch - 1ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 29s - loss: 13.6850 - mse: 13.6850 - mae: 1.6116 - 29s/epoch - 1ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 30s - loss: 13.6651 - mse: 13.6651 - mae: 1.6101 - 30s/epoch - 1ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 30s - loss: 13.5899 - mse: 13.5899 - mae: 1.6087 - 30s/epoch - 2ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 30s - loss: 13.5801 - mse: 13.5801 - mae: 1.6073 - 30s/epoch - 1ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 30s - loss: 13.5324 - mse: 13.5324 - mae: 1.6051 - 30s/epoch - 1ms/step\n",
            "Score for fold 1: loss of 28.564184188842773\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 30s - loss: 13.4679 - mse: 13.4679 - mae: 1.6043 - 30s/epoch - 2ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 30s - loss: 13.4141 - mse: 13.4141 - mae: 1.6017 - 30s/epoch - 2ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 30s - loss: 13.4129 - mse: 13.4129 - mae: 1.6026 - 30s/epoch - 1ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 31s - loss: 13.6355 - mse: 13.6355 - mae: 1.6110 - 31s/epoch - 2ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 30s - loss: 13.4985 - mse: 13.4985 - mae: 1.6044 - 30s/epoch - 1ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 30s - loss: 13.3790 - mse: 13.3790 - mae: 1.6016 - 30s/epoch - 2ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 30s - loss: 13.3878 - mse: 13.3878 - mae: 1.6017 - 30s/epoch - 2ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 31s - loss: 13.2968 - mse: 13.2968 - mae: 1.5987 - 31s/epoch - 2ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 31s - loss: 13.3695 - mse: 13.3695 - mae: 1.6013 - 31s/epoch - 2ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 30s - loss: 13.3355 - mse: 13.3355 - mae: 1.6005 - 30s/epoch - 2ms/step\n",
            "Score for fold 2: loss of 18.616823196411133\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 30s - loss: 13.2317 - mse: 13.2317 - mae: 1.5968 - 30s/epoch - 1ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 30s - loss: 13.1970 - mse: 13.1970 - mae: 1.5970 - 30s/epoch - 1ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 30s - loss: 13.2627 - mse: 13.2627 - mae: 1.5996 - 30s/epoch - 2ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 30s - loss: 13.2242 - mse: 13.2242 - mae: 1.5983 - 30s/epoch - 1ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 30s - loss: 13.1346 - mse: 13.1346 - mae: 1.5953 - 30s/epoch - 2ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 31s - loss: 13.2806 - mse: 13.2806 - mae: 1.5996 - 31s/epoch - 2ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 30s - loss: 13.1854 - mse: 13.1854 - mae: 1.5967 - 30s/epoch - 2ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 31s - loss: 13.0836 - mse: 13.0836 - mae: 1.5925 - 31s/epoch - 2ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 30s - loss: 13.1088 - mse: 13.1088 - mae: 1.5936 - 30s/epoch - 1ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 31s - loss: 13.0158 - mse: 13.0158 - mae: 1.5913 - 31s/epoch - 2ms/step\n",
            "Score for fold 3: loss of 18.40165901184082\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-09-29 03:59:39,699]\u001b[0m Finished trial#24 resulted in value: 13.015810012817383. Current best value is 12.688554763793945 with parameters: {'activation': 'tanh', 'optimizer': 'adam', 'num_hidden_layer': 5, 'num_hidden_unit': 85, 'kernel_size': 4, 'filter': 77, 'learning_rate': 0.00041922751339262246}.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 36s - loss: 14.2311 - mse: 14.2311 - mae: 1.6437 - 36s/epoch - 2ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 36s - loss: 14.0600 - mse: 14.0600 - mae: 1.6308 - 36s/epoch - 2ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 36s - loss: 14.0368 - mse: 14.0368 - mae: 1.6285 - 36s/epoch - 2ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 36s - loss: 13.9949 - mse: 13.9949 - mae: 1.6272 - 36s/epoch - 2ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 36s - loss: 13.8951 - mse: 13.8951 - mae: 1.6243 - 36s/epoch - 2ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 35s - loss: 13.8922 - mse: 13.8922 - mae: 1.6237 - 35s/epoch - 2ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 36s - loss: 13.8653 - mse: 13.8653 - mae: 1.6216 - 36s/epoch - 2ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 35s - loss: 13.8181 - mse: 13.8181 - mae: 1.6223 - 35s/epoch - 2ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 36s - loss: 13.8222 - mse: 13.8222 - mae: 1.6252 - 36s/epoch - 2ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 35s - loss: 13.7945 - mse: 13.7945 - mae: 1.6228 - 35s/epoch - 2ms/step\n",
            "Score for fold 1: loss of 29.738874435424805\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 35s - loss: 13.7983 - mse: 13.7983 - mae: 1.6229 - 35s/epoch - 2ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 35s - loss: 13.7908 - mse: 13.7908 - mae: 1.6265 - 35s/epoch - 2ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 36s - loss: 13.7992 - mse: 13.7992 - mae: 1.6255 - 36s/epoch - 2ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 36s - loss: 13.7859 - mse: 13.7859 - mae: 1.6225 - 36s/epoch - 2ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 36s - loss: 13.7575 - mse: 13.7575 - mae: 1.6244 - 36s/epoch - 2ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 36s - loss: 13.7862 - mse: 13.7862 - mae: 1.6232 - 36s/epoch - 2ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 36s - loss: 13.7746 - mse: 13.7746 - mae: 1.6216 - 36s/epoch - 2ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 36s - loss: 13.7865 - mse: 13.7865 - mae: 1.6236 - 36s/epoch - 2ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 35s - loss: 13.7301 - mse: 13.7301 - mae: 1.6226 - 35s/epoch - 2ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 36s - loss: 13.7713 - mse: 13.7713 - mae: 1.6230 - 36s/epoch - 2ms/step\n",
            "Score for fold 2: loss of 18.573556900024414\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 36s - loss: 13.7652 - mse: 13.7652 - mae: 1.6228 - 36s/epoch - 2ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 36s - loss: 13.7682 - mse: 13.7682 - mae: 1.6231 - 36s/epoch - 2ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 35s - loss: 13.7781 - mse: 13.7781 - mae: 1.6255 - 35s/epoch - 2ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 36s - loss: 13.8019 - mse: 13.8019 - mae: 1.6252 - 36s/epoch - 2ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 36s - loss: 13.8090 - mse: 13.8090 - mae: 1.6270 - 36s/epoch - 2ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 36s - loss: 13.8169 - mse: 13.8169 - mae: 1.6267 - 36s/epoch - 2ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 36s - loss: 13.8061 - mse: 13.8061 - mae: 1.6267 - 36s/epoch - 2ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 36s - loss: 13.7623 - mse: 13.7623 - mae: 1.6274 - 36s/epoch - 2ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 36s - loss: 13.8260 - mse: 13.8260 - mae: 1.6302 - 36s/epoch - 2ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 36s - loss: 13.7841 - mse: 13.7841 - mae: 1.6309 - 36s/epoch - 2ms/step\n",
            "Score for fold 3: loss of 18.57413673400879\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-09-29 04:17:39,791]\u001b[0m Finished trial#25 resulted in value: 13.78410530090332. Current best value is 12.688554763793945 with parameters: {'activation': 'tanh', 'optimizer': 'adam', 'num_hidden_layer': 5, 'num_hidden_unit': 85, 'kernel_size': 4, 'filter': 77, 'learning_rate': 0.00041922751339262246}.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 30s - loss: 14.2889 - mse: 14.2889 - mae: 1.6555 - 30s/epoch - 2ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 31s - loss: 14.0956 - mse: 14.0956 - mae: 1.6419 - 31s/epoch - 2ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 31s - loss: 14.0530 - mse: 14.0530 - mae: 1.6400 - 31s/epoch - 2ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 31s - loss: 14.0113 - mse: 14.0113 - mae: 1.6408 - 31s/epoch - 2ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 31s - loss: 13.9651 - mse: 13.9651 - mae: 1.6393 - 31s/epoch - 2ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 31s - loss: 13.9522 - mse: 13.9522 - mae: 1.6396 - 31s/epoch - 2ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 31s - loss: 13.9287 - mse: 13.9287 - mae: 1.6384 - 31s/epoch - 2ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 31s - loss: 13.9038 - mse: 13.9038 - mae: 1.6365 - 31s/epoch - 2ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 30s - loss: 13.9146 - mse: 13.9146 - mae: 1.6375 - 30s/epoch - 2ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 31s - loss: 13.9032 - mse: 13.9032 - mae: 1.6361 - 31s/epoch - 2ms/step\n",
            "Score for fold 1: loss of 29.0233154296875\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 30s - loss: 13.8563 - mse: 13.8563 - mae: 1.6334 - 30s/epoch - 2ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 30s - loss: 13.8762 - mse: 13.8762 - mae: 1.6355 - 30s/epoch - 2ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 30s - loss: 13.8344 - mse: 13.8344 - mae: 1.6338 - 30s/epoch - 2ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 32s - loss: 13.8328 - mse: 13.8328 - mae: 1.6348 - 32s/epoch - 2ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 34s - loss: 13.7810 - mse: 13.7810 - mae: 1.6331 - 34s/epoch - 2ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 32s - loss: 13.8261 - mse: 13.8261 - mae: 1.6345 - 32s/epoch - 2ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 31s - loss: 13.8489 - mse: 13.8489 - mae: 1.6379 - 31s/epoch - 2ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 31s - loss: 13.9140 - mse: 13.9140 - mae: 1.6484 - 31s/epoch - 2ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 31s - loss: 13.8829 - mse: 13.8829 - mae: 1.6441 - 31s/epoch - 2ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 32s - loss: 13.8396 - mse: 13.8396 - mae: 1.6401 - 32s/epoch - 2ms/step\n",
            "Score for fold 2: loss of 18.71497917175293\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 31s - loss: 13.8950 - mse: 13.8950 - mae: 1.6457 - 31s/epoch - 2ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 31s - loss: 13.8938 - mse: 13.8938 - mae: 1.6432 - 31s/epoch - 2ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 31s - loss: 13.9382 - mse: 13.9382 - mae: 1.6481 - 31s/epoch - 2ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 31s - loss: 13.9477 - mse: 13.9477 - mae: 1.6481 - 31s/epoch - 2ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 31s - loss: 13.8905 - mse: 13.8905 - mae: 1.6453 - 31s/epoch - 2ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 32s - loss: 13.8397 - mse: 13.8397 - mae: 1.6428 - 32s/epoch - 2ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 32s - loss: 13.8470 - mse: 13.8470 - mae: 1.6451 - 32s/epoch - 2ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 31s - loss: 13.9291 - mse: 13.9291 - mae: 1.6511 - 31s/epoch - 2ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 31s - loss: 13.8630 - mse: 13.8630 - mae: 1.6460 - 31s/epoch - 2ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 30s - loss: 13.8305 - mse: 13.8305 - mae: 1.6416 - 30s/epoch - 2ms/step\n",
            "Score for fold 3: loss of 18.834636688232422\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-09-29 04:33:21,026]\u001b[0m Finished trial#26 resulted in value: 13.8305025100708. Current best value is 12.688554763793945 with parameters: {'activation': 'tanh', 'optimizer': 'adam', 'num_hidden_layer': 5, 'num_hidden_unit': 85, 'kernel_size': 4, 'filter': 77, 'learning_rate': 0.00041922751339262246}.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 34s - loss: 14.4263 - mse: 14.4263 - mae: 1.6464 - 34s/epoch - 2ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 35s - loss: 14.0584 - mse: 14.0584 - mae: 1.6250 - 35s/epoch - 2ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 35s - loss: 13.9350 - mse: 13.9350 - mae: 1.6192 - 35s/epoch - 2ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 34s - loss: 13.8687 - mse: 13.8687 - mae: 1.6148 - 34s/epoch - 2ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 34s - loss: 13.8358 - mse: 13.8358 - mae: 1.6123 - 34s/epoch - 2ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 34s - loss: 13.7828 - mse: 13.7828 - mae: 1.6093 - 34s/epoch - 2ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 34s - loss: 13.7451 - mse: 13.7451 - mae: 1.6071 - 34s/epoch - 2ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 34s - loss: 13.7117 - mse: 13.7117 - mae: 1.6063 - 34s/epoch - 2ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 33s - loss: 13.6702 - mse: 13.6702 - mae: 1.6041 - 33s/epoch - 2ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 33s - loss: 13.6262 - mse: 13.6262 - mae: 1.6022 - 33s/epoch - 2ms/step\n",
            "Score for fold 1: loss of 30.30801010131836\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 34s - loss: 13.5965 - mse: 13.5965 - mae: 1.6016 - 34s/epoch - 2ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 34s - loss: 13.5646 - mse: 13.5646 - mae: 1.5991 - 34s/epoch - 2ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 34s - loss: 13.5216 - mse: 13.5216 - mae: 1.5982 - 34s/epoch - 2ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 34s - loss: 13.5014 - mse: 13.5014 - mae: 1.5973 - 34s/epoch - 2ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 34s - loss: 13.4667 - mse: 13.4667 - mae: 1.5959 - 34s/epoch - 2ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 34s - loss: 13.4387 - mse: 13.4387 - mae: 1.5954 - 34s/epoch - 2ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 33s - loss: 13.4152 - mse: 13.4152 - mae: 1.5948 - 33s/epoch - 2ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 34s - loss: 13.3713 - mse: 13.3713 - mae: 1.5932 - 34s/epoch - 2ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 34s - loss: 13.3578 - mse: 13.3578 - mae: 1.5923 - 34s/epoch - 2ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 33s - loss: 13.3423 - mse: 13.3423 - mae: 1.5919 - 33s/epoch - 2ms/step\n",
            "Score for fold 2: loss of 18.569759368896484\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 34s - loss: 13.3237 - mse: 13.3237 - mae: 1.5915 - 34s/epoch - 2ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 33s - loss: 13.2820 - mse: 13.2820 - mae: 1.5907 - 33s/epoch - 2ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 34s - loss: 13.2794 - mse: 13.2794 - mae: 1.5897 - 34s/epoch - 2ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 34s - loss: 13.2639 - mse: 13.2639 - mae: 1.5900 - 34s/epoch - 2ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 33s - loss: 13.2388 - mse: 13.2388 - mae: 1.5882 - 33s/epoch - 2ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 33s - loss: 13.2229 - mse: 13.2229 - mae: 1.5882 - 33s/epoch - 2ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 34s - loss: 13.1822 - mse: 13.1822 - mae: 1.5882 - 34s/epoch - 2ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 34s - loss: 13.1671 - mse: 13.1671 - mae: 1.5859 - 34s/epoch - 2ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 34s - loss: 13.1529 - mse: 13.1529 - mae: 1.5855 - 34s/epoch - 2ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 34s - loss: 13.1226 - mse: 13.1226 - mae: 1.5858 - 34s/epoch - 2ms/step\n",
            "Score for fold 3: loss of 18.313274383544922\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-09-29 04:50:27,748]\u001b[0m Finished trial#27 resulted in value: 13.122593879699707. Current best value is 12.688554763793945 with parameters: {'activation': 'tanh', 'optimizer': 'adam', 'num_hidden_layer': 5, 'num_hidden_unit': 85, 'kernel_size': 4, 'filter': 77, 'learning_rate': 0.00041922751339262246}.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 40s - loss: 14.3145 - mse: 14.3145 - mae: 1.6343 - 40s/epoch - 2ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 40s - loss: 14.0001 - mse: 14.0001 - mae: 1.6185 - 40s/epoch - 2ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 40s - loss: 13.9199 - mse: 13.9199 - mae: 1.6131 - 40s/epoch - 2ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 40s - loss: 13.8786 - mse: 13.8786 - mae: 1.6107 - 40s/epoch - 2ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 40s - loss: 13.8271 - mse: 13.8271 - mae: 1.6084 - 40s/epoch - 2ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 40s - loss: 13.7940 - mse: 13.7940 - mae: 1.6065 - 40s/epoch - 2ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 40s - loss: 13.7591 - mse: 13.7591 - mae: 1.6052 - 40s/epoch - 2ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 41s - loss: 13.7277 - mse: 13.7277 - mae: 1.6041 - 41s/epoch - 2ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 40s - loss: 13.7034 - mse: 13.7034 - mae: 1.6025 - 40s/epoch - 2ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 40s - loss: 13.6848 - mse: 13.6848 - mae: 1.6018 - 40s/epoch - 2ms/step\n",
            "Score for fold 1: loss of 28.38222312927246\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 40s - loss: 13.6645 - mse: 13.6645 - mae: 1.6001 - 40s/epoch - 2ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 40s - loss: 13.6463 - mse: 13.6463 - mae: 1.6001 - 40s/epoch - 2ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 40s - loss: 13.6263 - mse: 13.6263 - mae: 1.5984 - 40s/epoch - 2ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 40s - loss: 13.5999 - mse: 13.5999 - mae: 1.5985 - 40s/epoch - 2ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 40s - loss: 13.5880 - mse: 13.5880 - mae: 1.5971 - 40s/epoch - 2ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 41s - loss: 13.5777 - mse: 13.5777 - mae: 1.5970 - 41s/epoch - 2ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 40s - loss: 13.5495 - mse: 13.5495 - mae: 1.5956 - 40s/epoch - 2ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 40s - loss: 13.5375 - mse: 13.5375 - mae: 1.5955 - 40s/epoch - 2ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 40s - loss: 13.5266 - mse: 13.5266 - mae: 1.5945 - 40s/epoch - 2ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 40s - loss: 13.5012 - mse: 13.5012 - mae: 1.5931 - 40s/epoch - 2ms/step\n",
            "Score for fold 2: loss of 18.570714950561523\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 40s - loss: 13.4830 - mse: 13.4830 - mae: 1.5934 - 40s/epoch - 2ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 40s - loss: 13.4689 - mse: 13.4689 - mae: 1.5921 - 40s/epoch - 2ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 41s - loss: 13.4448 - mse: 13.4448 - mae: 1.5916 - 41s/epoch - 2ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 40s - loss: 13.4282 - mse: 13.4282 - mae: 1.5910 - 40s/epoch - 2ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 40s - loss: 13.4181 - mse: 13.4181 - mae: 1.5902 - 40s/epoch - 2ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 40s - loss: 13.3993 - mse: 13.3993 - mae: 1.5891 - 40s/epoch - 2ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 40s - loss: 13.3863 - mse: 13.3863 - mae: 1.5887 - 40s/epoch - 2ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 40s - loss: 13.3693 - mse: 13.3693 - mae: 1.5883 - 40s/epoch - 2ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 40s - loss: 13.3508 - mse: 13.3508 - mae: 1.5878 - 40s/epoch - 2ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 40s - loss: 13.3321 - mse: 13.3321 - mae: 1.5868 - 40s/epoch - 2ms/step\n",
            "Score for fold 3: loss of 18.461803436279297\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-09-29 05:10:43,480]\u001b[0m Finished trial#28 resulted in value: 13.332144737243652. Current best value is 12.688554763793945 with parameters: {'activation': 'tanh', 'optimizer': 'adam', 'num_hidden_layer': 5, 'num_hidden_unit': 85, 'kernel_size': 4, 'filter': 77, 'learning_rate': 0.00041922751339262246}.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 34s - loss: 56.3377 - mse: 56.3377 - mae: 2.2777 - 34s/epoch - 2ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 33s - loss: 37.2305 - mse: 37.2305 - mae: 2.2527 - 33s/epoch - 2ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 32s - loss: 31.5359 - mse: 31.5359 - mae: 2.2644 - 32s/epoch - 2ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 32s - loss: 40.3181 - mse: 40.3181 - mae: 2.2649 - 32s/epoch - 2ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 34s - loss: 54.3383 - mse: 54.3383 - mae: 2.2840 - 34s/epoch - 2ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 34s - loss: 32.3663 - mse: 32.3663 - mae: 2.2863 - 34s/epoch - 2ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 34s - loss: 34.0314 - mse: 34.0314 - mae: 2.2749 - 34s/epoch - 2ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 32s - loss: 34.0989 - mse: 34.0989 - mae: 2.2648 - 32s/epoch - 2ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 33s - loss: 32.4262 - mse: 32.4262 - mae: 2.2466 - 33s/epoch - 2ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 34s - loss: 32.4163 - mse: 32.4163 - mae: 2.2413 - 34s/epoch - 2ms/step\n",
            "Score for fold 1: loss of 28.639455795288086\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 33s - loss: 35.1426 - mse: 35.1426 - mae: 2.2792 - 33s/epoch - 2ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 33s - loss: 39.3440 - mse: 39.3440 - mae: 2.2743 - 33s/epoch - 2ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 34s - loss: 80.5108 - mse: 80.5108 - mae: 2.3021 - 34s/epoch - 2ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 32s - loss: 35.4202 - mse: 35.4202 - mae: 2.2653 - 32s/epoch - 2ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 32s - loss: 44.5459 - mse: 44.5459 - mae: 2.2725 - 32s/epoch - 2ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 32s - loss: 31.3703 - mse: 31.3703 - mae: 2.2630 - 32s/epoch - 2ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 33s - loss: 42.3114 - mse: 42.3114 - mae: 2.2844 - 33s/epoch - 2ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 33s - loss: 50.0082 - mse: 50.0082 - mae: 2.2802 - 33s/epoch - 2ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 34s - loss: 34.4311 - mse: 34.4311 - mae: 2.2657 - 34s/epoch - 2ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 33s - loss: 33.9694 - mse: 33.9694 - mae: 2.2729 - 33s/epoch - 2ms/step\n",
            "Score for fold 2: loss of 21.317909240722656\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 33s - loss: 34.1825 - mse: 34.1825 - mae: 2.2472 - 33s/epoch - 2ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 33s - loss: 35.2832 - mse: 35.2832 - mae: 2.2505 - 33s/epoch - 2ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 34s - loss: 36.3618 - mse: 36.3618 - mae: 2.2921 - 34s/epoch - 2ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 33s - loss: 44.4692 - mse: 44.4692 - mae: 2.2820 - 33s/epoch - 2ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 34s - loss: 32.0587 - mse: 32.0587 - mae: 2.2622 - 34s/epoch - 2ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 33s - loss: 69.4642 - mse: 69.4642 - mae: 2.2750 - 33s/epoch - 2ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 33s - loss: 40.0712 - mse: 40.0712 - mae: 2.2857 - 33s/epoch - 2ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 34s - loss: 35.1834 - mse: 35.1834 - mae: 2.2630 - 34s/epoch - 2ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 35s - loss: 31.5185 - mse: 31.5185 - mae: 2.2523 - 35s/epoch - 2ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 34s - loss: 38.0041 - mse: 38.0041 - mae: 2.2821 - 34s/epoch - 2ms/step\n",
            "Score for fold 3: loss of 22.675148010253906\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-09-29 05:27:30,885]\u001b[0m Finished trial#29 resulted in value: 38.00408935546875. Current best value is 12.688554763793945 with parameters: {'activation': 'tanh', 'optimizer': 'adam', 'num_hidden_layer': 5, 'num_hidden_unit': 85, 'kernel_size': 4, 'filter': 77, 'learning_rate': 0.00041922751339262246}.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 33s - loss: 14.6171 - mse: 14.6171 - mae: 1.6489 - 33s/epoch - 2ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 34s - loss: 14.3662 - mse: 14.3662 - mae: 1.6333 - 34s/epoch - 2ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 33s - loss: 14.2777 - mse: 14.2777 - mae: 1.6264 - 33s/epoch - 2ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 34s - loss: 14.2160 - mse: 14.2160 - mae: 1.6221 - 34s/epoch - 2ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 32s - loss: 14.1713 - mse: 14.1713 - mae: 1.6193 - 32s/epoch - 2ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 32s - loss: 14.1366 - mse: 14.1366 - mae: 1.6173 - 32s/epoch - 2ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 33s - loss: 14.1081 - mse: 14.1081 - mae: 1.6159 - 33s/epoch - 2ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 32s - loss: 14.0869 - mse: 14.0869 - mae: 1.6144 - 32s/epoch - 2ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 31s - loss: 14.0690 - mse: 14.0690 - mae: 1.6145 - 31s/epoch - 2ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 31s - loss: 14.0546 - mse: 14.0546 - mae: 1.6136 - 31s/epoch - 2ms/step\n",
            "Score for fold 1: loss of 29.268775939941406\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 32s - loss: 14.0415 - mse: 14.0415 - mae: 1.6129 - 32s/epoch - 2ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 31s - loss: 14.0297 - mse: 14.0297 - mae: 1.6127 - 31s/epoch - 2ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 31s - loss: 14.0195 - mse: 14.0195 - mae: 1.6124 - 31s/epoch - 2ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 32s - loss: 14.0095 - mse: 14.0095 - mae: 1.6119 - 32s/epoch - 2ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 31s - loss: 14.0020 - mse: 14.0020 - mae: 1.6116 - 31s/epoch - 2ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 31s - loss: 13.9945 - mse: 13.9945 - mae: 1.6112 - 31s/epoch - 2ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 32s - loss: 13.9870 - mse: 13.9870 - mae: 1.6110 - 32s/epoch - 2ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 32s - loss: 13.9809 - mse: 13.9809 - mae: 1.6110 - 32s/epoch - 2ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 32s - loss: 13.9749 - mse: 13.9749 - mae: 1.6102 - 32s/epoch - 2ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 32s - loss: 13.9692 - mse: 13.9692 - mae: 1.6103 - 32s/epoch - 2ms/step\n",
            "Score for fold 2: loss of 19.05986785888672\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 38s - loss: 13.9635 - mse: 13.9635 - mae: 1.6103 - 38s/epoch - 2ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 38s - loss: 13.9585 - mse: 13.9585 - mae: 1.6096 - 38s/epoch - 2ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 38s - loss: 13.9535 - mse: 13.9535 - mae: 1.6097 - 38s/epoch - 2ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 38s - loss: 13.9485 - mse: 13.9485 - mae: 1.6094 - 38s/epoch - 2ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 38s - loss: 13.9441 - mse: 13.9441 - mae: 1.6094 - 38s/epoch - 2ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 39s - loss: 13.9403 - mse: 13.9403 - mae: 1.6091 - 39s/epoch - 2ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 38s - loss: 13.9362 - mse: 13.9362 - mae: 1.6091 - 38s/epoch - 2ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 38s - loss: 13.9324 - mse: 13.9324 - mae: 1.6090 - 38s/epoch - 2ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 38s - loss: 13.9292 - mse: 13.9292 - mae: 1.6086 - 38s/epoch - 2ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 38s - loss: 13.9255 - mse: 13.9255 - mae: 1.6084 - 38s/epoch - 2ms/step\n",
            "Score for fold 3: loss of 18.957172393798828\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-09-29 05:44:47,202]\u001b[0m Finished trial#30 resulted in value: 13.925491333007812. Current best value is 12.688554763793945 with parameters: {'activation': 'tanh', 'optimizer': 'adam', 'num_hidden_layer': 5, 'num_hidden_unit': 85, 'kernel_size': 4, 'filter': 77, 'learning_rate': 0.00041922751339262246}.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 40s - loss: 14.4323 - mse: 14.4323 - mae: 1.6476 - 40s/epoch - 2ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 39s - loss: 14.0298 - mse: 14.0298 - mae: 1.6248 - 39s/epoch - 2ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 40s - loss: 13.9372 - mse: 13.9372 - mae: 1.6175 - 40s/epoch - 2ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 40s - loss: 13.8809 - mse: 13.8809 - mae: 1.6140 - 40s/epoch - 2ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 40s - loss: 13.8403 - mse: 13.8403 - mae: 1.6111 - 40s/epoch - 2ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 41s - loss: 13.8120 - mse: 13.8120 - mae: 1.6082 - 41s/epoch - 2ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 40s - loss: 13.7787 - mse: 13.7787 - mae: 1.6074 - 40s/epoch - 2ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 40s - loss: 13.7526 - mse: 13.7526 - mae: 1.6059 - 40s/epoch - 2ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 40s - loss: 13.7308 - mse: 13.7308 - mae: 1.6049 - 40s/epoch - 2ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 41s - loss: 13.7048 - mse: 13.7048 - mae: 1.6039 - 41s/epoch - 2ms/step\n",
            "Score for fold 1: loss of 29.87560272216797\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 41s - loss: 13.6757 - mse: 13.6757 - mae: 1.6022 - 41s/epoch - 2ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 42s - loss: 13.6402 - mse: 13.6402 - mae: 1.6015 - 42s/epoch - 2ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 42s - loss: 13.6322 - mse: 13.6322 - mae: 1.6001 - 42s/epoch - 2ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 42s - loss: 13.6020 - mse: 13.6020 - mae: 1.5995 - 42s/epoch - 2ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 39s - loss: 13.5712 - mse: 13.5712 - mae: 1.5982 - 39s/epoch - 2ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 37s - loss: 13.5592 - mse: 13.5592 - mae: 1.5972 - 37s/epoch - 2ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 37s - loss: 13.5387 - mse: 13.5387 - mae: 1.5964 - 37s/epoch - 2ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 37s - loss: 13.5184 - mse: 13.5184 - mae: 1.5957 - 37s/epoch - 2ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 37s - loss: 13.5085 - mse: 13.5085 - mae: 1.5961 - 37s/epoch - 2ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 37s - loss: 13.4877 - mse: 13.4877 - mae: 1.5947 - 37s/epoch - 2ms/step\n",
            "Score for fold 2: loss of 18.769710540771484\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 37s - loss: 13.4554 - mse: 13.4554 - mae: 1.5943 - 37s/epoch - 2ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 37s - loss: 13.4428 - mse: 13.4428 - mae: 1.5932 - 37s/epoch - 2ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 36s - loss: 13.4299 - mse: 13.4299 - mae: 1.5926 - 36s/epoch - 2ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 37s - loss: 13.4010 - mse: 13.4010 - mae: 1.5917 - 37s/epoch - 2ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 37s - loss: 13.3713 - mse: 13.3713 - mae: 1.5913 - 37s/epoch - 2ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 37s - loss: 13.3753 - mse: 13.3753 - mae: 1.5908 - 37s/epoch - 2ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 37s - loss: 13.3236 - mse: 13.3236 - mae: 1.5902 - 37s/epoch - 2ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 37s - loss: 13.3100 - mse: 13.3100 - mae: 1.5899 - 37s/epoch - 2ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 37s - loss: 13.3153 - mse: 13.3153 - mae: 1.5898 - 37s/epoch - 2ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 36s - loss: 13.2737 - mse: 13.2737 - mae: 1.5885 - 36s/epoch - 2ms/step\n",
            "Score for fold 3: loss of 18.45741081237793\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-09-29 06:04:19,001]\u001b[0m Finished trial#31 resulted in value: 13.273743629455566. Current best value is 12.688554763793945 with parameters: {'activation': 'tanh', 'optimizer': 'adam', 'num_hidden_layer': 5, 'num_hidden_unit': 85, 'kernel_size': 4, 'filter': 77, 'learning_rate': 0.00041922751339262246}.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 35s - loss: 14.3016 - mse: 14.3016 - mae: 1.6490 - 35s/epoch - 2ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 34s - loss: 14.0408 - mse: 14.0408 - mae: 1.6298 - 34s/epoch - 2ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 34s - loss: 13.9812 - mse: 13.9812 - mae: 1.6266 - 34s/epoch - 2ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 34s - loss: 13.9091 - mse: 13.9091 - mae: 1.6213 - 34s/epoch - 2ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 35s - loss: 13.8737 - mse: 13.8737 - mae: 1.6215 - 35s/epoch - 2ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 36s - loss: 13.8274 - mse: 13.8274 - mae: 1.6195 - 36s/epoch - 2ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 35s - loss: 13.8323 - mse: 13.8323 - mae: 1.6219 - 35s/epoch - 2ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 35s - loss: 13.7401 - mse: 13.7401 - mae: 1.6168 - 35s/epoch - 2ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 35s - loss: 13.7422 - mse: 13.7422 - mae: 1.6180 - 35s/epoch - 2ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 35s - loss: 13.7326 - mse: 13.7326 - mae: 1.6189 - 35s/epoch - 2ms/step\n",
            "Score for fold 1: loss of 29.157503128051758\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 37s - loss: 13.6908 - mse: 13.6908 - mae: 1.6163 - 37s/epoch - 2ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 40s - loss: 13.6455 - mse: 13.6455 - mae: 1.6153 - 40s/epoch - 2ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 45s - loss: 13.6597 - mse: 13.6597 - mae: 1.6157 - 45s/epoch - 2ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 43s - loss: 13.5985 - mse: 13.5985 - mae: 1.6152 - 43s/epoch - 2ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 39s - loss: 13.7513 - mse: 13.7513 - mae: 1.6216 - 39s/epoch - 2ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 36s - loss: 13.7210 - mse: 13.7210 - mae: 1.6187 - 36s/epoch - 2ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 36s - loss: 13.7125 - mse: 13.7125 - mae: 1.6178 - 36s/epoch - 2ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 37s - loss: 13.6514 - mse: 13.6514 - mae: 1.6162 - 37s/epoch - 2ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 38s - loss: 13.6259 - mse: 13.6259 - mae: 1.6177 - 38s/epoch - 2ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 38s - loss: 13.6172 - mse: 13.6172 - mae: 1.6160 - 38s/epoch - 2ms/step\n",
            "Score for fold 2: loss of 18.678281784057617\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 37s - loss: 13.7083 - mse: 13.7083 - mae: 1.6192 - 37s/epoch - 2ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 38s - loss: 13.6184 - mse: 13.6184 - mae: 1.6176 - 38s/epoch - 2ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 39s - loss: 13.5805 - mse: 13.5805 - mae: 1.6158 - 39s/epoch - 2ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 39s - loss: 13.6187 - mse: 13.6187 - mae: 1.6157 - 39s/epoch - 2ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 41s - loss: 13.5941 - mse: 13.5941 - mae: 1.6180 - 41s/epoch - 2ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 40s - loss: 13.5928 - mse: 13.5928 - mae: 1.6168 - 40s/epoch - 2ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 40s - loss: 13.6909 - mse: 13.6909 - mae: 1.6221 - 40s/epoch - 2ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 40s - loss: 13.6723 - mse: 13.6723 - mae: 1.6207 - 40s/epoch - 2ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 40s - loss: 13.6339 - mse: 13.6339 - mae: 1.6179 - 40s/epoch - 2ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 40s - loss: 13.5769 - mse: 13.5769 - mae: 1.6172 - 40s/epoch - 2ms/step\n",
            "Score for fold 3: loss of 18.825685501098633\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-09-29 06:23:21,480]\u001b[0m Finished trial#32 resulted in value: 13.57693099975586. Current best value is 12.688554763793945 with parameters: {'activation': 'tanh', 'optimizer': 'adam', 'num_hidden_layer': 5, 'num_hidden_unit': 85, 'kernel_size': 4, 'filter': 77, 'learning_rate': 0.00041922751339262246}.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 43s - loss: 14.6197 - mse: 14.6197 - mae: 1.6788 - 43s/epoch - 2ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 42s - loss: 14.5770 - mse: 14.5770 - mae: 1.6758 - 42s/epoch - 2ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 42s - loss: 14.5689 - mse: 14.5689 - mae: 1.6762 - 42s/epoch - 2ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 41s - loss: 14.5608 - mse: 14.5608 - mae: 1.6754 - 41s/epoch - 2ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 41s - loss: 14.5513 - mse: 14.5513 - mae: 1.6757 - 41s/epoch - 2ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 42s - loss: 14.5573 - mse: 14.5573 - mae: 1.6757 - 42s/epoch - 2ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 42s - loss: 14.5567 - mse: 14.5567 - mae: 1.6753 - 42s/epoch - 2ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 42s - loss: 14.5572 - mse: 14.5572 - mae: 1.6756 - 42s/epoch - 2ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 42s - loss: 14.5544 - mse: 14.5544 - mae: 1.6749 - 42s/epoch - 2ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 41s - loss: 14.5533 - mse: 14.5533 - mae: 1.6752 - 41s/epoch - 2ms/step\n",
            "Score for fold 1: loss of 28.592832565307617\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 41s - loss: 14.5530 - mse: 14.5530 - mae: 1.6751 - 41s/epoch - 2ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 42s - loss: 14.5531 - mse: 14.5531 - mae: 1.6744 - 42s/epoch - 2ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 41s - loss: 14.5497 - mse: 14.5497 - mae: 1.6754 - 41s/epoch - 2ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 41s - loss: 14.5463 - mse: 14.5463 - mae: 1.6750 - 41s/epoch - 2ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 41s - loss: 14.5403 - mse: 14.5403 - mae: 1.6758 - 41s/epoch - 2ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 41s - loss: 14.5416 - mse: 14.5416 - mae: 1.6754 - 41s/epoch - 2ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 41s - loss: 14.5482 - mse: 14.5482 - mae: 1.6749 - 41s/epoch - 2ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 41s - loss: 14.5455 - mse: 14.5455 - mae: 1.6754 - 41s/epoch - 2ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 41s - loss: 14.5480 - mse: 14.5480 - mae: 1.6749 - 41s/epoch - 2ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 41s - loss: 14.5451 - mse: 14.5451 - mae: 1.6752 - 41s/epoch - 2ms/step\n",
            "Score for fold 2: loss of 19.56191635131836\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 41s - loss: 14.5417 - mse: 14.5417 - mae: 1.6757 - 41s/epoch - 2ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 44s - loss: 14.5438 - mse: 14.5438 - mae: 1.6741 - 44s/epoch - 2ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 44s - loss: 14.5436 - mse: 14.5436 - mae: 1.6755 - 44s/epoch - 2ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 39s - loss: 14.5496 - mse: 14.5496 - mae: 1.6752 - 39s/epoch - 2ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 39s - loss: 14.5441 - mse: 14.5441 - mae: 1.6754 - 39s/epoch - 2ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 37s - loss: 14.5411 - mse: 14.5411 - mae: 1.6749 - 37s/epoch - 2ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 37s - loss: 14.5426 - mse: 14.5426 - mae: 1.6750 - 37s/epoch - 2ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 37s - loss: 14.5464 - mse: 14.5464 - mae: 1.6750 - 37s/epoch - 2ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 37s - loss: 14.5412 - mse: 14.5412 - mae: 1.6739 - 37s/epoch - 2ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 37s - loss: 14.5400 - mse: 14.5400 - mae: 1.6752 - 37s/epoch - 2ms/step\n",
            "Score for fold 3: loss of 19.542015075683594\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-09-29 06:43:56,728]\u001b[0m Finished trial#33 resulted in value: 14.540033340454102. Current best value is 12.688554763793945 with parameters: {'activation': 'tanh', 'optimizer': 'adam', 'num_hidden_layer': 5, 'num_hidden_unit': 85, 'kernel_size': 4, 'filter': 77, 'learning_rate': 0.00041922751339262246}.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 35s - loss: 14.1982 - mse: 14.1982 - mae: 1.6416 - 35s/epoch - 2ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 34s - loss: 13.9626 - mse: 13.9626 - mae: 1.6249 - 34s/epoch - 2ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 34s - loss: 13.8650 - mse: 13.8650 - mae: 1.6204 - 34s/epoch - 2ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 34s - loss: 13.8062 - mse: 13.8062 - mae: 1.6161 - 34s/epoch - 2ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 35s - loss: 13.7453 - mse: 13.7453 - mae: 1.6131 - 35s/epoch - 2ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 35s - loss: 13.7004 - mse: 13.7004 - mae: 1.6116 - 35s/epoch - 2ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 35s - loss: 13.6706 - mse: 13.6706 - mae: 1.6112 - 35s/epoch - 2ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 35s - loss: 13.6264 - mse: 13.6264 - mae: 1.6095 - 35s/epoch - 2ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 35s - loss: 13.6377 - mse: 13.6377 - mae: 1.6097 - 35s/epoch - 2ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 36s - loss: 13.5623 - mse: 13.5623 - mae: 1.6067 - 36s/epoch - 2ms/step\n",
            "Score for fold 1: loss of 29.762537002563477\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 34s - loss: 13.5163 - mse: 13.5163 - mae: 1.6051 - 34s/epoch - 2ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 35s - loss: 13.5241 - mse: 13.5241 - mae: 1.6061 - 35s/epoch - 2ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 35s - loss: 13.4494 - mse: 13.4494 - mae: 1.6033 - 35s/epoch - 2ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 35s - loss: 13.4989 - mse: 13.4989 - mae: 1.6065 - 35s/epoch - 2ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 35s - loss: 13.3809 - mse: 13.3809 - mae: 1.6017 - 35s/epoch - 2ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 34s - loss: 13.4173 - mse: 13.4173 - mae: 1.6038 - 34s/epoch - 2ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 35s - loss: 13.4203 - mse: 13.4203 - mae: 1.6019 - 35s/epoch - 2ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 35s - loss: 13.3566 - mse: 13.3566 - mae: 1.6009 - 35s/epoch - 2ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 35s - loss: 13.3119 - mse: 13.3119 - mae: 1.6000 - 35s/epoch - 2ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 34s - loss: 13.3918 - mse: 13.3918 - mae: 1.6022 - 34s/epoch - 2ms/step\n",
            "Score for fold 2: loss of 18.49165153503418\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 34s - loss: 13.3544 - mse: 13.3544 - mae: 1.6019 - 34s/epoch - 2ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 35s - loss: 13.4166 - mse: 13.4166 - mae: 1.6050 - 35s/epoch - 2ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 34s - loss: 13.2819 - mse: 13.2819 - mae: 1.5993 - 34s/epoch - 2ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 35s - loss: 13.2478 - mse: 13.2478 - mae: 1.5979 - 35s/epoch - 2ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 35s - loss: 13.3135 - mse: 13.3135 - mae: 1.6006 - 35s/epoch - 2ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 35s - loss: 13.1714 - mse: 13.1714 - mae: 1.5958 - 35s/epoch - 2ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 34s - loss: 13.3091 - mse: 13.3091 - mae: 1.6026 - 34s/epoch - 2ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 34s - loss: 13.1830 - mse: 13.1830 - mae: 1.5972 - 34s/epoch - 2ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 34s - loss: 13.1050 - mse: 13.1050 - mae: 1.5945 - 34s/epoch - 2ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 35s - loss: 13.1419 - mse: 13.1419 - mae: 1.5975 - 35s/epoch - 2ms/step\n",
            "Score for fold 3: loss of 18.555675506591797\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-09-29 07:01:28,987]\u001b[0m Finished trial#34 resulted in value: 13.141915321350098. Current best value is 12.688554763793945 with parameters: {'activation': 'tanh', 'optimizer': 'adam', 'num_hidden_layer': 5, 'num_hidden_unit': 85, 'kernel_size': 4, 'filter': 77, 'learning_rate': 0.00041922751339262246}.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 36s - loss: 14.4543 - mse: 14.4543 - mae: 1.6531 - 36s/epoch - 2ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 36s - loss: 14.3741 - mse: 14.3741 - mae: 1.6495 - 36s/epoch - 2ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 36s - loss: 14.3685 - mse: 14.3685 - mae: 1.6490 - 36s/epoch - 2ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 37s - loss: 14.3522 - mse: 14.3522 - mae: 1.6499 - 37s/epoch - 2ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 39s - loss: 14.3501 - mse: 14.3501 - mae: 1.6490 - 39s/epoch - 2ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 38s - loss: 14.3496 - mse: 14.3496 - mae: 1.6486 - 38s/epoch - 2ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 38s - loss: 14.3444 - mse: 14.3444 - mae: 1.6492 - 38s/epoch - 2ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 37s - loss: 14.3398 - mse: 14.3398 - mae: 1.6492 - 37s/epoch - 2ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 36s - loss: 14.3278 - mse: 14.3278 - mae: 1.6501 - 36s/epoch - 2ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 36s - loss: 14.3307 - mse: 14.3307 - mae: 1.6497 - 36s/epoch - 2ms/step\n",
            "Score for fold 1: loss of 28.574424743652344\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 36s - loss: 14.3385 - mse: 14.3385 - mae: 1.6498 - 36s/epoch - 2ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 37s - loss: 14.3301 - mse: 14.3301 - mae: 1.6505 - 37s/epoch - 2ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 38s - loss: 14.3251 - mse: 14.3251 - mae: 1.6499 - 38s/epoch - 2ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 37s - loss: 14.3236 - mse: 14.3236 - mae: 1.6499 - 37s/epoch - 2ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 38s - loss: 14.3242 - mse: 14.3242 - mae: 1.6507 - 38s/epoch - 2ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 38s - loss: 14.3234 - mse: 14.3234 - mae: 1.6495 - 38s/epoch - 2ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 54s - loss: 14.3220 - mse: 14.3220 - mae: 1.6509 - 54s/epoch - 3ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 45s - loss: 14.3315 - mse: 14.3315 - mae: 1.6503 - 45s/epoch - 2ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 45s - loss: 14.3229 - mse: 14.3229 - mae: 1.6502 - 45s/epoch - 2ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 44s - loss: 14.3237 - mse: 14.3237 - mae: 1.6504 - 44s/epoch - 2ms/step\n",
            "Score for fold 2: loss of 19.29546546936035\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 40s - loss: 14.3188 - mse: 14.3188 - mae: 1.6504 - 40s/epoch - 2ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 40s - loss: 14.3213 - mse: 14.3213 - mae: 1.6508 - 40s/epoch - 2ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 42s - loss: 14.3253 - mse: 14.3253 - mae: 1.6500 - 42s/epoch - 2ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 43s - loss: 14.3207 - mse: 14.3207 - mae: 1.6496 - 43s/epoch - 2ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 43s - loss: 14.3224 - mse: 14.3224 - mae: 1.6495 - 43s/epoch - 2ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 42s - loss: 14.3152 - mse: 14.3152 - mae: 1.6502 - 42s/epoch - 2ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 41s - loss: 14.3111 - mse: 14.3111 - mae: 1.6508 - 41s/epoch - 2ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 40s - loss: 14.3153 - mse: 14.3153 - mae: 1.6502 - 40s/epoch - 2ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 39s - loss: 14.3182 - mse: 14.3182 - mae: 1.6497 - 39s/epoch - 2ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 39s - loss: 14.3145 - mse: 14.3145 - mae: 1.6510 - 39s/epoch - 2ms/step\n",
            "Score for fold 3: loss of 19.27350616455078\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-09-29 07:21:30,768]\u001b[0m Finished trial#35 resulted in value: 14.314528465270996. Current best value is 12.688554763793945 with parameters: {'activation': 'tanh', 'optimizer': 'adam', 'num_hidden_layer': 5, 'num_hidden_unit': 85, 'kernel_size': 4, 'filter': 77, 'learning_rate': 0.00041922751339262246}.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 30s - loss: 14.4315 - mse: 14.4315 - mae: 1.6642 - 30s/epoch - 1ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 28s - loss: 14.2453 - mse: 14.2453 - mae: 1.6489 - 28s/epoch - 1ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 30s - loss: 14.1972 - mse: 14.1972 - mae: 1.6464 - 30s/epoch - 1ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 30s - loss: 14.1644 - mse: 14.1644 - mae: 1.6428 - 30s/epoch - 1ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 30s - loss: 14.1318 - mse: 14.1318 - mae: 1.6410 - 30s/epoch - 1ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 30s - loss: 14.1041 - mse: 14.1041 - mae: 1.6404 - 30s/epoch - 1ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 29s - loss: 14.0539 - mse: 14.0539 - mae: 1.6387 - 29s/epoch - 1ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 28s - loss: 14.0314 - mse: 14.0314 - mae: 1.6389 - 28s/epoch - 1ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 30s - loss: 14.0217 - mse: 14.0217 - mae: 1.6376 - 30s/epoch - 2ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 30s - loss: 14.0097 - mse: 14.0097 - mae: 1.6369 - 30s/epoch - 2ms/step\n",
            "Score for fold 1: loss of 29.763320922851562\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 30s - loss: 14.0317 - mse: 14.0317 - mae: 1.6398 - 30s/epoch - 1ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 30s - loss: 14.0186 - mse: 14.0186 - mae: 1.6383 - 30s/epoch - 1ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 30s - loss: 13.9840 - mse: 13.9840 - mae: 1.6369 - 30s/epoch - 1ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 29s - loss: 14.0223 - mse: 14.0223 - mae: 1.6409 - 29s/epoch - 1ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 30s - loss: 13.9537 - mse: 13.9537 - mae: 1.6354 - 30s/epoch - 2ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 30s - loss: 13.9631 - mse: 13.9631 - mae: 1.6367 - 30s/epoch - 1ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 29s - loss: 13.9467 - mse: 13.9467 - mae: 1.6367 - 29s/epoch - 1ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 29s - loss: 13.9617 - mse: 13.9617 - mae: 1.6363 - 29s/epoch - 1ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 29s - loss: 14.0000 - mse: 14.0000 - mae: 1.6388 - 29s/epoch - 1ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 30s - loss: 13.9521 - mse: 13.9521 - mae: 1.6361 - 30s/epoch - 1ms/step\n",
            "Score for fold 2: loss of 19.87591552734375\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 28s - loss: 13.9524 - mse: 13.9524 - mae: 1.6362 - 28s/epoch - 1ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 29s - loss: 13.8886 - mse: 13.8886 - mae: 1.6326 - 29s/epoch - 1ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 29s - loss: 13.8881 - mse: 13.8881 - mae: 1.6324 - 29s/epoch - 1ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 29s - loss: 13.8746 - mse: 13.8746 - mae: 1.6317 - 29s/epoch - 1ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 31s - loss: 13.8714 - mse: 13.8714 - mae: 1.6336 - 31s/epoch - 2ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 31s - loss: 13.9185 - mse: 13.9185 - mae: 1.6335 - 31s/epoch - 2ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 30s - loss: 13.9166 - mse: 13.9166 - mae: 1.6353 - 30s/epoch - 2ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 30s - loss: 13.8625 - mse: 13.8625 - mae: 1.6337 - 30s/epoch - 1ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 30s - loss: 13.8957 - mse: 13.8957 - mae: 1.6346 - 30s/epoch - 1ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 30s - loss: 13.8905 - mse: 13.8905 - mae: 1.6330 - 30s/epoch - 1ms/step\n",
            "Score for fold 3: loss of 18.796863555908203\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-09-29 07:36:29,470]\u001b[0m Finished trial#36 resulted in value: 13.89054012298584. Current best value is 12.688554763793945 with parameters: {'activation': 'tanh', 'optimizer': 'adam', 'num_hidden_layer': 5, 'num_hidden_unit': 85, 'kernel_size': 4, 'filter': 77, 'learning_rate': 0.00041922751339262246}.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 48s - loss: 14.1793 - mse: 14.1793 - mae: 1.6272 - 48s/epoch - 2ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 48s - loss: 13.8856 - mse: 13.8856 - mae: 1.6138 - 48s/epoch - 2ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 48s - loss: 13.7580 - mse: 13.7580 - mae: 1.6069 - 48s/epoch - 2ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 48s - loss: 13.6403 - mse: 13.6403 - mae: 1.6031 - 48s/epoch - 2ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 48s - loss: 13.5624 - mse: 13.5624 - mae: 1.5992 - 48s/epoch - 2ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 49s - loss: 13.5064 - mse: 13.5064 - mae: 1.5969 - 49s/epoch - 2ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 51s - loss: 13.4579 - mse: 13.4579 - mae: 1.5949 - 51s/epoch - 3ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 49s - loss: 13.4217 - mse: 13.4217 - mae: 1.5943 - 49s/epoch - 2ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 49s - loss: 13.3752 - mse: 13.3752 - mae: 1.5931 - 49s/epoch - 2ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 48s - loss: 13.3636 - mse: 13.3636 - mae: 1.5910 - 48s/epoch - 2ms/step\n",
            "Score for fold 1: loss of 29.49994659423828\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 48s - loss: 13.3266 - mse: 13.3266 - mae: 1.5908 - 48s/epoch - 2ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 47s - loss: 13.2725 - mse: 13.2725 - mae: 1.5892 - 47s/epoch - 2ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 47s - loss: 13.2794 - mse: 13.2794 - mae: 1.5892 - 47s/epoch - 2ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 47s - loss: 13.2478 - mse: 13.2478 - mae: 1.5877 - 47s/epoch - 2ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 46s - loss: 13.2302 - mse: 13.2302 - mae: 1.5872 - 46s/epoch - 2ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 47s - loss: 13.1805 - mse: 13.1805 - mae: 1.5875 - 47s/epoch - 2ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 47s - loss: 13.1952 - mse: 13.1952 - mae: 1.5856 - 47s/epoch - 2ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 47s - loss: 13.1589 - mse: 13.1589 - mae: 1.5847 - 47s/epoch - 2ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 50s - loss: 13.1146 - mse: 13.1146 - mae: 1.5842 - 50s/epoch - 3ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 50s - loss: 13.0954 - mse: 13.0954 - mae: 1.5831 - 50s/epoch - 3ms/step\n",
            "Score for fold 2: loss of 18.262863159179688\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 49s - loss: 13.1176 - mse: 13.1176 - mae: 1.5829 - 49s/epoch - 2ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 49s - loss: 13.0873 - mse: 13.0873 - mae: 1.5830 - 49s/epoch - 2ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 47s - loss: 13.0685 - mse: 13.0685 - mae: 1.5816 - 47s/epoch - 2ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 47s - loss: 13.0791 - mse: 13.0791 - mae: 1.5818 - 47s/epoch - 2ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 47s - loss: 13.0653 - mse: 13.0653 - mae: 1.5818 - 47s/epoch - 2ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 48s - loss: 12.9982 - mse: 12.9982 - mae: 1.5804 - 48s/epoch - 2ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 48s - loss: 13.0039 - mse: 13.0039 - mae: 1.5807 - 48s/epoch - 2ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 50s - loss: 12.9849 - mse: 12.9849 - mae: 1.5800 - 50s/epoch - 2ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 49s - loss: 12.9232 - mse: 12.9232 - mae: 1.5786 - 49s/epoch - 2ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 48s - loss: 12.9369 - mse: 12.9369 - mae: 1.5788 - 48s/epoch - 2ms/step\n",
            "Score for fold 3: loss of 18.33247947692871\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-09-29 08:00:43,942]\u001b[0m Finished trial#37 resulted in value: 12.9368896484375. Current best value is 12.688554763793945 with parameters: {'activation': 'tanh', 'optimizer': 'adam', 'num_hidden_layer': 5, 'num_hidden_unit': 85, 'kernel_size': 4, 'filter': 77, 'learning_rate': 0.00041922751339262246}.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 45s - loss: 14.4143 - mse: 14.4143 - mae: 1.6382 - 45s/epoch - 2ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 47s - loss: 13.9863 - mse: 13.9863 - mae: 1.6172 - 47s/epoch - 2ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 48s - loss: 13.8980 - mse: 13.8980 - mae: 1.6123 - 48s/epoch - 2ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 47s - loss: 13.8535 - mse: 13.8535 - mae: 1.6099 - 47s/epoch - 2ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 48s - loss: 13.8152 - mse: 13.8152 - mae: 1.6079 - 48s/epoch - 2ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 48s - loss: 13.7887 - mse: 13.7887 - mae: 1.6059 - 48s/epoch - 2ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 52s - loss: 13.7702 - mse: 13.7702 - mae: 1.6049 - 52s/epoch - 3ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 52s - loss: 13.7391 - mse: 13.7391 - mae: 1.6038 - 52s/epoch - 3ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 48s - loss: 13.7193 - mse: 13.7193 - mae: 1.6027 - 48s/epoch - 2ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 48s - loss: 13.6958 - mse: 13.6958 - mae: 1.6011 - 48s/epoch - 2ms/step\n",
            "Score for fold 1: loss of 29.67786407470703\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 47s - loss: 13.6796 - mse: 13.6796 - mae: 1.6011 - 47s/epoch - 2ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 46s - loss: 13.6602 - mse: 13.6602 - mae: 1.6001 - 46s/epoch - 2ms/step\n",
            "Epoch 3/10\n"
          ]
        }
      ],
      "source": [
        "model_list_cnn=[]\n",
        "history_list_cnn=[]\n",
        "study_name = 'CNN_study'\n",
        "study = optuna.create_study(study_name=study_name, load_if_exists=True)\n",
        "study.optimize(objective_cnn, n_trials=50, )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zuq9ewDGh7q1"
      },
      "source": [
        "## Select best CNN model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Txq539Zpu_PG"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Boj8VWFbeGit",
        "outputId": "498e3048-7e79-411a-a00e-230df4b44a9c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv1d (Conv1D)             (None, 8, 77)             385       \n",
            "                                                                 \n",
            " max_pooling1d (MaxPooling1D  (None, 4, 77)            0         \n",
            " )                                                               \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 308)               0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 85)                26265     \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 85)                7310      \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 85)                7310      \n",
            "                                                                 \n",
            " dense_3 (Dense)             (None, 85)                7310      \n",
            "                                                                 \n",
            " dense_4 (Dense)             (None, 1)                 86        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 48,666\n",
            "Trainable params: 48,666\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "#Without maxpooling:{'activation': 'tanh', 'optimizer': 'adam', 'num_hidden_layer': 4, 'num_hidden_unit': 79, 'kernel_size': 3, 'filter': 1, 'learning_rate': 0.0006459535879377347}\n",
        "#{'activation': 'tanh', 'optimizer': 'adam', 'num_hidden_layer': 5, 'num_hidden_unit': 85, 'kernel_size': 4, 'filter': 77, 'learning_rate': 0.00041922751339262246}\n",
        "model_cnn_best = create_model_cnn(activation='tanh', num_hidden_layer=5, num_hidden_unit=85, kernel_size=4, filter=77)\n",
        "model_cnn_best.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "jPl7s0gmfRWa",
        "outputId": "a6ce3cec-09ea-44c0-c6de-3e3e1d9adc57"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv1d (Conv1D)             (None, 8, 77)             385       \n",
            "                                                                 \n",
            " max_pooling1d (MaxPooling1D  (None, 4, 77)            0         \n",
            " )                                                               \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 308)               0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 85)                26265     \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 85)                7310      \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 85)                7310      \n",
            "                                                                 \n",
            " dense_3 (Dense)             (None, 85)                7310      \n",
            "                                                                 \n",
            " dense_4 (Dense)             (None, 1)                 86        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 48,666\n",
            "Trainable params: 48,666\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "20000/20000 - 38s - loss: 14.3480 - mse: 14.3480 - mae: 1.6543 - 38s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "20000/20000 - 36s - loss: 14.1735 - mse: 14.1735 - mae: 1.6430 - 36s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "20000/20000 - 38s - loss: 14.1278 - mse: 14.1278 - mae: 1.6386 - 38s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "20000/20000 - 37s - loss: 14.0463 - mse: 14.0463 - mae: 1.6330 - 37s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "20000/20000 - 38s - loss: 14.0707 - mse: 14.0707 - mae: 1.6346 - 38s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "20000/20000 - 39s - loss: 14.0643 - mse: 14.0643 - mae: 1.6362 - 39s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "20000/20000 - 38s - loss: 14.0594 - mse: 14.0594 - mae: 1.6394 - 38s/epoch - 2ms/step\n",
            "Epoch 8/100\n",
            "20000/20000 - 36s - loss: 14.0461 - mse: 14.0461 - mae: 1.6392 - 36s/epoch - 2ms/step\n",
            "Epoch 9/100\n",
            "20000/20000 - 36s - loss: 14.0640 - mse: 14.0640 - mae: 1.6374 - 36s/epoch - 2ms/step\n",
            "Epoch 10/100\n",
            "20000/20000 - 36s - loss: 14.0198 - mse: 14.0198 - mae: 1.6381 - 36s/epoch - 2ms/step\n",
            "Epoch 11/100\n",
            "20000/20000 - 36s - loss: 13.9615 - mse: 13.9615 - mae: 1.6327 - 36s/epoch - 2ms/step\n",
            "Epoch 12/100\n",
            "20000/20000 - 36s - loss: 13.9658 - mse: 13.9658 - mae: 1.6332 - 36s/epoch - 2ms/step\n",
            "Epoch 13/100\n",
            "20000/20000 - 36s - loss: 13.9798 - mse: 13.9798 - mae: 1.6340 - 36s/epoch - 2ms/step\n",
            "Epoch 14/100\n",
            "20000/20000 - 35s - loss: 13.9661 - mse: 13.9661 - mae: 1.6342 - 35s/epoch - 2ms/step\n",
            "Epoch 15/100\n",
            "20000/20000 - 36s - loss: 13.9846 - mse: 13.9846 - mae: 1.6351 - 36s/epoch - 2ms/step\n",
            "Epoch 16/100\n",
            "20000/20000 - 36s - loss: 14.0130 - mse: 14.0130 - mae: 1.6367 - 36s/epoch - 2ms/step\n",
            "Epoch 17/100\n",
            "20000/20000 - 36s - loss: 13.9852 - mse: 13.9852 - mae: 1.6372 - 36s/epoch - 2ms/step\n",
            "Epoch 18/100\n",
            "20000/20000 - 36s - loss: 14.0035 - mse: 14.0035 - mae: 1.6381 - 36s/epoch - 2ms/step\n",
            "Epoch 19/100\n",
            "20000/20000 - 36s - loss: 13.9957 - mse: 13.9957 - mae: 1.6366 - 36s/epoch - 2ms/step\n",
            "Epoch 20/100\n",
            "20000/20000 - 36s - loss: 14.0374 - mse: 14.0374 - mae: 1.6363 - 36s/epoch - 2ms/step\n",
            "Epoch 21/100\n",
            "20000/20000 - 36s - loss: 14.0461 - mse: 14.0461 - mae: 1.6399 - 36s/epoch - 2ms/step\n",
            "Epoch 22/100\n",
            "20000/20000 - 35s - loss: 14.0724 - mse: 14.0724 - mae: 1.6434 - 35s/epoch - 2ms/step\n",
            "Epoch 23/100\n",
            "20000/20000 - 36s - loss: 14.0690 - mse: 14.0690 - mae: 1.6428 - 36s/epoch - 2ms/step\n",
            "Epoch 24/100\n",
            "20000/20000 - 36s - loss: 14.0324 - mse: 14.0324 - mae: 1.6409 - 36s/epoch - 2ms/step\n",
            "Epoch 25/100\n",
            "20000/20000 - 35s - loss: 14.0675 - mse: 14.0675 - mae: 1.6466 - 35s/epoch - 2ms/step\n",
            "Epoch 26/100\n",
            "20000/20000 - 35s - loss: 14.0526 - mse: 14.0526 - mae: 1.6433 - 35s/epoch - 2ms/step\n",
            "Epoch 27/100\n",
            "20000/20000 - 37s - loss: 14.0594 - mse: 14.0594 - mae: 1.6428 - 37s/epoch - 2ms/step\n",
            "Epoch 28/100\n",
            "20000/20000 - 37s - loss: 14.0453 - mse: 14.0453 - mae: 1.6445 - 37s/epoch - 2ms/step\n",
            "Epoch 29/100\n",
            "20000/20000 - 38s - loss: 14.0122 - mse: 14.0122 - mae: 1.6404 - 38s/epoch - 2ms/step\n",
            "Epoch 30/100\n",
            "20000/20000 - 40s - loss: 14.0224 - mse: 14.0224 - mae: 1.6427 - 40s/epoch - 2ms/step\n",
            "Epoch 31/100\n",
            "20000/20000 - 39s - loss: 13.9982 - mse: 13.9982 - mae: 1.6430 - 39s/epoch - 2ms/step\n",
            "Epoch 32/100\n",
            "20000/20000 - 38s - loss: 14.0515 - mse: 14.0515 - mae: 1.6458 - 38s/epoch - 2ms/step\n",
            "Epoch 33/100\n",
            "20000/20000 - 37s - loss: 14.0625 - mse: 14.0625 - mae: 1.6445 - 37s/epoch - 2ms/step\n",
            "Epoch 34/100\n",
            "20000/20000 - 36s - loss: 14.0056 - mse: 14.0056 - mae: 1.6441 - 36s/epoch - 2ms/step\n",
            "Epoch 35/100\n",
            "20000/20000 - 36s - loss: 14.0881 - mse: 14.0881 - mae: 1.6453 - 36s/epoch - 2ms/step\n",
            "Epoch 36/100\n",
            "20000/20000 - 37s - loss: 14.0008 - mse: 14.0008 - mae: 1.6436 - 37s/epoch - 2ms/step\n",
            "Epoch 37/100\n",
            "20000/20000 - 39s - loss: 14.0141 - mse: 14.0141 - mae: 1.6440 - 39s/epoch - 2ms/step\n",
            "Epoch 38/100\n",
            "20000/20000 - 40s - loss: 14.0285 - mse: 14.0285 - mae: 1.6464 - 40s/epoch - 2ms/step\n",
            "Epoch 39/100\n",
            "20000/20000 - 39s - loss: 14.0170 - mse: 14.0170 - mae: 1.6461 - 39s/epoch - 2ms/step\n",
            "Epoch 40/100\n",
            "20000/20000 - 38s - loss: 14.0552 - mse: 14.0552 - mae: 1.6476 - 38s/epoch - 2ms/step\n",
            "Epoch 41/100\n",
            "20000/20000 - 38s - loss: 14.0268 - mse: 14.0268 - mae: 1.6435 - 38s/epoch - 2ms/step\n",
            "Epoch 42/100\n",
            "20000/20000 - 38s - loss: 14.0918 - mse: 14.0918 - mae: 1.6500 - 38s/epoch - 2ms/step\n",
            "Epoch 43/100\n",
            "20000/20000 - 37s - loss: 14.0875 - mse: 14.0875 - mae: 1.6511 - 37s/epoch - 2ms/step\n",
            "Epoch 44/100\n",
            "20000/20000 - 37s - loss: 14.0809 - mse: 14.0809 - mae: 1.6483 - 37s/epoch - 2ms/step\n",
            "Epoch 45/100\n",
            "20000/20000 - 41s - loss: 13.9971 - mse: 13.9971 - mae: 1.6442 - 41s/epoch - 2ms/step\n",
            "Epoch 46/100\n",
            "20000/20000 - 41s - loss: 13.9780 - mse: 13.9780 - mae: 1.6412 - 41s/epoch - 2ms/step\n",
            "Epoch 47/100\n",
            "20000/20000 - 40s - loss: 13.9828 - mse: 13.9828 - mae: 1.6416 - 40s/epoch - 2ms/step\n",
            "Epoch 48/100\n",
            "20000/20000 - 40s - loss: 14.0741 - mse: 14.0741 - mae: 1.6456 - 40s/epoch - 2ms/step\n",
            "Epoch 49/100\n",
            "20000/20000 - 37s - loss: 14.0843 - mse: 14.0843 - mae: 1.6472 - 37s/epoch - 2ms/step\n",
            "Epoch 50/100\n",
            "20000/20000 - 37s - loss: 14.0519 - mse: 14.0519 - mae: 1.6450 - 37s/epoch - 2ms/step\n",
            "Epoch 51/100\n",
            "20000/20000 - 37s - loss: 13.9731 - mse: 13.9731 - mae: 1.6432 - 37s/epoch - 2ms/step\n",
            "Epoch 52/100\n",
            "20000/20000 - 38s - loss: 14.0249 - mse: 14.0249 - mae: 1.6473 - 38s/epoch - 2ms/step\n",
            "Epoch 53/100\n",
            "20000/20000 - 39s - loss: 14.0262 - mse: 14.0262 - mae: 1.6453 - 39s/epoch - 2ms/step\n",
            "Epoch 54/100\n",
            "20000/20000 - 38s - loss: 14.0455 - mse: 14.0455 - mae: 1.6505 - 38s/epoch - 2ms/step\n",
            "Epoch 55/100\n",
            "20000/20000 - 40s - loss: 14.0284 - mse: 14.0284 - mae: 1.6476 - 40s/epoch - 2ms/step\n",
            "Epoch 56/100\n",
            "20000/20000 - 38s - loss: 14.0641 - mse: 14.0641 - mae: 1.6479 - 38s/epoch - 2ms/step\n",
            "Epoch 57/100\n",
            "20000/20000 - 37s - loss: 14.0398 - mse: 14.0398 - mae: 1.6494 - 37s/epoch - 2ms/step\n",
            "Epoch 58/100\n",
            "20000/20000 - 39s - loss: 14.1465 - mse: 14.1465 - mae: 1.6566 - 39s/epoch - 2ms/step\n",
            "Epoch 59/100\n",
            "20000/20000 - 38s - loss: 14.1937 - mse: 14.1937 - mae: 1.6551 - 38s/epoch - 2ms/step\n",
            "Epoch 60/100\n",
            "20000/20000 - 37s - loss: 14.1227 - mse: 14.1227 - mae: 1.6530 - 37s/epoch - 2ms/step\n",
            "Epoch 61/100\n",
            "20000/20000 - 36s - loss: 14.0905 - mse: 14.0905 - mae: 1.6510 - 36s/epoch - 2ms/step\n",
            "Epoch 62/100\n",
            "20000/20000 - 36s - loss: 14.1056 - mse: 14.1056 - mae: 1.6527 - 36s/epoch - 2ms/step\n",
            "Epoch 63/100\n",
            "20000/20000 - 36s - loss: 14.0461 - mse: 14.0461 - mae: 1.6507 - 36s/epoch - 2ms/step\n",
            "Epoch 64/100\n",
            "20000/20000 - 36s - loss: 14.0259 - mse: 14.0259 - mae: 1.6450 - 36s/epoch - 2ms/step\n",
            "Epoch 65/100\n",
            "20000/20000 - 36s - loss: 14.0636 - mse: 14.0636 - mae: 1.6486 - 36s/epoch - 2ms/step\n",
            "Epoch 66/100\n",
            "20000/20000 - 38s - loss: 14.0760 - mse: 14.0760 - mae: 1.6468 - 38s/epoch - 2ms/step\n",
            "Epoch 67/100\n",
            "20000/20000 - 38s - loss: 14.0236 - mse: 14.0236 - mae: 1.6452 - 38s/epoch - 2ms/step\n",
            "Epoch 68/100\n",
            "20000/20000 - 38s - loss: 14.1225 - mse: 14.1225 - mae: 1.6480 - 38s/epoch - 2ms/step\n",
            "Epoch 69/100\n",
            "20000/20000 - 38s - loss: 14.2251 - mse: 14.2251 - mae: 1.6550 - 38s/epoch - 2ms/step\n",
            "Epoch 70/100\n",
            "20000/20000 - 38s - loss: 14.2236 - mse: 14.2236 - mae: 1.6564 - 38s/epoch - 2ms/step\n",
            "Epoch 71/100\n",
            "20000/20000 - 38s - loss: 14.1918 - mse: 14.1918 - mae: 1.6525 - 38s/epoch - 2ms/step\n",
            "Epoch 72/100\n",
            "20000/20000 - 39s - loss: 14.1764 - mse: 14.1764 - mae: 1.6551 - 39s/epoch - 2ms/step\n",
            "Epoch 73/100\n",
            "20000/20000 - 38s - loss: 14.1794 - mse: 14.1794 - mae: 1.6552 - 38s/epoch - 2ms/step\n",
            "Epoch 74/100\n",
            "20000/20000 - 39s - loss: 14.1848 - mse: 14.1848 - mae: 1.6573 - 39s/epoch - 2ms/step\n",
            "Epoch 75/100\n",
            "20000/20000 - 39s - loss: 14.1751 - mse: 14.1751 - mae: 1.6558 - 39s/epoch - 2ms/step\n",
            "Epoch 76/100\n",
            "20000/20000 - 39s - loss: 14.1380 - mse: 14.1380 - mae: 1.6528 - 39s/epoch - 2ms/step\n",
            "Epoch 77/100\n",
            "20000/20000 - 38s - loss: 14.0792 - mse: 14.0792 - mae: 1.6510 - 38s/epoch - 2ms/step\n",
            "Epoch 78/100\n",
            "20000/20000 - 38s - loss: 14.1175 - mse: 14.1175 - mae: 1.6469 - 38s/epoch - 2ms/step\n",
            "Epoch 79/100\n",
            "20000/20000 - 37s - loss: 14.1031 - mse: 14.1031 - mae: 1.6498 - 37s/epoch - 2ms/step\n",
            "Epoch 80/100\n",
            "20000/20000 - 37s - loss: 14.1400 - mse: 14.1400 - mae: 1.6542 - 37s/epoch - 2ms/step\n",
            "Epoch 81/100\n",
            "20000/20000 - 37s - loss: 14.1572 - mse: 14.1572 - mae: 1.6516 - 37s/epoch - 2ms/step\n",
            "Epoch 82/100\n",
            "20000/20000 - 37s - loss: 14.1972 - mse: 14.1972 - mae: 1.6563 - 37s/epoch - 2ms/step\n",
            "Epoch 83/100\n",
            "20000/20000 - 37s - loss: 14.2138 - mse: 14.2138 - mae: 1.6591 - 37s/epoch - 2ms/step\n",
            "Epoch 84/100\n",
            "20000/20000 - 40s - loss: 14.2395 - mse: 14.2395 - mae: 1.6610 - 40s/epoch - 2ms/step\n",
            "Epoch 85/100\n",
            "20000/20000 - 41s - loss: 14.2478 - mse: 14.2478 - mae: 1.6618 - 41s/epoch - 2ms/step\n",
            "Epoch 86/100\n",
            "20000/20000 - 39s - loss: 14.2379 - mse: 14.2379 - mae: 1.6603 - 39s/epoch - 2ms/step\n",
            "Epoch 87/100\n",
            "20000/20000 - 39s - loss: 14.2384 - mse: 14.2384 - mae: 1.6604 - 39s/epoch - 2ms/step\n",
            "Epoch 88/100\n",
            "20000/20000 - 37s - loss: 14.2305 - mse: 14.2305 - mae: 1.6619 - 37s/epoch - 2ms/step\n",
            "Epoch 89/100\n",
            "20000/20000 - 38s - loss: 14.1664 - mse: 14.1664 - mae: 1.6549 - 38s/epoch - 2ms/step\n",
            "Epoch 90/100\n",
            "20000/20000 - 39s - loss: 14.1116 - mse: 14.1116 - mae: 1.6525 - 39s/epoch - 2ms/step\n",
            "Epoch 91/100\n",
            "20000/20000 - 38s - loss: 14.1642 - mse: 14.1642 - mae: 1.6563 - 38s/epoch - 2ms/step\n",
            "Epoch 92/100\n",
            "20000/20000 - 39s - loss: 14.1607 - mse: 14.1607 - mae: 1.6550 - 39s/epoch - 2ms/step\n",
            "Epoch 93/100\n",
            "20000/20000 - 40s - loss: 14.1646 - mse: 14.1646 - mae: 1.6552 - 40s/epoch - 2ms/step\n",
            "Epoch 94/100\n",
            "20000/20000 - 40s - loss: 14.1651 - mse: 14.1651 - mae: 1.6535 - 40s/epoch - 2ms/step\n",
            "Epoch 95/100\n",
            "20000/20000 - 37s - loss: 14.1386 - mse: 14.1386 - mae: 1.6555 - 37s/epoch - 2ms/step\n",
            "Epoch 96/100\n",
            "20000/20000 - 36s - loss: 14.1734 - mse: 14.1734 - mae: 1.6567 - 36s/epoch - 2ms/step\n",
            "Epoch 97/100\n",
            "20000/20000 - 36s - loss: 14.1418 - mse: 14.1418 - mae: 1.6542 - 36s/epoch - 2ms/step\n",
            "Epoch 98/100\n",
            "20000/20000 - 40s - loss: 14.1661 - mse: 14.1661 - mae: 1.6547 - 40s/epoch - 2ms/step\n",
            "Epoch 99/100\n",
            "20000/20000 - 37s - loss: 14.2856 - mse: 14.2856 - mae: 1.6645 - 37s/epoch - 2ms/step\n",
            "Epoch 100/100\n",
            "20000/20000 - 36s - loss: 14.1385 - mse: 14.1385 - mae: 1.6542 - 36s/epoch - 2ms/step\n",
            "Score for fold 1: loss of 29.11175537109375\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "20000/20000 - 36s - loss: 14.1871 - mse: 14.1871 - mae: 1.6549 - 36s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "20000/20000 - 36s - loss: 14.1323 - mse: 14.1323 - mae: 1.6528 - 36s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "20000/20000 - 36s - loss: 14.2160 - mse: 14.2160 - mae: 1.6624 - 36s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "20000/20000 - 36s - loss: 14.2207 - mse: 14.2207 - mae: 1.6645 - 36s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "20000/20000 - 37s - loss: 14.2032 - mse: 14.2032 - mae: 1.6606 - 37s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "20000/20000 - 39s - loss: 14.1732 - mse: 14.1732 - mae: 1.6577 - 39s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "20000/20000 - 39s - loss: 14.2368 - mse: 14.2368 - mae: 1.6653 - 39s/epoch - 2ms/step\n",
            "Epoch 8/100\n",
            "20000/20000 - 37s - loss: 14.3022 - mse: 14.3022 - mae: 1.6696 - 37s/epoch - 2ms/step\n",
            "Epoch 9/100\n",
            "20000/20000 - 36s - loss: 14.2678 - mse: 14.2678 - mae: 1.6664 - 36s/epoch - 2ms/step\n",
            "Epoch 10/100\n",
            "20000/20000 - 36s - loss: 14.2689 - mse: 14.2689 - mae: 1.6671 - 36s/epoch - 2ms/step\n",
            "Epoch 11/100\n",
            "20000/20000 - 36s - loss: 14.2965 - mse: 14.2965 - mae: 1.6711 - 36s/epoch - 2ms/step\n",
            "Epoch 12/100\n",
            "20000/20000 - 36s - loss: 14.3336 - mse: 14.3336 - mae: 1.6742 - 36s/epoch - 2ms/step\n",
            "Epoch 13/100\n",
            "20000/20000 - 36s - loss: 14.2967 - mse: 14.2967 - mae: 1.6699 - 36s/epoch - 2ms/step\n",
            "Epoch 14/100\n",
            "20000/20000 - 36s - loss: 14.4783 - mse: 14.4783 - mae: 1.6859 - 36s/epoch - 2ms/step\n",
            "Epoch 15/100\n",
            "20000/20000 - 36s - loss: 14.2628 - mse: 14.2628 - mae: 1.6687 - 36s/epoch - 2ms/step\n",
            "Epoch 16/100\n",
            "20000/20000 - 36s - loss: 14.2928 - mse: 14.2928 - mae: 1.6719 - 36s/epoch - 2ms/step\n",
            "Epoch 17/100\n",
            "20000/20000 - 36s - loss: 14.3601 - mse: 14.3601 - mae: 1.6713 - 36s/epoch - 2ms/step\n",
            "Epoch 18/100\n",
            "20000/20000 - 36s - loss: 14.4469 - mse: 14.4469 - mae: 1.6809 - 36s/epoch - 2ms/step\n",
            "Epoch 19/100\n",
            "20000/20000 - 36s - loss: 14.2991 - mse: 14.2991 - mae: 1.6687 - 36s/epoch - 2ms/step\n",
            "Epoch 20/100\n",
            "20000/20000 - 36s - loss: 14.3227 - mse: 14.3227 - mae: 1.6746 - 36s/epoch - 2ms/step\n",
            "Epoch 21/100\n",
            "20000/20000 - 36s - loss: 14.3211 - mse: 14.3211 - mae: 1.6728 - 36s/epoch - 2ms/step\n",
            "Epoch 22/100\n",
            "20000/20000 - 36s - loss: 14.2063 - mse: 14.2063 - mae: 1.6679 - 36s/epoch - 2ms/step\n",
            "Epoch 23/100\n",
            "20000/20000 - 36s - loss: 14.1781 - mse: 14.1781 - mae: 1.6569 - 36s/epoch - 2ms/step\n",
            "Epoch 24/100\n",
            "20000/20000 - 36s - loss: 14.1798 - mse: 14.1798 - mae: 1.6584 - 36s/epoch - 2ms/step\n",
            "Epoch 25/100\n",
            "20000/20000 - 35s - loss: 14.2300 - mse: 14.2300 - mae: 1.6585 - 35s/epoch - 2ms/step\n",
            "Epoch 26/100\n",
            "20000/20000 - 36s - loss: 14.2836 - mse: 14.2836 - mae: 1.6676 - 36s/epoch - 2ms/step\n",
            "Epoch 27/100\n",
            "20000/20000 - 36s - loss: 14.3056 - mse: 14.3056 - mae: 1.6665 - 36s/epoch - 2ms/step\n",
            "Epoch 28/100\n",
            "20000/20000 - 37s - loss: 14.2761 - mse: 14.2761 - mae: 1.6625 - 37s/epoch - 2ms/step\n",
            "Epoch 29/100\n",
            "20000/20000 - 36s - loss: 14.3675 - mse: 14.3675 - mae: 1.6719 - 36s/epoch - 2ms/step\n",
            "Epoch 30/100\n",
            "20000/20000 - 36s - loss: 14.3143 - mse: 14.3143 - mae: 1.6692 - 36s/epoch - 2ms/step\n",
            "Epoch 31/100\n",
            "20000/20000 - 37s - loss: 14.4447 - mse: 14.4447 - mae: 1.6812 - 37s/epoch - 2ms/step\n",
            "Epoch 32/100\n",
            "20000/20000 - 36s - loss: 14.5448 - mse: 14.5448 - mae: 1.6946 - 36s/epoch - 2ms/step\n",
            "Epoch 33/100\n",
            "20000/20000 - 36s - loss: 14.5476 - mse: 14.5476 - mae: 1.6976 - 36s/epoch - 2ms/step\n",
            "Epoch 34/100\n",
            "20000/20000 - 35s - loss: 14.5045 - mse: 14.5045 - mae: 1.7013 - 35s/epoch - 2ms/step\n",
            "Epoch 35/100\n",
            "20000/20000 - 36s - loss: 14.4068 - mse: 14.4068 - mae: 1.6826 - 36s/epoch - 2ms/step\n",
            "Epoch 36/100\n",
            "20000/20000 - 36s - loss: 14.3897 - mse: 14.3897 - mae: 1.6762 - 36s/epoch - 2ms/step\n",
            "Epoch 37/100\n",
            "20000/20000 - 36s - loss: 14.3258 - mse: 14.3258 - mae: 1.6665 - 36s/epoch - 2ms/step\n",
            "Epoch 38/100\n",
            "20000/20000 - 35s - loss: 14.4178 - mse: 14.4178 - mae: 1.6728 - 35s/epoch - 2ms/step\n",
            "Epoch 39/100\n",
            "20000/20000 - 36s - loss: 14.5036 - mse: 14.5036 - mae: 1.6905 - 36s/epoch - 2ms/step\n",
            "Epoch 40/100\n",
            "20000/20000 - 38s - loss: 14.3704 - mse: 14.3704 - mae: 1.6746 - 38s/epoch - 2ms/step\n",
            "Epoch 41/100\n",
            "20000/20000 - 40s - loss: 14.4316 - mse: 14.4316 - mae: 1.6828 - 40s/epoch - 2ms/step\n",
            "Epoch 42/100\n",
            "20000/20000 - 41s - loss: 14.5663 - mse: 14.5663 - mae: 1.6927 - 41s/epoch - 2ms/step\n",
            "Epoch 43/100\n",
            "20000/20000 - 45s - loss: 14.4922 - mse: 14.4922 - mae: 1.6884 - 45s/epoch - 2ms/step\n",
            "Epoch 44/100\n",
            "20000/20000 - 41s - loss: 14.4268 - mse: 14.4268 - mae: 1.6885 - 41s/epoch - 2ms/step\n",
            "Epoch 45/100\n",
            "20000/20000 - 38s - loss: 14.3513 - mse: 14.3513 - mae: 1.6762 - 38s/epoch - 2ms/step\n",
            "Epoch 46/100\n",
            "20000/20000 - 38s - loss: 14.3810 - mse: 14.3810 - mae: 1.6823 - 38s/epoch - 2ms/step\n",
            "Epoch 47/100\n",
            "20000/20000 - 38s - loss: 14.3544 - mse: 14.3544 - mae: 1.6794 - 38s/epoch - 2ms/step\n",
            "Epoch 48/100\n",
            "20000/20000 - 39s - loss: 14.3118 - mse: 14.3118 - mae: 1.6688 - 39s/epoch - 2ms/step\n",
            "Epoch 49/100\n",
            "20000/20000 - 36s - loss: 14.3508 - mse: 14.3508 - mae: 1.6767 - 36s/epoch - 2ms/step\n",
            "Epoch 50/100\n",
            "20000/20000 - 36s - loss: 14.4124 - mse: 14.4124 - mae: 1.6796 - 36s/epoch - 2ms/step\n",
            "Epoch 51/100\n",
            "20000/20000 - 36s - loss: 14.3341 - mse: 14.3341 - mae: 1.6750 - 36s/epoch - 2ms/step\n",
            "Epoch 52/100\n",
            "20000/20000 - 38s - loss: 14.2730 - mse: 14.2730 - mae: 1.6724 - 38s/epoch - 2ms/step\n",
            "Epoch 53/100\n",
            "20000/20000 - 38s - loss: 14.2469 - mse: 14.2469 - mae: 1.6633 - 38s/epoch - 2ms/step\n",
            "Epoch 54/100\n",
            "20000/20000 - 36s - loss: 14.2858 - mse: 14.2858 - mae: 1.6618 - 36s/epoch - 2ms/step\n",
            "Epoch 55/100\n",
            "20000/20000 - 36s - loss: 14.2817 - mse: 14.2817 - mae: 1.6641 - 36s/epoch - 2ms/step\n",
            "Epoch 56/100\n",
            "20000/20000 - 36s - loss: 14.3201 - mse: 14.3201 - mae: 1.6711 - 36s/epoch - 2ms/step\n",
            "Epoch 57/100\n",
            "20000/20000 - 36s - loss: 14.2717 - mse: 14.2717 - mae: 1.6673 - 36s/epoch - 2ms/step\n",
            "Epoch 58/100\n",
            "20000/20000 - 36s - loss: 14.2465 - mse: 14.2465 - mae: 1.6741 - 36s/epoch - 2ms/step\n",
            "Epoch 59/100\n",
            "20000/20000 - 36s - loss: 14.3180 - mse: 14.3180 - mae: 1.6767 - 36s/epoch - 2ms/step\n",
            "Epoch 60/100\n",
            "20000/20000 - 36s - loss: 14.3411 - mse: 14.3411 - mae: 1.6790 - 36s/epoch - 2ms/step\n",
            "Epoch 61/100\n",
            "20000/20000 - 36s - loss: 14.3791 - mse: 14.3791 - mae: 1.6775 - 36s/epoch - 2ms/step\n",
            "Epoch 62/100\n",
            "20000/20000 - 35s - loss: 14.4718 - mse: 14.4718 - mae: 1.6963 - 35s/epoch - 2ms/step\n",
            "Epoch 63/100\n",
            "20000/20000 - 35s - loss: 14.3220 - mse: 14.3220 - mae: 1.6727 - 35s/epoch - 2ms/step\n",
            "Epoch 64/100\n",
            "20000/20000 - 35s - loss: 14.2853 - mse: 14.2853 - mae: 1.6670 - 35s/epoch - 2ms/step\n",
            "Epoch 65/100\n",
            "20000/20000 - 35s - loss: 14.2408 - mse: 14.2408 - mae: 1.6593 - 35s/epoch - 2ms/step\n",
            "Epoch 66/100\n",
            "20000/20000 - 35s - loss: 14.3085 - mse: 14.3085 - mae: 1.6752 - 35s/epoch - 2ms/step\n",
            "Epoch 67/100\n",
            "20000/20000 - 35s - loss: 14.3212 - mse: 14.3212 - mae: 1.6754 - 35s/epoch - 2ms/step\n",
            "Epoch 68/100\n",
            "20000/20000 - 36s - loss: 14.2726 - mse: 14.2726 - mae: 1.6707 - 36s/epoch - 2ms/step\n",
            "Epoch 69/100\n",
            "20000/20000 - 37s - loss: 14.2497 - mse: 14.2497 - mae: 1.6655 - 37s/epoch - 2ms/step\n",
            "Epoch 70/100\n",
            "20000/20000 - 36s - loss: 14.2644 - mse: 14.2644 - mae: 1.6739 - 36s/epoch - 2ms/step\n",
            "Epoch 71/100\n",
            "20000/20000 - 36s - loss: 14.2236 - mse: 14.2236 - mae: 1.6599 - 36s/epoch - 2ms/step\n",
            "Epoch 72/100\n",
            "20000/20000 - 37s - loss: 14.2067 - mse: 14.2067 - mae: 1.6611 - 37s/epoch - 2ms/step\n",
            "Epoch 73/100\n",
            "20000/20000 - 36s - loss: 14.3449 - mse: 14.3449 - mae: 1.6835 - 36s/epoch - 2ms/step\n",
            "Epoch 74/100\n",
            "20000/20000 - 36s - loss: 14.3217 - mse: 14.3217 - mae: 1.6789 - 36s/epoch - 2ms/step\n",
            "Epoch 75/100\n",
            "20000/20000 - 36s - loss: 14.3985 - mse: 14.3985 - mae: 1.6808 - 36s/epoch - 2ms/step\n",
            "Epoch 76/100\n",
            "20000/20000 - 36s - loss: 14.3263 - mse: 14.3263 - mae: 1.6820 - 36s/epoch - 2ms/step\n",
            "Epoch 77/100\n",
            "20000/20000 - 36s - loss: 14.4453 - mse: 14.4453 - mae: 1.7050 - 36s/epoch - 2ms/step\n",
            "Epoch 78/100\n",
            "20000/20000 - 37s - loss: 14.3709 - mse: 14.3709 - mae: 1.6988 - 37s/epoch - 2ms/step\n",
            "Epoch 79/100\n",
            "20000/20000 - 43s - loss: 14.5506 - mse: 14.5506 - mae: 1.7103 - 43s/epoch - 2ms/step\n",
            "Epoch 80/100\n",
            "20000/20000 - 44s - loss: 14.4260 - mse: 14.4260 - mae: 1.7006 - 44s/epoch - 2ms/step\n",
            "Epoch 81/100\n",
            "20000/20000 - 42s - loss: 14.5040 - mse: 14.5040 - mae: 1.7041 - 42s/epoch - 2ms/step\n",
            "Epoch 82/100\n",
            "20000/20000 - 41s - loss: 14.7222 - mse: 14.7222 - mae: 1.7279 - 41s/epoch - 2ms/step\n",
            "Epoch 83/100\n",
            "20000/20000 - 43s - loss: 14.8887 - mse: 14.8887 - mae: 1.7605 - 43s/epoch - 2ms/step\n",
            "Epoch 84/100\n",
            "20000/20000 - 49s - loss: 14.8182 - mse: 14.8182 - mae: 1.7618 - 49s/epoch - 2ms/step\n",
            "Epoch 85/100\n",
            "20000/20000 - 50s - loss: 14.8584 - mse: 14.8584 - mae: 1.7674 - 50s/epoch - 2ms/step\n",
            "Epoch 86/100\n",
            "20000/20000 - 48s - loss: 14.9183 - mse: 14.9183 - mae: 1.7750 - 48s/epoch - 2ms/step\n",
            "Epoch 87/100\n",
            "20000/20000 - 45s - loss: 14.9897 - mse: 14.9897 - mae: 1.7735 - 45s/epoch - 2ms/step\n",
            "Epoch 88/100\n",
            "20000/20000 - 48s - loss: 14.9438 - mse: 14.9438 - mae: 1.7630 - 48s/epoch - 2ms/step\n",
            "Epoch 89/100\n",
            "20000/20000 - 46s - loss: 14.9090 - mse: 14.9090 - mae: 1.7625 - 46s/epoch - 2ms/step\n",
            "Epoch 90/100\n",
            "20000/20000 - 43s - loss: 15.0504 - mse: 15.0504 - mae: 1.7824 - 43s/epoch - 2ms/step\n",
            "Epoch 91/100\n",
            "20000/20000 - 42s - loss: 14.8571 - mse: 14.8571 - mae: 1.7630 - 42s/epoch - 2ms/step\n",
            "Epoch 92/100\n",
            "20000/20000 - 41s - loss: 14.9231 - mse: 14.9231 - mae: 1.7730 - 41s/epoch - 2ms/step\n",
            "Epoch 93/100\n",
            "20000/20000 - 39s - loss: 14.8787 - mse: 14.8787 - mae: 1.7625 - 39s/epoch - 2ms/step\n",
            "Epoch 94/100\n",
            "20000/20000 - 41s - loss: 14.9086 - mse: 14.9086 - mae: 1.7669 - 41s/epoch - 2ms/step\n",
            "Epoch 95/100\n",
            "20000/20000 - 38s - loss: 15.0041 - mse: 15.0041 - mae: 1.7791 - 38s/epoch - 2ms/step\n",
            "Epoch 96/100\n",
            "20000/20000 - 38s - loss: 15.5785 - mse: 15.5785 - mae: 1.8315 - 38s/epoch - 2ms/step\n",
            "Epoch 97/100\n",
            "20000/20000 - 38s - loss: 15.0550 - mse: 15.0550 - mae: 1.7853 - 38s/epoch - 2ms/step\n",
            "Epoch 98/100\n",
            "20000/20000 - 38s - loss: 14.9353 - mse: 14.9353 - mae: 1.7693 - 38s/epoch - 2ms/step\n",
            "Epoch 99/100\n",
            "20000/20000 - 37s - loss: 15.0899 - mse: 15.0899 - mae: 1.7867 - 37s/epoch - 2ms/step\n",
            "Epoch 100/100\n",
            "20000/20000 - 38s - loss: 15.0216 - mse: 15.0216 - mae: 1.7789 - 38s/epoch - 2ms/step\n",
            "Score for fold 2: loss of 18.977975845336914\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "20000/20000 - 38s - loss: 14.9331 - mse: 14.9331 - mae: 1.7590 - 38s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "20000/20000 - 36s - loss: 14.8770 - mse: 14.8770 - mae: 1.7524 - 36s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "20000/20000 - 36s - loss: 14.7759 - mse: 14.7759 - mae: 1.7449 - 36s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "20000/20000 - 36s - loss: 14.9362 - mse: 14.9362 - mae: 1.7750 - 36s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "20000/20000 - 36s - loss: 15.0154 - mse: 15.0154 - mae: 1.7813 - 36s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "20000/20000 - 36s - loss: 15.3085 - mse: 15.3085 - mae: 1.8101 - 36s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "20000/20000 - 36s - loss: 15.3336 - mse: 15.3336 - mae: 1.8096 - 36s/epoch - 2ms/step\n",
            "Epoch 8/100\n",
            "20000/20000 - 36s - loss: 15.4251 - mse: 15.4251 - mae: 1.8145 - 36s/epoch - 2ms/step\n",
            "Epoch 9/100\n",
            "20000/20000 - 37s - loss: 15.2824 - mse: 15.2824 - mae: 1.8055 - 37s/epoch - 2ms/step\n",
            "Epoch 10/100\n",
            "20000/20000 - 36s - loss: 15.1989 - mse: 15.1989 - mae: 1.7929 - 36s/epoch - 2ms/step\n",
            "Epoch 11/100\n",
            "20000/20000 - 36s - loss: 15.3831 - mse: 15.3831 - mae: 1.8100 - 36s/epoch - 2ms/step\n",
            "Epoch 12/100\n",
            "20000/20000 - 36s - loss: 15.3036 - mse: 15.3036 - mae: 1.8082 - 36s/epoch - 2ms/step\n",
            "Epoch 13/100\n",
            "20000/20000 - 36s - loss: 15.2155 - mse: 15.2155 - mae: 1.8019 - 36s/epoch - 2ms/step\n",
            "Epoch 14/100\n",
            "20000/20000 - 36s - loss: 15.1714 - mse: 15.1714 - mae: 1.7954 - 36s/epoch - 2ms/step\n",
            "Epoch 15/100\n",
            "20000/20000 - 42s - loss: 15.2561 - mse: 15.2561 - mae: 1.8065 - 42s/epoch - 2ms/step\n",
            "Epoch 16/100\n",
            "20000/20000 - 41s - loss: 15.2211 - mse: 15.2211 - mae: 1.8030 - 41s/epoch - 2ms/step\n",
            "Epoch 17/100\n",
            "20000/20000 - 42s - loss: 15.1086 - mse: 15.1086 - mae: 1.7911 - 42s/epoch - 2ms/step\n",
            "Epoch 18/100\n",
            "20000/20000 - 39s - loss: 15.0751 - mse: 15.0751 - mae: 1.7867 - 39s/epoch - 2ms/step\n",
            "Epoch 19/100\n",
            "20000/20000 - 37s - loss: 15.2153 - mse: 15.2153 - mae: 1.7986 - 37s/epoch - 2ms/step\n",
            "Epoch 20/100\n",
            "20000/20000 - 37s - loss: 15.1626 - mse: 15.1626 - mae: 1.7891 - 37s/epoch - 2ms/step\n",
            "Epoch 21/100\n",
            "20000/20000 - 36s - loss: 15.3186 - mse: 15.3186 - mae: 1.8055 - 36s/epoch - 2ms/step\n",
            "Epoch 22/100\n",
            "20000/20000 - 36s - loss: 14.9196 - mse: 14.9196 - mae: 1.7549 - 36s/epoch - 2ms/step\n",
            "Epoch 23/100\n",
            "20000/20000 - 39s - loss: 14.7054 - mse: 14.7054 - mae: 1.7285 - 39s/epoch - 2ms/step\n",
            "Epoch 24/100\n",
            "20000/20000 - 38s - loss: 15.0316 - mse: 15.0316 - mae: 1.7658 - 38s/epoch - 2ms/step\n",
            "Epoch 25/100\n",
            "20000/20000 - 38s - loss: 15.7352 - mse: 15.7352 - mae: 1.8358 - 38s/epoch - 2ms/step\n",
            "Epoch 26/100\n",
            "20000/20000 - 37s - loss: 14.9598 - mse: 14.9598 - mae: 1.7668 - 37s/epoch - 2ms/step\n",
            "Epoch 27/100\n",
            "20000/20000 - 36s - loss: 15.2150 - mse: 15.2150 - mae: 1.7868 - 36s/epoch - 2ms/step\n",
            "Epoch 28/100\n",
            "20000/20000 - 36s - loss: 15.3160 - mse: 15.3160 - mae: 1.8090 - 36s/epoch - 2ms/step\n",
            "Epoch 29/100\n",
            "20000/20000 - 36s - loss: 14.9586 - mse: 14.9586 - mae: 1.7775 - 36s/epoch - 2ms/step\n",
            "Epoch 30/100\n",
            "20000/20000 - 36s - loss: 15.2302 - mse: 15.2302 - mae: 1.7934 - 36s/epoch - 2ms/step\n",
            "Epoch 31/100\n",
            "20000/20000 - 36s - loss: 15.0977 - mse: 15.0977 - mae: 1.7875 - 36s/epoch - 2ms/step\n",
            "Epoch 32/100\n",
            "20000/20000 - 36s - loss: 14.9627 - mse: 14.9627 - mae: 1.7753 - 36s/epoch - 2ms/step\n",
            "Epoch 33/100\n",
            "20000/20000 - 36s - loss: 14.9381 - mse: 14.9381 - mae: 1.7734 - 36s/epoch - 2ms/step\n",
            "Epoch 34/100\n",
            "20000/20000 - 39s - loss: 15.0178 - mse: 15.0178 - mae: 1.7816 - 39s/epoch - 2ms/step\n",
            "Epoch 35/100\n",
            "20000/20000 - 41s - loss: 15.0800 - mse: 15.0800 - mae: 1.7863 - 41s/epoch - 2ms/step\n",
            "Epoch 36/100\n",
            "20000/20000 - 41s - loss: 15.4813 - mse: 15.4813 - mae: 1.8151 - 41s/epoch - 2ms/step\n",
            "Epoch 37/100\n",
            "20000/20000 - 42s - loss: 15.3865 - mse: 15.3865 - mae: 1.8128 - 42s/epoch - 2ms/step\n",
            "Epoch 38/100\n",
            "20000/20000 - 41s - loss: 15.2758 - mse: 15.2758 - mae: 1.8077 - 41s/epoch - 2ms/step\n",
            "Epoch 39/100\n",
            "20000/20000 - 41s - loss: 15.1535 - mse: 15.1535 - mae: 1.7914 - 41s/epoch - 2ms/step\n",
            "Epoch 40/100\n",
            "20000/20000 - 42s - loss: 15.0347 - mse: 15.0347 - mae: 1.7836 - 42s/epoch - 2ms/step\n",
            "Epoch 41/100\n",
            "20000/20000 - 43s - loss: 15.2360 - mse: 15.2360 - mae: 1.8107 - 43s/epoch - 2ms/step\n",
            "Epoch 42/100\n",
            "20000/20000 - 41s - loss: 15.0996 - mse: 15.0996 - mae: 1.7948 - 41s/epoch - 2ms/step\n",
            "Epoch 43/100\n",
            "20000/20000 - 41s - loss: 15.1989 - mse: 15.1989 - mae: 1.7999 - 41s/epoch - 2ms/step\n",
            "Epoch 44/100\n",
            "20000/20000 - 42s - loss: 15.3557 - mse: 15.3557 - mae: 1.8160 - 42s/epoch - 2ms/step\n",
            "Epoch 45/100\n",
            "20000/20000 - 41s - loss: 15.4037 - mse: 15.4037 - mae: 1.8158 - 41s/epoch - 2ms/step\n",
            "Epoch 46/100\n",
            "20000/20000 - 43s - loss: 15.4268 - mse: 15.4268 - mae: 1.8197 - 43s/epoch - 2ms/step\n",
            "Epoch 47/100\n",
            "20000/20000 - 42s - loss: 15.5367 - mse: 15.5367 - mae: 1.8277 - 42s/epoch - 2ms/step\n",
            "Epoch 48/100\n",
            "20000/20000 - 42s - loss: 15.5179 - mse: 15.5179 - mae: 1.8257 - 42s/epoch - 2ms/step\n",
            "Epoch 49/100\n",
            "20000/20000 - 42s - loss: 15.4955 - mse: 15.4955 - mae: 1.8232 - 42s/epoch - 2ms/step\n",
            "Epoch 50/100\n",
            "20000/20000 - 42s - loss: 15.1427 - mse: 15.1427 - mae: 1.7861 - 42s/epoch - 2ms/step\n",
            "Epoch 51/100\n",
            "20000/20000 - 41s - loss: 15.1294 - mse: 15.1294 - mae: 1.7904 - 41s/epoch - 2ms/step\n",
            "Epoch 52/100\n",
            "20000/20000 - 41s - loss: 15.1901 - mse: 15.1901 - mae: 1.7967 - 41s/epoch - 2ms/step\n",
            "Epoch 53/100\n",
            "20000/20000 - 42s - loss: 15.3941 - mse: 15.3941 - mae: 1.8096 - 42s/epoch - 2ms/step\n",
            "Epoch 54/100\n",
            "20000/20000 - 41s - loss: 15.2394 - mse: 15.2394 - mae: 1.8009 - 41s/epoch - 2ms/step\n",
            "Epoch 55/100\n",
            "20000/20000 - 42s - loss: 15.4165 - mse: 15.4165 - mae: 1.8171 - 42s/epoch - 2ms/step\n",
            "Epoch 56/100\n",
            "20000/20000 - 42s - loss: 15.4806 - mse: 15.4806 - mae: 1.8199 - 42s/epoch - 2ms/step\n",
            "Epoch 57/100\n",
            "20000/20000 - 41s - loss: 15.4241 - mse: 15.4241 - mae: 1.8169 - 41s/epoch - 2ms/step\n",
            "Epoch 58/100\n",
            "20000/20000 - 41s - loss: 15.2376 - mse: 15.2376 - mae: 1.8048 - 41s/epoch - 2ms/step\n",
            "Epoch 59/100\n",
            "20000/20000 - 41s - loss: 15.4204 - mse: 15.4204 - mae: 1.8185 - 41s/epoch - 2ms/step\n",
            "Epoch 60/100\n",
            "20000/20000 - 41s - loss: 15.3195 - mse: 15.3195 - mae: 1.8076 - 41s/epoch - 2ms/step\n",
            "Epoch 61/100\n",
            "20000/20000 - 41s - loss: 15.3021 - mse: 15.3021 - mae: 1.8035 - 41s/epoch - 2ms/step\n",
            "Epoch 62/100\n",
            "20000/20000 - 41s - loss: 15.2694 - mse: 15.2694 - mae: 1.7937 - 41s/epoch - 2ms/step\n",
            "Epoch 63/100\n",
            "20000/20000 - 41s - loss: 15.9138 - mse: 15.9138 - mae: 1.8349 - 41s/epoch - 2ms/step\n",
            "Epoch 64/100\n",
            "20000/20000 - 44s - loss: 15.1933 - mse: 15.1933 - mae: 1.7911 - 44s/epoch - 2ms/step\n",
            "Epoch 65/100\n",
            "20000/20000 - 42s - loss: 15.4362 - mse: 15.4362 - mae: 1.8080 - 42s/epoch - 2ms/step\n",
            "Epoch 66/100\n",
            "20000/20000 - 43s - loss: 15.0917 - mse: 15.0917 - mae: 1.7841 - 43s/epoch - 2ms/step\n",
            "Epoch 67/100\n",
            "20000/20000 - 41s - loss: 15.3021 - mse: 15.3021 - mae: 1.8030 - 41s/epoch - 2ms/step\n",
            "Epoch 68/100\n",
            "20000/20000 - 40s - loss: 15.5261 - mse: 15.5261 - mae: 1.8253 - 40s/epoch - 2ms/step\n",
            "Epoch 69/100\n",
            "20000/20000 - 41s - loss: 15.3746 - mse: 15.3746 - mae: 1.8141 - 41s/epoch - 2ms/step\n",
            "Epoch 70/100\n",
            "20000/20000 - 41s - loss: 15.2285 - mse: 15.2285 - mae: 1.8037 - 41s/epoch - 2ms/step\n",
            "Epoch 71/100\n",
            "20000/20000 - 41s - loss: 15.1141 - mse: 15.1141 - mae: 1.7881 - 41s/epoch - 2ms/step\n",
            "Epoch 72/100\n",
            "20000/20000 - 42s - loss: 15.1602 - mse: 15.1602 - mae: 1.7953 - 42s/epoch - 2ms/step\n",
            "Epoch 73/100\n",
            "20000/20000 - 43s - loss: 15.4317 - mse: 15.4317 - mae: 1.8218 - 43s/epoch - 2ms/step\n",
            "Epoch 74/100\n",
            "20000/20000 - 42s - loss: 15.4103 - mse: 15.4103 - mae: 1.8217 - 42s/epoch - 2ms/step\n",
            "Epoch 75/100\n",
            "20000/20000 - 42s - loss: 15.5431 - mse: 15.5431 - mae: 1.8239 - 42s/epoch - 2ms/step\n",
            "Epoch 76/100\n",
            "20000/20000 - 42s - loss: 15.5579 - mse: 15.5579 - mae: 1.8250 - 42s/epoch - 2ms/step\n",
            "Epoch 77/100\n",
            "20000/20000 - 44s - loss: 15.4804 - mse: 15.4804 - mae: 1.8201 - 44s/epoch - 2ms/step\n",
            "Epoch 78/100\n",
            "20000/20000 - 45s - loss: 15.7319 - mse: 15.7319 - mae: 1.8380 - 45s/epoch - 2ms/step\n",
            "Epoch 79/100\n",
            "20000/20000 - 47s - loss: 15.5572 - mse: 15.5572 - mae: 1.8174 - 47s/epoch - 2ms/step\n",
            "Epoch 80/100\n",
            "20000/20000 - 44s - loss: 15.4293 - mse: 15.4293 - mae: 1.8124 - 44s/epoch - 2ms/step\n",
            "Epoch 81/100\n",
            "20000/20000 - 42s - loss: 15.4279 - mse: 15.4279 - mae: 1.8161 - 42s/epoch - 2ms/step\n",
            "Epoch 82/100\n",
            "20000/20000 - 41s - loss: 15.7139 - mse: 15.7139 - mae: 1.8361 - 41s/epoch - 2ms/step\n",
            "Epoch 83/100\n",
            "20000/20000 - 41s - loss: 15.4655 - mse: 15.4655 - mae: 1.8218 - 41s/epoch - 2ms/step\n",
            "Epoch 84/100\n",
            "20000/20000 - 42s - loss: 15.4077 - mse: 15.4077 - mae: 1.8153 - 42s/epoch - 2ms/step\n",
            "Epoch 85/100\n",
            "20000/20000 - 43s - loss: 15.3128 - mse: 15.3128 - mae: 1.8035 - 43s/epoch - 2ms/step\n",
            "Epoch 86/100\n",
            "20000/20000 - 43s - loss: 15.3010 - mse: 15.3010 - mae: 1.8069 - 43s/epoch - 2ms/step\n",
            "Epoch 87/100\n",
            "20000/20000 - 42s - loss: 15.4792 - mse: 15.4792 - mae: 1.8199 - 42s/epoch - 2ms/step\n",
            "Epoch 88/100\n",
            "20000/20000 - 42s - loss: 15.4243 - mse: 15.4243 - mae: 1.8114 - 42s/epoch - 2ms/step\n",
            "Epoch 89/100\n",
            "20000/20000 - 42s - loss: 15.6016 - mse: 15.6016 - mae: 1.8286 - 42s/epoch - 2ms/step\n",
            "Epoch 90/100\n",
            "20000/20000 - 42s - loss: 15.3485 - mse: 15.3485 - mae: 1.7986 - 42s/epoch - 2ms/step\n",
            "Epoch 91/100\n",
            "20000/20000 - 42s - loss: 15.3273 - mse: 15.3273 - mae: 1.7996 - 42s/epoch - 2ms/step\n",
            "Epoch 92/100\n",
            "20000/20000 - 44s - loss: 15.3223 - mse: 15.3223 - mae: 1.7983 - 44s/epoch - 2ms/step\n",
            "Epoch 93/100\n",
            "20000/20000 - 44s - loss: 16.0099 - mse: 16.0099 - mae: 1.8519 - 44s/epoch - 2ms/step\n",
            "Epoch 94/100\n",
            "20000/20000 - 41s - loss: 15.3836 - mse: 15.3836 - mae: 1.8010 - 41s/epoch - 2ms/step\n",
            "Epoch 95/100\n",
            "20000/20000 - 42s - loss: 15.3224 - mse: 15.3224 - mae: 1.7988 - 42s/epoch - 2ms/step\n",
            "Epoch 96/100\n",
            "20000/20000 - 41s - loss: 15.4208 - mse: 15.4208 - mae: 1.8014 - 41s/epoch - 2ms/step\n",
            "Epoch 97/100\n",
            "20000/20000 - 41s - loss: 15.3247 - mse: 15.3247 - mae: 1.7959 - 41s/epoch - 2ms/step\n",
            "Epoch 98/100\n",
            "20000/20000 - 42s - loss: 15.3883 - mse: 15.3883 - mae: 1.8017 - 42s/epoch - 2ms/step\n",
            "Epoch 99/100\n",
            "20000/20000 - 41s - loss: 15.3020 - mse: 15.3020 - mae: 1.7925 - 41s/epoch - 2ms/step\n",
            "Epoch 100/100\n",
            "20000/20000 - 43s - loss: 15.4881 - mse: 15.4881 - mae: 1.8163 - 43s/epoch - 2ms/step\n",
            "Score for fold 3: loss of 19.958026885986328\n",
            "Epoch 1/200\n",
            "20000/20000 - 43s - loss: 15.8376 - mse: 15.8376 - mae: 1.8388 - 43s/epoch - 2ms/step\n",
            "Epoch 2/200\n",
            "20000/20000 - 41s - loss: 15.5455 - mse: 15.5455 - mae: 1.8187 - 41s/epoch - 2ms/step\n",
            "Epoch 3/200\n",
            "20000/20000 - 41s - loss: 15.5482 - mse: 15.5482 - mae: 1.8257 - 41s/epoch - 2ms/step\n",
            "Epoch 4/200\n",
            "20000/20000 - 39s - loss: 15.4866 - mse: 15.4866 - mae: 1.8204 - 39s/epoch - 2ms/step\n",
            "Epoch 5/200\n",
            "20000/20000 - 38s - loss: 15.6647 - mse: 15.6647 - mae: 1.8349 - 38s/epoch - 2ms/step\n",
            "Epoch 6/200\n",
            "20000/20000 - 38s - loss: 15.6635 - mse: 15.6635 - mae: 1.8338 - 38s/epoch - 2ms/step\n",
            "Epoch 7/200\n",
            "20000/20000 - 37s - loss: 15.5114 - mse: 15.5114 - mae: 1.8155 - 37s/epoch - 2ms/step\n",
            "Epoch 8/200\n",
            "20000/20000 - 38s - loss: 15.2923 - mse: 15.2923 - mae: 1.7918 - 38s/epoch - 2ms/step\n",
            "Epoch 9/200\n",
            "20000/20000 - 38s - loss: 15.3515 - mse: 15.3515 - mae: 1.7981 - 38s/epoch - 2ms/step\n",
            "Epoch 10/200\n",
            "20000/20000 - 41s - loss: 15.3798 - mse: 15.3798 - mae: 1.8119 - 41s/epoch - 2ms/step\n",
            "Epoch 11/200\n",
            "20000/20000 - 40s - loss: 15.4546 - mse: 15.4546 - mae: 1.8201 - 40s/epoch - 2ms/step\n",
            "Epoch 12/200\n",
            "20000/20000 - 38s - loss: 15.3752 - mse: 15.3752 - mae: 1.8130 - 38s/epoch - 2ms/step\n",
            "Epoch 13/200\n",
            "20000/20000 - 37s - loss: 15.4164 - mse: 15.4164 - mae: 1.8180 - 37s/epoch - 2ms/step\n",
            "Epoch 14/200\n",
            "20000/20000 - 38s - loss: 15.4419 - mse: 15.4419 - mae: 1.8181 - 38s/epoch - 2ms/step\n",
            "Epoch 15/200\n",
            "20000/20000 - 37s - loss: 15.3462 - mse: 15.3462 - mae: 1.8016 - 37s/epoch - 2ms/step\n",
            "Epoch 16/200\n",
            "20000/20000 - 36s - loss: 15.4110 - mse: 15.4110 - mae: 1.8150 - 36s/epoch - 2ms/step\n",
            "Epoch 17/200\n",
            "20000/20000 - 37s - loss: 15.4342 - mse: 15.4342 - mae: 1.8185 - 37s/epoch - 2ms/step\n",
            "Epoch 18/200\n",
            "20000/20000 - 38s - loss: 15.4134 - mse: 15.4134 - mae: 1.7998 - 38s/epoch - 2ms/step\n",
            "Epoch 19/200\n",
            "20000/20000 - 39s - loss: 15.3404 - mse: 15.3404 - mae: 1.7916 - 39s/epoch - 2ms/step\n",
            "Epoch 20/200\n",
            "20000/20000 - 39s - loss: 15.2849 - mse: 15.2849 - mae: 1.7969 - 39s/epoch - 2ms/step\n",
            "Epoch 21/200\n",
            "20000/20000 - 36s - loss: 15.4001 - mse: 15.4001 - mae: 1.8128 - 36s/epoch - 2ms/step\n",
            "Epoch 22/200\n",
            "20000/20000 - 36s - loss: 15.3623 - mse: 15.3623 - mae: 1.8050 - 36s/epoch - 2ms/step\n",
            "Epoch 23/200\n",
            "20000/20000 - 36s - loss: 15.3090 - mse: 15.3090 - mae: 1.7937 - 36s/epoch - 2ms/step\n",
            "Epoch 24/200\n",
            "20000/20000 - 35s - loss: 15.3782 - mse: 15.3782 - mae: 1.8068 - 35s/epoch - 2ms/step\n",
            "Epoch 25/200\n",
            "20000/20000 - 36s - loss: 15.5797 - mse: 15.5797 - mae: 1.8266 - 36s/epoch - 2ms/step\n",
            "Epoch 26/200\n",
            "20000/20000 - 36s - loss: 15.5432 - mse: 15.5432 - mae: 1.8220 - 36s/epoch - 2ms/step\n",
            "Epoch 27/200\n",
            "20000/20000 - 36s - loss: 15.8129 - mse: 15.8129 - mae: 1.8410 - 36s/epoch - 2ms/step\n",
            "Epoch 28/200\n",
            "20000/20000 - 36s - loss: 15.3949 - mse: 15.3949 - mae: 1.8104 - 36s/epoch - 2ms/step\n",
            "Epoch 29/200\n",
            "20000/20000 - 36s - loss: 15.6255 - mse: 15.6255 - mae: 1.8281 - 36s/epoch - 2ms/step\n",
            "Epoch 30/200\n",
            "20000/20000 - 36s - loss: 15.8587 - mse: 15.8587 - mae: 1.8433 - 36s/epoch - 2ms/step\n",
            "Epoch 31/200\n",
            "20000/20000 - 35s - loss: 15.8713 - mse: 15.8713 - mae: 1.8445 - 35s/epoch - 2ms/step\n",
            "Epoch 32/200\n",
            "20000/20000 - 35s - loss: 16.1352 - mse: 16.1352 - mae: 1.8583 - 35s/epoch - 2ms/step\n",
            "Epoch 33/200\n",
            "20000/20000 - 35s - loss: 15.7967 - mse: 15.7967 - mae: 1.8398 - 35s/epoch - 2ms/step\n",
            "Epoch 34/200\n",
            "20000/20000 - 36s - loss: 15.4612 - mse: 15.4612 - mae: 1.8178 - 36s/epoch - 2ms/step\n",
            "Epoch 35/200\n",
            "20000/20000 - 35s - loss: 15.4375 - mse: 15.4375 - mae: 1.8192 - 35s/epoch - 2ms/step\n",
            "Epoch 36/200\n",
            "20000/20000 - 36s - loss: 15.7214 - mse: 15.7214 - mae: 1.8386 - 36s/epoch - 2ms/step\n",
            "Epoch 37/200\n",
            "20000/20000 - 36s - loss: 15.7101 - mse: 15.7101 - mae: 1.8303 - 36s/epoch - 2ms/step\n",
            "Epoch 38/200\n",
            "20000/20000 - 36s - loss: 15.5363 - mse: 15.5363 - mae: 1.8110 - 36s/epoch - 2ms/step\n",
            "Epoch 39/200\n",
            "20000/20000 - 36s - loss: 15.5797 - mse: 15.5797 - mae: 1.8301 - 36s/epoch - 2ms/step\n",
            "Epoch 40/200\n",
            "20000/20000 - 36s - loss: 15.4447 - mse: 15.4447 - mae: 1.8016 - 36s/epoch - 2ms/step\n",
            "Epoch 41/200\n",
            "20000/20000 - 35s - loss: 15.3322 - mse: 15.3322 - mae: 1.8028 - 35s/epoch - 2ms/step\n",
            "Epoch 42/200\n",
            "20000/20000 - 36s - loss: 15.4372 - mse: 15.4372 - mae: 1.8150 - 36s/epoch - 2ms/step\n",
            "Epoch 43/200\n",
            "20000/20000 - 36s - loss: 15.4147 - mse: 15.4147 - mae: 1.8159 - 36s/epoch - 2ms/step\n",
            "Epoch 44/200\n",
            "20000/20000 - 35s - loss: 15.4481 - mse: 15.4481 - mae: 1.8178 - 35s/epoch - 2ms/step\n",
            "Epoch 45/200\n",
            "20000/20000 - 35s - loss: 15.3882 - mse: 15.3882 - mae: 1.8127 - 35s/epoch - 2ms/step\n",
            "Epoch 46/200\n",
            "20000/20000 - 36s - loss: 15.3955 - mse: 15.3955 - mae: 1.8112 - 36s/epoch - 2ms/step\n",
            "Epoch 47/200\n",
            "20000/20000 - 36s - loss: 15.5235 - mse: 15.5235 - mae: 1.8223 - 36s/epoch - 2ms/step\n",
            "Epoch 48/200\n",
            "20000/20000 - 36s - loss: 15.4007 - mse: 15.4007 - mae: 1.8130 - 36s/epoch - 2ms/step\n",
            "Epoch 49/200\n",
            "20000/20000 - 35s - loss: 15.4382 - mse: 15.4382 - mae: 1.8171 - 35s/epoch - 2ms/step\n",
            "Epoch 50/200\n",
            "20000/20000 - 35s - loss: 15.2853 - mse: 15.2853 - mae: 1.7994 - 35s/epoch - 2ms/step\n",
            "Epoch 51/200\n",
            "20000/20000 - 36s - loss: 15.1660 - mse: 15.1660 - mae: 1.7884 - 36s/epoch - 2ms/step\n",
            "Epoch 52/200\n",
            "20000/20000 - 36s - loss: 15.0590 - mse: 15.0590 - mae: 1.7736 - 36s/epoch - 2ms/step\n",
            "Epoch 53/200\n",
            "20000/20000 - 35s - loss: 14.9491 - mse: 14.9491 - mae: 1.7574 - 35s/epoch - 2ms/step\n",
            "Epoch 54/200\n",
            "20000/20000 - 35s - loss: 15.0282 - mse: 15.0282 - mae: 1.7605 - 35s/epoch - 2ms/step\n",
            "Epoch 55/200\n",
            "20000/20000 - 36s - loss: 15.4712 - mse: 15.4712 - mae: 1.8155 - 36s/epoch - 2ms/step\n",
            "Epoch 56/200\n",
            "20000/20000 - 35s - loss: 15.7994 - mse: 15.7994 - mae: 1.8361 - 35s/epoch - 2ms/step\n",
            "Epoch 57/200\n",
            "20000/20000 - 35s - loss: 15.6484 - mse: 15.6484 - mae: 1.8331 - 35s/epoch - 2ms/step\n",
            "Epoch 58/200\n",
            "20000/20000 - 35s - loss: 15.8332 - mse: 15.8332 - mae: 1.8412 - 35s/epoch - 2ms/step\n",
            "Epoch 59/200\n",
            "20000/20000 - 35s - loss: 15.4999 - mse: 15.4999 - mae: 1.8151 - 35s/epoch - 2ms/step\n",
            "Epoch 60/200\n",
            "20000/20000 - 36s - loss: 15.7847 - mse: 15.7847 - mae: 1.8408 - 36s/epoch - 2ms/step\n",
            "Epoch 61/200\n",
            "20000/20000 - 36s - loss: 15.4127 - mse: 15.4127 - mae: 1.8136 - 36s/epoch - 2ms/step\n",
            "Epoch 62/200\n",
            "20000/20000 - 35s - loss: 15.3640 - mse: 15.3640 - mae: 1.8055 - 35s/epoch - 2ms/step\n",
            "Epoch 63/200\n",
            "20000/20000 - 35s - loss: 15.4264 - mse: 15.4264 - mae: 1.8122 - 35s/epoch - 2ms/step\n",
            "Epoch 64/200\n",
            "20000/20000 - 35s - loss: 15.4027 - mse: 15.4027 - mae: 1.8119 - 35s/epoch - 2ms/step\n",
            "Epoch 65/200\n",
            "20000/20000 - 36s - loss: 15.3346 - mse: 15.3346 - mae: 1.7942 - 36s/epoch - 2ms/step\n",
            "Epoch 66/200\n",
            "20000/20000 - 37s - loss: 15.3279 - mse: 15.3279 - mae: 1.7980 - 37s/epoch - 2ms/step\n",
            "Epoch 67/200\n",
            "20000/20000 - 40s - loss: 15.3168 - mse: 15.3168 - mae: 1.8022 - 40s/epoch - 2ms/step\n",
            "Epoch 68/200\n",
            "20000/20000 - 47s - loss: 15.2527 - mse: 15.2527 - mae: 1.7924 - 47s/epoch - 2ms/step\n",
            "Epoch 69/200\n",
            "20000/20000 - 46s - loss: 15.3103 - mse: 15.3103 - mae: 1.8022 - 46s/epoch - 2ms/step\n",
            "Epoch 70/200\n",
            "20000/20000 - 46s - loss: 15.4836 - mse: 15.4836 - mae: 1.8238 - 46s/epoch - 2ms/step\n",
            "Epoch 71/200\n",
            "20000/20000 - 45s - loss: 15.3931 - mse: 15.3931 - mae: 1.8099 - 45s/epoch - 2ms/step\n",
            "Epoch 72/200\n",
            "20000/20000 - 45s - loss: 15.2925 - mse: 15.2925 - mae: 1.7918 - 45s/epoch - 2ms/step\n",
            "Epoch 73/200\n",
            "20000/20000 - 44s - loss: 15.3672 - mse: 15.3672 - mae: 1.7891 - 44s/epoch - 2ms/step\n",
            "Epoch 74/200\n",
            "20000/20000 - 43s - loss: 15.2679 - mse: 15.2679 - mae: 1.7930 - 43s/epoch - 2ms/step\n",
            "Epoch 75/200\n",
            "20000/20000 - 42s - loss: 15.2837 - mse: 15.2837 - mae: 1.7878 - 42s/epoch - 2ms/step\n",
            "Epoch 76/200\n",
            "20000/20000 - 41s - loss: 15.2399 - mse: 15.2399 - mae: 1.7813 - 41s/epoch - 2ms/step\n",
            "Epoch 77/200\n",
            "20000/20000 - 41s - loss: 15.2705 - mse: 15.2705 - mae: 1.8001 - 41s/epoch - 2ms/step\n",
            "Epoch 78/200\n",
            "20000/20000 - 40s - loss: 15.2693 - mse: 15.2693 - mae: 1.7956 - 40s/epoch - 2ms/step\n",
            "Epoch 79/200\n",
            "20000/20000 - 43s - loss: 15.3403 - mse: 15.3403 - mae: 1.8038 - 43s/epoch - 2ms/step\n",
            "Epoch 80/200\n",
            "20000/20000 - 47s - loss: 15.2553 - mse: 15.2553 - mae: 1.7927 - 47s/epoch - 2ms/step\n",
            "Epoch 81/200\n",
            "20000/20000 - 43s - loss: 15.2045 - mse: 15.2045 - mae: 1.7816 - 43s/epoch - 2ms/step\n",
            "Epoch 82/200\n",
            "20000/20000 - 44s - loss: 15.0884 - mse: 15.0884 - mae: 1.7713 - 44s/epoch - 2ms/step\n",
            "Epoch 83/200\n",
            "20000/20000 - 47s - loss: 15.0456 - mse: 15.0456 - mae: 1.7646 - 47s/epoch - 2ms/step\n",
            "Epoch 84/200\n",
            "20000/20000 - 44s - loss: 15.0107 - mse: 15.0107 - mae: 1.7712 - 44s/epoch - 2ms/step\n",
            "Epoch 85/200\n",
            "20000/20000 - 42s - loss: 15.0324 - mse: 15.0324 - mae: 1.7668 - 42s/epoch - 2ms/step\n",
            "Epoch 86/200\n",
            "20000/20000 - 41s - loss: 14.7647 - mse: 14.7647 - mae: 1.7353 - 41s/epoch - 2ms/step\n",
            "Epoch 87/200\n",
            "20000/20000 - 42s - loss: 14.8568 - mse: 14.8568 - mae: 1.7572 - 42s/epoch - 2ms/step\n",
            "Epoch 88/200\n",
            "20000/20000 - 41s - loss: 14.9912 - mse: 14.9912 - mae: 1.7699 - 41s/epoch - 2ms/step\n",
            "Epoch 89/200\n",
            "20000/20000 - 40s - loss: 14.8796 - mse: 14.8796 - mae: 1.7463 - 40s/epoch - 2ms/step\n",
            "Epoch 90/200\n",
            "20000/20000 - 41s - loss: 14.8284 - mse: 14.8284 - mae: 1.7452 - 41s/epoch - 2ms/step\n",
            "Epoch 91/200\n",
            "20000/20000 - 43s - loss: 15.2944 - mse: 15.2944 - mae: 1.7987 - 43s/epoch - 2ms/step\n",
            "Epoch 92/200\n",
            "20000/20000 - 41s - loss: 15.2643 - mse: 15.2643 - mae: 1.8013 - 41s/epoch - 2ms/step\n",
            "Epoch 93/200\n",
            "20000/20000 - 40s - loss: 15.0938 - mse: 15.0938 - mae: 1.7846 - 40s/epoch - 2ms/step\n",
            "Epoch 94/200\n",
            "20000/20000 - 42s - loss: 15.0860 - mse: 15.0860 - mae: 1.7826 - 42s/epoch - 2ms/step\n",
            "Epoch 95/200\n",
            "20000/20000 - 40s - loss: 15.0932 - mse: 15.0932 - mae: 1.7850 - 40s/epoch - 2ms/step\n",
            "Epoch 96/200\n",
            "20000/20000 - 40s - loss: 15.0356 - mse: 15.0356 - mae: 1.7827 - 40s/epoch - 2ms/step\n",
            "Epoch 97/200\n",
            "20000/20000 - 40s - loss: 15.2361 - mse: 15.2361 - mae: 1.8005 - 40s/epoch - 2ms/step\n",
            "Epoch 98/200\n",
            "20000/20000 - 39s - loss: 15.3574 - mse: 15.3574 - mae: 1.8120 - 39s/epoch - 2ms/step\n",
            "Epoch 99/200\n",
            "20000/20000 - 37s - loss: 15.3200 - mse: 15.3200 - mae: 1.8102 - 37s/epoch - 2ms/step\n",
            "Epoch 100/200\n",
            "20000/20000 - 37s - loss: 15.2076 - mse: 15.2076 - mae: 1.7952 - 37s/epoch - 2ms/step\n",
            "Epoch 101/200\n",
            "20000/20000 - 37s - loss: 14.7218 - mse: 14.7218 - mae: 1.7324 - 37s/epoch - 2ms/step\n",
            "Epoch 102/200\n",
            "20000/20000 - 37s - loss: 14.6184 - mse: 14.6184 - mae: 1.7134 - 37s/epoch - 2ms/step\n",
            "Epoch 103/200\n",
            "20000/20000 - 37s - loss: 14.5500 - mse: 14.5500 - mae: 1.7044 - 37s/epoch - 2ms/step\n",
            "Epoch 104/200\n",
            "20000/20000 - 36s - loss: 14.5366 - mse: 14.5366 - mae: 1.7030 - 36s/epoch - 2ms/step\n",
            "Epoch 105/200\n",
            "20000/20000 - 36s - loss: 14.6211 - mse: 14.6211 - mae: 1.7164 - 36s/epoch - 2ms/step\n",
            "Epoch 106/200\n",
            "20000/20000 - 37s - loss: 14.5637 - mse: 14.5637 - mae: 1.7053 - 37s/epoch - 2ms/step\n",
            "Epoch 107/200\n",
            "20000/20000 - 37s - loss: 14.6190 - mse: 14.6190 - mae: 1.7226 - 37s/epoch - 2ms/step\n",
            "Epoch 108/200\n",
            "20000/20000 - 37s - loss: 14.5827 - mse: 14.5827 - mae: 1.7046 - 37s/epoch - 2ms/step\n",
            "Epoch 109/200\n",
            "20000/20000 - 36s - loss: 14.5471 - mse: 14.5471 - mae: 1.7052 - 36s/epoch - 2ms/step\n",
            "Epoch 110/200\n",
            "20000/20000 - 36s - loss: 14.5997 - mse: 14.5997 - mae: 1.7182 - 36s/epoch - 2ms/step\n",
            "Epoch 111/200\n",
            "20000/20000 - 35s - loss: 14.5729 - mse: 14.5729 - mae: 1.7166 - 35s/epoch - 2ms/step\n",
            "Epoch 112/200\n",
            "20000/20000 - 36s - loss: 14.5209 - mse: 14.5209 - mae: 1.7093 - 36s/epoch - 2ms/step\n",
            "Epoch 113/200\n",
            "20000/20000 - 35s - loss: 14.6213 - mse: 14.6213 - mae: 1.7214 - 35s/epoch - 2ms/step\n",
            "Epoch 114/200\n",
            "20000/20000 - 36s - loss: 14.6865 - mse: 14.6865 - mae: 1.7337 - 36s/epoch - 2ms/step\n",
            "Epoch 115/200\n",
            "20000/20000 - 36s - loss: 14.8793 - mse: 14.8793 - mae: 1.7652 - 36s/epoch - 2ms/step\n",
            "Epoch 116/200\n",
            "20000/20000 - 42s - loss: 14.8550 - mse: 14.8550 - mae: 1.7616 - 42s/epoch - 2ms/step\n",
            "Epoch 117/200\n",
            "20000/20000 - 42s - loss: 14.6212 - mse: 14.6212 - mae: 1.7277 - 42s/epoch - 2ms/step\n",
            "Epoch 118/200\n",
            "20000/20000 - 42s - loss: 14.8254 - mse: 14.8254 - mae: 1.7545 - 42s/epoch - 2ms/step\n",
            "Epoch 119/200\n",
            "20000/20000 - 43s - loss: 14.8892 - mse: 14.8892 - mae: 1.7578 - 43s/epoch - 2ms/step\n",
            "Epoch 120/200\n",
            "20000/20000 - 42s - loss: 14.9076 - mse: 14.9076 - mae: 1.7644 - 42s/epoch - 2ms/step\n",
            "Epoch 121/200\n",
            "20000/20000 - 41s - loss: 14.8654 - mse: 14.8654 - mae: 1.7661 - 41s/epoch - 2ms/step\n",
            "Epoch 122/200\n",
            "20000/20000 - 40s - loss: 14.5871 - mse: 14.5871 - mae: 1.7230 - 40s/epoch - 2ms/step\n",
            "Epoch 123/200\n",
            "20000/20000 - 38s - loss: 14.6770 - mse: 14.6770 - mae: 1.7304 - 38s/epoch - 2ms/step\n",
            "Epoch 124/200\n",
            "20000/20000 - 37s - loss: 14.8592 - mse: 14.8592 - mae: 1.7570 - 37s/epoch - 2ms/step\n",
            "Epoch 125/200\n",
            "20000/20000 - 36s - loss: 14.9856 - mse: 14.9856 - mae: 1.7747 - 36s/epoch - 2ms/step\n",
            "Epoch 126/200\n",
            "20000/20000 - 37s - loss: 14.7773 - mse: 14.7773 - mae: 1.7456 - 37s/epoch - 2ms/step\n",
            "Epoch 127/200\n",
            "20000/20000 - 36s - loss: 14.8388 - mse: 14.8388 - mae: 1.7604 - 36s/epoch - 2ms/step\n",
            "Epoch 128/200\n",
            "20000/20000 - 36s - loss: 15.0416 - mse: 15.0416 - mae: 1.7772 - 36s/epoch - 2ms/step\n",
            "Epoch 129/200\n",
            "20000/20000 - 36s - loss: 15.0636 - mse: 15.0636 - mae: 1.7816 - 36s/epoch - 2ms/step\n",
            "Epoch 130/200\n",
            "20000/20000 - 36s - loss: 14.9641 - mse: 14.9641 - mae: 1.7695 - 36s/epoch - 2ms/step\n",
            "Epoch 131/200\n",
            "20000/20000 - 36s - loss: 14.9401 - mse: 14.9401 - mae: 1.7626 - 36s/epoch - 2ms/step\n",
            "Epoch 132/200\n",
            "20000/20000 - 37s - loss: 14.8894 - mse: 14.8894 - mae: 1.7578 - 37s/epoch - 2ms/step\n",
            "Epoch 133/200\n",
            "20000/20000 - 38s - loss: 14.5254 - mse: 14.5254 - mae: 1.7010 - 38s/epoch - 2ms/step\n",
            "Epoch 134/200\n",
            "20000/20000 - 38s - loss: 14.5129 - mse: 14.5129 - mae: 1.6909 - 38s/epoch - 2ms/step\n",
            "Epoch 135/200\n",
            "20000/20000 - 38s - loss: 14.5350 - mse: 14.5350 - mae: 1.6928 - 38s/epoch - 2ms/step\n",
            "Epoch 136/200\n",
            "20000/20000 - 36s - loss: 14.8419 - mse: 14.8419 - mae: 1.7501 - 36s/epoch - 2ms/step\n",
            "Epoch 137/200\n",
            "20000/20000 - 36s - loss: 14.7305 - mse: 14.7305 - mae: 1.7367 - 36s/epoch - 2ms/step\n",
            "Epoch 138/200\n",
            "20000/20000 - 36s - loss: 14.5700 - mse: 14.5700 - mae: 1.7073 - 36s/epoch - 2ms/step\n",
            "Epoch 139/200\n",
            "20000/20000 - 36s - loss: 14.6230 - mse: 14.6230 - mae: 1.7243 - 36s/epoch - 2ms/step\n",
            "Epoch 140/200\n",
            "20000/20000 - 36s - loss: 14.6157 - mse: 14.6157 - mae: 1.7242 - 36s/epoch - 2ms/step\n",
            "Epoch 141/200\n",
            "20000/20000 - 36s - loss: 14.5962 - mse: 14.5962 - mae: 1.7203 - 36s/epoch - 2ms/step\n",
            "Epoch 142/200\n",
            "20000/20000 - 36s - loss: 14.6352 - mse: 14.6352 - mae: 1.7180 - 36s/epoch - 2ms/step\n",
            "Epoch 143/200\n",
            "20000/20000 - 35s - loss: 14.7394 - mse: 14.7394 - mae: 1.7488 - 35s/epoch - 2ms/step\n",
            "Epoch 144/200\n",
            "20000/20000 - 36s - loss: 14.6836 - mse: 14.6836 - mae: 1.7231 - 36s/epoch - 2ms/step\n",
            "Epoch 145/200\n",
            "20000/20000 - 36s - loss: 14.6817 - mse: 14.6817 - mae: 1.7265 - 36s/epoch - 2ms/step\n",
            "Epoch 146/200\n",
            "20000/20000 - 35s - loss: 14.6418 - mse: 14.6418 - mae: 1.7329 - 35s/epoch - 2ms/step\n",
            "Epoch 147/200\n",
            "20000/20000 - 35s - loss: 14.7176 - mse: 14.7176 - mae: 1.7420 - 35s/epoch - 2ms/step\n",
            "Epoch 148/200\n",
            "20000/20000 - 36s - loss: 14.5200 - mse: 14.5200 - mae: 1.7088 - 36s/epoch - 2ms/step\n",
            "Epoch 149/200\n",
            "20000/20000 - 36s - loss: 14.7604 - mse: 14.7604 - mae: 1.7407 - 36s/epoch - 2ms/step\n",
            "Epoch 150/200\n",
            "20000/20000 - 36s - loss: 14.6234 - mse: 14.6234 - mae: 1.7284 - 36s/epoch - 2ms/step\n",
            "Epoch 151/200\n",
            "20000/20000 - 35s - loss: 14.6533 - mse: 14.6533 - mae: 1.7227 - 35s/epoch - 2ms/step\n",
            "Epoch 152/200\n",
            "20000/20000 - 35s - loss: 14.6304 - mse: 14.6304 - mae: 1.7341 - 35s/epoch - 2ms/step\n",
            "Epoch 153/200\n",
            "20000/20000 - 35s - loss: 14.9810 - mse: 14.9810 - mae: 1.7759 - 35s/epoch - 2ms/step\n",
            "Epoch 154/200\n",
            "20000/20000 - 35s - loss: 15.0642 - mse: 15.0642 - mae: 1.7835 - 35s/epoch - 2ms/step\n",
            "Epoch 155/200\n",
            "20000/20000 - 38s - loss: 14.8385 - mse: 14.8385 - mae: 1.7565 - 38s/epoch - 2ms/step\n",
            "Epoch 156/200\n",
            "20000/20000 - 41s - loss: 14.6066 - mse: 14.6066 - mae: 1.7145 - 41s/epoch - 2ms/step\n",
            "Epoch 157/200\n",
            "20000/20000 - 39s - loss: 14.5269 - mse: 14.5269 - mae: 1.7057 - 39s/epoch - 2ms/step\n",
            "Epoch 158/200\n",
            "20000/20000 - 38s - loss: 14.5865 - mse: 14.5865 - mae: 1.7172 - 38s/epoch - 2ms/step\n",
            "Epoch 159/200\n",
            "20000/20000 - 37s - loss: 14.7891 - mse: 14.7891 - mae: 1.7474 - 37s/epoch - 2ms/step\n",
            "Epoch 160/200\n",
            "20000/20000 - 36s - loss: 15.1484 - mse: 15.1484 - mae: 1.7864 - 36s/epoch - 2ms/step\n",
            "Epoch 161/200\n",
            "20000/20000 - 36s - loss: 14.8499 - mse: 14.8499 - mae: 1.7523 - 36s/epoch - 2ms/step\n",
            "Epoch 162/200\n",
            "20000/20000 - 38s - loss: 14.7176 - mse: 14.7176 - mae: 1.7328 - 38s/epoch - 2ms/step\n",
            "Epoch 163/200\n",
            "20000/20000 - 36s - loss: 14.8650 - mse: 14.8650 - mae: 1.7543 - 36s/epoch - 2ms/step\n",
            "Epoch 164/200\n",
            "20000/20000 - 37s - loss: 14.8976 - mse: 14.8976 - mae: 1.7640 - 37s/epoch - 2ms/step\n",
            "Epoch 165/200\n",
            "20000/20000 - 36s - loss: 15.0874 - mse: 15.0874 - mae: 1.7863 - 36s/epoch - 2ms/step\n",
            "Epoch 166/200\n",
            "20000/20000 - 36s - loss: 15.1598 - mse: 15.1598 - mae: 1.7907 - 36s/epoch - 2ms/step\n",
            "Epoch 167/200\n",
            "20000/20000 - 35s - loss: 14.9563 - mse: 14.9563 - mae: 1.7723 - 35s/epoch - 2ms/step\n",
            "Epoch 168/200\n",
            "20000/20000 - 35s - loss: 15.0157 - mse: 15.0157 - mae: 1.7836 - 35s/epoch - 2ms/step\n",
            "Epoch 169/200\n",
            "20000/20000 - 35s - loss: 14.9875 - mse: 14.9875 - mae: 1.7811 - 35s/epoch - 2ms/step\n",
            "Epoch 170/200\n",
            "20000/20000 - 35s - loss: 15.0254 - mse: 15.0254 - mae: 1.7785 - 35s/epoch - 2ms/step\n",
            "Epoch 171/200\n",
            "20000/20000 - 35s - loss: 14.7031 - mse: 14.7031 - mae: 1.7390 - 35s/epoch - 2ms/step\n",
            "Epoch 172/200\n",
            "20000/20000 - 36s - loss: 14.8413 - mse: 14.8413 - mae: 1.7460 - 36s/epoch - 2ms/step\n",
            "Epoch 173/200\n",
            "20000/20000 - 41s - loss: 14.9705 - mse: 14.9705 - mae: 1.7511 - 41s/epoch - 2ms/step\n",
            "Epoch 174/200\n",
            "20000/20000 - 40s - loss: 14.7891 - mse: 14.7891 - mae: 1.7366 - 40s/epoch - 2ms/step\n",
            "Epoch 175/200\n",
            "20000/20000 - 41s - loss: 14.7776 - mse: 14.7776 - mae: 1.7306 - 41s/epoch - 2ms/step\n",
            "Epoch 176/200\n",
            "20000/20000 - 40s - loss: 14.7111 - mse: 14.7111 - mae: 1.7174 - 40s/epoch - 2ms/step\n",
            "Epoch 177/200\n",
            "20000/20000 - 40s - loss: 14.9930 - mse: 14.9930 - mae: 1.7625 - 40s/epoch - 2ms/step\n",
            "Epoch 178/200\n",
            "20000/20000 - 40s - loss: 14.8638 - mse: 14.8638 - mae: 1.7505 - 40s/epoch - 2ms/step\n",
            "Epoch 179/200\n",
            "20000/20000 - 39s - loss: 14.8254 - mse: 14.8254 - mae: 1.7432 - 39s/epoch - 2ms/step\n",
            "Epoch 180/200\n",
            "20000/20000 - 40s - loss: 15.0077 - mse: 15.0077 - mae: 1.7789 - 40s/epoch - 2ms/step\n",
            "Epoch 181/200\n",
            "20000/20000 - 42s - loss: 15.0139 - mse: 15.0139 - mae: 1.7794 - 42s/epoch - 2ms/step\n",
            "Epoch 182/200\n",
            "20000/20000 - 40s - loss: 14.9856 - mse: 14.9856 - mae: 1.7705 - 40s/epoch - 2ms/step\n",
            "Epoch 183/200\n",
            "20000/20000 - 39s - loss: 14.9819 - mse: 14.9819 - mae: 1.7690 - 39s/epoch - 2ms/step\n",
            "Epoch 184/200\n",
            "20000/20000 - 39s - loss: 15.0601 - mse: 15.0601 - mae: 1.7675 - 39s/epoch - 2ms/step\n",
            "Epoch 185/200\n",
            "20000/20000 - 39s - loss: 15.0386 - mse: 15.0386 - mae: 1.7675 - 39s/epoch - 2ms/step\n",
            "Epoch 186/200\n",
            "20000/20000 - 39s - loss: 15.0276 - mse: 15.0276 - mae: 1.7689 - 39s/epoch - 2ms/step\n",
            "Epoch 187/200\n",
            "20000/20000 - 39s - loss: 15.4106 - mse: 15.4106 - mae: 1.8180 - 39s/epoch - 2ms/step\n",
            "Epoch 188/200\n",
            "20000/20000 - 41s - loss: 15.0949 - mse: 15.0949 - mae: 1.7896 - 41s/epoch - 2ms/step\n",
            "Epoch 189/200\n",
            "20000/20000 - 41s - loss: 14.9410 - mse: 14.9410 - mae: 1.7694 - 41s/epoch - 2ms/step\n",
            "Epoch 190/200\n",
            "20000/20000 - 40s - loss: 15.1403 - mse: 15.1403 - mae: 1.7946 - 40s/epoch - 2ms/step\n",
            "Epoch 191/200\n",
            "20000/20000 - 39s - loss: 15.0374 - mse: 15.0374 - mae: 1.7859 - 39s/epoch - 2ms/step\n",
            "Epoch 192/200\n",
            "20000/20000 - 39s - loss: 15.1332 - mse: 15.1332 - mae: 1.7978 - 39s/epoch - 2ms/step\n",
            "Epoch 193/200\n",
            "20000/20000 - 40s - loss: 15.0229 - mse: 15.0229 - mae: 1.7889 - 40s/epoch - 2ms/step\n",
            "Epoch 194/200\n",
            "20000/20000 - 40s - loss: 15.0021 - mse: 15.0021 - mae: 1.7795 - 40s/epoch - 2ms/step\n",
            "Epoch 195/200\n",
            "20000/20000 - 41s - loss: 15.0724 - mse: 15.0724 - mae: 1.7880 - 41s/epoch - 2ms/step\n",
            "Epoch 196/200\n",
            "20000/20000 - 41s - loss: 15.0582 - mse: 15.0582 - mae: 1.7850 - 41s/epoch - 2ms/step\n",
            "Epoch 197/200\n",
            "20000/20000 - 41s - loss: 15.3631 - mse: 15.3631 - mae: 1.8159 - 41s/epoch - 2ms/step\n",
            "Epoch 198/200\n",
            "20000/20000 - 42s - loss: 15.2841 - mse: 15.2841 - mae: 1.8094 - 42s/epoch - 2ms/step\n",
            "Epoch 199/200\n",
            "20000/20000 - 43s - loss: 15.3095 - mse: 15.3095 - mae: 1.8106 - 43s/epoch - 2ms/step\n",
            "Epoch 200/200\n",
            "20000/20000 - 41s - loss: 15.3891 - mse: 15.3891 - mae: 1.8115 - 41s/epoch - 2ms/step\n"
          ]
        }
      ],
      "source": [
        "num_folds = 3\n",
        "kfold=KFold(n_splits=3,shuffle=True)\n",
        "fold_no=1\n",
        "loss_per_fold = []\n",
        "model_cnn_best = create_model_cnn(activation='tanh', num_hidden_layer=5, num_hidden_unit=85, kernel_size=4, filter=77)\n",
        "model_cnn_best.summary()\n",
        "model_cnn_best.compile(loss='mse',optimizer='adam',metrics=['mse','mae'])\n",
        "for train,test in kfold.split(training,labelsForTrain):\n",
        "  scores=model_cnn_best.evaluate(testing,labelsForTest,verbose=0)\n",
        "\n",
        "  print('------------------------------------------------------------------------')\n",
        "  print(f'Training for fold {fold_no} ...')\n",
        "  \n",
        "  # Fit data to model\n",
        "  history = model_cnn_best.fit(training, labelsForTrain,\n",
        "              batch_size=20,\n",
        "              #The result from epoch 20 and epoch 200 don't make too much differenct\n",
        "              epochs=100,\n",
        "              verbose=2,)\n",
        "    \n",
        "  print(f'Score for fold {fold_no}: {model_cnn_best.metrics_names[0]} of {scores[0]}')\n",
        "  loss_per_fold.append(scores[0])\n",
        "  # Increase fold number\n",
        "  fold_no = fold_no + 1 \n",
        "\n",
        "model_cnn_best.compile(loss='mse',optimizer='adam',metrics=['mse','mae'])\n",
        "history = model_cnn_best.fit(training, labelsForTrain,\n",
        "                batch_size=20,\n",
        "                epochs=200,\n",
        "                verbose=2)\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iLuz0oqakMDZ",
        "outputId": "ac569c7a-5aef-4b83-fd5a-3ee7f7af1d8c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5000/5000 [==============================] - 6s 1ms/step - loss: 20.3916 - mse: 20.3916 - mae: 1.8200\n"
          ]
        }
      ],
      "source": [
        "results = model_cnn_best.evaluate(testing, labelsForTest, batch_size=20)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GOMJvBe1jmMn",
        "outputId": "075ef501-3ac5-4f23-b2a0-cc72b0948ae3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The Mean Square Error for test (CNN) is:  18.479924529298682\n",
            "The Root Mean Square Error for test (CNN) is:  4.298828273994983\n"
          ]
        }
      ],
      "source": [
        "summation = 0  #variable to store the summation of differences\n",
        "test_predictions_cnn = model_cnn_best.predict(testing).flatten()\n",
        "n = len(test_predictions_cnn) #finding total number of items in list\n",
        "for i in range (0,n):  #looping through each element of the list\n",
        "  difference = labelsForTest[i] - test_predictions_cnn[i]  #finding the difference between observed and predicted value\n",
        "  squared_difference = difference**2  #taking square of the differene \n",
        "  summation = summation + squared_difference  #taking a sum of all the differences\n",
        "MSE = summation/n  #dividing summation by total values to obtain average\n",
        "RMSE = math.sqrt(MSE)\n",
        "print (\"The Mean Square Error for test (CNN) is: \" , MSE)\n",
        "print (\"The Root Mean Square Error for test (CNN) is: \" , RMSE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "voo5pITPZime",
        "outputId": "608cbd34-f3db-4e5d-b8b1-109e85b2de7c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 26s - loss: 14.7860 - mse: 14.7860 - mape: 81.2645 - 26s/epoch - 1ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 26s - loss: 14.6950 - mse: 14.6950 - mape: 80.9104 - 26s/epoch - 1ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 25s - loss: 14.6744 - mse: 14.6744 - mape: 80.6904 - 25s/epoch - 1ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 25s - loss: 14.6636 - mse: 14.6636 - mape: 80.7697 - 25s/epoch - 1ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 26s - loss: 14.6661 - mse: 14.6661 - mape: 80.6036 - 26s/epoch - 1ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 25s - loss: 14.6694 - mse: 14.6694 - mape: 80.7526 - 25s/epoch - 1ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 25s - loss: 14.6539 - mse: 14.6539 - mape: 80.6988 - 25s/epoch - 1ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 25s - loss: 14.6513 - mse: 14.6513 - mape: 80.6673 - 25s/epoch - 1ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 26s - loss: 14.6469 - mse: 14.6469 - mape: 80.8592 - 26s/epoch - 1ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 26s - loss: 14.6412 - mse: 14.6412 - mape: 80.8076 - 26s/epoch - 1ms/step\n",
            "Score for fold 1: loss of 28.182327270507812\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 26s - loss: 14.6557 - mse: 14.6557 - mape: 80.7923 - 26s/epoch - 1ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 26s - loss: 14.6391 - mse: 14.6391 - mape: 80.6833 - 26s/epoch - 1ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 26s - loss: 14.6535 - mse: 14.6535 - mape: 80.7237 - 26s/epoch - 1ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 27s - loss: 14.6432 - mse: 14.6432 - mape: 80.7230 - 27s/epoch - 1ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 26s - loss: 14.6602 - mse: 14.6602 - mape: 80.6780 - 26s/epoch - 1ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 26s - loss: 14.6436 - mse: 14.6436 - mape: 80.7240 - 26s/epoch - 1ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 26s - loss: 14.6548 - mse: 14.6548 - mape: 80.6740 - 26s/epoch - 1ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 26s - loss: 14.6526 - mse: 14.6526 - mape: 80.5718 - 26s/epoch - 1ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 26s - loss: 14.6440 - mse: 14.6440 - mape: 80.7843 - 26s/epoch - 1ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 26s - loss: 14.6604 - mse: 14.6604 - mape: 80.7519 - 26s/epoch - 1ms/step\n",
            "Score for fold 2: loss of 19.72196388244629\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 25s - loss: 14.6482 - mse: 14.6482 - mape: 80.7151 - 25s/epoch - 1ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 26s - loss: 14.6500 - mse: 14.6500 - mape: 80.7954 - 26s/epoch - 1ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 26s - loss: 14.6501 - mse: 14.6501 - mape: 80.5445 - 26s/epoch - 1ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 26s - loss: 14.6448 - mse: 14.6448 - mape: 80.6768 - 26s/epoch - 1ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 25s - loss: 14.6445 - mse: 14.6445 - mape: 80.7596 - 25s/epoch - 1ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 25s - loss: 14.6370 - mse: 14.6370 - mape: 80.6913 - 25s/epoch - 1ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 25s - loss: 14.6466 - mse: 14.6466 - mape: 80.6062 - 25s/epoch - 1ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 26s - loss: 14.6373 - mse: 14.6373 - mape: 80.6762 - 26s/epoch - 1ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 26s - loss: 14.6471 - mse: 14.6471 - mape: 80.6135 - 26s/epoch - 1ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 26s - loss: 14.6417 - mse: 14.6417 - mape: 80.6915 - 26s/epoch - 1ms/step\n",
            "Score for fold 3: loss of 19.72260284423828\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-09-27 11:06:34,306]\u001b[0m Finished trial#0 resulted in value: 14.641683578491211. Current best value is 14.641683578491211 with parameters: {'activation': 'linear', 'optimizer': 'rmsprop', 'num_hidden_layer': 4, 'num_hidden_unit': 84, 'learning_rate': 0.003470769400535044}.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 25s - loss: 14.8915 - mse: 14.8915 - mape: 84.9923 - 25s/epoch - 1ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 24s - loss: 14.7340 - mse: 14.7340 - mape: 85.6471 - 24s/epoch - 1ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 24s - loss: 14.7298 - mse: 14.7298 - mape: 85.5716 - 24s/epoch - 1ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 24s - loss: 14.7173 - mse: 14.7173 - mape: 85.4801 - 24s/epoch - 1ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 24s - loss: 14.7254 - mse: 14.7254 - mape: 85.7689 - 24s/epoch - 1ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 24s - loss: 14.7013 - mse: 14.7013 - mape: 85.7187 - 24s/epoch - 1ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 24s - loss: 14.7400 - mse: 14.7400 - mape: 86.0393 - 24s/epoch - 1ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 24s - loss: 14.7284 - mse: 14.7284 - mape: 85.8007 - 24s/epoch - 1ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 24s - loss: 14.7324 - mse: 14.7324 - mape: 85.9275 - 24s/epoch - 1ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 24s - loss: 14.7673 - mse: 14.7673 - mape: 85.7741 - 24s/epoch - 1ms/step\n",
            "Score for fold 1: loss of 28.514991760253906\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 25s - loss: 14.7835 - mse: 14.7835 - mape: 86.0843 - 25s/epoch - 1ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 24s - loss: 14.7602 - mse: 14.7602 - mape: 86.0512 - 24s/epoch - 1ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 24s - loss: 14.7471 - mse: 14.7471 - mape: 85.7044 - 24s/epoch - 1ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 25s - loss: 14.7904 - mse: 14.7904 - mape: 85.7678 - 25s/epoch - 1ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 24s - loss: 14.7366 - mse: 14.7366 - mape: 85.6174 - 24s/epoch - 1ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 24s - loss: 14.7687 - mse: 14.7687 - mape: 85.5361 - 24s/epoch - 1ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 24s - loss: 14.7224 - mse: 14.7224 - mape: 85.4850 - 24s/epoch - 1ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 25s - loss: 14.7110 - mse: 14.7110 - mape: 85.5910 - 25s/epoch - 1ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 24s - loss: 14.7599 - mse: 14.7599 - mape: 85.6388 - 24s/epoch - 1ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 24s - loss: 14.7184 - mse: 14.7184 - mape: 85.6093 - 24s/epoch - 1ms/step\n",
            "Score for fold 2: loss of 19.472183227539062\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 24s - loss: 14.7186 - mse: 14.7186 - mape: 85.5863 - 24s/epoch - 1ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 24s - loss: 14.7397 - mse: 14.7397 - mape: 85.7708 - 24s/epoch - 1ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 24s - loss: 14.7412 - mse: 14.7412 - mape: 85.4593 - 24s/epoch - 1ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 25s - loss: 14.7604 - mse: 14.7604 - mape: 85.5274 - 25s/epoch - 1ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 25s - loss: 14.7639 - mse: 14.7639 - mape: 85.4502 - 25s/epoch - 1ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 24s - loss: 14.7112 - mse: 14.7112 - mape: 85.3734 - 24s/epoch - 1ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 24s - loss: 14.7737 - mse: 14.7737 - mape: 85.3700 - 24s/epoch - 1ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 24s - loss: 14.7070 - mse: 14.7070 - mape: 85.4035 - 24s/epoch - 1ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 24s - loss: 14.7687 - mse: 14.7687 - mape: 85.4197 - 24s/epoch - 1ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 24s - loss: 14.7429 - mse: 14.7429 - mape: 85.5421 - 24s/epoch - 1ms/step\n",
            "Score for fold 3: loss of 19.620166778564453\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-09-27 11:18:51,246]\u001b[0m Finished trial#1 resulted in value: 14.742877960205078. Current best value is 14.641683578491211 with parameters: {'activation': 'linear', 'optimizer': 'rmsprop', 'num_hidden_layer': 4, 'num_hidden_unit': 84, 'learning_rate': 0.003470769400535044}.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 21s - loss: 17.0342 - mse: 17.0342 - mape: 73.8339 - 21s/epoch - 1ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 21s - loss: 14.7361 - mse: 14.7361 - mape: 80.4231 - 21s/epoch - 1ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 21s - loss: 14.6013 - mse: 14.6013 - mape: 80.4133 - 21s/epoch - 1ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 21s - loss: 14.5268 - mse: 14.5268 - mape: 80.3706 - 21s/epoch - 1ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 21s - loss: 14.4750 - mse: 14.4750 - mape: 80.3662 - 21s/epoch - 1ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 21s - loss: 14.4326 - mse: 14.4326 - mape: 80.2567 - 21s/epoch - 1ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 21s - loss: 14.3981 - mse: 14.3981 - mape: 80.0197 - 21s/epoch - 1ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 20s - loss: 14.3649 - mse: 14.3649 - mape: 79.9947 - 20s/epoch - 1ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 21s - loss: 14.3330 - mse: 14.3330 - mape: 80.0816 - 21s/epoch - 1ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 21s - loss: 14.3016 - mse: 14.3016 - mape: 79.8916 - 21s/epoch - 1ms/step\n",
            "Score for fold 1: loss of 28.6832275390625\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 21s - loss: 14.2702 - mse: 14.2702 - mape: 79.8122 - 21s/epoch - 1ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 21s - loss: 14.2411 - mse: 14.2411 - mape: 79.7476 - 21s/epoch - 1ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 20s - loss: 14.2137 - mse: 14.2137 - mape: 79.7109 - 20s/epoch - 1ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 21s - loss: 14.1891 - mse: 14.1891 - mape: 79.4016 - 21s/epoch - 1ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 20s - loss: 14.1658 - mse: 14.1658 - mape: 79.4792 - 20s/epoch - 1ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 21s - loss: 14.1446 - mse: 14.1446 - mape: 79.2535 - 21s/epoch - 1ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 20s - loss: 14.1257 - mse: 14.1257 - mape: 79.1245 - 20s/epoch - 1ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 21s - loss: 14.1085 - mse: 14.1085 - mape: 79.1012 - 21s/epoch - 1ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 21s - loss: 14.0932 - mse: 14.0932 - mape: 79.1386 - 21s/epoch - 1ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 21s - loss: 14.0797 - mse: 14.0797 - mape: 78.9807 - 21s/epoch - 1ms/step\n",
            "Score for fold 2: loss of 19.30586814880371\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 21s - loss: 14.0682 - mse: 14.0682 - mape: 78.7578 - 21s/epoch - 1ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 21s - loss: 14.0581 - mse: 14.0581 - mape: 78.9774 - 21s/epoch - 1ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 21s - loss: 14.0493 - mse: 14.0493 - mape: 78.6585 - 21s/epoch - 1ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 20s - loss: 14.0411 - mse: 14.0411 - mape: 78.8684 - 20s/epoch - 1ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 21s - loss: 14.0336 - mse: 14.0336 - mape: 78.6158 - 21s/epoch - 1ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 21s - loss: 14.0257 - mse: 14.0257 - mape: 78.7915 - 21s/epoch - 1ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 21s - loss: 14.0188 - mse: 14.0188 - mape: 78.6118 - 21s/epoch - 1ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 21s - loss: 14.0127 - mse: 14.0127 - mape: 78.8090 - 21s/epoch - 1ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 20s - loss: 14.0067 - mse: 14.0067 - mape: 78.5332 - 20s/epoch - 1ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 21s - loss: 14.0001 - mse: 14.0001 - mape: 78.5716 - 21s/epoch - 1ms/step\n",
            "Score for fold 3: loss of 19.075998306274414\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-09-27 11:29:21,978]\u001b[0m Finished trial#2 resulted in value: 14.000121116638184. Current best value is 14.000121116638184 with parameters: {'activation': 'relu', 'optimizer': 'sgd', 'num_hidden_layer': 2, 'num_hidden_unit': 48, 'learning_rate': 2.249312000174846e-05}.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 24s - loss: 19.0873 - mse: 19.0873 - mape: 112.9232 - 24s/epoch - 1ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 24s - loss: 22.3296 - mse: 22.3296 - mape: 107.3897 - 24s/epoch - 1ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 24s - loss: 21.4129 - mse: 21.4129 - mape: 108.6139 - 24s/epoch - 1ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 24s - loss: 21.6553 - mse: 21.6553 - mape: 108.3486 - 24s/epoch - 1ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 24s - loss: 22.4297 - mse: 22.4297 - mape: 104.1371 - 24s/epoch - 1ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 25s - loss: 23.2029 - mse: 23.2029 - mape: 101.6245 - 25s/epoch - 1ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 24s - loss: 21.4839 - mse: 21.4839 - mape: 107.1259 - 24s/epoch - 1ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 24s - loss: 21.6597 - mse: 21.6597 - mape: 106.1402 - 24s/epoch - 1ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 24s - loss: 22.7645 - mse: 22.7645 - mape: 103.5774 - 24s/epoch - 1ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 24s - loss: 23.4695 - mse: 23.4695 - mape: 100.4764 - 24s/epoch - 1ms/step\n",
            "Score for fold 1: loss of 27.57624053955078\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 24s - loss: 23.6738 - mse: 23.6738 - mape: 100.0000 - 24s/epoch - 1ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 24s - loss: 23.6738 - mse: 23.6738 - mape: 100.0000 - 24s/epoch - 1ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 24s - loss: 23.6738 - mse: 23.6738 - mape: 100.0000 - 24s/epoch - 1ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 24s - loss: 23.6738 - mse: 23.6738 - mape: 100.0000 - 24s/epoch - 1ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 24s - loss: 23.6738 - mse: 23.6738 - mape: 100.0000 - 24s/epoch - 1ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 24s - loss: 23.6738 - mse: 23.6738 - mape: 100.0000 - 24s/epoch - 1ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 24s - loss: 23.6738 - mse: 23.6738 - mape: 100.0000 - 24s/epoch - 1ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 24s - loss: 23.6738 - mse: 23.6738 - mape: 100.0000 - 24s/epoch - 1ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 24s - loss: 23.6738 - mse: 23.6738 - mape: 100.0000 - 24s/epoch - 1ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 24s - loss: 23.6738 - mse: 23.6738 - mape: 100.0000 - 24s/epoch - 1ms/step\n",
            "Score for fold 2: loss of 29.109249114990234\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 24s - loss: 23.6738 - mse: 23.6738 - mape: 100.0000 - 24s/epoch - 1ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 24s - loss: 23.6738 - mse: 23.6738 - mape: 100.0000 - 24s/epoch - 1ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 24s - loss: 23.6738 - mse: 23.6738 - mape: 100.0000 - 24s/epoch - 1ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 24s - loss: 23.6738 - mse: 23.6738 - mape: 100.0000 - 24s/epoch - 1ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 24s - loss: 23.6738 - mse: 23.6738 - mape: 100.0000 - 24s/epoch - 1ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 24s - loss: 23.6738 - mse: 23.6738 - mape: 100.0000 - 24s/epoch - 1ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 24s - loss: 23.6738 - mse: 23.6738 - mape: 100.0000 - 24s/epoch - 1ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 24s - loss: 23.6738 - mse: 23.6738 - mape: 100.0000 - 24s/epoch - 1ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 24s - loss: 23.6738 - mse: 23.6738 - mape: 100.0000 - 24s/epoch - 1ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 24s - loss: 23.6738 - mse: 23.6738 - mape: 100.0000 - 24s/epoch - 1ms/step\n",
            "Score for fold 3: loss of 29.109249114990234\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-09-27 11:41:35,651]\u001b[0m Finished trial#3 resulted in value: 23.673847198486328. Current best value is 14.000121116638184 with parameters: {'activation': 'relu', 'optimizer': 'sgd', 'num_hidden_layer': 2, 'num_hidden_unit': 48, 'learning_rate': 2.249312000174846e-05}.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 20s - loss: 16.5869 - mse: 16.5869 - mape: 76.8483 - 20s/epoch - 1ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 20s - loss: 14.7667 - mse: 14.7667 - mape: 81.3203 - 20s/epoch - 992us/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 20s - loss: 14.6321 - mse: 14.6321 - mape: 80.7828 - 20s/epoch - 999us/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 20s - loss: 14.5949 - mse: 14.5949 - mape: 80.8602 - 20s/epoch - 978us/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 20s - loss: 14.5748 - mse: 14.5748 - mape: 80.7676 - 20s/epoch - 993us/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 21s - loss: 14.5613 - mse: 14.5613 - mape: 81.0712 - 21s/epoch - 1ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 20s - loss: 14.5526 - mse: 14.5526 - mape: 81.0045 - 20s/epoch - 994us/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 20s - loss: 14.5457 - mse: 14.5457 - mape: 80.9517 - 20s/epoch - 1ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 20s - loss: 14.5406 - mse: 14.5406 - mape: 81.2085 - 20s/epoch - 994us/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 20s - loss: 14.5367 - mse: 14.5367 - mape: 81.1160 - 20s/epoch - 975us/step\n",
            "Score for fold 1: loss of 27.745895385742188\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 20s - loss: 14.5331 - mse: 14.5331 - mape: 81.1780 - 20s/epoch - 984us/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 20s - loss: 14.5306 - mse: 14.5306 - mape: 81.2738 - 20s/epoch - 987us/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 20s - loss: 14.5278 - mse: 14.5278 - mape: 81.3703 - 20s/epoch - 1ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 20s - loss: 14.5262 - mse: 14.5262 - mape: 81.3119 - 20s/epoch - 995us/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 20s - loss: 14.5249 - mse: 14.5249 - mape: 81.0351 - 20s/epoch - 1ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 20s - loss: 14.5227 - mse: 14.5227 - mape: 81.3493 - 20s/epoch - 993us/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 21s - loss: 14.5227 - mse: 14.5227 - mape: 81.3647 - 21s/epoch - 1ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 19s - loss: 14.5216 - mse: 14.5216 - mape: 81.3354 - 19s/epoch - 964us/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 19s - loss: 14.5206 - mse: 14.5206 - mape: 81.2832 - 19s/epoch - 944us/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 20s - loss: 14.5204 - mse: 14.5204 - mape: 81.2967 - 20s/epoch - 996us/step\n",
            "Score for fold 2: loss of 19.567522048950195\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 20s - loss: 14.5196 - mse: 14.5196 - mape: 81.2786 - 20s/epoch - 1ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 20s - loss: 14.5188 - mse: 14.5188 - mape: 81.3660 - 20s/epoch - 1ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 20s - loss: 14.5189 - mse: 14.5189 - mape: 81.2957 - 20s/epoch - 1ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 20s - loss: 14.5184 - mse: 14.5184 - mape: 81.4373 - 20s/epoch - 1ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 20s - loss: 14.5180 - mse: 14.5180 - mape: 81.3305 - 20s/epoch - 1ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 21s - loss: 14.5176 - mse: 14.5176 - mape: 81.3115 - 21s/epoch - 1ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 20s - loss: 14.5173 - mse: 14.5173 - mape: 81.4614 - 20s/epoch - 986us/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 20s - loss: 14.5172 - mse: 14.5172 - mape: 81.2882 - 20s/epoch - 1ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 20s - loss: 14.5165 - mse: 14.5165 - mape: 81.4120 - 20s/epoch - 1ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 21s - loss: 14.5170 - mse: 14.5170 - mape: 81.3025 - 21s/epoch - 1ms/step\n",
            "Score for fold 3: loss of 19.552593231201172\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-09-27 11:51:46,021]\u001b[0m Finished trial#4 resulted in value: 14.5169677734375. Current best value is 14.000121116638184 with parameters: {'activation': 'relu', 'optimizer': 'sgd', 'num_hidden_layer': 2, 'num_hidden_unit': 48, 'learning_rate': 2.249312000174846e-05}.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 21s - loss: 19.8369 - mse: 19.8369 - mape: 73.7992 - 21s/epoch - 1ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 22s - loss: 16.1823 - mse: 16.1823 - mape: 80.5896 - 22s/epoch - 1ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 22s - loss: 15.3172 - mse: 15.3172 - mape: 83.9387 - 22s/epoch - 1ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 22s - loss: 14.9752 - mse: 14.9752 - mape: 82.0871 - 22s/epoch - 1ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 22s - loss: 14.7929 - mse: 14.7929 - mape: 80.8500 - 22s/epoch - 1ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 22s - loss: 14.6680 - mse: 14.6680 - mape: 79.9939 - 22s/epoch - 1ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 21s - loss: 14.5730 - mse: 14.5730 - mape: 79.6977 - 21s/epoch - 1ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 22s - loss: 14.4978 - mse: 14.4978 - mape: 79.5794 - 22s/epoch - 1ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 21s - loss: 14.4374 - mse: 14.4374 - mape: 79.0358 - 21s/epoch - 1ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 22s - loss: 14.3897 - mse: 14.3897 - mape: 79.3086 - 22s/epoch - 1ms/step\n",
            "Score for fold 1: loss of 28.093671798706055\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 22s - loss: 14.3511 - mse: 14.3511 - mape: 78.7979 - 22s/epoch - 1ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 22s - loss: 14.3198 - mse: 14.3198 - mape: 79.1206 - 22s/epoch - 1ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 21s - loss: 14.2928 - mse: 14.2928 - mape: 78.7484 - 21s/epoch - 1ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 22s - loss: 14.2684 - mse: 14.2684 - mape: 78.7109 - 22s/epoch - 1ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 22s - loss: 14.2475 - mse: 14.2475 - mape: 78.7377 - 22s/epoch - 1ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 22s - loss: 14.2290 - mse: 14.2290 - mape: 78.8318 - 22s/epoch - 1ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 22s - loss: 14.2122 - mse: 14.2122 - mape: 78.7698 - 22s/epoch - 1ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 22s - loss: 14.1974 - mse: 14.1974 - mape: 78.7270 - 22s/epoch - 1ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 22s - loss: 14.1838 - mse: 14.1838 - mape: 78.6721 - 22s/epoch - 1ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 21s - loss: 14.1718 - mse: 14.1718 - mape: 78.7543 - 21s/epoch - 1ms/step\n",
            "Score for fold 2: loss of 19.414173126220703\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 21s - loss: 14.1604 - mse: 14.1604 - mape: 78.6834 - 21s/epoch - 1ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 21s - loss: 14.1501 - mse: 14.1501 - mape: 78.8240 - 21s/epoch - 1ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 22s - loss: 14.1413 - mse: 14.1413 - mape: 78.6848 - 22s/epoch - 1ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 21s - loss: 14.1325 - mse: 14.1325 - mape: 78.9006 - 21s/epoch - 1ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 21s - loss: 14.1243 - mse: 14.1243 - mape: 78.8434 - 21s/epoch - 1ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 21s - loss: 14.1173 - mse: 14.1173 - mape: 78.5138 - 21s/epoch - 1ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 21s - loss: 14.1104 - mse: 14.1104 - mape: 78.9334 - 21s/epoch - 1ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 22s - loss: 14.1040 - mse: 14.1040 - mape: 78.7859 - 22s/epoch - 1ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 23s - loss: 14.0980 - mse: 14.0980 - mape: 78.7270 - 23s/epoch - 1ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 22s - loss: 14.0924 - mse: 14.0924 - mape: 78.8946 - 22s/epoch - 1ms/step\n",
            "Score for fold 3: loss of 19.186174392700195\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-09-27 12:02:44,465]\u001b[0m Finished trial#5 resulted in value: 14.092447280883789. Current best value is 14.000121116638184 with parameters: {'activation': 'relu', 'optimizer': 'sgd', 'num_hidden_layer': 2, 'num_hidden_unit': 48, 'learning_rate': 2.249312000174846e-05}.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 21s - loss: 21.6406 - mse: 21.6406 - mape: 83.5488 - 21s/epoch - 1ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 21s - loss: 20.9717 - mse: 20.9717 - mape: 79.5416 - 21s/epoch - 1ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 20s - loss: 20.5794 - mse: 20.5794 - mape: 78.3558 - 20s/epoch - 1ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 21s - loss: 20.2848 - mse: 20.2848 - mape: 77.7401 - 21s/epoch - 1ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 21s - loss: 20.0460 - mse: 20.0460 - mape: 77.3704 - 21s/epoch - 1ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 21s - loss: 19.8437 - mse: 19.8437 - mape: 77.1240 - 21s/epoch - 1ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 21s - loss: 19.6675 - mse: 19.6675 - mape: 76.9681 - 21s/epoch - 1ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 20s - loss: 19.5106 - mse: 19.5106 - mape: 76.8664 - 20s/epoch - 1ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 21s - loss: 19.3690 - mse: 19.3690 - mape: 76.7949 - 21s/epoch - 1ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 21s - loss: 19.2397 - mse: 19.2397 - mape: 76.7391 - 21s/epoch - 1ms/step\n",
            "Score for fold 1: loss of 27.93497085571289\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 21s - loss: 19.1203 - mse: 19.1203 - mape: 76.6911 - 21s/epoch - 1ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 21s - loss: 19.0091 - mse: 19.0091 - mape: 76.6468 - 21s/epoch - 1ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 21s - loss: 18.9050 - mse: 18.9050 - mape: 76.5905 - 21s/epoch - 1ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 21s - loss: 18.8069 - mse: 18.8069 - mape: 76.5271 - 21s/epoch - 1ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 21s - loss: 18.7138 - mse: 18.7138 - mape: 76.4612 - 21s/epoch - 1ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 21s - loss: 18.6252 - mse: 18.6252 - mape: 76.3769 - 21s/epoch - 1ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 21s - loss: 18.5402 - mse: 18.5402 - mape: 76.2810 - 21s/epoch - 1ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 21s - loss: 18.4585 - mse: 18.4585 - mape: 76.1635 - 21s/epoch - 1ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 21s - loss: 18.3798 - mse: 18.3798 - mape: 76.0388 - 21s/epoch - 1ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 21s - loss: 18.3035 - mse: 18.3035 - mape: 75.8854 - 21s/epoch - 1ms/step\n",
            "Score for fold 2: loss of 24.523075103759766\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 21s - loss: 18.2296 - mse: 18.2296 - mape: 75.7219 - 21s/epoch - 1ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 21s - loss: 18.1576 - mse: 18.1576 - mape: 75.5369 - 21s/epoch - 1ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 21s - loss: 18.0874 - mse: 18.0874 - mape: 75.3382 - 21s/epoch - 1ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 21s - loss: 18.0186 - mse: 18.0186 - mape: 75.1179 - 21s/epoch - 1ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 21s - loss: 17.9513 - mse: 17.9513 - mape: 74.8775 - 21s/epoch - 1ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 21s - loss: 17.8854 - mse: 17.8854 - mape: 74.6189 - 21s/epoch - 1ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 21s - loss: 17.8205 - mse: 17.8205 - mape: 74.3414 - 21s/epoch - 1ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 21s - loss: 17.7566 - mse: 17.7566 - mape: 74.0412 - 21s/epoch - 1ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 21s - loss: 17.6936 - mse: 17.6936 - mape: 73.7345 - 21s/epoch - 1ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 21s - loss: 17.6315 - mse: 17.6315 - mape: 73.3987 - 21s/epoch - 1ms/step\n",
            "Score for fold 3: loss of 23.567319869995117\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-09-27 12:13:22,179]\u001b[0m Finished trial#6 resulted in value: 17.631547927856445. Current best value is 14.000121116638184 with parameters: {'activation': 'relu', 'optimizer': 'sgd', 'num_hidden_layer': 2, 'num_hidden_unit': 48, 'learning_rate': 2.249312000174846e-05}.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 30s - loss: 14.6808 - mse: 14.6808 - mape: 81.5268 - 30s/epoch - 2ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 29s - loss: 14.5793 - mse: 14.5793 - mape: 81.4265 - 29s/epoch - 1ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 29s - loss: 14.5741 - mse: 14.5741 - mape: 81.4797 - 29s/epoch - 1ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 30s - loss: 14.5679 - mse: 14.5679 - mape: 81.4539 - 30s/epoch - 1ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 29s - loss: 14.5659 - mse: 14.5659 - mape: 81.4821 - 29s/epoch - 1ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 30s - loss: 14.5589 - mse: 14.5589 - mape: 81.4312 - 30s/epoch - 1ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 30s - loss: 14.5511 - mse: 14.5511 - mape: 81.4194 - 30s/epoch - 1ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 29s - loss: 14.5551 - mse: 14.5551 - mape: 81.4323 - 29s/epoch - 1ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 30s - loss: 14.5555 - mse: 14.5555 - mape: 81.4683 - 30s/epoch - 1ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 30s - loss: 14.5521 - mse: 14.5521 - mape: 81.4544 - 30s/epoch - 2ms/step\n",
            "Score for fold 1: loss of 27.979764938354492\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 30s - loss: 14.5498 - mse: 14.5498 - mape: 81.4599 - 30s/epoch - 1ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 30s - loss: 14.5449 - mse: 14.5449 - mape: 81.4828 - 30s/epoch - 1ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 30s - loss: 14.5473 - mse: 14.5473 - mape: 81.4736 - 30s/epoch - 1ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 30s - loss: 14.5473 - mse: 14.5473 - mape: 81.4419 - 30s/epoch - 1ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 30s - loss: 14.5439 - mse: 14.5439 - mape: 81.4872 - 30s/epoch - 1ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 30s - loss: 14.5480 - mse: 14.5480 - mape: 81.4639 - 30s/epoch - 1ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 30s - loss: 14.5456 - mse: 14.5456 - mape: 81.4309 - 30s/epoch - 1ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 30s - loss: 14.5444 - mse: 14.5444 - mape: 81.4277 - 30s/epoch - 1ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 30s - loss: 14.5482 - mse: 14.5482 - mape: 81.4668 - 30s/epoch - 1ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 30s - loss: 14.5446 - mse: 14.5446 - mape: 81.4789 - 30s/epoch - 2ms/step\n",
            "Score for fold 2: loss of 19.640907287597656\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 30s - loss: 14.5411 - mse: 14.5411 - mape: 81.4958 - 30s/epoch - 1ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 30s - loss: 14.5410 - mse: 14.5410 - mape: 81.4541 - 30s/epoch - 1ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 30s - loss: 14.5390 - mse: 14.5390 - mape: 81.4707 - 30s/epoch - 1ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 30s - loss: 14.5425 - mse: 14.5425 - mape: 81.5014 - 30s/epoch - 1ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 30s - loss: 14.5379 - mse: 14.5379 - mape: 81.4725 - 30s/epoch - 1ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 30s - loss: 14.5443 - mse: 14.5443 - mape: 81.4704 - 30s/epoch - 1ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 30s - loss: 14.5408 - mse: 14.5408 - mape: 81.4649 - 30s/epoch - 1ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 30s - loss: 14.5399 - mse: 14.5399 - mape: 81.4913 - 30s/epoch - 1ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 30s - loss: 14.5398 - mse: 14.5398 - mape: 81.4238 - 30s/epoch - 1ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 30s - loss: 14.5425 - mse: 14.5425 - mape: 81.4566 - 30s/epoch - 1ms/step\n",
            "Score for fold 3: loss of 19.630163192749023\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-09-27 12:28:23,885]\u001b[0m Finished trial#7 resulted in value: 14.54245376586914. Current best value is 14.000121116638184 with parameters: {'activation': 'relu', 'optimizer': 'sgd', 'num_hidden_layer': 2, 'num_hidden_unit': 48, 'learning_rate': 2.249312000174846e-05}.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 25s - loss: 14.2784 - mse: 14.2784 - mape: 79.8156 - 25s/epoch - 1ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 24s - loss: 13.9590 - mse: 13.9590 - mape: 78.8203 - 24s/epoch - 1ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 24s - loss: 13.8561 - mse: 13.8561 - mape: 78.4922 - 24s/epoch - 1ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 25s - loss: 13.7888 - mse: 13.7888 - mape: 78.2135 - 25s/epoch - 1ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 25s - loss: 13.7307 - mse: 13.7307 - mape: 77.9863 - 25s/epoch - 1ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 25s - loss: 13.6772 - mse: 13.6772 - mape: 77.8352 - 25s/epoch - 1ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 25s - loss: 13.6469 - mse: 13.6469 - mape: 77.7638 - 25s/epoch - 1ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 24s - loss: 13.6012 - mse: 13.6012 - mape: 77.6986 - 24s/epoch - 1ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 24s - loss: 13.6088 - mse: 13.6088 - mape: 77.6253 - 24s/epoch - 1ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 24s - loss: 13.5606 - mse: 13.5606 - mape: 77.5755 - 24s/epoch - 1ms/step\n",
            "Score for fold 1: loss of 27.737333297729492\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 24s - loss: 13.5402 - mse: 13.5402 - mape: 77.4858 - 24s/epoch - 1ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 25s - loss: 13.4810 - mse: 13.4810 - mape: 77.4418 - 25s/epoch - 1ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 25s - loss: 13.4761 - mse: 13.4761 - mape: 77.4733 - 25s/epoch - 1ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 25s - loss: 13.4740 - mse: 13.4740 - mape: 77.4633 - 25s/epoch - 1ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 24s - loss: 13.4356 - mse: 13.4356 - mape: 77.4258 - 24s/epoch - 1ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 25s - loss: 13.4168 - mse: 13.4168 - mape: 77.4415 - 25s/epoch - 1ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 24s - loss: 13.3814 - mse: 13.3814 - mape: 77.3376 - 24s/epoch - 1ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 25s - loss: 13.3516 - mse: 13.3516 - mape: 77.3636 - 25s/epoch - 1ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 25s - loss: 13.3608 - mse: 13.3608 - mape: 77.3910 - 25s/epoch - 1ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 24s - loss: 13.3184 - mse: 13.3184 - mape: 77.2887 - 24s/epoch - 1ms/step\n",
            "Score for fold 2: loss of 18.496328353881836\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 25s - loss: 13.3253 - mse: 13.3253 - mape: 77.3746 - 25s/epoch - 1ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 25s - loss: 13.3222 - mse: 13.3222 - mape: 77.2341 - 25s/epoch - 1ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 24s - loss: 13.2638 - mse: 13.2638 - mape: 77.2671 - 24s/epoch - 1ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 25s - loss: 13.2729 - mse: 13.2729 - mape: 77.3105 - 25s/epoch - 1ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 24s - loss: 13.2688 - mse: 13.2688 - mape: 77.2692 - 24s/epoch - 1ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 25s - loss: 13.1847 - mse: 13.1847 - mape: 77.1574 - 25s/epoch - 1ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 25s - loss: 13.2071 - mse: 13.2071 - mape: 77.2048 - 25s/epoch - 1ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 25s - loss: 13.1890 - mse: 13.1890 - mape: 77.2006 - 25s/epoch - 1ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 24s - loss: 13.1580 - mse: 13.1580 - mape: 77.2079 - 24s/epoch - 1ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 24s - loss: 13.2264 - mse: 13.2264 - mape: 77.3155 - 24s/epoch - 1ms/step\n",
            "Score for fold 3: loss of 18.689659118652344\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-09-27 12:40:51,316]\u001b[0m Finished trial#8 resulted in value: 13.226425170898438. Current best value is 13.226425170898438 with parameters: {'activation': 'tanh', 'optimizer': 'sgd', 'num_hidden_layer': 5, 'num_hidden_unit': 51, 'learning_rate': 0.0009012175506023924}.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 20s - loss: 17.0659 - mse: 17.0659 - mape: 89.9105 - 20s/epoch - 983us/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 20s - loss: 17.2762 - mse: 17.2762 - mape: 90.2451 - 20s/epoch - 995us/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 19s - loss: 17.3646 - mse: 17.3646 - mape: 89.8022 - 19s/epoch - 970us/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 19s - loss: 17.3634 - mse: 17.3634 - mape: 90.1495 - 19s/epoch - 971us/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 19s - loss: 17.2898 - mse: 17.2898 - mape: 89.7402 - 19s/epoch - 958us/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 20s - loss: 16.9678 - mse: 16.9678 - mape: 89.0131 - 20s/epoch - 985us/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 19s - loss: 17.5046 - mse: 17.5046 - mape: 90.2152 - 19s/epoch - 973us/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 19s - loss: 17.2989 - mse: 17.2989 - mape: 90.2743 - 19s/epoch - 935us/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 20s - loss: 18.6686 - mse: 18.6686 - mape: 91.9054 - 20s/epoch - 978us/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 20s - loss: 19.8830 - mse: 19.8830 - mape: 93.3211 - 20s/epoch - 992us/step\n",
            "Score for fold 1: loss of 27.267955780029297\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 19s - loss: 18.0370 - mse: 18.0370 - mape: 91.5992 - 19s/epoch - 974us/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 20s - loss: 17.4332 - mse: 17.4332 - mape: 89.9669 - 20s/epoch - 992us/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 19s - loss: 17.3006 - mse: 17.3006 - mape: 89.8140 - 19s/epoch - 950us/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 20s - loss: 16.9841 - mse: 16.9841 - mape: 89.5199 - 20s/epoch - 1ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 20s - loss: 17.2053 - mse: 17.2053 - mape: 89.6263 - 20s/epoch - 989us/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 19s - loss: 17.3660 - mse: 17.3660 - mape: 90.2252 - 19s/epoch - 966us/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 20s - loss: 17.0029 - mse: 17.0029 - mape: 89.5145 - 20s/epoch - 977us/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 20s - loss: 17.5224 - mse: 17.5224 - mape: 90.4023 - 20s/epoch - 999us/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 20s - loss: 16.9956 - mse: 16.9956 - mape: 89.8254 - 20s/epoch - 978us/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 20s - loss: 17.3826 - mse: 17.3826 - mape: 89.7139 - 20s/epoch - 1ms/step\n",
            "Score for fold 2: loss of 21.93839454650879\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 19s - loss: 17.8901 - mse: 17.8901 - mape: 90.5620 - 19s/epoch - 965us/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 20s - loss: 17.4606 - mse: 17.4606 - mape: 90.3274 - 20s/epoch - 1ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 20s - loss: 17.1442 - mse: 17.1442 - mape: 89.4022 - 20s/epoch - 994us/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 20s - loss: 16.9565 - mse: 16.9565 - mape: 89.3278 - 20s/epoch - 993us/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 20s - loss: 17.4268 - mse: 17.4268 - mape: 89.8697 - 20s/epoch - 988us/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 20s - loss: 17.3701 - mse: 17.3701 - mape: 90.1522 - 20s/epoch - 995us/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 20s - loss: 17.3282 - mse: 17.3282 - mape: 90.2965 - 20s/epoch - 975us/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 19s - loss: 17.5311 - mse: 17.5311 - mape: 90.0482 - 19s/epoch - 955us/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 19s - loss: 18.0789 - mse: 18.0789 - mape: 91.4451 - 19s/epoch - 965us/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 19s - loss: 17.3014 - mse: 17.3014 - mape: 89.9790 - 19s/epoch - 974us/step\n",
            "Score for fold 3: loss of 20.11872673034668\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-09-27 12:50:48,671]\u001b[0m Finished trial#9 resulted in value: 17.301382064819336. Current best value is 13.226425170898438 with parameters: {'activation': 'tanh', 'optimizer': 'sgd', 'num_hidden_layer': 5, 'num_hidden_unit': 51, 'learning_rate': 0.0009012175506023924}.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 24s - loss: 14.3202 - mse: 14.3202 - mape: 80.6536 - 24s/epoch - 1ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 24s - loss: 14.0887 - mse: 14.0887 - mape: 79.4872 - 24s/epoch - 1ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 24s - loss: 14.0316 - mse: 14.0316 - mape: 79.2534 - 24s/epoch - 1ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 24s - loss: 13.9319 - mse: 13.9319 - mape: 78.8967 - 24s/epoch - 1ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 24s - loss: 13.9900 - mse: 13.9900 - mape: 79.0797 - 24s/epoch - 1ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 24s - loss: 13.9336 - mse: 13.9336 - mape: 78.9668 - 24s/epoch - 1ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 24s - loss: 13.9287 - mse: 13.9287 - mape: 79.1357 - 24s/epoch - 1ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 24s - loss: 13.8935 - mse: 13.8935 - mape: 78.9543 - 24s/epoch - 1ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 24s - loss: 13.8928 - mse: 13.8928 - mape: 78.9647 - 24s/epoch - 1ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 24s - loss: 13.9229 - mse: 13.9229 - mape: 78.9260 - 24s/epoch - 1ms/step\n",
            "Score for fold 1: loss of 28.14208221435547\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 24s - loss: 13.8573 - mse: 13.8573 - mape: 78.8158 - 24s/epoch - 1ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 24s - loss: 13.8616 - mse: 13.8616 - mape: 78.8363 - 24s/epoch - 1ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 24s - loss: 13.9103 - mse: 13.9103 - mape: 79.0430 - 24s/epoch - 1ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 24s - loss: 13.9479 - mse: 13.9479 - mape: 79.0826 - 24s/epoch - 1ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 25s - loss: 13.9463 - mse: 13.9463 - mape: 79.0839 - 25s/epoch - 1ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 24s - loss: 13.8839 - mse: 13.8839 - mape: 78.9447 - 24s/epoch - 1ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 24s - loss: 13.8988 - mse: 13.8988 - mape: 79.1173 - 24s/epoch - 1ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 24s - loss: 13.8727 - mse: 13.8727 - mape: 79.0747 - 24s/epoch - 1ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 24s - loss: 13.8366 - mse: 13.8366 - mape: 78.8218 - 24s/epoch - 1ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 24s - loss: 13.8208 - mse: 13.8208 - mape: 78.9219 - 24s/epoch - 1ms/step\n",
            "Score for fold 2: loss of 18.7332706451416\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 24s - loss: 13.9082 - mse: 13.9082 - mape: 79.1906 - 24s/epoch - 1ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 25s - loss: 13.9561 - mse: 13.9561 - mape: 79.3840 - 25s/epoch - 1ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 24s - loss: 13.8957 - mse: 13.8957 - mape: 79.0558 - 24s/epoch - 1ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 24s - loss: 13.8734 - mse: 13.8734 - mape: 79.2344 - 24s/epoch - 1ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 24s - loss: 13.8953 - mse: 13.8953 - mape: 79.2705 - 24s/epoch - 1ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 24s - loss: 13.8689 - mse: 13.8689 - mape: 79.2432 - 24s/epoch - 1ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 24s - loss: 13.9663 - mse: 13.9663 - mape: 79.3238 - 24s/epoch - 1ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 24s - loss: 13.9186 - mse: 13.9186 - mape: 79.1686 - 24s/epoch - 1ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 24s - loss: 13.9041 - mse: 13.9041 - mape: 79.0854 - 24s/epoch - 1ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 24s - loss: 13.9303 - mse: 13.9303 - mape: 79.2521 - 24s/epoch - 1ms/step\n",
            "Score for fold 3: loss of 18.50423812866211\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-09-27 13:03:00,361]\u001b[0m Finished trial#10 resulted in value: 13.930298805236816. Current best value is 13.226425170898438 with parameters: {'activation': 'tanh', 'optimizer': 'sgd', 'num_hidden_layer': 5, 'num_hidden_unit': 51, 'learning_rate': 0.0009012175506023924}.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 24s - loss: 14.3119 - mse: 14.3119 - mape: 80.5735 - 24s/epoch - 1ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 24s - loss: 14.1002 - mse: 14.1002 - mape: 79.5884 - 24s/epoch - 1ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 24s - loss: 14.0347 - mse: 14.0347 - mape: 79.3762 - 24s/epoch - 1ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 25s - loss: 13.9942 - mse: 13.9942 - mape: 79.2709 - 25s/epoch - 1ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 27s - loss: 13.9509 - mse: 13.9509 - mape: 79.1096 - 27s/epoch - 1ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 25s - loss: 13.9450 - mse: 13.9450 - mape: 79.1629 - 25s/epoch - 1ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 26s - loss: 13.9612 - mse: 13.9612 - mape: 79.2024 - 26s/epoch - 1ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 26s - loss: 13.9312 - mse: 13.9312 - mape: 79.1573 - 26s/epoch - 1ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 25s - loss: 13.8960 - mse: 13.8960 - mape: 78.9884 - 25s/epoch - 1ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 24s - loss: 13.9094 - mse: 13.9094 - mape: 78.9820 - 24s/epoch - 1ms/step\n",
            "Score for fold 1: loss of 28.7164306640625\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 25s - loss: 13.9100 - mse: 13.9100 - mape: 79.1632 - 25s/epoch - 1ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 25s - loss: 13.9024 - mse: 13.9024 - mape: 79.1871 - 25s/epoch - 1ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 25s - loss: 13.8151 - mse: 13.8151 - mape: 78.8713 - 25s/epoch - 1ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 26s - loss: 13.8285 - mse: 13.8285 - mape: 78.8934 - 26s/epoch - 1ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 26s - loss: 13.7781 - mse: 13.7781 - mape: 78.7455 - 26s/epoch - 1ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 27s - loss: 13.8676 - mse: 13.8676 - mape: 79.0110 - 27s/epoch - 1ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 27s - loss: 13.9121 - mse: 13.9121 - mape: 79.1968 - 27s/epoch - 1ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 27s - loss: 13.8322 - mse: 13.8322 - mape: 79.0095 - 27s/epoch - 1ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 27s - loss: 13.8069 - mse: 13.8069 - mape: 78.8707 - 27s/epoch - 1ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 27s - loss: 13.8359 - mse: 13.8359 - mape: 78.8817 - 27s/epoch - 1ms/step\n",
            "Score for fold 2: loss of 18.91634750366211\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 26s - loss: 13.8265 - mse: 13.8265 - mape: 78.9920 - 26s/epoch - 1ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 26s - loss: 13.8485 - mse: 13.8485 - mape: 79.0132 - 26s/epoch - 1ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 26s - loss: 13.8126 - mse: 13.8126 - mape: 78.8590 - 26s/epoch - 1ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 26s - loss: 13.8576 - mse: 13.8576 - mape: 79.0359 - 26s/epoch - 1ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 27s - loss: 13.8591 - mse: 13.8591 - mape: 79.0183 - 27s/epoch - 1ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 27s - loss: 13.8579 - mse: 13.8579 - mape: 79.0243 - 27s/epoch - 1ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 27s - loss: 13.8816 - mse: 13.8816 - mape: 79.0373 - 27s/epoch - 1ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 26s - loss: 13.8363 - mse: 13.8363 - mape: 79.0973 - 26s/epoch - 1ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 26s - loss: 13.8151 - mse: 13.8151 - mape: 78.9814 - 26s/epoch - 1ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 25s - loss: 13.8414 - mse: 13.8414 - mape: 79.1491 - 25s/epoch - 1ms/step\n",
            "Score for fold 3: loss of 18.62685203552246\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-09-27 13:16:03,181]\u001b[0m Finished trial#11 resulted in value: 13.841442108154297. Current best value is 13.226425170898438 with parameters: {'activation': 'tanh', 'optimizer': 'sgd', 'num_hidden_layer': 5, 'num_hidden_unit': 51, 'learning_rate': 0.0009012175506023924}.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 27s - loss: 14.3506 - mse: 14.3506 - mape: 80.4555 - 27s/epoch - 1ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 27s - loss: 14.1391 - mse: 14.1391 - mape: 79.7675 - 27s/epoch - 1ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 27s - loss: 14.1162 - mse: 14.1162 - mape: 79.7523 - 27s/epoch - 1ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 27s - loss: 14.0704 - mse: 14.0704 - mape: 79.5288 - 27s/epoch - 1ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 27s - loss: 14.0258 - mse: 14.0258 - mape: 79.2837 - 27s/epoch - 1ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 27s - loss: 13.9973 - mse: 13.9973 - mape: 79.1538 - 27s/epoch - 1ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 26s - loss: 14.0159 - mse: 14.0159 - mape: 79.2682 - 26s/epoch - 1ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 26s - loss: 13.9641 - mse: 13.9641 - mape: 78.9887 - 26s/epoch - 1ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 26s - loss: 13.9931 - mse: 13.9931 - mape: 79.2491 - 26s/epoch - 1ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 26s - loss: 13.9760 - mse: 13.9760 - mape: 79.0822 - 26s/epoch - 1ms/step\n",
            "Score for fold 1: loss of 28.01389503479004\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 27s - loss: 14.0855 - mse: 14.0855 - mape: 79.5073 - 27s/epoch - 1ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 27s - loss: 13.9806 - mse: 13.9806 - mape: 79.1671 - 27s/epoch - 1ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 26s - loss: 13.9486 - mse: 13.9486 - mape: 78.9601 - 26s/epoch - 1ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 26s - loss: 13.9425 - mse: 13.9425 - mape: 78.6267 - 26s/epoch - 1ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 26s - loss: 13.9622 - mse: 13.9622 - mape: 78.9676 - 26s/epoch - 1ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 25s - loss: 13.9652 - mse: 13.9652 - mape: 79.0593 - 25s/epoch - 1ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 25s - loss: 13.9868 - mse: 13.9868 - mape: 78.7469 - 25s/epoch - 1ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 24s - loss: 13.9741 - mse: 13.9741 - mape: 78.9445 - 24s/epoch - 1ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 25s - loss: 13.9595 - mse: 13.9595 - mape: 78.9053 - 25s/epoch - 1ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 24s - loss: 14.0066 - mse: 14.0066 - mape: 79.2516 - 24s/epoch - 1ms/step\n",
            "Score for fold 2: loss of 18.8552188873291\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 25s - loss: 14.0096 - mse: 14.0096 - mape: 79.1624 - 25s/epoch - 1ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 25s - loss: 14.1285 - mse: 14.1285 - mape: 79.7943 - 25s/epoch - 1ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 24s - loss: 14.1466 - mse: 14.1466 - mape: 79.6187 - 24s/epoch - 1ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 25s - loss: 14.0567 - mse: 14.0567 - mape: 79.5330 - 25s/epoch - 1ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 24s - loss: 14.0578 - mse: 14.0578 - mape: 79.7659 - 24s/epoch - 1ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 24s - loss: 14.1110 - mse: 14.1110 - mape: 79.7841 - 24s/epoch - 1ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 25s - loss: 14.0356 - mse: 14.0356 - mape: 79.6543 - 25s/epoch - 1ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 24s - loss: 14.0703 - mse: 14.0703 - mape: 79.6215 - 24s/epoch - 1ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 24s - loss: 13.9911 - mse: 13.9911 - mape: 79.4465 - 24s/epoch - 1ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 24s - loss: 14.1345 - mse: 14.1345 - mape: 79.8229 - 24s/epoch - 1ms/step\n",
            "Score for fold 3: loss of 18.95660400390625\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-09-27 13:29:01,337]\u001b[0m Finished trial#12 resulted in value: 14.134482383728027. Current best value is 13.226425170898438 with parameters: {'activation': 'tanh', 'optimizer': 'sgd', 'num_hidden_layer': 5, 'num_hidden_unit': 51, 'learning_rate': 0.0009012175506023924}.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 25s - loss: 14.2494 - mse: 14.2494 - mape: 79.6249 - 25s/epoch - 1ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 25s - loss: 13.9315 - mse: 13.9315 - mape: 78.6285 - 25s/epoch - 1ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 24s - loss: 13.8342 - mse: 13.8342 - mape: 78.3470 - 24s/epoch - 1ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 25s - loss: 13.7708 - mse: 13.7708 - mape: 78.1996 - 25s/epoch - 1ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 25s - loss: 13.7219 - mse: 13.7219 - mape: 78.0377 - 25s/epoch - 1ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 25s - loss: 13.6750 - mse: 13.6750 - mape: 77.9777 - 25s/epoch - 1ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 25s - loss: 13.6290 - mse: 13.6290 - mape: 77.8886 - 25s/epoch - 1ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 26s - loss: 13.6522 - mse: 13.6522 - mape: 77.9826 - 26s/epoch - 1ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 25s - loss: 13.5968 - mse: 13.5968 - mape: 77.8326 - 25s/epoch - 1ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 25s - loss: 13.5350 - mse: 13.5350 - mape: 77.6613 - 25s/epoch - 1ms/step\n",
            "Score for fold 1: loss of 28.57204246520996\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 25s - loss: 13.5557 - mse: 13.5557 - mape: 77.7059 - 25s/epoch - 1ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 26s - loss: 13.5219 - mse: 13.5219 - mape: 77.6194 - 26s/epoch - 1ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 26s - loss: 13.5406 - mse: 13.5406 - mape: 77.6885 - 26s/epoch - 1ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 25s - loss: 13.4997 - mse: 13.4997 - mape: 77.6151 - 25s/epoch - 1ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 25s - loss: 13.4594 - mse: 13.4594 - mape: 77.5973 - 25s/epoch - 1ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 25s - loss: 13.4412 - mse: 13.4412 - mape: 77.6465 - 25s/epoch - 1ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 26s - loss: 13.4602 - mse: 13.4602 - mape: 77.5688 - 26s/epoch - 1ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 25s - loss: 13.3975 - mse: 13.3975 - mape: 77.6169 - 25s/epoch - 1ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 25s - loss: 13.3739 - mse: 13.3739 - mape: 77.5891 - 25s/epoch - 1ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 25s - loss: 13.3661 - mse: 13.3661 - mape: 77.4917 - 25s/epoch - 1ms/step\n",
            "Score for fold 2: loss of 18.590232849121094\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 25s - loss: 13.3152 - mse: 13.3152 - mape: 77.5573 - 25s/epoch - 1ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 25s - loss: 13.3031 - mse: 13.3031 - mape: 77.4809 - 25s/epoch - 1ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 24s - loss: 13.2988 - mse: 13.2988 - mape: 77.5134 - 24s/epoch - 1ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 25s - loss: 13.3217 - mse: 13.3217 - mape: 77.5360 - 25s/epoch - 1ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 25s - loss: 13.2981 - mse: 13.2981 - mape: 77.4313 - 25s/epoch - 1ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 25s - loss: 13.2424 - mse: 13.2424 - mape: 77.3853 - 25s/epoch - 1ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 25s - loss: 13.2527 - mse: 13.2527 - mape: 77.4490 - 25s/epoch - 1ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 25s - loss: 13.2081 - mse: 13.2081 - mape: 77.4428 - 25s/epoch - 1ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 25s - loss: 13.1950 - mse: 13.1950 - mape: 77.4188 - 25s/epoch - 1ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 25s - loss: 13.1791 - mse: 13.1791 - mape: 77.4696 - 25s/epoch - 1ms/step\n",
            "Score for fold 3: loss of 18.249189376831055\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-09-27 13:41:43,104]\u001b[0m Finished trial#13 resulted in value: 13.179082870483398. Current best value is 13.179082870483398 with parameters: {'activation': 'tanh', 'optimizer': 'sgd', 'num_hidden_layer': 5, 'num_hidden_unit': 42, 'learning_rate': 0.0011985079039226632}.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 24s - loss: 14.8764 - mse: 14.8764 - mape: 80.1142 - 24s/epoch - 1ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 23s - loss: 14.2260 - mse: 14.2260 - mape: 80.1602 - 23s/epoch - 1ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 24s - loss: 14.0271 - mse: 14.0271 - mape: 79.1906 - 24s/epoch - 1ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 23s - loss: 13.9495 - mse: 13.9495 - mape: 78.7529 - 23s/epoch - 1ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 24s - loss: 13.9019 - mse: 13.9019 - mape: 78.4786 - 24s/epoch - 1ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 24s - loss: 13.8659 - mse: 13.8659 - mape: 78.3000 - 24s/epoch - 1ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 23s - loss: 13.8419 - mse: 13.8419 - mape: 78.1845 - 23s/epoch - 1ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 24s - loss: 13.8196 - mse: 13.8196 - mape: 78.0164 - 24s/epoch - 1ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 24s - loss: 13.7842 - mse: 13.7842 - mape: 77.9306 - 24s/epoch - 1ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 24s - loss: 13.7602 - mse: 13.7602 - mape: 77.8421 - 24s/epoch - 1ms/step\n",
            "Score for fold 1: loss of 28.432880401611328\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 24s - loss: 13.7379 - mse: 13.7379 - mape: 77.7525 - 24s/epoch - 1ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 24s - loss: 13.7169 - mse: 13.7169 - mape: 77.6656 - 24s/epoch - 1ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 24s - loss: 13.7056 - mse: 13.7056 - mape: 77.6459 - 24s/epoch - 1ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 25s - loss: 13.6800 - mse: 13.6800 - mape: 77.6115 - 25s/epoch - 1ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 25s - loss: 13.6663 - mse: 13.6663 - mape: 77.5509 - 25s/epoch - 1ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 25s - loss: 13.6527 - mse: 13.6527 - mape: 77.5224 - 25s/epoch - 1ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 25s - loss: 13.6470 - mse: 13.6470 - mape: 77.4864 - 25s/epoch - 1ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 25s - loss: 13.6272 - mse: 13.6272 - mape: 77.4211 - 25s/epoch - 1ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 24s - loss: 13.6154 - mse: 13.6154 - mape: 77.4651 - 24s/epoch - 1ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 25s - loss: 13.6048 - mse: 13.6048 - mape: 77.3836 - 25s/epoch - 1ms/step\n",
            "Score for fold 2: loss of 18.705171585083008\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 24s - loss: 13.5918 - mse: 13.5918 - mape: 77.3806 - 24s/epoch - 1ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 25s - loss: 13.5797 - mse: 13.5797 - mape: 77.3739 - 25s/epoch - 1ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 26s - loss: 13.5647 - mse: 13.5647 - mape: 77.3362 - 26s/epoch - 1ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 26s - loss: 13.5515 - mse: 13.5515 - mape: 77.3360 - 26s/epoch - 1ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 27s - loss: 13.5396 - mse: 13.5396 - mape: 77.3040 - 27s/epoch - 1ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 26s - loss: 13.5407 - mse: 13.5407 - mape: 77.2488 - 26s/epoch - 1ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 26s - loss: 13.5126 - mse: 13.5126 - mape: 77.2371 - 26s/epoch - 1ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 26s - loss: 13.5043 - mse: 13.5043 - mape: 77.2574 - 26s/epoch - 1ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 26s - loss: 13.4913 - mse: 13.4913 - mape: 77.2073 - 26s/epoch - 1ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 26s - loss: 13.4845 - mse: 13.4845 - mape: 77.1911 - 26s/epoch - 1ms/step\n",
            "Score for fold 3: loss of 18.512065887451172\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-09-27 13:54:11,977]\u001b[0m Finished trial#14 resulted in value: 13.484539985656738. Current best value is 13.179082870483398 with parameters: {'activation': 'tanh', 'optimizer': 'sgd', 'num_hidden_layer': 5, 'num_hidden_unit': 42, 'learning_rate': 0.0011985079039226632}.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 29s - loss: 14.9334 - mse: 14.9334 - mape: 80.6279 - 29s/epoch - 1ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 28s - loss: 14.5826 - mse: 14.5826 - mape: 80.0664 - 28s/epoch - 1ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 28s - loss: 14.4974 - mse: 14.4974 - mape: 80.0835 - 28s/epoch - 1ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 27s - loss: 14.4454 - mse: 14.4454 - mape: 80.2299 - 27s/epoch - 1ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 28s - loss: 14.4074 - mse: 14.4074 - mape: 80.1637 - 28s/epoch - 1ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 28s - loss: 14.3767 - mse: 14.3767 - mape: 80.0224 - 28s/epoch - 1ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 29s - loss: 14.3514 - mse: 14.3514 - mape: 79.8645 - 29s/epoch - 1ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 30s - loss: 14.3293 - mse: 14.3293 - mape: 79.9846 - 30s/epoch - 1ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 27s - loss: 14.3103 - mse: 14.3103 - mape: 79.8459 - 27s/epoch - 1ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 27s - loss: 14.2933 - mse: 14.2933 - mape: 79.7937 - 27s/epoch - 1ms/step\n",
            "Score for fold 1: loss of 28.537946701049805\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 28s - loss: 14.2781 - mse: 14.2781 - mape: 79.7482 - 28s/epoch - 1ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 28s - loss: 14.2634 - mse: 14.2634 - mape: 79.5542 - 28s/epoch - 1ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 28s - loss: 14.2508 - mse: 14.2508 - mape: 79.7499 - 28s/epoch - 1ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 28s - loss: 14.2393 - mse: 14.2393 - mape: 79.4033 - 28s/epoch - 1ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 28s - loss: 14.2284 - mse: 14.2284 - mape: 79.6387 - 28s/epoch - 1ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 28s - loss: 14.2184 - mse: 14.2184 - mape: 79.4539 - 28s/epoch - 1ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 28s - loss: 14.2088 - mse: 14.2088 - mape: 79.4513 - 28s/epoch - 1ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 27s - loss: 14.2000 - mse: 14.2000 - mape: 79.3338 - 27s/epoch - 1ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 27s - loss: 14.1915 - mse: 14.1915 - mape: 79.4033 - 27s/epoch - 1ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 27s - loss: 14.1837 - mse: 14.1837 - mape: 79.1739 - 27s/epoch - 1ms/step\n",
            "Score for fold 2: loss of 19.35370445251465\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 27s - loss: 14.1765 - mse: 14.1765 - mape: 79.3331 - 27s/epoch - 1ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 27s - loss: 14.1691 - mse: 14.1691 - mape: 79.2011 - 27s/epoch - 1ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 26s - loss: 14.1625 - mse: 14.1625 - mape: 79.2464 - 26s/epoch - 1ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 27s - loss: 14.1562 - mse: 14.1562 - mape: 79.2373 - 27s/epoch - 1ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 26s - loss: 14.1501 - mse: 14.1501 - mape: 79.0246 - 26s/epoch - 1ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 27s - loss: 14.1440 - mse: 14.1440 - mape: 79.1408 - 27s/epoch - 1ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 27s - loss: 14.1388 - mse: 14.1388 - mape: 79.0704 - 27s/epoch - 1ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 27s - loss: 14.1331 - mse: 14.1331 - mape: 79.1889 - 27s/epoch - 1ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 27s - loss: 14.1281 - mse: 14.1281 - mape: 78.9714 - 27s/epoch - 1ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 27s - loss: 14.1233 - mse: 14.1233 - mape: 78.9945 - 27s/epoch - 1ms/step\n",
            "Score for fold 3: loss of 19.2391357421875\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-09-27 14:08:05,868]\u001b[0m Finished trial#15 resulted in value: 14.12325668334961. Current best value is 13.179082870483398 with parameters: {'activation': 'tanh', 'optimizer': 'sgd', 'num_hidden_layer': 5, 'num_hidden_unit': 42, 'learning_rate': 0.0011985079039226632}.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 24s - loss: 14.4263 - mse: 14.4263 - mape: 79.7989 - 24s/epoch - 1ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 25s - loss: 14.0056 - mse: 14.0056 - mape: 78.9339 - 25s/epoch - 1ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 24s - loss: 13.9123 - mse: 13.9123 - mape: 78.6129 - 24s/epoch - 1ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 24s - loss: 13.8302 - mse: 13.8302 - mape: 78.3390 - 24s/epoch - 1ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 24s - loss: 13.7891 - mse: 13.7891 - mape: 78.1244 - 24s/epoch - 1ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 24s - loss: 13.7394 - mse: 13.7394 - mape: 78.0852 - 24s/epoch - 1ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 24s - loss: 13.7383 - mse: 13.7383 - mape: 78.1174 - 24s/epoch - 1ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 24s - loss: 13.7563 - mse: 13.7563 - mape: 78.1956 - 24s/epoch - 1ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 24s - loss: 13.7137 - mse: 13.7137 - mape: 77.9925 - 24s/epoch - 1ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 24s - loss: 13.6709 - mse: 13.6709 - mape: 78.0067 - 24s/epoch - 1ms/step\n",
            "Score for fold 1: loss of 28.47732925415039\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 25s - loss: 13.6640 - mse: 13.6640 - mape: 77.9962 - 25s/epoch - 1ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 24s - loss: 13.6977 - mse: 13.6977 - mape: 78.0111 - 24s/epoch - 1ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 24s - loss: 13.6736 - mse: 13.6736 - mape: 77.8974 - 24s/epoch - 1ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 24s - loss: 13.6515 - mse: 13.6515 - mape: 77.9697 - 24s/epoch - 1ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 24s - loss: 13.6008 - mse: 13.6008 - mape: 77.8679 - 24s/epoch - 1ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 24s - loss: 13.6104 - mse: 13.6104 - mape: 77.8576 - 24s/epoch - 1ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 24s - loss: 13.5998 - mse: 13.5998 - mape: 77.8534 - 24s/epoch - 1ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 24s - loss: 13.5450 - mse: 13.5450 - mape: 77.7706 - 24s/epoch - 1ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 24s - loss: 13.6438 - mse: 13.6438 - mape: 77.8215 - 24s/epoch - 1ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 24s - loss: 13.6386 - mse: 13.6386 - mape: 77.8220 - 24s/epoch - 1ms/step\n",
            "Score for fold 2: loss of 18.57283592224121\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 24s - loss: 13.6586 - mse: 13.6586 - mape: 77.9103 - 24s/epoch - 1ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 24s - loss: 13.6237 - mse: 13.6237 - mape: 77.8582 - 24s/epoch - 1ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 24s - loss: 13.5771 - mse: 13.5771 - mape: 77.7789 - 24s/epoch - 1ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 25s - loss: 13.5692 - mse: 13.5692 - mape: 77.7835 - 25s/epoch - 1ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 24s - loss: 13.6101 - mse: 13.6101 - mape: 77.8734 - 24s/epoch - 1ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 25s - loss: 13.5542 - mse: 13.5542 - mape: 77.7145 - 25s/epoch - 1ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 25s - loss: 13.5901 - mse: 13.5901 - mape: 77.9325 - 25s/epoch - 1ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 25s - loss: 13.5668 - mse: 13.5668 - mape: 77.9489 - 25s/epoch - 1ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 25s - loss: 13.5692 - mse: 13.5692 - mape: 77.8413 - 25s/epoch - 1ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 25s - loss: 13.5947 - mse: 13.5947 - mape: 77.8835 - 25s/epoch - 1ms/step\n",
            "Score for fold 3: loss of 18.418428421020508\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-09-27 14:20:23,484]\u001b[0m Finished trial#16 resulted in value: 13.594736099243164. Current best value is 13.179082870483398 with parameters: {'activation': 'tanh', 'optimizer': 'sgd', 'num_hidden_layer': 5, 'num_hidden_unit': 42, 'learning_rate': 0.0011985079039226632}.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 27s - loss: 15.1862 - mse: 15.1862 - mape: 86.9243 - 27s/epoch - 1ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 27s - loss: 15.3223 - mse: 15.3223 - mape: 88.5156 - 27s/epoch - 1ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 27s - loss: 16.8342 - mse: 16.8342 - mape: 94.9302 - 27s/epoch - 1ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 27s - loss: 16.3508 - mse: 16.3508 - mape: 96.1135 - 27s/epoch - 1ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 26s - loss: 16.5025 - mse: 16.5025 - mape: 96.7998 - 26s/epoch - 1ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 27s - loss: 20.4725 - mse: 20.4725 - mape: 99.1414 - 27s/epoch - 1ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 28s - loss: 16.5025 - mse: 16.5025 - mape: 97.0642 - 28s/epoch - 1ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 28s - loss: 22.5320 - mse: 22.5320 - mape: 99.4622 - 28s/epoch - 1ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 28s - loss: 23.6738 - mse: 23.6738 - mape: 100.0000 - 28s/epoch - 1ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 28s - loss: 23.6738 - mse: 23.6738 - mape: 100.0000 - 28s/epoch - 1ms/step\n",
            "Score for fold 1: loss of 28.399234771728516\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 27s - loss: 23.6738 - mse: 23.6738 - mape: 100.0000 - 27s/epoch - 1ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 26s - loss: 23.6738 - mse: 23.6738 - mape: 100.0000 - 26s/epoch - 1ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 26s - loss: 23.6738 - mse: 23.6738 - mape: 100.0000 - 26s/epoch - 1ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 26s - loss: 23.6738 - mse: 23.6738 - mape: 100.0000 - 26s/epoch - 1ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 27s - loss: 23.6738 - mse: 23.6738 - mape: 100.0000 - 27s/epoch - 1ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 28s - loss: 23.6738 - mse: 23.6738 - mape: 100.0000 - 28s/epoch - 1ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 28s - loss: 23.6738 - mse: 23.6738 - mape: 100.0000 - 28s/epoch - 1ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 28s - loss: 23.6738 - mse: 23.6738 - mape: 100.0000 - 28s/epoch - 1ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 26s - loss: 23.6738 - mse: 23.6738 - mape: 100.0000 - 26s/epoch - 1ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 27s - loss: 23.6738 - mse: 23.6738 - mape: 100.0000 - 27s/epoch - 1ms/step\n",
            "Score for fold 2: loss of 29.109249114990234\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 28s - loss: 23.6738 - mse: 23.6738 - mape: 100.0000 - 28s/epoch - 1ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 28s - loss: 23.6738 - mse: 23.6738 - mape: 100.0000 - 28s/epoch - 1ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 28s - loss: 23.6738 - mse: 23.6738 - mape: 100.0000 - 28s/epoch - 1ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 28s - loss: 23.6738 - mse: 23.6738 - mape: 100.0000 - 28s/epoch - 1ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 28s - loss: 23.6738 - mse: 23.6738 - mape: 100.0000 - 28s/epoch - 1ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 27s - loss: 23.6738 - mse: 23.6738 - mape: 100.0000 - 27s/epoch - 1ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 27s - loss: 23.6738 - mse: 23.6738 - mape: 100.0000 - 27s/epoch - 1ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 27s - loss: 23.6738 - mse: 23.6738 - mape: 100.0000 - 27s/epoch - 1ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 27s - loss: 23.6738 - mse: 23.6738 - mape: 100.0000 - 27s/epoch - 1ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 26s - loss: 23.6738 - mse: 23.6738 - mape: 100.0000 - 26s/epoch - 1ms/step\n",
            "Score for fold 3: loss of 29.109249114990234\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-09-27 14:34:11,433]\u001b[0m Finished trial#17 resulted in value: 23.673847198486328. Current best value is 13.179082870483398 with parameters: {'activation': 'tanh', 'optimizer': 'sgd', 'num_hidden_layer': 5, 'num_hidden_unit': 42, 'learning_rate': 0.0011985079039226632}.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 25s - loss: 14.6821 - mse: 14.6821 - mape: 80.2386 - 25s/epoch - 1ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 24s - loss: 14.0758 - mse: 14.0758 - mape: 79.1639 - 24s/epoch - 1ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 26s - loss: 13.9494 - mse: 13.9494 - mape: 78.4798 - 26s/epoch - 1ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 26s - loss: 13.8994 - mse: 13.8994 - mape: 78.3106 - 26s/epoch - 1ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 26s - loss: 13.8603 - mse: 13.8603 - mape: 78.1446 - 26s/epoch - 1ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 26s - loss: 13.8235 - mse: 13.8235 - mape: 78.0807 - 26s/epoch - 1ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 26s - loss: 13.7901 - mse: 13.7901 - mape: 77.9361 - 26s/epoch - 1ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 25s - loss: 13.7455 - mse: 13.7455 - mape: 77.8032 - 25s/epoch - 1ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 26s - loss: 13.7190 - mse: 13.7190 - mape: 77.6532 - 26s/epoch - 1ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 26s - loss: 13.6886 - mse: 13.6886 - mape: 77.6022 - 26s/epoch - 1ms/step\n",
            "Score for fold 1: loss of 28.408918380737305\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 26s - loss: 13.6609 - mse: 13.6609 - mape: 77.4888 - 26s/epoch - 1ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 25s - loss: 13.6212 - mse: 13.6212 - mape: 77.3906 - 25s/epoch - 1ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 25s - loss: 13.5941 - mse: 13.5941 - mape: 77.3780 - 25s/epoch - 1ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 25s - loss: 13.5765 - mse: 13.5765 - mape: 77.3595 - 25s/epoch - 1ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 24s - loss: 13.5494 - mse: 13.5494 - mape: 77.2979 - 24s/epoch - 1ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 25s - loss: 13.5398 - mse: 13.5398 - mape: 77.2911 - 25s/epoch - 1ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 25s - loss: 13.4996 - mse: 13.4996 - mape: 77.1954 - 25s/epoch - 1ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 25s - loss: 13.4968 - mse: 13.4968 - mape: 77.2005 - 25s/epoch - 1ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 25s - loss: 13.4848 - mse: 13.4848 - mape: 77.2003 - 25s/epoch - 1ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 24s - loss: 13.4678 - mse: 13.4678 - mape: 77.1085 - 24s/epoch - 1ms/step\n",
            "Score for fold 2: loss of 18.558704376220703\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 24s - loss: 13.4550 - mse: 13.4550 - mape: 77.1216 - 24s/epoch - 1ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 24s - loss: 13.4299 - mse: 13.4299 - mape: 77.0474 - 24s/epoch - 1ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 24s - loss: 13.4225 - mse: 13.4225 - mape: 77.0791 - 24s/epoch - 1ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 24s - loss: 13.4074 - mse: 13.4074 - mape: 77.0454 - 24s/epoch - 1ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 25s - loss: 13.3902 - mse: 13.3902 - mape: 76.9638 - 25s/epoch - 1ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 25s - loss: 13.4038 - mse: 13.4038 - mape: 77.0329 - 25s/epoch - 1ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 25s - loss: 13.3740 - mse: 13.3740 - mape: 76.9717 - 25s/epoch - 1ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 25s - loss: 13.3677 - mse: 13.3677 - mape: 76.9629 - 25s/epoch - 1ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 26s - loss: 13.3437 - mse: 13.3437 - mape: 76.9214 - 26s/epoch - 1ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 26s - loss: 13.3392 - mse: 13.3392 - mape: 76.9334 - 26s/epoch - 1ms/step\n",
            "Score for fold 3: loss of 18.312509536743164\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-09-27 14:46:57,030]\u001b[0m Finished trial#18 resulted in value: 13.33922004699707. Current best value is 13.179082870483398 with parameters: {'activation': 'tanh', 'optimizer': 'sgd', 'num_hidden_layer': 5, 'num_hidden_unit': 42, 'learning_rate': 0.0011985079039226632}.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 28s - loss: 21.0086 - mse: 21.0086 - mape: 72.1749 - 28s/epoch - 1ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 28s - loss: 19.7211 - mse: 19.7211 - mape: 67.1028 - 28s/epoch - 1ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 27s - loss: 19.1546 - mse: 19.1546 - mape: 70.3857 - 27s/epoch - 1ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 25s - loss: 18.8032 - mse: 18.8032 - mape: 73.5096 - 25s/epoch - 1ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 26s - loss: 18.5430 - mse: 18.5430 - mape: 75.7928 - 26s/epoch - 1ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 27s - loss: 18.3260 - mse: 18.3260 - mape: 77.2955 - 27s/epoch - 1ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 27s - loss: 18.1321 - mse: 18.1321 - mape: 78.2317 - 27s/epoch - 1ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 26s - loss: 17.9497 - mse: 17.9497 - mape: 78.6388 - 26s/epoch - 1ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 26s - loss: 17.7705 - mse: 17.7705 - mape: 78.7532 - 26s/epoch - 1ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 26s - loss: 17.5900 - mse: 17.5900 - mape: 78.6166 - 26s/epoch - 1ms/step\n",
            "Score for fold 1: loss of 28.279619216918945\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 26s - loss: 17.4005 - mse: 17.4005 - mape: 78.2325 - 26s/epoch - 1ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 25s - loss: 17.1977 - mse: 17.1977 - mape: 77.6811 - 25s/epoch - 1ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 26s - loss: 16.9789 - mse: 16.9789 - mape: 76.9313 - 26s/epoch - 1ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 26s - loss: 16.7471 - mse: 16.7471 - mape: 76.1192 - 26s/epoch - 1ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 25s - loss: 16.5209 - mse: 16.5209 - mape: 75.4297 - 25s/epoch - 1ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 25s - loss: 16.3211 - mse: 16.3211 - mape: 75.1097 - 25s/epoch - 1ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 26s - loss: 16.1592 - mse: 16.1592 - mape: 75.2067 - 26s/epoch - 1ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 26s - loss: 16.0353 - mse: 16.0353 - mape: 75.6892 - 26s/epoch - 1ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 25s - loss: 15.9435 - mse: 15.9435 - mape: 76.3893 - 25s/epoch - 1ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 26s - loss: 15.8754 - mse: 15.8754 - mape: 77.2077 - 26s/epoch - 1ms/step\n",
            "Score for fold 2: loss of 22.690631866455078\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 25s - loss: 15.8245 - mse: 15.8245 - mape: 78.0871 - 25s/epoch - 1ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 25s - loss: 15.7857 - mse: 15.7857 - mape: 78.8409 - 25s/epoch - 1ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 26s - loss: 15.7552 - mse: 15.7552 - mape: 79.6171 - 26s/epoch - 1ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 25s - loss: 15.7307 - mse: 15.7307 - mape: 80.1977 - 25s/epoch - 1ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 26s - loss: 15.7108 - mse: 15.7108 - mape: 80.8212 - 26s/epoch - 1ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 26s - loss: 15.6939 - mse: 15.6939 - mape: 81.3144 - 26s/epoch - 1ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 26s - loss: 15.6796 - mse: 15.6796 - mape: 81.7511 - 26s/epoch - 1ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 26s - loss: 15.6673 - mse: 15.6673 - mape: 82.1283 - 26s/epoch - 1ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 26s - loss: 15.6562 - mse: 15.6562 - mape: 82.4197 - 26s/epoch - 1ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 26s - loss: 15.6464 - mse: 15.6464 - mape: 82.6919 - 26s/epoch - 1ms/step\n",
            "Score for fold 3: loss of 21.017911911010742\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-09-27 15:00:06,091]\u001b[0m Finished trial#19 resulted in value: 15.64642333984375. Current best value is 13.179082870483398 with parameters: {'activation': 'tanh', 'optimizer': 'sgd', 'num_hidden_layer': 5, 'num_hidden_unit': 42, 'learning_rate': 0.0011985079039226632}.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 24s - loss: 14.6697 - mse: 14.6697 - mape: 81.4433 - 24s/epoch - 1ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 24s - loss: 14.5951 - mse: 14.5951 - mape: 81.4542 - 24s/epoch - 1ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 24s - loss: 14.5907 - mse: 14.5907 - mape: 81.5305 - 24s/epoch - 1ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 24s - loss: 14.5727 - mse: 14.5727 - mape: 81.5510 - 24s/epoch - 1ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 23s - loss: 14.5794 - mse: 14.5794 - mape: 81.5538 - 23s/epoch - 1ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 23s - loss: 14.5786 - mse: 14.5786 - mape: 81.6305 - 23s/epoch - 1ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 23s - loss: 14.5682 - mse: 14.5682 - mape: 81.6555 - 23s/epoch - 1ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 23s - loss: 14.5713 - mse: 14.5713 - mape: 81.5992 - 23s/epoch - 1ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 23s - loss: 14.5733 - mse: 14.5733 - mape: 81.6732 - 23s/epoch - 1ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 23s - loss: 14.5736 - mse: 14.5736 - mape: 81.6364 - 23s/epoch - 1ms/step\n",
            "Score for fold 1: loss of 26.87640953063965\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 23s - loss: 14.5600 - mse: 14.5600 - mape: 81.6998 - 23s/epoch - 1ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 24s - loss: 14.5622 - mse: 14.5622 - mape: 81.7313 - 24s/epoch - 1ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 24s - loss: 14.5688 - mse: 14.5688 - mape: 81.6994 - 24s/epoch - 1ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 24s - loss: 14.5651 - mse: 14.5651 - mape: 81.6838 - 24s/epoch - 1ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 24s - loss: 14.5665 - mse: 14.5665 - mape: 81.7171 - 24s/epoch - 1ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 24s - loss: 14.5641 - mse: 14.5641 - mape: 81.7064 - 24s/epoch - 1ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 25s - loss: 14.5617 - mse: 14.5617 - mape: 81.7968 - 25s/epoch - 1ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 24s - loss: 14.5592 - mse: 14.5592 - mape: 81.7777 - 24s/epoch - 1ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 24s - loss: 14.5645 - mse: 14.5645 - mape: 81.7589 - 24s/epoch - 1ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 24s - loss: 14.5663 - mse: 14.5663 - mape: 81.7225 - 24s/epoch - 1ms/step\n",
            "Score for fold 2: loss of 19.570741653442383\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 24s - loss: 14.5567 - mse: 14.5567 - mape: 81.7986 - 24s/epoch - 1ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 23s - loss: 14.5669 - mse: 14.5669 - mape: 81.7953 - 23s/epoch - 1ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 24s - loss: 14.5672 - mse: 14.5672 - mape: 81.7105 - 24s/epoch - 1ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 23s - loss: 14.5681 - mse: 14.5681 - mape: 81.7206 - 23s/epoch - 1ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 26s - loss: 14.5615 - mse: 14.5615 - mape: 81.8484 - 26s/epoch - 1ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 25s - loss: 14.5669 - mse: 14.5669 - mape: 81.7282 - 25s/epoch - 1ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 25s - loss: 14.5641 - mse: 14.5641 - mape: 81.7579 - 25s/epoch - 1ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 25s - loss: 14.5644 - mse: 14.5644 - mape: 81.7538 - 25s/epoch - 1ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 26s - loss: 14.5657 - mse: 14.5657 - mape: 81.7223 - 26s/epoch - 1ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 26s - loss: 14.5635 - mse: 14.5635 - mape: 81.7906 - 26s/epoch - 1ms/step\n",
            "Score for fold 3: loss of 19.54879379272461\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-09-27 15:12:19,559]\u001b[0m Finished trial#20 resulted in value: 14.563481330871582. Current best value is 13.179082870483398 with parameters: {'activation': 'tanh', 'optimizer': 'sgd', 'num_hidden_layer': 5, 'num_hidden_unit': 42, 'learning_rate': 0.0011985079039226632}.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 27s - loss: 14.6090 - mse: 14.6090 - mape: 80.4321 - 27s/epoch - 1ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 26s - loss: 14.0732 - mse: 14.0732 - mape: 79.1951 - 26s/epoch - 1ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 27s - loss: 13.9749 - mse: 13.9749 - mape: 78.6876 - 27s/epoch - 1ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 26s - loss: 13.9151 - mse: 13.9151 - mape: 78.3766 - 26s/epoch - 1ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 25s - loss: 13.8747 - mse: 13.8747 - mape: 78.2556 - 25s/epoch - 1ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 26s - loss: 13.8328 - mse: 13.8328 - mape: 78.1470 - 26s/epoch - 1ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 24s - loss: 13.7977 - mse: 13.7977 - mape: 78.0118 - 24s/epoch - 1ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 24s - loss: 13.7653 - mse: 13.7653 - mape: 77.8490 - 24s/epoch - 1ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 24s - loss: 13.7255 - mse: 13.7255 - mape: 77.7644 - 24s/epoch - 1ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 24s - loss: 13.6937 - mse: 13.6937 - mape: 77.7007 - 24s/epoch - 1ms/step\n",
            "Score for fold 1: loss of 27.551712036132812\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 24s - loss: 13.6596 - mse: 13.6596 - mape: 77.6082 - 24s/epoch - 1ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 24s - loss: 13.6349 - mse: 13.6349 - mape: 77.5462 - 24s/epoch - 1ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 24s - loss: 13.6164 - mse: 13.6164 - mape: 77.4927 - 24s/epoch - 1ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 24s - loss: 13.5873 - mse: 13.5873 - mape: 77.4209 - 24s/epoch - 1ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 24s - loss: 13.5829 - mse: 13.5829 - mape: 77.4083 - 24s/epoch - 1ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 23s - loss: 13.5464 - mse: 13.5464 - mape: 77.3601 - 23s/epoch - 1ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 24s - loss: 13.5485 - mse: 13.5485 - mape: 77.3464 - 24s/epoch - 1ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 24s - loss: 13.5151 - mse: 13.5151 - mape: 77.3033 - 24s/epoch - 1ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 24s - loss: 13.5038 - mse: 13.5038 - mape: 77.3018 - 24s/epoch - 1ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 25s - loss: 13.4994 - mse: 13.4994 - mape: 77.2987 - 25s/epoch - 1ms/step\n",
            "Score for fold 2: loss of 18.611326217651367\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 24s - loss: 13.4890 - mse: 13.4890 - mape: 77.2300 - 24s/epoch - 1ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 24s - loss: 13.4481 - mse: 13.4481 - mape: 77.1824 - 24s/epoch - 1ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 24s - loss: 13.4484 - mse: 13.4484 - mape: 77.2056 - 24s/epoch - 1ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 24s - loss: 13.4275 - mse: 13.4275 - mape: 77.1869 - 24s/epoch - 1ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 24s - loss: 13.4308 - mse: 13.4308 - mape: 77.1988 - 24s/epoch - 1ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 24s - loss: 13.4139 - mse: 13.4139 - mape: 77.1451 - 24s/epoch - 1ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 24s - loss: 13.3898 - mse: 13.3898 - mape: 77.1354 - 24s/epoch - 1ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 24s - loss: 13.3833 - mse: 13.3833 - mape: 77.1269 - 24s/epoch - 1ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 24s - loss: 13.3853 - mse: 13.3853 - mape: 77.1214 - 24s/epoch - 1ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 24s - loss: 13.3706 - mse: 13.3706 - mape: 77.1082 - 24s/epoch - 1ms/step\n",
            "Score for fold 3: loss of 18.365415573120117\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-09-27 15:24:44,862]\u001b[0m Finished trial#21 resulted in value: 13.370624542236328. Current best value is 13.179082870483398 with parameters: {'activation': 'tanh', 'optimizer': 'sgd', 'num_hidden_layer': 5, 'num_hidden_unit': 42, 'learning_rate': 0.0011985079039226632}.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 24s - loss: 14.4380 - mse: 14.4380 - mape: 79.9070 - 24s/epoch - 1ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 24s - loss: 13.9833 - mse: 13.9833 - mape: 78.7131 - 24s/epoch - 1ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 24s - loss: 13.8814 - mse: 13.8814 - mape: 78.3029 - 24s/epoch - 1ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 24s - loss: 13.8240 - mse: 13.8240 - mape: 78.0820 - 24s/epoch - 1ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 24s - loss: 13.7658 - mse: 13.7658 - mape: 77.9581 - 24s/epoch - 1ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 24s - loss: 13.7193 - mse: 13.7193 - mape: 77.8676 - 24s/epoch - 1ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 25s - loss: 13.6961 - mse: 13.6961 - mape: 77.7985 - 25s/epoch - 1ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 24s - loss: 13.6551 - mse: 13.6551 - mape: 77.5980 - 24s/epoch - 1ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 24s - loss: 13.6194 - mse: 13.6194 - mape: 77.5428 - 24s/epoch - 1ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 24s - loss: 13.5862 - mse: 13.5862 - mape: 77.4785 - 24s/epoch - 1ms/step\n",
            "Score for fold 1: loss of 28.355201721191406\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 25s - loss: 13.5602 - mse: 13.5602 - mape: 77.4449 - 25s/epoch - 1ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 25s - loss: 13.5305 - mse: 13.5305 - mape: 77.3921 - 25s/epoch - 1ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 25s - loss: 13.5094 - mse: 13.5094 - mape: 77.3104 - 25s/epoch - 1ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 25s - loss: 13.4903 - mse: 13.4903 - mape: 77.3505 - 25s/epoch - 1ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 25s - loss: 13.4638 - mse: 13.4638 - mape: 77.2841 - 25s/epoch - 1ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 25s - loss: 13.4363 - mse: 13.4363 - mape: 77.2865 - 25s/epoch - 1ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 25s - loss: 13.4294 - mse: 13.4294 - mape: 77.2893 - 25s/epoch - 1ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 25s - loss: 13.4189 - mse: 13.4189 - mape: 77.2242 - 25s/epoch - 1ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 25s - loss: 13.4065 - mse: 13.4065 - mape: 77.2337 - 25s/epoch - 1ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 24s - loss: 13.3812 - mse: 13.3812 - mape: 77.2295 - 24s/epoch - 1ms/step\n",
            "Score for fold 2: loss of 18.45962905883789\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 25s - loss: 13.3684 - mse: 13.3684 - mape: 77.2192 - 25s/epoch - 1ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 25s - loss: 13.3471 - mse: 13.3471 - mape: 77.1791 - 25s/epoch - 1ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 24s - loss: 13.3358 - mse: 13.3358 - mape: 77.1294 - 24s/epoch - 1ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 25s - loss: 13.3317 - mse: 13.3317 - mape: 77.1945 - 25s/epoch - 1ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 25s - loss: 13.3031 - mse: 13.3031 - mape: 77.1253 - 25s/epoch - 1ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 25s - loss: 13.3019 - mse: 13.3019 - mape: 77.0680 - 25s/epoch - 1ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 25s - loss: 13.2926 - mse: 13.2926 - mape: 77.1072 - 25s/epoch - 1ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 25s - loss: 13.2921 - mse: 13.2921 - mape: 77.0862 - 25s/epoch - 1ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 25s - loss: 13.2453 - mse: 13.2453 - mape: 77.0551 - 25s/epoch - 1ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 24s - loss: 13.2517 - mse: 13.2517 - mape: 77.0252 - 24s/epoch - 1ms/step\n",
            "Score for fold 3: loss of 18.408477783203125\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-09-27 15:37:13,759]\u001b[0m Finished trial#22 resulted in value: 13.251696586608887. Current best value is 13.179082870483398 with parameters: {'activation': 'tanh', 'optimizer': 'sgd', 'num_hidden_layer': 5, 'num_hidden_unit': 42, 'learning_rate': 0.0011985079039226632}.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 26s - loss: 14.2542 - mse: 14.2542 - mape: 80.1290 - 26s/epoch - 1ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 26s - loss: 13.9991 - mse: 13.9991 - mape: 79.0657 - 26s/epoch - 1ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 25s - loss: 13.9014 - mse: 13.9014 - mape: 78.7148 - 25s/epoch - 1ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 26s - loss: 13.8309 - mse: 13.8309 - mape: 78.3968 - 26s/epoch - 1ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 25s - loss: 13.7536 - mse: 13.7536 - mape: 78.1713 - 25s/epoch - 1ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 25s - loss: 13.7173 - mse: 13.7173 - mape: 78.0958 - 25s/epoch - 1ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 26s - loss: 13.6638 - mse: 13.6638 - mape: 77.9247 - 26s/epoch - 1ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 26s - loss: 13.6604 - mse: 13.6604 - mape: 78.0602 - 26s/epoch - 1ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 26s - loss: 13.6169 - mse: 13.6169 - mape: 77.8495 - 26s/epoch - 1ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 26s - loss: 13.5891 - mse: 13.5891 - mape: 77.9065 - 26s/epoch - 1ms/step\n",
            "Score for fold 1: loss of 28.476716995239258\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 26s - loss: 13.5571 - mse: 13.5571 - mape: 77.9057 - 26s/epoch - 1ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 26s - loss: 13.6140 - mse: 13.6140 - mape: 77.8803 - 26s/epoch - 1ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 26s - loss: 13.5093 - mse: 13.5093 - mape: 77.7475 - 26s/epoch - 1ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 25s - loss: 13.5655 - mse: 13.5655 - mape: 77.8487 - 25s/epoch - 1ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 26s - loss: 13.5965 - mse: 13.5965 - mape: 77.9564 - 26s/epoch - 1ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 26s - loss: 13.5296 - mse: 13.5296 - mape: 77.8427 - 26s/epoch - 1ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 25s - loss: 13.4725 - mse: 13.4725 - mape: 77.7259 - 25s/epoch - 1ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 25s - loss: 13.4882 - mse: 13.4882 - mape: 77.7268 - 25s/epoch - 1ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 26s - loss: 13.4341 - mse: 13.4341 - mape: 77.6209 - 26s/epoch - 1ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 26s - loss: 13.3818 - mse: 13.3818 - mape: 77.6652 - 26s/epoch - 1ms/step\n",
            "Score for fold 2: loss of 18.87736701965332\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 25s - loss: 13.4496 - mse: 13.4496 - mape: 77.6292 - 25s/epoch - 1ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 26s - loss: 13.4265 - mse: 13.4265 - mape: 77.7288 - 26s/epoch - 1ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 25s - loss: 13.4819 - mse: 13.4819 - mape: 77.7958 - 25s/epoch - 1ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 25s - loss: 13.3869 - mse: 13.3869 - mape: 77.7266 - 25s/epoch - 1ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 25s - loss: 13.3948 - mse: 13.3948 - mape: 77.6532 - 25s/epoch - 1ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 25s - loss: 13.3920 - mse: 13.3920 - mape: 77.7218 - 25s/epoch - 1ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 25s - loss: 13.3844 - mse: 13.3844 - mape: 77.8030 - 25s/epoch - 1ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 26s - loss: 13.3572 - mse: 13.3572 - mape: 77.6878 - 26s/epoch - 1ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 26s - loss: 13.3624 - mse: 13.3624 - mape: 77.7509 - 26s/epoch - 1ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 26s - loss: 13.4313 - mse: 13.4313 - mape: 77.7707 - 26s/epoch - 1ms/step\n",
            "Score for fold 3: loss of 18.418840408325195\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-09-27 15:50:12,784]\u001b[0m Finished trial#23 resulted in value: 13.431344032287598. Current best value is 13.179082870483398 with parameters: {'activation': 'tanh', 'optimizer': 'sgd', 'num_hidden_layer': 5, 'num_hidden_unit': 42, 'learning_rate': 0.0011985079039226632}.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 26s - loss: 14.3441 - mse: 14.3441 - mape: 80.1376 - 26s/epoch - 1ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 26s - loss: 13.9898 - mse: 13.9898 - mape: 78.8454 - 26s/epoch - 1ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 26s - loss: 13.9014 - mse: 13.9014 - mape: 78.3651 - 26s/epoch - 1ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 25s - loss: 13.8218 - mse: 13.8218 - mape: 78.0142 - 25s/epoch - 1ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 24s - loss: 13.7802 - mse: 13.7802 - mape: 77.8843 - 24s/epoch - 1ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 25s - loss: 13.7143 - mse: 13.7143 - mape: 77.7467 - 25s/epoch - 1ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 25s - loss: 13.6759 - mse: 13.6759 - mape: 77.6537 - 25s/epoch - 1ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 25s - loss: 13.6390 - mse: 13.6390 - mape: 77.5596 - 25s/epoch - 1ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 25s - loss: 13.5947 - mse: 13.5947 - mape: 77.5617 - 25s/epoch - 1ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 25s - loss: 13.5739 - mse: 13.5739 - mape: 77.4166 - 25s/epoch - 1ms/step\n",
            "Score for fold 1: loss of 28.447254180908203\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 25s - loss: 13.5199 - mse: 13.5199 - mape: 77.3308 - 25s/epoch - 1ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 25s - loss: 13.4990 - mse: 13.4990 - mape: 77.3311 - 25s/epoch - 1ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 25s - loss: 13.4763 - mse: 13.4763 - mape: 77.3280 - 25s/epoch - 1ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 25s - loss: 13.4478 - mse: 13.4478 - mape: 77.2913 - 25s/epoch - 1ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 25s - loss: 13.4174 - mse: 13.4174 - mape: 77.2427 - 25s/epoch - 1ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 25s - loss: 13.3666 - mse: 13.3666 - mape: 77.1830 - 25s/epoch - 1ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 25s - loss: 13.3558 - mse: 13.3558 - mape: 77.1957 - 25s/epoch - 1ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 25s - loss: 13.3470 - mse: 13.3470 - mape: 77.2337 - 25s/epoch - 1ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 25s - loss: 13.2968 - mse: 13.2968 - mape: 77.1179 - 25s/epoch - 1ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 25s - loss: 13.3042 - mse: 13.3042 - mape: 77.1491 - 25s/epoch - 1ms/step\n",
            "Score for fold 2: loss of 18.520774841308594\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 25s - loss: 13.2915 - mse: 13.2915 - mape: 77.1333 - 25s/epoch - 1ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 25s - loss: 13.2792 - mse: 13.2792 - mape: 77.1342 - 25s/epoch - 1ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 25s - loss: 13.2335 - mse: 13.2335 - mape: 77.0806 - 25s/epoch - 1ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 25s - loss: 13.2248 - mse: 13.2248 - mape: 77.0347 - 25s/epoch - 1ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 25s - loss: 13.1983 - mse: 13.1983 - mape: 77.0813 - 25s/epoch - 1ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 25s - loss: 13.1809 - mse: 13.1809 - mape: 77.0226 - 25s/epoch - 1ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 25s - loss: 13.1744 - mse: 13.1744 - mape: 77.0087 - 25s/epoch - 1ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 25s - loss: 13.1684 - mse: 13.1684 - mape: 77.0905 - 25s/epoch - 1ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 25s - loss: 13.1360 - mse: 13.1360 - mape: 77.0343 - 25s/epoch - 1ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 25s - loss: 13.1383 - mse: 13.1383 - mape: 77.0149 - 25s/epoch - 1ms/step\n",
            "Score for fold 3: loss of 18.24336814880371\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-09-27 16:02:56,439]\u001b[0m Finished trial#24 resulted in value: 13.138277053833008. Current best value is 13.138277053833008 with parameters: {'activation': 'tanh', 'optimizer': 'sgd', 'num_hidden_layer': 4, 'num_hidden_unit': 65, 'learning_rate': 0.0005646965831999246}.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 27s - loss: 14.8626 - mse: 14.8626 - mape: 84.1008 - 27s/epoch - 1ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 27s - loss: 15.0344 - mse: 15.0344 - mape: 86.0884 - 27s/epoch - 1ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 26s - loss: 15.0901 - mse: 15.0901 - mape: 86.8789 - 26s/epoch - 1ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 27s - loss: 15.6456 - mse: 15.6456 - mape: 90.7446 - 27s/epoch - 1ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 27s - loss: 15.3507 - mse: 15.3507 - mape: 89.8247 - 27s/epoch - 1ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 28s - loss: 15.7015 - mse: 15.7015 - mape: 92.6393 - 28s/epoch - 1ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 28s - loss: 15.9082 - mse: 15.9082 - mape: 93.6586 - 28s/epoch - 1ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 28s - loss: 16.0116 - mse: 16.0116 - mape: 94.2463 - 28s/epoch - 1ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 28s - loss: 15.7938 - mse: 15.7938 - mape: 93.3751 - 28s/epoch - 1ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 28s - loss: 15.9209 - mse: 15.9209 - mape: 92.9074 - 28s/epoch - 1ms/step\n",
            "Score for fold 1: loss of 28.060029983520508\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 28s - loss: 15.9293 - mse: 15.9293 - mape: 93.1032 - 28s/epoch - 1ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 28s - loss: 15.6827 - mse: 15.6827 - mape: 91.0569 - 28s/epoch - 1ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 28s - loss: 15.7883 - mse: 15.7883 - mape: 93.3068 - 28s/epoch - 1ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 28s - loss: 15.9089 - mse: 15.9089 - mape: 93.7069 - 28s/epoch - 1ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 28s - loss: 15.9621 - mse: 15.9621 - mape: 94.1758 - 28s/epoch - 1ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 28s - loss: 16.1087 - mse: 16.1087 - mape: 94.8989 - 28s/epoch - 1ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 27s - loss: 20.1924 - mse: 20.1924 - mape: 98.7571 - 27s/epoch - 1ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 27s - loss: 23.2630 - mse: 23.2630 - mape: 99.9938 - 27s/epoch - 1ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 27s - loss: 21.0374 - mse: 21.0374 - mape: 98.7514 - 27s/epoch - 1ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 27s - loss: 23.6738 - mse: 23.6738 - mape: 100.0000 - 27s/epoch - 1ms/step\n",
            "Score for fold 2: loss of 41.27994918823242\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 27s - loss: 23.6738 - mse: 23.6738 - mape: 100.0000 - 27s/epoch - 1ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 26s - loss: 23.6738 - mse: 23.6738 - mape: 100.0000 - 26s/epoch - 1ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 27s - loss: 23.6738 - mse: 23.6738 - mape: 100.0000 - 27s/epoch - 1ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 27s - loss: 23.6738 - mse: 23.6738 - mape: 100.0000 - 27s/epoch - 1ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 26s - loss: 23.6738 - mse: 23.6738 - mape: 100.0000 - 26s/epoch - 1ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 26s - loss: 23.6738 - mse: 23.6738 - mape: 100.0000 - 26s/epoch - 1ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 26s - loss: 23.6738 - mse: 23.6738 - mape: 100.0000 - 26s/epoch - 1ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 26s - loss: 23.6738 - mse: 23.6738 - mape: 100.0000 - 26s/epoch - 1ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 26s - loss: 23.6738 - mse: 23.6738 - mape: 100.0000 - 26s/epoch - 1ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 26s - loss: 23.6738 - mse: 23.6738 - mape: 100.0000 - 26s/epoch - 1ms/step\n",
            "Score for fold 3: loss of 29.109249114990234\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-09-27 16:16:42,499]\u001b[0m Finished trial#25 resulted in value: 23.673847198486328. Current best value is 13.138277053833008 with parameters: {'activation': 'tanh', 'optimizer': 'sgd', 'num_hidden_layer': 4, 'num_hidden_unit': 65, 'learning_rate': 0.0005646965831999246}.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 25s - loss: 14.2704 - mse: 14.2704 - mape: 80.2165 - 25s/epoch - 1ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 24s - loss: 14.0084 - mse: 14.0084 - mape: 79.0995 - 24s/epoch - 1ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 25s - loss: 13.9004 - mse: 13.9004 - mape: 78.6116 - 25s/epoch - 1ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 25s - loss: 13.8284 - mse: 13.8284 - mape: 78.4983 - 25s/epoch - 1ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 24s - loss: 13.7587 - mse: 13.7587 - mape: 78.2479 - 24s/epoch - 1ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 25s - loss: 13.7397 - mse: 13.7397 - mape: 78.1949 - 25s/epoch - 1ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 24s - loss: 13.6855 - mse: 13.6855 - mape: 78.1508 - 24s/epoch - 1ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 24s - loss: 13.6260 - mse: 13.6260 - mape: 77.9044 - 24s/epoch - 1ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 24s - loss: 13.6205 - mse: 13.6205 - mape: 77.9456 - 24s/epoch - 1ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 24s - loss: 13.5776 - mse: 13.5776 - mape: 77.8796 - 24s/epoch - 1ms/step\n",
            "Score for fold 1: loss of 27.99357032775879\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 24s - loss: 13.5285 - mse: 13.5285 - mape: 77.6923 - 24s/epoch - 1ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 25s - loss: 13.5126 - mse: 13.5126 - mape: 77.6534 - 25s/epoch - 1ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 24s - loss: 13.5243 - mse: 13.5243 - mape: 77.7549 - 24s/epoch - 1ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 25s - loss: 13.4955 - mse: 13.4955 - mape: 77.7122 - 25s/epoch - 1ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 25s - loss: 13.4378 - mse: 13.4378 - mape: 77.5785 - 25s/epoch - 1ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 25s - loss: 13.4379 - mse: 13.4379 - mape: 77.5952 - 25s/epoch - 1ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 24s - loss: 13.3667 - mse: 13.3667 - mape: 77.4558 - 24s/epoch - 1ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 24s - loss: 13.4365 - mse: 13.4365 - mape: 77.5850 - 24s/epoch - 1ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 24s - loss: 13.4498 - mse: 13.4498 - mape: 77.5909 - 24s/epoch - 1ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 24s - loss: 13.4028 - mse: 13.4028 - mape: 77.5745 - 24s/epoch - 1ms/step\n",
            "Score for fold 2: loss of 18.345449447631836\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 24s - loss: 13.3867 - mse: 13.3867 - mape: 77.4878 - 24s/epoch - 1ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 25s - loss: 13.4195 - mse: 13.4195 - mape: 77.6471 - 25s/epoch - 1ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 24s - loss: 13.3555 - mse: 13.3555 - mape: 77.4393 - 24s/epoch - 1ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 25s - loss: 13.3143 - mse: 13.3143 - mape: 77.4087 - 25s/epoch - 1ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 24s - loss: 13.2900 - mse: 13.2900 - mape: 77.4238 - 24s/epoch - 1ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 24s - loss: 13.3154 - mse: 13.3154 - mape: 77.5069 - 24s/epoch - 1ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 25s - loss: 13.2883 - mse: 13.2883 - mape: 77.3212 - 25s/epoch - 1ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 25s - loss: 13.2843 - mse: 13.2843 - mape: 77.3805 - 25s/epoch - 1ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 24s - loss: 13.2396 - mse: 13.2396 - mape: 77.4215 - 24s/epoch - 1ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 24s - loss: 13.2400 - mse: 13.2400 - mape: 77.3674 - 24s/epoch - 1ms/step\n",
            "Score for fold 3: loss of 18.29267120361328\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-09-27 16:29:08,157]\u001b[0m Finished trial#26 resulted in value: 13.240036010742188. Current best value is 13.138277053833008 with parameters: {'activation': 'tanh', 'optimizer': 'sgd', 'num_hidden_layer': 4, 'num_hidden_unit': 65, 'learning_rate': 0.0005646965831999246}.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 27s - loss: 14.3261 - mse: 14.3261 - mape: 80.0050 - 27s/epoch - 1ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 26s - loss: 13.9849 - mse: 13.9849 - mape: 78.7975 - 26s/epoch - 1ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 26s - loss: 13.8932 - mse: 13.8932 - mape: 78.4238 - 26s/epoch - 1ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 26s - loss: 13.8270 - mse: 13.8270 - mape: 78.2382 - 26s/epoch - 1ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 26s - loss: 13.7713 - mse: 13.7713 - mape: 78.0009 - 26s/epoch - 1ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 26s - loss: 13.7278 - mse: 13.7278 - mape: 77.9023 - 26s/epoch - 1ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 26s - loss: 13.6606 - mse: 13.6606 - mape: 77.7448 - 26s/epoch - 1ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 26s - loss: 13.6360 - mse: 13.6360 - mape: 77.6653 - 26s/epoch - 1ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 29s - loss: 13.6041 - mse: 13.6041 - mape: 77.5822 - 29s/epoch - 1ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 32s - loss: 13.5494 - mse: 13.5494 - mape: 77.4856 - 32s/epoch - 2ms/step\n",
            "Score for fold 1: loss of 28.71932029724121\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 31s - loss: 13.5228 - mse: 13.5228 - mape: 77.4582 - 31s/epoch - 2ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 31s - loss: 13.5007 - mse: 13.5007 - mape: 77.3977 - 31s/epoch - 2ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 31s - loss: 13.4748 - mse: 13.4748 - mape: 77.3845 - 31s/epoch - 2ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 30s - loss: 13.4343 - mse: 13.4343 - mape: 77.3810 - 30s/epoch - 1ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 29s - loss: 13.3890 - mse: 13.3890 - mape: 77.2624 - 29s/epoch - 1ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 31s - loss: 13.3884 - mse: 13.3884 - mape: 77.2777 - 31s/epoch - 2ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 32s - loss: 13.3475 - mse: 13.3475 - mape: 77.1953 - 32s/epoch - 2ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 32s - loss: 13.3276 - mse: 13.3276 - mape: 77.1411 - 32s/epoch - 2ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 33s - loss: 13.3257 - mse: 13.3257 - mape: 77.1304 - 33s/epoch - 2ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 36s - loss: 13.2894 - mse: 13.2894 - mape: 77.1043 - 36s/epoch - 2ms/step\n",
            "Score for fold 2: loss of 18.480764389038086\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 35s - loss: 13.2473 - mse: 13.2473 - mape: 77.1539 - 35s/epoch - 2ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 36s - loss: 13.2689 - mse: 13.2689 - mape: 77.1217 - 36s/epoch - 2ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 35s - loss: 13.1997 - mse: 13.1997 - mape: 77.0496 - 35s/epoch - 2ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 35s - loss: 13.2650 - mse: 13.2650 - mape: 77.1624 - 35s/epoch - 2ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 38s - loss: 13.1827 - mse: 13.1827 - mape: 77.0377 - 38s/epoch - 2ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 36s - loss: 13.1607 - mse: 13.1607 - mape: 77.0076 - 36s/epoch - 2ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 35s - loss: 13.1316 - mse: 13.1316 - mape: 77.0036 - 35s/epoch - 2ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 34s - loss: 13.1214 - mse: 13.1214 - mape: 77.0061 - 34s/epoch - 2ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 33s - loss: 13.0754 - mse: 13.0754 - mape: 76.9462 - 33s/epoch - 2ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 30s - loss: 13.1017 - mse: 13.1017 - mape: 76.9704 - 30s/epoch - 1ms/step\n",
            "Score for fold 3: loss of 18.18138313293457\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-09-27 16:44:52,680]\u001b[0m Finished trial#27 resulted in value: 13.101720809936523. Current best value is 13.101720809936523 with parameters: {'activation': 'tanh', 'optimizer': 'sgd', 'num_hidden_layer': 5, 'num_hidden_unit': 82, 'learning_rate': 0.0004920219633478297}.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 29s - loss: 14.9566 - mse: 14.9566 - mape: 80.7148 - 29s/epoch - 1ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 30s - loss: 14.5634 - mse: 14.5634 - mape: 80.1927 - 30s/epoch - 1ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 30s - loss: 14.4853 - mse: 14.4853 - mape: 79.9241 - 30s/epoch - 1ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 28s - loss: 14.4365 - mse: 14.4365 - mape: 79.8759 - 28s/epoch - 1ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 28s - loss: 14.4017 - mse: 14.4017 - mape: 79.7230 - 28s/epoch - 1ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 27s - loss: 14.3750 - mse: 14.3750 - mape: 79.8840 - 27s/epoch - 1ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 28s - loss: 14.3534 - mse: 14.3534 - mape: 79.9341 - 28s/epoch - 1ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 30s - loss: 14.3356 - mse: 14.3356 - mape: 79.8708 - 30s/epoch - 1ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 28s - loss: 14.3201 - mse: 14.3201 - mape: 79.8655 - 28s/epoch - 1ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 27s - loss: 14.3065 - mse: 14.3065 - mape: 80.0751 - 27s/epoch - 1ms/step\n",
            "Score for fold 1: loss of 28.470928192138672\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 27s - loss: 14.2940 - mse: 14.2940 - mape: 79.8588 - 27s/epoch - 1ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 27s - loss: 14.2832 - mse: 14.2832 - mape: 80.0907 - 27s/epoch - 1ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 29s - loss: 14.2730 - mse: 14.2730 - mape: 80.0023 - 29s/epoch - 1ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 30s - loss: 14.2633 - mse: 14.2633 - mape: 79.9143 - 30s/epoch - 2ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 30s - loss: 14.2546 - mse: 14.2546 - mape: 80.1883 - 30s/epoch - 1ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 30s - loss: 14.2464 - mse: 14.2464 - mape: 79.9784 - 30s/epoch - 2ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 30s - loss: 14.2386 - mse: 14.2386 - mape: 79.9193 - 30s/epoch - 1ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 29s - loss: 14.2310 - mse: 14.2310 - mape: 80.0570 - 29s/epoch - 1ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 29s - loss: 14.2240 - mse: 14.2240 - mape: 79.9476 - 29s/epoch - 1ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 29s - loss: 14.2172 - mse: 14.2172 - mape: 79.9590 - 29s/epoch - 1ms/step\n",
            "Score for fold 2: loss of 19.343156814575195\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 28s - loss: 14.2107 - mse: 14.2108 - mape: 79.9352 - 28s/epoch - 1ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 27s - loss: 14.2043 - mse: 14.2043 - mape: 80.0816 - 27s/epoch - 1ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 28s - loss: 14.1985 - mse: 14.1985 - mape: 79.9436 - 28s/epoch - 1ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 29s - loss: 14.1930 - mse: 14.1930 - mape: 79.8808 - 29s/epoch - 1ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 29s - loss: 14.1872 - mse: 14.1872 - mape: 79.8476 - 29s/epoch - 1ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 29s - loss: 14.1818 - mse: 14.1818 - mape: 79.7518 - 29s/epoch - 1ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 29s - loss: 14.1767 - mse: 14.1767 - mape: 79.8820 - 29s/epoch - 1ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 29s - loss: 14.1718 - mse: 14.1718 - mape: 79.8987 - 29s/epoch - 1ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 28s - loss: 14.1668 - mse: 14.1668 - mape: 79.6587 - 28s/epoch - 1ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 28s - loss: 14.1620 - mse: 14.1620 - mape: 79.8061 - 28s/epoch - 1ms/step\n",
            "Score for fold 3: loss of 19.25124168395996\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-09-27 16:59:21,394]\u001b[0m Finished trial#28 resulted in value: 14.161965370178223. Current best value is 13.101720809936523 with parameters: {'activation': 'tanh', 'optimizer': 'sgd', 'num_hidden_layer': 5, 'num_hidden_unit': 82, 'learning_rate': 0.0004920219633478297}.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 27s - loss: 14.6842 - mse: 14.6842 - mape: 79.2264 - 27s/epoch - 1ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 26s - loss: 14.5825 - mse: 14.5825 - mape: 79.4354 - 26s/epoch - 1ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 26s - loss: 14.5850 - mse: 14.5850 - mape: 79.6291 - 26s/epoch - 1ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 26s - loss: 14.5906 - mse: 14.5906 - mape: 79.7679 - 26s/epoch - 1ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 26s - loss: 14.5862 - mse: 14.5862 - mape: 79.8992 - 26s/epoch - 1ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 26s - loss: 14.5822 - mse: 14.5822 - mape: 79.8976 - 26s/epoch - 1ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 26s - loss: 14.5887 - mse: 14.5887 - mape: 79.9901 - 26s/epoch - 1ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 26s - loss: 14.5938 - mse: 14.5938 - mape: 79.8461 - 26s/epoch - 1ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 26s - loss: 14.5932 - mse: 14.5932 - mape: 79.9268 - 26s/epoch - 1ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 26s - loss: 14.5894 - mse: 14.5894 - mape: 79.9412 - 26s/epoch - 1ms/step\n",
            "Score for fold 1: loss of 27.75181770324707\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 25s - loss: 14.5881 - mse: 14.5881 - mape: 79.9075 - 25s/epoch - 1ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 25s - loss: 14.5983 - mse: 14.5983 - mape: 79.8963 - 25s/epoch - 1ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 26s - loss: 14.5939 - mse: 14.5939 - mape: 79.9306 - 26s/epoch - 1ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 25s - loss: 14.5938 - mse: 14.5938 - mape: 79.9525 - 25s/epoch - 1ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 26s - loss: 14.5958 - mse: 14.5958 - mape: 79.9582 - 26s/epoch - 1ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 25s - loss: 14.5889 - mse: 14.5889 - mape: 80.0345 - 25s/epoch - 1ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 25s - loss: 14.6017 - mse: 14.6017 - mape: 80.0667 - 25s/epoch - 1ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 25s - loss: 14.5956 - mse: 14.5956 - mape: 79.9852 - 25s/epoch - 1ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 25s - loss: 14.5961 - mse: 14.5961 - mape: 80.1856 - 25s/epoch - 1ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 25s - loss: 14.5994 - mse: 14.5994 - mape: 80.1405 - 25s/epoch - 1ms/step\n",
            "Score for fold 2: loss of 19.61429786682129\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 25s - loss: 14.6025 - mse: 14.6025 - mape: 80.1882 - 25s/epoch - 1ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 25s - loss: 14.5937 - mse: 14.5937 - mape: 80.1972 - 25s/epoch - 1ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 26s - loss: 14.6000 - mse: 14.6000 - mape: 80.0753 - 26s/epoch - 1ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 26s - loss: 14.5953 - mse: 14.5953 - mape: 80.1446 - 26s/epoch - 1ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 26s - loss: 14.5901 - mse: 14.5901 - mape: 80.2077 - 26s/epoch - 1ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 25s - loss: 14.6066 - mse: 14.6066 - mape: 80.2031 - 25s/epoch - 1ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 25s - loss: 14.5840 - mse: 14.5840 - mape: 80.1322 - 25s/epoch - 1ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 26s - loss: 14.5907 - mse: 14.5907 - mape: 80.0700 - 26s/epoch - 1ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 26s - loss: 14.6008 - mse: 14.6008 - mape: 80.1511 - 26s/epoch - 1ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 26s - loss: 14.5915 - mse: 14.5915 - mape: 80.1350 - 26s/epoch - 1ms/step\n",
            "Score for fold 3: loss of 19.61524772644043\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-09-27 17:12:21,761]\u001b[0m Finished trial#29 resulted in value: 14.591506004333496. Current best value is 13.101720809936523 with parameters: {'activation': 'tanh', 'optimizer': 'sgd', 'num_hidden_layer': 5, 'num_hidden_unit': 82, 'learning_rate': 0.0004920219633478297}.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 36s - loss: 14.6084 - mse: 14.6084 - mape: 80.3859 - 36s/epoch - 2ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 36s - loss: 14.1440 - mse: 14.1440 - mape: 79.4256 - 36s/epoch - 2ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 36s - loss: 13.9988 - mse: 13.9988 - mape: 78.7489 - 36s/epoch - 2ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 35s - loss: 13.9308 - mse: 13.9308 - mape: 78.5647 - 35s/epoch - 2ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 36s - loss: 13.8852 - mse: 13.8852 - mape: 78.2594 - 36s/epoch - 2ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 35s - loss: 13.8565 - mse: 13.8565 - mape: 78.0932 - 35s/epoch - 2ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 34s - loss: 13.8297 - mse: 13.8297 - mape: 77.9758 - 34s/epoch - 2ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 35s - loss: 13.8083 - mse: 13.8083 - mape: 77.8837 - 35s/epoch - 2ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 35s - loss: 13.7856 - mse: 13.7856 - mape: 77.8095 - 35s/epoch - 2ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 34s - loss: 13.7685 - mse: 13.7685 - mape: 77.7287 - 34s/epoch - 2ms/step\n",
            "Score for fold 1: loss of 27.916217803955078\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 35s - loss: 13.7494 - mse: 13.7494 - mape: 77.6496 - 35s/epoch - 2ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 35s - loss: 13.7336 - mse: 13.7336 - mape: 77.6420 - 35s/epoch - 2ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 35s - loss: 13.7158 - mse: 13.7158 - mape: 77.5358 - 35s/epoch - 2ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 36s - loss: 13.7045 - mse: 13.7045 - mape: 77.4939 - 36s/epoch - 2ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 35s - loss: 13.6870 - mse: 13.6870 - mape: 77.4304 - 35s/epoch - 2ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 36s - loss: 13.6732 - mse: 13.6732 - mape: 77.3534 - 36s/epoch - 2ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 35s - loss: 13.6561 - mse: 13.6561 - mape: 77.3315 - 35s/epoch - 2ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 35s - loss: 13.6402 - mse: 13.6402 - mape: 77.2379 - 35s/epoch - 2ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 33s - loss: 13.6285 - mse: 13.6285 - mape: 77.2597 - 33s/epoch - 2ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 34s - loss: 13.6192 - mse: 13.6192 - mape: 77.2267 - 34s/epoch - 2ms/step\n",
            "Score for fold 2: loss of 18.691307067871094\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 34s - loss: 13.6030 - mse: 13.6030 - mape: 77.1242 - 34s/epoch - 2ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 34s - loss: 13.5918 - mse: 13.5918 - mape: 77.0658 - 34s/epoch - 2ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 35s - loss: 13.5770 - mse: 13.5770 - mape: 77.0805 - 35s/epoch - 2ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 33s - loss: 13.5701 - mse: 13.5701 - mape: 77.0872 - 33s/epoch - 2ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 33s - loss: 13.5557 - mse: 13.5557 - mape: 77.0150 - 33s/epoch - 2ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 34s - loss: 13.5422 - mse: 13.5422 - mape: 76.9967 - 34s/epoch - 2ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 34s - loss: 13.5321 - mse: 13.5321 - mape: 77.0330 - 34s/epoch - 2ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 34s - loss: 13.5219 - mse: 13.5219 - mape: 76.9760 - 34s/epoch - 2ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 34s - loss: 13.5136 - mse: 13.5136 - mape: 76.9526 - 34s/epoch - 2ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 34s - loss: 13.5020 - mse: 13.5020 - mape: 76.9267 - 34s/epoch - 2ms/step\n",
            "Score for fold 3: loss of 18.55948257446289\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-09-27 17:29:50,557]\u001b[0m Finished trial#30 resulted in value: 13.501995086669922. Current best value is 13.101720809936523 with parameters: {'activation': 'tanh', 'optimizer': 'sgd', 'num_hidden_layer': 5, 'num_hidden_unit': 82, 'learning_rate': 0.0004920219633478297}.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 27s - loss: 14.2651 - mse: 14.2651 - mape: 80.3267 - 27s/epoch - 1ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 26s - loss: 14.0226 - mse: 14.0226 - mape: 79.2519 - 26s/epoch - 1ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 26s - loss: 13.9183 - mse: 13.9183 - mape: 78.9062 - 26s/epoch - 1ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 25s - loss: 13.8362 - mse: 13.8362 - mape: 78.6031 - 25s/epoch - 1ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 25s - loss: 13.7968 - mse: 13.7968 - mape: 78.4747 - 25s/epoch - 1ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 26s - loss: 13.7626 - mse: 13.7626 - mape: 78.2938 - 26s/epoch - 1ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 26s - loss: 13.7121 - mse: 13.7121 - mape: 78.3257 - 26s/epoch - 1ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 26s - loss: 13.6883 - mse: 13.6883 - mape: 78.2224 - 26s/epoch - 1ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 26s - loss: 13.6659 - mse: 13.6659 - mape: 78.1473 - 26s/epoch - 1ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 26s - loss: 13.5926 - mse: 13.5926 - mape: 78.0978 - 26s/epoch - 1ms/step\n",
            "Score for fold 1: loss of 28.2989501953125\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 26s - loss: 13.6377 - mse: 13.6377 - mape: 78.1307 - 26s/epoch - 1ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 26s - loss: 13.5444 - mse: 13.5444 - mape: 77.9543 - 26s/epoch - 1ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 26s - loss: 13.5318 - mse: 13.5318 - mape: 78.0159 - 26s/epoch - 1ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 26s - loss: 13.6080 - mse: 13.6080 - mape: 78.0344 - 26s/epoch - 1ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 27s - loss: 13.5244 - mse: 13.5244 - mape: 77.8536 - 27s/epoch - 1ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 27s - loss: 13.5135 - mse: 13.5135 - mape: 77.9202 - 27s/epoch - 1ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 26s - loss: 13.4401 - mse: 13.4401 - mape: 77.6865 - 26s/epoch - 1ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 26s - loss: 13.4533 - mse: 13.4533 - mape: 77.8291 - 26s/epoch - 1ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 26s - loss: 13.4676 - mse: 13.4676 - mape: 77.8249 - 26s/epoch - 1ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 26s - loss: 13.3725 - mse: 13.3725 - mape: 77.6666 - 26s/epoch - 1ms/step\n",
            "Score for fold 2: loss of 18.58572769165039\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 26s - loss: 13.3930 - mse: 13.3930 - mape: 77.7793 - 26s/epoch - 1ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 25s - loss: 13.3353 - mse: 13.3353 - mape: 77.6953 - 25s/epoch - 1ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 25s - loss: 13.3843 - mse: 13.3843 - mape: 77.7624 - 25s/epoch - 1ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 26s - loss: 13.3304 - mse: 13.3304 - mape: 77.7456 - 26s/epoch - 1ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 26s - loss: 13.3008 - mse: 13.3008 - mape: 77.7263 - 26s/epoch - 1ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 26s - loss: 13.3213 - mse: 13.3213 - mape: 77.8439 - 26s/epoch - 1ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 27s - loss: 13.3333 - mse: 13.3333 - mape: 77.7555 - 27s/epoch - 1ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 26s - loss: 13.3173 - mse: 13.3173 - mape: 77.8295 - 26s/epoch - 1ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 25s - loss: 13.2478 - mse: 13.2478 - mape: 77.7017 - 25s/epoch - 1ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 26s - loss: 13.2230 - mse: 13.2230 - mape: 77.7453 - 26s/epoch - 1ms/step\n",
            "Score for fold 3: loss of 18.403661727905273\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-09-27 17:43:00,733]\u001b[0m Finished trial#31 resulted in value: 13.222972869873047. Current best value is 13.101720809936523 with parameters: {'activation': 'tanh', 'optimizer': 'sgd', 'num_hidden_layer': 5, 'num_hidden_unit': 82, 'learning_rate': 0.0004920219633478297}.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 26s - loss: 14.2902 - mse: 14.2902 - mape: 80.4415 - 26s/epoch - 1ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 26s - loss: 14.0723 - mse: 14.0723 - mape: 79.5511 - 26s/epoch - 1ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 26s - loss: 13.9608 - mse: 13.9608 - mape: 79.0907 - 26s/epoch - 1ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 26s - loss: 13.8638 - mse: 13.8638 - mape: 78.7635 - 26s/epoch - 1ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 27s - loss: 13.8237 - mse: 13.8237 - mape: 78.6391 - 27s/epoch - 1ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 26s - loss: 13.7656 - mse: 13.7656 - mape: 78.5095 - 26s/epoch - 1ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 26s - loss: 13.7640 - mse: 13.7640 - mape: 78.4078 - 26s/epoch - 1ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 26s - loss: 13.7215 - mse: 13.7215 - mape: 78.3060 - 26s/epoch - 1ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 26s - loss: 13.7099 - mse: 13.7099 - mape: 78.3965 - 26s/epoch - 1ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 26s - loss: 13.7658 - mse: 13.7658 - mape: 78.4226 - 26s/epoch - 1ms/step\n",
            "Score for fold 1: loss of 28.520008087158203\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 26s - loss: 13.7030 - mse: 13.7030 - mape: 78.2889 - 26s/epoch - 1ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 27s - loss: 13.6124 - mse: 13.6124 - mape: 78.1459 - 27s/epoch - 1ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 26s - loss: 13.6326 - mse: 13.6326 - mape: 78.2178 - 26s/epoch - 1ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 26s - loss: 13.5886 - mse: 13.5886 - mape: 78.1670 - 26s/epoch - 1ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 27s - loss: 13.5582 - mse: 13.5582 - mape: 78.1812 - 27s/epoch - 1ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 26s - loss: 13.6205 - mse: 13.6205 - mape: 78.2587 - 26s/epoch - 1ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 26s - loss: 13.4886 - mse: 13.4886 - mape: 78.0190 - 26s/epoch - 1ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 27s - loss: 13.4862 - mse: 13.4862 - mape: 77.9752 - 27s/epoch - 1ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 26s - loss: 13.4713 - mse: 13.4713 - mape: 78.0155 - 26s/epoch - 1ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 27s - loss: 13.4232 - mse: 13.4232 - mape: 77.9777 - 27s/epoch - 1ms/step\n",
            "Score for fold 2: loss of 18.444664001464844\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 26s - loss: 13.4374 - mse: 13.4374 - mape: 78.0489 - 26s/epoch - 1ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 26s - loss: 13.4644 - mse: 13.4644 - mape: 78.0540 - 26s/epoch - 1ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 26s - loss: 13.4494 - mse: 13.4494 - mape: 78.1783 - 26s/epoch - 1ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 27s - loss: 13.3971 - mse: 13.3971 - mape: 78.1256 - 27s/epoch - 1ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 26s - loss: 13.3964 - mse: 13.3964 - mape: 78.2026 - 26s/epoch - 1ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 26s - loss: 13.4479 - mse: 13.4479 - mape: 78.2790 - 26s/epoch - 1ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 27s - loss: 13.3573 - mse: 13.3573 - mape: 78.0845 - 27s/epoch - 1ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 27s - loss: 13.3492 - mse: 13.3492 - mape: 78.1796 - 27s/epoch - 1ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 26s - loss: 13.4709 - mse: 13.4709 - mape: 78.2818 - 26s/epoch - 1ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 27s - loss: 13.3811 - mse: 13.3811 - mape: 78.2391 - 27s/epoch - 1ms/step\n",
            "Score for fold 3: loss of 18.242189407348633\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-09-27 17:56:22,887]\u001b[0m Finished trial#32 resulted in value: 13.381103515625. Current best value is 13.101720809936523 with parameters: {'activation': 'tanh', 'optimizer': 'sgd', 'num_hidden_layer': 5, 'num_hidden_unit': 82, 'learning_rate': 0.0004920219633478297}.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 29s - loss: 14.6591 - mse: 14.6591 - mape: 82.5741 - 29s/epoch - 1ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 29s - loss: 14.5031 - mse: 14.5031 - mape: 83.0350 - 29s/epoch - 1ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 28s - loss: 14.5925 - mse: 14.5925 - mape: 83.2892 - 28s/epoch - 1ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 28s - loss: 14.5300 - mse: 14.5300 - mape: 82.6744 - 28s/epoch - 1ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 28s - loss: 14.6133 - mse: 14.6133 - mape: 83.4611 - 28s/epoch - 1ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 28s - loss: 14.5408 - mse: 14.5408 - mape: 83.5651 - 28s/epoch - 1ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 28s - loss: 14.6148 - mse: 14.6148 - mape: 82.8703 - 28s/epoch - 1ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 28s - loss: 14.5387 - mse: 14.5387 - mape: 84.2781 - 28s/epoch - 1ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 28s - loss: 14.5383 - mse: 14.5383 - mape: 84.8876 - 28s/epoch - 1ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 28s - loss: 14.5256 - mse: 14.5256 - mape: 83.7328 - 28s/epoch - 1ms/step\n",
            "Score for fold 1: loss of 29.04352569580078\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 28s - loss: 14.5305 - mse: 14.5305 - mape: 84.0175 - 28s/epoch - 1ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 29s - loss: 14.5032 - mse: 14.5032 - mape: 83.8231 - 29s/epoch - 1ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 29s - loss: 14.5670 - mse: 14.5670 - mape: 84.4906 - 29s/epoch - 1ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 29s - loss: 14.6344 - mse: 14.6344 - mape: 83.4403 - 29s/epoch - 1ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 29s - loss: 14.7286 - mse: 14.7286 - mape: 83.0427 - 29s/epoch - 1ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 29s - loss: 14.7369 - mse: 14.7369 - mape: 84.0909 - 29s/epoch - 1ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 28s - loss: 14.8122 - mse: 14.8122 - mape: 83.4249 - 28s/epoch - 1ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 28s - loss: 14.7428 - mse: 14.7428 - mape: 83.9380 - 28s/epoch - 1ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 28s - loss: 14.7709 - mse: 14.7709 - mape: 83.7068 - 28s/epoch - 1ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 29s - loss: 14.9253 - mse: 14.9253 - mape: 83.3422 - 29s/epoch - 1ms/step\n",
            "Score for fold 2: loss of 19.299062728881836\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 28s - loss: 15.0191 - mse: 15.0191 - mape: 83.5256 - 28s/epoch - 1ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 28s - loss: 14.8448 - mse: 14.8448 - mape: 83.0806 - 28s/epoch - 1ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 29s - loss: 14.8773 - mse: 14.8773 - mape: 83.0252 - 29s/epoch - 1ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 29s - loss: 14.9648 - mse: 14.9648 - mape: 83.2938 - 29s/epoch - 1ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 29s - loss: 14.7778 - mse: 14.7778 - mape: 83.7604 - 29s/epoch - 1ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 29s - loss: 14.6377 - mse: 14.6377 - mape: 83.6907 - 29s/epoch - 1ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 30s - loss: 14.8236 - mse: 14.8236 - mape: 84.1842 - 30s/epoch - 2ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 30s - loss: 14.7911 - mse: 14.7911 - mape: 84.5508 - 30s/epoch - 2ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 29s - loss: 14.6494 - mse: 14.6494 - mape: 84.8428 - 29s/epoch - 1ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 29s - loss: 14.6872 - mse: 14.6872 - mape: 84.7382 - 29s/epoch - 1ms/step\n",
            "Score for fold 3: loss of 19.87771224975586\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-09-27 18:10:51,012]\u001b[0m Finished trial#33 resulted in value: 14.687161445617676. Current best value is 13.101720809936523 with parameters: {'activation': 'tanh', 'optimizer': 'sgd', 'num_hidden_layer': 5, 'num_hidden_unit': 82, 'learning_rate': 0.0004920219633478297}.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 27s - loss: 14.5719 - mse: 14.5719 - mape: 82.5490 - 27s/epoch - 1ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 27s - loss: 14.4956 - mse: 14.4956 - mape: 82.1628 - 27s/epoch - 1ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 27s - loss: 14.4491 - mse: 14.4491 - mape: 81.9961 - 27s/epoch - 1ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 27s - loss: 14.3665 - mse: 14.3665 - mape: 81.7084 - 27s/epoch - 1ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 27s - loss: 14.6158 - mse: 14.6158 - mape: 82.7584 - 27s/epoch - 1ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 27s - loss: 14.6150 - mse: 14.6150 - mape: 83.2317 - 27s/epoch - 1ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 27s - loss: 14.5772 - mse: 14.5772 - mape: 83.3462 - 27s/epoch - 1ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 26s - loss: 14.5103 - mse: 14.5103 - mape: 83.0837 - 26s/epoch - 1ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 27s - loss: 14.4412 - mse: 14.4412 - mape: 82.6570 - 27s/epoch - 1ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 27s - loss: 14.4511 - mse: 14.4511 - mape: 82.7429 - 27s/epoch - 1ms/step\n",
            "Score for fold 1: loss of 28.38383674621582\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 27s - loss: 14.3763 - mse: 14.3763 - mape: 82.3298 - 27s/epoch - 1ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 27s - loss: 14.3957 - mse: 14.3957 - mape: 82.3952 - 27s/epoch - 1ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 27s - loss: 14.5387 - mse: 14.5387 - mape: 83.1776 - 27s/epoch - 1ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 27s - loss: 14.4771 - mse: 14.4771 - mape: 82.8449 - 27s/epoch - 1ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 27s - loss: 14.4770 - mse: 14.4770 - mape: 83.0320 - 27s/epoch - 1ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 29s - loss: 14.5130 - mse: 14.5130 - mape: 83.5046 - 29s/epoch - 1ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 31s - loss: 14.4339 - mse: 14.4339 - mape: 82.8837 - 31s/epoch - 2ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 33s - loss: 14.4096 - mse: 14.4096 - mape: 82.8335 - 33s/epoch - 2ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 32s - loss: 14.3772 - mse: 14.3772 - mape: 83.0491 - 32s/epoch - 2ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 32s - loss: 14.5395 - mse: 14.5395 - mape: 83.8708 - 32s/epoch - 2ms/step\n",
            "Score for fold 2: loss of 25.056161880493164\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 29s - loss: 14.6298 - mse: 14.6298 - mape: 84.5860 - 29s/epoch - 1ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 28s - loss: 14.6856 - mse: 14.6856 - mape: 85.0803 - 28s/epoch - 1ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 29s - loss: 14.6618 - mse: 14.6618 - mape: 85.2971 - 29s/epoch - 1ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 29s - loss: 14.5900 - mse: 14.5900 - mape: 84.7373 - 29s/epoch - 1ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 28s - loss: 14.7464 - mse: 14.7464 - mape: 85.9775 - 28s/epoch - 1ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 29s - loss: 14.8255 - mse: 14.8255 - mape: 86.4536 - 29s/epoch - 1ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 29s - loss: 14.7542 - mse: 14.7542 - mape: 86.1374 - 29s/epoch - 1ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 30s - loss: 14.7426 - mse: 14.7426 - mape: 85.4859 - 30s/epoch - 1ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 30s - loss: 14.7490 - mse: 14.7490 - mape: 85.5136 - 30s/epoch - 2ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 30s - loss: 14.8917 - mse: 14.8917 - mape: 87.0312 - 30s/epoch - 1ms/step\n",
            "Score for fold 3: loss of 19.05974578857422\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-09-27 18:25:15,117]\u001b[0m Finished trial#34 resulted in value: 14.891707420349121. Current best value is 13.101720809936523 with parameters: {'activation': 'tanh', 'optimizer': 'sgd', 'num_hidden_layer': 5, 'num_hidden_unit': 82, 'learning_rate': 0.0004920219633478297}.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 27s - loss: 14.4029 - mse: 14.4029 - mape: 79.8589 - 27s/epoch - 1ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 26s - loss: 14.0454 - mse: 14.0454 - mape: 79.4039 - 26s/epoch - 1ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 26s - loss: 13.9467 - mse: 13.9467 - mape: 78.8536 - 26s/epoch - 1ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 27s - loss: 13.9057 - mse: 13.9057 - mape: 78.5344 - 27s/epoch - 1ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 27s - loss: 13.8734 - mse: 13.8734 - mape: 78.3055 - 27s/epoch - 1ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 27s - loss: 13.8451 - mse: 13.8451 - mape: 78.1369 - 27s/epoch - 1ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 27s - loss: 13.8172 - mse: 13.8172 - mape: 78.0209 - 27s/epoch - 1ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 28s - loss: 13.7890 - mse: 13.7890 - mape: 77.8788 - 28s/epoch - 1ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 27s - loss: 13.7627 - mse: 13.7627 - mape: 77.8143 - 27s/epoch - 1ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 27s - loss: 13.7375 - mse: 13.7375 - mape: 77.5693 - 27s/epoch - 1ms/step\n",
            "Score for fold 1: loss of 28.275108337402344\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 28s - loss: 13.7224 - mse: 13.7224 - mape: 77.7192 - 28s/epoch - 1ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 27s - loss: 13.7005 - mse: 13.7005 - mape: 77.5445 - 27s/epoch - 1ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 27s - loss: 13.6794 - mse: 13.6794 - mape: 77.5519 - 27s/epoch - 1ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 27s - loss: 13.6614 - mse: 13.6614 - mape: 77.4167 - 27s/epoch - 1ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 27s - loss: 13.6451 - mse: 13.6451 - mape: 77.3761 - 27s/epoch - 1ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 26s - loss: 13.6258 - mse: 13.6258 - mape: 77.3917 - 26s/epoch - 1ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 26s - loss: 13.6105 - mse: 13.6105 - mape: 77.3383 - 26s/epoch - 1ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 27s - loss: 13.5950 - mse: 13.5950 - mape: 77.3073 - 27s/epoch - 1ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 27s - loss: 13.5863 - mse: 13.5863 - mape: 77.3253 - 27s/epoch - 1ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 27s - loss: 13.5703 - mse: 13.5703 - mape: 77.2618 - 27s/epoch - 1ms/step\n",
            "Score for fold 2: loss of 19.782352447509766\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 26s - loss: 13.5550 - mse: 13.5550 - mape: 77.2878 - 26s/epoch - 1ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 27s - loss: 13.5422 - mse: 13.5422 - mape: 77.2424 - 27s/epoch - 1ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 27s - loss: 13.5156 - mse: 13.5156 - mape: 77.1951 - 27s/epoch - 1ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 27s - loss: 13.5104 - mse: 13.5104 - mape: 77.2241 - 27s/epoch - 1ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 26s - loss: 13.5058 - mse: 13.5058 - mape: 77.2152 - 26s/epoch - 1ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 27s - loss: 13.4843 - mse: 13.4843 - mape: 77.2111 - 27s/epoch - 1ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 27s - loss: 13.4806 - mse: 13.4806 - mape: 77.1730 - 27s/epoch - 1ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 27s - loss: 13.4586 - mse: 13.4586 - mape: 77.2051 - 27s/epoch - 1ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 26s - loss: 13.4539 - mse: 13.4539 - mape: 77.1894 - 26s/epoch - 1ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 27s - loss: 13.4363 - mse: 13.4363 - mape: 77.1522 - 27s/epoch - 1ms/step\n",
            "Score for fold 3: loss of 18.509719848632812\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-09-27 18:38:49,784]\u001b[0m Finished trial#35 resulted in value: 13.436314582824707. Current best value is 13.101720809936523 with parameters: {'activation': 'tanh', 'optimizer': 'sgd', 'num_hidden_layer': 5, 'num_hidden_unit': 82, 'learning_rate': 0.0004920219633478297}.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 31s - loss: 14.2098 - mse: 14.2098 - mape: 78.3472 - 31s/epoch - 2ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 30s - loss: 14.1732 - mse: 14.1732 - mape: 78.4339 - 30s/epoch - 2ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 30s - loss: 14.1286 - mse: 14.1286 - mape: 78.4002 - 30s/epoch - 1ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 30s - loss: 14.1514 - mse: 14.1514 - mape: 78.1293 - 30s/epoch - 1ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 29s - loss: 14.1438 - mse: 14.1438 - mape: 78.3546 - 29s/epoch - 1ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 28s - loss: 14.1698 - mse: 14.1698 - mape: 77.8413 - 28s/epoch - 1ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 28s - loss: 14.2223 - mse: 14.2223 - mape: 77.7999 - 28s/epoch - 1ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 28s - loss: 14.2136 - mse: 14.2136 - mape: 78.6237 - 28s/epoch - 1ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 28s - loss: 14.1454 - mse: 14.1454 - mape: 79.1993 - 28s/epoch - 1ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 28s - loss: 14.1156 - mse: 14.1156 - mape: 77.1188 - 28s/epoch - 1ms/step\n",
            "Score for fold 1: loss of 29.01658821105957\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 29s - loss: 14.0959 - mse: 14.0959 - mape: 77.9249 - 29s/epoch - 1ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 28s - loss: 14.1295 - mse: 14.1295 - mape: 78.3405 - 28s/epoch - 1ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 28s - loss: 14.1400 - mse: 14.1400 - mape: 78.5410 - 28s/epoch - 1ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 28s - loss: 14.1600 - mse: 14.1600 - mape: 78.4252 - 28s/epoch - 1ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 28s - loss: 14.1616 - mse: 14.1616 - mape: 78.7541 - 28s/epoch - 1ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 28s - loss: 14.1823 - mse: 14.1823 - mape: 78.6719 - 28s/epoch - 1ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 28s - loss: 14.2221 - mse: 14.2221 - mape: 79.1233 - 28s/epoch - 1ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 28s - loss: 14.2109 - mse: 14.2109 - mape: 80.1729 - 28s/epoch - 1ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 28s - loss: 14.1624 - mse: 14.1624 - mape: 79.5331 - 28s/epoch - 1ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 28s - loss: 14.2008 - mse: 14.2008 - mape: 79.0538 - 28s/epoch - 1ms/step\n",
            "Score for fold 2: loss of 20.189666748046875\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 28s - loss: 14.2046 - mse: 14.2046 - mape: 78.9407 - 28s/epoch - 1ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 28s - loss: 14.2559 - mse: 14.2559 - mape: 79.0975 - 28s/epoch - 1ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 29s - loss: 14.2581 - mse: 14.2581 - mape: 79.9307 - 29s/epoch - 1ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 27s - loss: 14.2761 - mse: 14.2761 - mape: 79.7945 - 27s/epoch - 1ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 27s - loss: 14.2711 - mse: 14.2711 - mape: 79.5380 - 27s/epoch - 1ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 27s - loss: 14.2640 - mse: 14.2640 - mape: 80.0316 - 27s/epoch - 1ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 27s - loss: 14.2948 - mse: 14.2948 - mape: 80.5350 - 27s/epoch - 1ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 26s - loss: 14.2940 - mse: 14.2940 - mape: 80.3919 - 26s/epoch - 1ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 27s - loss: 14.2888 - mse: 14.2888 - mape: 80.1415 - 27s/epoch - 1ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 27s - loss: 14.2706 - mse: 14.2706 - mape: 80.4420 - 27s/epoch - 1ms/step\n",
            "Score for fold 3: loss of 19.343496322631836\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-09-27 18:53:08,425]\u001b[0m Finished trial#36 resulted in value: 14.270612716674805. Current best value is 13.101720809936523 with parameters: {'activation': 'tanh', 'optimizer': 'sgd', 'num_hidden_layer': 5, 'num_hidden_unit': 82, 'learning_rate': 0.0004920219633478297}.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 27s - loss: 14.5054 - mse: 14.5054 - mape: 81.7851 - 27s/epoch - 1ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 27s - loss: 14.3322 - mse: 14.3322 - mape: 81.2748 - 27s/epoch - 1ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 27s - loss: 14.3552 - mse: 14.3552 - mape: 81.5390 - 27s/epoch - 1ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 27s - loss: 14.2700 - mse: 14.2700 - mape: 81.0690 - 27s/epoch - 1ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 27s - loss: 14.2434 - mse: 14.2434 - mape: 81.2362 - 27s/epoch - 1ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 27s - loss: 14.2204 - mse: 14.2204 - mape: 80.9189 - 27s/epoch - 1ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 26s - loss: 14.1277 - mse: 14.1277 - mape: 80.5406 - 26s/epoch - 1ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 26s - loss: 14.1838 - mse: 14.1838 - mape: 81.2835 - 26s/epoch - 1ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 26s - loss: 14.2670 - mse: 14.2670 - mape: 81.4605 - 26s/epoch - 1ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 26s - loss: 14.3075 - mse: 14.3075 - mape: 81.9285 - 26s/epoch - 1ms/step\n",
            "Score for fold 1: loss of 28.55937957763672\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 27s - loss: 14.3161 - mse: 14.3161 - mape: 81.9262 - 27s/epoch - 1ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 27s - loss: 14.2943 - mse: 14.2943 - mape: 81.9706 - 27s/epoch - 1ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 26s - loss: 14.3561 - mse: 14.3561 - mape: 82.1316 - 26s/epoch - 1ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 27s - loss: 14.3028 - mse: 14.3028 - mape: 82.0830 - 27s/epoch - 1ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 27s - loss: 14.2775 - mse: 14.2775 - mape: 81.7747 - 27s/epoch - 1ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 28s - loss: 14.3221 - mse: 14.3221 - mape: 82.3719 - 28s/epoch - 1ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 27s - loss: 14.4505 - mse: 14.4505 - mape: 83.1598 - 27s/epoch - 1ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 27s - loss: 14.4973 - mse: 14.4973 - mape: 83.2237 - 27s/epoch - 1ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 27s - loss: 14.5186 - mse: 14.5186 - mape: 83.1485 - 27s/epoch - 1ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 27s - loss: 14.4876 - mse: 14.4876 - mape: 83.1438 - 27s/epoch - 1ms/step\n",
            "Score for fold 2: loss of 18.736774444580078\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 28s - loss: 14.4199 - mse: 14.4199 - mape: 82.8378 - 28s/epoch - 1ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 29s - loss: 14.4154 - mse: 14.4154 - mape: 83.1256 - 29s/epoch - 1ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 29s - loss: 14.5123 - mse: 14.5123 - mape: 83.3747 - 29s/epoch - 1ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 28s - loss: 14.4287 - mse: 14.4287 - mape: 83.1691 - 28s/epoch - 1ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 28s - loss: 14.4629 - mse: 14.4629 - mape: 83.3544 - 28s/epoch - 1ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 28s - loss: 14.5195 - mse: 14.5195 - mape: 83.4127 - 28s/epoch - 1ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 28s - loss: 14.4514 - mse: 14.4514 - mape: 83.0188 - 28s/epoch - 1ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 28s - loss: 14.4683 - mse: 14.4683 - mape: 82.9385 - 28s/epoch - 1ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 28s - loss: 14.4602 - mse: 14.4602 - mape: 83.0005 - 28s/epoch - 1ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 29s - loss: 14.5018 - mse: 14.5018 - mape: 83.0052 - 29s/epoch - 1ms/step\n",
            "Score for fold 3: loss of 18.818649291992188\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-09-27 19:07:00,440]\u001b[0m Finished trial#37 resulted in value: 14.501790046691895. Current best value is 13.101720809936523 with parameters: {'activation': 'tanh', 'optimizer': 'sgd', 'num_hidden_layer': 5, 'num_hidden_unit': 82, 'learning_rate': 0.0004920219633478297}.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 36s - loss: 14.2470 - mse: 14.2470 - mape: 79.4563 - 36s/epoch - 2ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 35s - loss: 13.9104 - mse: 13.9104 - mape: 78.3099 - 35s/epoch - 2ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 35s - loss: 13.8155 - mse: 13.8155 - mape: 77.8987 - 35s/epoch - 2ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 36s - loss: 13.7260 - mse: 13.7260 - mape: 77.6676 - 36s/epoch - 2ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 37s - loss: 13.6469 - mse: 13.6469 - mape: 77.5275 - 37s/epoch - 2ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 38s - loss: 13.6044 - mse: 13.6044 - mape: 77.3829 - 38s/epoch - 2ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 37s - loss: 13.5526 - mse: 13.5526 - mape: 77.2946 - 37s/epoch - 2ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 37s - loss: 13.5306 - mse: 13.5306 - mape: 77.2823 - 37s/epoch - 2ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 36s - loss: 13.4779 - mse: 13.4779 - mape: 77.1759 - 36s/epoch - 2ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 35s - loss: 13.4349 - mse: 13.4349 - mape: 77.0908 - 35s/epoch - 2ms/step\n",
            "Score for fold 1: loss of 27.778234481811523\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 35s - loss: 13.4129 - mse: 13.4129 - mape: 77.1214 - 35s/epoch - 2ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 35s - loss: 13.3812 - mse: 13.3812 - mape: 77.0023 - 35s/epoch - 2ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 34s - loss: 13.3476 - mse: 13.3476 - mape: 76.9868 - 34s/epoch - 2ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 34s - loss: 13.3114 - mse: 13.3114 - mape: 76.9258 - 34s/epoch - 2ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 34s - loss: 13.2832 - mse: 13.2832 - mape: 76.8922 - 34s/epoch - 2ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 34s - loss: 13.2324 - mse: 13.2324 - mape: 76.8494 - 34s/epoch - 2ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 33s - loss: 13.2130 - mse: 13.2130 - mape: 76.8120 - 33s/epoch - 2ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 34s - loss: 13.1867 - mse: 13.1867 - mape: 76.8388 - 34s/epoch - 2ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 34s - loss: 13.1647 - mse: 13.1647 - mape: 76.7802 - 34s/epoch - 2ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 33s - loss: 13.1236 - mse: 13.1236 - mape: 76.7774 - 33s/epoch - 2ms/step\n",
            "Score for fold 2: loss of 18.521625518798828\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 32s - loss: 13.1034 - mse: 13.1034 - mape: 76.7468 - 32s/epoch - 2ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 32s - loss: 13.0725 - mse: 13.0725 - mape: 76.7775 - 32s/epoch - 2ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 32s - loss: 13.0583 - mse: 13.0583 - mape: 76.6788 - 32s/epoch - 2ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 33s - loss: 13.0258 - mse: 13.0258 - mape: 76.6970 - 33s/epoch - 2ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 32s - loss: 12.9984 - mse: 12.9984 - mape: 76.6740 - 32s/epoch - 2ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 32s - loss: 12.9759 - mse: 12.9759 - mape: 76.6074 - 32s/epoch - 2ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 32s - loss: 12.9647 - mse: 12.9647 - mape: 76.5980 - 32s/epoch - 2ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 33s - loss: 12.9289 - mse: 12.9289 - mape: 76.5372 - 33s/epoch - 2ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 34s - loss: 12.9088 - mse: 12.9088 - mape: 76.5774 - 34s/epoch - 2ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 34s - loss: 12.8802 - mse: 12.8802 - mape: 76.5282 - 34s/epoch - 2ms/step\n",
            "Score for fold 3: loss of 18.07354164123535\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-09-27 19:24:20,038]\u001b[0m Finished trial#38 resulted in value: 12.880197525024414. Current best value is 12.880197525024414 with parameters: {'activation': 'tanh', 'optimizer': 'adam', 'num_hidden_layer': 5, 'num_hidden_unit': 59, 'learning_rate': 0.00018586446397436457}.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 31s - loss: 14.6570 - mse: 14.6570 - mape: 81.1021 - 31s/epoch - 2ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 30s - loss: 14.5651 - mse: 14.5651 - mape: 81.3956 - 30s/epoch - 1ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 30s - loss: 14.5571 - mse: 14.5571 - mape: 81.4276 - 30s/epoch - 1ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 30s - loss: 14.5528 - mse: 14.5528 - mape: 81.3929 - 30s/epoch - 2ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 30s - loss: 14.5481 - mse: 14.5481 - mape: 81.4291 - 30s/epoch - 1ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 29s - loss: 14.5438 - mse: 14.5438 - mape: 81.4227 - 29s/epoch - 1ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 29s - loss: 14.5441 - mse: 14.5441 - mape: 81.3921 - 29s/epoch - 1ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 29s - loss: 14.5413 - mse: 14.5413 - mape: 81.4536 - 29s/epoch - 1ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 29s - loss: 14.5398 - mse: 14.5398 - mape: 81.3668 - 29s/epoch - 1ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 29s - loss: 14.5430 - mse: 14.5430 - mape: 81.4421 - 29s/epoch - 1ms/step\n",
            "Score for fold 1: loss of 27.763347625732422\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 29s - loss: 14.5386 - mse: 14.5386 - mape: 81.3887 - 29s/epoch - 1ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 29s - loss: 14.5347 - mse: 14.5347 - mape: 81.3568 - 29s/epoch - 1ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 29s - loss: 14.5313 - mse: 14.5313 - mape: 81.4094 - 29s/epoch - 1ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 30s - loss: 14.5336 - mse: 14.5336 - mape: 81.4357 - 30s/epoch - 1ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 30s - loss: 14.5357 - mse: 14.5357 - mape: 81.3687 - 30s/epoch - 1ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 30s - loss: 14.5335 - mse: 14.5335 - mape: 81.3963 - 30s/epoch - 1ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 29s - loss: 14.5358 - mse: 14.5358 - mape: 81.4444 - 29s/epoch - 1ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 29s - loss: 14.5346 - mse: 14.5346 - mape: 81.3339 - 29s/epoch - 1ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 30s - loss: 14.5311 - mse: 14.5311 - mape: 81.4244 - 30s/epoch - 1ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 29s - loss: 14.5326 - mse: 14.5326 - mape: 81.4517 - 29s/epoch - 1ms/step\n",
            "Score for fold 2: loss of 19.546194076538086\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 29s - loss: 14.5306 - mse: 14.5306 - mape: 81.3661 - 29s/epoch - 1ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 29s - loss: 14.5350 - mse: 14.5350 - mape: 81.3744 - 29s/epoch - 1ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 29s - loss: 14.5308 - mse: 14.5308 - mape: 81.4417 - 29s/epoch - 1ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 29s - loss: 14.5313 - mse: 14.5313 - mape: 81.4135 - 29s/epoch - 1ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 30s - loss: 14.5311 - mse: 14.5311 - mape: 81.3761 - 30s/epoch - 1ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 29s - loss: 14.5320 - mse: 14.5320 - mape: 81.4601 - 29s/epoch - 1ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 29s - loss: 14.5298 - mse: 14.5298 - mape: 81.3899 - 29s/epoch - 1ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 29s - loss: 14.5322 - mse: 14.5322 - mape: 81.3620 - 29s/epoch - 1ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 29s - loss: 14.5280 - mse: 14.5280 - mape: 81.5106 - 29s/epoch - 1ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 29s - loss: 14.5305 - mse: 14.5305 - mape: 81.3730 - 29s/epoch - 1ms/step\n",
            "Score for fold 3: loss of 19.544265747070312\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-09-27 19:39:10,462]\u001b[0m Finished trial#39 resulted in value: 14.530492782592773. Current best value is 12.880197525024414 with parameters: {'activation': 'tanh', 'optimizer': 'adam', 'num_hidden_layer': 5, 'num_hidden_unit': 59, 'learning_rate': 0.00018586446397436457}.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 31s - loss: 15.2413 - mse: 15.2413 - mape: 79.5774 - 31s/epoch - 2ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 31s - loss: 14.3791 - mse: 14.3791 - mape: 79.6454 - 31s/epoch - 2ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 31s - loss: 14.2302 - mse: 14.2302 - mape: 79.3587 - 31s/epoch - 2ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 31s - loss: 14.1340 - mse: 14.1340 - mape: 79.0334 - 31s/epoch - 2ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 31s - loss: 14.0634 - mse: 14.0634 - mape: 78.7592 - 31s/epoch - 2ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 31s - loss: 14.0099 - mse: 14.0099 - mape: 78.5194 - 31s/epoch - 2ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 30s - loss: 13.9686 - mse: 13.9686 - mape: 78.3347 - 30s/epoch - 2ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 31s - loss: 13.9381 - mse: 13.9381 - mape: 78.2240 - 31s/epoch - 2ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 31s - loss: 13.9111 - mse: 13.9111 - mape: 78.1235 - 31s/epoch - 2ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 30s - loss: 13.8906 - mse: 13.8906 - mape: 78.0096 - 30s/epoch - 2ms/step\n",
            "Score for fold 1: loss of 28.02040672302246\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 31s - loss: 13.8731 - mse: 13.8731 - mape: 78.0185 - 31s/epoch - 2ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 31s - loss: 13.8591 - mse: 13.8591 - mape: 77.8973 - 31s/epoch - 2ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 31s - loss: 13.8465 - mse: 13.8465 - mape: 77.8293 - 31s/epoch - 2ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 31s - loss: 13.8323 - mse: 13.8323 - mape: 77.7786 - 31s/epoch - 2ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 31s - loss: 13.8197 - mse: 13.8197 - mape: 77.7890 - 31s/epoch - 2ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 31s - loss: 13.8104 - mse: 13.8104 - mape: 77.7242 - 31s/epoch - 2ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 31s - loss: 13.8010 - mse: 13.8010 - mape: 77.6370 - 31s/epoch - 2ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 31s - loss: 13.7914 - mse: 13.7914 - mape: 77.6541 - 31s/epoch - 2ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 31s - loss: 13.7806 - mse: 13.7806 - mape: 77.5610 - 31s/epoch - 2ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 31s - loss: 13.7718 - mse: 13.7718 - mape: 77.6299 - 31s/epoch - 2ms/step\n",
            "Score for fold 2: loss of 18.858779907226562\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 31s - loss: 13.7656 - mse: 13.7656 - mape: 77.5359 - 31s/epoch - 2ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 31s - loss: 13.7571 - mse: 13.7571 - mape: 77.5253 - 31s/epoch - 2ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 31s - loss: 13.7495 - mse: 13.7495 - mape: 77.4258 - 31s/epoch - 2ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 32s - loss: 13.7398 - mse: 13.7398 - mape: 77.4892 - 32s/epoch - 2ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 33s - loss: 13.7318 - mse: 13.7318 - mape: 77.4393 - 33s/epoch - 2ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 32s - loss: 13.7252 - mse: 13.7252 - mape: 77.4144 - 32s/epoch - 2ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 32s - loss: 13.7179 - mse: 13.7179 - mape: 77.4291 - 32s/epoch - 2ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 31s - loss: 13.7092 - mse: 13.7092 - mape: 77.3210 - 31s/epoch - 2ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 31s - loss: 13.7016 - mse: 13.7016 - mape: 77.3892 - 31s/epoch - 2ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 30s - loss: 13.6962 - mse: 13.6962 - mape: 77.3403 - 30s/epoch - 2ms/step\n",
            "Score for fold 3: loss of 18.746795654296875\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-09-27 19:54:48,557]\u001b[0m Finished trial#40 resulted in value: 13.69624137878418. Current best value is 12.880197525024414 with parameters: {'activation': 'tanh', 'optimizer': 'adam', 'num_hidden_layer': 5, 'num_hidden_unit': 59, 'learning_rate': 0.00018586446397436457}.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 33s - loss: 14.2109 - mse: 14.2109 - mape: 79.9638 - 33s/epoch - 2ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 32s - loss: 13.9343 - mse: 13.9343 - mape: 78.7088 - 32s/epoch - 2ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 33s - loss: 13.8112 - mse: 13.8112 - mape: 78.3001 - 33s/epoch - 2ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 32s - loss: 13.7152 - mse: 13.7152 - mape: 78.0157 - 32s/epoch - 2ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 32s - loss: 13.6505 - mse: 13.6505 - mape: 77.7352 - 32s/epoch - 2ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 32s - loss: 13.5886 - mse: 13.5886 - mape: 77.7462 - 32s/epoch - 2ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 31s - loss: 13.5079 - mse: 13.5079 - mape: 77.7920 - 31s/epoch - 2ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 32s - loss: 13.5175 - mse: 13.5175 - mape: 77.6215 - 32s/epoch - 2ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 32s - loss: 13.4845 - mse: 13.4845 - mape: 77.6572 - 32s/epoch - 2ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 32s - loss: 13.4195 - mse: 13.4195 - mape: 77.4831 - 32s/epoch - 2ms/step\n",
            "Score for fold 1: loss of 28.399295806884766\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 32s - loss: 13.4073 - mse: 13.4073 - mape: 77.4540 - 32s/epoch - 2ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 32s - loss: 13.3627 - mse: 13.3627 - mape: 77.4060 - 32s/epoch - 2ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 32s - loss: 13.3041 - mse: 13.3041 - mape: 77.3722 - 32s/epoch - 2ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 32s - loss: 13.2839 - mse: 13.2839 - mape: 77.3806 - 32s/epoch - 2ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 32s - loss: 13.2433 - mse: 13.2433 - mape: 77.3926 - 32s/epoch - 2ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 32s - loss: 13.2043 - mse: 13.2043 - mape: 77.2944 - 32s/epoch - 2ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 32s - loss: 13.1503 - mse: 13.1503 - mape: 77.3367 - 32s/epoch - 2ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 33s - loss: 13.1672 - mse: 13.1672 - mape: 77.3320 - 33s/epoch - 2ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 32s - loss: 13.2075 - mse: 13.2075 - mape: 77.3195 - 32s/epoch - 2ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 32s - loss: 13.1443 - mse: 13.1443 - mape: 77.2128 - 32s/epoch - 2ms/step\n",
            "Score for fold 2: loss of 18.42840576171875\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 32s - loss: 13.0591 - mse: 13.0591 - mape: 77.2029 - 32s/epoch - 2ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 31s - loss: 13.0792 - mse: 13.0792 - mape: 77.2484 - 31s/epoch - 2ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 31s - loss: 13.0227 - mse: 13.0227 - mape: 77.1647 - 31s/epoch - 2ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 32s - loss: 12.9878 - mse: 12.9878 - mape: 77.1858 - 32s/epoch - 2ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 31s - loss: 13.0314 - mse: 13.0314 - mape: 77.1899 - 31s/epoch - 2ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 31s - loss: 13.0175 - mse: 13.0175 - mape: 77.1847 - 31s/epoch - 2ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 31s - loss: 12.9474 - mse: 12.9474 - mape: 77.2631 - 31s/epoch - 2ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 31s - loss: 12.9742 - mse: 12.9742 - mape: 77.1537 - 31s/epoch - 2ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 31s - loss: 12.8569 - mse: 12.8569 - mape: 77.0811 - 31s/epoch - 2ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 31s - loss: 12.9029 - mse: 12.9029 - mape: 77.0386 - 31s/epoch - 2ms/step\n",
            "Score for fold 3: loss of 18.21017074584961\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-09-27 20:10:53,729]\u001b[0m Finished trial#41 resulted in value: 12.902924537658691. Current best value is 12.880197525024414 with parameters: {'activation': 'tanh', 'optimizer': 'adam', 'num_hidden_layer': 5, 'num_hidden_unit': 59, 'learning_rate': 0.00018586446397436457}.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 32s - loss: 14.1877 - mse: 14.1877 - mape: 79.4392 - 32s/epoch - 2ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 32s - loss: 13.8671 - mse: 13.8671 - mape: 78.3237 - 32s/epoch - 2ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 32s - loss: 13.7324 - mse: 13.7324 - mape: 78.0098 - 32s/epoch - 2ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 33s - loss: 13.6313 - mse: 13.6313 - mape: 77.7176 - 33s/epoch - 2ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 33s - loss: 13.5630 - mse: 13.5630 - mape: 77.5085 - 33s/epoch - 2ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 33s - loss: 13.5191 - mse: 13.5191 - mape: 77.4327 - 33s/epoch - 2ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 33s - loss: 13.4531 - mse: 13.4531 - mape: 77.3631 - 33s/epoch - 2ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 33s - loss: 13.4168 - mse: 13.4168 - mape: 77.3393 - 33s/epoch - 2ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 34s - loss: 13.3495 - mse: 13.3495 - mape: 77.2847 - 34s/epoch - 2ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 34s - loss: 13.3169 - mse: 13.3169 - mape: 77.1737 - 34s/epoch - 2ms/step\n",
            "Score for fold 1: loss of 28.629074096679688\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 35s - loss: 13.2800 - mse: 13.2800 - mape: 77.1207 - 35s/epoch - 2ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 33s - loss: 13.1981 - mse: 13.1981 - mape: 76.9957 - 33s/epoch - 2ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 34s - loss: 13.1787 - mse: 13.1787 - mape: 77.0076 - 34s/epoch - 2ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 34s - loss: 13.1572 - mse: 13.1572 - mape: 76.8929 - 34s/epoch - 2ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 33s - loss: 13.0853 - mse: 13.0853 - mape: 76.9184 - 33s/epoch - 2ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 33s - loss: 13.0803 - mse: 13.0803 - mape: 76.9052 - 33s/epoch - 2ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 33s - loss: 13.0065 - mse: 13.0065 - mape: 76.7957 - 33s/epoch - 2ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 33s - loss: 12.9957 - mse: 12.9957 - mape: 76.8319 - 33s/epoch - 2ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 33s - loss: 12.9853 - mse: 12.9853 - mape: 76.7830 - 33s/epoch - 2ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 33s - loss: 12.9278 - mse: 12.9278 - mape: 76.7887 - 33s/epoch - 2ms/step\n",
            "Score for fold 2: loss of 18.350072860717773\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 33s - loss: 12.8890 - mse: 12.8890 - mape: 76.8244 - 33s/epoch - 2ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 33s - loss: 12.8404 - mse: 12.8404 - mape: 76.7187 - 33s/epoch - 2ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 32s - loss: 12.8077 - mse: 12.8077 - mape: 76.6773 - 32s/epoch - 2ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 33s - loss: 12.7968 - mse: 12.7968 - mape: 76.7676 - 33s/epoch - 2ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 32s - loss: 12.7208 - mse: 12.7208 - mape: 76.6810 - 32s/epoch - 2ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 32s - loss: 12.6944 - mse: 12.6944 - mape: 76.6619 - 32s/epoch - 2ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 32s - loss: 12.6602 - mse: 12.6602 - mape: 76.6185 - 32s/epoch - 2ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 32s - loss: 12.6172 - mse: 12.6172 - mape: 76.6656 - 32s/epoch - 2ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 32s - loss: 12.5974 - mse: 12.5974 - mape: 76.6858 - 32s/epoch - 2ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 32s - loss: 12.5543 - mse: 12.5543 - mape: 76.6695 - 32s/epoch - 2ms/step\n",
            "Score for fold 3: loss of 18.230514526367188\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-09-27 20:27:30,106]\u001b[0m Finished trial#42 resulted in value: 12.554261207580566. Current best value is 12.554261207580566 with parameters: {'activation': 'tanh', 'optimizer': 'adam', 'num_hidden_layer': 5, 'num_hidden_unit': 68, 'learning_rate': 0.00036577231287438313}.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 33s - loss: 14.3402 - mse: 14.3402 - mape: 79.7210 - 33s/epoch - 2ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 32s - loss: 13.9657 - mse: 13.9657 - mape: 78.4443 - 32s/epoch - 2ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 33s - loss: 13.8629 - mse: 13.8629 - mape: 78.0813 - 33s/epoch - 2ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 34s - loss: 13.7878 - mse: 13.7878 - mape: 77.7995 - 34s/epoch - 2ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 33s - loss: 13.7293 - mse: 13.7293 - mape: 77.5620 - 33s/epoch - 2ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 32s - loss: 13.6741 - mse: 13.6741 - mape: 77.4730 - 32s/epoch - 2ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 32s - loss: 13.6363 - mse: 13.6363 - mape: 77.3394 - 32s/epoch - 2ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 32s - loss: 13.6033 - mse: 13.6033 - mape: 77.2451 - 32s/epoch - 2ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 32s - loss: 13.5620 - mse: 13.5620 - mape: 77.1414 - 32s/epoch - 2ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 31s - loss: 13.5347 - mse: 13.5347 - mape: 77.1137 - 31s/epoch - 2ms/step\n",
            "Score for fold 1: loss of 28.131990432739258\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 33s - loss: 13.4960 - mse: 13.4960 - mape: 77.0618 - 33s/epoch - 2ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 33s - loss: 13.4672 - mse: 13.4672 - mape: 76.9955 - 33s/epoch - 2ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 33s - loss: 13.4417 - mse: 13.4417 - mape: 76.9447 - 33s/epoch - 2ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 33s - loss: 13.4072 - mse: 13.4072 - mape: 76.8843 - 33s/epoch - 2ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 33s - loss: 13.3898 - mse: 13.3898 - mape: 76.8990 - 33s/epoch - 2ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 33s - loss: 13.3552 - mse: 13.3552 - mape: 76.8176 - 33s/epoch - 2ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 32s - loss: 13.3281 - mse: 13.3281 - mape: 76.7668 - 32s/epoch - 2ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 32s - loss: 13.3172 - mse: 13.3172 - mape: 76.7518 - 32s/epoch - 2ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 32s - loss: 13.2738 - mse: 13.2738 - mape: 76.7556 - 32s/epoch - 2ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 31s - loss: 13.2717 - mse: 13.2717 - mape: 76.7340 - 31s/epoch - 2ms/step\n",
            "Score for fold 2: loss of 18.429611206054688\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 32s - loss: 13.2372 - mse: 13.2372 - mape: 76.6877 - 32s/epoch - 2ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 31s - loss: 13.2091 - mse: 13.2091 - mape: 76.7017 - 31s/epoch - 2ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 31s - loss: 13.1936 - mse: 13.1936 - mape: 76.6872 - 31s/epoch - 2ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 32s - loss: 13.1709 - mse: 13.1709 - mape: 76.6243 - 32s/epoch - 2ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 32s - loss: 13.1389 - mse: 13.1389 - mape: 76.5850 - 32s/epoch - 2ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 32s - loss: 13.1178 - mse: 13.1178 - mape: 76.6159 - 32s/epoch - 2ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 32s - loss: 13.1034 - mse: 13.1034 - mape: 76.5987 - 32s/epoch - 2ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 31s - loss: 13.0795 - mse: 13.0795 - mape: 76.5375 - 31s/epoch - 2ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 31s - loss: 13.0514 - mse: 13.0514 - mape: 76.4946 - 31s/epoch - 2ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 32s - loss: 13.0273 - mse: 13.0273 - mape: 76.4883 - 32s/epoch - 2ms/step\n",
            "Score for fold 3: loss of 18.23187828063965\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-09-27 20:43:45,317]\u001b[0m Finished trial#43 resulted in value: 13.02727222442627. Current best value is 12.554261207580566 with parameters: {'activation': 'tanh', 'optimizer': 'adam', 'num_hidden_layer': 5, 'num_hidden_unit': 68, 'learning_rate': 0.00036577231287438313}.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 32s - loss: 14.3874 - mse: 14.3874 - mape: 80.0699 - 32s/epoch - 2ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 31s - loss: 13.9802 - mse: 13.9802 - mape: 78.5273 - 31s/epoch - 2ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 32s - loss: 13.8979 - mse: 13.8979 - mape: 78.2440 - 32s/epoch - 2ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 31s - loss: 13.8393 - mse: 13.8393 - mape: 78.0217 - 31s/epoch - 2ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 31s - loss: 13.7835 - mse: 13.7835 - mape: 77.8189 - 31s/epoch - 2ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 31s - loss: 13.7202 - mse: 13.7202 - mape: 77.6402 - 31s/epoch - 2ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 32s - loss: 13.6749 - mse: 13.6749 - mape: 77.4549 - 32s/epoch - 2ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 31s - loss: 13.6252 - mse: 13.6252 - mape: 77.3276 - 31s/epoch - 2ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 32s - loss: 13.5810 - mse: 13.5810 - mape: 77.2633 - 32s/epoch - 2ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 32s - loss: 13.5451 - mse: 13.5451 - mape: 77.1533 - 32s/epoch - 2ms/step\n",
            "Score for fold 1: loss of 28.396957397460938\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 32s - loss: 13.5164 - mse: 13.5164 - mape: 77.1303 - 32s/epoch - 2ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 32s - loss: 13.4872 - mse: 13.4872 - mape: 77.0191 - 32s/epoch - 2ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 32s - loss: 13.4534 - mse: 13.4534 - mape: 77.0035 - 32s/epoch - 2ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 32s - loss: 13.4193 - mse: 13.4193 - mape: 76.9710 - 32s/epoch - 2ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 32s - loss: 13.3870 - mse: 13.3870 - mape: 76.9000 - 32s/epoch - 2ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 32s - loss: 13.3578 - mse: 13.3578 - mape: 76.8504 - 32s/epoch - 2ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 32s - loss: 13.3352 - mse: 13.3352 - mape: 76.8323 - 32s/epoch - 2ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 32s - loss: 13.3064 - mse: 13.3064 - mape: 76.7930 - 32s/epoch - 2ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 32s - loss: 13.2801 - mse: 13.2801 - mape: 76.7651 - 32s/epoch - 2ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 32s - loss: 13.2673 - mse: 13.2673 - mape: 76.7136 - 32s/epoch - 2ms/step\n",
            "Score for fold 2: loss of 18.60749626159668\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 32s - loss: 13.2421 - mse: 13.2421 - mape: 76.7181 - 32s/epoch - 2ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 32s - loss: 13.2145 - mse: 13.2145 - mape: 76.6965 - 32s/epoch - 2ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 32s - loss: 13.1848 - mse: 13.1848 - mape: 76.6268 - 32s/epoch - 2ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 32s - loss: 13.1610 - mse: 13.1610 - mape: 76.6090 - 32s/epoch - 2ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 32s - loss: 13.1346 - mse: 13.1346 - mape: 76.6535 - 32s/epoch - 2ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 32s - loss: 13.1263 - mse: 13.1263 - mape: 76.5768 - 32s/epoch - 2ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 32s - loss: 13.1023 - mse: 13.1023 - mape: 76.5895 - 32s/epoch - 2ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 32s - loss: 13.0835 - mse: 13.0835 - mape: 76.5319 - 32s/epoch - 2ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 34s - loss: 13.0628 - mse: 13.0628 - mape: 76.5710 - 34s/epoch - 2ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 34s - loss: 13.0492 - mse: 13.0492 - mape: 76.4860 - 34s/epoch - 2ms/step\n",
            "Score for fold 3: loss of 18.180479049682617\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-09-27 20:59:54,783]\u001b[0m Finished trial#44 resulted in value: 13.04922866821289. Current best value is 12.554261207580566 with parameters: {'activation': 'tanh', 'optimizer': 'adam', 'num_hidden_layer': 5, 'num_hidden_unit': 68, 'learning_rate': 0.00036577231287438313}.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 29s - loss: 14.7836 - mse: 14.7836 - mape: 77.2842 - 29s/epoch - 1ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 28s - loss: 14.1624 - mse: 14.1624 - mape: 78.6507 - 28s/epoch - 1ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 28s - loss: 14.0742 - mse: 14.0742 - mape: 78.5746 - 28s/epoch - 1ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 28s - loss: 14.0234 - mse: 14.0234 - mape: 78.6588 - 28s/epoch - 1ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 28s - loss: 13.9890 - mse: 13.9890 - mape: 78.5705 - 28s/epoch - 1ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 28s - loss: 13.9657 - mse: 13.9657 - mape: 78.4621 - 28s/epoch - 1ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 29s - loss: 13.9452 - mse: 13.9452 - mape: 78.4417 - 29s/epoch - 1ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 28s - loss: 13.9286 - mse: 13.9286 - mape: 78.3917 - 28s/epoch - 1ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 29s - loss: 13.9120 - mse: 13.9120 - mape: 78.2936 - 29s/epoch - 1ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 29s - loss: 13.8972 - mse: 13.8972 - mape: 78.1853 - 29s/epoch - 1ms/step\n",
            "Score for fold 1: loss of 27.310544967651367\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 28s - loss: 13.8850 - mse: 13.8850 - mape: 78.1406 - 28s/epoch - 1ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 29s - loss: 13.8731 - mse: 13.8731 - mape: 78.0886 - 29s/epoch - 1ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 29s - loss: 13.8610 - mse: 13.8610 - mape: 77.9827 - 29s/epoch - 1ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 29s - loss: 13.8505 - mse: 13.8505 - mape: 77.9388 - 29s/epoch - 1ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 29s - loss: 13.8401 - mse: 13.8401 - mape: 77.8848 - 29s/epoch - 1ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 28s - loss: 13.8307 - mse: 13.8307 - mape: 77.8616 - 28s/epoch - 1ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 28s - loss: 13.8217 - mse: 13.8217 - mape: 77.8254 - 28s/epoch - 1ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 28s - loss: 13.8133 - mse: 13.8133 - mape: 77.7980 - 28s/epoch - 1ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 28s - loss: 13.8067 - mse: 13.8067 - mape: 77.6979 - 28s/epoch - 1ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 29s - loss: 13.7988 - mse: 13.7988 - mape: 77.6700 - 29s/epoch - 1ms/step\n",
            "Score for fold 2: loss of 18.851335525512695\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 28s - loss: 13.7932 - mse: 13.7932 - mape: 77.6540 - 28s/epoch - 1ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 28s - loss: 13.7888 - mse: 13.7888 - mape: 77.6024 - 28s/epoch - 1ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 28s - loss: 13.7802 - mse: 13.7802 - mape: 77.6147 - 28s/epoch - 1ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 28s - loss: 13.7754 - mse: 13.7754 - mape: 77.5193 - 28s/epoch - 1ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 28s - loss: 13.7724 - mse: 13.7724 - mape: 77.5599 - 28s/epoch - 1ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 28s - loss: 13.7688 - mse: 13.7688 - mape: 77.5080 - 28s/epoch - 1ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 28s - loss: 13.7634 - mse: 13.7634 - mape: 77.4987 - 28s/epoch - 1ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 29s - loss: 13.7604 - mse: 13.7604 - mape: 77.5068 - 29s/epoch - 1ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 29s - loss: 13.7559 - mse: 13.7559 - mape: 77.4891 - 29s/epoch - 1ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 29s - loss: 13.7503 - mse: 13.7503 - mape: 77.4448 - 29s/epoch - 1ms/step\n",
            "Score for fold 3: loss of 18.753524780273438\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-09-27 21:14:18,088]\u001b[0m Finished trial#45 resulted in value: 13.750313758850098. Current best value is 12.554261207580566 with parameters: {'activation': 'tanh', 'optimizer': 'adam', 'num_hidden_layer': 5, 'num_hidden_unit': 68, 'learning_rate': 0.00036577231287438313}.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 33s - loss: 14.9265 - mse: 14.9265 - mape: 80.1145 - 33s/epoch - 2ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 32s - loss: 14.2376 - mse: 14.2376 - mape: 79.5557 - 32s/epoch - 2ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 32s - loss: 14.0937 - mse: 14.0937 - mape: 78.8476 - 32s/epoch - 2ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 33s - loss: 14.0075 - mse: 14.0075 - mape: 78.5537 - 33s/epoch - 2ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 33s - loss: 13.9457 - mse: 13.9457 - mape: 78.2715 - 33s/epoch - 2ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 33s - loss: 13.9026 - mse: 13.9026 - mape: 78.1682 - 33s/epoch - 2ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 32s - loss: 13.8741 - mse: 13.8741 - mape: 77.9852 - 32s/epoch - 2ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 33s - loss: 13.8497 - mse: 13.8497 - mape: 77.9404 - 33s/epoch - 2ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 33s - loss: 13.8345 - mse: 13.8345 - mape: 77.8483 - 33s/epoch - 2ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 33s - loss: 13.8083 - mse: 13.8083 - mape: 77.7808 - 33s/epoch - 2ms/step\n",
            "Score for fold 1: loss of 28.43315315246582\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 35s - loss: 13.7971 - mse: 13.7971 - mape: 77.7315 - 35s/epoch - 2ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 33s - loss: 13.7859 - mse: 13.7859 - mape: 77.6834 - 33s/epoch - 2ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 33s - loss: 13.7721 - mse: 13.7721 - mape: 77.6039 - 33s/epoch - 2ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 33s - loss: 13.7559 - mse: 13.7559 - mape: 77.5622 - 33s/epoch - 2ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 33s - loss: 13.7454 - mse: 13.7454 - mape: 77.4835 - 33s/epoch - 2ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 33s - loss: 13.7332 - mse: 13.7332 - mape: 77.4164 - 33s/epoch - 2ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 33s - loss: 13.7180 - mse: 13.7180 - mape: 77.3958 - 33s/epoch - 2ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 33s - loss: 13.7037 - mse: 13.7037 - mape: 77.2974 - 33s/epoch - 2ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 33s - loss: 13.6930 - mse: 13.6930 - mape: 77.2616 - 33s/epoch - 2ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 34s - loss: 13.6827 - mse: 13.6827 - mape: 77.2338 - 34s/epoch - 2ms/step\n",
            "Score for fold 2: loss of 18.770790100097656\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 33s - loss: 13.6724 - mse: 13.6724 - mape: 77.1391 - 33s/epoch - 2ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 33s - loss: 13.6617 - mse: 13.6617 - mape: 77.1497 - 33s/epoch - 2ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 33s - loss: 13.6502 - mse: 13.6502 - mape: 77.1125 - 33s/epoch - 2ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 33s - loss: 13.6359 - mse: 13.6359 - mape: 77.0723 - 33s/epoch - 2ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 33s - loss: 13.6324 - mse: 13.6324 - mape: 77.0347 - 33s/epoch - 2ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 33s - loss: 13.6199 - mse: 13.6199 - mape: 76.9858 - 33s/epoch - 2ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 33s - loss: 13.6127 - mse: 13.6127 - mape: 76.9739 - 33s/epoch - 2ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 33s - loss: 13.5998 - mse: 13.5998 - mape: 76.9559 - 33s/epoch - 2ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 34s - loss: 13.5916 - mse: 13.5916 - mape: 76.9550 - 34s/epoch - 2ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 33s - loss: 13.5810 - mse: 13.5810 - mape: 76.9173 - 33s/epoch - 2ms/step\n",
            "Score for fold 3: loss of 18.62843132019043\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-09-27 21:30:59,577]\u001b[0m Finished trial#46 resulted in value: 13.581049919128418. Current best value is 12.554261207580566 with parameters: {'activation': 'tanh', 'optimizer': 'adam', 'num_hidden_layer': 5, 'num_hidden_unit': 68, 'learning_rate': 0.00036577231287438313}.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 36s - loss: 14.2729 - mse: 14.2729 - mape: 79.4525 - 36s/epoch - 2ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 35s - loss: 13.9384 - mse: 13.9384 - mape: 78.5081 - 35s/epoch - 2ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 37s - loss: 13.8493 - mse: 13.8493 - mape: 78.2098 - 37s/epoch - 2ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 35s - loss: 13.7792 - mse: 13.7792 - mape: 77.9593 - 35s/epoch - 2ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 34s - loss: 13.7061 - mse: 13.7061 - mape: 77.7004 - 34s/epoch - 2ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 34s - loss: 13.6455 - mse: 13.6455 - mape: 77.5212 - 34s/epoch - 2ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 35s - loss: 13.5972 - mse: 13.5972 - mape: 77.3785 - 35s/epoch - 2ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 34s - loss: 13.5424 - mse: 13.5424 - mape: 77.2210 - 34s/epoch - 2ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 33s - loss: 13.4993 - mse: 13.4993 - mape: 77.0613 - 33s/epoch - 2ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 33s - loss: 13.4641 - mse: 13.4641 - mape: 77.0451 - 33s/epoch - 2ms/step\n",
            "Score for fold 1: loss of 28.488693237304688\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 33s - loss: 13.4100 - mse: 13.4100 - mape: 76.8519 - 33s/epoch - 2ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 33s - loss: 13.3804 - mse: 13.3804 - mape: 76.9127 - 33s/epoch - 2ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 33s - loss: 13.3358 - mse: 13.3358 - mape: 76.8205 - 33s/epoch - 2ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 33s - loss: 13.2893 - mse: 13.2893 - mape: 76.7209 - 33s/epoch - 2ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 34s - loss: 13.2692 - mse: 13.2692 - mape: 76.7290 - 34s/epoch - 2ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 33s - loss: 13.2412 - mse: 13.2412 - mape: 76.7005 - 33s/epoch - 2ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 33s - loss: 13.1933 - mse: 13.1933 - mape: 76.6599 - 33s/epoch - 2ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 32s - loss: 13.1801 - mse: 13.1801 - mape: 76.6303 - 32s/epoch - 2ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 32s - loss: 13.1480 - mse: 13.1480 - mape: 76.6146 - 32s/epoch - 2ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 32s - loss: 13.1169 - mse: 13.1169 - mape: 76.5997 - 32s/epoch - 2ms/step\n",
            "Score for fold 2: loss of 18.35991859436035\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 32s - loss: 13.0723 - mse: 13.0723 - mape: 76.5569 - 32s/epoch - 2ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 32s - loss: 13.0697 - mse: 13.0697 - mape: 76.5735 - 32s/epoch - 2ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 32s - loss: 13.0281 - mse: 13.0281 - mape: 76.5388 - 32s/epoch - 2ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 32s - loss: 13.0083 - mse: 13.0083 - mape: 76.5035 - 32s/epoch - 2ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 32s - loss: 12.9688 - mse: 12.9688 - mape: 76.4634 - 32s/epoch - 2ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 32s - loss: 12.9744 - mse: 12.9744 - mape: 76.4701 - 32s/epoch - 2ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 33s - loss: 12.9312 - mse: 12.9312 - mape: 76.4042 - 33s/epoch - 2ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 32s - loss: 12.9077 - mse: 12.9077 - mape: 76.4264 - 32s/epoch - 2ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 32s - loss: 12.8831 - mse: 12.8831 - mape: 76.3845 - 32s/epoch - 2ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 32s - loss: 12.8578 - mse: 12.8578 - mape: 76.3699 - 32s/epoch - 2ms/step\n",
            "Score for fold 3: loss of 18.130189895629883\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-09-27 21:47:50,527]\u001b[0m Finished trial#47 resulted in value: 12.857813835144043. Current best value is 12.554261207580566 with parameters: {'activation': 'tanh', 'optimizer': 'adam', 'num_hidden_layer': 5, 'num_hidden_unit': 68, 'learning_rate': 0.00036577231287438313}.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 32s - loss: 15.2360 - mse: 15.2360 - mape: 80.0052 - 32s/epoch - 2ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 32s - loss: 14.3695 - mse: 14.3695 - mape: 80.1033 - 32s/epoch - 2ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 32s - loss: 14.2167 - mse: 14.2167 - mape: 79.6103 - 32s/epoch - 2ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 32s - loss: 14.1183 - mse: 14.1183 - mape: 79.1894 - 32s/epoch - 2ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 32s - loss: 14.0533 - mse: 14.0533 - mape: 78.9209 - 32s/epoch - 2ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 32s - loss: 14.0039 - mse: 14.0039 - mape: 78.6484 - 32s/epoch - 2ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 32s - loss: 13.9697 - mse: 13.9697 - mape: 78.4300 - 32s/epoch - 2ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 32s - loss: 13.9395 - mse: 13.9395 - mape: 78.2842 - 32s/epoch - 2ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 32s - loss: 13.9148 - mse: 13.9148 - mape: 78.1720 - 32s/epoch - 2ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 32s - loss: 13.8938 - mse: 13.8938 - mape: 78.0706 - 32s/epoch - 2ms/step\n",
            "Score for fold 1: loss of 28.573230743408203\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 32s - loss: 13.8755 - mse: 13.8755 - mape: 77.9723 - 32s/epoch - 2ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 32s - loss: 13.8622 - mse: 13.8622 - mape: 77.9243 - 32s/epoch - 2ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 32s - loss: 13.8508 - mse: 13.8508 - mape: 77.8774 - 32s/epoch - 2ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 33s - loss: 13.8394 - mse: 13.8394 - mape: 77.7972 - 33s/epoch - 2ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 33s - loss: 13.8265 - mse: 13.8265 - mape: 77.8023 - 33s/epoch - 2ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 33s - loss: 13.8177 - mse: 13.8177 - mape: 77.7455 - 33s/epoch - 2ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 32s - loss: 13.8093 - mse: 13.8093 - mape: 77.7170 - 32s/epoch - 2ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 32s - loss: 13.8005 - mse: 13.8005 - mape: 77.6755 - 32s/epoch - 2ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 33s - loss: 13.7915 - mse: 13.7915 - mape: 77.6543 - 33s/epoch - 2ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 34s - loss: 13.7851 - mse: 13.7851 - mape: 77.6372 - 34s/epoch - 2ms/step\n",
            "Score for fold 2: loss of 18.880855560302734\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 34s - loss: 13.7747 - mse: 13.7747 - mape: 77.6216 - 34s/epoch - 2ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 36s - loss: 13.7671 - mse: 13.7671 - mape: 77.6088 - 36s/epoch - 2ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 37s - loss: 13.7622 - mse: 13.7622 - mape: 77.5649 - 37s/epoch - 2ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 36s - loss: 13.7549 - mse: 13.7549 - mape: 77.5109 - 36s/epoch - 2ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 36s - loss: 13.7475 - mse: 13.7475 - mape: 77.5180 - 36s/epoch - 2ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 34s - loss: 13.7416 - mse: 13.7416 - mape: 77.4735 - 34s/epoch - 2ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 34s - loss: 13.7355 - mse: 13.7355 - mape: 77.4709 - 34s/epoch - 2ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 34s - loss: 13.7289 - mse: 13.7289 - mape: 77.4320 - 34s/epoch - 2ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 35s - loss: 13.7209 - mse: 13.7209 - mape: 77.4626 - 35s/epoch - 2ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 34s - loss: 13.7162 - mse: 13.7162 - mape: 77.3829 - 34s/epoch - 2ms/step\n",
            "Score for fold 3: loss of 18.7497615814209\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-09-27 22:04:35,016]\u001b[0m Finished trial#48 resulted in value: 13.716154098510742. Current best value is 12.554261207580566 with parameters: {'activation': 'tanh', 'optimizer': 'adam', 'num_hidden_layer': 5, 'num_hidden_unit': 68, 'learning_rate': 0.00036577231287438313}.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 33s - loss: 14.6964 - mse: 14.6964 - mape: 80.1054 - 33s/epoch - 2ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 33s - loss: 14.1313 - mse: 14.1313 - mape: 79.0275 - 33s/epoch - 2ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 34s - loss: 14.0052 - mse: 14.0052 - mape: 78.5805 - 34s/epoch - 2ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 34s - loss: 13.9346 - mse: 13.9346 - mape: 78.2646 - 34s/epoch - 2ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 33s - loss: 13.8907 - mse: 13.8907 - mape: 78.1063 - 33s/epoch - 2ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 33s - loss: 13.8602 - mse: 13.8602 - mape: 77.9916 - 33s/epoch - 2ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 33s - loss: 13.8381 - mse: 13.8381 - mape: 77.8805 - 33s/epoch - 2ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 32s - loss: 13.8107 - mse: 13.8107 - mape: 77.8492 - 32s/epoch - 2ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 32s - loss: 13.7895 - mse: 13.7895 - mape: 77.7222 - 32s/epoch - 2ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 31s - loss: 13.7676 - mse: 13.7677 - mape: 77.5870 - 31s/epoch - 2ms/step\n",
            "Score for fold 1: loss of 28.38389015197754\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 32s - loss: 13.7443 - mse: 13.7443 - mape: 77.5518 - 32s/epoch - 2ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 31s - loss: 13.7242 - mse: 13.7242 - mape: 77.4331 - 31s/epoch - 2ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 32s - loss: 13.7070 - mse: 13.7070 - mape: 77.3835 - 32s/epoch - 2ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 31s - loss: 13.6879 - mse: 13.6879 - mape: 77.3425 - 31s/epoch - 2ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 31s - loss: 13.6673 - mse: 13.6673 - mape: 77.2542 - 31s/epoch - 2ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 31s - loss: 13.6499 - mse: 13.6499 - mape: 77.2316 - 31s/epoch - 2ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 31s - loss: 13.6346 - mse: 13.6346 - mape: 77.1945 - 31s/epoch - 2ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 31s - loss: 13.6178 - mse: 13.6178 - mape: 77.1615 - 31s/epoch - 2ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 31s - loss: 13.6061 - mse: 13.6061 - mape: 77.1095 - 31s/epoch - 2ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 31s - loss: 13.5984 - mse: 13.5984 - mape: 77.0710 - 31s/epoch - 2ms/step\n",
            "Score for fold 2: loss of 18.691930770874023\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 31s - loss: 13.5793 - mse: 13.5793 - mape: 77.0555 - 31s/epoch - 2ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 31s - loss: 13.5697 - mse: 13.5697 - mape: 77.0320 - 31s/epoch - 2ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 32s - loss: 13.5512 - mse: 13.5512 - mape: 76.9852 - 32s/epoch - 2ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 32s - loss: 13.5389 - mse: 13.5389 - mape: 76.9160 - 32s/epoch - 2ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 32s - loss: 13.5319 - mse: 13.5319 - mape: 76.9752 - 32s/epoch - 2ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 33s - loss: 13.5132 - mse: 13.5132 - mape: 76.8465 - 33s/epoch - 2ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 33s - loss: 13.5049 - mse: 13.5049 - mape: 76.8681 - 33s/epoch - 2ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 33s - loss: 13.4925 - mse: 13.4925 - mape: 76.8386 - 33s/epoch - 2ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 33s - loss: 13.4816 - mse: 13.4816 - mape: 76.8115 - 33s/epoch - 2ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 31s - loss: 13.4613 - mse: 13.4613 - mape: 76.7829 - 31s/epoch - 2ms/step\n",
            "Score for fold 3: loss of 18.55854034423828\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-09-27 22:20:50,043]\u001b[0m Finished trial#49 resulted in value: 13.46130657196045. Current best value is 12.554261207580566 with parameters: {'activation': 'tanh', 'optimizer': 'adam', 'num_hidden_layer': 5, 'num_hidden_unit': 68, 'learning_rate': 0.00036577231287438313}.\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "model_list=[]\n",
        "history_list=[]\n",
        "study_name = 'NN_study'\n",
        "study = optuna.create_study(study_name=study_name, load_if_exists=True)\n",
        "#15.1\n",
        "study.optimize(objective, n_trials=50, )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WJeAossUT_yj",
        "outputId": "7c3d9857-92df-4723-94ac-24c0e4bccbf6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_1 (InputLayer)        [(None, 11)]              0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 68)                816       \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 68)                4692      \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 68)                4692      \n",
            "                                                                 \n",
            " dense_3 (Dense)             (None, 68)                4692      \n",
            "                                                                 \n",
            " dense_4 (Dense)             (None, 1)                 69        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 14,961\n",
            "Trainable params: 14,961\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/20\n",
            "20000/20000 - 28s - loss: 14.2372 - mse: 14.2372 - mae: 1.6434 - 28s/epoch - 1ms/step\n",
            "Epoch 2/20\n",
            "20000/20000 - 28s - loss: 14.0261 - mse: 14.0261 - mae: 1.6281 - 28s/epoch - 1ms/step\n",
            "Epoch 3/20\n",
            "20000/20000 - 27s - loss: 13.8977 - mse: 13.8977 - mae: 1.6237 - 27s/epoch - 1ms/step\n",
            "Epoch 4/20\n",
            "20000/20000 - 27s - loss: 13.8392 - mse: 13.8392 - mae: 1.6180 - 27s/epoch - 1ms/step\n",
            "Epoch 5/20\n",
            "20000/20000 - 27s - loss: 13.7951 - mse: 13.7951 - mae: 1.6162 - 27s/epoch - 1ms/step\n",
            "Epoch 6/20\n",
            "20000/20000 - 28s - loss: 13.7237 - mse: 13.7237 - mae: 1.6158 - 28s/epoch - 1ms/step\n",
            "Epoch 7/20\n",
            "20000/20000 - 28s - loss: 13.6861 - mse: 13.6861 - mae: 1.6159 - 28s/epoch - 1ms/step\n",
            "Epoch 8/20\n",
            "20000/20000 - 27s - loss: 13.5766 - mse: 13.5766 - mae: 1.6123 - 27s/epoch - 1ms/step\n",
            "Epoch 9/20\n",
            "20000/20000 - 26s - loss: 13.5620 - mse: 13.5620 - mae: 1.6109 - 26s/epoch - 1ms/step\n",
            "Epoch 10/20\n",
            "20000/20000 - 25s - loss: 13.5287 - mse: 13.5287 - mae: 1.6092 - 25s/epoch - 1ms/step\n",
            "Epoch 11/20\n",
            "20000/20000 - 27s - loss: 13.5337 - mse: 13.5337 - mae: 1.6094 - 27s/epoch - 1ms/step\n",
            "Epoch 12/20\n",
            "20000/20000 - 28s - loss: 13.4460 - mse: 13.4460 - mae: 1.6074 - 28s/epoch - 1ms/step\n",
            "Epoch 13/20\n",
            "20000/20000 - 29s - loss: 13.4666 - mse: 13.4666 - mae: 1.6062 - 29s/epoch - 1ms/step\n",
            "Epoch 14/20\n",
            "20000/20000 - 27s - loss: 13.4395 - mse: 13.4395 - mae: 1.6074 - 27s/epoch - 1ms/step\n",
            "Epoch 15/20\n",
            "20000/20000 - 27s - loss: 13.4180 - mse: 13.4180 - mae: 1.6068 - 27s/epoch - 1ms/step\n",
            "Epoch 16/20\n",
            "20000/20000 - 28s - loss: 13.3633 - mse: 13.3633 - mae: 1.6042 - 28s/epoch - 1ms/step\n",
            "Epoch 17/20\n",
            "20000/20000 - 27s - loss: 13.3610 - mse: 13.3610 - mae: 1.6058 - 27s/epoch - 1ms/step\n",
            "Epoch 18/20\n",
            "20000/20000 - 27s - loss: 13.3743 - mse: 13.3743 - mae: 1.6054 - 27s/epoch - 1ms/step\n",
            "Epoch 19/20\n",
            "20000/20000 - 28s - loss: 13.4175 - mse: 13.4175 - mae: 1.6065 - 28s/epoch - 1ms/step\n",
            "Epoch 20/20\n",
            "20000/20000 - 27s - loss: 13.3744 - mse: 13.3744 - mae: 1.6049 - 27s/epoch - 1ms/step\n",
            "Score for fold 1: loss of 29.09212303161621\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/20\n",
            "20000/20000 - 26s - loss: 13.3811 - mse: 13.3811 - mae: 1.6040 - 26s/epoch - 1ms/step\n",
            "Epoch 2/20\n",
            "20000/20000 - 26s - loss: 13.3629 - mse: 13.3629 - mae: 1.6046 - 26s/epoch - 1ms/step\n",
            "Epoch 3/20\n",
            "20000/20000 - 26s - loss: 13.3743 - mse: 13.3743 - mae: 1.6054 - 26s/epoch - 1ms/step\n",
            "Epoch 4/20\n",
            "20000/20000 - 30s - loss: 13.3210 - mse: 13.3210 - mae: 1.6046 - 30s/epoch - 2ms/step\n",
            "Epoch 5/20\n",
            "20000/20000 - 30s - loss: 13.3416 - mse: 13.3416 - mae: 1.6037 - 30s/epoch - 1ms/step\n",
            "Epoch 6/20\n",
            "20000/20000 - 45s - loss: 13.3407 - mse: 13.3407 - mae: 1.6041 - 45s/epoch - 2ms/step\n",
            "Epoch 7/20\n",
            "20000/20000 - 50s - loss: 13.3588 - mse: 13.3588 - mae: 1.6054 - 50s/epoch - 2ms/step\n",
            "Epoch 8/20\n",
            "20000/20000 - 44s - loss: 13.2606 - mse: 13.2606 - mae: 1.6020 - 44s/epoch - 2ms/step\n",
            "Epoch 9/20\n",
            "20000/20000 - 56s - loss: 13.3373 - mse: 13.3373 - mae: 1.6049 - 56s/epoch - 3ms/step\n",
            "Epoch 10/20\n",
            "20000/20000 - 50s - loss: 13.2906 - mse: 13.2906 - mae: 1.6066 - 50s/epoch - 3ms/step\n",
            "Epoch 11/20\n",
            "20000/20000 - 50s - loss: 13.3233 - mse: 13.3233 - mae: 1.6053 - 50s/epoch - 2ms/step\n",
            "Epoch 12/20\n",
            "20000/20000 - 46s - loss: 13.2729 - mse: 13.2729 - mae: 1.6025 - 46s/epoch - 2ms/step\n",
            "Epoch 13/20\n",
            "20000/20000 - 45s - loss: 13.2432 - mse: 13.2432 - mae: 1.6021 - 45s/epoch - 2ms/step\n",
            "Epoch 14/20\n",
            "20000/20000 - 44s - loss: 13.3540 - mse: 13.3540 - mae: 1.6060 - 44s/epoch - 2ms/step\n",
            "Epoch 15/20\n",
            "20000/20000 - 41s - loss: 13.2950 - mse: 13.2950 - mae: 1.6056 - 41s/epoch - 2ms/step\n",
            "Epoch 16/20\n",
            "20000/20000 - 41s - loss: 13.2736 - mse: 13.2736 - mae: 1.6050 - 41s/epoch - 2ms/step\n",
            "Epoch 17/20\n",
            "20000/20000 - 40s - loss: 13.3322 - mse: 13.3322 - mae: 1.6053 - 40s/epoch - 2ms/step\n",
            "Epoch 18/20\n",
            "20000/20000 - 26s - loss: 13.2947 - mse: 13.2947 - mae: 1.6062 - 26s/epoch - 1ms/step\n",
            "Epoch 19/20\n",
            "20000/20000 - 26s - loss: 13.3384 - mse: 13.3384 - mae: 1.6064 - 26s/epoch - 1ms/step\n",
            "Epoch 20/20\n",
            "20000/20000 - 26s - loss: 13.3030 - mse: 13.3030 - mae: 1.6057 - 26s/epoch - 1ms/step\n",
            "Score for fold 2: loss of 18.15971565246582\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/20\n",
            "20000/20000 - 26s - loss: 13.3408 - mse: 13.3408 - mae: 1.6065 - 26s/epoch - 1ms/step\n",
            "Epoch 2/20\n",
            "20000/20000 - 26s - loss: 13.3195 - mse: 13.3195 - mae: 1.6074 - 26s/epoch - 1ms/step\n",
            "Epoch 3/20\n",
            "20000/20000 - 29s - loss: 13.2876 - mse: 13.2876 - mae: 1.6072 - 29s/epoch - 1ms/step\n",
            "Epoch 4/20\n",
            "20000/20000 - 28s - loss: 13.2663 - mse: 13.2663 - mae: 1.6057 - 28s/epoch - 1ms/step\n",
            "Epoch 5/20\n",
            "20000/20000 - 28s - loss: 13.1980 - mse: 13.1980 - mae: 1.6031 - 28s/epoch - 1ms/step\n",
            "Epoch 6/20\n",
            "20000/20000 - 28s - loss: 13.2284 - mse: 13.2284 - mae: 1.6045 - 28s/epoch - 1ms/step\n",
            "Epoch 7/20\n",
            "20000/20000 - 27s - loss: 13.1718 - mse: 13.1718 - mae: 1.6029 - 27s/epoch - 1ms/step\n",
            "Epoch 8/20\n",
            "20000/20000 - 34s - loss: 13.3114 - mse: 13.3114 - mae: 1.6062 - 34s/epoch - 2ms/step\n",
            "Epoch 9/20\n",
            "20000/20000 - 35s - loss: 13.2463 - mse: 13.2463 - mae: 1.6064 - 35s/epoch - 2ms/step\n",
            "Epoch 10/20\n",
            "20000/20000 - 38s - loss: 13.2669 - mse: 13.2669 - mae: 1.6060 - 38s/epoch - 2ms/step\n",
            "Epoch 11/20\n",
            "20000/20000 - 38s - loss: 13.2443 - mse: 13.2443 - mae: 1.6048 - 38s/epoch - 2ms/step\n",
            "Epoch 12/20\n",
            "20000/20000 - 38s - loss: 13.1767 - mse: 13.1767 - mae: 1.6043 - 38s/epoch - 2ms/step\n",
            "Epoch 13/20\n",
            "20000/20000 - 39s - loss: 13.1487 - mse: 13.1487 - mae: 1.6031 - 39s/epoch - 2ms/step\n",
            "Epoch 14/20\n",
            "20000/20000 - 39s - loss: 13.1808 - mse: 13.1808 - mae: 1.6022 - 39s/epoch - 2ms/step\n",
            "Epoch 15/20\n",
            "20000/20000 - 39s - loss: 13.1582 - mse: 13.1582 - mae: 1.6015 - 39s/epoch - 2ms/step\n",
            "Epoch 16/20\n",
            "20000/20000 - 39s - loss: 13.1635 - mse: 13.1635 - mae: 1.6012 - 39s/epoch - 2ms/step\n",
            "Epoch 17/20\n",
            "20000/20000 - 38s - loss: 13.3019 - mse: 13.3019 - mae: 1.6053 - 38s/epoch - 2ms/step\n",
            "Epoch 18/20\n",
            "20000/20000 - 37s - loss: 13.1582 - mse: 13.1582 - mae: 1.6017 - 37s/epoch - 2ms/step\n",
            "Epoch 19/20\n",
            "20000/20000 - 39s - loss: 13.1524 - mse: 13.1524 - mae: 1.6010 - 39s/epoch - 2ms/step\n",
            "Epoch 20/20\n",
            "20000/20000 - 39s - loss: 13.1275 - mse: 13.1275 - mae: 1.5990 - 39s/epoch - 2ms/step\n",
            "Score for fold 3: loss of 18.349903106689453\n"
          ]
        }
      ],
      "source": [
        "num_folds = 3\n",
        "kfold=KFold(n_splits=3,shuffle=True)\n",
        "#{'activation': 'tanh', 'optimizer': 'adam', 'num_hidden_layer': 5, 'num_hidden_unit': 68, 'learning_rate': 0.00036577231287438313}\n",
        "fold_no=1\n",
        "loss_per_fold = []\n",
        "es = EarlyStopping(monitor='mse', patience=50)\n",
        "model_nn_best = create_model(activation='tanh', num_hidden_layer=5, num_hidden_unit=68)\n",
        "model_nn_best.summary()\n",
        "model_nn_best.compile(loss='mse',optimizer='adam',metrics=['mse','mae'])\n",
        "for train,test in kfold.split(training,labelsForTrain):\n",
        "  scores=model_nn_best.evaluate(testing,labelsForTest,verbose=0)\n",
        "\n",
        "  print('------------------------------------------------------------------------')\n",
        "  print(f'Training for fold {fold_no} ...')\n",
        "  \n",
        "  # Fit data to model\n",
        "  history = model_nn_best.fit(training, labelsForTrain,\n",
        "              batch_size=20,\n",
        "              #The result from epoch 20 and epoch 200 don't make too much differenct\n",
        "              epochs=20,\n",
        "              verbose=2,)\n",
        "    \n",
        "  print(f'Score for fold {fold_no}: {model_nn_best.metrics_names[0]} of {scores[0]}')\n",
        "  loss_per_fold.append(scores[0])\n",
        "  # Increase fold number\n",
        "  fold_no = fold_no + 1  \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2eWGhrzeE3Yn",
        "outputId": "2d620134-1809-4ede-cf2a-d3d204dc51fb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "10000/10000 [==============================] - 20s 2ms/step - loss: 18.3104 - mse: 18.3104 - mae: 1.6508\n"
          ]
        }
      ],
      "source": [
        "results = model_nn_best.evaluate(testing, labelsForTest, batch_size=10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LbzfcvonjpV_",
        "outputId": "6a109270-0257-40e3-a5b6-862baba357a2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The Mean Square Error for test is:  18.344845788207742\n",
            "The Root Mean Square Error for test is:  4.2830883470000645\n"
          ]
        }
      ],
      "source": [
        "summation = 0  #variable to store the summation of differences\n",
        "test_predictions = model_nn_best.predict(testing).flatten()\n",
        "n = len(test_predictions) #finding total number of items in list\n",
        "for i in range (0,n):  #looping through each element of the list\n",
        "  difference = labelsForTest[i] - test_predictions[i]  #finding the difference between observed and predicted value\n",
        "  squared_difference = difference**2  #taking square of the differene \n",
        "  summation = summation + squared_difference  #taking a sum of all the differences\n",
        "MSE = summation/n  #dividing summation by total values to obtain average\n",
        "RMSE = math.sqrt(MSE)\n",
        "print (\"The Mean Square Error for test is: \" , MSE)\n",
        "print (\"The Root Mean Square Error for test is: \" , RMSE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RqRgSClzv_bR",
        "outputId": "c4c3a06e-4873-41c7-f6cd-80b037fa24cf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 24s - loss: 14.0593 - mse: 14.0593 - 24s/epoch - 1ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 24s - loss: 13.8439 - mse: 13.8439 - 24s/epoch - 1ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 24s - loss: 13.7533 - mse: 13.7533 - 24s/epoch - 1ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 24s - loss: 13.6438 - mse: 13.6438 - 24s/epoch - 1ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 24s - loss: 13.6091 - mse: 13.6091 - 24s/epoch - 1ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 24s - loss: 13.5218 - mse: 13.5218 - 24s/epoch - 1ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 24s - loss: 13.4684 - mse: 13.4684 - 24s/epoch - 1ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 24s - loss: 13.4305 - mse: 13.4305 - 24s/epoch - 1ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 23s - loss: 13.3944 - mse: 13.3944 - 23s/epoch - 1ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 24s - loss: 13.3802 - mse: 13.3802 - 24s/epoch - 1ms/step\n",
            "Score for fold 1: loss of 18.295801162719727\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 24s - loss: 13.3511 - mse: 13.3511 - 24s/epoch - 1ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 23s - loss: 13.3355 - mse: 13.3355 - 23s/epoch - 1ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 23s - loss: 13.3035 - mse: 13.3035 - 23s/epoch - 1ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 23s - loss: 13.3179 - mse: 13.3179 - 23s/epoch - 1ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 23s - loss: 13.2483 - mse: 13.2483 - 23s/epoch - 1ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 23s - loss: 13.2665 - mse: 13.2665 - 23s/epoch - 1ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 24s - loss: 13.2407 - mse: 13.2407 - 24s/epoch - 1ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 23s - loss: 13.2276 - mse: 13.2276 - 23s/epoch - 1ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 23s - loss: 13.2385 - mse: 13.2385 - 23s/epoch - 1ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 23s - loss: 13.2163 - mse: 13.2163 - 23s/epoch - 1ms/step\n",
            "Score for fold 2: loss of 18.295801162719727\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 23s - loss: 13.1662 - mse: 13.1662 - 23s/epoch - 1ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 23s - loss: 13.1905 - mse: 13.1905 - 23s/epoch - 1ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 23s - loss: 13.1912 - mse: 13.1912 - 23s/epoch - 1ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 24s - loss: 13.1198 - mse: 13.1198 - 24s/epoch - 1ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 23s - loss: 13.0970 - mse: 13.0970 - 23s/epoch - 1ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 23s - loss: 13.1043 - mse: 13.1043 - 23s/epoch - 1ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 23s - loss: 13.1381 - mse: 13.1381 - 23s/epoch - 1ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 23s - loss: 13.1162 - mse: 13.1162 - 23s/epoch - 1ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 23s - loss: 13.0933 - mse: 13.0933 - 23s/epoch - 1ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 23s - loss: 13.0947 - mse: 13.0947 - 23s/epoch - 1ms/step\n",
            "Score for fold 3: loss of 18.295801162719727\n"
          ]
        }
      ],
      "source": [
        "num_folds = 3\n",
        "kfold=KFold(n_splits=3,shuffle=True)\n",
        "fold_no=1\n",
        "loss_per_fold = []\n",
        "for train,test in kfold.split(training,labelsForTrain):\n",
        "  scores=model_list[43].evaluate(testing,labelsForTest,verbose=0)\n",
        "  model.compile(loss='mse',optimizer='adam',metrics=['mse'])\n",
        "\n",
        "  print('------------------------------------------------------------------------')\n",
        "  print(f'Training for fold {fold_no} ...')\n",
        " \n",
        "  # Fit data to model\n",
        "  history = model.fit(training, labelsForTrain,\n",
        "              batch_size=20,\n",
        "              epochs=10,\n",
        "              verbose=2)\n",
        "  \n",
        "  print(f'Score for fold {fold_no}: {model.metrics_names[0]} of {scores[0]}')\n",
        "  loss_per_fold.append(scores[0])\n",
        "  # Increase fold number\n",
        "  fold_no = fold_no + 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0l_f_SK4ZoMc",
        "outputId": "300c1b80-3200-400f-b5d1-a5d35402a4d3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'activation': 'relu', 'optimizer': 'adam', 'num_hidden_layer': 4, 'num_hidden_unit': 59, 'learning_rate': 0.0024291088439318004}\n",
            "\n",
            "13.41602611541748\n"
          ]
        }
      ],
      "source": [
        "print(study.best_params)\n",
        "print('')\n",
        "print(study.best_value)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Read New Data"
      ],
      "metadata": {
        "id": "3RdRhEV2BZro"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_df = pd.read_parquet(\"/content/drive/MyDrive/New Data/18features/18_train_main.parquet.snappy\",engine='fastparquet')\n",
        "test_df = pd.read_parquet(\"/content/drive/MyDrive/New Data/18features/18_test_main.parquet.snappy\",engine='fastparquet')\n",
        "# prepare the cross-validation procedure\n",
        "kfold = KFold(n_splits=3, shuffle=True)\n",
        "\n",
        "training_withaim=train_df.drop(labels=\"r_id\", axis=1)\n",
        "testing_withaim=test_df.drop(labels=\"r_id\", axis=1)\n",
        "\n",
        "#Check the NaN in data and drop them\n",
        "imp_train=SimpleImputer(missing_values=np.NaN)\n",
        "training=pd.DataFrame(imp_train.fit_transform(training_withaim))\n",
        "\n",
        "imp_test=SimpleImputer(missing_values=np.NaN)\n",
        "testing=pd.DataFrame(imp_test.fit_transform(testing_withaim))\n",
        "print(training)\n",
        "# There aren't any nan data in the dataframe \n",
        "training.isnull().values.sum()\n",
        "testing.isnull().values.sum()\n",
        "training = training.iloc[: , 0:18]\n",
        "testing = testing.iloc[: , 0:18]\n",
        "scaler = StandardScaler()\n",
        "\n",
        "# scale skewed and target features\n",
        "std_train_df = train_df.copy(deep=True)\n",
        "std_train_df = scaler.fit_transform(training)\n",
        "std_test_df = test_df.copy(deep=True)\n",
        "std_test_df = scaler.transform(testing)\n",
        "#std_test_df[['r_stars','r_stars_square','r_length', 'u_friends_count', 'u_review_count', 'u_month_age', 'b_stars','b_review_count','r_rea']] = scaler.transform(test_df[['r_stars','r_stars_square','r_length', 'u_friends_count', 'u_review_count', 'u_month_age', 'b_stars','b_review_count','r_rea']])\n",
        "\n",
        "print(std_train_df[0])\n",
        "std_train_df = pd.DataFrame(std_train_df)\n",
        "std_test_df = pd.DataFrame(std_test_df)\n",
        "training = std_train_df.iloc[: , 0:18]\n",
        "testing = std_test_df.iloc[: , 0:18]\n",
        "labelsForTrain=training_withaim.iloc[: , -1]\n",
        "labelsForTest=testing_withaim.iloc[: , -1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PyAjSGq8AM_y",
        "outputId": "c9cb0cd0-22e5-472b-deca-a0bfd765bf29"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "         0     1      2      3       4           5         6    7     8   \\\n",
            "0       5.0  25.0   80.0  173.0   129.0  108.203988  0.178295  3.0   6.0   \n",
            "1       4.0  16.0  171.0  128.0   330.0   58.652416  0.909091  7.0  21.0   \n",
            "2       3.0   9.0  114.0  277.0  1064.0   64.378573  1.753759  2.0  57.0   \n",
            "3       5.0  25.0   34.0   35.0    59.0  102.077493  0.067797  0.0   0.0   \n",
            "4       1.0   1.0  261.0  137.0     5.0   44.994772  0.000000  0.0   0.0   \n",
            "...     ...   ...    ...    ...     ...         ...       ...  ...   ...   \n",
            "399995  5.0  25.0  191.0    1.0    16.0    0.000110  0.000000  0.0   0.0   \n",
            "399996  3.0   9.0  434.0  164.0   471.0   70.677502  0.116773  4.0  22.0   \n",
            "399997  5.0  25.0  131.0  211.0     7.0   23.186602  0.142857  0.0   1.0   \n",
            "399998  5.0  25.0   49.0    1.0    16.0    6.598250  0.062500  0.0   0.0   \n",
            "399999  1.0   1.0   25.0    1.0    58.0   27.243015  0.034483  0.0   0.0   \n",
            "\n",
            "          9       10   11      12   13     14        15        16     17    18  \n",
            "0       4.50   118.0  5.0    10.0  6.0   60.0  0.279954  0.433241  66.44   2.0  \n",
            "1       3.84   891.0  4.0   711.0  6.0   48.0  0.272727  0.474242  72.66   3.0  \n",
            "2       4.21  9948.0  3.5   154.0  6.0   33.0  0.146667  0.294583  68.13  18.0  \n",
            "3       4.30    48.0  4.0   109.0  7.0   71.0  0.383333  0.541667  76.93   1.0  \n",
            "4       3.40     5.0  4.0    18.0  5.0   42.5  0.039118  0.300063  88.97   3.0  \n",
            "...      ...     ...  ...     ...  ...    ...       ...       ...    ...   ...  \n",
            "399995  4.72     9.0  4.0    30.0  6.0   60.0 -0.130221  0.420113  69.11   3.0  \n",
            "399996  3.62   716.0  4.5   454.0  6.0   64.0  0.241433  0.523121  77.16   1.0  \n",
            "399997  2.93     3.0  4.0   380.0  7.0   72.0  0.084722  0.475076  82.24   1.0  \n",
            "399998  5.00     3.0  4.0  1984.0  7.0   60.0  0.340000  0.460000  78.45   1.0  \n",
            "399999  3.45    40.0  2.0    30.0  7.0  112.0 -0.500000  0.566667  88.74   1.0  \n",
            "\n",
            "[400000 rows x 19 columns]\n",
            "[ 0.90216754  0.99645674 -0.4633628  -0.02815708 -0.12709163  1.889782\n",
            " -0.11838612  0.3622633  -0.14541076  0.94854255 -0.15259145  1.58161918\n",
            " -0.48965899 -0.58778166 -0.41108489  0.50629158 -0.08015604 -1.15116974]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pAKR2fq9hNMg"
      },
      "source": [
        "# Old Part (No need to use this ,contain the data with extrack RGB)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eiWtNOjBQvBv"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "from google.colab.patches import cv2_imshow as cv2\n",
        "from itertools import product\n",
        "from keras import models,layers\n",
        "from PIL import Image\n",
        "from keras.layers.convolutional import Conv2D, MaxPooling2D\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Activation, Flatten, Conv1D\n",
        "import glob\n",
        "import os\n",
        "import csv\n",
        "import math \n",
        "import json\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from sklearn.datasets import load_boston\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import optuna\n",
        "from optkeras.optkeras import OptKeras"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eYm9c7HzCYmQ",
        "outputId": "8852c77d-cd8d-40d5-abd6-5bd213674d1d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mon Aug 22 05:33:10 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   44C    P0    26W / 250W |      0MiB / 16280MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "#gpu_info = !nvidia-smi\n",
        "#gpu_info = '\\n'.join(gpu_info)\n",
        "#if gpu_info.find('failed') >= 0:\n",
        "#  print('Not connected to a GPU')\n",
        "#else:\n",
        "#  print(gpu_info)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KaZ6GPZtHP0g"
      },
      "outputs": [],
      "source": [
        "#\n",
        "#model = Sequential(\n",
        "#    [\n",
        "#     Dense(12, activation='relu', input_shape=(11,)),\n",
        "#     Dense(12, activation='relu'),\n",
        "#     Dense(training.size),\n",
        "#     ])\n",
        "#model.build(input_shape=(11,1))\n",
        "#model.compile(optimizer='sgd', loss='mse',metrics=['accuracy'])\n",
        "#model.summary()\n",
        "\n",
        "\n",
        "#model = tf.keras.Sequential([\n",
        "#    tf.keras.layers.Dense(12, activation='relu'),\n",
        "#    tf.keras.layers.Dense(8, activation='relu'),\n",
        "#    tf.keras.layers.Dense(1, activation='sigmoid')\n",
        "#])\n",
        "#model.build(input_shape=(13,2))\n",
        "#model.compile(optimizer='sgd', loss='mse',metrics=['accuracy'])\n",
        "#model.summary()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W_hXz3RpIuca"
      },
      "outputs": [],
      "source": [
        "#num_epochs = 50\n",
        "#history = model.fit(training, labelsForTrain, epochs=num_epochs, validation_data=(validing, labelsForValid), verbose=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5UjvJjxlBhmg"
      },
      "outputs": [],
      "source": [
        "#vocab_size = 50000\n",
        "#embedding_dim = 16\n",
        "#max_length = 200\n",
        "#model = tf.keras.Sequential([\n",
        "#    tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length=max_length),\n",
        "#    tf.keras.layers.Dense(24, activation='relu'),\n",
        "#    tf.keras.layers.Dense(8, activation='relu'),\n",
        "#    tf.keras.layers.Dense(1, activation='sigmoid')\n",
        "#])\n",
        "\n",
        "\n",
        "#model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
        "#model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C2H53lwZz5SQ"
      },
      "outputs": [],
      "source": [
        "# Check list: \n",
        "#1. Review rating \n",
        "#2. Review Rating Extremity (?)\n",
        "#3. Word Count\n",
        "#4. User No of friends\n",
        "#5. User review count\n",
        "#6. User Age of account (months)\n",
        "#7. Restaurant avg. rating\n",
        "#8. Restaurant review count\n",
        "#9. Review text total sentiment polarity score of each sentence / total # sentences (need to be done)\n",
        "#10. Subjectivity score (subjective vs objective)\n",
        "#11. Readability"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KLptDhhn1k8T"
      },
      "source": [
        "Below is the code for extracting the RGB data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oF_xA-tPQnlu",
        "outputId": "8c6dc413-1082-420c-c007-9a2b5c49a165"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "oIlZM9LN-Ol_WfQQQpg0Zw.jpg\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:1015: UserWarning: Couldn't allocate palette entry for transparency\n",
            "  warnings.warn(\"Couldn't allocate palette entry for transparency\")\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0GJJhj1lh8V6zLeLkFwbyQ.jpg\n",
            "Va9Zo6Y6zrWzpm9iV4GXZw.jpg\n",
            "mKI1GpWQMs2rckjI3D9dhw.jpg\n",
            "2q_8hrvKQCFD18MyJzoVmw.jpg\n",
            "pm5uBNJCVRB3kZXq1eOyiA.jpg\n",
            "6qqRxbjA2HquZZnNQrvxEQ.jpg\n",
            "MDF6GetsTNRYWyIzzxea7A.jpg\n",
            "HNgnhFdN15zIw8HzsRzWOA.jpg\n",
            "Z7r1dYDC9eR6DQq01yGU-Q.jpg\n",
            "O8eLVHCthPJuv02-EY7GdA.jpg\n",
            "bWnSyaeOJgbykBL56ldM9w.jpg\n",
            "ogQp0M7B1OiS8AqP55eirQ.jpg\n",
            "A7FepQWBzC9eIXDUYImtbQ.jpg\n",
            "HyFN7L1PYIeTTBTY2-G6ZQ.jpg\n",
            "phGmoKO6rJpyQo8TDQofqQ.jpg\n",
            "Qi-lxakzwDIPVRFPyvMpUQ.jpg\n",
            "c9KWE6TeU4tRRkhxuYJCAQ.jpg\n",
            "NX1a9ZNo8a34RuALiBqROw.jpg\n",
            "lz-22DatK0pT7CkGLg7OgQ.jpg\n",
            "EhqaIciplglkJBPvufErUA.jpg\n",
            "JmWCtmZ1V3NrSTHs0mB4OA.jpg\n",
            "-_seY1mjA62XKXwi5FZGBQ.jpg\n",
            "Na1sCel7pdY2CRQnWBo84g.jpg\n",
            "QuxU8mtIK7P-glLX7kZmog.jpg\n"
          ]
        }
      ],
      "source": [
        "#with open('output_file.csv', 'w', newline='') as f_output:\n",
        " #   csv_output = csv.writer(f_output)\n",
        "  #  csv_output.writerow([\"img_name\", \"R\", \"G\", \"B\"])\n",
        "\n",
        "with open(\"RGB.json\", \"w\") as outfile:\n",
        "    for img_path in glob.glob(\"/content/drive/MyDrive/photos/*.jpg\"):\n",
        "    #img_path = '/content/drive/MyDrive/test/--3P3fjPrScy06XVLi4vhg.jpg'\n",
        "     try:\n",
        "      image = Image.open(img_path)\n",
        "      img_name = os.path.basename(img_path)\n",
        "      # Number of prime color we gonna extrace\n",
        "\n",
        "      num_colors=1\n",
        "\n",
        "      #Generate small image for calculation\n",
        "      small_image=image.resize((20,20))\n",
        "\n",
        "      result=small_image.convert('P',palette=Image.ADAPTIVE,colors=num_colors)\n",
        "      result=result.convert('RGB')\n",
        "\n",
        "      main_colors = result.getcolors(80*80)\n",
        "      main_rgb = main_colors[0][1]\n",
        "      #csv_output.writerows([img_name, map(lambda x: [x], main_rgb)])\n",
        "      # Data to be written\n",
        "      dictionary ={\n",
        "          \"r_name\": img_name,\n",
        "          \"R\": main_rgb[0],\n",
        "          \"G\": main_rgb[1],\n",
        "          \"B\": main_rgb[2]\n",
        "      }\n",
        "      outfile.write('\\n')\n",
        "      json.dump(dictionary, outfile)\n",
        "     except:\n",
        "      print(img_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "NicFdZ2Pa6SD",
        "outputId": "e1074f68-36a7-4dfa-bc04-09ad42646e32"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-13ecb7b3-19ea-49b2-b37e-4417bc1d688d\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>photo_id</th>\n",
              "      <th>R</th>\n",
              "      <th>G</th>\n",
              "      <th>B</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>--0h6FMC0V8aMtKQylojEg</td>\n",
              "      <td>108</td>\n",
              "      <td>97</td>\n",
              "      <td>87</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>--3JQ4MlO-jHT9xbo7liug</td>\n",
              "      <td>165</td>\n",
              "      <td>134</td>\n",
              "      <td>122</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>--3P3fjPrScy06XVLi4vhg</td>\n",
              "      <td>119</td>\n",
              "      <td>101</td>\n",
              "      <td>82</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>--4DfhW2sJvjeC90KEDX3g</td>\n",
              "      <td>139</td>\n",
              "      <td>126</td>\n",
              "      <td>111</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>--5V3mRlhb3zqE5yF4Gpaw</td>\n",
              "      <td>108</td>\n",
              "      <td>98</td>\n",
              "      <td>88</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-13ecb7b3-19ea-49b2-b37e-4417bc1d688d')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-13ecb7b3-19ea-49b2-b37e-4417bc1d688d button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-13ecb7b3-19ea-49b2-b37e-4417bc1d688d');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "                 photo_id    R    G    B\n",
              "0  --0h6FMC0V8aMtKQylojEg  108   97   87\n",
              "1  --3JQ4MlO-jHT9xbo7liug  165  134  122\n",
              "2  --3P3fjPrScy06XVLi4vhg  119  101   82\n",
              "3  --4DfhW2sJvjeC90KEDX3g  139  126  111\n",
              "4  --5V3mRlhb3zqE5yF4Gpaw  108   98   88"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#data= pd.read_json('/content/RGB.json', lines=True, orient='records')\n",
        "data= pd.read_json(open(\"/content/drive/MyDrive/RGB.json\", \"r\"), lines = True)\n",
        "data.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fox6mPzwNcEn",
        "outputId": "e80f6572-9116-41e9-c459-cae4f5da2a71"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l7gMcGo6XCX1",
        "outputId": "6ddcf993-aa7b-4265-9321-1400f22c11c9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(255, 51, 51)\n"
          ]
        }
      ],
      "source": [
        "for filename in glob.glob(\"/content/drive/MyDrive/test/20GZaQkAup3g1U9ENd-bcQ.jpg\"):\n",
        "    image = Image.open(filename) \n",
        "    img_name = os.path.basename(filename)\n",
        "    ## Number of prime color we gonna extrace\n",
        "    num_colors=1\n",
        "\n",
        "    #Generate small image for calculation\n",
        "    small_image=image.resize((20,20))\n",
        "\n",
        "    result=small_image.convert('P',colors=num_colors)\n",
        "    result=result.convert('RGB')\n",
        "\n",
        "    main_colors = result.getcolors(20*20)\n",
        "    main_rgb = main_colors[0][1]\n",
        "    print(main_rgb)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v6r6J7JyMHiW",
        "outputId": "1db61346-b4bd-48b3-85d1-64b4e68481a5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--3JQ4MlO-jHT9xbo7liug.jpg, Width 400, Height 400\n"
          ]
        }
      ],
      "source": [
        "#Open a file to write the pixel data\n",
        "from itertools import product\n",
        "from PIL import Image\n",
        "import glob\n",
        "import os\n",
        "import csv\n",
        "\n",
        "#Open a file to write the pixel data\n",
        "with open('output_file.csv', 'w', newline='') as f_output:\n",
        "    csv_output = csv.writer(f_output)\n",
        "    csv_output.writerow([\"img_name\", \"R\", \"G\", \"B\"])\n",
        "\n",
        "    #Path to file \n",
        "    for filename in glob.glob(\"*.jpg\"):\n",
        "        im = Image.open(filename) \n",
        "        img_name = os.path.basename(filename)\n",
        "\n",
        "        #Load the pixel info\n",
        "        pix = im.load()\n",
        "\n",
        "        #Get a tuple of the x and y dimensions of the image\n",
        "        width, height = im.size\n",
        "\n",
        "        print(f'{filename}, Width {width}, Height {height}') # show \n",
        "\n",
        "        #Read the details of each pixel and write them to the file\n",
        "        csv_output.writerows([img_name, *pix[x,y]] for x, y in product(range(width), range(height)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M5QZtzWyOcBb"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4y2fruRzJylA",
        "outputId": "abc55ec4-9a3f-4599-ccf2-c51750f37ee5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "collapsed_sections": [],
      "machine_shape": "hm",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}