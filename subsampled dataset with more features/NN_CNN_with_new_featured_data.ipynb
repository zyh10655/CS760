{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "J5uIHs3qKGJX",
        "outputId": "991a1d7b-12d2-4931-bff5-ad0f21442f4b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting optuna==0.14.0\n",
            "  Downloading optuna-0.14.0.tar.gz (91 kB)\n",
            "\u001b[K     |████████████████████████████████| 91 kB 4.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: sqlalchemy>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from optuna==0.14.0) (1.4.41)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from optuna==0.14.0) (1.21.6)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from optuna==0.14.0) (1.7.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from optuna==0.14.0) (1.15.0)\n",
            "Collecting cliff\n",
            "  Downloading cliff-3.10.1-py3-none-any.whl (81 kB)\n",
            "\u001b[K     |████████████████████████████████| 81 kB 8.8 MB/s \n",
            "\u001b[?25hCollecting colorlog\n",
            "  Downloading colorlog-6.7.0-py2.py3-none-any.whl (11 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from optuna==0.14.0) (1.3.5)\n",
            "Collecting alembic\n",
            "  Downloading alembic-1.8.1-py3-none-any.whl (209 kB)\n",
            "\u001b[K     |████████████████████████████████| 209 kB 44.2 MB/s \n",
            "\u001b[?25hCollecting typing\n",
            "  Downloading typing-3.7.4.3.tar.gz (78 kB)\n",
            "\u001b[K     |████████████████████████████████| 78 kB 7.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from sqlalchemy>=1.1.0->optuna==0.14.0) (4.12.0)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.7/dist-packages (from sqlalchemy>=1.1.0->optuna==0.14.0) (1.1.3)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.7/dist-packages (from alembic->optuna==0.14.0) (5.9.0)\n",
            "Collecting Mako\n",
            "  Downloading Mako-1.2.3-py3-none-any.whl (78 kB)\n",
            "\u001b[K     |████████████████████████████████| 78 kB 5.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pyparsing>=2.1.0 in /usr/local/lib/python3.7/dist-packages (from cliff->optuna==0.14.0) (3.0.9)\n",
            "Requirement already satisfied: PrettyTable>=0.7.2 in /usr/local/lib/python3.7/dist-packages (from cliff->optuna==0.14.0) (3.4.1)\n",
            "Collecting pbr!=2.1.0,>=2.0.0\n",
            "  Downloading pbr-5.10.0-py2.py3-none-any.whl (112 kB)\n",
            "\u001b[K     |████████████████████████████████| 112 kB 83.6 MB/s \n",
            "\u001b[?25hCollecting stevedore>=2.0.1\n",
            "  Downloading stevedore-3.5.0-py3-none-any.whl (49 kB)\n",
            "\u001b[K     |████████████████████████████████| 49 kB 6.2 MB/s \n",
            "\u001b[?25hCollecting autopage>=0.4.0\n",
            "  Downloading autopage-0.5.1-py3-none-any.whl (29 kB)\n",
            "Collecting cmd2>=1.0.0\n",
            "  Downloading cmd2-2.4.2-py3-none-any.whl (147 kB)\n",
            "\u001b[K     |████████████████████████████████| 147 kB 74.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: PyYAML>=3.12 in /usr/local/lib/python3.7/dist-packages (from cliff->optuna==0.14.0) (6.0)\n",
            "Requirement already satisfied: wcwidth>=0.1.7 in /usr/local/lib/python3.7/dist-packages (from cmd2>=1.0.0->cliff->optuna==0.14.0) (0.2.5)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from cmd2>=1.0.0->cliff->optuna==0.14.0) (4.1.1)\n",
            "Requirement already satisfied: attrs>=16.3.0 in /usr/local/lib/python3.7/dist-packages (from cmd2>=1.0.0->cliff->optuna==0.14.0) (22.1.0)\n",
            "Collecting pyperclip>=1.6\n",
            "  Downloading pyperclip-1.8.2.tar.gz (20 kB)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->sqlalchemy>=1.1.0->optuna==0.14.0) (3.8.1)\n",
            "Requirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.7/dist-packages (from Mako->alembic->optuna==0.14.0) (2.0.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->optuna==0.14.0) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas->optuna==0.14.0) (2022.2.1)\n",
            "Building wheels for collected packages: optuna, pyperclip, typing\n",
            "  Building wheel for optuna (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for optuna: filename=optuna-0.14.0-py3-none-any.whl size=125709 sha256=ab4ef902244a10dd0084674f07e692659f81404efdb173f3a01c17e27753a793\n",
            "  Stored in directory: /root/.cache/pip/wheels/c1/41/64/03b183676c5d5e978de160cab6268d5b4fb095dff63f720e01\n",
            "  Building wheel for pyperclip (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyperclip: filename=pyperclip-1.8.2-py3-none-any.whl size=11137 sha256=61565d86d76ebe0573f40f91abeb88c0af81f95b20a955e7fff0f373fd1a22da\n",
            "  Stored in directory: /root/.cache/pip/wheels/9f/18/84/8f69f8b08169c7bae2dde6bd7daf0c19fca8c8e500ee620a28\n",
            "  Building wheel for typing (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for typing: filename=typing-3.7.4.3-py3-none-any.whl size=26325 sha256=8fae6d9293963699fa7702694b10512440e615c203819a938dcbd1af912ec9a2\n",
            "  Stored in directory: /root/.cache/pip/wheels/35/f3/15/01aa6571f0a72ee6ae7b827c1491c37a1f72d686fd22b43b0e\n",
            "Successfully built optuna pyperclip typing\n",
            "Installing collected packages: pyperclip, pbr, stevedore, Mako, cmd2, autopage, typing, colorlog, cliff, alembic, optuna\n",
            "Successfully installed Mako-1.2.3 alembic-1.8.1 autopage-0.5.1 cliff-3.10.1 cmd2-2.4.2 colorlog-6.7.0 optuna-0.14.0 pbr-5.10.0 pyperclip-1.8.2 stevedore-3.5.0 typing-3.7.4.3\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "typing"
                ]
              }
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting optkeras==0.0.7\n",
            "  Downloading optkeras-0.0.7-py3-none-any.whl (6.9 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from optkeras==0.0.7) (1.21.6)\n",
            "Requirement already satisfied: optuna>=0.9.0 in /usr/local/lib/python3.7/dist-packages (from optkeras==0.0.7) (0.14.0)\n",
            "Requirement already satisfied: keras in /usr/local/lib/python3.7/dist-packages (from optkeras==0.0.7) (2.8.0)\n",
            "Requirement already satisfied: cliff in /usr/local/lib/python3.7/dist-packages (from optuna>=0.9.0->optkeras==0.0.7) (3.10.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from optuna>=0.9.0->optkeras==0.0.7) (1.7.3)\n",
            "Requirement already satisfied: sqlalchemy>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from optuna>=0.9.0->optkeras==0.0.7) (1.4.41)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from optuna>=0.9.0->optkeras==0.0.7) (1.15.0)\n",
            "Requirement already satisfied: alembic in /usr/local/lib/python3.7/dist-packages (from optuna>=0.9.0->optkeras==0.0.7) (1.8.1)\n",
            "Requirement already satisfied: typing in /usr/local/lib/python3.7/dist-packages (from optuna>=0.9.0->optkeras==0.0.7) (3.7.4.3)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from optuna>=0.9.0->optkeras==0.0.7) (1.3.5)\n",
            "Requirement already satisfied: colorlog in /usr/local/lib/python3.7/dist-packages (from optuna>=0.9.0->optkeras==0.0.7) (6.7.0)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.7/dist-packages (from sqlalchemy>=1.1.0->optuna>=0.9.0->optkeras==0.0.7) (1.1.3)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from sqlalchemy>=1.1.0->optuna>=0.9.0->optkeras==0.0.7) (4.12.0)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.7/dist-packages (from alembic->optuna>=0.9.0->optkeras==0.0.7) (5.9.0)\n",
            "Requirement already satisfied: Mako in /usr/local/lib/python3.7/dist-packages (from alembic->optuna>=0.9.0->optkeras==0.0.7) (1.2.3)\n",
            "Requirement already satisfied: cmd2>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from cliff->optuna>=0.9.0->optkeras==0.0.7) (2.4.2)\n",
            "Requirement already satisfied: stevedore>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from cliff->optuna>=0.9.0->optkeras==0.0.7) (3.5.0)\n",
            "Requirement already satisfied: PrettyTable>=0.7.2 in /usr/local/lib/python3.7/dist-packages (from cliff->optuna>=0.9.0->optkeras==0.0.7) (3.4.1)\n",
            "Requirement already satisfied: pyparsing>=2.1.0 in /usr/local/lib/python3.7/dist-packages (from cliff->optuna>=0.9.0->optkeras==0.0.7) (3.0.9)\n",
            "Requirement already satisfied: autopage>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from cliff->optuna>=0.9.0->optkeras==0.0.7) (0.5.1)\n",
            "Requirement already satisfied: PyYAML>=3.12 in /usr/local/lib/python3.7/dist-packages (from cliff->optuna>=0.9.0->optkeras==0.0.7) (6.0)\n",
            "Requirement already satisfied: pbr!=2.1.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from cliff->optuna>=0.9.0->optkeras==0.0.7) (5.10.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from cmd2>=1.0.0->cliff->optuna>=0.9.0->optkeras==0.0.7) (4.1.1)\n",
            "Requirement already satisfied: wcwidth>=0.1.7 in /usr/local/lib/python3.7/dist-packages (from cmd2>=1.0.0->cliff->optuna>=0.9.0->optkeras==0.0.7) (0.2.5)\n",
            "Requirement already satisfied: attrs>=16.3.0 in /usr/local/lib/python3.7/dist-packages (from cmd2>=1.0.0->cliff->optuna>=0.9.0->optkeras==0.0.7) (22.1.0)\n",
            "Requirement already satisfied: pyperclip>=1.6 in /usr/local/lib/python3.7/dist-packages (from cmd2>=1.0.0->cliff->optuna>=0.9.0->optkeras==0.0.7) (1.8.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->sqlalchemy>=1.1.0->optuna>=0.9.0->optkeras==0.0.7) (3.8.1)\n",
            "Requirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.7/dist-packages (from Mako->alembic->optuna>=0.9.0->optkeras==0.0.7) (2.0.1)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas->optuna>=0.9.0->optkeras==0.0.7) (2022.2.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->optuna>=0.9.0->optkeras==0.0.7) (2.8.2)\n",
            "Installing collected packages: optkeras\n",
            "Successfully installed optkeras-0.0.7\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting fastparquet\n",
            "  Downloading fastparquet-0.8.1-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.5 MB 5.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pandas>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from fastparquet) (1.3.5)\n",
            "Requirement already satisfied: numpy>=1.18 in /usr/local/lib/python3.7/dist-packages (from fastparquet) (1.21.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.7/dist-packages (from fastparquet) (2022.8.2)\n",
            "Collecting cramjam>=2.3.0\n",
            "  Downloading cramjam-2.5.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.whl (1.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.6 MB 70.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=1.1.0->fastparquet) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=1.1.0->fastparquet) (2022.2.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas>=1.1.0->fastparquet) (1.15.0)\n",
            "Installing collected packages: cramjam, fastparquet\n",
            "Successfully installed cramjam-2.5.0 fastparquet-0.8.1\n"
          ]
        }
      ],
      "source": [
        "!pip3 install optuna==0.14.0\n",
        "!pip3 install optkeras==0.0.7\n",
        "!pip install fastparquet"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from keras.datasets import mnist\n",
        "from keras.models import Sequential,Model\n",
        "from keras.layers import Activation, Flatten, Dense, Conv2D, Conv1D,Input\n",
        "from keras.layers import MaxPooling1D, Dropout, BatchNormalization\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.optimizers import SGD, Adagrad, RMSprop, Adam, Adadelta, Adamax, Nadam\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.impute import SimpleImputer\n",
        "from keras.layers.core import *\n",
        "from keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "import keras.backend as K\n",
        "import optuna\n",
        "import math"
      ],
      "metadata": {
        "id": "8o4dbzGuLONQ"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import keras\n",
        "print('Keras', keras.__version__)\n",
        "\n",
        "import tensorflow as tf\n",
        "print('TensorFlow', tf.__version__)\n",
        "\n",
        "# import Optuna and OptKeras after Keras\n",
        "import optuna \n",
        "print('Optuna', optuna.__version__)\n",
        "\n",
        "from optkeras.optkeras import OptKeras\n",
        "import optkeras\n",
        "print('OptKeras', optkeras.__version__)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EwpYWLIqLRIA",
        "outputId": "655ff9c1-201f-49f0-c446-551e5d876600"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Keras 2.8.0\n",
            "TensorFlow 2.8.2\n",
            "Optuna 0.14.0\n",
            "OptKeras 0.0.7\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_df = pd.read_parquet(\"/content/drive/MyDrive/New Data/18features/18_train_main.parquet.snappy\",engine='fastparquet')\n",
        "test_df = pd.read_parquet(\"/content/drive/MyDrive/New Data/18features/18_test_main.parquet.snappy\",engine='fastparquet')"
      ],
      "metadata": {
        "id": "MPqujRB7LSOH"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prepare the cross-validation procedure\n",
        "kfold = KFold(n_splits=3, shuffle=True)\n",
        "\n",
        "training_withaim=train_df.drop(labels=\"r_id\", axis=1)\n",
        "testing_withaim=test_df.drop(labels=\"r_id\", axis=1)\n",
        "\n",
        "#Check the NaN in data and drop them\n",
        "imp_train=SimpleImputer(missing_values=np.NaN)\n",
        "training=pd.DataFrame(imp_train.fit_transform(training_withaim))\n",
        "\n",
        "imp_test=SimpleImputer(missing_values=np.NaN)\n",
        "testing=pd.DataFrame(imp_test.fit_transform(testing_withaim))\n",
        "print(training)\n",
        "# There aren't any nan data in the dataframe \n",
        "training.isnull().values.sum()\n",
        "testing.isnull().values.sum()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "esfqiU3EMyUr",
        "outputId": "8cc6193d-cd9c-4055-80b5-3e4d4bb397d4"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "         0     1      2      3       4           5         6    7     8   \\\n",
            "0       5.0  25.0   80.0  173.0   129.0  108.203988  0.178295  3.0   6.0   \n",
            "1       4.0  16.0  171.0  128.0   330.0   58.652416  0.909091  7.0  21.0   \n",
            "2       3.0   9.0  114.0  277.0  1064.0   64.378573  1.753759  2.0  57.0   \n",
            "3       5.0  25.0   34.0   35.0    59.0  102.077493  0.067797  0.0   0.0   \n",
            "4       1.0   1.0  261.0  137.0     5.0   44.994772  0.000000  0.0   0.0   \n",
            "...     ...   ...    ...    ...     ...         ...       ...  ...   ...   \n",
            "399995  5.0  25.0  191.0    1.0    16.0    0.000110  0.000000  0.0   0.0   \n",
            "399996  3.0   9.0  434.0  164.0   471.0   70.677502  0.116773  4.0  22.0   \n",
            "399997  5.0  25.0  131.0  211.0     7.0   23.186602  0.142857  0.0   1.0   \n",
            "399998  5.0  25.0   49.0    1.0    16.0    6.598250  0.062500  0.0   0.0   \n",
            "399999  1.0   1.0   25.0    1.0    58.0   27.243015  0.034483  0.0   0.0   \n",
            "\n",
            "          9       10   11      12   13     14        15        16     17    18  \n",
            "0       4.50   118.0  5.0    10.0  6.0   60.0  0.279954  0.433241  66.44   2.0  \n",
            "1       3.84   891.0  4.0   711.0  6.0   48.0  0.272727  0.474242  72.66   3.0  \n",
            "2       4.21  9948.0  3.5   154.0  6.0   33.0  0.146667  0.294583  68.13  18.0  \n",
            "3       4.30    48.0  4.0   109.0  7.0   71.0  0.383333  0.541667  76.93   1.0  \n",
            "4       3.40     5.0  4.0    18.0  5.0   42.5  0.039118  0.300063  88.97   3.0  \n",
            "...      ...     ...  ...     ...  ...    ...       ...       ...    ...   ...  \n",
            "399995  4.72     9.0  4.0    30.0  6.0   60.0 -0.130221  0.420113  69.11   3.0  \n",
            "399996  3.62   716.0  4.5   454.0  6.0   64.0  0.241433  0.523121  77.16   1.0  \n",
            "399997  2.93     3.0  4.0   380.0  7.0   72.0  0.084722  0.475076  82.24   1.0  \n",
            "399998  5.00     3.0  4.0  1984.0  7.0   60.0  0.340000  0.460000  78.45   1.0  \n",
            "399999  3.45    40.0  2.0    30.0  7.0  112.0 -0.500000  0.566667  88.74   1.0  \n",
            "\n",
            "[400000 rows x 19 columns]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "training = training.iloc[: , 0:18]\n",
        "testing = testing.iloc[: , 0:18]\n",
        "training"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "Te12QBIeR-3t",
        "outputId": "7cbc1e92-ea3c-4679-fbb2-55d6983f25a6"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "         0     1      2      3       4           5         6    7     8   \\\n",
              "0       5.0  25.0   80.0  173.0   129.0  108.203988  0.178295  3.0   6.0   \n",
              "1       4.0  16.0  171.0  128.0   330.0   58.652416  0.909091  7.0  21.0   \n",
              "2       3.0   9.0  114.0  277.0  1064.0   64.378573  1.753759  2.0  57.0   \n",
              "3       5.0  25.0   34.0   35.0    59.0  102.077493  0.067797  0.0   0.0   \n",
              "4       1.0   1.0  261.0  137.0     5.0   44.994772  0.000000  0.0   0.0   \n",
              "...     ...   ...    ...    ...     ...         ...       ...  ...   ...   \n",
              "399995  5.0  25.0  191.0    1.0    16.0    0.000110  0.000000  0.0   0.0   \n",
              "399996  3.0   9.0  434.0  164.0   471.0   70.677502  0.116773  4.0  22.0   \n",
              "399997  5.0  25.0  131.0  211.0     7.0   23.186602  0.142857  0.0   1.0   \n",
              "399998  5.0  25.0   49.0    1.0    16.0    6.598250  0.062500  0.0   0.0   \n",
              "399999  1.0   1.0   25.0    1.0    58.0   27.243015  0.034483  0.0   0.0   \n",
              "\n",
              "          9       10   11      12   13     14        15        16     17  \n",
              "0       4.50   118.0  5.0    10.0  6.0   60.0  0.279954  0.433241  66.44  \n",
              "1       3.84   891.0  4.0   711.0  6.0   48.0  0.272727  0.474242  72.66  \n",
              "2       4.21  9948.0  3.5   154.0  6.0   33.0  0.146667  0.294583  68.13  \n",
              "3       4.30    48.0  4.0   109.0  7.0   71.0  0.383333  0.541667  76.93  \n",
              "4       3.40     5.0  4.0    18.0  5.0   42.5  0.039118  0.300063  88.97  \n",
              "...      ...     ...  ...     ...  ...    ...       ...       ...    ...  \n",
              "399995  4.72     9.0  4.0    30.0  6.0   60.0 -0.130221  0.420113  69.11  \n",
              "399996  3.62   716.0  4.5   454.0  6.0   64.0  0.241433  0.523121  77.16  \n",
              "399997  2.93     3.0  4.0   380.0  7.0   72.0  0.084722  0.475076  82.24  \n",
              "399998  5.00     3.0  4.0  1984.0  7.0   60.0  0.340000  0.460000  78.45  \n",
              "399999  3.45    40.0  2.0    30.0  7.0  112.0 -0.500000  0.566667  88.74  \n",
              "\n",
              "[400000 rows x 18 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-69a5eb36-9a88-4765-99b2-a0683b961feb\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>5.0</td>\n",
              "      <td>25.0</td>\n",
              "      <td>80.0</td>\n",
              "      <td>173.0</td>\n",
              "      <td>129.0</td>\n",
              "      <td>108.203988</td>\n",
              "      <td>0.178295</td>\n",
              "      <td>3.0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>4.50</td>\n",
              "      <td>118.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>10.0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>60.0</td>\n",
              "      <td>0.279954</td>\n",
              "      <td>0.433241</td>\n",
              "      <td>66.44</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>4.0</td>\n",
              "      <td>16.0</td>\n",
              "      <td>171.0</td>\n",
              "      <td>128.0</td>\n",
              "      <td>330.0</td>\n",
              "      <td>58.652416</td>\n",
              "      <td>0.909091</td>\n",
              "      <td>7.0</td>\n",
              "      <td>21.0</td>\n",
              "      <td>3.84</td>\n",
              "      <td>891.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>711.0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>48.0</td>\n",
              "      <td>0.272727</td>\n",
              "      <td>0.474242</td>\n",
              "      <td>72.66</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3.0</td>\n",
              "      <td>9.0</td>\n",
              "      <td>114.0</td>\n",
              "      <td>277.0</td>\n",
              "      <td>1064.0</td>\n",
              "      <td>64.378573</td>\n",
              "      <td>1.753759</td>\n",
              "      <td>2.0</td>\n",
              "      <td>57.0</td>\n",
              "      <td>4.21</td>\n",
              "      <td>9948.0</td>\n",
              "      <td>3.5</td>\n",
              "      <td>154.0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>33.0</td>\n",
              "      <td>0.146667</td>\n",
              "      <td>0.294583</td>\n",
              "      <td>68.13</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>5.0</td>\n",
              "      <td>25.0</td>\n",
              "      <td>34.0</td>\n",
              "      <td>35.0</td>\n",
              "      <td>59.0</td>\n",
              "      <td>102.077493</td>\n",
              "      <td>0.067797</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>4.30</td>\n",
              "      <td>48.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>109.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>71.0</td>\n",
              "      <td>0.383333</td>\n",
              "      <td>0.541667</td>\n",
              "      <td>76.93</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>261.0</td>\n",
              "      <td>137.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>44.994772</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>3.40</td>\n",
              "      <td>5.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>18.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>42.5</td>\n",
              "      <td>0.039118</td>\n",
              "      <td>0.300063</td>\n",
              "      <td>88.97</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>399995</th>\n",
              "      <td>5.0</td>\n",
              "      <td>25.0</td>\n",
              "      <td>191.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>16.0</td>\n",
              "      <td>0.000110</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>4.72</td>\n",
              "      <td>9.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>30.0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>60.0</td>\n",
              "      <td>-0.130221</td>\n",
              "      <td>0.420113</td>\n",
              "      <td>69.11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>399996</th>\n",
              "      <td>3.0</td>\n",
              "      <td>9.0</td>\n",
              "      <td>434.0</td>\n",
              "      <td>164.0</td>\n",
              "      <td>471.0</td>\n",
              "      <td>70.677502</td>\n",
              "      <td>0.116773</td>\n",
              "      <td>4.0</td>\n",
              "      <td>22.0</td>\n",
              "      <td>3.62</td>\n",
              "      <td>716.0</td>\n",
              "      <td>4.5</td>\n",
              "      <td>454.0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>64.0</td>\n",
              "      <td>0.241433</td>\n",
              "      <td>0.523121</td>\n",
              "      <td>77.16</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>399997</th>\n",
              "      <td>5.0</td>\n",
              "      <td>25.0</td>\n",
              "      <td>131.0</td>\n",
              "      <td>211.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>23.186602</td>\n",
              "      <td>0.142857</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>2.93</td>\n",
              "      <td>3.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>380.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>72.0</td>\n",
              "      <td>0.084722</td>\n",
              "      <td>0.475076</td>\n",
              "      <td>82.24</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>399998</th>\n",
              "      <td>5.0</td>\n",
              "      <td>25.0</td>\n",
              "      <td>49.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>16.0</td>\n",
              "      <td>6.598250</td>\n",
              "      <td>0.062500</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>5.00</td>\n",
              "      <td>3.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>1984.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>60.0</td>\n",
              "      <td>0.340000</td>\n",
              "      <td>0.460000</td>\n",
              "      <td>78.45</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>399999</th>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>25.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>58.0</td>\n",
              "      <td>27.243015</td>\n",
              "      <td>0.034483</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>3.45</td>\n",
              "      <td>40.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>30.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>112.0</td>\n",
              "      <td>-0.500000</td>\n",
              "      <td>0.566667</td>\n",
              "      <td>88.74</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>400000 rows × 18 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-69a5eb36-9a88-4765-99b2-a0683b961feb')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-69a5eb36-9a88-4765-99b2-a0683b961feb button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-69a5eb36-9a88-4765-99b2-a0683b961feb');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "scaler = StandardScaler()\n",
        "\n",
        "# scale skewed and target features\n",
        "std_train_df = train_df.copy(deep=True)\n",
        "std_train_df = scaler.fit_transform(training)\n",
        "std_test_df = test_df.copy(deep=True)\n",
        "std_test_df = scaler.transform(testing)\n",
        "#std_test_df[['r_stars','r_stars_square','r_length', 'u_friends_count', 'u_review_count', 'u_month_age', 'b_stars','b_review_count','r_rea']] = scaler.transform(test_df[['r_stars','r_stars_square','r_length', 'u_friends_count', 'u_review_count', 'u_month_age', 'b_stars','b_review_count','r_rea']])\n",
        "\n",
        "print(std_train_df[0])\n",
        "std_train_df = pd.DataFrame(std_train_df)\n",
        "std_test_df = pd.DataFrame(std_test_df)\n",
        "std_train_df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 476
        },
        "id": "qo216mLySLVU",
        "outputId": "89ea1f52-308d-4a25-d922-637ef716c818"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ 0.90216754  0.99645674 -0.4633628  -0.02815708 -0.12709163  1.889782\n",
            " -0.11838612  0.3622633  -0.14541076  0.94854255 -0.15259145  1.58161918\n",
            " -0.48965899 -0.58778166 -0.41108489  0.50629158 -0.08015604 -1.15116974]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "              0         1         2         3         4         5         6   \\\n",
              "0       0.902168  0.996457 -0.463363 -0.028157 -0.127092  1.889782 -0.118386   \n",
              "1       0.252164  0.060934  0.330297 -0.108118  0.288456  0.458212  0.116132   \n",
              "2      -0.397840 -0.666695 -0.166830  0.156643  1.805931  0.623643  0.387193   \n",
              "3       0.902168  0.996457 -0.864554 -0.273372 -0.271810  1.712784 -0.153846   \n",
              "4      -1.697847 -1.498271  1.115236 -0.092126 -0.383450  0.063635 -0.175602   \n",
              "...          ...       ...       ...       ...       ...       ...       ...   \n",
              "399995  0.902168  0.996457  0.504728 -0.333787 -0.360708 -1.236284 -0.175602   \n",
              "399996 -0.397840 -0.666695  2.624063 -0.044149  0.579960  0.805623 -0.138129   \n",
              "399997  0.902168  0.996457 -0.018564  0.039366 -0.379315 -0.566414 -0.129758   \n",
              "399998  0.902168  0.996457 -0.733731 -0.333787 -0.360708 -1.045660 -0.155546   \n",
              "399999 -1.697847 -1.498271 -0.943048 -0.333787 -0.273877 -0.449222 -0.164536   \n",
              "\n",
              "              7         8         9         10        11        12        13  \\\n",
              "0       0.362263 -0.145411  0.948543 -0.152591  1.581619 -0.489659 -0.587782   \n",
              "1       1.662180 -0.029172  0.144179  0.016366  0.321906  0.559060 -0.587782   \n",
              "2       0.037284  0.249799  0.595110  1.995984 -0.307951 -0.274230 -0.587782   \n",
              "3      -0.612674 -0.191906  0.704796 -0.167892  0.321906 -0.341552  0.617819   \n",
              "4      -0.612674 -0.191906 -0.392064 -0.177290  0.321906 -0.477691 -1.793382   \n",
              "...          ...       ...       ...       ...       ...       ...       ...   \n",
              "399995 -0.612674 -0.191906  1.216664 -0.176416  0.321906 -0.459738 -0.587782   \n",
              "399996  0.687243 -0.021423 -0.123943 -0.021885  0.951762  0.174580 -0.587782   \n",
              "399997 -0.612674 -0.184157 -0.964869 -0.177727  0.321906  0.063873  0.617819   \n",
              "399998 -0.612674 -0.191906  1.557909 -0.177727  0.321906  2.463511  0.617819   \n",
              "399999 -0.612674 -0.191906 -0.331127 -0.169640 -2.197521 -0.459738  0.617819   \n",
              "\n",
              "              14        15        16        17  \n",
              "0      -0.411085  0.506292 -0.080156 -1.151170  \n",
              "1      -0.797300  0.468778  0.199067 -0.623535  \n",
              "2      -1.280069 -0.185575 -1.024437 -1.007809  \n",
              "3      -0.057054  1.042912  0.658240 -0.261316  \n",
              "4      -0.974315 -0.743841 -0.987117  0.760023  \n",
              "...          ...       ...       ...       ...  \n",
              "399995 -0.411085 -1.622847 -0.169560 -0.924677  \n",
              "399996 -0.282347  0.306337  0.531939 -0.241805  \n",
              "399997 -0.024870 -0.507120  0.204746  0.189125  \n",
              "399998 -0.411085  0.817979  0.102077 -0.132376  \n",
              "399999  1.262513 -3.542297  0.828494  0.740512  \n",
              "\n",
              "[400000 rows x 18 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-9f02b995-a8ef-430b-ac7b-96f12532218c\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.902168</td>\n",
              "      <td>0.996457</td>\n",
              "      <td>-0.463363</td>\n",
              "      <td>-0.028157</td>\n",
              "      <td>-0.127092</td>\n",
              "      <td>1.889782</td>\n",
              "      <td>-0.118386</td>\n",
              "      <td>0.362263</td>\n",
              "      <td>-0.145411</td>\n",
              "      <td>0.948543</td>\n",
              "      <td>-0.152591</td>\n",
              "      <td>1.581619</td>\n",
              "      <td>-0.489659</td>\n",
              "      <td>-0.587782</td>\n",
              "      <td>-0.411085</td>\n",
              "      <td>0.506292</td>\n",
              "      <td>-0.080156</td>\n",
              "      <td>-1.151170</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.252164</td>\n",
              "      <td>0.060934</td>\n",
              "      <td>0.330297</td>\n",
              "      <td>-0.108118</td>\n",
              "      <td>0.288456</td>\n",
              "      <td>0.458212</td>\n",
              "      <td>0.116132</td>\n",
              "      <td>1.662180</td>\n",
              "      <td>-0.029172</td>\n",
              "      <td>0.144179</td>\n",
              "      <td>0.016366</td>\n",
              "      <td>0.321906</td>\n",
              "      <td>0.559060</td>\n",
              "      <td>-0.587782</td>\n",
              "      <td>-0.797300</td>\n",
              "      <td>0.468778</td>\n",
              "      <td>0.199067</td>\n",
              "      <td>-0.623535</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>-0.397840</td>\n",
              "      <td>-0.666695</td>\n",
              "      <td>-0.166830</td>\n",
              "      <td>0.156643</td>\n",
              "      <td>1.805931</td>\n",
              "      <td>0.623643</td>\n",
              "      <td>0.387193</td>\n",
              "      <td>0.037284</td>\n",
              "      <td>0.249799</td>\n",
              "      <td>0.595110</td>\n",
              "      <td>1.995984</td>\n",
              "      <td>-0.307951</td>\n",
              "      <td>-0.274230</td>\n",
              "      <td>-0.587782</td>\n",
              "      <td>-1.280069</td>\n",
              "      <td>-0.185575</td>\n",
              "      <td>-1.024437</td>\n",
              "      <td>-1.007809</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.902168</td>\n",
              "      <td>0.996457</td>\n",
              "      <td>-0.864554</td>\n",
              "      <td>-0.273372</td>\n",
              "      <td>-0.271810</td>\n",
              "      <td>1.712784</td>\n",
              "      <td>-0.153846</td>\n",
              "      <td>-0.612674</td>\n",
              "      <td>-0.191906</td>\n",
              "      <td>0.704796</td>\n",
              "      <td>-0.167892</td>\n",
              "      <td>0.321906</td>\n",
              "      <td>-0.341552</td>\n",
              "      <td>0.617819</td>\n",
              "      <td>-0.057054</td>\n",
              "      <td>1.042912</td>\n",
              "      <td>0.658240</td>\n",
              "      <td>-0.261316</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>-1.697847</td>\n",
              "      <td>-1.498271</td>\n",
              "      <td>1.115236</td>\n",
              "      <td>-0.092126</td>\n",
              "      <td>-0.383450</td>\n",
              "      <td>0.063635</td>\n",
              "      <td>-0.175602</td>\n",
              "      <td>-0.612674</td>\n",
              "      <td>-0.191906</td>\n",
              "      <td>-0.392064</td>\n",
              "      <td>-0.177290</td>\n",
              "      <td>0.321906</td>\n",
              "      <td>-0.477691</td>\n",
              "      <td>-1.793382</td>\n",
              "      <td>-0.974315</td>\n",
              "      <td>-0.743841</td>\n",
              "      <td>-0.987117</td>\n",
              "      <td>0.760023</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>399995</th>\n",
              "      <td>0.902168</td>\n",
              "      <td>0.996457</td>\n",
              "      <td>0.504728</td>\n",
              "      <td>-0.333787</td>\n",
              "      <td>-0.360708</td>\n",
              "      <td>-1.236284</td>\n",
              "      <td>-0.175602</td>\n",
              "      <td>-0.612674</td>\n",
              "      <td>-0.191906</td>\n",
              "      <td>1.216664</td>\n",
              "      <td>-0.176416</td>\n",
              "      <td>0.321906</td>\n",
              "      <td>-0.459738</td>\n",
              "      <td>-0.587782</td>\n",
              "      <td>-0.411085</td>\n",
              "      <td>-1.622847</td>\n",
              "      <td>-0.169560</td>\n",
              "      <td>-0.924677</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>399996</th>\n",
              "      <td>-0.397840</td>\n",
              "      <td>-0.666695</td>\n",
              "      <td>2.624063</td>\n",
              "      <td>-0.044149</td>\n",
              "      <td>0.579960</td>\n",
              "      <td>0.805623</td>\n",
              "      <td>-0.138129</td>\n",
              "      <td>0.687243</td>\n",
              "      <td>-0.021423</td>\n",
              "      <td>-0.123943</td>\n",
              "      <td>-0.021885</td>\n",
              "      <td>0.951762</td>\n",
              "      <td>0.174580</td>\n",
              "      <td>-0.587782</td>\n",
              "      <td>-0.282347</td>\n",
              "      <td>0.306337</td>\n",
              "      <td>0.531939</td>\n",
              "      <td>-0.241805</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>399997</th>\n",
              "      <td>0.902168</td>\n",
              "      <td>0.996457</td>\n",
              "      <td>-0.018564</td>\n",
              "      <td>0.039366</td>\n",
              "      <td>-0.379315</td>\n",
              "      <td>-0.566414</td>\n",
              "      <td>-0.129758</td>\n",
              "      <td>-0.612674</td>\n",
              "      <td>-0.184157</td>\n",
              "      <td>-0.964869</td>\n",
              "      <td>-0.177727</td>\n",
              "      <td>0.321906</td>\n",
              "      <td>0.063873</td>\n",
              "      <td>0.617819</td>\n",
              "      <td>-0.024870</td>\n",
              "      <td>-0.507120</td>\n",
              "      <td>0.204746</td>\n",
              "      <td>0.189125</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>399998</th>\n",
              "      <td>0.902168</td>\n",
              "      <td>0.996457</td>\n",
              "      <td>-0.733731</td>\n",
              "      <td>-0.333787</td>\n",
              "      <td>-0.360708</td>\n",
              "      <td>-1.045660</td>\n",
              "      <td>-0.155546</td>\n",
              "      <td>-0.612674</td>\n",
              "      <td>-0.191906</td>\n",
              "      <td>1.557909</td>\n",
              "      <td>-0.177727</td>\n",
              "      <td>0.321906</td>\n",
              "      <td>2.463511</td>\n",
              "      <td>0.617819</td>\n",
              "      <td>-0.411085</td>\n",
              "      <td>0.817979</td>\n",
              "      <td>0.102077</td>\n",
              "      <td>-0.132376</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>399999</th>\n",
              "      <td>-1.697847</td>\n",
              "      <td>-1.498271</td>\n",
              "      <td>-0.943048</td>\n",
              "      <td>-0.333787</td>\n",
              "      <td>-0.273877</td>\n",
              "      <td>-0.449222</td>\n",
              "      <td>-0.164536</td>\n",
              "      <td>-0.612674</td>\n",
              "      <td>-0.191906</td>\n",
              "      <td>-0.331127</td>\n",
              "      <td>-0.169640</td>\n",
              "      <td>-2.197521</td>\n",
              "      <td>-0.459738</td>\n",
              "      <td>0.617819</td>\n",
              "      <td>1.262513</td>\n",
              "      <td>-3.542297</td>\n",
              "      <td>0.828494</td>\n",
              "      <td>0.740512</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>400000 rows × 18 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-9f02b995-a8ef-430b-ac7b-96f12532218c')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-9f02b995-a8ef-430b-ac7b-96f12532218c button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-9f02b995-a8ef-430b-ac7b-96f12532218c');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "training = std_train_df.iloc[: , 0:18]\n",
        "testing = std_test_df.iloc[: , 0:18]\n",
        "labelsForTrain=training_withaim.iloc[: , -1]\n",
        "labelsForTest=testing_withaim.iloc[: , -1]\n",
        "input_shape = training.shape"
      ],
      "metadata": {
        "id": "5rblUnW1VXFc"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QvKzHGkaWDzX",
        "outputId": "049bc0dc-18bb-4732-af23-6d3cf3fa9f33"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(400000, 18)"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def create_model(activation, num_hidden_layer, num_hidden_unit):\n",
        "  inputs = Input(shape=(training.shape[1],))\n",
        "  model = inputs\n",
        "  for i in range(1,num_hidden_layer):\n",
        "    model = Dense(num_hidden_unit, activation=activation,)(model)\n",
        "        \n",
        "        \n",
        "  model = Dense(1,)(model)\n",
        "  model = Model(inputs, model)\n",
        "\n",
        "  return model"
      ],
      "metadata": {
        "id": "p3kN7ae7O-Pt"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_model_cnn(activation, num_hidden_layer, num_hidden_unit, kernel_size, filter):\n",
        "  model_cnn = Sequential()\n",
        "  model_cnn.add(Conv1D(filter, kernel_size, activation=activation, input_shape=(18,1)))\n",
        "  model_cnn.add(MaxPooling1D())\n",
        "  model_cnn.add(Flatten())\n",
        "  for i in range(1,num_hidden_layer):\n",
        "    model_cnn.add(Dense(num_hidden_unit, activation=activation,))\n",
        "\n",
        "  model_cnn.add(Dense(1))\n",
        "\n",
        "  return model_cnn"
      ],
      "metadata": {
        "id": "H1iEIoI9WcY8"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def objective_cnn(trial):\n",
        "  K.clear_session()\n",
        "    \n",
        "  activation = trial.suggest_categorical('activation',['relu','tanh','linear'])\n",
        "  optimizer = trial.suggest_categorical('optimizer',['adam','rmsprop','adagrad', 'sgd'])\n",
        "    \n",
        "  num_hidden_layer = trial.suggest_int('num_hidden_layer',1,5)\n",
        "  num_hidden_unit = trial.suggest_int('num_hidden_unit',10,100)\n",
        "  kernel_size = trial.suggest_int('kernel_size',1,5)\n",
        "  filter = trial.suggest_int('filter',1,100)\n",
        "    \n",
        "  learning_rate = trial.suggest_loguniform('learning_rate', 0.00001,0.1)\n",
        "  if optimizer == 'adam':\n",
        "    optimizer = Adam(learning_rate=learning_rate)\n",
        "  elif optimizer == 'adagrad':\n",
        "    optimizer = Adagrad(learning_rate=learning_rate)\n",
        "  elif optimizer =='rmsprop':\n",
        "    optimizer = RMSprop(learning_rate=learning_rate)\n",
        "  elif optimizer =='sgd':\n",
        "    optimizer = SGD(learning_rate=learning_rate)\n",
        "    \n",
        "  num_folds = 3\n",
        "  kfold=KFold(n_splits=3,shuffle=True)\n",
        "  fold_no=1\n",
        "  loss_per_fold = []\n",
        "  es = EarlyStopping(monitor='mse', patience=50)\n",
        "  model = create_model_cnn(activation, num_hidden_layer, num_hidden_unit, kernel_size, filter)\n",
        "  model_list_cnn.append(model)\n",
        "  model.compile(loss='mse',optimizer=optimizer,metrics=['mse','mae'])\n",
        "  for train,test in kfold.split(training,labelsForTrain):\n",
        "    scores=model.evaluate(testing,labelsForTest,verbose=0)\n",
        "\n",
        "    print('------------------------------------------------------------------------')\n",
        "    print(f'Training for fold {fold_no} ...')\n",
        "  \n",
        "    # Fit data to model\n",
        "    history = model.fit(training, labelsForTrain,\n",
        "                batch_size=20,\n",
        "                epochs=10,\n",
        "                verbose=2,\n",
        "                callbacks=[es])\n",
        "    \n",
        "    print(f'Score for fold {fold_no}: {model.metrics_names[0]} of {scores[0]}')\n",
        "    loss_per_fold.append(scores[0])\n",
        "    # Increase fold number\n",
        "    fold_no = fold_no + 1  \n",
        "  \n",
        "  history_list_cnn.append(history)\n",
        "    \n",
        "  mse = np.array(history.history['mse'])\n",
        "    \n",
        "  return mse[-1]"
      ],
      "metadata": {
        "id": "VDfPgkNyWfoW"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def objective(trial):\n",
        "  K.clear_session()\n",
        "    \n",
        "  activation = trial.suggest_categorical('activation',['relu','tanh','linear'])\n",
        "  optimizer = trial.suggest_categorical('optimizer',['adam','rmsprop','adagrad', 'sgd'])\n",
        "    \n",
        "  num_hidden_layer = trial.suggest_int('num_hidden_layer',1,5)\n",
        "  #get more features per layer, add num of hidden unit if have time\n",
        "  num_hidden_unit = trial.suggest_int('num_hidden_unit',10,100)\n",
        "    \n",
        "  learning_rate = trial.suggest_loguniform('learning_rate', 0.00001,0.1)\n",
        "  if optimizer == 'adam':\n",
        "    optimizer = Adam(learning_rate=learning_rate)\n",
        "  elif optimizer == 'adagrad':\n",
        "    optimizer = Adagrad(learning_rate=learning_rate)\n",
        "  elif optimizer =='rmsprop':\n",
        "    optimizer = RMSprop(learning_rate=learning_rate)\n",
        "  elif optimizer =='sgd':\n",
        "    optimizer = SGD(learning_rate=learning_rate)\n",
        "    \n",
        "  num_folds = 3\n",
        "  kfold=KFold(n_splits=3,shuffle=True)\n",
        "  fold_no=1\n",
        "  loss_per_fold = []\n",
        "  es = EarlyStopping(monitor='mse', patience=50)\n",
        "  model = create_model(activation, num_hidden_layer, num_hidden_unit)\n",
        "  model_list.append(model)\n",
        "  model.compile(loss='mse',optimizer=optimizer,metrics=['mse','mae'])\n",
        "  for train,test in kfold.split(training,labelsForTrain):\n",
        "    scores=model.evaluate(testing,labelsForTest,verbose=0)\n",
        "\n",
        "    print('------------------------------------------------------------------------')\n",
        "    print(f'Training for fold {fold_no} ...')\n",
        "  \n",
        "    # Fit data to model\n",
        "    history = model.fit(training, labelsForTrain,\n",
        "                batch_size=20,\n",
        "                epochs=10,\n",
        "                verbose=2,\n",
        "                callbacks=[es])\n",
        "    \n",
        "    print(f'Score for fold {fold_no}: {model.metrics_names[0]} of {scores[0]}')\n",
        "    loss_per_fold.append(scores[0])\n",
        "    # Increase fold number\n",
        "    fold_no = fold_no + 1  \n",
        "  \n",
        "  history_list.append(history)\n",
        "    \n",
        "  mse = np.array(history.history['mse'])\n",
        "    \n",
        "  return mse[-1]"
      ],
      "metadata": {
        "id": "bDu0tWUvWgyF"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_list_cnn=[]\n",
        "history_list_cnn=[]\n",
        "study_name = 'CNN_study'\n",
        "study = optuna.create_study(study_name=study_name, load_if_exists=True)\n",
        "study.optimize(objective_cnn, n_trials=50, )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hPSkjO23Wj5p",
        "outputId": "424c26a2-1e32-498c-fc9d-df41e3633f46"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 39s - loss: 18.1846 - mse: 18.1846 - mae: 1.6748 - 39s/epoch - 2ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 39s - loss: 18.1968 - mse: 18.1968 - mae: 1.6916 - 39s/epoch - 2ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 41s - loss: 17.9370 - mse: 17.9370 - mae: 1.6810 - 41s/epoch - 2ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 38s - loss: 17.7152 - mse: 17.7152 - mae: 1.6856 - 38s/epoch - 2ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 38s - loss: 17.9484 - mse: 17.9484 - mae: 1.6666 - 38s/epoch - 2ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 38s - loss: 17.6137 - mse: 17.6137 - mae: 1.6608 - 38s/epoch - 2ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 37s - loss: 17.6848 - mse: 17.6848 - mae: 1.6780 - 37s/epoch - 2ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 37s - loss: 17.8561 - mse: 17.8561 - mae: 1.6794 - 37s/epoch - 2ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 36s - loss: 18.0299 - mse: 18.0299 - mae: 1.7310 - 36s/epoch - 2ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 37s - loss: 18.4054 - mse: 18.4054 - mae: 1.7309 - 37s/epoch - 2ms/step\n",
            "Score for fold 1: loss of 25.36672592163086\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 37s - loss: 17.9831 - mse: 17.9831 - mae: 1.7217 - 37s/epoch - 2ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 37s - loss: 18.0957 - mse: 18.0957 - mae: 1.7268 - 37s/epoch - 2ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 37s - loss: 18.3610 - mse: 18.3610 - mae: 1.7569 - 37s/epoch - 2ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 37s - loss: 18.4121 - mse: 18.4121 - mae: 1.7804 - 37s/epoch - 2ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 37s - loss: 19.2244 - mse: 19.2244 - mae: 1.8138 - 37s/epoch - 2ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 37s - loss: 18.7338 - mse: 18.7338 - mae: 1.7806 - 37s/epoch - 2ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 36s - loss: 19.0049 - mse: 19.0049 - mae: 1.8174 - 36s/epoch - 2ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 36s - loss: 18.8103 - mse: 18.8103 - mae: 1.7884 - 36s/epoch - 2ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 36s - loss: 18.8508 - mse: 18.8508 - mae: 1.7836 - 36s/epoch - 2ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 36s - loss: 18.9176 - mse: 18.9176 - mae: 1.8104 - 36s/epoch - 2ms/step\n",
            "Score for fold 2: loss of 13.956217765808105\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 36s - loss: 19.4610 - mse: 19.4610 - mae: 1.8388 - 36s/epoch - 2ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 36s - loss: 20.3769 - mse: 20.3769 - mae: 1.8884 - 36s/epoch - 2ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 36s - loss: 19.1540 - mse: 19.1540 - mae: 1.8033 - 36s/epoch - 2ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 36s - loss: 20.1172 - mse: 20.1172 - mae: 1.8763 - 36s/epoch - 2ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 36s - loss: 19.1629 - mse: 19.1629 - mae: 1.8361 - 36s/epoch - 2ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 36s - loss: 20.0430 - mse: 20.0430 - mae: 1.8656 - 36s/epoch - 2ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 36s - loss: 19.4236 - mse: 19.4236 - mae: 1.8391 - 36s/epoch - 2ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 36s - loss: 19.7096 - mse: 19.7096 - mae: 1.8486 - 36s/epoch - 2ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 35s - loss: 19.1369 - mse: 19.1369 - mae: 1.8329 - 35s/epoch - 2ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 36s - loss: 19.5105 - mse: 19.5105 - mae: 1.8471 - 36s/epoch - 2ms/step\n",
            "Score for fold 3: loss of 15.548145294189453\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-09-30 03:23:48,408]\u001b[0m Finished trial#0 resulted in value: 19.510541915893555. Current best value is 19.510541915893555 with parameters: {'activation': 'tanh', 'optimizer': 'adam', 'num_hidden_layer': 5, 'num_hidden_unit': 35, 'kernel_size': 5, 'filter': 60, 'learning_rate': 0.004538346443656373}.\u001b[0m\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 29s - loss: 18.1093 - mse: 18.1093 - mae: 1.6580 - 29s/epoch - 1ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 28s - loss: 17.7154 - mse: 17.7154 - mae: 1.6366 - 28s/epoch - 1ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 29s - loss: 17.5829 - mse: 17.5829 - mae: 1.6334 - 29s/epoch - 1ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 29s - loss: 17.8670 - mse: 17.8670 - mae: 1.6332 - 29s/epoch - 1ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 28s - loss: 17.5546 - mse: 17.5546 - mae: 1.6355 - 28s/epoch - 1ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 29s - loss: 17.6939 - mse: 17.6939 - mae: 1.6335 - 29s/epoch - 1ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 29s - loss: 17.4680 - mse: 17.4680 - mae: 1.6288 - 29s/epoch - 1ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 28s - loss: 17.9172 - mse: 17.9172 - mae: 1.6347 - 28s/epoch - 1ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 28s - loss: 17.5001 - mse: 17.5001 - mae: 1.6333 - 28s/epoch - 1ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 28s - loss: 19.1402 - mse: 19.1402 - mae: 1.6378 - 28s/epoch - 1ms/step\n",
            "Score for fold 1: loss of 23.236820220947266\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 27s - loss: 17.5051 - mse: 17.5051 - mae: 1.6307 - 27s/epoch - 1ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 27s - loss: 17.5873 - mse: 17.5873 - mae: 1.6360 - 27s/epoch - 1ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 28s - loss: 17.8733 - mse: 17.8733 - mae: 1.6361 - 28s/epoch - 1ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 28s - loss: 17.5823 - mse: 17.5823 - mae: 1.6358 - 28s/epoch - 1ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 28s - loss: 17.5613 - mse: 17.5613 - mae: 1.6348 - 28s/epoch - 1ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 29s - loss: 17.6490 - mse: 17.6490 - mae: 1.6332 - 29s/epoch - 1ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 29s - loss: 17.6356 - mse: 17.6356 - mae: 1.6337 - 29s/epoch - 1ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 27s - loss: 17.8235 - mse: 17.8235 - mae: 1.6352 - 27s/epoch - 1ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 28s - loss: 17.6647 - mse: 17.6647 - mae: 1.6376 - 28s/epoch - 1ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 27s - loss: 17.6882 - mse: 17.6882 - mae: 1.6375 - 27s/epoch - 1ms/step\n",
            "Score for fold 2: loss of 12.43328857421875\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 27s - loss: 17.9548 - mse: 17.9548 - mae: 1.6309 - 27s/epoch - 1ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 28s - loss: 17.6919 - mse: 17.6919 - mae: 1.6348 - 28s/epoch - 1ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 28s - loss: 17.6847 - mse: 17.6847 - mae: 1.6351 - 28s/epoch - 1ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 28s - loss: 17.4210 - mse: 17.4210 - mae: 1.6325 - 28s/epoch - 1ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 28s - loss: 17.5600 - mse: 17.5600 - mae: 1.6371 - 28s/epoch - 1ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 28s - loss: 17.6582 - mse: 17.6582 - mae: 1.6342 - 28s/epoch - 1ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 27s - loss: 17.5296 - mse: 17.5296 - mae: 1.6333 - 27s/epoch - 1ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 28s - loss: 17.6474 - mse: 17.6474 - mae: 1.6364 - 28s/epoch - 1ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 28s - loss: 17.6299 - mse: 17.6299 - mae: 1.6362 - 28s/epoch - 1ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 28s - loss: 17.7411 - mse: 17.7411 - mae: 1.6394 - 28s/epoch - 1ms/step\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-09-30 03:37:56,692]\u001b[0m Finished trial#1 resulted in value: 17.741100311279297. Current best value is 17.741100311279297 with parameters: {'activation': 'linear', 'optimizer': 'rmsprop', 'num_hidden_layer': 5, 'num_hidden_unit': 96, 'kernel_size': 2, 'filter': 14, 'learning_rate': 0.004102079349565866}.\u001b[0m\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Score for fold 3: loss of 12.804410934448242\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 29s - loss: 19.9669 - mse: 19.9669 - mae: 1.7517 - 29s/epoch - 1ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 28s - loss: 18.9033 - mse: 18.9033 - mae: 1.6926 - 28s/epoch - 1ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 29s - loss: 18.6682 - mse: 18.6682 - mae: 1.6692 - 29s/epoch - 1ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 29s - loss: 18.5256 - mse: 18.5256 - mae: 1.6599 - 29s/epoch - 1ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 30s - loss: 18.4173 - mse: 18.4173 - mae: 1.6531 - 30s/epoch - 1ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 30s - loss: 18.3251 - mse: 18.3251 - mae: 1.6463 - 30s/epoch - 2ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 30s - loss: 18.2454 - mse: 18.2454 - mae: 1.6404 - 30s/epoch - 1ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 30s - loss: 18.1761 - mse: 18.1761 - mae: 1.6351 - 30s/epoch - 2ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 31s - loss: 18.1150 - mse: 18.1150 - mae: 1.6352 - 31s/epoch - 2ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 30s - loss: 18.0596 - mse: 18.0596 - mae: 1.6330 - 30s/epoch - 2ms/step\n",
            "Score for fold 1: loss of 23.6226749420166\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 29s - loss: 18.0095 - mse: 18.0095 - mae: 1.6297 - 29s/epoch - 1ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 30s - loss: 17.9636 - mse: 17.9636 - mae: 1.6276 - 30s/epoch - 1ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 29s - loss: 17.9210 - mse: 17.9210 - mae: 1.6283 - 29s/epoch - 1ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 30s - loss: 17.8814 - mse: 17.8814 - mae: 1.6246 - 30s/epoch - 2ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 30s - loss: 17.8441 - mse: 17.8441 - mae: 1.6247 - 30s/epoch - 2ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 29s - loss: 17.8091 - mse: 17.8091 - mae: 1.6238 - 29s/epoch - 1ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 29s - loss: 17.7759 - mse: 17.7759 - mae: 1.6224 - 29s/epoch - 1ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 28s - loss: 17.7443 - mse: 17.7443 - mae: 1.6218 - 28s/epoch - 1ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 27s - loss: 17.7137 - mse: 17.7137 - mae: 1.6187 - 27s/epoch - 1ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 28s - loss: 17.6838 - mse: 17.6838 - mae: 1.6198 - 28s/epoch - 1ms/step\n",
            "Score for fold 2: loss of 13.336681365966797\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 28s - loss: 17.6554 - mse: 17.6554 - mae: 1.6189 - 28s/epoch - 1ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 28s - loss: 17.6279 - mse: 17.6279 - mae: 1.6191 - 28s/epoch - 1ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 28s - loss: 17.6014 - mse: 17.6014 - mae: 1.6179 - 28s/epoch - 1ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 28s - loss: 17.5759 - mse: 17.5759 - mae: 1.6162 - 28s/epoch - 1ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 28s - loss: 17.5514 - mse: 17.5514 - mae: 1.6154 - 28s/epoch - 1ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 27s - loss: 17.5276 - mse: 17.5276 - mae: 1.6161 - 27s/epoch - 1ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 28s - loss: 17.5050 - mse: 17.5050 - mae: 1.6160 - 28s/epoch - 1ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 28s - loss: 17.4838 - mse: 17.4838 - mae: 1.6122 - 28s/epoch - 1ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 28s - loss: 17.4637 - mse: 17.4637 - mae: 1.6136 - 28s/epoch - 1ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 27s - loss: 17.4449 - mse: 17.4449 - mae: 1.6136 - 27s/epoch - 1ms/step\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-09-30 03:52:31,628]\u001b[0m Finished trial#2 resulted in value: 17.444852828979492. Current best value is 17.444852828979492 with parameters: {'activation': 'relu', 'optimizer': 'adagrad', 'num_hidden_layer': 5, 'num_hidden_unit': 20, 'kernel_size': 1, 'filter': 54, 'learning_rate': 0.0002886793263424461}.\u001b[0m\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Score for fold 3: loss of 12.959259033203125\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 43s - loss: 23.7443 - mse: 23.7443 - mae: 1.8900 - 43s/epoch - 2ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 43s - loss: 20.4917 - mse: 20.4917 - mae: 1.6693 - 43s/epoch - 2ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 42s - loss: 19.9146 - mse: 19.9146 - mae: 1.7378 - 42s/epoch - 2ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 41s - loss: 19.6980 - mse: 19.6980 - mae: 1.7564 - 41s/epoch - 2ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 40s - loss: 19.5529 - mse: 19.5529 - mae: 1.7537 - 40s/epoch - 2ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 41s - loss: 19.4369 - mse: 19.4369 - mae: 1.7455 - 41s/epoch - 2ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 41s - loss: 19.3402 - mse: 19.3402 - mae: 1.7386 - 41s/epoch - 2ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 41s - loss: 19.2574 - mse: 19.2574 - mae: 1.7308 - 41s/epoch - 2ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 41s - loss: 19.1855 - mse: 19.1855 - mae: 1.7239 - 41s/epoch - 2ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 40s - loss: 19.1214 - mse: 19.1214 - mae: 1.7167 - 40s/epoch - 2ms/step\n",
            "Score for fold 1: loss of 23.847829818725586\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 40s - loss: 19.0642 - mse: 19.0642 - mae: 1.7084 - 40s/epoch - 2ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 41s - loss: 19.0131 - mse: 19.0131 - mae: 1.7042 - 41s/epoch - 2ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 42s - loss: 18.9671 - mse: 18.9671 - mae: 1.6989 - 42s/epoch - 2ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 42s - loss: 18.9249 - mse: 18.9249 - mae: 1.6948 - 42s/epoch - 2ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 42s - loss: 18.8862 - mse: 18.8862 - mae: 1.6906 - 42s/epoch - 2ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 41s - loss: 18.8504 - mse: 18.8504 - mae: 1.6865 - 41s/epoch - 2ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 41s - loss: 18.8169 - mse: 18.8169 - mae: 1.6825 - 41s/epoch - 2ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 42s - loss: 18.7856 - mse: 18.7856 - mae: 1.6798 - 42s/epoch - 2ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 42s - loss: 18.7561 - mse: 18.7561 - mae: 1.6764 - 42s/epoch - 2ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 42s - loss: 18.7283 - mse: 18.7283 - mae: 1.6738 - 42s/epoch - 2ms/step\n",
            "Score for fold 2: loss of 14.452120780944824\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 42s - loss: 18.7020 - mse: 18.7020 - mae: 1.6708 - 42s/epoch - 2ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 43s - loss: 18.6772 - mse: 18.6772 - mae: 1.6683 - 43s/epoch - 2ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 42s - loss: 18.6535 - mse: 18.6535 - mae: 1.6664 - 42s/epoch - 2ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 42s - loss: 18.6309 - mse: 18.6309 - mae: 1.6650 - 42s/epoch - 2ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 41s - loss: 18.6094 - mse: 18.6094 - mae: 1.6615 - 41s/epoch - 2ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 41s - loss: 18.5886 - mse: 18.5886 - mae: 1.6600 - 41s/epoch - 2ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 41s - loss: 18.5686 - mse: 18.5686 - mae: 1.6583 - 41s/epoch - 2ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 41s - loss: 18.5491 - mse: 18.5491 - mae: 1.6563 - 41s/epoch - 2ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 41s - loss: 18.5303 - mse: 18.5303 - mae: 1.6558 - 41s/epoch - 2ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 41s - loss: 18.5120 - mse: 18.5120 - mae: 1.6534 - 41s/epoch - 2ms/step\n",
            "Score for fold 3: loss of 14.056706428527832\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-09-30 04:13:28,413]\u001b[0m Finished trial#3 resulted in value: 18.51199722290039. Current best value is 17.444852828979492 with parameters: {'activation': 'relu', 'optimizer': 'adagrad', 'num_hidden_layer': 5, 'num_hidden_unit': 20, 'kernel_size': 1, 'filter': 54, 'learning_rate': 0.0002886793263424461}.\u001b[0m\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 33s - loss: nan - mse: nan - mae: nan - 33s/epoch - 2ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 33s - loss: nan - mse: nan - mae: nan - 33s/epoch - 2ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 33s - loss: nan - mse: nan - mae: nan - 33s/epoch - 2ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 34s - loss: nan - mse: nan - mae: nan - 34s/epoch - 2ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 32s - loss: nan - mse: nan - mae: nan - 32s/epoch - 2ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 34s - loss: nan - mse: nan - mae: nan - 34s/epoch - 2ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 33s - loss: nan - mse: nan - mae: nan - 33s/epoch - 2ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 33s - loss: nan - mse: nan - mae: nan - 33s/epoch - 2ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 33s - loss: nan - mse: nan - mae: nan - 33s/epoch - 2ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 33s - loss: nan - mse: nan - mae: nan - 33s/epoch - 2ms/step\n",
            "Score for fold 1: loss of 24.599163055419922\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 33s - loss: nan - mse: nan - mae: nan - 33s/epoch - 2ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 32s - loss: nan - mse: nan - mae: nan - 32s/epoch - 2ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 32s - loss: nan - mse: nan - mae: nan - 32s/epoch - 2ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 33s - loss: nan - mse: nan - mae: nan - 33s/epoch - 2ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 33s - loss: nan - mse: nan - mae: nan - 33s/epoch - 2ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 32s - loss: nan - mse: nan - mae: nan - 32s/epoch - 2ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 33s - loss: nan - mse: nan - mae: nan - 33s/epoch - 2ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 32s - loss: nan - mse: nan - mae: nan - 32s/epoch - 2ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 33s - loss: nan - mse: nan - mae: nan - 33s/epoch - 2ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 32s - loss: nan - mse: nan - mae: nan - 32s/epoch - 2ms/step\n",
            "Score for fold 2: loss of nan\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 32s - loss: nan - mse: nan - mae: nan - 32s/epoch - 2ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 33s - loss: nan - mse: nan - mae: nan - 33s/epoch - 2ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 32s - loss: nan - mse: nan - mae: nan - 32s/epoch - 2ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 33s - loss: nan - mse: nan - mae: nan - 33s/epoch - 2ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 34s - loss: nan - mse: nan - mae: nan - 34s/epoch - 2ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 33s - loss: nan - mse: nan - mae: nan - 33s/epoch - 2ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 32s - loss: nan - mse: nan - mae: nan - 32s/epoch - 2ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 33s - loss: nan - mse: nan - mae: nan - 33s/epoch - 2ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 33s - loss: nan - mse: nan - mae: nan - 33s/epoch - 2ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 32s - loss: nan - mse: nan - mae: nan - 32s/epoch - 2ms/step\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[33m[W 2022-09-30 04:30:01,080]\u001b[0m Setting status of trial#4 as TrialState.FAIL because the objective function returned nan.\u001b[0m\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Score for fold 3: loss of nan\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 34s - loss: 23707.2598 - mse: 23707.2598 - mae: 9.1901 - 34s/epoch - 2ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 34s - loss: 517241.6250 - mse: 517241.6250 - mae: 36.9493 - 34s/epoch - 2ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 34s - loss: 23368.9668 - mse: 23368.9668 - mae: 9.1042 - 34s/epoch - 2ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 34s - loss: 45231.0273 - mse: 45231.0273 - mae: 16.0267 - 34s/epoch - 2ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 34s - loss: 104860.2109 - mse: 104860.2109 - mae: 23.4389 - 34s/epoch - 2ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 34s - loss: 2743.0671 - mse: 2743.0671 - mae: 5.4888 - 34s/epoch - 2ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 34s - loss: 15259.8154 - mse: 15259.8154 - mae: 10.2614 - 34s/epoch - 2ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 34s - loss: 6728.3394 - mse: 6728.3394 - mae: 6.8161 - 34s/epoch - 2ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 34s - loss: 67151.9688 - mse: 67151.9688 - mae: 18.6490 - 34s/epoch - 2ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 34s - loss: 64645.1719 - mse: 64645.1719 - mae: 13.5229 - 34s/epoch - 2ms/step\n",
            "Score for fold 1: loss of 23.041975021362305\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 34s - loss: 679685.3125 - mse: 679685.3125 - mae: 53.4278 - 34s/epoch - 2ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 34s - loss: 116943.2969 - mse: 116943.2969 - mae: 23.8667 - 34s/epoch - 2ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 34s - loss: 11077.7080 - mse: 11077.7080 - mae: 9.6813 - 34s/epoch - 2ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 34s - loss: 13953.3096 - mse: 13953.3086 - mae: 8.9985 - 34s/epoch - 2ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 34s - loss: 251352.0469 - mse: 251352.0469 - mae: 33.5864 - 34s/epoch - 2ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 34s - loss: 158339.1406 - mse: 158339.1406 - mae: 31.3378 - 34s/epoch - 2ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 34s - loss: 25365.8887 - mse: 25365.8887 - mae: 15.6258 - 34s/epoch - 2ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 34s - loss: 21701.0664 - mse: 21701.0664 - mae: 11.5714 - 34s/epoch - 2ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 34s - loss: 13733.4775 - mse: 13733.4775 - mae: 8.5167 - 34s/epoch - 2ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 34s - loss: 14803.8105 - mse: 14803.8105 - mae: 10.0549 - 34s/epoch - 2ms/step\n",
            "Score for fold 2: loss of 20.017545700073242\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 34s - loss: 8352.1279 - mse: 8352.1279 - mae: 6.7632 - 34s/epoch - 2ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 34s - loss: 29268.4551 - mse: 29268.4551 - mae: 13.7519 - 34s/epoch - 2ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 35s - loss: 54813.3438 - mse: 54813.3438 - mae: 15.1766 - 35s/epoch - 2ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 34s - loss: 12058.0771 - mse: 12058.0771 - mae: 8.8205 - 34s/epoch - 2ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 34s - loss: 275611.7812 - mse: 275611.7812 - mae: 36.5977 - 34s/epoch - 2ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 34s - loss: 58948.2031 - mse: 58948.2031 - mae: 17.3195 - 34s/epoch - 2ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 34s - loss: 44595.0820 - mse: 44595.0820 - mae: 16.6128 - 34s/epoch - 2ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 34s - loss: 25842.8027 - mse: 25842.8027 - mae: 12.1169 - 34s/epoch - 2ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 34s - loss: 5365.7930 - mse: 5365.7930 - mae: 6.5479 - 34s/epoch - 2ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 34s - loss: 49720.6562 - mse: 49720.6562 - mae: 16.6892 - 34s/epoch - 2ms/step\n",
            "Score for fold 3: loss of 14.110597610473633\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-09-30 04:47:12,020]\u001b[0m Finished trial#5 resulted in value: 49720.65625. Current best value is 17.444852828979492 with parameters: {'activation': 'relu', 'optimizer': 'adagrad', 'num_hidden_layer': 5, 'num_hidden_unit': 20, 'kernel_size': 1, 'filter': 54, 'learning_rate': 0.0002886793263424461}.\u001b[0m\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 23s - loss: 18.3157 - mse: 18.3157 - mae: 1.6158 - 23s/epoch - 1ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 23s - loss: 17.4108 - mse: 17.4108 - mae: 1.5803 - 23s/epoch - 1ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 23s - loss: 17.1841 - mse: 17.1841 - mae: 1.5723 - 23s/epoch - 1ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 23s - loss: 17.0497 - mse: 17.0497 - mae: 1.5652 - 23s/epoch - 1ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 23s - loss: 16.9525 - mse: 16.9525 - mae: 1.5585 - 23s/epoch - 1ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 23s - loss: 16.8597 - mse: 16.8597 - mae: 1.5518 - 23s/epoch - 1ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 23s - loss: 16.7863 - mse: 16.7863 - mae: 1.5463 - 23s/epoch - 1ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 22s - loss: 16.7303 - mse: 16.7303 - mae: 1.5427 - 22s/epoch - 1ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 28s - loss: 16.6722 - mse: 16.6722 - mae: 1.5397 - 28s/epoch - 1ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 27s - loss: 16.6527 - mse: 16.6527 - mae: 1.5378 - 27s/epoch - 1ms/step\n",
            "Score for fold 1: loss of 24.213401794433594\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 25s - loss: 16.6170 - mse: 16.6170 - mae: 1.5363 - 25s/epoch - 1ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 25s - loss: 16.5841 - mse: 16.5841 - mae: 1.5343 - 25s/epoch - 1ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 24s - loss: 16.5510 - mse: 16.5510 - mae: 1.5339 - 24s/epoch - 1ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 23s - loss: 16.5344 - mse: 16.5344 - mae: 1.5322 - 23s/epoch - 1ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 22s - loss: 16.5040 - mse: 16.5040 - mae: 1.5318 - 22s/epoch - 1ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 22s - loss: 16.4861 - mse: 16.4861 - mae: 1.5307 - 22s/epoch - 1ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 22s - loss: 16.4586 - mse: 16.4586 - mae: 1.5297 - 22s/epoch - 1ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 22s - loss: 16.4304 - mse: 16.4304 - mae: 1.5297 - 22s/epoch - 1ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 23s - loss: 16.4119 - mse: 16.4119 - mae: 1.5289 - 23s/epoch - 1ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 22s - loss: 16.3936 - mse: 16.3936 - mae: 1.5272 - 22s/epoch - 1ms/step\n",
            "Score for fold 2: loss of 12.318285942077637\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 23s - loss: 16.3707 - mse: 16.3707 - mae: 1.5266 - 23s/epoch - 1ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 22s - loss: 16.3601 - mse: 16.3601 - mae: 1.5260 - 22s/epoch - 1ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 22s - loss: 16.3442 - mse: 16.3442 - mae: 1.5269 - 22s/epoch - 1ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 23s - loss: 16.3420 - mse: 16.3420 - mae: 1.5259 - 23s/epoch - 1ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 23s - loss: 16.3348 - mse: 16.3348 - mae: 1.5256 - 23s/epoch - 1ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 23s - loss: 16.3379 - mse: 16.3379 - mae: 1.5257 - 23s/epoch - 1ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 23s - loss: 16.3362 - mse: 16.3362 - mae: 1.5259 - 23s/epoch - 1ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 23s - loss: 16.3249 - mse: 16.3249 - mae: 1.5272 - 23s/epoch - 1ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 22s - loss: 16.3197 - mse: 16.3197 - mae: 1.5270 - 22s/epoch - 1ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 22s - loss: 16.2988 - mse: 16.2988 - mae: 1.5275 - 22s/epoch - 1ms/step\n",
            "Score for fold 3: loss of 12.039624214172363\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-09-30 04:58:56,594]\u001b[0m Finished trial#6 resulted in value: 16.298831939697266. Current best value is 16.298831939697266 with parameters: {'activation': 'tanh', 'optimizer': 'rmsprop', 'num_hidden_layer': 1, 'num_hidden_unit': 85, 'kernel_size': 3, 'filter': 41, 'learning_rate': 0.0001157368746299737}.\u001b[0m\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 24s - loss: 17.0585 - mse: 17.0585 - mae: 1.5826 - 24s/epoch - 1ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 24s - loss: 16.7975 - mse: 16.7975 - mae: 1.5697 - 24s/epoch - 1ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 24s - loss: 16.6752 - mse: 16.6752 - mae: 1.5677 - 24s/epoch - 1ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 24s - loss: 16.6036 - mse: 16.6036 - mae: 1.5660 - 24s/epoch - 1ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 25s - loss: 16.5738 - mse: 16.5738 - mae: 1.5645 - 25s/epoch - 1ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 24s - loss: 16.5447 - mse: 16.5447 - mae: 1.5647 - 24s/epoch - 1ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 24s - loss: 16.5153 - mse: 16.5153 - mae: 1.5656 - 24s/epoch - 1ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 24s - loss: 16.4330 - mse: 16.4330 - mae: 1.5666 - 24s/epoch - 1ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 24s - loss: 16.5386 - mse: 16.5386 - mae: 1.5662 - 24s/epoch - 1ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 25s - loss: 16.4521 - mse: 16.4521 - mae: 1.5675 - 25s/epoch - 1ms/step\n",
            "Score for fold 1: loss of 24.230140686035156\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 25s - loss: 16.4256 - mse: 16.4256 - mae: 1.5692 - 25s/epoch - 1ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 24s - loss: 16.4841 - mse: 16.4841 - mae: 1.5688 - 24s/epoch - 1ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 25s - loss: 16.3987 - mse: 16.3987 - mae: 1.5701 - 25s/epoch - 1ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 25s - loss: 16.3929 - mse: 16.3929 - mae: 1.5711 - 25s/epoch - 1ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 25s - loss: 16.3548 - mse: 16.3548 - mae: 1.5720 - 25s/epoch - 1ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 24s - loss: 16.4276 - mse: 16.4276 - mae: 1.5731 - 24s/epoch - 1ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 25s - loss: 16.3903 - mse: 16.3903 - mae: 1.5722 - 25s/epoch - 1ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 25s - loss: 16.3709 - mse: 16.3709 - mae: 1.5726 - 25s/epoch - 1ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 25s - loss: 16.3290 - mse: 16.3290 - mae: 1.5718 - 25s/epoch - 1ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 26s - loss: 16.4411 - mse: 16.4411 - mae: 1.5725 - 26s/epoch - 1ms/step\n",
            "Score for fold 2: loss of 11.810317039489746\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 26s - loss: 16.2764 - mse: 16.2764 - mae: 1.5731 - 26s/epoch - 1ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 27s - loss: 16.3162 - mse: 16.3162 - mae: 1.5742 - 27s/epoch - 1ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 27s - loss: 16.3398 - mse: 16.3398 - mae: 1.5744 - 27s/epoch - 1ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 27s - loss: 16.4030 - mse: 16.4030 - mae: 1.5751 - 27s/epoch - 1ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 27s - loss: 16.3134 - mse: 16.3134 - mae: 1.5753 - 27s/epoch - 1ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 27s - loss: 16.4523 - mse: 16.4523 - mae: 1.5787 - 27s/epoch - 1ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 27s - loss: 16.3257 - mse: 16.3257 - mae: 1.5782 - 27s/epoch - 1ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 27s - loss: 16.3350 - mse: 16.3350 - mae: 1.5798 - 27s/epoch - 1ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 27s - loss: 16.3046 - mse: 16.3046 - mae: 1.5804 - 27s/epoch - 1ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 26s - loss: 16.3594 - mse: 16.3594 - mae: 1.5802 - 26s/epoch - 1ms/step\n",
            "Score for fold 3: loss of 11.786415100097656\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-09-30 05:11:43,261]\u001b[0m Finished trial#7 resulted in value: 16.359373092651367. Current best value is 16.298831939697266 with parameters: {'activation': 'tanh', 'optimizer': 'rmsprop', 'num_hidden_layer': 1, 'num_hidden_unit': 85, 'kernel_size': 3, 'filter': 41, 'learning_rate': 0.0001157368746299737}.\u001b[0m\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 29s - loss: 17.0364 - mse: 17.0364 - mae: 1.5495 - 29s/epoch - 1ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 29s - loss: 16.4805 - mse: 16.4805 - mae: 1.5398 - 29s/epoch - 1ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 29s - loss: 16.3505 - mse: 16.3505 - mae: 1.5391 - 29s/epoch - 1ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 29s - loss: 16.1564 - mse: 16.1564 - mae: 1.5389 - 29s/epoch - 1ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 29s - loss: 16.1408 - mse: 16.1408 - mae: 1.5421 - 29s/epoch - 1ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 30s - loss: 16.1540 - mse: 16.1540 - mae: 1.5427 - 30s/epoch - 1ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 29s - loss: 16.1802 - mse: 16.1802 - mae: 1.5449 - 29s/epoch - 1ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 29s - loss: 16.1449 - mse: 16.1449 - mae: 1.5412 - 29s/epoch - 1ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 29s - loss: 16.1657 - mse: 16.1657 - mae: 1.5412 - 29s/epoch - 1ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 28s - loss: 16.1810 - mse: 16.1810 - mae: 1.5488 - 28s/epoch - 1ms/step\n",
            "Score for fold 1: loss of 22.873083114624023\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 29s - loss: 16.0885 - mse: 16.0885 - mae: 1.5514 - 29s/epoch - 1ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 29s - loss: 16.1368 - mse: 16.1368 - mae: 1.5487 - 29s/epoch - 1ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 29s - loss: 16.0383 - mse: 16.0383 - mae: 1.5459 - 29s/epoch - 1ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 29s - loss: 16.0410 - mse: 16.0410 - mae: 1.5449 - 29s/epoch - 1ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 29s - loss: 16.0543 - mse: 16.0543 - mae: 1.5439 - 29s/epoch - 1ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 29s - loss: 16.0436 - mse: 16.0436 - mae: 1.5438 - 29s/epoch - 1ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 28s - loss: 16.0657 - mse: 16.0657 - mae: 1.5441 - 28s/epoch - 1ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 29s - loss: 16.1055 - mse: 16.1055 - mae: 1.5487 - 29s/epoch - 1ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 28s - loss: 15.9861 - mse: 15.9861 - mae: 1.5452 - 28s/epoch - 1ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 28s - loss: 16.0693 - mse: 16.0693 - mae: 1.5463 - 28s/epoch - 1ms/step\n",
            "Score for fold 2: loss of 12.002470016479492\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 29s - loss: 16.0270 - mse: 16.0270 - mae: 1.5476 - 29s/epoch - 1ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 29s - loss: 16.0362 - mse: 16.0362 - mae: 1.5469 - 29s/epoch - 1ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 29s - loss: 15.9667 - mse: 15.9667 - mae: 1.5429 - 29s/epoch - 1ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 28s - loss: 15.9989 - mse: 15.9989 - mae: 1.5435 - 28s/epoch - 1ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 28s - loss: 16.0303 - mse: 16.0303 - mae: 1.5421 - 28s/epoch - 1ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 28s - loss: 15.9954 - mse: 15.9954 - mae: 1.5457 - 28s/epoch - 1ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 29s - loss: 16.0417 - mse: 16.0417 - mae: 1.5491 - 29s/epoch - 1ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 28s - loss: 15.9918 - mse: 15.9918 - mae: 1.5499 - 28s/epoch - 1ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 30s - loss: 16.0536 - mse: 16.0536 - mae: 1.5475 - 30s/epoch - 1ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 28s - loss: 15.9916 - mse: 15.9916 - mae: 1.5480 - 28s/epoch - 1ms/step\n",
            "Score for fold 3: loss of 11.647722244262695\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-09-30 05:26:16,284]\u001b[0m Finished trial#8 resulted in value: 15.991597175598145. Current best value is 15.991597175598145 with parameters: {'activation': 'tanh', 'optimizer': 'rmsprop', 'num_hidden_layer': 2, 'num_hidden_unit': 20, 'kernel_size': 5, 'filter': 73, 'learning_rate': 0.001434913997681979}.\u001b[0m\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 28s - loss: 19.3515 - mse: 19.3515 - mae: 1.6764 - 28s/epoch - 1ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 27s - loss: 18.3548 - mse: 18.3548 - mae: 1.6263 - 27s/epoch - 1ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 27s - loss: 17.7974 - mse: 17.7974 - mae: 1.5938 - 27s/epoch - 1ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 28s - loss: 17.4453 - mse: 17.4453 - mae: 1.5771 - 28s/epoch - 1ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 27s - loss: 17.2150 - mse: 17.2150 - mae: 1.5682 - 27s/epoch - 1ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 27s - loss: 17.0253 - mse: 17.0253 - mae: 1.5578 - 27s/epoch - 1ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 27s - loss: 16.8476 - mse: 16.8476 - mae: 1.5498 - 27s/epoch - 1ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 26s - loss: 16.6901 - mse: 16.6901 - mae: 1.5404 - 26s/epoch - 1ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 26s - loss: 16.5577 - mse: 16.5577 - mae: 1.5344 - 26s/epoch - 1ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 26s - loss: 16.4522 - mse: 16.4522 - mae: 1.5297 - 26s/epoch - 1ms/step\n",
            "Score for fold 1: loss of 22.717004776000977\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 27s - loss: 16.3598 - mse: 16.3598 - mae: 1.5263 - 27s/epoch - 1ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 27s - loss: 16.2823 - mse: 16.2823 - mae: 1.5226 - 27s/epoch - 1ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 26s - loss: 16.2084 - mse: 16.2084 - mae: 1.5197 - 26s/epoch - 1ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 26s - loss: 16.1420 - mse: 16.1420 - mae: 1.5171 - 26s/epoch - 1ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 27s - loss: 16.0786 - mse: 16.0786 - mae: 1.5149 - 27s/epoch - 1ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 26s - loss: 16.0271 - mse: 16.0271 - mae: 1.5123 - 26s/epoch - 1ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 26s - loss: 15.9753 - mse: 15.9753 - mae: 1.5108 - 26s/epoch - 1ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 26s - loss: 15.9312 - mse: 15.9312 - mae: 1.5082 - 26s/epoch - 1ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 26s - loss: 15.8870 - mse: 15.8870 - mae: 1.5072 - 26s/epoch - 1ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 27s - loss: 15.8481 - mse: 15.8481 - mae: 1.5048 - 27s/epoch - 1ms/step\n",
            "Score for fold 2: loss of 12.177204132080078\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 27s - loss: 15.8067 - mse: 15.8067 - mae: 1.5032 - 27s/epoch - 1ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 26s - loss: 15.7734 - mse: 15.7734 - mae: 1.5022 - 26s/epoch - 1ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 26s - loss: 15.7394 - mse: 15.7394 - mae: 1.5003 - 26s/epoch - 1ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 26s - loss: 15.7085 - mse: 15.7085 - mae: 1.4993 - 26s/epoch - 1ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 26s - loss: 15.6738 - mse: 15.6738 - mae: 1.4972 - 26s/epoch - 1ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 26s - loss: 15.6450 - mse: 15.6450 - mae: 1.4966 - 26s/epoch - 1ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 26s - loss: 15.6179 - mse: 15.6179 - mae: 1.4950 - 26s/epoch - 1ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 26s - loss: 15.5871 - mse: 15.5871 - mae: 1.4943 - 26s/epoch - 1ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 27s - loss: 15.5661 - mse: 15.5661 - mae: 1.4932 - 27s/epoch - 1ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 26s - loss: 15.5349 - mse: 15.5349 - mae: 1.4918 - 26s/epoch - 1ms/step\n",
            "Score for fold 3: loss of 11.68696403503418\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-09-30 05:39:42,596]\u001b[0m Finished trial#9 resulted in value: 15.534948348999023. Current best value is 15.534948348999023 with parameters: {'activation': 'tanh', 'optimizer': 'sgd', 'num_hidden_layer': 2, 'num_hidden_unit': 26, 'kernel_size': 2, 'filter': 58, 'learning_rate': 6.003867710257596e-05}.\u001b[0m\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 30s - loss: 17.7915 - mse: 17.7915 - mae: 1.6364 - 30s/epoch - 2ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 29s - loss: 17.0702 - mse: 17.0702 - mae: 1.6129 - 29s/epoch - 1ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 29s - loss: 16.9995 - mse: 16.9995 - mae: 1.6059 - 29s/epoch - 1ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 29s - loss: 16.9677 - mse: 16.9677 - mae: 1.6034 - 29s/epoch - 1ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 29s - loss: 16.9542 - mse: 16.9542 - mae: 1.6041 - 29s/epoch - 1ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 29s - loss: 16.8006 - mse: 16.8006 - mae: 1.5956 - 29s/epoch - 1ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 28s - loss: 16.8637 - mse: 16.8637 - mae: 1.5998 - 28s/epoch - 1ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 29s - loss: 16.8230 - mse: 16.8230 - mae: 1.5987 - 29s/epoch - 1ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 29s - loss: 16.7055 - mse: 16.7055 - mae: 1.5913 - 29s/epoch - 1ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 29s - loss: 16.6818 - mse: 16.6818 - mae: 1.5925 - 29s/epoch - 1ms/step\n",
            "Score for fold 1: loss of 24.460508346557617\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 29s - loss: 16.7965 - mse: 16.7965 - mae: 1.5917 - 29s/epoch - 1ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 29s - loss: 16.6404 - mse: 16.6404 - mae: 1.5855 - 29s/epoch - 1ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 29s - loss: 16.6089 - mse: 16.6089 - mae: 1.5880 - 29s/epoch - 1ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 28s - loss: 16.6708 - mse: 16.6708 - mae: 1.5862 - 28s/epoch - 1ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 28s - loss: 16.6018 - mse: 16.6018 - mae: 1.5887 - 28s/epoch - 1ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 29s - loss: 16.6495 - mse: 16.6495 - mae: 1.5834 - 29s/epoch - 1ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 28s - loss: 16.6945 - mse: 16.6945 - mae: 1.5874 - 28s/epoch - 1ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 28s - loss: 16.6242 - mse: 16.6242 - mae: 1.5859 - 28s/epoch - 1ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 28s - loss: 16.5863 - mse: 16.5863 - mae: 1.5849 - 28s/epoch - 1ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 29s - loss: 16.6057 - mse: 16.6057 - mae: 1.5847 - 29s/epoch - 1ms/step\n",
            "Score for fold 2: loss of 12.32502269744873\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 28s - loss: 16.6021 - mse: 16.6021 - mae: 1.5831 - 28s/epoch - 1ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 28s - loss: 16.4975 - mse: 16.4975 - mae: 1.5811 - 28s/epoch - 1ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 28s - loss: 16.4671 - mse: 16.4671 - mae: 1.5811 - 28s/epoch - 1ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 28s - loss: 16.5536 - mse: 16.5536 - mae: 1.5804 - 28s/epoch - 1ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 29s - loss: 16.5514 - mse: 16.5514 - mae: 1.5799 - 29s/epoch - 1ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 29s - loss: 16.7024 - mse: 16.7024 - mae: 1.5813 - 29s/epoch - 1ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 29s - loss: 16.4904 - mse: 16.4904 - mae: 1.5813 - 29s/epoch - 1ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 29s - loss: 16.5300 - mse: 16.5300 - mae: 1.5788 - 29s/epoch - 1ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 29s - loss: 16.5505 - mse: 16.5505 - mae: 1.5793 - 29s/epoch - 1ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 29s - loss: 16.4493 - mse: 16.4493 - mae: 1.5788 - 29s/epoch - 1ms/step\n",
            "Score for fold 3: loss of 11.873122215270996\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-09-30 05:54:13,753]\u001b[0m Finished trial#10 resulted in value: 16.449277877807617. Current best value is 15.534948348999023 with parameters: {'activation': 'tanh', 'optimizer': 'sgd', 'num_hidden_layer': 2, 'num_hidden_unit': 26, 'kernel_size': 2, 'filter': 58, 'learning_rate': 6.003867710257596e-05}.\u001b[0m\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 38s - loss: 19.7809 - mse: 19.7809 - mae: 1.6961 - 38s/epoch - 2ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 39s - loss: 18.6779 - mse: 18.6779 - mae: 1.6476 - 39s/epoch - 2ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 38s - loss: 18.1797 - mse: 18.1797 - mae: 1.6188 - 38s/epoch - 2ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 39s - loss: 17.7807 - mse: 17.7807 - mae: 1.5959 - 39s/epoch - 2ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 38s - loss: 17.5060 - mse: 17.5060 - mae: 1.5836 - 38s/epoch - 2ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 38s - loss: 17.3249 - mse: 17.3249 - mae: 1.5773 - 38s/epoch - 2ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 39s - loss: 17.1985 - mse: 17.1985 - mae: 1.5732 - 39s/epoch - 2ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 39s - loss: 17.0918 - mse: 17.0918 - mae: 1.5689 - 39s/epoch - 2ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 38s - loss: 16.9937 - mse: 16.9937 - mae: 1.5656 - 38s/epoch - 2ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 38s - loss: 16.8982 - mse: 16.8982 - mae: 1.5611 - 38s/epoch - 2ms/step\n",
            "Score for fold 1: loss of 23.323932647705078\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 39s - loss: 16.8052 - mse: 16.8052 - mae: 1.5558 - 39s/epoch - 2ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 37s - loss: 16.7148 - mse: 16.7148 - mae: 1.5522 - 37s/epoch - 2ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 38s - loss: 16.6248 - mse: 16.6248 - mae: 1.5477 - 38s/epoch - 2ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 39s - loss: 16.5421 - mse: 16.5421 - mae: 1.5436 - 39s/epoch - 2ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 39s - loss: 16.4595 - mse: 16.4595 - mae: 1.5390 - 39s/epoch - 2ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 38s - loss: 16.3889 - mse: 16.3889 - mae: 1.5357 - 38s/epoch - 2ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 39s - loss: 16.3211 - mse: 16.3211 - mae: 1.5320 - 39s/epoch - 2ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 40s - loss: 16.2601 - mse: 16.2601 - mae: 1.5294 - 40s/epoch - 2ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 39s - loss: 16.2082 - mse: 16.2082 - mae: 1.5267 - 39s/epoch - 2ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 39s - loss: 16.1559 - mse: 16.1559 - mae: 1.5237 - 39s/epoch - 2ms/step\n",
            "Score for fold 2: loss of 12.586248397827148\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 39s - loss: 16.1094 - mse: 16.1094 - mae: 1.5231 - 39s/epoch - 2ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 39s - loss: 16.0679 - mse: 16.0679 - mae: 1.5207 - 39s/epoch - 2ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 40s - loss: 16.0299 - mse: 16.0299 - mae: 1.5196 - 40s/epoch - 2ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 40s - loss: 15.9920 - mse: 15.9920 - mae: 1.5182 - 40s/epoch - 2ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 40s - loss: 15.9614 - mse: 15.9614 - mae: 1.5162 - 40s/epoch - 2ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 39s - loss: 15.9279 - mse: 15.9279 - mae: 1.5156 - 39s/epoch - 2ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 40s - loss: 15.9005 - mse: 15.9005 - mae: 1.5138 - 40s/epoch - 2ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 38s - loss: 15.8750 - mse: 15.8750 - mae: 1.5125 - 38s/epoch - 2ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 38s - loss: 15.8462 - mse: 15.8462 - mae: 1.5116 - 38s/epoch - 2ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 39s - loss: 15.8180 - mse: 15.8180 - mae: 1.5101 - 39s/epoch - 2ms/step\n",
            "Score for fold 3: loss of 12.196752548217773\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-09-30 06:13:50,928]\u001b[0m Finished trial#11 resulted in value: 15.81804084777832. Current best value is 15.534948348999023 with parameters: {'activation': 'tanh', 'optimizer': 'sgd', 'num_hidden_layer': 2, 'num_hidden_unit': 26, 'kernel_size': 2, 'filter': 58, 'learning_rate': 6.003867710257596e-05}.\u001b[0m\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 39s - loss: 20.3577 - mse: 20.3577 - mae: 1.7133 - 39s/epoch - 2ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 38s - loss: 19.0689 - mse: 19.0689 - mae: 1.6661 - 38s/epoch - 2ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 38s - loss: 18.7348 - mse: 18.7348 - mae: 1.6474 - 38s/epoch - 2ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 39s - loss: 18.4385 - mse: 18.4385 - mae: 1.6312 - 39s/epoch - 2ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 38s - loss: 18.1620 - mse: 18.1620 - mae: 1.6151 - 38s/epoch - 2ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 39s - loss: 17.9214 - mse: 17.9214 - mae: 1.6016 - 39s/epoch - 2ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 45s - loss: 17.7248 - mse: 17.7248 - mae: 1.5915 - 45s/epoch - 2ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 47s - loss: 17.5676 - mse: 17.5676 - mae: 1.5846 - 47s/epoch - 2ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 47s - loss: 17.4426 - mse: 17.4426 - mae: 1.5789 - 47s/epoch - 2ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 45s - loss: 17.3425 - mse: 17.3425 - mae: 1.5749 - 45s/epoch - 2ms/step\n",
            "Score for fold 1: loss of 23.607194900512695\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 40s - loss: 17.2598 - mse: 17.2598 - mae: 1.5728 - 40s/epoch - 2ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 40s - loss: 17.1883 - mse: 17.1883 - mae: 1.5708 - 40s/epoch - 2ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 40s - loss: 17.1254 - mse: 17.1254 - mae: 1.5682 - 40s/epoch - 2ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 38s - loss: 17.0652 - mse: 17.0652 - mae: 1.5666 - 38s/epoch - 2ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 37s - loss: 17.0083 - mse: 17.0083 - mae: 1.5637 - 37s/epoch - 2ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 38s - loss: 16.9518 - mse: 16.9518 - mae: 1.5603 - 38s/epoch - 2ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 39s - loss: 16.8992 - mse: 16.8992 - mae: 1.5597 - 39s/epoch - 2ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 38s - loss: 16.8441 - mse: 16.8441 - mae: 1.5562 - 38s/epoch - 2ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 39s - loss: 16.7913 - mse: 16.7913 - mae: 1.5534 - 39s/epoch - 2ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 39s - loss: 16.7373 - mse: 16.7373 - mae: 1.5502 - 39s/epoch - 2ms/step\n",
            "Score for fold 2: loss of 12.989178657531738\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 39s - loss: 16.6835 - mse: 16.6835 - mae: 1.5475 - 39s/epoch - 2ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 39s - loss: 16.6321 - mse: 16.6321 - mae: 1.5443 - 39s/epoch - 2ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 39s - loss: 16.5828 - mse: 16.5828 - mae: 1.5417 - 39s/epoch - 2ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 39s - loss: 16.5348 - mse: 16.5348 - mae: 1.5387 - 39s/epoch - 2ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 39s - loss: 16.4910 - mse: 16.4910 - mae: 1.5371 - 39s/epoch - 2ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 38s - loss: 16.4469 - mse: 16.4469 - mae: 1.5354 - 38s/epoch - 2ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 39s - loss: 16.4070 - mse: 16.4070 - mae: 1.5330 - 39s/epoch - 2ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 39s - loss: 16.3688 - mse: 16.3688 - mae: 1.5309 - 39s/epoch - 2ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 39s - loss: 16.3318 - mse: 16.3318 - mae: 1.5297 - 39s/epoch - 2ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 38s - loss: 16.2990 - mse: 16.2990 - mae: 1.5282 - 38s/epoch - 2ms/step\n",
            "Score for fold 3: loss of 12.471705436706543\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-09-30 06:33:53,423]\u001b[0m Finished trial#12 resulted in value: 16.29897689819336. Current best value is 15.534948348999023 with parameters: {'activation': 'tanh', 'optimizer': 'sgd', 'num_hidden_layer': 2, 'num_hidden_unit': 26, 'kernel_size': 2, 'filter': 58, 'learning_rate': 6.003867710257596e-05}.\u001b[0m\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 26s - loss: 18.9340 - mse: 18.9340 - mae: 1.6582 - 26s/epoch - 1ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 26s - loss: 17.5290 - mse: 17.5290 - mae: 1.5821 - 26s/epoch - 1ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 26s - loss: 17.0493 - mse: 17.0493 - mae: 1.5673 - 26s/epoch - 1ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 25s - loss: 16.7581 - mse: 16.7581 - mae: 1.5552 - 25s/epoch - 1ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 25s - loss: 16.5120 - mse: 16.5120 - mae: 1.5421 - 25s/epoch - 1ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 25s - loss: 16.3148 - mse: 16.3148 - mae: 1.5323 - 25s/epoch - 1ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 25s - loss: 16.1531 - mse: 16.1531 - mae: 1.5246 - 25s/epoch - 1ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 25s - loss: 16.0307 - mse: 16.0307 - mae: 1.5181 - 25s/epoch - 1ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 25s - loss: 15.9227 - mse: 15.9227 - mae: 1.5145 - 25s/epoch - 1ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 25s - loss: 15.8298 - mse: 15.8298 - mae: 1.5108 - 25s/epoch - 1ms/step\n",
            "Score for fold 1: loss of 24.428470611572266\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 25s - loss: 15.7581 - mse: 15.7581 - mae: 1.5085 - 25s/epoch - 1ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 25s - loss: 15.6932 - mse: 15.6932 - mae: 1.5042 - 25s/epoch - 1ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 24s - loss: 15.6295 - mse: 15.6295 - mae: 1.5015 - 24s/epoch - 1ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 25s - loss: 15.5851 - mse: 15.5851 - mae: 1.5000 - 25s/epoch - 1ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 25s - loss: 15.5208 - mse: 15.5208 - mae: 1.4971 - 25s/epoch - 1ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 25s - loss: 15.4873 - mse: 15.4873 - mae: 1.4963 - 25s/epoch - 1ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 24s - loss: 15.4392 - mse: 15.4392 - mae: 1.4937 - 24s/epoch - 1ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 26s - loss: 15.4043 - mse: 15.4043 - mae: 1.4940 - 26s/epoch - 1ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 25s - loss: 15.3563 - mse: 15.3563 - mae: 1.4908 - 25s/epoch - 1ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 25s - loss: 15.3323 - mse: 15.3323 - mae: 1.4910 - 25s/epoch - 1ms/step\n",
            "Score for fold 2: loss of 11.61575984954834\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 25s - loss: 15.2951 - mse: 15.2951 - mae: 1.4889 - 25s/epoch - 1ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 26s - loss: 15.2562 - mse: 15.2562 - mae: 1.4876 - 26s/epoch - 1ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 25s - loss: 15.2248 - mse: 15.2248 - mae: 1.4861 - 25s/epoch - 1ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 26s - loss: 15.2033 - mse: 15.2033 - mae: 1.4860 - 26s/epoch - 1ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 25s - loss: 15.1781 - mse: 15.1781 - mae: 1.4845 - 25s/epoch - 1ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 25s - loss: 15.1445 - mse: 15.1445 - mae: 1.4830 - 25s/epoch - 1ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 25s - loss: 15.1179 - mse: 15.1179 - mae: 1.4825 - 25s/epoch - 1ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 25s - loss: 15.0851 - mse: 15.0851 - mae: 1.4812 - 25s/epoch - 1ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 25s - loss: 15.0649 - mse: 15.0649 - mae: 1.4804 - 25s/epoch - 1ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 25s - loss: 15.0352 - mse: 15.0352 - mae: 1.4802 - 25s/epoch - 1ms/step\n",
            "Score for fold 3: loss of 11.134897232055664\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-09-30 06:46:39,099]\u001b[0m Finished trial#13 resulted in value: 15.035204887390137. Current best value is 15.035204887390137 with parameters: {'activation': 'tanh', 'optimizer': 'sgd', 'num_hidden_layer': 3, 'num_hidden_unit': 63, 'kernel_size': 2, 'filter': 25, 'learning_rate': 7.817684817378859e-05}.\u001b[0m\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 25s - loss: 18.5825 - mse: 18.5825 - mae: 1.6607 - 25s/epoch - 1ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 25s - loss: 17.4218 - mse: 17.4218 - mae: 1.6015 - 25s/epoch - 1ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 24s - loss: 17.0064 - mse: 17.0064 - mae: 1.5844 - 24s/epoch - 1ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 23s - loss: 16.7567 - mse: 16.7567 - mae: 1.5735 - 23s/epoch - 1ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 24s - loss: 16.5538 - mse: 16.5538 - mae: 1.5616 - 24s/epoch - 1ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 24s - loss: 16.3722 - mse: 16.3722 - mae: 1.5508 - 24s/epoch - 1ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 23s - loss: 16.2318 - mse: 16.2318 - mae: 1.5426 - 23s/epoch - 1ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 24s - loss: 16.1050 - mse: 16.1050 - mae: 1.5358 - 24s/epoch - 1ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 23s - loss: 16.0368 - mse: 16.0368 - mae: 1.5312 - 23s/epoch - 1ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 23s - loss: 15.9369 - mse: 15.9369 - mae: 1.5254 - 23s/epoch - 1ms/step\n",
            "Score for fold 1: loss of 22.99694061279297\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 24s - loss: 15.8515 - mse: 15.8515 - mae: 1.5214 - 24s/epoch - 1ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 23s - loss: 15.7972 - mse: 15.7972 - mae: 1.5181 - 23s/epoch - 1ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 23s - loss: 15.7598 - mse: 15.7598 - mae: 1.5162 - 23s/epoch - 1ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 24s - loss: 15.6858 - mse: 15.6858 - mae: 1.5112 - 24s/epoch - 1ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 24s - loss: 15.6401 - mse: 15.6401 - mae: 1.5080 - 24s/epoch - 1ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 24s - loss: 15.6454 - mse: 15.6454 - mae: 1.5089 - 24s/epoch - 1ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 23s - loss: 15.5428 - mse: 15.5428 - mae: 1.5051 - 23s/epoch - 1ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 24s - loss: 15.5578 - mse: 15.5578 - mae: 1.5057 - 24s/epoch - 1ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 24s - loss: 15.5150 - mse: 15.5150 - mae: 1.5012 - 24s/epoch - 1ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 24s - loss: 15.4892 - mse: 15.4892 - mae: 1.5017 - 24s/epoch - 1ms/step\n",
            "Score for fold 2: loss of 11.664942741394043\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 24s - loss: 15.4822 - mse: 15.4822 - mae: 1.5005 - 24s/epoch - 1ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 24s - loss: 15.4424 - mse: 15.4424 - mae: 1.4984 - 24s/epoch - 1ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 23s - loss: 15.4127 - mse: 15.4127 - mae: 1.4984 - 23s/epoch - 1ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 23s - loss: 15.4020 - mse: 15.4020 - mae: 1.4975 - 23s/epoch - 1ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 24s - loss: 15.3822 - mse: 15.3822 - mae: 1.4977 - 24s/epoch - 1ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 23s - loss: 15.3400 - mse: 15.3400 - mae: 1.4934 - 23s/epoch - 1ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 23s - loss: 15.3115 - mse: 15.3115 - mae: 1.4932 - 23s/epoch - 1ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 24s - loss: 15.2982 - mse: 15.2982 - mae: 1.4932 - 24s/epoch - 1ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 23s - loss: 15.2724 - mse: 15.2724 - mae: 1.4909 - 23s/epoch - 1ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 24s - loss: 15.2926 - mse: 15.2926 - mae: 1.4923 - 24s/epoch - 1ms/step\n",
            "Score for fold 3: loss of 11.227965354919434\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-09-30 06:58:37,946]\u001b[0m Finished trial#14 resulted in value: 15.292609214782715. Current best value is 15.035204887390137 with parameters: {'activation': 'tanh', 'optimizer': 'sgd', 'num_hidden_layer': 3, 'num_hidden_unit': 63, 'kernel_size': 2, 'filter': 25, 'learning_rate': 7.817684817378859e-05}.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 24s - loss: 18.4859 - mse: 18.4859 - mae: 1.6496 - 24s/epoch - 1ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 23s - loss: 17.2808 - mse: 17.2808 - mae: 1.5984 - 23s/epoch - 1ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 23s - loss: 16.9052 - mse: 16.9052 - mae: 1.5837 - 23s/epoch - 1ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 22s - loss: 16.6583 - mse: 16.6583 - mae: 1.5691 - 22s/epoch - 1ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 23s - loss: 16.4657 - mse: 16.4657 - mae: 1.5566 - 23s/epoch - 1ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 23s - loss: 16.2790 - mse: 16.2790 - mae: 1.5444 - 23s/epoch - 1ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 22s - loss: 16.1505 - mse: 16.1505 - mae: 1.5378 - 22s/epoch - 1ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 23s - loss: 16.0135 - mse: 16.0135 - mae: 1.5297 - 23s/epoch - 1ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 24s - loss: 15.9601 - mse: 15.9601 - mae: 1.5267 - 24s/epoch - 1ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 24s - loss: 15.8236 - mse: 15.8236 - mae: 1.5195 - 24s/epoch - 1ms/step\n",
            "Score for fold 1: loss of 24.304744720458984\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 23s - loss: 15.7689 - mse: 15.7689 - mae: 1.5164 - 23s/epoch - 1ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 24s - loss: 15.7706 - mse: 15.7706 - mae: 1.5169 - 24s/epoch - 1ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 23s - loss: 15.7095 - mse: 15.7095 - mae: 1.5120 - 23s/epoch - 1ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 23s - loss: 15.6648 - mse: 15.6648 - mae: 1.5104 - 23s/epoch - 1ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 26s - loss: 15.6094 - mse: 15.6094 - mae: 1.5084 - 26s/epoch - 1ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 22s - loss: 15.5940 - mse: 15.5940 - mae: 1.5087 - 22s/epoch - 1ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 23s - loss: 15.5621 - mse: 15.5621 - mae: 1.5052 - 23s/epoch - 1ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 23s - loss: 15.5734 - mse: 15.5734 - mae: 1.5068 - 23s/epoch - 1ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 24s - loss: 15.4507 - mse: 15.4507 - mae: 1.5011 - 24s/epoch - 1ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 25s - loss: 15.4658 - mse: 15.4658 - mae: 1.4999 - 25s/epoch - 1ms/step\n",
            "Score for fold 2: loss of 11.622879981994629\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 25s - loss: 15.4635 - mse: 15.4635 - mae: 1.5013 - 25s/epoch - 1ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 24s - loss: 15.4155 - mse: 15.4155 - mae: 1.4976 - 24s/epoch - 1ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 23s - loss: 15.3624 - mse: 15.3624 - mae: 1.4960 - 23s/epoch - 1ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 24s - loss: 15.3542 - mse: 15.3542 - mae: 1.4955 - 24s/epoch - 1ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 24s - loss: 15.3317 - mse: 15.3317 - mae: 1.4938 - 24s/epoch - 1ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 24s - loss: 15.3268 - mse: 15.3268 - mae: 1.4939 - 24s/epoch - 1ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 24s - loss: 15.3022 - mse: 15.3022 - mae: 1.4921 - 24s/epoch - 1ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 24s - loss: 15.2496 - mse: 15.2496 - mae: 1.4917 - 24s/epoch - 1ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 25s - loss: 15.2610 - mse: 15.2610 - mae: 1.4919 - 25s/epoch - 1ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 25s - loss: 15.2149 - mse: 15.2149 - mae: 1.4892 - 25s/epoch - 1ms/step\n",
            "Score for fold 3: loss of 11.266279220581055\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-09-30 07:10:38,315]\u001b[0m Finished trial#15 resulted in value: 15.214879035949707. Current best value is 15.035204887390137 with parameters: {'activation': 'tanh', 'optimizer': 'sgd', 'num_hidden_layer': 3, 'num_hidden_unit': 63, 'kernel_size': 2, 'filter': 25, 'learning_rate': 7.817684817378859e-05}.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 26s - loss: 17.9957 - mse: 17.9957 - mae: 1.6324 - 26s/epoch - 1ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 26s - loss: 16.5966 - mse: 16.5966 - mae: 1.5554 - 26s/epoch - 1ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 26s - loss: 16.3339 - mse: 16.3339 - mae: 1.5434 - 26s/epoch - 1ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 26s - loss: 16.0693 - mse: 16.0693 - mae: 1.5323 - 26s/epoch - 1ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 26s - loss: 15.9214 - mse: 15.9214 - mae: 1.5255 - 26s/epoch - 1ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 26s - loss: 15.8024 - mse: 15.8024 - mae: 1.5219 - 26s/epoch - 1ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 26s - loss: 15.8588 - mse: 15.8588 - mae: 1.5232 - 26s/epoch - 1ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 26s - loss: 15.7107 - mse: 15.7107 - mae: 1.5171 - 26s/epoch - 1ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 25s - loss: 15.7488 - mse: 15.7488 - mae: 1.5223 - 25s/epoch - 1ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 25s - loss: 15.5851 - mse: 15.5850 - mae: 1.5124 - 25s/epoch - 1ms/step\n",
            "Score for fold 1: loss of 24.514427185058594\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 25s - loss: 15.5789 - mse: 15.5789 - mae: 1.5134 - 25s/epoch - 1ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 24s - loss: 15.5197 - mse: 15.5197 - mae: 1.5114 - 24s/epoch - 1ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 25s - loss: 15.5949 - mse: 15.5949 - mae: 1.5148 - 25s/epoch - 1ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 24s - loss: 15.6506 - mse: 15.6506 - mae: 1.5192 - 24s/epoch - 1ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 24s - loss: 15.3946 - mse: 15.3946 - mae: 1.5083 - 24s/epoch - 1ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 24s - loss: 15.7740 - mse: 15.7740 - mae: 1.5273 - 24s/epoch - 1ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 25s - loss: 15.5137 - mse: 15.5137 - mae: 1.5170 - 25s/epoch - 1ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 24s - loss: 15.4706 - mse: 15.4706 - mae: 1.5126 - 24s/epoch - 1ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 24s - loss: 15.3133 - mse: 15.3133 - mae: 1.5073 - 24s/epoch - 1ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 25s - loss: 15.2558 - mse: 15.2558 - mae: 1.5061 - 25s/epoch - 1ms/step\n",
            "Score for fold 2: loss of 11.426512718200684\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 24s - loss: 15.3624 - mse: 15.3624 - mae: 1.5135 - 24s/epoch - 1ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 24s - loss: 15.4794 - mse: 15.4794 - mae: 1.5161 - 24s/epoch - 1ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 24s - loss: 15.3847 - mse: 15.3847 - mae: 1.5141 - 24s/epoch - 1ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 24s - loss: 15.2237 - mse: 15.2237 - mae: 1.5060 - 24s/epoch - 1ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 24s - loss: 15.1833 - mse: 15.1833 - mae: 1.5056 - 24s/epoch - 1ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 24s - loss: 15.1387 - mse: 15.1387 - mae: 1.5026 - 24s/epoch - 1ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 24s - loss: 15.0824 - mse: 15.0824 - mae: 1.5020 - 24s/epoch - 1ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 24s - loss: 14.9940 - mse: 14.9940 - mae: 1.4984 - 24s/epoch - 1ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 24s - loss: 14.9226 - mse: 14.9226 - mae: 1.4971 - 24s/epoch - 1ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 25s - loss: 14.9072 - mse: 14.9072 - mae: 1.4962 - 25s/epoch - 1ms/step\n",
            "Score for fold 3: loss of 12.217475891113281\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-09-30 07:23:10,683]\u001b[0m Finished trial#16 resulted in value: 14.90717601776123. Current best value is 14.90717601776123 with parameters: {'activation': 'tanh', 'optimizer': 'sgd', 'num_hidden_layer': 4, 'num_hidden_unit': 56, 'kernel_size': 3, 'filter': 7, 'learning_rate': 0.00032117094663704654}.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 24s - loss: 20.4651 - mse: 20.4651 - mae: 1.7468 - 24s/epoch - 1ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 24s - loss: 21.3690 - mse: 21.3690 - mae: 1.9071 - 24s/epoch - 1ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 24s - loss: 21.3690 - mse: 21.3690 - mae: 1.9073 - 24s/epoch - 1ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 24s - loss: 21.3673 - mse: 21.3673 - mae: 1.9077 - 24s/epoch - 1ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 23s - loss: 21.3677 - mse: 21.3677 - mae: 1.9070 - 23s/epoch - 1ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 23s - loss: 21.3678 - mse: 21.3678 - mae: 1.9083 - 23s/epoch - 1ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 23s - loss: 21.3677 - mse: 21.3677 - mae: 1.9079 - 23s/epoch - 1ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 22s - loss: 21.3671 - mse: 21.3671 - mae: 1.9079 - 22s/epoch - 1ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 23s - loss: 21.3670 - mse: 21.3670 - mae: 1.9074 - 23s/epoch - 1ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 23s - loss: 21.3668 - mse: 21.3668 - mae: 1.9085 - 23s/epoch - 1ms/step\n",
            "Score for fold 1: loss of 23.658903121948242\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 23s - loss: 21.3673 - mse: 21.3673 - mae: 1.9083 - 23s/epoch - 1ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 23s - loss: 21.3672 - mse: 21.3672 - mae: 1.9075 - 23s/epoch - 1ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 23s - loss: 21.3667 - mse: 21.3667 - mae: 1.9084 - 23s/epoch - 1ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 23s - loss: 21.3667 - mse: 21.3667 - mae: 1.9078 - 23s/epoch - 1ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 23s - loss: 21.3666 - mse: 21.3666 - mae: 1.9083 - 23s/epoch - 1ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 23s - loss: 21.3668 - mse: 21.3668 - mae: 1.9086 - 23s/epoch - 1ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 24s - loss: 21.3668 - mse: 21.3668 - mae: 1.9075 - 24s/epoch - 1ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 23s - loss: 21.3670 - mse: 21.3670 - mae: 1.9077 - 23s/epoch - 1ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 23s - loss: 21.3667 - mse: 21.3667 - mae: 1.9084 - 23s/epoch - 1ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 23s - loss: 21.3670 - mse: 21.3670 - mae: 1.9081 - 23s/epoch - 1ms/step\n",
            "Score for fold 2: loss of 16.962770462036133\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 23s - loss: 21.3662 - mse: 21.3662 - mae: 1.9084 - 23s/epoch - 1ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 23s - loss: 21.3666 - mse: 21.3666 - mae: 1.9081 - 23s/epoch - 1ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 23s - loss: 21.3663 - mse: 21.3663 - mae: 1.9068 - 23s/epoch - 1ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 23s - loss: 21.3663 - mse: 21.3663 - mae: 1.9091 - 23s/epoch - 1ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 23s - loss: 21.3666 - mse: 21.3666 - mae: 1.9079 - 23s/epoch - 1ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 23s - loss: 21.3662 - mse: 21.3662 - mae: 1.9081 - 23s/epoch - 1ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 23s - loss: 21.3662 - mse: 21.3662 - mae: 1.9083 - 23s/epoch - 1ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 23s - loss: 21.3668 - mse: 21.3668 - mae: 1.9082 - 23s/epoch - 1ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 23s - loss: 21.3671 - mse: 21.3671 - mae: 1.9084 - 23s/epoch - 1ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 23s - loss: 21.3662 - mse: 21.3662 - mae: 1.9070 - 23s/epoch - 1ms/step\n",
            "Score for fold 3: loss of 16.96268653869629\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-09-30 07:34:54,683]\u001b[0m Finished trial#17 resulted in value: 21.366195678710938. Current best value is 14.90717601776123 with parameters: {'activation': 'tanh', 'optimizer': 'sgd', 'num_hidden_layer': 4, 'num_hidden_unit': 56, 'kernel_size': 3, 'filter': 7, 'learning_rate': 0.00032117094663704654}.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 23s - loss: 19.1286 - mse: 19.1286 - mae: 1.7212 - 23s/epoch - 1ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 23s - loss: 18.1999 - mse: 18.1999 - mae: 1.6923 - 23s/epoch - 1ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 24s - loss: 17.6877 - mse: 17.6877 - mae: 1.6638 - 24s/epoch - 1ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 24s - loss: 17.5739 - mse: 17.5739 - mae: 1.6536 - 24s/epoch - 1ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 24s - loss: 17.5238 - mse: 17.5238 - mae: 1.6487 - 24s/epoch - 1ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 24s - loss: 17.5231 - mse: 17.5231 - mae: 1.6450 - 24s/epoch - 1ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 23s - loss: 17.5333 - mse: 17.5333 - mae: 1.6475 - 23s/epoch - 1ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 23s - loss: 17.5085 - mse: 17.5085 - mae: 1.6446 - 23s/epoch - 1ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 24s - loss: 17.4864 - mse: 17.4864 - mae: 1.6391 - 24s/epoch - 1ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 23s - loss: 17.3750 - mse: 17.3750 - mae: 1.6341 - 23s/epoch - 1ms/step\n",
            "Score for fold 1: loss of 23.228532791137695\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 24s - loss: 17.3600 - mse: 17.3600 - mae: 1.6351 - 24s/epoch - 1ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 25s - loss: 17.4989 - mse: 17.4989 - mae: 1.6414 - 25s/epoch - 1ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 24s - loss: 17.2873 - mse: 17.2873 - mae: 1.6261 - 24s/epoch - 1ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 23s - loss: 17.2357 - mse: 17.2357 - mae: 1.6242 - 23s/epoch - 1ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 23s - loss: 17.3051 - mse: 17.3051 - mae: 1.6231 - 23s/epoch - 1ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 23s - loss: 17.2768 - mse: 17.2768 - mae: 1.6240 - 23s/epoch - 1ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 23s - loss: 17.1755 - mse: 17.1755 - mae: 1.6178 - 23s/epoch - 1ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 23s - loss: 17.2292 - mse: 17.2292 - mae: 1.6188 - 23s/epoch - 1ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 23s - loss: 17.1713 - mse: 17.1713 - mae: 1.6166 - 23s/epoch - 1ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 23s - loss: 17.1343 - mse: 17.1343 - mae: 1.6137 - 23s/epoch - 1ms/step\n",
            "Score for fold 2: loss of 16.96833038330078\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 23s - loss: 17.1543 - mse: 17.1543 - mae: 1.6157 - 23s/epoch - 1ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 23s - loss: 17.0582 - mse: 17.0582 - mae: 1.6107 - 23s/epoch - 1ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 23s - loss: 19.7287 - mse: 19.7287 - mae: 1.7734 - 23s/epoch - 1ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 23s - loss: 18.7086 - mse: 18.7086 - mae: 1.6653 - 23s/epoch - 1ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 23s - loss: 18.3086 - mse: 18.3086 - mae: 1.6303 - 23s/epoch - 1ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 23s - loss: 18.1970 - mse: 18.1970 - mae: 1.6200 - 23s/epoch - 1ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 22s - loss: 18.1616 - mse: 18.1616 - mae: 1.6172 - 22s/epoch - 1ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 22s - loss: 18.1279 - mse: 18.1279 - mae: 1.6149 - 22s/epoch - 1ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 23s - loss: 18.0998 - mse: 18.0998 - mae: 1.6128 - 23s/epoch - 1ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 23s - loss: 18.0739 - mse: 18.0739 - mae: 1.6114 - 23s/epoch - 1ms/step\n",
            "Score for fold 3: loss of 12.847992897033691\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-09-30 07:46:40,207]\u001b[0m Finished trial#18 resulted in value: 18.073877334594727. Current best value is 14.90717601776123 with parameters: {'activation': 'tanh', 'optimizer': 'sgd', 'num_hidden_layer': 4, 'num_hidden_unit': 56, 'kernel_size': 3, 'filter': 7, 'learning_rate': 0.00032117094663704654}.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 26s - loss: 17.2723 - mse: 17.2723 - mae: 1.6121 - 26s/epoch - 1ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 27s - loss: 17.2858 - mse: 17.2858 - mae: 1.6226 - 27s/epoch - 1ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 27s - loss: 17.2320 - mse: 17.2320 - mae: 1.6198 - 27s/epoch - 1ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 27s - loss: 17.6176 - mse: 17.6176 - mae: 1.6495 - 27s/epoch - 1ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 27s - loss: 17.8808 - mse: 17.8808 - mae: 1.6624 - 27s/epoch - 1ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 27s - loss: 18.2012 - mse: 18.2012 - mae: 1.6845 - 27s/epoch - 1ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 28s - loss: 17.9880 - mse: 17.9880 - mae: 1.6657 - 28s/epoch - 1ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 27s - loss: 18.0873 - mse: 18.0873 - mae: 1.6697 - 27s/epoch - 1ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 26s - loss: 17.7011 - mse: 17.7011 - mae: 1.6580 - 26s/epoch - 1ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 26s - loss: 17.9849 - mse: 17.9849 - mae: 1.6842 - 26s/epoch - 1ms/step\n",
            "Score for fold 1: loss of 23.341449737548828\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 26s - loss: 18.2117 - mse: 18.2117 - mae: 1.6896 - 26s/epoch - 1ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 27s - loss: 18.2564 - mse: 18.2564 - mae: 1.6952 - 27s/epoch - 1ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 27s - loss: 18.5084 - mse: 18.5084 - mae: 1.6869 - 27s/epoch - 1ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 27s - loss: 18.3492 - mse: 18.3492 - mae: 1.6758 - 27s/epoch - 1ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 27s - loss: 18.0775 - mse: 18.0775 - mae: 1.6621 - 27s/epoch - 1ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 27s - loss: 17.9309 - mse: 17.9309 - mae: 1.6668 - 27s/epoch - 1ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 28s - loss: 18.2399 - mse: 18.2399 - mae: 1.6721 - 28s/epoch - 1ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 28s - loss: 17.9632 - mse: 17.9632 - mae: 1.6627 - 28s/epoch - 1ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 29s - loss: 18.2578 - mse: 18.2578 - mae: 1.6907 - 29s/epoch - 1ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 28s - loss: 18.5107 - mse: 18.5107 - mae: 1.7038 - 28s/epoch - 1ms/step\n",
            "Score for fold 2: loss of 14.534157752990723\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 28s - loss: 18.3222 - mse: 18.3222 - mae: 1.6875 - 28s/epoch - 1ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 28s - loss: 18.2584 - mse: 18.2584 - mae: 1.6785 - 28s/epoch - 1ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 27s - loss: 18.0351 - mse: 18.0351 - mae: 1.6653 - 27s/epoch - 1ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 27s - loss: 17.8182 - mse: 17.8182 - mae: 1.6585 - 27s/epoch - 1ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 27s - loss: 17.7594 - mse: 17.7594 - mae: 1.6456 - 27s/epoch - 1ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 27s - loss: 18.0387 - mse: 18.0387 - mae: 1.6532 - 27s/epoch - 1ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 27s - loss: 17.8955 - mse: 17.8955 - mae: 1.6571 - 27s/epoch - 1ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 27s - loss: 17.8702 - mse: 17.8702 - mae: 1.6805 - 27s/epoch - 1ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 26s - loss: 18.2616 - mse: 18.2616 - mae: 1.6819 - 26s/epoch - 1ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 27s - loss: 18.1049 - mse: 18.1049 - mae: 1.6744 - 27s/epoch - 1ms/step\n",
            "Score for fold 3: loss of 13.918411254882812\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-09-30 08:00:23,056]\u001b[0m Finished trial#19 resulted in value: 18.10491180419922. Current best value is 14.90717601776123 with parameters: {'activation': 'tanh', 'optimizer': 'sgd', 'num_hidden_layer': 4, 'num_hidden_unit': 56, 'kernel_size': 3, 'filter': 7, 'learning_rate': 0.00032117094663704654}.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 31s - loss: 21.9259 - mse: 21.9259 - mae: 2.0343 - 31s/epoch - 2ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 29s - loss: 22.7539 - mse: 22.7539 - mae: 2.0892 - 29s/epoch - 1ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 29s - loss: 22.7162 - mse: 22.7162 - mae: 2.0828 - 29s/epoch - 1ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 30s - loss: 22.7689 - mse: 22.7689 - mae: 2.0886 - 30s/epoch - 1ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 31s - loss: 22.7527 - mse: 22.7527 - mae: 2.0881 - 31s/epoch - 2ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 32s - loss: 22.7875 - mse: 22.7875 - mae: 2.0949 - 32s/epoch - 2ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 36s - loss: 22.7570 - mse: 22.7570 - mae: 2.0866 - 36s/epoch - 2ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 35s - loss: 22.7657 - mse: 22.7657 - mae: 2.0908 - 35s/epoch - 2ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 35s - loss: 22.7649 - mse: 22.7649 - mae: 2.0904 - 35s/epoch - 2ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 32s - loss: 22.7414 - mse: 22.7414 - mae: 2.0819 - 32s/epoch - 2ms/step\n",
            "Score for fold 1: loss of 25.245424270629883\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 35s - loss: 22.7338 - mse: 22.7338 - mae: 2.0872 - 35s/epoch - 2ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 36s - loss: 22.7544 - mse: 22.7544 - mae: 2.0905 - 36s/epoch - 2ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 34s - loss: 22.7553 - mse: 22.7553 - mae: 2.0871 - 34s/epoch - 2ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 34s - loss: 22.7215 - mse: 22.7215 - mae: 2.0858 - 34s/epoch - 2ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 34s - loss: 22.8028 - mse: 22.8028 - mae: 2.0927 - 34s/epoch - 2ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 33s - loss: 22.8410 - mse: 22.8410 - mae: 2.1007 - 33s/epoch - 2ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 34s - loss: 22.7967 - mse: 22.7967 - mae: 2.0915 - 34s/epoch - 2ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 33s - loss: 22.7539 - mse: 22.7539 - mae: 2.0880 - 33s/epoch - 2ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 34s - loss: 22.7390 - mse: 22.7390 - mae: 2.0871 - 34s/epoch - 2ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 33s - loss: 22.7903 - mse: 22.7903 - mae: 2.0906 - 33s/epoch - 2ms/step\n",
            "Score for fold 2: loss of 19.510173797607422\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 32s - loss: 22.8294 - mse: 22.8294 - mae: 2.0933 - 32s/epoch - 2ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 32s - loss: 22.7590 - mse: 22.7590 - mae: 2.0912 - 32s/epoch - 2ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 32s - loss: 22.7879 - mse: 22.7879 - mae: 2.0947 - 32s/epoch - 2ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 32s - loss: 22.7964 - mse: 22.7964 - mae: 2.0882 - 32s/epoch - 2ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 31s - loss: 22.7659 - mse: 22.7659 - mae: 2.0876 - 31s/epoch - 2ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 31s - loss: 22.7724 - mse: 22.7724 - mae: 2.0860 - 31s/epoch - 2ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 31s - loss: 22.7834 - mse: 22.7834 - mae: 2.0907 - 31s/epoch - 2ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 31s - loss: 22.7498 - mse: 22.7498 - mae: 2.0888 - 31s/epoch - 2ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 32s - loss: 22.7515 - mse: 22.7515 - mae: 2.0918 - 32s/epoch - 2ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 32s - loss: 22.8025 - mse: 22.8025 - mae: 2.0987 - 32s/epoch - 2ms/step\n",
            "Score for fold 3: loss of 18.424545288085938\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-09-30 08:16:49,933]\u001b[0m Finished trial#20 resulted in value: 22.802457809448242. Current best value is 14.90717601776123 with parameters: {'activation': 'tanh', 'optimizer': 'sgd', 'num_hidden_layer': 4, 'num_hidden_unit': 56, 'kernel_size': 3, 'filter': 7, 'learning_rate': 0.00032117094663704654}.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 30s - loss: 24.0753 - mse: 24.0753 - mae: 1.9406 - 30s/epoch - 1ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 29s - loss: 21.5672 - mse: 21.5672 - mae: 1.6193 - 29s/epoch - 1ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 29s - loss: 20.6043 - mse: 20.6043 - mae: 1.6127 - 29s/epoch - 1ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 28s - loss: 20.1593 - mse: 20.1593 - mae: 1.6422 - 28s/epoch - 1ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 28s - loss: 19.9310 - mse: 19.9310 - mae: 1.6712 - 28s/epoch - 1ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 28s - loss: 19.7966 - mse: 19.7966 - mae: 1.6941 - 28s/epoch - 1ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 29s - loss: 19.7072 - mse: 19.7072 - mae: 1.7109 - 29s/epoch - 1ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 29s - loss: 19.6412 - mse: 19.6412 - mae: 1.7219 - 29s/epoch - 1ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 29s - loss: 19.5887 - mse: 19.5887 - mae: 1.7286 - 29s/epoch - 1ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 32s - loss: 19.5444 - mse: 19.5444 - mae: 1.7335 - 32s/epoch - 2ms/step\n",
            "Score for fold 1: loss of 22.842863082885742\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 28s - loss: 19.5058 - mse: 19.5058 - mae: 1.7354 - 28s/epoch - 1ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 28s - loss: 19.4714 - mse: 19.4714 - mae: 1.7373 - 28s/epoch - 1ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 28s - loss: 19.4401 - mse: 19.4401 - mae: 1.7381 - 28s/epoch - 1ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 28s - loss: 19.4112 - mse: 19.4112 - mae: 1.7382 - 28s/epoch - 1ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 28s - loss: 19.3841 - mse: 19.3841 - mae: 1.7377 - 28s/epoch - 1ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 28s - loss: 19.3588 - mse: 19.3588 - mae: 1.7371 - 28s/epoch - 1ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 28s - loss: 19.3346 - mse: 19.3346 - mae: 1.7361 - 28s/epoch - 1ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 27s - loss: 19.3119 - mse: 19.3119 - mae: 1.7350 - 27s/epoch - 1ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 27s - loss: 19.2901 - mse: 19.2901 - mae: 1.7334 - 27s/epoch - 1ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 27s - loss: 19.2693 - mse: 19.2693 - mae: 1.7318 - 27s/epoch - 1ms/step\n",
            "Score for fold 2: loss of 14.939772605895996\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 27s - loss: 19.2493 - mse: 19.2493 - mae: 1.7299 - 27s/epoch - 1ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 27s - loss: 19.2300 - mse: 19.2300 - mae: 1.7286 - 27s/epoch - 1ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 28s - loss: 19.2114 - mse: 19.2114 - mae: 1.7272 - 28s/epoch - 1ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 28s - loss: 19.1933 - mse: 19.1933 - mae: 1.7248 - 28s/epoch - 1ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 27s - loss: 19.1757 - mse: 19.1757 - mae: 1.7231 - 27s/epoch - 1ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 27s - loss: 19.1586 - mse: 19.1586 - mae: 1.7210 - 27s/epoch - 1ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 27s - loss: 19.1419 - mse: 19.1419 - mae: 1.7197 - 27s/epoch - 1ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 27s - loss: 19.1257 - mse: 19.1257 - mae: 1.7186 - 27s/epoch - 1ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 27s - loss: 19.1095 - mse: 19.1095 - mae: 1.7158 - 27s/epoch - 1ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 27s - loss: 19.0935 - mse: 19.0935 - mae: 1.7138 - 27s/epoch - 1ms/step\n",
            "Score for fold 3: loss of 14.666855812072754\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-09-30 08:31:00,437]\u001b[0m Finished trial#21 resulted in value: 19.093542098999023. Current best value is 14.90717601776123 with parameters: {'activation': 'tanh', 'optimizer': 'sgd', 'num_hidden_layer': 4, 'num_hidden_unit': 56, 'kernel_size': 3, 'filter': 7, 'learning_rate': 0.00032117094663704654}.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 26s - loss: 18.4951 - mse: 18.4951 - mae: 1.6517 - 26s/epoch - 1ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 26s - loss: 17.3205 - mse: 17.3205 - mae: 1.6034 - 26s/epoch - 1ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 25s - loss: 16.9346 - mse: 16.9346 - mae: 1.5869 - 25s/epoch - 1ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 24s - loss: 16.6710 - mse: 16.6710 - mae: 1.5689 - 24s/epoch - 1ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 24s - loss: 16.4661 - mse: 16.4661 - mae: 1.5554 - 24s/epoch - 1ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 24s - loss: 16.3074 - mse: 16.3074 - mae: 1.5445 - 24s/epoch - 1ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 24s - loss: 16.2051 - mse: 16.2051 - mae: 1.5406 - 24s/epoch - 1ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 24s - loss: 16.0592 - mse: 16.0592 - mae: 1.5321 - 24s/epoch - 1ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 24s - loss: 15.9624 - mse: 15.9624 - mae: 1.5275 - 24s/epoch - 1ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 24s - loss: 15.9076 - mse: 15.9076 - mae: 1.5251 - 24s/epoch - 1ms/step\n",
            "Score for fold 1: loss of 23.089365005493164\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 24s - loss: 15.8721 - mse: 15.8721 - mae: 1.5203 - 24s/epoch - 1ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 23s - loss: 15.8022 - mse: 15.8022 - mae: 1.5180 - 23s/epoch - 1ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 23s - loss: 15.7516 - mse: 15.7516 - mae: 1.5153 - 23s/epoch - 1ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 23s - loss: 15.6816 - mse: 15.6816 - mae: 1.5136 - 23s/epoch - 1ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 24s - loss: 15.6845 - mse: 15.6845 - mae: 1.5114 - 24s/epoch - 1ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 24s - loss: 15.6827 - mse: 15.6827 - mae: 1.5117 - 24s/epoch - 1ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 24s - loss: 15.6284 - mse: 15.6284 - mae: 1.5082 - 24s/epoch - 1ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 24s - loss: 15.6351 - mse: 15.6351 - mae: 1.5097 - 24s/epoch - 1ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 24s - loss: 15.5913 - mse: 15.5913 - mae: 1.5067 - 24s/epoch - 1ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 24s - loss: 15.5531 - mse: 15.5531 - mae: 1.5046 - 24s/epoch - 1ms/step\n",
            "Score for fold 2: loss of 11.761058807373047\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 26s - loss: 15.5287 - mse: 15.5287 - mae: 1.5046 - 26s/epoch - 1ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 26s - loss: 15.4841 - mse: 15.4841 - mae: 1.5020 - 26s/epoch - 1ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 25s - loss: 15.4677 - mse: 15.4677 - mae: 1.5031 - 25s/epoch - 1ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 25s - loss: 15.4255 - mse: 15.4255 - mae: 1.4994 - 25s/epoch - 1ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 25s - loss: 15.4543 - mse: 15.4543 - mae: 1.5002 - 25s/epoch - 1ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 24s - loss: 15.4501 - mse: 15.4501 - mae: 1.5001 - 24s/epoch - 1ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 24s - loss: 15.3645 - mse: 15.3645 - mae: 1.4968 - 24s/epoch - 1ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 24s - loss: 15.3861 - mse: 15.3861 - mae: 1.4978 - 24s/epoch - 1ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 23s - loss: 15.3494 - mse: 15.3494 - mae: 1.4945 - 23s/epoch - 1ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 24s - loss: 15.3049 - mse: 15.3049 - mae: 1.4941 - 24s/epoch - 1ms/step\n",
            "Score for fold 3: loss of 11.267330169677734\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-09-30 08:43:18,771]\u001b[0m Finished trial#22 resulted in value: 15.304940223693848. Current best value is 14.90717601776123 with parameters: {'activation': 'tanh', 'optimizer': 'sgd', 'num_hidden_layer': 4, 'num_hidden_unit': 56, 'kernel_size': 3, 'filter': 7, 'learning_rate': 0.00032117094663704654}.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 23s - loss: 18.3339 - mse: 18.3339 - mae: 1.6419 - 23s/epoch - 1ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 23s - loss: 16.9640 - mse: 16.9640 - mae: 1.5735 - 23s/epoch - 1ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 23s - loss: 16.5853 - mse: 16.5853 - mae: 1.5564 - 23s/epoch - 1ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 23s - loss: 16.3854 - mse: 16.3854 - mae: 1.5464 - 23s/epoch - 1ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 23s - loss: 16.2134 - mse: 16.2134 - mae: 1.5394 - 23s/epoch - 1ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 24s - loss: 16.1072 - mse: 16.1072 - mae: 1.5362 - 24s/epoch - 1ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 23s - loss: 16.0382 - mse: 16.0382 - mae: 1.5311 - 23s/epoch - 1ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 24s - loss: 15.9221 - mse: 15.9221 - mae: 1.5263 - 24s/epoch - 1ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 24s - loss: 15.8530 - mse: 15.8530 - mae: 1.5239 - 24s/epoch - 1ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 23s - loss: 15.7835 - mse: 15.7835 - mae: 1.5204 - 23s/epoch - 1ms/step\n",
            "Score for fold 1: loss of 25.74936294555664\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 24s - loss: 15.7727 - mse: 15.7727 - mae: 1.5199 - 24s/epoch - 1ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 24s - loss: 15.7335 - mse: 15.7335 - mae: 1.5178 - 24s/epoch - 1ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 23s - loss: 15.6574 - mse: 15.6574 - mae: 1.5136 - 23s/epoch - 1ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 23s - loss: 15.6237 - mse: 15.6237 - mae: 1.5129 - 23s/epoch - 1ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 23s - loss: 15.5473 - mse: 15.5473 - mae: 1.5106 - 23s/epoch - 1ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 23s - loss: 15.6609 - mse: 15.6609 - mae: 1.5144 - 23s/epoch - 1ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 24s - loss: 15.6612 - mse: 15.6612 - mae: 1.5147 - 24s/epoch - 1ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 23s - loss: 15.6391 - mse: 15.6391 - mae: 1.5185 - 23s/epoch - 1ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 23s - loss: 15.4675 - mse: 15.4675 - mae: 1.5073 - 23s/epoch - 1ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 24s - loss: 15.4697 - mse: 15.4697 - mae: 1.5079 - 24s/epoch - 1ms/step\n",
            "Score for fold 2: loss of 11.555278778076172\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 24s - loss: 15.4041 - mse: 15.4041 - mae: 1.5054 - 24s/epoch - 1ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 23s - loss: 15.4473 - mse: 15.4473 - mae: 1.5073 - 23s/epoch - 1ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 23s - loss: 15.5092 - mse: 15.5092 - mae: 1.5119 - 23s/epoch - 1ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 23s - loss: 15.3742 - mse: 15.3742 - mae: 1.5046 - 23s/epoch - 1ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 23s - loss: 15.7003 - mse: 15.7003 - mae: 1.5170 - 23s/epoch - 1ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 22s - loss: 15.5507 - mse: 15.5507 - mae: 1.5135 - 22s/epoch - 1ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 23s - loss: 15.4998 - mse: 15.4998 - mae: 1.5100 - 23s/epoch - 1ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 23s - loss: 15.3914 - mse: 15.3914 - mae: 1.5065 - 23s/epoch - 1ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 24s - loss: 15.3941 - mse: 15.3941 - mae: 1.5056 - 24s/epoch - 1ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 24s - loss: 15.2958 - mse: 15.2958 - mae: 1.5031 - 24s/epoch - 1ms/step\n",
            "Score for fold 3: loss of 11.615846633911133\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-09-30 08:55:07,820]\u001b[0m Finished trial#23 resulted in value: 15.295763969421387. Current best value is 14.90717601776123 with parameters: {'activation': 'tanh', 'optimizer': 'sgd', 'num_hidden_layer': 4, 'num_hidden_unit': 56, 'kernel_size': 3, 'filter': 7, 'learning_rate': 0.00032117094663704654}.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 24s - loss: 17.4862 - mse: 17.4862 - mae: 1.6140 - 24s/epoch - 1ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 24s - loss: 16.3872 - mse: 16.3872 - mae: 1.5589 - 24s/epoch - 1ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 25s - loss: 15.9201 - mse: 15.9201 - mae: 1.5301 - 25s/epoch - 1ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 25s - loss: 15.6822 - mse: 15.6822 - mae: 1.5169 - 25s/epoch - 1ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 25s - loss: 15.6454 - mse: 15.6454 - mae: 1.5175 - 25s/epoch - 1ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 25s - loss: 15.5268 - mse: 15.5268 - mae: 1.5133 - 25s/epoch - 1ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 26s - loss: 15.5290 - mse: 15.5290 - mae: 1.5137 - 26s/epoch - 1ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 25s - loss: 15.3970 - mse: 15.3970 - mae: 1.5086 - 25s/epoch - 1ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 24s - loss: 15.4360 - mse: 15.4360 - mae: 1.5118 - 24s/epoch - 1ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 24s - loss: 15.4917 - mse: 15.4917 - mae: 1.5156 - 24s/epoch - 1ms/step\n",
            "Score for fold 1: loss of 24.828332901000977\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 41s - loss: 15.2782 - mse: 15.2782 - mae: 1.5065 - 41s/epoch - 2ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 40s - loss: 15.2748 - mse: 15.2748 - mae: 1.5035 - 40s/epoch - 2ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 41s - loss: 15.2156 - mse: 15.2156 - mae: 1.5023 - 41s/epoch - 2ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 40s - loss: 15.2232 - mse: 15.2232 - mae: 1.5015 - 40s/epoch - 2ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 40s - loss: 15.1354 - mse: 15.1354 - mae: 1.4987 - 40s/epoch - 2ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 39s - loss: 15.0855 - mse: 15.0855 - mae: 1.4975 - 39s/epoch - 2ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 38s - loss: 15.1101 - mse: 15.1101 - mae: 1.5023 - 38s/epoch - 2ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 38s - loss: 14.9751 - mse: 14.9751 - mae: 1.4938 - 38s/epoch - 2ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 37s - loss: 14.9219 - mse: 14.9219 - mae: 1.4919 - 37s/epoch - 2ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 36s - loss: 14.9266 - mse: 14.9266 - mae: 1.4917 - 36s/epoch - 2ms/step\n",
            "Score for fold 2: loss of 11.07700252532959\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 25s - loss: 14.9011 - mse: 14.9011 - mae: 1.4901 - 25s/epoch - 1ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 26s - loss: 14.9153 - mse: 14.9153 - mae: 1.4904 - 26s/epoch - 1ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 25s - loss: 14.9276 - mse: 14.9276 - mae: 1.4908 - 25s/epoch - 1ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 24s - loss: 14.9588 - mse: 14.9588 - mae: 1.4961 - 24s/epoch - 1ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 25s - loss: 14.8525 - mse: 14.8525 - mae: 1.4904 - 25s/epoch - 1ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 25s - loss: 14.7951 - mse: 14.7951 - mae: 1.4874 - 25s/epoch - 1ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 29s - loss: 14.8741 - mse: 14.8741 - mae: 1.4924 - 29s/epoch - 1ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 37s - loss: 14.8177 - mse: 14.8177 - mae: 1.4894 - 37s/epoch - 2ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 36s - loss: 14.7735 - mse: 14.7735 - mae: 1.4866 - 36s/epoch - 2ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 35s - loss: 14.7075 - mse: 14.7075 - mae: 1.4855 - 35s/epoch - 2ms/step\n",
            "Score for fold 3: loss of 10.629579544067383\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-09-30 09:10:44,031]\u001b[0m Finished trial#24 resulted in value: 14.707450866699219. Current best value is 14.707450866699219 with parameters: {'activation': 'tanh', 'optimizer': 'sgd', 'num_hidden_layer': 2, 'num_hidden_unit': 67, 'kernel_size': 2, 'filter': 22, 'learning_rate': 0.000689843638273629}.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 29s - loss: 17.2209 - mse: 17.2209 - mae: 1.5951 - 29s/epoch - 1ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 38s - loss: 16.0866 - mse: 16.0866 - mae: 1.5348 - 38s/epoch - 2ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 38s - loss: 15.8045 - mse: 15.8045 - mae: 1.5223 - 38s/epoch - 2ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 37s - loss: 15.5589 - mse: 15.5589 - mae: 1.5122 - 37s/epoch - 2ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 36s - loss: 15.4475 - mse: 15.4475 - mae: 1.5080 - 36s/epoch - 2ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 35s - loss: 15.3481 - mse: 15.3481 - mae: 1.5044 - 35s/epoch - 2ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 34s - loss: 15.2527 - mse: 15.2527 - mae: 1.5011 - 34s/epoch - 2ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 33s - loss: 15.2027 - mse: 15.2027 - mae: 1.4994 - 33s/epoch - 2ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 33s - loss: 15.1356 - mse: 15.1356 - mae: 1.4976 - 33s/epoch - 2ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 32s - loss: 15.1554 - mse: 15.1554 - mae: 1.4977 - 32s/epoch - 2ms/step\n",
            "Score for fold 1: loss of 24.713560104370117\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 32s - loss: 15.1084 - mse: 15.1084 - mae: 1.4962 - 32s/epoch - 2ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 33s - loss: 15.0449 - mse: 15.0449 - mae: 1.4943 - 33s/epoch - 2ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 31s - loss: 14.9552 - mse: 14.9552 - mae: 1.4920 - 31s/epoch - 2ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 30s - loss: 14.9944 - mse: 14.9944 - mae: 1.4944 - 30s/epoch - 1ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 30s - loss: 14.9149 - mse: 14.9149 - mae: 1.4899 - 30s/epoch - 1ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 29s - loss: 14.9275 - mse: 14.9275 - mae: 1.4902 - 29s/epoch - 1ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 28s - loss: 14.9355 - mse: 14.9355 - mae: 1.4934 - 28s/epoch - 1ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 28s - loss: 14.7735 - mse: 14.7735 - mae: 1.4874 - 28s/epoch - 1ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 28s - loss: 14.7540 - mse: 14.7540 - mae: 1.4884 - 28s/epoch - 1ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 28s - loss: 14.8798 - mse: 14.8798 - mae: 1.4919 - 28s/epoch - 1ms/step\n",
            "Score for fold 2: loss of 10.956591606140137\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 28s - loss: 14.8213 - mse: 14.8213 - mae: 1.4901 - 28s/epoch - 1ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 28s - loss: 14.7213 - mse: 14.7213 - mae: 1.4850 - 28s/epoch - 1ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 28s - loss: 14.7245 - mse: 14.7245 - mae: 1.4832 - 28s/epoch - 1ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 28s - loss: 14.6961 - mse: 14.6961 - mae: 1.4821 - 28s/epoch - 1ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 28s - loss: 14.6924 - mse: 14.6924 - mae: 1.4878 - 28s/epoch - 1ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 28s - loss: 14.5993 - mse: 14.5993 - mae: 1.4847 - 28s/epoch - 1ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 28s - loss: 14.6023 - mse: 14.6023 - mae: 1.4880 - 28s/epoch - 1ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 27s - loss: 14.5102 - mse: 14.5102 - mae: 1.4806 - 27s/epoch - 1ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 28s - loss: 14.6205 - mse: 14.6205 - mae: 1.4860 - 28s/epoch - 1ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 28s - loss: 14.5853 - mse: 14.5853 - mae: 1.4871 - 28s/epoch - 1ms/step\n",
            "Score for fold 3: loss of 10.906414031982422\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-09-30 09:26:15,847]\u001b[0m Finished trial#25 resulted in value: 14.5852689743042. Current best value is 14.5852689743042 with parameters: {'activation': 'tanh', 'optimizer': 'sgd', 'num_hidden_layer': 3, 'num_hidden_unit': 69, 'kernel_size': 2, 'filter': 38, 'learning_rate': 0.0006622826526133496}.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 24s - loss: 18.2413 - mse: 18.2413 - mae: 1.6802 - 24s/epoch - 1ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 23s - loss: 17.5125 - mse: 17.5125 - mae: 1.6473 - 23s/epoch - 1ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 23s - loss: 17.1226 - mse: 17.1226 - mae: 1.6161 - 23s/epoch - 1ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 23s - loss: 16.9800 - mse: 16.9800 - mae: 1.6084 - 23s/epoch - 1ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 23s - loss: 16.9858 - mse: 16.9858 - mae: 1.6094 - 23s/epoch - 1ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 23s - loss: 17.0000 - mse: 17.0000 - mae: 1.6058 - 23s/epoch - 1ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 23s - loss: 16.9800 - mse: 16.9800 - mae: 1.6064 - 23s/epoch - 1ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 23s - loss: 17.0102 - mse: 17.0102 - mae: 1.6114 - 23s/epoch - 1ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 22s - loss: 17.0058 - mse: 17.0058 - mae: 1.6053 - 22s/epoch - 1ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 22s - loss: 16.9487 - mse: 16.9487 - mae: 1.6050 - 22s/epoch - 1ms/step\n",
            "Score for fold 1: loss of 23.693187713623047\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 22s - loss: 17.1398 - mse: 17.1398 - mae: 1.6075 - 22s/epoch - 1ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 23s - loss: 17.0308 - mse: 17.0308 - mae: 1.6097 - 23s/epoch - 1ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 23s - loss: 17.0434 - mse: 17.0434 - mae: 1.6108 - 23s/epoch - 1ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 22s - loss: 17.0712 - mse: 17.0712 - mae: 1.6103 - 22s/epoch - 1ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 22s - loss: 17.0609 - mse: 17.0609 - mae: 1.6144 - 22s/epoch - 1ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 22s - loss: 17.0224 - mse: 17.0224 - mae: 1.6100 - 22s/epoch - 1ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 22s - loss: 17.0519 - mse: 17.0519 - mae: 1.6188 - 22s/epoch - 1ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 22s - loss: 17.0140 - mse: 17.0140 - mae: 1.6093 - 22s/epoch - 1ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 23s - loss: 17.0536 - mse: 17.0536 - mae: 1.6096 - 23s/epoch - 1ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 23s - loss: 17.0321 - mse: 17.0321 - mae: 1.6117 - 23s/epoch - 1ms/step\n",
            "Score for fold 2: loss of 12.574555397033691\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 23s - loss: 17.0769 - mse: 17.0769 - mae: 1.6108 - 23s/epoch - 1ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 23s - loss: 17.0397 - mse: 17.0397 - mae: 1.6200 - 23s/epoch - 1ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 23s - loss: 16.9586 - mse: 16.9586 - mae: 1.6120 - 23s/epoch - 1ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 23s - loss: 16.9682 - mse: 16.9682 - mae: 1.6104 - 23s/epoch - 1ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 23s - loss: 17.0497 - mse: 17.0497 - mae: 1.6110 - 23s/epoch - 1ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 23s - loss: 17.0328 - mse: 17.0328 - mae: 1.6136 - 23s/epoch - 1ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 23s - loss: 17.0352 - mse: 17.0352 - mae: 1.6151 - 23s/epoch - 1ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 23s - loss: 17.0146 - mse: 17.0146 - mae: 1.6130 - 23s/epoch - 1ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 23s - loss: 17.3558 - mse: 17.3558 - mae: 1.6188 - 23s/epoch - 1ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 22s - loss: 17.2645 - mse: 17.2645 - mae: 1.6226 - 22s/epoch - 1ms/step\n",
            "Score for fold 3: loss of 12.729856491088867\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-09-30 09:37:46,367]\u001b[0m Finished trial#26 resulted in value: 17.264522552490234. Current best value is 14.5852689743042 with parameters: {'activation': 'tanh', 'optimizer': 'sgd', 'num_hidden_layer': 3, 'num_hidden_unit': 69, 'kernel_size': 2, 'filter': 38, 'learning_rate': 0.0006622826526133496}.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 26s - loss: 17.4227 - mse: 17.4227 - mae: 1.6101 - 26s/epoch - 1ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 26s - loss: 16.2957 - mse: 16.2957 - mae: 1.5484 - 26s/epoch - 1ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 26s - loss: 15.8913 - mse: 15.8913 - mae: 1.5244 - 26s/epoch - 1ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 25s - loss: 15.6758 - mse: 15.6758 - mae: 1.5165 - 25s/epoch - 1ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 25s - loss: 15.5645 - mse: 15.5645 - mae: 1.5122 - 25s/epoch - 1ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 25s - loss: 15.4437 - mse: 15.4437 - mae: 1.5059 - 25s/epoch - 1ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 26s - loss: 15.4185 - mse: 15.4185 - mae: 1.5055 - 26s/epoch - 1ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 25s - loss: 15.3427 - mse: 15.3427 - mae: 1.5043 - 25s/epoch - 1ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 26s - loss: 15.3576 - mse: 15.3576 - mae: 1.5052 - 26s/epoch - 1ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 26s - loss: 15.2173 - mse: 15.2173 - mae: 1.4995 - 26s/epoch - 1ms/step\n",
            "Score for fold 1: loss of 25.589588165283203\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 25s - loss: 15.1490 - mse: 15.1490 - mae: 1.4955 - 25s/epoch - 1ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 25s - loss: 15.2216 - mse: 15.2216 - mae: 1.4990 - 25s/epoch - 1ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 25s - loss: 15.1064 - mse: 15.1064 - mae: 1.4976 - 25s/epoch - 1ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 26s - loss: 15.0655 - mse: 15.0655 - mae: 1.4941 - 26s/epoch - 1ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 25s - loss: 15.1128 - mse: 15.1128 - mae: 1.5014 - 25s/epoch - 1ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 26s - loss: 15.0482 - mse: 15.0482 - mae: 1.4945 - 26s/epoch - 1ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 25s - loss: 14.9880 - mse: 14.9880 - mae: 1.4918 - 25s/epoch - 1ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 25s - loss: 15.0002 - mse: 15.0002 - mae: 1.4918 - 25s/epoch - 1ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 25s - loss: 14.8954 - mse: 14.8954 - mae: 1.4859 - 25s/epoch - 1ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 26s - loss: 14.8725 - mse: 14.8725 - mae: 1.4880 - 26s/epoch - 1ms/step\n",
            "Score for fold 2: loss of 11.406647682189941\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 26s - loss: 14.8487 - mse: 14.8487 - mae: 1.4861 - 26s/epoch - 1ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 26s - loss: 14.7925 - mse: 14.7925 - mae: 1.4816 - 26s/epoch - 1ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 26s - loss: 14.8627 - mse: 14.8627 - mae: 1.4902 - 26s/epoch - 1ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 25s - loss: 14.7878 - mse: 14.7878 - mae: 1.4854 - 25s/epoch - 1ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 26s - loss: 14.7501 - mse: 14.7501 - mae: 1.4820 - 26s/epoch - 1ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 26s - loss: 14.6577 - mse: 14.6577 - mae: 1.4799 - 26s/epoch - 1ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 25s - loss: 14.5891 - mse: 14.5891 - mae: 1.4761 - 25s/epoch - 1ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 26s - loss: 14.5987 - mse: 14.5987 - mae: 1.4759 - 26s/epoch - 1ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 25s - loss: 14.6036 - mse: 14.6036 - mae: 1.4802 - 25s/epoch - 1ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 25s - loss: 14.5887 - mse: 14.5887 - mae: 1.4825 - 25s/epoch - 1ms/step\n",
            "Score for fold 3: loss of 10.738475799560547\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-09-30 09:50:42,123]\u001b[0m Finished trial#27 resulted in value: 14.588662147521973. Current best value is 14.5852689743042 with parameters: {'activation': 'tanh', 'optimizer': 'sgd', 'num_hidden_layer': 3, 'num_hidden_unit': 69, 'kernel_size': 2, 'filter': 38, 'learning_rate': 0.0006622826526133496}.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 27s - loss: 17.3219 - mse: 17.3219 - mae: 1.6102 - 27s/epoch - 1ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 27s - loss: 16.3053 - mse: 16.3053 - mae: 1.5631 - 27s/epoch - 1ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 27s - loss: 15.8850 - mse: 15.8850 - mae: 1.5318 - 27s/epoch - 1ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 26s - loss: 15.6603 - mse: 15.6603 - mae: 1.5196 - 26s/epoch - 1ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 26s - loss: 15.4977 - mse: 15.4977 - mae: 1.5121 - 26s/epoch - 1ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 26s - loss: 15.3728 - mse: 15.3728 - mae: 1.5072 - 26s/epoch - 1ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 27s - loss: 15.3232 - mse: 15.3232 - mae: 1.5070 - 27s/epoch - 1ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 27s - loss: 15.1937 - mse: 15.1937 - mae: 1.4981 - 27s/epoch - 1ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 27s - loss: 15.1408 - mse: 15.1408 - mae: 1.4970 - 27s/epoch - 1ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 27s - loss: 15.0908 - mse: 15.0908 - mae: 1.4950 - 27s/epoch - 1ms/step\n",
            "Score for fold 1: loss of 24.705875396728516\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 27s - loss: 15.1082 - mse: 15.1082 - mae: 1.4959 - 27s/epoch - 1ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 27s - loss: 15.0870 - mse: 15.0870 - mae: 1.4955 - 27s/epoch - 1ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 26s - loss: 15.0201 - mse: 15.0201 - mae: 1.4916 - 26s/epoch - 1ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 26s - loss: 15.0082 - mse: 15.0082 - mae: 1.4919 - 26s/epoch - 1ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 27s - loss: 14.9354 - mse: 14.9354 - mae: 1.4887 - 27s/epoch - 1ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 27s - loss: 14.8407 - mse: 14.8407 - mae: 1.4845 - 27s/epoch - 1ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 27s - loss: 14.8879 - mse: 14.8879 - mae: 1.4878 - 27s/epoch - 1ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 26s - loss: 14.7929 - mse: 14.7929 - mae: 1.4839 - 26s/epoch - 1ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 27s - loss: 14.7548 - mse: 14.7548 - mae: 1.4836 - 27s/epoch - 1ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 26s - loss: 14.6879 - mse: 14.6879 - mae: 1.4790 - 26s/epoch - 1ms/step\n",
            "Score for fold 2: loss of 10.923906326293945\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 27s - loss: 14.6595 - mse: 14.6595 - mae: 1.4768 - 27s/epoch - 1ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 28s - loss: 14.7226 - mse: 14.7226 - mae: 1.4817 - 28s/epoch - 1ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 27s - loss: 14.6682 - mse: 14.6682 - mae: 1.4786 - 27s/epoch - 1ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 27s - loss: 14.5956 - mse: 14.5956 - mae: 1.4759 - 27s/epoch - 1ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 28s - loss: 14.5703 - mse: 14.5703 - mae: 1.4735 - 28s/epoch - 1ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 27s - loss: 14.5071 - mse: 14.5071 - mae: 1.4720 - 27s/epoch - 1ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 28s - loss: 14.4620 - mse: 14.4620 - mae: 1.4697 - 28s/epoch - 1ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 29s - loss: 14.4430 - mse: 14.4430 - mae: 1.4720 - 29s/epoch - 1ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 28s - loss: 14.3812 - mse: 14.3812 - mae: 1.4680 - 28s/epoch - 1ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 29s - loss: 14.3818 - mse: 14.3818 - mae: 1.4662 - 29s/epoch - 1ms/step\n",
            "Score for fold 3: loss of 10.813464164733887\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-09-30 10:04:21,667]\u001b[0m Finished trial#28 resulted in value: 14.38178539276123. Current best value is 14.38178539276123 with parameters: {'activation': 'tanh', 'optimizer': 'sgd', 'num_hidden_layer': 2, 'num_hidden_unit': 81, 'kernel_size': 2, 'filter': 40, 'learning_rate': 0.000693576191437307}.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 25s - loss: 18.2375 - mse: 18.2375 - mae: 1.6973 - 25s/epoch - 1ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 24s - loss: 17.5338 - mse: 17.5338 - mae: 1.6615 - 24s/epoch - 1ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 26s - loss: 17.1127 - mse: 17.1127 - mae: 1.6189 - 26s/epoch - 1ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 26s - loss: 17.1439 - mse: 17.1440 - mae: 1.6223 - 26s/epoch - 1ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 25s - loss: 17.1919 - mse: 17.1919 - mae: 1.6336 - 25s/epoch - 1ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 25s - loss: 17.2281 - mse: 17.2281 - mae: 1.6305 - 25s/epoch - 1ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 24s - loss: 17.0748 - mse: 17.0748 - mae: 1.6254 - 24s/epoch - 1ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 23s - loss: 17.1153 - mse: 17.1153 - mae: 1.6256 - 23s/epoch - 1ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 23s - loss: 17.3180 - mse: 17.3180 - mae: 1.6332 - 23s/epoch - 1ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 23s - loss: 17.2387 - mse: 17.2387 - mae: 1.6316 - 23s/epoch - 1ms/step\n",
            "Score for fold 1: loss of 24.712106704711914\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 23s - loss: 17.1490 - mse: 17.1490 - mae: 1.6266 - 23s/epoch - 1ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 22s - loss: 17.1210 - mse: 17.1210 - mae: 1.6241 - 22s/epoch - 1ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 23s - loss: 17.2793 - mse: 17.2793 - mae: 1.6274 - 23s/epoch - 1ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 23s - loss: 17.1366 - mse: 17.1366 - mae: 1.6246 - 23s/epoch - 1ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 23s - loss: 17.1170 - mse: 17.1170 - mae: 1.6236 - 23s/epoch - 1ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 24s - loss: 17.1761 - mse: 17.1761 - mae: 1.6204 - 24s/epoch - 1ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 23s - loss: 17.2885 - mse: 17.2885 - mae: 1.6305 - 23s/epoch - 1ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 24s - loss: 17.5446 - mse: 17.5446 - mae: 1.6729 - 24s/epoch - 1ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 23s - loss: 17.4173 - mse: 17.4173 - mae: 1.6358 - 23s/epoch - 1ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 23s - loss: 17.2962 - mse: 17.2962 - mae: 1.6325 - 23s/epoch - 1ms/step\n",
            "Score for fold 2: loss of 12.757411003112793\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 23s - loss: 17.4296 - mse: 17.4296 - mae: 1.6316 - 23s/epoch - 1ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 24s - loss: 17.3918 - mse: 17.3918 - mae: 1.6312 - 24s/epoch - 1ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 24s - loss: 17.4282 - mse: 17.4282 - mae: 1.6497 - 24s/epoch - 1ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 24s - loss: 17.2483 - mse: 17.2483 - mae: 1.6265 - 24s/epoch - 1ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 23s - loss: 17.2138 - mse: 17.2138 - mae: 1.6273 - 23s/epoch - 1ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 23s - loss: 17.1582 - mse: 17.1582 - mae: 1.6280 - 23s/epoch - 1ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 23s - loss: 17.2202 - mse: 17.2202 - mae: 1.6261 - 23s/epoch - 1ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 22s - loss: 17.2053 - mse: 17.2053 - mae: 1.6270 - 22s/epoch - 1ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 23s - loss: 17.1699 - mse: 17.1699 - mae: 1.6243 - 23s/epoch - 1ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 22s - loss: 17.1833 - mse: 17.1833 - mae: 1.6295 - 22s/epoch - 1ms/step\n",
            "Score for fold 3: loss of 13.215439796447754\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-09-30 10:16:19,293]\u001b[0m Finished trial#29 resulted in value: 17.18332290649414. Current best value is 14.38178539276123 with parameters: {'activation': 'tanh', 'optimizer': 'sgd', 'num_hidden_layer': 2, 'num_hidden_unit': 81, 'kernel_size': 2, 'filter': 40, 'learning_rate': 0.000693576191437307}.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 36s - loss: 18.6714 - mse: 18.6714 - mae: 1.7494 - 36s/epoch - 2ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 35s - loss: 18.1829 - mse: 18.1829 - mae: 1.7270 - 35s/epoch - 2ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 36s - loss: 18.2936 - mse: 18.2936 - mae: 1.7399 - 36s/epoch - 2ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 36s - loss: 18.4457 - mse: 18.4457 - mae: 1.7455 - 36s/epoch - 2ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 37s - loss: 18.0440 - mse: 18.0440 - mae: 1.7484 - 37s/epoch - 2ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 36s - loss: 18.0634 - mse: 18.0634 - mae: 1.7351 - 36s/epoch - 2ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 37s - loss: 18.6488 - mse: 18.6488 - mae: 1.7806 - 37s/epoch - 2ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 37s - loss: 19.1109 - mse: 19.1109 - mae: 1.8289 - 37s/epoch - 2ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 37s - loss: 18.8874 - mse: 18.8874 - mae: 1.8308 - 37s/epoch - 2ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 36s - loss: 18.7304 - mse: 18.7304 - mae: 1.8024 - 36s/epoch - 2ms/step\n",
            "Score for fold 1: loss of 23.807310104370117\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 36s - loss: 19.0607 - mse: 19.0607 - mae: 1.8195 - 36s/epoch - 2ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 36s - loss: 18.9505 - mse: 18.9505 - mae: 1.8420 - 36s/epoch - 2ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 36s - loss: 18.5671 - mse: 18.5671 - mae: 1.7982 - 36s/epoch - 2ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 36s - loss: 18.1132 - mse: 18.1132 - mae: 1.7603 - 36s/epoch - 2ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 36s - loss: 18.6675 - mse: 18.6675 - mae: 1.8232 - 36s/epoch - 2ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 36s - loss: 18.5647 - mse: 18.5647 - mae: 1.8059 - 36s/epoch - 2ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 36s - loss: 18.2275 - mse: 18.2275 - mae: 1.7727 - 36s/epoch - 2ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 36s - loss: 18.4924 - mse: 18.4924 - mae: 1.7901 - 36s/epoch - 2ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 36s - loss: 18.9327 - mse: 18.9327 - mae: 1.8494 - 36s/epoch - 2ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 36s - loss: 18.2856 - mse: 18.2856 - mae: 1.7734 - 36s/epoch - 2ms/step\n",
            "Score for fold 2: loss of 14.312089920043945\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 36s - loss: 17.9450 - mse: 17.9450 - mae: 1.7588 - 36s/epoch - 2ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 36s - loss: 18.0112 - mse: 18.0112 - mae: 1.7550 - 36s/epoch - 2ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 36s - loss: 18.7983 - mse: 18.7983 - mae: 1.8129 - 36s/epoch - 2ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 36s - loss: 18.1798 - mse: 18.1798 - mae: 1.7681 - 36s/epoch - 2ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 37s - loss: 18.4410 - mse: 18.4410 - mae: 1.8069 - 37s/epoch - 2ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 36s - loss: 18.2737 - mse: 18.2737 - mae: 1.8008 - 36s/epoch - 2ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 36s - loss: 18.7559 - mse: 18.7559 - mae: 1.8459 - 36s/epoch - 2ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 36s - loss: 18.8842 - mse: 18.8842 - mae: 1.8289 - 36s/epoch - 2ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 36s - loss: 18.3446 - mse: 18.3446 - mae: 1.8138 - 36s/epoch - 2ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 36s - loss: 18.4844 - mse: 18.4844 - mae: 1.8087 - 36s/epoch - 2ms/step\n",
            "Score for fold 3: loss of 13.367654800415039\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-09-30 10:34:35,262]\u001b[0m Finished trial#30 resulted in value: 18.48440933227539. Current best value is 14.38178539276123 with parameters: {'activation': 'tanh', 'optimizer': 'sgd', 'num_hidden_layer': 2, 'num_hidden_unit': 81, 'kernel_size': 2, 'filter': 40, 'learning_rate': 0.000693576191437307}.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 29s - loss: 17.8741 - mse: 17.8742 - mae: 1.5884 - 29s/epoch - 1ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 29s - loss: 17.3386 - mse: 17.3386 - mae: 1.5585 - 29s/epoch - 1ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 29s - loss: 17.1320 - mse: 17.1320 - mae: 1.5466 - 29s/epoch - 1ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 29s - loss: 16.9958 - mse: 16.9958 - mae: 1.5399 - 29s/epoch - 1ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 28s - loss: 16.8942 - mse: 16.8942 - mae: 1.5350 - 28s/epoch - 1ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 29s - loss: 16.8165 - mse: 16.8165 - mae: 1.5306 - 29s/epoch - 1ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 29s - loss: 16.7507 - mse: 16.7507 - mae: 1.5282 - 29s/epoch - 1ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 29s - loss: 16.6960 - mse: 16.6960 - mae: 1.5263 - 29s/epoch - 1ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 29s - loss: 16.6508 - mse: 16.6508 - mae: 1.5240 - 29s/epoch - 1ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 29s - loss: 16.6094 - mse: 16.6094 - mae: 1.5225 - 29s/epoch - 1ms/step\n",
            "Score for fold 1: loss of 23.89369773864746\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 28s - loss: 16.5720 - mse: 16.5720 - mae: 1.5211 - 28s/epoch - 1ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 29s - loss: 16.5411 - mse: 16.5411 - mae: 1.5199 - 29s/epoch - 1ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 28s - loss: 16.5094 - mse: 16.5094 - mae: 1.5186 - 28s/epoch - 1ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 28s - loss: 16.4835 - mse: 16.4835 - mae: 1.5174 - 28s/epoch - 1ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 29s - loss: 16.4584 - mse: 16.4584 - mae: 1.5161 - 29s/epoch - 1ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 28s - loss: 16.4357 - mse: 16.4357 - mae: 1.5162 - 28s/epoch - 1ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 27s - loss: 16.4118 - mse: 16.4118 - mae: 1.5149 - 27s/epoch - 1ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 28s - loss: 16.3936 - mse: 16.3936 - mae: 1.5142 - 28s/epoch - 1ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 28s - loss: 16.3752 - mse: 16.3752 - mae: 1.5134 - 28s/epoch - 1ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 28s - loss: 16.3559 - mse: 16.3559 - mae: 1.5127 - 28s/epoch - 1ms/step\n",
            "Score for fold 2: loss of 12.36133098602295\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 28s - loss: 16.3405 - mse: 16.3405 - mae: 1.5124 - 28s/epoch - 1ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 28s - loss: 16.3231 - mse: 16.3231 - mae: 1.5114 - 28s/epoch - 1ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 28s - loss: 16.3089 - mse: 16.3089 - mae: 1.5107 - 28s/epoch - 1ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 28s - loss: 16.2937 - mse: 16.2937 - mae: 1.5102 - 28s/epoch - 1ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 28s - loss: 16.2802 - mse: 16.2802 - mae: 1.5099 - 28s/epoch - 1ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 28s - loss: 16.2662 - mse: 16.2662 - mae: 1.5092 - 28s/epoch - 1ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 28s - loss: 16.2512 - mse: 16.2512 - mae: 1.5098 - 28s/epoch - 1ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 29s - loss: 16.2414 - mse: 16.2414 - mae: 1.5081 - 29s/epoch - 1ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 29s - loss: 16.2280 - mse: 16.2280 - mae: 1.5079 - 29s/epoch - 1ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 28s - loss: 16.2167 - mse: 16.2167 - mae: 1.5075 - 28s/epoch - 1ms/step\n",
            "Score for fold 3: loss of 12.154995918273926\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-09-30 10:49:00,136]\u001b[0m Finished trial#31 resulted in value: 16.216733932495117. Current best value is 14.38178539276123 with parameters: {'activation': 'tanh', 'optimizer': 'sgd', 'num_hidden_layer': 2, 'num_hidden_unit': 81, 'kernel_size': 2, 'filter': 40, 'learning_rate': 0.000693576191437307}.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 26s - loss: 17.4289 - mse: 17.4289 - mae: 1.6132 - 26s/epoch - 1ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 25s - loss: 16.3243 - mse: 16.3243 - mae: 1.5547 - 25s/epoch - 1ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 26s - loss: 15.8948 - mse: 15.8948 - mae: 1.5270 - 26s/epoch - 1ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 26s - loss: 15.6761 - mse: 15.6761 - mae: 1.5188 - 26s/epoch - 1ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 26s - loss: 15.5558 - mse: 15.5558 - mae: 1.5121 - 26s/epoch - 1ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 26s - loss: 15.4486 - mse: 15.4486 - mae: 1.5073 - 26s/epoch - 1ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 26s - loss: 15.3801 - mse: 15.3801 - mae: 1.5053 - 26s/epoch - 1ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 26s - loss: 15.3342 - mse: 15.3342 - mae: 1.5038 - 26s/epoch - 1ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 26s - loss: 15.2060 - mse: 15.2060 - mae: 1.4990 - 26s/epoch - 1ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 25s - loss: 15.2443 - mse: 15.2443 - mae: 1.5018 - 25s/epoch - 1ms/step\n",
            "Score for fold 1: loss of 25.521242141723633\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 26s - loss: 15.1931 - mse: 15.1931 - mae: 1.4976 - 26s/epoch - 1ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 26s - loss: 15.1701 - mse: 15.1701 - mae: 1.5017 - 26s/epoch - 1ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 26s - loss: 15.0660 - mse: 15.0660 - mae: 1.4949 - 26s/epoch - 1ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 25s - loss: 15.0232 - mse: 15.0232 - mae: 1.4922 - 25s/epoch - 1ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 25s - loss: 14.9853 - mse: 14.9853 - mae: 1.4909 - 25s/epoch - 1ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 25s - loss: 14.9684 - mse: 14.9684 - mae: 1.4907 - 25s/epoch - 1ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 24s - loss: 14.9407 - mse: 14.9407 - mae: 1.4886 - 24s/epoch - 1ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 25s - loss: 14.8953 - mse: 14.8953 - mae: 1.4872 - 25s/epoch - 1ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 25s - loss: 14.9960 - mse: 14.9960 - mae: 1.4988 - 25s/epoch - 1ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 25s - loss: 14.9628 - mse: 14.9628 - mae: 1.4906 - 25s/epoch - 1ms/step\n",
            "Score for fold 2: loss of 10.929169654846191\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 25s - loss: 14.8482 - mse: 14.8482 - mae: 1.4857 - 25s/epoch - 1ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 25s - loss: 14.7640 - mse: 14.7640 - mae: 1.4836 - 25s/epoch - 1ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 25s - loss: 14.7247 - mse: 14.7247 - mae: 1.4810 - 25s/epoch - 1ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 26s - loss: 14.7860 - mse: 14.7860 - mae: 1.4927 - 26s/epoch - 1ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 25s - loss: 14.7909 - mse: 14.7909 - mae: 1.4896 - 25s/epoch - 1ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 26s - loss: 14.6401 - mse: 14.6401 - mae: 1.4813 - 26s/epoch - 1ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 26s - loss: 14.6424 - mse: 14.6424 - mae: 1.4831 - 26s/epoch - 1ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 26s - loss: 14.6194 - mse: 14.6194 - mae: 1.4806 - 26s/epoch - 1ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 26s - loss: 14.6040 - mse: 14.6040 - mae: 1.4835 - 26s/epoch - 1ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 25s - loss: 14.6514 - mse: 14.6514 - mae: 1.4884 - 25s/epoch - 1ms/step\n",
            "Score for fold 3: loss of 10.656362533569336\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-09-30 11:01:56,945]\u001b[0m Finished trial#32 resulted in value: 14.651439666748047. Current best value is 14.38178539276123 with parameters: {'activation': 'tanh', 'optimizer': 'sgd', 'num_hidden_layer': 2, 'num_hidden_unit': 81, 'kernel_size': 2, 'filter': 40, 'learning_rate': 0.000693576191437307}.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 26s - loss: 17.3727 - mse: 17.3727 - mae: 1.6116 - 26s/epoch - 1ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 25s - loss: 16.3254 - mse: 16.3254 - mae: 1.5579 - 25s/epoch - 1ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 26s - loss: 15.8984 - mse: 15.8984 - mae: 1.5308 - 26s/epoch - 1ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 26s - loss: 15.7130 - mse: 15.7130 - mae: 1.5205 - 26s/epoch - 1ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 26s - loss: 15.5670 - mse: 15.5670 - mae: 1.5136 - 26s/epoch - 1ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 26s - loss: 15.4736 - mse: 15.4736 - mae: 1.5090 - 26s/epoch - 1ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 26s - loss: 15.3797 - mse: 15.3797 - mae: 1.5051 - 26s/epoch - 1ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 27s - loss: 15.2705 - mse: 15.2705 - mae: 1.5021 - 27s/epoch - 1ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 27s - loss: 15.2475 - mse: 15.2475 - mae: 1.5001 - 27s/epoch - 1ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 28s - loss: 15.1847 - mse: 15.1847 - mae: 1.4987 - 28s/epoch - 1ms/step\n",
            "Score for fold 1: loss of 24.790664672851562\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 27s - loss: 15.1611 - mse: 15.1611 - mae: 1.4973 - 27s/epoch - 1ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 27s - loss: 15.0826 - mse: 15.0826 - mae: 1.4932 - 27s/epoch - 1ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 28s - loss: 15.0666 - mse: 15.0666 - mae: 1.4911 - 28s/epoch - 1ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 27s - loss: 14.9791 - mse: 14.9791 - mae: 1.4907 - 27s/epoch - 1ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 26s - loss: 14.9764 - mse: 14.9764 - mae: 1.4896 - 26s/epoch - 1ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 27s - loss: 15.0417 - mse: 15.0417 - mae: 1.4935 - 27s/epoch - 1ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 27s - loss: 14.9667 - mse: 14.9667 - mae: 1.4918 - 27s/epoch - 1ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 28s - loss: 14.9279 - mse: 14.9279 - mae: 1.4882 - 28s/epoch - 1ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 27s - loss: 14.9202 - mse: 14.9202 - mae: 1.4869 - 27s/epoch - 1ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 26s - loss: 14.8881 - mse: 14.8881 - mae: 1.4898 - 26s/epoch - 1ms/step\n",
            "Score for fold 2: loss of 10.867837905883789\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 25s - loss: 14.8676 - mse: 14.8676 - mae: 1.4915 - 25s/epoch - 1ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 25s - loss: 14.8397 - mse: 14.8397 - mae: 1.4861 - 25s/epoch - 1ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 25s - loss: 14.7889 - mse: 14.7889 - mae: 1.4834 - 25s/epoch - 1ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 26s - loss: 14.8116 - mse: 14.8116 - mae: 1.4849 - 26s/epoch - 1ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 26s - loss: 14.7264 - mse: 14.7264 - mae: 1.4816 - 26s/epoch - 1ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 25s - loss: 14.6559 - mse: 14.6559 - mae: 1.4797 - 25s/epoch - 1ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 25s - loss: 14.6474 - mse: 14.6474 - mae: 1.4785 - 25s/epoch - 1ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 25s - loss: 14.5914 - mse: 14.5914 - mae: 1.4767 - 25s/epoch - 1ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 25s - loss: 14.6250 - mse: 14.6250 - mae: 1.4770 - 25s/epoch - 1ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 26s - loss: 14.5643 - mse: 14.5643 - mae: 1.4755 - 26s/epoch - 1ms/step\n",
            "Score for fold 3: loss of 10.685906410217285\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-09-30 11:15:14,475]\u001b[0m Finished trial#33 resulted in value: 14.564294815063477. Current best value is 14.38178539276123 with parameters: {'activation': 'tanh', 'optimizer': 'sgd', 'num_hidden_layer': 2, 'num_hidden_unit': 81, 'kernel_size': 2, 'filter': 40, 'learning_rate': 0.000693576191437307}.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 28s - loss: 19.4287 - mse: 19.4287 - mae: 1.8157 - 28s/epoch - 1ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 27s - loss: 20.7127 - mse: 20.7127 - mae: 2.0242 - 27s/epoch - 1ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 28s - loss: 21.0122 - mse: 21.0122 - mae: 2.0499 - 28s/epoch - 1ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 28s - loss: 21.9686 - mse: 21.9686 - mae: 2.1305 - 28s/epoch - 1ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 28s - loss: 21.6944 - mse: 21.6944 - mae: 2.0984 - 28s/epoch - 1ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 28s - loss: 21.6259 - mse: 21.6259 - mae: 2.1077 - 28s/epoch - 1ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 27s - loss: 22.5719 - mse: 22.5719 - mae: 2.1794 - 27s/epoch - 1ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 27s - loss: 22.0212 - mse: 22.0212 - mae: 2.1253 - 27s/epoch - 1ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 28s - loss: 21.7642 - mse: 21.7642 - mae: 2.1101 - 28s/epoch - 1ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 29s - loss: 21.8854 - mse: 21.8854 - mae: 2.1116 - 29s/epoch - 1ms/step\n",
            "Score for fold 1: loss of 23.38573455810547\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 29s - loss: 22.1270 - mse: 22.1270 - mae: 2.1342 - 29s/epoch - 1ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 28s - loss: 21.6489 - mse: 21.6489 - mae: 2.0829 - 28s/epoch - 1ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 28s - loss: 21.6540 - mse: 21.6540 - mae: 2.1003 - 28s/epoch - 1ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 28s - loss: 22.0144 - mse: 22.0144 - mae: 2.1382 - 28s/epoch - 1ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 28s - loss: 21.6765 - mse: 21.6765 - mae: 2.1023 - 28s/epoch - 1ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 28s - loss: 21.3509 - mse: 21.3509 - mae: 2.0894 - 28s/epoch - 1ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 28s - loss: 21.5568 - mse: 21.5568 - mae: 2.1055 - 28s/epoch - 1ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 28s - loss: 22.0770 - mse: 22.0770 - mae: 2.1402 - 28s/epoch - 1ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 28s - loss: 22.3907 - mse: 22.3907 - mae: 2.1672 - 28s/epoch - 1ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 28s - loss: 21.9626 - mse: 21.9626 - mae: 2.1164 - 28s/epoch - 1ms/step\n",
            "Score for fold 2: loss of 15.396944999694824\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 28s - loss: 22.0889 - mse: 22.0889 - mae: 2.1336 - 28s/epoch - 1ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 27s - loss: 21.7981 - mse: 21.7981 - mae: 2.1079 - 27s/epoch - 1ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 28s - loss: 21.7765 - mse: 21.7765 - mae: 2.1240 - 28s/epoch - 1ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 27s - loss: 21.5990 - mse: 21.5990 - mae: 2.0911 - 27s/epoch - 1ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 28s - loss: 21.5044 - mse: 21.5044 - mae: 2.0837 - 28s/epoch - 1ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 27s - loss: 21.7298 - mse: 21.7298 - mae: 2.0977 - 27s/epoch - 1ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 27s - loss: 21.5005 - mse: 21.5005 - mae: 2.0756 - 27s/epoch - 1ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 28s - loss: 21.2704 - mse: 21.2704 - mae: 2.0450 - 28s/epoch - 1ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 27s - loss: 21.3830 - mse: 21.3830 - mae: 2.0670 - 27s/epoch - 1ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 27s - loss: 21.8753 - mse: 21.8753 - mae: 2.1119 - 27s/epoch - 1ms/step\n",
            "Score for fold 3: loss of 14.114496231079102\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-09-30 11:29:16,406]\u001b[0m Finished trial#34 resulted in value: 21.875328063964844. Current best value is 14.38178539276123 with parameters: {'activation': 'tanh', 'optimizer': 'sgd', 'num_hidden_layer': 2, 'num_hidden_unit': 81, 'kernel_size': 2, 'filter': 40, 'learning_rate': 0.000693576191437307}.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 23s - loss: nan - mse: nan - mae: nan - 23s/epoch - 1ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 23s - loss: nan - mse: nan - mae: nan - 23s/epoch - 1ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 23s - loss: nan - mse: nan - mae: nan - 23s/epoch - 1ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 23s - loss: nan - mse: nan - mae: nan - 23s/epoch - 1ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 23s - loss: nan - mse: nan - mae: nan - 23s/epoch - 1ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 22s - loss: nan - mse: nan - mae: nan - 22s/epoch - 1ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 22s - loss: nan - mse: nan - mae: nan - 22s/epoch - 1ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 23s - loss: nan - mse: nan - mae: nan - 23s/epoch - 1ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 23s - loss: nan - mse: nan - mae: nan - 23s/epoch - 1ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 23s - loss: nan - mse: nan - mae: nan - 23s/epoch - 1ms/step\n",
            "Score for fold 1: loss of 24.36428451538086\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 23s - loss: nan - mse: nan - mae: nan - 23s/epoch - 1ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 23s - loss: nan - mse: nan - mae: nan - 23s/epoch - 1ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 23s - loss: nan - mse: nan - mae: nan - 23s/epoch - 1ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 23s - loss: nan - mse: nan - mae: nan - 23s/epoch - 1ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 23s - loss: nan - mse: nan - mae: nan - 23s/epoch - 1ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 23s - loss: nan - mse: nan - mae: nan - 23s/epoch - 1ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 22s - loss: nan - mse: nan - mae: nan - 22s/epoch - 1ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 22s - loss: nan - mse: nan - mae: nan - 22s/epoch - 1ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 22s - loss: nan - mse: nan - mae: nan - 22s/epoch - 1ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 22s - loss: nan - mse: nan - mae: nan - 22s/epoch - 1ms/step\n",
            "Score for fold 2: loss of nan\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 23s - loss: nan - mse: nan - mae: nan - 23s/epoch - 1ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 23s - loss: nan - mse: nan - mae: nan - 23s/epoch - 1ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 22s - loss: nan - mse: nan - mae: nan - 22s/epoch - 1ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 23s - loss: nan - mse: nan - mae: nan - 23s/epoch - 1ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 22s - loss: nan - mse: nan - mae: nan - 22s/epoch - 1ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 22s - loss: nan - mse: nan - mae: nan - 22s/epoch - 1ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 23s - loss: nan - mse: nan - mae: nan - 23s/epoch - 1ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 23s - loss: nan - mse: nan - mae: nan - 23s/epoch - 1ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 22s - loss: nan - mse: nan - mae: nan - 22s/epoch - 1ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 22s - loss: nan - mse: nan - mae: nan - 22s/epoch - 1ms/step\n",
            "Score for fold 3: loss of nan\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[33m[W 2022-09-30 11:40:47,370]\u001b[0m Setting status of trial#35 as TrialState.FAIL because the objective function returned nan.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 23s - loss: 18.8119 - mse: 18.8119 - mae: 1.7163 - 23s/epoch - 1ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 22s - loss: 19.2819 - mse: 19.2819 - mae: 1.7424 - 22s/epoch - 1ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 23s - loss: 19.2442 - mse: 19.2442 - mae: 1.7466 - 23s/epoch - 1ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 23s - loss: 20.3114 - mse: 20.3114 - mae: 1.7636 - 23s/epoch - 1ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 23s - loss: 19.1407 - mse: 19.1407 - mae: 1.7329 - 23s/epoch - 1ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 22s - loss: 19.0839 - mse: 19.0839 - mae: 1.7266 - 22s/epoch - 1ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 22s - loss: nan - mse: nan - mae: nan - 22s/epoch - 1ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 23s - loss: nan - mse: nan - mae: nan - 23s/epoch - 1ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 23s - loss: nan - mse: nan - mae: nan - 23s/epoch - 1ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 23s - loss: nan - mse: nan - mae: nan - 23s/epoch - 1ms/step\n",
            "Score for fold 1: loss of 24.130361557006836\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 22s - loss: nan - mse: nan - mae: nan - 22s/epoch - 1ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 23s - loss: nan - mse: nan - mae: nan - 23s/epoch - 1ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 22s - loss: nan - mse: nan - mae: nan - 22s/epoch - 1ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 22s - loss: nan - mse: nan - mae: nan - 22s/epoch - 1ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 22s - loss: nan - mse: nan - mae: nan - 22s/epoch - 1ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 22s - loss: nan - mse: nan - mae: nan - 22s/epoch - 1ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 22s - loss: nan - mse: nan - mae: nan - 22s/epoch - 1ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 22s - loss: nan - mse: nan - mae: nan - 22s/epoch - 1ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 22s - loss: nan - mse: nan - mae: nan - 22s/epoch - 1ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 22s - loss: nan - mse: nan - mae: nan - 22s/epoch - 1ms/step\n",
            "Score for fold 2: loss of nan\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 22s - loss: nan - mse: nan - mae: nan - 22s/epoch - 1ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 22s - loss: nan - mse: nan - mae: nan - 22s/epoch - 1ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 22s - loss: nan - mse: nan - mae: nan - 22s/epoch - 1ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 22s - loss: nan - mse: nan - mae: nan - 22s/epoch - 1ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 22s - loss: nan - mse: nan - mae: nan - 22s/epoch - 1ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 22s - loss: nan - mse: nan - mae: nan - 22s/epoch - 1ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 22s - loss: nan - mse: nan - mae: nan - 22s/epoch - 1ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 22s - loss: nan - mse: nan - mae: nan - 22s/epoch - 1ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 23s - loss: nan - mse: nan - mae: nan - 23s/epoch - 1ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 22s - loss: nan - mse: nan - mae: nan - 22s/epoch - 1ms/step\n",
            "Score for fold 3: loss of nan\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[33m[W 2022-09-30 11:52:05,130]\u001b[0m Setting status of trial#36 as TrialState.FAIL because the objective function returned nan.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 23s - loss: nan - mse: nan - mae: nan - 23s/epoch - 1ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 23s - loss: nan - mse: nan - mae: nan - 23s/epoch - 1ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 24s - loss: nan - mse: nan - mae: nan - 24s/epoch - 1ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 23s - loss: nan - mse: nan - mae: nan - 23s/epoch - 1ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 24s - loss: nan - mse: nan - mae: nan - 24s/epoch - 1ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 24s - loss: nan - mse: nan - mae: nan - 24s/epoch - 1ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 24s - loss: nan - mse: nan - mae: nan - 24s/epoch - 1ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 23s - loss: nan - mse: nan - mae: nan - 23s/epoch - 1ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 23s - loss: nan - mse: nan - mae: nan - 23s/epoch - 1ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 23s - loss: nan - mse: nan - mae: nan - 23s/epoch - 1ms/step\n",
            "Score for fold 1: loss of 24.373947143554688\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 23s - loss: nan - mse: nan - mae: nan - 23s/epoch - 1ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 23s - loss: nan - mse: nan - mae: nan - 23s/epoch - 1ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 23s - loss: nan - mse: nan - mae: nan - 23s/epoch - 1ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 23s - loss: nan - mse: nan - mae: nan - 23s/epoch - 1ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 23s - loss: nan - mse: nan - mae: nan - 23s/epoch - 1ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 24s - loss: nan - mse: nan - mae: nan - 24s/epoch - 1ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 23s - loss: nan - mse: nan - mae: nan - 23s/epoch - 1ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 24s - loss: nan - mse: nan - mae: nan - 24s/epoch - 1ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 23s - loss: nan - mse: nan - mae: nan - 23s/epoch - 1ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 23s - loss: nan - mse: nan - mae: nan - 23s/epoch - 1ms/step\n",
            "Score for fold 2: loss of nan\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 24s - loss: nan - mse: nan - mae: nan - 24s/epoch - 1ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 23s - loss: nan - mse: nan - mae: nan - 23s/epoch - 1ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 23s - loss: nan - mse: nan - mae: nan - 23s/epoch - 1ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 23s - loss: nan - mse: nan - mae: nan - 23s/epoch - 1ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 24s - loss: nan - mse: nan - mae: nan - 24s/epoch - 1ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 24s - loss: nan - mse: nan - mae: nan - 24s/epoch - 1ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 24s - loss: nan - mse: nan - mae: nan - 24s/epoch - 1ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 25s - loss: nan - mse: nan - mae: nan - 25s/epoch - 1ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 24s - loss: nan - mse: nan - mae: nan - 24s/epoch - 1ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 25s - loss: nan - mse: nan - mae: nan - 25s/epoch - 1ms/step\n",
            "Score for fold 3: loss of nan\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[33m[W 2022-09-30 12:04:00,466]\u001b[0m Setting status of trial#37 as TrialState.FAIL because the objective function returned nan.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 31s - loss: nan - mse: nan - mae: nan - 31s/epoch - 2ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 33s - loss: nan - mse: nan - mae: nan - 33s/epoch - 2ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 30s - loss: nan - mse: nan - mae: nan - 30s/epoch - 2ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 31s - loss: nan - mse: nan - mae: nan - 31s/epoch - 2ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 25s - loss: nan - mse: nan - mae: nan - 25s/epoch - 1ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 24s - loss: nan - mse: nan - mae: nan - 24s/epoch - 1ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 25s - loss: nan - mse: nan - mae: nan - 25s/epoch - 1ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 24s - loss: nan - mse: nan - mae: nan - 24s/epoch - 1ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 24s - loss: nan - mse: nan - mae: nan - 24s/epoch - 1ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 23s - loss: nan - mse: nan - mae: nan - 23s/epoch - 1ms/step\n",
            "Score for fold 1: loss of 25.607328414916992\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 25s - loss: nan - mse: nan - mae: nan - 25s/epoch - 1ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 26s - loss: nan - mse: nan - mae: nan - 26s/epoch - 1ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 26s - loss: nan - mse: nan - mae: nan - 26s/epoch - 1ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 25s - loss: nan - mse: nan - mae: nan - 25s/epoch - 1ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 25s - loss: nan - mse: nan - mae: nan - 25s/epoch - 1ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 24s - loss: nan - mse: nan - mae: nan - 24s/epoch - 1ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 25s - loss: nan - mse: nan - mae: nan - 25s/epoch - 1ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 25s - loss: nan - mse: nan - mae: nan - 25s/epoch - 1ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 24s - loss: nan - mse: nan - mae: nan - 24s/epoch - 1ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 24s - loss: nan - mse: nan - mae: nan - 24s/epoch - 1ms/step\n",
            "Score for fold 2: loss of nan\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 24s - loss: nan - mse: nan - mae: nan - 24s/epoch - 1ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 24s - loss: nan - mse: nan - mae: nan - 24s/epoch - 1ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 24s - loss: nan - mse: nan - mae: nan - 24s/epoch - 1ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 24s - loss: nan - mse: nan - mae: nan - 24s/epoch - 1ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 24s - loss: nan - mse: nan - mae: nan - 24s/epoch - 1ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 24s - loss: nan - mse: nan - mae: nan - 24s/epoch - 1ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 24s - loss: nan - mse: nan - mae: nan - 24s/epoch - 1ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 24s - loss: nan - mse: nan - mae: nan - 24s/epoch - 1ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 24s - loss: nan - mse: nan - mae: nan - 24s/epoch - 1ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 24s - loss: nan - mse: nan - mae: nan - 24s/epoch - 1ms/step\n",
            "Score for fold 3: loss of nan\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[33m[W 2022-09-30 12:16:51,249]\u001b[0m Setting status of trial#38 as TrialState.FAIL because the objective function returned nan.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 25s - loss: 17.6529 - mse: 17.6529 - mae: 1.6414 - 25s/epoch - 1ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 25s - loss: 17.3671 - mse: 17.3671 - mae: 1.6377 - 25s/epoch - 1ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 25s - loss: 17.9401 - mse: 17.9401 - mae: 1.6741 - 25s/epoch - 1ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 27s - loss: 17.5022 - mse: 17.5022 - mae: 1.6574 - 27s/epoch - 1ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 25s - loss: 17.2625 - mse: 17.2625 - mae: 1.6417 - 25s/epoch - 1ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 25s - loss: 17.2728 - mse: 17.2728 - mae: 1.6393 - 25s/epoch - 1ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 25s - loss: 17.4930 - mse: 17.4930 - mae: 1.6501 - 25s/epoch - 1ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 25s - loss: 17.5367 - mse: 17.5367 - mae: 1.6565 - 25s/epoch - 1ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 25s - loss: 17.2752 - mse: 17.2752 - mae: 1.6358 - 25s/epoch - 1ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 24s - loss: 17.5365 - mse: 17.5365 - mae: 1.6538 - 24s/epoch - 1ms/step\n",
            "Score for fold 1: loss of 24.12310028076172\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 25s - loss: 17.6912 - mse: 17.6912 - mae: 1.6677 - 25s/epoch - 1ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 25s - loss: 17.5937 - mse: 17.5937 - mae: 1.6556 - 25s/epoch - 1ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 25s - loss: 17.4331 - mse: 17.4331 - mae: 1.6597 - 25s/epoch - 1ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 24s - loss: 17.3064 - mse: 17.3064 - mae: 1.6573 - 24s/epoch - 1ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 25s - loss: 17.3031 - mse: 17.3031 - mae: 1.6485 - 25s/epoch - 1ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 25s - loss: 17.0831 - mse: 17.0831 - mae: 1.6398 - 25s/epoch - 1ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 25s - loss: 17.5807 - mse: 17.5807 - mae: 1.6560 - 25s/epoch - 1ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 24s - loss: 17.5141 - mse: 17.5141 - mae: 1.6583 - 24s/epoch - 1ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 24s - loss: 17.4474 - mse: 17.4474 - mae: 1.6650 - 24s/epoch - 1ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 24s - loss: 17.6610 - mse: 17.6610 - mae: 1.6650 - 24s/epoch - 1ms/step\n",
            "Score for fold 2: loss of 12.791524887084961\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 24s - loss: 17.3917 - mse: 17.3917 - mae: 1.6489 - 24s/epoch - 1ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 24s - loss: 17.4922 - mse: 17.4922 - mae: 1.6519 - 24s/epoch - 1ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 24s - loss: 17.2977 - mse: 17.2977 - mae: 1.6526 - 24s/epoch - 1ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 24s - loss: 17.0564 - mse: 17.0564 - mae: 1.6429 - 24s/epoch - 1ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 23s - loss: 17.1420 - mse: 17.1420 - mae: 1.6354 - 23s/epoch - 1ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 24s - loss: 17.1975 - mse: 17.1975 - mae: 1.6412 - 24s/epoch - 1ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 24s - loss: 17.6639 - mse: 17.6639 - mae: 1.6670 - 24s/epoch - 1ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 25s - loss: 17.3422 - mse: 17.3422 - mae: 1.6522 - 25s/epoch - 1ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 24s - loss: 17.4836 - mse: 17.4836 - mae: 1.6549 - 24s/epoch - 1ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 24s - loss: 17.5149 - mse: 17.5149 - mae: 1.6643 - 24s/epoch - 1ms/step\n",
            "Score for fold 3: loss of 12.886427879333496\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-09-30 12:29:17,727]\u001b[0m Finished trial#39 resulted in value: 17.514904022216797. Current best value is 14.38178539276123 with parameters: {'activation': 'tanh', 'optimizer': 'sgd', 'num_hidden_layer': 2, 'num_hidden_unit': 81, 'kernel_size': 2, 'filter': 40, 'learning_rate': 0.000693576191437307}.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 41s - loss: 17.2446 - mse: 17.2446 - mae: 1.5998 - 41s/epoch - 2ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 43s - loss: 16.6507 - mse: 16.6507 - mae: 1.5776 - 43s/epoch - 2ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 41s - loss: 16.4684 - mse: 16.4684 - mae: 1.5728 - 41s/epoch - 2ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 42s - loss: 16.4912 - mse: 16.4912 - mae: 1.5682 - 42s/epoch - 2ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 41s - loss: 16.3788 - mse: 16.3788 - mae: 1.5656 - 41s/epoch - 2ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 41s - loss: 16.3208 - mse: 16.3208 - mae: 1.5631 - 41s/epoch - 2ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 40s - loss: 16.2788 - mse: 16.2788 - mae: 1.5616 - 40s/epoch - 2ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 40s - loss: 16.1532 - mse: 16.1532 - mae: 1.5592 - 40s/epoch - 2ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 40s - loss: 16.1081 - mse: 16.1081 - mae: 1.5599 - 40s/epoch - 2ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 41s - loss: 16.2602 - mse: 16.2602 - mae: 1.5578 - 41s/epoch - 2ms/step\n",
            "Score for fold 1: loss of 23.293851852416992\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 40s - loss: 16.0798 - mse: 16.0798 - mae: 1.5545 - 40s/epoch - 2ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 40s - loss: 16.0059 - mse: 16.0059 - mae: 1.5526 - 40s/epoch - 2ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 40s - loss: 16.0080 - mse: 16.0080 - mae: 1.5535 - 40s/epoch - 2ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 40s - loss: 16.0356 - mse: 16.0356 - mae: 1.5442 - 40s/epoch - 2ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 40s - loss: 16.0288 - mse: 16.0288 - mae: 1.5419 - 40s/epoch - 2ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 44s - loss: 15.9194 - mse: 15.9194 - mae: 1.5416 - 44s/epoch - 2ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 44s - loss: 15.9041 - mse: 15.9041 - mae: 1.5384 - 44s/epoch - 2ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 42s - loss: 16.0584 - mse: 16.0584 - mae: 1.5375 - 42s/epoch - 2ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 42s - loss: 16.0089 - mse: 16.0089 - mae: 1.5394 - 42s/epoch - 2ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 42s - loss: 15.9363 - mse: 15.9363 - mae: 1.5381 - 42s/epoch - 2ms/step\n",
            "Score for fold 2: loss of 12.104045867919922\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 42s - loss: 15.9202 - mse: 15.9202 - mae: 1.5375 - 42s/epoch - 2ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 41s - loss: 15.8742 - mse: 15.8742 - mae: 1.5388 - 41s/epoch - 2ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 43s - loss: 15.8350 - mse: 15.8350 - mae: 1.5364 - 43s/epoch - 2ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 42s - loss: 15.8912 - mse: 15.8912 - mae: 1.5362 - 42s/epoch - 2ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 42s - loss: 15.8117 - mse: 15.8117 - mae: 1.5350 - 42s/epoch - 2ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 40s - loss: 15.8635 - mse: 15.8635 - mae: 1.5351 - 40s/epoch - 2ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 41s - loss: 15.8960 - mse: 15.8960 - mae: 1.5353 - 41s/epoch - 2ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 41s - loss: 15.8397 - mse: 15.8397 - mae: 1.5340 - 41s/epoch - 2ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 41s - loss: 15.8286 - mse: 15.8286 - mae: 1.5359 - 41s/epoch - 2ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 41s - loss: 16.0104 - mse: 16.0104 - mae: 1.5350 - 41s/epoch - 2ms/step\n",
            "Score for fold 3: loss of 11.552197456359863\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-09-30 12:50:04,933]\u001b[0m Finished trial#40 resulted in value: 16.0103816986084. Current best value is 14.38178539276123 with parameters: {'activation': 'tanh', 'optimizer': 'sgd', 'num_hidden_layer': 2, 'num_hidden_unit': 81, 'kernel_size': 2, 'filter': 40, 'learning_rate': 0.000693576191437307}.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 30s - loss: 17.9462 - mse: 17.9462 - mae: 1.6294 - 30s/epoch - 2ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 30s - loss: 16.8324 - mse: 16.8324 - mae: 1.5757 - 30s/epoch - 1ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 30s - loss: 16.4106 - mse: 16.4106 - mae: 1.5548 - 30s/epoch - 2ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 30s - loss: 16.1334 - mse: 16.1334 - mae: 1.5420 - 30s/epoch - 2ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 31s - loss: 15.9644 - mse: 15.9644 - mae: 1.5342 - 31s/epoch - 2ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 31s - loss: 15.8045 - mse: 15.8045 - mae: 1.5243 - 31s/epoch - 2ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 30s - loss: 15.6811 - mse: 15.6811 - mae: 1.5174 - 30s/epoch - 2ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 30s - loss: 15.5723 - mse: 15.5723 - mae: 1.5107 - 30s/epoch - 1ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 31s - loss: 15.4797 - mse: 15.4797 - mae: 1.5053 - 31s/epoch - 2ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 31s - loss: 15.4043 - mse: 15.4043 - mae: 1.5006 - 31s/epoch - 2ms/step\n",
            "Score for fold 1: loss of 23.92235565185547\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 31s - loss: 15.3573 - mse: 15.3573 - mae: 1.4988 - 31s/epoch - 2ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 31s - loss: 15.2937 - mse: 15.2937 - mae: 1.4959 - 31s/epoch - 2ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 31s - loss: 15.2162 - mse: 15.2162 - mae: 1.4934 - 31s/epoch - 2ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 31s - loss: 15.1886 - mse: 15.1886 - mae: 1.4904 - 31s/epoch - 2ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 30s - loss: 15.1430 - mse: 15.1430 - mae: 1.4888 - 30s/epoch - 2ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 30s - loss: 15.0949 - mse: 15.0949 - mae: 1.4872 - 30s/epoch - 1ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 30s - loss: 15.0990 - mse: 15.0990 - mae: 1.4876 - 30s/epoch - 1ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 30s - loss: 15.0393 - mse: 15.0393 - mae: 1.4850 - 30s/epoch - 1ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 30s - loss: 15.0195 - mse: 15.0195 - mae: 1.4836 - 30s/epoch - 1ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 30s - loss: 14.9682 - mse: 14.9682 - mae: 1.4823 - 30s/epoch - 2ms/step\n",
            "Score for fold 2: loss of 11.217016220092773\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 29s - loss: 14.9393 - mse: 14.9393 - mae: 1.4801 - 29s/epoch - 1ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 29s - loss: 14.9001 - mse: 14.9001 - mae: 1.4795 - 29s/epoch - 1ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 30s - loss: 14.8818 - mse: 14.8818 - mae: 1.4787 - 30s/epoch - 1ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 29s - loss: 14.8463 - mse: 14.8463 - mae: 1.4768 - 29s/epoch - 1ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 29s - loss: 14.8065 - mse: 14.8065 - mae: 1.4761 - 29s/epoch - 1ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 29s - loss: 14.8380 - mse: 14.8380 - mae: 1.4767 - 29s/epoch - 1ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 29s - loss: 14.7634 - mse: 14.7634 - mae: 1.4743 - 29s/epoch - 1ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 29s - loss: 14.7296 - mse: 14.7296 - mae: 1.4742 - 29s/epoch - 1ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 29s - loss: 14.7079 - mse: 14.7079 - mae: 1.4724 - 29s/epoch - 1ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 29s - loss: 14.6843 - mse: 14.6843 - mae: 1.4723 - 29s/epoch - 1ms/step\n",
            "Score for fold 3: loss of 10.68216609954834\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-09-30 13:05:17,414]\u001b[0m Finished trial#41 resulted in value: 14.684320449829102. Current best value is 14.38178539276123 with parameters: {'activation': 'tanh', 'optimizer': 'sgd', 'num_hidden_layer': 2, 'num_hidden_unit': 81, 'kernel_size': 2, 'filter': 40, 'learning_rate': 0.000693576191437307}.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 28s - loss: 18.3279 - mse: 18.3279 - mae: 1.6373 - 28s/epoch - 1ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 27s - loss: 17.7134 - mse: 17.7134 - mae: 1.6021 - 27s/epoch - 1ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 27s - loss: 17.3230 - mse: 17.3230 - mae: 1.5938 - 27s/epoch - 1ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 27s - loss: 17.1677 - mse: 17.1677 - mae: 1.5919 - 27s/epoch - 1ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 27s - loss: 17.0725 - mse: 17.0725 - mae: 1.5895 - 27s/epoch - 1ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 27s - loss: 17.0157 - mse: 17.0157 - mae: 1.5899 - 27s/epoch - 1ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 27s - loss: 16.9602 - mse: 16.9602 - mae: 1.5889 - 27s/epoch - 1ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 28s - loss: 16.9216 - mse: 16.9216 - mae: 1.5879 - 28s/epoch - 1ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 28s - loss: 16.8837 - mse: 16.8837 - mae: 1.5880 - 28s/epoch - 1ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 28s - loss: 16.8626 - mse: 16.8626 - mae: 1.5888 - 28s/epoch - 1ms/step\n",
            "Score for fold 1: loss of 23.221689224243164\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 27s - loss: 16.8213 - mse: 16.8213 - mae: 1.5890 - 27s/epoch - 1ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 27s - loss: 16.8087 - mse: 16.8087 - mae: 1.5866 - 27s/epoch - 1ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 27s - loss: 16.8173 - mse: 16.8173 - mae: 1.5880 - 27s/epoch - 1ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 27s - loss: 16.8346 - mse: 16.8346 - mae: 1.5887 - 27s/epoch - 1ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 27s - loss: 16.8155 - mse: 16.8155 - mae: 1.5896 - 27s/epoch - 1ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 28s - loss: 16.8204 - mse: 16.8204 - mae: 1.5899 - 28s/epoch - 1ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 27s - loss: 16.8249 - mse: 16.8249 - mae: 1.5905 - 27s/epoch - 1ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 27s - loss: 16.8361 - mse: 16.8361 - mae: 1.5904 - 27s/epoch - 1ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 27s - loss: 16.8102 - mse: 16.8102 - mae: 1.5913 - 27s/epoch - 1ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 27s - loss: 16.7813 - mse: 16.7813 - mae: 1.5925 - 27s/epoch - 1ms/step\n",
            "Score for fold 2: loss of 12.423508644104004\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 27s - loss: 16.7687 - mse: 16.7687 - mae: 1.5926 - 27s/epoch - 1ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 27s - loss: 16.7901 - mse: 16.7901 - mae: 1.5928 - 27s/epoch - 1ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 27s - loss: 16.7640 - mse: 16.7640 - mae: 1.5913 - 27s/epoch - 1ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 28s - loss: 16.7645 - mse: 16.7645 - mae: 1.5920 - 28s/epoch - 1ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 28s - loss: 16.7569 - mse: 16.7569 - mae: 1.5915 - 28s/epoch - 1ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 31s - loss: 16.7459 - mse: 16.7459 - mae: 1.5922 - 31s/epoch - 2ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 27s - loss: 16.7868 - mse: 16.7868 - mae: 1.5923 - 27s/epoch - 1ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 28s - loss: 16.7662 - mse: 16.7662 - mae: 1.5922 - 28s/epoch - 1ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 28s - loss: 16.7605 - mse: 16.7605 - mae: 1.5950 - 28s/epoch - 1ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 27s - loss: 16.7589 - mse: 16.7589 - mae: 1.5941 - 27s/epoch - 1ms/step\n",
            "Score for fold 3: loss of 12.424995422363281\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-09-30 13:19:14,698]\u001b[0m Finished trial#42 resulted in value: 16.75894546508789. Current best value is 14.38178539276123 with parameters: {'activation': 'tanh', 'optimizer': 'sgd', 'num_hidden_layer': 2, 'num_hidden_unit': 81, 'kernel_size': 2, 'filter': 40, 'learning_rate': 0.000693576191437307}.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 31s - loss: 17.8704 - mse: 17.8704 - mae: 1.6315 - 31s/epoch - 2ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 31s - loss: 17.2433 - mse: 17.2433 - mae: 1.6064 - 31s/epoch - 2ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 31s - loss: 17.0856 - mse: 17.0856 - mae: 1.6032 - 31s/epoch - 2ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 31s - loss: 17.0127 - mse: 17.0127 - mae: 1.6034 - 31s/epoch - 2ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 32s - loss: 16.9733 - mse: 16.9733 - mae: 1.6030 - 32s/epoch - 2ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 30s - loss: 16.9460 - mse: 16.9460 - mae: 1.6031 - 30s/epoch - 2ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 28s - loss: 16.9258 - mse: 16.9258 - mae: 1.6031 - 28s/epoch - 1ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 28s - loss: 16.9101 - mse: 16.9101 - mae: 1.6036 - 28s/epoch - 1ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 28s - loss: 16.8980 - mse: 16.8980 - mae: 1.6029 - 28s/epoch - 1ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 29s - loss: 16.8878 - mse: 16.8878 - mae: 1.6047 - 29s/epoch - 1ms/step\n",
            "Score for fold 1: loss of 23.444196701049805\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 28s - loss: 16.8791 - mse: 16.8791 - mae: 1.6040 - 28s/epoch - 1ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 28s - loss: 16.8712 - mse: 16.8712 - mae: 1.6032 - 28s/epoch - 1ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 27s - loss: 16.8643 - mse: 16.8643 - mae: 1.6026 - 27s/epoch - 1ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 27s - loss: 16.8582 - mse: 16.8582 - mae: 1.6035 - 27s/epoch - 1ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 28s - loss: 16.8526 - mse: 16.8526 - mae: 1.6025 - 28s/epoch - 1ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 27s - loss: 16.8483 - mse: 16.8483 - mae: 1.6027 - 27s/epoch - 1ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 27s - loss: 16.8431 - mse: 16.8431 - mae: 1.6020 - 27s/epoch - 1ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 27s - loss: 16.8383 - mse: 16.8383 - mae: 1.6022 - 27s/epoch - 1ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 27s - loss: 16.8339 - mse: 16.8339 - mae: 1.6020 - 27s/epoch - 1ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 28s - loss: 16.8305 - mse: 16.8305 - mae: 1.6020 - 28s/epoch - 1ms/step\n",
            "Score for fold 2: loss of 12.24199104309082\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 27s - loss: 16.8271 - mse: 16.8271 - mae: 1.6013 - 27s/epoch - 1ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 28s - loss: 16.8234 - mse: 16.8234 - mae: 1.6017 - 28s/epoch - 1ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 27s - loss: 16.8202 - mse: 16.8202 - mae: 1.6009 - 27s/epoch - 1ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 27s - loss: 16.8170 - mse: 16.8170 - mae: 1.6001 - 27s/epoch - 1ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 28s - loss: 16.8143 - mse: 16.8143 - mae: 1.6017 - 28s/epoch - 1ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 27s - loss: 16.8117 - mse: 16.8117 - mae: 1.6007 - 27s/epoch - 1ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 28s - loss: 16.8084 - mse: 16.8084 - mae: 1.5988 - 28s/epoch - 1ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 27s - loss: 16.8065 - mse: 16.8065 - mae: 1.6015 - 27s/epoch - 1ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 28s - loss: 16.8034 - mse: 16.8034 - mae: 1.6013 - 28s/epoch - 1ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 28s - loss: 16.8014 - mse: 16.8014 - mae: 1.5991 - 28s/epoch - 1ms/step\n",
            "Score for fold 3: loss of 12.207408905029297\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-09-30 13:33:35,318]\u001b[0m Finished trial#43 resulted in value: 16.801410675048828. Current best value is 14.38178539276123 with parameters: {'activation': 'tanh', 'optimizer': 'sgd', 'num_hidden_layer': 2, 'num_hidden_unit': 81, 'kernel_size': 2, 'filter': 40, 'learning_rate': 0.000693576191437307}.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 29s - loss: 16.9491 - mse: 16.9491 - mae: 1.5822 - 29s/epoch - 1ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 28s - loss: 16.8484 - mse: 16.8484 - mae: 1.5826 - 28s/epoch - 1ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 28s - loss: 17.2963 - mse: 17.2963 - mae: 1.6010 - 28s/epoch - 1ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 28s - loss: 17.7343 - mse: 17.7343 - mae: 1.6168 - 28s/epoch - 1ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 29s - loss: 17.8177 - mse: 17.8177 - mae: 1.6282 - 29s/epoch - 1ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 28s - loss: 17.6641 - mse: 17.6642 - mae: 1.6370 - 28s/epoch - 1ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 28s - loss: 17.6396 - mse: 17.6396 - mae: 1.6398 - 28s/epoch - 1ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 29s - loss: 17.6573 - mse: 17.6573 - mae: 1.6428 - 29s/epoch - 1ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 28s - loss: 17.7231 - mse: 17.7231 - mae: 1.6471 - 28s/epoch - 1ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 28s - loss: 17.8088 - mse: 17.8088 - mae: 1.6500 - 28s/epoch - 1ms/step\n",
            "Score for fold 1: loss of 26.394392013549805\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 28s - loss: 17.7284 - mse: 17.7284 - mae: 1.6437 - 28s/epoch - 1ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 29s - loss: 17.7989 - mse: 17.7989 - mae: 1.6419 - 29s/epoch - 1ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 28s - loss: 17.8847 - mse: 17.8847 - mae: 1.6436 - 28s/epoch - 1ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 28s - loss: 17.9272 - mse: 17.9272 - mae: 1.6430 - 28s/epoch - 1ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 28s - loss: 18.0520 - mse: 18.0520 - mae: 1.6392 - 28s/epoch - 1ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 28s - loss: 17.9998 - mse: 17.9998 - mae: 1.6351 - 28s/epoch - 1ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 27s - loss: 18.1303 - mse: 18.1303 - mae: 1.6354 - 27s/epoch - 1ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 28s - loss: 18.0131 - mse: 18.0131 - mae: 1.6385 - 28s/epoch - 1ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 28s - loss: 18.1751 - mse: 18.1751 - mae: 1.6423 - 28s/epoch - 1ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 27s - loss: 18.1254 - mse: 18.1254 - mae: 1.6374 - 27s/epoch - 1ms/step\n",
            "Score for fold 2: loss of 13.717713356018066\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 28s - loss: 18.4183 - mse: 18.4183 - mae: 1.6297 - 28s/epoch - 1ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 28s - loss: 18.1455 - mse: 18.1455 - mae: 1.6290 - 28s/epoch - 1ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 27s - loss: 17.7860 - mse: 17.7860 - mae: 1.6294 - 27s/epoch - 1ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 27s - loss: 18.5610 - mse: 18.5610 - mae: 1.6394 - 27s/epoch - 1ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 28s - loss: 18.7188 - mse: 18.7188 - mae: 1.6436 - 28s/epoch - 1ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 27s - loss: 18.5460 - mse: 18.5460 - mae: 1.6454 - 27s/epoch - 1ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 27s - loss: 20.7476 - mse: 20.7476 - mae: 1.6514 - 27s/epoch - 1ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 28s - loss: 19.6748 - mse: 19.6748 - mae: 1.6573 - 28s/epoch - 1ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 27s - loss: 18.8849 - mse: 18.8849 - mae: 1.6633 - 27s/epoch - 1ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 27s - loss: 18.4983 - mse: 18.4983 - mae: 1.6393 - 27s/epoch - 1ms/step\n",
            "Score for fold 3: loss of 13.230597496032715\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-09-30 13:47:38,475]\u001b[0m Finished trial#44 resulted in value: 18.49834442138672. Current best value is 14.38178539276123 with parameters: {'activation': 'tanh', 'optimizer': 'sgd', 'num_hidden_layer': 2, 'num_hidden_unit': 81, 'kernel_size': 2, 'filter': 40, 'learning_rate': 0.000693576191437307}.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 21s - loss: 18.0377 - mse: 18.0377 - mae: 1.6768 - 21s/epoch - 1ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 20s - loss: 17.3926 - mse: 17.3926 - mae: 1.6411 - 20s/epoch - 1ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 21s - loss: 17.2502 - mse: 17.2502 - mae: 1.6288 - 21s/epoch - 1ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 21s - loss: 17.2661 - mse: 17.2661 - mae: 1.6295 - 21s/epoch - 1ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 21s - loss: 17.3371 - mse: 17.3371 - mae: 1.6328 - 21s/epoch - 1ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 20s - loss: 17.5206 - mse: 17.5206 - mae: 1.6387 - 20s/epoch - 1ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 21s - loss: 17.6886 - mse: 17.6886 - mae: 1.6642 - 21s/epoch - 1ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 20s - loss: 17.6323 - mse: 17.6323 - mae: 1.6478 - 20s/epoch - 1ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 20s - loss: 17.5850 - mse: 17.5850 - mae: 1.6501 - 20s/epoch - 1ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 21s - loss: 17.6211 - mse: 17.6211 - mae: 1.6485 - 21s/epoch - 1ms/step\n",
            "Score for fold 1: loss of 21.84690284729004\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 20s - loss: 17.5888 - mse: 17.5888 - mae: 1.6464 - 20s/epoch - 999us/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 21s - loss: 17.5287 - mse: 17.5287 - mae: 1.6463 - 21s/epoch - 1ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 21s - loss: 17.4330 - mse: 17.4330 - mae: 1.6430 - 21s/epoch - 1ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 21s - loss: 17.5955 - mse: 17.5955 - mae: 1.6455 - 21s/epoch - 1ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 20s - loss: 17.6663 - mse: 17.6663 - mae: 1.6506 - 20s/epoch - 1ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 20s - loss: 17.6503 - mse: 17.6503 - mae: 1.6526 - 20s/epoch - 1ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 20s - loss: 17.6041 - mse: 17.6041 - mae: 1.6543 - 20s/epoch - 992us/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 21s - loss: 17.5224 - mse: 17.5224 - mae: 1.6514 - 21s/epoch - 1ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 21s - loss: 17.4287 - mse: 17.4287 - mae: 1.6513 - 21s/epoch - 1ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 20s - loss: 17.8388 - mse: 17.8388 - mae: 1.6935 - 20s/epoch - 1ms/step\n",
            "Score for fold 2: loss of 13.104246139526367\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 20s - loss: 17.5947 - mse: 17.5947 - mae: 1.6493 - 20s/epoch - 1ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 20s - loss: 17.4477 - mse: 17.4477 - mae: 1.6489 - 20s/epoch - 1ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 20s - loss: 17.8472 - mse: 17.8472 - mae: 1.6591 - 20s/epoch - 1ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 20s - loss: 17.7959 - mse: 17.7959 - mae: 1.6600 - 20s/epoch - 1ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 20s - loss: 17.6347 - mse: 17.6347 - mae: 1.6500 - 20s/epoch - 978us/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 20s - loss: 17.5068 - mse: 17.5068 - mae: 1.6552 - 20s/epoch - 998us/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 20s - loss: 17.4376 - mse: 17.4376 - mae: 1.6529 - 20s/epoch - 1ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 20s - loss: 17.3848 - mse: 17.3848 - mae: 1.6580 - 20s/epoch - 986us/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 20s - loss: 18.2322 - mse: 18.2322 - mae: 1.7303 - 20s/epoch - 1ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 20s - loss: 18.2681 - mse: 18.2681 - mae: 1.6933 - 20s/epoch - 999us/step\n",
            "Score for fold 3: loss of 13.160038948059082\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-09-30 13:57:57,737]\u001b[0m Finished trial#45 resulted in value: 18.268056869506836. Current best value is 14.38178539276123 with parameters: {'activation': 'tanh', 'optimizer': 'sgd', 'num_hidden_layer': 2, 'num_hidden_unit': 81, 'kernel_size': 2, 'filter': 40, 'learning_rate': 0.000693576191437307}.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 27s - loss: 17.3636 - mse: 17.3636 - mae: 1.6061 - 27s/epoch - 1ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 28s - loss: 16.3302 - mse: 16.3302 - mae: 1.5561 - 28s/epoch - 1ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 28s - loss: 15.8979 - mse: 15.8979 - mae: 1.5327 - 28s/epoch - 1ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 26s - loss: 15.6989 - mse: 15.6989 - mae: 1.5204 - 26s/epoch - 1ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 25s - loss: 15.5506 - mse: 15.5506 - mae: 1.5132 - 25s/epoch - 1ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 25s - loss: 15.4306 - mse: 15.4306 - mae: 1.5054 - 25s/epoch - 1ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 25s - loss: 15.3349 - mse: 15.3349 - mae: 1.5045 - 25s/epoch - 1ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 25s - loss: 15.2772 - mse: 15.2772 - mae: 1.5017 - 25s/epoch - 1ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 25s - loss: 15.1546 - mse: 15.1546 - mae: 1.4968 - 25s/epoch - 1ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 25s - loss: 15.1052 - mse: 15.1052 - mae: 1.4943 - 25s/epoch - 1ms/step\n",
            "Score for fold 1: loss of 23.97454071044922\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 25s - loss: 15.0702 - mse: 15.0702 - mae: 1.4931 - 25s/epoch - 1ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 25s - loss: 15.0451 - mse: 15.0451 - mae: 1.4946 - 25s/epoch - 1ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 25s - loss: 14.9917 - mse: 14.9917 - mae: 1.4880 - 25s/epoch - 1ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 25s - loss: 14.9437 - mse: 14.9437 - mae: 1.4877 - 25s/epoch - 1ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 25s - loss: 14.9254 - mse: 14.9254 - mae: 1.4879 - 25s/epoch - 1ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 25s - loss: 14.8723 - mse: 14.8723 - mae: 1.4848 - 25s/epoch - 1ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 25s - loss: 14.8339 - mse: 14.8339 - mae: 1.4825 - 25s/epoch - 1ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 25s - loss: 14.7896 - mse: 14.7896 - mae: 1.4808 - 25s/epoch - 1ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 26s - loss: 14.7536 - mse: 14.7536 - mae: 1.4801 - 26s/epoch - 1ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 25s - loss: 14.7379 - mse: 14.7379 - mae: 1.4776 - 25s/epoch - 1ms/step\n",
            "Score for fold 2: loss of 10.86792278289795\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 26s - loss: 14.6904 - mse: 14.6904 - mae: 1.4771 - 26s/epoch - 1ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 26s - loss: 14.6228 - mse: 14.6228 - mae: 1.4753 - 26s/epoch - 1ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 25s - loss: 14.6264 - mse: 14.6264 - mae: 1.4785 - 25s/epoch - 1ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 25s - loss: 14.6050 - mse: 14.6050 - mae: 1.4755 - 25s/epoch - 1ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 25s - loss: 14.6591 - mse: 14.6591 - mae: 1.4936 - 25s/epoch - 1ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 25s - loss: 14.5745 - mse: 14.5745 - mae: 1.4761 - 25s/epoch - 1ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 25s - loss: 14.5083 - mse: 14.5083 - mae: 1.4723 - 25s/epoch - 1ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 25s - loss: 14.5504 - mse: 14.5504 - mae: 1.4795 - 25s/epoch - 1ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 25s - loss: 14.4783 - mse: 14.4783 - mae: 1.4717 - 25s/epoch - 1ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 26s - loss: 14.3950 - mse: 14.3950 - mae: 1.4675 - 26s/epoch - 1ms/step\n",
            "Score for fold 3: loss of 10.54679012298584\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-09-30 14:10:51,226]\u001b[0m Finished trial#46 resulted in value: 14.395009994506836. Current best value is 14.38178539276123 with parameters: {'activation': 'tanh', 'optimizer': 'sgd', 'num_hidden_layer': 2, 'num_hidden_unit': 81, 'kernel_size': 2, 'filter': 40, 'learning_rate': 0.000693576191437307}.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 25s - loss: 17.1156 - mse: 17.1156 - mae: 1.6059 - 25s/epoch - 1ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 24s - loss: 16.1366 - mse: 16.1366 - mae: 1.5490 - 24s/epoch - 1ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 24s - loss: 15.9060 - mse: 15.9060 - mae: 1.5370 - 24s/epoch - 1ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 24s - loss: 15.7938 - mse: 15.7938 - mae: 1.5371 - 24s/epoch - 1ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 24s - loss: 15.6450 - mse: 15.6450 - mae: 1.5292 - 24s/epoch - 1ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 24s - loss: 15.6468 - mse: 15.6468 - mae: 1.5248 - 24s/epoch - 1ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 23s - loss: 15.7050 - mse: 15.7050 - mae: 1.5294 - 23s/epoch - 1ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 23s - loss: 15.6968 - mse: 15.6968 - mae: 1.5313 - 23s/epoch - 1ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 25s - loss: 15.5835 - mse: 15.5835 - mae: 1.5255 - 25s/epoch - 1ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 24s - loss: 15.5085 - mse: 15.5085 - mae: 1.5202 - 24s/epoch - 1ms/step\n",
            "Score for fold 1: loss of 24.659317016601562\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 23s - loss: 15.4539 - mse: 15.4539 - mae: 1.5204 - 23s/epoch - 1ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 25s - loss: 15.3187 - mse: 15.3187 - mae: 1.5151 - 25s/epoch - 1ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 24s - loss: 15.2608 - mse: 15.2608 - mae: 1.5171 - 24s/epoch - 1ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 23s - loss: 15.2519 - mse: 15.2519 - mae: 1.5125 - 23s/epoch - 1ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 23s - loss: 15.1706 - mse: 15.1706 - mae: 1.5107 - 23s/epoch - 1ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 25s - loss: 15.2134 - mse: 15.2134 - mae: 1.5148 - 25s/epoch - 1ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 23s - loss: 15.4229 - mse: 15.4229 - mae: 1.5238 - 23s/epoch - 1ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 24s - loss: 15.2016 - mse: 15.2016 - mae: 1.5144 - 24s/epoch - 1ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 23s - loss: 15.0894 - mse: 15.0894 - mae: 1.5094 - 23s/epoch - 1ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 23s - loss: 15.0622 - mse: 15.0622 - mae: 1.5065 - 23s/epoch - 1ms/step\n",
            "Score for fold 2: loss of 11.136448860168457\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 22s - loss: 15.1068 - mse: 15.1068 - mae: 1.5092 - 22s/epoch - 1ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 23s - loss: 15.0038 - mse: 15.0038 - mae: 1.5039 - 23s/epoch - 1ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 23s - loss: 14.9902 - mse: 14.9902 - mae: 1.5031 - 23s/epoch - 1ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 23s - loss: 15.1487 - mse: 15.1487 - mae: 1.5213 - 23s/epoch - 1ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 23s - loss: 15.0016 - mse: 15.0016 - mae: 1.5084 - 23s/epoch - 1ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 23s - loss: 15.0176 - mse: 15.0176 - mae: 1.5111 - 23s/epoch - 1ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 23s - loss: 15.0091 - mse: 15.0091 - mae: 1.5066 - 23s/epoch - 1ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 23s - loss: 14.9175 - mse: 14.9175 - mae: 1.5052 - 23s/epoch - 1ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 23s - loss: 14.9871 - mse: 14.9871 - mae: 1.5165 - 23s/epoch - 1ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 23s - loss: 14.8921 - mse: 14.8921 - mae: 1.5083 - 23s/epoch - 1ms/step\n",
            "Score for fold 3: loss of 11.155153274536133\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-09-30 14:22:44,173]\u001b[0m Finished trial#47 resulted in value: 14.892133712768555. Current best value is 14.38178539276123 with parameters: {'activation': 'tanh', 'optimizer': 'sgd', 'num_hidden_layer': 2, 'num_hidden_unit': 81, 'kernel_size': 2, 'filter': 40, 'learning_rate': 0.000693576191437307}.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 26s - loss: 17.5871 - mse: 17.5871 - mae: 1.6167 - 26s/epoch - 1ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 26s - loss: 16.5663 - mse: 16.5663 - mae: 1.5715 - 26s/epoch - 1ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 26s - loss: 16.1452 - mse: 16.1452 - mae: 1.5472 - 26s/epoch - 1ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 26s - loss: 15.8713 - mse: 15.8713 - mae: 1.5282 - 26s/epoch - 1ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 26s - loss: 15.7001 - mse: 15.7001 - mae: 1.5195 - 26s/epoch - 1ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 27s - loss: 15.5796 - mse: 15.5796 - mae: 1.5129 - 27s/epoch - 1ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 26s - loss: 15.4305 - mse: 15.4305 - mae: 1.5063 - 26s/epoch - 1ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 26s - loss: 15.3667 - mse: 15.3667 - mae: 1.5023 - 26s/epoch - 1ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 27s - loss: 15.3119 - mse: 15.3119 - mae: 1.5006 - 27s/epoch - 1ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 27s - loss: 15.2601 - mse: 15.2601 - mae: 1.4985 - 27s/epoch - 1ms/step\n",
            "Score for fold 1: loss of 23.429912567138672\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 27s - loss: 15.2055 - mse: 15.2055 - mae: 1.4966 - 27s/epoch - 1ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 27s - loss: 15.1517 - mse: 15.1517 - mae: 1.4935 - 27s/epoch - 1ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 26s - loss: 15.0481 - mse: 15.0481 - mae: 1.4904 - 26s/epoch - 1ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 26s - loss: 15.0190 - mse: 15.0190 - mae: 1.4889 - 26s/epoch - 1ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 30s - loss: 14.9658 - mse: 14.9658 - mae: 1.4868 - 30s/epoch - 1ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 66s - loss: 14.9360 - mse: 14.9360 - mae: 1.4856 - 66s/epoch - 3ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 66s - loss: 14.9095 - mse: 14.9095 - mae: 1.4848 - 66s/epoch - 3ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 65s - loss: 14.8519 - mse: 14.8519 - mae: 1.4811 - 65s/epoch - 3ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 65s - loss: 14.8293 - mse: 14.8293 - mae: 1.4803 - 65s/epoch - 3ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 59s - loss: 14.7878 - mse: 14.7878 - mae: 1.4793 - 59s/epoch - 3ms/step\n",
            "Score for fold 2: loss of 10.90858268737793\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 52s - loss: 14.7649 - mse: 14.7649 - mae: 1.4787 - 52s/epoch - 3ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 57s - loss: 14.7179 - mse: 14.7179 - mae: 1.4759 - 57s/epoch - 3ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 57s - loss: 14.6903 - mse: 14.6903 - mae: 1.4747 - 57s/epoch - 3ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 56s - loss: 14.6750 - mse: 14.6750 - mae: 1.4754 - 56s/epoch - 3ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 55s - loss: 14.6306 - mse: 14.6306 - mae: 1.4729 - 55s/epoch - 3ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 55s - loss: 14.6003 - mse: 14.6003 - mae: 1.4713 - 55s/epoch - 3ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 55s - loss: 14.6317 - mse: 14.6317 - mae: 1.4746 - 55s/epoch - 3ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 54s - loss: 14.5580 - mse: 14.5580 - mae: 1.4715 - 54s/epoch - 3ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 53s - loss: 14.5223 - mse: 14.5223 - mae: 1.4695 - 53s/epoch - 3ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 48s - loss: 14.4846 - mse: 14.4846 - mae: 1.4676 - 48s/epoch - 2ms/step\n",
            "Score for fold 3: loss of 10.97659969329834\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-09-30 14:43:55,882]\u001b[0m Finished trial#48 resulted in value: 14.484635353088379. Current best value is 14.38178539276123 with parameters: {'activation': 'tanh', 'optimizer': 'sgd', 'num_hidden_layer': 2, 'num_hidden_unit': 81, 'kernel_size': 2, 'filter': 40, 'learning_rate': 0.000693576191437307}.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 27s - loss: 18.0008 - mse: 18.0008 - mae: 1.6323 - 27s/epoch - 1ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 27s - loss: 16.9701 - mse: 16.9701 - mae: 1.5841 - 27s/epoch - 1ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 28s - loss: 16.5931 - mse: 16.5931 - mae: 1.5699 - 28s/epoch - 1ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 27s - loss: 16.2968 - mse: 16.2968 - mae: 1.5553 - 27s/epoch - 1ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 26s - loss: 16.0707 - mse: 16.0707 - mae: 1.5410 - 26s/epoch - 1ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 27s - loss: 15.8990 - mse: 15.8990 - mae: 1.5293 - 27s/epoch - 1ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 26s - loss: 15.7580 - mse: 15.7580 - mae: 1.5211 - 26s/epoch - 1ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 31s - loss: 15.6356 - mse: 15.6356 - mae: 1.5131 - 31s/epoch - 2ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 32s - loss: 15.5570 - mse: 15.5570 - mae: 1.5076 - 32s/epoch - 2ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 31s - loss: 15.4548 - mse: 15.4548 - mae: 1.5021 - 31s/epoch - 2ms/step\n",
            "Score for fold 1: loss of 23.360483169555664\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 28s - loss: 15.3917 - mse: 15.3917 - mae: 1.4979 - 28s/epoch - 1ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 30s - loss: 15.3263 - mse: 15.3263 - mae: 1.4949 - 30s/epoch - 1ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 28s - loss: 15.2813 - mse: 15.2813 - mae: 1.4932 - 28s/epoch - 1ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 27s - loss: 15.2240 - mse: 15.2240 - mae: 1.4900 - 27s/epoch - 1ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 32s - loss: 15.1549 - mse: 15.1549 - mae: 1.4879 - 32s/epoch - 2ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 49s - loss: 15.1147 - mse: 15.1147 - mae: 1.4863 - 49s/epoch - 2ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 48s - loss: 15.0718 - mse: 15.0718 - mae: 1.4847 - 48s/epoch - 2ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 48s - loss: 15.0456 - mse: 15.0456 - mae: 1.4836 - 48s/epoch - 2ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 47s - loss: 14.9912 - mse: 14.9912 - mae: 1.4820 - 47s/epoch - 2ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 44s - loss: 14.9618 - mse: 14.9618 - mae: 1.4817 - 44s/epoch - 2ms/step\n",
            "Score for fold 2: loss of 11.380929946899414\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 34s - loss: 14.9284 - mse: 14.9284 - mae: 1.4798 - 34s/epoch - 2ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 38s - loss: 14.9026 - mse: 14.9026 - mae: 1.4799 - 38s/epoch - 2ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 38s - loss: 14.8597 - mse: 14.8597 - mae: 1.4778 - 38s/epoch - 2ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 37s - loss: 14.8450 - mse: 14.8450 - mae: 1.4768 - 37s/epoch - 2ms/step\n",
            "Epoch 5/10\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#{'activation': 'tanh', 'optimizer': 'sgd', 'num_hidden_layer': 2, 'num_hidden_unit': 81, 'kernel_size': 2, 'filter': 40, 'learning_rate': 0.000693576191437307}\n",
        "optimizer = SGD(learning_rate=0.000693576191437307)\n",
        "\n",
        "num_folds = 3\n",
        "kfold=KFold(n_splits=3,shuffle=True)\n",
        "fold_no=1\n",
        "loss_per_fold = []\n",
        "model = create_model_cnn(activation='tanh', num_hidden_layer=2, num_hidden_unit=81, kernel_size=2, filter=40)\n",
        "model.summary()\n",
        "from keras.utils.vis_utils import plot_model\n",
        "plot_model(model, to_file='model_plot.png', show_shapes=True, show_layer_names=True)\n",
        "model.compile(loss='mse',optimizer=optimizer,metrics=['mse','mae'])\n",
        "for train,test in kfold.split(training,labelsForTrain):\n",
        "  scores=model.evaluate(testing,labelsForTest,verbose=0)\n",
        "\n",
        "  print('------------------------------------------------------------------------')\n",
        "  print(f'Training for fold {fold_no} ...')\n",
        "  \n",
        "  # Fit data to model\n",
        "  history = model.fit(training, labelsForTrain,\n",
        "                batch_size=20,\n",
        "                epochs=20,\n",
        "                verbose=2)\n",
        "    \n",
        "  print(f'Score for fold {fold_no}: {model.metrics_names[0]} of {scores[0]}')\n",
        "  loss_per_fold.append(scores[0])\n",
        "  # Increase fold number\n",
        "  fold_no = fold_no + 1    "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xEJ-3GH-Zqdq",
        "outputId": "f4fef15c-bdd4-48c8-bc35-9f66431bbe78"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv1d_1 (Conv1D)           (None, 17, 40)            120       \n",
            "                                                                 \n",
            " max_pooling1d_1 (MaxPooling  (None, 8, 40)            0         \n",
            " 1D)                                                             \n",
            "                                                                 \n",
            " flatten_1 (Flatten)         (None, 320)               0         \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 81)                26001     \n",
            "                                                                 \n",
            " dense_3 (Dense)             (None, 1)                 82        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 26,203\n",
            "Trainable params: 26,203\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/20\n",
            "20000/20000 - 26s - loss: 17.2971 - mse: 17.2971 - mae: 1.6127 - 26s/epoch - 1ms/step\n",
            "Epoch 2/20\n",
            "20000/20000 - 26s - loss: 16.2696 - mse: 16.2696 - mae: 1.5584 - 26s/epoch - 1ms/step\n",
            "Epoch 3/20\n",
            "20000/20000 - 25s - loss: 15.8710 - mse: 15.8710 - mae: 1.5314 - 25s/epoch - 1ms/step\n",
            "Epoch 4/20\n",
            "20000/20000 - 25s - loss: 15.6570 - mse: 15.6570 - mae: 1.5199 - 25s/epoch - 1ms/step\n",
            "Epoch 5/20\n",
            "20000/20000 - 25s - loss: 15.4993 - mse: 15.4993 - mae: 1.5131 - 25s/epoch - 1ms/step\n",
            "Epoch 6/20\n",
            "20000/20000 - 25s - loss: 15.4390 - mse: 15.4390 - mae: 1.5088 - 25s/epoch - 1ms/step\n",
            "Epoch 7/20\n",
            "20000/20000 - 25s - loss: 15.3349 - mse: 15.3349 - mae: 1.5055 - 25s/epoch - 1ms/step\n",
            "Epoch 8/20\n",
            "20000/20000 - 25s - loss: 15.2470 - mse: 15.2470 - mae: 1.5010 - 25s/epoch - 1ms/step\n",
            "Epoch 9/20\n",
            "20000/20000 - 25s - loss: 15.1692 - mse: 15.1692 - mae: 1.4996 - 25s/epoch - 1ms/step\n",
            "Epoch 10/20\n",
            "20000/20000 - 25s - loss: 15.1160 - mse: 15.1160 - mae: 1.4968 - 25s/epoch - 1ms/step\n",
            "Epoch 11/20\n",
            "20000/20000 - 25s - loss: 15.0399 - mse: 15.0399 - mae: 1.4938 - 25s/epoch - 1ms/step\n",
            "Epoch 12/20\n",
            "20000/20000 - 25s - loss: 15.0006 - mse: 15.0006 - mae: 1.4924 - 25s/epoch - 1ms/step\n",
            "Epoch 13/20\n",
            "20000/20000 - 25s - loss: 14.9715 - mse: 14.9715 - mae: 1.4909 - 25s/epoch - 1ms/step\n",
            "Epoch 14/20\n",
            "20000/20000 - 24s - loss: 14.9218 - mse: 14.9218 - mae: 1.4915 - 24s/epoch - 1ms/step\n",
            "Epoch 15/20\n",
            "20000/20000 - 24s - loss: 14.9357 - mse: 14.9357 - mae: 1.4924 - 24s/epoch - 1ms/step\n",
            "Epoch 16/20\n",
            "20000/20000 - 25s - loss: 14.8799 - mse: 14.8799 - mae: 1.4896 - 25s/epoch - 1ms/step\n",
            "Epoch 17/20\n",
            "20000/20000 - 24s - loss: 14.8531 - mse: 14.8531 - mae: 1.4992 - 24s/epoch - 1ms/step\n",
            "Epoch 18/20\n",
            "20000/20000 - 24s - loss: 14.8006 - mse: 14.8006 - mae: 1.4881 - 24s/epoch - 1ms/step\n",
            "Epoch 19/20\n",
            "20000/20000 - 24s - loss: 14.8052 - mse: 14.8052 - mae: 1.4849 - 24s/epoch - 1ms/step\n",
            "Epoch 20/20\n",
            "20000/20000 - 24s - loss: 14.7864 - mse: 14.7864 - mae: 1.4849 - 24s/epoch - 1ms/step\n",
            "Score for fold 1: loss of 24.219444274902344\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/20\n",
            "20000/20000 - 25s - loss: 14.7672 - mse: 14.7672 - mae: 1.4847 - 25s/epoch - 1ms/step\n",
            "Epoch 2/20\n",
            "20000/20000 - 25s - loss: 14.7043 - mse: 14.7043 - mae: 1.4816 - 25s/epoch - 1ms/step\n",
            "Epoch 3/20\n",
            "20000/20000 - 24s - loss: 14.6537 - mse: 14.6537 - mae: 1.4812 - 24s/epoch - 1ms/step\n",
            "Epoch 4/20\n",
            "20000/20000 - 25s - loss: 14.6511 - mse: 14.6511 - mae: 1.4807 - 25s/epoch - 1ms/step\n",
            "Epoch 5/20\n",
            "20000/20000 - 24s - loss: 14.5717 - mse: 14.5717 - mae: 1.4784 - 24s/epoch - 1ms/step\n",
            "Epoch 6/20\n",
            "20000/20000 - 25s - loss: 14.5461 - mse: 14.5461 - mae: 1.4765 - 25s/epoch - 1ms/step\n",
            "Epoch 7/20\n",
            "20000/20000 - 25s - loss: 14.4608 - mse: 14.4608 - mae: 1.4745 - 25s/epoch - 1ms/step\n",
            "Epoch 8/20\n",
            "20000/20000 - 25s - loss: 14.4869 - mse: 14.4869 - mae: 1.4738 - 25s/epoch - 1ms/step\n",
            "Epoch 9/20\n",
            "20000/20000 - 25s - loss: 14.4208 - mse: 14.4208 - mae: 1.4736 - 25s/epoch - 1ms/step\n",
            "Epoch 10/20\n",
            "20000/20000 - 25s - loss: 14.5488 - mse: 14.5488 - mae: 1.4896 - 25s/epoch - 1ms/step\n",
            "Epoch 11/20\n",
            "20000/20000 - 25s - loss: 14.4390 - mse: 14.4390 - mae: 1.4769 - 25s/epoch - 1ms/step\n",
            "Epoch 12/20\n",
            "20000/20000 - 25s - loss: 14.3832 - mse: 14.3832 - mae: 1.4724 - 25s/epoch - 1ms/step\n",
            "Epoch 13/20\n",
            "20000/20000 - 25s - loss: 14.4130 - mse: 14.4130 - mae: 1.4831 - 25s/epoch - 1ms/step\n",
            "Epoch 14/20\n",
            "20000/20000 - 25s - loss: 14.3474 - mse: 14.3474 - mae: 1.4785 - 25s/epoch - 1ms/step\n",
            "Epoch 15/20\n",
            "20000/20000 - 25s - loss: 14.4150 - mse: 14.4150 - mae: 1.4812 - 25s/epoch - 1ms/step\n",
            "Epoch 16/20\n",
            "20000/20000 - 25s - loss: 14.3391 - mse: 14.3391 - mae: 1.4755 - 25s/epoch - 1ms/step\n",
            "Epoch 17/20\n",
            "20000/20000 - 24s - loss: 14.3736 - mse: 14.3736 - mae: 1.4791 - 24s/epoch - 1ms/step\n",
            "Epoch 18/20\n",
            "20000/20000 - 25s - loss: 14.3436 - mse: 14.3436 - mae: 1.4822 - 25s/epoch - 1ms/step\n",
            "Epoch 19/20\n",
            "20000/20000 - 25s - loss: 14.2991 - mse: 14.2991 - mae: 1.4797 - 25s/epoch - 1ms/step\n",
            "Epoch 20/20\n",
            "20000/20000 - 25s - loss: 14.2781 - mse: 14.2781 - mae: 1.4752 - 25s/epoch - 1ms/step\n",
            "Score for fold 2: loss of 11.015496253967285\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/20\n",
            "20000/20000 - 25s - loss: 14.1821 - mse: 14.1821 - mae: 1.4685 - 25s/epoch - 1ms/step\n",
            "Epoch 2/20\n",
            "20000/20000 - 25s - loss: 14.1719 - mse: 14.1719 - mae: 1.4699 - 25s/epoch - 1ms/step\n",
            "Epoch 3/20\n",
            "20000/20000 - 25s - loss: 14.1838 - mse: 14.1838 - mae: 1.4685 - 25s/epoch - 1ms/step\n",
            "Epoch 4/20\n",
            "20000/20000 - 25s - loss: 14.1523 - mse: 14.1523 - mae: 1.4657 - 25s/epoch - 1ms/step\n",
            "Epoch 5/20\n",
            "20000/20000 - 25s - loss: 14.1143 - mse: 14.1143 - mae: 1.4666 - 25s/epoch - 1ms/step\n",
            "Epoch 6/20\n",
            "20000/20000 - 26s - loss: 14.0762 - mse: 14.0762 - mae: 1.4646 - 26s/epoch - 1ms/step\n",
            "Epoch 7/20\n",
            "20000/20000 - 26s - loss: 14.0453 - mse: 14.0453 - mae: 1.4639 - 26s/epoch - 1ms/step\n",
            "Epoch 8/20\n",
            "20000/20000 - 25s - loss: 14.0397 - mse: 14.0397 - mae: 1.4600 - 25s/epoch - 1ms/step\n",
            "Epoch 9/20\n",
            "20000/20000 - 25s - loss: 13.9944 - mse: 13.9944 - mae: 1.4609 - 25s/epoch - 1ms/step\n",
            "Epoch 10/20\n",
            "20000/20000 - 24s - loss: 13.9512 - mse: 13.9512 - mae: 1.4584 - 24s/epoch - 1ms/step\n",
            "Epoch 11/20\n",
            "20000/20000 - 24s - loss: 13.9744 - mse: 13.9744 - mae: 1.4598 - 24s/epoch - 1ms/step\n",
            "Epoch 12/20\n",
            "20000/20000 - 25s - loss: 13.9064 - mse: 13.9064 - mae: 1.4547 - 25s/epoch - 1ms/step\n",
            "Epoch 13/20\n",
            "20000/20000 - 24s - loss: 13.9160 - mse: 13.9160 - mae: 1.4572 - 24s/epoch - 1ms/step\n",
            "Epoch 14/20\n",
            "20000/20000 - 25s - loss: 13.8819 - mse: 13.8819 - mae: 1.4551 - 25s/epoch - 1ms/step\n",
            "Epoch 15/20\n",
            "20000/20000 - 25s - loss: 13.8736 - mse: 13.8736 - mae: 1.4549 - 25s/epoch - 1ms/step\n",
            "Epoch 16/20\n",
            "20000/20000 - 24s - loss: 13.8448 - mse: 13.8448 - mae: 1.4555 - 24s/epoch - 1ms/step\n",
            "Epoch 17/20\n",
            "20000/20000 - 24s - loss: 13.9372 - mse: 13.9372 - mae: 1.4679 - 24s/epoch - 1ms/step\n",
            "Epoch 18/20\n",
            "20000/20000 - 25s - loss: 13.8507 - mse: 13.8507 - mae: 1.4561 - 25s/epoch - 1ms/step\n",
            "Epoch 19/20\n",
            "20000/20000 - 24s - loss: 13.8282 - mse: 13.8282 - mae: 1.4562 - 24s/epoch - 1ms/step\n",
            "Epoch 20/20\n",
            "20000/20000 - 25s - loss: 13.7823 - mse: 13.7823 - mae: 1.4517 - 25s/epoch - 1ms/step\n",
            "Score for fold 3: loss of 10.295652389526367\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Mean mse for train is 14.289 (3.780) best is 13.844(3.72), mean mae for train is 1.469\n",
        "results = model_cnn_best.evaluate(testing, labelsForTest, batch_size=20)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KkfAxdyxqoei",
        "outputId": "b8a17b82-1744-4c70-f537-55a078d07c4c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5000/5000 [==============================] - 8s 2ms/step - loss: 13.6385 - mse: 13.6385 - mae: 2.0673\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_list=[]\n",
        "history_list=[]\n",
        "study_name = 'NN_study'\n",
        "study = optuna.create_study(study_name=study_name, load_if_exists=True)\n",
        "#15.1\n",
        "study.optimize(objective, n_trials=50, )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1qnvHMyp9TQ8",
        "outputId": "b76a4770-a08c-466b-be1f-b8eeaeac3c46"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 24s - loss: 21.3438 - mse: 21.3438 - mae: 1.6281 - 24s/epoch - 1ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 24s - loss: 17.2346 - mse: 17.2346 - mae: 1.6124 - 24s/epoch - 1ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 22s - loss: 17.0615 - mse: 17.0615 - mae: 1.6092 - 22s/epoch - 1ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 22s - loss: 17.1603 - mse: 17.1603 - mae: 1.6069 - 22s/epoch - 1ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 21s - loss: 17.0885 - mse: 17.0885 - mae: 1.6054 - 21s/epoch - 1ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 22s - loss: 17.0414 - mse: 17.0414 - mae: 1.6084 - 22s/epoch - 1ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 21s - loss: 17.0386 - mse: 17.0386 - mae: 1.6001 - 21s/epoch - 1ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 21s - loss: 17.0484 - mse: 17.0484 - mae: 1.6071 - 21s/epoch - 1ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 21s - loss: 17.1014 - mse: 17.1014 - mae: 1.6059 - 21s/epoch - 1ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 21s - loss: 17.0409 - mse: 17.0409 - mae: 1.6048 - 21s/epoch - 1ms/step\n",
            "Score for fold 1: loss of 25.447511672973633\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 22s - loss: 17.0226 - mse: 17.0226 - mae: 1.6031 - 22s/epoch - 1ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 22s - loss: 17.0262 - mse: 17.0262 - mae: 1.6018 - 22s/epoch - 1ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 22s - loss: 16.9892 - mse: 16.9892 - mae: 1.6042 - 22s/epoch - 1ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 20s - loss: 17.0028 - mse: 17.0028 - mae: 1.6015 - 20s/epoch - 1ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 21s - loss: 17.0326 - mse: 17.0326 - mae: 1.6006 - 21s/epoch - 1ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 21s - loss: 17.0007 - mse: 17.0007 - mae: 1.6057 - 21s/epoch - 1ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 20s - loss: 17.0102 - mse: 17.0102 - mae: 1.6018 - 20s/epoch - 1ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 21s - loss: 17.0212 - mse: 17.0212 - mae: 1.5996 - 21s/epoch - 1ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 20s - loss: 17.0012 - mse: 17.0012 - mae: 1.6065 - 20s/epoch - 1ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 20s - loss: 17.0331 - mse: 17.0331 - mae: 1.6017 - 20s/epoch - 1ms/step\n",
            "Score for fold 2: loss of 12.334875106811523\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 21s - loss: 16.9998 - mse: 16.9998 - mae: 1.6047 - 21s/epoch - 1ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 21s - loss: 16.9950 - mse: 16.9950 - mae: 1.6029 - 21s/epoch - 1ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 21s - loss: 17.0033 - mse: 17.0033 - mae: 1.6020 - 21s/epoch - 1ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 20s - loss: 17.0053 - mse: 17.0053 - mae: 1.6006 - 20s/epoch - 1ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 21s - loss: 16.9991 - mse: 16.9991 - mae: 1.6015 - 21s/epoch - 1ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 21s - loss: 16.9886 - mse: 16.9886 - mae: 1.6014 - 21s/epoch - 1ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 21s - loss: 17.0071 - mse: 17.0071 - mae: 1.6021 - 21s/epoch - 1ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 21s - loss: 16.9865 - mse: 16.9865 - mae: 1.6007 - 21s/epoch - 1ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 21s - loss: 16.9804 - mse: 16.9804 - mae: 1.6031 - 21s/epoch - 1ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 21s - loss: 16.9979 - mse: 16.9979 - mae: 1.6009 - 21s/epoch - 1ms/step\n",
            "Score for fold 3: loss of 12.36998462677002\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-09-30 23:28:03,962]\u001b[0m Finished trial#0 resulted in value: 16.997882843017578. Current best value is 16.997882843017578 with parameters: {'activation': 'linear', 'optimizer': 'adagrad', 'num_hidden_layer': 5, 'num_hidden_unit': 54, 'lear  ning_rate': 0.07760938712352267}.\u001b[0m\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 26s - loss: 17.7250 - mse: 17.7250 - mae: 1.5966 - 26s/epoch - 1ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 25s - loss: 17.4547 - mse: 17.4547 - mae: 1.5840 - 25s/epoch - 1ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 26s - loss: 17.4883 - mse: 17.4883 - mae: 1.5842 - 26s/epoch - 1ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 25s - loss: 17.4228 - mse: 17.4228 - mae: 1.5821 - 25s/epoch - 1ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 25s - loss: 17.5935 - mse: 17.5935 - mae: 1.5837 - 25s/epoch - 1ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 24s - loss: 17.4588 - mse: 17.4588 - mae: 1.5835 - 24s/epoch - 1ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 25s - loss: 17.6610 - mse: 17.6610 - mae: 1.5848 - 25s/epoch - 1ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 25s - loss: 17.4965 - mse: 17.4964 - mae: 1.5816 - 25s/epoch - 1ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 25s - loss: 17.5655 - mse: 17.5655 - mae: 1.5826 - 25s/epoch - 1ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 25s - loss: 17.4245 - mse: 17.4245 - mae: 1.5818 - 25s/epoch - 1ms/step\n",
            "Score for fold 1: loss of 23.46293830871582\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 26s - loss: 17.4955 - mse: 17.4955 - mae: 1.5819 - 26s/epoch - 1ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 26s - loss: 17.5634 - mse: 17.5634 - mae: 1.5824 - 26s/epoch - 1ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 25s - loss: 17.4035 - mse: 17.4035 - mae: 1.5814 - 25s/epoch - 1ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 25s - loss: 17.6003 - mse: 17.6003 - mae: 1.5813 - 25s/epoch - 1ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 25s - loss: 17.6062 - mse: 17.6062 - mae: 1.5809 - 25s/epoch - 1ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 26s - loss: 17.5454 - mse: 17.5454 - mae: 1.5815 - 26s/epoch - 1ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 26s - loss: 17.4774 - mse: 17.4774 - mae: 1.5817 - 26s/epoch - 1ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 24s - loss: 17.5288 - mse: 17.5288 - mae: 1.5801 - 24s/epoch - 1ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 24s - loss: 17.5230 - mse: 17.5230 - mae: 1.5811 - 24s/epoch - 1ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 24s - loss: 17.6418 - mse: 17.6418 - mae: 1.5799 - 24s/epoch - 1ms/step\n",
            "Score for fold 2: loss of 13.999115943908691\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 24s - loss: 17.9411 - mse: 17.9411 - mae: 1.5810 - 24s/epoch - 1ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 25s - loss: 17.5368 - mse: 17.5368 - mae: 1.5809 - 25s/epoch - 1ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 26s - loss: 17.8134 - mse: 17.8134 - mae: 1.5814 - 26s/epoch - 1ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 26s - loss: 17.4507 - mse: 17.4507 - mae: 1.5823 - 26s/epoch - 1ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 26s - loss: 17.6241 - mse: 17.6241 - mae: 1.5831 - 26s/epoch - 1ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 25s - loss: 17.6474 - mse: 17.6474 - mae: 1.5819 - 25s/epoch - 1ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 24s - loss: 17.6635 - mse: 17.6635 - mae: 1.5812 - 24s/epoch - 1ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 25s - loss: 17.5518 - mse: 17.5518 - mae: 1.5811 - 25s/epoch - 1ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 26s - loss: 17.3435 - mse: 17.3435 - mae: 1.5815 - 26s/epoch - 1ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 26s - loss: 17.5271 - mse: 17.5271 - mae: 1.5824 - 26s/epoch - 1ms/step\n",
            "Score for fold 3: loss of 12.689850807189941\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-09-30 23:40:46,662]\u001b[0m Finished trial#1 resulted in value: 17.527057647705078. Current best value is 16.997882843017578 with parameters: {'activation': 'linear', 'optimizer': 'adagrad', 'num_hidden_layer': 5, 'num_hidden_unit': 54, 'lear  ning_rate': 0.07760938712352267}.\u001b[0m\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 20s - loss: 17.3850 - mse: 17.3850 - mae: 1.6221 - 20s/epoch - 990us/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 19s - loss: 17.1986 - mse: 17.1986 - mae: 1.6197 - 19s/epoch - 968us/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 20s - loss: 17.2591 - mse: 17.2591 - mae: 1.6204 - 20s/epoch - 991us/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 19s - loss: 17.1495 - mse: 17.1495 - mae: 1.6179 - 19s/epoch - 950us/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 20s - loss: 17.2746 - mse: 17.2746 - mae: 1.6223 - 20s/epoch - 989us/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 20s - loss: 17.2683 - mse: 17.2683 - mae: 1.6185 - 20s/epoch - 993us/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 20s - loss: 17.1730 - mse: 17.1730 - mae: 1.6179 - 20s/epoch - 980us/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 19s - loss: 17.2308 - mse: 17.2308 - mae: 1.6219 - 19s/epoch - 962us/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 20s - loss: 17.2804 - mse: 17.2804 - mae: 1.6189 - 20s/epoch - 995us/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 20s - loss: 17.1640 - mse: 17.1640 - mae: 1.6199 - 20s/epoch - 1ms/step\n",
            "Score for fold 1: loss of 29.324188232421875\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 20s - loss: 17.2699 - mse: 17.2699 - mae: 1.6190 - 20s/epoch - 1000us/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 20s - loss: 17.2476 - mse: 17.2476 - mae: 1.6188 - 20s/epoch - 1ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 19s - loss: 17.2153 - mse: 17.2153 - mae: 1.6200 - 19s/epoch - 959us/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 19s - loss: 17.2068 - mse: 17.2068 - mae: 1.6207 - 19s/epoch - 930us/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 18s - loss: 17.2333 - mse: 17.2333 - mae: 1.6193 - 18s/epoch - 912us/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 19s - loss: 17.2001 - mse: 17.2001 - mae: 1.6179 - 19s/epoch - 927us/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 20s - loss: 17.2436 - mse: 17.2436 - mae: 1.6182 - 20s/epoch - 1ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 20s - loss: 17.1476 - mse: 17.1476 - mae: 1.6192 - 20s/epoch - 1ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 20s - loss: 17.1243 - mse: 17.1243 - mae: 1.6173 - 20s/epoch - 1ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 19s - loss: 17.2242 - mse: 17.2242 - mae: 1.6188 - 19s/epoch - 947us/step\n",
            "Score for fold 2: loss of 12.582648277282715\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 20s - loss: 17.1781 - mse: 17.1781 - mae: 1.6176 - 20s/epoch - 980us/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 20s - loss: 17.1886 - mse: 17.1886 - mae: 1.6184 - 20s/epoch - 995us/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 20s - loss: 17.2284 - mse: 17.2284 - mae: 1.6195 - 20s/epoch - 999us/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 19s - loss: 17.1852 - mse: 17.1852 - mae: 1.6194 - 19s/epoch - 927us/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 18s - loss: 17.2035 - mse: 17.2035 - mae: 1.6209 - 18s/epoch - 919us/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 21s - loss: 17.2009 - mse: 17.2009 - mae: 1.6189 - 21s/epoch - 1ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 22s - loss: 17.1761 - mse: 17.1761 - mae: 1.6208 - 22s/epoch - 1ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 23s - loss: 17.2224 - mse: 17.2224 - mae: 1.6196 - 23s/epoch - 1ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 21s - loss: 17.2362 - mse: 17.2362 - mae: 1.6213 - 21s/epoch - 1ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 22s - loss: 17.1367 - mse: 17.1367 - mae: 1.6202 - 22s/epoch - 1ms/step\n",
            "Score for fold 3: loss of 12.799975395202637\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-09-30 23:50:51,366]\u001b[0m Finished trial#2 resulted in value: 17.13673210144043. Current best value is 16.997882843017578 with parameters: {'activation': 'linear', 'optimizer': 'adagrad', 'num_hidden_layer': 5, 'num_hidden_unit': 54, 'lear  ning_rate': 0.07760938712352267}.\u001b[0m\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 23s - loss: 20.9283 - mse: 20.9283 - mae: 1.6683 - 23s/epoch - 1ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 22s - loss: 19.1674 - mse: 19.1674 - mae: 1.6268 - 22s/epoch - 1ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 22s - loss: 18.7132 - mse: 18.7132 - mae: 1.6358 - 22s/epoch - 1ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 22s - loss: 18.4840 - mse: 18.4840 - mae: 1.6398 - 22s/epoch - 1ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 22s - loss: 18.3383 - mse: 18.3383 - mae: 1.6405 - 22s/epoch - 1ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 22s - loss: 18.2314 - mse: 18.2314 - mae: 1.6383 - 22s/epoch - 1ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 22s - loss: 18.1472 - mse: 18.1472 - mae: 1.6365 - 22s/epoch - 1ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 22s - loss: 18.0781 - mse: 18.0781 - mae: 1.6338 - 22s/epoch - 1ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 21s - loss: 18.0191 - mse: 18.0191 - mae: 1.6312 - 21s/epoch - 1ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 22s - loss: 17.9672 - mse: 17.9672 - mae: 1.6293 - 22s/epoch - 1ms/step\n",
            "Score for fold 1: loss of 22.975095748901367\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 22s - loss: 17.9211 - mse: 17.9211 - mae: 1.6268 - 22s/epoch - 1ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 23s - loss: 17.8795 - mse: 17.8795 - mae: 1.6252 - 23s/epoch - 1ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 23s - loss: 17.8417 - mse: 17.8417 - mae: 1.6228 - 23s/epoch - 1ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 23s - loss: 17.8069 - mse: 17.8069 - mae: 1.6218 - 23s/epoch - 1ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 22s - loss: 17.7748 - mse: 17.7748 - mae: 1.6198 - 22s/epoch - 1ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 22s - loss: 17.7447 - mse: 17.7447 - mae: 1.6187 - 22s/epoch - 1ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 23s - loss: 17.7165 - mse: 17.7165 - mae: 1.6163 - 23s/epoch - 1ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 23s - loss: 17.6901 - mse: 17.6901 - mae: 1.6154 - 23s/epoch - 1ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 22s - loss: 17.6653 - mse: 17.6653 - mae: 1.6137 - 22s/epoch - 1ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 24s - loss: 17.6418 - mse: 17.6418 - mae: 1.6125 - 24s/epoch - 1ms/step\n",
            "Score for fold 2: loss of 13.219635009765625\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 24s - loss: 17.6194 - mse: 17.6194 - mae: 1.6116 - 24s/epoch - 1ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 23s - loss: 17.5980 - mse: 17.5980 - mae: 1.6103 - 23s/epoch - 1ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 23s - loss: 17.5777 - mse: 17.5777 - mae: 1.6098 - 23s/epoch - 1ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 22s - loss: 17.5581 - mse: 17.5581 - mae: 1.6085 - 22s/epoch - 1ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 22s - loss: 17.5395 - mse: 17.5395 - mae: 1.6077 - 22s/epoch - 1ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 22s - loss: 17.5216 - mse: 17.5216 - mae: 1.6059 - 22s/epoch - 1ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 21s - loss: 17.5043 - mse: 17.5043 - mae: 1.6057 - 21s/epoch - 1ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 22s - loss: 17.4876 - mse: 17.4876 - mae: 1.6049 - 22s/epoch - 1ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 23s - loss: 17.4716 - mse: 17.4716 - mae: 1.6039 - 23s/epoch - 1ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 22s - loss: 17.4561 - mse: 17.4561 - mae: 1.6030 - 22s/epoch - 1ms/step\n",
            "Score for fold 3: loss of 12.899341583251953\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-10-01 00:02:10,042]\u001b[0m Finished trial#3 resulted in value: 17.456104278564453. Current best value is 16.997882843017578 with parameters: {'activation': 'linear', 'optimizer': 'adagrad', 'num_hidden_layer': 5, 'num_hidden_unit': 54, 'lear  ning_rate': 0.07760938712352267}.\u001b[0m\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 34s - loss: 17.4899 - mse: 17.4899 - mae: 1.6013 - 34s/epoch - 2ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 34s - loss: 17.0598 - mse: 17.0598 - mae: 1.6011 - 34s/epoch - 2ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 35s - loss: 17.0359 - mse: 17.0359 - mae: 1.5998 - 35s/epoch - 2ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 32s - loss: 17.0242 - mse: 17.0242 - mae: 1.5984 - 32s/epoch - 2ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 31s - loss: 17.0145 - mse: 17.0145 - mae: 1.5994 - 31s/epoch - 2ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 30s - loss: 17.0076 - mse: 17.0076 - mae: 1.6030 - 30s/epoch - 1ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 30s - loss: 17.0021 - mse: 17.0021 - mae: 1.6008 - 30s/epoch - 2ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 33s - loss: 16.9953 - mse: 16.9953 - mae: 1.6004 - 33s/epoch - 2ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 34s - loss: 16.9966 - mse: 16.9966 - mae: 1.6010 - 34s/epoch - 2ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 36s - loss: 16.9923 - mse: 16.9923 - mae: 1.6022 - 36s/epoch - 2ms/step\n",
            "Score for fold 1: loss of 24.8087158203125\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 34s - loss: 16.9868 - mse: 16.9868 - mae: 1.6006 - 34s/epoch - 2ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 32s - loss: 16.9871 - mse: 16.9871 - mae: 1.6019 - 32s/epoch - 2ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 34s - loss: 16.9876 - mse: 16.9876 - mae: 1.6011 - 34s/epoch - 2ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 33s - loss: 16.9845 - mse: 16.9845 - mae: 1.6031 - 33s/epoch - 2ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 33s - loss: 16.9839 - mse: 16.9839 - mae: 1.5996 - 33s/epoch - 2ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 48s - loss: 16.9821 - mse: 16.9821 - mae: 1.6007 - 48s/epoch - 2ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 52s - loss: 16.9807 - mse: 16.9807 - mae: 1.6015 - 52s/epoch - 3ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 54s - loss: 16.9794 - mse: 16.9794 - mae: 1.6025 - 54s/epoch - 3ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 53s - loss: 16.9779 - mse: 16.9779 - mae: 1.6011 - 53s/epoch - 3ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 51s - loss: 16.9782 - mse: 16.9782 - mae: 1.6016 - 51s/epoch - 3ms/step\n",
            "Score for fold 2: loss of 12.344927787780762\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 62s - loss: 16.9768 - mse: 16.9768 - mae: 1.6019 - 62s/epoch - 3ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 39s - loss: 16.9748 - mse: 16.9748 - mae: 1.5998 - 39s/epoch - 2ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 36s - loss: 16.9749 - mse: 16.9749 - mae: 1.6014 - 36s/epoch - 2ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 40s - loss: 16.9742 - mse: 16.9742 - mae: 1.6035 - 40s/epoch - 2ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 36s - loss: 16.9725 - mse: 16.9725 - mae: 1.6019 - 36s/epoch - 2ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 34s - loss: 16.9725 - mse: 16.9725 - mae: 1.5994 - 34s/epoch - 2ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 33s - loss: 16.9714 - mse: 16.9714 - mae: 1.6017 - 33s/epoch - 2ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 35s - loss: 16.9712 - mse: 16.9712 - mae: 1.5996 - 35s/epoch - 2ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 36s - loss: 16.9704 - mse: 16.9704 - mae: 1.6008 - 36s/epoch - 2ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 38s - loss: 16.9693 - mse: 16.9693 - mae: 1.6020 - 38s/epoch - 2ms/step\n",
            "Score for fold 3: loss of 12.340293884277344\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-10-01 00:21:26,506]\u001b[0m Finished trial#4 resulted in value: 16.969337463378906. Current best value is 16.969337463378906 with parameters: {'activation': 'linear', 'optimizer': 'adagrad', 'num_hidden_layer': 5, 'num_hidden_unit': 92, 'lear  ning_rate': 0.00045367458341197617}.\u001b[0m\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 36s - loss: 16.6941 - mse: 16.6941 - mae: 1.5671 - 36s/epoch - 2ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 32s - loss: 16.0785 - mse: 16.0785 - mae: 1.5472 - 32s/epoch - 2ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 33s - loss: 15.7893 - mse: 15.7893 - mae: 1.5363 - 33s/epoch - 2ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 32s - loss: 15.6715 - mse: 15.6715 - mae: 1.5281 - 32s/epoch - 2ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 31s - loss: 15.6024 - mse: 15.6024 - mae: 1.5227 - 31s/epoch - 2ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 31s - loss: 15.5553 - mse: 15.5553 - mae: 1.5222 - 31s/epoch - 2ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 31s - loss: 15.6459 - mse: 15.6459 - mae: 1.5203 - 31s/epoch - 2ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 33s - loss: 15.3193 - mse: 15.3193 - mae: 1.5119 - 33s/epoch - 2ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 30s - loss: 15.4188 - mse: 15.4188 - mae: 1.5112 - 30s/epoch - 1ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 29s - loss: 15.3512 - mse: 15.3512 - mae: 1.5106 - 29s/epoch - 1ms/step\n",
            "Score for fold 1: loss of 26.47606086730957\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 29s - loss: 15.2042 - mse: 15.2042 - mae: 1.5072 - 29s/epoch - 1ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 27s - loss: 15.3403 - mse: 15.3403 - mae: 1.5148 - 27s/epoch - 1ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 28s - loss: 15.2513 - mse: 15.2513 - mae: 1.5087 - 28s/epoch - 1ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 27s - loss: 15.1960 - mse: 15.1960 - mae: 1.5096 - 27s/epoch - 1ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 27s - loss: 15.0788 - mse: 15.0788 - mae: 1.5044 - 27s/epoch - 1ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 27s - loss: 15.2455 - mse: 15.2455 - mae: 1.5077 - 27s/epoch - 1ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 25s - loss: 15.1023 - mse: 15.1023 - mae: 1.5103 - 25s/epoch - 1ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 27s - loss: 15.1519 - mse: 15.1519 - mae: 1.5100 - 27s/epoch - 1ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 28s - loss: 15.0692 - mse: 15.0692 - mae: 1.5057 - 28s/epoch - 1ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 27s - loss: 15.0163 - mse: 15.0163 - mae: 1.5100 - 27s/epoch - 1ms/step\n",
            "Score for fold 2: loss of 11.132284164428711\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 28s - loss: 15.0159 - mse: 15.0159 - mae: 1.5079 - 28s/epoch - 1ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 28s - loss: 15.0917 - mse: 15.0917 - mae: 1.5064 - 28s/epoch - 1ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 27s - loss: 15.1200 - mse: 15.1200 - mae: 1.5084 - 27s/epoch - 1ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 26s - loss: 15.0315 - mse: 15.0315 - mae: 1.5033 - 26s/epoch - 1ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 27s - loss: 15.0363 - mse: 15.0363 - mae: 1.5095 - 27s/epoch - 1ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 27s - loss: 15.0688 - mse: 15.0688 - mae: 1.5069 - 27s/epoch - 1ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 27s - loss: 14.9054 - mse: 14.9054 - mae: 1.5030 - 27s/epoch - 1ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 28s - loss: 15.1251 - mse: 15.1251 - mae: 1.5088 - 28s/epoch - 1ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 28s - loss: 14.9892 - mse: 14.9892 - mae: 1.5051 - 28s/epoch - 1ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 26s - loss: 14.9707 - mse: 14.9707 - mae: 1.5046 - 26s/epoch - 1ms/step\n",
            "Score for fold 3: loss of 10.844123840332031\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-10-01 00:35:57,480]\u001b[0m Finished trial#5 resulted in value: 14.97073745727539. Current best value is 14.97073745727539 with parameters: {'activation': 'relu', 'optimizer': 'adam', 'num_hidden_layer': 2, 'num_hidden_unit': 73, 'lear  ning_rate': 0.005419337348062135}.\u001b[0m\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 15s - loss: 23.1562 - mse: 23.1562 - mae: 2.0364 - 15s/epoch - 746us/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 14s - loss: 18.2774 - mse: 18.2774 - mae: 1.5376 - 14s/epoch - 719us/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 14s - loss: 17.4114 - mse: 17.4114 - mae: 1.5160 - 14s/epoch - 719us/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 14s - loss: 17.1945 - mse: 17.1945 - mae: 1.5524 - 14s/epoch - 724us/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 14s - loss: 17.1224 - mse: 17.1224 - mae: 1.5758 - 14s/epoch - 705us/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 14s - loss: 17.0893 - mse: 17.0893 - mae: 1.5865 - 14s/epoch - 717us/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 15s - loss: 17.0675 - mse: 17.0675 - mae: 1.5917 - 15s/epoch - 743us/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 16s - loss: 17.0546 - mse: 17.0546 - mae: 1.5949 - 16s/epoch - 809us/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 15s - loss: 17.0430 - mse: 17.0430 - mae: 1.5964 - 15s/epoch - 768us/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 16s - loss: 17.0342 - mse: 17.0342 - mae: 1.5969 - 16s/epoch - 803us/step\n",
            "Score for fold 1: loss of 27.986923217773438\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 16s - loss: 17.0262 - mse: 17.0262 - mae: 1.5983 - 16s/epoch - 782us/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 15s - loss: 17.0214 - mse: 17.0214 - mae: 1.5969 - 15s/epoch - 745us/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 15s - loss: 17.0159 - mse: 17.0159 - mae: 1.5987 - 15s/epoch - 747us/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 15s - loss: 17.0120 - mse: 17.0120 - mae: 1.5985 - 15s/epoch - 737us/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 15s - loss: 17.0074 - mse: 17.0074 - mae: 1.5995 - 15s/epoch - 756us/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 15s - loss: 17.0059 - mse: 17.0059 - mae: 1.5990 - 15s/epoch - 753us/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 16s - loss: 17.0024 - mse: 17.0024 - mae: 1.6009 - 16s/epoch - 789us/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 16s - loss: 17.0013 - mse: 17.0013 - mae: 1.5999 - 16s/epoch - 795us/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 15s - loss: 16.9992 - mse: 16.9992 - mae: 1.6005 - 15s/epoch - 761us/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 16s - loss: 16.9961 - mse: 16.9961 - mae: 1.6005 - 16s/epoch - 802us/step\n",
            "Score for fold 2: loss of 12.373037338256836\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 16s - loss: 16.9963 - mse: 16.9963 - mae: 1.6005 - 16s/epoch - 777us/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 16s - loss: 16.9898 - mse: 16.9898 - mae: 1.6022 - 16s/epoch - 793us/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 16s - loss: 16.9944 - mse: 16.9944 - mae: 1.6006 - 16s/epoch - 809us/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 16s - loss: 16.9922 - mse: 16.9922 - mae: 1.6012 - 16s/epoch - 799us/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 18s - loss: 16.9903 - mse: 16.9903 - mae: 1.6017 - 18s/epoch - 885us/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 18s - loss: 16.9857 - mse: 16.9857 - mae: 1.6029 - 18s/epoch - 886us/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 19s - loss: 16.9902 - mse: 16.9902 - mae: 1.6008 - 19s/epoch - 926us/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 18s - loss: 16.9873 - mse: 16.9873 - mae: 1.6013 - 18s/epoch - 903us/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 19s - loss: 16.9853 - mse: 16.9853 - mae: 1.6012 - 19s/epoch - 938us/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 19s - loss: 16.9861 - mse: 16.9861 - mae: 1.6021 - 19s/epoch - 964us/step\n",
            "Score for fold 3: loss of 12.354808807373047\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-10-01 00:44:01,916]\u001b[0m Finished trial#6 resulted in value: 16.9860782623291. Current best value is 14.97073745727539 with parameters: {'activation': 'relu', 'optimizer': 'adam', 'num_hidden_layer': 2, 'num_hidden_unit': 73, 'lear  ning_rate': 0.005419337348062135}.\u001b[0m\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 28s - loss: 27.7739 - mse: 27.7739 - mae: 2.5687 - 28s/epoch - 1ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 28s - loss: 26.7573 - mse: 26.7573 - mae: 2.4885 - 28s/epoch - 1ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 27s - loss: 26.1821 - mse: 26.1821 - mae: 2.4388 - 27s/epoch - 1ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 29s - loss: 25.7475 - mse: 25.7475 - mae: 2.3995 - 29s/epoch - 1ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 29s - loss: 25.3925 - mse: 25.3925 - mae: 2.3662 - 29s/epoch - 1ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 28s - loss: 25.0893 - mse: 25.0893 - mae: 2.3365 - 28s/epoch - 1ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 27s - loss: 24.8227 - mse: 24.8227 - mae: 2.3098 - 27s/epoch - 1ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 28s - loss: 24.5836 - mse: 24.5836 - mae: 2.2852 - 28s/epoch - 1ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 28s - loss: 24.3660 - mse: 24.3660 - mae: 2.2622 - 28s/epoch - 1ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 28s - loss: 24.1660 - mse: 24.1660 - mae: 2.2408 - 28s/epoch - 1ms/step\n",
            "Score for fold 1: loss of 25.068086624145508\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 29s - loss: 23.9808 - mse: 23.9808 - mae: 2.2206 - 29s/epoch - 1ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 29s - loss: 23.8080 - mse: 23.8080 - mae: 2.2016 - 29s/epoch - 1ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 30s - loss: 23.6460 - mse: 23.6460 - mae: 2.1833 - 30s/epoch - 1ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 29s - loss: 23.4929 - mse: 23.4929 - mae: 2.1658 - 29s/epoch - 1ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 28s - loss: 23.3480 - mse: 23.3480 - mae: 2.1491 - 28s/epoch - 1ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 32s - loss: 23.2104 - mse: 23.2104 - mae: 2.1331 - 32s/epoch - 2ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 31s - loss: 23.0791 - mse: 23.0791 - mae: 2.1177 - 31s/epoch - 2ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 33s - loss: 22.9539 - mse: 22.9539 - mae: 2.1028 - 33s/epoch - 2ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 33s - loss: 22.8340 - mse: 22.8340 - mae: 2.0884 - 33s/epoch - 2ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 31s - loss: 22.7188 - mse: 22.7188 - mae: 2.0744 - 31s/epoch - 2ms/step\n",
            "Score for fold 2: loss of 19.449506759643555\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 31s - loss: 22.6082 - mse: 22.6082 - mae: 2.0610 - 31s/epoch - 2ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 32s - loss: 22.5016 - mse: 22.5016 - mae: 2.0480 - 32s/epoch - 2ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 30s - loss: 22.3987 - mse: 22.3987 - mae: 2.0352 - 30s/epoch - 2ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 29s - loss: 22.2994 - mse: 22.2994 - mae: 2.0230 - 29s/epoch - 1ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 29s - loss: 22.2034 - mse: 22.2034 - mae: 2.0109 - 29s/epoch - 1ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 29s - loss: 22.1104 - mse: 22.1104 - mae: 1.9993 - 29s/epoch - 1ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 30s - loss: 22.0203 - mse: 22.0203 - mae: 1.9880 - 30s/epoch - 1ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 32s - loss: 21.9329 - mse: 21.9329 - mae: 1.9769 - 32s/epoch - 2ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 32s - loss: 21.8481 - mse: 21.8481 - mae: 1.9661 - 32s/epoch - 2ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 32s - loss: 21.7656 - mse: 21.7656 - mae: 1.9556 - 32s/epoch - 2ms/step\n",
            "Score for fold 3: loss of 17.995845794677734\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-10-01 00:59:03,298]\u001b[0m Finished trial#7 resulted in value: 21.765609741210938. Current best value is 14.97073745727539 with parameters: {'activation': 'relu', 'optimizer': 'adam', 'num_hidden_layer': 2, 'num_hidden_unit': 73, 'lear  ning_rate': 0.005419337348062135}.\u001b[0m\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 26s - loss: 17.3520 - mse: 17.3520 - mae: 1.6903 - 26s/epoch - 1ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 24s - loss: 16.9445 - mse: 16.9445 - mae: 1.6825 - 24s/epoch - 1ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 25s - loss: 16.8244 - mse: 16.8244 - mae: 1.6874 - 25s/epoch - 1ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 24s - loss: 16.7698 - mse: 16.7698 - mae: 1.6875 - 24s/epoch - 1ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 24s - loss: 16.7949 - mse: 16.7949 - mae: 1.6952 - 24s/epoch - 1ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 23s - loss: 16.7997 - mse: 16.7997 - mae: 1.6982 - 23s/epoch - 1ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 22s - loss: 16.7613 - mse: 16.7613 - mae: 1.7011 - 22s/epoch - 1ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 23s - loss: 16.8761 - mse: 16.8761 - mae: 1.7127 - 23s/epoch - 1ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 23s - loss: 16.8445 - mse: 16.8445 - mae: 1.7125 - 23s/epoch - 1ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 23s - loss: 16.8595 - mse: 16.8595 - mae: 1.7167 - 23s/epoch - 1ms/step\n",
            "Score for fold 1: loss of 24.525070190429688\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 22s - loss: 16.8522 - mse: 16.8522 - mae: 1.7163 - 22s/epoch - 1ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 23s - loss: 16.8766 - mse: 16.8766 - mae: 1.7193 - 23s/epoch - 1ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 24s - loss: 16.8459 - mse: 16.8459 - mae: 1.7252 - 24s/epoch - 1ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 23s - loss: 16.9515 - mse: 16.9515 - mae: 1.7253 - 23s/epoch - 1ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 22s - loss: 16.8997 - mse: 16.8997 - mae: 1.7264 - 22s/epoch - 1ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 23s - loss: 16.8385 - mse: 16.8385 - mae: 1.7282 - 23s/epoch - 1ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 21s - loss: 16.8357 - mse: 16.8357 - mae: 1.7246 - 21s/epoch - 1ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 21s - loss: 16.8942 - mse: 16.8942 - mae: 1.7241 - 21s/epoch - 1ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 23s - loss: 16.9554 - mse: 16.9554 - mae: 1.7323 - 23s/epoch - 1ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 23s - loss: 16.9119 - mse: 16.9119 - mae: 1.7300 - 23s/epoch - 1ms/step\n",
            "Score for fold 2: loss of 12.068408966064453\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 25s - loss: 16.9133 - mse: 16.9133 - mae: 1.7273 - 25s/epoch - 1ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 25s - loss: 16.9410 - mse: 16.9410 - mae: 1.7268 - 25s/epoch - 1ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 23s - loss: 16.9894 - mse: 16.9894 - mae: 1.7330 - 23s/epoch - 1ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 23s - loss: 17.0004 - mse: 17.0004 - mae: 1.7331 - 23s/epoch - 1ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 22s - loss: 17.0470 - mse: 17.0470 - mae: 1.7387 - 22s/epoch - 1ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 22s - loss: 17.0143 - mse: 17.0143 - mae: 1.7410 - 22s/epoch - 1ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 24s - loss: 16.9679 - mse: 16.9679 - mae: 1.7326 - 24s/epoch - 1ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 23s - loss: 16.9550 - mse: 16.9550 - mae: 1.7371 - 23s/epoch - 1ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 22s - loss: 16.9380 - mse: 16.9380 - mae: 1.7407 - 22s/epoch - 1ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 23s - loss: 16.9688 - mse: 16.9688 - mae: 1.7473 - 23s/epoch - 1ms/step\n",
            "Score for fold 3: loss of 14.018131256103516\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-10-01 01:10:47,961]\u001b[0m Finished trial#8 resulted in value: 16.968841552734375. Current best value is 14.97073745727539 with parameters: {'activation': 'relu', 'optimizer': 'adam', 'num_hidden_layer': 2, 'num_hidden_unit': 73, 'lear  ning_rate': 0.005419337348062135}.\u001b[0m\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 22s - loss: 18.8425 - mse: 18.8425 - mae: 1.5939 - 22s/epoch - 1ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 22s - loss: 17.8107 - mse: 17.8107 - mae: 1.5893 - 22s/epoch - 1ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 23s - loss: 17.6177 - mse: 17.6177 - mae: 1.5787 - 23s/epoch - 1ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 23s - loss: 17.4932 - mse: 17.4932 - mae: 1.5721 - 23s/epoch - 1ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 22s - loss: 17.4000 - mse: 17.4000 - mae: 1.5692 - 22s/epoch - 1ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 22s - loss: 17.3268 - mse: 17.3268 - mae: 1.5642 - 22s/epoch - 1ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 22s - loss: 17.2670 - mse: 17.2670 - mae: 1.5620 - 22s/epoch - 1ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 24s - loss: 17.2159 - mse: 17.2159 - mae: 1.5608 - 24s/epoch - 1ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 24s - loss: 17.1722 - mse: 17.1722 - mae: 1.5573 - 24s/epoch - 1ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 24s - loss: 17.1330 - mse: 17.1330 - mae: 1.5548 - 24s/epoch - 1ms/step\n",
            "Score for fold 1: loss of 24.146902084350586\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 23s - loss: 17.0975 - mse: 17.0975 - mae: 1.5527 - 23s/epoch - 1ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 23s - loss: 17.0652 - mse: 17.0652 - mae: 1.5517 - 23s/epoch - 1ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 23s - loss: 17.0354 - mse: 17.0354 - mae: 1.5517 - 23s/epoch - 1ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 23s - loss: 17.0074 - mse: 17.0074 - mae: 1.5485 - 23s/epoch - 1ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 23s - loss: 16.9814 - mse: 16.9814 - mae: 1.5477 - 23s/epoch - 1ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 23s - loss: 16.9566 - mse: 16.9566 - mae: 1.5456 - 23s/epoch - 1ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 24s - loss: 16.9332 - mse: 16.9332 - mae: 1.5440 - 24s/epoch - 1ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 23s - loss: 16.9109 - mse: 16.9109 - mae: 1.5446 - 23s/epoch - 1ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 23s - loss: 16.8894 - mse: 16.8894 - mae: 1.5431 - 23s/epoch - 1ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 23s - loss: 16.8688 - mse: 16.8688 - mae: 1.5411 - 23s/epoch - 1ms/step\n",
            "Score for fold 2: loss of 12.790619850158691\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 23s - loss: 16.8490 - mse: 16.8490 - mae: 1.5395 - 23s/epoch - 1ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 23s - loss: 16.8299 - mse: 16.8299 - mae: 1.5402 - 23s/epoch - 1ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 23s - loss: 16.8115 - mse: 16.8115 - mae: 1.5389 - 23s/epoch - 1ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 23s - loss: 16.7939 - mse: 16.7939 - mae: 1.5389 - 23s/epoch - 1ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 22s - loss: 16.7768 - mse: 16.7768 - mae: 1.5365 - 22s/epoch - 1ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 22s - loss: 16.7605 - mse: 16.7605 - mae: 1.5361 - 22s/epoch - 1ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 21s - loss: 16.7446 - mse: 16.7446 - mae: 1.5352 - 21s/epoch - 1ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 21s - loss: 16.7296 - mse: 16.7296 - mae: 1.5352 - 21s/epoch - 1ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 21s - loss: 16.7148 - mse: 16.7148 - mae: 1.5341 - 21s/epoch - 1ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 20s - loss: 16.7006 - mse: 16.7006 - mae: 1.5336 - 20s/epoch - 1ms/step\n",
            "Score for fold 3: loss of 12.560107231140137\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-10-01 01:22:17,503]\u001b[0m Finished trial#9 resulted in value: 16.700626373291016. Current best value is 14.97073745727539 with parameters: {'activation': 'relu', 'optimizer': 'adam', 'num_hidden_layer': 2, 'num_hidden_unit': 73, 'lear  ning_rate': 0.005419337348062135}.\u001b[0m\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 30s - loss: 17.3416 - mse: 17.3416 - mae: 1.5857 - 30s/epoch - 1ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 31s - loss: 16.0731 - mse: 16.0731 - mae: 1.5185 - 31s/epoch - 2ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 30s - loss: 15.8227 - mse: 15.8226 - mae: 1.5037 - 30s/epoch - 1ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 30s - loss: 15.6757 - mse: 15.6757 - mae: 1.4972 - 30s/epoch - 1ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 29s - loss: 15.5419 - mse: 15.5419 - mae: 1.4911 - 29s/epoch - 1ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 29s - loss: 15.4246 - mse: 15.4246 - mae: 1.4869 - 29s/epoch - 1ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 34s - loss: 15.3345 - mse: 15.3345 - mae: 1.4831 - 34s/epoch - 2ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 32s - loss: 15.2494 - mse: 15.2494 - mae: 1.4800 - 32s/epoch - 2ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 33s - loss: 15.1559 - mse: 15.1559 - mae: 1.4781 - 33s/epoch - 2ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 31s - loss: 15.0330 - mse: 15.0330 - mae: 1.4742 - 31s/epoch - 2ms/step\n",
            "Score for fold 1: loss of 19.101961135864258\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 31s - loss: 15.0031 - mse: 15.0031 - mae: 1.4737 - 31s/epoch - 2ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 30s - loss: 14.9369 - mse: 14.9369 - mae: 1.4707 - 30s/epoch - 2ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 32s - loss: 14.8852 - mse: 14.8852 - mae: 1.4694 - 32s/epoch - 2ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 30s - loss: 14.8572 - mse: 14.8572 - mae: 1.4677 - 30s/epoch - 2ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 30s - loss: 14.8125 - mse: 14.8125 - mae: 1.4653 - 30s/epoch - 1ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 31s - loss: 14.7770 - mse: 14.7770 - mae: 1.4650 - 31s/epoch - 2ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 31s - loss: 14.7455 - mse: 14.7455 - mae: 1.4633 - 31s/epoch - 2ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 31s - loss: 14.6925 - mse: 14.6925 - mae: 1.4631 - 31s/epoch - 2ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 31s - loss: 14.6738 - mse: 14.6738 - mae: 1.4611 - 31s/epoch - 2ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 31s - loss: 14.6466 - mse: 14.6466 - mae: 1.4601 - 31s/epoch - 2ms/step\n",
            "Score for fold 2: loss of 10.727810859680176\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 31s - loss: 14.6200 - mse: 14.6200 - mae: 1.4592 - 31s/epoch - 2ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 31s - loss: 14.6113 - mse: 14.6113 - mae: 1.4582 - 31s/epoch - 2ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 30s - loss: 14.5678 - mse: 14.5678 - mae: 1.4580 - 30s/epoch - 2ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 29s - loss: 14.5710 - mse: 14.5710 - mae: 1.4570 - 29s/epoch - 1ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 29s - loss: 14.5306 - mse: 14.5306 - mae: 1.4557 - 29s/epoch - 1ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 28s - loss: 14.5354 - mse: 14.5354 - mae: 1.4550 - 28s/epoch - 1ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 28s - loss: 14.5011 - mse: 14.5011 - mae: 1.4547 - 28s/epoch - 1ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 29s - loss: 14.4663 - mse: 14.4663 - mae: 1.4532 - 29s/epoch - 1ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 30s - loss: 14.4770 - mse: 14.4770 - mae: 1.4536 - 30s/epoch - 1ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 30s - loss: 14.4277 - mse: 14.4277 - mae: 1.4525 - 30s/epoch - 2ms/step\n",
            "Score for fold 3: loss of 10.432677268981934\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-10-01 01:37:39,047]\u001b[0m Finished trial#10 resulted in value: 14.42773723602295. Current best value is 14.42773723602295 with parameters: {'activation': 'relu', 'optimizer': 'adam', 'num_hidden_layer': 3, 'num_hidden_unit': 39, 'lear  ning_rate': 0.00014096098537527108}.\u001b[0m\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 32s - loss: 17.1096 - mse: 17.1096 - mae: 1.5813 - 32s/epoch - 2ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 30s - loss: 16.1694 - mse: 16.1694 - mae: 1.5255 - 30s/epoch - 2ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 31s - loss: 15.9188 - mse: 15.9188 - mae: 1.5100 - 31s/epoch - 2ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 31s - loss: 15.7193 - mse: 15.7193 - mae: 1.5026 - 31s/epoch - 2ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 31s - loss: 15.6358 - mse: 15.6358 - mae: 1.4955 - 31s/epoch - 2ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 30s - loss: 15.5297 - mse: 15.5297 - mae: 1.4920 - 30s/epoch - 1ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 30s - loss: 15.4406 - mse: 15.4406 - mae: 1.4891 - 30s/epoch - 1ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 29s - loss: 15.3409 - mse: 15.3409 - mae: 1.4863 - 29s/epoch - 1ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 29s - loss: 15.3050 - mse: 15.3050 - mae: 1.4837 - 29s/epoch - 1ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 30s - loss: 15.2263 - mse: 15.2263 - mae: 1.4825 - 30s/epoch - 2ms/step\n",
            "Score for fold 1: loss of 21.09974479675293\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 30s - loss: 15.1531 - mse: 15.1531 - mae: 1.4811 - 30s/epoch - 2ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 30s - loss: 15.1136 - mse: 15.1136 - mae: 1.4796 - 30s/epoch - 2ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 30s - loss: 15.0605 - mse: 15.0605 - mae: 1.4786 - 30s/epoch - 2ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 32s - loss: 14.9739 - mse: 14.9739 - mae: 1.4757 - 32s/epoch - 2ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 33s - loss: 14.9618 - mse: 14.9618 - mae: 1.4750 - 33s/epoch - 2ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 34s - loss: 14.9329 - mse: 14.9329 - mae: 1.4731 - 34s/epoch - 2ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 32s - loss: 14.8803 - mse: 14.8803 - mae: 1.4715 - 32s/epoch - 2ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 32s - loss: 14.8352 - mse: 14.8352 - mae: 1.4704 - 32s/epoch - 2ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 31s - loss: 14.8344 - mse: 14.8344 - mae: 1.4682 - 31s/epoch - 2ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 31s - loss: 14.8026 - mse: 14.8026 - mae: 1.4668 - 31s/epoch - 2ms/step\n",
            "Score for fold 2: loss of 10.829924583435059\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 31s - loss: 14.7612 - mse: 14.7612 - mae: 1.4657 - 31s/epoch - 2ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 31s - loss: 14.7275 - mse: 14.7275 - mae: 1.4642 - 31s/epoch - 2ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 32s - loss: 14.7039 - mse: 14.7039 - mae: 1.4628 - 32s/epoch - 2ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 31s - loss: 14.6854 - mse: 14.6854 - mae: 1.4626 - 31s/epoch - 2ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 30s - loss: 14.6208 - mse: 14.6208 - mae: 1.4600 - 30s/epoch - 2ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 32s - loss: 14.6334 - mse: 14.6334 - mae: 1.4599 - 32s/epoch - 2ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 31s - loss: 14.6262 - mse: 14.6262 - mae: 1.4594 - 31s/epoch - 2ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 30s - loss: 14.5994 - mse: 14.5994 - mae: 1.4577 - 30s/epoch - 2ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 30s - loss: 14.5790 - mse: 14.5790 - mae: 1.4576 - 30s/epoch - 2ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 29s - loss: 14.5169 - mse: 14.5169 - mae: 1.4556 - 29s/epoch - 1ms/step\n",
            "Score for fold 3: loss of 10.552093505859375\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-10-01 01:53:14,298]\u001b[0m Finished trial#11 resulted in value: 14.516929626464844. Current best value is 14.42773723602295 with parameters: {'activation': 'relu', 'optimizer': 'adam', 'num_hidden_layer': 3, 'num_hidden_unit': 39, 'lear  ning_rate': 0.00014096098537527108}.\u001b[0m\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 28s - loss: 17.5126 - mse: 17.5126 - mae: 1.5964 - 28s/epoch - 1ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 27s - loss: 16.2039 - mse: 16.2039 - mae: 1.5322 - 27s/epoch - 1ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 28s - loss: 15.9743 - mse: 15.9743 - mae: 1.5155 - 28s/epoch - 1ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 29s - loss: 15.8285 - mse: 15.8285 - mae: 1.5063 - 29s/epoch - 1ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 29s - loss: 15.7352 - mse: 15.7352 - mae: 1.5006 - 29s/epoch - 1ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 29s - loss: 15.6204 - mse: 15.6204 - mae: 1.4961 - 29s/epoch - 1ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 29s - loss: 15.5273 - mse: 15.5273 - mae: 1.4922 - 29s/epoch - 1ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 29s - loss: 15.4588 - mse: 15.4588 - mae: 1.4896 - 29s/epoch - 1ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 29s - loss: 15.3674 - mse: 15.3674 - mae: 1.4864 - 29s/epoch - 1ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 29s - loss: 15.2743 - mse: 15.2743 - mae: 1.4840 - 29s/epoch - 1ms/step\n",
            "Score for fold 1: loss of 22.566131591796875\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 29s - loss: 15.2152 - mse: 15.2152 - mae: 1.4822 - 29s/epoch - 1ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 28s - loss: 15.1615 - mse: 15.1615 - mae: 1.4806 - 28s/epoch - 1ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 29s - loss: 15.0924 - mse: 15.0924 - mae: 1.4785 - 29s/epoch - 1ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 27s - loss: 15.0355 - mse: 15.0355 - mae: 1.4750 - 27s/epoch - 1ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 29s - loss: 14.9795 - mse: 14.9795 - mae: 1.4742 - 29s/epoch - 1ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 28s - loss: 14.9457 - mse: 14.9457 - mae: 1.4722 - 28s/epoch - 1ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 28s - loss: 14.8905 - mse: 14.8905 - mae: 1.4698 - 28s/epoch - 1ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 29s - loss: 14.8569 - mse: 14.8569 - mae: 1.4694 - 29s/epoch - 1ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 29s - loss: 14.8296 - mse: 14.8296 - mae: 1.4675 - 29s/epoch - 1ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 29s - loss: 14.7870 - mse: 14.7870 - mae: 1.4650 - 29s/epoch - 1ms/step\n",
            "Score for fold 2: loss of 10.838214874267578\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 30s - loss: 14.7665 - mse: 14.7665 - mae: 1.4644 - 30s/epoch - 1ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 29s - loss: 14.7174 - mse: 14.7174 - mae: 1.4629 - 29s/epoch - 1ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 32s - loss: 14.7165 - mse: 14.7165 - mae: 1.4620 - 32s/epoch - 2ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 38s - loss: 14.6823 - mse: 14.6823 - mae: 1.4602 - 38s/epoch - 2ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 37s - loss: 14.6630 - mse: 14.6630 - mae: 1.4598 - 37s/epoch - 2ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 31s - loss: 14.6306 - mse: 14.6306 - mae: 1.4582 - 31s/epoch - 2ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 29s - loss: 14.5935 - mse: 14.5935 - mae: 1.4562 - 29s/epoch - 1ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 28s - loss: 14.5921 - mse: 14.5921 - mae: 1.4565 - 28s/epoch - 1ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 28s - loss: 14.5730 - mse: 14.5730 - mae: 1.4559 - 28s/epoch - 1ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 28s - loss: 14.5628 - mse: 14.5628 - mae: 1.4540 - 28s/epoch - 1ms/step\n",
            "Score for fold 3: loss of 10.565286636352539\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-10-01 02:08:11,118]\u001b[0m Finished trial#12 resulted in value: 14.562769889831543. Current best value is 14.42773723602295 with parameters: {'activation': 'relu', 'optimizer': 'adam', 'num_hidden_layer': 3, 'num_hidden_unit': 39, 'lear  ning_rate': 0.00014096098537527108}.\u001b[0m\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 30s - loss: 17.3812 - mse: 17.3812 - mae: 1.5924 - 30s/epoch - 1ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 31s - loss: 16.2071 - mse: 16.2071 - mae: 1.5286 - 31s/epoch - 2ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 31s - loss: 15.9371 - mse: 15.9371 - mae: 1.5120 - 31s/epoch - 2ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 32s - loss: 15.7786 - mse: 15.7786 - mae: 1.5051 - 32s/epoch - 2ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 31s - loss: 15.6742 - mse: 15.6742 - mae: 1.5004 - 31s/epoch - 2ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 32s - loss: 15.6152 - mse: 15.6152 - mae: 1.4969 - 32s/epoch - 2ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 32s - loss: 15.5356 - mse: 15.5356 - mae: 1.4951 - 32s/epoch - 2ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 31s - loss: 15.4570 - mse: 15.4570 - mae: 1.4920 - 31s/epoch - 2ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 33s - loss: 15.4171 - mse: 15.4171 - mae: 1.4902 - 33s/epoch - 2ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 32s - loss: 15.3345 - mse: 15.3345 - mae: 1.4886 - 32s/epoch - 2ms/step\n",
            "Score for fold 1: loss of 23.463014602661133\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 32s - loss: 15.2850 - mse: 15.2850 - mae: 1.4864 - 32s/epoch - 2ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 32s - loss: 15.2284 - mse: 15.2284 - mae: 1.4841 - 32s/epoch - 2ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 31s - loss: 15.1766 - mse: 15.1766 - mae: 1.4821 - 31s/epoch - 2ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 32s - loss: 15.1392 - mse: 15.1392 - mae: 1.4810 - 32s/epoch - 2ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 31s - loss: 15.0912 - mse: 15.0912 - mae: 1.4793 - 31s/epoch - 2ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 30s - loss: 15.0343 - mse: 15.0343 - mae: 1.4771 - 30s/epoch - 1ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 28s - loss: 15.0077 - mse: 15.0077 - mae: 1.4751 - 28s/epoch - 1ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 29s - loss: 14.9788 - mse: 14.9788 - mae: 1.4744 - 29s/epoch - 1ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 32s - loss: 14.9509 - mse: 14.9509 - mae: 1.4731 - 32s/epoch - 2ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 33s - loss: 14.9120 - mse: 14.9120 - mae: 1.4714 - 33s/epoch - 2ms/step\n",
            "Score for fold 2: loss of 10.886730194091797\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 34s - loss: 14.8784 - mse: 14.8784 - mae: 1.4694 - 34s/epoch - 2ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 32s - loss: 14.8627 - mse: 14.8627 - mae: 1.4685 - 32s/epoch - 2ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 31s - loss: 14.8376 - mse: 14.8376 - mae: 1.4664 - 31s/epoch - 2ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 31s - loss: 14.8150 - mse: 14.8150 - mae: 1.4653 - 31s/epoch - 2ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 30s - loss: 14.7856 - mse: 14.7856 - mae: 1.4649 - 30s/epoch - 2ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 30s - loss: 14.7724 - mse: 14.7724 - mae: 1.4642 - 30s/epoch - 2ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 30s - loss: 14.7484 - mse: 14.7484 - mae: 1.4629 - 30s/epoch - 2ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 29s - loss: 14.7158 - mse: 14.7158 - mae: 1.4630 - 29s/epoch - 1ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 31s - loss: 14.7071 - mse: 14.7071 - mae: 1.4613 - 31s/epoch - 2ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 30s - loss: 14.6891 - mse: 14.6891 - mae: 1.4609 - 30s/epoch - 2ms/step\n",
            "Score for fold 3: loss of 10.5773286819458\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-10-01 02:23:54,268]\u001b[0m Finished trial#13 resulted in value: 14.689101219177246. Current best value is 14.42773723602295 with parameters: {'activation': 'relu', 'optimizer': 'adam', 'num_hidden_layer': 3, 'num_hidden_unit': 39, 'lear  ning_rate': 0.00014096098537527108}.\u001b[0m\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 34s - loss: 18.5582 - mse: 18.5582 - mae: 1.6842 - 34s/epoch - 2ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 33s - loss: 16.7746 - mse: 16.7746 - mae: 1.5726 - 33s/epoch - 2ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 33s - loss: 16.4684 - mse: 16.4684 - mae: 1.5462 - 33s/epoch - 2ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 34s - loss: 16.2208 - mse: 16.2208 - mae: 1.5352 - 34s/epoch - 2ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 32s - loss: 16.0494 - mse: 16.0494 - mae: 1.5266 - 32s/epoch - 2ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 32s - loss: 15.9532 - mse: 15.9532 - mae: 1.5203 - 32s/epoch - 2ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 33s - loss: 15.8553 - mse: 15.8553 - mae: 1.5152 - 33s/epoch - 2ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 33s - loss: 15.7669 - mse: 15.7669 - mae: 1.5106 - 33s/epoch - 2ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 32s - loss: 15.7095 - mse: 15.7095 - mae: 1.5077 - 32s/epoch - 2ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 33s - loss: 15.6370 - mse: 15.6370 - mae: 1.5039 - 33s/epoch - 2ms/step\n",
            "Score for fold 1: loss of 26.836217880249023\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 34s - loss: 15.6055 - mse: 15.6055 - mae: 1.4999 - 34s/epoch - 2ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 34s - loss: 15.5450 - mse: 15.5450 - mae: 1.4993 - 34s/epoch - 2ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 34s - loss: 15.4897 - mse: 15.4897 - mae: 1.4972 - 34s/epoch - 2ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 34s - loss: 15.4453 - mse: 15.4453 - mae: 1.4942 - 34s/epoch - 2ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 35s - loss: 15.3971 - mse: 15.3971 - mae: 1.4933 - 35s/epoch - 2ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 33s - loss: 15.3631 - mse: 15.3631 - mae: 1.4920 - 33s/epoch - 2ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 33s - loss: 15.3419 - mse: 15.3419 - mae: 1.4919 - 33s/epoch - 2ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 34s - loss: 15.3169 - mse: 15.3169 - mae: 1.4906 - 34s/epoch - 2ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 33s - loss: 15.2768 - mse: 15.2768 - mae: 1.4895 - 33s/epoch - 2ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 33s - loss: 15.2472 - mse: 15.2472 - mae: 1.4897 - 33s/epoch - 2ms/step\n",
            "Score for fold 2: loss of 11.255097389221191\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 34s - loss: 15.2288 - mse: 15.2288 - mae: 1.4881 - 34s/epoch - 2ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 34s - loss: 15.2024 - mse: 15.2024 - mae: 1.4873 - 34s/epoch - 2ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 34s - loss: 15.1683 - mse: 15.1683 - mae: 1.4871 - 34s/epoch - 2ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 33s - loss: 15.1403 - mse: 15.1403 - mae: 1.4863 - 33s/epoch - 2ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 35s - loss: 15.0978 - mse: 15.0978 - mae: 1.4858 - 35s/epoch - 2ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 34s - loss: 15.0715 - mse: 15.0715 - mae: 1.4848 - 34s/epoch - 2ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 35s - loss: 15.0363 - mse: 15.0363 - mae: 1.4847 - 35s/epoch - 2ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 35s - loss: 14.9999 - mse: 14.9999 - mae: 1.4841 - 35s/epoch - 2ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 35s - loss: 14.9882 - mse: 14.9882 - mae: 1.4836 - 35s/epoch - 2ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 33s - loss: 14.9612 - mse: 14.9612 - mae: 1.4812 - 33s/epoch - 2ms/step\n",
            "Score for fold 3: loss of 10.826138496398926\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-10-01 02:40:56,348]\u001b[0m Finished trial#14 resulted in value: 14.961247444152832. Current best value is 14.42773723602295 with parameters: {'activation': 'relu', 'optimizer': 'adam', 'num_hidden_layer': 3, 'num_hidden_unit': 39, 'lear  ning_rate': 0.00014096098537527108}.\u001b[0m\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 24s - loss: 17.7902 - mse: 17.7902 - mae: 1.6169 - 24s/epoch - 1ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 24s - loss: 17.3369 - mse: 17.3369 - mae: 1.5894 - 24s/epoch - 1ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 24s - loss: 16.4063 - mse: 16.4063 - mae: 1.5436 - 24s/epoch - 1ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 24s - loss: 16.2194 - mse: 16.2194 - mae: 1.5274 - 24s/epoch - 1ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 23s - loss: 16.6271 - mse: 16.6271 - mae: 1.5242 - 23s/epoch - 1ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 23s - loss: 16.0919 - mse: 16.0919 - mae: 1.5169 - 23s/epoch - 1ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 23s - loss: 16.4496 - mse: 16.4496 - mae: 1.5263 - 23s/epoch - 1ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 23s - loss: 16.0668 - mse: 16.0668 - mae: 1.5127 - 23s/epoch - 1ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 23s - loss: 16.5197 - mse: 16.5197 - mae: 1.5247 - 23s/epoch - 1ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 23s - loss: 15.9813 - mse: 15.9813 - mae: 1.5088 - 23s/epoch - 1ms/step\n",
            "Score for fold 1: loss of 25.49420166015625\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 24s - loss: 15.7996 - mse: 15.7996 - mae: 1.5016 - 24s/epoch - 1ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 24s - loss: 15.8945 - mse: 15.8945 - mae: 1.5034 - 24s/epoch - 1ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 24s - loss: 15.7400 - mse: 15.7400 - mae: 1.4990 - 24s/epoch - 1ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 24s - loss: 16.1434 - mse: 16.1434 - mae: 1.5142 - 24s/epoch - 1ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 24s - loss: 16.0839 - mse: 16.0839 - mae: 1.5069 - 24s/epoch - 1ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 24s - loss: 16.4390 - mse: 16.4390 - mae: 1.5109 - 24s/epoch - 1ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 24s - loss: 15.7782 - mse: 15.7782 - mae: 1.4969 - 24s/epoch - 1ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 24s - loss: 16.0804 - mse: 16.0804 - mae: 1.4999 - 24s/epoch - 1ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 23s - loss: 15.7932 - mse: 15.7932 - mae: 1.4973 - 23s/epoch - 1ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 24s - loss: 16.1191 - mse: 16.1191 - mae: 1.5068 - 24s/epoch - 1ms/step\n",
            "Score for fold 2: loss of 12.330535888671875\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 24s - loss: 15.9488 - mse: 15.9488 - mae: 1.5013 - 24s/epoch - 1ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 24s - loss: 16.0862 - mse: 16.0862 - mae: 1.5231 - 24s/epoch - 1ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 23s - loss: 15.9559 - mse: 15.9559 - mae: 1.5157 - 23s/epoch - 1ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 23s - loss: 15.8512 - mse: 15.8512 - mae: 1.5069 - 23s/epoch - 1ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 24s - loss: 15.8836 - mse: 15.8836 - mae: 1.4961 - 24s/epoch - 1ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 24s - loss: 15.8605 - mse: 15.8605 - mae: 1.4956 - 24s/epoch - 1ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 24s - loss: 15.6675 - mse: 15.6675 - mae: 1.4893 - 24s/epoch - 1ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 25s - loss: 15.5740 - mse: 15.5740 - mae: 1.4878 - 25s/epoch - 1ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 24s - loss: 15.4915 - mse: 15.4915 - mae: 1.4846 - 24s/epoch - 1ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 24s - loss: 16.0674 - mse: 16.0674 - mae: 1.5080 - 24s/epoch - 1ms/step\n",
            "Score for fold 3: loss of 11.099654197692871\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-10-01 02:53:00,752]\u001b[0m Finished trial#15 resulted in value: 16.067394256591797. Current best value is 14.42773723602295 with parameters: {'activation': 'relu', 'optimizer': 'adam', 'num_hidden_layer': 3, 'num_hidden_unit': 39, 'lear  ning_rate': 0.00014096098537527108}.\u001b[0m\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 32s - loss: 17.8968 - mse: 17.8968 - mae: 1.6310 - 32s/epoch - 2ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 32s - loss: 16.4517 - mse: 16.4517 - mae: 1.5501 - 32s/epoch - 2ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 34s - loss: 16.2223 - mse: 16.2223 - mae: 1.5321 - 34s/epoch - 2ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 34s - loss: 16.0524 - mse: 16.0524 - mae: 1.5223 - 34s/epoch - 2ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 34s - loss: 15.9285 - mse: 15.9285 - mae: 1.5140 - 34s/epoch - 2ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 34s - loss: 15.8398 - mse: 15.8398 - mae: 1.5099 - 34s/epoch - 2ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 32s - loss: 15.7652 - mse: 15.7652 - mae: 1.5075 - 32s/epoch - 2ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 32s - loss: 15.6556 - mse: 15.6556 - mae: 1.5036 - 32s/epoch - 2ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 33s - loss: 15.5900 - mse: 15.5900 - mae: 1.5006 - 33s/epoch - 2ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 34s - loss: 15.5296 - mse: 15.5296 - mae: 1.4985 - 34s/epoch - 2ms/step\n",
            "Score for fold 1: loss of 23.356117248535156\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 34s - loss: 15.4806 - mse: 15.4806 - mae: 1.4967 - 34s/epoch - 2ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 33s - loss: 15.4136 - mse: 15.4136 - mae: 1.4943 - 33s/epoch - 2ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 34s - loss: 15.3812 - mse: 15.3812 - mae: 1.4929 - 34s/epoch - 2ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 33s - loss: 15.3299 - mse: 15.3299 - mae: 1.4908 - 33s/epoch - 2ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 35s - loss: 15.2829 - mse: 15.2829 - mae: 1.4893 - 35s/epoch - 2ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 35s - loss: 15.2559 - mse: 15.2559 - mae: 1.4874 - 35s/epoch - 2ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 35s - loss: 15.2190 - mse: 15.2190 - mae: 1.4859 - 35s/epoch - 2ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 34s - loss: 15.1816 - mse: 15.1816 - mae: 1.4851 - 34s/epoch - 2ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 33s - loss: 15.1545 - mse: 15.1545 - mae: 1.4830 - 33s/epoch - 2ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 33s - loss: 15.1113 - mse: 15.1113 - mae: 1.4811 - 33s/epoch - 2ms/step\n",
            "Score for fold 2: loss of 11.0799560546875\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 34s - loss: 15.1006 - mse: 15.1006 - mae: 1.4798 - 34s/epoch - 2ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 35s - loss: 15.0604 - mse: 15.0604 - mae: 1.4781 - 35s/epoch - 2ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 33s - loss: 15.0239 - mse: 15.0239 - mae: 1.4777 - 33s/epoch - 2ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 33s - loss: 14.9710 - mse: 14.9710 - mae: 1.4763 - 33s/epoch - 2ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 35s - loss: 14.9588 - mse: 14.9588 - mae: 1.4748 - 35s/epoch - 2ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 34s - loss: 14.9204 - mse: 14.9204 - mae: 1.4739 - 34s/epoch - 2ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 33s - loss: 14.9255 - mse: 14.9255 - mae: 1.4732 - 33s/epoch - 2ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 33s - loss: 14.8995 - mse: 14.8995 - mae: 1.4718 - 33s/epoch - 2ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 30s - loss: 14.8811 - mse: 14.8811 - mae: 1.4712 - 30s/epoch - 2ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 31s - loss: 14.8645 - mse: 14.8645 - mae: 1.4698 - 31s/epoch - 2ms/step\n",
            "Score for fold 3: loss of 10.885963439941406\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-10-01 03:09:53,890]\u001b[0m Finished trial#16 resulted in value: 14.86447525024414. Current best value is 14.42773723602295 with parameters: {'activation': 'relu', 'optimizer': 'adam', 'num_hidden_layer': 3, 'num_hidden_unit': 39, 'lear  ning_rate': 0.00014096098537527108}.\u001b[0m\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 31s - loss: 17.9610 - mse: 17.9610 - mae: 1.6299 - 31s/epoch - 2ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 31s - loss: 16.7054 - mse: 16.7054 - mae: 1.5563 - 31s/epoch - 2ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 30s - loss: 16.3259 - mse: 16.3259 - mae: 1.5385 - 30s/epoch - 1ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 31s - loss: 16.1450 - mse: 16.1450 - mae: 1.5283 - 31s/epoch - 2ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 29s - loss: 16.0239 - mse: 16.0239 - mae: 1.5204 - 29s/epoch - 1ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 29s - loss: 15.9406 - mse: 15.9406 - mae: 1.5139 - 29s/epoch - 1ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 33s - loss: 15.8721 - mse: 15.8721 - mae: 1.5087 - 33s/epoch - 2ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 32s - loss: 15.8115 - mse: 15.8115 - mae: 1.5053 - 32s/epoch - 2ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 32s - loss: 15.7492 - mse: 15.7492 - mae: 1.5026 - 32s/epoch - 2ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 32s - loss: 15.7091 - mse: 15.7091 - mae: 1.5006 - 32s/epoch - 2ms/step\n",
            "Score for fold 1: loss of 25.023487091064453\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 32s - loss: 15.6618 - mse: 15.6618 - mae: 1.4988 - 32s/epoch - 2ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 32s - loss: 15.6208 - mse: 15.6208 - mae: 1.4975 - 32s/epoch - 2ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 30s - loss: 15.5715 - mse: 15.5715 - mae: 1.4950 - 30s/epoch - 2ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 31s - loss: 15.5569 - mse: 15.5569 - mae: 1.4950 - 31s/epoch - 2ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 30s - loss: 15.5194 - mse: 15.5194 - mae: 1.4930 - 30s/epoch - 1ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 30s - loss: 15.4873 - mse: 15.4873 - mae: 1.4925 - 30s/epoch - 1ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 30s - loss: 15.4549 - mse: 15.4549 - mae: 1.4907 - 30s/epoch - 1ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 30s - loss: 15.4046 - mse: 15.4046 - mae: 1.4899 - 30s/epoch - 1ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 30s - loss: 15.3822 - mse: 15.3822 - mae: 1.4882 - 30s/epoch - 2ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 30s - loss: 15.3474 - mse: 15.3474 - mae: 1.4881 - 30s/epoch - 2ms/step\n",
            "Score for fold 2: loss of 11.138604164123535\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 30s - loss: 15.3184 - mse: 15.3184 - mae: 1.4858 - 30s/epoch - 2ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 30s - loss: 15.2915 - mse: 15.2915 - mae: 1.4864 - 30s/epoch - 1ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 29s - loss: 15.2621 - mse: 15.2621 - mae: 1.4852 - 29s/epoch - 1ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 30s - loss: 15.2346 - mse: 15.2346 - mae: 1.4843 - 30s/epoch - 1ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 29s - loss: 15.2207 - mse: 15.2207 - mae: 1.4830 - 29s/epoch - 1ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 28s - loss: 15.1780 - mse: 15.1780 - mae: 1.4822 - 28s/epoch - 1ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 28s - loss: 15.1443 - mse: 15.1443 - mae: 1.4807 - 28s/epoch - 1ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 28s - loss: 15.1151 - mse: 15.1151 - mae: 1.4799 - 28s/epoch - 1ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 30s - loss: 15.0983 - mse: 15.0983 - mae: 1.4791 - 30s/epoch - 1ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 29s - loss: 15.0849 - mse: 15.0849 - mae: 1.4781 - 29s/epoch - 1ms/step\n",
            "Score for fold 3: loss of 10.884063720703125\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-10-01 03:25:07,814]\u001b[0m Finished trial#17 resulted in value: 15.084877967834473. Current best value is 14.42773723602295 with parameters: {'activation': 'relu', 'optimizer': 'adam', 'num_hidden_layer': 3, 'num_hidden_unit': 39, 'lear  ning_rate': 0.00014096098537527108}.\u001b[0m\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 31s - loss: 21.0670 - mse: 21.0670 - mae: 1.7837 - 31s/epoch - 2ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 29s - loss: 18.0260 - mse: 18.0260 - mae: 1.5873 - 29s/epoch - 1ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 29s - loss: 17.6226 - mse: 17.6226 - mae: 1.5652 - 29s/epoch - 1ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 30s - loss: 17.3752 - mse: 17.3752 - mae: 1.5536 - 30s/epoch - 1ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 29s - loss: 17.1798 - mse: 17.1798 - mae: 1.5427 - 29s/epoch - 1ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 29s - loss: 17.0156 - mse: 17.0156 - mae: 1.5361 - 29s/epoch - 1ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 29s - loss: 16.8735 - mse: 16.8735 - mae: 1.5296 - 29s/epoch - 1ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 29s - loss: 16.7559 - mse: 16.7559 - mae: 1.5251 - 29s/epoch - 1ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 29s - loss: 16.6545 - mse: 16.6545 - mae: 1.5198 - 29s/epoch - 1ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 27s - loss: 16.5695 - mse: 16.5695 - mae: 1.5193 - 27s/epoch - 1ms/step\n",
            "Score for fold 1: loss of 24.363428115844727\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 29s - loss: 16.4930 - mse: 16.4930 - mae: 1.5147 - 29s/epoch - 1ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 29s - loss: 16.4234 - mse: 16.4234 - mae: 1.5130 - 29s/epoch - 1ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 30s - loss: 16.3626 - mse: 16.3626 - mae: 1.5103 - 30s/epoch - 2ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 29s - loss: 16.3064 - mse: 16.3064 - mae: 1.5091 - 29s/epoch - 1ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 29s - loss: 16.2532 - mse: 16.2532 - mae: 1.5063 - 29s/epoch - 1ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 29s - loss: 16.2045 - mse: 16.2045 - mae: 1.5054 - 29s/epoch - 1ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 29s - loss: 16.1606 - mse: 16.1606 - mae: 1.5042 - 29s/epoch - 1ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 30s - loss: 16.1179 - mse: 16.1179 - mae: 1.5030 - 30s/epoch - 1ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 30s - loss: 16.0787 - mse: 16.0787 - mae: 1.5007 - 30s/epoch - 1ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 30s - loss: 16.0405 - mse: 16.0405 - mae: 1.5006 - 30s/epoch - 2ms/step\n",
            "Score for fold 2: loss of 12.279685974121094\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 30s - loss: 16.0054 - mse: 16.0054 - mae: 1.4987 - 30s/epoch - 2ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 30s - loss: 15.9720 - mse: 15.9720 - mae: 1.4976 - 30s/epoch - 2ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 33s - loss: 15.9407 - mse: 15.9407 - mae: 1.4975 - 33s/epoch - 2ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 31s - loss: 15.9097 - mse: 15.9097 - mae: 1.4962 - 31s/epoch - 2ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 30s - loss: 15.8817 - mse: 15.8817 - mae: 1.4948 - 30s/epoch - 2ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 31s - loss: 15.8525 - mse: 15.8525 - mae: 1.4941 - 31s/epoch - 2ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 30s - loss: 15.8261 - mse: 15.8261 - mae: 1.4934 - 30s/epoch - 1ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 30s - loss: 15.8003 - mse: 15.8003 - mae: 1.4926 - 30s/epoch - 2ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 30s - loss: 15.7770 - mse: 15.7770 - mae: 1.4920 - 30s/epoch - 1ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 31s - loss: 15.7549 - mse: 15.7549 - mae: 1.4900 - 31s/epoch - 2ms/step\n",
            "Score for fold 3: loss of 11.840571403503418\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-10-01 03:40:08,694]\u001b[0m Finished trial#18 resulted in value: 15.754895210266113. Current best value is 14.42773723602295 with parameters: {'activation': 'relu', 'optimizer': 'adam', 'num_hidden_layer': 3, 'num_hidden_unit': 39, 'lear  ning_rate': 0.00014096098537527108}.\u001b[0m\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 20s - loss: 17.8714 - mse: 17.8714 - mae: 1.5871 - 20s/epoch - 988us/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 19s - loss: 16.5666 - mse: 16.5666 - mae: 1.5255 - 19s/epoch - 938us/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 19s - loss: 16.4054 - mse: 16.4054 - mae: 1.5191 - 19s/epoch - 935us/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 19s - loss: 16.2211 - mse: 16.2211 - mae: 1.5127 - 19s/epoch - 951us/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 18s - loss: 16.0804 - mse: 16.0804 - mae: 1.5062 - 18s/epoch - 903us/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 18s - loss: 15.9604 - mse: 15.9604 - mae: 1.5001 - 18s/epoch - 915us/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 18s - loss: 15.8527 - mse: 15.8527 - mae: 1.4963 - 18s/epoch - 921us/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 20s - loss: 15.7880 - mse: 15.7880 - mae: 1.4956 - 20s/epoch - 987us/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 19s - loss: 15.7569 - mse: 15.7569 - mae: 1.4948 - 19s/epoch - 935us/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 20s - loss: 15.7599 - mse: 15.7599 - mae: 1.4947 - 20s/epoch - 976us/step\n",
            "Score for fold 1: loss of 25.82966423034668\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 21s - loss: 15.7516 - mse: 15.7516 - mae: 1.4937 - 21s/epoch - 1ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 21s - loss: 15.7724 - mse: 15.7724 - mae: 1.4933 - 21s/epoch - 1ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 21s - loss: 15.7664 - mse: 15.7664 - mae: 1.4928 - 21s/epoch - 1ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 20s - loss: 15.7738 - mse: 15.7738 - mae: 1.4940 - 20s/epoch - 1ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 20s - loss: 15.7902 - mse: 15.7902 - mae: 1.4925 - 20s/epoch - 1ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 22s - loss: 15.7908 - mse: 15.7908 - mae: 1.4925 - 22s/epoch - 1ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 21s - loss: 15.8120 - mse: 15.8120 - mae: 1.4927 - 21s/epoch - 1ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 21s - loss: 15.8308 - mse: 15.8308 - mae: 1.4937 - 21s/epoch - 1ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 22s - loss: 15.8313 - mse: 15.8313 - mae: 1.4929 - 22s/epoch - 1ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 20s - loss: 15.8553 - mse: 15.8553 - mae: 1.4938 - 20s/epoch - 992us/step\n",
            "Score for fold 2: loss of 11.178794860839844\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 20s - loss: 15.8662 - mse: 15.8662 - mae: 1.4936 - 20s/epoch - 1ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 19s - loss: 15.8933 - mse: 15.8933 - mae: 1.4944 - 19s/epoch - 946us/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 19s - loss: 15.9148 - mse: 15.9148 - mae: 1.4937 - 19s/epoch - 950us/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 20s - loss: 15.9298 - mse: 15.9298 - mae: 1.4925 - 20s/epoch - 1ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 20s - loss: 15.9103 - mse: 15.9103 - mae: 1.4929 - 20s/epoch - 975us/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 18s - loss: 15.8975 - mse: 15.8975 - mae: 1.4925 - 18s/epoch - 921us/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 18s - loss: 15.9256 - mse: 15.9256 - mae: 1.4923 - 18s/epoch - 923us/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 19s - loss: 15.9240 - mse: 15.9240 - mae: 1.4931 - 19s/epoch - 964us/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 19s - loss: 15.9263 - mse: 15.9263 - mae: 1.4924 - 19s/epoch - 933us/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 19s - loss: 15.9248 - mse: 15.9248 - mae: 1.4930 - 19s/epoch - 940us/step\n",
            "Score for fold 3: loss of 11.329452514648438\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-10-01 03:50:07,572]\u001b[0m Finished trial#19 resulted in value: 15.924788475036621. Current best value is 14.42773723602295 with parameters: {'activation': 'relu', 'optimizer': 'adam', 'num_hidden_layer': 3, 'num_hidden_unit': 39, 'lear  ning_rate': 0.00014096098537527108}.\u001b[0m\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 23s - loss: 19.1699 - mse: 19.1699 - mae: 1.6296 - 23s/epoch - 1ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 23s - loss: 17.9281 - mse: 17.9281 - mae: 1.5912 - 23s/epoch - 1ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 24s - loss: 17.1573 - mse: 17.1573 - mae: 1.5669 - 24s/epoch - 1ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 24s - loss: 17.6758 - mse: 17.6758 - mae: 1.5985 - 24s/epoch - 1ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 23s - loss: 24.6231 - mse: 24.6231 - mae: 1.8458 - 23s/epoch - 1ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 24s - loss: 20.5894 - mse: 20.5894 - mae: 1.8096 - 24s/epoch - 1ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 24s - loss: 22.4801 - mse: 22.4801 - mae: 1.9065 - 24s/epoch - 1ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 23s - loss: 21.3615 - mse: 21.3615 - mae: 1.9024 - 23s/epoch - 1ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 23s - loss: 21.3593 - mse: 21.3593 - mae: 1.9026 - 23s/epoch - 1ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 24s - loss: 21.3572 - mse: 21.3572 - mae: 1.9016 - 24s/epoch - 1ms/step\n",
            "Score for fold 1: loss of 22.10536003112793\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 24s - loss: 21.3535 - mse: 21.3535 - mae: 1.9004 - 24s/epoch - 1ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 23s - loss: 21.3473 - mse: 21.3473 - mae: 1.8977 - 23s/epoch - 1ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 24s - loss: 21.3483 - mse: 21.3483 - mae: 1.8980 - 24s/epoch - 1ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 24s - loss: 21.3456 - mse: 21.3456 - mae: 1.8979 - 24s/epoch - 1ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 23s - loss: 21.3407 - mse: 21.3407 - mae: 1.9000 - 23s/epoch - 1ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 23s - loss: 21.3344 - mse: 21.3344 - mae: 1.8965 - 23s/epoch - 1ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 24s - loss: 21.3313 - mse: 21.3313 - mae: 1.8982 - 24s/epoch - 1ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 23s - loss: 21.3313 - mse: 21.3313 - mae: 1.8969 - 23s/epoch - 1ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 24s - loss: 21.3345 - mse: 21.3345 - mae: 1.8929 - 24s/epoch - 1ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 23s - loss: 21.3310 - mse: 21.3310 - mae: 1.8937 - 23s/epoch - 1ms/step\n",
            "Score for fold 2: loss of 16.937976837158203\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 24s - loss: 21.3262 - mse: 21.3262 - mae: 1.8929 - 24s/epoch - 1ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 23s - loss: 21.3209 - mse: 21.3209 - mae: 1.8934 - 23s/epoch - 1ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 24s - loss: 21.3186 - mse: 21.3186 - mae: 1.8929 - 24s/epoch - 1ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 23s - loss: 21.3132 - mse: 21.3132 - mae: 1.8942 - 23s/epoch - 1ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 23s - loss: 21.3096 - mse: 21.3096 - mae: 1.8961 - 23s/epoch - 1ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 24s - loss: 21.3030 - mse: 21.3030 - mae: 1.8956 - 24s/epoch - 1ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 24s - loss: 21.3006 - mse: 21.3006 - mae: 1.8957 - 24s/epoch - 1ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 24s - loss: 21.2933 - mse: 21.2933 - mae: 1.8950 - 24s/epoch - 1ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 23s - loss: 21.2962 - mse: 21.2962 - mae: 1.8960 - 23s/epoch - 1ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 23s - loss: 21.2939 - mse: 21.2939 - mae: 1.8922 - 23s/epoch - 1ms/step\n",
            "Score for fold 3: loss of 16.93136215209961\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-10-01 04:02:01,648]\u001b[0m Finished trial#20 resulted in value: 21.293930053710938. Current best value is 14.42773723602295 with parameters: {'activation': 'relu', 'optimizer': 'adam', 'num_hidden_layer': 3, 'num_hidden_unit': 39, 'lear  ning_rate': 0.00014096098537527108}.\u001b[0m\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 27s - loss: 17.3358 - mse: 17.3358 - mae: 1.5857 - 27s/epoch - 1ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 28s - loss: 16.1221 - mse: 16.1221 - mae: 1.5261 - 28s/epoch - 1ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 27s - loss: 15.8862 - mse: 15.8862 - mae: 1.5128 - 27s/epoch - 1ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 29s - loss: 15.7527 - mse: 15.7527 - mae: 1.5041 - 29s/epoch - 1ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 27s - loss: 15.6747 - mse: 15.6747 - mae: 1.4992 - 27s/epoch - 1ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 28s - loss: 15.5523 - mse: 15.5523 - mae: 1.4960 - 28s/epoch - 1ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 29s - loss: 15.4712 - mse: 15.4712 - mae: 1.4922 - 29s/epoch - 1ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 27s - loss: 15.3632 - mse: 15.3632 - mae: 1.4892 - 27s/epoch - 1ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 28s - loss: 15.2897 - mse: 15.2897 - mae: 1.4870 - 28s/epoch - 1ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 28s - loss: 15.2414 - mse: 15.2414 - mae: 1.4833 - 28s/epoch - 1ms/step\n",
            "Score for fold 1: loss of 27.064350128173828\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 29s - loss: 15.1639 - mse: 15.1639 - mae: 1.4810 - 29s/epoch - 1ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 27s - loss: 15.0957 - mse: 15.0957 - mae: 1.4800 - 27s/epoch - 1ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 27s - loss: 15.0276 - mse: 15.0276 - mae: 1.4771 - 27s/epoch - 1ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 29s - loss: 14.9816 - mse: 14.9816 - mae: 1.4749 - 29s/epoch - 1ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 30s - loss: 14.9407 - mse: 14.9407 - mae: 1.4736 - 30s/epoch - 2ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 29s - loss: 14.9174 - mse: 14.9174 - mae: 1.4725 - 29s/epoch - 1ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 29s - loss: 14.8767 - mse: 14.8767 - mae: 1.4714 - 29s/epoch - 1ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 29s - loss: 14.8158 - mse: 14.8158 - mae: 1.4694 - 29s/epoch - 1ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 28s - loss: 14.8164 - mse: 14.8164 - mae: 1.4686 - 28s/epoch - 1ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 31s - loss: 14.7673 - mse: 14.7673 - mae: 1.4668 - 31s/epoch - 2ms/step\n",
            "Score for fold 2: loss of 10.790671348571777\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 33s - loss: 14.7469 - mse: 14.7469 - mae: 1.4653 - 33s/epoch - 2ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 34s - loss: 14.7136 - mse: 14.7136 - mae: 1.4652 - 34s/epoch - 2ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 34s - loss: 14.7015 - mse: 14.7015 - mae: 1.4641 - 34s/epoch - 2ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 30s - loss: 14.6660 - mse: 14.6660 - mae: 1.4633 - 30s/epoch - 1ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 28s - loss: 14.6366 - mse: 14.6366 - mae: 1.4614 - 28s/epoch - 1ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 28s - loss: 14.6229 - mse: 14.6229 - mae: 1.4608 - 28s/epoch - 1ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 28s - loss: 14.5971 - mse: 14.5971 - mae: 1.4595 - 28s/epoch - 1ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 28s - loss: 14.5660 - mse: 14.5660 - mae: 1.4594 - 28s/epoch - 1ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 27s - loss: 14.5443 - mse: 14.5443 - mae: 1.4588 - 27s/epoch - 1ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 26s - loss: 14.5367 - mse: 14.5367 - mae: 1.4578 - 26s/epoch - 1ms/step\n",
            "Score for fold 3: loss of 10.511683464050293\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-10-01 04:16:34,554]\u001b[0m Finished trial#21 resulted in value: 14.536712646484375. Current best value is 14.42773723602295 with parameters: {'activation': 'relu', 'optimizer': 'adam', 'num_hidden_layer': 3, 'num_hidden_unit': 39, 'lear  ning_rate': 0.00014096098537527108}.\u001b[0m\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 27s - loss: 16.9091 - mse: 16.9091 - mae: 1.5703 - 27s/epoch - 1ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 26s - loss: 16.0164 - mse: 16.0164 - mae: 1.5173 - 26s/epoch - 1ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 24s - loss: 15.7801 - mse: 15.7801 - mae: 1.5047 - 24s/epoch - 1ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 26s - loss: 15.6004 - mse: 15.6004 - mae: 1.4975 - 26s/epoch - 1ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 26s - loss: 15.4793 - mse: 15.4793 - mae: 1.4921 - 26s/epoch - 1ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 25s - loss: 15.3198 - mse: 15.3198 - mae: 1.4896 - 25s/epoch - 1ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 25s - loss: 15.2338 - mse: 15.2338 - mae: 1.4847 - 25s/epoch - 1ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 25s - loss: 15.1743 - mse: 15.1743 - mae: 1.4831 - 25s/epoch - 1ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 26s - loss: 15.0810 - mse: 15.0810 - mae: 1.4781 - 26s/epoch - 1ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 26s - loss: 15.0022 - mse: 15.0022 - mae: 1.4750 - 26s/epoch - 1ms/step\n",
            "Score for fold 1: loss of 25.031461715698242\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 26s - loss: 14.9047 - mse: 14.9047 - mae: 1.4724 - 26s/epoch - 1ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 26s - loss: 14.8526 - mse: 14.8526 - mae: 1.4709 - 26s/epoch - 1ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 26s - loss: 14.8225 - mse: 14.8225 - mae: 1.4685 - 26s/epoch - 1ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 27s - loss: 14.7680 - mse: 14.7680 - mae: 1.4671 - 27s/epoch - 1ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 27s - loss: 14.7540 - mse: 14.7540 - mae: 1.4663 - 27s/epoch - 1ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 28s - loss: 14.7463 - mse: 14.7463 - mae: 1.4647 - 28s/epoch - 1ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 28s - loss: 14.7092 - mse: 14.7092 - mae: 1.4618 - 28s/epoch - 1ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 27s - loss: 14.6634 - mse: 14.6634 - mae: 1.4613 - 27s/epoch - 1ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 27s - loss: 14.6120 - mse: 14.6120 - mae: 1.4597 - 27s/epoch - 1ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 27s - loss: 14.5937 - mse: 14.5937 - mae: 1.4585 - 27s/epoch - 1ms/step\n",
            "Score for fold 2: loss of 10.703873634338379\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 26s - loss: 14.5531 - mse: 14.5531 - mae: 1.4573 - 26s/epoch - 1ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 26s - loss: 14.5356 - mse: 14.5356 - mae: 1.4569 - 26s/epoch - 1ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 27s - loss: 14.5030 - mse: 14.5030 - mae: 1.4562 - 27s/epoch - 1ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 26s - loss: 14.5161 - mse: 14.5161 - mae: 1.4556 - 26s/epoch - 1ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 27s - loss: 14.4905 - mse: 14.4905 - mae: 1.4556 - 27s/epoch - 1ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 27s - loss: 14.4519 - mse: 14.4519 - mae: 1.4540 - 27s/epoch - 1ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 26s - loss: 14.4274 - mse: 14.4274 - mae: 1.4538 - 26s/epoch - 1ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 27s - loss: 14.3855 - mse: 14.3855 - mae: 1.4538 - 27s/epoch - 1ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 27s - loss: 14.3807 - mse: 14.3807 - mae: 1.4512 - 27s/epoch - 1ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 27s - loss: 14.3758 - mse: 14.3758 - mae: 1.4515 - 27s/epoch - 1ms/step\n",
            "Score for fold 3: loss of 10.469969749450684\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-10-01 04:29:58,111]\u001b[0m Finished trial#22 resulted in value: 14.375819206237793. Current best value is 14.375819206237793 with parameters: {'activation': 'relu', 'optimizer': 'adam', 'num_hidden_layer': 3, 'num_hidden_unit': 45, 'lear  ning_rate': 0.00019156770909701436}.\u001b[0m\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 27s - loss: 17.7164 - mse: 17.7164 - mae: 1.6175 - 27s/epoch - 1ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 27s - loss: 16.6367 - mse: 16.6367 - mae: 1.5514 - 27s/epoch - 1ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 27s - loss: 16.3187 - mse: 16.3187 - mae: 1.5318 - 27s/epoch - 1ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 27s - loss: 16.1288 - mse: 16.1288 - mae: 1.5223 - 27s/epoch - 1ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 23s - loss: 16.0115 - mse: 16.0115 - mae: 1.5155 - 23s/epoch - 1ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 26s - loss: 15.9202 - mse: 15.9202 - mae: 1.5097 - 26s/epoch - 1ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 26s - loss: 15.8454 - mse: 15.8454 - mae: 1.5059 - 26s/epoch - 1ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 27s - loss: 15.7688 - mse: 15.7688 - mae: 1.5021 - 27s/epoch - 1ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 27s - loss: 15.7197 - mse: 15.7197 - mae: 1.4984 - 27s/epoch - 1ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 26s - loss: 15.6740 - mse: 15.6740 - mae: 1.4951 - 26s/epoch - 1ms/step\n",
            "Score for fold 1: loss of 26.551427841186523\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 27s - loss: 15.6225 - mse: 15.6225 - mae: 1.4937 - 27s/epoch - 1ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 28s - loss: 15.5835 - mse: 15.5835 - mae: 1.4917 - 28s/epoch - 1ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 27s - loss: 15.5127 - mse: 15.5127 - mae: 1.4900 - 27s/epoch - 1ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 27s - loss: 15.4914 - mse: 15.4914 - mae: 1.4883 - 27s/epoch - 1ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 28s - loss: 15.4453 - mse: 15.4453 - mae: 1.4854 - 28s/epoch - 1ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 27s - loss: 15.4165 - mse: 15.4165 - mae: 1.4845 - 27s/epoch - 1ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 28s - loss: 15.3468 - mse: 15.3468 - mae: 1.4836 - 28s/epoch - 1ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 24s - loss: 15.3133 - mse: 15.3133 - mae: 1.4816 - 24s/epoch - 1ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 27s - loss: 15.2756 - mse: 15.2756 - mae: 1.4816 - 27s/epoch - 1ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 28s - loss: 15.2370 - mse: 15.2370 - mae: 1.4787 - 28s/epoch - 1ms/step\n",
            "Score for fold 2: loss of 11.073234558105469\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 28s - loss: 15.2039 - mse: 15.2039 - mae: 1.4792 - 28s/epoch - 1ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 27s - loss: 15.1596 - mse: 15.1596 - mae: 1.4773 - 27s/epoch - 1ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 26s - loss: 15.1498 - mse: 15.1498 - mae: 1.4754 - 26s/epoch - 1ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 28s - loss: 15.1015 - mse: 15.1015 - mae: 1.4748 - 28s/epoch - 1ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 27s - loss: 15.0846 - mse: 15.0846 - mae: 1.4731 - 27s/epoch - 1ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 28s - loss: 15.0383 - mse: 15.0383 - mae: 1.4731 - 28s/epoch - 1ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 28s - loss: 15.0163 - mse: 15.0163 - mae: 1.4718 - 28s/epoch - 1ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 28s - loss: 14.9909 - mse: 14.9909 - mae: 1.4707 - 28s/epoch - 1ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 27s - loss: 14.9688 - mse: 14.9688 - mae: 1.4699 - 27s/epoch - 1ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 28s - loss: 14.9481 - mse: 14.9481 - mae: 1.4687 - 28s/epoch - 1ms/step\n",
            "Score for fold 3: loss of 10.778486251831055\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-10-01 04:43:36,013]\u001b[0m Finished trial#23 resulted in value: 14.9481201171875. Current best value is 14.375819206237793 with parameters: {'activation': 'relu', 'optimizer': 'adam', 'num_hidden_layer': 3, 'num_hidden_unit': 45, 'lear  ning_rate': 0.00019156770909701436}.\u001b[0m\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 24s - loss: 16.6350 - mse: 16.6350 - mae: 1.5528 - 24s/epoch - 1ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 23s - loss: 15.9046 - mse: 15.9046 - mae: 1.5118 - 23s/epoch - 1ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 24s - loss: 15.7134 - mse: 15.7134 - mae: 1.5041 - 24s/epoch - 1ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 24s - loss: 15.5562 - mse: 15.5562 - mae: 1.5023 - 24s/epoch - 1ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 24s - loss: 15.4599 - mse: 15.4599 - mae: 1.4995 - 24s/epoch - 1ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 23s - loss: 15.4026 - mse: 15.4026 - mae: 1.4967 - 23s/epoch - 1ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 22s - loss: 15.3383 - mse: 15.3383 - mae: 1.4963 - 22s/epoch - 1ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 23s - loss: 15.2765 - mse: 15.2765 - mae: 1.4946 - 23s/epoch - 1ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 23s - loss: 15.2588 - mse: 15.2588 - mae: 1.4948 - 23s/epoch - 1ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 22s - loss: 15.2388 - mse: 15.2388 - mae: 1.4934 - 22s/epoch - 1ms/step\n",
            "Score for fold 1: loss of 25.3197078704834\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 23s - loss: 15.1882 - mse: 15.1882 - mae: 1.4914 - 23s/epoch - 1ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 23s - loss: 15.1274 - mse: 15.1274 - mae: 1.4913 - 23s/epoch - 1ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 23s - loss: 15.1005 - mse: 15.1005 - mae: 1.4890 - 23s/epoch - 1ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 23s - loss: 15.0218 - mse: 15.0218 - mae: 1.4903 - 23s/epoch - 1ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 23s - loss: 15.0518 - mse: 15.0518 - mae: 1.4871 - 23s/epoch - 1ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 23s - loss: 15.0160 - mse: 15.0160 - mae: 1.4867 - 23s/epoch - 1ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 23s - loss: 14.9491 - mse: 14.9491 - mae: 1.4852 - 23s/epoch - 1ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 23s - loss: 14.9461 - mse: 14.9461 - mae: 1.4846 - 23s/epoch - 1ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 23s - loss: 14.9099 - mse: 14.9099 - mae: 1.4843 - 23s/epoch - 1ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 23s - loss: 14.9202 - mse: 14.9202 - mae: 1.4834 - 23s/epoch - 1ms/step\n",
            "Score for fold 2: loss of 10.724599838256836\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 24s - loss: 14.8579 - mse: 14.8580 - mae: 1.4836 - 24s/epoch - 1ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 24s - loss: 14.8516 - mse: 14.8516 - mae: 1.4809 - 24s/epoch - 1ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 22s - loss: 14.8499 - mse: 14.8499 - mae: 1.4821 - 22s/epoch - 1ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 24s - loss: 14.8430 - mse: 14.8430 - mae: 1.4833 - 24s/epoch - 1ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 24s - loss: 14.8071 - mse: 14.8071 - mae: 1.4801 - 24s/epoch - 1ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 23s - loss: 14.8243 - mse: 14.8243 - mae: 1.4808 - 23s/epoch - 1ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 23s - loss: 14.8172 - mse: 14.8172 - mae: 1.4786 - 23s/epoch - 1ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 24s - loss: 14.7768 - mse: 14.7768 - mae: 1.4799 - 24s/epoch - 1ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 24s - loss: 14.7663 - mse: 14.7663 - mae: 1.4793 - 24s/epoch - 1ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 24s - loss: 14.7799 - mse: 14.7799 - mae: 1.4784 - 24s/epoch - 1ms/step\n",
            "Score for fold 3: loss of 10.581694602966309\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-10-01 04:55:25,012]\u001b[0m Finished trial#24 resulted in value: 14.779866218566895. Current best value is 14.375819206237793 with parameters: {'activation': 'relu', 'optimizer': 'adam', 'num_hidden_layer': 3, 'num_hidden_unit': 45, 'lear  ning_rate': 0.00019156770909701436}.\u001b[0m\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 31s - loss: 16.5450 - mse: 16.5450 - mae: 1.5438 - 31s/epoch - 2ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 30s - loss: 15.8597 - mse: 15.8597 - mae: 1.5039 - 30s/epoch - 2ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 31s - loss: 15.6530 - mse: 15.6530 - mae: 1.4920 - 31s/epoch - 2ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 29s - loss: 15.3494 - mse: 15.3494 - mae: 1.4824 - 29s/epoch - 1ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 29s - loss: 15.1528 - mse: 15.1528 - mae: 1.4761 - 29s/epoch - 1ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 32s - loss: 14.9833 - mse: 14.9833 - mae: 1.4703 - 32s/epoch - 2ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 29s - loss: 14.8281 - mse: 14.8281 - mae: 1.4655 - 29s/epoch - 1ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 29s - loss: 14.7701 - mse: 14.7701 - mae: 1.4619 - 29s/epoch - 1ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 30s - loss: 14.6253 - mse: 14.6253 - mae: 1.4594 - 30s/epoch - 1ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 30s - loss: 14.6166 - mse: 14.6166 - mae: 1.4569 - 30s/epoch - 2ms/step\n",
            "Score for fold 1: loss of 25.950607299804688\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 29s - loss: 14.5148 - mse: 14.5148 - mae: 1.4535 - 29s/epoch - 1ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 30s - loss: 14.4052 - mse: 14.4052 - mae: 1.4524 - 30s/epoch - 2ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 31s - loss: 14.3773 - mse: 14.3773 - mae: 1.4506 - 31s/epoch - 2ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 32s - loss: 14.3536 - mse: 14.3536 - mae: 1.4494 - 32s/epoch - 2ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 32s - loss: 14.2869 - mse: 14.2869 - mae: 1.4473 - 32s/epoch - 2ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 33s - loss: 14.1729 - mse: 14.1729 - mae: 1.4472 - 33s/epoch - 2ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 32s - loss: 14.1946 - mse: 14.1946 - mae: 1.4455 - 32s/epoch - 2ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 31s - loss: 14.0241 - mse: 14.0241 - mae: 1.4450 - 31s/epoch - 2ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 32s - loss: 13.9787 - mse: 13.9787 - mae: 1.4437 - 32s/epoch - 2ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 31s - loss: 13.9398 - mse: 13.9398 - mae: 1.4424 - 31s/epoch - 2ms/step\n",
            "Score for fold 2: loss of 11.188611030578613\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 33s - loss: 13.8127 - mse: 13.8127 - mae: 1.4421 - 33s/epoch - 2ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 33s - loss: 13.8105 - mse: 13.8105 - mae: 1.4418 - 33s/epoch - 2ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 33s - loss: 13.7236 - mse: 13.7236 - mae: 1.4401 - 33s/epoch - 2ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 32s - loss: 13.6817 - mse: 13.6817 - mae: 1.4390 - 32s/epoch - 2ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 33s - loss: 13.5970 - mse: 13.5970 - mae: 1.4386 - 33s/epoch - 2ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 31s - loss: 13.4321 - mse: 13.4321 - mae: 1.4371 - 31s/epoch - 2ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 30s - loss: 13.4919 - mse: 13.4919 - mae: 1.4358 - 30s/epoch - 1ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 29s - loss: 13.3432 - mse: 13.3432 - mae: 1.4353 - 29s/epoch - 1ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 29s - loss: 13.1350 - mse: 13.1350 - mae: 1.4337 - 29s/epoch - 1ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 29s - loss: 13.0602 - mse: 13.0602 - mae: 1.4339 - 29s/epoch - 1ms/step\n",
            "Score for fold 3: loss of 11.275287628173828\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-10-01 05:10:58,180]\u001b[0m Finished trial#25 resulted in value: 13.060222625732422. Current best value is 13.060222625732422 with parameters: {'activation': 'relu', 'optimizer': 'adam', 'num_hidden_layer': 4, 'num_hidden_unit': 59, 'lear  ning_rate': 0.00023139516662104315}.\u001b[0m\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 33s - loss: 16.4618 - mse: 16.4618 - mae: 1.5381 - 33s/epoch - 2ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 32s - loss: 15.7723 - mse: 15.7723 - mae: 1.4992 - 32s/epoch - 2ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 32s - loss: 15.3898 - mse: 15.3898 - mae: 1.4866 - 32s/epoch - 2ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 31s - loss: 15.1479 - mse: 15.1479 - mae: 1.4773 - 31s/epoch - 2ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 31s - loss: 15.0597 - mse: 15.0597 - mae: 1.4715 - 31s/epoch - 2ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 32s - loss: 14.8688 - mse: 14.8688 - mae: 1.4657 - 32s/epoch - 2ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 32s - loss: 14.7553 - mse: 14.7553 - mae: 1.4630 - 32s/epoch - 2ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 31s - loss: 14.6679 - mse: 14.6679 - mae: 1.4586 - 31s/epoch - 2ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 31s - loss: 14.5760 - mse: 14.5760 - mae: 1.4576 - 31s/epoch - 2ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 33s - loss: 14.5138 - mse: 14.5138 - mae: 1.4543 - 33s/epoch - 2ms/step\n",
            "Score for fold 1: loss of 24.062341690063477\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 33s - loss: 14.3653 - mse: 14.3653 - mae: 1.4514 - 33s/epoch - 2ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 33s - loss: 14.3177 - mse: 14.3177 - mae: 1.4507 - 33s/epoch - 2ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 31s - loss: 14.2223 - mse: 14.2223 - mae: 1.4483 - 31s/epoch - 2ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 31s - loss: 14.0875 - mse: 14.0875 - mae: 1.4463 - 31s/epoch - 2ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 33s - loss: 14.1028 - mse: 14.1028 - mae: 1.4458 - 33s/epoch - 2ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 33s - loss: 13.8445 - mse: 13.8445 - mae: 1.4455 - 33s/epoch - 2ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 33s - loss: 13.7380 - mse: 13.7380 - mae: 1.4414 - 33s/epoch - 2ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 31s - loss: 13.4232 - mse: 13.4232 - mae: 1.4424 - 31s/epoch - 2ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 31s - loss: 13.3779 - mse: 13.3779 - mae: 1.4426 - 31s/epoch - 2ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 30s - loss: 12.9870 - mse: 12.9870 - mae: 1.4422 - 30s/epoch - 2ms/step\n",
            "Score for fold 2: loss of 10.38895034790039\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 30s - loss: 12.8655 - mse: 12.8655 - mae: 1.4409 - 30s/epoch - 1ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 33s - loss: 12.1761 - mse: 12.1761 - mae: 1.4391 - 33s/epoch - 2ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 33s - loss: 11.9369 - mse: 11.9369 - mae: 1.4383 - 33s/epoch - 2ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 33s - loss: 11.9146 - mse: 11.9146 - mae: 1.4359 - 33s/epoch - 2ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 33s - loss: 11.8466 - mse: 11.8466 - mae: 1.4345 - 33s/epoch - 2ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 33s - loss: 10.9748 - mse: 10.9748 - mae: 1.4340 - 33s/epoch - 2ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 32s - loss: 10.0416 - mse: 10.0416 - mae: 1.4335 - 32s/epoch - 2ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 33s - loss: 10.0633 - mse: 10.0633 - mae: 1.4322 - 33s/epoch - 2ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 34s - loss: 10.7180 - mse: 10.7180 - mae: 1.4322 - 34s/epoch - 2ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 33s - loss: 10.7644 - mse: 10.7644 - mae: 1.4313 - 33s/epoch - 2ms/step\n",
            "Score for fold 3: loss of 10.007440567016602\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-10-01 05:27:13,491]\u001b[0m Finished trial#26 resulted in value: 10.764413833618164. Current best value is 10.764413833618164 with parameters: {'activation': 'relu', 'optimizer': 'adam', 'num_hidden_layer': 5, 'num_hidden_unit': 60, 'lear  ning_rate': 0.0003135813689247155}.\u001b[0m\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 33s - loss: 16.8675 - mse: 16.8675 - mae: 1.5653 - 33s/epoch - 2ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 29s - loss: 16.0025 - mse: 16.0025 - mae: 1.5323 - 29s/epoch - 1ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 30s - loss: 15.6642 - mse: 15.6642 - mae: 1.5090 - 30s/epoch - 2ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 31s - loss: 15.4942 - mse: 15.4942 - mae: 1.4966 - 31s/epoch - 2ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 33s - loss: 15.3319 - mse: 15.3319 - mae: 1.4878 - 33s/epoch - 2ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 32s - loss: 15.2363 - mse: 15.2363 - mae: 1.4815 - 32s/epoch - 2ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 30s - loss: 15.1341 - mse: 15.1341 - mae: 1.4758 - 30s/epoch - 1ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 32s - loss: 14.9886 - mse: 14.9886 - mae: 1.4711 - 32s/epoch - 2ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 30s - loss: 14.8687 - mse: 14.8687 - mae: 1.4670 - 30s/epoch - 2ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 30s - loss: 14.7368 - mse: 14.7368 - mae: 1.4664 - 30s/epoch - 1ms/step\n",
            "Score for fold 1: loss of 23.445676803588867\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 29s - loss: 14.5420 - mse: 14.5420 - mae: 1.4638 - 29s/epoch - 1ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 30s - loss: 14.4979 - mse: 14.4979 - mae: 1.4625 - 30s/epoch - 2ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 31s - loss: 14.2899 - mse: 14.2899 - mae: 1.4581 - 31s/epoch - 2ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 31s - loss: 14.0582 - mse: 14.0582 - mae: 1.4572 - 31s/epoch - 2ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 31s - loss: 14.1063 - mse: 14.1063 - mae: 1.4544 - 31s/epoch - 2ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 34s - loss: 13.9571 - mse: 13.9571 - mae: 1.4536 - 34s/epoch - 2ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 32s - loss: 13.8261 - mse: 13.8261 - mae: 1.4503 - 32s/epoch - 2ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 31s - loss: 13.6683 - mse: 13.6683 - mae: 1.4496 - 31s/epoch - 2ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 33s - loss: 13.7666 - mse: 13.7666 - mae: 1.4497 - 33s/epoch - 2ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 33s - loss: 13.5331 - mse: 13.5331 - mae: 1.4462 - 33s/epoch - 2ms/step\n",
            "Score for fold 2: loss of 10.793086051940918\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 32s - loss: 13.4234 - mse: 13.4234 - mae: 1.4462 - 32s/epoch - 2ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 32s - loss: 13.2607 - mse: 13.2607 - mae: 1.4454 - 32s/epoch - 2ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 32s - loss: 13.2549 - mse: 13.2549 - mae: 1.4440 - 32s/epoch - 2ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 31s - loss: 13.0643 - mse: 13.0643 - mae: 1.4404 - 31s/epoch - 2ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 31s - loss: 13.1556 - mse: 13.1556 - mae: 1.4429 - 31s/epoch - 2ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 34s - loss: 12.8842 - mse: 12.8842 - mae: 1.4410 - 34s/epoch - 2ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 31s - loss: 13.0566 - mse: 13.0566 - mae: 1.4394 - 31s/epoch - 2ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 34s - loss: 12.8118 - mse: 12.8118 - mae: 1.4379 - 34s/epoch - 2ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 34s - loss: 12.9859 - mse: 12.9859 - mae: 1.4385 - 34s/epoch - 2ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 34s - loss: 12.6599 - mse: 12.6599 - mae: 1.4379 - 34s/epoch - 2ms/step\n",
            "Score for fold 3: loss of 10.495189666748047\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-10-01 05:43:15,171]\u001b[0m Finished trial#27 resulted in value: 12.659872055053711. Current best value is 10.764413833618164 with parameters: {'activation': 'relu', 'optimizer': 'adam', 'num_hidden_layer': 5, 'num_hidden_unit': 60, 'lear  ning_rate': 0.0003135813689247155}.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 35s - loss: 16.7354 - mse: 16.7354 - mae: 1.5662 - 35s/epoch - 2ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 31s - loss: 16.1867 - mse: 16.1867 - mae: 1.5279 - 31s/epoch - 2ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 32s - loss: 15.9657 - mse: 15.9657 - mae: 1.5248 - 32s/epoch - 2ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 32s - loss: 15.9423 - mse: 15.9423 - mae: 1.5200 - 32s/epoch - 2ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 34s - loss: 15.9394 - mse: 15.9394 - mae: 1.5244 - 34s/epoch - 2ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 34s - loss: 15.7307 - mse: 15.7307 - mae: 1.5208 - 34s/epoch - 2ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 35s - loss: 15.5323 - mse: 15.5323 - mae: 1.5112 - 35s/epoch - 2ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 36s - loss: 15.4111 - mse: 15.4111 - mae: 1.5065 - 36s/epoch - 2ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 34s - loss: 15.4923 - mse: 15.4923 - mae: 1.5110 - 34s/epoch - 2ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 34s - loss: 15.4440 - mse: 15.4440 - mae: 1.5130 - 34s/epoch - 2ms/step\n",
            "Score for fold 1: loss of 23.97101593017578\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 36s - loss: 15.4558 - mse: 15.4558 - mae: 1.5138 - 36s/epoch - 2ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 36s - loss: 15.4074 - mse: 15.4074 - mae: 1.5122 - 36s/epoch - 2ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 37s - loss: 15.1714 - mse: 15.1714 - mae: 1.5041 - 37s/epoch - 2ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 35s - loss: 15.5818 - mse: 15.5818 - mae: 1.5075 - 35s/epoch - 2ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 34s - loss: 15.6080 - mse: 15.6080 - mae: 1.5078 - 34s/epoch - 2ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 37s - loss: 15.2323 - mse: 15.2323 - mae: 1.5080 - 37s/epoch - 2ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 34s - loss: 15.1034 - mse: 15.1034 - mae: 1.5012 - 34s/epoch - 2ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 35s - loss: 15.0675 - mse: 15.0675 - mae: 1.5031 - 35s/epoch - 2ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 34s - loss: 15.2558 - mse: 15.2558 - mae: 1.5385 - 34s/epoch - 2ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 35s - loss: 15.2120 - mse: 15.2120 - mae: 1.5106 - 35s/epoch - 2ms/step\n",
            "Score for fold 2: loss of 11.470162391662598\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 35s - loss: 14.9931 - mse: 14.9931 - mae: 1.5077 - 35s/epoch - 2ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 34s - loss: 15.0459 - mse: 15.0459 - mae: 1.5138 - 34s/epoch - 2ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 35s - loss: 15.0625 - mse: 15.0625 - mae: 1.5191 - 35s/epoch - 2ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 36s - loss: 15.0014 - mse: 15.0014 - mae: 1.5118 - 36s/epoch - 2ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 37s - loss: 15.0490 - mse: 15.0490 - mae: 1.5128 - 37s/epoch - 2ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 37s - loss: 15.0803 - mse: 15.0803 - mae: 1.5100 - 37s/epoch - 2ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 35s - loss: 14.9499 - mse: 14.9499 - mae: 1.5070 - 35s/epoch - 2ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 33s - loss: 15.0639 - mse: 15.0639 - mae: 1.5146 - 33s/epoch - 2ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 34s - loss: 14.9565 - mse: 14.9565 - mae: 1.5112 - 34s/epoch - 2ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 35s - loss: 14.7513 - mse: 14.7513 - mae: 1.5079 - 35s/epoch - 2ms/step\n",
            "Score for fold 3: loss of 11.030317306518555\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-10-01 06:00:48,165]\u001b[0m Finished trial#28 resulted in value: 14.751312255859375. Current best value is 10.764413833618164 with parameters: {'activation': 'relu', 'optimizer': 'adam', 'num_hidden_layer': 5, 'num_hidden_unit': 60, 'lear  ning_rate': 0.0003135813689247155}.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 36s - loss: 16.7825 - mse: 16.7825 - mae: 1.5690 - 36s/epoch - 2ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 35s - loss: 15.9105 - mse: 15.9105 - mae: 1.5275 - 35s/epoch - 2ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 35s - loss: 15.6426 - mse: 15.6426 - mae: 1.5080 - 35s/epoch - 2ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 35s - loss: 15.4827 - mse: 15.4827 - mae: 1.4969 - 35s/epoch - 2ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 35s - loss: 15.2697 - mse: 15.2697 - mae: 1.4873 - 35s/epoch - 2ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 34s - loss: 15.1597 - mse: 15.1597 - mae: 1.4823 - 34s/epoch - 2ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 34s - loss: 14.9174 - mse: 14.9174 - mae: 1.4750 - 34s/epoch - 2ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 35s - loss: 14.8801 - mse: 14.8801 - mae: 1.4726 - 35s/epoch - 2ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 35s - loss: 14.7334 - mse: 14.7334 - mae: 1.4711 - 35s/epoch - 2ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 34s - loss: 14.6746 - mse: 14.6746 - mae: 1.4671 - 34s/epoch - 2ms/step\n",
            "Score for fold 1: loss of 23.90768814086914\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 35s - loss: 14.4595 - mse: 14.4595 - mae: 1.4619 - 35s/epoch - 2ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 35s - loss: 14.3371 - mse: 14.3371 - mae: 1.4640 - 35s/epoch - 2ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 34s - loss: 14.2673 - mse: 14.2673 - mae: 1.4628 - 34s/epoch - 2ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 35s - loss: 14.2585 - mse: 14.2585 - mae: 1.4570 - 35s/epoch - 2ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 35s - loss: 14.0344 - mse: 14.0344 - mae: 1.4540 - 35s/epoch - 2ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 35s - loss: 13.9042 - mse: 13.9042 - mae: 1.4546 - 35s/epoch - 2ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 35s - loss: 13.8224 - mse: 13.8224 - mae: 1.4501 - 35s/epoch - 2ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 35s - loss: 13.7057 - mse: 13.7057 - mae: 1.4525 - 35s/epoch - 2ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 36s - loss: 13.5112 - mse: 13.5112 - mae: 1.4494 - 36s/epoch - 2ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 36s - loss: 13.4511 - mse: 13.4511 - mae: 1.4473 - 36s/epoch - 2ms/step\n",
            "Score for fold 2: loss of 10.581388473510742\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 35s - loss: 13.3201 - mse: 13.3201 - mae: 1.4466 - 35s/epoch - 2ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 36s - loss: 13.2947 - mse: 13.2947 - mae: 1.4436 - 36s/epoch - 2ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 36s - loss: 13.1838 - mse: 13.1838 - mae: 1.4438 - 36s/epoch - 2ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 35s - loss: 12.8933 - mse: 12.8933 - mae: 1.4406 - 35s/epoch - 2ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 35s - loss: 13.0030 - mse: 13.0030 - mae: 1.4424 - 35s/epoch - 2ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 35s - loss: 12.7466 - mse: 12.7466 - mae: 1.4398 - 35s/epoch - 2ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 36s - loss: 12.6416 - mse: 12.6416 - mae: 1.4385 - 36s/epoch - 2ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 36s - loss: 12.5888 - mse: 12.5888 - mae: 1.4387 - 36s/epoch - 2ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 36s - loss: 12.4559 - mse: 12.4559 - mae: 1.4363 - 36s/epoch - 2ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 37s - loss: 12.4290 - mse: 12.4290 - mae: 1.4375 - 37s/epoch - 2ms/step\n",
            "Score for fold 3: loss of 10.030274391174316\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-10-01 06:18:36,115]\u001b[0m Finished trial#29 resulted in value: 12.428984642028809. Current best value is 10.764413833618164 with parameters: {'activation': 'relu', 'optimizer': 'adam', 'num_hidden_layer': 5, 'num_hidden_unit': 60, 'lear  ning_rate': 0.0003135813689247155}.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 26s - loss: 17.2316 - mse: 17.2316 - mae: 1.6091 - 26s/epoch - 1ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 25s - loss: 17.1388 - mse: 17.1388 - mae: 1.5879 - 25s/epoch - 1ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 26s - loss: 17.1567 - mse: 17.1567 - mae: 1.5979 - 26s/epoch - 1ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 27s - loss: 16.9904 - mse: 16.9904 - mae: 1.5845 - 27s/epoch - 1ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 27s - loss: 17.1674 - mse: 17.1674 - mae: 1.5917 - 27s/epoch - 1ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 26s - loss: 16.8881 - mse: 16.8881 - mae: 1.5754 - 26s/epoch - 1ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 26s - loss: 17.2479 - mse: 17.2479 - mae: 1.6010 - 26s/epoch - 1ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 27s - loss: 17.2726 - mse: 17.2726 - mae: 1.6084 - 27s/epoch - 1ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 26s - loss: 17.3630 - mse: 17.3630 - mae: 1.6193 - 26s/epoch - 1ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 27s - loss: 17.1326 - mse: 17.1326 - mae: 1.6077 - 27s/epoch - 1ms/step\n",
            "Score for fold 1: loss of 23.844911575317383\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 28s - loss: 17.2082 - mse: 17.2082 - mae: 1.6151 - 28s/epoch - 1ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 29s - loss: 17.0915 - mse: 17.0915 - mae: 1.5989 - 29s/epoch - 1ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 27s - loss: 17.1392 - mse: 17.1392 - mae: 1.6022 - 27s/epoch - 1ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 26s - loss: 16.9766 - mse: 16.9766 - mae: 1.5900 - 26s/epoch - 1ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 26s - loss: 16.8836 - mse: 16.8836 - mae: 1.5926 - 26s/epoch - 1ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 26s - loss: 16.6706 - mse: 16.6706 - mae: 1.5733 - 26s/epoch - 1ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 26s - loss: 16.7802 - mse: 16.7802 - mae: 1.5817 - 26s/epoch - 1ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 27s - loss: 17.0613 - mse: 17.0613 - mae: 1.5970 - 27s/epoch - 1ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 27s - loss: 17.1533 - mse: 17.1533 - mae: 1.6082 - 27s/epoch - 1ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 25s - loss: 17.1900 - mse: 17.1900 - mae: 1.6124 - 25s/epoch - 1ms/step\n",
            "Score for fold 2: loss of 13.173012733459473\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 27s - loss: 17.0678 - mse: 17.0678 - mae: 1.6036 - 27s/epoch - 1ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 28s - loss: 17.4530 - mse: 17.4530 - mae: 1.6317 - 28s/epoch - 1ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 28s - loss: 17.3145 - mse: 17.3145 - mae: 1.6230 - 28s/epoch - 1ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 28s - loss: 17.2532 - mse: 17.2532 - mae: 1.6169 - 28s/epoch - 1ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 27s - loss: 17.4006 - mse: 17.4006 - mae: 1.6407 - 27s/epoch - 1ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 26s - loss: 17.2140 - mse: 17.2140 - mae: 1.6055 - 26s/epoch - 1ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 26s - loss: 17.3029 - mse: 17.3029 - mae: 1.6261 - 26s/epoch - 1ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 26s - loss: 17.2760 - mse: 17.2760 - mae: 1.6253 - 26s/epoch - 1ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 26s - loss: 17.4257 - mse: 17.4257 - mae: 1.6450 - 26s/epoch - 1ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 26s - loss: 17.5403 - mse: 17.5403 - mae: 1.6377 - 26s/epoch - 1ms/step\n",
            "Score for fold 3: loss of 12.609533309936523\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-10-01 06:32:04,718]\u001b[0m Finished trial#30 resulted in value: 17.540279388427734. Current best value is 10.764413833618164 with parameters: {'activation': 'relu', 'optimizer': 'adam', 'num_hidden_layer': 5, 'num_hidden_unit': 60, 'lear  ning_rate': 0.0003135813689247155}.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 33s - loss: 16.9324 - mse: 16.9324 - mae: 1.5690 - 33s/epoch - 2ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 33s - loss: 16.0264 - mse: 16.0264 - mae: 1.5287 - 33s/epoch - 2ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 33s - loss: 15.6737 - mse: 15.6737 - mae: 1.5095 - 33s/epoch - 2ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 32s - loss: 15.5066 - mse: 15.5066 - mae: 1.4965 - 32s/epoch - 2ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 30s - loss: 15.3295 - mse: 15.3295 - mae: 1.4849 - 30s/epoch - 2ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 32s - loss: 15.2103 - mse: 15.2103 - mae: 1.4798 - 32s/epoch - 2ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 31s - loss: 15.0243 - mse: 15.0243 - mae: 1.4735 - 31s/epoch - 2ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 33s - loss: 14.8865 - mse: 14.8865 - mae: 1.4718 - 33s/epoch - 2ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 34s - loss: 14.7475 - mse: 14.7475 - mae: 1.4679 - 34s/epoch - 2ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 35s - loss: 14.5134 - mse: 14.5134 - mae: 1.4663 - 35s/epoch - 2ms/step\n",
            "Score for fold 1: loss of 24.797761917114258\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 35s - loss: 14.4362 - mse: 14.4362 - mae: 1.4655 - 35s/epoch - 2ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 34s - loss: 14.2191 - mse: 14.2191 - mae: 1.4596 - 34s/epoch - 2ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 34s - loss: 14.2234 - mse: 14.2234 - mae: 1.4594 - 34s/epoch - 2ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 34s - loss: 14.1438 - mse: 14.1438 - mae: 1.4570 - 34s/epoch - 2ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 35s - loss: 13.9339 - mse: 13.9339 - mae: 1.4561 - 35s/epoch - 2ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 34s - loss: 13.7968 - mse: 13.7968 - mae: 1.4505 - 34s/epoch - 2ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 37s - loss: 13.8182 - mse: 13.8182 - mae: 1.4506 - 37s/epoch - 2ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 36s - loss: 13.5787 - mse: 13.5787 - mae: 1.4478 - 36s/epoch - 2ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 34s - loss: 13.4639 - mse: 13.4639 - mae: 1.4470 - 34s/epoch - 2ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 33s - loss: 13.5712 - mse: 13.5712 - mae: 1.4474 - 33s/epoch - 2ms/step\n",
            "Score for fold 2: loss of 10.74803638458252\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 37s - loss: 13.1894 - mse: 13.1894 - mae: 1.4428 - 37s/epoch - 2ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 36s - loss: 13.2796 - mse: 13.2796 - mae: 1.4450 - 36s/epoch - 2ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 35s - loss: 13.0469 - mse: 13.0469 - mae: 1.4431 - 35s/epoch - 2ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 35s - loss: 13.0035 - mse: 13.0035 - mae: 1.4411 - 35s/epoch - 2ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 35s - loss: 12.9421 - mse: 12.9421 - mae: 1.4426 - 35s/epoch - 2ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 34s - loss: 13.0125 - mse: 13.0125 - mae: 1.4402 - 34s/epoch - 2ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 34s - loss: 12.6783 - mse: 12.6783 - mae: 1.4379 - 34s/epoch - 2ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 33s - loss: 12.6433 - mse: 12.6433 - mae: 1.4374 - 33s/epoch - 2ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 33s - loss: 12.5060 - mse: 12.5060 - mae: 1.4336 - 33s/epoch - 2ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 31s - loss: 12.5332 - mse: 12.5332 - mae: 1.4354 - 31s/epoch - 2ms/step\n",
            "Score for fold 3: loss of 10.49802017211914\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-10-01 06:49:09,860]\u001b[0m Finished trial#31 resulted in value: 12.53321647644043. Current best value is 10.764413833618164 with parameters: {'activation': 'relu', 'optimizer': 'adam', 'num_hidden_layer': 5, 'num_hidden_unit': 60, 'lear  ning_rate': 0.0003135813689247155}.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 30s - loss: 16.7531 - mse: 16.7531 - mae: 1.5671 - 30s/epoch - 2ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 32s - loss: 15.9546 - mse: 15.9546 - mae: 1.5292 - 32s/epoch - 2ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 37s - loss: 15.6396 - mse: 15.6396 - mae: 1.5079 - 37s/epoch - 2ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 36s - loss: 15.4614 - mse: 15.4614 - mae: 1.4946 - 36s/epoch - 2ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 36s - loss: 15.3018 - mse: 15.3018 - mae: 1.4856 - 36s/epoch - 2ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 34s - loss: 15.2121 - mse: 15.2121 - mae: 1.4791 - 34s/epoch - 2ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 33s - loss: 15.0192 - mse: 15.0192 - mae: 1.4757 - 33s/epoch - 2ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 33s - loss: 14.9238 - mse: 14.9238 - mae: 1.4710 - 33s/epoch - 2ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 35s - loss: 14.7571 - mse: 14.7571 - mae: 1.4673 - 35s/epoch - 2ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 36s - loss: 14.6842 - mse: 14.6842 - mae: 1.4642 - 36s/epoch - 2ms/step\n",
            "Score for fold 1: loss of 23.91924285888672\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 37s - loss: 14.5525 - mse: 14.5525 - mae: 1.4623 - 37s/epoch - 2ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 36s - loss: 14.4726 - mse: 14.4726 - mae: 1.4581 - 36s/epoch - 2ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 37s - loss: 14.3080 - mse: 14.3080 - mae: 1.4549 - 37s/epoch - 2ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 37s - loss: 14.1664 - mse: 14.1664 - mae: 1.4558 - 37s/epoch - 2ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 38s - loss: 13.9832 - mse: 13.9832 - mae: 1.4504 - 38s/epoch - 2ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 38s - loss: 13.9013 - mse: 13.9013 - mae: 1.4531 - 38s/epoch - 2ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 37s - loss: 13.8391 - mse: 13.8391 - mae: 1.4484 - 37s/epoch - 2ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 36s - loss: 13.7327 - mse: 13.7327 - mae: 1.4496 - 36s/epoch - 2ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 38s - loss: 13.4723 - mse: 13.4723 - mae: 1.4468 - 38s/epoch - 2ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 38s - loss: 13.5046 - mse: 13.5046 - mae: 1.4470 - 38s/epoch - 2ms/step\n",
            "Score for fold 2: loss of 10.82261848449707\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 37s - loss: 13.5151 - mse: 13.5151 - mae: 1.4449 - 37s/epoch - 2ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 37s - loss: 13.2578 - mse: 13.2578 - mae: 1.4427 - 37s/epoch - 2ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 36s - loss: 13.3686 - mse: 13.3686 - mae: 1.4444 - 36s/epoch - 2ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 36s - loss: 13.3123 - mse: 13.3123 - mae: 1.4451 - 36s/epoch - 2ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 35s - loss: 13.1204 - mse: 13.1204 - mae: 1.4406 - 35s/epoch - 2ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 36s - loss: 13.0253 - mse: 13.0253 - mae: 1.4418 - 36s/epoch - 2ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 36s - loss: 12.7821 - mse: 12.7821 - mae: 1.4373 - 36s/epoch - 2ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 37s - loss: 12.7377 - mse: 12.7377 - mae: 1.4365 - 37s/epoch - 2ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 35s - loss: 12.6681 - mse: 12.6681 - mae: 1.4341 - 35s/epoch - 2ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 34s - loss: 12.5068 - mse: 12.5068 - mae: 1.4346 - 34s/epoch - 2ms/step\n",
            "Score for fold 3: loss of 10.515625\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-10-01 07:07:12,572]\u001b[0m Finished trial#32 resulted in value: 12.50677490234375. Current best value is 10.764413833618164 with parameters: {'activation': 'relu', 'optimizer': 'adam', 'num_hidden_layer': 5, 'num_hidden_unit': 60, 'lear  ning_rate': 0.0003135813689247155}.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 33s - loss: 16.6505 - mse: 16.6505 - mae: 1.5429 - 33s/epoch - 2ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 31s - loss: 16.0181 - mse: 16.0181 - mae: 1.5043 - 31s/epoch - 2ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 31s - loss: 15.8946 - mse: 15.8946 - mae: 1.5003 - 31s/epoch - 2ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 30s - loss: 15.7584 - mse: 15.7584 - mae: 1.4938 - 30s/epoch - 1ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 30s - loss: 15.6632 - mse: 15.6632 - mae: 1.4926 - 30s/epoch - 2ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 30s - loss: 15.6173 - mse: 15.6173 - mae: 1.4969 - 30s/epoch - 2ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 31s - loss: 15.5971 - mse: 15.5971 - mae: 1.4948 - 31s/epoch - 2ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 31s - loss: 15.5660 - mse: 15.5660 - mae: 1.5012 - 31s/epoch - 2ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 32s - loss: 15.4953 - mse: 15.4953 - mae: 1.4973 - 32s/epoch - 2ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 31s - loss: 15.5324 - mse: 15.5324 - mae: 1.4953 - 31s/epoch - 2ms/step\n",
            "Score for fold 1: loss of 24.439453125\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 31s - loss: 15.4361 - mse: 15.4361 - mae: 1.4973 - 31s/epoch - 2ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 31s - loss: 15.5161 - mse: 15.5161 - mae: 1.5021 - 31s/epoch - 2ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 31s - loss: 15.4457 - mse: 15.4457 - mae: 1.4880 - 31s/epoch - 2ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 31s - loss: 15.3817 - mse: 15.3817 - mae: 1.4904 - 31s/epoch - 2ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 31s - loss: 15.3434 - mse: 15.3434 - mae: 1.4912 - 31s/epoch - 2ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 31s - loss: 15.4026 - mse: 15.4026 - mae: 1.4955 - 31s/epoch - 2ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 32s - loss: 15.3790 - mse: 15.3790 - mae: 1.5021 - 32s/epoch - 2ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 31s - loss: 15.4426 - mse: 15.4426 - mae: 1.4999 - 31s/epoch - 2ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 31s - loss: 15.4693 - mse: 15.4693 - mae: 1.4905 - 31s/epoch - 2ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 31s - loss: 15.4728 - mse: 15.4728 - mae: 1.4913 - 31s/epoch - 2ms/step\n",
            "Score for fold 2: loss of 11.61843490600586\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 31s - loss: 15.4400 - mse: 15.4400 - mae: 1.4928 - 31s/epoch - 2ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 30s - loss: 15.4464 - mse: 15.4464 - mae: 1.4898 - 30s/epoch - 2ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 31s - loss: 15.3769 - mse: 15.3769 - mae: 1.4885 - 31s/epoch - 2ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 33s - loss: 15.3760 - mse: 15.3760 - mae: 1.4922 - 33s/epoch - 2ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 32s - loss: 15.3791 - mse: 15.3791 - mae: 1.4895 - 32s/epoch - 2ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 32s - loss: 15.2860 - mse: 15.2860 - mae: 1.4912 - 32s/epoch - 2ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 31s - loss: 15.2335 - mse: 15.2335 - mae: 1.4879 - 31s/epoch - 2ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 31s - loss: 15.2053 - mse: 15.2053 - mae: 1.4919 - 31s/epoch - 2ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 30s - loss: 15.1608 - mse: 15.1608 - mae: 1.4948 - 30s/epoch - 1ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 29s - loss: 15.2576 - mse: 15.2576 - mae: 1.4886 - 29s/epoch - 1ms/step\n",
            "Score for fold 3: loss of 11.448383331298828\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-10-01 07:22:56,739]\u001b[0m Finished trial#33 resulted in value: 15.2576322555542. Current best value is 10.764413833618164 with parameters: {'activation': 'relu', 'optimizer': 'adam', 'num_hidden_layer': 5, 'num_hidden_unit': 60, 'lear  ning_rate': 0.0003135813689247155}.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 40s - loss: 16.7663 - mse: 16.7663 - mae: 1.5751 - 40s/epoch - 2ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 40s - loss: 15.9939 - mse: 15.9939 - mae: 1.5320 - 40s/epoch - 2ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 37s - loss: 15.7239 - mse: 15.7239 - mae: 1.5070 - 37s/epoch - 2ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 37s - loss: 15.4662 - mse: 15.4662 - mae: 1.4959 - 37s/epoch - 2ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 37s - loss: 15.2977 - mse: 15.2977 - mae: 1.4854 - 37s/epoch - 2ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 38s - loss: 15.1453 - mse: 15.1453 - mae: 1.4807 - 38s/epoch - 2ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 37s - loss: 15.0153 - mse: 15.0153 - mae: 1.4768 - 37s/epoch - 2ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 38s - loss: 14.8124 - mse: 14.8124 - mae: 1.4709 - 38s/epoch - 2ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 41s - loss: 14.7100 - mse: 14.7100 - mae: 1.4684 - 41s/epoch - 2ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 48s - loss: 14.5608 - mse: 14.5608 - mae: 1.4673 - 48s/epoch - 2ms/step\n",
            "Score for fold 1: loss of 23.9591007232666\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 66s - loss: 14.4542 - mse: 14.4542 - mae: 1.4650 - 66s/epoch - 3ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 72s - loss: 14.2584 - mse: 14.2584 - mae: 1.4589 - 72s/epoch - 4ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 73s - loss: 14.2378 - mse: 14.2378 - mae: 1.4602 - 73s/epoch - 4ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 59s - loss: 14.2004 - mse: 14.2004 - mae: 1.4583 - 59s/epoch - 3ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 53s - loss: 14.0685 - mse: 14.0685 - mae: 1.4565 - 53s/epoch - 3ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 51s - loss: 13.8529 - mse: 13.8529 - mae: 1.4547 - 51s/epoch - 3ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 49s - loss: 13.7291 - mse: 13.7291 - mae: 1.4534 - 49s/epoch - 2ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 51s - loss: 13.8501 - mse: 13.8501 - mae: 1.4552 - 51s/epoch - 3ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 48s - loss: 13.5149 - mse: 13.5149 - mae: 1.4516 - 48s/epoch - 2ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 41s - loss: 13.4350 - mse: 13.4350 - mae: 1.4517 - 41s/epoch - 2ms/step\n",
            "Score for fold 2: loss of 10.829529762268066\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 40s - loss: 13.2398 - mse: 13.2398 - mae: 1.4480 - 40s/epoch - 2ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 38s - loss: 13.4064 - mse: 13.4064 - mae: 1.4459 - 38s/epoch - 2ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 41s - loss: 13.1859 - mse: 13.1859 - mae: 1.4448 - 41s/epoch - 2ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 41s - loss: 13.1414 - mse: 13.1414 - mae: 1.4436 - 41s/epoch - 2ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 40s - loss: 13.0157 - mse: 13.0157 - mae: 1.4436 - 40s/epoch - 2ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 41s - loss: 12.9206 - mse: 12.9206 - mae: 1.4430 - 41s/epoch - 2ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 40s - loss: 12.8697 - mse: 12.8697 - mae: 1.4434 - 40s/epoch - 2ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 41s - loss: 12.9426 - mse: 12.9426 - mae: 1.4433 - 41s/epoch - 2ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 42s - loss: 12.7969 - mse: 12.7969 - mae: 1.4415 - 42s/epoch - 2ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 41s - loss: 12.6050 - mse: 12.6050 - mae: 1.4431 - 41s/epoch - 2ms/step\n",
            "Score for fold 3: loss of 10.428834915161133\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-10-01 07:45:51,631]\u001b[0m Finished trial#34 resulted in value: 12.604970932006836. Current best value is 10.764413833618164 with parameters: {'activation': 'relu', 'optimizer': 'adam', 'num_hidden_layer': 5, 'num_hidden_unit': 60, 'lear  ning_rate': 0.0003135813689247155}.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 41s - loss: 16.6727 - mse: 16.6727 - mae: 1.5720 - 41s/epoch - 2ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 41s - loss: 15.9530 - mse: 15.9530 - mae: 1.5245 - 41s/epoch - 2ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 43s - loss: 15.7294 - mse: 15.7294 - mae: 1.5132 - 43s/epoch - 2ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 43s - loss: 15.5504 - mse: 15.5504 - mae: 1.5048 - 43s/epoch - 2ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 43s - loss: 15.4908 - mse: 15.4908 - mae: 1.4998 - 43s/epoch - 2ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 42s - loss: 15.3660 - mse: 15.3660 - mae: 1.4937 - 42s/epoch - 2ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 42s - loss: 15.1915 - mse: 15.1915 - mae: 1.4895 - 42s/epoch - 2ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 41s - loss: 15.1639 - mse: 15.1639 - mae: 1.4879 - 41s/epoch - 2ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 45s - loss: 14.9347 - mse: 14.9347 - mae: 1.4843 - 45s/epoch - 2ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 47s - loss: 14.8275 - mse: 14.8275 - mae: 1.4780 - 47s/epoch - 2ms/step\n",
            "Score for fold 1: loss of 23.896867752075195\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 46s - loss: 14.8383 - mse: 14.8383 - mae: 1.4790 - 46s/epoch - 2ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 44s - loss: 14.6989 - mse: 14.6989 - mae: 1.4797 - 44s/epoch - 2ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 43s - loss: 14.5285 - mse: 14.5285 - mae: 1.4757 - 43s/epoch - 2ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 43s - loss: 14.4583 - mse: 14.4583 - mae: 1.4738 - 43s/epoch - 2ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 42s - loss: 14.4499 - mse: 14.4499 - mae: 1.4736 - 42s/epoch - 2ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 42s - loss: 14.3560 - mse: 14.3560 - mae: 1.4718 - 42s/epoch - 2ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 40s - loss: 14.2796 - mse: 14.2796 - mae: 1.4718 - 40s/epoch - 2ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 41s - loss: 14.2414 - mse: 14.2414 - mae: 1.4732 - 41s/epoch - 2ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 40s - loss: 13.7729 - mse: 13.7729 - mae: 1.4655 - 40s/epoch - 2ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 39s - loss: 13.7008 - mse: 13.7008 - mae: 1.4662 - 39s/epoch - 2ms/step\n",
            "Score for fold 2: loss of 10.943605422973633\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 41s - loss: 13.6461 - mse: 13.6461 - mae: 1.4640 - 41s/epoch - 2ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 41s - loss: 13.6295 - mse: 13.6295 - mae: 1.4664 - 41s/epoch - 2ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 41s - loss: 13.4932 - mse: 13.4932 - mae: 1.4604 - 41s/epoch - 2ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 41s - loss: 13.3011 - mse: 13.3011 - mae: 1.4620 - 41s/epoch - 2ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 42s - loss: 13.4416 - mse: 13.4416 - mae: 1.4631 - 42s/epoch - 2ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 42s - loss: 13.1519 - mse: 13.1519 - mae: 1.4605 - 42s/epoch - 2ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 41s - loss: 13.3165 - mse: 13.3165 - mae: 1.4582 - 41s/epoch - 2ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 41s - loss: 12.9868 - mse: 12.9868 - mae: 1.4554 - 41s/epoch - 2ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 42s - loss: 12.8822 - mse: 12.8822 - mae: 1.4555 - 42s/epoch - 2ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 43s - loss: 12.9830 - mse: 12.9830 - mae: 1.4552 - 43s/epoch - 2ms/step\n",
            "Score for fold 3: loss of 10.526944160461426\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-10-01 08:07:04,454]\u001b[0m Finished trial#35 resulted in value: 12.982998847961426. Current best value is 10.764413833618164 with parameters: {'activation': 'relu', 'optimizer': 'adam', 'num_hidden_layer': 5, 'num_hidden_unit': 60, 'lear  ning_rate': 0.0003135813689247155}.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 39s - loss: 16.8335 - mse: 16.8335 - mae: 1.5749 - 39s/epoch - 2ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 38s - loss: 16.3372 - mse: 16.3372 - mae: 1.5532 - 38s/epoch - 2ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 39s - loss: 16.1665 - mse: 16.1665 - mae: 1.5493 - 39s/epoch - 2ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 38s - loss: 16.2583 - mse: 16.2583 - mae: 1.5502 - 38s/epoch - 2ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 37s - loss: 16.0711 - mse: 16.0711 - mae: 1.5526 - 37s/epoch - 2ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 36s - loss: 16.1115 - mse: 16.1115 - mae: 1.5565 - 36s/epoch - 2ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 37s - loss: 16.0594 - mse: 16.0594 - mae: 1.5549 - 37s/epoch - 2ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 37s - loss: 16.0838 - mse: 16.0838 - mae: 1.5504 - 37s/epoch - 2ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 37s - loss: 16.0240 - mse: 16.0240 - mae: 1.5491 - 37s/epoch - 2ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 37s - loss: 16.1507 - mse: 16.1507 - mae: 1.5561 - 37s/epoch - 2ms/step\n",
            "Score for fold 1: loss of 24.173505783081055\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 40s - loss: 16.1327 - mse: 16.1327 - mae: 1.5519 - 40s/epoch - 2ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 39s - loss: 15.9504 - mse: 15.9504 - mae: 1.5534 - 39s/epoch - 2ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 37s - loss: 15.7843 - mse: 15.7843 - mae: 1.5512 - 37s/epoch - 2ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 37s - loss: 15.6442 - mse: 15.6442 - mae: 1.5449 - 37s/epoch - 2ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 37s - loss: 15.6984 - mse: 15.6984 - mae: 1.5438 - 37s/epoch - 2ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 37s - loss: 15.8534 - mse: 15.8534 - mae: 1.5497 - 37s/epoch - 2ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 38s - loss: 15.9183 - mse: 15.9183 - mae: 1.5580 - 38s/epoch - 2ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 38s - loss: 16.1148 - mse: 16.1148 - mae: 1.5721 - 38s/epoch - 2ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 37s - loss: 16.0039 - mse: 16.0039 - mae: 1.5600 - 37s/epoch - 2ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 37s - loss: 15.9552 - mse: 15.9552 - mae: 1.5587 - 37s/epoch - 2ms/step\n",
            "Score for fold 2: loss of 12.031947135925293\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 37s - loss: 16.1269 - mse: 16.1269 - mae: 1.5609 - 37s/epoch - 2ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 37s - loss: 16.2709 - mse: 16.2709 - mae: 1.5663 - 37s/epoch - 2ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 38s - loss: 15.9959 - mse: 15.9959 - mae: 1.5610 - 38s/epoch - 2ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 37s - loss: 16.3739 - mse: 16.3739 - mae: 1.5604 - 37s/epoch - 2ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 39s - loss: 16.1200 - mse: 16.1200 - mae: 1.5655 - 39s/epoch - 2ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 40s - loss: 16.1327 - mse: 16.1327 - mae: 1.5716 - 40s/epoch - 2ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 38s - loss: 16.0296 - mse: 16.0296 - mae: 1.5674 - 38s/epoch - 2ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 39s - loss: 16.2888 - mse: 16.2888 - mae: 1.5734 - 39s/epoch - 2ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 39s - loss: 16.1557 - mse: 16.1557 - mae: 1.5732 - 39s/epoch - 2ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 38s - loss: 16.1052 - mse: 16.1052 - mae: 1.5689 - 38s/epoch - 2ms/step\n",
            "Score for fold 3: loss of 13.057788848876953\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-10-01 08:26:06,817]\u001b[0m Finished trial#36 resulted in value: 16.105241775512695. Current best value is 10.764413833618164 with parameters: {'activation': 'relu', 'optimizer': 'adam', 'num_hidden_layer': 5, 'num_hidden_unit': 60, 'lear  ning_rate': 0.0003135813689247155}.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 36s - loss: 16.8017 - mse: 16.8017 - mae: 1.5659 - 36s/epoch - 2ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 38s - loss: 15.9590 - mse: 15.9590 - mae: 1.5313 - 38s/epoch - 2ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 39s - loss: 15.6673 - mse: 15.6673 - mae: 1.5116 - 39s/epoch - 2ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 39s - loss: 15.5018 - mse: 15.5018 - mae: 1.4997 - 39s/epoch - 2ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 39s - loss: 15.3930 - mse: 15.3930 - mae: 1.4920 - 39s/epoch - 2ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 37s - loss: 15.2115 - mse: 15.2115 - mae: 1.4846 - 37s/epoch - 2ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 39s - loss: 15.1032 - mse: 15.1032 - mae: 1.4795 - 39s/epoch - 2ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 38s - loss: 14.8901 - mse: 14.8901 - mae: 1.4720 - 38s/epoch - 2ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 38s - loss: 14.8012 - mse: 14.8012 - mae: 1.4688 - 38s/epoch - 2ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 39s - loss: 14.7107 - mse: 14.7107 - mae: 1.4679 - 39s/epoch - 2ms/step\n",
            "Score for fold 1: loss of 23.942956924438477\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 39s - loss: 14.5487 - mse: 14.5487 - mae: 1.4626 - 39s/epoch - 2ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 40s - loss: 14.4159 - mse: 14.4159 - mae: 1.4654 - 40s/epoch - 2ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 37s - loss: 14.1900 - mse: 14.1900 - mae: 1.4584 - 37s/epoch - 2ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 38s - loss: 14.0661 - mse: 14.0661 - mae: 1.4549 - 38s/epoch - 2ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 40s - loss: 13.9520 - mse: 13.9520 - mae: 1.4534 - 40s/epoch - 2ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 39s - loss: 13.7644 - mse: 13.7644 - mae: 1.4544 - 39s/epoch - 2ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 37s - loss: 13.7608 - mse: 13.7608 - mae: 1.4523 - 37s/epoch - 2ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 38s - loss: 13.6286 - mse: 13.6286 - mae: 1.4480 - 38s/epoch - 2ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 37s - loss: 13.5669 - mse: 13.5669 - mae: 1.4458 - 37s/epoch - 2ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 37s - loss: 13.3593 - mse: 13.3593 - mae: 1.4444 - 37s/epoch - 2ms/step\n",
            "Score for fold 2: loss of 10.989753723144531\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 36s - loss: 13.2716 - mse: 13.2716 - mae: 1.4435 - 36s/epoch - 2ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 37s - loss: 13.2753 - mse: 13.2753 - mae: 1.4418 - 37s/epoch - 2ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 37s - loss: 13.0486 - mse: 13.0486 - mae: 1.4379 - 37s/epoch - 2ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 36s - loss: 13.0364 - mse: 13.0364 - mae: 1.4393 - 36s/epoch - 2ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 38s - loss: 12.9284 - mse: 12.9284 - mae: 1.4383 - 38s/epoch - 2ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 36s - loss: 12.8262 - mse: 12.8262 - mae: 1.4360 - 36s/epoch - 2ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 36s - loss: 12.8254 - mse: 12.8254 - mae: 1.4358 - 36s/epoch - 2ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 38s - loss: 12.5868 - mse: 12.5868 - mae: 1.4322 - 38s/epoch - 2ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 36s - loss: 12.6731 - mse: 12.6731 - mae: 1.4335 - 36s/epoch - 2ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 37s - loss: 12.5926 - mse: 12.5926 - mae: 1.4341 - 37s/epoch - 2ms/step\n",
            "Score for fold 3: loss of 10.65288257598877\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-10-01 08:45:09,317]\u001b[0m Finished trial#37 resulted in value: 12.592552185058594. Current best value is 10.764413833618164 with parameters: {'activation': 'relu', 'optimizer': 'adam', 'num_hidden_layer': 5, 'num_hidden_unit': 60, 'lear  ning_rate': 0.0003135813689247155}.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 34s - loss: 18.9788 - mse: 18.9788 - mae: 1.6370 - 34s/epoch - 2ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 33s - loss: 18.1270 - mse: 18.1270 - mae: 1.5906 - 33s/epoch - 2ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 32s - loss: 17.9334 - mse: 17.9334 - mae: 1.5772 - 32s/epoch - 2ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 32s - loss: 17.8171 - mse: 17.8171 - mae: 1.5718 - 32s/epoch - 2ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 33s - loss: 17.7330 - mse: 17.7330 - mae: 1.5660 - 33s/epoch - 2ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 31s - loss: 17.6657 - mse: 17.6657 - mae: 1.5643 - 31s/epoch - 2ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 31s - loss: 17.6086 - mse: 17.6086 - mae: 1.5605 - 31s/epoch - 2ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 32s - loss: 17.5591 - mse: 17.5591 - mae: 1.5580 - 32s/epoch - 2ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 33s - loss: 17.5144 - mse: 17.5144 - mae: 1.5566 - 33s/epoch - 2ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 32s - loss: 17.4748 - mse: 17.4748 - mae: 1.5537 - 32s/epoch - 2ms/step\n",
            "Score for fold 1: loss of 23.94672203063965\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 32s - loss: 17.4391 - mse: 17.4391 - mae: 1.5516 - 32s/epoch - 2ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 32s - loss: 17.4062 - mse: 17.4062 - mae: 1.5511 - 32s/epoch - 2ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 33s - loss: 17.3757 - mse: 17.3757 - mae: 1.5470 - 33s/epoch - 2ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 31s - loss: 17.3478 - mse: 17.3478 - mae: 1.5492 - 31s/epoch - 2ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 31s - loss: 17.3219 - mse: 17.3219 - mae: 1.5461 - 31s/epoch - 2ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 32s - loss: 17.2976 - mse: 17.2976 - mae: 1.5457 - 32s/epoch - 2ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 33s - loss: 17.2748 - mse: 17.2748 - mae: 1.5442 - 33s/epoch - 2ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 32s - loss: 17.2533 - mse: 17.2533 - mae: 1.5432 - 32s/epoch - 2ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 33s - loss: 17.2331 - mse: 17.2331 - mae: 1.5429 - 33s/epoch - 2ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 32s - loss: 17.2137 - mse: 17.2137 - mae: 1.5404 - 32s/epoch - 2ms/step\n",
            "Score for fold 2: loss of 13.113704681396484\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 32s - loss: 17.1954 - mse: 17.1954 - mae: 1.5401 - 32s/epoch - 2ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 31s - loss: 17.1781 - mse: 17.1781 - mae: 1.5405 - 31s/epoch - 2ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 30s - loss: 17.1613 - mse: 17.1613 - mae: 1.5395 - 30s/epoch - 2ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 30s - loss: 17.1453 - mse: 17.1453 - mae: 1.5398 - 30s/epoch - 1ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 34s - loss: 17.1299 - mse: 17.1299 - mae: 1.5376 - 34s/epoch - 2ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 32s - loss: 17.1150 - mse: 17.1150 - mae: 1.5378 - 32s/epoch - 2ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 33s - loss: 17.1015 - mse: 17.1015 - mae: 1.5369 - 33s/epoch - 2ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 34s - loss: 17.0871 - mse: 17.0871 - mae: 1.5348 - 34s/epoch - 2ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 34s - loss: 17.0748 - mse: 17.0748 - mae: 1.5364 - 34s/epoch - 2ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 37s - loss: 17.0619 - mse: 17.0619 - mae: 1.5345 - 37s/epoch - 2ms/step\n",
            "Score for fold 3: loss of 12.884684562683105\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-10-01 09:01:32,371]\u001b[0m Finished trial#38 resulted in value: 17.06185531616211. Current best value is 10.764413833618164 with parameters: {'activation': 'relu', 'optimizer': 'adam', 'num_hidden_layer': 5, 'num_hidden_unit': 60, 'lear  ning_rate': 0.0003135813689247155}.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 34s - loss: 17.4333 - mse: 17.4333 - mae: 1.6630 - 34s/epoch - 2ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 32s - loss: 17.0160 - mse: 17.0160 - mae: 1.6652 - 32s/epoch - 2ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 33s - loss: 16.9377 - mse: 16.9377 - mae: 1.6438 - 33s/epoch - 2ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 33s - loss: 17.1310 - mse: 17.1310 - mae: 1.6688 - 33s/epoch - 2ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 34s - loss: 17.1850 - mse: 17.1850 - mae: 1.6525 - 34s/epoch - 2ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 33s - loss: 17.0258 - mse: 17.0258 - mae: 1.6541 - 33s/epoch - 2ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 31s - loss: 16.9604 - mse: 16.9604 - mae: 1.6501 - 31s/epoch - 2ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 31s - loss: 16.9535 - mse: 16.9535 - mae: 1.6474 - 31s/epoch - 2ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 32s - loss: 16.9897 - mse: 16.9897 - mae: 1.6609 - 32s/epoch - 2ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 31s - loss: 16.9872 - mse: 16.9872 - mae: 1.6574 - 31s/epoch - 2ms/step\n",
            "Score for fold 1: loss of 23.800247192382812\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 32s - loss: 17.0139 - mse: 17.0139 - mae: 1.6719 - 32s/epoch - 2ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 32s - loss: 17.0594 - mse: 17.0594 - mae: 1.6532 - 32s/epoch - 2ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 30s - loss: 17.1372 - mse: 17.1372 - mae: 1.6495 - 30s/epoch - 2ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 28s - loss: 17.0941 - mse: 17.0941 - mae: 1.6536 - 28s/epoch - 1ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 30s - loss: 16.9843 - mse: 16.9843 - mae: 1.6486 - 30s/epoch - 1ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 30s - loss: 17.1311 - mse: 17.1311 - mae: 1.6598 - 30s/epoch - 2ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 30s - loss: 17.0359 - mse: 17.0359 - mae: 1.6493 - 30s/epoch - 2ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 30s - loss: 17.0832 - mse: 17.0832 - mae: 1.6551 - 30s/epoch - 1ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 30s - loss: 17.1352 - mse: 17.1352 - mae: 1.6669 - 30s/epoch - 1ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 30s - loss: 17.2286 - mse: 17.2286 - mae: 1.6702 - 30s/epoch - 1ms/step\n",
            "Score for fold 2: loss of 12.70418643951416\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 30s - loss: 17.2330 - mse: 17.2330 - mae: 1.6558 - 30s/epoch - 1ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 29s - loss: 17.1464 - mse: 17.1464 - mae: 1.6650 - 29s/epoch - 1ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 30s - loss: 17.3876 - mse: 17.3876 - mae: 1.6720 - 30s/epoch - 1ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 29s - loss: 17.5029 - mse: 17.5029 - mae: 1.6789 - 29s/epoch - 1ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 29s - loss: 17.4764 - mse: 17.4764 - mae: 1.6670 - 29s/epoch - 1ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 29s - loss: 17.3776 - mse: 17.3776 - mae: 1.6651 - 29s/epoch - 1ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 29s - loss: 17.2668 - mse: 17.2668 - mae: 1.6621 - 29s/epoch - 1ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 28s - loss: 17.2102 - mse: 17.2102 - mae: 1.6623 - 28s/epoch - 1ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 29s - loss: 17.2026 - mse: 17.2026 - mae: 1.6624 - 29s/epoch - 1ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 29s - loss: 17.1635 - mse: 17.1635 - mae: 1.6558 - 29s/epoch - 1ms/step\n",
            "Score for fold 3: loss of 13.039012908935547\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-10-01 09:17:01,185]\u001b[0m Finished trial#39 resulted in value: 17.163469314575195. Current best value is 10.764413833618164 with parameters: {'activation': 'relu', 'optimizer': 'adam', 'num_hidden_layer': 5, 'num_hidden_unit': 60, 'lear  ning_rate': 0.0003135813689247155}.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 39s - loss: 18.6139 - mse: 18.6139 - mae: 1.6729 - 39s/epoch - 2ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 38s - loss: 18.2722 - mse: 18.2722 - mae: 1.6676 - 38s/epoch - 2ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 38s - loss: 17.7451 - mse: 17.7451 - mae: 1.6499 - 38s/epoch - 2ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 38s - loss: 17.7081 - mse: 17.7081 - mae: 1.6522 - 38s/epoch - 2ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 37s - loss: 17.7031 - mse: 17.7031 - mae: 1.6456 - 37s/epoch - 2ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 38s - loss: 17.7266 - mse: 17.7266 - mae: 1.6420 - 38s/epoch - 2ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 38s - loss: 17.5827 - mse: 17.5827 - mae: 1.6454 - 38s/epoch - 2ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 39s - loss: 17.5714 - mse: 17.5714 - mae: 1.6301 - 39s/epoch - 2ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 40s - loss: 17.7418 - mse: 17.7418 - mae: 1.6299 - 40s/epoch - 2ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 38s - loss: 17.4958 - mse: 17.4958 - mae: 1.6312 - 38s/epoch - 2ms/step\n",
            "Score for fold 1: loss of 24.802324295043945\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 39s - loss: 17.4375 - mse: 17.4375 - mae: 1.6314 - 39s/epoch - 2ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 39s - loss: 17.3721 - mse: 17.3721 - mae: 1.6298 - 39s/epoch - 2ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 38s - loss: 17.3865 - mse: 17.3865 - mae: 1.6238 - 38s/epoch - 2ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 39s - loss: 17.5430 - mse: 17.5430 - mae: 1.6292 - 39s/epoch - 2ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 40s - loss: 17.4557 - mse: 17.4557 - mae: 1.6258 - 40s/epoch - 2ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 40s - loss: 17.4114 - mse: 17.4114 - mae: 1.6276 - 40s/epoch - 2ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 39s - loss: 17.3884 - mse: 17.3884 - mae: 1.6267 - 39s/epoch - 2ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 40s - loss: 17.5334 - mse: 17.5334 - mae: 1.6283 - 40s/epoch - 2ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 40s - loss: 17.4862 - mse: 17.4862 - mae: 1.6269 - 40s/epoch - 2ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 39s - loss: 17.3533 - mse: 17.3533 - mae: 1.6298 - 39s/epoch - 2ms/step\n",
            "Score for fold 2: loss of 12.382296562194824\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 40s - loss: 17.3903 - mse: 17.3903 - mae: 1.6249 - 40s/epoch - 2ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 40s - loss: 17.2693 - mse: 17.2693 - mae: 1.6281 - 40s/epoch - 2ms/step\n",
            "Epoch 3/10\n",
            "20000/20000 - 39s - loss: 17.4371 - mse: 17.4371 - mae: 1.6294 - 39s/epoch - 2ms/step\n",
            "Epoch 4/10\n",
            "20000/20000 - 39s - loss: 17.4507 - mse: 17.4507 - mae: 1.6244 - 39s/epoch - 2ms/step\n",
            "Epoch 5/10\n",
            "20000/20000 - 39s - loss: 17.3585 - mse: 17.3585 - mae: 1.6261 - 39s/epoch - 2ms/step\n",
            "Epoch 6/10\n",
            "20000/20000 - 39s - loss: 17.4180 - mse: 17.4180 - mae: 1.6272 - 39s/epoch - 2ms/step\n",
            "Epoch 7/10\n",
            "20000/20000 - 39s - loss: 17.3910 - mse: 17.3910 - mae: 1.6224 - 39s/epoch - 2ms/step\n",
            "Epoch 8/10\n",
            "20000/20000 - 39s - loss: 17.4019 - mse: 17.4019 - mae: 1.6298 - 39s/epoch - 2ms/step\n",
            "Epoch 9/10\n",
            "20000/20000 - 38s - loss: 17.3478 - mse: 17.3478 - mae: 1.6245 - 38s/epoch - 2ms/step\n",
            "Epoch 10/10\n",
            "20000/20000 - 38s - loss: 17.4483 - mse: 17.4483 - mae: 1.6278 - 38s/epoch - 2ms/step\n",
            "Score for fold 3: loss of 12.623113632202148\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-10-01 09:36:36,951]\u001b[0m Finished trial#40 resulted in value: 17.44825553894043. Current best value is 10.764413833618164 with parameters: {'activation': 'relu', 'optimizer': 'adam', 'num_hidden_layer': 5, 'num_hidden_unit': 60, 'lear  ning_rate': 0.0003135813689247155}.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/10\n",
            "20000/20000 - 40s - loss: 16.8133 - mse: 16.8133 - mae: 1.5636 - 40s/epoch - 2ms/step\n",
            "Epoch 2/10\n",
            "20000/20000 - 40s - loss: 15.9406 - mse: 15.9406 - mae: 1.5195 - 40s/epoch - 2ms/step\n",
            "Epoch 3/10\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "num_folds = 3\n",
        "\n",
        "optimizer = Adam(learning_rate=0.0003135813689247155)\n",
        "\n",
        "kfold=KFold(n_splits=3,shuffle=True)\n",
        "fold_no=1\n",
        "loss_per_fold = []\n",
        "#{'activation': 'relu', 'optimizer': 'adam', 'num_hidden_layer': 5, 'num_hidden_unit': 60, 'learning_rate': 0.0003135813689247155}\n",
        "model_nn_best = create_model(activation='relu', num_hidden_layer=5, num_hidden_unit=60)\n",
        "model_nn_best.summary()\n",
        "model_nn_best.compile(loss='mse',optimizer=optimizer,metrics=['mse','mae'])\n",
        "for train,test in kfold.split(training,labelsForTrain):\n",
        "  scores=model_nn_best.evaluate(testing,labelsForTest,verbose=0)\n",
        "\n",
        "  print('------------------------------------------------------------------------')\n",
        "  print(f'Training for fold {fold_no} ...')\n",
        "  \n",
        "  # Fit data to model\n",
        "  history = model_nn_best.fit(training, labelsForTrain,\n",
        "              batch_size=20,\n",
        "              #The result from epoch 20 and epoch 200 don't make too much differenct\n",
        "              epochs=100,\n",
        "              verbose=2,)\n",
        "    \n",
        "  print(f'Score for fold {fold_no}: {model_nn_best.metrics_names[0]} of {scores[0]}')\n",
        "  loss_per_fold.append(scores[0])\n",
        "  # Increase fold number\n",
        "  fold_no = fold_no + 1  "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i9yP0G4CIydk",
        "outputId": "1c41593d-9d79-49aa-d995-3907b544e4ad"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_1 (InputLayer)        [(None, 18)]              0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 60)                1140      \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 60)                3660      \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 60)                3660      \n",
            "                                                                 \n",
            " dense_3 (Dense)             (None, 60)                3660      \n",
            "                                                                 \n",
            " dense_4 (Dense)             (None, 1)                 61        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 12,181\n",
            "Trainable params: 12,181\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "20000/20000 - 25s - loss: 16.4224 - mse: 16.4224 - mae: 1.5396 - 25s/epoch - 1ms/step\n",
            "Epoch 2/100\n",
            "20000/20000 - 25s - loss: 15.6687 - mse: 15.6687 - mae: 1.5012 - 25s/epoch - 1ms/step\n",
            "Epoch 3/100\n",
            "20000/20000 - 25s - loss: 15.4015 - mse: 15.4015 - mae: 1.4901 - 25s/epoch - 1ms/step\n",
            "Epoch 4/100\n",
            "20000/20000 - 25s - loss: 15.2443 - mse: 15.2443 - mae: 1.4808 - 25s/epoch - 1ms/step\n",
            "Epoch 5/100\n",
            "20000/20000 - 24s - loss: 15.1252 - mse: 15.1252 - mae: 1.4755 - 24s/epoch - 1ms/step\n",
            "Epoch 6/100\n",
            "20000/20000 - 24s - loss: 14.9577 - mse: 14.9577 - mae: 1.4714 - 24s/epoch - 1ms/step\n",
            "Epoch 7/100\n",
            "20000/20000 - 25s - loss: 14.8216 - mse: 14.8216 - mae: 1.4673 - 25s/epoch - 1ms/step\n",
            "Epoch 8/100\n",
            "20000/20000 - 25s - loss: 14.8014 - mse: 14.8014 - mae: 1.4648 - 25s/epoch - 1ms/step\n",
            "Epoch 9/100\n",
            "20000/20000 - 25s - loss: 14.6206 - mse: 14.6206 - mae: 1.4606 - 25s/epoch - 1ms/step\n",
            "Epoch 10/100\n",
            "20000/20000 - 25s - loss: 14.6016 - mse: 14.6016 - mae: 1.4586 - 25s/epoch - 1ms/step\n",
            "Epoch 11/100\n",
            "20000/20000 - 25s - loss: 14.4987 - mse: 14.4987 - mae: 1.4551 - 25s/epoch - 1ms/step\n",
            "Epoch 12/100\n",
            "20000/20000 - 25s - loss: 14.4179 - mse: 14.4179 - mae: 1.4534 - 25s/epoch - 1ms/step\n",
            "Epoch 13/100\n",
            "20000/20000 - 25s - loss: 14.2848 - mse: 14.2848 - mae: 1.4509 - 25s/epoch - 1ms/step\n",
            "Epoch 14/100\n",
            "20000/20000 - 25s - loss: 14.2252 - mse: 14.2252 - mae: 1.4507 - 25s/epoch - 1ms/step\n",
            "Epoch 15/100\n",
            "20000/20000 - 24s - loss: 14.0406 - mse: 14.0406 - mae: 1.4465 - 24s/epoch - 1ms/step\n",
            "Epoch 16/100\n",
            "20000/20000 - 25s - loss: 13.9909 - mse: 13.9909 - mae: 1.4448 - 25s/epoch - 1ms/step\n",
            "Epoch 17/100\n",
            "20000/20000 - 25s - loss: 13.9385 - mse: 13.9385 - mae: 1.4427 - 25s/epoch - 1ms/step\n",
            "Epoch 18/100\n",
            "20000/20000 - 25s - loss: 13.8290 - mse: 13.8290 - mae: 1.4428 - 25s/epoch - 1ms/step\n",
            "Epoch 19/100\n",
            "20000/20000 - 25s - loss: 13.5022 - mse: 13.5022 - mae: 1.4417 - 25s/epoch - 1ms/step\n",
            "Epoch 20/100\n",
            "20000/20000 - 25s - loss: 13.4691 - mse: 13.4691 - mae: 1.4424 - 25s/epoch - 1ms/step\n",
            "Epoch 21/100\n",
            "20000/20000 - 25s - loss: 13.1611 - mse: 13.1611 - mae: 1.4392 - 25s/epoch - 1ms/step\n",
            "Epoch 22/100\n",
            "20000/20000 - 25s - loss: 12.7602 - mse: 12.7602 - mae: 1.4383 - 25s/epoch - 1ms/step\n",
            "Epoch 23/100\n",
            "20000/20000 - 25s - loss: 12.3913 - mse: 12.3913 - mae: 1.4390 - 25s/epoch - 1ms/step\n",
            "Epoch 24/100\n",
            "20000/20000 - 25s - loss: 11.9640 - mse: 11.9640 - mae: 1.4383 - 25s/epoch - 1ms/step\n",
            "Epoch 25/100\n",
            "20000/20000 - 25s - loss: 12.2147 - mse: 12.2147 - mae: 1.4382 - 25s/epoch - 1ms/step\n",
            "Epoch 26/100\n",
            "20000/20000 - 25s - loss: 11.2800 - mse: 11.2800 - mae: 1.4369 - 25s/epoch - 1ms/step\n",
            "Epoch 27/100\n",
            "20000/20000 - 24s - loss: 10.8285 - mse: 10.8285 - mae: 1.4339 - 24s/epoch - 1ms/step\n",
            "Epoch 28/100\n",
            "20000/20000 - 25s - loss: 10.3204 - mse: 10.3204 - mae: 1.4308 - 25s/epoch - 1ms/step\n",
            "Epoch 29/100\n",
            "20000/20000 - 24s - loss: 9.2845 - mse: 9.2845 - mae: 1.4299 - 24s/epoch - 1ms/step\n",
            "Epoch 30/100\n",
            "20000/20000 - 24s - loss: 9.8392 - mse: 9.8392 - mae: 1.4320 - 24s/epoch - 1ms/step\n",
            "Epoch 31/100\n",
            "20000/20000 - 24s - loss: 9.5977 - mse: 9.5977 - mae: 1.4288 - 24s/epoch - 1ms/step\n",
            "Epoch 32/100\n",
            "20000/20000 - 25s - loss: 9.6752 - mse: 9.6752 - mae: 1.4267 - 25s/epoch - 1ms/step\n",
            "Epoch 33/100\n",
            "20000/20000 - 25s - loss: 9.6480 - mse: 9.6480 - mae: 1.4228 - 25s/epoch - 1ms/step\n",
            "Epoch 34/100\n",
            "20000/20000 - 25s - loss: 9.5273 - mse: 9.5273 - mae: 1.4217 - 25s/epoch - 1ms/step\n",
            "Epoch 35/100\n",
            "20000/20000 - 25s - loss: 9.1746 - mse: 9.1746 - mae: 1.4229 - 25s/epoch - 1ms/step\n",
            "Epoch 36/100\n",
            "20000/20000 - 25s - loss: 9.1810 - mse: 9.1810 - mae: 1.4220 - 25s/epoch - 1ms/step\n",
            "Epoch 37/100\n",
            "20000/20000 - 25s - loss: 9.2365 - mse: 9.2365 - mae: 1.4211 - 25s/epoch - 1ms/step\n",
            "Epoch 38/100\n",
            "20000/20000 - 25s - loss: 9.6168 - mse: 9.6168 - mae: 1.4241 - 25s/epoch - 1ms/step\n",
            "Epoch 39/100\n",
            "20000/20000 - 25s - loss: 9.1326 - mse: 9.1326 - mae: 1.4245 - 25s/epoch - 1ms/step\n",
            "Epoch 40/100\n",
            "20000/20000 - 24s - loss: 9.3211 - mse: 9.3211 - mae: 1.4233 - 24s/epoch - 1ms/step\n",
            "Epoch 41/100\n",
            "20000/20000 - 25s - loss: 8.9769 - mse: 8.9769 - mae: 1.4218 - 25s/epoch - 1ms/step\n",
            "Epoch 42/100\n",
            "20000/20000 - 24s - loss: 8.7730 - mse: 8.7730 - mae: 1.4209 - 24s/epoch - 1ms/step\n",
            "Epoch 43/100\n",
            "20000/20000 - 24s - loss: 8.8003 - mse: 8.8003 - mae: 1.4170 - 24s/epoch - 1ms/step\n",
            "Epoch 44/100\n",
            "20000/20000 - 24s - loss: 8.8042 - mse: 8.8042 - mae: 1.4209 - 24s/epoch - 1ms/step\n",
            "Epoch 45/100\n",
            "20000/20000 - 24s - loss: 9.0939 - mse: 9.0939 - mae: 1.4215 - 24s/epoch - 1ms/step\n",
            "Epoch 46/100\n",
            "20000/20000 - 25s - loss: 9.4976 - mse: 9.4976 - mae: 1.4162 - 25s/epoch - 1ms/step\n",
            "Epoch 47/100\n",
            "20000/20000 - 24s - loss: 9.7111 - mse: 9.7111 - mae: 1.4196 - 24s/epoch - 1ms/step\n",
            "Epoch 48/100\n",
            "20000/20000 - 24s - loss: 8.6138 - mse: 8.6138 - mae: 1.4137 - 24s/epoch - 1ms/step\n",
            "Epoch 49/100\n",
            "20000/20000 - 25s - loss: 9.0846 - mse: 9.0846 - mae: 1.4153 - 25s/epoch - 1ms/step\n",
            "Epoch 50/100\n",
            "20000/20000 - 24s - loss: 9.6797 - mse: 9.6797 - mae: 1.4182 - 24s/epoch - 1ms/step\n",
            "Epoch 51/100\n",
            "20000/20000 - 25s - loss: 8.8768 - mse: 8.8768 - mae: 1.4174 - 25s/epoch - 1ms/step\n",
            "Epoch 52/100\n",
            "20000/20000 - 24s - loss: 8.7806 - mse: 8.7806 - mae: 1.4182 - 24s/epoch - 1ms/step\n",
            "Epoch 53/100\n",
            "20000/20000 - 24s - loss: 9.0075 - mse: 9.0075 - mae: 1.4143 - 24s/epoch - 1ms/step\n",
            "Epoch 54/100\n",
            "20000/20000 - 24s - loss: 8.9671 - mse: 8.9671 - mae: 1.4174 - 24s/epoch - 1ms/step\n",
            "Epoch 55/100\n",
            "20000/20000 - 24s - loss: 9.1049 - mse: 9.1049 - mae: 1.4164 - 24s/epoch - 1ms/step\n",
            "Epoch 56/100\n",
            "20000/20000 - 25s - loss: 8.7738 - mse: 8.7738 - mae: 1.4176 - 25s/epoch - 1ms/step\n",
            "Epoch 57/100\n",
            "20000/20000 - 25s - loss: 8.6564 - mse: 8.6564 - mae: 1.4157 - 25s/epoch - 1ms/step\n",
            "Epoch 58/100\n",
            "20000/20000 - 25s - loss: 9.5911 - mse: 9.5911 - mae: 1.4147 - 25s/epoch - 1ms/step\n",
            "Epoch 59/100\n",
            "20000/20000 - 25s - loss: 9.1032 - mse: 9.1032 - mae: 1.4156 - 25s/epoch - 1ms/step\n",
            "Epoch 60/100\n",
            "20000/20000 - 25s - loss: 8.5082 - mse: 8.5082 - mae: 1.4135 - 25s/epoch - 1ms/step\n",
            "Epoch 61/100\n",
            "20000/20000 - 24s - loss: 8.8021 - mse: 8.8021 - mae: 1.4136 - 24s/epoch - 1ms/step\n",
            "Epoch 62/100\n",
            "20000/20000 - 24s - loss: 8.7506 - mse: 8.7506 - mae: 1.4105 - 24s/epoch - 1ms/step\n",
            "Epoch 63/100\n",
            "20000/20000 - 25s - loss: 8.3141 - mse: 8.3141 - mae: 1.4097 - 25s/epoch - 1ms/step\n",
            "Epoch 64/100\n",
            "20000/20000 - 25s - loss: 8.7370 - mse: 8.7370 - mae: 1.4133 - 25s/epoch - 1ms/step\n",
            "Epoch 65/100\n",
            "20000/20000 - 24s - loss: 8.9051 - mse: 8.9051 - mae: 1.4099 - 24s/epoch - 1ms/step\n",
            "Epoch 66/100\n",
            "20000/20000 - 25s - loss: 8.2988 - mse: 8.2988 - mae: 1.4087 - 25s/epoch - 1ms/step\n",
            "Epoch 67/100\n",
            "20000/20000 - 25s - loss: 9.2619 - mse: 9.2619 - mae: 1.4101 - 25s/epoch - 1ms/step\n",
            "Epoch 68/100\n",
            "20000/20000 - 24s - loss: 8.5702 - mse: 8.5702 - mae: 1.4069 - 24s/epoch - 1ms/step\n",
            "Epoch 69/100\n",
            "20000/20000 - 24s - loss: 8.6192 - mse: 8.6192 - mae: 1.4088 - 24s/epoch - 1ms/step\n",
            "Epoch 70/100\n",
            "20000/20000 - 24s - loss: 8.3751 - mse: 8.3751 - mae: 1.4089 - 24s/epoch - 1ms/step\n",
            "Epoch 71/100\n",
            "20000/20000 - 24s - loss: 8.3027 - mse: 8.3027 - mae: 1.4091 - 24s/epoch - 1ms/step\n",
            "Epoch 72/100\n",
            "20000/20000 - 24s - loss: 8.5016 - mse: 8.5016 - mae: 1.4087 - 24s/epoch - 1ms/step\n",
            "Epoch 73/100\n",
            "20000/20000 - 25s - loss: 10.1371 - mse: 10.1371 - mae: 1.4079 - 25s/epoch - 1ms/step\n",
            "Epoch 74/100\n",
            "20000/20000 - 25s - loss: 8.3555 - mse: 8.3555 - mae: 1.4035 - 25s/epoch - 1ms/step\n",
            "Epoch 75/100\n",
            "20000/20000 - 24s - loss: 8.5317 - mse: 8.5317 - mae: 1.4041 - 24s/epoch - 1ms/step\n",
            "Epoch 76/100\n",
            "20000/20000 - 25s - loss: 8.5604 - mse: 8.5604 - mae: 1.4057 - 25s/epoch - 1ms/step\n",
            "Epoch 77/100\n",
            "20000/20000 - 24s - loss: 8.3732 - mse: 8.3732 - mae: 1.4058 - 24s/epoch - 1ms/step\n",
            "Epoch 78/100\n",
            "20000/20000 - 24s - loss: 8.4365 - mse: 8.4365 - mae: 1.4074 - 24s/epoch - 1ms/step\n",
            "Epoch 79/100\n",
            "20000/20000 - 24s - loss: 8.0969 - mse: 8.0969 - mae: 1.4045 - 24s/epoch - 1ms/step\n",
            "Epoch 80/100\n",
            "20000/20000 - 24s - loss: 8.9877 - mse: 8.9877 - mae: 1.4032 - 24s/epoch - 1ms/step\n",
            "Epoch 81/100\n",
            "20000/20000 - 25s - loss: 8.5059 - mse: 8.5059 - mae: 1.4005 - 25s/epoch - 1ms/step\n",
            "Epoch 82/100\n",
            "20000/20000 - 24s - loss: 8.1990 - mse: 8.1990 - mae: 1.3992 - 24s/epoch - 1ms/step\n",
            "Epoch 83/100\n",
            "20000/20000 - 25s - loss: 7.9634 - mse: 7.9634 - mae: 1.4006 - 25s/epoch - 1ms/step\n",
            "Epoch 84/100\n",
            "20000/20000 - 25s - loss: 9.3399 - mse: 9.3399 - mae: 1.4001 - 25s/epoch - 1ms/step\n",
            "Epoch 85/100\n",
            "20000/20000 - 24s - loss: 8.5055 - mse: 8.5055 - mae: 1.4031 - 24s/epoch - 1ms/step\n",
            "Epoch 86/100\n",
            "20000/20000 - 24s - loss: 8.4826 - mse: 8.4826 - mae: 1.4008 - 24s/epoch - 1ms/step\n",
            "Epoch 87/100\n",
            "20000/20000 - 24s - loss: 7.7847 - mse: 7.7847 - mae: 1.3999 - 24s/epoch - 1ms/step\n",
            "Epoch 88/100\n",
            "20000/20000 - 24s - loss: 7.8778 - mse: 7.8778 - mae: 1.3973 - 24s/epoch - 1ms/step\n",
            "Epoch 89/100\n",
            "20000/20000 - 25s - loss: 8.4053 - mse: 8.4053 - mae: 1.4023 - 25s/epoch - 1ms/step\n",
            "Epoch 90/100\n",
            "20000/20000 - 24s - loss: 7.8807 - mse: 7.8807 - mae: 1.3998 - 24s/epoch - 1ms/step\n",
            "Epoch 91/100\n",
            "20000/20000 - 24s - loss: 8.6021 - mse: 8.6021 - mae: 1.3983 - 24s/epoch - 1ms/step\n",
            "Epoch 92/100\n",
            "20000/20000 - 25s - loss: 8.4070 - mse: 8.4070 - mae: 1.3962 - 25s/epoch - 1ms/step\n",
            "Epoch 93/100\n",
            "20000/20000 - 25s - loss: 8.7613 - mse: 8.7613 - mae: 1.3947 - 25s/epoch - 1ms/step\n",
            "Epoch 94/100\n",
            "20000/20000 - 24s - loss: 8.8108 - mse: 8.8108 - mae: 1.3977 - 24s/epoch - 1ms/step\n",
            "Epoch 95/100\n",
            "20000/20000 - 24s - loss: 8.0352 - mse: 8.0352 - mae: 1.3957 - 24s/epoch - 1ms/step\n",
            "Epoch 96/100\n",
            "20000/20000 - 25s - loss: 8.6022 - mse: 8.6022 - mae: 1.3953 - 25s/epoch - 1ms/step\n",
            "Epoch 97/100\n",
            "20000/20000 - 25s - loss: 7.8507 - mse: 7.8507 - mae: 1.3989 - 25s/epoch - 1ms/step\n",
            "Epoch 98/100\n",
            "20000/20000 - 24s - loss: 7.6350 - mse: 7.6350 - mae: 1.3939 - 24s/epoch - 1ms/step\n",
            "Epoch 99/100\n",
            "20000/20000 - 24s - loss: 9.4739 - mse: 9.4739 - mae: 1.3938 - 24s/epoch - 1ms/step\n",
            "Epoch 100/100\n",
            "20000/20000 - 24s - loss: 7.8999 - mse: 7.8999 - mae: 1.3940 - 24s/epoch - 1ms/step\n",
            "Score for fold 1: loss of 22.6796817779541\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "20000/20000 - 25s - loss: 7.6113 - mse: 7.6113 - mae: 1.3937 - 25s/epoch - 1ms/step\n",
            "Epoch 2/100\n",
            "20000/20000 - 25s - loss: 7.8341 - mse: 7.8341 - mae: 1.3949 - 25s/epoch - 1ms/step\n",
            "Epoch 3/100\n",
            "20000/20000 - 25s - loss: 7.6999 - mse: 7.6999 - mae: 1.3901 - 25s/epoch - 1ms/step\n",
            "Epoch 4/100\n",
            "20000/20000 - 24s - loss: 7.7059 - mse: 7.7059 - mae: 1.3876 - 24s/epoch - 1ms/step\n",
            "Epoch 5/100\n",
            "20000/20000 - 24s - loss: 7.6588 - mse: 7.6588 - mae: 1.3880 - 24s/epoch - 1ms/step\n",
            "Epoch 6/100\n",
            "20000/20000 - 25s - loss: 7.9495 - mse: 7.9495 - mae: 1.3981 - 25s/epoch - 1ms/step\n",
            "Epoch 7/100\n",
            "20000/20000 - 25s - loss: 7.9412 - mse: 7.9412 - mae: 1.3900 - 25s/epoch - 1ms/step\n",
            "Epoch 8/100\n",
            "20000/20000 - 25s - loss: 8.4159 - mse: 8.4159 - mae: 1.3905 - 25s/epoch - 1ms/step\n",
            "Epoch 9/100\n",
            "20000/20000 - 25s - loss: 7.8080 - mse: 7.8080 - mae: 1.3860 - 25s/epoch - 1ms/step\n",
            "Epoch 10/100\n",
            "20000/20000 - 25s - loss: 7.6637 - mse: 7.6637 - mae: 1.3902 - 25s/epoch - 1ms/step\n",
            "Epoch 11/100\n",
            "20000/20000 - 24s - loss: 8.5750 - mse: 8.5750 - mae: 1.3887 - 24s/epoch - 1ms/step\n",
            "Epoch 12/100\n",
            "20000/20000 - 25s - loss: 7.8431 - mse: 7.8431 - mae: 1.3887 - 25s/epoch - 1ms/step\n",
            "Epoch 13/100\n",
            "20000/20000 - 24s - loss: 7.6397 - mse: 7.6397 - mae: 1.3902 - 24s/epoch - 1ms/step\n",
            "Epoch 14/100\n",
            "20000/20000 - 25s - loss: 7.7340 - mse: 7.7340 - mae: 1.3899 - 25s/epoch - 1ms/step\n",
            "Epoch 15/100\n",
            "20000/20000 - 25s - loss: 7.3977 - mse: 7.3977 - mae: 1.3863 - 25s/epoch - 1ms/step\n",
            "Epoch 16/100\n",
            "20000/20000 - 25s - loss: 8.0542 - mse: 8.0542 - mae: 1.3914 - 25s/epoch - 1ms/step\n",
            "Epoch 17/100\n",
            "20000/20000 - 25s - loss: 8.0625 - mse: 8.0625 - mae: 1.3902 - 25s/epoch - 1ms/step\n",
            "Epoch 18/100\n",
            "20000/20000 - 25s - loss: 8.0226 - mse: 8.0226 - mae: 1.3862 - 25s/epoch - 1ms/step\n",
            "Epoch 19/100\n",
            "20000/20000 - 25s - loss: 7.8997 - mse: 7.8997 - mae: 1.3900 - 25s/epoch - 1ms/step\n",
            "Epoch 20/100\n",
            "20000/20000 - 25s - loss: 7.6492 - mse: 7.6492 - mae: 1.3858 - 25s/epoch - 1ms/step\n",
            "Epoch 21/100\n",
            "20000/20000 - 25s - loss: 7.8120 - mse: 7.8120 - mae: 1.3871 - 25s/epoch - 1ms/step\n",
            "Epoch 22/100\n",
            "20000/20000 - 25s - loss: 7.7165 - mse: 7.7165 - mae: 1.3895 - 25s/epoch - 1ms/step\n",
            "Epoch 23/100\n",
            "20000/20000 - 25s - loss: 8.0268 - mse: 8.0268 - mae: 1.3862 - 25s/epoch - 1ms/step\n",
            "Epoch 24/100\n",
            "20000/20000 - 25s - loss: 7.4032 - mse: 7.4032 - mae: 1.3864 - 25s/epoch - 1ms/step\n",
            "Epoch 25/100\n",
            "20000/20000 - 25s - loss: 8.5004 - mse: 8.5004 - mae: 1.3827 - 25s/epoch - 1ms/step\n",
            "Epoch 26/100\n",
            "20000/20000 - 25s - loss: 7.5712 - mse: 7.5712 - mae: 1.3872 - 25s/epoch - 1ms/step\n",
            "Epoch 27/100\n",
            "20000/20000 - 25s - loss: 7.5035 - mse: 7.5035 - mae: 1.3851 - 25s/epoch - 1ms/step\n",
            "Epoch 28/100\n",
            "20000/20000 - 25s - loss: 7.6028 - mse: 7.6028 - mae: 1.3859 - 25s/epoch - 1ms/step\n",
            "Epoch 29/100\n",
            "20000/20000 - 25s - loss: 8.0319 - mse: 8.0319 - mae: 1.3870 - 25s/epoch - 1ms/step\n",
            "Epoch 30/100\n",
            "20000/20000 - 25s - loss: 7.3242 - mse: 7.3242 - mae: 1.3829 - 25s/epoch - 1ms/step\n",
            "Epoch 31/100\n",
            "20000/20000 - 25s - loss: 7.0874 - mse: 7.0874 - mae: 1.3797 - 25s/epoch - 1ms/step\n",
            "Epoch 32/100\n",
            "20000/20000 - 25s - loss: 7.6543 - mse: 7.6543 - mae: 1.3856 - 25s/epoch - 1ms/step\n",
            "Epoch 33/100\n",
            "20000/20000 - 25s - loss: 8.1188 - mse: 8.1188 - mae: 1.3846 - 25s/epoch - 1ms/step\n",
            "Epoch 34/100\n",
            "20000/20000 - 25s - loss: 7.7243 - mse: 7.7243 - mae: 1.3902 - 25s/epoch - 1ms/step\n",
            "Epoch 35/100\n",
            "20000/20000 - 25s - loss: 7.5963 - mse: 7.5963 - mae: 1.3824 - 25s/epoch - 1ms/step\n",
            "Epoch 36/100\n",
            "20000/20000 - 25s - loss: 7.6981 - mse: 7.6981 - mae: 1.3837 - 25s/epoch - 1ms/step\n",
            "Epoch 37/100\n",
            "20000/20000 - 25s - loss: 7.1800 - mse: 7.1800 - mae: 1.3790 - 25s/epoch - 1ms/step\n",
            "Epoch 38/100\n",
            "20000/20000 - 25s - loss: 7.3933 - mse: 7.3933 - mae: 1.3778 - 25s/epoch - 1ms/step\n",
            "Epoch 39/100\n",
            "20000/20000 - 25s - loss: 7.2019 - mse: 7.2019 - mae: 1.3784 - 25s/epoch - 1ms/step\n",
            "Epoch 40/100\n",
            "20000/20000 - 25s - loss: 7.2580 - mse: 7.2580 - mae: 1.3798 - 25s/epoch - 1ms/step\n",
            "Epoch 41/100\n",
            "20000/20000 - 25s - loss: 7.8844 - mse: 7.8844 - mae: 1.3802 - 25s/epoch - 1ms/step\n",
            "Epoch 42/100\n",
            "20000/20000 - 25s - loss: 7.2977 - mse: 7.2977 - mae: 1.3783 - 25s/epoch - 1ms/step\n",
            "Epoch 43/100\n",
            "20000/20000 - 24s - loss: 7.3834 - mse: 7.3834 - mae: 1.3771 - 24s/epoch - 1ms/step\n",
            "Epoch 44/100\n",
            "20000/20000 - 25s - loss: 7.4510 - mse: 7.4510 - mae: 1.3771 - 25s/epoch - 1ms/step\n",
            "Epoch 45/100\n",
            "20000/20000 - 24s - loss: 7.1657 - mse: 7.1657 - mae: 1.3793 - 24s/epoch - 1ms/step\n",
            "Epoch 46/100\n",
            "20000/20000 - 24s - loss: 8.7841 - mse: 8.7841 - mae: 1.3844 - 24s/epoch - 1ms/step\n",
            "Epoch 47/100\n",
            "20000/20000 - 24s - loss: 7.1954 - mse: 7.1954 - mae: 1.3786 - 24s/epoch - 1ms/step\n",
            "Epoch 48/100\n",
            "20000/20000 - 25s - loss: 7.4157 - mse: 7.4157 - mae: 1.3781 - 25s/epoch - 1ms/step\n",
            "Epoch 49/100\n",
            "20000/20000 - 25s - loss: 7.9623 - mse: 7.9623 - mae: 1.3794 - 25s/epoch - 1ms/step\n",
            "Epoch 50/100\n",
            "20000/20000 - 25s - loss: 7.1422 - mse: 7.1422 - mae: 1.3761 - 25s/epoch - 1ms/step\n",
            "Epoch 51/100\n",
            "20000/20000 - 25s - loss: 8.2801 - mse: 8.2801 - mae: 1.3815 - 25s/epoch - 1ms/step\n",
            "Epoch 52/100\n",
            "20000/20000 - 25s - loss: 7.0457 - mse: 7.0457 - mae: 1.3756 - 25s/epoch - 1ms/step\n",
            "Epoch 53/100\n",
            "20000/20000 - 25s - loss: 7.3187 - mse: 7.3187 - mae: 1.3780 - 25s/epoch - 1ms/step\n",
            "Epoch 54/100\n",
            "20000/20000 - 25s - loss: 8.5911 - mse: 8.5911 - mae: 1.3819 - 25s/epoch - 1ms/step\n",
            "Epoch 55/100\n",
            "20000/20000 - 25s - loss: 7.1594 - mse: 7.1594 - mae: 1.3764 - 25s/epoch - 1ms/step\n",
            "Epoch 56/100\n",
            "20000/20000 - 25s - loss: 8.3280 - mse: 8.3280 - mae: 1.3793 - 25s/epoch - 1ms/step\n",
            "Epoch 57/100\n",
            "20000/20000 - 25s - loss: 7.1515 - mse: 7.1515 - mae: 1.3764 - 25s/epoch - 1ms/step\n",
            "Epoch 58/100\n",
            "20000/20000 - 25s - loss: 7.5561 - mse: 7.5561 - mae: 1.3780 - 25s/epoch - 1ms/step\n",
            "Epoch 59/100\n",
            "20000/20000 - 25s - loss: 7.2575 - mse: 7.2575 - mae: 1.3791 - 25s/epoch - 1ms/step\n",
            "Epoch 60/100\n",
            "20000/20000 - 25s - loss: 7.3094 - mse: 7.3094 - mae: 1.3753 - 25s/epoch - 1ms/step\n",
            "Epoch 61/100\n",
            "20000/20000 - 24s - loss: 7.3965 - mse: 7.3965 - mae: 1.3788 - 24s/epoch - 1ms/step\n",
            "Epoch 62/100\n",
            "20000/20000 - 24s - loss: 6.9336 - mse: 6.9336 - mae: 1.3742 - 24s/epoch - 1ms/step\n",
            "Epoch 63/100\n",
            "20000/20000 - 25s - loss: 7.2168 - mse: 7.2168 - mae: 1.3734 - 25s/epoch - 1ms/step\n",
            "Epoch 64/100\n",
            "20000/20000 - 25s - loss: 7.8131 - mse: 7.8131 - mae: 1.3762 - 25s/epoch - 1ms/step\n",
            "Epoch 65/100\n",
            "20000/20000 - 25s - loss: 7.3937 - mse: 7.3937 - mae: 1.3742 - 25s/epoch - 1ms/step\n",
            "Epoch 66/100\n",
            "20000/20000 - 24s - loss: 7.3961 - mse: 7.3961 - mae: 1.3759 - 24s/epoch - 1ms/step\n",
            "Epoch 67/100\n",
            "20000/20000 - 24s - loss: 7.4968 - mse: 7.4968 - mae: 1.3816 - 24s/epoch - 1ms/step\n",
            "Epoch 68/100\n",
            "20000/20000 - 25s - loss: 7.0788 - mse: 7.0788 - mae: 1.3745 - 25s/epoch - 1ms/step\n",
            "Epoch 69/100\n",
            "20000/20000 - 25s - loss: 8.9591 - mse: 8.9591 - mae: 1.3844 - 25s/epoch - 1ms/step\n",
            "Epoch 70/100\n",
            "20000/20000 - 24s - loss: 6.9752 - mse: 6.9752 - mae: 1.3745 - 24s/epoch - 1ms/step\n",
            "Epoch 71/100\n",
            "20000/20000 - 25s - loss: 7.8533 - mse: 7.8533 - mae: 1.3756 - 25s/epoch - 1ms/step\n",
            "Epoch 72/100\n",
            "20000/20000 - 25s - loss: 7.4225 - mse: 7.4225 - mae: 1.3758 - 25s/epoch - 1ms/step\n",
            "Epoch 73/100\n",
            "20000/20000 - 25s - loss: 7.0840 - mse: 7.0840 - mae: 1.3760 - 25s/epoch - 1ms/step\n",
            "Epoch 74/100\n",
            "20000/20000 - 25s - loss: 7.3797 - mse: 7.3797 - mae: 1.3737 - 25s/epoch - 1ms/step\n",
            "Epoch 75/100\n",
            "20000/20000 - 25s - loss: 7.6059 - mse: 7.6059 - mae: 1.3751 - 25s/epoch - 1ms/step\n",
            "Epoch 76/100\n",
            "20000/20000 - 25s - loss: 7.5733 - mse: 7.5733 - mae: 1.3767 - 25s/epoch - 1ms/step\n",
            "Epoch 77/100\n",
            "20000/20000 - 25s - loss: 7.5032 - mse: 7.5032 - mae: 1.3773 - 25s/epoch - 1ms/step\n",
            "Epoch 78/100\n",
            "20000/20000 - 25s - loss: 6.9903 - mse: 6.9903 - mae: 1.3745 - 25s/epoch - 1ms/step\n",
            "Epoch 79/100\n",
            "20000/20000 - 25s - loss: 8.7273 - mse: 8.7273 - mae: 1.3732 - 25s/epoch - 1ms/step\n",
            "Epoch 80/100\n",
            "20000/20000 - 25s - loss: 7.1057 - mse: 7.1057 - mae: 1.3729 - 25s/epoch - 1ms/step\n",
            "Epoch 81/100\n",
            "20000/20000 - 25s - loss: 7.6557 - mse: 7.6557 - mae: 1.3773 - 25s/epoch - 1ms/step\n",
            "Epoch 82/100\n",
            "20000/20000 - 25s - loss: 7.1916 - mse: 7.1916 - mae: 1.3749 - 25s/epoch - 1ms/step\n",
            "Epoch 83/100\n",
            "20000/20000 - 25s - loss: 7.3600 - mse: 7.3600 - mae: 1.3736 - 25s/epoch - 1ms/step\n",
            "Epoch 84/100\n",
            "20000/20000 - 25s - loss: 7.0858 - mse: 7.0858 - mae: 1.3755 - 25s/epoch - 1ms/step\n",
            "Epoch 85/100\n",
            "20000/20000 - 25s - loss: 6.9865 - mse: 6.9865 - mae: 1.3727 - 25s/epoch - 1ms/step\n",
            "Epoch 86/100\n",
            "20000/20000 - 25s - loss: 6.9720 - mse: 6.9720 - mae: 1.3709 - 25s/epoch - 1ms/step\n",
            "Epoch 87/100\n",
            "20000/20000 - 25s - loss: 8.5387 - mse: 8.5387 - mae: 1.3721 - 25s/epoch - 1ms/step\n",
            "Epoch 88/100\n",
            "20000/20000 - 25s - loss: 7.2197 - mse: 7.2197 - mae: 1.3741 - 25s/epoch - 1ms/step\n",
            "Epoch 89/100\n",
            "20000/20000 - 25s - loss: 7.0491 - mse: 7.0491 - mae: 1.3737 - 25s/epoch - 1ms/step\n",
            "Epoch 90/100\n",
            "20000/20000 - 25s - loss: 7.0805 - mse: 7.0805 - mae: 1.3706 - 25s/epoch - 1ms/step\n",
            "Epoch 91/100\n",
            "20000/20000 - 25s - loss: 7.4201 - mse: 7.4201 - mae: 1.3721 - 25s/epoch - 1ms/step\n",
            "Epoch 92/100\n",
            "20000/20000 - 25s - loss: 6.8725 - mse: 6.8725 - mae: 1.3729 - 25s/epoch - 1ms/step\n",
            "Epoch 93/100\n",
            "20000/20000 - 25s - loss: 6.9478 - mse: 6.9478 - mae: 1.3703 - 25s/epoch - 1ms/step\n",
            "Epoch 94/100\n",
            "20000/20000 - 25s - loss: 7.3883 - mse: 7.3883 - mae: 1.3747 - 25s/epoch - 1ms/step\n",
            "Epoch 95/100\n",
            "20000/20000 - 25s - loss: 7.0761 - mse: 7.0761 - mae: 1.3733 - 25s/epoch - 1ms/step\n",
            "Epoch 96/100\n",
            "20000/20000 - 25s - loss: 7.2328 - mse: 7.2328 - mae: 1.3698 - 25s/epoch - 1ms/step\n",
            "Epoch 97/100\n",
            "20000/20000 - 25s - loss: 7.2374 - mse: 7.2374 - mae: 1.3726 - 25s/epoch - 1ms/step\n",
            "Epoch 98/100\n",
            "20000/20000 - 25s - loss: 7.1505 - mse: 7.1505 - mae: 1.3727 - 25s/epoch - 1ms/step\n",
            "Epoch 99/100\n",
            "20000/20000 - 25s - loss: 7.6869 - mse: 7.6869 - mae: 1.3725 - 25s/epoch - 1ms/step\n",
            "Epoch 100/100\n",
            "20000/20000 - 25s - loss: 6.9855 - mse: 6.9855 - mae: 1.3701 - 25s/epoch - 1ms/step\n",
            "Score for fold 2: loss of 10.89925479888916\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "20000/20000 - 25s - loss: 7.2247 - mse: 7.2247 - mae: 1.3707 - 25s/epoch - 1ms/step\n",
            "Epoch 2/100\n",
            "20000/20000 - 25s - loss: 6.9739 - mse: 6.9739 - mae: 1.3694 - 25s/epoch - 1ms/step\n",
            "Epoch 3/100\n",
            "20000/20000 - 25s - loss: 7.2247 - mse: 7.2247 - mae: 1.3707 - 25s/epoch - 1ms/step\n",
            "Epoch 4/100\n",
            "20000/20000 - 25s - loss: 6.8330 - mse: 6.8330 - mae: 1.3712 - 25s/epoch - 1ms/step\n",
            "Epoch 5/100\n",
            "20000/20000 - 25s - loss: 7.1842 - mse: 7.1842 - mae: 1.3708 - 25s/epoch - 1ms/step\n",
            "Epoch 6/100\n",
            "20000/20000 - 25s - loss: 6.9641 - mse: 6.9641 - mae: 1.3727 - 25s/epoch - 1ms/step\n",
            "Epoch 7/100\n",
            "20000/20000 - 25s - loss: 6.7643 - mse: 6.7643 - mae: 1.3677 - 25s/epoch - 1ms/step\n",
            "Epoch 8/100\n",
            "20000/20000 - 25s - loss: 7.1530 - mse: 7.1530 - mae: 1.3705 - 25s/epoch - 1ms/step\n",
            "Epoch 9/100\n",
            "20000/20000 - 25s - loss: 6.9390 - mse: 6.9390 - mae: 1.3712 - 25s/epoch - 1ms/step\n",
            "Epoch 10/100\n",
            "20000/20000 - 25s - loss: 7.1749 - mse: 7.1749 - mae: 1.3701 - 25s/epoch - 1ms/step\n",
            "Epoch 11/100\n",
            "20000/20000 - 25s - loss: 7.5219 - mse: 7.5219 - mae: 1.3720 - 25s/epoch - 1ms/step\n",
            "Epoch 12/100\n",
            "20000/20000 - 25s - loss: 7.1226 - mse: 7.1226 - mae: 1.3670 - 25s/epoch - 1ms/step\n",
            "Epoch 13/100\n",
            "20000/20000 - 25s - loss: 7.4602 - mse: 7.4602 - mae: 1.3728 - 25s/epoch - 1ms/step\n",
            "Epoch 14/100\n",
            "20000/20000 - 26s - loss: 6.8597 - mse: 6.8597 - mae: 1.3718 - 26s/epoch - 1ms/step\n",
            "Epoch 15/100\n",
            "20000/20000 - 25s - loss: 7.0077 - mse: 7.0077 - mae: 1.3691 - 25s/epoch - 1ms/step\n",
            "Epoch 16/100\n",
            "20000/20000 - 25s - loss: 7.0619 - mse: 7.0619 - mae: 1.3673 - 25s/epoch - 1ms/step\n",
            "Epoch 17/100\n",
            "20000/20000 - 25s - loss: 7.2266 - mse: 7.2266 - mae: 1.3714 - 25s/epoch - 1ms/step\n",
            "Epoch 18/100\n",
            "20000/20000 - 25s - loss: 6.8272 - mse: 6.8272 - mae: 1.3668 - 25s/epoch - 1ms/step\n",
            "Epoch 19/100\n",
            "20000/20000 - 25s - loss: 7.2566 - mse: 7.2566 - mae: 1.3711 - 25s/epoch - 1ms/step\n",
            "Epoch 20/100\n",
            "20000/20000 - 25s - loss: 6.8832 - mse: 6.8832 - mae: 1.3675 - 25s/epoch - 1ms/step\n",
            "Epoch 21/100\n",
            "20000/20000 - 25s - loss: 6.6847 - mse: 6.6847 - mae: 1.3678 - 25s/epoch - 1ms/step\n",
            "Epoch 22/100\n",
            "20000/20000 - 26s - loss: 7.0728 - mse: 7.0728 - mae: 1.3671 - 26s/epoch - 1ms/step\n",
            "Epoch 23/100\n",
            "20000/20000 - 26s - loss: 7.2298 - mse: 7.2298 - mae: 1.3679 - 26s/epoch - 1ms/step\n",
            "Epoch 24/100\n",
            "20000/20000 - 25s - loss: 6.6646 - mse: 6.6646 - mae: 1.3667 - 25s/epoch - 1ms/step\n",
            "Epoch 25/100\n",
            "20000/20000 - 25s - loss: 7.1959 - mse: 7.1959 - mae: 1.3664 - 25s/epoch - 1ms/step\n",
            "Epoch 26/100\n",
            "20000/20000 - 25s - loss: 6.7998 - mse: 6.7998 - mae: 1.3696 - 25s/epoch - 1ms/step\n",
            "Epoch 27/100\n",
            "20000/20000 - 25s - loss: 7.2994 - mse: 7.2994 - mae: 1.3699 - 25s/epoch - 1ms/step\n",
            "Epoch 28/100\n",
            "20000/20000 - 25s - loss: 6.7767 - mse: 6.7767 - mae: 1.3682 - 25s/epoch - 1ms/step\n",
            "Epoch 29/100\n",
            "20000/20000 - 25s - loss: 6.7639 - mse: 6.7639 - mae: 1.3662 - 25s/epoch - 1ms/step\n",
            "Epoch 30/100\n",
            "20000/20000 - 25s - loss: 6.7501 - mse: 6.7501 - mae: 1.3666 - 25s/epoch - 1ms/step\n",
            "Epoch 31/100\n",
            "20000/20000 - 25s - loss: 6.7231 - mse: 6.7231 - mae: 1.3653 - 25s/epoch - 1ms/step\n",
            "Epoch 32/100\n",
            "20000/20000 - 25s - loss: 7.0023 - mse: 7.0023 - mae: 1.3689 - 25s/epoch - 1ms/step\n",
            "Epoch 33/100\n",
            "20000/20000 - 25s - loss: 7.3311 - mse: 7.3311 - mae: 1.3684 - 25s/epoch - 1ms/step\n",
            "Epoch 34/100\n",
            "20000/20000 - 25s - loss: 6.7948 - mse: 6.7948 - mae: 1.3633 - 25s/epoch - 1ms/step\n",
            "Epoch 35/100\n",
            "20000/20000 - 25s - loss: 7.0732 - mse: 7.0732 - mae: 1.3668 - 25s/epoch - 1ms/step\n",
            "Epoch 36/100\n",
            "20000/20000 - 25s - loss: 6.9154 - mse: 6.9154 - mae: 1.3667 - 25s/epoch - 1ms/step\n",
            "Epoch 37/100\n",
            "20000/20000 - 25s - loss: 8.1430 - mse: 8.1430 - mae: 1.3672 - 25s/epoch - 1ms/step\n",
            "Epoch 38/100\n",
            "20000/20000 - 25s - loss: 7.1981 - mse: 7.1981 - mae: 1.3666 - 25s/epoch - 1ms/step\n",
            "Epoch 39/100\n",
            "20000/20000 - 26s - loss: 7.1042 - mse: 7.1042 - mae: 1.3700 - 26s/epoch - 1ms/step\n",
            "Epoch 40/100\n",
            "20000/20000 - 25s - loss: 6.5448 - mse: 6.5448 - mae: 1.3636 - 25s/epoch - 1ms/step\n",
            "Epoch 41/100\n",
            "20000/20000 - 25s - loss: 7.3744 - mse: 7.3744 - mae: 1.3651 - 25s/epoch - 1ms/step\n",
            "Epoch 42/100\n",
            "20000/20000 - 25s - loss: 6.8256 - mse: 6.8256 - mae: 1.3645 - 25s/epoch - 1ms/step\n",
            "Epoch 43/100\n",
            "20000/20000 - 25s - loss: 6.7296 - mse: 6.7296 - mae: 1.3650 - 25s/epoch - 1ms/step\n",
            "Epoch 44/100\n",
            "20000/20000 - 25s - loss: 6.8165 - mse: 6.8165 - mae: 1.3650 - 25s/epoch - 1ms/step\n",
            "Epoch 45/100\n",
            "20000/20000 - 25s - loss: 6.9374 - mse: 6.9374 - mae: 1.3623 - 25s/epoch - 1ms/step\n",
            "Epoch 46/100\n",
            "20000/20000 - 25s - loss: 6.9933 - mse: 6.9933 - mae: 1.3640 - 25s/epoch - 1ms/step\n",
            "Epoch 47/100\n",
            "20000/20000 - 25s - loss: 7.2547 - mse: 7.2547 - mae: 1.3646 - 25s/epoch - 1ms/step\n",
            "Epoch 48/100\n",
            "20000/20000 - 25s - loss: 6.6669 - mse: 6.6669 - mae: 1.3637 - 25s/epoch - 1ms/step\n",
            "Epoch 49/100\n",
            "20000/20000 - 25s - loss: 6.6629 - mse: 6.6629 - mae: 1.3647 - 25s/epoch - 1ms/step\n",
            "Epoch 50/100\n",
            "20000/20000 - 25s - loss: 6.7442 - mse: 6.7442 - mae: 1.3612 - 25s/epoch - 1ms/step\n",
            "Epoch 51/100\n",
            "20000/20000 - 25s - loss: 6.6072 - mse: 6.6072 - mae: 1.3602 - 25s/epoch - 1ms/step\n",
            "Epoch 52/100\n",
            "20000/20000 - 25s - loss: 7.2876 - mse: 7.2876 - mae: 1.3652 - 25s/epoch - 1ms/step\n",
            "Epoch 53/100\n",
            "20000/20000 - 25s - loss: 6.7385 - mse: 6.7385 - mae: 1.3670 - 25s/epoch - 1ms/step\n",
            "Epoch 54/100\n",
            "20000/20000 - 25s - loss: 6.8105 - mse: 6.8105 - mae: 1.3657 - 25s/epoch - 1ms/step\n",
            "Epoch 55/100\n",
            "20000/20000 - 25s - loss: 6.7767 - mse: 6.7767 - mae: 1.3670 - 25s/epoch - 1ms/step\n",
            "Epoch 56/100\n",
            "20000/20000 - 25s - loss: 7.1180 - mse: 7.1180 - mae: 1.3666 - 25s/epoch - 1ms/step\n",
            "Epoch 57/100\n",
            "20000/20000 - 25s - loss: 6.7071 - mse: 6.7071 - mae: 1.3621 - 25s/epoch - 1ms/step\n",
            "Epoch 58/100\n",
            "20000/20000 - 25s - loss: 6.8340 - mse: 6.8340 - mae: 1.3679 - 25s/epoch - 1ms/step\n",
            "Epoch 59/100\n",
            "20000/20000 - 25s - loss: 6.8778 - mse: 6.8778 - mae: 1.3599 - 25s/epoch - 1ms/step\n",
            "Epoch 60/100\n",
            "20000/20000 - 25s - loss: 6.5059 - mse: 6.5059 - mae: 1.3613 - 25s/epoch - 1ms/step\n",
            "Epoch 61/100\n",
            "20000/20000 - 25s - loss: 6.7182 - mse: 6.7182 - mae: 1.3637 - 25s/epoch - 1ms/step\n",
            "Epoch 62/100\n",
            "20000/20000 - 25s - loss: 6.7349 - mse: 6.7349 - mae: 1.3623 - 25s/epoch - 1ms/step\n",
            "Epoch 63/100\n",
            "20000/20000 - 25s - loss: 6.6685 - mse: 6.6685 - mae: 1.3635 - 25s/epoch - 1ms/step\n",
            "Epoch 64/100\n",
            "20000/20000 - 25s - loss: 6.8012 - mse: 6.8012 - mae: 1.3623 - 25s/epoch - 1ms/step\n",
            "Epoch 65/100\n",
            "20000/20000 - 25s - loss: 6.8249 - mse: 6.8249 - mae: 1.3602 - 25s/epoch - 1ms/step\n",
            "Epoch 66/100\n",
            "20000/20000 - 25s - loss: 6.5381 - mse: 6.5381 - mae: 1.3595 - 25s/epoch - 1ms/step\n",
            "Epoch 67/100\n",
            "20000/20000 - 25s - loss: 6.7016 - mse: 6.7016 - mae: 1.3602 - 25s/epoch - 1ms/step\n",
            "Epoch 68/100\n",
            "20000/20000 - 25s - loss: 6.7820 - mse: 6.7820 - mae: 1.3643 - 25s/epoch - 1ms/step\n",
            "Epoch 69/100\n",
            "20000/20000 - 25s - loss: 6.6308 - mse: 6.6308 - mae: 1.3603 - 25s/epoch - 1ms/step\n",
            "Epoch 70/100\n",
            "20000/20000 - 25s - loss: 6.6574 - mse: 6.6574 - mae: 1.3595 - 25s/epoch - 1ms/step\n",
            "Epoch 71/100\n",
            "20000/20000 - 25s - loss: 6.5545 - mse: 6.5545 - mae: 1.3600 - 25s/epoch - 1ms/step\n",
            "Epoch 72/100\n",
            "20000/20000 - 25s - loss: 6.9778 - mse: 6.9778 - mae: 1.3607 - 25s/epoch - 1ms/step\n",
            "Epoch 73/100\n",
            "20000/20000 - 25s - loss: 6.8518 - mse: 6.8518 - mae: 1.3621 - 25s/epoch - 1ms/step\n",
            "Epoch 74/100\n",
            "20000/20000 - 25s - loss: 6.8542 - mse: 6.8542 - mae: 1.3614 - 25s/epoch - 1ms/step\n",
            "Epoch 75/100\n",
            "20000/20000 - 25s - loss: 6.6114 - mse: 6.6114 - mae: 1.3603 - 25s/epoch - 1ms/step\n",
            "Epoch 76/100\n",
            "20000/20000 - 25s - loss: 7.8395 - mse: 7.8395 - mae: 1.3608 - 25s/epoch - 1ms/step\n",
            "Epoch 77/100\n",
            "20000/20000 - 25s - loss: 6.8699 - mse: 6.8699 - mae: 1.3639 - 25s/epoch - 1ms/step\n",
            "Epoch 78/100\n",
            "20000/20000 - 25s - loss: 7.0046 - mse: 7.0046 - mae: 1.3629 - 25s/epoch - 1ms/step\n",
            "Epoch 79/100\n",
            "20000/20000 - 25s - loss: 6.5149 - mse: 6.5149 - mae: 1.3601 - 25s/epoch - 1ms/step\n",
            "Epoch 80/100\n",
            "20000/20000 - 25s - loss: 6.7433 - mse: 6.7433 - mae: 1.3599 - 25s/epoch - 1ms/step\n",
            "Epoch 81/100\n",
            "20000/20000 - 25s - loss: 6.7345 - mse: 6.7345 - mae: 1.3602 - 25s/epoch - 1ms/step\n",
            "Epoch 82/100\n",
            "20000/20000 - 25s - loss: 6.5183 - mse: 6.5183 - mae: 1.3563 - 25s/epoch - 1ms/step\n",
            "Epoch 83/100\n",
            "20000/20000 - 25s - loss: 6.5259 - mse: 6.5259 - mae: 1.3605 - 25s/epoch - 1ms/step\n",
            "Epoch 84/100\n",
            "20000/20000 - 25s - loss: 6.5769 - mse: 6.5769 - mae: 1.3595 - 25s/epoch - 1ms/step\n",
            "Epoch 85/100\n",
            "20000/20000 - 25s - loss: 6.5155 - mse: 6.5155 - mae: 1.3624 - 25s/epoch - 1ms/step\n",
            "Epoch 86/100\n",
            "20000/20000 - 25s - loss: 7.0414 - mse: 7.0414 - mae: 1.3606 - 25s/epoch - 1ms/step\n",
            "Epoch 87/100\n",
            "20000/20000 - 25s - loss: 6.4356 - mse: 6.4356 - mae: 1.3569 - 25s/epoch - 1ms/step\n",
            "Epoch 88/100\n",
            "20000/20000 - 25s - loss: 6.6331 - mse: 6.6331 - mae: 1.3595 - 25s/epoch - 1ms/step\n",
            "Epoch 89/100\n",
            "20000/20000 - 25s - loss: 6.7730 - mse: 6.7730 - mae: 1.3568 - 25s/epoch - 1ms/step\n",
            "Epoch 90/100\n",
            "20000/20000 - 25s - loss: 6.4960 - mse: 6.4960 - mae: 1.3547 - 25s/epoch - 1ms/step\n",
            "Epoch 91/100\n",
            "20000/20000 - 25s - loss: 6.6741 - mse: 6.6741 - mae: 1.3570 - 25s/epoch - 1ms/step\n",
            "Epoch 92/100\n",
            "20000/20000 - 25s - loss: 6.7631 - mse: 6.7631 - mae: 1.3562 - 25s/epoch - 1ms/step\n",
            "Epoch 93/100\n",
            "20000/20000 - 25s - loss: 6.5691 - mse: 6.5691 - mae: 1.3574 - 25s/epoch - 1ms/step\n",
            "Epoch 94/100\n",
            "20000/20000 - 25s - loss: 6.7395 - mse: 6.7395 - mae: 1.3579 - 25s/epoch - 1ms/step\n",
            "Epoch 95/100\n",
            "20000/20000 - 25s - loss: 6.6368 - mse: 6.6368 - mae: 1.3561 - 25s/epoch - 1ms/step\n",
            "Epoch 96/100\n",
            "20000/20000 - 25s - loss: 6.4519 - mse: 6.4519 - mae: 1.3545 - 25s/epoch - 1ms/step\n",
            "Epoch 97/100\n",
            "20000/20000 - 25s - loss: 6.5499 - mse: 6.5499 - mae: 1.3582 - 25s/epoch - 1ms/step\n",
            "Epoch 98/100\n",
            "20000/20000 - 25s - loss: 6.5586 - mse: 6.5586 - mae: 1.3571 - 25s/epoch - 1ms/step\n",
            "Epoch 99/100\n",
            "20000/20000 - 25s - loss: 7.3563 - mse: 7.3563 - mae: 1.3594 - 25s/epoch - 1ms/step\n",
            "Epoch 100/100\n",
            "20000/20000 - 25s - loss: 6.7530 - mse: 6.7530 - mae: 1.3591 - 25s/epoch - 1ms/step\n",
            "Score for fold 3: loss of 9.945383071899414\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "results = model_nn_best.evaluate(testing, labelsForTest, batch_size=10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QTlrRj1zLccs",
        "outputId": "dbc012b2-2b27-4cc7-fc8c-f026e2de3aeb"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10000/10000 [==============================] - 13s 1ms/step - loss: 10.6730 - mse: 10.6730 - mae: 1.4169\n"
          ]
        }
      ]
    }
  ]
}