{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "_4bmjRCyDcQK",
        "outputId": "b8073ea0-43db-4fd8-da18-8e69345418bb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting optuna==0.14.0\n",
            "  Downloading optuna-0.14.0.tar.gz (91 kB)\n",
            "\u001b[K     |████████████████████████████████| 91 kB 3.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: sqlalchemy>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from optuna==0.14.0) (1.4.41)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from optuna==0.14.0) (1.21.6)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from optuna==0.14.0) (1.7.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from optuna==0.14.0) (1.15.0)\n",
            "Collecting cliff\n",
            "  Downloading cliff-3.10.1-py3-none-any.whl (81 kB)\n",
            "\u001b[K     |████████████████████████████████| 81 kB 6.2 MB/s \n",
            "\u001b[?25hCollecting colorlog\n",
            "  Downloading colorlog-6.7.0-py2.py3-none-any.whl (11 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from optuna==0.14.0) (1.3.5)\n",
            "Collecting alembic\n",
            "  Downloading alembic-1.8.1-py3-none-any.whl (209 kB)\n",
            "\u001b[K     |████████████████████████████████| 209 kB 43.5 MB/s \n",
            "\u001b[?25hCollecting typing\n",
            "  Downloading typing-3.7.4.3.tar.gz (78 kB)\n",
            "\u001b[K     |████████████████████████████████| 78 kB 6.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from sqlalchemy>=1.1.0->optuna==0.14.0) (5.0.0)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.7/dist-packages (from sqlalchemy>=1.1.0->optuna==0.14.0) (1.1.3.post0)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.7/dist-packages (from alembic->optuna==0.14.0) (5.10.0)\n",
            "Collecting Mako\n",
            "  Downloading Mako-1.2.3-py3-none-any.whl (78 kB)\n",
            "\u001b[K     |████████████████████████████████| 78 kB 7.0 MB/s \n",
            "\u001b[?25hCollecting autopage>=0.4.0\n",
            "  Downloading autopage-0.5.1-py3-none-any.whl (29 kB)\n",
            "Collecting cmd2>=1.0.0\n",
            "  Downloading cmd2-2.4.2-py3-none-any.whl (147 kB)\n",
            "\u001b[K     |████████████████████████████████| 147 kB 65.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: PrettyTable>=0.7.2 in /usr/local/lib/python3.7/dist-packages (from cliff->optuna==0.14.0) (3.4.1)\n",
            "Requirement already satisfied: pyparsing>=2.1.0 in /usr/local/lib/python3.7/dist-packages (from cliff->optuna==0.14.0) (3.0.9)\n",
            "Collecting pbr!=2.1.0,>=2.0.0\n",
            "  Downloading pbr-5.10.0-py2.py3-none-any.whl (112 kB)\n",
            "\u001b[K     |████████████████████████████████| 112 kB 62.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: PyYAML>=3.12 in /usr/local/lib/python3.7/dist-packages (from cliff->optuna==0.14.0) (6.0)\n",
            "Collecting stevedore>=2.0.1\n",
            "  Downloading stevedore-3.5.1-py3-none-any.whl (50 kB)\n",
            "\u001b[K     |████████████████████████████████| 50 kB 5.8 MB/s \n",
            "\u001b[?25hCollecting pyperclip>=1.6\n",
            "  Downloading pyperclip-1.8.2.tar.gz (20 kB)\n",
            "Requirement already satisfied: attrs>=16.3.0 in /usr/local/lib/python3.7/dist-packages (from cmd2>=1.0.0->cliff->optuna==0.14.0) (22.1.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from cmd2>=1.0.0->cliff->optuna==0.14.0) (4.1.1)\n",
            "Requirement already satisfied: wcwidth>=0.1.7 in /usr/local/lib/python3.7/dist-packages (from cmd2>=1.0.0->cliff->optuna==0.14.0) (0.2.5)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->sqlalchemy>=1.1.0->optuna==0.14.0) (3.9.0)\n",
            "Requirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.7/dist-packages (from Mako->alembic->optuna==0.14.0) (2.0.1)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas->optuna==0.14.0) (2022.4)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->optuna==0.14.0) (2.8.2)\n",
            "Building wheels for collected packages: optuna, pyperclip, typing\n",
            "  Building wheel for optuna (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for optuna: filename=optuna-0.14.0-py3-none-any.whl size=125709 sha256=c30b0f03953706b7512631298988110db5a889d3e2f854961e61e556d95926c8\n",
            "  Stored in directory: /root/.cache/pip/wheels/c1/41/64/03b183676c5d5e978de160cab6268d5b4fb095dff63f720e01\n",
            "  Building wheel for pyperclip (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyperclip: filename=pyperclip-1.8.2-py3-none-any.whl size=11137 sha256=3a6076015077cebc4f373c611f3cfec7933fea1125eb3f9133d1146128dfcfae\n",
            "  Stored in directory: /root/.cache/pip/wheels/9f/18/84/8f69f8b08169c7bae2dde6bd7daf0c19fca8c8e500ee620a28\n",
            "  Building wheel for typing (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for typing: filename=typing-3.7.4.3-py3-none-any.whl size=26325 sha256=1b8f3d0cc30be91e523cb72000c086c4d16a8e19c4f4d7605b9c6d0c4a35b777\n",
            "  Stored in directory: /root/.cache/pip/wheels/35/f3/15/01aa6571f0a72ee6ae7b827c1491c37a1f72d686fd22b43b0e\n",
            "Successfully built optuna pyperclip typing\n",
            "Installing collected packages: pyperclip, pbr, stevedore, Mako, cmd2, autopage, typing, colorlog, cliff, alembic, optuna\n",
            "Successfully installed Mako-1.2.3 alembic-1.8.1 autopage-0.5.1 cliff-3.10.1 cmd2-2.4.2 colorlog-6.7.0 optuna-0.14.0 pbr-5.10.0 pyperclip-1.8.2 stevedore-3.5.1 typing-3.7.4.3\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "typing"
                ]
              }
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting fastparquet\n",
            "  Downloading fastparquet-0.8.1-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.5 MB 5.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: fsspec in /usr/local/lib/python3.7/dist-packages (from fastparquet) (2022.8.2)\n",
            "Requirement already satisfied: pandas>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from fastparquet) (1.3.5)\n",
            "Collecting cramjam>=2.3.0\n",
            "  Downloading cramjam-2.5.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.whl (1.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.6 MB 56.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.18 in /usr/local/lib/python3.7/dist-packages (from fastparquet) (1.21.6)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=1.1.0->fastparquet) (2022.4)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=1.1.0->fastparquet) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas>=1.1.0->fastparquet) (1.15.0)\n",
            "Installing collected packages: cramjam, fastparquet\n",
            "Successfully installed cramjam-2.5.0 fastparquet-0.8.1\n"
          ]
        }
      ],
      "source": [
        "!pip3 install optuna==0.14.0\n",
        "!pip install fastparquet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mIDHemXODo7d"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import keras\n",
        "import tensorflow as tf\n",
        "\n",
        "from keras.datasets import mnist\n",
        "from keras.models import Sequential,Model\n",
        "from keras.layers import Activation, Flatten, Dense, Conv2D, Conv1D,Input\n",
        "from keras.layers import MaxPooling1D, Dropout, BatchNormalization\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.optimizers import SGD, Adagrad, RMSprop, Adam, Adadelta, Adamax, Nadam\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.impute import SimpleImputer\n",
        "from keras.layers.core import *\n",
        "from keras.callbacks import EarlyStopping\n",
        "from keras.callbacks import ReduceLROnPlateau\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "import keras.backend as K\n",
        "import optuna\n",
        "import math\n",
        "import random\n",
        "from copy import deepcopy\n",
        "from sklearn.utils import shuffle\n",
        "\n",
        "from scipy.stats import sem\n",
        "from numpy import mean\n",
        "from numpy import std\n",
        "from sklearn.model_selection import RepeatedKFold\n",
        "from sklearn.model_selection import cross_val_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aM7NmKnGDqs0",
        "outputId": "d8c668cc-1622-45db-8458-aaaec0d6d9aa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Keras 2.9.0\n",
            "TensorFlow 2.9.2\n",
            "Optuna 0.14.0\n"
          ]
        }
      ],
      "source": [
        "print('Keras', keras.__version__)\n",
        "print('TensorFlow', tf.__version__)\n",
        "# import Optuna and OptKeras after Keras\n",
        "print('Optuna', optuna.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iK4fWJp4Dstt"
      },
      "outputs": [],
      "source": [
        "train_df = pd.read_parquet(\"/content/drive/MyDrive/New Data/35features/100K35F_train_main.parquet.snappy\",engine='fastparquet')\n",
        "val_df = pd.read_parquet(\"/content/drive/MyDrive/New Data/35features/100K35F_val_main.parquet.snappy\",engine='fastparquet')\n",
        "test_df = pd.read_parquet(\"/content/drive/MyDrive/New Data/35features/100K35F_test_main.parquet.snappy\",engine='fastparquet')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 488
        },
        "id": "DzJVbvkxFP92",
        "outputId": "e1d9b311-b5ce-4a70-be10-ceced9bac2a0"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "          r_id  r_stars  r_stars_square  r_length  u_friends_count  \\\n",
              "0      2486117        5              25        41                1   \n",
              "1      3238107        5              25        64              174   \n",
              "2      5581800        5              25       239              525   \n",
              "3      5953680        4              16       129              130   \n",
              "4      2208283        1               1       537              223   \n",
              "...        ...      ...             ...       ...              ...   \n",
              "79995  2065766        2               4        61              165   \n",
              "79996  4640630        4              16        74               78   \n",
              "79997  4218981        5              25        98               12   \n",
              "79998  4633311        4              16       656              173   \n",
              "79999   531591        3               9       190                9   \n",
              "\n",
              "       u_review_count  u_month_age  u_comp_avg  u_n_elite_yrs  u_fans  ...  \\\n",
              "0                   8    35.657224    0.000000              0       0  ...   \n",
              "1                 247     1.024218    0.222672              5      27  ...   \n",
              "2                 603   125.863690    2.844113             12      57  ...   \n",
              "3                 183   129.975625    2.005464              5      26  ...   \n",
              "4                   1     0.000111    1.000000              0       0  ...   \n",
              "...               ...          ...         ...            ...     ...  ...   \n",
              "79995             108     0.640257    0.046296              0       2  ...   \n",
              "79996             467    47.038862    0.353319              6      35  ...   \n",
              "79997              88    79.607721    0.295455              2       5  ...   \n",
              "79998             206    41.955031    0.626214              4      13  ...   \n",
              "79999             187    32.499059    0.213904              3       2  ...   \n",
              "\n",
              "       r_stopwords/words  r_digit_cnt  r_noun_cnt  r_Adj_cnt  r_Adv_cnt  \\\n",
              "0                   0.46            2           8          3          2   \n",
              "1                   0.35            1          13          5          2   \n",
              "2                   0.41            2          53         16         20   \n",
              "3                   0.46            0          32         12          6   \n",
              "4                   0.47            2          93         23         42   \n",
              "...                  ...          ...         ...        ...        ...   \n",
              "79995               0.41            0          10          4          5   \n",
              "79996               0.45            0          18          9          2   \n",
              "79997               0.34            0          28          7          7   \n",
              "79998               0.43            2         117         39         61   \n",
              "79999               0.44            2          44         11         12   \n",
              "\n",
              "       r_capital_word_cnt  r_quoted_word_cnt  r_hashtag_cnt  r_exclam_cnt  \\\n",
              "0                       1                  0              0             0   \n",
              "1                       4                  0              0             2   \n",
              "2                       8                  0              0             1   \n",
              "3                       2                  0              0             0   \n",
              "4                      26                  0              0             0   \n",
              "...                   ...                ...            ...           ...   \n",
              "79995                   5                  0              0             1   \n",
              "79996                   3                  0              0             1   \n",
              "79997                   3                  0              0             1   \n",
              "79998                  35                  0              0             7   \n",
              "79999                   7                  0              0             0   \n",
              "\n",
              "       r_useful  \n",
              "0             2  \n",
              "1             2  \n",
              "2             3  \n",
              "3             4  \n",
              "4             6  \n",
              "...         ...  \n",
              "79995         2  \n",
              "79996         2  \n",
              "79997         2  \n",
              "79998         6  \n",
              "79999         2  \n",
              "\n",
              "[80000 rows x 37 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-dd826936-ea89-4b0b-8030-27e38d45a666\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>r_id</th>\n",
              "      <th>r_stars</th>\n",
              "      <th>r_stars_square</th>\n",
              "      <th>r_length</th>\n",
              "      <th>u_friends_count</th>\n",
              "      <th>u_review_count</th>\n",
              "      <th>u_month_age</th>\n",
              "      <th>u_comp_avg</th>\n",
              "      <th>u_n_elite_yrs</th>\n",
              "      <th>u_fans</th>\n",
              "      <th>...</th>\n",
              "      <th>r_stopwords/words</th>\n",
              "      <th>r_digit_cnt</th>\n",
              "      <th>r_noun_cnt</th>\n",
              "      <th>r_Adj_cnt</th>\n",
              "      <th>r_Adv_cnt</th>\n",
              "      <th>r_capital_word_cnt</th>\n",
              "      <th>r_quoted_word_cnt</th>\n",
              "      <th>r_hashtag_cnt</th>\n",
              "      <th>r_exclam_cnt</th>\n",
              "      <th>r_useful</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2486117</td>\n",
              "      <td>5</td>\n",
              "      <td>25</td>\n",
              "      <td>41</td>\n",
              "      <td>1</td>\n",
              "      <td>8</td>\n",
              "      <td>35.657224</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.46</td>\n",
              "      <td>2</td>\n",
              "      <td>8</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>3238107</td>\n",
              "      <td>5</td>\n",
              "      <td>25</td>\n",
              "      <td>64</td>\n",
              "      <td>174</td>\n",
              "      <td>247</td>\n",
              "      <td>1.024218</td>\n",
              "      <td>0.222672</td>\n",
              "      <td>5</td>\n",
              "      <td>27</td>\n",
              "      <td>...</td>\n",
              "      <td>0.35</td>\n",
              "      <td>1</td>\n",
              "      <td>13</td>\n",
              "      <td>5</td>\n",
              "      <td>2</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>5581800</td>\n",
              "      <td>5</td>\n",
              "      <td>25</td>\n",
              "      <td>239</td>\n",
              "      <td>525</td>\n",
              "      <td>603</td>\n",
              "      <td>125.863690</td>\n",
              "      <td>2.844113</td>\n",
              "      <td>12</td>\n",
              "      <td>57</td>\n",
              "      <td>...</td>\n",
              "      <td>0.41</td>\n",
              "      <td>2</td>\n",
              "      <td>53</td>\n",
              "      <td>16</td>\n",
              "      <td>20</td>\n",
              "      <td>8</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>5953680</td>\n",
              "      <td>4</td>\n",
              "      <td>16</td>\n",
              "      <td>129</td>\n",
              "      <td>130</td>\n",
              "      <td>183</td>\n",
              "      <td>129.975625</td>\n",
              "      <td>2.005464</td>\n",
              "      <td>5</td>\n",
              "      <td>26</td>\n",
              "      <td>...</td>\n",
              "      <td>0.46</td>\n",
              "      <td>0</td>\n",
              "      <td>32</td>\n",
              "      <td>12</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2208283</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>537</td>\n",
              "      <td>223</td>\n",
              "      <td>1</td>\n",
              "      <td>0.000111</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.47</td>\n",
              "      <td>2</td>\n",
              "      <td>93</td>\n",
              "      <td>23</td>\n",
              "      <td>42</td>\n",
              "      <td>26</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>79995</th>\n",
              "      <td>2065766</td>\n",
              "      <td>2</td>\n",
              "      <td>4</td>\n",
              "      <td>61</td>\n",
              "      <td>165</td>\n",
              "      <td>108</td>\n",
              "      <td>0.640257</td>\n",
              "      <td>0.046296</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>...</td>\n",
              "      <td>0.41</td>\n",
              "      <td>0</td>\n",
              "      <td>10</td>\n",
              "      <td>4</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>79996</th>\n",
              "      <td>4640630</td>\n",
              "      <td>4</td>\n",
              "      <td>16</td>\n",
              "      <td>74</td>\n",
              "      <td>78</td>\n",
              "      <td>467</td>\n",
              "      <td>47.038862</td>\n",
              "      <td>0.353319</td>\n",
              "      <td>6</td>\n",
              "      <td>35</td>\n",
              "      <td>...</td>\n",
              "      <td>0.45</td>\n",
              "      <td>0</td>\n",
              "      <td>18</td>\n",
              "      <td>9</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>79997</th>\n",
              "      <td>4218981</td>\n",
              "      <td>5</td>\n",
              "      <td>25</td>\n",
              "      <td>98</td>\n",
              "      <td>12</td>\n",
              "      <td>88</td>\n",
              "      <td>79.607721</td>\n",
              "      <td>0.295455</td>\n",
              "      <td>2</td>\n",
              "      <td>5</td>\n",
              "      <td>...</td>\n",
              "      <td>0.34</td>\n",
              "      <td>0</td>\n",
              "      <td>28</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>79998</th>\n",
              "      <td>4633311</td>\n",
              "      <td>4</td>\n",
              "      <td>16</td>\n",
              "      <td>656</td>\n",
              "      <td>173</td>\n",
              "      <td>206</td>\n",
              "      <td>41.955031</td>\n",
              "      <td>0.626214</td>\n",
              "      <td>4</td>\n",
              "      <td>13</td>\n",
              "      <td>...</td>\n",
              "      <td>0.43</td>\n",
              "      <td>2</td>\n",
              "      <td>117</td>\n",
              "      <td>39</td>\n",
              "      <td>61</td>\n",
              "      <td>35</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>7</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>79999</th>\n",
              "      <td>531591</td>\n",
              "      <td>3</td>\n",
              "      <td>9</td>\n",
              "      <td>190</td>\n",
              "      <td>9</td>\n",
              "      <td>187</td>\n",
              "      <td>32.499059</td>\n",
              "      <td>0.213904</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>...</td>\n",
              "      <td>0.44</td>\n",
              "      <td>2</td>\n",
              "      <td>44</td>\n",
              "      <td>11</td>\n",
              "      <td>12</td>\n",
              "      <td>7</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>80000 rows × 37 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-dd826936-ea89-4b0b-8030-27e38d45a666')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-dd826936-ea89-4b0b-8030-27e38d45a666 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-dd826936-ea89-4b0b-8030-27e38d45a666');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "train_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r1F2B-gBEMiM"
      },
      "outputs": [],
      "source": [
        "training_withaim=train_df.drop(labels=\"r_id\", axis=1)\n",
        "valing_withaim=val_df.drop(labels=\"r_id\",axis=1)\n",
        "testing_withaim=test_df.drop(labels=\"r_id\", axis=1)\n",
        "\n",
        "#Check the NaN in data and drop them\n",
        "imp_train=SimpleImputer(missing_values=np.NaN)\n",
        "training=pd.DataFrame(imp_train.fit_transform(training_withaim))\n",
        "\n",
        "imp_test=SimpleImputer(missing_values=np.NaN)\n",
        "testing=pd.DataFrame(imp_test.fit_transform(testing_withaim))\n",
        "\n",
        "imp_val=SimpleImputer(missing_values=np.NaN)\n",
        "valing=pd.DataFrame(imp_val.fit_transform(valing_withaim))\n",
        "\n",
        "# There aren't any nan data in the dataframe \n",
        "training = training.iloc[: , 0:35]\n",
        "testing = testing.iloc[: , 0:35]\n",
        "valing=valing.iloc[:,0:35]\n",
        "scaler = StandardScaler()\n",
        "\n",
        "# scale skewed features\n",
        "std_train_df = train_df.copy(deep=True)\n",
        "std_train_df = scaler.fit_transform(training)\n",
        "std_test_df = test_df.copy(deep=True)\n",
        "std_test_df = scaler.transform(testing)\n",
        "std_val_df = val_df.copy(deep=True)\n",
        "std_val_df = scaler.transform(valing)\n",
        "#std_test_df[['r_stars','r_stars_square','r_length', 'u_friends_count', 'u_review_count', 'u_month_age', 'b_stars','b_review_count','r_rea']] = scaler.transform(test_df[['r_stars','r_stars_square','r_length', 'u_friends_count', 'u_review_count', 'u_month_age', 'b_stars','b_review_count','r_rea']])\n",
        "\n",
        "std_train_df = pd.DataFrame(std_train_df)\n",
        "std_val_df = pd.DataFrame(std_val_df)\n",
        "std_test_df = pd.DataFrame(std_test_df)\n",
        "\n",
        "training = std_train_df.iloc[: , 0:36]\n",
        "valing = std_val_df.iloc[: , 0:36]\n",
        "testing = std_test_df.iloc[: , 0:36]\n",
        "\n",
        "labelsForTrain=training_withaim.iloc[: , -1]\n",
        "labelsForVal=valing_withaim.iloc[: , -1]\n",
        "labelsForTest=testing_withaim.iloc[: , -1]\n",
        "input_shape = training.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kt9gM3cRieQ9",
        "outputId": "f86d526d-e4f6-4e60-e409-71f163b8a7aa"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0        2\n",
              "1        2\n",
              "2        3\n",
              "3        4\n",
              "4        6\n",
              "        ..\n",
              "79995    2\n",
              "79996    2\n",
              "79997    2\n",
              "79998    6\n",
              "79999    2\n",
              "Name: r_useful, Length: 80000, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "labelsForTrain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "Ohik52yFigXK",
        "outputId": "09911d6f-4b38-4c7f-819f-1cfb0cb8eb51"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "             0         1         2         3         4         5         6   \\\n",
              "0      0.902431  0.996586 -0.805509 -0.329454 -0.380837 -0.205818 -0.188112   \n",
              "1      0.902431  0.996586 -0.603075 -0.026023  0.120685 -1.203206 -0.110662   \n",
              "2      0.902431  0.996586  0.937184  0.589607  0.867722  2.392019  0.801132   \n",
              "3      0.253649  0.062198 -0.030979 -0.103196 -0.013614  2.510438  0.509432   \n",
              "4     -1.692696 -1.495115  3.560026  0.059919 -0.395526 -1.232699  0.159709   \n",
              "...         ...       ...       ...       ...       ...       ...       ...   \n",
              "79995 -1.043914 -1.183653 -0.629480 -0.041809 -0.170995 -1.214264 -0.172010   \n",
              "79996  0.253649  0.062198 -0.515060 -0.194401  0.582337  0.121960 -0.065220   \n",
              "79997  0.902431  0.996586 -0.303825 -0.310161 -0.212964  1.059903 -0.085347   \n",
              "79998  0.253649  0.062198  4.607402 -0.027777  0.034650 -0.024449  0.029698   \n",
              "79999 -0.395132 -0.664548  0.505912 -0.315422 -0.005220 -0.296769 -0.113712   \n",
              "\n",
              "             7         8         9   ...        25        26        27  \\\n",
              "0     -0.609417 -0.167995  1.408241  ...  1.009502  0.729169  1.198104   \n",
              "1      1.019071  0.012418  0.583210  ...  0.361877 -1.005015  0.350241   \n",
              "2      3.298953  0.212877  0.534678  ... -1.025892 -0.059096  1.198104   \n",
              "3      1.019071  0.005736 -0.084095  ... -1.488482  0.729169 -0.497621   \n",
              "4     -0.609417 -0.167995 -3.299292  ... -2.691215  0.886823  1.198104   \n",
              "...         ...       ...       ...  ...       ...       ...       ...   \n",
              "79995 -0.609417 -0.154631 -0.241822  ...  0.361877 -0.059096 -0.497621   \n",
              "79996  1.344768  0.065874 -0.023431  ...  0.454395  0.571516 -0.497621   \n",
              "79997  0.041978 -0.134585  0.316288  ...  0.269359 -1.162668 -0.497621   \n",
              "79998  0.693373 -0.081129  0.619608  ... -2.136107  0.256210  1.198104   \n",
              "79999  0.367676 -0.154631 -0.351017  ... -0.563302  0.413863  1.198104   \n",
              "\n",
              "             28        29        30        31        32        33        34  \n",
              "0     -0.797991 -0.916574 -0.879018 -0.602286 -0.036305 -0.012863 -0.565147  \n",
              "1     -0.567289 -0.699387 -0.879018 -0.069673 -0.036305 -0.012863  0.222741  \n",
              "2      1.278321  0.495144  1.127443  0.640479 -0.036305 -0.012863 -0.171203  \n",
              "3      0.309376  0.060769 -0.433138 -0.424748 -0.036305 -0.012863 -0.565147  \n",
              "4      3.123932  1.255300  3.579784  3.836160 -0.036305 -0.012863 -0.565147  \n",
              "...         ...       ...       ...       ...       ...       ...       ...  \n",
              "79995 -0.705710 -0.807980 -0.544608  0.107865 -0.036305 -0.012863 -0.171203  \n",
              "79996 -0.336588 -0.265012 -0.879018 -0.247210 -0.036305 -0.012863 -0.171203  \n",
              "79997  0.124815 -0.482199 -0.321668 -0.247210 -0.036305 -0.012863 -0.171203  \n",
              "79998  4.231298  2.992799  5.697715  5.434001 -0.036305 -0.012863  2.192459  \n",
              "79999  0.863059 -0.047825  0.235683  0.462941 -0.036305 -0.012863 -0.565147  \n",
              "\n",
              "[80000 rows x 35 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-7f2bd76f-6101-42fc-b654-1351c845342d\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>...</th>\n",
              "      <th>25</th>\n",
              "      <th>26</th>\n",
              "      <th>27</th>\n",
              "      <th>28</th>\n",
              "      <th>29</th>\n",
              "      <th>30</th>\n",
              "      <th>31</th>\n",
              "      <th>32</th>\n",
              "      <th>33</th>\n",
              "      <th>34</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.902431</td>\n",
              "      <td>0.996586</td>\n",
              "      <td>-0.805509</td>\n",
              "      <td>-0.329454</td>\n",
              "      <td>-0.380837</td>\n",
              "      <td>-0.205818</td>\n",
              "      <td>-0.188112</td>\n",
              "      <td>-0.609417</td>\n",
              "      <td>-0.167995</td>\n",
              "      <td>1.408241</td>\n",
              "      <td>...</td>\n",
              "      <td>1.009502</td>\n",
              "      <td>0.729169</td>\n",
              "      <td>1.198104</td>\n",
              "      <td>-0.797991</td>\n",
              "      <td>-0.916574</td>\n",
              "      <td>-0.879018</td>\n",
              "      <td>-0.602286</td>\n",
              "      <td>-0.036305</td>\n",
              "      <td>-0.012863</td>\n",
              "      <td>-0.565147</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.902431</td>\n",
              "      <td>0.996586</td>\n",
              "      <td>-0.603075</td>\n",
              "      <td>-0.026023</td>\n",
              "      <td>0.120685</td>\n",
              "      <td>-1.203206</td>\n",
              "      <td>-0.110662</td>\n",
              "      <td>1.019071</td>\n",
              "      <td>0.012418</td>\n",
              "      <td>0.583210</td>\n",
              "      <td>...</td>\n",
              "      <td>0.361877</td>\n",
              "      <td>-1.005015</td>\n",
              "      <td>0.350241</td>\n",
              "      <td>-0.567289</td>\n",
              "      <td>-0.699387</td>\n",
              "      <td>-0.879018</td>\n",
              "      <td>-0.069673</td>\n",
              "      <td>-0.036305</td>\n",
              "      <td>-0.012863</td>\n",
              "      <td>0.222741</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.902431</td>\n",
              "      <td>0.996586</td>\n",
              "      <td>0.937184</td>\n",
              "      <td>0.589607</td>\n",
              "      <td>0.867722</td>\n",
              "      <td>2.392019</td>\n",
              "      <td>0.801132</td>\n",
              "      <td>3.298953</td>\n",
              "      <td>0.212877</td>\n",
              "      <td>0.534678</td>\n",
              "      <td>...</td>\n",
              "      <td>-1.025892</td>\n",
              "      <td>-0.059096</td>\n",
              "      <td>1.198104</td>\n",
              "      <td>1.278321</td>\n",
              "      <td>0.495144</td>\n",
              "      <td>1.127443</td>\n",
              "      <td>0.640479</td>\n",
              "      <td>-0.036305</td>\n",
              "      <td>-0.012863</td>\n",
              "      <td>-0.171203</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.253649</td>\n",
              "      <td>0.062198</td>\n",
              "      <td>-0.030979</td>\n",
              "      <td>-0.103196</td>\n",
              "      <td>-0.013614</td>\n",
              "      <td>2.510438</td>\n",
              "      <td>0.509432</td>\n",
              "      <td>1.019071</td>\n",
              "      <td>0.005736</td>\n",
              "      <td>-0.084095</td>\n",
              "      <td>...</td>\n",
              "      <td>-1.488482</td>\n",
              "      <td>0.729169</td>\n",
              "      <td>-0.497621</td>\n",
              "      <td>0.309376</td>\n",
              "      <td>0.060769</td>\n",
              "      <td>-0.433138</td>\n",
              "      <td>-0.424748</td>\n",
              "      <td>-0.036305</td>\n",
              "      <td>-0.012863</td>\n",
              "      <td>-0.565147</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>-1.692696</td>\n",
              "      <td>-1.495115</td>\n",
              "      <td>3.560026</td>\n",
              "      <td>0.059919</td>\n",
              "      <td>-0.395526</td>\n",
              "      <td>-1.232699</td>\n",
              "      <td>0.159709</td>\n",
              "      <td>-0.609417</td>\n",
              "      <td>-0.167995</td>\n",
              "      <td>-3.299292</td>\n",
              "      <td>...</td>\n",
              "      <td>-2.691215</td>\n",
              "      <td>0.886823</td>\n",
              "      <td>1.198104</td>\n",
              "      <td>3.123932</td>\n",
              "      <td>1.255300</td>\n",
              "      <td>3.579784</td>\n",
              "      <td>3.836160</td>\n",
              "      <td>-0.036305</td>\n",
              "      <td>-0.012863</td>\n",
              "      <td>-0.565147</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>79995</th>\n",
              "      <td>-1.043914</td>\n",
              "      <td>-1.183653</td>\n",
              "      <td>-0.629480</td>\n",
              "      <td>-0.041809</td>\n",
              "      <td>-0.170995</td>\n",
              "      <td>-1.214264</td>\n",
              "      <td>-0.172010</td>\n",
              "      <td>-0.609417</td>\n",
              "      <td>-0.154631</td>\n",
              "      <td>-0.241822</td>\n",
              "      <td>...</td>\n",
              "      <td>0.361877</td>\n",
              "      <td>-0.059096</td>\n",
              "      <td>-0.497621</td>\n",
              "      <td>-0.705710</td>\n",
              "      <td>-0.807980</td>\n",
              "      <td>-0.544608</td>\n",
              "      <td>0.107865</td>\n",
              "      <td>-0.036305</td>\n",
              "      <td>-0.012863</td>\n",
              "      <td>-0.171203</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>79996</th>\n",
              "      <td>0.253649</td>\n",
              "      <td>0.062198</td>\n",
              "      <td>-0.515060</td>\n",
              "      <td>-0.194401</td>\n",
              "      <td>0.582337</td>\n",
              "      <td>0.121960</td>\n",
              "      <td>-0.065220</td>\n",
              "      <td>1.344768</td>\n",
              "      <td>0.065874</td>\n",
              "      <td>-0.023431</td>\n",
              "      <td>...</td>\n",
              "      <td>0.454395</td>\n",
              "      <td>0.571516</td>\n",
              "      <td>-0.497621</td>\n",
              "      <td>-0.336588</td>\n",
              "      <td>-0.265012</td>\n",
              "      <td>-0.879018</td>\n",
              "      <td>-0.247210</td>\n",
              "      <td>-0.036305</td>\n",
              "      <td>-0.012863</td>\n",
              "      <td>-0.171203</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>79997</th>\n",
              "      <td>0.902431</td>\n",
              "      <td>0.996586</td>\n",
              "      <td>-0.303825</td>\n",
              "      <td>-0.310161</td>\n",
              "      <td>-0.212964</td>\n",
              "      <td>1.059903</td>\n",
              "      <td>-0.085347</td>\n",
              "      <td>0.041978</td>\n",
              "      <td>-0.134585</td>\n",
              "      <td>0.316288</td>\n",
              "      <td>...</td>\n",
              "      <td>0.269359</td>\n",
              "      <td>-1.162668</td>\n",
              "      <td>-0.497621</td>\n",
              "      <td>0.124815</td>\n",
              "      <td>-0.482199</td>\n",
              "      <td>-0.321668</td>\n",
              "      <td>-0.247210</td>\n",
              "      <td>-0.036305</td>\n",
              "      <td>-0.012863</td>\n",
              "      <td>-0.171203</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>79998</th>\n",
              "      <td>0.253649</td>\n",
              "      <td>0.062198</td>\n",
              "      <td>4.607402</td>\n",
              "      <td>-0.027777</td>\n",
              "      <td>0.034650</td>\n",
              "      <td>-0.024449</td>\n",
              "      <td>0.029698</td>\n",
              "      <td>0.693373</td>\n",
              "      <td>-0.081129</td>\n",
              "      <td>0.619608</td>\n",
              "      <td>...</td>\n",
              "      <td>-2.136107</td>\n",
              "      <td>0.256210</td>\n",
              "      <td>1.198104</td>\n",
              "      <td>4.231298</td>\n",
              "      <td>2.992799</td>\n",
              "      <td>5.697715</td>\n",
              "      <td>5.434001</td>\n",
              "      <td>-0.036305</td>\n",
              "      <td>-0.012863</td>\n",
              "      <td>2.192459</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>79999</th>\n",
              "      <td>-0.395132</td>\n",
              "      <td>-0.664548</td>\n",
              "      <td>0.505912</td>\n",
              "      <td>-0.315422</td>\n",
              "      <td>-0.005220</td>\n",
              "      <td>-0.296769</td>\n",
              "      <td>-0.113712</td>\n",
              "      <td>0.367676</td>\n",
              "      <td>-0.154631</td>\n",
              "      <td>-0.351017</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.563302</td>\n",
              "      <td>0.413863</td>\n",
              "      <td>1.198104</td>\n",
              "      <td>0.863059</td>\n",
              "      <td>-0.047825</td>\n",
              "      <td>0.235683</td>\n",
              "      <td>0.462941</td>\n",
              "      <td>-0.036305</td>\n",
              "      <td>-0.012863</td>\n",
              "      <td>-0.565147</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>80000 rows × 35 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-7f2bd76f-6101-42fc-b654-1351c845342d')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-7f2bd76f-6101-42fc-b654-1351c845342d button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-7f2bd76f-6101-42fc-b654-1351c845342d');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bz522wVVHDEh"
      },
      "outputs": [],
      "source": [
        "def create_model(activation, num_hidden_layer, num_hidden_unit):\n",
        "  inputs = Input(shape=(training.shape[1],))\n",
        "  model = inputs\n",
        "  for i in range(1,num_hidden_layer):\n",
        "    model = Dense(num_hidden_unit, activation=activation,)(model)\n",
        "        \n",
        "        \n",
        "  model = Dense(1,)(model)\n",
        "  model = Model(inputs, model)\n",
        "\n",
        "  return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kDK6LaeoHXd6"
      },
      "outputs": [],
      "source": [
        "kfold = KFold(n_splits=5, shuffle=True)\n",
        "def objective(trial:optuna.Trial,data_train,result_train):\n",
        "  K.clear_session()\n",
        "    \n",
        "  activation = trial.suggest_categorical('activation',['relu','tanh','linear'])\n",
        "  #Leave fewer optimizer\n",
        "    \n",
        "  num_hidden_layer = trial.suggest_int('num_hidden_layer',2,4)\n",
        "  #get more features per layer, add num of hidden unit if have time\n",
        "\n",
        "  #define the number of unit with 2^n\n",
        "  i = trial.suggest_int('i',6,10)\n",
        "  num_hidden_unit = 2**i\n",
        "  \n",
        "  #Try to adjust learning_rate\n",
        "\n",
        "  learning_rate = trial.suggest_loguniform('learning_rate', 0.0001,0.01)\n",
        "  \n",
        "  # Gradient Clipping\n",
        "\n",
        "  optimizer = Adam(learning_rate=learning_rate,clipnorm=1.0)\n",
        "    \n",
        "  num_folds = 3\n",
        "\n",
        "  loss_per_fold = []\n",
        "  es = EarlyStopping(monitor='val_mse', patience=5)\n",
        "\n",
        "  model = create_model(activation,num_hidden_layer,num_hidden_unit)\n",
        "  model_list.append(model)\n",
        "  model.compile(loss='mse',optimizer=optimizer,metrics=['mse','mae'])\n",
        "  #learning scheduler\n",
        "  reduce_lr = ReduceLROnPlateau(monitor='val_mse', factor=0.2,\n",
        "                              patience=5, min_lr=0.001)\n",
        "  fold_no=1\n",
        "      # Fit data to model\n",
        "  score_int=[]\n",
        "\n",
        "  for train,test in kfold.split(data_train,result_train):\n",
        "\n",
        "    print('------------------------------------------------------------------------')\n",
        "    print(f'Training for fold {fold_no} ...')\n",
        "\n",
        "    new_train_CV=data_train.iloc[train]\n",
        "    new_label=result_train.iloc[train]\n",
        "\n",
        "    val_CV=data_train.iloc[test]\n",
        "    val_result=result_train.iloc[test]\n",
        "\n",
        "    # Fit data to model\n",
        "    history = model.fit(new_train_CV,new_label,\n",
        "                batch_size=64,\n",
        "                epochs=100,\n",
        "                verbose=2,\n",
        "                validation_data=(val_CV,val_result),\n",
        "                validation_batch_size=64,\n",
        "                callbacks=[es,reduce_lr])\n",
        "    \n",
        "    scores=model.evaluate(val_CV,val_result,verbose=0)\n",
        "    print(f'Score for fold {fold_no}: {model.metrics_names[0]} of {scores[0]}')\n",
        "    loss_per_fold.append(scores[0])\n",
        "    # Increase fold number\n",
        "    score_int.append(round(scores[1],2))\n",
        "    fold_no = fold_no + 1  \n",
        "    \n",
        "  mse = np.array(history.history['mse'])\n",
        "\n",
        "  #loss_per_fold.append(scores[0])\n",
        "\n",
        "  history_list.append(history)\n",
        "  return np.mean(score_int)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-svTHldV9Wi5",
        "outputId": "ee2797b9-96ba-40be-d131-99daf503de1e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 14.8704 - mse: 14.8704 - mae: 1.6258 - val_loss: 13.2813 - val_mse: 13.2813 - val_mae: 1.5626 - lr: 0.0012 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 13.2753 - mse: 13.2753 - mae: 1.5576 - val_loss: 13.2724 - val_mse: 13.2724 - val_mae: 1.5540 - lr: 0.0012 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 13.3046 - mse: 13.3046 - mae: 1.5554 - val_loss: 13.2669 - val_mse: 13.2669 - val_mae: 1.5321 - lr: 0.0012 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 13.3289 - mse: 13.3289 - mae: 1.5515 - val_loss: 13.0617 - val_mse: 13.0617 - val_mae: 1.5443 - lr: 0.0012 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 13.3160 - mse: 13.3160 - mae: 1.5527 - val_loss: 13.0665 - val_mse: 13.0665 - val_mae: 1.5413 - lr: 0.0012 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 13.3394 - mse: 13.3394 - mae: 1.5474 - val_loss: 13.0365 - val_mse: 13.0365 - val_mae: 1.5520 - lr: 0.0012 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 13.2657 - mse: 13.2657 - mae: 1.5476 - val_loss: 13.0703 - val_mse: 13.0703 - val_mae: 1.5517 - lr: 0.0012 - 2s/epoch - 2ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 2s - loss: 13.2570 - mse: 13.2570 - mae: 1.5497 - val_loss: 13.1010 - val_mse: 13.1010 - val_mae: 1.5350 - lr: 0.0012 - 2s/epoch - 2ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 2s - loss: 13.2616 - mse: 13.2616 - mae: 1.5485 - val_loss: 13.1255 - val_mse: 13.1255 - val_mae: 1.5432 - lr: 0.0012 - 2s/epoch - 2ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 2s - loss: 13.3045 - mse: 13.3045 - mae: 1.5517 - val_loss: 13.0549 - val_mse: 13.0549 - val_mae: 1.5708 - lr: 0.0012 - 2s/epoch - 2ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 2s - loss: 13.2620 - mse: 13.2620 - mae: 1.5501 - val_loss: 13.1238 - val_mse: 13.1238 - val_mae: 1.5702 - lr: 0.0012 - 2s/epoch - 2ms/step\n",
            "Score for fold 1: loss of 13.123797416687012\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 14.3119 - mse: 14.3119 - mae: 1.5593 - val_loss: 8.8277 - val_mse: 8.8277 - val_mae: 1.5085 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 1s - loss: 14.2917 - mse: 14.2917 - mae: 1.5632 - val_loss: 8.8884 - val_mse: 8.8884 - val_mae: 1.4984 - lr: 0.0010 - 1s/epoch - 1ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 14.2124 - mse: 14.2124 - mae: 1.5603 - val_loss: 8.8899 - val_mse: 8.8899 - val_mae: 1.5107 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 14.2949 - mse: 14.2949 - mae: 1.5575 - val_loss: 8.8629 - val_mse: 8.8629 - val_mae: 1.5263 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 1s - loss: 14.2334 - mse: 14.2334 - mae: 1.5626 - val_loss: 8.8940 - val_mse: 8.8940 - val_mae: 1.5007 - lr: 0.0010 - 1s/epoch - 1ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 14.2696 - mse: 14.2696 - mae: 1.5606 - val_loss: 8.8559 - val_mse: 8.8559 - val_mae: 1.5207 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Score for fold 2: loss of 8.855948448181152\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 1s - loss: 13.7936 - mse: 13.7936 - mae: 1.5444 - val_loss: 10.8063 - val_mse: 10.8063 - val_mae: 1.5454 - lr: 0.0010 - 1s/epoch - 1ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 1s - loss: 13.8199 - mse: 13.8199 - mae: 1.5438 - val_loss: 10.8189 - val_mse: 10.8189 - val_mae: 1.5875 - lr: 0.0010 - 1s/epoch - 1ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 1s - loss: 13.8098 - mse: 13.8098 - mae: 1.5539 - val_loss: 11.0046 - val_mse: 11.0046 - val_mae: 1.5353 - lr: 0.0010 - 1s/epoch - 1ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 13.8370 - mse: 13.8370 - mae: 1.5403 - val_loss: 10.8382 - val_mse: 10.8382 - val_mae: 1.5635 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 1s - loss: 13.7638 - mse: 13.7638 - mae: 1.5467 - val_loss: 10.9310 - val_mse: 10.9310 - val_mae: 1.5477 - lr: 0.0010 - 1s/epoch - 1ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 1s - loss: 13.7576 - mse: 13.7576 - mae: 1.5443 - val_loss: 10.8537 - val_mse: 10.8537 - val_mae: 1.5465 - lr: 0.0010 - 1s/epoch - 1ms/step\n",
            "Score for fold 3: loss of 10.853745460510254\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 11.6486 - mse: 11.6486 - mae: 1.5464 - val_loss: 19.4182 - val_mse: 19.4182 - val_mae: 1.6051 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 1s - loss: 11.7338 - mse: 11.7338 - mae: 1.5429 - val_loss: 19.3906 - val_mse: 19.3906 - val_mae: 1.5808 - lr: 0.0010 - 1s/epoch - 1ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 11.6930 - mse: 11.6930 - mae: 1.5430 - val_loss: 19.3779 - val_mse: 19.3779 - val_mae: 1.5869 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 1s - loss: 11.7138 - mse: 11.7138 - mae: 1.5431 - val_loss: 19.5039 - val_mse: 19.5039 - val_mae: 1.5962 - lr: 0.0010 - 1s/epoch - 1ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 1s - loss: 11.7654 - mse: 11.7654 - mae: 1.5425 - val_loss: 19.3914 - val_mse: 19.3914 - val_mae: 1.5583 - lr: 0.0010 - 1s/epoch - 1ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 1s - loss: 11.6806 - mse: 11.6806 - mae: 1.5439 - val_loss: 19.3479 - val_mse: 19.3479 - val_mae: 1.5253 - lr: 0.0010 - 1s/epoch - 1ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 1s - loss: 11.6670 - mse: 11.6670 - mae: 1.5398 - val_loss: 19.4024 - val_mse: 19.4024 - val_mae: 1.5664 - lr: 0.0010 - 1s/epoch - 1ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 2s - loss: 11.6993 - mse: 11.6993 - mae: 1.5412 - val_loss: 19.3261 - val_mse: 19.3261 - val_mae: 1.5643 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 1s - loss: 11.7017 - mse: 11.7017 - mae: 1.5446 - val_loss: 19.4600 - val_mse: 19.4600 - val_mae: 1.5894 - lr: 0.0010 - 1s/epoch - 1ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 1s - loss: 11.6936 - mse: 11.6936 - mae: 1.5464 - val_loss: 19.4049 - val_mse: 19.4049 - val_mae: 1.5275 - lr: 0.0010 - 1s/epoch - 1ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 2s - loss: 11.7145 - mse: 11.7145 - mae: 1.5369 - val_loss: 19.3758 - val_mse: 19.3758 - val_mae: 1.5965 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 1s - loss: 11.6144 - mse: 11.6144 - mae: 1.5441 - val_loss: 19.4070 - val_mse: 19.4070 - val_mae: 1.6019 - lr: 0.0010 - 1s/epoch - 1ms/step\n",
            "Epoch 13/100\n",
            "1000/1000 - 1s - loss: 11.6617 - mse: 11.6617 - mae: 1.5403 - val_loss: 19.4598 - val_mse: 19.4598 - val_mae: 1.5968 - lr: 0.0010 - 1s/epoch - 1ms/step\n",
            "Score for fold 4: loss of 19.45980453491211\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 1s - loss: 13.0812 - mse: 13.0812 - mae: 1.5466 - val_loss: 13.8069 - val_mse: 13.8069 - val_mae: 1.5576 - lr: 0.0010 - 1s/epoch - 1ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 1s - loss: 13.0381 - mse: 13.0381 - mae: 1.5418 - val_loss: 13.9265 - val_mse: 13.9265 - val_mae: 1.5699 - lr: 0.0010 - 1s/epoch - 1ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 1s - loss: 13.0683 - mse: 13.0683 - mae: 1.5468 - val_loss: 14.0176 - val_mse: 14.0176 - val_mae: 1.5479 - lr: 0.0010 - 1s/epoch - 1ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 13.0138 - mse: 13.0138 - mae: 1.5440 - val_loss: 14.0388 - val_mse: 14.0388 - val_mae: 1.5402 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 1s - loss: 13.0131 - mse: 13.0131 - mae: 1.5439 - val_loss: 13.8031 - val_mse: 13.8031 - val_mae: 1.5739 - lr: 0.0010 - 1s/epoch - 1ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 13.0595 - mse: 13.0595 - mae: 1.5472 - val_loss: 13.8284 - val_mse: 13.8284 - val_mae: 1.5681 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 1s - loss: 13.0619 - mse: 13.0619 - mae: 1.5482 - val_loss: 13.8443 - val_mse: 13.8443 - val_mae: 1.5784 - lr: 0.0010 - 1s/epoch - 1ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 1s - loss: 13.0502 - mse: 13.0502 - mae: 1.5449 - val_loss: 13.7925 - val_mse: 13.7925 - val_mae: 1.5872 - lr: 0.0010 - 1s/epoch - 1ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 1s - loss: 13.1041 - mse: 13.1041 - mae: 1.5497 - val_loss: 13.9517 - val_mse: 13.9517 - val_mae: 1.5321 - lr: 0.0010 - 1s/epoch - 1ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 1s - loss: 13.0222 - mse: 13.0222 - mae: 1.5407 - val_loss: 14.1206 - val_mse: 14.1206 - val_mae: 1.5245 - lr: 0.0010 - 1s/epoch - 1ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 1s - loss: 13.0544 - mse: 13.0544 - mae: 1.5479 - val_loss: 13.8410 - val_mse: 13.8410 - val_mae: 1.5699 - lr: 0.0010 - 1s/epoch - 1ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 1s - loss: 13.0111 - mse: 13.0111 - mae: 1.5463 - val_loss: 13.8965 - val_mse: 13.8965 - val_mae: 1.5514 - lr: 0.0010 - 1s/epoch - 1ms/step\n",
            "Epoch 13/100\n",
            "1000/1000 - 1s - loss: 13.0734 - mse: 13.0734 - mae: 1.5455 - val_loss: 13.8494 - val_mse: 13.8494 - val_mae: 1.5942 - lr: 0.0010 - 1s/epoch - 1ms/step\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-10-11 10:27:08,761]\u001b[0m Finished trial#0 resulted in value: 13.228. Current best value is 13.228 with parameters: {'activation': 'linear', 'num_hidden_layer': 2, 'i': 6, 'learning_rate': 0.0012023809405782575}.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Score for fold 5: loss of 13.849413871765137\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 4s - loss: 15.0915 - mse: 15.0915 - mae: 1.6374 - val_loss: 10.7218 - val_mse: 10.7218 - val_mae: 1.6433 - lr: 6.6993e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 3s - loss: 14.1508 - mse: 14.1508 - mae: 1.5631 - val_loss: 10.7670 - val_mse: 10.7670 - val_mae: 1.5193 - lr: 6.6993e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 3s - loss: 14.0885 - mse: 14.0885 - mae: 1.5565 - val_loss: 10.6908 - val_mse: 10.6908 - val_mae: 1.5662 - lr: 6.6993e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 3s - loss: 14.1342 - mse: 14.1342 - mae: 1.5568 - val_loss: 10.6253 - val_mse: 10.6253 - val_mae: 1.5569 - lr: 6.6993e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 3s - loss: 14.1243 - mse: 14.1243 - mae: 1.5526 - val_loss: 10.8727 - val_mse: 10.8727 - val_mae: 1.5630 - lr: 6.6993e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 3s - loss: 13.9294 - mse: 13.9294 - mae: 1.5493 - val_loss: 10.6079 - val_mse: 10.6079 - val_mae: 1.6129 - lr: 6.6993e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 3s - loss: 13.9786 - mse: 13.9786 - mae: 1.5442 - val_loss: 10.6088 - val_mse: 10.6088 - val_mae: 1.5935 - lr: 6.6993e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 3s - loss: 14.0286 - mse: 14.0286 - mae: 1.5486 - val_loss: 10.6092 - val_mse: 10.6092 - val_mae: 1.6107 - lr: 6.6993e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 3s - loss: 14.0303 - mse: 14.0303 - mae: 1.5486 - val_loss: 10.7895 - val_mse: 10.7895 - val_mae: 1.5007 - lr: 6.6993e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 3s - loss: 13.9869 - mse: 13.9869 - mae: 1.5477 - val_loss: 10.7716 - val_mse: 10.7716 - val_mae: 1.5872 - lr: 6.6993e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 3s - loss: 13.9430 - mse: 13.9430 - mae: 1.5422 - val_loss: 10.5495 - val_mse: 10.5495 - val_mae: 1.6041 - lr: 6.6993e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 3s - loss: 13.8918 - mse: 13.8918 - mae: 1.5454 - val_loss: 10.5531 - val_mse: 10.5531 - val_mae: 1.5845 - lr: 6.6993e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 13/100\n",
            "1000/1000 - 3s - loss: 13.9812 - mse: 13.9812 - mae: 1.5481 - val_loss: 11.0457 - val_mse: 11.0457 - val_mae: 1.4665 - lr: 6.6993e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 14/100\n",
            "1000/1000 - 3s - loss: 13.8635 - mse: 13.8635 - mae: 1.5499 - val_loss: 10.5989 - val_mse: 10.5989 - val_mae: 1.5822 - lr: 6.6993e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 15/100\n",
            "1000/1000 - 3s - loss: 13.9935 - mse: 13.9935 - mae: 1.5493 - val_loss: 10.6465 - val_mse: 10.6465 - val_mae: 1.6259 - lr: 6.6993e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 16/100\n",
            "1000/1000 - 3s - loss: 14.0151 - mse: 14.0151 - mae: 1.5447 - val_loss: 10.6032 - val_mse: 10.6032 - val_mae: 1.6007 - lr: 6.6993e-04 - 3s/epoch - 3ms/step\n",
            "Score for fold 1: loss of 10.603178977966309\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 3s - loss: 13.4278 - mse: 13.4278 - mae: 1.5554 - val_loss: 12.5872 - val_mse: 12.5872 - val_mae: 1.5199 - lr: 6.6993e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 3s - loss: 13.5226 - mse: 13.5226 - mae: 1.5572 - val_loss: 12.7509 - val_mse: 12.7509 - val_mae: 1.4367 - lr: 6.6993e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 3s - loss: 13.4824 - mse: 13.4824 - mae: 1.5554 - val_loss: 12.5824 - val_mse: 12.5824 - val_mae: 1.5527 - lr: 6.6993e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 3s - loss: 13.4454 - mse: 13.4454 - mae: 1.5558 - val_loss: 12.6176 - val_mse: 12.6176 - val_mae: 1.5137 - lr: 6.6993e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 3s - loss: 13.4568 - mse: 13.4568 - mae: 1.5533 - val_loss: 12.6448 - val_mse: 12.6448 - val_mae: 1.5172 - lr: 6.6993e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 3s - loss: 13.4796 - mse: 13.4796 - mae: 1.5597 - val_loss: 13.0925 - val_mse: 13.0925 - val_mae: 1.6295 - lr: 6.6993e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 3s - loss: 13.4251 - mse: 13.4251 - mae: 1.5537 - val_loss: 12.9641 - val_mse: 12.9641 - val_mae: 1.6187 - lr: 6.6993e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 3s - loss: 13.4385 - mse: 13.4385 - mae: 1.5494 - val_loss: 12.5909 - val_mse: 12.5909 - val_mae: 1.5568 - lr: 6.6993e-04 - 3s/epoch - 3ms/step\n",
            "Score for fold 2: loss of 12.59086799621582\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 3s - loss: 13.6659 - mse: 13.6659 - mae: 1.5745 - val_loss: 12.2382 - val_mse: 12.2382 - val_mae: 1.5715 - lr: 6.6993e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 3s - loss: 13.5992 - mse: 13.5992 - mae: 1.5590 - val_loss: 12.0815 - val_mse: 12.0815 - val_mae: 1.5206 - lr: 6.6993e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 3s - loss: 13.5930 - mse: 13.5930 - mae: 1.5604 - val_loss: 12.1197 - val_mse: 12.1197 - val_mae: 1.4670 - lr: 6.6993e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 3s - loss: 13.4957 - mse: 13.4957 - mae: 1.5617 - val_loss: 12.5504 - val_mse: 12.5504 - val_mae: 1.6748 - lr: 6.6993e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 3s - loss: 13.5679 - mse: 13.5679 - mae: 1.5600 - val_loss: 12.1222 - val_mse: 12.1222 - val_mae: 1.4719 - lr: 6.6993e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 3s - loss: 13.6219 - mse: 13.6219 - mae: 1.5644 - val_loss: 12.2686 - val_mse: 12.2686 - val_mae: 1.5573 - lr: 6.6993e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 3s - loss: 13.5870 - mse: 13.5870 - mae: 1.5661 - val_loss: 12.0339 - val_mse: 12.0339 - val_mae: 1.4954 - lr: 6.6993e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 3s - loss: 13.6647 - mse: 13.6647 - mae: 1.5593 - val_loss: 12.0576 - val_mse: 12.0576 - val_mae: 1.5137 - lr: 6.6993e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 3s - loss: 13.5934 - mse: 13.5934 - mae: 1.5618 - val_loss: 12.0924 - val_mse: 12.0924 - val_mae: 1.5569 - lr: 6.6993e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 3s - loss: 13.6556 - mse: 13.6556 - mae: 1.5687 - val_loss: 12.0575 - val_mse: 12.0575 - val_mae: 1.5003 - lr: 6.6993e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 3s - loss: 13.5591 - mse: 13.5591 - mae: 1.5639 - val_loss: 12.1212 - val_mse: 12.1212 - val_mae: 1.5294 - lr: 6.6993e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 3s - loss: 13.5700 - mse: 13.5700 - mae: 1.5644 - val_loss: 12.0966 - val_mse: 12.0966 - val_mae: 1.4887 - lr: 6.6993e-04 - 3s/epoch - 3ms/step\n",
            "Score for fold 3: loss of 12.096624374389648\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 3s - loss: 11.7697 - mse: 11.7697 - mae: 1.5555 - val_loss: 19.5100 - val_mse: 19.5100 - val_mae: 1.4894 - lr: 6.6993e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 3s - loss: 11.7199 - mse: 11.7199 - mae: 1.5545 - val_loss: 19.3196 - val_mse: 19.3196 - val_mae: 1.5240 - lr: 6.6993e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 3s - loss: 11.8305 - mse: 11.8305 - mae: 1.5560 - val_loss: 19.4748 - val_mse: 19.4748 - val_mae: 1.6579 - lr: 6.6993e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 3s - loss: 11.8432 - mse: 11.8432 - mae: 1.5527 - val_loss: 19.3830 - val_mse: 19.3830 - val_mae: 1.5542 - lr: 6.6993e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 4s - loss: 11.8377 - mse: 11.8377 - mae: 1.5558 - val_loss: 19.2235 - val_mse: 19.2235 - val_mae: 1.5603 - lr: 6.6993e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 3s - loss: 11.7736 - mse: 11.7736 - mae: 1.5560 - val_loss: 19.2660 - val_mse: 19.2660 - val_mae: 1.6161 - lr: 6.6993e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 3s - loss: 11.7787 - mse: 11.7787 - mae: 1.5524 - val_loss: 19.2325 - val_mse: 19.2325 - val_mae: 1.4998 - lr: 6.6993e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 3s - loss: 11.8706 - mse: 11.8706 - mae: 1.5567 - val_loss: 19.2110 - val_mse: 19.2110 - val_mae: 1.5509 - lr: 6.6993e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 3s - loss: 11.8375 - mse: 11.8375 - mae: 1.5571 - val_loss: 19.2794 - val_mse: 19.2794 - val_mae: 1.5091 - lr: 6.6993e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 3s - loss: 11.7701 - mse: 11.7701 - mae: 1.5600 - val_loss: 19.3503 - val_mse: 19.3503 - val_mae: 1.5618 - lr: 6.6993e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 3s - loss: 11.8429 - mse: 11.8429 - mae: 1.5555 - val_loss: 19.2770 - val_mse: 19.2770 - val_mae: 1.5547 - lr: 6.6993e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 3s - loss: 11.8331 - mse: 11.8331 - mae: 1.5596 - val_loss: 19.3090 - val_mse: 19.3090 - val_mae: 1.5682 - lr: 6.6993e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 13/100\n",
            "1000/1000 - 3s - loss: 11.9128 - mse: 11.9128 - mae: 1.5604 - val_loss: 19.2451 - val_mse: 19.2451 - val_mae: 1.5900 - lr: 6.6993e-04 - 3s/epoch - 3ms/step\n",
            "Score for fold 4: loss of 19.24515151977539\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 3s - loss: 13.6228 - mse: 13.6228 - mae: 1.5549 - val_loss: 11.5417 - val_mse: 11.5417 - val_mae: 1.5601 - lr: 6.6993e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 3s - loss: 13.6954 - mse: 13.6954 - mae: 1.5542 - val_loss: 11.7369 - val_mse: 11.7369 - val_mae: 1.5450 - lr: 6.6993e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 3s - loss: 13.7077 - mse: 13.7077 - mae: 1.5551 - val_loss: 11.6641 - val_mse: 11.6641 - val_mae: 1.5063 - lr: 6.6993e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 3s - loss: 13.7455 - mse: 13.7455 - mae: 1.5596 - val_loss: 11.5246 - val_mse: 11.5246 - val_mae: 1.5575 - lr: 6.6993e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 4s - loss: 13.7694 - mse: 13.7694 - mae: 1.5576 - val_loss: 11.6415 - val_mse: 11.6415 - val_mae: 1.6375 - lr: 6.6993e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 4s - loss: 13.7695 - mse: 13.7695 - mae: 1.5601 - val_loss: 11.5881 - val_mse: 11.5881 - val_mae: 1.5237 - lr: 6.6993e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 4s - loss: 13.7816 - mse: 13.7816 - mae: 1.5591 - val_loss: 11.5399 - val_mse: 11.5399 - val_mae: 1.5755 - lr: 6.6993e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 3s - loss: 13.7135 - mse: 13.7135 - mae: 1.5597 - val_loss: 11.5187 - val_mse: 11.5187 - val_mae: 1.5404 - lr: 6.6993e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 3s - loss: 13.7113 - mse: 13.7113 - mae: 1.5555 - val_loss: 11.5563 - val_mse: 11.5563 - val_mae: 1.5487 - lr: 6.6993e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 3s - loss: 13.7600 - mse: 13.7600 - mae: 1.5542 - val_loss: 11.8326 - val_mse: 11.8326 - val_mae: 1.4615 - lr: 6.6993e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 3s - loss: 13.7385 - mse: 13.7385 - mae: 1.5541 - val_loss: 11.6818 - val_mse: 11.6818 - val_mae: 1.5233 - lr: 6.6993e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 3s - loss: 13.7749 - mse: 13.7749 - mae: 1.5649 - val_loss: 11.5712 - val_mse: 11.5712 - val_mae: 1.5312 - lr: 6.6993e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 13/100\n",
            "1000/1000 - 3s - loss: 13.7529 - mse: 13.7529 - mae: 1.5533 - val_loss: 11.6240 - val_mse: 11.6240 - val_mae: 1.5323 - lr: 6.6993e-04 - 3s/epoch - 3ms/step\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-10-11 10:30:39,906]\u001b[0m Finished trial#1 resulted in value: 13.232. Current best value is 13.228 with parameters: {'activation': 'linear', 'num_hidden_layer': 2, 'i': 6, 'learning_rate': 0.0012023809405782575}.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Score for fold 5: loss of 11.624039649963379\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 4s - loss: 15.3855 - mse: 15.3855 - mae: 1.6047 - val_loss: 10.4450 - val_mse: 10.4450 - val_mae: 1.5019 - lr: 6.9620e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 3s - loss: 14.5670 - mse: 14.5670 - mae: 1.5341 - val_loss: 10.2372 - val_mse: 10.2372 - val_mae: 1.5526 - lr: 6.9620e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 3s - loss: 14.3467 - mse: 14.3467 - mae: 1.5245 - val_loss: 10.1095 - val_mse: 10.1095 - val_mae: 1.5104 - lr: 6.9620e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 3s - loss: 14.1048 - mse: 14.1048 - mae: 1.5072 - val_loss: 10.0609 - val_mse: 10.0609 - val_mae: 1.5479 - lr: 6.9620e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 3s - loss: 13.9526 - mse: 13.9526 - mae: 1.5001 - val_loss: 9.9188 - val_mse: 9.9188 - val_mae: 1.5252 - lr: 6.9620e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 3s - loss: 13.9138 - mse: 13.9138 - mae: 1.4934 - val_loss: 9.7133 - val_mse: 9.7133 - val_mae: 1.5213 - lr: 6.9620e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 4s - loss: 13.7630 - mse: 13.7630 - mae: 1.4883 - val_loss: 9.8810 - val_mse: 9.8810 - val_mae: 1.5553 - lr: 6.9620e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 3s - loss: 13.6254 - mse: 13.6254 - mae: 1.4860 - val_loss: 9.8573 - val_mse: 9.8573 - val_mae: 1.5271 - lr: 6.9620e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 4s - loss: 13.5513 - mse: 13.5513 - mae: 1.4759 - val_loss: 9.6416 - val_mse: 9.6416 - val_mae: 1.4728 - lr: 6.9620e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 3s - loss: 13.4456 - mse: 13.4456 - mae: 1.4699 - val_loss: 10.0709 - val_mse: 10.0709 - val_mae: 1.5045 - lr: 6.9620e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 3s - loss: 13.3386 - mse: 13.3386 - mae: 1.4644 - val_loss: 9.7608 - val_mse: 9.7608 - val_mae: 1.4349 - lr: 6.9620e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 3s - loss: 13.2051 - mse: 13.2051 - mae: 1.4552 - val_loss: 9.6822 - val_mse: 9.6822 - val_mae: 1.5034 - lr: 6.9620e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 13/100\n",
            "1000/1000 - 4s - loss: 12.9806 - mse: 12.9806 - mae: 1.4458 - val_loss: 9.6062 - val_mse: 9.6062 - val_mae: 1.5026 - lr: 6.9620e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 14/100\n",
            "1000/1000 - 4s - loss: 12.9754 - mse: 12.9754 - mae: 1.4461 - val_loss: 9.5250 - val_mse: 9.5250 - val_mae: 1.5137 - lr: 6.9620e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 15/100\n",
            "1000/1000 - 3s - loss: 12.8161 - mse: 12.8161 - mae: 1.4337 - val_loss: 9.6445 - val_mse: 9.6445 - val_mae: 1.5174 - lr: 6.9620e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 16/100\n",
            "1000/1000 - 3s - loss: 12.7161 - mse: 12.7161 - mae: 1.4269 - val_loss: 9.7157 - val_mse: 9.7157 - val_mae: 1.5279 - lr: 6.9620e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 17/100\n",
            "1000/1000 - 4s - loss: 12.5349 - mse: 12.5349 - mae: 1.4156 - val_loss: 9.6665 - val_mse: 9.6665 - val_mae: 1.5606 - lr: 6.9620e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 18/100\n",
            "1000/1000 - 4s - loss: 12.2345 - mse: 12.2345 - mae: 1.4092 - val_loss: 9.7243 - val_mse: 9.7243 - val_mae: 1.5134 - lr: 6.9620e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 19/100\n",
            "1000/1000 - 3s - loss: 12.1794 - mse: 12.1794 - mae: 1.4023 - val_loss: 9.5451 - val_mse: 9.5451 - val_mae: 1.5162 - lr: 6.9620e-04 - 3s/epoch - 3ms/step\n",
            "Score for fold 1: loss of 9.545082092285156\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 3s - loss: 9.7255 - mse: 9.7255 - mae: 1.4309 - val_loss: 18.9629 - val_mse: 18.9629 - val_mae: 1.3835 - lr: 6.9620e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 3s - loss: 9.5760 - mse: 9.5760 - mae: 1.4165 - val_loss: 19.1743 - val_mse: 19.1743 - val_mae: 1.3608 - lr: 6.9620e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 3s - loss: 9.4139 - mse: 9.4139 - mae: 1.4031 - val_loss: 19.3321 - val_mse: 19.3321 - val_mae: 1.3739 - lr: 6.9620e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 3s - loss: 9.1291 - mse: 9.1291 - mae: 1.3901 - val_loss: 19.2092 - val_mse: 19.2092 - val_mae: 1.4118 - lr: 6.9620e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 3s - loss: 8.9889 - mse: 8.9889 - mae: 1.3775 - val_loss: 19.1430 - val_mse: 19.1430 - val_mae: 1.4591 - lr: 6.9620e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 3s - loss: 8.7786 - mse: 8.7786 - mae: 1.3685 - val_loss: 19.2961 - val_mse: 19.2961 - val_mae: 1.4327 - lr: 6.9620e-04 - 3s/epoch - 3ms/step\n",
            "Score for fold 2: loss of 19.2961483001709\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 3s - loss: 11.4973 - mse: 11.4973 - mae: 1.3802 - val_loss: 7.7661 - val_mse: 7.7661 - val_mae: 1.3369 - lr: 6.9620e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 3s - loss: 11.3737 - mse: 11.3737 - mae: 1.3649 - val_loss: 7.7134 - val_mse: 7.7134 - val_mae: 1.3610 - lr: 6.9620e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 3s - loss: 11.0480 - mse: 11.0480 - mae: 1.3433 - val_loss: 7.8921 - val_mse: 7.8921 - val_mae: 1.4409 - lr: 6.9620e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 3s - loss: 10.8269 - mse: 10.8269 - mae: 1.3322 - val_loss: 8.0177 - val_mse: 8.0177 - val_mae: 1.4178 - lr: 6.9620e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 3s - loss: 10.6902 - mse: 10.6902 - mae: 1.3220 - val_loss: 7.9559 - val_mse: 7.9559 - val_mae: 1.4187 - lr: 6.9620e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 3s - loss: 10.3962 - mse: 10.3962 - mae: 1.3064 - val_loss: 7.8614 - val_mse: 7.8614 - val_mae: 1.3896 - lr: 6.9620e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 3s - loss: 10.2519 - mse: 10.2519 - mae: 1.2936 - val_loss: 7.9270 - val_mse: 7.9270 - val_mae: 1.4125 - lr: 6.9620e-04 - 3s/epoch - 3ms/step\n",
            "Score for fold 3: loss of 7.927031517028809\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 3s - loss: 10.3656 - mse: 10.3656 - mae: 1.3374 - val_loss: 7.7304 - val_mse: 7.7304 - val_mae: 1.3039 - lr: 6.9620e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 3s - loss: 9.9624 - mse: 9.9624 - mae: 1.3149 - val_loss: 7.7752 - val_mse: 7.7752 - val_mae: 1.2547 - lr: 6.9620e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 3s - loss: 9.7568 - mse: 9.7568 - mae: 1.3002 - val_loss: 7.9290 - val_mse: 7.9290 - val_mae: 1.3153 - lr: 6.9620e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 3s - loss: 9.4917 - mse: 9.4917 - mae: 1.2770 - val_loss: 8.0638 - val_mse: 8.0638 - val_mae: 1.3292 - lr: 6.9620e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 3s - loss: 9.2130 - mse: 9.2130 - mae: 1.2620 - val_loss: 8.1397 - val_mse: 8.1397 - val_mae: 1.3208 - lr: 6.9620e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 3s - loss: 8.9953 - mse: 8.9953 - mae: 1.2471 - val_loss: 8.3288 - val_mse: 8.3288 - val_mae: 1.3790 - lr: 6.9620e-04 - 3s/epoch - 3ms/step\n",
            "Score for fold 4: loss of 8.328789710998535\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 3s - loss: 9.0299 - mse: 9.0299 - mae: 1.2846 - val_loss: 7.9551 - val_mse: 7.9551 - val_mae: 1.2156 - lr: 6.9620e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 3s - loss: 8.7180 - mse: 8.7180 - mae: 1.2578 - val_loss: 8.1938 - val_mse: 8.1938 - val_mae: 1.2719 - lr: 6.9620e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 3s - loss: 8.4340 - mse: 8.4340 - mae: 1.2398 - val_loss: 8.2743 - val_mse: 8.2743 - val_mae: 1.2604 - lr: 6.9620e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 3s - loss: 8.2558 - mse: 8.2558 - mae: 1.2229 - val_loss: 8.4162 - val_mse: 8.4162 - val_mae: 1.2818 - lr: 6.9620e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 3s - loss: 8.0088 - mse: 8.0088 - mae: 1.2066 - val_loss: 8.5299 - val_mse: 8.5299 - val_mae: 1.3034 - lr: 6.9620e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 3s - loss: 7.8377 - mse: 7.8377 - mae: 1.1842 - val_loss: 8.6298 - val_mse: 8.6298 - val_mae: 1.3497 - lr: 6.9620e-04 - 3s/epoch - 3ms/step\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-10-11 10:33:16,068]\u001b[0m Finished trial#2 resulted in value: 10.748000000000001. Current best value is 10.748000000000001 with parameters: {'activation': 'tanh', 'num_hidden_layer': 4, 'i': 8, 'learning_rate': 0.0006962014347671436}.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Score for fold 5: loss of 8.629791259765625\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 6s - loss: 13.6913 - mse: 13.6913 - mae: 1.5395 - val_loss: 11.8341 - val_mse: 11.8341 - val_mae: 1.6061 - lr: 2.3072e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 5s - loss: 12.7220 - mse: 12.7220 - mae: 1.4859 - val_loss: 10.5679 - val_mse: 10.5679 - val_mae: 1.4652 - lr: 2.3072e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 5s - loss: 12.4872 - mse: 12.4872 - mae: 1.4672 - val_loss: 11.1449 - val_mse: 11.1449 - val_mae: 1.4817 - lr: 2.3072e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 5s - loss: 12.3252 - mse: 12.3252 - mae: 1.4562 - val_loss: 11.8559 - val_mse: 11.8559 - val_mae: 1.5353 - lr: 2.3072e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 6s - loss: 12.2618 - mse: 12.2618 - mae: 1.4458 - val_loss: 12.1506 - val_mse: 12.1506 - val_mae: 1.4459 - lr: 2.3072e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 5s - loss: 12.0190 - mse: 12.0190 - mae: 1.4390 - val_loss: 11.3228 - val_mse: 11.3228 - val_mae: 1.4627 - lr: 2.3072e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 5s - loss: 11.9749 - mse: 11.9749 - mae: 1.4288 - val_loss: 10.4184 - val_mse: 10.4184 - val_mae: 1.4701 - lr: 2.3072e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 5s - loss: 11.7509 - mse: 11.7509 - mae: 1.4187 - val_loss: 12.2661 - val_mse: 12.2661 - val_mae: 1.4574 - lr: 2.3072e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 5s - loss: 11.6536 - mse: 11.6536 - mae: 1.4046 - val_loss: 10.8440 - val_mse: 10.8440 - val_mae: 1.5117 - lr: 2.3072e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 5s - loss: 11.5376 - mse: 11.5376 - mae: 1.4018 - val_loss: 10.7774 - val_mse: 10.7774 - val_mae: 1.4675 - lr: 2.3072e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 6s - loss: 11.2497 - mse: 11.2497 - mae: 1.3857 - val_loss: 10.3462 - val_mse: 10.3462 - val_mae: 1.5060 - lr: 2.3072e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 5s - loss: 10.9482 - mse: 10.9482 - mae: 1.3714 - val_loss: 10.1155 - val_mse: 10.1155 - val_mae: 1.4849 - lr: 2.3072e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 13/100\n",
            "1000/1000 - 6s - loss: 10.8424 - mse: 10.8424 - mae: 1.3619 - val_loss: 11.2815 - val_mse: 11.2815 - val_mae: 1.4633 - lr: 2.3072e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 14/100\n",
            "1000/1000 - 5s - loss: 10.4724 - mse: 10.4724 - mae: 1.3466 - val_loss: 11.5892 - val_mse: 11.5892 - val_mae: 1.5586 - lr: 2.3072e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 15/100\n",
            "1000/1000 - 5s - loss: 10.3225 - mse: 10.3225 - mae: 1.3319 - val_loss: 10.8250 - val_mse: 10.8250 - val_mae: 1.4834 - lr: 2.3072e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 16/100\n",
            "1000/1000 - 5s - loss: 10.0709 - mse: 10.0709 - mae: 1.3188 - val_loss: 12.0888 - val_mse: 12.0888 - val_mae: 1.4354 - lr: 2.3072e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 17/100\n",
            "1000/1000 - 6s - loss: 9.7252 - mse: 9.7252 - mae: 1.2962 - val_loss: 13.1545 - val_mse: 13.1545 - val_mae: 1.5278 - lr: 2.3072e-04 - 6s/epoch - 6ms/step\n",
            "Score for fold 1: loss of 13.154443740844727\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 5s - loss: 10.8455 - mse: 10.8455 - mae: 1.3611 - val_loss: 7.3521 - val_mse: 7.3521 - val_mae: 1.4035 - lr: 2.3072e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 5s - loss: 10.7204 - mse: 10.7204 - mae: 1.3395 - val_loss: 7.5874 - val_mse: 7.5874 - val_mae: 1.3016 - lr: 2.3072e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 6s - loss: 9.9274 - mse: 9.9274 - mae: 1.3169 - val_loss: 7.6924 - val_mse: 7.6924 - val_mae: 1.3699 - lr: 2.3072e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 6s - loss: 10.0163 - mse: 10.0163 - mae: 1.3080 - val_loss: 7.7575 - val_mse: 7.7575 - val_mae: 1.3421 - lr: 2.3072e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 6s - loss: 9.3760 - mse: 9.3760 - mae: 1.2884 - val_loss: 7.7947 - val_mse: 7.7947 - val_mae: 1.2993 - lr: 2.3072e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 6s - loss: 9.0385 - mse: 9.0385 - mae: 1.2677 - val_loss: 7.7901 - val_mse: 7.7901 - val_mae: 1.3766 - lr: 2.3072e-04 - 6s/epoch - 6ms/step\n",
            "Score for fold 2: loss of 7.7900590896606445\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 6s - loss: 9.1015 - mse: 9.1015 - mae: 1.3023 - val_loss: 7.2038 - val_mse: 7.2038 - val_mae: 1.1834 - lr: 2.3072e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 6s - loss: 9.1704 - mse: 9.1704 - mae: 1.2799 - val_loss: 7.6502 - val_mse: 7.6502 - val_mae: 1.2485 - lr: 2.3072e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 6s - loss: 8.6143 - mse: 8.6143 - mae: 1.2592 - val_loss: 7.9863 - val_mse: 7.9863 - val_mae: 1.2718 - lr: 2.3072e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 6s - loss: 8.5090 - mse: 8.5090 - mae: 1.2502 - val_loss: 8.2843 - val_mse: 8.2843 - val_mae: 1.2540 - lr: 2.3072e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 6s - loss: 7.9424 - mse: 7.9424 - mae: 1.2310 - val_loss: 8.1628 - val_mse: 8.1628 - val_mae: 1.3050 - lr: 2.3072e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 6s - loss: 8.1022 - mse: 8.1022 - mae: 1.2168 - val_loss: 8.3755 - val_mse: 8.3755 - val_mae: 1.3081 - lr: 2.3072e-04 - 6s/epoch - 6ms/step\n",
            "Score for fold 3: loss of 8.37553882598877\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 6s - loss: 8.6686 - mse: 8.6686 - mae: 1.2541 - val_loss: 4.1690 - val_mse: 4.1690 - val_mae: 1.0753 - lr: 2.3072e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 6s - loss: 8.3934 - mse: 8.3934 - mae: 1.2358 - val_loss: 3.8704 - val_mse: 3.8704 - val_mae: 1.1901 - lr: 2.3072e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 6s - loss: 8.0255 - mse: 8.0255 - mae: 1.2121 - val_loss: 4.1106 - val_mse: 4.1106 - val_mae: 1.1713 - lr: 2.3072e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 6s - loss: 7.8792 - mse: 7.8792 - mae: 1.1939 - val_loss: 4.2566 - val_mse: 4.2566 - val_mae: 1.1819 - lr: 2.3072e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 6s - loss: 7.6910 - mse: 7.6910 - mae: 1.1865 - val_loss: 4.1347 - val_mse: 4.1347 - val_mae: 1.1756 - lr: 2.3072e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 6s - loss: 7.5019 - mse: 7.5019 - mae: 1.1669 - val_loss: 4.2242 - val_mse: 4.2242 - val_mae: 1.2038 - lr: 2.3072e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 6s - loss: 6.9814 - mse: 6.9814 - mae: 1.1533 - val_loss: 4.2828 - val_mse: 4.2828 - val_mae: 1.1781 - lr: 2.3072e-04 - 6s/epoch - 6ms/step\n",
            "Score for fold 4: loss of 4.282754898071289\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 6s - loss: 5.2051 - mse: 5.2051 - mae: 1.1723 - val_loss: 12.0080 - val_mse: 12.0080 - val_mae: 1.1222 - lr: 2.3072e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 6s - loss: 4.8405 - mse: 4.8405 - mae: 1.1497 - val_loss: 12.9626 - val_mse: 12.9626 - val_mae: 1.1399 - lr: 2.3072e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 5s - loss: 5.0613 - mse: 5.0613 - mae: 1.1400 - val_loss: 12.1956 - val_mse: 12.1956 - val_mae: 1.1438 - lr: 2.3072e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 5s - loss: 4.8891 - mse: 4.8891 - mae: 1.1173 - val_loss: 12.9432 - val_mse: 12.9432 - val_mae: 1.2180 - lr: 2.3072e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 5s - loss: 4.2076 - mse: 4.2076 - mae: 1.0985 - val_loss: 13.0246 - val_mse: 13.0246 - val_mae: 1.2443 - lr: 2.3072e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 5s - loss: 4.2833 - mse: 4.2833 - mae: 1.0879 - val_loss: 13.0742 - val_mse: 13.0742 - val_mae: 1.2012 - lr: 2.3072e-04 - 5s/epoch - 5ms/step\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-10-11 10:37:14,617]\u001b[0m Finished trial#3 resulted in value: 9.334. Current best value is 9.334 with parameters: {'activation': 'relu', 'num_hidden_layer': 4, 'i': 9, 'learning_rate': 0.00023071835273936203}.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Score for fold 5: loss of 13.074200630187988\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 11s - loss: 12.0086 - mse: 12.0086 - mae: 1.5815 - val_loss: 20.3423 - val_mse: 20.3423 - val_mae: 1.5459 - lr: 0.0026 - 11s/epoch - 11ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 11s - loss: 11.0923 - mse: 11.0923 - mae: 1.5347 - val_loss: 19.5553 - val_mse: 19.5553 - val_mae: 1.6012 - lr: 0.0026 - 11s/epoch - 11ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 11s - loss: 10.9302 - mse: 10.9302 - mae: 1.5240 - val_loss: 18.4318 - val_mse: 18.4318 - val_mae: 1.5139 - lr: 0.0026 - 11s/epoch - 11ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 10s - loss: 10.9329 - mse: 10.9329 - mae: 1.5206 - val_loss: 19.2247 - val_mse: 19.2247 - val_mae: 1.5398 - lr: 0.0026 - 10s/epoch - 10ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 10s - loss: 10.8006 - mse: 10.8006 - mae: 1.5220 - val_loss: 19.2983 - val_mse: 19.2983 - val_mae: 1.4759 - lr: 0.0026 - 10s/epoch - 10ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 10s - loss: 10.7064 - mse: 10.7064 - mae: 1.5092 - val_loss: 19.4070 - val_mse: 19.4070 - val_mae: 1.4539 - lr: 0.0026 - 10s/epoch - 10ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 10s - loss: 10.5579 - mse: 10.5579 - mae: 1.4809 - val_loss: 18.3235 - val_mse: 18.3235 - val_mae: 1.4970 - lr: 0.0026 - 10s/epoch - 10ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 11s - loss: 10.4952 - mse: 10.4952 - mae: 1.4706 - val_loss: 18.8226 - val_mse: 18.8226 - val_mae: 1.4302 - lr: 0.0026 - 11s/epoch - 11ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 11s - loss: 10.2850 - mse: 10.2850 - mae: 1.4606 - val_loss: 19.1739 - val_mse: 19.1739 - val_mae: 1.4773 - lr: 0.0026 - 11s/epoch - 11ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 10s - loss: 10.2424 - mse: 10.2424 - mae: 1.4571 - val_loss: 18.3060 - val_mse: 18.3060 - val_mae: 1.4749 - lr: 0.0026 - 10s/epoch - 10ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 11s - loss: 10.1568 - mse: 10.1568 - mae: 1.4519 - val_loss: 19.2350 - val_mse: 19.2350 - val_mae: 1.5043 - lr: 0.0026 - 11s/epoch - 11ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 11s - loss: 10.0130 - mse: 10.0130 - mae: 1.4479 - val_loss: 18.5442 - val_mse: 18.5442 - val_mae: 1.4247 - lr: 0.0026 - 11s/epoch - 11ms/step\n",
            "Epoch 13/100\n",
            "1000/1000 - 11s - loss: 10.8447 - mse: 10.8447 - mae: 1.4421 - val_loss: 18.3496 - val_mse: 18.3496 - val_mae: 1.4586 - lr: 0.0026 - 11s/epoch - 11ms/step\n",
            "Epoch 14/100\n",
            "1000/1000 - 10s - loss: 9.9107 - mse: 9.9107 - mae: 1.4295 - val_loss: 18.4990 - val_mse: 18.4990 - val_mae: 1.5205 - lr: 0.0026 - 10s/epoch - 10ms/step\n",
            "Epoch 15/100\n",
            "1000/1000 - 10s - loss: 9.6049 - mse: 9.6049 - mae: 1.4252 - val_loss: 18.3245 - val_mse: 18.3245 - val_mae: 1.5056 - lr: 0.0026 - 10s/epoch - 10ms/step\n",
            "Score for fold 1: loss of 18.324541091918945\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 10s - loss: 10.5620 - mse: 10.5620 - mae: 1.3997 - val_loss: 12.8456 - val_mse: 12.8456 - val_mae: 1.4419 - lr: 0.0010 - 10s/epoch - 10ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 10s - loss: 10.2187 - mse: 10.2187 - mae: 1.3863 - val_loss: 12.8702 - val_mse: 12.8702 - val_mae: 1.4249 - lr: 0.0010 - 10s/epoch - 10ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 10s - loss: 10.0969 - mse: 10.0969 - mae: 1.3799 - val_loss: 13.0584 - val_mse: 13.0584 - val_mae: 1.4216 - lr: 0.0010 - 10s/epoch - 10ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 11s - loss: 9.8065 - mse: 9.8065 - mae: 1.3686 - val_loss: 13.1205 - val_mse: 13.1205 - val_mae: 1.4860 - lr: 0.0010 - 11s/epoch - 11ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 10s - loss: 9.6704 - mse: 9.6704 - mae: 1.3627 - val_loss: 13.1172 - val_mse: 13.1172 - val_mae: 1.4386 - lr: 0.0010 - 10s/epoch - 10ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 10s - loss: 9.6718 - mse: 9.6718 - mae: 1.3558 - val_loss: 13.2278 - val_mse: 13.2278 - val_mae: 1.4242 - lr: 0.0010 - 10s/epoch - 10ms/step\n",
            "Score for fold 2: loss of 13.227789878845215\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 11s - loss: 10.8085 - mse: 10.8085 - mae: 1.3726 - val_loss: 8.1155 - val_mse: 8.1155 - val_mae: 1.3696 - lr: 0.0010 - 11s/epoch - 11ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 10s - loss: 10.6052 - mse: 10.6052 - mae: 1.3585 - val_loss: 8.1651 - val_mse: 8.1651 - val_mae: 1.3555 - lr: 0.0010 - 10s/epoch - 10ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 11s - loss: 10.5179 - mse: 10.5179 - mae: 1.3535 - val_loss: 8.3324 - val_mse: 8.3324 - val_mae: 1.4019 - lr: 0.0010 - 11s/epoch - 11ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 11s - loss: 10.3027 - mse: 10.3027 - mae: 1.3455 - val_loss: 8.8444 - val_mse: 8.8444 - val_mae: 1.3561 - lr: 0.0010 - 11s/epoch - 11ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 10s - loss: 10.2475 - mse: 10.2475 - mae: 1.3357 - val_loss: 8.6318 - val_mse: 8.6318 - val_mae: 1.4330 - lr: 0.0010 - 10s/epoch - 10ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 10s - loss: 10.1813 - mse: 10.1813 - mae: 1.3279 - val_loss: 8.5512 - val_mse: 8.5512 - val_mae: 1.3666 - lr: 0.0010 - 10s/epoch - 10ms/step\n",
            "Score for fold 3: loss of 8.551207542419434\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 10s - loss: 10.5076 - mse: 10.5076 - mae: 1.3502 - val_loss: 6.6585 - val_mse: 6.6585 - val_mae: 1.3001 - lr: 0.0010 - 10s/epoch - 10ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 10s - loss: 10.2452 - mse: 10.2452 - mae: 1.3334 - val_loss: 6.6619 - val_mse: 6.6619 - val_mae: 1.3354 - lr: 0.0010 - 10s/epoch - 10ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 11s - loss: 10.1071 - mse: 10.1071 - mae: 1.3218 - val_loss: 6.7169 - val_mse: 6.7169 - val_mae: 1.3265 - lr: 0.0010 - 11s/epoch - 11ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 11s - loss: 10.0277 - mse: 10.0277 - mae: 1.3123 - val_loss: 6.7072 - val_mse: 6.7072 - val_mae: 1.3598 - lr: 0.0010 - 11s/epoch - 11ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 11s - loss: 9.9621 - mse: 9.9621 - mae: 1.3093 - val_loss: 6.9282 - val_mse: 6.9282 - val_mae: 1.3988 - lr: 0.0010 - 11s/epoch - 11ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 11s - loss: 9.7736 - mse: 9.7736 - mae: 1.2967 - val_loss: 7.0277 - val_mse: 7.0277 - val_mae: 1.3910 - lr: 0.0010 - 11s/epoch - 11ms/step\n",
            "Score for fold 4: loss of 7.027742385864258\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 10s - loss: 10.4373 - mse: 10.4373 - mae: 1.3171 - val_loss: 4.9771 - val_mse: 4.9771 - val_mae: 1.3097 - lr: 0.0010 - 10s/epoch - 10ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 10s - loss: 10.5125 - mse: 10.5125 - mae: 1.3132 - val_loss: 5.3119 - val_mse: 5.3119 - val_mae: 1.2922 - lr: 0.0010 - 10s/epoch - 10ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 10s - loss: 9.9209 - mse: 9.9209 - mae: 1.2988 - val_loss: 4.9623 - val_mse: 4.9623 - val_mae: 1.2716 - lr: 0.0010 - 10s/epoch - 10ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 10s - loss: 9.8557 - mse: 9.8557 - mae: 1.2863 - val_loss: 5.7941 - val_mse: 5.7941 - val_mae: 1.3354 - lr: 0.0010 - 10s/epoch - 10ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 10s - loss: 9.6515 - mse: 9.6515 - mae: 1.2759 - val_loss: 5.1848 - val_mse: 5.1848 - val_mae: 1.2757 - lr: 0.0010 - 10s/epoch - 10ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 10s - loss: 9.5724 - mse: 9.5724 - mae: 1.2710 - val_loss: 5.3392 - val_mse: 5.3392 - val_mae: 1.3316 - lr: 0.0010 - 10s/epoch - 10ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 10s - loss: 9.6675 - mse: 9.6675 - mae: 1.2613 - val_loss: 5.5422 - val_mse: 5.5422 - val_mae: 1.3030 - lr: 0.0010 - 10s/epoch - 10ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 10s - loss: 9.4579 - mse: 9.4579 - mae: 1.2565 - val_loss: 5.6875 - val_mse: 5.6875 - val_mae: 1.3202 - lr: 0.0010 - 10s/epoch - 10ms/step\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-10-11 10:44:29,455]\u001b[0m Finished trial#4 resulted in value: 10.564. Current best value is 9.334 with parameters: {'activation': 'relu', 'num_hidden_layer': 4, 'i': 9, 'learning_rate': 0.00023071835273936203}.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Score for fold 5: loss of 5.687545299530029\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 4s - loss: 15.3547 - mse: 15.3547 - mae: 1.6333 - val_loss: 10.4131 - val_mse: 10.4131 - val_mae: 1.4679 - lr: 0.0012 - 4s/epoch - 4ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 4s - loss: 14.7120 - mse: 14.7120 - mae: 1.5593 - val_loss: 10.5226 - val_mse: 10.5226 - val_mae: 1.5807 - lr: 0.0012 - 4s/epoch - 4ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 4s - loss: 14.5254 - mse: 14.5254 - mae: 1.5500 - val_loss: 10.2959 - val_mse: 10.2959 - val_mae: 1.5557 - lr: 0.0012 - 4s/epoch - 4ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 4s - loss: 14.3186 - mse: 14.3186 - mae: 1.5379 - val_loss: 10.0572 - val_mse: 10.0572 - val_mae: 1.5003 - lr: 0.0012 - 4s/epoch - 4ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 4s - loss: 14.1013 - mse: 14.1013 - mae: 1.5283 - val_loss: 9.7978 - val_mse: 9.7978 - val_mae: 1.4161 - lr: 0.0012 - 4s/epoch - 4ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 4s - loss: 13.9909 - mse: 13.9909 - mae: 1.5203 - val_loss: 10.1701 - val_mse: 10.1701 - val_mae: 1.4171 - lr: 0.0012 - 4s/epoch - 4ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 4s - loss: 13.9160 - mse: 13.9160 - mae: 1.5183 - val_loss: 9.9999 - val_mse: 9.9999 - val_mae: 1.3992 - lr: 0.0012 - 4s/epoch - 4ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 4s - loss: 13.8000 - mse: 13.8000 - mae: 1.5148 - val_loss: 9.7821 - val_mse: 9.7821 - val_mae: 1.4732 - lr: 0.0012 - 4s/epoch - 4ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 4s - loss: 13.7451 - mse: 13.7451 - mae: 1.5075 - val_loss: 9.8551 - val_mse: 9.8551 - val_mae: 1.4497 - lr: 0.0012 - 4s/epoch - 4ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 4s - loss: 13.6370 - mse: 13.6370 - mae: 1.5067 - val_loss: 9.7247 - val_mse: 9.7247 - val_mae: 1.4369 - lr: 0.0012 - 4s/epoch - 4ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 4s - loss: 13.5557 - mse: 13.5557 - mae: 1.5102 - val_loss: 9.9335 - val_mse: 9.9335 - val_mae: 1.5099 - lr: 0.0012 - 4s/epoch - 4ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 4s - loss: 13.4870 - mse: 13.4870 - mae: 1.5039 - val_loss: 9.7371 - val_mse: 9.7371 - val_mae: 1.4335 - lr: 0.0012 - 4s/epoch - 4ms/step\n",
            "Epoch 13/100\n",
            "1000/1000 - 4s - loss: 13.3980 - mse: 13.3980 - mae: 1.5062 - val_loss: 9.9421 - val_mse: 9.9421 - val_mae: 1.7263 - lr: 0.0012 - 4s/epoch - 4ms/step\n",
            "Epoch 14/100\n",
            "1000/1000 - 4s - loss: 13.2377 - mse: 13.2377 - mae: 1.5020 - val_loss: 9.5903 - val_mse: 9.5903 - val_mae: 1.4736 - lr: 0.0012 - 4s/epoch - 4ms/step\n",
            "Epoch 15/100\n",
            "1000/1000 - 4s - loss: 13.2215 - mse: 13.2215 - mae: 1.4930 - val_loss: 9.7231 - val_mse: 9.7231 - val_mae: 1.5235 - lr: 0.0012 - 4s/epoch - 4ms/step\n",
            "Epoch 16/100\n",
            "1000/1000 - 4s - loss: 13.1261 - mse: 13.1261 - mae: 1.4913 - val_loss: 9.9109 - val_mse: 9.9109 - val_mae: 1.4465 - lr: 0.0012 - 4s/epoch - 4ms/step\n",
            "Epoch 17/100\n",
            "1000/1000 - 4s - loss: 13.0851 - mse: 13.0851 - mae: 1.4964 - val_loss: 9.9640 - val_mse: 9.9640 - val_mae: 1.4281 - lr: 0.0012 - 4s/epoch - 4ms/step\n",
            "Epoch 18/100\n",
            "1000/1000 - 4s - loss: 12.9759 - mse: 12.9759 - mae: 1.4839 - val_loss: 9.7535 - val_mse: 9.7535 - val_mae: 1.4953 - lr: 0.0012 - 4s/epoch - 4ms/step\n",
            "Epoch 19/100\n",
            "1000/1000 - 4s - loss: 12.9897 - mse: 12.9897 - mae: 1.4890 - val_loss: 9.7392 - val_mse: 9.7392 - val_mae: 1.4140 - lr: 0.0012 - 4s/epoch - 4ms/step\n",
            "Score for fold 1: loss of 9.739154815673828\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 4s - loss: 10.0530 - mse: 10.0530 - mae: 1.4643 - val_loss: 20.8078 - val_mse: 20.8078 - val_mae: 1.4637 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 4s - loss: 9.9360 - mse: 9.9360 - mae: 1.4535 - val_loss: 20.6328 - val_mse: 20.6328 - val_mae: 1.4576 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 4s - loss: 9.7505 - mse: 9.7505 - mae: 1.4514 - val_loss: 20.4902 - val_mse: 20.4902 - val_mae: 1.5648 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 4s - loss: 9.6090 - mse: 9.6090 - mae: 1.4425 - val_loss: 20.7215 - val_mse: 20.7215 - val_mae: 1.5392 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 4s - loss: 9.4648 - mse: 9.4648 - mae: 1.4343 - val_loss: 20.7895 - val_mse: 20.7895 - val_mae: 1.5129 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 4s - loss: 9.3760 - mse: 9.3760 - mae: 1.4313 - val_loss: 21.2027 - val_mse: 21.2027 - val_mae: 1.5185 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 4s - loss: 9.2857 - mse: 9.2857 - mae: 1.4208 - val_loss: 20.4655 - val_mse: 20.4655 - val_mae: 1.5482 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 4s - loss: 9.0944 - mse: 9.0944 - mae: 1.4153 - val_loss: 20.5324 - val_mse: 20.5324 - val_mae: 1.4925 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 4s - loss: 9.0685 - mse: 9.0685 - mae: 1.4183 - val_loss: 20.5068 - val_mse: 20.5068 - val_mae: 1.5698 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 4s - loss: 8.8605 - mse: 8.8605 - mae: 1.4047 - val_loss: 20.6681 - val_mse: 20.6681 - val_mae: 1.5354 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 4s - loss: 8.7554 - mse: 8.7554 - mae: 1.3998 - val_loss: 20.6238 - val_mse: 20.6238 - val_mae: 1.5932 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 4s - loss: 8.6174 - mse: 8.6174 - mae: 1.3927 - val_loss: 20.6455 - val_mse: 20.6455 - val_mae: 1.5794 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Score for fold 2: loss of 20.645511627197266\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 4s - loss: 12.0490 - mse: 12.0490 - mae: 1.4463 - val_loss: 7.5220 - val_mse: 7.5220 - val_mae: 1.3945 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 4s - loss: 11.9540 - mse: 11.9540 - mae: 1.4288 - val_loss: 7.5799 - val_mse: 7.5799 - val_mae: 1.3974 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 4s - loss: 11.8471 - mse: 11.8471 - mae: 1.4140 - val_loss: 7.7205 - val_mse: 7.7205 - val_mae: 1.4906 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 4s - loss: 11.5233 - mse: 11.5233 - mae: 1.4077 - val_loss: 7.8948 - val_mse: 7.8948 - val_mae: 1.3942 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 4s - loss: 11.4426 - mse: 11.4426 - mae: 1.3991 - val_loss: 7.8948 - val_mse: 7.8948 - val_mae: 1.4549 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 4s - loss: 11.2779 - mse: 11.2779 - mae: 1.3898 - val_loss: 7.9556 - val_mse: 7.9556 - val_mae: 1.4319 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Score for fold 3: loss of 7.955565452575684\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 4s - loss: 10.9685 - mse: 10.9685 - mae: 1.4221 - val_loss: 9.0390 - val_mse: 9.0390 - val_mae: 1.3476 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 4s - loss: 10.7674 - mse: 10.7674 - mae: 1.4014 - val_loss: 9.0610 - val_mse: 9.0610 - val_mae: 1.3556 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 4s - loss: 10.6099 - mse: 10.6099 - mae: 1.3962 - val_loss: 9.3632 - val_mse: 9.3632 - val_mae: 1.4755 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 4s - loss: 10.3530 - mse: 10.3530 - mae: 1.3881 - val_loss: 9.4571 - val_mse: 9.4571 - val_mae: 1.4741 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 4s - loss: 10.2662 - mse: 10.2662 - mae: 1.3732 - val_loss: 9.3576 - val_mse: 9.3576 - val_mae: 1.4414 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 4s - loss: 10.0895 - mse: 10.0895 - mae: 1.3694 - val_loss: 9.4558 - val_mse: 9.4558 - val_mae: 1.4562 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Score for fold 4: loss of 9.455827713012695\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 4s - loss: 10.2583 - mse: 10.2583 - mae: 1.3846 - val_loss: 9.2214 - val_mse: 9.2214 - val_mae: 1.3774 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 4s - loss: 10.0703 - mse: 10.0703 - mae: 1.3646 - val_loss: 9.5369 - val_mse: 9.5369 - val_mae: 1.3598 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 4s - loss: 9.9353 - mse: 9.9353 - mae: 1.3523 - val_loss: 9.7179 - val_mse: 9.7179 - val_mae: 1.4054 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 4s - loss: 9.7426 - mse: 9.7426 - mae: 1.3423 - val_loss: 9.6757 - val_mse: 9.6757 - val_mae: 1.5150 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 4s - loss: 9.6054 - mse: 9.6054 - mae: 1.3311 - val_loss: 10.0817 - val_mse: 10.0817 - val_mae: 1.4308 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 4s - loss: 9.4001 - mse: 9.4001 - mae: 1.3255 - val_loss: 10.0536 - val_mse: 10.0536 - val_mae: 1.4717 - lr: 0.0010 - 4s/epoch - 4ms/step\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-10-11 10:47:59,406]\u001b[0m Finished trial#5 resulted in value: 11.572. Current best value is 9.334 with parameters: {'activation': 'relu', 'num_hidden_layer': 4, 'i': 9, 'learning_rate': 0.00023071835273936203}.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Score for fold 5: loss of 10.053594589233398\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 7s - loss: 14.1396 - mse: 14.1396 - mae: 1.5602 - val_loss: 9.4856 - val_mse: 9.4856 - val_mae: 1.5444 - lr: 0.0025 - 7s/epoch - 7ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 6s - loss: 13.0829 - mse: 13.0829 - mae: 1.5008 - val_loss: 9.6225 - val_mse: 9.6225 - val_mae: 1.4640 - lr: 0.0025 - 6s/epoch - 6ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 7s - loss: 13.0361 - mse: 13.0361 - mae: 1.4859 - val_loss: 9.4929 - val_mse: 9.4929 - val_mae: 1.5383 - lr: 0.0025 - 7s/epoch - 7ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 7s - loss: 13.0766 - mse: 13.0766 - mae: 1.4690 - val_loss: 9.2526 - val_mse: 9.2526 - val_mae: 1.4310 - lr: 0.0025 - 7s/epoch - 7ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 7s - loss: 12.9811 - mse: 12.9811 - mae: 1.4676 - val_loss: 9.7439 - val_mse: 9.7439 - val_mae: 1.6095 - lr: 0.0025 - 7s/epoch - 7ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 7s - loss: 12.7629 - mse: 12.7629 - mae: 1.4700 - val_loss: 9.0812 - val_mse: 9.0812 - val_mae: 1.4670 - lr: 0.0025 - 7s/epoch - 7ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 7s - loss: 12.6941 - mse: 12.6941 - mae: 1.4767 - val_loss: 9.0130 - val_mse: 9.0130 - val_mae: 1.4921 - lr: 0.0025 - 7s/epoch - 7ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 7s - loss: 12.6114 - mse: 12.6114 - mae: 1.4741 - val_loss: 9.3419 - val_mse: 9.3419 - val_mae: 1.4551 - lr: 0.0025 - 7s/epoch - 7ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 7s - loss: 12.4773 - mse: 12.4773 - mae: 1.4653 - val_loss: 9.0684 - val_mse: 9.0684 - val_mae: 1.4614 - lr: 0.0025 - 7s/epoch - 7ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 7s - loss: 12.4695 - mse: 12.4695 - mae: 1.4658 - val_loss: 9.4292 - val_mse: 9.4292 - val_mae: 1.4567 - lr: 0.0025 - 7s/epoch - 7ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 7s - loss: 12.4022 - mse: 12.4022 - mae: 1.4656 - val_loss: 9.1273 - val_mse: 9.1273 - val_mae: 1.4892 - lr: 0.0025 - 7s/epoch - 7ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 7s - loss: 12.4987 - mse: 12.4987 - mae: 1.4568 - val_loss: 9.3485 - val_mse: 9.3485 - val_mae: 1.4908 - lr: 0.0025 - 7s/epoch - 7ms/step\n",
            "Score for fold 1: loss of 9.34854793548584\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 6s - loss: 11.7211 - mse: 11.7211 - mae: 1.4338 - val_loss: 9.6271 - val_mse: 9.6271 - val_mae: 1.4660 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 6s - loss: 11.4952 - mse: 11.4952 - mae: 1.4115 - val_loss: 9.8482 - val_mse: 9.8482 - val_mae: 1.4468 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 7s - loss: 11.2963 - mse: 11.2963 - mae: 1.3997 - val_loss: 10.1446 - val_mse: 10.1446 - val_mae: 1.4782 - lr: 0.0010 - 7s/epoch - 7ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 7s - loss: 11.1205 - mse: 11.1205 - mae: 1.3976 - val_loss: 10.0116 - val_mse: 10.0116 - val_mae: 1.4309 - lr: 0.0010 - 7s/epoch - 7ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 7s - loss: 11.0314 - mse: 11.0314 - mae: 1.3858 - val_loss: 10.0835 - val_mse: 10.0835 - val_mae: 1.4561 - lr: 0.0010 - 7s/epoch - 7ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 7s - loss: 11.0042 - mse: 11.0042 - mae: 1.3854 - val_loss: 10.1222 - val_mse: 10.1222 - val_mae: 1.4140 - lr: 0.0010 - 7s/epoch - 7ms/step\n",
            "Score for fold 2: loss of 10.122154235839844\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 7s - loss: 11.3345 - mse: 11.3345 - mae: 1.3988 - val_loss: 8.5002 - val_mse: 8.5002 - val_mae: 1.3826 - lr: 0.0010 - 7s/epoch - 7ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 7s - loss: 11.2675 - mse: 11.2675 - mae: 1.3907 - val_loss: 8.3959 - val_mse: 8.3959 - val_mae: 1.3615 - lr: 0.0010 - 7s/epoch - 7ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 7s - loss: 11.1148 - mse: 11.1148 - mae: 1.3832 - val_loss: 9.0292 - val_mse: 9.0292 - val_mae: 1.3695 - lr: 0.0010 - 7s/epoch - 7ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 7s - loss: 11.1284 - mse: 11.1284 - mae: 1.3757 - val_loss: 8.6591 - val_mse: 8.6591 - val_mae: 1.4101 - lr: 0.0010 - 7s/epoch - 7ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 7s - loss: 10.9040 - mse: 10.9040 - mae: 1.3708 - val_loss: 8.6542 - val_mse: 8.6542 - val_mae: 1.4234 - lr: 0.0010 - 7s/epoch - 7ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 7s - loss: 10.9000 - mse: 10.9000 - mae: 1.3638 - val_loss: 8.5050 - val_mse: 8.5050 - val_mae: 1.3693 - lr: 0.0010 - 7s/epoch - 7ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 7s - loss: 10.8629 - mse: 10.8629 - mae: 1.3572 - val_loss: 8.8632 - val_mse: 8.8632 - val_mae: 1.3534 - lr: 0.0010 - 7s/epoch - 7ms/step\n",
            "Score for fold 3: loss of 8.863167762756348\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 7s - loss: 10.0162 - mse: 10.0162 - mae: 1.3711 - val_loss: 12.3190 - val_mse: 12.3190 - val_mae: 1.3019 - lr: 0.0010 - 7s/epoch - 7ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 7s - loss: 9.6244 - mse: 9.6244 - mae: 1.3647 - val_loss: 12.6743 - val_mse: 12.6743 - val_mae: 1.3434 - lr: 0.0010 - 7s/epoch - 7ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 7s - loss: 9.6132 - mse: 9.6132 - mae: 1.3544 - val_loss: 12.1425 - val_mse: 12.1425 - val_mae: 1.3376 - lr: 0.0010 - 7s/epoch - 7ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 7s - loss: 9.3785 - mse: 9.3785 - mae: 1.3467 - val_loss: 13.1109 - val_mse: 13.1109 - val_mae: 1.3491 - lr: 0.0010 - 7s/epoch - 7ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 7s - loss: 9.2160 - mse: 9.2160 - mae: 1.3398 - val_loss: 12.4482 - val_mse: 12.4482 - val_mae: 1.3546 - lr: 0.0010 - 7s/epoch - 7ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 7s - loss: 9.2396 - mse: 9.2396 - mae: 1.3305 - val_loss: 12.5470 - val_mse: 12.5470 - val_mae: 1.3682 - lr: 0.0010 - 7s/epoch - 7ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 7s - loss: 8.9701 - mse: 8.9701 - mae: 1.3191 - val_loss: 12.6425 - val_mse: 12.6425 - val_mae: 1.3804 - lr: 0.0010 - 7s/epoch - 7ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 7s - loss: 9.0103 - mse: 9.0103 - mae: 1.3167 - val_loss: 12.9448 - val_mse: 12.9448 - val_mae: 1.3308 - lr: 0.0010 - 7s/epoch - 7ms/step\n",
            "Score for fold 4: loss of 12.944788932800293\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 7s - loss: 8.6825 - mse: 8.6825 - mae: 1.3339 - val_loss: 13.9469 - val_mse: 13.9469 - val_mae: 1.3009 - lr: 0.0010 - 7s/epoch - 7ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 7s - loss: 8.4440 - mse: 8.4440 - mae: 1.3229 - val_loss: 13.7842 - val_mse: 13.7842 - val_mae: 1.3065 - lr: 0.0010 - 7s/epoch - 7ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 7s - loss: 8.2643 - mse: 8.2643 - mae: 1.3100 - val_loss: 14.2611 - val_mse: 14.2611 - val_mae: 1.2883 - lr: 0.0010 - 7s/epoch - 7ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 7s - loss: 8.1649 - mse: 8.1649 - mae: 1.3008 - val_loss: 13.9514 - val_mse: 13.9514 - val_mae: 1.3064 - lr: 0.0010 - 7s/epoch - 7ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 7s - loss: 7.8515 - mse: 7.8515 - mae: 1.2883 - val_loss: 14.2309 - val_mse: 14.2309 - val_mae: 1.2963 - lr: 0.0010 - 7s/epoch - 7ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 7s - loss: 7.7910 - mse: 7.7910 - mae: 1.2787 - val_loss: 14.4254 - val_mse: 14.4254 - val_mae: 1.2998 - lr: 0.0010 - 7s/epoch - 7ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 7s - loss: 7.7499 - mse: 7.7499 - mae: 1.2711 - val_loss: 14.0752 - val_mse: 14.0752 - val_mae: 1.3721 - lr: 0.0010 - 7s/epoch - 7ms/step\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-10-11 10:52:36,432]\u001b[0m Finished trial#6 resulted in value: 11.069999999999999. Current best value is 9.334 with parameters: {'activation': 'relu', 'num_hidden_layer': 4, 'i': 9, 'learning_rate': 0.00023071835273936203}.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Score for fold 5: loss of 14.075221061706543\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 3s - loss: 14.2504 - mse: 14.2504 - mae: 1.6128 - val_loss: 14.0697 - val_mse: 14.0697 - val_mae: 1.5552 - lr: 5.8987e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 13.1531 - mse: 13.1531 - mae: 1.5647 - val_loss: 14.1178 - val_mse: 14.1178 - val_mae: 1.5529 - lr: 5.8987e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 13.2047 - mse: 13.2047 - mae: 1.5577 - val_loss: 14.0560 - val_mse: 14.0560 - val_mae: 1.5277 - lr: 5.8987e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 13.1099 - mse: 13.1099 - mae: 1.5511 - val_loss: 13.8678 - val_mse: 13.8678 - val_mae: 1.5715 - lr: 5.8987e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 13.0573 - mse: 13.0573 - mae: 1.5569 - val_loss: 13.9715 - val_mse: 13.9715 - val_mae: 1.5547 - lr: 5.8987e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 13.1556 - mse: 13.1556 - mae: 1.5504 - val_loss: 13.9423 - val_mse: 13.9423 - val_mae: 1.5711 - lr: 5.8987e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 13.1334 - mse: 13.1334 - mae: 1.5523 - val_loss: 13.9197 - val_mse: 13.9197 - val_mae: 1.5106 - lr: 5.8987e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 2s - loss: 13.1182 - mse: 13.1182 - mae: 1.5527 - val_loss: 13.9838 - val_mse: 13.9838 - val_mae: 1.5331 - lr: 5.8987e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 2s - loss: 13.0466 - mse: 13.0466 - mae: 1.5521 - val_loss: 14.1190 - val_mse: 14.1190 - val_mae: 1.6095 - lr: 5.8987e-04 - 2s/epoch - 2ms/step\n",
            "Score for fold 1: loss of 14.119017601013184\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 14.0270 - mse: 14.0270 - mae: 1.5597 - val_loss: 10.4079 - val_mse: 10.4079 - val_mae: 1.5882 - lr: 5.8987e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 13.9840 - mse: 13.9840 - mae: 1.5543 - val_loss: 10.4629 - val_mse: 10.4629 - val_mae: 1.4994 - lr: 5.8987e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 14.0172 - mse: 14.0172 - mae: 1.5566 - val_loss: 10.3579 - val_mse: 10.3579 - val_mae: 1.5465 - lr: 5.8987e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 13.9715 - mse: 13.9715 - mae: 1.5534 - val_loss: 10.3524 - val_mse: 10.3524 - val_mae: 1.5303 - lr: 5.8987e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 14.0542 - mse: 14.0542 - mae: 1.5583 - val_loss: 10.4928 - val_mse: 10.4928 - val_mae: 1.4908 - lr: 5.8987e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 13.9499 - mse: 13.9499 - mae: 1.5553 - val_loss: 10.4005 - val_mse: 10.4005 - val_mae: 1.5498 - lr: 5.8987e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 13.9740 - mse: 13.9740 - mae: 1.5622 - val_loss: 10.4394 - val_mse: 10.4394 - val_mae: 1.5046 - lr: 5.8987e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 2s - loss: 13.9931 - mse: 13.9931 - mae: 1.5551 - val_loss: 10.3277 - val_mse: 10.3277 - val_mae: 1.5655 - lr: 5.8987e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 2s - loss: 13.9737 - mse: 13.9737 - mae: 1.5610 - val_loss: 10.3973 - val_mse: 10.3973 - val_mae: 1.5097 - lr: 5.8987e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 2s - loss: 13.9970 - mse: 13.9970 - mae: 1.5541 - val_loss: 10.4719 - val_mse: 10.4719 - val_mae: 1.4988 - lr: 5.8987e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 2s - loss: 14.0857 - mse: 14.0857 - mae: 1.5571 - val_loss: 10.3799 - val_mse: 10.3799 - val_mae: 1.5916 - lr: 5.8987e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 2s - loss: 14.0024 - mse: 14.0024 - mae: 1.5577 - val_loss: 10.4111 - val_mse: 10.4111 - val_mae: 1.4793 - lr: 5.8987e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 13/100\n",
            "1000/1000 - 2s - loss: 13.9129 - mse: 13.9129 - mae: 1.5538 - val_loss: 10.3959 - val_mse: 10.3959 - val_mae: 1.5196 - lr: 5.8987e-04 - 2s/epoch - 2ms/step\n",
            "Score for fold 2: loss of 10.395918846130371\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 13.7170 - mse: 13.7170 - mae: 1.5564 - val_loss: 11.5427 - val_mse: 11.5427 - val_mae: 1.4970 - lr: 5.8987e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 13.7078 - mse: 13.7078 - mae: 1.5599 - val_loss: 11.5214 - val_mse: 11.5214 - val_mae: 1.5488 - lr: 5.8987e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 13.6817 - mse: 13.6817 - mae: 1.5564 - val_loss: 11.9766 - val_mse: 11.9766 - val_mae: 1.6221 - lr: 5.8987e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 13.7699 - mse: 13.7699 - mae: 1.5551 - val_loss: 11.4995 - val_mse: 11.4995 - val_mae: 1.5010 - lr: 5.8987e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 13.7589 - mse: 13.7589 - mae: 1.5576 - val_loss: 11.5021 - val_mse: 11.5021 - val_mae: 1.4976 - lr: 5.8987e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 13.7174 - mse: 13.7174 - mae: 1.5597 - val_loss: 11.5265 - val_mse: 11.5265 - val_mae: 1.5363 - lr: 5.8987e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 13.7347 - mse: 13.7347 - mae: 1.5608 - val_loss: 11.5150 - val_mse: 11.5150 - val_mae: 1.5047 - lr: 5.8987e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 2s - loss: 13.6225 - mse: 13.6225 - mae: 1.5579 - val_loss: 11.5859 - val_mse: 11.5859 - val_mae: 1.4830 - lr: 5.8987e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 2s - loss: 13.7636 - mse: 13.7636 - mae: 1.5543 - val_loss: 11.5024 - val_mse: 11.5024 - val_mae: 1.4908 - lr: 5.8987e-04 - 2s/epoch - 2ms/step\n",
            "Score for fold 3: loss of 11.502391815185547\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 13.9829 - mse: 13.9829 - mae: 1.5473 - val_loss: 10.5697 - val_mse: 10.5697 - val_mae: 1.6118 - lr: 5.8987e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 14.0134 - mse: 14.0134 - mae: 1.5447 - val_loss: 10.5447 - val_mse: 10.5447 - val_mae: 1.6025 - lr: 5.8987e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 13.9633 - mse: 13.9633 - mae: 1.5422 - val_loss: 10.5096 - val_mse: 10.5096 - val_mae: 1.5843 - lr: 5.8987e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 13.9056 - mse: 13.9056 - mae: 1.5420 - val_loss: 10.9274 - val_mse: 10.9274 - val_mae: 1.5507 - lr: 5.8987e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 13.9688 - mse: 13.9688 - mae: 1.5424 - val_loss: 10.5083 - val_mse: 10.5083 - val_mae: 1.5856 - lr: 5.8987e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 13.8066 - mse: 13.8066 - mae: 1.5446 - val_loss: 10.4935 - val_mse: 10.4935 - val_mae: 1.5487 - lr: 5.8987e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 13.8965 - mse: 13.8965 - mae: 1.5447 - val_loss: 10.6464 - val_mse: 10.6464 - val_mae: 1.5670 - lr: 5.8987e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 2s - loss: 13.9363 - mse: 13.9363 - mae: 1.5435 - val_loss: 10.5438 - val_mse: 10.5438 - val_mae: 1.5932 - lr: 5.8987e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 2s - loss: 13.9590 - mse: 13.9590 - mae: 1.5459 - val_loss: 10.5281 - val_mse: 10.5281 - val_mae: 1.5846 - lr: 5.8987e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 2s - loss: 13.9629 - mse: 13.9629 - mae: 1.5425 - val_loss: 10.6776 - val_mse: 10.6776 - val_mae: 1.5484 - lr: 5.8987e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 2s - loss: 13.9106 - mse: 13.9106 - mae: 1.5410 - val_loss: 10.6083 - val_mse: 10.6083 - val_mae: 1.5676 - lr: 5.8987e-04 - 2s/epoch - 2ms/step\n",
            "Score for fold 4: loss of 10.608301162719727\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 11.6589 - mse: 11.6589 - mae: 1.5420 - val_loss: 19.8669 - val_mse: 19.8669 - val_mae: 1.5610 - lr: 5.8987e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 11.5362 - mse: 11.5362 - mae: 1.5366 - val_loss: 19.8137 - val_mse: 19.8137 - val_mae: 1.5143 - lr: 5.8987e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 11.6024 - mse: 11.6024 - mae: 1.5444 - val_loss: 19.7461 - val_mse: 19.7461 - val_mae: 1.5842 - lr: 5.8987e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 11.6558 - mse: 11.6558 - mae: 1.5432 - val_loss: 19.7959 - val_mse: 19.7959 - val_mae: 1.5548 - lr: 5.8987e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 11.5060 - mse: 11.5060 - mae: 1.5447 - val_loss: 19.8802 - val_mse: 19.8802 - val_mae: 1.5812 - lr: 5.8987e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 11.6232 - mse: 11.6232 - mae: 1.5443 - val_loss: 19.7943 - val_mse: 19.7943 - val_mae: 1.5414 - lr: 5.8987e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 11.5189 - mse: 11.5189 - mae: 1.5441 - val_loss: 19.8422 - val_mse: 19.8422 - val_mae: 1.5369 - lr: 5.8987e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 2s - loss: 11.5222 - mse: 11.5222 - mae: 1.5465 - val_loss: 19.7983 - val_mse: 19.7983 - val_mae: 1.5660 - lr: 5.8987e-04 - 2s/epoch - 2ms/step\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-10-11 10:54:39,417]\u001b[0m Finished trial#7 resulted in value: 13.285999999999998. Current best value is 9.334 with parameters: {'activation': 'relu', 'num_hidden_layer': 4, 'i': 9, 'learning_rate': 0.00023071835273936203}.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Score for fold 5: loss of 19.79832649230957\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 6s - loss: 12.3183 - mse: 12.3183 - mae: 1.5358 - val_loss: 16.7669 - val_mse: 16.7669 - val_mae: 1.5131 - lr: 3.1729e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 6s - loss: 11.3651 - mse: 11.3651 - mae: 1.4847 - val_loss: 16.5348 - val_mse: 16.5348 - val_mae: 1.4944 - lr: 3.1729e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 6s - loss: 11.2229 - mse: 11.2229 - mae: 1.4774 - val_loss: 16.8629 - val_mse: 16.8629 - val_mae: 1.4349 - lr: 3.1729e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 6s - loss: 10.8657 - mse: 10.8657 - mae: 1.4607 - val_loss: 16.8664 - val_mse: 16.8664 - val_mae: 1.4768 - lr: 3.1729e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 6s - loss: 10.6968 - mse: 10.6968 - mae: 1.4481 - val_loss: 16.5384 - val_mse: 16.5384 - val_mae: 1.4302 - lr: 3.1729e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 6s - loss: 10.7741 - mse: 10.7741 - mae: 1.4373 - val_loss: 16.5074 - val_mse: 16.5074 - val_mae: 1.4948 - lr: 3.1729e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 6s - loss: 10.3696 - mse: 10.3696 - mae: 1.4349 - val_loss: 17.2114 - val_mse: 17.2114 - val_mae: 1.4944 - lr: 3.1729e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 6s - loss: 10.3478 - mse: 10.3478 - mae: 1.4272 - val_loss: 16.9190 - val_mse: 16.9190 - val_mae: 1.4680 - lr: 3.1729e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 6s - loss: 10.0026 - mse: 10.0026 - mae: 1.4153 - val_loss: 16.6923 - val_mse: 16.6923 - val_mae: 1.5335 - lr: 3.1729e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 6s - loss: 9.9401 - mse: 9.9401 - mae: 1.4012 - val_loss: 17.1083 - val_mse: 17.1083 - val_mae: 1.4669 - lr: 3.1729e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 6s - loss: 9.4270 - mse: 9.4270 - mae: 1.3918 - val_loss: 16.7474 - val_mse: 16.7474 - val_mae: 1.5163 - lr: 3.1729e-04 - 6s/epoch - 6ms/step\n",
            "Score for fold 1: loss of 16.747426986694336\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 6s - loss: 9.9876 - mse: 9.9876 - mae: 1.4025 - val_loss: 14.1486 - val_mse: 14.1486 - val_mae: 1.4019 - lr: 3.1729e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 6s - loss: 9.7763 - mse: 9.7763 - mae: 1.3880 - val_loss: 14.4775 - val_mse: 14.4775 - val_mae: 1.4131 - lr: 3.1729e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 7s - loss: 9.4741 - mse: 9.4741 - mae: 1.3702 - val_loss: 14.6606 - val_mse: 14.6606 - val_mae: 1.4451 - lr: 3.1729e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 6s - loss: 9.3157 - mse: 9.3157 - mae: 1.3622 - val_loss: 14.8244 - val_mse: 14.8244 - val_mae: 1.5108 - lr: 3.1729e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 6s - loss: 8.9871 - mse: 8.9871 - mae: 1.3434 - val_loss: 18.5738 - val_mse: 18.5738 - val_mae: 1.5758 - lr: 3.1729e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 6s - loss: 8.7458 - mse: 8.7458 - mae: 1.3330 - val_loss: 15.2973 - val_mse: 15.2973 - val_mae: 1.3837 - lr: 3.1729e-04 - 6s/epoch - 6ms/step\n",
            "Score for fold 2: loss of 15.297273635864258\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 6s - loss: 10.4032 - mse: 10.4032 - mae: 1.3517 - val_loss: 8.9668 - val_mse: 8.9668 - val_mae: 1.4107 - lr: 3.1729e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 6s - loss: 9.9708 - mse: 9.9708 - mae: 1.3356 - val_loss: 8.5744 - val_mse: 8.5744 - val_mae: 1.3099 - lr: 3.1729e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 6s - loss: 9.8105 - mse: 9.8105 - mae: 1.3173 - val_loss: 11.1964 - val_mse: 11.1964 - val_mae: 1.3594 - lr: 3.1729e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 6s - loss: 9.5788 - mse: 9.5788 - mae: 1.3096 - val_loss: 11.6389 - val_mse: 11.6389 - val_mae: 1.3657 - lr: 3.1729e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 6s - loss: 9.2049 - mse: 9.2049 - mae: 1.2898 - val_loss: 8.8894 - val_mse: 8.8894 - val_mae: 1.3753 - lr: 3.1729e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 6s - loss: 9.0646 - mse: 9.0646 - mae: 1.2748 - val_loss: 9.6679 - val_mse: 9.6679 - val_mae: 1.4173 - lr: 3.1729e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 6s - loss: 8.8557 - mse: 8.8557 - mae: 1.2611 - val_loss: 10.0015 - val_mse: 10.0015 - val_mae: 1.3703 - lr: 3.1729e-04 - 6s/epoch - 6ms/step\n",
            "Score for fold 3: loss of 10.001542091369629\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 6s - loss: 9.9781 - mse: 9.9781 - mae: 1.3012 - val_loss: 4.3590 - val_mse: 4.3590 - val_mae: 1.3143 - lr: 3.1729e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 6s - loss: 9.5098 - mse: 9.5098 - mae: 1.2823 - val_loss: 4.6103 - val_mse: 4.6103 - val_mae: 1.1934 - lr: 3.1729e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 6s - loss: 9.4205 - mse: 9.4205 - mae: 1.2626 - val_loss: 4.4085 - val_mse: 4.4085 - val_mae: 1.2625 - lr: 3.1729e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 6s - loss: 9.3690 - mse: 9.3690 - mae: 1.2529 - val_loss: 4.6687 - val_mse: 4.6687 - val_mae: 1.2335 - lr: 3.1729e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 6s - loss: 9.3385 - mse: 9.3385 - mae: 1.2383 - val_loss: 5.0030 - val_mse: 5.0030 - val_mae: 1.2820 - lr: 3.1729e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 6s - loss: 9.0261 - mse: 9.0261 - mae: 1.2291 - val_loss: 5.0843 - val_mse: 5.0843 - val_mae: 1.2487 - lr: 3.1729e-04 - 6s/epoch - 6ms/step\n",
            "Score for fold 4: loss of 5.08427619934082\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 6s - loss: 9.0319 - mse: 9.0319 - mae: 1.2512 - val_loss: 5.8066 - val_mse: 5.8066 - val_mae: 1.1848 - lr: 3.1729e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 6s - loss: 8.2471 - mse: 8.2471 - mae: 1.2331 - val_loss: 4.6053 - val_mse: 4.6053 - val_mae: 1.1765 - lr: 3.1729e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 6s - loss: 8.0157 - mse: 8.0157 - mae: 1.2126 - val_loss: 4.5653 - val_mse: 4.5653 - val_mae: 1.2462 - lr: 3.1729e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 6s - loss: 7.6927 - mse: 7.6927 - mae: 1.1989 - val_loss: 5.4515 - val_mse: 5.4515 - val_mae: 1.2094 - lr: 3.1729e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 6s - loss: 7.7595 - mse: 7.7595 - mae: 1.1807 - val_loss: 6.0090 - val_mse: 6.0090 - val_mae: 1.2193 - lr: 3.1729e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 6s - loss: 7.6975 - mse: 7.6975 - mae: 1.1838 - val_loss: 5.2025 - val_mse: 5.2025 - val_mae: 1.1866 - lr: 3.1729e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 6s - loss: 7.3532 - mse: 7.3532 - mae: 1.1664 - val_loss: 4.9412 - val_mse: 4.9412 - val_mae: 1.2199 - lr: 3.1729e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 6s - loss: 7.0788 - mse: 7.0788 - mae: 1.1474 - val_loss: 6.1580 - val_mse: 6.1580 - val_mae: 1.3152 - lr: 3.1729e-04 - 6s/epoch - 6ms/step\n",
            "Score for fold 5: loss of 6.15797233581543\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-10-11 10:58:37,945]\u001b[0m Finished trial#8 resulted in value: 10.657999999999998. Current best value is 9.334 with parameters: {'activation': 'relu', 'num_hidden_layer': 4, 'i': 9, 'learning_rate': 0.00023071835273936203}.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 16.6195 - mse: 16.6195 - mae: 1.7684 - val_loss: 10.7148 - val_mse: 10.7148 - val_mae: 1.5480 - lr: 2.0494e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 13.9846 - mse: 13.9846 - mae: 1.5561 - val_loss: 10.5603 - val_mse: 10.5603 - val_mae: 1.5335 - lr: 2.0494e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 1s - loss: 13.9042 - mse: 13.9042 - mae: 1.5520 - val_loss: 10.4740 - val_mse: 10.4740 - val_mae: 1.5224 - lr: 2.0494e-04 - 1s/epoch - 1ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 1s - loss: 13.9211 - mse: 13.9211 - mae: 1.5478 - val_loss: 10.4858 - val_mse: 10.4858 - val_mae: 1.5267 - lr: 2.0494e-04 - 1s/epoch - 1ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 1s - loss: 13.8878 - mse: 13.8878 - mae: 1.5469 - val_loss: 10.5191 - val_mse: 10.5191 - val_mae: 1.5642 - lr: 2.0494e-04 - 1s/epoch - 1ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 1s - loss: 13.8881 - mse: 13.8881 - mae: 1.5499 - val_loss: 10.4507 - val_mse: 10.4507 - val_mae: 1.5398 - lr: 2.0494e-04 - 1s/epoch - 1ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 1s - loss: 13.9144 - mse: 13.9144 - mae: 1.5448 - val_loss: 10.4754 - val_mse: 10.4754 - val_mae: 1.5320 - lr: 2.0494e-04 - 1s/epoch - 1ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 1s - loss: 13.8864 - mse: 13.8864 - mae: 1.5458 - val_loss: 10.4906 - val_mse: 10.4906 - val_mae: 1.5365 - lr: 2.0494e-04 - 1s/epoch - 1ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 1s - loss: 13.8796 - mse: 13.8796 - mae: 1.5475 - val_loss: 10.5047 - val_mse: 10.5047 - val_mae: 1.5220 - lr: 2.0494e-04 - 1s/epoch - 1ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 2s - loss: 13.8658 - mse: 13.8658 - mae: 1.5429 - val_loss: 10.5173 - val_mse: 10.5173 - val_mae: 1.5350 - lr: 2.0494e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 2s - loss: 13.9405 - mse: 13.9405 - mae: 1.5462 - val_loss: 10.4537 - val_mse: 10.4537 - val_mae: 1.5359 - lr: 2.0494e-04 - 2s/epoch - 2ms/step\n",
            "Score for fold 1: loss of 10.45373249053955\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 11.6232 - mse: 11.6232 - mae: 1.5443 - val_loss: 19.5775 - val_mse: 19.5775 - val_mae: 1.5513 - lr: 2.0494e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 11.6138 - mse: 11.6138 - mae: 1.5382 - val_loss: 19.6862 - val_mse: 19.6862 - val_mae: 1.5636 - lr: 2.0494e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 11.6090 - mse: 11.6090 - mae: 1.5399 - val_loss: 19.6441 - val_mse: 19.6441 - val_mae: 1.5664 - lr: 2.0494e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 1s - loss: 11.5649 - mse: 11.5649 - mae: 1.5378 - val_loss: 19.6135 - val_mse: 19.6135 - val_mae: 1.5647 - lr: 2.0494e-04 - 1s/epoch - 1ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 11.6134 - mse: 11.6134 - mae: 1.5429 - val_loss: 19.6420 - val_mse: 19.6420 - val_mae: 1.5681 - lr: 2.0494e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 11.5812 - mse: 11.5812 - mae: 1.5401 - val_loss: 19.6174 - val_mse: 19.6174 - val_mae: 1.5493 - lr: 2.0494e-04 - 2s/epoch - 2ms/step\n",
            "Score for fold 2: loss of 19.617448806762695\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 13.4956 - mse: 13.4956 - mae: 1.5372 - val_loss: 12.1613 - val_mse: 12.1613 - val_mae: 1.5544 - lr: 2.0494e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 13.4753 - mse: 13.4753 - mae: 1.5352 - val_loss: 12.3030 - val_mse: 12.3030 - val_mae: 1.5629 - lr: 2.0494e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 13.4632 - mse: 13.4632 - mae: 1.5377 - val_loss: 12.4237 - val_mse: 12.4237 - val_mae: 1.5588 - lr: 2.0494e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 13.4681 - mse: 13.4681 - mae: 1.5342 - val_loss: 12.2629 - val_mse: 12.2629 - val_mae: 1.5566 - lr: 2.0494e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 13.4664 - mse: 13.4664 - mae: 1.5321 - val_loss: 12.2480 - val_mse: 12.2480 - val_mae: 1.5845 - lr: 2.0494e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 13.4739 - mse: 13.4739 - mae: 1.5349 - val_loss: 12.3923 - val_mse: 12.3923 - val_mae: 1.5769 - lr: 2.0494e-04 - 2s/epoch - 2ms/step\n",
            "Score for fold 3: loss of 12.392269134521484\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 12.9770 - mse: 12.9770 - mae: 1.5449 - val_loss: 14.1572 - val_mse: 14.1572 - val_mae: 1.5234 - lr: 2.0494e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 12.9713 - mse: 12.9713 - mae: 1.5458 - val_loss: 14.2227 - val_mse: 14.2227 - val_mae: 1.5371 - lr: 2.0494e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 12.9707 - mse: 12.9707 - mae: 1.5439 - val_loss: 14.2169 - val_mse: 14.2169 - val_mae: 1.5433 - lr: 2.0494e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 12.9693 - mse: 12.9693 - mae: 1.5397 - val_loss: 14.2445 - val_mse: 14.2445 - val_mae: 1.5477 - lr: 2.0494e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 13.0036 - mse: 13.0036 - mae: 1.5426 - val_loss: 14.2087 - val_mse: 14.2087 - val_mae: 1.5533 - lr: 2.0494e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 12.9567 - mse: 12.9567 - mae: 1.5440 - val_loss: 14.2349 - val_mse: 14.2349 - val_mae: 1.5279 - lr: 2.0494e-04 - 2s/epoch - 2ms/step\n",
            "Score for fold 4: loss of 14.234891891479492\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 14.1011 - mse: 14.1011 - mae: 1.5471 - val_loss: 9.5898 - val_mse: 9.5898 - val_mae: 1.5102 - lr: 2.0494e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 14.0880 - mse: 14.0880 - mae: 1.5434 - val_loss: 9.5908 - val_mse: 9.5908 - val_mae: 1.5144 - lr: 2.0494e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 14.0892 - mse: 14.0892 - mae: 1.5433 - val_loss: 9.5734 - val_mse: 9.5734 - val_mae: 1.5226 - lr: 2.0494e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 1s - loss: 14.1023 - mse: 14.1023 - mae: 1.5472 - val_loss: 9.5998 - val_mse: 9.5998 - val_mae: 1.5381 - lr: 2.0494e-04 - 1s/epoch - 1ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 14.0955 - mse: 14.0955 - mae: 1.5477 - val_loss: 9.6159 - val_mse: 9.6159 - val_mae: 1.5240 - lr: 2.0494e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 1s - loss: 14.1158 - mse: 14.1158 - mae: 1.5458 - val_loss: 9.5596 - val_mse: 9.5596 - val_mae: 1.5262 - lr: 2.0494e-04 - 1s/epoch - 1ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 14.1030 - mse: 14.1030 - mae: 1.5476 - val_loss: 9.5747 - val_mse: 9.5747 - val_mae: 1.5240 - lr: 2.0494e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 1s - loss: 14.1018 - mse: 14.1018 - mae: 1.5477 - val_loss: 9.6115 - val_mse: 9.6115 - val_mae: 1.5223 - lr: 2.0494e-04 - 1s/epoch - 1ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 2s - loss: 14.0895 - mse: 14.0895 - mae: 1.5440 - val_loss: 9.5811 - val_mse: 9.5811 - val_mae: 1.5200 - lr: 2.0494e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 2s - loss: 14.0763 - mse: 14.0763 - mae: 1.5515 - val_loss: 9.6074 - val_mse: 9.6074 - val_mae: 1.5260 - lr: 2.0494e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 2s - loss: 14.1000 - mse: 14.1000 - mae: 1.5468 - val_loss: 9.5922 - val_mse: 9.5922 - val_mae: 1.5224 - lr: 2.0494e-04 - 2s/epoch - 2ms/step\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-10-11 10:59:43,716]\u001b[0m Finished trial#9 resulted in value: 13.256. Current best value is 9.334 with parameters: {'activation': 'relu', 'num_hidden_layer': 4, 'i': 9, 'learning_rate': 0.00023071835273936203}.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Score for fold 5: loss of 9.59223461151123\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 4s - loss: 12.2538 - mse: 12.2538 - mae: 1.5929 - val_loss: 18.6373 - val_mse: 18.6373 - val_mae: 1.5012 - lr: 0.0086 - 4s/epoch - 4ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 3s - loss: 11.4549 - mse: 11.4549 - mae: 1.5373 - val_loss: 18.1190 - val_mse: 18.1190 - val_mae: 1.4944 - lr: 0.0086 - 3s/epoch - 3ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 3s - loss: 11.1087 - mse: 11.1087 - mae: 1.5218 - val_loss: 17.9011 - val_mse: 17.9011 - val_mae: 1.4947 - lr: 0.0086 - 3s/epoch - 3ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 3s - loss: 10.8121 - mse: 10.8121 - mae: 1.5096 - val_loss: 18.1076 - val_mse: 18.1076 - val_mae: 1.5153 - lr: 0.0086 - 3s/epoch - 3ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 3s - loss: 11.0548 - mse: 11.0548 - mae: 1.5131 - val_loss: 18.0707 - val_mse: 18.0707 - val_mae: 1.4866 - lr: 0.0086 - 3s/epoch - 3ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 4s - loss: 11.3225 - mse: 11.3225 - mae: 1.5147 - val_loss: 18.4901 - val_mse: 18.4901 - val_mae: 1.5568 - lr: 0.0086 - 4s/epoch - 4ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 3s - loss: 11.2025 - mse: 11.2025 - mae: 1.5446 - val_loss: 18.3470 - val_mse: 18.3470 - val_mae: 1.6876 - lr: 0.0086 - 3s/epoch - 3ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 4s - loss: 11.5483 - mse: 11.5483 - mae: 1.5688 - val_loss: 19.4186 - val_mse: 19.4186 - val_mae: 1.6235 - lr: 0.0086 - 4s/epoch - 4ms/step\n",
            "Score for fold 1: loss of 19.418598175048828\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 4s - loss: 12.9782 - mse: 12.9782 - mae: 1.5826 - val_loss: 12.4419 - val_mse: 12.4419 - val_mae: 1.5247 - lr: 0.0017 - 4s/epoch - 4ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 4s - loss: 13.0180 - mse: 13.0180 - mae: 1.5783 - val_loss: 12.1600 - val_mse: 12.1600 - val_mae: 1.5933 - lr: 0.0017 - 4s/epoch - 4ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 3s - loss: 13.0118 - mse: 13.0118 - mae: 1.5899 - val_loss: 12.6342 - val_mse: 12.6342 - val_mae: 1.6043 - lr: 0.0017 - 3s/epoch - 3ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 4s - loss: 12.8511 - mse: 12.8511 - mae: 1.5876 - val_loss: 12.4971 - val_mse: 12.4971 - val_mae: 1.6092 - lr: 0.0017 - 4s/epoch - 4ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 3s - loss: 12.7552 - mse: 12.7552 - mae: 1.5783 - val_loss: 12.2427 - val_mse: 12.2427 - val_mae: 1.5605 - lr: 0.0017 - 3s/epoch - 3ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 4s - loss: 12.9065 - mse: 12.9065 - mae: 1.5884 - val_loss: 12.1775 - val_mse: 12.1775 - val_mae: 1.6188 - lr: 0.0017 - 4s/epoch - 4ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 4s - loss: 13.9966 - mse: 13.9966 - mae: 1.7203 - val_loss: 12.4425 - val_mse: 12.4425 - val_mae: 1.6309 - lr: 0.0017 - 4s/epoch - 4ms/step\n",
            "Score for fold 2: loss of 12.442508697509766\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 4s - loss: 13.1454 - mse: 13.1454 - mae: 1.6308 - val_loss: 13.0409 - val_mse: 13.0409 - val_mae: 1.6280 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 3s - loss: 12.6126 - mse: 12.6126 - mae: 1.5791 - val_loss: 13.2153 - val_mse: 13.2153 - val_mae: 1.5483 - lr: 0.0010 - 3s/epoch - 3ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 3s - loss: 12.5412 - mse: 12.5412 - mae: 1.5473 - val_loss: 12.4141 - val_mse: 12.4141 - val_mae: 1.5194 - lr: 0.0010 - 3s/epoch - 3ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 3s - loss: 12.2576 - mse: 12.2576 - mae: 1.5278 - val_loss: 12.3709 - val_mse: 12.3709 - val_mae: 1.5632 - lr: 0.0010 - 3s/epoch - 3ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 3s - loss: 12.0488 - mse: 12.0488 - mae: 1.5160 - val_loss: 12.3383 - val_mse: 12.3383 - val_mae: 1.5000 - lr: 0.0010 - 3s/epoch - 3ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 3s - loss: 11.9784 - mse: 11.9784 - mae: 1.5119 - val_loss: 12.3462 - val_mse: 12.3462 - val_mae: 1.4752 - lr: 0.0010 - 3s/epoch - 3ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 4s - loss: 11.9584 - mse: 11.9584 - mae: 1.5066 - val_loss: 12.5816 - val_mse: 12.5816 - val_mae: 1.4813 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 4s - loss: 12.0109 - mse: 12.0109 - mae: 1.5034 - val_loss: 12.2656 - val_mse: 12.2656 - val_mae: 1.4652 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 4s - loss: 11.9323 - mse: 11.9323 - mae: 1.4924 - val_loss: 12.6108 - val_mse: 12.6108 - val_mae: 1.4538 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 4s - loss: 11.9965 - mse: 11.9965 - mae: 1.4933 - val_loss: 12.1133 - val_mse: 12.1133 - val_mae: 1.4856 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 4s - loss: 11.8221 - mse: 11.8221 - mae: 1.4890 - val_loss: 12.4664 - val_mse: 12.4664 - val_mae: 1.4839 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 4s - loss: 11.8919 - mse: 11.8919 - mae: 1.4895 - val_loss: 12.2724 - val_mse: 12.2724 - val_mae: 1.4728 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 13/100\n",
            "1000/1000 - 3s - loss: 11.8334 - mse: 11.8334 - mae: 1.4922 - val_loss: 12.1546 - val_mse: 12.1546 - val_mae: 1.4633 - lr: 0.0010 - 3s/epoch - 3ms/step\n",
            "Epoch 14/100\n",
            "1000/1000 - 3s - loss: 11.7786 - mse: 11.7786 - mae: 1.4758 - val_loss: 12.3695 - val_mse: 12.3695 - val_mae: 1.5329 - lr: 0.0010 - 3s/epoch - 3ms/step\n",
            "Epoch 15/100\n",
            "1000/1000 - 4s - loss: 11.8204 - mse: 11.8204 - mae: 1.4920 - val_loss: 12.3310 - val_mse: 12.3310 - val_mae: 1.5708 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Score for fold 3: loss of 12.33095645904541\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 3s - loss: 12.5220 - mse: 12.5220 - mae: 1.5071 - val_loss: 10.0625 - val_mse: 10.0625 - val_mae: 1.4901 - lr: 0.0010 - 3s/epoch - 3ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 3s - loss: 12.4824 - mse: 12.4824 - mae: 1.5140 - val_loss: 10.1933 - val_mse: 10.1933 - val_mae: 1.4566 - lr: 0.0010 - 3s/epoch - 3ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 4s - loss: 12.5165 - mse: 12.5165 - mae: 1.5218 - val_loss: 10.1242 - val_mse: 10.1242 - val_mae: 1.5017 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 4s - loss: 12.5718 - mse: 12.5718 - mae: 1.5233 - val_loss: 10.4646 - val_mse: 10.4646 - val_mae: 1.5636 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 4s - loss: 12.5917 - mse: 12.5917 - mae: 1.5336 - val_loss: 11.0014 - val_mse: 11.0014 - val_mae: 1.4817 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 4s - loss: 12.5644 - mse: 12.5644 - mae: 1.5344 - val_loss: 10.3700 - val_mse: 10.3700 - val_mae: 1.5691 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Score for fold 4: loss of 10.369956016540527\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 3s - loss: 13.1097 - mse: 13.1097 - mae: 1.5272 - val_loss: 8.2579 - val_mse: 8.2579 - val_mae: 1.5923 - lr: 0.0010 - 3s/epoch - 3ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 3s - loss: 12.9857 - mse: 12.9857 - mae: 1.5195 - val_loss: 8.2753 - val_mse: 8.2753 - val_mae: 1.5161 - lr: 0.0010 - 3s/epoch - 3ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 3s - loss: 13.1478 - mse: 13.1478 - mae: 1.5305 - val_loss: 8.3536 - val_mse: 8.3536 - val_mae: 1.5198 - lr: 0.0010 - 3s/epoch - 3ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 3s - loss: 13.2573 - mse: 13.2573 - mae: 1.5366 - val_loss: 8.6251 - val_mse: 8.6251 - val_mae: 1.4910 - lr: 0.0010 - 3s/epoch - 3ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 3s - loss: 13.1730 - mse: 13.1730 - mae: 1.5302 - val_loss: 8.3609 - val_mse: 8.3609 - val_mae: 1.4886 - lr: 0.0010 - 3s/epoch - 3ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 3s - loss: 13.1527 - mse: 13.1527 - mae: 1.5271 - val_loss: 8.4574 - val_mse: 8.4574 - val_mae: 1.4998 - lr: 0.0010 - 3s/epoch - 3ms/step\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-10-11 11:02:15,493]\u001b[0m Finished trial#10 resulted in value: 12.604. Current best value is 9.334 with parameters: {'activation': 'relu', 'num_hidden_layer': 4, 'i': 9, 'learning_rate': 0.00023071835273936203}.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Score for fold 5: loss of 8.457393646240234\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 11s - loss: 14.3310 - mse: 14.3310 - mae: 1.6199 - val_loss: 11.8930 - val_mse: 11.8930 - val_mae: 1.6624 - lr: 0.0043 - 11s/epoch - 11ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 10s - loss: 13.3176 - mse: 13.3176 - mae: 1.5404 - val_loss: 11.1453 - val_mse: 11.1453 - val_mae: 1.5167 - lr: 0.0043 - 10s/epoch - 10ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 11s - loss: 12.7411 - mse: 12.7411 - mae: 1.5174 - val_loss: 10.9776 - val_mse: 10.9776 - val_mae: 1.6037 - lr: 0.0043 - 11s/epoch - 11ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 10s - loss: 12.8132 - mse: 12.8132 - mae: 1.5185 - val_loss: 11.3446 - val_mse: 11.3446 - val_mae: 1.4965 - lr: 0.0043 - 10s/epoch - 10ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 11s - loss: 13.0166 - mse: 13.0166 - mae: 1.5383 - val_loss: 11.0961 - val_mse: 11.0961 - val_mae: 1.5838 - lr: 0.0043 - 11s/epoch - 11ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 11s - loss: 13.2428 - mse: 13.2428 - mae: 1.5930 - val_loss: 14.3507 - val_mse: 14.3507 - val_mae: 1.5605 - lr: 0.0043 - 11s/epoch - 11ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 11s - loss: 13.3462 - mse: 13.3462 - mae: 1.5745 - val_loss: 11.9125 - val_mse: 11.9125 - val_mae: 1.4914 - lr: 0.0043 - 11s/epoch - 11ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 11s - loss: 12.9534 - mse: 12.9534 - mae: 1.5476 - val_loss: 11.3819 - val_mse: 11.3819 - val_mae: 1.5931 - lr: 0.0043 - 11s/epoch - 11ms/step\n",
            "Score for fold 1: loss of 11.381928443908691\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 10s - loss: 10.7110 - mse: 10.7110 - mae: 1.5073 - val_loss: 19.0394 - val_mse: 19.0394 - val_mae: 1.4948 - lr: 0.0010 - 10s/epoch - 10ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 11s - loss: 10.6434 - mse: 10.6434 - mae: 1.5089 - val_loss: 18.9410 - val_mse: 18.9410 - val_mae: 1.5198 - lr: 0.0010 - 11s/epoch - 11ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 11s - loss: 10.6999 - mse: 10.6999 - mae: 1.5176 - val_loss: 18.8532 - val_mse: 18.8532 - val_mae: 1.5195 - lr: 0.0010 - 11s/epoch - 11ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 11s - loss: 10.7341 - mse: 10.7341 - mae: 1.5110 - val_loss: 19.1470 - val_mse: 19.1470 - val_mae: 1.5155 - lr: 0.0010 - 11s/epoch - 11ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 11s - loss: 10.7008 - mse: 10.7008 - mae: 1.5095 - val_loss: 18.8618 - val_mse: 18.8618 - val_mae: 1.5586 - lr: 0.0010 - 11s/epoch - 11ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 11s - loss: 10.7512 - mse: 10.7512 - mae: 1.5131 - val_loss: 18.9942 - val_mse: 18.9942 - val_mae: 1.5920 - lr: 0.0010 - 11s/epoch - 11ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 11s - loss: 10.6040 - mse: 10.6040 - mae: 1.5030 - val_loss: 18.9127 - val_mse: 18.9127 - val_mae: 1.5055 - lr: 0.0010 - 11s/epoch - 11ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 10s - loss: 10.6032 - mse: 10.6032 - mae: 1.5032 - val_loss: 19.0520 - val_mse: 19.0520 - val_mae: 1.5380 - lr: 0.0010 - 10s/epoch - 10ms/step\n",
            "Score for fold 2: loss of 19.05199432373047\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 11s - loss: 12.8698 - mse: 12.8698 - mae: 1.5363 - val_loss: 11.6398 - val_mse: 11.6398 - val_mae: 1.4884 - lr: 0.0010 - 11s/epoch - 11ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 11s - loss: 12.7496 - mse: 12.7496 - mae: 1.5433 - val_loss: 11.7171 - val_mse: 11.7171 - val_mae: 1.5318 - lr: 0.0010 - 11s/epoch - 11ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 11s - loss: 12.9732 - mse: 12.9732 - mae: 1.5608 - val_loss: 11.6210 - val_mse: 11.6210 - val_mae: 1.5858 - lr: 0.0010 - 11s/epoch - 11ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 11s - loss: 12.9753 - mse: 12.9753 - mae: 1.5531 - val_loss: 12.0635 - val_mse: 12.0635 - val_mae: 1.5285 - lr: 0.0010 - 11s/epoch - 11ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 11s - loss: 12.7024 - mse: 12.7024 - mae: 1.5493 - val_loss: 14.1542 - val_mse: 14.1542 - val_mae: 1.5814 - lr: 0.0010 - 11s/epoch - 11ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 11s - loss: 12.9333 - mse: 12.9333 - mae: 1.5486 - val_loss: 11.6465 - val_mse: 11.6465 - val_mae: 1.6231 - lr: 0.0010 - 11s/epoch - 11ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 11s - loss: 12.7817 - mse: 12.7817 - mae: 1.5624 - val_loss: 11.9619 - val_mse: 11.9619 - val_mae: 1.5817 - lr: 0.0010 - 11s/epoch - 11ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 11s - loss: 12.5746 - mse: 12.5746 - mae: 1.5697 - val_loss: 12.4066 - val_mse: 12.4066 - val_mae: 1.5638 - lr: 0.0010 - 11s/epoch - 11ms/step\n",
            "Score for fold 3: loss of 12.406646728515625\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 11s - loss: 13.0633 - mse: 13.0633 - mae: 1.5885 - val_loss: 10.5349 - val_mse: 10.5349 - val_mae: 1.5360 - lr: 0.0010 - 11s/epoch - 11ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 10s - loss: 12.9837 - mse: 12.9837 - mae: 1.5833 - val_loss: 10.7329 - val_mse: 10.7329 - val_mae: 1.5340 - lr: 0.0010 - 10s/epoch - 10ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 10s - loss: 12.9008 - mse: 12.9008 - mae: 1.5825 - val_loss: 10.9345 - val_mse: 10.9345 - val_mae: 1.5306 - lr: 0.0010 - 10s/epoch - 10ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 11s - loss: 12.8763 - mse: 12.8763 - mae: 1.5796 - val_loss: 10.6347 - val_mse: 10.6347 - val_mae: 1.5017 - lr: 0.0010 - 11s/epoch - 11ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 11s - loss: 12.5850 - mse: 12.5850 - mae: 1.5523 - val_loss: 10.4588 - val_mse: 10.4588 - val_mae: 1.5054 - lr: 0.0010 - 11s/epoch - 11ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 11s - loss: 12.5868 - mse: 12.5868 - mae: 1.5269 - val_loss: 10.3643 - val_mse: 10.3643 - val_mae: 1.4673 - lr: 0.0010 - 11s/epoch - 11ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 11s - loss: 12.3652 - mse: 12.3652 - mae: 1.5126 - val_loss: 10.7813 - val_mse: 10.7813 - val_mae: 1.4703 - lr: 0.0010 - 11s/epoch - 11ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 11s - loss: 12.3760 - mse: 12.3760 - mae: 1.5109 - val_loss: 10.3972 - val_mse: 10.3972 - val_mae: 1.4462 - lr: 0.0010 - 11s/epoch - 11ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 11s - loss: 12.4975 - mse: 12.4975 - mae: 1.4945 - val_loss: 10.2125 - val_mse: 10.2125 - val_mae: 1.4495 - lr: 0.0010 - 11s/epoch - 11ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 10s - loss: 12.2135 - mse: 12.2135 - mae: 1.4992 - val_loss: 10.4915 - val_mse: 10.4915 - val_mae: 1.4913 - lr: 0.0010 - 10s/epoch - 10ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 11s - loss: 12.3339 - mse: 12.3339 - mae: 1.4982 - val_loss: 10.4618 - val_mse: 10.4618 - val_mae: 1.4364 - lr: 0.0010 - 11s/epoch - 11ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 11s - loss: 12.4620 - mse: 12.4620 - mae: 1.4892 - val_loss: 10.1457 - val_mse: 10.1457 - val_mae: 1.4357 - lr: 0.0010 - 11s/epoch - 11ms/step\n",
            "Epoch 13/100\n",
            "1000/1000 - 11s - loss: 12.4490 - mse: 12.4490 - mae: 1.4950 - val_loss: 10.2024 - val_mse: 10.2024 - val_mae: 1.5016 - lr: 0.0010 - 11s/epoch - 11ms/step\n",
            "Epoch 14/100\n",
            "1000/1000 - 11s - loss: 12.2066 - mse: 12.2066 - mae: 1.4981 - val_loss: 10.3061 - val_mse: 10.3061 - val_mae: 1.5387 - lr: 0.0010 - 11s/epoch - 11ms/step\n",
            "Epoch 15/100\n",
            "1000/1000 - 11s - loss: 12.4034 - mse: 12.4034 - mae: 1.5072 - val_loss: 10.2728 - val_mse: 10.2728 - val_mae: 1.4801 - lr: 0.0010 - 11s/epoch - 11ms/step\n",
            "Epoch 16/100\n",
            "1000/1000 - 11s - loss: 12.4124 - mse: 12.4124 - mae: 1.5076 - val_loss: 10.4087 - val_mse: 10.4087 - val_mae: 1.5581 - lr: 0.0010 - 11s/epoch - 11ms/step\n",
            "Epoch 17/100\n",
            "1000/1000 - 11s - loss: 12.4149 - mse: 12.4149 - mae: 1.5259 - val_loss: 10.7867 - val_mse: 10.7867 - val_mae: 1.5594 - lr: 0.0010 - 11s/epoch - 11ms/step\n",
            "Score for fold 4: loss of 10.786697387695312\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 11s - loss: 12.9304 - mse: 12.9304 - mae: 1.5311 - val_loss: 8.7671 - val_mse: 8.7671 - val_mae: 1.5122 - lr: 0.0010 - 11s/epoch - 11ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 11s - loss: 12.8497 - mse: 12.8497 - mae: 1.5472 - val_loss: 9.0647 - val_mse: 9.0647 - val_mae: 1.5458 - lr: 0.0010 - 11s/epoch - 11ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 11s - loss: 12.9834 - mse: 12.9834 - mae: 1.5427 - val_loss: 9.2230 - val_mse: 9.2230 - val_mae: 1.6173 - lr: 0.0010 - 11s/epoch - 11ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 11s - loss: 13.4059 - mse: 13.4059 - mae: 1.5537 - val_loss: 9.0978 - val_mse: 9.0978 - val_mae: 1.5127 - lr: 0.0010 - 11s/epoch - 11ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 11s - loss: 12.7789 - mse: 12.7789 - mae: 1.5176 - val_loss: 9.1278 - val_mse: 9.1278 - val_mae: 1.5261 - lr: 0.0010 - 11s/epoch - 11ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 11s - loss: 12.5721 - mse: 12.5721 - mae: 1.5207 - val_loss: 8.9989 - val_mse: 8.9989 - val_mae: 1.5364 - lr: 0.0010 - 11s/epoch - 11ms/step\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-10-11 11:10:44,466]\u001b[0m Finished trial#11 resulted in value: 12.526. Current best value is 9.334 with parameters: {'activation': 'relu', 'num_hidden_layer': 4, 'i': 9, 'learning_rate': 0.00023071835273936203}.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Score for fold 5: loss of 8.99890422821045\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 7s - loss: 12.0408 - mse: 12.0408 - mae: 1.5348 - val_loss: 18.5255 - val_mse: 18.5255 - val_mae: 1.4972 - lr: 1.2814e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 6s - loss: 10.9316 - mse: 10.9316 - mae: 1.4750 - val_loss: 18.2397 - val_mse: 18.2397 - val_mae: 1.5347 - lr: 1.2814e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 6s - loss: 10.7623 - mse: 10.7623 - mae: 1.4576 - val_loss: 18.2457 - val_mse: 18.2457 - val_mae: 1.5051 - lr: 1.2814e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 6s - loss: 10.4801 - mse: 10.4801 - mae: 1.4496 - val_loss: 18.0966 - val_mse: 18.0966 - val_mae: 1.4894 - lr: 1.2814e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 6s - loss: 10.4694 - mse: 10.4694 - mae: 1.4373 - val_loss: 17.9811 - val_mse: 17.9811 - val_mae: 1.5320 - lr: 1.2814e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 6s - loss: 10.3388 - mse: 10.3388 - mae: 1.4320 - val_loss: 17.9258 - val_mse: 17.9258 - val_mae: 1.5256 - lr: 1.2814e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 6s - loss: 10.1274 - mse: 10.1274 - mae: 1.4208 - val_loss: 18.0767 - val_mse: 18.0767 - val_mae: 1.5718 - lr: 1.2814e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 6s - loss: 10.2657 - mse: 10.2657 - mae: 1.4201 - val_loss: 18.0044 - val_mse: 18.0044 - val_mae: 1.5029 - lr: 1.2814e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 7s - loss: 9.9814 - mse: 9.9814 - mae: 1.4088 - val_loss: 18.1618 - val_mse: 18.1618 - val_mae: 1.5052 - lr: 1.2814e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 6s - loss: 9.9196 - mse: 9.9196 - mae: 1.3978 - val_loss: 18.0213 - val_mse: 18.0213 - val_mae: 1.4721 - lr: 1.2814e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 6s - loss: 9.8786 - mse: 9.8786 - mae: 1.4003 - val_loss: 18.0226 - val_mse: 18.0226 - val_mae: 1.4805 - lr: 1.2814e-04 - 6s/epoch - 6ms/step\n",
            "Score for fold 1: loss of 18.022645950317383\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 6s - loss: 11.7474 - mse: 11.7474 - mae: 1.4166 - val_loss: 9.9163 - val_mse: 9.9163 - val_mae: 1.3991 - lr: 1.2814e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 6s - loss: 11.5077 - mse: 11.5077 - mae: 1.4060 - val_loss: 9.7599 - val_mse: 9.7599 - val_mae: 1.4169 - lr: 1.2814e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 6s - loss: 11.1930 - mse: 11.1930 - mae: 1.3917 - val_loss: 10.1165 - val_mse: 10.1165 - val_mae: 1.4106 - lr: 1.2814e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 6s - loss: 11.1862 - mse: 11.1862 - mae: 1.3836 - val_loss: 9.7625 - val_mse: 9.7625 - val_mae: 1.4379 - lr: 1.2814e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 6s - loss: 10.9756 - mse: 10.9756 - mae: 1.3759 - val_loss: 10.1429 - val_mse: 10.1429 - val_mae: 1.4194 - lr: 1.2814e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 6s - loss: 10.8305 - mse: 10.8305 - mae: 1.3640 - val_loss: 10.4767 - val_mse: 10.4767 - val_mae: 1.4168 - lr: 1.2814e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 6s - loss: 10.7550 - mse: 10.7550 - mae: 1.3551 - val_loss: 10.3069 - val_mse: 10.3069 - val_mae: 1.3697 - lr: 1.2814e-04 - 6s/epoch - 6ms/step\n",
            "Score for fold 2: loss of 10.306946754455566\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 6s - loss: 11.3821 - mse: 11.3821 - mae: 1.3736 - val_loss: 7.8347 - val_mse: 7.8347 - val_mae: 1.3549 - lr: 1.2814e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 6s - loss: 11.1574 - mse: 11.1574 - mae: 1.3655 - val_loss: 8.1379 - val_mse: 8.1379 - val_mae: 1.3413 - lr: 1.2814e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 6s - loss: 10.7408 - mse: 10.7408 - mae: 1.3435 - val_loss: 8.0124 - val_mse: 8.0124 - val_mae: 1.3875 - lr: 1.2814e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 6s - loss: 10.4321 - mse: 10.4321 - mae: 1.3324 - val_loss: 8.0781 - val_mse: 8.0781 - val_mae: 1.4060 - lr: 1.2814e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 6s - loss: 10.5706 - mse: 10.5706 - mae: 1.3228 - val_loss: 8.2400 - val_mse: 8.2400 - val_mae: 1.4008 - lr: 1.2814e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 6s - loss: 10.1549 - mse: 10.1549 - mae: 1.3096 - val_loss: 8.1195 - val_mse: 8.1195 - val_mae: 1.3959 - lr: 1.2814e-04 - 6s/epoch - 6ms/step\n",
            "Score for fold 3: loss of 8.119477272033691\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 6s - loss: 10.2833 - mse: 10.2833 - mae: 1.3335 - val_loss: 7.0706 - val_mse: 7.0706 - val_mae: 1.2616 - lr: 1.2814e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 6s - loss: 10.4651 - mse: 10.4651 - mae: 1.3297 - val_loss: 7.1344 - val_mse: 7.1344 - val_mae: 1.2096 - lr: 1.2814e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 6s - loss: 10.2732 - mse: 10.2732 - mae: 1.3146 - val_loss: 7.5896 - val_mse: 7.5896 - val_mae: 1.2354 - lr: 1.2814e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 6s - loss: 9.8009 - mse: 9.8009 - mae: 1.2932 - val_loss: 7.4514 - val_mse: 7.4514 - val_mae: 1.2531 - lr: 1.2814e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 7s - loss: 9.5525 - mse: 9.5525 - mae: 1.2778 - val_loss: 7.4722 - val_mse: 7.4722 - val_mae: 1.3765 - lr: 1.2814e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 6s - loss: 9.3796 - mse: 9.3796 - mae: 1.2610 - val_loss: 8.2424 - val_mse: 8.2424 - val_mae: 1.2734 - lr: 1.2814e-04 - 6s/epoch - 6ms/step\n",
            "Score for fold 4: loss of 8.242432594299316\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 6s - loss: 8.7255 - mse: 8.7255 - mae: 1.2795 - val_loss: 9.5897 - val_mse: 9.5897 - val_mae: 1.1944 - lr: 1.2814e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 6s - loss: 8.6695 - mse: 8.6695 - mae: 1.2680 - val_loss: 9.3713 - val_mse: 9.3713 - val_mae: 1.1998 - lr: 1.2814e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 6s - loss: 8.4892 - mse: 8.4892 - mae: 1.2525 - val_loss: 9.1140 - val_mse: 9.1140 - val_mae: 1.2969 - lr: 1.2814e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 6s - loss: 8.2918 - mse: 8.2918 - mae: 1.2327 - val_loss: 9.0836 - val_mse: 9.0836 - val_mae: 1.2601 - lr: 1.2814e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 6s - loss: 8.0050 - mse: 8.0050 - mae: 1.2177 - val_loss: 10.2160 - val_mse: 10.2160 - val_mae: 1.3056 - lr: 1.2814e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 6s - loss: 7.7879 - mse: 7.7879 - mae: 1.2062 - val_loss: 9.1540 - val_mse: 9.1540 - val_mae: 1.2729 - lr: 1.2814e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 6s - loss: 7.4406 - mse: 7.4406 - mae: 1.1870 - val_loss: 10.8383 - val_mse: 10.8383 - val_mae: 1.2869 - lr: 1.2814e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 6s - loss: 7.3267 - mse: 7.3267 - mae: 1.1779 - val_loss: 10.1224 - val_mse: 10.1224 - val_mae: 1.3068 - lr: 1.2814e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 6s - loss: 7.0065 - mse: 7.0065 - mae: 1.1608 - val_loss: 9.8957 - val_mse: 9.8957 - val_mae: 1.2658 - lr: 1.2814e-04 - 6s/epoch - 6ms/step\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-10-11 11:14:54,658]\u001b[0m Finished trial#12 resulted in value: 10.918. Current best value is 9.334 with parameters: {'activation': 'relu', 'num_hidden_layer': 4, 'i': 9, 'learning_rate': 0.00023071835273936203}.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Score for fold 5: loss of 9.895682334899902\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 11s - loss: 14.6980 - mse: 14.6980 - mae: 1.5903 - val_loss: 8.1824 - val_mse: 8.1824 - val_mae: 1.4880 - lr: 0.0025 - 11s/epoch - 11ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 11s - loss: 14.0773 - mse: 14.0773 - mae: 1.5266 - val_loss: 9.4640 - val_mse: 9.4640 - val_mae: 1.5342 - lr: 0.0025 - 11s/epoch - 11ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 11s - loss: 13.8432 - mse: 13.8432 - mae: 1.5200 - val_loss: 8.3326 - val_mse: 8.3326 - val_mae: 1.6126 - lr: 0.0025 - 11s/epoch - 11ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 11s - loss: 14.1218 - mse: 14.1218 - mae: 1.5056 - val_loss: 8.1168 - val_mse: 8.1168 - val_mae: 1.4234 - lr: 0.0025 - 11s/epoch - 11ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 11s - loss: 13.7025 - mse: 13.7025 - mae: 1.4877 - val_loss: 7.7057 - val_mse: 7.7057 - val_mae: 1.4654 - lr: 0.0025 - 11s/epoch - 11ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 11s - loss: 13.6466 - mse: 13.6466 - mae: 1.4791 - val_loss: 7.7626 - val_mse: 7.7626 - val_mae: 1.5162 - lr: 0.0025 - 11s/epoch - 11ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 11s - loss: 13.6395 - mse: 13.6395 - mae: 1.4821 - val_loss: 7.6513 - val_mse: 7.6513 - val_mae: 1.4232 - lr: 0.0025 - 11s/epoch - 11ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 11s - loss: 13.1137 - mse: 13.1137 - mae: 1.4674 - val_loss: 16.5690 - val_mse: 16.5690 - val_mae: 1.4725 - lr: 0.0025 - 11s/epoch - 11ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 11s - loss: 13.2584 - mse: 13.2584 - mae: 1.4611 - val_loss: 7.3971 - val_mse: 7.3971 - val_mae: 1.4350 - lr: 0.0025 - 11s/epoch - 11ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 11s - loss: 12.9716 - mse: 12.9716 - mae: 1.4509 - val_loss: 7.5960 - val_mse: 7.5960 - val_mae: 1.4727 - lr: 0.0025 - 11s/epoch - 11ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 11s - loss: 13.0943 - mse: 13.0943 - mae: 1.4553 - val_loss: 7.4226 - val_mse: 7.4226 - val_mae: 1.4564 - lr: 0.0025 - 11s/epoch - 11ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 11s - loss: 12.7810 - mse: 12.7810 - mae: 1.4446 - val_loss: 7.8129 - val_mse: 7.8129 - val_mae: 1.4082 - lr: 0.0025 - 11s/epoch - 11ms/step\n",
            "Epoch 13/100\n",
            "1000/1000 - 11s - loss: 12.6427 - mse: 12.6427 - mae: 1.4426 - val_loss: 8.2867 - val_mse: 8.2867 - val_mae: 1.4277 - lr: 0.0025 - 11s/epoch - 11ms/step\n",
            "Epoch 14/100\n",
            "1000/1000 - 11s - loss: 12.8303 - mse: 12.8303 - mae: 1.4390 - val_loss: 7.7474 - val_mse: 7.7474 - val_mae: 1.4631 - lr: 0.0025 - 11s/epoch - 11ms/step\n",
            "Score for fold 1: loss of 7.747377872467041\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 10s - loss: 10.1379 - mse: 10.1379 - mae: 1.4057 - val_loss: 15.4967 - val_mse: 15.4967 - val_mae: 1.4189 - lr: 0.0010 - 10s/epoch - 10ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 11s - loss: 9.7730 - mse: 9.7730 - mae: 1.3876 - val_loss: 15.5670 - val_mse: 15.5670 - val_mae: 1.4892 - lr: 0.0010 - 11s/epoch - 11ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 11s - loss: 9.6010 - mse: 9.6010 - mae: 1.3756 - val_loss: 15.3553 - val_mse: 15.3553 - val_mae: 1.4450 - lr: 0.0010 - 11s/epoch - 11ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 11s - loss: 9.3350 - mse: 9.3350 - mae: 1.3643 - val_loss: 15.8596 - val_mse: 15.8596 - val_mae: 1.4571 - lr: 0.0010 - 11s/epoch - 11ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 11s - loss: 9.2451 - mse: 9.2451 - mae: 1.3539 - val_loss: 15.0934 - val_mse: 15.0934 - val_mae: 1.4605 - lr: 0.0010 - 11s/epoch - 11ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 11s - loss: 8.8909 - mse: 8.8909 - mae: 1.3395 - val_loss: 16.1134 - val_mse: 16.1134 - val_mae: 1.4351 - lr: 0.0010 - 11s/epoch - 11ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 11s - loss: 10.1918 - mse: 10.1918 - mae: 1.3356 - val_loss: 15.2270 - val_mse: 15.2270 - val_mae: 1.4623 - lr: 0.0010 - 11s/epoch - 11ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 11s - loss: 8.5634 - mse: 8.5634 - mae: 1.3186 - val_loss: 15.3433 - val_mse: 15.3433 - val_mae: 1.4746 - lr: 0.0010 - 11s/epoch - 11ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 11s - loss: 8.4405 - mse: 8.4405 - mae: 1.3102 - val_loss: 15.5138 - val_mse: 15.5138 - val_mae: 1.5038 - lr: 0.0010 - 11s/epoch - 11ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 11s - loss: 8.3787 - mse: 8.3787 - mae: 1.3004 - val_loss: 15.5772 - val_mse: 15.5772 - val_mae: 1.5345 - lr: 0.0010 - 11s/epoch - 11ms/step\n",
            "Score for fold 2: loss of 15.577208518981934\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 11s - loss: 10.5507 - mse: 10.5507 - mae: 1.3428 - val_loss: 5.8255 - val_mse: 5.8255 - val_mae: 1.3003 - lr: 0.0010 - 11s/epoch - 11ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 11s - loss: 10.5062 - mse: 10.5062 - mae: 1.3289 - val_loss: 5.8018 - val_mse: 5.8018 - val_mae: 1.3011 - lr: 0.0010 - 11s/epoch - 11ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 11s - loss: 10.0575 - mse: 10.0575 - mae: 1.3143 - val_loss: 6.0498 - val_mse: 6.0498 - val_mae: 1.3181 - lr: 0.0010 - 11s/epoch - 11ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 11s - loss: 9.9323 - mse: 9.9323 - mae: 1.3020 - val_loss: 6.1795 - val_mse: 6.1795 - val_mae: 1.3403 - lr: 0.0010 - 11s/epoch - 11ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 11s - loss: 9.6546 - mse: 9.6546 - mae: 1.2901 - val_loss: 6.5281 - val_mse: 6.5281 - val_mae: 1.3195 - lr: 0.0010 - 11s/epoch - 11ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 11s - loss: 9.4654 - mse: 9.4654 - mae: 1.2802 - val_loss: 6.1787 - val_mse: 6.1787 - val_mae: 1.3747 - lr: 0.0010 - 11s/epoch - 11ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 11s - loss: 9.3898 - mse: 9.3898 - mae: 1.2710 - val_loss: 6.5776 - val_mse: 6.5776 - val_mae: 1.3127 - lr: 0.0010 - 11s/epoch - 11ms/step\n",
            "Score for fold 3: loss of 6.577642440795898\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 11s - loss: 9.3361 - mse: 9.3361 - mae: 1.3003 - val_loss: 7.7743 - val_mse: 7.7743 - val_mae: 1.2392 - lr: 0.0010 - 11s/epoch - 11ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 11s - loss: 8.6649 - mse: 8.6649 - mae: 1.2791 - val_loss: 8.1468 - val_mse: 8.1468 - val_mae: 1.2583 - lr: 0.0010 - 11s/epoch - 11ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 11s - loss: 8.7048 - mse: 8.7048 - mae: 1.2705 - val_loss: 7.9778 - val_mse: 7.9778 - val_mae: 1.3057 - lr: 0.0010 - 11s/epoch - 11ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 11s - loss: 8.2687 - mse: 8.2687 - mae: 1.2571 - val_loss: 8.1737 - val_mse: 8.1737 - val_mae: 1.3070 - lr: 0.0010 - 11s/epoch - 11ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 11s - loss: 8.0919 - mse: 8.0919 - mae: 1.2437 - val_loss: 8.3162 - val_mse: 8.3162 - val_mae: 1.2753 - lr: 0.0010 - 11s/epoch - 11ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 11s - loss: 8.1096 - mse: 8.1096 - mae: 1.2399 - val_loss: 8.4008 - val_mse: 8.4008 - val_mae: 1.2516 - lr: 0.0010 - 11s/epoch - 11ms/step\n",
            "Score for fold 4: loss of 8.400847434997559\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 11s - loss: 7.4458 - mse: 7.4458 - mae: 1.2541 - val_loss: 11.0565 - val_mse: 11.0565 - val_mae: 1.2191 - lr: 0.0010 - 11s/epoch - 11ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 11s - loss: 7.4425 - mse: 7.4425 - mae: 1.2429 - val_loss: 11.8056 - val_mse: 11.8056 - val_mae: 1.2429 - lr: 0.0010 - 11s/epoch - 11ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 11s - loss: 7.2097 - mse: 7.2097 - mae: 1.2333 - val_loss: 10.3700 - val_mse: 10.3700 - val_mae: 1.2336 - lr: 0.0010 - 11s/epoch - 11ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 11s - loss: 7.0639 - mse: 7.0639 - mae: 1.2195 - val_loss: 11.9078 - val_mse: 11.9078 - val_mae: 1.2552 - lr: 0.0010 - 11s/epoch - 11ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 11s - loss: 7.1068 - mse: 7.1068 - mae: 1.2136 - val_loss: 13.1437 - val_mse: 13.1437 - val_mae: 1.2472 - lr: 0.0010 - 11s/epoch - 11ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 11s - loss: 6.8157 - mse: 6.8157 - mae: 1.2040 - val_loss: 12.2312 - val_mse: 12.2312 - val_mae: 1.2652 - lr: 0.0010 - 11s/epoch - 11ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 11s - loss: 6.8407 - mse: 6.8407 - mae: 1.2012 - val_loss: 12.6253 - val_mse: 12.6253 - val_mae: 1.2926 - lr: 0.0010 - 11s/epoch - 11ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 11s - loss: 6.9085 - mse: 6.9085 - mae: 1.1944 - val_loss: 12.6328 - val_mse: 12.6328 - val_mae: 1.2692 - lr: 0.0010 - 11s/epoch - 11ms/step\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-10-11 11:23:06,768]\u001b[0m Finished trial#13 resulted in value: 10.187999999999999. Current best value is 9.334 with parameters: {'activation': 'relu', 'num_hidden_layer': 4, 'i': 9, 'learning_rate': 0.00023071835273936203}.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Score for fold 5: loss of 12.632806777954102\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 5s - loss: 11.1544 - mse: 11.1544 - mae: 1.5377 - val_loss: 23.3651 - val_mse: 23.3651 - val_mae: 1.5674 - lr: 1.0200e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 4s - loss: 10.1886 - mse: 10.1886 - mae: 1.4902 - val_loss: 22.9152 - val_mse: 22.9152 - val_mae: 1.5450 - lr: 1.0200e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 4s - loss: 9.8533 - mse: 9.8533 - mae: 1.4676 - val_loss: 22.4743 - val_mse: 22.4743 - val_mae: 1.5241 - lr: 1.0200e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 4s - loss: 9.5885 - mse: 9.5885 - mae: 1.4581 - val_loss: 22.3426 - val_mse: 22.3426 - val_mae: 1.5637 - lr: 1.0200e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 4s - loss: 9.5498 - mse: 9.5498 - mae: 1.4481 - val_loss: 22.2381 - val_mse: 22.2381 - val_mae: 1.5570 - lr: 1.0200e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 4s - loss: 9.4949 - mse: 9.4949 - mae: 1.4411 - val_loss: 22.0481 - val_mse: 22.0481 - val_mae: 1.5957 - lr: 1.0200e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 5s - loss: 9.3664 - mse: 9.3664 - mae: 1.4365 - val_loss: 23.0949 - val_mse: 23.0949 - val_mae: 1.5670 - lr: 1.0200e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 4s - loss: 9.4130 - mse: 9.4130 - mae: 1.4351 - val_loss: 21.9913 - val_mse: 21.9913 - val_mae: 1.4883 - lr: 1.0200e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 4s - loss: 9.3099 - mse: 9.3099 - mae: 1.4317 - val_loss: 22.1481 - val_mse: 22.1481 - val_mae: 1.5224 - lr: 1.0200e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 4s - loss: 9.2523 - mse: 9.2523 - mae: 1.4251 - val_loss: 21.8787 - val_mse: 21.8787 - val_mae: 1.4817 - lr: 1.0200e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 4s - loss: 9.1757 - mse: 9.1757 - mae: 1.4175 - val_loss: 22.2829 - val_mse: 22.2829 - val_mae: 1.5082 - lr: 1.0200e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 4s - loss: 9.1874 - mse: 9.1874 - mae: 1.4195 - val_loss: 21.9187 - val_mse: 21.9187 - val_mae: 1.4568 - lr: 1.0200e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 13/100\n",
            "1000/1000 - 4s - loss: 9.1610 - mse: 9.1610 - mae: 1.4159 - val_loss: 21.7805 - val_mse: 21.7805 - val_mae: 1.5080 - lr: 1.0200e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 14/100\n",
            "1000/1000 - 4s - loss: 9.1395 - mse: 9.1395 - mae: 1.4116 - val_loss: 21.9025 - val_mse: 21.9025 - val_mae: 1.4787 - lr: 1.0200e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 15/100\n",
            "1000/1000 - 4s - loss: 9.0699 - mse: 9.0699 - mae: 1.4086 - val_loss: 21.8924 - val_mse: 21.8924 - val_mae: 1.5310 - lr: 1.0200e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 16/100\n",
            "1000/1000 - 4s - loss: 9.0350 - mse: 9.0350 - mae: 1.4084 - val_loss: 21.8745 - val_mse: 21.8745 - val_mae: 1.5189 - lr: 1.0200e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 17/100\n",
            "1000/1000 - 4s - loss: 9.0344 - mse: 9.0344 - mae: 1.4019 - val_loss: 21.8187 - val_mse: 21.8187 - val_mae: 1.5014 - lr: 1.0200e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 18/100\n",
            "1000/1000 - 4s - loss: 8.9928 - mse: 8.9928 - mae: 1.4010 - val_loss: 21.9036 - val_mse: 21.9036 - val_mae: 1.4646 - lr: 1.0200e-04 - 4s/epoch - 4ms/step\n",
            "Score for fold 1: loss of 21.903587341308594\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 4s - loss: 12.0133 - mse: 12.0133 - mae: 1.4220 - val_loss: 9.6547 - val_mse: 9.6547 - val_mae: 1.4080 - lr: 1.0200e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 4s - loss: 11.9259 - mse: 11.9259 - mae: 1.4187 - val_loss: 9.6266 - val_mse: 9.6266 - val_mae: 1.4112 - lr: 1.0200e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 4s - loss: 11.8040 - mse: 11.8040 - mae: 1.4107 - val_loss: 9.7043 - val_mse: 9.7043 - val_mae: 1.4062 - lr: 1.0200e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 4s - loss: 11.7979 - mse: 11.7979 - mae: 1.4117 - val_loss: 10.0097 - val_mse: 10.0097 - val_mae: 1.4751 - lr: 1.0200e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 4s - loss: 11.7449 - mse: 11.7449 - mae: 1.4110 - val_loss: 9.7025 - val_mse: 9.7025 - val_mae: 1.4027 - lr: 1.0200e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 4s - loss: 11.7299 - mse: 11.7299 - mae: 1.4054 - val_loss: 9.7199 - val_mse: 9.7199 - val_mae: 1.4005 - lr: 1.0200e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 4s - loss: 11.6909 - mse: 11.6909 - mae: 1.4018 - val_loss: 9.8285 - val_mse: 9.8285 - val_mae: 1.3743 - lr: 1.0200e-04 - 4s/epoch - 4ms/step\n",
            "Score for fold 2: loss of 9.828527450561523\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 4s - loss: 11.5174 - mse: 11.5174 - mae: 1.4001 - val_loss: 10.1384 - val_mse: 10.1384 - val_mae: 1.3970 - lr: 1.0200e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 4s - loss: 11.5615 - mse: 11.5615 - mae: 1.4019 - val_loss: 10.1243 - val_mse: 10.1243 - val_mae: 1.3834 - lr: 1.0200e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 4s - loss: 11.4427 - mse: 11.4427 - mae: 1.3983 - val_loss: 10.1208 - val_mse: 10.1208 - val_mae: 1.4340 - lr: 1.0200e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 4s - loss: 11.4151 - mse: 11.4151 - mae: 1.3954 - val_loss: 10.1987 - val_mse: 10.1987 - val_mae: 1.4036 - lr: 1.0200e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 4s - loss: 11.3033 - mse: 11.3033 - mae: 1.3887 - val_loss: 10.2600 - val_mse: 10.2600 - val_mae: 1.5171 - lr: 1.0200e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 4s - loss: 11.2162 - mse: 11.2162 - mae: 1.3849 - val_loss: 10.6615 - val_mse: 10.6615 - val_mae: 1.4471 - lr: 1.0200e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 4s - loss: 11.2311 - mse: 11.2311 - mae: 1.3836 - val_loss: 10.3902 - val_mse: 10.3902 - val_mae: 1.3716 - lr: 1.0200e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 4s - loss: 11.2490 - mse: 11.2490 - mae: 1.3786 - val_loss: 10.2742 - val_mse: 10.2742 - val_mae: 1.4691 - lr: 1.0200e-04 - 4s/epoch - 4ms/step\n",
            "Score for fold 3: loss of 10.274169921875\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 4s - loss: 11.7127 - mse: 11.7127 - mae: 1.3990 - val_loss: 8.1246 - val_mse: 8.1246 - val_mae: 1.3593 - lr: 1.0200e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 4s - loss: 11.6836 - mse: 11.6836 - mae: 1.3948 - val_loss: 8.2000 - val_mse: 8.2000 - val_mae: 1.3547 - lr: 1.0200e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 4s - loss: 11.5495 - mse: 11.5495 - mae: 1.3918 - val_loss: 8.1422 - val_mse: 8.1422 - val_mae: 1.3518 - lr: 1.0200e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 4s - loss: 11.5786 - mse: 11.5786 - mae: 1.3848 - val_loss: 8.3721 - val_mse: 8.3721 - val_mae: 1.3444 - lr: 1.0200e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 4s - loss: 11.5207 - mse: 11.5207 - mae: 1.3823 - val_loss: 8.1309 - val_mse: 8.1309 - val_mae: 1.3570 - lr: 1.0200e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 4s - loss: 11.4105 - mse: 11.4105 - mae: 1.3778 - val_loss: 8.5782 - val_mse: 8.5782 - val_mae: 1.4238 - lr: 1.0200e-04 - 4s/epoch - 4ms/step\n",
            "Score for fold 4: loss of 8.578200340270996\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 4s - loss: 11.8075 - mse: 11.8075 - mae: 1.3869 - val_loss: 6.4008 - val_mse: 6.4008 - val_mae: 1.3497 - lr: 1.0200e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 4s - loss: 11.8792 - mse: 11.8792 - mae: 1.3812 - val_loss: 6.5191 - val_mse: 6.5191 - val_mae: 1.3544 - lr: 1.0200e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 4s - loss: 11.6641 - mse: 11.6641 - mae: 1.3765 - val_loss: 6.6033 - val_mse: 6.6033 - val_mae: 1.4230 - lr: 1.0200e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 4s - loss: 11.6918 - mse: 11.6918 - mae: 1.3756 - val_loss: 6.6542 - val_mse: 6.6542 - val_mae: 1.3564 - lr: 1.0200e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 4s - loss: 11.7278 - mse: 11.7278 - mae: 1.3676 - val_loss: 6.6801 - val_mse: 6.6801 - val_mae: 1.4069 - lr: 1.0200e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 4s - loss: 11.6139 - mse: 11.6139 - mae: 1.3682 - val_loss: 6.6710 - val_mse: 6.6710 - val_mae: 1.3149 - lr: 1.0200e-04 - 4s/epoch - 4ms/step\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-10-11 11:26:27,521]\u001b[0m Finished trial#14 resulted in value: 11.45. Current best value is 9.334 with parameters: {'activation': 'relu', 'num_hidden_layer': 4, 'i': 9, 'learning_rate': 0.00023071835273936203}.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Score for fold 5: loss of 6.6710028648376465\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 4s - loss: 14.9310 - mse: 14.9310 - mae: 1.6120 - val_loss: 9.0434 - val_mse: 9.0434 - val_mae: 1.6112 - lr: 0.0080 - 4s/epoch - 4ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 3s - loss: 13.8695 - mse: 13.8695 - mae: 1.5521 - val_loss: 8.7537 - val_mse: 8.7537 - val_mae: 1.4362 - lr: 0.0080 - 3s/epoch - 3ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 3s - loss: 13.4411 - mse: 13.4411 - mae: 1.5341 - val_loss: 8.6773 - val_mse: 8.6773 - val_mae: 1.4722 - lr: 0.0080 - 3s/epoch - 3ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 3s - loss: 13.5000 - mse: 13.5000 - mae: 1.5319 - val_loss: 8.6048 - val_mse: 8.6048 - val_mae: 1.4536 - lr: 0.0080 - 3s/epoch - 3ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 3s - loss: 13.5973 - mse: 13.5973 - mae: 1.5237 - val_loss: 8.6827 - val_mse: 8.6827 - val_mae: 1.4751 - lr: 0.0080 - 3s/epoch - 3ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 3s - loss: 14.1619 - mse: 14.1619 - mae: 1.5781 - val_loss: 8.8734 - val_mse: 8.8734 - val_mae: 1.5305 - lr: 0.0080 - 3s/epoch - 3ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 3s - loss: 13.8072 - mse: 13.8072 - mae: 1.5946 - val_loss: 9.3746 - val_mse: 9.3746 - val_mae: 1.6532 - lr: 0.0080 - 3s/epoch - 3ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 3s - loss: 14.0038 - mse: 14.0038 - mae: 1.6082 - val_loss: 10.3034 - val_mse: 10.3034 - val_mae: 1.5851 - lr: 0.0080 - 3s/epoch - 3ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 3s - loss: 14.0647 - mse: 14.0647 - mae: 1.6270 - val_loss: 9.2638 - val_mse: 9.2638 - val_mae: 1.6215 - lr: 0.0080 - 3s/epoch - 3ms/step\n",
            "Score for fold 1: loss of 9.263782501220703\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 4s - loss: 10.3951 - mse: 10.3951 - mae: 1.5497 - val_loss: 20.9952 - val_mse: 20.9952 - val_mae: 1.5438 - lr: 0.0016 - 4s/epoch - 4ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 4s - loss: 9.9177 - mse: 9.9177 - mae: 1.4769 - val_loss: 20.8258 - val_mse: 20.8258 - val_mae: 1.4970 - lr: 0.0016 - 4s/epoch - 4ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 4s - loss: 9.7017 - mse: 9.7017 - mae: 1.4671 - val_loss: 20.9072 - val_mse: 20.9072 - val_mae: 1.4788 - lr: 0.0016 - 4s/epoch - 4ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 4s - loss: 9.8630 - mse: 9.8630 - mae: 1.4721 - val_loss: 21.0388 - val_mse: 21.0388 - val_mae: 1.4834 - lr: 0.0016 - 4s/epoch - 4ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 4s - loss: 9.8054 - mse: 9.8054 - mae: 1.4747 - val_loss: 21.1029 - val_mse: 21.1029 - val_mae: 1.5622 - lr: 0.0016 - 4s/epoch - 4ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 4s - loss: 9.7767 - mse: 9.7767 - mae: 1.4831 - val_loss: 21.0018 - val_mse: 21.0018 - val_mae: 1.5295 - lr: 0.0016 - 4s/epoch - 4ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 4s - loss: 9.7891 - mse: 9.7891 - mae: 1.4847 - val_loss: 21.1389 - val_mse: 21.1389 - val_mae: 1.5636 - lr: 0.0016 - 4s/epoch - 4ms/step\n",
            "Score for fold 2: loss of 21.13888931274414\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 3s - loss: 12.1062 - mse: 12.1062 - mae: 1.5289 - val_loss: 12.1050 - val_mse: 12.1050 - val_mae: 1.5235 - lr: 0.0010 - 3s/epoch - 3ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 3s - loss: 12.1276 - mse: 12.1276 - mae: 1.5321 - val_loss: 11.6846 - val_mse: 11.6846 - val_mae: 1.5199 - lr: 0.0010 - 3s/epoch - 3ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 4s - loss: 11.9712 - mse: 11.9712 - mae: 1.5251 - val_loss: 11.9352 - val_mse: 11.9352 - val_mae: 1.5504 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 3s - loss: 11.9981 - mse: 11.9981 - mae: 1.5355 - val_loss: 12.0687 - val_mse: 12.0687 - val_mae: 1.5600 - lr: 0.0010 - 3s/epoch - 3ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 3s - loss: 12.0006 - mse: 12.0006 - mae: 1.5336 - val_loss: 11.5836 - val_mse: 11.5836 - val_mae: 1.5886 - lr: 0.0010 - 3s/epoch - 3ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 4s - loss: 11.9305 - mse: 11.9305 - mae: 1.5330 - val_loss: 12.1143 - val_mse: 12.1143 - val_mae: 1.6179 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 3s - loss: 11.8984 - mse: 11.8984 - mae: 1.5233 - val_loss: 12.1688 - val_mse: 12.1688 - val_mae: 1.6039 - lr: 0.0010 - 3s/epoch - 3ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 3s - loss: 11.7673 - mse: 11.7673 - mae: 1.5114 - val_loss: 11.4534 - val_mse: 11.4534 - val_mae: 1.5661 - lr: 0.0010 - 3s/epoch - 3ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 3s - loss: 11.7626 - mse: 11.7626 - mae: 1.5125 - val_loss: 11.7499 - val_mse: 11.7499 - val_mae: 1.5597 - lr: 0.0010 - 3s/epoch - 3ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 3s - loss: 11.5745 - mse: 11.5745 - mae: 1.5023 - val_loss: 11.5788 - val_mse: 11.5788 - val_mae: 1.5139 - lr: 0.0010 - 3s/epoch - 3ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 3s - loss: 11.5855 - mse: 11.5855 - mae: 1.4979 - val_loss: 11.5650 - val_mse: 11.5650 - val_mae: 1.5639 - lr: 0.0010 - 3s/epoch - 3ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 3s - loss: 11.5672 - mse: 11.5672 - mae: 1.4970 - val_loss: 11.3589 - val_mse: 11.3589 - val_mae: 1.5422 - lr: 0.0010 - 3s/epoch - 3ms/step\n",
            "Epoch 13/100\n",
            "1000/1000 - 3s - loss: 11.4392 - mse: 11.4392 - mae: 1.4843 - val_loss: 11.1437 - val_mse: 11.1437 - val_mae: 1.5815 - lr: 0.0010 - 3s/epoch - 3ms/step\n",
            "Epoch 14/100\n",
            "1000/1000 - 4s - loss: 11.3486 - mse: 11.3486 - mae: 1.4679 - val_loss: 11.6989 - val_mse: 11.6989 - val_mae: 1.5041 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 15/100\n",
            "1000/1000 - 4s - loss: 11.2269 - mse: 11.2269 - mae: 1.4469 - val_loss: 11.5413 - val_mse: 11.5413 - val_mae: 1.5183 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 16/100\n",
            "1000/1000 - 3s - loss: 11.0463 - mse: 11.0463 - mae: 1.4354 - val_loss: 10.9270 - val_mse: 10.9270 - val_mae: 1.5015 - lr: 0.0010 - 3s/epoch - 3ms/step\n",
            "Epoch 17/100\n",
            "1000/1000 - 4s - loss: 10.9008 - mse: 10.9008 - mae: 1.4231 - val_loss: 11.5019 - val_mse: 11.5019 - val_mae: 1.4837 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 18/100\n",
            "1000/1000 - 4s - loss: 10.8565 - mse: 10.8565 - mae: 1.4229 - val_loss: 11.2476 - val_mse: 11.2476 - val_mae: 1.4807 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 19/100\n",
            "1000/1000 - 3s - loss: 10.7293 - mse: 10.7293 - mae: 1.4085 - val_loss: 11.3726 - val_mse: 11.3726 - val_mae: 1.4824 - lr: 0.0010 - 3s/epoch - 3ms/step\n",
            "Epoch 20/100\n",
            "1000/1000 - 3s - loss: 10.6453 - mse: 10.6453 - mae: 1.4036 - val_loss: 11.5324 - val_mse: 11.5324 - val_mae: 1.5062 - lr: 0.0010 - 3s/epoch - 3ms/step\n",
            "Epoch 21/100\n",
            "1000/1000 - 4s - loss: 10.6336 - mse: 10.6336 - mae: 1.4021 - val_loss: 11.5292 - val_mse: 11.5292 - val_mae: 1.4840 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Score for fold 3: loss of 11.529210090637207\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 3s - loss: 11.4646 - mse: 11.4646 - mae: 1.4202 - val_loss: 7.5706 - val_mse: 7.5706 - val_mae: 1.3774 - lr: 0.0010 - 3s/epoch - 3ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 3s - loss: 11.2143 - mse: 11.2143 - mae: 1.4104 - val_loss: 7.9320 - val_mse: 7.9320 - val_mae: 1.4071 - lr: 0.0010 - 3s/epoch - 3ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 3s - loss: 11.1697 - mse: 11.1697 - mae: 1.4109 - val_loss: 7.7131 - val_mse: 7.7131 - val_mae: 1.4169 - lr: 0.0010 - 3s/epoch - 3ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 3s - loss: 11.2045 - mse: 11.2045 - mae: 1.4052 - val_loss: 7.8427 - val_mse: 7.8427 - val_mae: 1.3915 - lr: 0.0010 - 3s/epoch - 3ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 3s - loss: 11.0592 - mse: 11.0592 - mae: 1.3989 - val_loss: 8.9976 - val_mse: 8.9976 - val_mae: 1.4238 - lr: 0.0010 - 3s/epoch - 3ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 3s - loss: 11.1097 - mse: 11.1097 - mae: 1.3937 - val_loss: 8.4684 - val_mse: 8.4684 - val_mae: 1.4033 - lr: 0.0010 - 3s/epoch - 3ms/step\n",
            "Score for fold 4: loss of 8.468358993530273\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 3s - loss: 10.9559 - mse: 10.9559 - mae: 1.3991 - val_loss: 8.1553 - val_mse: 8.1553 - val_mae: 1.3441 - lr: 0.0010 - 3s/epoch - 3ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 3s - loss: 10.8190 - mse: 10.8190 - mae: 1.4051 - val_loss: 8.6126 - val_mse: 8.6126 - val_mae: 1.3576 - lr: 0.0010 - 3s/epoch - 3ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 3s - loss: 10.9485 - mse: 10.9485 - mae: 1.3976 - val_loss: 8.3692 - val_mse: 8.3692 - val_mae: 1.3604 - lr: 0.0010 - 3s/epoch - 3ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 3s - loss: 10.7955 - mse: 10.7955 - mae: 1.3928 - val_loss: 8.1591 - val_mse: 8.1591 - val_mae: 1.3540 - lr: 0.0010 - 3s/epoch - 3ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 3s - loss: 10.5868 - mse: 10.5868 - mae: 1.3835 - val_loss: 8.3808 - val_mse: 8.3808 - val_mae: 1.4217 - lr: 0.0010 - 3s/epoch - 3ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 3s - loss: 10.5527 - mse: 10.5527 - mae: 1.3813 - val_loss: 8.3083 - val_mse: 8.3083 - val_mae: 1.4244 - lr: 0.0010 - 3s/epoch - 3ms/step\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-10-11 11:29:23,737]\u001b[0m Finished trial#15 resulted in value: 11.742. Current best value is 9.334 with parameters: {'activation': 'relu', 'num_hidden_layer': 4, 'i': 9, 'learning_rate': 0.00023071835273936203}.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Score for fold 5: loss of 8.308334350585938\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 3s - loss: 13.6614 - mse: 13.6614 - mae: 1.5920 - val_loss: 18.0379 - val_mse: 18.0379 - val_mae: 1.5656 - lr: 0.0023 - 3s/epoch - 3ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 12.8729 - mse: 12.8729 - mae: 1.5370 - val_loss: 17.6213 - val_mse: 17.6213 - val_mae: 1.5172 - lr: 0.0023 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 12.5342 - mse: 12.5342 - mae: 1.5181 - val_loss: 17.3823 - val_mse: 17.3823 - val_mae: 1.4965 - lr: 0.0023 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 12.4240 - mse: 12.4240 - mae: 1.5053 - val_loss: 17.1607 - val_mse: 17.1607 - val_mae: 1.5979 - lr: 0.0023 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 12.2665 - mse: 12.2665 - mae: 1.5022 - val_loss: 17.0052 - val_mse: 17.0052 - val_mae: 1.5365 - lr: 0.0023 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 12.0836 - mse: 12.0836 - mae: 1.4898 - val_loss: 16.9842 - val_mse: 16.9842 - val_mae: 1.5157 - lr: 0.0023 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 12.0517 - mse: 12.0517 - mae: 1.4887 - val_loss: 17.0806 - val_mse: 17.0806 - val_mae: 1.5483 - lr: 0.0023 - 2s/epoch - 2ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 2s - loss: 11.9394 - mse: 11.9394 - mae: 1.4822 - val_loss: 16.8114 - val_mse: 16.8114 - val_mae: 1.5587 - lr: 0.0023 - 2s/epoch - 2ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 2s - loss: 11.7919 - mse: 11.7919 - mae: 1.4750 - val_loss: 16.9413 - val_mse: 16.9413 - val_mae: 1.5374 - lr: 0.0023 - 2s/epoch - 2ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 2s - loss: 11.7100 - mse: 11.7100 - mae: 1.4751 - val_loss: 16.9572 - val_mse: 16.9572 - val_mae: 1.4769 - lr: 0.0023 - 2s/epoch - 2ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 2s - loss: 11.6525 - mse: 11.6525 - mae: 1.4702 - val_loss: 16.9780 - val_mse: 16.9780 - val_mae: 1.5836 - lr: 0.0023 - 2s/epoch - 2ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 2s - loss: 11.5054 - mse: 11.5054 - mae: 1.4657 - val_loss: 16.8535 - val_mse: 16.8535 - val_mae: 1.4969 - lr: 0.0023 - 2s/epoch - 2ms/step\n",
            "Epoch 13/100\n",
            "1000/1000 - 2s - loss: 11.4066 - mse: 11.4066 - mae: 1.4612 - val_loss: 16.9281 - val_mse: 16.9281 - val_mae: 1.6116 - lr: 0.0023 - 2s/epoch - 2ms/step\n",
            "Score for fold 1: loss of 16.928146362304688\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 11.3175 - mse: 11.3175 - mae: 1.4363 - val_loss: 15.3694 - val_mse: 15.3694 - val_mae: 1.4439 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 11.1095 - mse: 11.1095 - mae: 1.4244 - val_loss: 15.4752 - val_mse: 15.4752 - val_mae: 1.4283 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 10.9937 - mse: 10.9937 - mae: 1.4183 - val_loss: 15.4255 - val_mse: 15.4255 - val_mae: 1.4356 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 10.8984 - mse: 10.8984 - mae: 1.4159 - val_loss: 15.3898 - val_mse: 15.3898 - val_mae: 1.4671 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 10.8191 - mse: 10.8191 - mae: 1.4130 - val_loss: 15.4383 - val_mse: 15.4383 - val_mae: 1.4749 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 10.7030 - mse: 10.7030 - mae: 1.4056 - val_loss: 15.3363 - val_mse: 15.3363 - val_mae: 1.4878 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 10.6446 - mse: 10.6446 - mae: 1.4080 - val_loss: 15.3048 - val_mse: 15.3048 - val_mae: 1.4786 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 2s - loss: 10.5179 - mse: 10.5179 - mae: 1.4045 - val_loss: 15.2850 - val_mse: 15.2850 - val_mae: 1.4734 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 2s - loss: 10.4421 - mse: 10.4421 - mae: 1.3960 - val_loss: 15.2701 - val_mse: 15.2701 - val_mae: 1.4918 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 2s - loss: 10.3949 - mse: 10.3949 - mae: 1.3977 - val_loss: 15.2777 - val_mse: 15.2777 - val_mae: 1.5183 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 2s - loss: 10.3032 - mse: 10.3032 - mae: 1.3955 - val_loss: 15.3715 - val_mse: 15.3715 - val_mae: 1.5029 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 2s - loss: 10.2252 - mse: 10.2252 - mae: 1.3951 - val_loss: 15.2842 - val_mse: 15.2842 - val_mae: 1.4693 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 13/100\n",
            "1000/1000 - 2s - loss: 10.1546 - mse: 10.1546 - mae: 1.3877 - val_loss: 15.3217 - val_mse: 15.3217 - val_mae: 1.5226 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 14/100\n",
            "1000/1000 - 2s - loss: 10.0624 - mse: 10.0624 - mae: 1.3882 - val_loss: 15.3668 - val_mse: 15.3668 - val_mae: 1.4758 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Score for fold 2: loss of 15.36683177947998\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 11.1035 - mse: 11.1035 - mae: 1.4009 - val_loss: 11.3980 - val_mse: 11.3980 - val_mae: 1.4262 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 11.0125 - mse: 11.0125 - mae: 1.3944 - val_loss: 11.4690 - val_mse: 11.4690 - val_mae: 1.4350 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 10.9287 - mse: 10.9287 - mae: 1.3840 - val_loss: 11.6248 - val_mse: 11.6248 - val_mae: 1.4316 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 10.8222 - mse: 10.8222 - mae: 1.3803 - val_loss: 11.6624 - val_mse: 11.6624 - val_mae: 1.4445 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 10.7958 - mse: 10.7958 - mae: 1.3747 - val_loss: 11.6983 - val_mse: 11.6983 - val_mae: 1.4352 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 10.6982 - mse: 10.6982 - mae: 1.3694 - val_loss: 11.8165 - val_mse: 11.8165 - val_mae: 1.4418 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Score for fold 3: loss of 11.816530227661133\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 11.9988 - mse: 11.9988 - mae: 1.4021 - val_loss: 6.5724 - val_mse: 6.5724 - val_mae: 1.3365 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 11.9245 - mse: 11.9245 - mae: 1.3947 - val_loss: 6.8340 - val_mse: 6.8340 - val_mae: 1.3420 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 11.7909 - mse: 11.7909 - mae: 1.3868 - val_loss: 6.7937 - val_mse: 6.7937 - val_mae: 1.3700 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 11.7181 - mse: 11.7181 - mae: 1.3881 - val_loss: 6.8704 - val_mse: 6.8704 - val_mae: 1.3892 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 11.6739 - mse: 11.6739 - mae: 1.3827 - val_loss: 6.8574 - val_mse: 6.8574 - val_mae: 1.3753 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 11.5460 - mse: 11.5460 - mae: 1.3764 - val_loss: 6.8774 - val_mse: 6.8774 - val_mae: 1.3971 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Score for fold 4: loss of 6.877384185791016\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 11.6452 - mse: 11.6452 - mae: 1.4032 - val_loss: 6.5613 - val_mse: 6.5613 - val_mae: 1.2866 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 11.5016 - mse: 11.5016 - mae: 1.3935 - val_loss: 6.7073 - val_mse: 6.7073 - val_mae: 1.2934 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 11.4617 - mse: 11.4617 - mae: 1.3861 - val_loss: 6.7292 - val_mse: 6.7292 - val_mae: 1.3639 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 11.3700 - mse: 11.3700 - mae: 1.3840 - val_loss: 6.8246 - val_mse: 6.8246 - val_mae: 1.3202 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 11.2834 - mse: 11.2834 - mae: 1.3793 - val_loss: 6.8464 - val_mse: 6.8464 - val_mae: 1.3364 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 11.2398 - mse: 11.2398 - mae: 1.3767 - val_loss: 7.0526 - val_mse: 7.0526 - val_mae: 1.3313 - lr: 0.0010 - 2s/epoch - 2ms/step\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-10-11 11:30:56,859]\u001b[0m Finished trial#16 resulted in value: 11.61. Current best value is 9.334 with parameters: {'activation': 'relu', 'num_hidden_layer': 4, 'i': 9, 'learning_rate': 0.00023071835273936203}.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Score for fold 5: loss of 7.05258321762085\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 11s - loss: 13.5798 - mse: 13.5798 - mae: 1.5509 - val_loss: 11.3676 - val_mse: 11.3676 - val_mae: 1.4118 - lr: 3.3976e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 11s - loss: 12.6085 - mse: 12.6085 - mae: 1.4949 - val_loss: 11.2199 - val_mse: 11.2199 - val_mae: 1.5134 - lr: 3.3976e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 11s - loss: 12.4016 - mse: 12.4016 - mae: 1.4763 - val_loss: 11.5089 - val_mse: 11.5089 - val_mae: 1.5438 - lr: 3.3976e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 11s - loss: 12.2961 - mse: 12.2961 - mae: 1.4737 - val_loss: 11.4100 - val_mse: 11.4100 - val_mae: 1.5498 - lr: 3.3976e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 11s - loss: 12.4225 - mse: 12.4225 - mae: 1.4645 - val_loss: 11.5521 - val_mse: 11.5521 - val_mae: 1.4518 - lr: 3.3976e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 11s - loss: 12.1301 - mse: 12.1301 - mae: 1.4542 - val_loss: 11.3859 - val_mse: 11.3859 - val_mae: 1.5087 - lr: 3.3976e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 11s - loss: 11.7840 - mse: 11.7840 - mae: 1.4424 - val_loss: 11.3002 - val_mse: 11.3002 - val_mae: 1.4861 - lr: 3.3976e-04 - 11s/epoch - 11ms/step\n",
            "Score for fold 1: loss of 11.300213813781738\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 11s - loss: 12.4206 - mse: 12.4206 - mae: 1.4534 - val_loss: 8.3738 - val_mse: 8.3738 - val_mae: 1.4668 - lr: 3.3976e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 11s - loss: 12.1704 - mse: 12.1704 - mae: 1.4404 - val_loss: 8.4068 - val_mse: 8.4068 - val_mae: 1.4115 - lr: 3.3976e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 11s - loss: 12.1514 - mse: 12.1514 - mae: 1.4311 - val_loss: 8.3591 - val_mse: 8.3591 - val_mae: 1.4223 - lr: 3.3976e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 11s - loss: 11.6061 - mse: 11.6061 - mae: 1.4160 - val_loss: 8.4982 - val_mse: 8.4982 - val_mae: 1.4215 - lr: 3.3976e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 10s - loss: 11.7860 - mse: 11.7860 - mae: 1.4053 - val_loss: 8.4540 - val_mse: 8.4540 - val_mae: 1.4552 - lr: 3.3976e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 11s - loss: 11.1516 - mse: 11.1516 - mae: 1.3924 - val_loss: 8.7698 - val_mse: 8.7698 - val_mae: 1.3702 - lr: 3.3976e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 11s - loss: 11.0485 - mse: 11.0485 - mae: 1.3803 - val_loss: 8.4515 - val_mse: 8.4515 - val_mae: 1.3976 - lr: 3.3976e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 11s - loss: 10.6607 - mse: 10.6607 - mae: 1.3657 - val_loss: 8.7732 - val_mse: 8.7732 - val_mae: 1.4130 - lr: 3.3976e-04 - 11s/epoch - 11ms/step\n",
            "Score for fold 2: loss of 8.773215293884277\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 11s - loss: 9.9207 - mse: 9.9207 - mae: 1.3776 - val_loss: 13.0047 - val_mse: 13.0047 - val_mae: 1.3579 - lr: 3.3976e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 11s - loss: 9.6388 - mse: 9.6388 - mae: 1.3641 - val_loss: 11.6626 - val_mse: 11.6626 - val_mae: 1.3790 - lr: 3.3976e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 11s - loss: 9.4521 - mse: 9.4521 - mae: 1.3479 - val_loss: 14.4115 - val_mse: 14.4115 - val_mae: 1.4120 - lr: 3.3976e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 11s - loss: 9.2239 - mse: 9.2239 - mae: 1.3310 - val_loss: 12.1142 - val_mse: 12.1142 - val_mae: 1.4169 - lr: 3.3976e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 11s - loss: 8.9361 - mse: 8.9361 - mae: 1.3232 - val_loss: 12.3863 - val_mse: 12.3863 - val_mae: 1.5878 - lr: 3.3976e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 11s - loss: 8.4470 - mse: 8.4470 - mae: 1.2983 - val_loss: 13.7735 - val_mse: 13.7735 - val_mae: 1.3869 - lr: 3.3976e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 11s - loss: 8.6062 - mse: 8.6062 - mae: 1.2943 - val_loss: 12.5018 - val_mse: 12.5018 - val_mae: 1.5028 - lr: 3.3976e-04 - 11s/epoch - 11ms/step\n",
            "Score for fold 3: loss of 12.501848220825195\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 10s - loss: 10.1570 - mse: 10.1570 - mae: 1.3296 - val_loss: 6.2850 - val_mse: 6.2850 - val_mae: 1.3292 - lr: 3.3976e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 11s - loss: 9.8511 - mse: 9.8511 - mae: 1.3069 - val_loss: 6.7587 - val_mse: 6.7587 - val_mae: 1.3583 - lr: 3.3976e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 11s - loss: 9.6718 - mse: 9.6718 - mae: 1.2848 - val_loss: 6.7225 - val_mse: 6.7225 - val_mae: 1.2579 - lr: 3.3976e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 11s - loss: 9.4312 - mse: 9.4312 - mae: 1.2777 - val_loss: 6.8391 - val_mse: 6.8391 - val_mae: 1.4286 - lr: 3.3976e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 11s - loss: 9.2318 - mse: 9.2318 - mae: 1.2672 - val_loss: 7.2532 - val_mse: 7.2532 - val_mae: 1.2746 - lr: 3.3976e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 11s - loss: 8.7738 - mse: 8.7738 - mae: 1.2500 - val_loss: 7.0058 - val_mse: 7.0058 - val_mae: 1.2846 - lr: 3.3976e-04 - 11s/epoch - 11ms/step\n",
            "Score for fold 4: loss of 7.00579833984375\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 11s - loss: 7.7038 - mse: 7.7038 - mae: 1.2847 - val_loss: 11.3332 - val_mse: 11.3332 - val_mae: 1.2499 - lr: 3.3976e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 11s - loss: 7.1377 - mse: 7.1377 - mae: 1.2571 - val_loss: 11.2814 - val_mse: 11.2814 - val_mae: 1.1807 - lr: 3.3976e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 11s - loss: 7.5744 - mse: 7.5744 - mae: 1.2567 - val_loss: 12.0676 - val_mse: 12.0676 - val_mae: 1.4042 - lr: 3.3976e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 11s - loss: 6.9801 - mse: 6.9801 - mae: 1.2359 - val_loss: 11.9384 - val_mse: 11.9384 - val_mae: 1.2307 - lr: 3.3976e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 10s - loss: 6.8589 - mse: 6.8589 - mae: 1.2207 - val_loss: 11.9256 - val_mse: 11.9256 - val_mae: 1.3429 - lr: 3.3976e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 11s - loss: 6.6140 - mse: 6.6140 - mae: 1.1985 - val_loss: 12.2827 - val_mse: 12.2827 - val_mae: 1.2988 - lr: 3.3976e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 11s - loss: 6.2846 - mse: 6.2846 - mae: 1.1905 - val_loss: 12.9345 - val_mse: 12.9345 - val_mae: 1.3464 - lr: 3.3976e-04 - 11s/epoch - 11ms/step\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-10-11 11:37:22,188]\u001b[0m Finished trial#17 resulted in value: 10.501999999999999. Current best value is 9.334 with parameters: {'activation': 'relu', 'num_hidden_layer': 4, 'i': 9, 'learning_rate': 0.00023071835273936203}.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Score for fold 5: loss of 12.934479713439941\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 7s - loss: 14.6046 - mse: 14.6046 - mae: 1.5931 - val_loss: 9.9784 - val_mse: 9.9784 - val_mae: 1.7135 - lr: 0.0048 - 7s/epoch - 7ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 6s - loss: 13.5784 - mse: 13.5784 - mae: 1.5450 - val_loss: 9.0883 - val_mse: 9.0883 - val_mae: 1.5640 - lr: 0.0048 - 6s/epoch - 6ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 6s - loss: 13.3475 - mse: 13.3475 - mae: 1.5255 - val_loss: 9.1455 - val_mse: 9.1455 - val_mae: 1.5225 - lr: 0.0048 - 6s/epoch - 6ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 6s - loss: 13.2336 - mse: 13.2336 - mae: 1.5114 - val_loss: 8.9434 - val_mse: 8.9434 - val_mae: 1.5647 - lr: 0.0048 - 6s/epoch - 6ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 6s - loss: 13.2063 - mse: 13.2063 - mae: 1.5164 - val_loss: 8.6553 - val_mse: 8.6553 - val_mae: 1.4877 - lr: 0.0048 - 6s/epoch - 6ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 6s - loss: 13.3902 - mse: 13.3902 - mae: 1.5107 - val_loss: 8.9083 - val_mse: 8.9083 - val_mae: 1.5380 - lr: 0.0048 - 6s/epoch - 6ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 6s - loss: 13.2347 - mse: 13.2347 - mae: 1.5250 - val_loss: 8.8840 - val_mse: 8.8840 - val_mae: 1.5674 - lr: 0.0048 - 6s/epoch - 6ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 6s - loss: 13.6938 - mse: 13.6938 - mae: 1.5517 - val_loss: 9.9308 - val_mse: 9.9308 - val_mae: 1.6038 - lr: 0.0048 - 6s/epoch - 6ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 6s - loss: 13.4859 - mse: 13.4859 - mae: 1.5739 - val_loss: 9.1575 - val_mse: 9.1575 - val_mae: 1.6128 - lr: 0.0048 - 6s/epoch - 6ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 6s - loss: 13.8285 - mse: 13.8285 - mae: 1.5930 - val_loss: 9.2475 - val_mse: 9.2475 - val_mae: 1.5385 - lr: 0.0048 - 6s/epoch - 6ms/step\n",
            "Score for fold 1: loss of 9.247495651245117\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 6s - loss: 12.4058 - mse: 12.4058 - mae: 1.5051 - val_loss: 11.5466 - val_mse: 11.5466 - val_mae: 1.4693 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 6s - loss: 12.0865 - mse: 12.0865 - mae: 1.4659 - val_loss: 11.4196 - val_mse: 11.4196 - val_mae: 1.4661 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 6s - loss: 11.8849 - mse: 11.8849 - mae: 1.4600 - val_loss: 11.4646 - val_mse: 11.4646 - val_mae: 1.4745 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 6s - loss: 11.8276 - mse: 11.8276 - mae: 1.4576 - val_loss: 11.3193 - val_mse: 11.3193 - val_mae: 1.5010 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 6s - loss: 11.6984 - mse: 11.6984 - mae: 1.4531 - val_loss: 11.5835 - val_mse: 11.5835 - val_mae: 1.4780 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 6s - loss: 11.7014 - mse: 11.7014 - mae: 1.4524 - val_loss: 11.3251 - val_mse: 11.3251 - val_mae: 1.4788 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 6s - loss: 11.6097 - mse: 11.6097 - mae: 1.4476 - val_loss: 11.4837 - val_mse: 11.4837 - val_mae: 1.4619 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 6s - loss: 11.6190 - mse: 11.6190 - mae: 1.4467 - val_loss: 11.3875 - val_mse: 11.3875 - val_mae: 1.4519 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 6s - loss: 11.5497 - mse: 11.5497 - mae: 1.4443 - val_loss: 11.4708 - val_mse: 11.4708 - val_mae: 1.5095 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
            "Score for fold 2: loss of 11.470752716064453\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 6s - loss: 12.4718 - mse: 12.4718 - mae: 1.4784 - val_loss: 8.1126 - val_mse: 8.1126 - val_mae: 1.4291 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 6s - loss: 12.4637 - mse: 12.4637 - mae: 1.4737 - val_loss: 8.2677 - val_mse: 8.2677 - val_mae: 1.3691 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 6s - loss: 12.3432 - mse: 12.3432 - mae: 1.4686 - val_loss: 8.0367 - val_mse: 8.0367 - val_mae: 1.3754 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 6s - loss: 12.2717 - mse: 12.2717 - mae: 1.4678 - val_loss: 8.1036 - val_mse: 8.1036 - val_mae: 1.4171 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 6s - loss: 12.1837 - mse: 12.1837 - mae: 1.4646 - val_loss: 8.4147 - val_mse: 8.4147 - val_mae: 1.4069 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 6s - loss: 12.1086 - mse: 12.1086 - mae: 1.4611 - val_loss: 8.2410 - val_mse: 8.2410 - val_mae: 1.4550 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 6s - loss: 12.1339 - mse: 12.1339 - mae: 1.4591 - val_loss: 8.2481 - val_mse: 8.2481 - val_mae: 1.4370 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 6s - loss: 11.9599 - mse: 11.9599 - mae: 1.4570 - val_loss: 8.0366 - val_mse: 8.0366 - val_mae: 1.3851 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 6s - loss: 11.8491 - mse: 11.8491 - mae: 1.4508 - val_loss: 8.1662 - val_mse: 8.1662 - val_mae: 1.4258 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 6s - loss: 11.8444 - mse: 11.8444 - mae: 1.4455 - val_loss: 8.1821 - val_mse: 8.1821 - val_mae: 1.4000 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 6s - loss: 11.6891 - mse: 11.6891 - mae: 1.4442 - val_loss: 8.1717 - val_mse: 8.1717 - val_mae: 1.4064 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 6s - loss: 11.5550 - mse: 11.5550 - mae: 1.4363 - val_loss: 8.3191 - val_mse: 8.3191 - val_mae: 1.3973 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
            "Epoch 13/100\n",
            "1000/1000 - 7s - loss: 11.5203 - mse: 11.5203 - mae: 1.4336 - val_loss: 8.3082 - val_mse: 8.3082 - val_mae: 1.4244 - lr: 0.0010 - 7s/epoch - 7ms/step\n",
            "Score for fold 3: loss of 8.308216094970703\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 6s - loss: 8.8772 - mse: 8.8772 - mae: 1.4215 - val_loss: 19.0330 - val_mse: 19.0330 - val_mae: 1.4635 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 6s - loss: 8.9130 - mse: 8.9130 - mae: 1.4193 - val_loss: 19.3824 - val_mse: 19.3824 - val_mae: 1.4276 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 6s - loss: 8.6994 - mse: 8.6994 - mae: 1.4194 - val_loss: 20.1220 - val_mse: 20.1220 - val_mae: 1.4435 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 6s - loss: 8.7202 - mse: 8.7202 - mae: 1.4157 - val_loss: 19.2591 - val_mse: 19.2591 - val_mae: 1.4497 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 6s - loss: 8.6437 - mse: 8.6437 - mae: 1.4099 - val_loss: 19.2853 - val_mse: 19.2853 - val_mae: 1.4252 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 6s - loss: 8.6029 - mse: 8.6029 - mae: 1.4092 - val_loss: 19.6152 - val_mse: 19.6152 - val_mae: 1.4675 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
            "Score for fold 4: loss of 19.615257263183594\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 6s - loss: 11.1303 - mse: 11.1303 - mae: 1.4208 - val_loss: 9.5813 - val_mse: 9.5813 - val_mae: 1.3917 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 6s - loss: 11.4811 - mse: 11.4811 - mae: 1.4158 - val_loss: 9.1977 - val_mse: 9.1977 - val_mae: 1.4187 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 6s - loss: 11.0318 - mse: 11.0318 - mae: 1.4154 - val_loss: 10.1897 - val_mse: 10.1897 - val_mae: 1.4809 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 6s - loss: 10.9084 - mse: 10.9084 - mae: 1.4076 - val_loss: 9.1868 - val_mse: 9.1868 - val_mae: 1.4233 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 6s - loss: 10.8702 - mse: 10.8702 - mae: 1.3944 - val_loss: 10.2083 - val_mse: 10.2083 - val_mae: 1.4098 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 6s - loss: 10.6961 - mse: 10.6961 - mae: 1.3985 - val_loss: 9.7260 - val_mse: 9.7260 - val_mae: 1.4194 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 6s - loss: 10.8818 - mse: 10.8818 - mae: 1.3913 - val_loss: 9.9442 - val_mse: 9.9442 - val_mae: 1.4000 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 6s - loss: 10.6201 - mse: 10.6201 - mae: 1.3874 - val_loss: 9.7041 - val_mse: 9.7041 - val_mae: 1.4260 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 6s - loss: 10.3981 - mse: 10.3981 - mae: 1.3802 - val_loss: 9.4137 - val_mse: 9.4137 - val_mae: 1.4428 - lr: 0.0010 - 6s/epoch - 6ms/step\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-10-11 11:42:24,398]\u001b[0m Finished trial#18 resulted in value: 11.612. Current best value is 9.334 with parameters: {'activation': 'relu', 'num_hidden_layer': 4, 'i': 9, 'learning_rate': 0.00023071835273936203}.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Score for fold 5: loss of 9.413737297058105\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 8s - loss: 13.6275 - mse: 13.6275 - mae: 1.5673 - val_loss: 10.5410 - val_mse: 10.5410 - val_mae: 1.5347 - lr: 0.0018 - 8s/epoch - 8ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 7s - loss: 13.1141 - mse: 13.1141 - mae: 1.5075 - val_loss: 10.3509 - val_mse: 10.3509 - val_mae: 1.4686 - lr: 0.0018 - 7s/epoch - 7ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 7s - loss: 12.7728 - mse: 12.7728 - mae: 1.4973 - val_loss: 10.1445 - val_mse: 10.1445 - val_mae: 1.4582 - lr: 0.0018 - 7s/epoch - 7ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 7s - loss: 12.6870 - mse: 12.6870 - mae: 1.4803 - val_loss: 10.0190 - val_mse: 10.0190 - val_mae: 1.4551 - lr: 0.0018 - 7s/epoch - 7ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 7s - loss: 12.6013 - mse: 12.6013 - mae: 1.4767 - val_loss: 10.1095 - val_mse: 10.1095 - val_mae: 1.5043 - lr: 0.0018 - 7s/epoch - 7ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 7s - loss: 12.4158 - mse: 12.4158 - mae: 1.4623 - val_loss: 10.0355 - val_mse: 10.0355 - val_mae: 1.4754 - lr: 0.0018 - 7s/epoch - 7ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 7s - loss: 12.2802 - mse: 12.2802 - mae: 1.4544 - val_loss: 10.0234 - val_mse: 10.0234 - val_mae: 1.4218 - lr: 0.0018 - 7s/epoch - 7ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 7s - loss: 12.2691 - mse: 12.2691 - mae: 1.4626 - val_loss: 10.1055 - val_mse: 10.1055 - val_mae: 1.4535 - lr: 0.0018 - 7s/epoch - 7ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 7s - loss: 12.2084 - mse: 12.2084 - mae: 1.4646 - val_loss: 10.2420 - val_mse: 10.2420 - val_mae: 1.4514 - lr: 0.0018 - 7s/epoch - 7ms/step\n",
            "Score for fold 1: loss of 10.242000579833984\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 7s - loss: 10.0328 - mse: 10.0328 - mae: 1.4323 - val_loss: 17.1194 - val_mse: 17.1194 - val_mae: 1.4236 - lr: 0.0010 - 7s/epoch - 7ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 7s - loss: 9.9289 - mse: 9.9289 - mae: 1.4185 - val_loss: 17.0492 - val_mse: 17.0492 - val_mae: 1.4850 - lr: 0.0010 - 7s/epoch - 7ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 7s - loss: 9.6873 - mse: 9.6873 - mae: 1.4145 - val_loss: 17.4718 - val_mse: 17.4718 - val_mae: 1.4730 - lr: 0.0010 - 7s/epoch - 7ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 7s - loss: 9.5335 - mse: 9.5335 - mae: 1.4034 - val_loss: 17.4740 - val_mse: 17.4740 - val_mae: 1.4444 - lr: 0.0010 - 7s/epoch - 7ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 7s - loss: 9.4092 - mse: 9.4092 - mae: 1.3982 - val_loss: 16.9772 - val_mse: 16.9772 - val_mae: 1.4537 - lr: 0.0010 - 7s/epoch - 7ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 7s - loss: 9.3946 - mse: 9.3946 - mae: 1.3914 - val_loss: 17.1367 - val_mse: 17.1367 - val_mae: 1.4267 - lr: 0.0010 - 7s/epoch - 7ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 7s - loss: 9.2068 - mse: 9.2068 - mae: 1.3820 - val_loss: 19.0998 - val_mse: 19.0998 - val_mae: 1.5106 - lr: 0.0010 - 7s/epoch - 7ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 7s - loss: 9.0677 - mse: 9.0677 - mae: 1.3740 - val_loss: 17.5588 - val_mse: 17.5588 - val_mae: 1.4573 - lr: 0.0010 - 7s/epoch - 7ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 7s - loss: 8.9846 - mse: 8.9846 - mae: 1.3700 - val_loss: 18.1675 - val_mse: 18.1675 - val_mae: 1.4828 - lr: 0.0010 - 7s/epoch - 7ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 7s - loss: 8.9455 - mse: 8.9455 - mae: 1.3601 - val_loss: 17.4219 - val_mse: 17.4219 - val_mae: 1.5058 - lr: 0.0010 - 7s/epoch - 7ms/step\n",
            "Score for fold 2: loss of 17.421884536743164\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 7s - loss: 10.5833 - mse: 10.5833 - mae: 1.3806 - val_loss: 10.6194 - val_mse: 10.6194 - val_mae: 1.3846 - lr: 0.0010 - 7s/epoch - 7ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 7s - loss: 10.2936 - mse: 10.2936 - mae: 1.3717 - val_loss: 10.6999 - val_mse: 10.6999 - val_mae: 1.3884 - lr: 0.0010 - 7s/epoch - 7ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 7s - loss: 10.1177 - mse: 10.1177 - mae: 1.3627 - val_loss: 10.7517 - val_mse: 10.7517 - val_mae: 1.3689 - lr: 0.0010 - 7s/epoch - 7ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 7s - loss: 9.8867 - mse: 9.8867 - mae: 1.3462 - val_loss: 10.9196 - val_mse: 10.9196 - val_mae: 1.4078 - lr: 0.0010 - 7s/epoch - 7ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 7s - loss: 9.8137 - mse: 9.8137 - mae: 1.3415 - val_loss: 11.1295 - val_mse: 11.1295 - val_mae: 1.3618 - lr: 0.0010 - 7s/epoch - 7ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 7s - loss: 9.7475 - mse: 9.7475 - mae: 1.3342 - val_loss: 10.9380 - val_mse: 10.9380 - val_mae: 1.4232 - lr: 0.0010 - 7s/epoch - 7ms/step\n",
            "Score for fold 3: loss of 10.93796157836914\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 7s - loss: 10.6639 - mse: 10.6639 - mae: 1.3559 - val_loss: 7.3308 - val_mse: 7.3308 - val_mae: 1.2949 - lr: 0.0010 - 7s/epoch - 7ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 7s - loss: 10.2971 - mse: 10.2971 - mae: 1.3374 - val_loss: 7.4219 - val_mse: 7.4219 - val_mae: 1.3213 - lr: 0.0010 - 7s/epoch - 7ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 7s - loss: 10.1943 - mse: 10.1943 - mae: 1.3287 - val_loss: 8.5119 - val_mse: 8.5119 - val_mae: 1.3810 - lr: 0.0010 - 7s/epoch - 7ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 7s - loss: 10.0995 - mse: 10.0995 - mae: 1.3206 - val_loss: 7.9675 - val_mse: 7.9675 - val_mae: 1.3157 - lr: 0.0010 - 7s/epoch - 7ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 7s - loss: 9.8247 - mse: 9.8247 - mae: 1.3052 - val_loss: 8.3017 - val_mse: 8.3017 - val_mae: 1.3487 - lr: 0.0010 - 7s/epoch - 7ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 7s - loss: 9.8096 - mse: 9.8096 - mae: 1.2967 - val_loss: 7.8878 - val_mse: 7.8878 - val_mae: 1.3506 - lr: 0.0010 - 7s/epoch - 7ms/step\n",
            "Score for fold 4: loss of 7.887781143188477\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 7s - loss: 10.1373 - mse: 10.1373 - mae: 1.3136 - val_loss: 6.1620 - val_mse: 6.1620 - val_mae: 1.2528 - lr: 0.0010 - 7s/epoch - 7ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 7s - loss: 9.8865 - mse: 9.8865 - mae: 1.3002 - val_loss: 7.0298 - val_mse: 7.0298 - val_mae: 1.3006 - lr: 0.0010 - 7s/epoch - 7ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 7s - loss: 9.9987 - mse: 9.9987 - mae: 1.2896 - val_loss: 6.8277 - val_mse: 6.8277 - val_mae: 1.2588 - lr: 0.0010 - 7s/epoch - 7ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 7s - loss: 9.5406 - mse: 9.5406 - mae: 1.2751 - val_loss: 6.8556 - val_mse: 6.8556 - val_mae: 1.2724 - lr: 0.0010 - 7s/epoch - 7ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 7s - loss: 10.2274 - mse: 10.2274 - mae: 1.2673 - val_loss: 7.0672 - val_mse: 7.0672 - val_mae: 1.2819 - lr: 0.0010 - 7s/epoch - 7ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 7s - loss: 9.3322 - mse: 9.3322 - mae: 1.2501 - val_loss: 6.9924 - val_mse: 6.9924 - val_mae: 1.3289 - lr: 0.0010 - 7s/epoch - 7ms/step\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-10-11 11:46:48,295]\u001b[0m Finished trial#19 resulted in value: 10.696000000000002. Current best value is 9.334 with parameters: {'activation': 'relu', 'num_hidden_layer': 4, 'i': 9, 'learning_rate': 0.00023071835273936203}.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Score for fold 5: loss of 6.992433071136475\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 3s - loss: 15.2606 - mse: 15.2606 - mae: 1.6331 - val_loss: 12.9750 - val_mse: 12.9750 - val_mae: 1.6262 - lr: 0.0054 - 3s/epoch - 3ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 14.4013 - mse: 14.4013 - mae: 1.5757 - val_loss: 12.2709 - val_mse: 12.2709 - val_mae: 1.5559 - lr: 0.0054 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 14.1851 - mse: 14.1851 - mae: 1.5704 - val_loss: 11.8024 - val_mse: 11.8024 - val_mae: 1.5482 - lr: 0.0054 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 13.9879 - mse: 13.9879 - mae: 1.5522 - val_loss: 11.7437 - val_mse: 11.7437 - val_mae: 1.5776 - lr: 0.0054 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 13.9531 - mse: 13.9531 - mae: 1.5492 - val_loss: 11.7916 - val_mse: 11.7916 - val_mae: 1.4753 - lr: 0.0054 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 13.7411 - mse: 13.7411 - mae: 1.5403 - val_loss: 11.2094 - val_mse: 11.2094 - val_mae: 1.5711 - lr: 0.0054 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 13.6805 - mse: 13.6805 - mae: 1.5476 - val_loss: 11.1058 - val_mse: 11.1058 - val_mae: 1.5030 - lr: 0.0054 - 2s/epoch - 2ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 2s - loss: 13.6181 - mse: 13.6181 - mae: 1.5438 - val_loss: 11.5193 - val_mse: 11.5193 - val_mae: 1.6138 - lr: 0.0054 - 2s/epoch - 2ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 2s - loss: 13.6790 - mse: 13.6790 - mae: 1.5636 - val_loss: 11.6170 - val_mse: 11.6170 - val_mae: 1.7258 - lr: 0.0054 - 2s/epoch - 2ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 2s - loss: 13.7154 - mse: 13.7154 - mae: 1.5607 - val_loss: 11.0503 - val_mse: 11.0503 - val_mae: 1.4766 - lr: 0.0054 - 2s/epoch - 2ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 2s - loss: 13.7229 - mse: 13.7229 - mae: 1.5551 - val_loss: 11.3020 - val_mse: 11.3020 - val_mae: 1.6362 - lr: 0.0054 - 2s/epoch - 2ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 2s - loss: 13.7092 - mse: 13.7092 - mae: 1.5528 - val_loss: 11.6457 - val_mse: 11.6457 - val_mae: 1.6290 - lr: 0.0054 - 2s/epoch - 2ms/step\n",
            "Epoch 13/100\n",
            "1000/1000 - 2s - loss: 13.6362 - mse: 13.6362 - mae: 1.5498 - val_loss: 11.4029 - val_mse: 11.4029 - val_mae: 1.6163 - lr: 0.0054 - 2s/epoch - 2ms/step\n",
            "Epoch 14/100\n",
            "1000/1000 - 2s - loss: 13.6949 - mse: 13.6949 - mae: 1.5637 - val_loss: 11.9599 - val_mse: 11.9599 - val_mae: 1.4492 - lr: 0.0054 - 2s/epoch - 2ms/step\n",
            "Epoch 15/100\n",
            "1000/1000 - 2s - loss: 13.5382 - mse: 13.5382 - mae: 1.5560 - val_loss: 11.3835 - val_mse: 11.3835 - val_mae: 1.5755 - lr: 0.0054 - 2s/epoch - 2ms/step\n",
            "Score for fold 1: loss of 11.383552551269531\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 13.2962 - mse: 13.2962 - mae: 1.4973 - val_loss: 10.1979 - val_mse: 10.1979 - val_mae: 1.4891 - lr: 0.0011 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 12.8828 - mse: 12.8828 - mae: 1.4846 - val_loss: 10.2408 - val_mse: 10.2408 - val_mae: 1.5157 - lr: 0.0011 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 12.7139 - mse: 12.7139 - mae: 1.4717 - val_loss: 10.0272 - val_mse: 10.0272 - val_mae: 1.4922 - lr: 0.0011 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 12.5367 - mse: 12.5367 - mae: 1.4690 - val_loss: 10.0778 - val_mse: 10.0778 - val_mae: 1.5145 - lr: 0.0011 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 12.3734 - mse: 12.3734 - mae: 1.4624 - val_loss: 9.9591 - val_mse: 9.9591 - val_mae: 1.5043 - lr: 0.0011 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 12.2431 - mse: 12.2431 - mae: 1.4574 - val_loss: 10.0138 - val_mse: 10.0138 - val_mae: 1.4718 - lr: 0.0011 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 12.1117 - mse: 12.1117 - mae: 1.4473 - val_loss: 9.9402 - val_mse: 9.9402 - val_mae: 1.5242 - lr: 0.0011 - 2s/epoch - 2ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 2s - loss: 11.9967 - mse: 11.9967 - mae: 1.4434 - val_loss: 10.0104 - val_mse: 10.0104 - val_mae: 1.4933 - lr: 0.0011 - 2s/epoch - 2ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 2s - loss: 11.8841 - mse: 11.8841 - mae: 1.4418 - val_loss: 10.0265 - val_mse: 10.0265 - val_mae: 1.5116 - lr: 0.0011 - 2s/epoch - 2ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 2s - loss: 11.8062 - mse: 11.8062 - mae: 1.4266 - val_loss: 10.0249 - val_mse: 10.0249 - val_mae: 1.4714 - lr: 0.0011 - 2s/epoch - 2ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 2s - loss: 11.7904 - mse: 11.7904 - mae: 1.4250 - val_loss: 10.0579 - val_mse: 10.0579 - val_mae: 1.4569 - lr: 0.0011 - 2s/epoch - 2ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 2s - loss: 11.7490 - mse: 11.7490 - mae: 1.4223 - val_loss: 10.0447 - val_mse: 10.0447 - val_mae: 1.4876 - lr: 0.0011 - 2s/epoch - 2ms/step\n",
            "Score for fold 2: loss of 10.044715881347656\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 9.2503 - mse: 9.2503 - mae: 1.4416 - val_loss: 19.9047 - val_mse: 19.9047 - val_mae: 1.4237 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 9.1544 - mse: 9.1544 - mae: 1.4342 - val_loss: 20.0603 - val_mse: 20.0603 - val_mae: 1.4313 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 9.0785 - mse: 9.0785 - mae: 1.4349 - val_loss: 20.0918 - val_mse: 20.0918 - val_mae: 1.4448 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 8.9154 - mse: 8.9154 - mae: 1.4161 - val_loss: 20.0921 - val_mse: 20.0921 - val_mae: 1.4989 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 8.7844 - mse: 8.7844 - mae: 1.4118 - val_loss: 20.4523 - val_mse: 20.4523 - val_mae: 1.4084 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 8.7188 - mse: 8.7188 - mae: 1.4111 - val_loss: 20.3710 - val_mse: 20.3710 - val_mae: 1.4941 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Score for fold 3: loss of 20.37103843688965\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 11.5104 - mse: 11.5104 - mae: 1.4179 - val_loss: 9.4037 - val_mse: 9.4037 - val_mae: 1.4277 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 11.4561 - mse: 11.4561 - mae: 1.4124 - val_loss: 9.6111 - val_mse: 9.6111 - val_mae: 1.3933 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 11.2870 - mse: 11.2870 - mae: 1.4039 - val_loss: 9.4394 - val_mse: 9.4394 - val_mae: 1.4493 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 11.1550 - mse: 11.1550 - mae: 1.3916 - val_loss: 9.5236 - val_mse: 9.5236 - val_mae: 1.4448 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 11.0950 - mse: 11.0950 - mae: 1.3896 - val_loss: 9.5861 - val_mse: 9.5861 - val_mae: 1.3701 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 10.9759 - mse: 10.9759 - mae: 1.3806 - val_loss: 9.6153 - val_mse: 9.6153 - val_mae: 1.4519 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Score for fold 4: loss of 9.615289688110352\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 11.4255 - mse: 11.4255 - mae: 1.4017 - val_loss: 7.3458 - val_mse: 7.3458 - val_mae: 1.3815 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 11.4193 - mse: 11.4193 - mae: 1.4021 - val_loss: 7.6339 - val_mse: 7.6339 - val_mae: 1.3329 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 11.2941 - mse: 11.2941 - mae: 1.3932 - val_loss: 7.7059 - val_mse: 7.7059 - val_mae: 1.4250 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 11.2036 - mse: 11.2036 - mae: 1.3878 - val_loss: 7.8556 - val_mse: 7.8556 - val_mae: 1.3687 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 11.0398 - mse: 11.0398 - mae: 1.3839 - val_loss: 7.9175 - val_mse: 7.9175 - val_mae: 1.3604 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 10.9144 - mse: 10.9144 - mae: 1.3735 - val_loss: 7.9143 - val_mse: 7.9143 - val_mae: 1.4032 - lr: 0.0010 - 2s/epoch - 2ms/step\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-10-11 11:48:37,315]\u001b[0m Finished trial#20 resulted in value: 11.864. Current best value is 9.334 with parameters: {'activation': 'relu', 'num_hidden_layer': 4, 'i': 9, 'learning_rate': 0.00023071835273936203}.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Score for fold 5: loss of 7.914324760437012\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 11s - loss: 13.5357 - mse: 13.5357 - mae: 1.5408 - val_loss: 11.7744 - val_mse: 11.7744 - val_mae: 1.4753 - lr: 3.3766e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 10s - loss: 13.0436 - mse: 13.0436 - mae: 1.4963 - val_loss: 10.4104 - val_mse: 10.4104 - val_mae: 1.5939 - lr: 3.3766e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 11s - loss: 12.6194 - mse: 12.6194 - mae: 1.4791 - val_loss: 10.3297 - val_mse: 10.3297 - val_mae: 1.5323 - lr: 3.3766e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 11s - loss: 12.5430 - mse: 12.5430 - mae: 1.4683 - val_loss: 10.1090 - val_mse: 10.1090 - val_mae: 1.4480 - lr: 3.3766e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 11s - loss: 12.3437 - mse: 12.3437 - mae: 1.4588 - val_loss: 9.8433 - val_mse: 9.8433 - val_mae: 1.5604 - lr: 3.3766e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 11s - loss: 12.0968 - mse: 12.0968 - mae: 1.4505 - val_loss: 9.9808 - val_mse: 9.9808 - val_mae: 1.4252 - lr: 3.3766e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 10s - loss: 12.1253 - mse: 12.1253 - mae: 1.4415 - val_loss: 10.4111 - val_mse: 10.4111 - val_mae: 1.5919 - lr: 3.3766e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 11s - loss: 12.1261 - mse: 12.1261 - mae: 1.4334 - val_loss: 10.5448 - val_mse: 10.5448 - val_mae: 1.4853 - lr: 3.3766e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 11s - loss: 11.6729 - mse: 11.6729 - mae: 1.4217 - val_loss: 10.0437 - val_mse: 10.0437 - val_mae: 1.5334 - lr: 3.3766e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 11s - loss: 11.4949 - mse: 11.4949 - mae: 1.4116 - val_loss: 11.3038 - val_mse: 11.3038 - val_mae: 1.5326 - lr: 3.3766e-04 - 11s/epoch - 11ms/step\n",
            "Score for fold 1: loss of 11.303844451904297\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 11s - loss: 12.2254 - mse: 12.2254 - mae: 1.4290 - val_loss: 6.8030 - val_mse: 6.8030 - val_mae: 1.3635 - lr: 3.3766e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 11s - loss: 12.3154 - mse: 12.3154 - mae: 1.4222 - val_loss: 6.6264 - val_mse: 6.6264 - val_mae: 1.3893 - lr: 3.3766e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 10s - loss: 11.9395 - mse: 11.9395 - mae: 1.4012 - val_loss: 7.0124 - val_mse: 7.0124 - val_mae: 1.3465 - lr: 3.3766e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 11s - loss: 11.5768 - mse: 11.5768 - mae: 1.3869 - val_loss: 6.9377 - val_mse: 6.9377 - val_mae: 1.4009 - lr: 3.3766e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 11s - loss: 11.3929 - mse: 11.3929 - mae: 1.3807 - val_loss: 7.0922 - val_mse: 7.0922 - val_mae: 1.3648 - lr: 3.3766e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 11s - loss: 11.0614 - mse: 11.0614 - mae: 1.3641 - val_loss: 6.9314 - val_mse: 6.9314 - val_mae: 1.4387 - lr: 3.3766e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 11s - loss: 10.7735 - mse: 10.7735 - mae: 1.3521 - val_loss: 7.4881 - val_mse: 7.4881 - val_mae: 1.5396 - lr: 3.3766e-04 - 11s/epoch - 11ms/step\n",
            "Score for fold 2: loss of 7.48811674118042\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 10s - loss: 11.0709 - mse: 11.0709 - mae: 1.3773 - val_loss: 7.0073 - val_mse: 7.0073 - val_mae: 1.3747 - lr: 3.3766e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 11s - loss: 10.6887 - mse: 10.6887 - mae: 1.3547 - val_loss: 6.9369 - val_mse: 6.9369 - val_mae: 1.2890 - lr: 3.3766e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 10s - loss: 10.1989 - mse: 10.1989 - mae: 1.3322 - val_loss: 7.3836 - val_mse: 7.3836 - val_mae: 1.3885 - lr: 3.3766e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 10s - loss: 9.8233 - mse: 9.8233 - mae: 1.3149 - val_loss: 7.6750 - val_mse: 7.6750 - val_mae: 1.3002 - lr: 3.3766e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 10s - loss: 9.3499 - mse: 9.3499 - mae: 1.2992 - val_loss: 7.3677 - val_mse: 7.3677 - val_mae: 1.3903 - lr: 3.3766e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 10s - loss: 9.3877 - mse: 9.3877 - mae: 1.2854 - val_loss: 7.6698 - val_mse: 7.6698 - val_mae: 1.4126 - lr: 3.3766e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 11s - loss: 9.1310 - mse: 9.1310 - mae: 1.2781 - val_loss: 7.4418 - val_mse: 7.4418 - val_mae: 1.3685 - lr: 3.3766e-04 - 11s/epoch - 11ms/step\n",
            "Score for fold 3: loss of 7.441832542419434\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 11s - loss: 6.5197 - mse: 6.5197 - mae: 1.3014 - val_loss: 18.7461 - val_mse: 18.7461 - val_mae: 1.2508 - lr: 3.3766e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 11s - loss: 6.1172 - mse: 6.1172 - mae: 1.2847 - val_loss: 19.4676 - val_mse: 19.4676 - val_mae: 1.2630 - lr: 3.3766e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 10s - loss: 5.7501 - mse: 5.7501 - mae: 1.2655 - val_loss: 19.4525 - val_mse: 19.4525 - val_mae: 1.2783 - lr: 3.3766e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 10s - loss: 5.5409 - mse: 5.5409 - mae: 1.2532 - val_loss: 19.7396 - val_mse: 19.7396 - val_mae: 1.2864 - lr: 3.3766e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 11s - loss: 5.4618 - mse: 5.4618 - mae: 1.2363 - val_loss: 19.6383 - val_mse: 19.6383 - val_mae: 1.3084 - lr: 3.3766e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 10s - loss: 5.1523 - mse: 5.1523 - mae: 1.2220 - val_loss: 19.8583 - val_mse: 19.8583 - val_mae: 1.4132 - lr: 3.3766e-04 - 10s/epoch - 10ms/step\n",
            "Score for fold 4: loss of 19.85833740234375\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 11s - loss: 9.0969 - mse: 9.0969 - mae: 1.2664 - val_loss: 4.5966 - val_mse: 4.5966 - val_mae: 1.2234 - lr: 3.3766e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 11s - loss: 8.5023 - mse: 8.5023 - mae: 1.2424 - val_loss: 5.7379 - val_mse: 5.7379 - val_mae: 1.1371 - lr: 3.3766e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 11s - loss: 8.1382 - mse: 8.1382 - mae: 1.2250 - val_loss: 4.9665 - val_mse: 4.9665 - val_mae: 1.1910 - lr: 3.3766e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 11s - loss: 7.9909 - mse: 7.9909 - mae: 1.2122 - val_loss: 4.9686 - val_mse: 4.9686 - val_mae: 1.1814 - lr: 3.3766e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 10s - loss: 7.5617 - mse: 7.5617 - mae: 1.1903 - val_loss: 5.1231 - val_mse: 5.1231 - val_mae: 1.1898 - lr: 3.3766e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 10s - loss: 7.5122 - mse: 7.5122 - mae: 1.1917 - val_loss: 5.5456 - val_mse: 5.5456 - val_mae: 1.2937 - lr: 3.3766e-04 - 10s/epoch - 10ms/step\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-10-11 11:55:07,652]\u001b[0m Finished trial#21 resulted in value: 10.328. Current best value is 9.334 with parameters: {'activation': 'relu', 'num_hidden_layer': 4, 'i': 9, 'learning_rate': 0.00023071835273936203}.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Score for fold 5: loss of 5.545642375946045\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 11s - loss: 13.7543 - mse: 13.7543 - mae: 1.5389 - val_loss: 11.0861 - val_mse: 11.0861 - val_mae: 1.4510 - lr: 1.9940e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 10s - loss: 12.7460 - mse: 12.7460 - mae: 1.4806 - val_loss: 11.1344 - val_mse: 11.1344 - val_mae: 1.5132 - lr: 1.9940e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 11s - loss: 12.3989 - mse: 12.3989 - mae: 1.4708 - val_loss: 10.7246 - val_mse: 10.7246 - val_mae: 1.4817 - lr: 1.9940e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 11s - loss: 12.3680 - mse: 12.3680 - mae: 1.4582 - val_loss: 10.5998 - val_mse: 10.5998 - val_mae: 1.4929 - lr: 1.9940e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 10s - loss: 12.2734 - mse: 12.2734 - mae: 1.4485 - val_loss: 10.9642 - val_mse: 10.9642 - val_mae: 1.5049 - lr: 1.9940e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 11s - loss: 12.2878 - mse: 12.2878 - mae: 1.4442 - val_loss: 11.7740 - val_mse: 11.7740 - val_mae: 1.4687 - lr: 1.9940e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 11s - loss: 12.0299 - mse: 12.0299 - mae: 1.4326 - val_loss: 10.6898 - val_mse: 10.6898 - val_mae: 1.5739 - lr: 1.9940e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 11s - loss: 11.7191 - mse: 11.7191 - mae: 1.4237 - val_loss: 10.6802 - val_mse: 10.6802 - val_mae: 1.4394 - lr: 1.9940e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 11s - loss: 11.6893 - mse: 11.6893 - mae: 1.4104 - val_loss: 11.5674 - val_mse: 11.5674 - val_mae: 1.5260 - lr: 1.9940e-04 - 11s/epoch - 11ms/step\n",
            "Score for fold 1: loss of 11.567421913146973\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 10s - loss: 12.4071 - mse: 12.4071 - mae: 1.4261 - val_loss: 6.8532 - val_mse: 6.8532 - val_mae: 1.3640 - lr: 1.9940e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 10s - loss: 12.1859 - mse: 12.1859 - mae: 1.4150 - val_loss: 6.6734 - val_mse: 6.6734 - val_mae: 1.3514 - lr: 1.9940e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 11s - loss: 11.9184 - mse: 11.9184 - mae: 1.3983 - val_loss: 7.2306 - val_mse: 7.2306 - val_mae: 1.5033 - lr: 1.9940e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 11s - loss: 11.8081 - mse: 11.8081 - mae: 1.3867 - val_loss: 7.0071 - val_mse: 7.0071 - val_mae: 1.3165 - lr: 1.9940e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 11s - loss: 11.2408 - mse: 11.2408 - mae: 1.3691 - val_loss: 7.2279 - val_mse: 7.2279 - val_mae: 1.4377 - lr: 1.9940e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 11s - loss: 11.0312 - mse: 11.0312 - mae: 1.3470 - val_loss: 6.9566 - val_mse: 6.9566 - val_mae: 1.4395 - lr: 1.9940e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 10s - loss: 10.9342 - mse: 10.9342 - mae: 1.3290 - val_loss: 7.0593 - val_mse: 7.0593 - val_mae: 1.4182 - lr: 1.9940e-04 - 10s/epoch - 10ms/step\n",
            "Score for fold 2: loss of 7.059286594390869\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 11s - loss: 10.2722 - mse: 10.2722 - mae: 1.3425 - val_loss: 8.1060 - val_mse: 8.1060 - val_mae: 1.3179 - lr: 1.9940e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 11s - loss: 10.0411 - mse: 10.0411 - mae: 1.3196 - val_loss: 7.8423 - val_mse: 7.8423 - val_mae: 1.3399 - lr: 1.9940e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 11s - loss: 9.7902 - mse: 9.7902 - mae: 1.3097 - val_loss: 8.3714 - val_mse: 8.3714 - val_mae: 1.4369 - lr: 1.9940e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 11s - loss: 9.5282 - mse: 9.5282 - mae: 1.2917 - val_loss: 8.3310 - val_mse: 8.3310 - val_mae: 1.3864 - lr: 1.9940e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 10s - loss: 8.8596 - mse: 8.8596 - mae: 1.2615 - val_loss: 8.7525 - val_mse: 8.7525 - val_mae: 1.4252 - lr: 1.9940e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 10s - loss: 8.6763 - mse: 8.6763 - mae: 1.2486 - val_loss: 8.3276 - val_mse: 8.3276 - val_mae: 1.3339 - lr: 1.9940e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 11s - loss: 8.1782 - mse: 8.1782 - mae: 1.2203 - val_loss: 8.9910 - val_mse: 8.9910 - val_mae: 1.3391 - lr: 1.9940e-04 - 11s/epoch - 11ms/step\n",
            "Score for fold 3: loss of 8.991007804870605\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 11s - loss: 9.0786 - mse: 9.0786 - mae: 1.2866 - val_loss: 6.1221 - val_mse: 6.1221 - val_mae: 1.1865 - lr: 1.9940e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 11s - loss: 8.7341 - mse: 8.7341 - mae: 1.2615 - val_loss: 7.1985 - val_mse: 7.1985 - val_mae: 1.1498 - lr: 1.9940e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 11s - loss: 8.6105 - mse: 8.6105 - mae: 1.2361 - val_loss: 6.2283 - val_mse: 6.2283 - val_mae: 1.2072 - lr: 1.9940e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 11s - loss: 8.0736 - mse: 8.0736 - mae: 1.2236 - val_loss: 6.4142 - val_mse: 6.4142 - val_mae: 1.2230 - lr: 1.9940e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 11s - loss: 7.5640 - mse: 7.5640 - mae: 1.1968 - val_loss: 6.4131 - val_mse: 6.4131 - val_mae: 1.2280 - lr: 1.9940e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 11s - loss: 7.3798 - mse: 7.3798 - mae: 1.1826 - val_loss: 6.6833 - val_mse: 6.6833 - val_mae: 1.2462 - lr: 1.9940e-04 - 11s/epoch - 11ms/step\n",
            "Score for fold 4: loss of 6.68325138092041\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 11s - loss: 5.3690 - mse: 5.3690 - mae: 1.1964 - val_loss: 15.0663 - val_mse: 15.0663 - val_mae: 1.1065 - lr: 1.9940e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 11s - loss: 4.9674 - mse: 4.9674 - mae: 1.1741 - val_loss: 14.2635 - val_mse: 14.2635 - val_mae: 1.3256 - lr: 1.9940e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 11s - loss: 4.5570 - mse: 4.5570 - mae: 1.1554 - val_loss: 16.8022 - val_mse: 16.8022 - val_mae: 1.3873 - lr: 1.9940e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 10s - loss: 4.5197 - mse: 4.5197 - mae: 1.1363 - val_loss: 14.4953 - val_mse: 14.4953 - val_mae: 1.1873 - lr: 1.9940e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 11s - loss: 4.3761 - mse: 4.3761 - mae: 1.1208 - val_loss: 15.1244 - val_mse: 15.1244 - val_mae: 1.2020 - lr: 1.9940e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 11s - loss: 4.1785 - mse: 4.1785 - mae: 1.1060 - val_loss: 15.0515 - val_mse: 15.0515 - val_mae: 1.3437 - lr: 1.9940e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 11s - loss: 3.8487 - mse: 3.8487 - mae: 1.0926 - val_loss: 15.9286 - val_mse: 15.9286 - val_mae: 1.2024 - lr: 1.9940e-04 - 11s/epoch - 11ms/step\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-10-11 12:01:40,059]\u001b[0m Finished trial#22 resulted in value: 10.046. Current best value is 9.334 with parameters: {'activation': 'relu', 'num_hidden_layer': 4, 'i': 9, 'learning_rate': 0.00023071835273936203}.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Score for fold 5: loss of 15.928613662719727\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 7s - loss: 11.4954 - mse: 11.4954 - mae: 1.5226 - val_loss: 19.2346 - val_mse: 19.2346 - val_mae: 1.5349 - lr: 1.6462e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 6s - loss: 10.5907 - mse: 10.5907 - mae: 1.4594 - val_loss: 19.6356 - val_mse: 19.6356 - val_mae: 1.5268 - lr: 1.6462e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 6s - loss: 10.3402 - mse: 10.3402 - mae: 1.4469 - val_loss: 19.2490 - val_mse: 19.2490 - val_mae: 1.5220 - lr: 1.6462e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 6s - loss: 10.3259 - mse: 10.3259 - mae: 1.4357 - val_loss: 18.9950 - val_mse: 18.9950 - val_mae: 1.5258 - lr: 1.6462e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 6s - loss: 10.0737 - mse: 10.0737 - mae: 1.4249 - val_loss: 19.2086 - val_mse: 19.2086 - val_mae: 1.5195 - lr: 1.6462e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 6s - loss: 10.0006 - mse: 10.0006 - mae: 1.4201 - val_loss: 19.0960 - val_mse: 19.0960 - val_mae: 1.5849 - lr: 1.6462e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 6s - loss: 9.9523 - mse: 9.9523 - mae: 1.4107 - val_loss: 19.0062 - val_mse: 19.0062 - val_mae: 1.5401 - lr: 1.6462e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 6s - loss: 9.8834 - mse: 9.8834 - mae: 1.4015 - val_loss: 19.3952 - val_mse: 19.3952 - val_mae: 1.4900 - lr: 1.6462e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 6s - loss: 9.5596 - mse: 9.5596 - mae: 1.3945 - val_loss: 19.5218 - val_mse: 19.5218 - val_mae: 1.5249 - lr: 1.6462e-04 - 6s/epoch - 6ms/step\n",
            "Score for fold 1: loss of 19.521780014038086\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 6s - loss: 11.4927 - mse: 11.4927 - mae: 1.4281 - val_loss: 12.3363 - val_mse: 12.3363 - val_mae: 1.4044 - lr: 1.6462e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 6s - loss: 11.2017 - mse: 11.2017 - mae: 1.4190 - val_loss: 11.3377 - val_mse: 11.3377 - val_mae: 1.4550 - lr: 1.6462e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 6s - loss: 11.0326 - mse: 11.0326 - mae: 1.4107 - val_loss: 11.6019 - val_mse: 11.6019 - val_mae: 1.4008 - lr: 1.6462e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 6s - loss: 10.8936 - mse: 10.8936 - mae: 1.3958 - val_loss: 11.9219 - val_mse: 11.9219 - val_mae: 1.4157 - lr: 1.6462e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 6s - loss: 10.7604 - mse: 10.7604 - mae: 1.3853 - val_loss: 11.5083 - val_mse: 11.5083 - val_mae: 1.4077 - lr: 1.6462e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 7s - loss: 10.7163 - mse: 10.7163 - mae: 1.3766 - val_loss: 11.8872 - val_mse: 11.8872 - val_mae: 1.3830 - lr: 1.6462e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 7s - loss: 10.3916 - mse: 10.3916 - mae: 1.3596 - val_loss: 12.9245 - val_mse: 12.9245 - val_mae: 1.4198 - lr: 1.6462e-04 - 7s/epoch - 7ms/step\n",
            "Score for fold 2: loss of 12.92446231842041\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 6s - loss: 11.0564 - mse: 11.0564 - mae: 1.3790 - val_loss: 7.4829 - val_mse: 7.4829 - val_mae: 1.3463 - lr: 1.6462e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 6s - loss: 10.8933 - mse: 10.8933 - mae: 1.3644 - val_loss: 7.8731 - val_mse: 7.8731 - val_mae: 1.3359 - lr: 1.6462e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 6s - loss: 10.5776 - mse: 10.5776 - mae: 1.3490 - val_loss: 7.9464 - val_mse: 7.9464 - val_mae: 1.3557 - lr: 1.6462e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 6s - loss: 10.4617 - mse: 10.4617 - mae: 1.3354 - val_loss: 7.8229 - val_mse: 7.8229 - val_mae: 1.3672 - lr: 1.6462e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 6s - loss: 10.4486 - mse: 10.4486 - mae: 1.3183 - val_loss: 8.4822 - val_mse: 8.4822 - val_mae: 1.3704 - lr: 1.6462e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 6s - loss: 9.9556 - mse: 9.9556 - mae: 1.3033 - val_loss: 8.3513 - val_mse: 8.3513 - val_mae: 1.3940 - lr: 1.6462e-04 - 6s/epoch - 6ms/step\n",
            "Score for fold 3: loss of 8.351333618164062\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 6s - loss: 9.4595 - mse: 9.4595 - mae: 1.3209 - val_loss: 8.8912 - val_mse: 8.8912 - val_mae: 1.2736 - lr: 1.6462e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 6s - loss: 9.6440 - mse: 9.6440 - mae: 1.2979 - val_loss: 9.2216 - val_mse: 9.2216 - val_mae: 1.2972 - lr: 1.6462e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 6s - loss: 9.1294 - mse: 9.1294 - mae: 1.2774 - val_loss: 9.4398 - val_mse: 9.4398 - val_mae: 1.3641 - lr: 1.6462e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 6s - loss: 8.7255 - mse: 8.7255 - mae: 1.2596 - val_loss: 9.3157 - val_mse: 9.3157 - val_mae: 1.3031 - lr: 1.6462e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 6s - loss: 8.6539 - mse: 8.6539 - mae: 1.2396 - val_loss: 9.3243 - val_mse: 9.3243 - val_mae: 1.3751 - lr: 1.6462e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 6s - loss: 8.3827 - mse: 8.3827 - mae: 1.2294 - val_loss: 9.6255 - val_mse: 9.6255 - val_mae: 1.3411 - lr: 1.6462e-04 - 6s/epoch - 6ms/step\n",
            "Score for fold 4: loss of 9.62553882598877\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 6s - loss: 9.8466 - mse: 9.8466 - mae: 1.2750 - val_loss: 3.8608 - val_mse: 3.8608 - val_mae: 1.2171 - lr: 1.6462e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 6s - loss: 9.1618 - mse: 9.1618 - mae: 1.2494 - val_loss: 3.9443 - val_mse: 3.9443 - val_mae: 1.1343 - lr: 1.6462e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 6s - loss: 9.1322 - mse: 9.1322 - mae: 1.2375 - val_loss: 4.3096 - val_mse: 4.3096 - val_mae: 1.1897 - lr: 1.6462e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 6s - loss: 8.8124 - mse: 8.8124 - mae: 1.2206 - val_loss: 4.2591 - val_mse: 4.2591 - val_mae: 1.2412 - lr: 1.6462e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 6s - loss: 8.7306 - mse: 8.7306 - mae: 1.2072 - val_loss: 4.7720 - val_mse: 4.7720 - val_mae: 1.2148 - lr: 1.6462e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 6s - loss: 8.1444 - mse: 8.1444 - mae: 1.1943 - val_loss: 4.6525 - val_mse: 4.6525 - val_mae: 1.2620 - lr: 1.6462e-04 - 6s/epoch - 6ms/step\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-10-11 12:05:19,861]\u001b[0m Finished trial#23 resulted in value: 11.014. Current best value is 9.334 with parameters: {'activation': 'relu', 'num_hidden_layer': 4, 'i': 9, 'learning_rate': 0.00023071835273936203}.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Score for fold 5: loss of 4.652510643005371\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 11s - loss: 13.9885 - mse: 13.9885 - mae: 1.5506 - val_loss: 9.5672 - val_mse: 9.5672 - val_mae: 1.6167 - lr: 2.2764e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 11s - loss: 13.0517 - mse: 13.0517 - mae: 1.4899 - val_loss: 9.8202 - val_mse: 9.8202 - val_mae: 1.5093 - lr: 2.2764e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 10s - loss: 12.7954 - mse: 12.7954 - mae: 1.4751 - val_loss: 9.8116 - val_mse: 9.8116 - val_mae: 1.4296 - lr: 2.2764e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 11s - loss: 12.6520 - mse: 12.6520 - mae: 1.4679 - val_loss: 9.7384 - val_mse: 9.7384 - val_mae: 1.3818 - lr: 2.2764e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 11s - loss: 12.5888 - mse: 12.5888 - mae: 1.4577 - val_loss: 9.7355 - val_mse: 9.7355 - val_mae: 1.4590 - lr: 2.2764e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 11s - loss: 12.2719 - mse: 12.2719 - mae: 1.4443 - val_loss: 9.6423 - val_mse: 9.6423 - val_mae: 1.4542 - lr: 2.2764e-04 - 11s/epoch - 11ms/step\n",
            "Score for fold 1: loss of 9.642273902893066\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 11s - loss: 11.9913 - mse: 11.9913 - mae: 1.4523 - val_loss: 11.0185 - val_mse: 11.0185 - val_mae: 1.4318 - lr: 2.2764e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 11s - loss: 11.8056 - mse: 11.8056 - mae: 1.4400 - val_loss: 10.9455 - val_mse: 10.9455 - val_mae: 1.3853 - lr: 2.2764e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 11s - loss: 11.3977 - mse: 11.3977 - mae: 1.4250 - val_loss: 11.4406 - val_mse: 11.4406 - val_mae: 1.5163 - lr: 2.2764e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 11s - loss: 11.2141 - mse: 11.2141 - mae: 1.4114 - val_loss: 11.2171 - val_mse: 11.2171 - val_mae: 1.3971 - lr: 2.2764e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 10s - loss: 10.7964 - mse: 10.7964 - mae: 1.3943 - val_loss: 10.8137 - val_mse: 10.8137 - val_mae: 1.3808 - lr: 2.2764e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 11s - loss: 10.6028 - mse: 10.6028 - mae: 1.3818 - val_loss: 11.5180 - val_mse: 11.5180 - val_mae: 1.4387 - lr: 2.2764e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 11s - loss: 10.2499 - mse: 10.2499 - mae: 1.3678 - val_loss: 10.9889 - val_mse: 10.9889 - val_mae: 1.4008 - lr: 2.2764e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 11s - loss: 10.2457 - mse: 10.2457 - mae: 1.3648 - val_loss: 10.9227 - val_mse: 10.9227 - val_mae: 1.3986 - lr: 2.2764e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 11s - loss: 9.7905 - mse: 9.7905 - mae: 1.3322 - val_loss: 11.2047 - val_mse: 11.2047 - val_mae: 1.4134 - lr: 2.2764e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 11s - loss: 9.4708 - mse: 9.4708 - mae: 1.3166 - val_loss: 11.6928 - val_mse: 11.6928 - val_mae: 1.5465 - lr: 2.2764e-04 - 11s/epoch - 11ms/step\n",
            "Score for fold 2: loss of 11.692798614501953\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 11s - loss: 9.0231 - mse: 9.0231 - mae: 1.3580 - val_loss: 14.4058 - val_mse: 14.4058 - val_mae: 1.3070 - lr: 2.2764e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 11s - loss: 8.5791 - mse: 8.5791 - mae: 1.3327 - val_loss: 14.9786 - val_mse: 14.9786 - val_mae: 1.2671 - lr: 2.2764e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 11s - loss: 8.2154 - mse: 8.2154 - mae: 1.3086 - val_loss: 15.1183 - val_mse: 15.1183 - val_mae: 1.3774 - lr: 2.2764e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 11s - loss: 7.7708 - mse: 7.7708 - mae: 1.2916 - val_loss: 14.8603 - val_mse: 14.8603 - val_mae: 1.5635 - lr: 2.2764e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 11s - loss: 8.1765 - mse: 8.1765 - mae: 1.2784 - val_loss: 15.0690 - val_mse: 15.0690 - val_mae: 1.3491 - lr: 2.2764e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 11s - loss: 7.3438 - mse: 7.3438 - mae: 1.2487 - val_loss: 15.0711 - val_mse: 15.0711 - val_mae: 1.3682 - lr: 2.2764e-04 - 11s/epoch - 11ms/step\n",
            "Score for fold 3: loss of 15.071066856384277\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 11s - loss: 9.6617 - mse: 9.6617 - mae: 1.2904 - val_loss: 4.9655 - val_mse: 4.9655 - val_mae: 1.1776 - lr: 2.2764e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 11s - loss: 9.1005 - mse: 9.1005 - mae: 1.2695 - val_loss: 5.1927 - val_mse: 5.1927 - val_mae: 1.2575 - lr: 2.2764e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 11s - loss: 9.0304 - mse: 9.0304 - mae: 1.2476 - val_loss: 5.5835 - val_mse: 5.5835 - val_mae: 1.3628 - lr: 2.2764e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 11s - loss: 8.7856 - mse: 8.7856 - mae: 1.2343 - val_loss: 5.4120 - val_mse: 5.4120 - val_mae: 1.2689 - lr: 2.2764e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 11s - loss: 8.4250 - mse: 8.4250 - mae: 1.2170 - val_loss: 5.7957 - val_mse: 5.7957 - val_mae: 1.2469 - lr: 2.2764e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 11s - loss: 8.2546 - mse: 8.2546 - mae: 1.1998 - val_loss: 5.7158 - val_mse: 5.7158 - val_mae: 1.3407 - lr: 2.2764e-04 - 11s/epoch - 11ms/step\n",
            "Score for fold 4: loss of 5.715829849243164\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 11s - loss: 7.3447 - mse: 7.3447 - mae: 1.2168 - val_loss: 7.2850 - val_mse: 7.2850 - val_mae: 1.2404 - lr: 2.2764e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 11s - loss: 7.3413 - mse: 7.3413 - mae: 1.1924 - val_loss: 8.3777 - val_mse: 8.3777 - val_mae: 1.1395 - lr: 2.2764e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 11s - loss: 6.7064 - mse: 6.7064 - mae: 1.1615 - val_loss: 7.7977 - val_mse: 7.7977 - val_mae: 1.2806 - lr: 2.2764e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 11s - loss: 6.6379 - mse: 6.6379 - mae: 1.1535 - val_loss: 8.8126 - val_mse: 8.8126 - val_mae: 1.2541 - lr: 2.2764e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 11s - loss: 6.3119 - mse: 6.3119 - mae: 1.1390 - val_loss: 8.6896 - val_mse: 8.6896 - val_mae: 1.1855 - lr: 2.2764e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 11s - loss: 6.2642 - mse: 6.2642 - mae: 1.1247 - val_loss: 8.1381 - val_mse: 8.1381 - val_mae: 1.2320 - lr: 2.2764e-04 - 11s/epoch - 11ms/step\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-10-11 12:11:37,535]\u001b[0m Finished trial#24 resulted in value: 10.052. Current best value is 9.334 with parameters: {'activation': 'relu', 'num_hidden_layer': 4, 'i': 9, 'learning_rate': 0.00023071835273936203}.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Score for fold 5: loss of 8.138082504272461\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 7s - loss: 13.4930 - mse: 13.4930 - mae: 1.5412 - val_loss: 11.2633 - val_mse: 11.2633 - val_mae: 1.5310 - lr: 2.1756e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 6s - loss: 12.5833 - mse: 12.5833 - mae: 1.4869 - val_loss: 11.6224 - val_mse: 11.6224 - val_mae: 1.4216 - lr: 2.1756e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 6s - loss: 12.2755 - mse: 12.2755 - mae: 1.4630 - val_loss: 11.0910 - val_mse: 11.0910 - val_mae: 1.5777 - lr: 2.1756e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 6s - loss: 12.2955 - mse: 12.2955 - mae: 1.4584 - val_loss: 11.5785 - val_mse: 11.5785 - val_mae: 1.4653 - lr: 2.1756e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 6s - loss: 12.0636 - mse: 12.0636 - mae: 1.4463 - val_loss: 11.9825 - val_mse: 11.9825 - val_mae: 1.4756 - lr: 2.1756e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 6s - loss: 11.9476 - mse: 11.9476 - mae: 1.4397 - val_loss: 12.4504 - val_mse: 12.4504 - val_mae: 1.4675 - lr: 2.1756e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 6s - loss: 11.7646 - mse: 11.7646 - mae: 1.4285 - val_loss: 11.0369 - val_mse: 11.0369 - val_mae: 1.4869 - lr: 2.1756e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 6s - loss: 11.5721 - mse: 11.5721 - mae: 1.4159 - val_loss: 11.3187 - val_mse: 11.3187 - val_mae: 1.4713 - lr: 2.1756e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 6s - loss: 11.5234 - mse: 11.5234 - mae: 1.4129 - val_loss: 11.9857 - val_mse: 11.9857 - val_mae: 1.4518 - lr: 2.1756e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 6s - loss: 11.2804 - mse: 11.2804 - mae: 1.3985 - val_loss: 11.4620 - val_mse: 11.4620 - val_mae: 1.4828 - lr: 2.1756e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 6s - loss: 11.0881 - mse: 11.0881 - mae: 1.3857 - val_loss: 11.7987 - val_mse: 11.7987 - val_mae: 1.5673 - lr: 2.1756e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 6s - loss: 10.8858 - mse: 10.8858 - mae: 1.3725 - val_loss: 11.9801 - val_mse: 11.9801 - val_mae: 1.5489 - lr: 2.1756e-04 - 6s/epoch - 6ms/step\n",
            "Score for fold 1: loss of 11.980088233947754\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 6s - loss: 10.6165 - mse: 10.6165 - mae: 1.3923 - val_loss: 11.6341 - val_mse: 11.6341 - val_mae: 1.3754 - lr: 2.1756e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 6s - loss: 10.3553 - mse: 10.3553 - mae: 1.3814 - val_loss: 11.9081 - val_mse: 11.9081 - val_mae: 1.3915 - lr: 2.1756e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 6s - loss: 10.0573 - mse: 10.0573 - mae: 1.3606 - val_loss: 12.0880 - val_mse: 12.0880 - val_mae: 1.3710 - lr: 2.1756e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 6s - loss: 9.6357 - mse: 9.6357 - mae: 1.3449 - val_loss: 12.1047 - val_mse: 12.1047 - val_mae: 1.4009 - lr: 2.1756e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 6s - loss: 9.4299 - mse: 9.4299 - mae: 1.3292 - val_loss: 12.6870 - val_mse: 12.6870 - val_mae: 1.3943 - lr: 2.1756e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 6s - loss: 9.2514 - mse: 9.2514 - mae: 1.3100 - val_loss: 12.2945 - val_mse: 12.2945 - val_mae: 1.4867 - lr: 2.1756e-04 - 6s/epoch - 6ms/step\n",
            "Score for fold 2: loss of 12.294537544250488\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 7s - loss: 10.8933 - mse: 10.8933 - mae: 1.3529 - val_loss: 6.0576 - val_mse: 6.0576 - val_mae: 1.3247 - lr: 2.1756e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 7s - loss: 10.4951 - mse: 10.4951 - mae: 1.3269 - val_loss: 6.0582 - val_mse: 6.0582 - val_mae: 1.3872 - lr: 2.1756e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 6s - loss: 10.4492 - mse: 10.4492 - mae: 1.3080 - val_loss: 6.1418 - val_mse: 6.1418 - val_mae: 1.3651 - lr: 2.1756e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 6s - loss: 9.8614 - mse: 9.8614 - mae: 1.2901 - val_loss: 6.4115 - val_mse: 6.4115 - val_mae: 1.3327 - lr: 2.1756e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 6s - loss: 9.6258 - mse: 9.6258 - mae: 1.2761 - val_loss: 6.4327 - val_mse: 6.4327 - val_mae: 1.3711 - lr: 2.1756e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 6s - loss: 9.2402 - mse: 9.2402 - mae: 1.2595 - val_loss: 6.5237 - val_mse: 6.5237 - val_mae: 1.3652 - lr: 2.1756e-04 - 6s/epoch - 6ms/step\n",
            "Score for fold 3: loss of 6.523678302764893\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 6s - loss: 9.8482 - mse: 9.8482 - mae: 1.2927 - val_loss: 5.9722 - val_mse: 5.9722 - val_mae: 1.2498 - lr: 2.1756e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 6s - loss: 9.1635 - mse: 9.1635 - mae: 1.2652 - val_loss: 6.2385 - val_mse: 6.2385 - val_mae: 1.3070 - lr: 2.1756e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 6s - loss: 8.8707 - mse: 8.8707 - mae: 1.2592 - val_loss: 6.5470 - val_mse: 6.5470 - val_mae: 1.2955 - lr: 2.1756e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 6s - loss: 8.8845 - mse: 8.8845 - mae: 1.2357 - val_loss: 6.5230 - val_mse: 6.5230 - val_mae: 1.2354 - lr: 2.1756e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 6s - loss: 8.5200 - mse: 8.5200 - mae: 1.2205 - val_loss: 6.6926 - val_mse: 6.6926 - val_mae: 1.2972 - lr: 2.1756e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 6s - loss: 8.0745 - mse: 8.0745 - mae: 1.2085 - val_loss: 7.0357 - val_mse: 7.0357 - val_mae: 1.2805 - lr: 2.1756e-04 - 6s/epoch - 6ms/step\n",
            "Score for fold 4: loss of 7.035743713378906\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 6s - loss: 6.8555 - mse: 6.8555 - mae: 1.2328 - val_loss: 11.7880 - val_mse: 11.7880 - val_mae: 1.1748 - lr: 2.1756e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 6s - loss: 6.4962 - mse: 6.4962 - mae: 1.2089 - val_loss: 11.6756 - val_mse: 11.6756 - val_mae: 1.1619 - lr: 2.1756e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 6s - loss: 6.3434 - mse: 6.3434 - mae: 1.1875 - val_loss: 12.8340 - val_mse: 12.8340 - val_mae: 1.1452 - lr: 2.1756e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 6s - loss: 6.0768 - mse: 6.0768 - mae: 1.1764 - val_loss: 12.3418 - val_mse: 12.3418 - val_mae: 1.2132 - lr: 2.1756e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 6s - loss: 6.0338 - mse: 6.0338 - mae: 1.1665 - val_loss: 12.3211 - val_mse: 12.3211 - val_mae: 1.2500 - lr: 2.1756e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 6s - loss: 5.7834 - mse: 5.7834 - mae: 1.1443 - val_loss: 12.2619 - val_mse: 12.2619 - val_mae: 1.2305 - lr: 2.1756e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 6s - loss: 5.5241 - mse: 5.5241 - mae: 1.1313 - val_loss: 12.6732 - val_mse: 12.6732 - val_mae: 1.2258 - lr: 2.1756e-04 - 6s/epoch - 6ms/step\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-10-11 12:15:38,442]\u001b[0m Finished trial#25 resulted in value: 10.1. Current best value is 9.334 with parameters: {'activation': 'relu', 'num_hidden_layer': 4, 'i': 9, 'learning_rate': 0.00023071835273936203}.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Score for fold 5: loss of 12.6731538772583\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 8s - loss: 13.4529 - mse: 13.4529 - mae: 1.5322 - val_loss: 10.8856 - val_mse: 10.8856 - val_mae: 1.5147 - lr: 4.3163e-04 - 8s/epoch - 8ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 7s - loss: 12.7363 - mse: 12.7363 - mae: 1.4735 - val_loss: 10.7109 - val_mse: 10.7109 - val_mae: 1.5032 - lr: 4.3163e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 7s - loss: 12.5900 - mse: 12.5900 - mae: 1.4646 - val_loss: 10.1474 - val_mse: 10.1474 - val_mae: 1.4407 - lr: 4.3163e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 7s - loss: 12.3909 - mse: 12.3909 - mae: 1.4547 - val_loss: 10.5890 - val_mse: 10.5890 - val_mae: 1.4774 - lr: 4.3163e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 7s - loss: 12.4869 - mse: 12.4869 - mae: 1.4467 - val_loss: 10.4062 - val_mse: 10.4062 - val_mae: 1.4826 - lr: 4.3163e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 7s - loss: 12.1438 - mse: 12.1438 - mae: 1.4382 - val_loss: 9.9336 - val_mse: 9.9336 - val_mae: 1.4645 - lr: 4.3163e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 7s - loss: 12.0274 - mse: 12.0274 - mae: 1.4290 - val_loss: 9.9372 - val_mse: 9.9372 - val_mae: 1.5499 - lr: 4.3163e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 7s - loss: 12.0246 - mse: 12.0246 - mae: 1.4247 - val_loss: 10.2231 - val_mse: 10.2231 - val_mae: 1.5511 - lr: 4.3163e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 7s - loss: 11.9720 - mse: 11.9720 - mae: 1.4196 - val_loss: 10.4425 - val_mse: 10.4425 - val_mae: 1.4495 - lr: 4.3163e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 7s - loss: 11.8020 - mse: 11.8020 - mae: 1.4084 - val_loss: 9.9350 - val_mse: 9.9350 - val_mae: 1.4435 - lr: 4.3163e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 7s - loss: 11.4932 - mse: 11.4932 - mae: 1.4009 - val_loss: 10.1717 - val_mse: 10.1717 - val_mae: 1.4582 - lr: 4.3163e-04 - 7s/epoch - 7ms/step\n",
            "Score for fold 1: loss of 10.171744346618652\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 7s - loss: 9.7918 - mse: 9.7918 - mae: 1.4219 - val_loss: 17.6514 - val_mse: 17.6514 - val_mae: 1.4200 - lr: 4.3163e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 7s - loss: 9.9562 - mse: 9.9562 - mae: 1.4101 - val_loss: 17.5051 - val_mse: 17.5051 - val_mae: 1.4883 - lr: 4.3163e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 7s - loss: 9.5617 - mse: 9.5617 - mae: 1.4034 - val_loss: 17.8395 - val_mse: 17.8395 - val_mae: 1.4785 - lr: 4.3163e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 7s - loss: 9.1885 - mse: 9.1885 - mae: 1.3916 - val_loss: 17.3167 - val_mse: 17.3167 - val_mae: 1.4778 - lr: 4.3163e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 7s - loss: 9.2135 - mse: 9.2135 - mae: 1.3833 - val_loss: 17.7774 - val_mse: 17.7774 - val_mae: 1.3942 - lr: 4.3163e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 7s - loss: 9.2307 - mse: 9.2307 - mae: 1.3725 - val_loss: 17.6595 - val_mse: 17.6595 - val_mae: 1.4428 - lr: 4.3163e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 7s - loss: 8.7376 - mse: 8.7376 - mae: 1.3649 - val_loss: 17.7386 - val_mse: 17.7386 - val_mae: 1.4318 - lr: 4.3163e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 7s - loss: 8.6738 - mse: 8.6738 - mae: 1.3508 - val_loss: 17.6482 - val_mse: 17.6482 - val_mae: 1.4880 - lr: 4.3163e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 7s - loss: 8.4380 - mse: 8.4380 - mae: 1.3415 - val_loss: 17.4529 - val_mse: 17.4529 - val_mae: 1.4138 - lr: 4.3163e-04 - 7s/epoch - 7ms/step\n",
            "Score for fold 2: loss of 17.452838897705078\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 7s - loss: 11.0952 - mse: 11.0952 - mae: 1.3643 - val_loss: 7.4104 - val_mse: 7.4104 - val_mae: 1.3059 - lr: 4.3163e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 7s - loss: 10.8985 - mse: 10.8985 - mae: 1.3499 - val_loss: 7.6706 - val_mse: 7.6706 - val_mae: 1.3107 - lr: 4.3163e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 7s - loss: 10.6342 - mse: 10.6342 - mae: 1.3334 - val_loss: 6.8571 - val_mse: 6.8571 - val_mae: 1.3471 - lr: 4.3163e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 7s - loss: 10.4696 - mse: 10.4696 - mae: 1.3184 - val_loss: 6.7591 - val_mse: 6.7591 - val_mae: 1.3481 - lr: 4.3163e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 7s - loss: 10.2550 - mse: 10.2550 - mae: 1.3075 - val_loss: 8.0704 - val_mse: 8.0704 - val_mae: 1.3899 - lr: 4.3163e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 7s - loss: 10.2790 - mse: 10.2790 - mae: 1.2928 - val_loss: 7.5627 - val_mse: 7.5627 - val_mae: 1.4068 - lr: 4.3163e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 7s - loss: 9.7962 - mse: 9.7962 - mae: 1.2799 - val_loss: 7.1329 - val_mse: 7.1329 - val_mae: 1.3612 - lr: 4.3163e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 7s - loss: 9.7071 - mse: 9.7071 - mae: 1.2660 - val_loss: 8.6933 - val_mse: 8.6933 - val_mae: 1.3591 - lr: 4.3163e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 7s - loss: 9.6621 - mse: 9.6621 - mae: 1.2519 - val_loss: 7.0985 - val_mse: 7.0985 - val_mae: 1.4228 - lr: 4.3163e-04 - 7s/epoch - 7ms/step\n",
            "Score for fold 3: loss of 7.098527908325195\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 7s - loss: 10.0162 - mse: 10.0162 - mae: 1.2902 - val_loss: 5.2314 - val_mse: 5.2314 - val_mae: 1.2146 - lr: 4.3163e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 7s - loss: 9.6060 - mse: 9.6060 - mae: 1.2644 - val_loss: 5.3533 - val_mse: 5.3533 - val_mae: 1.2285 - lr: 4.3163e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 7s - loss: 9.2903 - mse: 9.2903 - mae: 1.2413 - val_loss: 5.5199 - val_mse: 5.5199 - val_mae: 1.2403 - lr: 4.3163e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 7s - loss: 9.1847 - mse: 9.1847 - mae: 1.2245 - val_loss: 5.7769 - val_mse: 5.7769 - val_mae: 1.2591 - lr: 4.3163e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 7s - loss: 8.9086 - mse: 8.9086 - mae: 1.2082 - val_loss: 5.8124 - val_mse: 5.8124 - val_mae: 1.2744 - lr: 4.3163e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 7s - loss: 8.8955 - mse: 8.8955 - mae: 1.1936 - val_loss: 5.8952 - val_mse: 5.8952 - val_mae: 1.2527 - lr: 4.3163e-04 - 7s/epoch - 7ms/step\n",
            "Score for fold 4: loss of 5.89517068862915\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 7s - loss: 7.9710 - mse: 7.9710 - mae: 1.2123 - val_loss: 9.3758 - val_mse: 9.3758 - val_mae: 1.1717 - lr: 4.3163e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 7s - loss: 7.5827 - mse: 7.5827 - mae: 1.1879 - val_loss: 9.6638 - val_mse: 9.6638 - val_mae: 1.2033 - lr: 4.3163e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 7s - loss: 7.2328 - mse: 7.2328 - mae: 1.1628 - val_loss: 9.6560 - val_mse: 9.6560 - val_mae: 1.2095 - lr: 4.3163e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 7s - loss: 6.9254 - mse: 6.9254 - mae: 1.1475 - val_loss: 9.7821 - val_mse: 9.7821 - val_mae: 1.2019 - lr: 4.3163e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 7s - loss: 7.0197 - mse: 7.0197 - mae: 1.1299 - val_loss: 9.8473 - val_mse: 9.8473 - val_mae: 1.2043 - lr: 4.3163e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 7s - loss: 6.5671 - mse: 6.5671 - mae: 1.1065 - val_loss: 10.0834 - val_mse: 10.0834 - val_mae: 1.2550 - lr: 4.3163e-04 - 7s/epoch - 7ms/step\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-10-11 12:20:29,516]\u001b[0m Finished trial#26 resulted in value: 10.139999999999999. Current best value is 9.334 with parameters: {'activation': 'relu', 'num_hidden_layer': 4, 'i': 9, 'learning_rate': 0.00023071835273936203}.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Score for fold 5: loss of 10.083356857299805\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 7s - loss: 14.3491 - mse: 14.3491 - mae: 1.5525 - val_loss: 9.2557 - val_mse: 9.2557 - val_mae: 1.5015 - lr: 1.0801e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 6s - loss: 13.4133 - mse: 13.4133 - mae: 1.4945 - val_loss: 8.7612 - val_mse: 8.7612 - val_mae: 1.5379 - lr: 1.0801e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 6s - loss: 13.0928 - mse: 13.0928 - mae: 1.4744 - val_loss: 9.0230 - val_mse: 9.0230 - val_mae: 1.5274 - lr: 1.0801e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 6s - loss: 12.8206 - mse: 12.8206 - mae: 1.4649 - val_loss: 9.7763 - val_mse: 9.7763 - val_mae: 1.4323 - lr: 1.0801e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 6s - loss: 12.7348 - mse: 12.7348 - mae: 1.4523 - val_loss: 9.3959 - val_mse: 9.3959 - val_mae: 1.4850 - lr: 1.0801e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 6s - loss: 12.5013 - mse: 12.5013 - mae: 1.4447 - val_loss: 9.3666 - val_mse: 9.3666 - val_mae: 1.5156 - lr: 1.0801e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 6s - loss: 12.3369 - mse: 12.3369 - mae: 1.4367 - val_loss: 9.0127 - val_mse: 9.0127 - val_mae: 1.4014 - lr: 1.0801e-04 - 6s/epoch - 6ms/step\n",
            "Score for fold 1: loss of 9.012737274169922\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 6s - loss: 12.5774 - mse: 12.5774 - mae: 1.4493 - val_loss: 8.3355 - val_mse: 8.3355 - val_mae: 1.3732 - lr: 1.0801e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 6s - loss: 12.4494 - mse: 12.4494 - mae: 1.4462 - val_loss: 8.5313 - val_mse: 8.5313 - val_mae: 1.3523 - lr: 1.0801e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 6s - loss: 12.1722 - mse: 12.1722 - mae: 1.4358 - val_loss: 8.5349 - val_mse: 8.5349 - val_mae: 1.4059 - lr: 1.0801e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 6s - loss: 12.4098 - mse: 12.4098 - mae: 1.4259 - val_loss: 8.4676 - val_mse: 8.4676 - val_mae: 1.3932 - lr: 1.0801e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 6s - loss: 12.0355 - mse: 12.0355 - mae: 1.4216 - val_loss: 8.4708 - val_mse: 8.4708 - val_mae: 1.3955 - lr: 1.0801e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 6s - loss: 12.0669 - mse: 12.0669 - mae: 1.4135 - val_loss: 8.5793 - val_mse: 8.5793 - val_mae: 1.3989 - lr: 1.0801e-04 - 6s/epoch - 6ms/step\n",
            "Score for fold 2: loss of 8.579331398010254\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 6s - loss: 11.6581 - mse: 11.6581 - mae: 1.4060 - val_loss: 9.1027 - val_mse: 9.1027 - val_mae: 1.3773 - lr: 1.0801e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 6s - loss: 11.6046 - mse: 11.6046 - mae: 1.3982 - val_loss: 8.9207 - val_mse: 8.9207 - val_mae: 1.3932 - lr: 1.0801e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 6s - loss: 11.5436 - mse: 11.5436 - mae: 1.3876 - val_loss: 10.0718 - val_mse: 10.0718 - val_mae: 1.4311 - lr: 1.0801e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 6s - loss: 11.2300 - mse: 11.2300 - mae: 1.3790 - val_loss: 9.2080 - val_mse: 9.2080 - val_mae: 1.4538 - lr: 1.0801e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 6s - loss: 11.0874 - mse: 11.0874 - mae: 1.3735 - val_loss: 9.0538 - val_mse: 9.0538 - val_mae: 1.3833 - lr: 1.0801e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 6s - loss: 11.0661 - mse: 11.0661 - mae: 1.3643 - val_loss: 9.3743 - val_mse: 9.3743 - val_mae: 1.4082 - lr: 1.0801e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 6s - loss: 10.9637 - mse: 10.9637 - mae: 1.3540 - val_loss: 9.5582 - val_mse: 9.5582 - val_mae: 1.3900 - lr: 1.0801e-04 - 6s/epoch - 6ms/step\n",
            "Score for fold 3: loss of 9.558161735534668\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 6s - loss: 11.2636 - mse: 11.2636 - mae: 1.3741 - val_loss: 6.9808 - val_mse: 6.9808 - val_mae: 1.3607 - lr: 1.0801e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 6s - loss: 11.0118 - mse: 11.0118 - mae: 1.3643 - val_loss: 7.1207 - val_mse: 7.1207 - val_mae: 1.2868 - lr: 1.0801e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 6s - loss: 10.8911 - mse: 10.8911 - mae: 1.3508 - val_loss: 7.2831 - val_mse: 7.2831 - val_mae: 1.3382 - lr: 1.0801e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 6s - loss: 10.7113 - mse: 10.7113 - mae: 1.3396 - val_loss: 7.4866 - val_mse: 7.4866 - val_mae: 1.3119 - lr: 1.0801e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 6s - loss: 10.5223 - mse: 10.5223 - mae: 1.3216 - val_loss: 7.4351 - val_mse: 7.4351 - val_mae: 1.3681 - lr: 1.0801e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 6s - loss: 10.2918 - mse: 10.2918 - mae: 1.3121 - val_loss: 7.6588 - val_mse: 7.6588 - val_mae: 1.3639 - lr: 1.0801e-04 - 6s/epoch - 6ms/step\n",
            "Score for fold 4: loss of 7.658792495727539\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 6s - loss: 7.2051 - mse: 7.2051 - mae: 1.3228 - val_loss: 20.0539 - val_mse: 20.0539 - val_mae: 1.3177 - lr: 1.0801e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 6s - loss: 6.8324 - mse: 6.8324 - mae: 1.2988 - val_loss: 20.3930 - val_mse: 20.3930 - val_mae: 1.3631 - lr: 1.0801e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 6s - loss: 6.7793 - mse: 6.7793 - mae: 1.2892 - val_loss: 20.5799 - val_mse: 20.5799 - val_mae: 1.3386 - lr: 1.0801e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 6s - loss: 6.6291 - mse: 6.6291 - mae: 1.2764 - val_loss: 21.0503 - val_mse: 21.0503 - val_mae: 1.4347 - lr: 1.0801e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 6s - loss: 6.5234 - mse: 6.5234 - mae: 1.2606 - val_loss: 20.5519 - val_mse: 20.5519 - val_mae: 1.3731 - lr: 1.0801e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 6s - loss: 6.1712 - mse: 6.1712 - mae: 1.2436 - val_loss: 20.6533 - val_mse: 20.6533 - val_mae: 1.4069 - lr: 1.0801e-04 - 6s/epoch - 6ms/step\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-10-11 12:23:58,005]\u001b[0m Finished trial#27 resulted in value: 11.092. Current best value is 9.334 with parameters: {'activation': 'relu', 'num_hidden_layer': 4, 'i': 9, 'learning_rate': 0.00023071835273936203}.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Score for fold 5: loss of 20.653291702270508\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 4s - loss: 11.6756 - mse: 11.6756 - mae: 1.5505 - val_loss: 19.6334 - val_mse: 19.6334 - val_mae: 1.4513 - lr: 2.3399e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 4s - loss: 10.7438 - mse: 10.7438 - mae: 1.4832 - val_loss: 19.0403 - val_mse: 19.0403 - val_mae: 1.5053 - lr: 2.3399e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 4s - loss: 10.4039 - mse: 10.4039 - mae: 1.4674 - val_loss: 18.9461 - val_mse: 18.9461 - val_mae: 1.4582 - lr: 2.3399e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 3s - loss: 10.3511 - mse: 10.3511 - mae: 1.4573 - val_loss: 19.3854 - val_mse: 19.3854 - val_mae: 1.5021 - lr: 2.3399e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 4s - loss: 10.2998 - mse: 10.2998 - mae: 1.4502 - val_loss: 18.7704 - val_mse: 18.7704 - val_mae: 1.5373 - lr: 2.3399e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 4s - loss: 9.9922 - mse: 9.9922 - mae: 1.4413 - val_loss: 19.0951 - val_mse: 19.0951 - val_mae: 1.4441 - lr: 2.3399e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 3s - loss: 9.9616 - mse: 9.9616 - mae: 1.4356 - val_loss: 18.9740 - val_mse: 18.9740 - val_mae: 1.5422 - lr: 2.3399e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 3s - loss: 9.9174 - mse: 9.9174 - mae: 1.4273 - val_loss: 19.3094 - val_mse: 19.3094 - val_mae: 1.5301 - lr: 2.3399e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 3s - loss: 9.8590 - mse: 9.8590 - mae: 1.4246 - val_loss: 19.0552 - val_mse: 19.0552 - val_mae: 1.5251 - lr: 2.3399e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 3s - loss: 9.8092 - mse: 9.8092 - mae: 1.4169 - val_loss: 19.4254 - val_mse: 19.4254 - val_mae: 1.4390 - lr: 2.3399e-04 - 3s/epoch - 3ms/step\n",
            "Score for fold 1: loss of 19.425382614135742\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 3s - loss: 11.7479 - mse: 11.7479 - mae: 1.4271 - val_loss: 10.6023 - val_mse: 10.6023 - val_mae: 1.4406 - lr: 2.3399e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 4s - loss: 11.6479 - mse: 11.6479 - mae: 1.4219 - val_loss: 10.6007 - val_mse: 10.6007 - val_mae: 1.4489 - lr: 2.3399e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 4s - loss: 11.5401 - mse: 11.5401 - mae: 1.4126 - val_loss: 10.6923 - val_mse: 10.6923 - val_mae: 1.4399 - lr: 2.3399e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 4s - loss: 11.1080 - mse: 11.1080 - mae: 1.4025 - val_loss: 10.7991 - val_mse: 10.7991 - val_mae: 1.4185 - lr: 2.3399e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 3s - loss: 11.1975 - mse: 11.1975 - mae: 1.3952 - val_loss: 10.6735 - val_mse: 10.6735 - val_mae: 1.4300 - lr: 2.3399e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 4s - loss: 10.8309 - mse: 10.8309 - mae: 1.3864 - val_loss: 10.7959 - val_mse: 10.7959 - val_mae: 1.4724 - lr: 2.3399e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 4s - loss: 10.7482 - mse: 10.7482 - mae: 1.3792 - val_loss: 11.0402 - val_mse: 11.0402 - val_mae: 1.4121 - lr: 2.3399e-04 - 4s/epoch - 4ms/step\n",
            "Score for fold 2: loss of 11.040238380432129\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 3s - loss: 10.6923 - mse: 10.6923 - mae: 1.3946 - val_loss: 11.0214 - val_mse: 11.0214 - val_mae: 1.3610 - lr: 2.3399e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 3s - loss: 10.5536 - mse: 10.5536 - mae: 1.3807 - val_loss: 11.4090 - val_mse: 11.4090 - val_mae: 1.4034 - lr: 2.3399e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 4s - loss: 10.4230 - mse: 10.4230 - mae: 1.3683 - val_loss: 11.4452 - val_mse: 11.4452 - val_mae: 1.3894 - lr: 2.3399e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 4s - loss: 10.2450 - mse: 10.2450 - mae: 1.3575 - val_loss: 11.0798 - val_mse: 11.0798 - val_mae: 1.4333 - lr: 2.3399e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 4s - loss: 10.0515 - mse: 10.0515 - mae: 1.3540 - val_loss: 11.7505 - val_mse: 11.7505 - val_mae: 1.3853 - lr: 2.3399e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 3s - loss: 9.8019 - mse: 9.8019 - mae: 1.3372 - val_loss: 11.2871 - val_mse: 11.2871 - val_mae: 1.3861 - lr: 2.3399e-04 - 3s/epoch - 3ms/step\n",
            "Score for fold 3: loss of 11.287053108215332\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 4s - loss: 11.0995 - mse: 11.0995 - mae: 1.3625 - val_loss: 6.8215 - val_mse: 6.8215 - val_mae: 1.3128 - lr: 2.3399e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 4s - loss: 11.0341 - mse: 11.0341 - mae: 1.3507 - val_loss: 7.0672 - val_mse: 7.0672 - val_mae: 1.3753 - lr: 2.3399e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 4s - loss: 10.6283 - mse: 10.6283 - mae: 1.3324 - val_loss: 7.3874 - val_mse: 7.3874 - val_mae: 1.3615 - lr: 2.3399e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 4s - loss: 10.4870 - mse: 10.4870 - mae: 1.3268 - val_loss: 7.4085 - val_mse: 7.4085 - val_mae: 1.3890 - lr: 2.3399e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 4s - loss: 10.2821 - mse: 10.2821 - mae: 1.3181 - val_loss: 8.1428 - val_mse: 8.1428 - val_mae: 1.3545 - lr: 2.3399e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 4s - loss: 10.2643 - mse: 10.2643 - mae: 1.3078 - val_loss: 7.9662 - val_mse: 7.9662 - val_mae: 1.4417 - lr: 2.3399e-04 - 4s/epoch - 4ms/step\n",
            "Score for fold 4: loss of 7.9661545753479\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 4s - loss: 10.6609 - mse: 10.6609 - mae: 1.3320 - val_loss: 5.5034 - val_mse: 5.5034 - val_mae: 1.2681 - lr: 2.3399e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 3s - loss: 10.5649 - mse: 10.5649 - mae: 1.3245 - val_loss: 5.5613 - val_mse: 5.5613 - val_mae: 1.2808 - lr: 2.3399e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 3s - loss: 10.1905 - mse: 10.1905 - mae: 1.3071 - val_loss: 5.6387 - val_mse: 5.6387 - val_mae: 1.2549 - lr: 2.3399e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 3s - loss: 9.8968 - mse: 9.8968 - mae: 1.2968 - val_loss: 6.0660 - val_mse: 6.0660 - val_mae: 1.2953 - lr: 2.3399e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 3s - loss: 10.0705 - mse: 10.0705 - mae: 1.2899 - val_loss: 6.4725 - val_mse: 6.4725 - val_mae: 1.3489 - lr: 2.3399e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 4s - loss: 9.6812 - mse: 9.6812 - mae: 1.2747 - val_loss: 6.0135 - val_mse: 6.0135 - val_mae: 1.2955 - lr: 2.3399e-04 - 4s/epoch - 4ms/step\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-10-11 12:26:06,768]\u001b[0m Finished trial#28 resulted in value: 11.148. Current best value is 9.334 with parameters: {'activation': 'relu', 'num_hidden_layer': 4, 'i': 9, 'learning_rate': 0.00023071835273936203}.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Score for fold 5: loss of 6.013494968414307\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 16.0433 - mse: 16.0433 - mae: 1.6468 - val_loss: 11.2385 - val_mse: 11.2385 - val_mae: 1.5491 - lr: 1.5305e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 14.1247 - mse: 14.1247 - mae: 1.5484 - val_loss: 10.8380 - val_mse: 10.8380 - val_mae: 1.5022 - lr: 1.5305e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 13.7677 - mse: 13.7677 - mae: 1.5338 - val_loss: 10.5444 - val_mse: 10.5444 - val_mae: 1.4900 - lr: 1.5305e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 13.5262 - mse: 13.5262 - mae: 1.5217 - val_loss: 10.3654 - val_mse: 10.3654 - val_mae: 1.4909 - lr: 1.5305e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 13.3748 - mse: 13.3748 - mae: 1.5122 - val_loss: 10.1969 - val_mse: 10.1969 - val_mae: 1.5265 - lr: 1.5305e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 13.2689 - mse: 13.2689 - mae: 1.5067 - val_loss: 10.0305 - val_mse: 10.0305 - val_mae: 1.4710 - lr: 1.5305e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 13.1713 - mse: 13.1713 - mae: 1.5005 - val_loss: 9.8763 - val_mse: 9.8763 - val_mae: 1.4651 - lr: 1.5305e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 2s - loss: 13.0764 - mse: 13.0764 - mae: 1.4943 - val_loss: 9.7104 - val_mse: 9.7104 - val_mae: 1.4849 - lr: 1.5305e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 2s - loss: 12.9786 - mse: 12.9786 - mae: 1.4892 - val_loss: 9.6526 - val_mse: 9.6526 - val_mae: 1.4711 - lr: 1.5305e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 2s - loss: 12.9204 - mse: 12.9204 - mae: 1.4833 - val_loss: 9.4638 - val_mse: 9.4638 - val_mae: 1.4860 - lr: 1.5305e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 2s - loss: 12.8620 - mse: 12.8620 - mae: 1.4763 - val_loss: 9.4539 - val_mse: 9.4539 - val_mae: 1.4741 - lr: 1.5305e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 2s - loss: 12.8209 - mse: 12.8209 - mae: 1.4780 - val_loss: 9.3538 - val_mse: 9.3538 - val_mae: 1.4714 - lr: 1.5305e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 13/100\n",
            "1000/1000 - 2s - loss: 12.7920 - mse: 12.7920 - mae: 1.4711 - val_loss: 9.2511 - val_mse: 9.2511 - val_mae: 1.4779 - lr: 1.5305e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 14/100\n",
            "1000/1000 - 2s - loss: 12.7057 - mse: 12.7057 - mae: 1.4676 - val_loss: 9.4860 - val_mse: 9.4860 - val_mae: 1.4453 - lr: 1.5305e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 15/100\n",
            "1000/1000 - 2s - loss: 12.6964 - mse: 12.6964 - mae: 1.4652 - val_loss: 9.1639 - val_mse: 9.1639 - val_mae: 1.4584 - lr: 1.5305e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 16/100\n",
            "1000/1000 - 2s - loss: 12.6444 - mse: 12.6444 - mae: 1.4587 - val_loss: 9.1078 - val_mse: 9.1078 - val_mae: 1.4870 - lr: 1.5305e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 17/100\n",
            "1000/1000 - 2s - loss: 12.6050 - mse: 12.6050 - mae: 1.4597 - val_loss: 9.1576 - val_mse: 9.1576 - val_mae: 1.4486 - lr: 1.5305e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 18/100\n",
            "1000/1000 - 2s - loss: 12.6354 - mse: 12.6354 - mae: 1.4572 - val_loss: 9.0372 - val_mse: 9.0372 - val_mae: 1.4835 - lr: 1.5305e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 19/100\n",
            "1000/1000 - 2s - loss: 12.5661 - mse: 12.5661 - mae: 1.4545 - val_loss: 8.9895 - val_mse: 8.9895 - val_mae: 1.4605 - lr: 1.5305e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 20/100\n",
            "1000/1000 - 2s - loss: 12.5563 - mse: 12.5563 - mae: 1.4523 - val_loss: 9.0134 - val_mse: 9.0134 - val_mae: 1.4732 - lr: 1.5305e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 21/100\n",
            "1000/1000 - 2s - loss: 12.5365 - mse: 12.5365 - mae: 1.4502 - val_loss: 8.9552 - val_mse: 8.9552 - val_mae: 1.4524 - lr: 1.5305e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 22/100\n",
            "1000/1000 - 2s - loss: 12.5824 - mse: 12.5824 - mae: 1.4493 - val_loss: 9.0165 - val_mse: 9.0165 - val_mae: 1.4366 - lr: 1.5305e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 23/100\n",
            "1000/1000 - 2s - loss: 12.5454 - mse: 12.5454 - mae: 1.4468 - val_loss: 8.8468 - val_mse: 8.8468 - val_mae: 1.4626 - lr: 1.5305e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 24/100\n",
            "1000/1000 - 2s - loss: 12.5057 - mse: 12.5057 - mae: 1.4446 - val_loss: 8.9063 - val_mse: 8.9063 - val_mae: 1.4352 - lr: 1.5305e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 25/100\n",
            "1000/1000 - 2s - loss: 12.4856 - mse: 12.4856 - mae: 1.4439 - val_loss: 8.8427 - val_mse: 8.8427 - val_mae: 1.4623 - lr: 1.5305e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 26/100\n",
            "1000/1000 - 2s - loss: 12.4847 - mse: 12.4847 - mae: 1.4411 - val_loss: 8.8018 - val_mse: 8.8018 - val_mae: 1.4546 - lr: 1.5305e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 27/100\n",
            "1000/1000 - 2s - loss: 12.4491 - mse: 12.4491 - mae: 1.4423 - val_loss: 8.8314 - val_mse: 8.8314 - val_mae: 1.4780 - lr: 1.5305e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 28/100\n",
            "1000/1000 - 2s - loss: 12.4711 - mse: 12.4711 - mae: 1.4416 - val_loss: 8.8705 - val_mse: 8.8705 - val_mae: 1.4206 - lr: 1.5305e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 29/100\n",
            "1000/1000 - 2s - loss: 12.4535 - mse: 12.4535 - mae: 1.4373 - val_loss: 8.8089 - val_mse: 8.8089 - val_mae: 1.4629 - lr: 1.5305e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 30/100\n",
            "1000/1000 - 2s - loss: 12.4194 - mse: 12.4194 - mae: 1.4394 - val_loss: 8.7323 - val_mse: 8.7323 - val_mae: 1.4405 - lr: 1.5305e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 31/100\n",
            "1000/1000 - 2s - loss: 12.4103 - mse: 12.4103 - mae: 1.4360 - val_loss: 8.7248 - val_mse: 8.7248 - val_mae: 1.4819 - lr: 1.5305e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 32/100\n",
            "1000/1000 - 2s - loss: 12.4505 - mse: 12.4505 - mae: 1.4389 - val_loss: 8.7224 - val_mse: 8.7224 - val_mae: 1.4452 - lr: 1.5305e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 33/100\n",
            "1000/1000 - 2s - loss: 12.3759 - mse: 12.3759 - mae: 1.4339 - val_loss: 8.6338 - val_mse: 8.6338 - val_mae: 1.4542 - lr: 1.5305e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 34/100\n",
            "1000/1000 - 2s - loss: 12.3841 - mse: 12.3841 - mae: 1.4324 - val_loss: 8.7572 - val_mse: 8.7572 - val_mae: 1.4337 - lr: 1.5305e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 35/100\n",
            "1000/1000 - 2s - loss: 12.3327 - mse: 12.3327 - mae: 1.4330 - val_loss: 8.7424 - val_mse: 8.7424 - val_mae: 1.4250 - lr: 1.5305e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 36/100\n",
            "1000/1000 - 2s - loss: 12.3563 - mse: 12.3563 - mae: 1.4309 - val_loss: 8.7017 - val_mse: 8.7017 - val_mae: 1.4492 - lr: 1.5305e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 37/100\n",
            "1000/1000 - 2s - loss: 12.3191 - mse: 12.3191 - mae: 1.4309 - val_loss: 8.7556 - val_mse: 8.7556 - val_mae: 1.4220 - lr: 1.5305e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 38/100\n",
            "1000/1000 - 2s - loss: 12.3204 - mse: 12.3204 - mae: 1.4300 - val_loss: 8.6816 - val_mse: 8.6816 - val_mae: 1.4355 - lr: 1.5305e-04 - 2s/epoch - 2ms/step\n",
            "Score for fold 1: loss of 8.681618690490723\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 12.3726 - mse: 12.3726 - mae: 1.4318 - val_loss: 8.3960 - val_mse: 8.3960 - val_mae: 1.4437 - lr: 1.5305e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 12.3512 - mse: 12.3512 - mae: 1.4306 - val_loss: 8.5034 - val_mse: 8.5034 - val_mae: 1.4212 - lr: 1.5305e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 12.3656 - mse: 12.3656 - mae: 1.4292 - val_loss: 8.5792 - val_mse: 8.5792 - val_mae: 1.4048 - lr: 1.5305e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 12.3073 - mse: 12.3073 - mae: 1.4274 - val_loss: 8.4480 - val_mse: 8.4480 - val_mae: 1.4326 - lr: 1.5305e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 12.2990 - mse: 12.2990 - mae: 1.4236 - val_loss: 8.4513 - val_mse: 8.4513 - val_mae: 1.4465 - lr: 1.5305e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 12.3206 - mse: 12.3206 - mae: 1.4258 - val_loss: 8.4825 - val_mse: 8.4825 - val_mae: 1.4286 - lr: 1.5305e-04 - 2s/epoch - 2ms/step\n",
            "Score for fold 2: loss of 8.482497215270996\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 10.7813 - mse: 10.7813 - mae: 1.4261 - val_loss: 14.5672 - val_mse: 14.5672 - val_mae: 1.4398 - lr: 1.5305e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 10.7513 - mse: 10.7513 - mae: 1.4255 - val_loss: 14.8092 - val_mse: 14.8092 - val_mae: 1.4398 - lr: 1.5305e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 10.7494 - mse: 10.7494 - mae: 1.4219 - val_loss: 14.7852 - val_mse: 14.7852 - val_mae: 1.4364 - lr: 1.5305e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 10.7605 - mse: 10.7605 - mae: 1.4238 - val_loss: 14.7141 - val_mse: 14.7141 - val_mae: 1.4293 - lr: 1.5305e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 10.7318 - mse: 10.7318 - mae: 1.4228 - val_loss: 14.8121 - val_mse: 14.8121 - val_mae: 1.4127 - lr: 1.5305e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 10.7091 - mse: 10.7091 - mae: 1.4194 - val_loss: 14.9114 - val_mse: 14.9114 - val_mae: 1.4246 - lr: 1.5305e-04 - 2s/epoch - 2ms/step\n",
            "Score for fold 3: loss of 14.911412239074707\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 12.4549 - mse: 12.4549 - mae: 1.4301 - val_loss: 7.9209 - val_mse: 7.9209 - val_mae: 1.3904 - lr: 1.5305e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 12.3974 - mse: 12.3974 - mae: 1.4313 - val_loss: 7.9614 - val_mse: 7.9614 - val_mae: 1.3750 - lr: 1.5305e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 12.3476 - mse: 12.3476 - mae: 1.4305 - val_loss: 7.9346 - val_mse: 7.9346 - val_mae: 1.3867 - lr: 1.5305e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 12.3150 - mse: 12.3150 - mae: 1.4282 - val_loss: 7.9792 - val_mse: 7.9792 - val_mae: 1.3819 - lr: 1.5305e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 12.3523 - mse: 12.3523 - mae: 1.4283 - val_loss: 8.0140 - val_mse: 8.0140 - val_mae: 1.3801 - lr: 1.5305e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 12.2900 - mse: 12.2900 - mae: 1.4272 - val_loss: 7.9416 - val_mse: 7.9416 - val_mae: 1.4157 - lr: 1.5305e-04 - 2s/epoch - 2ms/step\n",
            "Score for fold 4: loss of 7.941623687744141\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 9.7525 - mse: 9.7525 - mae: 1.4154 - val_loss: 18.1777 - val_mse: 18.1777 - val_mae: 1.4495 - lr: 1.5305e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 9.7180 - mse: 9.7180 - mae: 1.4149 - val_loss: 18.2161 - val_mse: 18.2161 - val_mae: 1.4566 - lr: 1.5305e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 9.7391 - mse: 9.7391 - mae: 1.4119 - val_loss: 18.1709 - val_mse: 18.1709 - val_mae: 1.4775 - lr: 1.5305e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 9.6569 - mse: 9.6569 - mae: 1.4120 - val_loss: 18.2637 - val_mse: 18.2637 - val_mae: 1.4592 - lr: 1.5305e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 9.6613 - mse: 9.6613 - mae: 1.4111 - val_loss: 18.2643 - val_mse: 18.2643 - val_mae: 1.4464 - lr: 1.5305e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 9.6910 - mse: 9.6910 - mae: 1.4090 - val_loss: 18.2516 - val_mse: 18.2516 - val_mae: 1.4687 - lr: 1.5305e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 9.6181 - mse: 9.6181 - mae: 1.4125 - val_loss: 18.2574 - val_mse: 18.2574 - val_mae: 1.4594 - lr: 1.5305e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 2s - loss: 9.6712 - mse: 9.6712 - mae: 1.4088 - val_loss: 18.2808 - val_mse: 18.2808 - val_mae: 1.4507 - lr: 1.5305e-04 - 2s/epoch - 2ms/step\n",
            "Score for fold 5: loss of 18.280832290649414\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-10-11 12:27:54,739]\u001b[0m Finished trial#29 resulted in value: 11.658. Current best value is 9.334 with parameters: {'activation': 'relu', 'num_hidden_layer': 4, 'i': 9, 'learning_rate': 0.00023071835273936203}.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 10s - loss: 13.3304 - mse: 13.3304 - mae: 1.6696 - val_loss: 17.7423 - val_mse: 17.7423 - val_mae: 1.5190 - lr: 4.7918e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 10s - loss: 12.6087 - mse: 12.6087 - mae: 1.5867 - val_loss: 17.5020 - val_mse: 17.5020 - val_mae: 1.4586 - lr: 4.7918e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 10s - loss: 12.5154 - mse: 12.5154 - mae: 1.5770 - val_loss: 17.4477 - val_mse: 17.4477 - val_mae: 1.4319 - lr: 4.7918e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 10s - loss: 12.3799 - mse: 12.3799 - mae: 1.5732 - val_loss: 17.2149 - val_mse: 17.2149 - val_mae: 1.4338 - lr: 4.7918e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 10s - loss: 12.3282 - mse: 12.3282 - mae: 1.5667 - val_loss: 17.0830 - val_mse: 17.0830 - val_mae: 1.5562 - lr: 4.7918e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 10s - loss: 12.4472 - mse: 12.4472 - mae: 1.5637 - val_loss: 17.8394 - val_mse: 17.8394 - val_mae: 1.6750 - lr: 4.7918e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 9s - loss: 12.3412 - mse: 12.3412 - mae: 1.5619 - val_loss: 17.0558 - val_mse: 17.0558 - val_mae: 1.5265 - lr: 4.7918e-04 - 9s/epoch - 9ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 10s - loss: 12.5256 - mse: 12.5256 - mae: 1.5637 - val_loss: 17.1539 - val_mse: 17.1539 - val_mae: 1.4397 - lr: 4.7918e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 10s - loss: 12.3908 - mse: 12.3908 - mae: 1.5702 - val_loss: 17.0910 - val_mse: 17.0910 - val_mae: 1.5645 - lr: 4.7918e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 10s - loss: 12.4488 - mse: 12.4488 - mae: 1.5674 - val_loss: 17.4759 - val_mse: 17.4759 - val_mae: 1.6690 - lr: 4.7918e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 10s - loss: 12.3939 - mse: 12.3939 - mae: 1.5649 - val_loss: 17.8029 - val_mse: 17.8029 - val_mae: 1.4600 - lr: 4.7918e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 10s - loss: 12.4542 - mse: 12.4542 - mae: 1.5610 - val_loss: 16.9761 - val_mse: 16.9761 - val_mae: 1.5192 - lr: 4.7918e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 13/100\n",
            "1000/1000 - 10s - loss: 12.5308 - mse: 12.5308 - mae: 1.5632 - val_loss: 16.9289 - val_mse: 16.9289 - val_mae: 1.5259 - lr: 4.7918e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 14/100\n",
            "1000/1000 - 10s - loss: 12.3474 - mse: 12.3474 - mae: 1.5642 - val_loss: 17.5932 - val_mse: 17.5932 - val_mae: 1.6238 - lr: 4.7918e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 15/100\n",
            "1000/1000 - 10s - loss: 12.6051 - mse: 12.6051 - mae: 1.5608 - val_loss: 17.2411 - val_mse: 17.2411 - val_mae: 1.5463 - lr: 4.7918e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 16/100\n",
            "1000/1000 - 10s - loss: 12.4185 - mse: 12.4185 - mae: 1.5695 - val_loss: 17.4661 - val_mse: 17.4661 - val_mae: 1.6071 - lr: 4.7918e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 17/100\n",
            "1000/1000 - 10s - loss: 12.3210 - mse: 12.3210 - mae: 1.5654 - val_loss: 17.1703 - val_mse: 17.1703 - val_mae: 1.5450 - lr: 4.7918e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 18/100\n",
            "1000/1000 - 10s - loss: 12.2476 - mse: 12.2476 - mae: 1.5578 - val_loss: 17.2046 - val_mse: 17.2046 - val_mae: 1.5806 - lr: 4.7918e-04 - 10s/epoch - 10ms/step\n",
            "Score for fold 1: loss of 17.204648971557617\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 10s - loss: 14.2021 - mse: 14.2021 - mae: 1.5678 - val_loss: 10.2130 - val_mse: 10.2130 - val_mae: 1.6669 - lr: 4.7918e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 10s - loss: 14.1262 - mse: 14.1262 - mae: 1.5564 - val_loss: 11.0135 - val_mse: 11.0135 - val_mae: 1.4396 - lr: 4.7918e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 10s - loss: 14.2437 - mse: 14.2437 - mae: 1.5620 - val_loss: 10.2486 - val_mse: 10.2486 - val_mae: 1.4738 - lr: 4.7918e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 10s - loss: 14.1869 - mse: 14.1869 - mae: 1.5553 - val_loss: 10.2225 - val_mse: 10.2225 - val_mae: 1.4867 - lr: 4.7918e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 10s - loss: 14.2659 - mse: 14.2659 - mae: 1.5634 - val_loss: 9.8541 - val_mse: 9.8541 - val_mae: 1.6608 - lr: 4.7918e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 10s - loss: 14.2122 - mse: 14.2122 - mae: 1.5669 - val_loss: 10.0220 - val_mse: 10.0220 - val_mae: 1.5114 - lr: 4.7918e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 9s - loss: 14.2439 - mse: 14.2439 - mae: 1.5591 - val_loss: 9.9501 - val_mse: 9.9501 - val_mae: 1.6147 - lr: 4.7918e-04 - 9s/epoch - 9ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 10s - loss: 14.2538 - mse: 14.2538 - mae: 1.5582 - val_loss: 9.8979 - val_mse: 9.8979 - val_mae: 1.5511 - lr: 4.7918e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 10s - loss: 14.3180 - mse: 14.3180 - mae: 1.5654 - val_loss: 9.8017 - val_mse: 9.8017 - val_mae: 1.5619 - lr: 4.7918e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 10s - loss: 14.2604 - mse: 14.2604 - mae: 1.5666 - val_loss: 10.4640 - val_mse: 10.4640 - val_mae: 1.4456 - lr: 4.7918e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 10s - loss: 14.2273 - mse: 14.2273 - mae: 1.5572 - val_loss: 9.8205 - val_mse: 9.8205 - val_mae: 1.5420 - lr: 4.7918e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 10s - loss: 14.3405 - mse: 14.3405 - mae: 1.5765 - val_loss: 9.9625 - val_mse: 9.9625 - val_mae: 1.4862 - lr: 4.7918e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 13/100\n",
            "1000/1000 - 10s - loss: 14.2958 - mse: 14.2958 - mae: 1.5626 - val_loss: 10.6799 - val_mse: 10.6799 - val_mae: 1.4549 - lr: 4.7918e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 14/100\n",
            "1000/1000 - 10s - loss: 14.2973 - mse: 14.2973 - mae: 1.5607 - val_loss: 9.8116 - val_mse: 9.8116 - val_mae: 1.5379 - lr: 4.7918e-04 - 10s/epoch - 10ms/step\n",
            "Score for fold 2: loss of 9.811641693115234\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 10s - loss: 12.7159 - mse: 12.7159 - mae: 1.5447 - val_loss: 15.6731 - val_mse: 15.6731 - val_mae: 1.5928 - lr: 4.7918e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 10s - loss: 12.7860 - mse: 12.7860 - mae: 1.5530 - val_loss: 15.7155 - val_mse: 15.7155 - val_mae: 1.5749 - lr: 4.7918e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 10s - loss: 12.7458 - mse: 12.7458 - mae: 1.5461 - val_loss: 15.6632 - val_mse: 15.6632 - val_mae: 1.6732 - lr: 4.7918e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 10s - loss: 12.7640 - mse: 12.7640 - mae: 1.5423 - val_loss: 15.6999 - val_mse: 15.6999 - val_mae: 1.7450 - lr: 4.7918e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 10s - loss: 12.7466 - mse: 12.7466 - mae: 1.5511 - val_loss: 15.5874 - val_mse: 15.5874 - val_mae: 1.6146 - lr: 4.7918e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 10s - loss: 12.5929 - mse: 12.5929 - mae: 1.5431 - val_loss: 15.6024 - val_mse: 15.6024 - val_mae: 1.6277 - lr: 4.7918e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 10s - loss: 12.6720 - mse: 12.6720 - mae: 1.5461 - val_loss: 17.1266 - val_mse: 17.1266 - val_mae: 1.5823 - lr: 4.7918e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 10s - loss: 12.6621 - mse: 12.6621 - mae: 1.5437 - val_loss: 15.7805 - val_mse: 15.7805 - val_mae: 1.5598 - lr: 4.7918e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 10s - loss: 12.6547 - mse: 12.6547 - mae: 1.5485 - val_loss: 15.8663 - val_mse: 15.8663 - val_mae: 1.8019 - lr: 4.7918e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 10s - loss: 12.6198 - mse: 12.6198 - mae: 1.5418 - val_loss: 15.6083 - val_mse: 15.6083 - val_mae: 1.6297 - lr: 4.7918e-04 - 10s/epoch - 10ms/step\n",
            "Score for fold 3: loss of 15.608331680297852\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 10s - loss: 13.3950 - mse: 13.3950 - mae: 1.5623 - val_loss: 13.2482 - val_mse: 13.2482 - val_mae: 1.4447 - lr: 4.7918e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 10s - loss: 13.4083 - mse: 13.4083 - mae: 1.5622 - val_loss: 14.3110 - val_mse: 14.3110 - val_mae: 1.8837 - lr: 4.7918e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 10s - loss: 13.3568 - mse: 13.3568 - mae: 1.5649 - val_loss: 13.1412 - val_mse: 13.1412 - val_mae: 1.5523 - lr: 4.7918e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 10s - loss: 13.5280 - mse: 13.5280 - mae: 1.5729 - val_loss: 13.3898 - val_mse: 13.3898 - val_mae: 1.6296 - lr: 4.7918e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 10s - loss: 13.2159 - mse: 13.2159 - mae: 1.5522 - val_loss: 13.1054 - val_mse: 13.1054 - val_mae: 1.5118 - lr: 4.7918e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 10s - loss: 13.4249 - mse: 13.4249 - mae: 1.5636 - val_loss: 13.7719 - val_mse: 13.7719 - val_mae: 1.4423 - lr: 4.7918e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 10s - loss: 13.3722 - mse: 13.3722 - mae: 1.5541 - val_loss: 13.0888 - val_mse: 13.0888 - val_mae: 1.5137 - lr: 4.7918e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 10s - loss: 13.4600 - mse: 13.4600 - mae: 1.5752 - val_loss: 13.1836 - val_mse: 13.1836 - val_mae: 1.4691 - lr: 4.7918e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 10s - loss: 13.5668 - mse: 13.5668 - mae: 1.5632 - val_loss: 13.1187 - val_mse: 13.1187 - val_mae: 1.5014 - lr: 4.7918e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 10s - loss: 13.4003 - mse: 13.4003 - mae: 1.5618 - val_loss: 13.1583 - val_mse: 13.1583 - val_mae: 1.5401 - lr: 4.7918e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 10s - loss: 13.5048 - mse: 13.5048 - mae: 1.5624 - val_loss: 13.1060 - val_mse: 13.1060 - val_mae: 1.5282 - lr: 4.7918e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 10s - loss: 13.3781 - mse: 13.3781 - mae: 1.5620 - val_loss: 13.1419 - val_mse: 13.1419 - val_mae: 1.5155 - lr: 4.7918e-04 - 10s/epoch - 10ms/step\n",
            "Score for fold 4: loss of 13.14189624786377\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 10s - loss: 14.0032 - mse: 14.0032 - mae: 1.5544 - val_loss: 11.3332 - val_mse: 11.3332 - val_mae: 1.6535 - lr: 4.7918e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 10s - loss: 14.1840 - mse: 14.1840 - mae: 1.5637 - val_loss: 10.3856 - val_mse: 10.3856 - val_mae: 1.5626 - lr: 4.7918e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 10s - loss: 14.1784 - mse: 14.1784 - mae: 1.5549 - val_loss: 11.0724 - val_mse: 11.0724 - val_mae: 1.6878 - lr: 4.7918e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 10s - loss: 14.0946 - mse: 14.0946 - mae: 1.5543 - val_loss: 10.2134 - val_mse: 10.2134 - val_mae: 1.5034 - lr: 4.7918e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 10s - loss: 13.9981 - mse: 13.9981 - mae: 1.5533 - val_loss: 10.5248 - val_mse: 10.5248 - val_mae: 1.6022 - lr: 4.7918e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 10s - loss: 13.9837 - mse: 13.9837 - mae: 1.5594 - val_loss: 10.3700 - val_mse: 10.3700 - val_mae: 1.5541 - lr: 4.7918e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 10s - loss: 14.0262 - mse: 14.0262 - mae: 1.5528 - val_loss: 10.9195 - val_mse: 10.9195 - val_mae: 1.6005 - lr: 4.7918e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 10s - loss: 14.1299 - mse: 14.1299 - mae: 1.5577 - val_loss: 11.7224 - val_mse: 11.7224 - val_mae: 1.6243 - lr: 4.7918e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 10s - loss: 14.1352 - mse: 14.1352 - mae: 1.5593 - val_loss: 10.3483 - val_mse: 10.3483 - val_mae: 1.5180 - lr: 4.7918e-04 - 10s/epoch - 10ms/step\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-10-11 12:38:28,134]\u001b[0m Finished trial#30 resulted in value: 13.222. Current best value is 9.334 with parameters: {'activation': 'relu', 'num_hidden_layer': 4, 'i': 9, 'learning_rate': 0.00023071835273936203}.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Score for fold 5: loss of 10.348329544067383\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 7s - loss: 12.5574 - mse: 12.5574 - mae: 1.5415 - val_loss: 17.3512 - val_mse: 17.3512 - val_mae: 1.5379 - lr: 2.3163e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 6s - loss: 11.2873 - mse: 11.2873 - mae: 1.4806 - val_loss: 17.8575 - val_mse: 17.8575 - val_mae: 1.5456 - lr: 2.3163e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 6s - loss: 11.1033 - mse: 11.1033 - mae: 1.4644 - val_loss: 17.1190 - val_mse: 17.1190 - val_mae: 1.4881 - lr: 2.3163e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 6s - loss: 10.9234 - mse: 10.9234 - mae: 1.4525 - val_loss: 17.4620 - val_mse: 17.4620 - val_mae: 1.4809 - lr: 2.3163e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 6s - loss: 10.7791 - mse: 10.7791 - mae: 1.4481 - val_loss: 17.0942 - val_mse: 17.0942 - val_mae: 1.4892 - lr: 2.3163e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 6s - loss: 10.5055 - mse: 10.5055 - mae: 1.4363 - val_loss: 16.8209 - val_mse: 16.8209 - val_mae: 1.4923 - lr: 2.3163e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 6s - loss: 10.2223 - mse: 10.2223 - mae: 1.4249 - val_loss: 17.1534 - val_mse: 17.1534 - val_mae: 1.4391 - lr: 2.3163e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 6s - loss: 10.1240 - mse: 10.1240 - mae: 1.4173 - val_loss: 17.0844 - val_mse: 17.0844 - val_mae: 1.4761 - lr: 2.3163e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 6s - loss: 10.0430 - mse: 10.0430 - mae: 1.4041 - val_loss: 17.3083 - val_mse: 17.3083 - val_mae: 1.5331 - lr: 2.3163e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 6s - loss: 9.7931 - mse: 9.7931 - mae: 1.3935 - val_loss: 16.9517 - val_mse: 16.9517 - val_mae: 1.4916 - lr: 2.3163e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 6s - loss: 9.7401 - mse: 9.7401 - mae: 1.3809 - val_loss: 16.9660 - val_mse: 16.9660 - val_mae: 1.5463 - lr: 2.3163e-04 - 6s/epoch - 6ms/step\n",
            "Score for fold 1: loss of 16.96596908569336\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 6s - loss: 11.7402 - mse: 11.7402 - mae: 1.4095 - val_loss: 8.0960 - val_mse: 8.0960 - val_mae: 1.3270 - lr: 2.3163e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 6s - loss: 11.4649 - mse: 11.4649 - mae: 1.3943 - val_loss: 8.1483 - val_mse: 8.1483 - val_mae: 1.3483 - lr: 2.3163e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 6s - loss: 11.3157 - mse: 11.3157 - mae: 1.3811 - val_loss: 8.6913 - val_mse: 8.6913 - val_mae: 1.4291 - lr: 2.3163e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 6s - loss: 10.9380 - mse: 10.9380 - mae: 1.3689 - val_loss: 8.7570 - val_mse: 8.7570 - val_mae: 1.4118 - lr: 2.3163e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 6s - loss: 10.7821 - mse: 10.7821 - mae: 1.3563 - val_loss: 8.9864 - val_mse: 8.9864 - val_mae: 1.3902 - lr: 2.3163e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 6s - loss: 10.5079 - mse: 10.5079 - mae: 1.3378 - val_loss: 8.5858 - val_mse: 8.5858 - val_mae: 1.3568 - lr: 2.3163e-04 - 6s/epoch - 6ms/step\n",
            "Score for fold 2: loss of 8.58580207824707\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 6s - loss: 10.1076 - mse: 10.1076 - mae: 1.3563 - val_loss: 10.2161 - val_mse: 10.2161 - val_mae: 1.3093 - lr: 2.3163e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 6s - loss: 9.6020 - mse: 9.6020 - mae: 1.3317 - val_loss: 10.3050 - val_mse: 10.3050 - val_mae: 1.3097 - lr: 2.3163e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 6s - loss: 9.4263 - mse: 9.4263 - mae: 1.3192 - val_loss: 10.3952 - val_mse: 10.3952 - val_mae: 1.3836 - lr: 2.3163e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 6s - loss: 9.2645 - mse: 9.2645 - mae: 1.2983 - val_loss: 10.2771 - val_mse: 10.2771 - val_mae: 1.4070 - lr: 2.3163e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 6s - loss: 8.9934 - mse: 8.9934 - mae: 1.2812 - val_loss: 11.1993 - val_mse: 11.1993 - val_mae: 1.3970 - lr: 2.3163e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 6s - loss: 8.5675 - mse: 8.5675 - mae: 1.2621 - val_loss: 10.9301 - val_mse: 10.9301 - val_mae: 1.2893 - lr: 2.3163e-04 - 6s/epoch - 6ms/step\n",
            "Score for fold 3: loss of 10.930086135864258\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 6s - loss: 9.4080 - mse: 9.4080 - mae: 1.2977 - val_loss: 6.5128 - val_mse: 6.5128 - val_mae: 1.2544 - lr: 2.3163e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 6s - loss: 9.4509 - mse: 9.4509 - mae: 1.2779 - val_loss: 6.6749 - val_mse: 6.6749 - val_mae: 1.2431 - lr: 2.3163e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 6s - loss: 8.8927 - mse: 8.8927 - mae: 1.2582 - val_loss: 7.0642 - val_mse: 7.0642 - val_mae: 1.2369 - lr: 2.3163e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 6s - loss: 9.0881 - mse: 9.0881 - mae: 1.2423 - val_loss: 8.0641 - val_mse: 8.0641 - val_mae: 1.4711 - lr: 2.3163e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 6s - loss: 8.2710 - mse: 8.2710 - mae: 1.2276 - val_loss: 7.2082 - val_mse: 7.2082 - val_mae: 1.2689 - lr: 2.3163e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 6s - loss: 8.7069 - mse: 8.7069 - mae: 1.2091 - val_loss: 7.4149 - val_mse: 7.4149 - val_mae: 1.2832 - lr: 2.3163e-04 - 6s/epoch - 6ms/step\n",
            "Score for fold 4: loss of 7.414947032928467\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 6s - loss: 8.3125 - mse: 8.3125 - mae: 1.2452 - val_loss: 6.3630 - val_mse: 6.3630 - val_mae: 1.1606 - lr: 2.3163e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 6s - loss: 8.1505 - mse: 8.1505 - mae: 1.2255 - val_loss: 6.2675 - val_mse: 6.2675 - val_mae: 1.1771 - lr: 2.3163e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 6s - loss: 7.7592 - mse: 7.7592 - mae: 1.2084 - val_loss: 6.8575 - val_mse: 6.8575 - val_mae: 1.2495 - lr: 2.3163e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 6s - loss: 7.9006 - mse: 7.9006 - mae: 1.1928 - val_loss: 6.7664 - val_mse: 6.7664 - val_mae: 1.1672 - lr: 2.3163e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 6s - loss: 7.4434 - mse: 7.4434 - mae: 1.1765 - val_loss: 6.8423 - val_mse: 6.8423 - val_mae: 1.2488 - lr: 2.3163e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 6s - loss: 7.3942 - mse: 7.3942 - mae: 1.1702 - val_loss: 6.9229 - val_mse: 6.9229 - val_mae: 1.2402 - lr: 2.3163e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 6s - loss: 6.8060 - mse: 6.8060 - mae: 1.1424 - val_loss: 7.2651 - val_mse: 7.2651 - val_mae: 1.1980 - lr: 2.3163e-04 - 6s/epoch - 6ms/step\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-10-11 12:42:16,623]\u001b[0m Finished trial#31 resulted in value: 10.233999999999998. Current best value is 9.334 with parameters: {'activation': 'relu', 'num_hidden_layer': 4, 'i': 9, 'learning_rate': 0.00023071835273936203}.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Score for fold 5: loss of 7.265141487121582\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 7s - loss: 11.3165 - mse: 11.3165 - mae: 1.5360 - val_loss: 19.9706 - val_mse: 19.9706 - val_mae: 1.5112 - lr: 1.8353e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 6s - loss: 10.4721 - mse: 10.4721 - mae: 1.4804 - val_loss: 19.9463 - val_mse: 19.9463 - val_mae: 1.4834 - lr: 1.8353e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 6s - loss: 10.2194 - mse: 10.2194 - mae: 1.4610 - val_loss: 20.0083 - val_mse: 20.0083 - val_mae: 1.5405 - lr: 1.8353e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 6s - loss: 10.0366 - mse: 10.0366 - mae: 1.4515 - val_loss: 20.7481 - val_mse: 20.7481 - val_mae: 1.4913 - lr: 1.8353e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 6s - loss: 10.0046 - mse: 10.0046 - mae: 1.4409 - val_loss: 20.7140 - val_mse: 20.7140 - val_mae: 1.4736 - lr: 1.8353e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 6s - loss: 9.8687 - mse: 9.8687 - mae: 1.4348 - val_loss: 20.0193 - val_mse: 20.0193 - val_mae: 1.5046 - lr: 1.8353e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 6s - loss: 9.6983 - mse: 9.6983 - mae: 1.4278 - val_loss: 20.1403 - val_mse: 20.1403 - val_mae: 1.5256 - lr: 1.8353e-04 - 6s/epoch - 6ms/step\n",
            "Score for fold 1: loss of 20.14027976989746\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 6s - loss: 12.2439 - mse: 12.2439 - mae: 1.4402 - val_loss: 9.5920 - val_mse: 9.5920 - val_mae: 1.3833 - lr: 1.8353e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 6s - loss: 12.0382 - mse: 12.0382 - mae: 1.4270 - val_loss: 9.3178 - val_mse: 9.3178 - val_mae: 1.4116 - lr: 1.8353e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 6s - loss: 12.0374 - mse: 12.0374 - mae: 1.4178 - val_loss: 9.7966 - val_mse: 9.7966 - val_mae: 1.5075 - lr: 1.8353e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 6s - loss: 11.8550 - mse: 11.8550 - mae: 1.4134 - val_loss: 9.3994 - val_mse: 9.3994 - val_mae: 1.4511 - lr: 1.8353e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 6s - loss: 11.7337 - mse: 11.7337 - mae: 1.4028 - val_loss: 9.6491 - val_mse: 9.6491 - val_mae: 1.5192 - lr: 1.8353e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 6s - loss: 11.3322 - mse: 11.3322 - mae: 1.3854 - val_loss: 9.6085 - val_mse: 9.6085 - val_mae: 1.4421 - lr: 1.8353e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 6s - loss: 11.3866 - mse: 11.3866 - mae: 1.3803 - val_loss: 9.3307 - val_mse: 9.3307 - val_mae: 1.3956 - lr: 1.8353e-04 - 6s/epoch - 6ms/step\n",
            "Score for fold 2: loss of 9.33066463470459\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 6s - loss: 11.7970 - mse: 11.7970 - mae: 1.3944 - val_loss: 7.0858 - val_mse: 7.0858 - val_mae: 1.3319 - lr: 1.8353e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 6s - loss: 11.5216 - mse: 11.5216 - mae: 1.3826 - val_loss: 6.9657 - val_mse: 6.9657 - val_mae: 1.4245 - lr: 1.8353e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 6s - loss: 11.2475 - mse: 11.2475 - mae: 1.3669 - val_loss: 7.1474 - val_mse: 7.1474 - val_mae: 1.3543 - lr: 1.8353e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 6s - loss: 11.2390 - mse: 11.2390 - mae: 1.3562 - val_loss: 7.2888 - val_mse: 7.2888 - val_mae: 1.3060 - lr: 1.8353e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 6s - loss: 11.0341 - mse: 11.0341 - mae: 1.3420 - val_loss: 7.1831 - val_mse: 7.1831 - val_mae: 1.4477 - lr: 1.8353e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 6s - loss: 10.5754 - mse: 10.5754 - mae: 1.3243 - val_loss: 7.4298 - val_mse: 7.4298 - val_mae: 1.4191 - lr: 1.8353e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 6s - loss: 10.2253 - mse: 10.2253 - mae: 1.3073 - val_loss: 7.4459 - val_mse: 7.4459 - val_mae: 1.3819 - lr: 1.8353e-04 - 6s/epoch - 6ms/step\n",
            "Score for fold 3: loss of 7.445903778076172\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 6s - loss: 10.6198 - mse: 10.6198 - mae: 1.3430 - val_loss: 6.0444 - val_mse: 6.0444 - val_mae: 1.2298 - lr: 1.8353e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 6s - loss: 10.3520 - mse: 10.3520 - mae: 1.3180 - val_loss: 6.1598 - val_mse: 6.1598 - val_mae: 1.2921 - lr: 1.8353e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 6s - loss: 10.2440 - mse: 10.2440 - mae: 1.2994 - val_loss: 6.7890 - val_mse: 6.7890 - val_mae: 1.2809 - lr: 1.8353e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 6s - loss: 9.9335 - mse: 9.9335 - mae: 1.2882 - val_loss: 6.5897 - val_mse: 6.5897 - val_mae: 1.2959 - lr: 1.8353e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 6s - loss: 9.3252 - mse: 9.3252 - mae: 1.2665 - val_loss: 6.8110 - val_mse: 6.8110 - val_mae: 1.3814 - lr: 1.8353e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 6s - loss: 9.1637 - mse: 9.1637 - mae: 1.2497 - val_loss: 7.1793 - val_mse: 7.1793 - val_mae: 1.3028 - lr: 1.8353e-04 - 6s/epoch - 6ms/step\n",
            "Score for fold 4: loss of 7.179340362548828\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 6s - loss: 8.4270 - mse: 8.4270 - mae: 1.2795 - val_loss: 9.3570 - val_mse: 9.3570 - val_mae: 1.1847 - lr: 1.8353e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 6s - loss: 8.1993 - mse: 8.1993 - mae: 1.2592 - val_loss: 9.5112 - val_mse: 9.5112 - val_mae: 1.2166 - lr: 1.8353e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 6s - loss: 7.8368 - mse: 7.8368 - mae: 1.2412 - val_loss: 11.1902 - val_mse: 11.1902 - val_mae: 1.2871 - lr: 1.8353e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 6s - loss: 7.8020 - mse: 7.8020 - mae: 1.2264 - val_loss: 9.6922 - val_mse: 9.6922 - val_mae: 1.2879 - lr: 1.8353e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 6s - loss: 7.4025 - mse: 7.4025 - mae: 1.2052 - val_loss: 10.1093 - val_mse: 10.1093 - val_mae: 1.2969 - lr: 1.8353e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 6s - loss: 7.1819 - mse: 7.1819 - mae: 1.1934 - val_loss: 9.7630 - val_mse: 9.7630 - val_mae: 1.2962 - lr: 1.8353e-04 - 6s/epoch - 6ms/step\n",
            "Score for fold 5: loss of 9.762972831726074\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-10-11 12:45:47,946]\u001b[0m Finished trial#32 resulted in value: 10.772. Current best value is 9.334 with parameters: {'activation': 'relu', 'num_hidden_layer': 4, 'i': 9, 'learning_rate': 0.00023071835273936203}.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 7s - loss: 13.2914 - mse: 13.2914 - mae: 1.5325 - val_loss: 12.8227 - val_mse: 12.8227 - val_mae: 1.5557 - lr: 2.7785e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 6s - loss: 12.4192 - mse: 12.4192 - mae: 1.4826 - val_loss: 12.4860 - val_mse: 12.4860 - val_mae: 1.4468 - lr: 2.7785e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 6s - loss: 12.0220 - mse: 12.0220 - mae: 1.4658 - val_loss: 12.6453 - val_mse: 12.6453 - val_mae: 1.4810 - lr: 2.7785e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 6s - loss: 11.9247 - mse: 11.9247 - mae: 1.4578 - val_loss: 12.6681 - val_mse: 12.6681 - val_mae: 1.5185 - lr: 2.7785e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 6s - loss: 11.7753 - mse: 11.7753 - mae: 1.4503 - val_loss: 12.5970 - val_mse: 12.5970 - val_mae: 1.5439 - lr: 2.7785e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 6s - loss: 11.5718 - mse: 11.5718 - mae: 1.4433 - val_loss: 13.2177 - val_mse: 13.2177 - val_mae: 1.4654 - lr: 2.7785e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 6s - loss: 11.4057 - mse: 11.4057 - mae: 1.4337 - val_loss: 12.6071 - val_mse: 12.6071 - val_mae: 1.4900 - lr: 2.7785e-04 - 6s/epoch - 6ms/step\n",
            "Score for fold 1: loss of 12.607076644897461\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 6s - loss: 10.5394 - mse: 10.5394 - mae: 1.4320 - val_loss: 16.2347 - val_mse: 16.2347 - val_mae: 1.4877 - lr: 2.7785e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 6s - loss: 10.8102 - mse: 10.8102 - mae: 1.4320 - val_loss: 16.0162 - val_mse: 16.0162 - val_mae: 1.4271 - lr: 2.7785e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 6s - loss: 10.2391 - mse: 10.2391 - mae: 1.4137 - val_loss: 16.3183 - val_mse: 16.3183 - val_mae: 1.4450 - lr: 2.7785e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 6s - loss: 10.1710 - mse: 10.1710 - mae: 1.4010 - val_loss: 15.9691 - val_mse: 15.9691 - val_mae: 1.4731 - lr: 2.7785e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 6s - loss: 10.1616 - mse: 10.1616 - mae: 1.3929 - val_loss: 16.1630 - val_mse: 16.1630 - val_mae: 1.4522 - lr: 2.7785e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 6s - loss: 9.6994 - mse: 9.6994 - mae: 1.3828 - val_loss: 16.1645 - val_mse: 16.1645 - val_mae: 1.5077 - lr: 2.7785e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 6s - loss: 9.3949 - mse: 9.3949 - mae: 1.3627 - val_loss: 17.4048 - val_mse: 17.4048 - val_mae: 1.5174 - lr: 2.7785e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 6s - loss: 9.0460 - mse: 9.0460 - mae: 1.3535 - val_loss: 16.2233 - val_mse: 16.2233 - val_mae: 1.5958 - lr: 2.7785e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 6s - loss: 8.9354 - mse: 8.9354 - mae: 1.3338 - val_loss: 16.1761 - val_mse: 16.1761 - val_mae: 1.4946 - lr: 2.7785e-04 - 6s/epoch - 6ms/step\n",
            "Score for fold 2: loss of 16.1761474609375\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 6s - loss: 11.2336 - mse: 11.2336 - mae: 1.3782 - val_loss: 6.4246 - val_mse: 6.4246 - val_mae: 1.2663 - lr: 2.7785e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 6s - loss: 11.4850 - mse: 11.4850 - mae: 1.3596 - val_loss: 7.2795 - val_mse: 7.2795 - val_mae: 1.3029 - lr: 2.7785e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 6s - loss: 10.6427 - mse: 10.6427 - mae: 1.3383 - val_loss: 7.2784 - val_mse: 7.2784 - val_mae: 1.3318 - lr: 2.7785e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 6s - loss: 10.2206 - mse: 10.2206 - mae: 1.3275 - val_loss: 7.2635 - val_mse: 7.2635 - val_mae: 1.2640 - lr: 2.7785e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 6s - loss: 9.9495 - mse: 9.9495 - mae: 1.3129 - val_loss: 7.0441 - val_mse: 7.0441 - val_mae: 1.3376 - lr: 2.7785e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 6s - loss: 9.7699 - mse: 9.7699 - mae: 1.3025 - val_loss: 7.0345 - val_mse: 7.0345 - val_mae: 1.3439 - lr: 2.7785e-04 - 6s/epoch - 6ms/step\n",
            "Score for fold 3: loss of 7.034533500671387\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 6s - loss: 10.1337 - mse: 10.1337 - mae: 1.3310 - val_loss: 6.2621 - val_mse: 6.2621 - val_mae: 1.2556 - lr: 2.7785e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 6s - loss: 9.7478 - mse: 9.7478 - mae: 1.3043 - val_loss: 6.1413 - val_mse: 6.1413 - val_mae: 1.1902 - lr: 2.7785e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 6s - loss: 9.5080 - mse: 9.5080 - mae: 1.2850 - val_loss: 6.4333 - val_mse: 6.4333 - val_mae: 1.4595 - lr: 2.7785e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 6s - loss: 9.2262 - mse: 9.2262 - mae: 1.2719 - val_loss: 6.4920 - val_mse: 6.4920 - val_mae: 1.3274 - lr: 2.7785e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 6s - loss: 8.9140 - mse: 8.9140 - mae: 1.2565 - val_loss: 6.8168 - val_mse: 6.8168 - val_mae: 1.4366 - lr: 2.7785e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 6s - loss: 8.6541 - mse: 8.6541 - mae: 1.2424 - val_loss: 8.1954 - val_mse: 8.1954 - val_mae: 1.2613 - lr: 2.7785e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 6s - loss: 8.5747 - mse: 8.5747 - mae: 1.2256 - val_loss: 6.6413 - val_mse: 6.6413 - val_mae: 1.3732 - lr: 2.7785e-04 - 6s/epoch - 6ms/step\n",
            "Score for fold 4: loss of 6.641294479370117\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 6s - loss: 8.0530 - mse: 8.0530 - mae: 1.2567 - val_loss: 7.9999 - val_mse: 7.9999 - val_mae: 1.1382 - lr: 2.7785e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 6s - loss: 7.6757 - mse: 7.6757 - mae: 1.2351 - val_loss: 7.9742 - val_mse: 7.9742 - val_mae: 1.1816 - lr: 2.7785e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 6s - loss: 7.6131 - mse: 7.6131 - mae: 1.2246 - val_loss: 8.4433 - val_mse: 8.4433 - val_mae: 1.2388 - lr: 2.7785e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 6s - loss: 7.1613 - mse: 7.1613 - mae: 1.2096 - val_loss: 8.4191 - val_mse: 8.4191 - val_mae: 1.2620 - lr: 2.7785e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 6s - loss: 7.0723 - mse: 7.0723 - mae: 1.1949 - val_loss: 9.0075 - val_mse: 9.0075 - val_mae: 1.2131 - lr: 2.7785e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 6s - loss: 6.8182 - mse: 6.8182 - mae: 1.1785 - val_loss: 8.9496 - val_mse: 8.9496 - val_mae: 1.2429 - lr: 2.7785e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 6s - loss: 6.8069 - mse: 6.8069 - mae: 1.1613 - val_loss: 8.7400 - val_mse: 8.7400 - val_mae: 1.2399 - lr: 2.7785e-04 - 6s/epoch - 6ms/step\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-10-11 12:49:38,434]\u001b[0m Finished trial#33 resulted in value: 10.24. Current best value is 9.334 with parameters: {'activation': 'relu', 'num_hidden_layer': 4, 'i': 9, 'learning_rate': 0.00023071835273936203}.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Score for fold 5: loss of 8.739954948425293\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 4s - loss: 13.5730 - mse: 13.5730 - mae: 1.5450 - val_loss: 11.3723 - val_mse: 11.3723 - val_mae: 1.4276 - lr: 9.0905e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 3s - loss: 12.9483 - mse: 12.9483 - mae: 1.5046 - val_loss: 10.7695 - val_mse: 10.7695 - val_mae: 1.4896 - lr: 9.0905e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 4s - loss: 12.6561 - mse: 12.6561 - mae: 1.4867 - val_loss: 10.6898 - val_mse: 10.6898 - val_mae: 1.4819 - lr: 9.0905e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 4s - loss: 12.6666 - mse: 12.6666 - mae: 1.4754 - val_loss: 10.4494 - val_mse: 10.4494 - val_mae: 1.4349 - lr: 9.0905e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 4s - loss: 12.5233 - mse: 12.5233 - mae: 1.4701 - val_loss: 10.6578 - val_mse: 10.6578 - val_mae: 1.4328 - lr: 9.0905e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 4s - loss: 12.3295 - mse: 12.3295 - mae: 1.4550 - val_loss: 10.6328 - val_mse: 10.6328 - val_mae: 1.5040 - lr: 9.0905e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 3s - loss: 12.1782 - mse: 12.1782 - mae: 1.4519 - val_loss: 11.4241 - val_mse: 11.4241 - val_mae: 1.4646 - lr: 9.0905e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 3s - loss: 12.1426 - mse: 12.1426 - mae: 1.4441 - val_loss: 10.6091 - val_mse: 10.6091 - val_mae: 1.4436 - lr: 9.0905e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 3s - loss: 11.9913 - mse: 11.9913 - mae: 1.4405 - val_loss: 11.2018 - val_mse: 11.2018 - val_mae: 1.4993 - lr: 9.0905e-04 - 3s/epoch - 3ms/step\n",
            "Score for fold 1: loss of 11.201789855957031\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 4s - loss: 11.8646 - mse: 11.8646 - mae: 1.4506 - val_loss: 10.5737 - val_mse: 10.5737 - val_mae: 1.4632 - lr: 9.0905e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 3s - loss: 11.8465 - mse: 11.8465 - mae: 1.4424 - val_loss: 10.2604 - val_mse: 10.2604 - val_mae: 1.4461 - lr: 9.0905e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 3s - loss: 11.6955 - mse: 11.6955 - mae: 1.4283 - val_loss: 9.7103 - val_mse: 9.7103 - val_mae: 1.3876 - lr: 9.0905e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 3s - loss: 11.3357 - mse: 11.3357 - mae: 1.4210 - val_loss: 10.1388 - val_mse: 10.1388 - val_mae: 1.3912 - lr: 9.0905e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 4s - loss: 11.0812 - mse: 11.0812 - mae: 1.4137 - val_loss: 10.2091 - val_mse: 10.2091 - val_mae: 1.4660 - lr: 9.0905e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 3s - loss: 11.0848 - mse: 11.0848 - mae: 1.4064 - val_loss: 10.5220 - val_mse: 10.5220 - val_mae: 1.4776 - lr: 9.0905e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 4s - loss: 11.0249 - mse: 11.0249 - mae: 1.4042 - val_loss: 12.4338 - val_mse: 12.4338 - val_mae: 1.5278 - lr: 9.0905e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 3s - loss: 10.9200 - mse: 10.9200 - mae: 1.3870 - val_loss: 10.0988 - val_mse: 10.0988 - val_mae: 1.4050 - lr: 9.0905e-04 - 3s/epoch - 3ms/step\n",
            "Score for fold 2: loss of 10.098785400390625\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 3s - loss: 11.0745 - mse: 11.0745 - mae: 1.4132 - val_loss: 9.6517 - val_mse: 9.6517 - val_mae: 1.3173 - lr: 9.0905e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 3s - loss: 10.7495 - mse: 10.7495 - mae: 1.4051 - val_loss: 9.6811 - val_mse: 9.6811 - val_mae: 1.3572 - lr: 9.0905e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 3s - loss: 10.7707 - mse: 10.7707 - mae: 1.3949 - val_loss: 9.9627 - val_mse: 9.9627 - val_mae: 1.3481 - lr: 9.0905e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 4s - loss: 10.1860 - mse: 10.1860 - mae: 1.3811 - val_loss: 9.8401 - val_mse: 9.8401 - val_mae: 1.4868 - lr: 9.0905e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 3s - loss: 10.2813 - mse: 10.2813 - mae: 1.3787 - val_loss: 10.1036 - val_mse: 10.1036 - val_mae: 1.4505 - lr: 9.0905e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 3s - loss: 10.5804 - mse: 10.5804 - mae: 1.3679 - val_loss: 10.1535 - val_mse: 10.1535 - val_mae: 1.4225 - lr: 9.0905e-04 - 3s/epoch - 3ms/step\n",
            "Score for fold 3: loss of 10.153522491455078\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 4s - loss: 10.8052 - mse: 10.8052 - mae: 1.3754 - val_loss: 8.2315 - val_mse: 8.2315 - val_mae: 1.3846 - lr: 9.0905e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 3s - loss: 10.5506 - mse: 10.5506 - mae: 1.3618 - val_loss: 8.5241 - val_mse: 8.5241 - val_mae: 1.4388 - lr: 9.0905e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 3s - loss: 10.1673 - mse: 10.1673 - mae: 1.3548 - val_loss: 8.5685 - val_mse: 8.5685 - val_mae: 1.3625 - lr: 9.0905e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 4s - loss: 10.1469 - mse: 10.1469 - mae: 1.3429 - val_loss: 9.2037 - val_mse: 9.2037 - val_mae: 1.4466 - lr: 9.0905e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 4s - loss: 9.8630 - mse: 9.8630 - mae: 1.3315 - val_loss: 8.5083 - val_mse: 8.5083 - val_mae: 1.4153 - lr: 9.0905e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 3s - loss: 9.8977 - mse: 9.8977 - mae: 1.3265 - val_loss: 8.8157 - val_mse: 8.8157 - val_mae: 1.3802 - lr: 9.0905e-04 - 3s/epoch - 3ms/step\n",
            "Score for fold 4: loss of 8.815652847290039\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 4s - loss: 8.5377 - mse: 8.5377 - mae: 1.3467 - val_loss: 14.5331 - val_mse: 14.5331 - val_mae: 1.3168 - lr: 9.0905e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 3s - loss: 8.4458 - mse: 8.4458 - mae: 1.3355 - val_loss: 14.3912 - val_mse: 14.3912 - val_mae: 1.3059 - lr: 9.0905e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 3s - loss: 8.2298 - mse: 8.2298 - mae: 1.3263 - val_loss: 14.7235 - val_mse: 14.7235 - val_mae: 1.4198 - lr: 9.0905e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 3s - loss: 7.9168 - mse: 7.9168 - mae: 1.3151 - val_loss: 14.5780 - val_mse: 14.5780 - val_mae: 1.3666 - lr: 9.0905e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 3s - loss: 8.0376 - mse: 8.0376 - mae: 1.3123 - val_loss: 15.2398 - val_mse: 15.2398 - val_mae: 1.3991 - lr: 9.0905e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 3s - loss: 7.7426 - mse: 7.7426 - mae: 1.2979 - val_loss: 14.7589 - val_mse: 14.7589 - val_mae: 1.4059 - lr: 9.0905e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 3s - loss: 7.8018 - mse: 7.8018 - mae: 1.2911 - val_loss: 15.1085 - val_mse: 15.1085 - val_mae: 1.3513 - lr: 9.0905e-04 - 3s/epoch - 3ms/step\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-10-11 12:51:49,695]\u001b[0m Finished trial#34 resulted in value: 11.075999999999999. Current best value is 9.334 with parameters: {'activation': 'relu', 'num_hidden_layer': 4, 'i': 9, 'learning_rate': 0.00023071835273936203}.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Score for fold 5: loss of 15.108458518981934\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 10s - loss: 14.5064 - mse: 14.5064 - mae: 1.6452 - val_loss: 13.0489 - val_mse: 13.0489 - val_mae: 1.5314 - lr: 1.3521e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 10s - loss: 13.5761 - mse: 13.5761 - mae: 1.5712 - val_loss: 14.2465 - val_mse: 14.2465 - val_mae: 1.4978 - lr: 1.3521e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 10s - loss: 13.6217 - mse: 13.6217 - mae: 1.5647 - val_loss: 13.0537 - val_mse: 13.0537 - val_mae: 1.5819 - lr: 1.3521e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 10s - loss: 13.4539 - mse: 13.4539 - mae: 1.5548 - val_loss: 13.3881 - val_mse: 13.3881 - val_mae: 1.5327 - lr: 1.3521e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 10s - loss: 13.4840 - mse: 13.4840 - mae: 1.5534 - val_loss: 12.6993 - val_mse: 12.6993 - val_mae: 1.6155 - lr: 1.3521e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 10s - loss: 13.4500 - mse: 13.4500 - mae: 1.5475 - val_loss: 12.7166 - val_mse: 12.7166 - val_mae: 1.6121 - lr: 1.3521e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 10s - loss: 13.4379 - mse: 13.4379 - mae: 1.5501 - val_loss: 12.6402 - val_mse: 12.6402 - val_mae: 1.5713 - lr: 1.3521e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 10s - loss: 13.4667 - mse: 13.4667 - mae: 1.5495 - val_loss: 12.7233 - val_mse: 12.7233 - val_mae: 1.5587 - lr: 1.3521e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 10s - loss: 13.3846 - mse: 13.3846 - mae: 1.5488 - val_loss: 13.0650 - val_mse: 13.0650 - val_mae: 1.5485 - lr: 1.3521e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 10s - loss: 13.4811 - mse: 13.4811 - mae: 1.5488 - val_loss: 12.8748 - val_mse: 12.8748 - val_mae: 1.5571 - lr: 1.3521e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 10s - loss: 13.4402 - mse: 13.4402 - mae: 1.5489 - val_loss: 12.6761 - val_mse: 12.6761 - val_mae: 1.5802 - lr: 1.3521e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 10s - loss: 13.4261 - mse: 13.4261 - mae: 1.5437 - val_loss: 12.8319 - val_mse: 12.8319 - val_mae: 1.5482 - lr: 1.3521e-04 - 10s/epoch - 10ms/step\n",
            "Score for fold 1: loss of 12.831914901733398\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 10s - loss: 12.0771 - mse: 12.0771 - mae: 1.5425 - val_loss: 18.0071 - val_mse: 18.0071 - val_mae: 1.6041 - lr: 1.3521e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 10s - loss: 12.0317 - mse: 12.0317 - mae: 1.5416 - val_loss: 18.2433 - val_mse: 18.2433 - val_mae: 1.6470 - lr: 1.3521e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 10s - loss: 12.1253 - mse: 12.1253 - mae: 1.5446 - val_loss: 17.9802 - val_mse: 17.9802 - val_mae: 1.5570 - lr: 1.3521e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 10s - loss: 12.0844 - mse: 12.0844 - mae: 1.5471 - val_loss: 17.9291 - val_mse: 17.9291 - val_mae: 1.5594 - lr: 1.3521e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 10s - loss: 12.0842 - mse: 12.0842 - mae: 1.5448 - val_loss: 18.0169 - val_mse: 18.0169 - val_mae: 1.5508 - lr: 1.3521e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 10s - loss: 11.9631 - mse: 11.9631 - mae: 1.5401 - val_loss: 17.9235 - val_mse: 17.9235 - val_mae: 1.5426 - lr: 1.3521e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 10s - loss: 12.0382 - mse: 12.0382 - mae: 1.5430 - val_loss: 18.1399 - val_mse: 18.1399 - val_mae: 1.5896 - lr: 1.3521e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 10s - loss: 12.1098 - mse: 12.1098 - mae: 1.5449 - val_loss: 17.9700 - val_mse: 17.9700 - val_mae: 1.5897 - lr: 1.3521e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 10s - loss: 12.0164 - mse: 12.0164 - mae: 1.5438 - val_loss: 18.1197 - val_mse: 18.1197 - val_mae: 1.5369 - lr: 1.3521e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 10s - loss: 12.0452 - mse: 12.0452 - mae: 1.5477 - val_loss: 17.9364 - val_mse: 17.9364 - val_mae: 1.5787 - lr: 1.3521e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 10s - loss: 12.0728 - mse: 12.0728 - mae: 1.5447 - val_loss: 18.0730 - val_mse: 18.0730 - val_mae: 1.5177 - lr: 1.3521e-04 - 10s/epoch - 10ms/step\n",
            "Score for fold 2: loss of 18.073040008544922\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 10s - loss: 13.5173 - mse: 13.5173 - mae: 1.5492 - val_loss: 12.5297 - val_mse: 12.5297 - val_mae: 1.5161 - lr: 1.3521e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 10s - loss: 13.4219 - mse: 13.4219 - mae: 1.5515 - val_loss: 12.4680 - val_mse: 12.4680 - val_mae: 1.5137 - lr: 1.3521e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 10s - loss: 13.5304 - mse: 13.5304 - mae: 1.5447 - val_loss: 12.4342 - val_mse: 12.4342 - val_mae: 1.5660 - lr: 1.3521e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 10s - loss: 13.4152 - mse: 13.4152 - mae: 1.5498 - val_loss: 12.4978 - val_mse: 12.4978 - val_mae: 1.5057 - lr: 1.3521e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 10s - loss: 13.4925 - mse: 13.4925 - mae: 1.5504 - val_loss: 12.4428 - val_mse: 12.4428 - val_mae: 1.5378 - lr: 1.3521e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 10s - loss: 13.4455 - mse: 13.4455 - mae: 1.5492 - val_loss: 12.4581 - val_mse: 12.4581 - val_mae: 1.5488 - lr: 1.3521e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 10s - loss: 13.4640 - mse: 13.4640 - mae: 1.5549 - val_loss: 12.4409 - val_mse: 12.4409 - val_mae: 1.5403 - lr: 1.3521e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 9s - loss: 13.4143 - mse: 13.4143 - mae: 1.5491 - val_loss: 12.5062 - val_mse: 12.5062 - val_mae: 1.5301 - lr: 1.3521e-04 - 9s/epoch - 9ms/step\n",
            "Score for fold 3: loss of 12.50617790222168\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 10s - loss: 13.2930 - mse: 13.2930 - mae: 1.5442 - val_loss: 13.6260 - val_mse: 13.6260 - val_mae: 1.6174 - lr: 1.3521e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 10s - loss: 13.4055 - mse: 13.4055 - mae: 1.5468 - val_loss: 12.6037 - val_mse: 12.6037 - val_mae: 1.4938 - lr: 1.3521e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 10s - loss: 13.3721 - mse: 13.3721 - mae: 1.5450 - val_loss: 13.0204 - val_mse: 13.0204 - val_mae: 1.5798 - lr: 1.3521e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 10s - loss: 13.4430 - mse: 13.4430 - mae: 1.5443 - val_loss: 12.6523 - val_mse: 12.6523 - val_mae: 1.5472 - lr: 1.3521e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 10s - loss: 13.4013 - mse: 13.4013 - mae: 1.5443 - val_loss: 12.7374 - val_mse: 12.7374 - val_mae: 1.5319 - lr: 1.3521e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 10s - loss: 13.2959 - mse: 13.2959 - mae: 1.5405 - val_loss: 12.7390 - val_mse: 12.7390 - val_mae: 1.5539 - lr: 1.3521e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 10s - loss: 13.3628 - mse: 13.3628 - mae: 1.5443 - val_loss: 12.7898 - val_mse: 12.7898 - val_mae: 1.5735 - lr: 1.3521e-04 - 10s/epoch - 10ms/step\n",
            "Score for fold 4: loss of 12.7898530960083\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 10s - loss: 14.0126 - mse: 14.0126 - mae: 1.5635 - val_loss: 10.5380 - val_mse: 10.5380 - val_mae: 1.4547 - lr: 1.3521e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 10s - loss: 14.0052 - mse: 14.0052 - mae: 1.5598 - val_loss: 10.3414 - val_mse: 10.3414 - val_mae: 1.4634 - lr: 1.3521e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 10s - loss: 13.9663 - mse: 13.9663 - mae: 1.5515 - val_loss: 10.3245 - val_mse: 10.3245 - val_mae: 1.4857 - lr: 1.3521e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 10s - loss: 14.0406 - mse: 14.0406 - mae: 1.5588 - val_loss: 10.3458 - val_mse: 10.3458 - val_mae: 1.5033 - lr: 1.3521e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 10s - loss: 13.9451 - mse: 13.9451 - mae: 1.5591 - val_loss: 10.3998 - val_mse: 10.3998 - val_mae: 1.5295 - lr: 1.3521e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 10s - loss: 14.0645 - mse: 14.0645 - mae: 1.5600 - val_loss: 10.4188 - val_mse: 10.4188 - val_mae: 1.5249 - lr: 1.3521e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 10s - loss: 13.9776 - mse: 13.9776 - mae: 1.5640 - val_loss: 10.4286 - val_mse: 10.4286 - val_mae: 1.4954 - lr: 1.3521e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 10s - loss: 14.0281 - mse: 14.0281 - mae: 1.5642 - val_loss: 10.4498 - val_mse: 10.4498 - val_mae: 1.5360 - lr: 1.3521e-04 - 10s/epoch - 10ms/step\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-10-11 12:59:40,844]\u001b[0m Finished trial#35 resulted in value: 13.329999999999998. Current best value is 9.334 with parameters: {'activation': 'relu', 'num_hidden_layer': 4, 'i': 9, 'learning_rate': 0.00023071835273936203}.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Score for fold 5: loss of 10.44983959197998\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 7s - loss: 15.2385 - mse: 15.2385 - mae: 1.6299 - val_loss: 10.9586 - val_mse: 10.9586 - val_mae: 1.4642 - lr: 4.6012e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 7s - loss: 14.5414 - mse: 14.5414 - mae: 1.5453 - val_loss: 11.1671 - val_mse: 11.1671 - val_mae: 1.5790 - lr: 4.6012e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 6s - loss: 14.3022 - mse: 14.3022 - mae: 1.5303 - val_loss: 10.8305 - val_mse: 10.8305 - val_mae: 1.4823 - lr: 4.6012e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 7s - loss: 14.0288 - mse: 14.0288 - mae: 1.5204 - val_loss: 10.8206 - val_mse: 10.8206 - val_mae: 1.4884 - lr: 4.6012e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 7s - loss: 13.9326 - mse: 13.9326 - mae: 1.5141 - val_loss: 10.6769 - val_mse: 10.6769 - val_mae: 1.4732 - lr: 4.6012e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 7s - loss: 13.7668 - mse: 13.7668 - mae: 1.5025 - val_loss: 10.6907 - val_mse: 10.6907 - val_mae: 1.4873 - lr: 4.6012e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 7s - loss: 13.6336 - mse: 13.6336 - mae: 1.4938 - val_loss: 10.3822 - val_mse: 10.3822 - val_mae: 1.4936 - lr: 4.6012e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 7s - loss: 13.5796 - mse: 13.5796 - mae: 1.4888 - val_loss: 10.3551 - val_mse: 10.3551 - val_mae: 1.4918 - lr: 4.6012e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 7s - loss: 13.4580 - mse: 13.4580 - mae: 1.4854 - val_loss: 10.3684 - val_mse: 10.3684 - val_mae: 1.4722 - lr: 4.6012e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 7s - loss: 13.3627 - mse: 13.3627 - mae: 1.4758 - val_loss: 10.5966 - val_mse: 10.5966 - val_mae: 1.4959 - lr: 4.6012e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 7s - loss: 13.2666 - mse: 13.2666 - mae: 1.4669 - val_loss: 10.6239 - val_mse: 10.6239 - val_mae: 1.4964 - lr: 4.6012e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 6s - loss: 13.1598 - mse: 13.1598 - mae: 1.4649 - val_loss: 10.3005 - val_mse: 10.3005 - val_mae: 1.4905 - lr: 4.6012e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 13/100\n",
            "1000/1000 - 6s - loss: 13.0809 - mse: 13.0809 - mae: 1.4614 - val_loss: 10.4025 - val_mse: 10.4025 - val_mae: 1.4508 - lr: 4.6012e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 14/100\n",
            "1000/1000 - 7s - loss: 12.9591 - mse: 12.9591 - mae: 1.4520 - val_loss: 10.2320 - val_mse: 10.2320 - val_mae: 1.4830 - lr: 4.6012e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 15/100\n",
            "1000/1000 - 6s - loss: 12.7570 - mse: 12.7570 - mae: 1.4425 - val_loss: 10.2351 - val_mse: 10.2351 - val_mae: 1.4639 - lr: 4.6012e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 16/100\n",
            "1000/1000 - 7s - loss: 12.6754 - mse: 12.6754 - mae: 1.4422 - val_loss: 10.1800 - val_mse: 10.1800 - val_mae: 1.5258 - lr: 4.6012e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 17/100\n",
            "1000/1000 - 6s - loss: 12.5219 - mse: 12.5219 - mae: 1.4311 - val_loss: 10.1909 - val_mse: 10.1909 - val_mae: 1.4678 - lr: 4.6012e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 18/100\n",
            "1000/1000 - 6s - loss: 12.3104 - mse: 12.3104 - mae: 1.4190 - val_loss: 10.0085 - val_mse: 10.0085 - val_mae: 1.4964 - lr: 4.6012e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 19/100\n",
            "1000/1000 - 6s - loss: 12.1071 - mse: 12.1071 - mae: 1.4067 - val_loss: 10.1983 - val_mse: 10.1983 - val_mae: 1.5519 - lr: 4.6012e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 20/100\n",
            "1000/1000 - 6s - loss: 11.9002 - mse: 11.9002 - mae: 1.3969 - val_loss: 10.1550 - val_mse: 10.1550 - val_mae: 1.5432 - lr: 4.6012e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 21/100\n",
            "1000/1000 - 6s - loss: 11.8049 - mse: 11.8049 - mae: 1.3842 - val_loss: 10.1116 - val_mse: 10.1116 - val_mae: 1.5138 - lr: 4.6012e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 22/100\n",
            "1000/1000 - 6s - loss: 11.4622 - mse: 11.4622 - mae: 1.3681 - val_loss: 10.0553 - val_mse: 10.0553 - val_mae: 1.5395 - lr: 4.6012e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 23/100\n",
            "1000/1000 - 6s - loss: 11.2211 - mse: 11.2211 - mae: 1.3659 - val_loss: 10.1310 - val_mse: 10.1310 - val_mae: 1.5246 - lr: 4.6012e-04 - 6s/epoch - 6ms/step\n",
            "Score for fold 1: loss of 10.131033897399902\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 6s - loss: 9.8576 - mse: 9.8576 - mae: 1.3982 - val_loss: 15.8628 - val_mse: 15.8628 - val_mae: 1.3862 - lr: 4.6012e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 7s - loss: 9.6174 - mse: 9.6174 - mae: 1.3764 - val_loss: 15.8465 - val_mse: 15.8465 - val_mae: 1.3826 - lr: 4.6012e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 6s - loss: 9.3991 - mse: 9.3991 - mae: 1.3525 - val_loss: 15.7896 - val_mse: 15.7896 - val_mae: 1.4182 - lr: 4.6012e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 6s - loss: 9.0800 - mse: 9.0800 - mae: 1.3351 - val_loss: 15.8740 - val_mse: 15.8740 - val_mae: 1.4963 - lr: 4.6012e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 7s - loss: 8.7027 - mse: 8.7027 - mae: 1.3170 - val_loss: 15.9396 - val_mse: 15.9396 - val_mae: 1.4880 - lr: 4.6012e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 7s - loss: 8.5189 - mse: 8.5189 - mae: 1.2918 - val_loss: 16.1740 - val_mse: 16.1740 - val_mae: 1.4837 - lr: 4.6012e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 7s - loss: 8.1227 - mse: 8.1227 - mae: 1.2710 - val_loss: 16.1834 - val_mse: 16.1834 - val_mae: 1.4861 - lr: 4.6012e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 7s - loss: 7.7817 - mse: 7.7817 - mae: 1.2434 - val_loss: 16.2385 - val_mse: 16.2385 - val_mae: 1.5055 - lr: 4.6012e-04 - 7s/epoch - 7ms/step\n",
            "Score for fold 2: loss of 16.238527297973633\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 6s - loss: 10.4833 - mse: 10.4833 - mae: 1.3123 - val_loss: 5.3399 - val_mse: 5.3399 - val_mae: 1.2072 - lr: 4.6012e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 6s - loss: 10.0274 - mse: 10.0274 - mae: 1.2837 - val_loss: 5.6465 - val_mse: 5.6465 - val_mae: 1.2476 - lr: 4.6012e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 6s - loss: 9.6805 - mse: 9.6805 - mae: 1.2498 - val_loss: 5.5185 - val_mse: 5.5185 - val_mae: 1.2583 - lr: 4.6012e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 6s - loss: 9.3636 - mse: 9.3636 - mae: 1.2227 - val_loss: 5.7659 - val_mse: 5.7659 - val_mae: 1.3037 - lr: 4.6012e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 6s - loss: 9.0140 - mse: 9.0140 - mae: 1.1959 - val_loss: 5.4664 - val_mse: 5.4664 - val_mae: 1.2723 - lr: 4.6012e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 6s - loss: 8.8065 - mse: 8.8065 - mae: 1.1702 - val_loss: 5.4629 - val_mse: 5.4629 - val_mae: 1.3142 - lr: 4.6012e-04 - 6s/epoch - 6ms/step\n",
            "Score for fold 3: loss of 5.462864875793457\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 6s - loss: 8.3664 - mse: 8.3664 - mae: 1.2151 - val_loss: 6.7692 - val_mse: 6.7692 - val_mae: 1.1708 - lr: 4.6012e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 6s - loss: 7.9326 - mse: 7.9326 - mae: 1.1652 - val_loss: 6.9243 - val_mse: 6.9243 - val_mae: 1.2372 - lr: 4.6012e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 6s - loss: 7.5887 - mse: 7.5887 - mae: 1.1395 - val_loss: 6.8401 - val_mse: 6.8401 - val_mae: 1.1789 - lr: 4.6012e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 6s - loss: 7.3451 - mse: 7.3451 - mae: 1.1100 - val_loss: 7.1058 - val_mse: 7.1058 - val_mae: 1.1996 - lr: 4.6012e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 6s - loss: 7.0099 - mse: 7.0099 - mae: 1.0766 - val_loss: 7.2347 - val_mse: 7.2347 - val_mae: 1.2566 - lr: 4.6012e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 6s - loss: 6.7695 - mse: 6.7695 - mae: 1.0496 - val_loss: 7.0389 - val_mse: 7.0389 - val_mae: 1.2496 - lr: 4.6012e-04 - 6s/epoch - 6ms/step\n",
            "Score for fold 4: loss of 7.038918972015381\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 6s - loss: 6.5453 - mse: 6.5453 - mae: 1.1066 - val_loss: 7.4445 - val_mse: 7.4445 - val_mae: 0.9862 - lr: 4.6012e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 6s - loss: 6.1868 - mse: 6.1868 - mae: 1.0648 - val_loss: 7.7909 - val_mse: 7.7909 - val_mae: 1.0697 - lr: 4.6012e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 6s - loss: 5.9276 - mse: 5.9276 - mae: 1.0345 - val_loss: 7.7881 - val_mse: 7.7881 - val_mae: 1.0757 - lr: 4.6012e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 6s - loss: 5.7068 - mse: 5.7068 - mae: 1.0019 - val_loss: 7.9018 - val_mse: 7.9018 - val_mae: 1.0636 - lr: 4.6012e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 6s - loss: 5.4198 - mse: 5.4198 - mae: 0.9729 - val_loss: 7.9867 - val_mse: 7.9867 - val_mae: 1.0871 - lr: 4.6012e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 6s - loss: 5.2151 - mse: 5.2151 - mae: 0.9430 - val_loss: 8.0247 - val_mse: 8.0247 - val_mae: 1.1243 - lr: 4.6012e-04 - 6s/epoch - 6ms/step\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-10-11 13:05:07,104]\u001b[0m Finished trial#36 resulted in value: 9.378. Current best value is 9.334 with parameters: {'activation': 'relu', 'num_hidden_layer': 4, 'i': 9, 'learning_rate': 0.00023071835273936203}.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Score for fold 5: loss of 8.02465534210205\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 8s - loss: 15.5096 - mse: 15.5096 - mae: 1.6334 - val_loss: 9.3087 - val_mse: 9.3087 - val_mae: 1.4840 - lr: 4.4134e-04 - 8s/epoch - 8ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 7s - loss: 14.7814 - mse: 14.7814 - mae: 1.5468 - val_loss: 9.5644 - val_mse: 9.5644 - val_mae: 1.4999 - lr: 4.4134e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 7s - loss: 14.7048 - mse: 14.7048 - mae: 1.5276 - val_loss: 9.4354 - val_mse: 9.4354 - val_mae: 1.4609 - lr: 4.4134e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 7s - loss: 14.5158 - mse: 14.5158 - mae: 1.5218 - val_loss: 9.0365 - val_mse: 9.0365 - val_mae: 1.4396 - lr: 4.4134e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 7s - loss: 14.3992 - mse: 14.3992 - mae: 1.5176 - val_loss: 9.2064 - val_mse: 9.2064 - val_mae: 1.4096 - lr: 4.4134e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 7s - loss: 14.3298 - mse: 14.3298 - mae: 1.5067 - val_loss: 9.0290 - val_mse: 9.0290 - val_mae: 1.4869 - lr: 4.4134e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 7s - loss: 14.1822 - mse: 14.1822 - mae: 1.4982 - val_loss: 9.0028 - val_mse: 9.0028 - val_mae: 1.4553 - lr: 4.4134e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 7s - loss: 14.0288 - mse: 14.0288 - mae: 1.4964 - val_loss: 8.8606 - val_mse: 8.8606 - val_mae: 1.5101 - lr: 4.4134e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 7s - loss: 14.0029 - mse: 14.0029 - mae: 1.4892 - val_loss: 8.7817 - val_mse: 8.7817 - val_mae: 1.5055 - lr: 4.4134e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 7s - loss: 13.8400 - mse: 13.8400 - mae: 1.4848 - val_loss: 8.7955 - val_mse: 8.7955 - val_mae: 1.4448 - lr: 4.4134e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 7s - loss: 13.7366 - mse: 13.7366 - mae: 1.4795 - val_loss: 8.7932 - val_mse: 8.7932 - val_mae: 1.4688 - lr: 4.4134e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 7s - loss: 13.6882 - mse: 13.6882 - mae: 1.4744 - val_loss: 8.6677 - val_mse: 8.6677 - val_mae: 1.4484 - lr: 4.4134e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 13/100\n",
            "1000/1000 - 7s - loss: 13.5198 - mse: 13.5198 - mae: 1.4688 - val_loss: 8.5139 - val_mse: 8.5139 - val_mae: 1.4207 - lr: 4.4134e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 14/100\n",
            "1000/1000 - 7s - loss: 13.3984 - mse: 13.3984 - mae: 1.4617 - val_loss: 8.7014 - val_mse: 8.7014 - val_mae: 1.5092 - lr: 4.4134e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 15/100\n",
            "1000/1000 - 7s - loss: 13.2756 - mse: 13.2756 - mae: 1.4588 - val_loss: 8.7605 - val_mse: 8.7605 - val_mae: 1.4543 - lr: 4.4134e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 16/100\n",
            "1000/1000 - 7s - loss: 13.2735 - mse: 13.2735 - mae: 1.4540 - val_loss: 8.6031 - val_mse: 8.6031 - val_mae: 1.4954 - lr: 4.4134e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 17/100\n",
            "1000/1000 - 7s - loss: 13.1020 - mse: 13.1020 - mae: 1.4514 - val_loss: 8.8050 - val_mse: 8.8050 - val_mae: 1.4279 - lr: 4.4134e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 18/100\n",
            "1000/1000 - 7s - loss: 13.0081 - mse: 13.0081 - mae: 1.4484 - val_loss: 8.9547 - val_mse: 8.9547 - val_mae: 1.5907 - lr: 4.4134e-04 - 7s/epoch - 7ms/step\n",
            "Score for fold 1: loss of 8.954693794250488\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 7s - loss: 10.8700 - mse: 10.8700 - mae: 1.4495 - val_loss: 16.8188 - val_mse: 16.8188 - val_mae: 1.4437 - lr: 4.4134e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 7s - loss: 10.6941 - mse: 10.6941 - mae: 1.4354 - val_loss: 17.1558 - val_mse: 17.1558 - val_mae: 1.4807 - lr: 4.4134e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 7s - loss: 10.6324 - mse: 10.6324 - mae: 1.4299 - val_loss: 17.0719 - val_mse: 17.0719 - val_mae: 1.5214 - lr: 4.4134e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 7s - loss: 10.4644 - mse: 10.4644 - mae: 1.4252 - val_loss: 16.9662 - val_mse: 16.9662 - val_mae: 1.4826 - lr: 4.4134e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 7s - loss: 10.3575 - mse: 10.3575 - mae: 1.4182 - val_loss: 17.0461 - val_mse: 17.0461 - val_mae: 1.4772 - lr: 4.4134e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 7s - loss: 10.2615 - mse: 10.2615 - mae: 1.4115 - val_loss: 16.9472 - val_mse: 16.9472 - val_mae: 1.5524 - lr: 4.4134e-04 - 7s/epoch - 7ms/step\n",
            "Score for fold 2: loss of 16.94721221923828\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 7s - loss: 11.4155 - mse: 11.4155 - mae: 1.4331 - val_loss: 12.0676 - val_mse: 12.0676 - val_mae: 1.4223 - lr: 4.4134e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 7s - loss: 11.2669 - mse: 11.2669 - mae: 1.4212 - val_loss: 12.2185 - val_mse: 12.2185 - val_mae: 1.4742 - lr: 4.4134e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 7s - loss: 11.1472 - mse: 11.1472 - mae: 1.4142 - val_loss: 12.3086 - val_mse: 12.3086 - val_mae: 1.4606 - lr: 4.4134e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 7s - loss: 11.0720 - mse: 11.0720 - mae: 1.4136 - val_loss: 12.1209 - val_mse: 12.1209 - val_mae: 1.4421 - lr: 4.4134e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 7s - loss: 10.9303 - mse: 10.9303 - mae: 1.4010 - val_loss: 12.4699 - val_mse: 12.4699 - val_mae: 1.4863 - lr: 4.4134e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 7s - loss: 10.7535 - mse: 10.7535 - mae: 1.3991 - val_loss: 12.5703 - val_mse: 12.5703 - val_mae: 1.5213 - lr: 4.4134e-04 - 7s/epoch - 7ms/step\n",
            "Score for fold 3: loss of 12.570316314697266\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 7s - loss: 11.3606 - mse: 11.3606 - mae: 1.4248 - val_loss: 10.1589 - val_mse: 10.1589 - val_mae: 1.4045 - lr: 4.4134e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 7s - loss: 11.1963 - mse: 11.1963 - mae: 1.4072 - val_loss: 10.0013 - val_mse: 10.0013 - val_mae: 1.4320 - lr: 4.4134e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 7s - loss: 10.9780 - mse: 10.9780 - mae: 1.3982 - val_loss: 10.1063 - val_mse: 10.1063 - val_mae: 1.4328 - lr: 4.4134e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 7s - loss: 10.8212 - mse: 10.8212 - mae: 1.3938 - val_loss: 10.2608 - val_mse: 10.2608 - val_mae: 1.3838 - lr: 4.4134e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 7s - loss: 10.7012 - mse: 10.7012 - mae: 1.3858 - val_loss: 10.6675 - val_mse: 10.6675 - val_mae: 1.6189 - lr: 4.4134e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 7s - loss: 10.5773 - mse: 10.5773 - mae: 1.3820 - val_loss: 10.5197 - val_mse: 10.5197 - val_mae: 1.4439 - lr: 4.4134e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 7s - loss: 10.4197 - mse: 10.4197 - mae: 1.3770 - val_loss: 10.4920 - val_mse: 10.4920 - val_mae: 1.4441 - lr: 4.4134e-04 - 7s/epoch - 7ms/step\n",
            "Score for fold 4: loss of 10.491984367370605\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 7s - loss: 10.6460 - mse: 10.6460 - mae: 1.4151 - val_loss: 9.1592 - val_mse: 9.1592 - val_mae: 1.3994 - lr: 4.4134e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 7s - loss: 10.4161 - mse: 10.4161 - mae: 1.3947 - val_loss: 9.3736 - val_mse: 9.3736 - val_mae: 1.3687 - lr: 4.4134e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 7s - loss: 10.2658 - mse: 10.2658 - mae: 1.3844 - val_loss: 9.5892 - val_mse: 9.5892 - val_mae: 1.4139 - lr: 4.4134e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 7s - loss: 10.1319 - mse: 10.1319 - mae: 1.3770 - val_loss: 9.8364 - val_mse: 9.8364 - val_mae: 1.4241 - lr: 4.4134e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 7s - loss: 9.9922 - mse: 9.9922 - mae: 1.3719 - val_loss: 9.9321 - val_mse: 9.9321 - val_mae: 1.4790 - lr: 4.4134e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 7s - loss: 9.8230 - mse: 9.8230 - mae: 1.3679 - val_loss: 9.9229 - val_mse: 9.9229 - val_mae: 1.5528 - lr: 4.4134e-04 - 7s/epoch - 7ms/step\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-10-11 13:10:16,504]\u001b[0m Finished trial#37 resulted in value: 11.776. Current best value is 9.334 with parameters: {'activation': 'relu', 'num_hidden_layer': 4, 'i': 9, 'learning_rate': 0.00023071835273936203}.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Score for fold 5: loss of 9.92287826538086\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 4s - loss: 15.2209 - mse: 15.2209 - mae: 1.6065 - val_loss: 10.6342 - val_mse: 10.6342 - val_mae: 1.5369 - lr: 7.8670e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 3s - loss: 14.5331 - mse: 14.5331 - mae: 1.5415 - val_loss: 10.4266 - val_mse: 10.4266 - val_mae: 1.4347 - lr: 7.8670e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 3s - loss: 14.3805 - mse: 14.3805 - mae: 1.5206 - val_loss: 10.2324 - val_mse: 10.2324 - val_mae: 1.5627 - lr: 7.8670e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 4s - loss: 14.2156 - mse: 14.2156 - mae: 1.5166 - val_loss: 10.1761 - val_mse: 10.1761 - val_mae: 1.4891 - lr: 7.8670e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 4s - loss: 14.0365 - mse: 14.0365 - mae: 1.5058 - val_loss: 9.8957 - val_mse: 9.8957 - val_mae: 1.4737 - lr: 7.8670e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 4s - loss: 13.8724 - mse: 13.8724 - mae: 1.4921 - val_loss: 10.0307 - val_mse: 10.0307 - val_mae: 1.4540 - lr: 7.8670e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 4s - loss: 13.7673 - mse: 13.7673 - mae: 1.4849 - val_loss: 10.2840 - val_mse: 10.2840 - val_mae: 1.4339 - lr: 7.8670e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 4s - loss: 13.6285 - mse: 13.6285 - mae: 1.4784 - val_loss: 9.9654 - val_mse: 9.9654 - val_mae: 1.4498 - lr: 7.8670e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 4s - loss: 13.5323 - mse: 13.5323 - mae: 1.4715 - val_loss: 9.8168 - val_mse: 9.8168 - val_mae: 1.4721 - lr: 7.8670e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 3s - loss: 13.4370 - mse: 13.4370 - mae: 1.4662 - val_loss: 10.0786 - val_mse: 10.0786 - val_mae: 1.5243 - lr: 7.8670e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 4s - loss: 13.2924 - mse: 13.2924 - mae: 1.4621 - val_loss: 10.1855 - val_mse: 10.1855 - val_mae: 1.5216 - lr: 7.8670e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 3s - loss: 13.2040 - mse: 13.2040 - mae: 1.4529 - val_loss: 9.4868 - val_mse: 9.4868 - val_mae: 1.4875 - lr: 7.8670e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 13/100\n",
            "1000/1000 - 3s - loss: 12.9985 - mse: 12.9985 - mae: 1.4437 - val_loss: 9.6451 - val_mse: 9.6451 - val_mae: 1.5111 - lr: 7.8670e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 14/100\n",
            "1000/1000 - 3s - loss: 12.8151 - mse: 12.8151 - mae: 1.4354 - val_loss: 10.0001 - val_mse: 10.0001 - val_mae: 1.4515 - lr: 7.8670e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 15/100\n",
            "1000/1000 - 3s - loss: 12.7049 - mse: 12.7049 - mae: 1.4287 - val_loss: 9.6695 - val_mse: 9.6695 - val_mae: 1.5359 - lr: 7.8670e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 16/100\n",
            "1000/1000 - 4s - loss: 12.4937 - mse: 12.4937 - mae: 1.4196 - val_loss: 9.6918 - val_mse: 9.6918 - val_mae: 1.5158 - lr: 7.8670e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 17/100\n",
            "1000/1000 - 4s - loss: 12.3497 - mse: 12.3497 - mae: 1.4105 - val_loss: 9.6044 - val_mse: 9.6044 - val_mae: 1.5242 - lr: 7.8670e-04 - 4s/epoch - 4ms/step\n",
            "Score for fold 1: loss of 9.604376792907715\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 4s - loss: 11.9894 - mse: 11.9894 - mae: 1.4369 - val_loss: 10.7894 - val_mse: 10.7894 - val_mae: 1.4607 - lr: 7.8670e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 4s - loss: 11.7823 - mse: 11.7823 - mae: 1.4226 - val_loss: 10.7525 - val_mse: 10.7525 - val_mae: 1.4104 - lr: 7.8670e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 4s - loss: 11.6122 - mse: 11.6122 - mae: 1.4134 - val_loss: 10.7853 - val_mse: 10.7853 - val_mae: 1.4778 - lr: 7.8670e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 3s - loss: 11.3731 - mse: 11.3731 - mae: 1.3919 - val_loss: 10.9815 - val_mse: 10.9815 - val_mae: 1.4758 - lr: 7.8670e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 4s - loss: 11.1615 - mse: 11.1615 - mae: 1.3822 - val_loss: 10.7051 - val_mse: 10.7051 - val_mae: 1.4162 - lr: 7.8670e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 4s - loss: 10.9835 - mse: 10.9835 - mae: 1.3738 - val_loss: 10.8253 - val_mse: 10.8253 - val_mae: 1.4322 - lr: 7.8670e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 4s - loss: 10.8851 - mse: 10.8851 - mae: 1.3601 - val_loss: 10.7919 - val_mse: 10.7919 - val_mae: 1.5312 - lr: 7.8670e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 3s - loss: 10.5321 - mse: 10.5321 - mae: 1.3465 - val_loss: 10.7766 - val_mse: 10.7766 - val_mae: 1.5226 - lr: 7.8670e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 3s - loss: 10.4086 - mse: 10.4086 - mae: 1.3319 - val_loss: 10.6900 - val_mse: 10.6900 - val_mae: 1.4602 - lr: 7.8670e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 3s - loss: 10.1861 - mse: 10.1861 - mae: 1.3166 - val_loss: 10.8572 - val_mse: 10.8572 - val_mae: 1.4847 - lr: 7.8670e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 4s - loss: 10.0473 - mse: 10.0473 - mae: 1.3063 - val_loss: 11.0013 - val_mse: 11.0013 - val_mae: 1.5285 - lr: 7.8670e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 3s - loss: 9.7114 - mse: 9.7114 - mae: 1.2905 - val_loss: 10.7942 - val_mse: 10.7942 - val_mae: 1.5430 - lr: 7.8670e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 13/100\n",
            "1000/1000 - 3s - loss: 9.5701 - mse: 9.5701 - mae: 1.2770 - val_loss: 10.8132 - val_mse: 10.8132 - val_mae: 1.5455 - lr: 7.8670e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 14/100\n",
            "1000/1000 - 3s - loss: 9.4393 - mse: 9.4393 - mae: 1.2656 - val_loss: 10.6306 - val_mse: 10.6306 - val_mae: 1.5696 - lr: 7.8670e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 15/100\n",
            "1000/1000 - 3s - loss: 9.2146 - mse: 9.2146 - mae: 1.2507 - val_loss: 10.8136 - val_mse: 10.8136 - val_mae: 1.5514 - lr: 7.8670e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 16/100\n",
            "1000/1000 - 4s - loss: 8.9217 - mse: 8.9217 - mae: 1.2352 - val_loss: 10.6362 - val_mse: 10.6362 - val_mae: 1.5812 - lr: 7.8670e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 17/100\n",
            "1000/1000 - 4s - loss: 8.7168 - mse: 8.7168 - mae: 1.2207 - val_loss: 10.7722 - val_mse: 10.7722 - val_mae: 1.6316 - lr: 7.8670e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 18/100\n",
            "1000/1000 - 4s - loss: 8.5200 - mse: 8.5200 - mae: 1.2083 - val_loss: 11.0632 - val_mse: 11.0632 - val_mae: 1.6150 - lr: 7.8670e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 19/100\n",
            "1000/1000 - 4s - loss: 8.2851 - mse: 8.2851 - mae: 1.1944 - val_loss: 10.8085 - val_mse: 10.8085 - val_mae: 1.6135 - lr: 7.8670e-04 - 4s/epoch - 4ms/step\n",
            "Score for fold 2: loss of 10.808479309082031\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 3s - loss: 9.8562 - mse: 9.8562 - mae: 1.3235 - val_loss: 5.3819 - val_mse: 5.3819 - val_mae: 1.2042 - lr: 7.8670e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 3s - loss: 9.5453 - mse: 9.5453 - mae: 1.2884 - val_loss: 5.7622 - val_mse: 5.7622 - val_mae: 1.2780 - lr: 7.8670e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 4s - loss: 9.2251 - mse: 9.2251 - mae: 1.2577 - val_loss: 5.6396 - val_mse: 5.6396 - val_mae: 1.2305 - lr: 7.8670e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 4s - loss: 9.0300 - mse: 9.0300 - mae: 1.2383 - val_loss: 5.7545 - val_mse: 5.7545 - val_mae: 1.2694 - lr: 7.8670e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 4s - loss: 8.7560 - mse: 8.7560 - mae: 1.2186 - val_loss: 5.6785 - val_mse: 5.6785 - val_mae: 1.2740 - lr: 7.8670e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 4s - loss: 8.5116 - mse: 8.5116 - mae: 1.1988 - val_loss: 5.8404 - val_mse: 5.8404 - val_mae: 1.3001 - lr: 7.8670e-04 - 4s/epoch - 4ms/step\n",
            "Score for fold 3: loss of 5.840417385101318\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 4s - loss: 6.6123 - mse: 6.6123 - mae: 1.2452 - val_loss: 13.1998 - val_mse: 13.1998 - val_mae: 1.1380 - lr: 7.8670e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 4s - loss: 6.3376 - mse: 6.3376 - mae: 1.2038 - val_loss: 13.2294 - val_mse: 13.2294 - val_mae: 1.1533 - lr: 7.8670e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 3s - loss: 6.0696 - mse: 6.0696 - mae: 1.1874 - val_loss: 13.4209 - val_mse: 13.4209 - val_mae: 1.2207 - lr: 7.8670e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 3s - loss: 5.8098 - mse: 5.8098 - mae: 1.1661 - val_loss: 13.3568 - val_mse: 13.3568 - val_mae: 1.2401 - lr: 7.8670e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 3s - loss: 5.6357 - mse: 5.6357 - mae: 1.1476 - val_loss: 13.6023 - val_mse: 13.6023 - val_mae: 1.2556 - lr: 7.8670e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 3s - loss: 5.5292 - mse: 5.5292 - mae: 1.1317 - val_loss: 13.7296 - val_mse: 13.7296 - val_mae: 1.2400 - lr: 7.8670e-04 - 3s/epoch - 3ms/step\n",
            "Score for fold 4: loss of 13.729551315307617\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 4s - loss: 6.8468 - mse: 6.8468 - mae: 1.1693 - val_loss: 8.2464 - val_mse: 8.2464 - val_mae: 1.0865 - lr: 7.8670e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 4s - loss: 6.5812 - mse: 6.5812 - mae: 1.1365 - val_loss: 8.2460 - val_mse: 8.2460 - val_mae: 1.1089 - lr: 7.8670e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 4s - loss: 6.4062 - mse: 6.4062 - mae: 1.1170 - val_loss: 8.4473 - val_mse: 8.4473 - val_mae: 1.1511 - lr: 7.8670e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 4s - loss: 6.2219 - mse: 6.2219 - mae: 1.0977 - val_loss: 8.5605 - val_mse: 8.5605 - val_mae: 1.1660 - lr: 7.8670e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 4s - loss: 6.1299 - mse: 6.1299 - mae: 1.0774 - val_loss: 8.9008 - val_mse: 8.9008 - val_mae: 1.1780 - lr: 7.8670e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 3s - loss: 5.9007 - mse: 5.9007 - mae: 1.0581 - val_loss: 8.9189 - val_mse: 8.9189 - val_mae: 1.2171 - lr: 7.8670e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 3s - loss: 5.8065 - mse: 5.8065 - mae: 1.0441 - val_loss: 9.0643 - val_mse: 9.0643 - val_mae: 1.2142 - lr: 7.8670e-04 - 3s/epoch - 3ms/step\n",
            "Score for fold 5: loss of 9.0642671585083\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-10-11 13:13:35,414]\u001b[0m Finished trial#38 resulted in value: 9.808000000000002. Current best value is 9.334 with parameters: {'activation': 'relu', 'num_hidden_layer': 4, 'i': 9, 'learning_rate': 0.00023071835273936203}.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 15.7781 - mse: 15.7781 - mae: 1.6082 - val_loss: 9.4257 - val_mse: 9.4257 - val_mae: 1.4730 - lr: 7.3492e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 14.6822 - mse: 14.6822 - mae: 1.5275 - val_loss: 9.2461 - val_mse: 9.2461 - val_mae: 1.4900 - lr: 7.3492e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 14.4598 - mse: 14.4598 - mae: 1.5079 - val_loss: 9.2164 - val_mse: 9.2164 - val_mae: 1.4970 - lr: 7.3492e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 14.2909 - mse: 14.2909 - mae: 1.4993 - val_loss: 9.0540 - val_mse: 9.0540 - val_mae: 1.4278 - lr: 7.3492e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 14.2181 - mse: 14.2181 - mae: 1.4922 - val_loss: 8.9803 - val_mse: 8.9803 - val_mae: 1.4487 - lr: 7.3492e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 14.1177 - mse: 14.1177 - mae: 1.4895 - val_loss: 8.8865 - val_mse: 8.8865 - val_mae: 1.4732 - lr: 7.3492e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 14.0563 - mse: 14.0563 - mae: 1.4827 - val_loss: 8.9649 - val_mse: 8.9649 - val_mae: 1.4452 - lr: 7.3492e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 2s - loss: 13.9775 - mse: 13.9775 - mae: 1.4769 - val_loss: 8.9524 - val_mse: 8.9524 - val_mae: 1.4593 - lr: 7.3492e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 2s - loss: 13.9319 - mse: 13.9319 - mae: 1.4730 - val_loss: 8.9665 - val_mse: 8.9665 - val_mae: 1.4137 - lr: 7.3492e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 2s - loss: 13.8880 - mse: 13.8880 - mae: 1.4739 - val_loss: 8.9177 - val_mse: 8.9177 - val_mae: 1.4394 - lr: 7.3492e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 2s - loss: 13.8806 - mse: 13.8806 - mae: 1.4687 - val_loss: 8.7985 - val_mse: 8.7985 - val_mae: 1.4664 - lr: 7.3492e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 2s - loss: 13.8213 - mse: 13.8213 - mae: 1.4680 - val_loss: 8.9816 - val_mse: 8.9816 - val_mae: 1.4533 - lr: 7.3492e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 13/100\n",
            "1000/1000 - 2s - loss: 13.7953 - mse: 13.7953 - mae: 1.4653 - val_loss: 8.7490 - val_mse: 8.7490 - val_mae: 1.4356 - lr: 7.3492e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 14/100\n",
            "1000/1000 - 2s - loss: 13.7493 - mse: 13.7493 - mae: 1.4665 - val_loss: 8.7550 - val_mse: 8.7550 - val_mae: 1.4266 - lr: 7.3492e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 15/100\n",
            "1000/1000 - 2s - loss: 13.7075 - mse: 13.7075 - mae: 1.4611 - val_loss: 8.7608 - val_mse: 8.7608 - val_mae: 1.4436 - lr: 7.3492e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 16/100\n",
            "1000/1000 - 2s - loss: 13.6547 - mse: 13.6547 - mae: 1.4622 - val_loss: 8.9552 - val_mse: 8.9552 - val_mae: 1.3958 - lr: 7.3492e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 17/100\n",
            "1000/1000 - 2s - loss: 13.6592 - mse: 13.6592 - mae: 1.4585 - val_loss: 8.7117 - val_mse: 8.7117 - val_mae: 1.4838 - lr: 7.3492e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 18/100\n",
            "1000/1000 - 2s - loss: 13.6017 - mse: 13.6017 - mae: 1.4559 - val_loss: 8.7523 - val_mse: 8.7523 - val_mae: 1.4905 - lr: 7.3492e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 19/100\n",
            "1000/1000 - 2s - loss: 13.5680 - mse: 13.5680 - mae: 1.4557 - val_loss: 8.7414 - val_mse: 8.7414 - val_mae: 1.4407 - lr: 7.3492e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 20/100\n",
            "1000/1000 - 2s - loss: 13.5428 - mse: 13.5428 - mae: 1.4573 - val_loss: 8.6592 - val_mse: 8.6592 - val_mae: 1.4125 - lr: 7.3492e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 21/100\n",
            "1000/1000 - 2s - loss: 13.5425 - mse: 13.5425 - mae: 1.4517 - val_loss: 8.6365 - val_mse: 8.6365 - val_mae: 1.4258 - lr: 7.3492e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 22/100\n",
            "1000/1000 - 2s - loss: 13.5034 - mse: 13.5034 - mae: 1.4532 - val_loss: 8.8433 - val_mse: 8.8433 - val_mae: 1.4438 - lr: 7.3492e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 23/100\n",
            "1000/1000 - 2s - loss: 13.5055 - mse: 13.5055 - mae: 1.4502 - val_loss: 8.5971 - val_mse: 8.5971 - val_mae: 1.4720 - lr: 7.3492e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 24/100\n",
            "1000/1000 - 2s - loss: 13.4162 - mse: 13.4162 - mae: 1.4504 - val_loss: 8.6865 - val_mse: 8.6865 - val_mae: 1.4492 - lr: 7.3492e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 25/100\n",
            "1000/1000 - 2s - loss: 13.3936 - mse: 13.3936 - mae: 1.4493 - val_loss: 8.6730 - val_mse: 8.6730 - val_mae: 1.4218 - lr: 7.3492e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 26/100\n",
            "1000/1000 - 2s - loss: 13.4185 - mse: 13.4185 - mae: 1.4485 - val_loss: 8.6671 - val_mse: 8.6671 - val_mae: 1.4665 - lr: 7.3492e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 27/100\n",
            "1000/1000 - 2s - loss: 13.3781 - mse: 13.3781 - mae: 1.4499 - val_loss: 8.6440 - val_mse: 8.6440 - val_mae: 1.4822 - lr: 7.3492e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 28/100\n",
            "1000/1000 - 2s - loss: 13.3369 - mse: 13.3369 - mae: 1.4520 - val_loss: 8.6621 - val_mse: 8.6621 - val_mae: 1.4679 - lr: 7.3492e-04 - 2s/epoch - 2ms/step\n",
            "Score for fold 1: loss of 8.66207504272461\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 12.6149 - mse: 12.6149 - mae: 1.4423 - val_loss: 11.4944 - val_mse: 11.4944 - val_mae: 1.4741 - lr: 7.3492e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 12.5505 - mse: 12.5505 - mae: 1.4455 - val_loss: 11.3866 - val_mse: 11.3866 - val_mae: 1.4908 - lr: 7.3492e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 12.5176 - mse: 12.5176 - mae: 1.4456 - val_loss: 11.4141 - val_mse: 11.4141 - val_mae: 1.4569 - lr: 7.3492e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 12.5024 - mse: 12.5024 - mae: 1.4396 - val_loss: 11.4220 - val_mse: 11.4220 - val_mae: 1.4567 - lr: 7.3492e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 12.4331 - mse: 12.4331 - mae: 1.4391 - val_loss: 11.3800 - val_mse: 11.3800 - val_mae: 1.4450 - lr: 7.3492e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 12.3966 - mse: 12.3966 - mae: 1.4377 - val_loss: 11.3952 - val_mse: 11.3952 - val_mae: 1.4293 - lr: 7.3492e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 12.3787 - mse: 12.3787 - mae: 1.4396 - val_loss: 11.4016 - val_mse: 11.4016 - val_mae: 1.4258 - lr: 7.3492e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 2s - loss: 12.3610 - mse: 12.3610 - mae: 1.4374 - val_loss: 11.3996 - val_mse: 11.3996 - val_mae: 1.4656 - lr: 7.3492e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 2s - loss: 12.3260 - mse: 12.3260 - mae: 1.4376 - val_loss: 11.4743 - val_mse: 11.4743 - val_mae: 1.4820 - lr: 7.3492e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 2s - loss: 12.2956 - mse: 12.2956 - mae: 1.4356 - val_loss: 11.5342 - val_mse: 11.5342 - val_mae: 1.4586 - lr: 7.3492e-04 - 2s/epoch - 2ms/step\n",
            "Score for fold 2: loss of 11.534220695495605\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 12.0385 - mse: 12.0385 - mae: 1.4415 - val_loss: 12.5671 - val_mse: 12.5671 - val_mae: 1.4196 - lr: 7.3492e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 11.9930 - mse: 11.9930 - mae: 1.4361 - val_loss: 12.6457 - val_mse: 12.6457 - val_mae: 1.4748 - lr: 7.3492e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 11.9598 - mse: 11.9598 - mae: 1.4358 - val_loss: 12.7521 - val_mse: 12.7521 - val_mae: 1.4680 - lr: 7.3492e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 11.9206 - mse: 11.9206 - mae: 1.4338 - val_loss: 12.8465 - val_mse: 12.8465 - val_mae: 1.4919 - lr: 7.3492e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 11.9445 - mse: 11.9445 - mae: 1.4348 - val_loss: 12.9040 - val_mse: 12.9040 - val_mae: 1.4668 - lr: 7.3492e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 11.9238 - mse: 11.9238 - mae: 1.4354 - val_loss: 13.1386 - val_mse: 13.1386 - val_mae: 1.4471 - lr: 7.3492e-04 - 2s/epoch - 2ms/step\n",
            "Score for fold 3: loss of 13.138614654541016\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 10.6842 - mse: 10.6842 - mae: 1.4288 - val_loss: 17.9742 - val_mse: 17.9742 - val_mae: 1.4571 - lr: 7.3492e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 10.6107 - mse: 10.6107 - mae: 1.4253 - val_loss: 17.9356 - val_mse: 17.9356 - val_mae: 1.4872 - lr: 7.3492e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 10.5790 - mse: 10.5790 - mae: 1.4274 - val_loss: 17.9378 - val_mse: 17.9378 - val_mae: 1.4693 - lr: 7.3492e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 10.5453 - mse: 10.5453 - mae: 1.4218 - val_loss: 17.8806 - val_mse: 17.8806 - val_mae: 1.4883 - lr: 7.3492e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 10.4992 - mse: 10.4992 - mae: 1.4269 - val_loss: 18.0484 - val_mse: 18.0484 - val_mae: 1.4558 - lr: 7.3492e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 10.4738 - mse: 10.4738 - mae: 1.4233 - val_loss: 18.0262 - val_mse: 18.0262 - val_mae: 1.4393 - lr: 7.3492e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 10.4186 - mse: 10.4186 - mae: 1.4222 - val_loss: 18.1412 - val_mse: 18.1412 - val_mae: 1.4873 - lr: 7.3492e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 2s - loss: 10.3823 - mse: 10.3823 - mae: 1.4184 - val_loss: 18.1461 - val_mse: 18.1461 - val_mae: 1.4872 - lr: 7.3492e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 2s - loss: 10.4313 - mse: 10.4313 - mae: 1.4199 - val_loss: 18.0854 - val_mse: 18.0854 - val_mae: 1.4849 - lr: 7.3492e-04 - 2s/epoch - 2ms/step\n",
            "Score for fold 4: loss of 18.08542251586914\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 12.3766 - mse: 12.3766 - mae: 1.4414 - val_loss: 10.0436 - val_mse: 10.0436 - val_mae: 1.3812 - lr: 7.3492e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 12.3582 - mse: 12.3582 - mae: 1.4385 - val_loss: 10.1921 - val_mse: 10.1921 - val_mae: 1.4380 - lr: 7.3492e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 12.3145 - mse: 12.3145 - mae: 1.4416 - val_loss: 10.4626 - val_mse: 10.4626 - val_mae: 1.4858 - lr: 7.3492e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 12.2698 - mse: 12.2698 - mae: 1.4369 - val_loss: 10.3132 - val_mse: 10.3132 - val_mae: 1.4306 - lr: 7.3492e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 12.2684 - mse: 12.2684 - mae: 1.4347 - val_loss: 10.2500 - val_mse: 10.2500 - val_mae: 1.4375 - lr: 7.3492e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 12.2475 - mse: 12.2475 - mae: 1.4320 - val_loss: 10.2601 - val_mse: 10.2601 - val_mae: 1.4187 - lr: 7.3492e-04 - 2s/epoch - 2ms/step\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-10-11 13:15:20,652]\u001b[0m Finished trial#39 resulted in value: 12.336. Current best value is 9.334 with parameters: {'activation': 'relu', 'num_hidden_layer': 4, 'i': 9, 'learning_rate': 0.00023071835273936203}.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Score for fold 5: loss of 10.2600736618042\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 3s - loss: 14.8208 - mse: 14.8208 - mae: 1.5951 - val_loss: 13.1555 - val_mse: 13.1555 - val_mae: 1.5313 - lr: 0.0012 - 3s/epoch - 3ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 13.9381 - mse: 13.9381 - mae: 1.5387 - val_loss: 12.8891 - val_mse: 12.8891 - val_mae: 1.5282 - lr: 0.0012 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 13.6075 - mse: 13.6075 - mae: 1.5202 - val_loss: 12.8204 - val_mse: 12.8204 - val_mae: 1.4545 - lr: 0.0012 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 13.4052 - mse: 13.4052 - mae: 1.5020 - val_loss: 12.6582 - val_mse: 12.6582 - val_mae: 1.5192 - lr: 0.0012 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 13.2679 - mse: 13.2679 - mae: 1.4951 - val_loss: 12.5875 - val_mse: 12.5875 - val_mae: 1.5031 - lr: 0.0012 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 13.1003 - mse: 13.1003 - mae: 1.4849 - val_loss: 12.5936 - val_mse: 12.5936 - val_mae: 1.4537 - lr: 0.0012 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 13.1015 - mse: 13.1015 - mae: 1.4852 - val_loss: 12.3569 - val_mse: 12.3569 - val_mae: 1.5373 - lr: 0.0012 - 2s/epoch - 2ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 2s - loss: 12.9596 - mse: 12.9596 - mae: 1.4751 - val_loss: 12.3063 - val_mse: 12.3063 - val_mae: 1.5055 - lr: 0.0012 - 2s/epoch - 2ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 2s - loss: 12.8714 - mse: 12.8714 - mae: 1.4720 - val_loss: 12.2732 - val_mse: 12.2732 - val_mae: 1.4806 - lr: 0.0012 - 2s/epoch - 2ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 2s - loss: 12.7592 - mse: 12.7592 - mae: 1.4669 - val_loss: 12.0817 - val_mse: 12.0817 - val_mae: 1.5173 - lr: 0.0012 - 2s/epoch - 2ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 2s - loss: 12.6582 - mse: 12.6582 - mae: 1.4650 - val_loss: 12.0893 - val_mse: 12.0893 - val_mae: 1.5478 - lr: 0.0012 - 2s/epoch - 2ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 2s - loss: 12.6003 - mse: 12.6003 - mae: 1.4596 - val_loss: 11.9641 - val_mse: 11.9641 - val_mae: 1.4750 - lr: 0.0012 - 2s/epoch - 2ms/step\n",
            "Epoch 13/100\n",
            "1000/1000 - 2s - loss: 12.4896 - mse: 12.4896 - mae: 1.4514 - val_loss: 12.3144 - val_mse: 12.3144 - val_mae: 1.5434 - lr: 0.0012 - 2s/epoch - 2ms/step\n",
            "Epoch 14/100\n",
            "1000/1000 - 2s - loss: 12.3964 - mse: 12.3964 - mae: 1.4483 - val_loss: 12.1660 - val_mse: 12.1660 - val_mae: 1.6065 - lr: 0.0012 - 2s/epoch - 2ms/step\n",
            "Epoch 15/100\n",
            "1000/1000 - 2s - loss: 12.2465 - mse: 12.2465 - mae: 1.4441 - val_loss: 12.1935 - val_mse: 12.1935 - val_mae: 1.5378 - lr: 0.0012 - 2s/epoch - 2ms/step\n",
            "Epoch 16/100\n",
            "1000/1000 - 2s - loss: 12.1344 - mse: 12.1344 - mae: 1.4369 - val_loss: 11.8725 - val_mse: 11.8725 - val_mae: 1.5629 - lr: 0.0012 - 2s/epoch - 2ms/step\n",
            "Epoch 17/100\n",
            "1000/1000 - 2s - loss: 11.9897 - mse: 11.9897 - mae: 1.4328 - val_loss: 12.3598 - val_mse: 12.3598 - val_mae: 1.5872 - lr: 0.0012 - 2s/epoch - 2ms/step\n",
            "Epoch 18/100\n",
            "1000/1000 - 2s - loss: 11.8610 - mse: 11.8610 - mae: 1.4231 - val_loss: 11.9053 - val_mse: 11.9053 - val_mae: 1.4541 - lr: 0.0012 - 2s/epoch - 2ms/step\n",
            "Epoch 19/100\n",
            "1000/1000 - 2s - loss: 11.7633 - mse: 11.7633 - mae: 1.4159 - val_loss: 11.9727 - val_mse: 11.9727 - val_mae: 1.5392 - lr: 0.0012 - 2s/epoch - 2ms/step\n",
            "Epoch 20/100\n",
            "1000/1000 - 2s - loss: 11.6988 - mse: 11.6988 - mae: 1.4134 - val_loss: 11.7970 - val_mse: 11.7970 - val_mae: 1.5254 - lr: 0.0012 - 2s/epoch - 2ms/step\n",
            "Epoch 21/100\n",
            "1000/1000 - 2s - loss: 11.5579 - mse: 11.5579 - mae: 1.4012 - val_loss: 11.8633 - val_mse: 11.8633 - val_mae: 1.5110 - lr: 0.0012 - 2s/epoch - 2ms/step\n",
            "Epoch 22/100\n",
            "1000/1000 - 2s - loss: 11.4300 - mse: 11.4300 - mae: 1.3938 - val_loss: 12.2283 - val_mse: 12.2283 - val_mae: 1.5725 - lr: 0.0012 - 2s/epoch - 2ms/step\n",
            "Epoch 23/100\n",
            "1000/1000 - 2s - loss: 11.3210 - mse: 11.3210 - mae: 1.3893 - val_loss: 11.9699 - val_mse: 11.9699 - val_mae: 1.5221 - lr: 0.0012 - 2s/epoch - 2ms/step\n",
            "Epoch 24/100\n",
            "1000/1000 - 2s - loss: 11.2193 - mse: 11.2193 - mae: 1.3870 - val_loss: 11.9901 - val_mse: 11.9901 - val_mae: 1.5918 - lr: 0.0012 - 2s/epoch - 2ms/step\n",
            "Epoch 25/100\n",
            "1000/1000 - 2s - loss: 11.0359 - mse: 11.0359 - mae: 1.3737 - val_loss: 12.1811 - val_mse: 12.1811 - val_mae: 1.5742 - lr: 0.0012 - 2s/epoch - 2ms/step\n",
            "Score for fold 1: loss of 12.18105411529541\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 11.8389 - mse: 11.8389 - mae: 1.4080 - val_loss: 8.7804 - val_mse: 8.7804 - val_mae: 1.3552 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 11.7302 - mse: 11.7302 - mae: 1.3956 - val_loss: 8.5075 - val_mse: 8.5075 - val_mae: 1.4268 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 11.5047 - mse: 11.5047 - mae: 1.3839 - val_loss: 8.6304 - val_mse: 8.6304 - val_mae: 1.3780 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 11.4025 - mse: 11.4025 - mae: 1.3744 - val_loss: 8.5130 - val_mse: 8.5130 - val_mae: 1.3957 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 11.2119 - mse: 11.2119 - mae: 1.3622 - val_loss: 8.6459 - val_mse: 8.6459 - val_mae: 1.3958 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 11.0675 - mse: 11.0675 - mae: 1.3537 - val_loss: 8.7442 - val_mse: 8.7442 - val_mae: 1.4170 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 10.9779 - mse: 10.9779 - mae: 1.3480 - val_loss: 8.6671 - val_mse: 8.6671 - val_mae: 1.4682 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Score for fold 2: loss of 8.667106628417969\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 10.4242 - mse: 10.4242 - mae: 1.3716 - val_loss: 10.7386 - val_mse: 10.7386 - val_mae: 1.3355 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 10.3291 - mse: 10.3291 - mae: 1.3535 - val_loss: 10.8519 - val_mse: 10.8519 - val_mae: 1.3685 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 10.1961 - mse: 10.1961 - mae: 1.3415 - val_loss: 11.2298 - val_mse: 11.2298 - val_mae: 1.3856 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 9.9514 - mse: 9.9514 - mae: 1.3372 - val_loss: 11.3882 - val_mse: 11.3882 - val_mae: 1.3805 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 9.9279 - mse: 9.9279 - mae: 1.3279 - val_loss: 11.2879 - val_mse: 11.2879 - val_mae: 1.3982 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 9.7609 - mse: 9.7609 - mae: 1.3177 - val_loss: 11.6538 - val_mse: 11.6538 - val_mae: 1.4506 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Score for fold 3: loss of 11.653775215148926\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 8.4084 - mse: 8.4084 - mae: 1.3385 - val_loss: 16.8575 - val_mse: 16.8575 - val_mae: 1.3370 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 8.1777 - mse: 8.1777 - mae: 1.3249 - val_loss: 16.7670 - val_mse: 16.7670 - val_mae: 1.3278 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 8.0727 - mse: 8.0727 - mae: 1.3136 - val_loss: 16.9009 - val_mse: 16.9009 - val_mae: 1.3545 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 7.9248 - mse: 7.9248 - mae: 1.3038 - val_loss: 17.0585 - val_mse: 17.0585 - val_mae: 1.3559 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 7.8270 - mse: 7.8270 - mae: 1.2988 - val_loss: 17.0688 - val_mse: 17.0688 - val_mae: 1.3950 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 7.6729 - mse: 7.6729 - mae: 1.2883 - val_loss: 17.0299 - val_mse: 17.0299 - val_mae: 1.3668 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 7.4645 - mse: 7.4645 - mae: 1.2762 - val_loss: 17.2435 - val_mse: 17.2435 - val_mae: 1.3933 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Score for fold 4: loss of 17.24346351623535\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 10.5187 - mse: 10.5187 - mae: 1.3243 - val_loss: 4.9280 - val_mse: 4.9280 - val_mae: 1.2285 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 10.2831 - mse: 10.2831 - mae: 1.3075 - val_loss: 5.1407 - val_mse: 5.1407 - val_mae: 1.2458 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 10.1732 - mse: 10.1732 - mae: 1.2945 - val_loss: 5.3052 - val_mse: 5.3052 - val_mae: 1.3024 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 10.0384 - mse: 10.0384 - mae: 1.2866 - val_loss: 5.3966 - val_mse: 5.3966 - val_mae: 1.2586 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 9.8876 - mse: 9.8876 - mae: 1.2783 - val_loss: 5.4516 - val_mse: 5.4516 - val_mae: 1.2880 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 9.7643 - mse: 9.7643 - mae: 1.2689 - val_loss: 5.9310 - val_mse: 5.9310 - val_mae: 1.3179 - lr: 0.0010 - 2s/epoch - 2ms/step\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-10-11 13:17:24,448]\u001b[0m Finished trial#40 resulted in value: 11.133999999999999. Current best value is 9.334 with parameters: {'activation': 'relu', 'num_hidden_layer': 4, 'i': 9, 'learning_rate': 0.00023071835273936203}.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Score for fold 5: loss of 5.931013584136963\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 4s - loss: 13.0612 - mse: 13.0612 - mae: 1.5863 - val_loss: 19.7163 - val_mse: 19.7163 - val_mae: 1.5890 - lr: 5.8222e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 3s - loss: 12.3061 - mse: 12.3061 - mae: 1.5148 - val_loss: 19.4006 - val_mse: 19.4006 - val_mae: 1.5055 - lr: 5.8222e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 3s - loss: 12.0175 - mse: 12.0175 - mae: 1.5017 - val_loss: 19.5377 - val_mse: 19.5377 - val_mae: 1.5939 - lr: 5.8222e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 4s - loss: 11.8216 - mse: 11.8216 - mae: 1.4882 - val_loss: 19.0288 - val_mse: 19.0288 - val_mae: 1.5227 - lr: 5.8222e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 4s - loss: 11.6740 - mse: 11.6740 - mae: 1.4840 - val_loss: 19.0931 - val_mse: 19.0931 - val_mae: 1.4860 - lr: 5.8222e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 3s - loss: 11.5295 - mse: 11.5295 - mae: 1.4757 - val_loss: 19.0932 - val_mse: 19.0932 - val_mae: 1.5229 - lr: 5.8222e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 3s - loss: 11.4273 - mse: 11.4273 - mae: 1.4716 - val_loss: 19.0111 - val_mse: 19.0111 - val_mae: 1.5062 - lr: 5.8222e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 3s - loss: 11.2699 - mse: 11.2699 - mae: 1.4629 - val_loss: 18.6789 - val_mse: 18.6789 - val_mae: 1.5521 - lr: 5.8222e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 3s - loss: 11.1488 - mse: 11.1488 - mae: 1.4599 - val_loss: 18.6172 - val_mse: 18.6172 - val_mae: 1.5323 - lr: 5.8222e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 3s - loss: 10.9620 - mse: 10.9620 - mae: 1.4470 - val_loss: 18.8395 - val_mse: 18.8395 - val_mae: 1.5379 - lr: 5.8222e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 3s - loss: 10.8752 - mse: 10.8752 - mae: 1.4448 - val_loss: 18.9613 - val_mse: 18.9613 - val_mae: 1.5238 - lr: 5.8222e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 3s - loss: 10.7478 - mse: 10.7478 - mae: 1.4353 - val_loss: 18.6624 - val_mse: 18.6624 - val_mae: 1.5393 - lr: 5.8222e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 13/100\n",
            "1000/1000 - 4s - loss: 10.5732 - mse: 10.5732 - mae: 1.4291 - val_loss: 18.8649 - val_mse: 18.8649 - val_mae: 1.5587 - lr: 5.8222e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 14/100\n",
            "1000/1000 - 4s - loss: 10.4282 - mse: 10.4282 - mae: 1.4188 - val_loss: 18.5940 - val_mse: 18.5940 - val_mae: 1.5191 - lr: 5.8222e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 15/100\n",
            "1000/1000 - 4s - loss: 10.3649 - mse: 10.3649 - mae: 1.4084 - val_loss: 18.5652 - val_mse: 18.5652 - val_mae: 1.6018 - lr: 5.8222e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 16/100\n",
            "1000/1000 - 3s - loss: 10.0683 - mse: 10.0683 - mae: 1.3991 - val_loss: 18.5833 - val_mse: 18.5833 - val_mae: 1.6018 - lr: 5.8222e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 17/100\n",
            "1000/1000 - 4s - loss: 9.9374 - mse: 9.9374 - mae: 1.3920 - val_loss: 18.5254 - val_mse: 18.5254 - val_mae: 1.5504 - lr: 5.8222e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 18/100\n",
            "1000/1000 - 4s - loss: 9.7632 - mse: 9.7632 - mae: 1.3791 - val_loss: 18.5195 - val_mse: 18.5195 - val_mae: 1.5745 - lr: 5.8222e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 19/100\n",
            "1000/1000 - 3s - loss: 9.5847 - mse: 9.5847 - mae: 1.3703 - val_loss: 18.6372 - val_mse: 18.6372 - val_mae: 1.5903 - lr: 5.8222e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 20/100\n",
            "1000/1000 - 4s - loss: 9.4664 - mse: 9.4664 - mae: 1.3660 - val_loss: 18.6686 - val_mse: 18.6686 - val_mae: 1.6153 - lr: 5.8222e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 21/100\n",
            "1000/1000 - 4s - loss: 9.2348 - mse: 9.2348 - mae: 1.3476 - val_loss: 18.5779 - val_mse: 18.5779 - val_mae: 1.6307 - lr: 5.8222e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 22/100\n",
            "1000/1000 - 3s - loss: 9.0813 - mse: 9.0813 - mae: 1.3409 - val_loss: 18.6446 - val_mse: 18.6446 - val_mae: 1.6306 - lr: 5.8222e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 23/100\n",
            "1000/1000 - 3s - loss: 8.8970 - mse: 8.8970 - mae: 1.3293 - val_loss: 18.6643 - val_mse: 18.6643 - val_mae: 1.6413 - lr: 5.8222e-04 - 3s/epoch - 3ms/step\n",
            "Score for fold 1: loss of 18.66429901123047\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 3s - loss: 11.6146 - mse: 11.6146 - mae: 1.3999 - val_loss: 8.2164 - val_mse: 8.2164 - val_mae: 1.3070 - lr: 5.8222e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 4s - loss: 11.2921 - mse: 11.2921 - mae: 1.3845 - val_loss: 8.5039 - val_mse: 8.5039 - val_mae: 1.3203 - lr: 5.8222e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 4s - loss: 11.0529 - mse: 11.0529 - mae: 1.3666 - val_loss: 8.6829 - val_mse: 8.6829 - val_mae: 1.3780 - lr: 5.8222e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 3s - loss: 10.8467 - mse: 10.8467 - mae: 1.3517 - val_loss: 8.4786 - val_mse: 8.4786 - val_mae: 1.3885 - lr: 5.8222e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 3s - loss: 10.5958 - mse: 10.5958 - mae: 1.3348 - val_loss: 8.5129 - val_mse: 8.5129 - val_mae: 1.4251 - lr: 5.8222e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 3s - loss: 10.4536 - mse: 10.4536 - mae: 1.3228 - val_loss: 8.7462 - val_mse: 8.7462 - val_mae: 1.3843 - lr: 5.8222e-04 - 3s/epoch - 3ms/step\n",
            "Score for fold 2: loss of 8.7461576461792\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 4s - loss: 10.1643 - mse: 10.1643 - mae: 1.3516 - val_loss: 9.2751 - val_mse: 9.2751 - val_mae: 1.2658 - lr: 5.8222e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 4s - loss: 9.8855 - mse: 9.8855 - mae: 1.3259 - val_loss: 9.5738 - val_mse: 9.5738 - val_mae: 1.3416 - lr: 5.8222e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 4s - loss: 9.6856 - mse: 9.6856 - mae: 1.3154 - val_loss: 9.8994 - val_mse: 9.8994 - val_mae: 1.3195 - lr: 5.8222e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 4s - loss: 9.4155 - mse: 9.4155 - mae: 1.2924 - val_loss: 9.6858 - val_mse: 9.6858 - val_mae: 1.3351 - lr: 5.8222e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 4s - loss: 9.2303 - mse: 9.2303 - mae: 1.2795 - val_loss: 9.9799 - val_mse: 9.9799 - val_mae: 1.3777 - lr: 5.8222e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 4s - loss: 9.0643 - mse: 9.0643 - mae: 1.2663 - val_loss: 10.2093 - val_mse: 10.2093 - val_mae: 1.3705 - lr: 5.8222e-04 - 4s/epoch - 4ms/step\n",
            "Score for fold 3: loss of 10.209309577941895\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 4s - loss: 9.3540 - mse: 9.3540 - mae: 1.3039 - val_loss: 8.7071 - val_mse: 8.7071 - val_mae: 1.2259 - lr: 5.8222e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 4s - loss: 9.0948 - mse: 9.0948 - mae: 1.2759 - val_loss: 8.9047 - val_mse: 8.9047 - val_mae: 1.2751 - lr: 5.8222e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 4s - loss: 8.7471 - mse: 8.7471 - mae: 1.2524 - val_loss: 9.0158 - val_mse: 9.0158 - val_mae: 1.2929 - lr: 5.8222e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 4s - loss: 8.5387 - mse: 8.5387 - mae: 1.2371 - val_loss: 8.9576 - val_mse: 8.9576 - val_mae: 1.2838 - lr: 5.8222e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 4s - loss: 8.3775 - mse: 8.3775 - mae: 1.2254 - val_loss: 9.2524 - val_mse: 9.2524 - val_mae: 1.3255 - lr: 5.8222e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 3s - loss: 8.1771 - mse: 8.1771 - mae: 1.2125 - val_loss: 9.1343 - val_mse: 9.1343 - val_mae: 1.3377 - lr: 5.8222e-04 - 3s/epoch - 3ms/step\n",
            "Score for fold 4: loss of 9.134282112121582\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 3s - loss: 9.3036 - mse: 9.3036 - mae: 1.2582 - val_loss: 4.6849 - val_mse: 4.6849 - val_mae: 1.1491 - lr: 5.8222e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 4s - loss: 8.9885 - mse: 8.9885 - mae: 1.2291 - val_loss: 4.6102 - val_mse: 4.6102 - val_mae: 1.1711 - lr: 5.8222e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 3s - loss: 8.7726 - mse: 8.7726 - mae: 1.2057 - val_loss: 5.0631 - val_mse: 5.0631 - val_mae: 1.2121 - lr: 5.8222e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 3s - loss: 8.5811 - mse: 8.5811 - mae: 1.1932 - val_loss: 5.0604 - val_mse: 5.0604 - val_mae: 1.2430 - lr: 5.8222e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 3s - loss: 8.3476 - mse: 8.3476 - mae: 1.1730 - val_loss: 4.8612 - val_mse: 4.8612 - val_mae: 1.2367 - lr: 5.8222e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 3s - loss: 8.1575 - mse: 8.1575 - mae: 1.1570 - val_loss: 5.0477 - val_mse: 5.0477 - val_mae: 1.2507 - lr: 5.8222e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 4s - loss: 8.0508 - mse: 8.0508 - mae: 1.1394 - val_loss: 5.1176 - val_mse: 5.1176 - val_mae: 1.2679 - lr: 5.8222e-04 - 4s/epoch - 4ms/step\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-10-11 13:20:18,202]\u001b[0m Finished trial#41 resulted in value: 10.374. Current best value is 9.334 with parameters: {'activation': 'relu', 'num_hidden_layer': 4, 'i': 9, 'learning_rate': 0.00023071835273936203}.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Score for fold 5: loss of 5.117586135864258\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 7s - loss: 14.7610 - mse: 14.7610 - mae: 1.6344 - val_loss: 14.3731 - val_mse: 14.3731 - val_mae: 1.6170 - lr: 8.3300e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 6s - loss: 13.8839 - mse: 13.8839 - mae: 1.5536 - val_loss: 13.9763 - val_mse: 13.9763 - val_mae: 1.5323 - lr: 8.3300e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 6s - loss: 13.5833 - mse: 13.5833 - mae: 1.5383 - val_loss: 13.6835 - val_mse: 13.6835 - val_mae: 1.5055 - lr: 8.3300e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 6s - loss: 13.3828 - mse: 13.3828 - mae: 1.5191 - val_loss: 13.8086 - val_mse: 13.8086 - val_mae: 1.4901 - lr: 8.3300e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 6s - loss: 13.1164 - mse: 13.1164 - mae: 1.5082 - val_loss: 13.5575 - val_mse: 13.5575 - val_mae: 1.5309 - lr: 8.3300e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 7s - loss: 12.8852 - mse: 12.8852 - mae: 1.4956 - val_loss: 13.5293 - val_mse: 13.5293 - val_mae: 1.3896 - lr: 8.3300e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 6s - loss: 12.8094 - mse: 12.8094 - mae: 1.4955 - val_loss: 13.3317 - val_mse: 13.3317 - val_mae: 1.6210 - lr: 8.3300e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 6s - loss: 12.8066 - mse: 12.8066 - mae: 1.4912 - val_loss: 13.4759 - val_mse: 13.4759 - val_mae: 1.4473 - lr: 8.3300e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 6s - loss: 12.6943 - mse: 12.6943 - mae: 1.4821 - val_loss: 13.2429 - val_mse: 13.2429 - val_mae: 1.5594 - lr: 8.3300e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 6s - loss: 12.6334 - mse: 12.6334 - mae: 1.4834 - val_loss: 13.4637 - val_mse: 13.4637 - val_mae: 1.4346 - lr: 8.3300e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 6s - loss: 12.4583 - mse: 12.4583 - mae: 1.4921 - val_loss: 13.3940 - val_mse: 13.3940 - val_mae: 1.5827 - lr: 8.3300e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 6s - loss: 12.4654 - mse: 12.4654 - mae: 1.4850 - val_loss: 13.1491 - val_mse: 13.1491 - val_mae: 1.5641 - lr: 8.3300e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 13/100\n",
            "1000/1000 - 6s - loss: 12.3597 - mse: 12.3597 - mae: 1.4860 - val_loss: 13.3789 - val_mse: 13.3789 - val_mae: 1.4350 - lr: 8.3300e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 14/100\n",
            "1000/1000 - 6s - loss: 12.3221 - mse: 12.3221 - mae: 1.4838 - val_loss: 13.1546 - val_mse: 13.1546 - val_mae: 1.4702 - lr: 8.3300e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 15/100\n",
            "1000/1000 - 6s - loss: 12.2588 - mse: 12.2588 - mae: 1.4788 - val_loss: 13.2635 - val_mse: 13.2635 - val_mae: 1.4911 - lr: 8.3300e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 16/100\n",
            "1000/1000 - 6s - loss: 12.2276 - mse: 12.2276 - mae: 1.4823 - val_loss: 13.3510 - val_mse: 13.3510 - val_mae: 1.5248 - lr: 8.3300e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 17/100\n",
            "1000/1000 - 6s - loss: 11.9311 - mse: 11.9311 - mae: 1.4701 - val_loss: 13.2541 - val_mse: 13.2541 - val_mae: 1.5550 - lr: 8.3300e-04 - 6s/epoch - 6ms/step\n",
            "Score for fold 1: loss of 13.254097938537598\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 6s - loss: 13.3742 - mse: 13.3742 - mae: 1.4871 - val_loss: 8.2448 - val_mse: 8.2448 - val_mae: 1.3912 - lr: 8.3300e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 6s - loss: 13.2439 - mse: 13.2439 - mae: 1.4799 - val_loss: 7.9249 - val_mse: 7.9249 - val_mae: 1.4477 - lr: 8.3300e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 6s - loss: 13.0568 - mse: 13.0568 - mae: 1.4841 - val_loss: 8.0538 - val_mse: 8.0538 - val_mae: 1.4535 - lr: 8.3300e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 6s - loss: 13.0344 - mse: 13.0344 - mae: 1.4740 - val_loss: 8.1843 - val_mse: 8.1843 - val_mae: 1.4291 - lr: 8.3300e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 6s - loss: 12.9450 - mse: 12.9450 - mae: 1.4826 - val_loss: 8.2555 - val_mse: 8.2555 - val_mae: 1.5354 - lr: 8.3300e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 6s - loss: 13.0184 - mse: 13.0184 - mae: 1.4813 - val_loss: 8.1395 - val_mse: 8.1395 - val_mae: 1.4733 - lr: 8.3300e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 6s - loss: 12.8708 - mse: 12.8708 - mae: 1.4772 - val_loss: 8.1556 - val_mse: 8.1556 - val_mae: 1.4727 - lr: 8.3300e-04 - 6s/epoch - 6ms/step\n",
            "Score for fold 2: loss of 8.155628204345703\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 6s - loss: 10.0613 - mse: 10.0613 - mae: 1.4659 - val_loss: 19.2257 - val_mse: 19.2257 - val_mae: 1.4826 - lr: 8.3300e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 6s - loss: 9.8939 - mse: 9.8939 - mae: 1.4613 - val_loss: 19.8432 - val_mse: 19.8432 - val_mae: 1.4379 - lr: 8.3300e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 6s - loss: 9.6848 - mse: 9.6848 - mae: 1.4551 - val_loss: 20.3065 - val_mse: 20.3065 - val_mae: 1.4509 - lr: 8.3300e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 6s - loss: 9.6817 - mse: 9.6817 - mae: 1.4558 - val_loss: 20.3548 - val_mse: 20.3548 - val_mae: 1.4528 - lr: 8.3300e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 6s - loss: 10.0450 - mse: 10.0450 - mae: 1.4618 - val_loss: 19.2942 - val_mse: 19.2942 - val_mae: 1.4800 - lr: 8.3300e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 6s - loss: 9.7499 - mse: 9.7499 - mae: 1.4518 - val_loss: 19.3770 - val_mse: 19.3770 - val_mae: 1.4708 - lr: 8.3300e-04 - 6s/epoch - 6ms/step\n",
            "Score for fold 3: loss of 19.377033233642578\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 6s - loss: 12.0209 - mse: 12.0209 - mae: 1.4568 - val_loss: 10.0777 - val_mse: 10.0777 - val_mae: 1.4410 - lr: 8.3300e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 6s - loss: 12.0876 - mse: 12.0876 - mae: 1.4670 - val_loss: 10.1475 - val_mse: 10.1475 - val_mae: 1.4519 - lr: 8.3300e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 6s - loss: 12.1519 - mse: 12.1519 - mae: 1.4505 - val_loss: 11.1199 - val_mse: 11.1199 - val_mae: 1.5460 - lr: 8.3300e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 6s - loss: 11.9463 - mse: 11.9463 - mae: 1.4460 - val_loss: 10.4987 - val_mse: 10.4987 - val_mae: 1.3847 - lr: 8.3300e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 6s - loss: 11.9093 - mse: 11.9093 - mae: 1.4469 - val_loss: 11.4714 - val_mse: 11.4714 - val_mae: 1.3655 - lr: 8.3300e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 6s - loss: 11.8820 - mse: 11.8820 - mae: 1.4438 - val_loss: 10.8349 - val_mse: 10.8349 - val_mae: 1.7226 - lr: 8.3300e-04 - 6s/epoch - 6ms/step\n",
            "Score for fold 4: loss of 10.834911346435547\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 7s - loss: 12.1757 - mse: 12.1757 - mae: 1.4448 - val_loss: 8.7041 - val_mse: 8.7041 - val_mae: 1.4388 - lr: 8.3300e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 6s - loss: 12.2232 - mse: 12.2232 - mae: 1.4478 - val_loss: 8.6066 - val_mse: 8.6066 - val_mae: 1.4432 - lr: 8.3300e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 6s - loss: 12.1824 - mse: 12.1824 - mae: 1.4353 - val_loss: 9.0627 - val_mse: 9.0627 - val_mae: 1.6166 - lr: 8.3300e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 6s - loss: 11.9952 - mse: 11.9952 - mae: 1.4292 - val_loss: 8.7528 - val_mse: 8.7528 - val_mae: 1.4011 - lr: 8.3300e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 6s - loss: 11.9640 - mse: 11.9640 - mae: 1.4396 - val_loss: 9.1179 - val_mse: 9.1179 - val_mae: 1.3986 - lr: 8.3300e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 6s - loss: 11.9381 - mse: 11.9381 - mae: 1.4324 - val_loss: 8.7020 - val_mse: 8.7020 - val_mae: 1.4804 - lr: 8.3300e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 6s - loss: 11.9381 - mse: 11.9381 - mae: 1.4308 - val_loss: 8.8744 - val_mse: 8.8744 - val_mae: 1.5846 - lr: 8.3300e-04 - 6s/epoch - 6ms/step\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-10-11 13:24:55,876]\u001b[0m Finished trial#42 resulted in value: 12.097999999999999. Current best value is 9.334 with parameters: {'activation': 'relu', 'num_hidden_layer': 4, 'i': 9, 'learning_rate': 0.00023071835273936203}.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Score for fold 5: loss of 8.874410629272461\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 4s - loss: 13.1609 - mse: 13.1609 - mae: 1.5985 - val_loss: 19.7826 - val_mse: 19.7826 - val_mae: 1.5823 - lr: 2.7997e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 3s - loss: 12.2259 - mse: 12.2259 - mae: 1.5195 - val_loss: 19.4605 - val_mse: 19.4605 - val_mae: 1.5254 - lr: 2.7997e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 4s - loss: 11.9568 - mse: 11.9568 - mae: 1.5049 - val_loss: 19.2896 - val_mse: 19.2896 - val_mae: 1.4902 - lr: 2.7997e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 4s - loss: 11.8578 - mse: 11.8578 - mae: 1.4965 - val_loss: 19.1600 - val_mse: 19.1600 - val_mae: 1.4973 - lr: 2.7997e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 4s - loss: 11.6805 - mse: 11.6805 - mae: 1.4914 - val_loss: 19.3119 - val_mse: 19.3119 - val_mae: 1.4764 - lr: 2.7997e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 4s - loss: 11.5833 - mse: 11.5833 - mae: 1.4850 - val_loss: 19.1358 - val_mse: 19.1358 - val_mae: 1.5393 - lr: 2.7997e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 4s - loss: 11.4372 - mse: 11.4372 - mae: 1.4788 - val_loss: 19.2046 - val_mse: 19.2046 - val_mae: 1.5112 - lr: 2.7997e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 4s - loss: 11.3601 - mse: 11.3601 - mae: 1.4725 - val_loss: 19.0410 - val_mse: 19.0410 - val_mae: 1.4679 - lr: 2.7997e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 4s - loss: 11.2626 - mse: 11.2626 - mae: 1.4690 - val_loss: 18.9850 - val_mse: 18.9850 - val_mae: 1.4945 - lr: 2.7997e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 4s - loss: 11.1276 - mse: 11.1276 - mae: 1.4599 - val_loss: 19.1246 - val_mse: 19.1246 - val_mae: 1.4922 - lr: 2.7997e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 4s - loss: 11.0376 - mse: 11.0376 - mae: 1.4554 - val_loss: 18.9507 - val_mse: 18.9507 - val_mae: 1.4818 - lr: 2.7997e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 4s - loss: 10.9323 - mse: 10.9323 - mae: 1.4550 - val_loss: 19.0051 - val_mse: 19.0051 - val_mae: 1.4955 - lr: 2.7997e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 13/100\n",
            "1000/1000 - 4s - loss: 10.8596 - mse: 10.8596 - mae: 1.4524 - val_loss: 19.0314 - val_mse: 19.0314 - val_mae: 1.5185 - lr: 2.7997e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 14/100\n",
            "1000/1000 - 4s - loss: 10.7627 - mse: 10.7627 - mae: 1.4484 - val_loss: 18.9649 - val_mse: 18.9649 - val_mae: 1.4945 - lr: 2.7997e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 15/100\n",
            "1000/1000 - 3s - loss: 10.7145 - mse: 10.7145 - mae: 1.4463 - val_loss: 19.0481 - val_mse: 19.0481 - val_mae: 1.4930 - lr: 2.7997e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 16/100\n",
            "1000/1000 - 3s - loss: 10.5851 - mse: 10.5851 - mae: 1.4362 - val_loss: 19.2258 - val_mse: 19.2258 - val_mae: 1.4890 - lr: 2.7997e-04 - 3s/epoch - 3ms/step\n",
            "Score for fold 1: loss of 19.225807189941406\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 4s - loss: 12.3428 - mse: 12.3428 - mae: 1.4519 - val_loss: 12.3597 - val_mse: 12.3597 - val_mae: 1.3786 - lr: 2.7997e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 4s - loss: 12.2489 - mse: 12.2489 - mae: 1.4429 - val_loss: 12.3601 - val_mse: 12.3601 - val_mae: 1.4321 - lr: 2.7997e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 4s - loss: 12.1775 - mse: 12.1775 - mae: 1.4388 - val_loss: 12.1479 - val_mse: 12.1479 - val_mae: 1.4263 - lr: 2.7997e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 4s - loss: 12.0965 - mse: 12.0965 - mae: 1.4326 - val_loss: 12.0656 - val_mse: 12.0656 - val_mae: 1.4785 - lr: 2.7997e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 4s - loss: 12.0299 - mse: 12.0299 - mae: 1.4291 - val_loss: 12.1284 - val_mse: 12.1284 - val_mae: 1.4276 - lr: 2.7997e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 4s - loss: 11.9053 - mse: 11.9053 - mae: 1.4220 - val_loss: 12.3176 - val_mse: 12.3176 - val_mae: 1.4661 - lr: 2.7997e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 4s - loss: 11.8938 - mse: 11.8938 - mae: 1.4175 - val_loss: 11.9195 - val_mse: 11.9195 - val_mae: 1.4608 - lr: 2.7997e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 3s - loss: 11.7487 - mse: 11.7487 - mae: 1.4103 - val_loss: 12.0282 - val_mse: 12.0282 - val_mae: 1.4763 - lr: 2.7997e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 4s - loss: 11.7131 - mse: 11.7131 - mae: 1.4118 - val_loss: 12.1884 - val_mse: 12.1884 - val_mae: 1.4383 - lr: 2.7997e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 4s - loss: 11.5503 - mse: 11.5503 - mae: 1.4001 - val_loss: 11.9233 - val_mse: 11.9233 - val_mae: 1.4892 - lr: 2.7997e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 4s - loss: 11.5198 - mse: 11.5198 - mae: 1.4013 - val_loss: 12.2245 - val_mse: 12.2245 - val_mae: 1.5148 - lr: 2.7997e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 3s - loss: 11.3986 - mse: 11.3986 - mae: 1.3893 - val_loss: 11.8899 - val_mse: 11.8899 - val_mae: 1.4737 - lr: 2.7997e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 13/100\n",
            "1000/1000 - 3s - loss: 11.3278 - mse: 11.3278 - mae: 1.3895 - val_loss: 11.9015 - val_mse: 11.9015 - val_mae: 1.4638 - lr: 2.7997e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 14/100\n",
            "1000/1000 - 3s - loss: 11.2288 - mse: 11.2288 - mae: 1.3829 - val_loss: 11.9876 - val_mse: 11.9876 - val_mae: 1.4833 - lr: 2.7997e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 15/100\n",
            "1000/1000 - 4s - loss: 11.1921 - mse: 11.1921 - mae: 1.3784 - val_loss: 12.0917 - val_mse: 12.0917 - val_mae: 1.4977 - lr: 2.7997e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 16/100\n",
            "1000/1000 - 3s - loss: 11.0515 - mse: 11.0515 - mae: 1.3729 - val_loss: 11.7928 - val_mse: 11.7928 - val_mae: 1.4627 - lr: 2.7997e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 17/100\n",
            "1000/1000 - 4s - loss: 10.9287 - mse: 10.9287 - mae: 1.3631 - val_loss: 11.7152 - val_mse: 11.7152 - val_mae: 1.4954 - lr: 2.7997e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 18/100\n",
            "1000/1000 - 4s - loss: 10.8595 - mse: 10.8595 - mae: 1.3610 - val_loss: 12.1141 - val_mse: 12.1141 - val_mae: 1.5106 - lr: 2.7997e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 19/100\n",
            "1000/1000 - 4s - loss: 10.8049 - mse: 10.8049 - mae: 1.3571 - val_loss: 11.7683 - val_mse: 11.7683 - val_mae: 1.5238 - lr: 2.7997e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 20/100\n",
            "1000/1000 - 4s - loss: 10.7105 - mse: 10.7105 - mae: 1.3497 - val_loss: 11.6505 - val_mse: 11.6505 - val_mae: 1.5272 - lr: 2.7997e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 21/100\n",
            "1000/1000 - 4s - loss: 10.5659 - mse: 10.5659 - mae: 1.3423 - val_loss: 11.8772 - val_mse: 11.8772 - val_mae: 1.5058 - lr: 2.7997e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 22/100\n",
            "1000/1000 - 4s - loss: 10.4416 - mse: 10.4416 - mae: 1.3367 - val_loss: 11.5309 - val_mse: 11.5309 - val_mae: 1.5038 - lr: 2.7997e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 23/100\n",
            "1000/1000 - 4s - loss: 10.3725 - mse: 10.3725 - mae: 1.3301 - val_loss: 12.0701 - val_mse: 12.0701 - val_mae: 1.5118 - lr: 2.7997e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 24/100\n",
            "1000/1000 - 4s - loss: 10.3050 - mse: 10.3050 - mae: 1.3272 - val_loss: 11.8034 - val_mse: 11.8034 - val_mae: 1.5488 - lr: 2.7997e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 25/100\n",
            "1000/1000 - 4s - loss: 10.1742 - mse: 10.1742 - mae: 1.3188 - val_loss: 11.9322 - val_mse: 11.9322 - val_mae: 1.5533 - lr: 2.7997e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 26/100\n",
            "1000/1000 - 4s - loss: 10.0885 - mse: 10.0885 - mae: 1.3162 - val_loss: 11.6109 - val_mse: 11.6109 - val_mae: 1.5455 - lr: 2.7997e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 27/100\n",
            "1000/1000 - 4s - loss: 9.9530 - mse: 9.9530 - mae: 1.3071 - val_loss: 11.9252 - val_mse: 11.9252 - val_mae: 1.5497 - lr: 2.7997e-04 - 4s/epoch - 4ms/step\n",
            "Score for fold 2: loss of 11.925153732299805\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 4s - loss: 11.2870 - mse: 11.2870 - mae: 1.3616 - val_loss: 6.5579 - val_mse: 6.5579 - val_mae: 1.2748 - lr: 2.7997e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 4s - loss: 11.0819 - mse: 11.0819 - mae: 1.3456 - val_loss: 6.7997 - val_mse: 6.7997 - val_mae: 1.3692 - lr: 2.7997e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 4s - loss: 10.9359 - mse: 10.9359 - mae: 1.3337 - val_loss: 6.7439 - val_mse: 6.7439 - val_mae: 1.3546 - lr: 2.7997e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 4s - loss: 10.7465 - mse: 10.7465 - mae: 1.3216 - val_loss: 6.9517 - val_mse: 6.9517 - val_mae: 1.3663 - lr: 2.7997e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 4s - loss: 10.6445 - mse: 10.6445 - mae: 1.3184 - val_loss: 6.9571 - val_mse: 6.9571 - val_mae: 1.3457 - lr: 2.7997e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 4s - loss: 10.5159 - mse: 10.5159 - mae: 1.3085 - val_loss: 7.0114 - val_mse: 7.0114 - val_mae: 1.3532 - lr: 2.7997e-04 - 4s/epoch - 4ms/step\n",
            "Score for fold 3: loss of 7.011406421661377\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 3s - loss: 10.2068 - mse: 10.2068 - mae: 1.3300 - val_loss: 8.0432 - val_mse: 8.0432 - val_mae: 1.2564 - lr: 2.7997e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 3s - loss: 10.0206 - mse: 10.0206 - mae: 1.3145 - val_loss: 8.0866 - val_mse: 8.0866 - val_mae: 1.2737 - lr: 2.7997e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 4s - loss: 9.9108 - mse: 9.9108 - mae: 1.3012 - val_loss: 8.1335 - val_mse: 8.1335 - val_mae: 1.3047 - lr: 2.7997e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 4s - loss: 9.7682 - mse: 9.7682 - mae: 1.2931 - val_loss: 8.2686 - val_mse: 8.2686 - val_mae: 1.3587 - lr: 2.7997e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 4s - loss: 9.6244 - mse: 9.6244 - mae: 1.2854 - val_loss: 8.3324 - val_mse: 8.3324 - val_mae: 1.3506 - lr: 2.7997e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 4s - loss: 9.4865 - mse: 9.4865 - mae: 1.2741 - val_loss: 8.3918 - val_mse: 8.3918 - val_mae: 1.3591 - lr: 2.7997e-04 - 4s/epoch - 4ms/step\n",
            "Score for fold 4: loss of 8.391794204711914\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 4s - loss: 9.7785 - mse: 9.7785 - mae: 1.2986 - val_loss: 7.2481 - val_mse: 7.2481 - val_mae: 1.2413 - lr: 2.7997e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 4s - loss: 9.5290 - mse: 9.5290 - mae: 1.2813 - val_loss: 7.3460 - val_mse: 7.3460 - val_mae: 1.2636 - lr: 2.7997e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 4s - loss: 9.4164 - mse: 9.4164 - mae: 1.2722 - val_loss: 7.4323 - val_mse: 7.4323 - val_mae: 1.2596 - lr: 2.7997e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 4s - loss: 9.2685 - mse: 9.2685 - mae: 1.2598 - val_loss: 7.6136 - val_mse: 7.6136 - val_mae: 1.3045 - lr: 2.7997e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 4s - loss: 9.1291 - mse: 9.1291 - mae: 1.2479 - val_loss: 7.6856 - val_mse: 7.6856 - val_mae: 1.3085 - lr: 2.7997e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 4s - loss: 8.9925 - mse: 8.9925 - mae: 1.2415 - val_loss: 7.6672 - val_mse: 7.6672 - val_mae: 1.3042 - lr: 2.7997e-04 - 4s/epoch - 4ms/step\n",
            "Score for fold 5: loss of 7.667237281799316\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-10-11 13:28:43,809]\u001b[0m Finished trial#43 resulted in value: 10.846. Current best value is 9.334 with parameters: {'activation': 'relu', 'num_hidden_layer': 4, 'i': 9, 'learning_rate': 0.00023071835273936203}.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 11s - loss: 15.4689 - mse: 15.4689 - mae: 1.6667 - val_loss: 12.1968 - val_mse: 12.1968 - val_mae: 1.6184 - lr: 6.0508e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 10s - loss: 14.8683 - mse: 14.8683 - mae: 1.5834 - val_loss: 11.9157 - val_mse: 11.9157 - val_mae: 1.5189 - lr: 6.0508e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 11s - loss: 14.4923 - mse: 14.4923 - mae: 1.5485 - val_loss: 11.1078 - val_mse: 11.1078 - val_mae: 1.3966 - lr: 6.0508e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 11s - loss: 14.1549 - mse: 14.1549 - mae: 1.5314 - val_loss: 11.0113 - val_mse: 11.0113 - val_mae: 1.3841 - lr: 6.0508e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 11s - loss: 13.9494 - mse: 13.9494 - mae: 1.5262 - val_loss: 11.0213 - val_mse: 11.0213 - val_mae: 1.3921 - lr: 6.0508e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 11s - loss: 13.7663 - mse: 13.7663 - mae: 1.5215 - val_loss: 10.6843 - val_mse: 10.6843 - val_mae: 1.4672 - lr: 6.0508e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 11s - loss: 13.6784 - mse: 13.6784 - mae: 1.5100 - val_loss: 10.4763 - val_mse: 10.4763 - val_mae: 1.4120 - lr: 6.0508e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 11s - loss: 13.8112 - mse: 13.8112 - mae: 1.5107 - val_loss: 10.4805 - val_mse: 10.4805 - val_mae: 1.6349 - lr: 6.0508e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 11s - loss: 13.6613 - mse: 13.6613 - mae: 1.5126 - val_loss: 10.3703 - val_mse: 10.3703 - val_mae: 1.4868 - lr: 6.0508e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 11s - loss: 13.5592 - mse: 13.5592 - mae: 1.5098 - val_loss: 10.3264 - val_mse: 10.3264 - val_mae: 1.4134 - lr: 6.0508e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 11s - loss: 13.4624 - mse: 13.4624 - mae: 1.5039 - val_loss: 10.5586 - val_mse: 10.5586 - val_mae: 1.5305 - lr: 6.0508e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 11s - loss: 13.3796 - mse: 13.3796 - mae: 1.5109 - val_loss: 10.1004 - val_mse: 10.1004 - val_mae: 1.3587 - lr: 6.0508e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 13/100\n",
            "1000/1000 - 10s - loss: 13.5888 - mse: 13.5888 - mae: 1.5065 - val_loss: 10.1805 - val_mse: 10.1805 - val_mae: 1.5672 - lr: 6.0508e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 14/100\n",
            "1000/1000 - 11s - loss: 13.4641 - mse: 13.4641 - mae: 1.5146 - val_loss: 10.5551 - val_mse: 10.5551 - val_mae: 1.3501 - lr: 6.0508e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 15/100\n",
            "1000/1000 - 11s - loss: 13.3179 - mse: 13.3179 - mae: 1.5023 - val_loss: 9.8270 - val_mse: 9.8270 - val_mae: 1.4946 - lr: 6.0508e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 16/100\n",
            "1000/1000 - 11s - loss: 13.1548 - mse: 13.1548 - mae: 1.5017 - val_loss: 10.1406 - val_mse: 10.1406 - val_mae: 1.5414 - lr: 6.0508e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 17/100\n",
            "1000/1000 - 11s - loss: 13.1894 - mse: 13.1894 - mae: 1.5105 - val_loss: 10.3953 - val_mse: 10.3953 - val_mae: 1.3605 - lr: 6.0508e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 18/100\n",
            "1000/1000 - 11s - loss: 13.1610 - mse: 13.1610 - mae: 1.4957 - val_loss: 9.8959 - val_mse: 9.8959 - val_mae: 1.4403 - lr: 6.0508e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 19/100\n",
            "1000/1000 - 11s - loss: 13.0233 - mse: 13.0233 - mae: 1.4990 - val_loss: 9.6785 - val_mse: 9.6785 - val_mae: 1.4096 - lr: 6.0508e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 20/100\n",
            "1000/1000 - 11s - loss: 12.9516 - mse: 12.9516 - mae: 1.4922 - val_loss: 9.7826 - val_mse: 9.7826 - val_mae: 1.3967 - lr: 6.0508e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 21/100\n",
            "1000/1000 - 11s - loss: 12.9703 - mse: 12.9703 - mae: 1.4943 - val_loss: 9.7245 - val_mse: 9.7245 - val_mae: 1.4942 - lr: 6.0508e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 22/100\n",
            "1000/1000 - 11s - loss: 13.0066 - mse: 13.0066 - mae: 1.4866 - val_loss: 9.9992 - val_mse: 9.9992 - val_mae: 1.6856 - lr: 6.0508e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 23/100\n",
            "1000/1000 - 10s - loss: 13.0055 - mse: 13.0055 - mae: 1.4938 - val_loss: 9.4608 - val_mse: 9.4608 - val_mae: 1.4477 - lr: 6.0508e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 24/100\n",
            "1000/1000 - 10s - loss: 12.8988 - mse: 12.8988 - mae: 1.4869 - val_loss: 9.5259 - val_mse: 9.5259 - val_mae: 1.6073 - lr: 6.0508e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 25/100\n",
            "1000/1000 - 11s - loss: 12.7851 - mse: 12.7851 - mae: 1.4822 - val_loss: 9.9390 - val_mse: 9.9390 - val_mae: 1.6668 - lr: 6.0508e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 26/100\n",
            "1000/1000 - 11s - loss: 12.8940 - mse: 12.8940 - mae: 1.4878 - val_loss: 9.7061 - val_mse: 9.7061 - val_mae: 1.3497 - lr: 6.0508e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 27/100\n",
            "1000/1000 - 11s - loss: 12.6578 - mse: 12.6578 - mae: 1.4796 - val_loss: 9.1078 - val_mse: 9.1078 - val_mae: 1.3774 - lr: 6.0508e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 28/100\n",
            "1000/1000 - 11s - loss: 12.6525 - mse: 12.6525 - mae: 1.4770 - val_loss: 9.2730 - val_mse: 9.2730 - val_mae: 1.3842 - lr: 6.0508e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 29/100\n",
            "1000/1000 - 10s - loss: 12.5832 - mse: 12.5832 - mae: 1.4774 - val_loss: 9.1857 - val_mse: 9.1857 - val_mae: 1.4280 - lr: 6.0508e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 30/100\n",
            "1000/1000 - 10s - loss: 12.6896 - mse: 12.6896 - mae: 1.4712 - val_loss: 9.4670 - val_mse: 9.4670 - val_mae: 1.4728 - lr: 6.0508e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 31/100\n",
            "1000/1000 - 11s - loss: 12.4623 - mse: 12.4623 - mae: 1.4790 - val_loss: 9.4085 - val_mse: 9.4085 - val_mae: 1.5559 - lr: 6.0508e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 32/100\n",
            "1000/1000 - 11s - loss: 12.3926 - mse: 12.3926 - mae: 1.4795 - val_loss: 9.0678 - val_mse: 9.0678 - val_mae: 1.4441 - lr: 6.0508e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 33/100\n",
            "1000/1000 - 11s - loss: 12.4743 - mse: 12.4743 - mae: 1.4688 - val_loss: 9.4069 - val_mse: 9.4069 - val_mae: 1.3461 - lr: 6.0508e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 34/100\n",
            "1000/1000 - 10s - loss: 12.4170 - mse: 12.4170 - mae: 1.4769 - val_loss: 9.0002 - val_mse: 9.0002 - val_mae: 1.4160 - lr: 6.0508e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 35/100\n",
            "1000/1000 - 11s - loss: 12.3069 - mse: 12.3069 - mae: 1.4756 - val_loss: 8.8392 - val_mse: 8.8392 - val_mae: 1.4434 - lr: 6.0508e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 36/100\n",
            "1000/1000 - 11s - loss: 12.1595 - mse: 12.1595 - mae: 1.4609 - val_loss: 8.7876 - val_mse: 8.7876 - val_mae: 1.5864 - lr: 6.0508e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 37/100\n",
            "1000/1000 - 11s - loss: 12.1172 - mse: 12.1172 - mae: 1.4665 - val_loss: 8.9400 - val_mse: 8.9400 - val_mae: 1.3549 - lr: 6.0508e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 38/100\n",
            "1000/1000 - 11s - loss: 12.2116 - mse: 12.2116 - mae: 1.4704 - val_loss: 8.7150 - val_mse: 8.7150 - val_mae: 1.4905 - lr: 6.0508e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 39/100\n",
            "1000/1000 - 11s - loss: 12.1062 - mse: 12.1062 - mae: 1.4611 - val_loss: 9.0410 - val_mse: 9.0410 - val_mae: 1.3731 - lr: 6.0508e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 40/100\n",
            "1000/1000 - 11s - loss: 11.9631 - mse: 11.9631 - mae: 1.4522 - val_loss: 9.1247 - val_mse: 9.1247 - val_mae: 1.3670 - lr: 6.0508e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 41/100\n",
            "1000/1000 - 10s - loss: 12.0645 - mse: 12.0645 - mae: 1.4588 - val_loss: 9.6798 - val_mse: 9.6798 - val_mae: 1.4466 - lr: 6.0508e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 42/100\n",
            "1000/1000 - 11s - loss: 11.9761 - mse: 11.9761 - mae: 1.4474 - val_loss: 9.1827 - val_mse: 9.1827 - val_mae: 1.3478 - lr: 6.0508e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 43/100\n",
            "1000/1000 - 11s - loss: 11.8864 - mse: 11.8864 - mae: 1.4531 - val_loss: 8.7276 - val_mse: 8.7276 - val_mae: 1.4014 - lr: 6.0508e-04 - 11s/epoch - 11ms/step\n",
            "Score for fold 1: loss of 8.727590560913086\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 13s - loss: 9.2691 - mse: 9.2691 - mae: 1.4386 - val_loss: 19.7420 - val_mse: 19.7420 - val_mae: 1.5689 - lr: 6.0508e-04 - 13s/epoch - 13ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 12s - loss: 9.3203 - mse: 9.3203 - mae: 1.4437 - val_loss: 19.5575 - val_mse: 19.5575 - val_mae: 1.6360 - lr: 6.0508e-04 - 12s/epoch - 12ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 12s - loss: 9.1173 - mse: 9.1173 - mae: 1.4204 - val_loss: 19.5859 - val_mse: 19.5859 - val_mae: 1.5939 - lr: 6.0508e-04 - 12s/epoch - 12ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 12s - loss: 9.1858 - mse: 9.1858 - mae: 1.4260 - val_loss: 19.1818 - val_mse: 19.1818 - val_mae: 1.4332 - lr: 6.0508e-04 - 12s/epoch - 12ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 12s - loss: 8.9229 - mse: 8.9229 - mae: 1.4240 - val_loss: 20.2874 - val_mse: 20.2874 - val_mae: 1.4192 - lr: 6.0508e-04 - 12s/epoch - 12ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 12s - loss: 8.9081 - mse: 8.9081 - mae: 1.4172 - val_loss: 19.2933 - val_mse: 19.2933 - val_mae: 1.7468 - lr: 6.0508e-04 - 12s/epoch - 12ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 12s - loss: 8.8206 - mse: 8.8206 - mae: 1.4156 - val_loss: 19.9776 - val_mse: 19.9776 - val_mae: 1.4879 - lr: 6.0508e-04 - 12s/epoch - 12ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 12s - loss: 8.7887 - mse: 8.7887 - mae: 1.4180 - val_loss: 19.7278 - val_mse: 19.7278 - val_mae: 1.5965 - lr: 6.0508e-04 - 12s/epoch - 12ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 12s - loss: 8.6656 - mse: 8.6656 - mae: 1.4113 - val_loss: 19.6133 - val_mse: 19.6133 - val_mae: 1.4380 - lr: 6.0508e-04 - 12s/epoch - 12ms/step\n",
            "Score for fold 2: loss of 19.613365173339844\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 12s - loss: 11.8121 - mse: 11.8121 - mae: 1.4493 - val_loss: 7.3011 - val_mse: 7.3011 - val_mae: 1.3747 - lr: 6.0508e-04 - 12s/epoch - 12ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 11s - loss: 11.5575 - mse: 11.5575 - mae: 1.4419 - val_loss: 8.0515 - val_mse: 8.0515 - val_mae: 1.3247 - lr: 6.0508e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 10s - loss: 11.4719 - mse: 11.4719 - mae: 1.4346 - val_loss: 8.2637 - val_mse: 8.2637 - val_mae: 1.4063 - lr: 6.0508e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 11s - loss: 11.5076 - mse: 11.5076 - mae: 1.4469 - val_loss: 8.0889 - val_mse: 8.0889 - val_mae: 1.4070 - lr: 6.0508e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 11s - loss: 11.2767 - mse: 11.2767 - mae: 1.4260 - val_loss: 7.8071 - val_mse: 7.8071 - val_mae: 1.5723 - lr: 6.0508e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 11s - loss: 11.1944 - mse: 11.1944 - mae: 1.4167 - val_loss: 7.9445 - val_mse: 7.9445 - val_mae: 1.4191 - lr: 6.0508e-04 - 11s/epoch - 11ms/step\n",
            "Score for fold 3: loss of 7.94448184967041\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 11s - loss: 11.0498 - mse: 11.0498 - mae: 1.4223 - val_loss: 8.0512 - val_mse: 8.0512 - val_mae: 1.3786 - lr: 6.0508e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 11s - loss: 11.0363 - mse: 11.0363 - mae: 1.4233 - val_loss: 8.3443 - val_mse: 8.3443 - val_mae: 1.3842 - lr: 6.0508e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 11s - loss: 10.9566 - mse: 10.9566 - mae: 1.4170 - val_loss: 8.1955 - val_mse: 8.1955 - val_mae: 1.4164 - lr: 6.0508e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 11s - loss: 10.7889 - mse: 10.7889 - mae: 1.4057 - val_loss: 8.3767 - val_mse: 8.3767 - val_mae: 1.3259 - lr: 6.0508e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 11s - loss: 10.7815 - mse: 10.7815 - mae: 1.4127 - val_loss: 8.4156 - val_mse: 8.4156 - val_mae: 1.4477 - lr: 6.0508e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 11s - loss: 10.6525 - mse: 10.6525 - mae: 1.4174 - val_loss: 8.7758 - val_mse: 8.7758 - val_mae: 1.5442 - lr: 6.0508e-04 - 11s/epoch - 11ms/step\n",
            "Score for fold 4: loss of 8.77576732635498\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 11s - loss: 10.1546 - mse: 10.1546 - mae: 1.4314 - val_loss: 10.7980 - val_mse: 10.7980 - val_mae: 1.3500 - lr: 6.0508e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 11s - loss: 10.0740 - mse: 10.0740 - mae: 1.4205 - val_loss: 11.2250 - val_mse: 11.2250 - val_mae: 1.3274 - lr: 6.0508e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 11s - loss: 9.9168 - mse: 9.9168 - mae: 1.4062 - val_loss: 11.3444 - val_mse: 11.3444 - val_mae: 1.5565 - lr: 6.0508e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 11s - loss: 9.8084 - mse: 9.8084 - mae: 1.3989 - val_loss: 11.4881 - val_mse: 11.4881 - val_mae: 1.5189 - lr: 6.0508e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 11s - loss: 9.8630 - mse: 9.8630 - mae: 1.4059 - val_loss: 11.7102 - val_mse: 11.7102 - val_mae: 1.3914 - lr: 6.0508e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 11s - loss: 9.7677 - mse: 9.7677 - mae: 1.4090 - val_loss: 11.5101 - val_mse: 11.5101 - val_mae: 1.4925 - lr: 6.0508e-04 - 11s/epoch - 11ms/step\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-10-11 13:41:41,357]\u001b[0m Finished trial#44 resulted in value: 11.314. Current best value is 9.334 with parameters: {'activation': 'relu', 'num_hidden_layer': 4, 'i': 9, 'learning_rate': 0.00023071835273936203}.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Score for fold 5: loss of 11.510079383850098\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 7s - loss: 12.9363 - mse: 12.9363 - mae: 1.6022 - val_loss: 20.0622 - val_mse: 20.0622 - val_mae: 1.4656 - lr: 3.7856e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 6s - loss: 12.2544 - mse: 12.2544 - mae: 1.5228 - val_loss: 19.1834 - val_mse: 19.1834 - val_mae: 1.4829 - lr: 3.7856e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 6s - loss: 12.0243 - mse: 12.0243 - mae: 1.5140 - val_loss: 19.0460 - val_mse: 19.0460 - val_mae: 1.5243 - lr: 3.7856e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 6s - loss: 11.9153 - mse: 11.9153 - mae: 1.5016 - val_loss: 18.8776 - val_mse: 18.8776 - val_mae: 1.4641 - lr: 3.7856e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 6s - loss: 11.7180 - mse: 11.7180 - mae: 1.4900 - val_loss: 18.8962 - val_mse: 18.8962 - val_mae: 1.5432 - lr: 3.7856e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 6s - loss: 11.6260 - mse: 11.6260 - mae: 1.4842 - val_loss: 18.7710 - val_mse: 18.7710 - val_mae: 1.5075 - lr: 3.7856e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 6s - loss: 11.4919 - mse: 11.4919 - mae: 1.4762 - val_loss: 18.7050 - val_mse: 18.7050 - val_mae: 1.4460 - lr: 3.7856e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 6s - loss: 11.3985 - mse: 11.3985 - mae: 1.4673 - val_loss: 18.7554 - val_mse: 18.7554 - val_mae: 1.4628 - lr: 3.7856e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 6s - loss: 11.3030 - mse: 11.3030 - mae: 1.4632 - val_loss: 18.5569 - val_mse: 18.5569 - val_mae: 1.5978 - lr: 3.7856e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 6s - loss: 11.1661 - mse: 11.1661 - mae: 1.4563 - val_loss: 18.6887 - val_mse: 18.6887 - val_mae: 1.5533 - lr: 3.7856e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 6s - loss: 11.1107 - mse: 11.1107 - mae: 1.4558 - val_loss: 18.5852 - val_mse: 18.5852 - val_mae: 1.4702 - lr: 3.7856e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 6s - loss: 11.0280 - mse: 11.0280 - mae: 1.4449 - val_loss: 18.5103 - val_mse: 18.5103 - val_mae: 1.5885 - lr: 3.7856e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 13/100\n",
            "1000/1000 - 6s - loss: 10.8310 - mse: 10.8310 - mae: 1.4379 - val_loss: 18.3971 - val_mse: 18.3971 - val_mae: 1.5022 - lr: 3.7856e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 14/100\n",
            "1000/1000 - 6s - loss: 10.7160 - mse: 10.7160 - mae: 1.4316 - val_loss: 18.3209 - val_mse: 18.3209 - val_mae: 1.5226 - lr: 3.7856e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 15/100\n",
            "1000/1000 - 6s - loss: 10.5470 - mse: 10.5470 - mae: 1.4254 - val_loss: 18.6406 - val_mse: 18.6406 - val_mae: 1.5472 - lr: 3.7856e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 16/100\n",
            "1000/1000 - 6s - loss: 10.5436 - mse: 10.5436 - mae: 1.4173 - val_loss: 18.7930 - val_mse: 18.7930 - val_mae: 1.4728 - lr: 3.7856e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 17/100\n",
            "1000/1000 - 6s - loss: 10.3553 - mse: 10.3553 - mae: 1.4057 - val_loss: 18.3708 - val_mse: 18.3708 - val_mae: 1.5496 - lr: 3.7856e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 18/100\n",
            "1000/1000 - 6s - loss: 10.1933 - mse: 10.1933 - mae: 1.3960 - val_loss: 18.1458 - val_mse: 18.1458 - val_mae: 1.5633 - lr: 3.7856e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 19/100\n",
            "1000/1000 - 6s - loss: 9.9976 - mse: 9.9976 - mae: 1.3868 - val_loss: 18.1892 - val_mse: 18.1892 - val_mae: 1.5618 - lr: 3.7856e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 20/100\n",
            "1000/1000 - 6s - loss: 9.8195 - mse: 9.8195 - mae: 1.3774 - val_loss: 18.1919 - val_mse: 18.1919 - val_mae: 1.5528 - lr: 3.7856e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 21/100\n",
            "1000/1000 - 6s - loss: 9.6131 - mse: 9.6131 - mae: 1.3639 - val_loss: 18.0549 - val_mse: 18.0549 - val_mae: 1.5473 - lr: 3.7856e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 22/100\n",
            "1000/1000 - 6s - loss: 9.3491 - mse: 9.3491 - mae: 1.3469 - val_loss: 18.0729 - val_mse: 18.0729 - val_mae: 1.5604 - lr: 3.7856e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 23/100\n",
            "1000/1000 - 6s - loss: 9.1775 - mse: 9.1775 - mae: 1.3361 - val_loss: 18.1755 - val_mse: 18.1755 - val_mae: 1.5696 - lr: 3.7856e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 24/100\n",
            "1000/1000 - 6s - loss: 9.0524 - mse: 9.0524 - mae: 1.3240 - val_loss: 18.1378 - val_mse: 18.1378 - val_mae: 1.6225 - lr: 3.7856e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 25/100\n",
            "1000/1000 - 6s - loss: 8.6475 - mse: 8.6475 - mae: 1.3050 - val_loss: 18.0649 - val_mse: 18.0649 - val_mae: 1.6170 - lr: 3.7856e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 26/100\n",
            "1000/1000 - 6s - loss: 8.4974 - mse: 8.4974 - mae: 1.2943 - val_loss: 18.1673 - val_mse: 18.1673 - val_mae: 1.5826 - lr: 3.7856e-04 - 6s/epoch - 6ms/step\n",
            "Score for fold 1: loss of 18.16729164123535\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 6s - loss: 11.4183 - mse: 11.4183 - mae: 1.3739 - val_loss: 6.4230 - val_mse: 6.4230 - val_mae: 1.3305 - lr: 3.7856e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 6s - loss: 10.9646 - mse: 10.9646 - mae: 1.3481 - val_loss: 6.6233 - val_mse: 6.6233 - val_mae: 1.3168 - lr: 3.7856e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 6s - loss: 10.7141 - mse: 10.7141 - mae: 1.3198 - val_loss: 6.9365 - val_mse: 6.9365 - val_mae: 1.3118 - lr: 3.7856e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 6s - loss: 10.5010 - mse: 10.5010 - mae: 1.3001 - val_loss: 6.7598 - val_mse: 6.7598 - val_mae: 1.3829 - lr: 3.7856e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 6s - loss: 10.1555 - mse: 10.1555 - mae: 1.2774 - val_loss: 6.7812 - val_mse: 6.7812 - val_mae: 1.3765 - lr: 3.7856e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 6s - loss: 9.8355 - mse: 9.8355 - mae: 1.2569 - val_loss: 6.7468 - val_mse: 6.7468 - val_mae: 1.3770 - lr: 3.7856e-04 - 6s/epoch - 6ms/step\n",
            "Score for fold 2: loss of 6.7468461990356445\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 6s - loss: 8.2794 - mse: 8.2794 - mae: 1.2810 - val_loss: 12.5407 - val_mse: 12.5407 - val_mae: 1.2595 - lr: 3.7856e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 6s - loss: 7.8796 - mse: 7.8796 - mae: 1.2457 - val_loss: 12.6814 - val_mse: 12.6814 - val_mae: 1.3234 - lr: 3.7856e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 6s - loss: 7.6027 - mse: 7.6027 - mae: 1.2205 - val_loss: 13.0708 - val_mse: 13.0708 - val_mae: 1.3142 - lr: 3.7856e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 6s - loss: 7.2243 - mse: 7.2243 - mae: 1.1890 - val_loss: 12.9593 - val_mse: 12.9593 - val_mae: 1.3205 - lr: 3.7856e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 6s - loss: 6.9318 - mse: 6.9318 - mae: 1.1579 - val_loss: 13.1015 - val_mse: 13.1015 - val_mae: 1.3739 - lr: 3.7856e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 6s - loss: 6.6746 - mse: 6.6746 - mae: 1.1342 - val_loss: 13.0476 - val_mse: 13.0476 - val_mae: 1.3589 - lr: 3.7856e-04 - 6s/epoch - 6ms/step\n",
            "Score for fold 3: loss of 13.047637939453125\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 6s - loss: 8.6571 - mse: 8.6571 - mae: 1.2010 - val_loss: 4.5337 - val_mse: 4.5337 - val_mae: 1.0981 - lr: 3.7856e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 6s - loss: 8.2633 - mse: 8.2633 - mae: 1.1628 - val_loss: 4.7608 - val_mse: 4.7608 - val_mae: 1.1410 - lr: 3.7856e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 6s - loss: 7.9565 - mse: 7.9565 - mae: 1.1260 - val_loss: 4.7081 - val_mse: 4.7081 - val_mae: 1.1197 - lr: 3.7856e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 7s - loss: 7.6869 - mse: 7.6869 - mae: 1.1006 - val_loss: 4.8581 - val_mse: 4.8581 - val_mae: 1.1429 - lr: 3.7856e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 7s - loss: 7.4205 - mse: 7.4205 - mae: 1.0667 - val_loss: 4.8562 - val_mse: 4.8562 - val_mae: 1.1753 - lr: 3.7856e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 7s - loss: 7.1811 - mse: 7.1811 - mae: 1.0445 - val_loss: 4.9051 - val_mse: 4.9051 - val_mae: 1.1968 - lr: 3.7856e-04 - 7s/epoch - 7ms/step\n",
            "Score for fold 4: loss of 4.905145168304443\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 6s - loss: 7.5163 - mse: 7.5163 - mae: 1.0859 - val_loss: 2.8819 - val_mse: 2.8819 - val_mae: 0.9492 - lr: 3.7856e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 6s - loss: 7.2438 - mse: 7.2438 - mae: 1.0421 - val_loss: 3.1481 - val_mse: 3.1481 - val_mae: 0.9828 - lr: 3.7856e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 6s - loss: 6.8808 - mse: 6.8808 - mae: 1.0105 - val_loss: 3.1282 - val_mse: 3.1282 - val_mae: 1.0216 - lr: 3.7856e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 6s - loss: 6.7795 - mse: 6.7795 - mae: 0.9820 - val_loss: 3.3191 - val_mse: 3.3191 - val_mae: 1.0801 - lr: 3.7856e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 6s - loss: 6.4872 - mse: 6.4872 - mae: 0.9525 - val_loss: 3.3210 - val_mse: 3.3210 - val_mae: 1.0611 - lr: 3.7856e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 6s - loss: 6.3003 - mse: 6.3003 - mae: 0.9239 - val_loss: 3.3538 - val_mse: 3.3538 - val_mae: 1.0835 - lr: 3.7856e-04 - 6s/epoch - 6ms/step\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-10-11 13:47:01,231]\u001b[0m Finished trial#45 resulted in value: 9.245999999999999. Current best value is 9.245999999999999 with parameters: {'activation': 'tanh', 'num_hidden_layer': 4, 'i': 9, 'learning_rate': 0.0003785646080419556}.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Score for fold 5: loss of 3.353761672973633\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 7s - loss: 12.0994 - mse: 12.0994 - mae: 1.6043 - val_loss: 23.1470 - val_mse: 23.1470 - val_mae: 1.5238 - lr: 3.9525e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 6s - loss: 11.4225 - mse: 11.4225 - mae: 1.5279 - val_loss: 23.5820 - val_mse: 23.5820 - val_mae: 1.5607 - lr: 3.9525e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 6s - loss: 11.2462 - mse: 11.2462 - mae: 1.5154 - val_loss: 23.1658 - val_mse: 23.1658 - val_mae: 1.5206 - lr: 3.9525e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 6s - loss: 11.0935 - mse: 11.0935 - mae: 1.5061 - val_loss: 23.3073 - val_mse: 23.3073 - val_mae: 1.6853 - lr: 3.9525e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 6s - loss: 10.9872 - mse: 10.9872 - mae: 1.4981 - val_loss: 22.8400 - val_mse: 22.8400 - val_mae: 1.5519 - lr: 3.9525e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 6s - loss: 10.8193 - mse: 10.8193 - mae: 1.4841 - val_loss: 22.9953 - val_mse: 22.9953 - val_mae: 1.5022 - lr: 3.9525e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 6s - loss: 10.6841 - mse: 10.6841 - mae: 1.4789 - val_loss: 22.4740 - val_mse: 22.4740 - val_mae: 1.5376 - lr: 3.9525e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 6s - loss: 10.6272 - mse: 10.6272 - mae: 1.4675 - val_loss: 22.2503 - val_mse: 22.2503 - val_mae: 1.4980 - lr: 3.9525e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 6s - loss: 10.4760 - mse: 10.4760 - mae: 1.4651 - val_loss: 22.5130 - val_mse: 22.5130 - val_mae: 1.5111 - lr: 3.9525e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 6s - loss: 10.4037 - mse: 10.4037 - mae: 1.4620 - val_loss: 22.1145 - val_mse: 22.1145 - val_mae: 1.5322 - lr: 3.9525e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 6s - loss: 10.2823 - mse: 10.2823 - mae: 1.4466 - val_loss: 22.1088 - val_mse: 22.1088 - val_mae: 1.5301 - lr: 3.9525e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 6s - loss: 10.0932 - mse: 10.0932 - mae: 1.4437 - val_loss: 22.1328 - val_mse: 22.1328 - val_mae: 1.5722 - lr: 3.9525e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 13/100\n",
            "1000/1000 - 6s - loss: 10.0017 - mse: 10.0017 - mae: 1.4361 - val_loss: 21.9816 - val_mse: 21.9816 - val_mae: 1.5728 - lr: 3.9525e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 14/100\n",
            "1000/1000 - 6s - loss: 9.8523 - mse: 9.8523 - mae: 1.4284 - val_loss: 22.1100 - val_mse: 22.1100 - val_mae: 1.5361 - lr: 3.9525e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 15/100\n",
            "1000/1000 - 6s - loss: 9.7353 - mse: 9.7353 - mae: 1.4210 - val_loss: 22.2423 - val_mse: 22.2423 - val_mae: 1.5562 - lr: 3.9525e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 16/100\n",
            "1000/1000 - 6s - loss: 9.6015 - mse: 9.6015 - mae: 1.4113 - val_loss: 21.6272 - val_mse: 21.6272 - val_mae: 1.4764 - lr: 3.9525e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 17/100\n",
            "1000/1000 - 6s - loss: 9.3544 - mse: 9.3544 - mae: 1.3972 - val_loss: 22.0128 - val_mse: 22.0128 - val_mae: 1.5168 - lr: 3.9525e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 18/100\n",
            "1000/1000 - 6s - loss: 9.2606 - mse: 9.2606 - mae: 1.3885 - val_loss: 21.6130 - val_mse: 21.6130 - val_mae: 1.5622 - lr: 3.9525e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 19/100\n",
            "1000/1000 - 6s - loss: 9.1167 - mse: 9.1167 - mae: 1.3788 - val_loss: 21.5878 - val_mse: 21.5878 - val_mae: 1.5748 - lr: 3.9525e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 20/100\n",
            "1000/1000 - 6s - loss: 8.9838 - mse: 8.9838 - mae: 1.3625 - val_loss: 21.5131 - val_mse: 21.5131 - val_mae: 1.5536 - lr: 3.9525e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 21/100\n",
            "1000/1000 - 7s - loss: 8.7207 - mse: 8.7207 - mae: 1.3555 - val_loss: 21.6273 - val_mse: 21.6273 - val_mae: 1.5895 - lr: 3.9525e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 22/100\n",
            "1000/1000 - 7s - loss: 8.4990 - mse: 8.4990 - mae: 1.3376 - val_loss: 21.4712 - val_mse: 21.4712 - val_mae: 1.5827 - lr: 3.9525e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 23/100\n",
            "1000/1000 - 7s - loss: 8.2895 - mse: 8.2895 - mae: 1.3216 - val_loss: 21.4977 - val_mse: 21.4977 - val_mae: 1.5674 - lr: 3.9525e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 24/100\n",
            "1000/1000 - 7s - loss: 8.1437 - mse: 8.1437 - mae: 1.3054 - val_loss: 21.3594 - val_mse: 21.3594 - val_mae: 1.5759 - lr: 3.9525e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 25/100\n",
            "1000/1000 - 7s - loss: 7.8119 - mse: 7.8119 - mae: 1.2937 - val_loss: 21.6375 - val_mse: 21.6375 - val_mae: 1.6608 - lr: 3.9525e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 26/100\n",
            "1000/1000 - 6s - loss: 7.6432 - mse: 7.6432 - mae: 1.2778 - val_loss: 21.6725 - val_mse: 21.6725 - val_mae: 1.6222 - lr: 3.9525e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 27/100\n",
            "1000/1000 - 6s - loss: 7.4135 - mse: 7.4135 - mae: 1.2616 - val_loss: 21.8613 - val_mse: 21.8613 - val_mae: 1.6401 - lr: 3.9525e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 28/100\n",
            "1000/1000 - 7s - loss: 7.1892 - mse: 7.1892 - mae: 1.2418 - val_loss: 21.7110 - val_mse: 21.7110 - val_mae: 1.6587 - lr: 3.9525e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 29/100\n",
            "1000/1000 - 7s - loss: 6.9013 - mse: 6.9013 - mae: 1.2210 - val_loss: 21.9857 - val_mse: 21.9857 - val_mae: 1.6448 - lr: 3.9525e-04 - 7s/epoch - 7ms/step\n",
            "Score for fold 1: loss of 21.98574447631836\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 6s - loss: 11.1478 - mse: 11.1478 - mae: 1.3373 - val_loss: 5.1786 - val_mse: 5.1786 - val_mae: 1.2242 - lr: 3.9525e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 6s - loss: 10.5744 - mse: 10.5744 - mae: 1.3011 - val_loss: 5.1251 - val_mse: 5.1251 - val_mae: 1.2426 - lr: 3.9525e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 7s - loss: 10.2682 - mse: 10.2682 - mae: 1.2738 - val_loss: 5.4863 - val_mse: 5.4863 - val_mae: 1.2360 - lr: 3.9525e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 7s - loss: 9.9731 - mse: 9.9731 - mae: 1.2493 - val_loss: 5.5326 - val_mse: 5.5326 - val_mae: 1.3166 - lr: 3.9525e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 7s - loss: 9.5685 - mse: 9.5685 - mae: 1.2266 - val_loss: 5.5668 - val_mse: 5.5668 - val_mae: 1.2783 - lr: 3.9525e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 7s - loss: 9.2777 - mse: 9.2777 - mae: 1.2031 - val_loss: 5.7505 - val_mse: 5.7505 - val_mae: 1.3142 - lr: 3.9525e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 7s - loss: 9.0673 - mse: 9.0673 - mae: 1.1727 - val_loss: 5.7207 - val_mse: 5.7207 - val_mae: 1.3317 - lr: 3.9525e-04 - 7s/epoch - 7ms/step\n",
            "Score for fold 2: loss of 5.720705032348633\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 6s - loss: 8.8864 - mse: 8.8864 - mae: 1.2187 - val_loss: 5.7039 - val_mse: 5.7039 - val_mae: 1.1306 - lr: 3.9525e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 6s - loss: 8.5350 - mse: 8.5350 - mae: 1.1814 - val_loss: 5.9016 - val_mse: 5.9016 - val_mae: 1.1654 - lr: 3.9525e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 6s - loss: 8.1017 - mse: 8.1017 - mae: 1.1418 - val_loss: 6.1241 - val_mse: 6.1241 - val_mae: 1.1927 - lr: 3.9525e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 6s - loss: 7.7711 - mse: 7.7711 - mae: 1.1117 - val_loss: 6.2472 - val_mse: 6.2472 - val_mae: 1.2296 - lr: 3.9525e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 6s - loss: 7.4406 - mse: 7.4406 - mae: 1.0787 - val_loss: 6.3959 - val_mse: 6.3959 - val_mae: 1.2285 - lr: 3.9525e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 6s - loss: 7.1849 - mse: 7.1849 - mae: 1.0546 - val_loss: 6.2993 - val_mse: 6.2993 - val_mae: 1.2519 - lr: 3.9525e-04 - 6s/epoch - 6ms/step\n",
            "Score for fold 3: loss of 6.299264907836914\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 6s - loss: 7.5404 - mse: 7.5404 - mae: 1.1097 - val_loss: 4.3715 - val_mse: 4.3715 - val_mae: 0.9832 - lr: 3.9525e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 6s - loss: 7.1733 - mse: 7.1733 - mae: 1.0606 - val_loss: 4.4910 - val_mse: 4.4910 - val_mae: 1.0212 - lr: 3.9525e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 6s - loss: 6.9668 - mse: 6.9668 - mae: 1.0293 - val_loss: 4.6590 - val_mse: 4.6590 - val_mae: 1.0952 - lr: 3.9525e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 6s - loss: 6.6598 - mse: 6.6598 - mae: 0.9984 - val_loss: 4.8974 - val_mse: 4.8974 - val_mae: 1.0706 - lr: 3.9525e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 6s - loss: 6.4823 - mse: 6.4823 - mae: 0.9732 - val_loss: 4.8812 - val_mse: 4.8812 - val_mae: 1.0957 - lr: 3.9525e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 6s - loss: 6.2683 - mse: 6.2683 - mae: 0.9395 - val_loss: 4.8036 - val_mse: 4.8036 - val_mae: 1.1355 - lr: 3.9525e-04 - 6s/epoch - 6ms/step\n",
            "Score for fold 4: loss of 4.8036417961120605\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 6s - loss: 6.2579 - mse: 6.2579 - mae: 0.9904 - val_loss: 4.1636 - val_mse: 4.1636 - val_mae: 0.8703 - lr: 3.9525e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 6s - loss: 5.9924 - mse: 5.9924 - mae: 0.9521 - val_loss: 4.4438 - val_mse: 4.4438 - val_mae: 0.9086 - lr: 3.9525e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 6s - loss: 5.7789 - mse: 5.7789 - mae: 0.9104 - val_loss: 4.3616 - val_mse: 4.3616 - val_mae: 0.9437 - lr: 3.9525e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 6s - loss: 5.6406 - mse: 5.6406 - mae: 0.8865 - val_loss: 4.4984 - val_mse: 4.4984 - val_mae: 0.9626 - lr: 3.9525e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 6s - loss: 5.4559 - mse: 5.4559 - mae: 0.8587 - val_loss: 4.5800 - val_mse: 4.5800 - val_mae: 0.9787 - lr: 3.9525e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 6s - loss: 5.2421 - mse: 5.2421 - mae: 0.8316 - val_loss: 4.7051 - val_mse: 4.7051 - val_mae: 1.0194 - lr: 3.9525e-04 - 6s/epoch - 6ms/step\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-10-11 13:52:52,481]\u001b[0m Finished trial#46 resulted in value: 8.703999999999999. Current best value is 8.703999999999999 with parameters: {'activation': 'tanh', 'num_hidden_layer': 4, 'i': 9, 'learning_rate': 0.00039524792654887494}.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Score for fold 5: loss of 4.705109596252441\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 7s - loss: 14.9313 - mse: 14.9313 - mae: 1.6239 - val_loss: 12.1488 - val_mse: 12.1488 - val_mae: 1.5201 - lr: 3.9228e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 6s - loss: 14.1806 - mse: 14.1806 - mae: 1.5397 - val_loss: 11.8492 - val_mse: 11.8492 - val_mae: 1.5506 - lr: 3.9228e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 6s - loss: 14.0165 - mse: 14.0165 - mae: 1.5213 - val_loss: 11.6184 - val_mse: 11.6184 - val_mae: 1.5096 - lr: 3.9228e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 6s - loss: 13.8186 - mse: 13.8186 - mae: 1.5115 - val_loss: 11.5454 - val_mse: 11.5454 - val_mae: 1.4415 - lr: 3.9228e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 6s - loss: 13.7298 - mse: 13.7298 - mae: 1.5090 - val_loss: 11.5397 - val_mse: 11.5397 - val_mae: 1.4670 - lr: 3.9228e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 6s - loss: 13.5714 - mse: 13.5714 - mae: 1.4959 - val_loss: 11.2707 - val_mse: 11.2707 - val_mae: 1.5392 - lr: 3.9228e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 6s - loss: 13.5167 - mse: 13.5167 - mae: 1.4884 - val_loss: 11.3741 - val_mse: 11.3741 - val_mae: 1.4394 - lr: 3.9228e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 6s - loss: 13.3849 - mse: 13.3849 - mae: 1.4847 - val_loss: 11.0743 - val_mse: 11.0743 - val_mae: 1.4529 - lr: 3.9228e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 6s - loss: 13.2759 - mse: 13.2759 - mae: 1.4751 - val_loss: 11.0216 - val_mse: 11.0216 - val_mae: 1.4889 - lr: 3.9228e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 6s - loss: 13.1363 - mse: 13.1363 - mae: 1.4709 - val_loss: 11.1126 - val_mse: 11.1126 - val_mae: 1.4978 - lr: 3.9228e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 6s - loss: 12.9302 - mse: 12.9302 - mae: 1.4589 - val_loss: 11.2289 - val_mse: 11.2289 - val_mae: 1.4919 - lr: 3.9228e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 6s - loss: 12.8299 - mse: 12.8299 - mae: 1.4521 - val_loss: 11.1807 - val_mse: 11.1807 - val_mae: 1.5271 - lr: 3.9228e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 13/100\n",
            "1000/1000 - 6s - loss: 12.6180 - mse: 12.6180 - mae: 1.4429 - val_loss: 11.1048 - val_mse: 11.1048 - val_mae: 1.4949 - lr: 3.9228e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 14/100\n",
            "1000/1000 - 6s - loss: 12.5279 - mse: 12.5279 - mae: 1.4354 - val_loss: 11.2259 - val_mse: 11.2259 - val_mae: 1.4296 - lr: 3.9228e-04 - 6s/epoch - 6ms/step\n",
            "Score for fold 1: loss of 11.225885391235352\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 6s - loss: 13.2452 - mse: 13.2452 - mae: 1.4588 - val_loss: 7.9442 - val_mse: 7.9442 - val_mae: 1.3677 - lr: 3.9228e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 6s - loss: 13.0787 - mse: 13.0787 - mae: 1.4483 - val_loss: 8.0547 - val_mse: 8.0547 - val_mae: 1.4195 - lr: 3.9228e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 6s - loss: 12.7884 - mse: 12.7884 - mae: 1.4372 - val_loss: 8.1905 - val_mse: 8.1905 - val_mae: 1.4673 - lr: 3.9228e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 6s - loss: 12.6708 - mse: 12.6708 - mae: 1.4248 - val_loss: 8.0613 - val_mse: 8.0613 - val_mae: 1.4360 - lr: 3.9228e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 6s - loss: 12.4274 - mse: 12.4274 - mae: 1.4123 - val_loss: 8.0775 - val_mse: 8.0775 - val_mae: 1.3920 - lr: 3.9228e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 6s - loss: 12.3247 - mse: 12.3247 - mae: 1.3992 - val_loss: 8.1401 - val_mse: 8.1401 - val_mae: 1.4670 - lr: 3.9228e-04 - 6s/epoch - 6ms/step\n",
            "Score for fold 2: loss of 8.14013671875\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 6s - loss: 10.1043 - mse: 10.1043 - mae: 1.3991 - val_loss: 16.7865 - val_mse: 16.7865 - val_mae: 1.4120 - lr: 3.9228e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 6s - loss: 9.8962 - mse: 9.8962 - mae: 1.3869 - val_loss: 16.4478 - val_mse: 16.4478 - val_mae: 1.3987 - lr: 3.9228e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 6s - loss: 9.6595 - mse: 9.6595 - mae: 1.3618 - val_loss: 16.4838 - val_mse: 16.4838 - val_mae: 1.4645 - lr: 3.9228e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 6s - loss: 9.4225 - mse: 9.4225 - mae: 1.3482 - val_loss: 16.5396 - val_mse: 16.5396 - val_mae: 1.4714 - lr: 3.9228e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 6s - loss: 9.1861 - mse: 9.1861 - mae: 1.3329 - val_loss: 16.7239 - val_mse: 16.7239 - val_mae: 1.4507 - lr: 3.9228e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 6s - loss: 8.9651 - mse: 8.9651 - mae: 1.3167 - val_loss: 16.9611 - val_mse: 16.9611 - val_mae: 1.4701 - lr: 3.9228e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 6s - loss: 8.7027 - mse: 8.7027 - mae: 1.2974 - val_loss: 16.7011 - val_mse: 16.7011 - val_mae: 1.4539 - lr: 3.9228e-04 - 6s/epoch - 6ms/step\n",
            "Score for fold 3: loss of 16.70110511779785\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 6s - loss: 9.9149 - mse: 9.9149 - mae: 1.3379 - val_loss: 11.2922 - val_mse: 11.2922 - val_mae: 1.3092 - lr: 3.9228e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 6s - loss: 9.5914 - mse: 9.5914 - mae: 1.3081 - val_loss: 11.5570 - val_mse: 11.5570 - val_mae: 1.3184 - lr: 3.9228e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 6s - loss: 9.3432 - mse: 9.3432 - mae: 1.2901 - val_loss: 11.5895 - val_mse: 11.5895 - val_mae: 1.3144 - lr: 3.9228e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 6s - loss: 8.9770 - mse: 8.9770 - mae: 1.2648 - val_loss: 11.8015 - val_mse: 11.8015 - val_mae: 1.3615 - lr: 3.9228e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 6s - loss: 8.6885 - mse: 8.6885 - mae: 1.2398 - val_loss: 11.7240 - val_mse: 11.7240 - val_mae: 1.3912 - lr: 3.9228e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 6s - loss: 8.4422 - mse: 8.4422 - mae: 1.2199 - val_loss: 11.7022 - val_mse: 11.7022 - val_mae: 1.3932 - lr: 3.9228e-04 - 6s/epoch - 6ms/step\n",
            "Score for fold 4: loss of 11.70218563079834\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 6s - loss: 9.5727 - mse: 9.5727 - mae: 1.2613 - val_loss: 6.3066 - val_mse: 6.3066 - val_mae: 1.1828 - lr: 3.9228e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 6s - loss: 9.1845 - mse: 9.1845 - mae: 1.2291 - val_loss: 6.5399 - val_mse: 6.5399 - val_mae: 1.2190 - lr: 3.9228e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 6s - loss: 8.8750 - mse: 8.8750 - mae: 1.2009 - val_loss: 6.5050 - val_mse: 6.5050 - val_mae: 1.2226 - lr: 3.9228e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 6s - loss: 8.6834 - mse: 8.6834 - mae: 1.1777 - val_loss: 6.5210 - val_mse: 6.5210 - val_mae: 1.2547 - lr: 3.9228e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 6s - loss: 8.2599 - mse: 8.2599 - mae: 1.1490 - val_loss: 6.7513 - val_mse: 6.7513 - val_mae: 1.2715 - lr: 3.9228e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 6s - loss: 7.9216 - mse: 7.9216 - mae: 1.1210 - val_loss: 6.6062 - val_mse: 6.6062 - val_mae: 1.2748 - lr: 3.9228e-04 - 6s/epoch - 6ms/step\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-10-11 13:57:01,602]\u001b[0m Finished trial#47 resulted in value: 10.876. Current best value is 8.703999999999999 with parameters: {'activation': 'tanh', 'num_hidden_layer': 4, 'i': 9, 'learning_rate': 0.00039524792654887494}.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Score for fold 5: loss of 6.606222152709961\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 7s - loss: 15.0604 - mse: 15.0604 - mae: 1.6176 - val_loss: 11.7817 - val_mse: 11.7817 - val_mae: 1.5839 - lr: 5.2307e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 7s - loss: 14.5126 - mse: 14.5126 - mae: 1.5473 - val_loss: 11.3585 - val_mse: 11.3585 - val_mae: 1.5560 - lr: 5.2307e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 6s - loss: 14.2250 - mse: 14.2250 - mae: 1.5364 - val_loss: 11.6114 - val_mse: 11.6114 - val_mae: 1.5828 - lr: 5.2307e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 6s - loss: 13.9856 - mse: 13.9856 - mae: 1.5138 - val_loss: 11.2513 - val_mse: 11.2513 - val_mae: 1.5050 - lr: 5.2307e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 6s - loss: 13.9122 - mse: 13.9122 - mae: 1.5050 - val_loss: 11.1934 - val_mse: 11.1934 - val_mae: 1.4822 - lr: 5.2307e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 6s - loss: 13.8426 - mse: 13.8426 - mae: 1.5010 - val_loss: 10.9995 - val_mse: 10.9995 - val_mae: 1.4352 - lr: 5.2307e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 6s - loss: 13.6517 - mse: 13.6517 - mae: 1.4940 - val_loss: 11.0946 - val_mse: 11.0946 - val_mae: 1.4993 - lr: 5.2307e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 6s - loss: 13.6458 - mse: 13.6458 - mae: 1.4880 - val_loss: 11.1158 - val_mse: 11.1158 - val_mae: 1.4103 - lr: 5.2307e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 6s - loss: 13.4734 - mse: 13.4734 - mae: 1.4754 - val_loss: 10.7155 - val_mse: 10.7155 - val_mae: 1.5075 - lr: 5.2307e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 6s - loss: 13.4202 - mse: 13.4202 - mae: 1.4736 - val_loss: 10.9104 - val_mse: 10.9104 - val_mae: 1.5887 - lr: 5.2307e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 6s - loss: 13.3937 - mse: 13.3937 - mae: 1.4759 - val_loss: 11.1933 - val_mse: 11.1933 - val_mae: 1.4342 - lr: 5.2307e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 6s - loss: 13.2604 - mse: 13.2604 - mae: 1.4692 - val_loss: 10.7894 - val_mse: 10.7894 - val_mae: 1.4356 - lr: 5.2307e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 13/100\n",
            "1000/1000 - 6s - loss: 13.2141 - mse: 13.2141 - mae: 1.4647 - val_loss: 10.7081 - val_mse: 10.7081 - val_mae: 1.4487 - lr: 5.2307e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 14/100\n",
            "1000/1000 - 6s - loss: 13.1685 - mse: 13.1685 - mae: 1.4618 - val_loss: 10.5608 - val_mse: 10.5608 - val_mae: 1.5674 - lr: 5.2307e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 15/100\n",
            "1000/1000 - 6s - loss: 13.0559 - mse: 13.0559 - mae: 1.4556 - val_loss: 10.5323 - val_mse: 10.5323 - val_mae: 1.5503 - lr: 5.2307e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 16/100\n",
            "1000/1000 - 6s - loss: 12.9609 - mse: 12.9609 - mae: 1.4496 - val_loss: 10.5770 - val_mse: 10.5770 - val_mae: 1.5003 - lr: 5.2307e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 17/100\n",
            "1000/1000 - 6s - loss: 12.8477 - mse: 12.8477 - mae: 1.4465 - val_loss: 10.6260 - val_mse: 10.6260 - val_mae: 1.5013 - lr: 5.2307e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 18/100\n",
            "1000/1000 - 6s - loss: 12.7712 - mse: 12.7712 - mae: 1.4409 - val_loss: 10.6090 - val_mse: 10.6090 - val_mae: 1.5068 - lr: 5.2307e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 19/100\n",
            "1000/1000 - 6s - loss: 12.6396 - mse: 12.6396 - mae: 1.4329 - val_loss: 10.3662 - val_mse: 10.3662 - val_mae: 1.4437 - lr: 5.2307e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 20/100\n",
            "1000/1000 - 6s - loss: 12.5720 - mse: 12.5720 - mae: 1.4304 - val_loss: 10.7952 - val_mse: 10.7952 - val_mae: 1.6004 - lr: 5.2307e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 21/100\n",
            "1000/1000 - 6s - loss: 12.3942 - mse: 12.3942 - mae: 1.4277 - val_loss: 10.4486 - val_mse: 10.4486 - val_mae: 1.5724 - lr: 5.2307e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 22/100\n",
            "1000/1000 - 6s - loss: 12.2743 - mse: 12.2743 - mae: 1.4194 - val_loss: 10.6305 - val_mse: 10.6305 - val_mae: 1.4570 - lr: 5.2307e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 23/100\n",
            "1000/1000 - 6s - loss: 12.0444 - mse: 12.0444 - mae: 1.4026 - val_loss: 10.4901 - val_mse: 10.4901 - val_mae: 1.5441 - lr: 5.2307e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 24/100\n",
            "1000/1000 - 6s - loss: 11.8852 - mse: 11.8852 - mae: 1.3912 - val_loss: 10.8506 - val_mse: 10.8506 - val_mae: 1.4446 - lr: 5.2307e-04 - 6s/epoch - 6ms/step\n",
            "Score for fold 1: loss of 10.85057258605957\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 6s - loss: 12.2125 - mse: 12.2125 - mae: 1.4256 - val_loss: 9.1483 - val_mse: 9.1483 - val_mae: 1.4107 - lr: 5.2307e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 6s - loss: 11.9859 - mse: 11.9859 - mae: 1.4115 - val_loss: 8.9946 - val_mse: 8.9946 - val_mae: 1.3786 - lr: 5.2307e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 6s - loss: 11.7276 - mse: 11.7276 - mae: 1.4071 - val_loss: 9.1879 - val_mse: 9.1879 - val_mae: 1.3348 - lr: 5.2307e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 6s - loss: 11.6742 - mse: 11.6742 - mae: 1.3898 - val_loss: 9.2367 - val_mse: 9.2367 - val_mae: 1.4008 - lr: 5.2307e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 6s - loss: 11.4822 - mse: 11.4822 - mae: 1.3803 - val_loss: 9.1177 - val_mse: 9.1177 - val_mae: 1.3949 - lr: 5.2307e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 6s - loss: 11.1554 - mse: 11.1554 - mae: 1.3584 - val_loss: 9.0940 - val_mse: 9.0940 - val_mae: 1.3682 - lr: 5.2307e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 6s - loss: 10.8769 - mse: 10.8769 - mae: 1.3430 - val_loss: 8.9107 - val_mse: 8.9107 - val_mae: 1.3621 - lr: 5.2307e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 6s - loss: 10.6090 - mse: 10.6090 - mae: 1.3260 - val_loss: 8.9751 - val_mse: 8.9751 - val_mae: 1.3940 - lr: 5.2307e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 6s - loss: 10.3196 - mse: 10.3196 - mae: 1.3083 - val_loss: 8.9265 - val_mse: 8.9265 - val_mae: 1.4603 - lr: 5.2307e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 7s - loss: 10.0849 - mse: 10.0849 - mae: 1.2950 - val_loss: 9.0502 - val_mse: 9.0502 - val_mae: 1.4243 - lr: 5.2307e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 7s - loss: 9.8164 - mse: 9.8164 - mae: 1.2786 - val_loss: 9.3607 - val_mse: 9.3607 - val_mae: 1.6525 - lr: 5.2307e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 7s - loss: 9.6551 - mse: 9.6551 - mae: 1.2639 - val_loss: 9.2072 - val_mse: 9.2072 - val_mae: 1.3941 - lr: 5.2307e-04 - 7s/epoch - 7ms/step\n",
            "Score for fold 2: loss of 9.207216262817383\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 6s - loss: 8.4384 - mse: 8.4384 - mae: 1.3228 - val_loss: 13.7752 - val_mse: 13.7752 - val_mae: 1.2573 - lr: 5.2307e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 6s - loss: 8.1848 - mse: 8.1848 - mae: 1.2971 - val_loss: 14.0182 - val_mse: 14.0182 - val_mae: 1.3942 - lr: 5.2307e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 6s - loss: 7.9800 - mse: 7.9800 - mae: 1.2734 - val_loss: 13.9533 - val_mse: 13.9533 - val_mae: 1.2854 - lr: 5.2307e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 6s - loss: 7.7056 - mse: 7.7056 - mae: 1.2510 - val_loss: 14.2153 - val_mse: 14.2153 - val_mae: 1.4228 - lr: 5.2307e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 6s - loss: 7.3070 - mse: 7.3070 - mae: 1.2292 - val_loss: 14.3539 - val_mse: 14.3539 - val_mae: 1.3352 - lr: 5.2307e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 6s - loss: 7.0201 - mse: 7.0201 - mae: 1.2066 - val_loss: 14.3716 - val_mse: 14.3716 - val_mae: 1.4674 - lr: 5.2307e-04 - 6s/epoch - 6ms/step\n",
            "Score for fold 3: loss of 14.371634483337402\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 6s - loss: 8.4733 - mse: 8.4733 - mae: 1.2394 - val_loss: 9.0420 - val_mse: 9.0420 - val_mae: 1.2521 - lr: 5.2307e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 6s - loss: 8.1479 - mse: 8.1479 - mae: 1.2054 - val_loss: 8.4185 - val_mse: 8.4185 - val_mae: 1.1973 - lr: 5.2307e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 6s - loss: 7.8951 - mse: 7.8951 - mae: 1.1816 - val_loss: 8.7364 - val_mse: 8.7364 - val_mae: 1.2493 - lr: 5.2307e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 6s - loss: 7.6007 - mse: 7.6007 - mae: 1.1602 - val_loss: 9.0360 - val_mse: 9.0360 - val_mae: 1.2824 - lr: 5.2307e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 6s - loss: 7.3962 - mse: 7.3962 - mae: 1.1366 - val_loss: 9.2454 - val_mse: 9.2454 - val_mae: 1.2849 - lr: 5.2307e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 6s - loss: 7.2264 - mse: 7.2264 - mae: 1.1181 - val_loss: 8.8804 - val_mse: 8.8804 - val_mae: 1.2666 - lr: 5.2307e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 7s - loss: 6.9865 - mse: 6.9865 - mae: 1.1022 - val_loss: 8.8981 - val_mse: 8.8981 - val_mae: 1.3016 - lr: 5.2307e-04 - 7s/epoch - 7ms/step\n",
            "Score for fold 4: loss of 8.89805793762207\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 7s - loss: 7.9542 - mse: 7.9542 - mae: 1.1690 - val_loss: 6.1397 - val_mse: 6.1397 - val_mae: 1.0634 - lr: 5.2307e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 6s - loss: 7.4354 - mse: 7.4354 - mae: 1.1344 - val_loss: 6.4563 - val_mse: 6.4563 - val_mae: 1.1308 - lr: 5.2307e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 6s - loss: 7.2196 - mse: 7.2196 - mae: 1.1092 - val_loss: 6.6739 - val_mse: 6.6739 - val_mae: 1.1265 - lr: 5.2307e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 6s - loss: 7.0215 - mse: 7.0215 - mae: 1.0904 - val_loss: 6.5587 - val_mse: 6.5587 - val_mae: 1.1336 - lr: 5.2307e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 6s - loss: 6.7742 - mse: 6.7742 - mae: 1.0716 - val_loss: 6.6225 - val_mse: 6.6225 - val_mae: 1.1696 - lr: 5.2307e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 6s - loss: 6.5291 - mse: 6.5291 - mae: 1.0472 - val_loss: 6.5826 - val_mse: 6.5826 - val_mae: 1.2173 - lr: 5.2307e-04 - 6s/epoch - 6ms/step\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-10-11 14:02:55,870]\u001b[0m Finished trial#48 resulted in value: 9.982. Current best value is 8.703999999999999 with parameters: {'activation': 'tanh', 'num_hidden_layer': 4, 'i': 9, 'learning_rate': 0.00039524792654887494}.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Score for fold 5: loss of 6.582625389099121\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 4s - loss: 14.8926 - mse: 14.8926 - mae: 1.6372 - val_loss: 12.5265 - val_mse: 12.5265 - val_mae: 1.5715 - lr: 0.0011 - 4s/epoch - 4ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 3s - loss: 14.0893 - mse: 14.0893 - mae: 1.5600 - val_loss: 12.2318 - val_mse: 12.2318 - val_mae: 1.4674 - lr: 0.0011 - 3s/epoch - 3ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 3s - loss: 13.8610 - mse: 13.8610 - mae: 1.5379 - val_loss: 12.1583 - val_mse: 12.1583 - val_mae: 1.4718 - lr: 0.0011 - 3s/epoch - 3ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 4s - loss: 13.6371 - mse: 13.6371 - mae: 1.5271 - val_loss: 11.9445 - val_mse: 11.9445 - val_mae: 1.4415 - lr: 0.0011 - 4s/epoch - 4ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 4s - loss: 13.4560 - mse: 13.4560 - mae: 1.5125 - val_loss: 12.0006 - val_mse: 12.0006 - val_mae: 1.5501 - lr: 0.0011 - 4s/epoch - 4ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 4s - loss: 13.3845 - mse: 13.3845 - mae: 1.5120 - val_loss: 12.1717 - val_mse: 12.1717 - val_mae: 1.5250 - lr: 0.0011 - 4s/epoch - 4ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 4s - loss: 13.3772 - mse: 13.3772 - mae: 1.5045 - val_loss: 12.1361 - val_mse: 12.1361 - val_mae: 1.5073 - lr: 0.0011 - 4s/epoch - 4ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 4s - loss: 13.2471 - mse: 13.2471 - mae: 1.4974 - val_loss: 11.6872 - val_mse: 11.6872 - val_mae: 1.3805 - lr: 0.0011 - 4s/epoch - 4ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 4s - loss: 13.1455 - mse: 13.1455 - mae: 1.4941 - val_loss: 11.7095 - val_mse: 11.7095 - val_mae: 1.3959 - lr: 0.0011 - 4s/epoch - 4ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 4s - loss: 13.0704 - mse: 13.0704 - mae: 1.4954 - val_loss: 12.2098 - val_mse: 12.2098 - val_mae: 1.4101 - lr: 0.0011 - 4s/epoch - 4ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 4s - loss: 12.9662 - mse: 12.9662 - mae: 1.4918 - val_loss: 11.6892 - val_mse: 11.6892 - val_mae: 1.3623 - lr: 0.0011 - 4s/epoch - 4ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 4s - loss: 12.7541 - mse: 12.7541 - mae: 1.4802 - val_loss: 12.0157 - val_mse: 12.0157 - val_mae: 1.4129 - lr: 0.0011 - 4s/epoch - 4ms/step\n",
            "Epoch 13/100\n",
            "1000/1000 - 4s - loss: 12.7284 - mse: 12.7284 - mae: 1.4744 - val_loss: 11.4977 - val_mse: 11.4977 - val_mae: 1.4946 - lr: 0.0011 - 4s/epoch - 4ms/step\n",
            "Epoch 14/100\n",
            "1000/1000 - 4s - loss: 12.7135 - mse: 12.7135 - mae: 1.4752 - val_loss: 11.3785 - val_mse: 11.3785 - val_mae: 1.4275 - lr: 0.0011 - 4s/epoch - 4ms/step\n",
            "Epoch 15/100\n",
            "1000/1000 - 4s - loss: 12.7138 - mse: 12.7138 - mae: 1.4656 - val_loss: 11.3447 - val_mse: 11.3447 - val_mae: 1.4042 - lr: 0.0011 - 4s/epoch - 4ms/step\n",
            "Epoch 16/100\n",
            "1000/1000 - 3s - loss: 12.4865 - mse: 12.4865 - mae: 1.4638 - val_loss: 11.3672 - val_mse: 11.3672 - val_mae: 1.4345 - lr: 0.0011 - 3s/epoch - 3ms/step\n",
            "Epoch 17/100\n",
            "1000/1000 - 4s - loss: 12.4510 - mse: 12.4510 - mae: 1.4629 - val_loss: 11.6762 - val_mse: 11.6762 - val_mae: 1.3953 - lr: 0.0011 - 4s/epoch - 4ms/step\n",
            "Epoch 18/100\n",
            "1000/1000 - 3s - loss: 12.4937 - mse: 12.4937 - mae: 1.4579 - val_loss: 11.6287 - val_mse: 11.6287 - val_mae: 1.4024 - lr: 0.0011 - 3s/epoch - 3ms/step\n",
            "Epoch 19/100\n",
            "1000/1000 - 3s - loss: 12.2694 - mse: 12.2694 - mae: 1.4548 - val_loss: 11.6280 - val_mse: 11.6280 - val_mae: 1.6246 - lr: 0.0011 - 3s/epoch - 3ms/step\n",
            "Epoch 20/100\n",
            "1000/1000 - 4s - loss: 12.1172 - mse: 12.1172 - mae: 1.4508 - val_loss: 11.3436 - val_mse: 11.3436 - val_mae: 1.3532 - lr: 0.0011 - 4s/epoch - 4ms/step\n",
            "Epoch 21/100\n",
            "1000/1000 - 3s - loss: 12.0128 - mse: 12.0128 - mae: 1.4429 - val_loss: 11.3229 - val_mse: 11.3229 - val_mae: 1.4478 - lr: 0.0011 - 3s/epoch - 3ms/step\n",
            "Epoch 22/100\n",
            "1000/1000 - 4s - loss: 11.8639 - mse: 11.8639 - mae: 1.4392 - val_loss: 11.6787 - val_mse: 11.6787 - val_mae: 1.5189 - lr: 0.0011 - 4s/epoch - 4ms/step\n",
            "Epoch 23/100\n",
            "1000/1000 - 3s - loss: 11.7129 - mse: 11.7129 - mae: 1.4351 - val_loss: 11.3783 - val_mse: 11.3783 - val_mae: 1.4505 - lr: 0.0011 - 3s/epoch - 3ms/step\n",
            "Epoch 24/100\n",
            "1000/1000 - 3s - loss: 11.6951 - mse: 11.6951 - mae: 1.4320 - val_loss: 10.9092 - val_mse: 10.9092 - val_mae: 1.3957 - lr: 0.0011 - 3s/epoch - 3ms/step\n",
            "Epoch 25/100\n",
            "1000/1000 - 4s - loss: 11.6097 - mse: 11.6097 - mae: 1.4372 - val_loss: 11.4184 - val_mse: 11.4184 - val_mae: 1.4261 - lr: 0.0011 - 4s/epoch - 4ms/step\n",
            "Epoch 26/100\n",
            "1000/1000 - 3s - loss: 11.6477 - mse: 11.6477 - mae: 1.4267 - val_loss: 11.4577 - val_mse: 11.4577 - val_mae: 1.4616 - lr: 0.0011 - 3s/epoch - 3ms/step\n",
            "Epoch 27/100\n",
            "1000/1000 - 4s - loss: 11.4407 - mse: 11.4407 - mae: 1.4200 - val_loss: 11.4477 - val_mse: 11.4477 - val_mae: 1.4882 - lr: 0.0011 - 4s/epoch - 4ms/step\n",
            "Epoch 28/100\n",
            "1000/1000 - 4s - loss: 11.4584 - mse: 11.4584 - mae: 1.4141 - val_loss: 11.1549 - val_mse: 11.1549 - val_mae: 1.4471 - lr: 0.0011 - 4s/epoch - 4ms/step\n",
            "Epoch 29/100\n",
            "1000/1000 - 3s - loss: 11.1868 - mse: 11.1868 - mae: 1.4071 - val_loss: 11.3837 - val_mse: 11.3837 - val_mae: 1.3804 - lr: 0.0011 - 3s/epoch - 3ms/step\n",
            "Score for fold 1: loss of 11.3837251663208\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 4s - loss: 11.4309 - mse: 11.4309 - mae: 1.4195 - val_loss: 11.3037 - val_mse: 11.3037 - val_mae: 1.4271 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 4s - loss: 11.3056 - mse: 11.3056 - mae: 1.4005 - val_loss: 10.2712 - val_mse: 10.2712 - val_mae: 1.4106 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 3s - loss: 11.2306 - mse: 11.2306 - mae: 1.4004 - val_loss: 10.3717 - val_mse: 10.3717 - val_mae: 1.5197 - lr: 0.0010 - 3s/epoch - 3ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 3s - loss: 10.9606 - mse: 10.9606 - mae: 1.3891 - val_loss: 11.5292 - val_mse: 11.5292 - val_mae: 1.5055 - lr: 0.0010 - 3s/epoch - 3ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 4s - loss: 10.9809 - mse: 10.9809 - mae: 1.3871 - val_loss: 11.3161 - val_mse: 11.3161 - val_mae: 1.3679 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 3s - loss: 10.7374 - mse: 10.7374 - mae: 1.3814 - val_loss: 10.9193 - val_mse: 10.9193 - val_mae: 1.5414 - lr: 0.0010 - 3s/epoch - 3ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 4s - loss: 10.6185 - mse: 10.6185 - mae: 1.3704 - val_loss: 11.1521 - val_mse: 11.1521 - val_mae: 1.4212 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Score for fold 2: loss of 11.152131080627441\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 4s - loss: 11.7271 - mse: 11.7271 - mae: 1.3954 - val_loss: 7.0718 - val_mse: 7.0718 - val_mae: 1.4976 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 3s - loss: 11.4839 - mse: 11.4839 - mae: 1.3937 - val_loss: 6.9851 - val_mse: 6.9851 - val_mae: 1.3870 - lr: 0.0010 - 3s/epoch - 3ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 3s - loss: 11.2867 - mse: 11.2867 - mae: 1.3751 - val_loss: 7.2128 - val_mse: 7.2128 - val_mae: 1.3950 - lr: 0.0010 - 3s/epoch - 3ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 3s - loss: 11.0442 - mse: 11.0442 - mae: 1.3704 - val_loss: 7.3278 - val_mse: 7.3278 - val_mae: 1.4703 - lr: 0.0010 - 3s/epoch - 3ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 3s - loss: 10.9025 - mse: 10.9025 - mae: 1.3670 - val_loss: 7.2285 - val_mse: 7.2285 - val_mae: 1.4151 - lr: 0.0010 - 3s/epoch - 3ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 3s - loss: 10.9775 - mse: 10.9775 - mae: 1.3598 - val_loss: 7.3891 - val_mse: 7.3891 - val_mae: 1.4334 - lr: 0.0010 - 3s/epoch - 3ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 3s - loss: 10.9677 - mse: 10.9677 - mae: 1.3515 - val_loss: 7.6184 - val_mse: 7.6184 - val_mae: 1.3872 - lr: 0.0010 - 3s/epoch - 3ms/step\n",
            "Score for fold 3: loss of 7.618447780609131\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 4s - loss: 10.8854 - mse: 10.8854 - mae: 1.3799 - val_loss: 6.7274 - val_mse: 6.7274 - val_mae: 1.4261 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 4s - loss: 10.8250 - mse: 10.8250 - mae: 1.3721 - val_loss: 6.8193 - val_mse: 6.8193 - val_mae: 1.3472 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 4s - loss: 10.5959 - mse: 10.5959 - mae: 1.3553 - val_loss: 6.6275 - val_mse: 6.6275 - val_mae: 1.3333 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 3s - loss: 10.4746 - mse: 10.4746 - mae: 1.3435 - val_loss: 6.8888 - val_mse: 6.8888 - val_mae: 1.3808 - lr: 0.0010 - 3s/epoch - 3ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 3s - loss: 10.2487 - mse: 10.2487 - mae: 1.3392 - val_loss: 6.8727 - val_mse: 6.8727 - val_mae: 1.3287 - lr: 0.0010 - 3s/epoch - 3ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 3s - loss: 10.0740 - mse: 10.0740 - mae: 1.3264 - val_loss: 6.7624 - val_mse: 6.7624 - val_mae: 1.3388 - lr: 0.0010 - 3s/epoch - 3ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 3s - loss: 10.1753 - mse: 10.1753 - mae: 1.3279 - val_loss: 6.8887 - val_mse: 6.8887 - val_mae: 1.2958 - lr: 0.0010 - 3s/epoch - 3ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 4s - loss: 9.8631 - mse: 9.8631 - mae: 1.3185 - val_loss: 7.2448 - val_mse: 7.2448 - val_mae: 1.2590 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Score for fold 4: loss of 7.24476432800293\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 3s - loss: 7.4648 - mse: 7.4648 - mae: 1.3334 - val_loss: 17.3222 - val_mse: 17.3222 - val_mae: 1.2257 - lr: 0.0010 - 3s/epoch - 3ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 4s - loss: 7.3403 - mse: 7.3403 - mae: 1.3176 - val_loss: 17.4603 - val_mse: 17.4603 - val_mae: 1.3032 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 3s - loss: 7.0390 - mse: 7.0390 - mae: 1.3144 - val_loss: 17.6221 - val_mse: 17.6221 - val_mae: 1.4446 - lr: 0.0010 - 3s/epoch - 3ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 3s - loss: 7.0103 - mse: 7.0103 - mae: 1.3021 - val_loss: 17.7861 - val_mse: 17.7861 - val_mae: 1.3413 - lr: 0.0010 - 3s/epoch - 3ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 4s - loss: 6.8221 - mse: 6.8221 - mae: 1.2989 - val_loss: 17.7159 - val_mse: 17.7159 - val_mae: 1.3475 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 3s - loss: 6.5776 - mse: 6.5776 - mae: 1.2786 - val_loss: 18.2158 - val_mse: 18.2158 - val_mae: 1.3459 - lr: 0.0010 - 3s/epoch - 3ms/step\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-10-11 14:06:21,073]\u001b[0m Finished trial#49 resulted in value: 11.122. Current best value is 8.703999999999999 with parameters: {'activation': 'tanh', 'num_hidden_layer': 4, 'i': 9, 'learning_rate': 0.00039524792654887494}.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Score for fold 5: loss of 18.215736389160156\n"
          ]
        }
      ],
      "source": [
        "model_list=[]\n",
        "history_list=[]\n",
        "study_name = 'NN_study'\n",
        "study = optuna.create_study(study_name=study_name, load_if_exists=True)\n",
        "func = lambda trial: objective(trial,training,labelsForTrain)\n",
        "study.optimize(func, n_trials=50,)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wvCnW6PlWjWe",
        "outputId": "3ef0e87a-cf67-4feb-9f03-a1285fd058a7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/500\n",
            "1250/1250 - 5s - loss: 14.3372 - mse: 14.3372 - mae: 1.5932 - val_loss: 11.5211 - val_mse: 11.5211 - val_mae: 1.5159 - lr: 3.9525e-04 - 5s/epoch - 4ms/step\n",
            "Epoch 2/500\n",
            "1250/1250 - 5s - loss: 13.7282 - mse: 13.7282 - mae: 1.5330 - val_loss: 11.1308 - val_mse: 11.1308 - val_mae: 1.5518 - lr: 3.9525e-04 - 5s/epoch - 4ms/step\n",
            "Epoch 3/500\n",
            "1250/1250 - 5s - loss: 13.4526 - mse: 13.4526 - mae: 1.5174 - val_loss: 10.9339 - val_mse: 10.9339 - val_mae: 1.5311 - lr: 3.9525e-04 - 5s/epoch - 4ms/step\n",
            "Epoch 4/500\n",
            "1250/1250 - 5s - loss: 13.2481 - mse: 13.2481 - mae: 1.5062 - val_loss: 11.0287 - val_mse: 11.0287 - val_mae: 1.5413 - lr: 3.9525e-04 - 5s/epoch - 4ms/step\n",
            "Epoch 5/500\n",
            "1250/1250 - 5s - loss: 13.1270 - mse: 13.1270 - mae: 1.4926 - val_loss: 10.6070 - val_mse: 10.6070 - val_mae: 1.5231 - lr: 3.9525e-04 - 5s/epoch - 4ms/step\n",
            "Epoch 6/500\n",
            "1250/1250 - 5s - loss: 12.9719 - mse: 12.9719 - mae: 1.4874 - val_loss: 10.4359 - val_mse: 10.4359 - val_mae: 1.4617 - lr: 3.9525e-04 - 5s/epoch - 4ms/step\n",
            "Epoch 7/500\n",
            "1250/1250 - 5s - loss: 12.8813 - mse: 12.8813 - mae: 1.4835 - val_loss: 10.2792 - val_mse: 10.2792 - val_mae: 1.5219 - lr: 3.9525e-04 - 5s/epoch - 4ms/step\n",
            "Epoch 8/500\n",
            "1250/1250 - 5s - loss: 12.8119 - mse: 12.8119 - mae: 1.4752 - val_loss: 10.2753 - val_mse: 10.2753 - val_mae: 1.4735 - lr: 3.9525e-04 - 5s/epoch - 4ms/step\n",
            "Epoch 9/500\n",
            "1250/1250 - 5s - loss: 12.7019 - mse: 12.7019 - mae: 1.4663 - val_loss: 10.6516 - val_mse: 10.6516 - val_mae: 1.4812 - lr: 3.9525e-04 - 5s/epoch - 4ms/step\n",
            "Epoch 10/500\n",
            "1250/1250 - 5s - loss: 12.5254 - mse: 12.5254 - mae: 1.4598 - val_loss: 10.3109 - val_mse: 10.3109 - val_mae: 1.5618 - lr: 3.9525e-04 - 5s/epoch - 4ms/step\n",
            "Epoch 11/500\n",
            "1250/1250 - 5s - loss: 12.4881 - mse: 12.4881 - mae: 1.4571 - val_loss: 10.2221 - val_mse: 10.2221 - val_mae: 1.4941 - lr: 3.9525e-04 - 5s/epoch - 4ms/step\n",
            "Epoch 12/500\n",
            "1250/1250 - 5s - loss: 12.3823 - mse: 12.3823 - mae: 1.4541 - val_loss: 10.2307 - val_mse: 10.2307 - val_mae: 1.4948 - lr: 3.9525e-04 - 5s/epoch - 4ms/step\n",
            "Epoch 13/500\n",
            "1250/1250 - 5s - loss: 12.2409 - mse: 12.2409 - mae: 1.4434 - val_loss: 10.0077 - val_mse: 10.0077 - val_mae: 1.4925 - lr: 3.9525e-04 - 5s/epoch - 4ms/step\n",
            "Epoch 14/500\n",
            "1250/1250 - 5s - loss: 12.0934 - mse: 12.0934 - mae: 1.4398 - val_loss: 10.0562 - val_mse: 10.0562 - val_mae: 1.5683 - lr: 3.9525e-04 - 5s/epoch - 4ms/step\n",
            "Epoch 15/500\n",
            "1250/1250 - 5s - loss: 12.0078 - mse: 12.0078 - mae: 1.4291 - val_loss: 9.9556 - val_mse: 9.9556 - val_mae: 1.5154 - lr: 3.9525e-04 - 5s/epoch - 4ms/step\n",
            "Epoch 16/500\n",
            "1250/1250 - 5s - loss: 11.8180 - mse: 11.8180 - mae: 1.4234 - val_loss: 10.4111 - val_mse: 10.4111 - val_mae: 1.5588 - lr: 3.9525e-04 - 5s/epoch - 4ms/step\n",
            "Epoch 17/500\n",
            "1250/1250 - 5s - loss: 11.7187 - mse: 11.7187 - mae: 1.4120 - val_loss: 10.2383 - val_mse: 10.2383 - val_mae: 1.5345 - lr: 3.9525e-04 - 5s/epoch - 4ms/step\n",
            "Epoch 18/500\n",
            "1250/1250 - 5s - loss: 11.6150 - mse: 11.6150 - mae: 1.4102 - val_loss: 9.9554 - val_mse: 9.9554 - val_mae: 1.5407 - lr: 3.9525e-04 - 5s/epoch - 4ms/step\n",
            "Epoch 19/500\n",
            "1250/1250 - 5s - loss: 11.4659 - mse: 11.4659 - mae: 1.3958 - val_loss: 10.2037 - val_mse: 10.2037 - val_mae: 1.6413 - lr: 3.9525e-04 - 5s/epoch - 4ms/step\n",
            "Epoch 20/500\n",
            "1250/1250 - 5s - loss: 11.1734 - mse: 11.1734 - mae: 1.3853 - val_loss: 9.7386 - val_mse: 9.7386 - val_mae: 1.5644 - lr: 3.9525e-04 - 5s/epoch - 4ms/step\n",
            "Epoch 21/500\n",
            "1250/1250 - 5s - loss: 10.9835 - mse: 10.9835 - mae: 1.3784 - val_loss: 10.1302 - val_mse: 10.1302 - val_mae: 1.5572 - lr: 3.9525e-04 - 5s/epoch - 4ms/step\n",
            "Epoch 22/500\n",
            "1250/1250 - 5s - loss: 10.7720 - mse: 10.7720 - mae: 1.3632 - val_loss: 10.1869 - val_mse: 10.1869 - val_mae: 1.5805 - lr: 3.9525e-04 - 5s/epoch - 4ms/step\n",
            "Epoch 23/500\n",
            "1250/1250 - 5s - loss: 10.5634 - mse: 10.5634 - mae: 1.3459 - val_loss: 10.0582 - val_mse: 10.0582 - val_mae: 1.5095 - lr: 3.9525e-04 - 5s/epoch - 4ms/step\n",
            "Epoch 24/500\n",
            "1250/1250 - 5s - loss: 10.3126 - mse: 10.3126 - mae: 1.3329 - val_loss: 9.8739 - val_mse: 9.8739 - val_mae: 1.5629 - lr: 3.9525e-04 - 5s/epoch - 4ms/step\n",
            "Epoch 25/500\n",
            "1250/1250 - 5s - loss: 10.0701 - mse: 10.0701 - mae: 1.3214 - val_loss: 10.2424 - val_mse: 10.2424 - val_mae: 1.6736 - lr: 3.9525e-04 - 5s/epoch - 4ms/step\n",
            "Epoch 26/500\n",
            "1250/1250 - 5s - loss: 9.8165 - mse: 9.8165 - mae: 1.3049 - val_loss: 9.3684 - val_mse: 9.3684 - val_mae: 1.6172 - lr: 3.9525e-04 - 5s/epoch - 4ms/step\n",
            "Epoch 27/500\n",
            "1250/1250 - 5s - loss: 9.5774 - mse: 9.5774 - mae: 1.2901 - val_loss: 10.0057 - val_mse: 10.0057 - val_mae: 1.6086 - lr: 3.9525e-04 - 5s/epoch - 4ms/step\n",
            "Epoch 28/500\n",
            "1250/1250 - 5s - loss: 9.3546 - mse: 9.3546 - mae: 1.2693 - val_loss: 9.6129 - val_mse: 9.6129 - val_mae: 1.6276 - lr: 3.9525e-04 - 5s/epoch - 4ms/step\n",
            "Epoch 29/500\n",
            "1250/1250 - 5s - loss: 9.1331 - mse: 9.1331 - mae: 1.2521 - val_loss: 9.7692 - val_mse: 9.7692 - val_mae: 1.6398 - lr: 3.9525e-04 - 5s/epoch - 4ms/step\n",
            "Epoch 30/500\n",
            "1250/1250 - 5s - loss: 8.7604 - mse: 8.7604 - mae: 1.2362 - val_loss: 9.6143 - val_mse: 9.6143 - val_mae: 1.6531 - lr: 3.9525e-04 - 5s/epoch - 4ms/step\n",
            "Epoch 31/500\n",
            "1250/1250 - 5s - loss: 8.5462 - mse: 8.5462 - mae: 1.2146 - val_loss: 10.1700 - val_mse: 10.1700 - val_mae: 1.6240 - lr: 3.9525e-04 - 5s/epoch - 4ms/step\n",
            "Epoch 32/500\n",
            "1250/1250 - 5s - loss: 8.2615 - mse: 8.2615 - mae: 1.1950 - val_loss: 9.8916 - val_mse: 9.8916 - val_mae: 1.6551 - lr: 3.9525e-04 - 5s/epoch - 4ms/step\n",
            "Epoch 33/500\n",
            "1250/1250 - 5s - loss: 8.0728 - mse: 8.0728 - mae: 1.1808 - val_loss: 10.4572 - val_mse: 10.4572 - val_mae: 1.6878 - lr: 3.9525e-04 - 5s/epoch - 4ms/step\n",
            "Epoch 34/500\n",
            "1250/1250 - 5s - loss: 7.7817 - mse: 7.7817 - mae: 1.1537 - val_loss: 9.9142 - val_mse: 9.9142 - val_mae: 1.7039 - lr: 3.9525e-04 - 5s/epoch - 4ms/step\n",
            "Epoch 35/500\n",
            "1250/1250 - 5s - loss: 7.5062 - mse: 7.5062 - mae: 1.1360 - val_loss: 9.6951 - val_mse: 9.6951 - val_mae: 1.7198 - lr: 3.9525e-04 - 5s/epoch - 4ms/step\n",
            "Epoch 36/500\n",
            "1250/1250 - 5s - loss: 7.2174 - mse: 7.2174 - mae: 1.1123 - val_loss: 9.7858 - val_mse: 9.7858 - val_mae: 1.7601 - lr: 3.9525e-04 - 5s/epoch - 4ms/step\n"
          ]
        }
      ],
      "source": [
        "#{'activation': 'tanh', 'num_hidden_layer': 4, 'i': 9, 'learning_rate': 0.00039524792654887494}.\n",
        "optimizer = Adam(learning_rate=0.000395247926548874942 ,clipnorm=1.0)\n",
        "model_1 = create_model(activation=\"tanh\",num_hidden_layer=4,num_hidden_unit=512)\n",
        "\n",
        "es = EarlyStopping(monitor='val_mse', patience=10)\n",
        "reduce_lr = ReduceLROnPlateau(monitor='val_mse', factor=0.2,\n",
        "                              patience=20, min_lr=0.001)\n",
        "model_1.compile(loss='mse',optimizer=optimizer,metrics=['mse','mae'])\n",
        "history = model_1.fit(training,labelsForTrain,\n",
        "                batch_size=64,\n",
        "                epochs=500,\n",
        "                verbose=2,\n",
        "                validation_data=(valing,labelsForVal),\n",
        "                validation_batch_size=64,\n",
        "                callbacks=[es,reduce_lr])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E7tRpJTHX7CY",
        "outputId": "379568cd-0530-455f-d8b6-d3e4691bb37b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "313/313 [==============================] - 1s 2ms/step - loss: 11.0657 - mse: 11.0657 - mae: 1.7606\n"
          ]
        }
      ],
      "source": [
        "results_model1 = model_1.evaluate(testing, labelsForTest, batch_size=32)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "amvicsmJGgjt"
      },
      "source": [
        "## Shuffle & Repetation 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 488
        },
        "id": "VzDLbL_7BIuK",
        "outputId": "d26e0bd4-40ba-4d3c-bdbe-8967f44ca2e0"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-faa2e79b-7b3f-43fb-83ba-b45690282a21\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>r_id</th>\n",
              "      <th>r_stars</th>\n",
              "      <th>r_stars_square</th>\n",
              "      <th>r_length</th>\n",
              "      <th>u_friends_count</th>\n",
              "      <th>u_review_count</th>\n",
              "      <th>u_month_age</th>\n",
              "      <th>u_comp_avg</th>\n",
              "      <th>u_n_elite_yrs</th>\n",
              "      <th>u_fans</th>\n",
              "      <th>...</th>\n",
              "      <th>r_stopwords/words</th>\n",
              "      <th>r_digit_cnt</th>\n",
              "      <th>r_noun_cnt</th>\n",
              "      <th>r_Adj_cnt</th>\n",
              "      <th>r_Adv_cnt</th>\n",
              "      <th>r_capital_word_cnt</th>\n",
              "      <th>r_quoted_word_cnt</th>\n",
              "      <th>r_hashtag_cnt</th>\n",
              "      <th>r_exclam_cnt</th>\n",
              "      <th>r_useful</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>7126</th>\n",
              "      <td>5530464</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>127</td>\n",
              "      <td>61</td>\n",
              "      <td>1</td>\n",
              "      <td>25.718844</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.43</td>\n",
              "      <td>3</td>\n",
              "      <td>25</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>7</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>46624</th>\n",
              "      <td>879574</td>\n",
              "      <td>4</td>\n",
              "      <td>16</td>\n",
              "      <td>105</td>\n",
              "      <td>140</td>\n",
              "      <td>377</td>\n",
              "      <td>34.271260</td>\n",
              "      <td>0.318302</td>\n",
              "      <td>5</td>\n",
              "      <td>25</td>\n",
              "      <td>...</td>\n",
              "      <td>0.38</td>\n",
              "      <td>0</td>\n",
              "      <td>22</td>\n",
              "      <td>15</td>\n",
              "      <td>7</td>\n",
              "      <td>5</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>67020</th>\n",
              "      <td>5031723</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>127</td>\n",
              "      <td>275</td>\n",
              "      <td>226</td>\n",
              "      <td>0.000321</td>\n",
              "      <td>0.566372</td>\n",
              "      <td>9</td>\n",
              "      <td>31</td>\n",
              "      <td>...</td>\n",
              "      <td>0.43</td>\n",
              "      <td>1</td>\n",
              "      <td>19</td>\n",
              "      <td>11</td>\n",
              "      <td>20</td>\n",
              "      <td>9</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>7</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>67356</th>\n",
              "      <td>2425698</td>\n",
              "      <td>5</td>\n",
              "      <td>25</td>\n",
              "      <td>191</td>\n",
              "      <td>2914</td>\n",
              "      <td>1403</td>\n",
              "      <td>96.331954</td>\n",
              "      <td>3.765502</td>\n",
              "      <td>6</td>\n",
              "      <td>646</td>\n",
              "      <td>...</td>\n",
              "      <td>0.33</td>\n",
              "      <td>0</td>\n",
              "      <td>60</td>\n",
              "      <td>23</td>\n",
              "      <td>14</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>8</td>\n",
              "      <td>21</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>61688</th>\n",
              "      <td>4139076</td>\n",
              "      <td>5</td>\n",
              "      <td>25</td>\n",
              "      <td>29</td>\n",
              "      <td>18</td>\n",
              "      <td>91</td>\n",
              "      <td>72.276727</td>\n",
              "      <td>0.186813</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>...</td>\n",
              "      <td>0.31</td>\n",
              "      <td>0</td>\n",
              "      <td>10</td>\n",
              "      <td>5</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21440</th>\n",
              "      <td>526846</td>\n",
              "      <td>5</td>\n",
              "      <td>25</td>\n",
              "      <td>113</td>\n",
              "      <td>780</td>\n",
              "      <td>100</td>\n",
              "      <td>95.468780</td>\n",
              "      <td>0.530000</td>\n",
              "      <td>3</td>\n",
              "      <td>6</td>\n",
              "      <td>...</td>\n",
              "      <td>0.43</td>\n",
              "      <td>0</td>\n",
              "      <td>18</td>\n",
              "      <td>14</td>\n",
              "      <td>11</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>8</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>73349</th>\n",
              "      <td>1328589</td>\n",
              "      <td>4</td>\n",
              "      <td>16</td>\n",
              "      <td>304</td>\n",
              "      <td>2048</td>\n",
              "      <td>231</td>\n",
              "      <td>66.689986</td>\n",
              "      <td>5.857143</td>\n",
              "      <td>2</td>\n",
              "      <td>167</td>\n",
              "      <td>...</td>\n",
              "      <td>0.41</td>\n",
              "      <td>1</td>\n",
              "      <td>58</td>\n",
              "      <td>19</td>\n",
              "      <td>20</td>\n",
              "      <td>15</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50057</th>\n",
              "      <td>6727839</td>\n",
              "      <td>5</td>\n",
              "      <td>25</td>\n",
              "      <td>43</td>\n",
              "      <td>35</td>\n",
              "      <td>36</td>\n",
              "      <td>66.814476</td>\n",
              "      <td>0.055556</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.47</td>\n",
              "      <td>0</td>\n",
              "      <td>8</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5192</th>\n",
              "      <td>1671443</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>51</td>\n",
              "      <td>5</td>\n",
              "      <td>7</td>\n",
              "      <td>41.747966</td>\n",
              "      <td>0.142857</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0</td>\n",
              "      <td>5</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>77708</th>\n",
              "      <td>1012859</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>104</td>\n",
              "      <td>291</td>\n",
              "      <td>352</td>\n",
              "      <td>110.089297</td>\n",
              "      <td>0.127841</td>\n",
              "      <td>9</td>\n",
              "      <td>14</td>\n",
              "      <td>...</td>\n",
              "      <td>0.25</td>\n",
              "      <td>1</td>\n",
              "      <td>28</td>\n",
              "      <td>14</td>\n",
              "      <td>7</td>\n",
              "      <td>6</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>80000 rows × 37 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-faa2e79b-7b3f-43fb-83ba-b45690282a21')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-faa2e79b-7b3f-43fb-83ba-b45690282a21 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-faa2e79b-7b3f-43fb-83ba-b45690282a21');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "          r_id  r_stars  r_stars_square  r_length  u_friends_count  \\\n",
              "7126   5530464        1               1       127               61   \n",
              "46624   879574        4              16       105              140   \n",
              "67020  5031723        1               1       127              275   \n",
              "67356  2425698        5              25       191             2914   \n",
              "61688  4139076        5              25        29               18   \n",
              "...        ...      ...             ...       ...              ...   \n",
              "21440   526846        5              25       113              780   \n",
              "73349  1328589        4              16       304             2048   \n",
              "50057  6727839        5              25        43               35   \n",
              "5192   1671443        1               1        51                5   \n",
              "77708  1012859        1               1       104              291   \n",
              "\n",
              "       u_review_count  u_month_age  u_comp_avg  u_n_elite_yrs  u_fans  ...  \\\n",
              "7126                1    25.718844    0.000000              0       0  ...   \n",
              "46624             377    34.271260    0.318302              5      25  ...   \n",
              "67020             226     0.000321    0.566372              9      31  ...   \n",
              "67356            1403    96.331954    3.765502              6     646  ...   \n",
              "61688              91    72.276727    0.186813              0       1  ...   \n",
              "...               ...          ...         ...            ...     ...  ...   \n",
              "21440             100    95.468780    0.530000              3       6  ...   \n",
              "73349             231    66.689986    5.857143              2     167  ...   \n",
              "50057              36    66.814476    0.055556              0       0  ...   \n",
              "5192                7    41.747966    0.142857              0       0  ...   \n",
              "77708             352   110.089297    0.127841              9      14  ...   \n",
              "\n",
              "       r_stopwords/words  r_digit_cnt  r_noun_cnt  r_Adj_cnt  r_Adv_cnt  \\\n",
              "7126                0.43            3          25          5          5   \n",
              "46624               0.38            0          22         15          7   \n",
              "67020               0.43            1          19         11         20   \n",
              "67356               0.33            0          60         23         14   \n",
              "61688               0.31            0          10          5          2   \n",
              "...                  ...          ...         ...        ...        ...   \n",
              "21440               0.43            0          18         14         11   \n",
              "73349               0.41            1          58         19         20   \n",
              "50057               0.47            0           8          2          3   \n",
              "5192                0.50            0           5          2          6   \n",
              "77708               0.25            1          28         14          7   \n",
              "\n",
              "       r_capital_word_cnt  r_quoted_word_cnt  r_hashtag_cnt  r_exclam_cnt  \\\n",
              "7126                    7                  0              0             0   \n",
              "46624                   5                  0              0             1   \n",
              "67020                   9                  0              0             7   \n",
              "67356                   1                  0              0             8   \n",
              "61688                   1                  0              0             0   \n",
              "...                   ...                ...            ...           ...   \n",
              "21440                   4                  0              0             8   \n",
              "73349                  15                  0              0             3   \n",
              "50057                   4                  0              0             1   \n",
              "5192                    1                  0              0             0   \n",
              "77708                   6                  0              0             0   \n",
              "\n",
              "       r_useful  \n",
              "7126          1  \n",
              "46624         4  \n",
              "67020         2  \n",
              "67356        21  \n",
              "61688         2  \n",
              "...         ...  \n",
              "21440         1  \n",
              "73349        11  \n",
              "50057         3  \n",
              "5192          1  \n",
              "77708         1  \n",
              "\n",
              "[80000 rows x 37 columns]"
            ]
          },
          "execution_count": 85,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Shuffle the data after train\n",
        "shuffled1 = shuffle(train_df, random_state=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DcTtPuUtjwCu"
      },
      "outputs": [],
      "source": [
        "def process_shuffle_dataset(dataset):\n",
        "  training_withaim=dataset.drop(labels=\"r_id\", axis=1)\n",
        "  imp_train=SimpleImputer(missing_values=np.NaN)\n",
        "  training_shuffled=pd.DataFrame(imp_train.fit_transform(training_withaim))\n",
        "  training_shuffled = training_shuffled.iloc[: , 0:35]\n",
        "  scaler = StandardScaler()\n",
        "  std_train_df = dataset.copy(deep=True)\n",
        "  std_train_df = scaler.fit_transform(training_shuffled)\n",
        "  std_train_df = pd.DataFrame(std_train_df)\n",
        "  training_shuffled = std_train_df.iloc[: , 0:36]\n",
        "  labelsForTrain_shuffled=training_withaim.iloc[: , -1]\n",
        "  return(training_shuffled,labelsForTrain_shuffled)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dm38hbJ3kF1t"
      },
      "outputs": [],
      "source": [
        "training_shuffled1,labelsForTrain_shuffled1=process_shuffle_dataset(shuffled1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "orNMxT13oi-z",
        "outputId": "3040e966-b1f8-4277-ffb8-032bf9343e2e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 5s - loss: 13.6645 - mse: 13.6645 - mae: 1.6297 - val_loss: 16.4643 - val_mse: 16.4643 - val_mae: 1.5253 - lr: 0.0012 - 5s/epoch - 5ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 4s - loss: 12.7454 - mse: 12.7454 - mae: 1.5639 - val_loss: 16.2787 - val_mse: 16.2787 - val_mae: 1.7193 - lr: 0.0012 - 4s/epoch - 4ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 4s - loss: 12.7496 - mse: 12.7496 - mae: 1.5603 - val_loss: 16.1542 - val_mse: 16.1542 - val_mae: 1.5721 - lr: 0.0012 - 4s/epoch - 4ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 4s - loss: 12.7378 - mse: 12.7378 - mae: 1.5571 - val_loss: 16.1323 - val_mse: 16.1323 - val_mae: 1.5385 - lr: 0.0012 - 4s/epoch - 4ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 4s - loss: 12.6311 - mse: 12.6311 - mae: 1.5528 - val_loss: 16.1544 - val_mse: 16.1544 - val_mae: 1.6354 - lr: 0.0012 - 4s/epoch - 4ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 4s - loss: 12.8246 - mse: 12.8246 - mae: 1.5593 - val_loss: 16.0810 - val_mse: 16.0810 - val_mae: 1.5351 - lr: 0.0012 - 4s/epoch - 4ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 4s - loss: 12.5983 - mse: 12.5983 - mae: 1.5472 - val_loss: 16.0700 - val_mse: 16.0700 - val_mae: 1.6953 - lr: 0.0012 - 4s/epoch - 4ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 4s - loss: 12.6341 - mse: 12.6341 - mae: 1.5558 - val_loss: 16.2226 - val_mse: 16.2226 - val_mae: 1.6212 - lr: 0.0012 - 4s/epoch - 4ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 4s - loss: 12.7845 - mse: 12.7845 - mae: 1.5460 - val_loss: 15.9883 - val_mse: 15.9883 - val_mae: 1.6298 - lr: 0.0012 - 4s/epoch - 4ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 4s - loss: 12.7288 - mse: 12.7288 - mae: 1.5538 - val_loss: 16.1724 - val_mse: 16.1724 - val_mae: 1.6045 - lr: 0.0012 - 4s/epoch - 4ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 4s - loss: 12.8557 - mse: 12.8557 - mae: 1.5569 - val_loss: 16.0535 - val_mse: 16.0535 - val_mae: 1.5891 - lr: 0.0012 - 4s/epoch - 4ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 4s - loss: 12.6737 - mse: 12.6737 - mae: 1.5550 - val_loss: 16.1974 - val_mse: 16.1974 - val_mae: 1.6461 - lr: 0.0012 - 4s/epoch - 4ms/step\n",
            "Epoch 13/100\n",
            "1000/1000 - 4s - loss: 12.6558 - mse: 12.6558 - mae: 1.5528 - val_loss: 16.6000 - val_mse: 16.6000 - val_mae: 1.5075 - lr: 0.0012 - 4s/epoch - 4ms/step\n",
            "Epoch 14/100\n",
            "1000/1000 - 4s - loss: 12.6794 - mse: 12.6794 - mae: 1.5533 - val_loss: 16.1075 - val_mse: 16.1075 - val_mae: 1.5300 - lr: 0.0012 - 4s/epoch - 4ms/step\n",
            "Score for fold 1: loss of 16.10749626159668\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 4s - loss: 13.9007 - mse: 13.9007 - mae: 1.5578 - val_loss: 11.1737 - val_mse: 11.1737 - val_mae: 1.6371 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 4s - loss: 13.9328 - mse: 13.9328 - mae: 1.5685 - val_loss: 11.5616 - val_mse: 11.5616 - val_mae: 1.5828 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 4s - loss: 13.9350 - mse: 13.9350 - mae: 1.5636 - val_loss: 11.0133 - val_mse: 11.0133 - val_mae: 1.5066 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 4s - loss: 13.9215 - mse: 13.9215 - mae: 1.5623 - val_loss: 11.1567 - val_mse: 11.1567 - val_mae: 1.4809 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 4s - loss: 13.9303 - mse: 13.9303 - mae: 1.5616 - val_loss: 11.0896 - val_mse: 11.0896 - val_mae: 1.5241 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 4s - loss: 13.9612 - mse: 13.9612 - mae: 1.5679 - val_loss: 12.2207 - val_mse: 12.2207 - val_mae: 1.6925 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 4s - loss: 13.8847 - mse: 13.8847 - mae: 1.5632 - val_loss: 10.8874 - val_mse: 10.8874 - val_mae: 1.5354 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 4s - loss: 13.9247 - mse: 13.9247 - mae: 1.5677 - val_loss: 11.2459 - val_mse: 11.2459 - val_mae: 1.4732 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 4s - loss: 13.9920 - mse: 13.9920 - mae: 1.5669 - val_loss: 11.0184 - val_mse: 11.0184 - val_mae: 1.5056 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 4s - loss: 13.8948 - mse: 13.8948 - mae: 1.5618 - val_loss: 10.9804 - val_mse: 10.9804 - val_mae: 1.5201 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 4s - loss: 13.9872 - mse: 13.9872 - mae: 1.5664 - val_loss: 11.5932 - val_mse: 11.5932 - val_mae: 1.6963 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 4s - loss: 13.9484 - mse: 13.9484 - mae: 1.5592 - val_loss: 10.9425 - val_mse: 10.9425 - val_mae: 1.5631 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Score for fold 2: loss of 10.94253158569336\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 4s - loss: 12.3608 - mse: 12.3608 - mae: 1.5595 - val_loss: 17.4905 - val_mse: 17.4905 - val_mae: 1.5512 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 4s - loss: 12.1695 - mse: 12.1695 - mae: 1.5600 - val_loss: 17.4820 - val_mse: 17.4820 - val_mae: 1.5400 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 4s - loss: 12.4122 - mse: 12.4122 - mae: 1.5679 - val_loss: 17.4415 - val_mse: 17.4415 - val_mae: 1.5639 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 4s - loss: 12.3166 - mse: 12.3166 - mae: 1.5616 - val_loss: 17.6110 - val_mse: 17.6110 - val_mae: 1.5030 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 4s - loss: 12.3304 - mse: 12.3304 - mae: 1.5589 - val_loss: 17.6656 - val_mse: 17.6656 - val_mae: 1.5082 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 4s - loss: 12.2456 - mse: 12.2456 - mae: 1.5595 - val_loss: 17.5165 - val_mse: 17.5165 - val_mae: 1.5155 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 4s - loss: 12.2226 - mse: 12.2226 - mae: 1.5591 - val_loss: 17.3206 - val_mse: 17.3206 - val_mae: 1.6222 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 4s - loss: 12.2476 - mse: 12.2476 - mae: 1.5586 - val_loss: 17.7514 - val_mse: 17.7514 - val_mae: 1.4882 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 4s - loss: 12.0982 - mse: 12.0982 - mae: 1.5541 - val_loss: 17.7764 - val_mse: 17.7764 - val_mae: 1.5118 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 4s - loss: 12.3470 - mse: 12.3470 - mae: 1.5582 - val_loss: 17.3790 - val_mse: 17.3790 - val_mae: 1.6204 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 4s - loss: 12.2486 - mse: 12.2486 - mae: 1.5589 - val_loss: 17.4697 - val_mse: 17.4697 - val_mae: 1.6562 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 4s - loss: 12.2815 - mse: 12.2815 - mae: 1.5577 - val_loss: 17.4652 - val_mse: 17.4652 - val_mae: 1.6956 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Score for fold 3: loss of 17.46523666381836\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 4s - loss: 13.8357 - mse: 13.8357 - mae: 1.5611 - val_loss: 11.0286 - val_mse: 11.0286 - val_mae: 1.5511 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 4s - loss: 13.9390 - mse: 13.9390 - mae: 1.5631 - val_loss: 11.5650 - val_mse: 11.5650 - val_mae: 1.6458 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 4s - loss: 13.8880 - mse: 13.8880 - mae: 1.5590 - val_loss: 11.0741 - val_mse: 11.0741 - val_mae: 1.6021 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 4s - loss: 13.9025 - mse: 13.9025 - mae: 1.5664 - val_loss: 11.1538 - val_mse: 11.1538 - val_mae: 1.5418 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 4s - loss: 13.7871 - mse: 13.7871 - mae: 1.5632 - val_loss: 13.1136 - val_mse: 13.1136 - val_mae: 1.8391 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 4s - loss: 14.0913 - mse: 14.0913 - mae: 1.5623 - val_loss: 11.2736 - val_mse: 11.2736 - val_mae: 1.4616 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Score for fold 4: loss of 11.273565292358398\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 4s - loss: 14.1073 - mse: 14.1073 - mae: 1.5585 - val_loss: 10.6094 - val_mse: 10.6094 - val_mae: 1.5270 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 4s - loss: 14.1081 - mse: 14.1081 - mae: 1.5742 - val_loss: 10.4527 - val_mse: 10.4527 - val_mae: 1.5140 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 4s - loss: 14.0066 - mse: 14.0066 - mae: 1.5567 - val_loss: 10.4345 - val_mse: 10.4345 - val_mae: 1.5340 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 4s - loss: 14.0121 - mse: 14.0121 - mae: 1.5657 - val_loss: 10.4837 - val_mse: 10.4837 - val_mae: 1.5513 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 4s - loss: 14.1227 - mse: 14.1227 - mae: 1.5645 - val_loss: 10.5989 - val_mse: 10.5989 - val_mae: 1.5563 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 4s - loss: 13.9518 - mse: 13.9518 - mae: 1.5595 - val_loss: 10.6155 - val_mse: 10.6155 - val_mae: 1.4978 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 4s - loss: 13.9398 - mse: 13.9398 - mae: 1.5654 - val_loss: 10.6132 - val_mse: 10.6132 - val_mae: 1.5815 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 4s - loss: 13.9266 - mse: 13.9266 - mae: 1.5739 - val_loss: 10.7637 - val_mse: 10.7637 - val_mae: 1.5554 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Score for fold 5: loss of 10.763690948486328\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-10-11 14:10:00,914]\u001b[0m Finished trial#0 resulted in value: 13.309999999999999. Current best value is 13.309999999999999 with parameters: {'activation': 'linear', 'num_hidden_layer': 3, 'i': 9, 'learning_rate': 0.0011785909999331011}.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 13.7507 - mse: 13.7507 - mae: 1.6214 - val_loss: 14.1300 - val_mse: 14.1300 - val_mae: 1.5490 - lr: 0.0011 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 13.2573 - mse: 13.2573 - mae: 1.5662 - val_loss: 13.9889 - val_mse: 13.9889 - val_mae: 1.5679 - lr: 0.0011 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 13.1996 - mse: 13.1996 - mae: 1.5656 - val_loss: 13.9209 - val_mse: 13.9209 - val_mae: 1.5201 - lr: 0.0011 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 13.1858 - mse: 13.1858 - mae: 1.5569 - val_loss: 13.8435 - val_mse: 13.8435 - val_mae: 1.5257 - lr: 0.0011 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 13.1426 - mse: 13.1426 - mae: 1.5622 - val_loss: 13.9220 - val_mse: 13.9220 - val_mae: 1.6235 - lr: 0.0011 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 13.1420 - mse: 13.1420 - mae: 1.5598 - val_loss: 13.8713 - val_mse: 13.8713 - val_mae: 1.5560 - lr: 0.0011 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 13.1750 - mse: 13.1750 - mae: 1.5597 - val_loss: 13.9655 - val_mse: 13.9655 - val_mae: 1.4923 - lr: 0.0011 - 2s/epoch - 2ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 2s - loss: 13.1157 - mse: 13.1157 - mae: 1.5552 - val_loss: 13.8624 - val_mse: 13.8624 - val_mae: 1.5606 - lr: 0.0011 - 2s/epoch - 2ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 2s - loss: 13.1270 - mse: 13.1270 - mae: 1.5581 - val_loss: 13.9867 - val_mse: 13.9867 - val_mae: 1.5924 - lr: 0.0011 - 2s/epoch - 2ms/step\n",
            "Score for fold 1: loss of 13.98672103881836\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 14.1056 - mse: 14.1056 - mae: 1.5618 - val_loss: 9.6589 - val_mse: 9.6589 - val_mae: 1.5309 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 14.1080 - mse: 14.1080 - mae: 1.5616 - val_loss: 9.6688 - val_mse: 9.6688 - val_mae: 1.4762 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 14.1217 - mse: 14.1217 - mae: 1.5611 - val_loss: 9.5039 - val_mse: 9.5039 - val_mae: 1.5371 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 14.1199 - mse: 14.1199 - mae: 1.5624 - val_loss: 9.8049 - val_mse: 9.8049 - val_mae: 1.4973 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 14.1259 - mse: 14.1259 - mae: 1.5585 - val_loss: 9.7553 - val_mse: 9.7553 - val_mae: 1.5115 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 14.1212 - mse: 14.1212 - mae: 1.5596 - val_loss: 9.7159 - val_mse: 9.7159 - val_mae: 1.5072 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 14.1486 - mse: 14.1486 - mae: 1.5651 - val_loss: 9.8028 - val_mse: 9.8028 - val_mae: 1.4661 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 2s - loss: 14.0800 - mse: 14.0800 - mae: 1.5569 - val_loss: 9.6449 - val_mse: 9.6449 - val_mae: 1.5427 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Score for fold 2: loss of 9.644954681396484\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 13.5536 - mse: 13.5536 - mae: 1.5438 - val_loss: 11.9869 - val_mse: 11.9869 - val_mae: 1.5398 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 13.5717 - mse: 13.5717 - mae: 1.5470 - val_loss: 12.1241 - val_mse: 12.1241 - val_mae: 1.6135 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 13.5383 - mse: 13.5383 - mae: 1.5416 - val_loss: 11.9356 - val_mse: 11.9356 - val_mae: 1.5752 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 13.5572 - mse: 13.5572 - mae: 1.5528 - val_loss: 12.0150 - val_mse: 12.0150 - val_mae: 1.5697 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 13.5621 - mse: 13.5621 - mae: 1.5449 - val_loss: 11.9674 - val_mse: 11.9674 - val_mae: 1.5351 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 13.4939 - mse: 13.4939 - mae: 1.5505 - val_loss: 11.9778 - val_mse: 11.9778 - val_mae: 1.5401 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 13.5481 - mse: 13.5481 - mae: 1.5399 - val_loss: 11.9615 - val_mse: 11.9615 - val_mae: 1.5437 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 2s - loss: 13.6640 - mse: 13.6640 - mae: 1.5507 - val_loss: 11.9994 - val_mse: 11.9994 - val_mae: 1.5581 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Score for fold 3: loss of 11.999449729919434\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 11.3205 - mse: 11.3205 - mae: 1.5283 - val_loss: 20.8566 - val_mse: 20.8566 - val_mae: 1.5341 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 11.3488 - mse: 11.3488 - mae: 1.5307 - val_loss: 20.8158 - val_mse: 20.8158 - val_mae: 1.5801 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 11.3329 - mse: 11.3329 - mae: 1.5317 - val_loss: 20.7840 - val_mse: 20.7840 - val_mae: 1.6067 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 11.3794 - mse: 11.3794 - mae: 1.5342 - val_loss: 20.8323 - val_mse: 20.8323 - val_mae: 1.5940 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 11.4000 - mse: 11.4000 - mae: 1.5325 - val_loss: 20.7441 - val_mse: 20.7441 - val_mae: 1.6064 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 11.3051 - mse: 11.3051 - mae: 1.5344 - val_loss: 20.8113 - val_mse: 20.8113 - val_mae: 1.5540 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 11.3609 - mse: 11.3609 - mae: 1.5315 - val_loss: 20.9296 - val_mse: 20.9296 - val_mae: 1.6460 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 2s - loss: 11.3435 - mse: 11.3435 - mae: 1.5320 - val_loss: 20.7940 - val_mse: 20.7940 - val_mae: 1.6209 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 2s - loss: 11.3966 - mse: 11.3966 - mae: 1.5372 - val_loss: 21.0684 - val_mse: 21.0684 - val_mae: 1.6632 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 2s - loss: 11.3668 - mse: 11.3668 - mae: 1.5334 - val_loss: 21.1761 - val_mse: 21.1761 - val_mae: 1.6531 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Score for fold 4: loss of 21.17612075805664\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 14.1112 - mse: 14.1112 - mae: 1.5527 - val_loss: 9.8710 - val_mse: 9.8710 - val_mae: 1.5535 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 14.0719 - mse: 14.0719 - mae: 1.5573 - val_loss: 9.9358 - val_mse: 9.9358 - val_mae: 1.5370 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 14.0463 - mse: 14.0463 - mae: 1.5559 - val_loss: 9.8392 - val_mse: 9.8392 - val_mae: 1.5186 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 14.0156 - mse: 14.0156 - mae: 1.5533 - val_loss: 9.8829 - val_mse: 9.8829 - val_mae: 1.5383 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 14.0877 - mse: 14.0877 - mae: 1.5584 - val_loss: 9.8881 - val_mse: 9.8881 - val_mae: 1.4563 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 14.0148 - mse: 14.0148 - mae: 1.5528 - val_loss: 9.8623 - val_mse: 9.8623 - val_mae: 1.5797 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 14.0418 - mse: 14.0418 - mae: 1.5569 - val_loss: 9.9326 - val_mse: 9.9326 - val_mae: 1.5491 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 2s - loss: 14.1012 - mse: 14.1012 - mae: 1.5516 - val_loss: 9.8496 - val_mse: 9.8496 - val_mae: 1.5549 - lr: 0.0010 - 2s/epoch - 2ms/step\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-10-11 14:11:16,670]\u001b[0m Finished trial#1 resulted in value: 13.331999999999999. Current best value is 13.309999999999999 with parameters: {'activation': 'linear', 'num_hidden_layer': 3, 'i': 9, 'learning_rate': 0.0011785909999331011}.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Score for fold 5: loss of 9.849637031555176\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 15.0110 - mse: 15.0110 - mae: 1.6458 - val_loss: 11.3564 - val_mse: 11.3564 - val_mae: 1.4741 - lr: 0.0074 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 14.3315 - mse: 14.3315 - mae: 1.6100 - val_loss: 11.8247 - val_mse: 11.8247 - val_mae: 1.4906 - lr: 0.0074 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 14.2568 - mse: 14.2568 - mae: 1.6003 - val_loss: 11.4013 - val_mse: 11.4013 - val_mae: 1.6838 - lr: 0.0074 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 14.0948 - mse: 14.0948 - mae: 1.5970 - val_loss: 11.3749 - val_mse: 11.3749 - val_mae: 1.5718 - lr: 0.0074 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 14.1105 - mse: 14.1105 - mae: 1.5935 - val_loss: 11.0063 - val_mse: 11.0063 - val_mae: 1.5608 - lr: 0.0074 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 14.0019 - mse: 14.0019 - mae: 1.6008 - val_loss: 11.3563 - val_mse: 11.3563 - val_mae: 1.6168 - lr: 0.0074 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 14.1608 - mse: 14.1608 - mae: 1.5993 - val_loss: 11.1094 - val_mse: 11.1094 - val_mae: 1.5769 - lr: 0.0074 - 2s/epoch - 2ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 2s - loss: 14.0506 - mse: 14.0506 - mae: 1.5971 - val_loss: 11.1896 - val_mse: 11.1896 - val_mae: 1.5586 - lr: 0.0074 - 2s/epoch - 2ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 2s - loss: 14.2433 - mse: 14.2433 - mae: 1.5974 - val_loss: 11.1200 - val_mse: 11.1200 - val_mae: 1.5011 - lr: 0.0074 - 2s/epoch - 2ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 2s - loss: 13.9555 - mse: 13.9555 - mae: 1.5965 - val_loss: 11.6000 - val_mse: 11.6000 - val_mae: 1.5981 - lr: 0.0074 - 2s/epoch - 2ms/step\n",
            "Score for fold 1: loss of 11.599961280822754\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 13.3646 - mse: 13.3646 - mae: 1.5439 - val_loss: 12.8098 - val_mse: 12.8098 - val_mae: 1.5994 - lr: 0.0015 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 13.3799 - mse: 13.3799 - mae: 1.5380 - val_loss: 12.6579 - val_mse: 12.6579 - val_mae: 1.6008 - lr: 0.0015 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 13.3329 - mse: 13.3329 - mae: 1.5430 - val_loss: 12.8342 - val_mse: 12.8342 - val_mae: 1.5958 - lr: 0.0015 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 13.4700 - mse: 13.4700 - mae: 1.5468 - val_loss: 12.6635 - val_mse: 12.6635 - val_mae: 1.5820 - lr: 0.0015 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 13.4079 - mse: 13.4079 - mae: 1.5425 - val_loss: 12.6551 - val_mse: 12.6551 - val_mae: 1.5631 - lr: 0.0015 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 13.3688 - mse: 13.3688 - mae: 1.5396 - val_loss: 12.7177 - val_mse: 12.7177 - val_mae: 1.6265 - lr: 0.0015 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 13.3002 - mse: 13.3002 - mae: 1.5417 - val_loss: 12.8024 - val_mse: 12.8024 - val_mae: 1.6043 - lr: 0.0015 - 2s/epoch - 2ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 2s - loss: 13.3627 - mse: 13.3627 - mae: 1.5410 - val_loss: 12.9505 - val_mse: 12.9505 - val_mae: 1.6161 - lr: 0.0015 - 2s/epoch - 2ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 2s - loss: 13.3352 - mse: 13.3352 - mae: 1.5424 - val_loss: 12.8941 - val_mse: 12.8941 - val_mae: 1.5896 - lr: 0.0015 - 2s/epoch - 2ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 2s - loss: 13.4696 - mse: 13.4696 - mae: 1.5439 - val_loss: 12.7272 - val_mse: 12.7272 - val_mae: 1.5819 - lr: 0.0015 - 2s/epoch - 2ms/step\n",
            "Score for fold 2: loss of 12.727189064025879\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 11.5327 - mse: 11.5327 - mae: 1.5429 - val_loss: 19.9623 - val_mse: 19.9623 - val_mae: 1.5397 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 11.5629 - mse: 11.5629 - mae: 1.5417 - val_loss: 20.0467 - val_mse: 20.0467 - val_mae: 1.5792 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 11.5282 - mse: 11.5282 - mae: 1.5448 - val_loss: 20.0637 - val_mse: 20.0637 - val_mae: 1.5643 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 11.5725 - mse: 11.5725 - mae: 1.5459 - val_loss: 20.4080 - val_mse: 20.4080 - val_mae: 1.6179 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 11.5390 - mse: 11.5390 - mae: 1.5491 - val_loss: 20.1764 - val_mse: 20.1764 - val_mae: 1.5619 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 11.5221 - mse: 11.5221 - mae: 1.5438 - val_loss: 20.1430 - val_mse: 20.1430 - val_mae: 1.5643 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Score for fold 3: loss of 20.143001556396484\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 14.1252 - mse: 14.1252 - mae: 1.5554 - val_loss: 9.8052 - val_mse: 9.8052 - val_mae: 1.5015 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 14.1045 - mse: 14.1045 - mae: 1.5555 - val_loss: 9.6583 - val_mse: 9.6583 - val_mae: 1.5246 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 14.0184 - mse: 14.0184 - mae: 1.5548 - val_loss: 9.6881 - val_mse: 9.6881 - val_mae: 1.5890 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 14.1656 - mse: 14.1656 - mae: 1.5568 - val_loss: 9.6144 - val_mse: 9.6144 - val_mae: 1.5346 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 14.1121 - mse: 14.1121 - mae: 1.5562 - val_loss: 9.8526 - val_mse: 9.8526 - val_mae: 1.5179 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 14.0780 - mse: 14.0780 - mae: 1.5605 - val_loss: 9.5734 - val_mse: 9.5734 - val_mae: 1.5454 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 14.1226 - mse: 14.1226 - mae: 1.5523 - val_loss: 9.7335 - val_mse: 9.7335 - val_mae: 1.5345 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 2s - loss: 14.0879 - mse: 14.0879 - mae: 1.5553 - val_loss: 9.9760 - val_mse: 9.9760 - val_mae: 1.4895 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 2s - loss: 14.1081 - mse: 14.1081 - mae: 1.5575 - val_loss: 9.7610 - val_mse: 9.7610 - val_mae: 1.5157 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 2s - loss: 14.0680 - mse: 14.0680 - mae: 1.5536 - val_loss: 9.8205 - val_mse: 9.8205 - val_mae: 1.5240 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 2s - loss: 14.1440 - mse: 14.1440 - mae: 1.5599 - val_loss: 9.7104 - val_mse: 9.7104 - val_mae: 1.5137 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Score for fold 4: loss of 9.710357666015625\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 13.3889 - mse: 13.3889 - mae: 1.5485 - val_loss: 12.6115 - val_mse: 12.6115 - val_mae: 1.5297 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 13.3417 - mse: 13.3417 - mae: 1.5528 - val_loss: 12.5935 - val_mse: 12.5935 - val_mae: 1.5360 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 13.4213 - mse: 13.4213 - mae: 1.5511 - val_loss: 12.5621 - val_mse: 12.5621 - val_mae: 1.5453 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 13.3099 - mse: 13.3099 - mae: 1.5496 - val_loss: 12.6377 - val_mse: 12.6377 - val_mae: 1.5363 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 13.4418 - mse: 13.4418 - mae: 1.5514 - val_loss: 12.6401 - val_mse: 12.6401 - val_mae: 1.5393 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 13.3733 - mse: 13.3733 - mae: 1.5494 - val_loss: 12.5788 - val_mse: 12.5788 - val_mae: 1.5499 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 13.3437 - mse: 13.3437 - mae: 1.5519 - val_loss: 12.6748 - val_mse: 12.6748 - val_mae: 1.5141 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 2s - loss: 13.3739 - mse: 13.3739 - mae: 1.5545 - val_loss: 12.5989 - val_mse: 12.5989 - val_mae: 1.5743 - lr: 0.0010 - 2s/epoch - 2ms/step\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-10-11 14:12:38,589]\u001b[0m Finished trial#2 resulted in value: 13.356. Current best value is 13.309999999999999 with parameters: {'activation': 'linear', 'num_hidden_layer': 3, 'i': 9, 'learning_rate': 0.0011785909999331011}.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Score for fold 5: loss of 12.598859786987305\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 3s - loss: 15.1560 - mse: 15.1560 - mae: 1.6339 - val_loss: 9.6032 - val_mse: 9.6032 - val_mae: 1.5668 - lr: 3.2060e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 14.3643 - mse: 14.3643 - mae: 1.5675 - val_loss: 9.4178 - val_mse: 9.4178 - val_mae: 1.4886 - lr: 3.2060e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 14.2895 - mse: 14.2895 - mae: 1.5657 - val_loss: 9.4214 - val_mse: 9.4214 - val_mae: 1.5085 - lr: 3.2060e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 14.2280 - mse: 14.2280 - mae: 1.5588 - val_loss: 9.9626 - val_mse: 9.9626 - val_mae: 1.6781 - lr: 3.2060e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 14.2643 - mse: 14.2643 - mae: 1.5633 - val_loss: 9.8649 - val_mse: 9.8649 - val_mae: 1.5453 - lr: 3.2060e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 14.2693 - mse: 14.2693 - mae: 1.5552 - val_loss: 9.3626 - val_mse: 9.3626 - val_mae: 1.4991 - lr: 3.2060e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 14.2597 - mse: 14.2597 - mae: 1.5549 - val_loss: 9.4086 - val_mse: 9.4086 - val_mae: 1.4970 - lr: 3.2060e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 2s - loss: 14.1928 - mse: 14.1928 - mae: 1.5592 - val_loss: 9.6354 - val_mse: 9.6354 - val_mae: 1.6020 - lr: 3.2060e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 2s - loss: 14.2896 - mse: 14.2896 - mae: 1.5573 - val_loss: 9.4008 - val_mse: 9.4008 - val_mae: 1.5161 - lr: 3.2060e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 2s - loss: 14.2027 - mse: 14.2027 - mae: 1.5552 - val_loss: 9.5936 - val_mse: 9.5936 - val_mae: 1.5396 - lr: 3.2060e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 2s - loss: 14.2833 - mse: 14.2833 - mae: 1.5548 - val_loss: 9.8057 - val_mse: 9.8057 - val_mae: 1.5707 - lr: 3.2060e-04 - 2s/epoch - 2ms/step\n",
            "Score for fold 1: loss of 9.805648803710938\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 12.8463 - mse: 12.8463 - mae: 1.5437 - val_loss: 14.8501 - val_mse: 14.8501 - val_mae: 1.5482 - lr: 3.2060e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 12.7990 - mse: 12.7990 - mae: 1.5445 - val_loss: 14.8892 - val_mse: 14.8892 - val_mae: 1.5484 - lr: 3.2060e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 12.7756 - mse: 12.7756 - mae: 1.5441 - val_loss: 14.9344 - val_mse: 14.9344 - val_mae: 1.5494 - lr: 3.2060e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 12.8731 - mse: 12.8731 - mae: 1.5449 - val_loss: 15.2646 - val_mse: 15.2646 - val_mae: 1.5392 - lr: 3.2060e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 12.8019 - mse: 12.8019 - mae: 1.5427 - val_loss: 15.0427 - val_mse: 15.0427 - val_mae: 1.5622 - lr: 3.2060e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 12.7960 - mse: 12.7960 - mae: 1.5469 - val_loss: 14.8497 - val_mse: 14.8497 - val_mae: 1.5820 - lr: 3.2060e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 12.7849 - mse: 12.7849 - mae: 1.5442 - val_loss: 14.9991 - val_mse: 14.9991 - val_mae: 1.5487 - lr: 3.2060e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 2s - loss: 12.8819 - mse: 12.8819 - mae: 1.5460 - val_loss: 14.8745 - val_mse: 14.8745 - val_mae: 1.5621 - lr: 3.2060e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 2s - loss: 12.8388 - mse: 12.8388 - mae: 1.5447 - val_loss: 14.8456 - val_mse: 14.8456 - val_mae: 1.5500 - lr: 3.2060e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 2s - loss: 12.8394 - mse: 12.8394 - mae: 1.5449 - val_loss: 14.8519 - val_mse: 14.8519 - val_mae: 1.5636 - lr: 3.2060e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 2s - loss: 12.7849 - mse: 12.7849 - mae: 1.5444 - val_loss: 14.9431 - val_mse: 14.9431 - val_mae: 1.5694 - lr: 3.2060e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 2s - loss: 12.8329 - mse: 12.8329 - mae: 1.5396 - val_loss: 14.8681 - val_mse: 14.8681 - val_mae: 1.5871 - lr: 3.2060e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 13/100\n",
            "1000/1000 - 2s - loss: 12.8053 - mse: 12.8053 - mae: 1.5440 - val_loss: 14.9150 - val_mse: 14.9150 - val_mae: 1.5785 - lr: 3.2060e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 14/100\n",
            "1000/1000 - 2s - loss: 12.8988 - mse: 12.8988 - mae: 1.5464 - val_loss: 14.9133 - val_mse: 14.9133 - val_mae: 1.5438 - lr: 3.2060e-04 - 2s/epoch - 2ms/step\n",
            "Score for fold 2: loss of 14.913261413574219\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 13.1830 - mse: 13.1830 - mae: 1.5510 - val_loss: 13.2717 - val_mse: 13.2717 - val_mae: 1.5527 - lr: 3.2060e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 13.2484 - mse: 13.2484 - mae: 1.5397 - val_loss: 13.1307 - val_mse: 13.1307 - val_mae: 1.5388 - lr: 3.2060e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 13.2627 - mse: 13.2627 - mae: 1.5506 - val_loss: 13.4410 - val_mse: 13.4410 - val_mae: 1.5865 - lr: 3.2060e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 13.2626 - mse: 13.2626 - mae: 1.5462 - val_loss: 13.2424 - val_mse: 13.2424 - val_mae: 1.5339 - lr: 3.2060e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 13.2666 - mse: 13.2666 - mae: 1.5476 - val_loss: 13.3594 - val_mse: 13.3594 - val_mae: 1.6044 - lr: 3.2060e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 13.2248 - mse: 13.2248 - mae: 1.5401 - val_loss: 13.1546 - val_mse: 13.1546 - val_mae: 1.5604 - lr: 3.2060e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 13.2036 - mse: 13.2036 - mae: 1.5466 - val_loss: 13.2235 - val_mse: 13.2235 - val_mae: 1.5661 - lr: 3.2060e-04 - 2s/epoch - 2ms/step\n",
            "Score for fold 3: loss of 13.223477363586426\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 12.0342 - mse: 12.0342 - mae: 1.5494 - val_loss: 18.0375 - val_mse: 18.0375 - val_mae: 1.5545 - lr: 3.2060e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 12.0263 - mse: 12.0263 - mae: 1.5507 - val_loss: 18.0213 - val_mse: 18.0213 - val_mae: 1.5443 - lr: 3.2060e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 11.9607 - mse: 11.9607 - mae: 1.5485 - val_loss: 18.0334 - val_mse: 18.0334 - val_mae: 1.5518 - lr: 3.2060e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 12.0035 - mse: 12.0035 - mae: 1.5493 - val_loss: 18.0890 - val_mse: 18.0890 - val_mae: 1.5345 - lr: 3.2060e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 11.9167 - mse: 11.9167 - mae: 1.5494 - val_loss: 18.0363 - val_mse: 18.0363 - val_mae: 1.5526 - lr: 3.2060e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 11.9617 - mse: 11.9617 - mae: 1.5486 - val_loss: 18.0154 - val_mse: 18.0154 - val_mae: 1.5481 - lr: 3.2060e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 12.0197 - mse: 12.0197 - mae: 1.5498 - val_loss: 18.0850 - val_mse: 18.0850 - val_mae: 1.5384 - lr: 3.2060e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 2s - loss: 12.0187 - mse: 12.0187 - mae: 1.5498 - val_loss: 18.1393 - val_mse: 18.1393 - val_mae: 1.5249 - lr: 3.2060e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 2s - loss: 12.0164 - mse: 12.0164 - mae: 1.5490 - val_loss: 18.0140 - val_mse: 18.0140 - val_mae: 1.5623 - lr: 3.2060e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 2s - loss: 11.9869 - mse: 11.9869 - mae: 1.5465 - val_loss: 18.0137 - val_mse: 18.0137 - val_mae: 1.5405 - lr: 3.2060e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 2s - loss: 11.9985 - mse: 11.9985 - mae: 1.5485 - val_loss: 18.1464 - val_mse: 18.1464 - val_mae: 1.5295 - lr: 3.2060e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 2s - loss: 11.9906 - mse: 11.9906 - mae: 1.5507 - val_loss: 18.0848 - val_mse: 18.0848 - val_mae: 1.6051 - lr: 3.2060e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 13/100\n",
            "1000/1000 - 2s - loss: 11.9676 - mse: 11.9676 - mae: 1.5507 - val_loss: 17.9995 - val_mse: 17.9995 - val_mae: 1.5787 - lr: 3.2060e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 14/100\n",
            "1000/1000 - 2s - loss: 11.9737 - mse: 11.9737 - mae: 1.5494 - val_loss: 18.0035 - val_mse: 18.0035 - val_mae: 1.5589 - lr: 3.2060e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 15/100\n",
            "1000/1000 - 2s - loss: 11.9916 - mse: 11.9916 - mae: 1.5508 - val_loss: 18.0281 - val_mse: 18.0281 - val_mae: 1.5392 - lr: 3.2060e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 16/100\n",
            "1000/1000 - 2s - loss: 12.0100 - mse: 12.0100 - mae: 1.5501 - val_loss: 18.1594 - val_mse: 18.1594 - val_mae: 1.5103 - lr: 3.2060e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 17/100\n",
            "1000/1000 - 2s - loss: 11.9062 - mse: 11.9062 - mae: 1.5471 - val_loss: 18.0219 - val_mse: 18.0219 - val_mae: 1.5589 - lr: 3.2060e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 18/100\n",
            "1000/1000 - 2s - loss: 11.9514 - mse: 11.9514 - mae: 1.5504 - val_loss: 18.0071 - val_mse: 18.0071 - val_mae: 1.5395 - lr: 3.2060e-04 - 2s/epoch - 2ms/step\n",
            "Score for fold 4: loss of 18.00713348388672\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 13.8888 - mse: 13.8888 - mae: 1.5462 - val_loss: 10.5520 - val_mse: 10.5520 - val_mae: 1.5737 - lr: 3.2060e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 13.9275 - mse: 13.9275 - mae: 1.5510 - val_loss: 10.7250 - val_mse: 10.7250 - val_mae: 1.5098 - lr: 3.2060e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 13.9375 - mse: 13.9375 - mae: 1.5452 - val_loss: 10.5742 - val_mse: 10.5742 - val_mae: 1.5207 - lr: 3.2060e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 13.9273 - mse: 13.9273 - mae: 1.5507 - val_loss: 10.5984 - val_mse: 10.5984 - val_mae: 1.6084 - lr: 3.2060e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 13.9547 - mse: 13.9547 - mae: 1.5471 - val_loss: 10.5031 - val_mse: 10.5031 - val_mae: 1.5556 - lr: 3.2060e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 13.9196 - mse: 13.9196 - mae: 1.5519 - val_loss: 10.5104 - val_mse: 10.5104 - val_mae: 1.5530 - lr: 3.2060e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 13.8659 - mse: 13.8659 - mae: 1.5521 - val_loss: 10.4965 - val_mse: 10.4965 - val_mae: 1.5527 - lr: 3.2060e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 2s - loss: 13.8834 - mse: 13.8834 - mae: 1.5495 - val_loss: 10.5447 - val_mse: 10.5447 - val_mae: 1.5769 - lr: 3.2060e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 2s - loss: 13.8969 - mse: 13.8969 - mae: 1.5519 - val_loss: 10.5785 - val_mse: 10.5785 - val_mae: 1.5044 - lr: 3.2060e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 2s - loss: 13.9097 - mse: 13.9097 - mae: 1.5510 - val_loss: 10.5283 - val_mse: 10.5283 - val_mae: 1.5104 - lr: 3.2060e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 2s - loss: 13.9136 - mse: 13.9136 - mae: 1.5478 - val_loss: 10.6383 - val_mse: 10.6383 - val_mae: 1.6125 - lr: 3.2060e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 2s - loss: 13.8429 - mse: 13.8429 - mae: 1.5430 - val_loss: 10.5157 - val_mse: 10.5157 - val_mae: 1.5375 - lr: 3.2060e-04 - 2s/epoch - 2ms/step\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-10-11 14:15:04,564]\u001b[0m Finished trial#3 resulted in value: 13.294. Current best value is 13.294 with parameters: {'activation': 'linear', 'num_hidden_layer': 4, 'i': 7, 'learning_rate': 0.00032059788489556567}.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Score for fold 5: loss of 10.515707015991211\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 7s - loss: 13.4825 - mse: 13.4825 - mae: 1.5514 - val_loss: 11.3967 - val_mse: 11.3967 - val_mae: 1.5276 - lr: 1.5599e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 6s - loss: 12.5818 - mse: 12.5818 - mae: 1.4953 - val_loss: 11.4373 - val_mse: 11.4373 - val_mae: 1.3884 - lr: 1.5599e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 6s - loss: 12.3832 - mse: 12.3832 - mae: 1.4808 - val_loss: 11.4208 - val_mse: 11.4208 - val_mae: 1.3839 - lr: 1.5599e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 6s - loss: 12.2720 - mse: 12.2720 - mae: 1.4637 - val_loss: 11.1265 - val_mse: 11.1265 - val_mae: 1.3960 - lr: 1.5599e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 6s - loss: 12.1332 - mse: 12.1332 - mae: 1.4539 - val_loss: 11.0866 - val_mse: 11.0866 - val_mae: 1.5127 - lr: 1.5599e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 6s - loss: 11.9629 - mse: 11.9629 - mae: 1.4497 - val_loss: 11.0515 - val_mse: 11.0515 - val_mae: 1.4430 - lr: 1.5599e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 6s - loss: 11.9345 - mse: 11.9345 - mae: 1.4379 - val_loss: 11.2861 - val_mse: 11.2861 - val_mae: 1.4663 - lr: 1.5599e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 6s - loss: 11.7232 - mse: 11.7232 - mae: 1.4311 - val_loss: 10.9102 - val_mse: 10.9102 - val_mae: 1.4798 - lr: 1.5599e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 6s - loss: 11.6244 - mse: 11.6244 - mae: 1.4243 - val_loss: 10.9437 - val_mse: 10.9437 - val_mae: 1.4191 - lr: 1.5599e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 6s - loss: 11.4355 - mse: 11.4355 - mae: 1.4165 - val_loss: 11.0225 - val_mse: 11.0225 - val_mae: 1.4589 - lr: 1.5599e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 6s - loss: 11.3624 - mse: 11.3624 - mae: 1.4038 - val_loss: 10.6989 - val_mse: 10.6989 - val_mae: 1.4379 - lr: 1.5599e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 6s - loss: 11.1170 - mse: 11.1170 - mae: 1.3945 - val_loss: 10.9786 - val_mse: 10.9786 - val_mae: 1.4858 - lr: 1.5599e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 13/100\n",
            "1000/1000 - 6s - loss: 10.8697 - mse: 10.8697 - mae: 1.3849 - val_loss: 12.8600 - val_mse: 12.8600 - val_mae: 1.5445 - lr: 1.5599e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 14/100\n",
            "1000/1000 - 6s - loss: 10.6731 - mse: 10.6731 - mae: 1.3726 - val_loss: 11.1357 - val_mse: 11.1357 - val_mae: 1.4691 - lr: 1.5599e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 15/100\n",
            "1000/1000 - 6s - loss: 10.6653 - mse: 10.6653 - mae: 1.3606 - val_loss: 11.0119 - val_mse: 11.0119 - val_mae: 1.4430 - lr: 1.5599e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 16/100\n",
            "1000/1000 - 6s - loss: 10.4371 - mse: 10.4371 - mae: 1.3475 - val_loss: 10.7612 - val_mse: 10.7612 - val_mae: 1.4575 - lr: 1.5599e-04 - 6s/epoch - 6ms/step\n",
            "Score for fold 1: loss of 10.761215209960938\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 6s - loss: 11.1367 - mse: 11.1367 - mae: 1.3804 - val_loss: 9.1816 - val_mse: 9.1816 - val_mae: 1.3401 - lr: 1.5599e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 6s - loss: 10.7817 - mse: 10.7817 - mae: 1.3599 - val_loss: 9.0548 - val_mse: 9.0548 - val_mae: 1.3156 - lr: 1.5599e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 6s - loss: 10.4210 - mse: 10.4210 - mae: 1.3430 - val_loss: 9.0896 - val_mse: 9.0896 - val_mae: 1.4247 - lr: 1.5599e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 6s - loss: 10.2465 - mse: 10.2465 - mae: 1.3290 - val_loss: 9.0063 - val_mse: 9.0063 - val_mae: 1.3547 - lr: 1.5599e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 7s - loss: 9.7337 - mse: 9.7337 - mae: 1.3138 - val_loss: 9.1444 - val_mse: 9.1444 - val_mae: 1.3639 - lr: 1.5599e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 6s - loss: 9.4971 - mse: 9.4971 - mae: 1.2948 - val_loss: 9.1982 - val_mse: 9.1982 - val_mae: 1.3330 - lr: 1.5599e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 6s - loss: 9.4587 - mse: 9.4587 - mae: 1.2846 - val_loss: 9.4136 - val_mse: 9.4136 - val_mae: 1.3282 - lr: 1.5599e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 6s - loss: 9.2079 - mse: 9.2079 - mae: 1.2646 - val_loss: 9.2328 - val_mse: 9.2328 - val_mae: 1.3983 - lr: 1.5599e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 6s - loss: 8.8882 - mse: 8.8882 - mae: 1.2475 - val_loss: 9.5295 - val_mse: 9.5295 - val_mae: 1.3370 - lr: 1.5599e-04 - 6s/epoch - 6ms/step\n",
            "Score for fold 2: loss of 9.529481887817383\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 6s - loss: 9.6902 - mse: 9.6902 - mae: 1.2916 - val_loss: 6.1389 - val_mse: 6.1389 - val_mae: 1.2248 - lr: 1.5599e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 6s - loss: 9.4689 - mse: 9.4689 - mae: 1.2671 - val_loss: 6.5800 - val_mse: 6.5800 - val_mae: 1.2784 - lr: 1.5599e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 6s - loss: 9.0017 - mse: 9.0017 - mae: 1.2541 - val_loss: 7.0574 - val_mse: 7.0574 - val_mae: 1.2721 - lr: 1.5599e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 6s - loss: 8.8534 - mse: 8.8534 - mae: 1.2353 - val_loss: 6.9759 - val_mse: 6.9759 - val_mae: 1.3302 - lr: 1.5599e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 6s - loss: 8.5953 - mse: 8.5953 - mae: 1.2182 - val_loss: 7.4809 - val_mse: 7.4809 - val_mae: 1.3418 - lr: 1.5599e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 6s - loss: 8.3133 - mse: 8.3133 - mae: 1.1999 - val_loss: 7.2996 - val_mse: 7.2996 - val_mae: 1.3246 - lr: 1.5599e-04 - 6s/epoch - 6ms/step\n",
            "Score for fold 3: loss of 7.29962158203125\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 6s - loss: 7.1584 - mse: 7.1584 - mae: 1.2300 - val_loss: 13.0166 - val_mse: 13.0166 - val_mae: 1.1390 - lr: 1.5599e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 6s - loss: 6.5553 - mse: 6.5553 - mae: 1.2068 - val_loss: 13.0832 - val_mse: 13.0832 - val_mae: 1.2694 - lr: 1.5599e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 6s - loss: 6.3880 - mse: 6.3880 - mae: 1.1929 - val_loss: 13.8838 - val_mse: 13.8838 - val_mae: 1.2745 - lr: 1.5599e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 6s - loss: 6.1087 - mse: 6.1087 - mae: 1.1743 - val_loss: 13.9039 - val_mse: 13.9039 - val_mae: 1.2501 - lr: 1.5599e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 6s - loss: 5.8375 - mse: 5.8375 - mae: 1.1572 - val_loss: 15.1273 - val_mse: 15.1273 - val_mae: 1.2870 - lr: 1.5599e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 6s - loss: 5.5587 - mse: 5.5587 - mae: 1.1393 - val_loss: 14.2437 - val_mse: 14.2437 - val_mae: 1.2643 - lr: 1.5599e-04 - 6s/epoch - 6ms/step\n",
            "Score for fold 4: loss of 14.243728637695312\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 6s - loss: 7.9133 - mse: 7.9133 - mae: 1.1744 - val_loss: 4.5593 - val_mse: 4.5593 - val_mae: 1.1479 - lr: 1.5599e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 6s - loss: 7.8249 - mse: 7.8249 - mae: 1.1539 - val_loss: 4.8103 - val_mse: 4.8103 - val_mae: 1.1142 - lr: 1.5599e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 6s - loss: 7.3722 - mse: 7.3722 - mae: 1.1353 - val_loss: 4.4821 - val_mse: 4.4821 - val_mae: 1.1154 - lr: 1.5599e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 6s - loss: 7.2728 - mse: 7.2728 - mae: 1.1206 - val_loss: 4.4594 - val_mse: 4.4594 - val_mae: 1.1775 - lr: 1.5599e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 6s - loss: 6.9753 - mse: 6.9753 - mae: 1.1054 - val_loss: 4.8390 - val_mse: 4.8390 - val_mae: 1.1701 - lr: 1.5599e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 6s - loss: 6.6259 - mse: 6.6259 - mae: 1.0958 - val_loss: 4.9613 - val_mse: 4.9613 - val_mae: 1.1832 - lr: 1.5599e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 6s - loss: 6.6245 - mse: 6.6245 - mae: 1.0857 - val_loss: 5.3684 - val_mse: 5.3684 - val_mae: 1.1518 - lr: 1.5599e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 6s - loss: 6.6473 - mse: 6.6473 - mae: 1.0716 - val_loss: 5.1613 - val_mse: 5.1613 - val_mae: 1.1915 - lr: 1.5599e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 6s - loss: 6.4994 - mse: 6.4994 - mae: 1.0610 - val_loss: 5.6833 - val_mse: 5.6833 - val_mae: 1.2685 - lr: 1.5599e-04 - 6s/epoch - 6ms/step\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-10-11 14:20:02,788]\u001b[0m Finished trial#4 resulted in value: 9.501999999999999. Current best value is 9.501999999999999 with parameters: {'activation': 'relu', 'num_hidden_layer': 4, 'i': 9, 'learning_rate': 0.00015599216930932077}.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Score for fold 5: loss of 5.6833038330078125\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 8s - loss: 15.6131 - mse: 15.6131 - mae: 1.6267 - val_loss: 9.9985 - val_mse: 9.9985 - val_mae: 1.5778 - lr: 8.3975e-04 - 8s/epoch - 8ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 7s - loss: 15.0388 - mse: 15.0388 - mae: 1.5503 - val_loss: 10.0111 - val_mse: 10.0111 - val_mae: 1.5192 - lr: 8.3975e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 7s - loss: 14.8896 - mse: 14.8896 - mae: 1.5459 - val_loss: 9.5781 - val_mse: 9.5781 - val_mae: 1.5993 - lr: 8.3975e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 7s - loss: 14.6320 - mse: 14.6320 - mae: 1.5257 - val_loss: 10.1630 - val_mse: 10.1630 - val_mae: 1.4553 - lr: 8.3975e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 7s - loss: 14.5723 - mse: 14.5723 - mae: 1.5183 - val_loss: 9.2028 - val_mse: 9.2028 - val_mae: 1.5546 - lr: 8.3975e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 7s - loss: 14.4723 - mse: 14.4723 - mae: 1.5140 - val_loss: 9.4468 - val_mse: 9.4468 - val_mae: 1.5220 - lr: 8.3975e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 7s - loss: 14.3683 - mse: 14.3683 - mae: 1.5121 - val_loss: 9.0425 - val_mse: 9.0425 - val_mae: 1.4927 - lr: 8.3975e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 7s - loss: 14.1737 - mse: 14.1737 - mae: 1.5097 - val_loss: 9.0825 - val_mse: 9.0825 - val_mae: 1.5734 - lr: 8.3975e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 7s - loss: 14.0944 - mse: 14.0944 - mae: 1.5079 - val_loss: 9.1783 - val_mse: 9.1783 - val_mae: 1.4982 - lr: 8.3975e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 7s - loss: 14.0647 - mse: 14.0647 - mae: 1.5122 - val_loss: 9.1433 - val_mse: 9.1433 - val_mae: 1.4522 - lr: 8.3975e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 7s - loss: 13.9946 - mse: 13.9946 - mae: 1.5100 - val_loss: 9.0113 - val_mse: 9.0113 - val_mae: 1.4825 - lr: 8.3975e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 7s - loss: 13.9770 - mse: 13.9770 - mae: 1.5077 - val_loss: 8.8478 - val_mse: 8.8478 - val_mae: 1.5490 - lr: 8.3975e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 13/100\n",
            "1000/1000 - 7s - loss: 13.9607 - mse: 13.9607 - mae: 1.5030 - val_loss: 9.2896 - val_mse: 9.2896 - val_mae: 1.7528 - lr: 8.3975e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 14/100\n",
            "1000/1000 - 7s - loss: 13.9114 - mse: 13.9114 - mae: 1.5023 - val_loss: 8.8583 - val_mse: 8.8583 - val_mae: 1.4500 - lr: 8.3975e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 15/100\n",
            "1000/1000 - 7s - loss: 13.9020 - mse: 13.9020 - mae: 1.5069 - val_loss: 8.9665 - val_mse: 8.9665 - val_mae: 1.6414 - lr: 8.3975e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 16/100\n",
            "1000/1000 - 7s - loss: 13.7957 - mse: 13.7957 - mae: 1.5009 - val_loss: 9.2091 - val_mse: 9.2091 - val_mae: 1.7998 - lr: 8.3975e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 17/100\n",
            "1000/1000 - 7s - loss: 13.7572 - mse: 13.7572 - mae: 1.4999 - val_loss: 8.8264 - val_mse: 8.8264 - val_mae: 1.4991 - lr: 8.3975e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 18/100\n",
            "1000/1000 - 7s - loss: 13.6856 - mse: 13.6856 - mae: 1.5096 - val_loss: 8.8803 - val_mse: 8.8803 - val_mae: 1.5450 - lr: 8.3975e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 19/100\n",
            "1000/1000 - 7s - loss: 13.7115 - mse: 13.7115 - mae: 1.5129 - val_loss: 8.8187 - val_mse: 8.8187 - val_mae: 1.4375 - lr: 8.3975e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 20/100\n",
            "1000/1000 - 7s - loss: 13.6517 - mse: 13.6517 - mae: 1.5047 - val_loss: 8.7081 - val_mse: 8.7081 - val_mae: 1.5250 - lr: 8.3975e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 21/100\n",
            "1000/1000 - 7s - loss: 13.5460 - mse: 13.5460 - mae: 1.4962 - val_loss: 8.8485 - val_mse: 8.8485 - val_mae: 1.6016 - lr: 8.3975e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 22/100\n",
            "1000/1000 - 7s - loss: 13.5199 - mse: 13.5199 - mae: 1.4990 - val_loss: 8.7196 - val_mse: 8.7196 - val_mae: 1.5444 - lr: 8.3975e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 23/100\n",
            "1000/1000 - 7s - loss: 13.6403 - mse: 13.6403 - mae: 1.4961 - val_loss: 8.8189 - val_mse: 8.8189 - val_mae: 1.4501 - lr: 8.3975e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 24/100\n",
            "1000/1000 - 7s - loss: 13.4636 - mse: 13.4636 - mae: 1.4892 - val_loss: 8.7931 - val_mse: 8.7931 - val_mae: 1.4627 - lr: 8.3975e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 25/100\n",
            "1000/1000 - 7s - loss: 13.4482 - mse: 13.4482 - mae: 1.4907 - val_loss: 8.8032 - val_mse: 8.8032 - val_mae: 1.4649 - lr: 8.3975e-04 - 7s/epoch - 7ms/step\n",
            "Score for fold 1: loss of 8.803165435791016\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 7s - loss: 12.0389 - mse: 12.0389 - mae: 1.5105 - val_loss: 13.7823 - val_mse: 13.7823 - val_mae: 1.4613 - lr: 8.3975e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 7s - loss: 12.0389 - mse: 12.0389 - mae: 1.4974 - val_loss: 14.3666 - val_mse: 14.3666 - val_mae: 1.4145 - lr: 8.3975e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 7s - loss: 11.9803 - mse: 11.9803 - mae: 1.4945 - val_loss: 14.7666 - val_mse: 14.7666 - val_mae: 1.7495 - lr: 8.3975e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 7s - loss: 11.9788 - mse: 11.9788 - mae: 1.5019 - val_loss: 14.4959 - val_mse: 14.4959 - val_mae: 1.4437 - lr: 8.3975e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 7s - loss: 11.9625 - mse: 11.9625 - mae: 1.4946 - val_loss: 14.3756 - val_mse: 14.3756 - val_mae: 1.4139 - lr: 8.3975e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 7s - loss: 11.8994 - mse: 11.8994 - mae: 1.4967 - val_loss: 14.8657 - val_mse: 14.8657 - val_mae: 1.6220 - lr: 8.3975e-04 - 7s/epoch - 7ms/step\n",
            "Score for fold 2: loss of 14.865710258483887\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 7s - loss: 12.5663 - mse: 12.5663 - mae: 1.4955 - val_loss: 11.8782 - val_mse: 11.8782 - val_mae: 1.5624 - lr: 8.3975e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 7s - loss: 12.5287 - mse: 12.5287 - mae: 1.4943 - val_loss: 11.7846 - val_mse: 11.7846 - val_mae: 1.4492 - lr: 8.3975e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 7s - loss: 12.4064 - mse: 12.4064 - mae: 1.4859 - val_loss: 11.9197 - val_mse: 11.9197 - val_mae: 1.4725 - lr: 8.3975e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 7s - loss: 12.2857 - mse: 12.2857 - mae: 1.4796 - val_loss: 11.9727 - val_mse: 11.9727 - val_mae: 1.4459 - lr: 8.3975e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 7s - loss: 12.2796 - mse: 12.2796 - mae: 1.4782 - val_loss: 12.0695 - val_mse: 12.0695 - val_mae: 1.5695 - lr: 8.3975e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 7s - loss: 12.3358 - mse: 12.3358 - mae: 1.4951 - val_loss: 12.6495 - val_mse: 12.6495 - val_mae: 1.4899 - lr: 8.3975e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 7s - loss: 12.2298 - mse: 12.2298 - mae: 1.5004 - val_loss: 12.1090 - val_mse: 12.1090 - val_mae: 1.4306 - lr: 8.3975e-04 - 7s/epoch - 7ms/step\n",
            "Score for fold 3: loss of 12.108981132507324\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 7s - loss: 10.1773 - mse: 10.1773 - mae: 1.4866 - val_loss: 19.9398 - val_mse: 19.9398 - val_mae: 1.4637 - lr: 8.3975e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 7s - loss: 10.2045 - mse: 10.2045 - mae: 1.4861 - val_loss: 20.1323 - val_mse: 20.1323 - val_mae: 1.4717 - lr: 8.3975e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 7s - loss: 10.1563 - mse: 10.1563 - mae: 1.4678 - val_loss: 20.2144 - val_mse: 20.2144 - val_mae: 1.5014 - lr: 8.3975e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 7s - loss: 10.0900 - mse: 10.0900 - mae: 1.4716 - val_loss: 20.2090 - val_mse: 20.2090 - val_mae: 1.4937 - lr: 8.3975e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 7s - loss: 10.1245 - mse: 10.1245 - mae: 1.4655 - val_loss: 20.4197 - val_mse: 20.4197 - val_mae: 1.4801 - lr: 8.3975e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 7s - loss: 9.9386 - mse: 9.9386 - mae: 1.4579 - val_loss: 20.5063 - val_mse: 20.5063 - val_mae: 1.8205 - lr: 8.3975e-04 - 7s/epoch - 7ms/step\n",
            "Score for fold 4: loss of 20.506349563598633\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 7s - loss: 13.3177 - mse: 13.3177 - mae: 1.4856 - val_loss: 6.9584 - val_mse: 6.9584 - val_mae: 1.5062 - lr: 8.3975e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 7s - loss: 13.2475 - mse: 13.2475 - mae: 1.4865 - val_loss: 7.1612 - val_mse: 7.1612 - val_mae: 1.4673 - lr: 8.3975e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 7s - loss: 13.1547 - mse: 13.1547 - mae: 1.4822 - val_loss: 7.1187 - val_mse: 7.1187 - val_mae: 1.4460 - lr: 8.3975e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 7s - loss: 12.9784 - mse: 12.9784 - mae: 1.4752 - val_loss: 7.2784 - val_mse: 7.2784 - val_mae: 1.3611 - lr: 8.3975e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 7s - loss: 13.0765 - mse: 13.0765 - mae: 1.4796 - val_loss: 7.7082 - val_mse: 7.7082 - val_mae: 1.3942 - lr: 8.3975e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 7s - loss: 12.9589 - mse: 12.9589 - mae: 1.4711 - val_loss: 7.1340 - val_mse: 7.1340 - val_mae: 1.4727 - lr: 8.3975e-04 - 7s/epoch - 7ms/step\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-10-11 14:26:01,693]\u001b[0m Finished trial#5 resulted in value: 12.684000000000001. Current best value is 9.501999999999999 with parameters: {'activation': 'relu', 'num_hidden_layer': 4, 'i': 9, 'learning_rate': 0.00015599216930932077}.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Score for fold 5: loss of 7.134038925170898\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 3s - loss: 14.9987 - mse: 14.9987 - mae: 1.6509 - val_loss: 11.9300 - val_mse: 11.9300 - val_mae: 1.5992 - lr: 0.0053 - 3s/epoch - 3ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 14.3897 - mse: 14.3897 - mae: 1.6121 - val_loss: 10.5861 - val_mse: 10.5861 - val_mae: 1.6051 - lr: 0.0053 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 14.3163 - mse: 14.3163 - mae: 1.6112 - val_loss: 10.7708 - val_mse: 10.7708 - val_mae: 1.5426 - lr: 0.0053 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 14.1771 - mse: 14.1771 - mae: 1.6071 - val_loss: 10.8205 - val_mse: 10.8205 - val_mae: 1.5347 - lr: 0.0053 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 14.4664 - mse: 14.4664 - mae: 1.6064 - val_loss: 10.8509 - val_mse: 10.8509 - val_mae: 1.5057 - lr: 0.0053 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 14.3822 - mse: 14.3822 - mae: 1.6100 - val_loss: 10.7680 - val_mse: 10.7680 - val_mae: 1.5063 - lr: 0.0053 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 14.0485 - mse: 14.0485 - mae: 1.6068 - val_loss: 11.8049 - val_mse: 11.8049 - val_mae: 1.5587 - lr: 0.0053 - 2s/epoch - 2ms/step\n",
            "Score for fold 1: loss of 11.80493450164795\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 13.4452 - mse: 13.4452 - mae: 1.5624 - val_loss: 13.0255 - val_mse: 13.0255 - val_mae: 1.5275 - lr: 0.0011 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 13.4493 - mse: 13.4493 - mae: 1.5577 - val_loss: 13.1100 - val_mse: 13.1100 - val_mae: 1.5252 - lr: 0.0011 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 13.2886 - mse: 13.2886 - mae: 1.5533 - val_loss: 12.9253 - val_mse: 12.9253 - val_mae: 1.5107 - lr: 0.0011 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 13.2972 - mse: 13.2972 - mae: 1.5538 - val_loss: 12.9532 - val_mse: 12.9532 - val_mae: 1.5725 - lr: 0.0011 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 13.3807 - mse: 13.3807 - mae: 1.5577 - val_loss: 12.9204 - val_mse: 12.9204 - val_mae: 1.5729 - lr: 0.0011 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 13.3470 - mse: 13.3470 - mae: 1.5652 - val_loss: 12.8864 - val_mse: 12.8864 - val_mae: 1.5326 - lr: 0.0011 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 13.3465 - mse: 13.3465 - mae: 1.5595 - val_loss: 13.0055 - val_mse: 13.0055 - val_mae: 1.5327 - lr: 0.0011 - 2s/epoch - 2ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 2s - loss: 13.2550 - mse: 13.2550 - mae: 1.5541 - val_loss: 12.8942 - val_mse: 12.8942 - val_mae: 1.5571 - lr: 0.0011 - 2s/epoch - 2ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 2s - loss: 13.2845 - mse: 13.2845 - mae: 1.5624 - val_loss: 13.0989 - val_mse: 13.0989 - val_mae: 1.5434 - lr: 0.0011 - 2s/epoch - 2ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 2s - loss: 13.3929 - mse: 13.3929 - mae: 1.5571 - val_loss: 12.9451 - val_mse: 12.9451 - val_mae: 1.5602 - lr: 0.0011 - 2s/epoch - 2ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 2s - loss: 13.2607 - mse: 13.2607 - mae: 1.5588 - val_loss: 12.9019 - val_mse: 12.9019 - val_mae: 1.5131 - lr: 0.0011 - 2s/epoch - 2ms/step\n",
            "Score for fold 2: loss of 12.901945114135742\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 13.2253 - mse: 13.2253 - mae: 1.5404 - val_loss: 13.8237 - val_mse: 13.8237 - val_mae: 1.5956 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 13.1500 - mse: 13.1500 - mae: 1.5397 - val_loss: 13.8382 - val_mse: 13.8382 - val_mae: 1.5655 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 13.0998 - mse: 13.0998 - mae: 1.5399 - val_loss: 13.8332 - val_mse: 13.8332 - val_mae: 1.6040 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 13.2034 - mse: 13.2034 - mae: 1.5329 - val_loss: 13.9671 - val_mse: 13.9671 - val_mae: 1.6224 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 13.2175 - mse: 13.2175 - mae: 1.5385 - val_loss: 13.8496 - val_mse: 13.8496 - val_mae: 1.5932 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 13.0448 - mse: 13.0448 - mae: 1.5345 - val_loss: 13.8710 - val_mse: 13.8710 - val_mae: 1.6005 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Score for fold 3: loss of 13.87099838256836\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 12.1579 - mse: 12.1579 - mae: 1.5555 - val_loss: 17.9336 - val_mse: 17.9336 - val_mae: 1.5183 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 12.0600 - mse: 12.0600 - mae: 1.5529 - val_loss: 17.8999 - val_mse: 17.8999 - val_mae: 1.5065 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 12.1217 - mse: 12.1217 - mae: 1.5542 - val_loss: 17.8839 - val_mse: 17.8839 - val_mae: 1.5175 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 12.0897 - mse: 12.0897 - mae: 1.5519 - val_loss: 17.9129 - val_mse: 17.9129 - val_mae: 1.5311 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 12.0114 - mse: 12.0114 - mae: 1.5543 - val_loss: 17.8904 - val_mse: 17.8904 - val_mae: 1.5203 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 12.0861 - mse: 12.0861 - mae: 1.5512 - val_loss: 18.1095 - val_mse: 18.1095 - val_mae: 1.5132 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 11.9827 - mse: 11.9827 - mae: 1.5535 - val_loss: 17.8501 - val_mse: 17.8501 - val_mae: 1.5590 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 2s - loss: 12.0239 - mse: 12.0239 - mae: 1.5515 - val_loss: 17.9985 - val_mse: 17.9985 - val_mae: 1.6037 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 2s - loss: 12.0324 - mse: 12.0324 - mae: 1.5553 - val_loss: 18.0399 - val_mse: 18.0399 - val_mae: 1.5690 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 2s - loss: 12.0623 - mse: 12.0623 - mae: 1.5532 - val_loss: 18.1030 - val_mse: 18.1030 - val_mae: 1.6322 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 2s - loss: 12.0690 - mse: 12.0690 - mae: 1.5482 - val_loss: 17.8843 - val_mse: 17.8843 - val_mae: 1.5772 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 2s - loss: 12.0156 - mse: 12.0156 - mae: 1.5528 - val_loss: 17.9520 - val_mse: 17.9520 - val_mae: 1.6105 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Score for fold 4: loss of 17.952037811279297\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 13.8001 - mse: 13.8001 - mae: 1.5511 - val_loss: 10.8292 - val_mse: 10.8292 - val_mae: 1.5453 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 13.8289 - mse: 13.8289 - mae: 1.5468 - val_loss: 11.0045 - val_mse: 11.0045 - val_mae: 1.5426 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 13.8978 - mse: 13.8978 - mae: 1.5468 - val_loss: 10.8717 - val_mse: 10.8717 - val_mae: 1.5684 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 13.8564 - mse: 13.8564 - mae: 1.5509 - val_loss: 10.8192 - val_mse: 10.8192 - val_mae: 1.5733 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 13.7646 - mse: 13.7646 - mae: 1.5453 - val_loss: 10.8644 - val_mse: 10.8644 - val_mae: 1.5557 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 13.7713 - mse: 13.7713 - mae: 1.5450 - val_loss: 10.9108 - val_mse: 10.9108 - val_mae: 1.5315 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 13.8699 - mse: 13.8699 - mae: 1.5457 - val_loss: 10.8927 - val_mse: 10.8927 - val_mae: 1.6183 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 2s - loss: 13.8174 - mse: 13.8174 - mae: 1.5463 - val_loss: 10.8525 - val_mse: 10.8525 - val_mae: 1.5517 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 2s - loss: 13.6982 - mse: 13.6982 - mae: 1.5437 - val_loss: 11.2954 - val_mse: 11.2954 - val_mae: 1.5261 - lr: 0.0010 - 2s/epoch - 2ms/step\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-10-11 14:27:36,143]\u001b[0m Finished trial#6 resulted in value: 13.563999999999998. Current best value is 9.501999999999999 with parameters: {'activation': 'relu', 'num_hidden_layer': 4, 'i': 9, 'learning_rate': 0.00015599216930932077}.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Score for fold 5: loss of 11.295406341552734\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 15.8474 - mse: 15.8474 - mae: 1.5968 - val_loss: 10.6785 - val_mse: 10.6785 - val_mae: 1.4510 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 14.7254 - mse: 14.7254 - mae: 1.5201 - val_loss: 10.3471 - val_mse: 10.3471 - val_mae: 1.4843 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 14.4178 - mse: 14.4178 - mae: 1.5081 - val_loss: 10.1916 - val_mse: 10.1916 - val_mae: 1.4604 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 14.2197 - mse: 14.2197 - mae: 1.4970 - val_loss: 10.2002 - val_mse: 10.2002 - val_mae: 1.5493 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 14.0864 - mse: 14.0864 - mae: 1.4963 - val_loss: 10.0372 - val_mse: 10.0372 - val_mae: 1.4462 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 13.9328 - mse: 13.9328 - mae: 1.4879 - val_loss: 10.0683 - val_mse: 10.0683 - val_mae: 1.4887 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 13.8592 - mse: 13.8592 - mae: 1.4880 - val_loss: 9.9524 - val_mse: 9.9524 - val_mae: 1.4340 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 2s - loss: 13.7549 - mse: 13.7549 - mae: 1.4786 - val_loss: 10.0132 - val_mse: 10.0132 - val_mae: 1.4525 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 2s - loss: 13.6654 - mse: 13.6654 - mae: 1.4766 - val_loss: 9.8609 - val_mse: 9.8609 - val_mae: 1.4845 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 2s - loss: 13.5966 - mse: 13.5966 - mae: 1.4767 - val_loss: 9.8475 - val_mse: 9.8475 - val_mae: 1.4389 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 2s - loss: 13.4982 - mse: 13.4982 - mae: 1.4731 - val_loss: 9.9397 - val_mse: 9.9397 - val_mae: 1.5087 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 2s - loss: 13.4516 - mse: 13.4516 - mae: 1.4704 - val_loss: 9.8459 - val_mse: 9.8459 - val_mae: 1.4635 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 13/100\n",
            "1000/1000 - 2s - loss: 13.3910 - mse: 13.3910 - mae: 1.4660 - val_loss: 9.9730 - val_mse: 9.9730 - val_mae: 1.4987 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 14/100\n",
            "1000/1000 - 2s - loss: 13.3160 - mse: 13.3160 - mae: 1.4630 - val_loss: 9.9025 - val_mse: 9.9025 - val_mae: 1.4807 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 15/100\n",
            "1000/1000 - 2s - loss: 13.2442 - mse: 13.2442 - mae: 1.4624 - val_loss: 9.8219 - val_mse: 9.8219 - val_mae: 1.4855 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 16/100\n",
            "1000/1000 - 2s - loss: 13.1901 - mse: 13.1901 - mae: 1.4605 - val_loss: 9.9428 - val_mse: 9.9428 - val_mae: 1.5094 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 17/100\n",
            "1000/1000 - 2s - loss: 13.1410 - mse: 13.1410 - mae: 1.4586 - val_loss: 9.7731 - val_mse: 9.7731 - val_mae: 1.4585 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 18/100\n",
            "1000/1000 - 2s - loss: 13.1216 - mse: 13.1216 - mae: 1.4596 - val_loss: 9.8247 - val_mse: 9.8247 - val_mae: 1.5014 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 19/100\n",
            "1000/1000 - 2s - loss: 13.0436 - mse: 13.0436 - mae: 1.4569 - val_loss: 9.8528 - val_mse: 9.8528 - val_mae: 1.4974 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 20/100\n",
            "1000/1000 - 2s - loss: 13.0212 - mse: 13.0212 - mae: 1.4558 - val_loss: 9.9025 - val_mse: 9.9025 - val_mae: 1.4869 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 21/100\n",
            "1000/1000 - 2s - loss: 12.9339 - mse: 12.9339 - mae: 1.4534 - val_loss: 9.8922 - val_mse: 9.8922 - val_mae: 1.4607 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 22/100\n",
            "1000/1000 - 2s - loss: 12.9731 - mse: 12.9731 - mae: 1.4463 - val_loss: 9.7840 - val_mse: 9.7840 - val_mae: 1.5130 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Score for fold 1: loss of 9.784017562866211\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 9.9604 - mse: 9.9604 - mae: 1.4419 - val_loss: 21.2824 - val_mse: 21.2824 - val_mae: 1.4854 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 9.9200 - mse: 9.9200 - mae: 1.4347 - val_loss: 21.3982 - val_mse: 21.3982 - val_mae: 1.4912 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 9.8660 - mse: 9.8660 - mae: 1.4299 - val_loss: 21.4439 - val_mse: 21.4439 - val_mae: 1.4741 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 9.8233 - mse: 9.8233 - mae: 1.4285 - val_loss: 21.6341 - val_mse: 21.6341 - val_mae: 1.5615 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 9.7742 - mse: 9.7742 - mae: 1.4252 - val_loss: 21.4269 - val_mse: 21.4269 - val_mae: 1.5309 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 9.7545 - mse: 9.7545 - mae: 1.4271 - val_loss: 21.5435 - val_mse: 21.5435 - val_mae: 1.5274 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Score for fold 2: loss of 21.54349136352539\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 12.9826 - mse: 12.9826 - mae: 1.4520 - val_loss: 8.4252 - val_mse: 8.4252 - val_mae: 1.3982 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 12.9530 - mse: 12.9530 - mae: 1.4481 - val_loss: 8.4789 - val_mse: 8.4789 - val_mae: 1.4051 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 12.8899 - mse: 12.8899 - mae: 1.4460 - val_loss: 8.5146 - val_mse: 8.5146 - val_mae: 1.3987 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 12.8248 - mse: 12.8248 - mae: 1.4425 - val_loss: 8.6824 - val_mse: 8.6824 - val_mae: 1.3897 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 12.8193 - mse: 12.8193 - mae: 1.4429 - val_loss: 8.5975 - val_mse: 8.5975 - val_mae: 1.4329 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 12.7810 - mse: 12.7810 - mae: 1.4410 - val_loss: 8.6300 - val_mse: 8.6300 - val_mae: 1.4354 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Score for fold 3: loss of 8.629950523376465\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 11.9843 - mse: 11.9843 - mae: 1.4393 - val_loss: 11.8448 - val_mse: 11.8448 - val_mae: 1.4328 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 11.9179 - mse: 11.9179 - mae: 1.4335 - val_loss: 11.9351 - val_mse: 11.9351 - val_mae: 1.4314 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 11.8749 - mse: 11.8749 - mae: 1.4322 - val_loss: 11.9073 - val_mse: 11.9073 - val_mae: 1.4240 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 11.8142 - mse: 11.8142 - mae: 1.4288 - val_loss: 11.8399 - val_mse: 11.8399 - val_mae: 1.4390 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 11.7798 - mse: 11.7798 - mae: 1.4286 - val_loss: 12.0209 - val_mse: 12.0209 - val_mae: 1.4196 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 11.7062 - mse: 11.7062 - mae: 1.4249 - val_loss: 12.1698 - val_mse: 12.1698 - val_mae: 1.4041 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 11.7028 - mse: 11.7028 - mae: 1.4234 - val_loss: 12.0008 - val_mse: 12.0008 - val_mae: 1.4329 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 2s - loss: 11.6515 - mse: 11.6515 - mae: 1.4203 - val_loss: 12.0880 - val_mse: 12.0880 - val_mae: 1.4692 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 2s - loss: 11.6085 - mse: 11.6085 - mae: 1.4206 - val_loss: 12.0521 - val_mse: 12.0521 - val_mae: 1.4636 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Score for fold 4: loss of 12.05213451385498\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 12.4662 - mse: 12.4662 - mae: 1.4392 - val_loss: 8.7201 - val_mse: 8.7201 - val_mae: 1.3640 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 12.3615 - mse: 12.3615 - mae: 1.4321 - val_loss: 8.6536 - val_mse: 8.6536 - val_mae: 1.4233 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 12.3372 - mse: 12.3372 - mae: 1.4278 - val_loss: 8.8956 - val_mse: 8.8956 - val_mae: 1.4270 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 12.2791 - mse: 12.2791 - mae: 1.4294 - val_loss: 8.8312 - val_mse: 8.8312 - val_mae: 1.4620 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 12.2580 - mse: 12.2580 - mae: 1.4226 - val_loss: 8.8432 - val_mse: 8.8432 - val_mae: 1.3926 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 12.2057 - mse: 12.2057 - mae: 1.4197 - val_loss: 8.7872 - val_mse: 8.7872 - val_mae: 1.4203 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 12.1412 - mse: 12.1412 - mae: 1.4168 - val_loss: 8.8405 - val_mse: 8.8405 - val_mae: 1.4565 - lr: 0.0010 - 2s/epoch - 2ms/step\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-10-11 14:29:03,836]\u001b[0m Finished trial#7 resulted in value: 12.168000000000001. Current best value is 9.501999999999999 with parameters: {'activation': 'relu', 'num_hidden_layer': 4, 'i': 9, 'learning_rate': 0.00015599216930932077}.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Score for fold 5: loss of 8.840530395507812\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 4s - loss: 17.9465 - mse: 17.9465 - mae: 1.8666 - val_loss: 15.3549 - val_mse: 15.3549 - val_mae: 1.9084 - lr: 0.0097 - 4s/epoch - 4ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 4s - loss: 16.0702 - mse: 16.0702 - mae: 1.8006 - val_loss: 17.8398 - val_mse: 17.8398 - val_mae: 1.9589 - lr: 0.0097 - 4s/epoch - 4ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 3s - loss: 15.9149 - mse: 15.9149 - mae: 1.7703 - val_loss: 19.0450 - val_mse: 19.0450 - val_mae: 1.7700 - lr: 0.0097 - 3s/epoch - 3ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 3s - loss: 15.4283 - mse: 15.4283 - mae: 1.7943 - val_loss: 13.6130 - val_mse: 13.6130 - val_mae: 1.7298 - lr: 0.0097 - 3s/epoch - 3ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 3s - loss: 15.2634 - mse: 15.2634 - mae: 1.7549 - val_loss: 14.7241 - val_mse: 14.7241 - val_mae: 1.7412 - lr: 0.0097 - 3s/epoch - 3ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 3s - loss: 16.3506 - mse: 16.3506 - mae: 1.7620 - val_loss: 20.4892 - val_mse: 20.4892 - val_mae: 1.8216 - lr: 0.0097 - 3s/epoch - 3ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 3s - loss: 14.9180 - mse: 14.9180 - mae: 1.7306 - val_loss: 13.8226 - val_mse: 13.8226 - val_mae: 1.6528 - lr: 0.0097 - 3s/epoch - 3ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 3s - loss: 14.9319 - mse: 14.9319 - mae: 1.7519 - val_loss: 20.0339 - val_mse: 20.0339 - val_mae: 2.3066 - lr: 0.0097 - 3s/epoch - 3ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 3s - loss: 15.6386 - mse: 15.6386 - mae: 1.7788 - val_loss: 18.5728 - val_mse: 18.5728 - val_mae: 1.7039 - lr: 0.0097 - 3s/epoch - 3ms/step\n",
            "Score for fold 1: loss of 18.572795867919922\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 3s - loss: 14.1370 - mse: 14.1370 - mae: 1.5723 - val_loss: 12.6895 - val_mse: 12.6895 - val_mae: 1.6294 - lr: 0.0019 - 3s/epoch - 3ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 3s - loss: 14.1605 - mse: 14.1605 - mae: 1.5711 - val_loss: 10.9262 - val_mse: 10.9262 - val_mae: 1.5588 - lr: 0.0019 - 3s/epoch - 3ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 3s - loss: 14.2926 - mse: 14.2926 - mae: 1.5725 - val_loss: 10.0317 - val_mse: 10.0317 - val_mae: 1.5369 - lr: 0.0019 - 3s/epoch - 3ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 3s - loss: 14.3152 - mse: 14.3152 - mae: 1.5751 - val_loss: 10.2434 - val_mse: 10.2434 - val_mae: 1.5011 - lr: 0.0019 - 3s/epoch - 3ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 3s - loss: 14.2325 - mse: 14.2325 - mae: 1.5756 - val_loss: 10.2637 - val_mse: 10.2637 - val_mae: 1.5202 - lr: 0.0019 - 3s/epoch - 3ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 3s - loss: 14.1548 - mse: 14.1548 - mae: 1.5742 - val_loss: 10.0402 - val_mse: 10.0402 - val_mae: 1.5409 - lr: 0.0019 - 3s/epoch - 3ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 3s - loss: 14.0654 - mse: 14.0654 - mae: 1.5709 - val_loss: 10.3434 - val_mse: 10.3434 - val_mae: 1.5266 - lr: 0.0019 - 3s/epoch - 3ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 3s - loss: 14.1108 - mse: 14.1108 - mae: 1.5727 - val_loss: 10.2117 - val_mse: 10.2117 - val_mae: 1.5147 - lr: 0.0019 - 3s/epoch - 3ms/step\n",
            "Score for fold 2: loss of 10.211701393127441\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 3s - loss: 13.6532 - mse: 13.6532 - mae: 1.5521 - val_loss: 11.8886 - val_mse: 11.8886 - val_mae: 1.5369 - lr: 0.0010 - 3s/epoch - 3ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 3s - loss: 13.5767 - mse: 13.5767 - mae: 1.5502 - val_loss: 11.8882 - val_mse: 11.8882 - val_mae: 1.5318 - lr: 0.0010 - 3s/epoch - 3ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 4s - loss: 13.6086 - mse: 13.6086 - mae: 1.5476 - val_loss: 11.9541 - val_mse: 11.9541 - val_mae: 1.5554 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 3s - loss: 13.5717 - mse: 13.5717 - mae: 1.5481 - val_loss: 12.0633 - val_mse: 12.0633 - val_mae: 1.5888 - lr: 0.0010 - 3s/epoch - 3ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 3s - loss: 13.6338 - mse: 13.6338 - mae: 1.5502 - val_loss: 11.8901 - val_mse: 11.8901 - val_mae: 1.5475 - lr: 0.0010 - 3s/epoch - 3ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 3s - loss: 13.5566 - mse: 13.5566 - mae: 1.5538 - val_loss: 11.9024 - val_mse: 11.9024 - val_mae: 1.5600 - lr: 0.0010 - 3s/epoch - 3ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 3s - loss: 13.6575 - mse: 13.6575 - mae: 1.5474 - val_loss: 11.9735 - val_mse: 11.9735 - val_mae: 1.5372 - lr: 0.0010 - 3s/epoch - 3ms/step\n",
            "Score for fold 3: loss of 11.973526954650879\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 3s - loss: 11.7724 - mse: 11.7724 - mae: 1.5493 - val_loss: 19.2010 - val_mse: 19.2010 - val_mae: 1.5907 - lr: 0.0010 - 3s/epoch - 3ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 3s - loss: 11.8274 - mse: 11.8274 - mae: 1.5519 - val_loss: 18.9674 - val_mse: 18.9674 - val_mae: 1.5500 - lr: 0.0010 - 3s/epoch - 3ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 3s - loss: 11.9063 - mse: 11.9063 - mae: 1.5497 - val_loss: 18.9781 - val_mse: 18.9781 - val_mae: 1.5434 - lr: 0.0010 - 3s/epoch - 3ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 3s - loss: 11.9332 - mse: 11.9332 - mae: 1.5572 - val_loss: 19.5610 - val_mse: 19.5610 - val_mae: 1.5982 - lr: 0.0010 - 3s/epoch - 3ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 3s - loss: 11.8069 - mse: 11.8069 - mae: 1.5523 - val_loss: 19.1203 - val_mse: 19.1203 - val_mae: 1.5685 - lr: 0.0010 - 3s/epoch - 3ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 3s - loss: 11.8211 - mse: 11.8211 - mae: 1.5530 - val_loss: 19.0446 - val_mse: 19.0446 - val_mae: 1.5620 - lr: 0.0010 - 3s/epoch - 3ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 3s - loss: 11.8314 - mse: 11.8314 - mae: 1.5529 - val_loss: 19.0487 - val_mse: 19.0487 - val_mae: 1.5476 - lr: 0.0010 - 3s/epoch - 3ms/step\n",
            "Score for fold 4: loss of 19.048688888549805\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 3s - loss: 13.6309 - mse: 13.6309 - mae: 1.5492 - val_loss: 11.9952 - val_mse: 11.9952 - val_mae: 1.5147 - lr: 0.0010 - 3s/epoch - 3ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 3s - loss: 13.5740 - mse: 13.5740 - mae: 1.5465 - val_loss: 11.9042 - val_mse: 11.9042 - val_mae: 1.5540 - lr: 0.0010 - 3s/epoch - 3ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 3s - loss: 13.6556 - mse: 13.6556 - mae: 1.5508 - val_loss: 11.9516 - val_mse: 11.9516 - val_mae: 1.5564 - lr: 0.0010 - 3s/epoch - 3ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 3s - loss: 13.5370 - mse: 13.5370 - mae: 1.5503 - val_loss: 12.1057 - val_mse: 12.1057 - val_mae: 1.5672 - lr: 0.0010 - 3s/epoch - 3ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 4s - loss: 13.5568 - mse: 13.5568 - mae: 1.5495 - val_loss: 11.9661 - val_mse: 11.9661 - val_mae: 1.5414 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 3s - loss: 13.6209 - mse: 13.6209 - mae: 1.5463 - val_loss: 12.0073 - val_mse: 12.0073 - val_mae: 1.5496 - lr: 0.0010 - 3s/epoch - 3ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 3s - loss: 13.5906 - mse: 13.5906 - mae: 1.5525 - val_loss: 12.4859 - val_mse: 12.4859 - val_mae: 1.5775 - lr: 0.0010 - 3s/epoch - 3ms/step\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-10-11 14:31:19,624]\u001b[0m Finished trial#8 resulted in value: 14.457999999999998. Current best value is 9.501999999999999 with parameters: {'activation': 'relu', 'num_hidden_layer': 4, 'i': 9, 'learning_rate': 0.00015599216930932077}.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Score for fold 5: loss of 12.485928535461426\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 16.4645 - mse: 16.4645 - mae: 1.6715 - val_loss: 11.9373 - val_mse: 11.9373 - val_mae: 1.5706 - lr: 3.2837e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 1s - loss: 13.7249 - mse: 13.7249 - mae: 1.5374 - val_loss: 11.4010 - val_mse: 11.4010 - val_mae: 1.5570 - lr: 3.2837e-04 - 1s/epoch - 1ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 13.3316 - mse: 13.3316 - mae: 1.5134 - val_loss: 11.2289 - val_mse: 11.2289 - val_mae: 1.5390 - lr: 3.2837e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 1s - loss: 13.1579 - mse: 13.1579 - mae: 1.5039 - val_loss: 11.0483 - val_mse: 11.0483 - val_mae: 1.5218 - lr: 3.2837e-04 - 1s/epoch - 1ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 1s - loss: 13.0964 - mse: 13.0964 - mae: 1.4971 - val_loss: 10.9823 - val_mse: 10.9823 - val_mae: 1.5166 - lr: 3.2837e-04 - 1s/epoch - 1ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 12.9516 - mse: 12.9516 - mae: 1.4914 - val_loss: 10.9284 - val_mse: 10.9284 - val_mae: 1.5058 - lr: 3.2837e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 1s - loss: 12.8956 - mse: 12.8956 - mae: 1.4856 - val_loss: 10.8444 - val_mse: 10.8444 - val_mae: 1.5264 - lr: 3.2837e-04 - 1s/epoch - 1ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 1s - loss: 12.8286 - mse: 12.8286 - mae: 1.4827 - val_loss: 10.8138 - val_mse: 10.8138 - val_mae: 1.5011 - lr: 3.2837e-04 - 1s/epoch - 1ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 1s - loss: 12.7301 - mse: 12.7301 - mae: 1.4797 - val_loss: 10.7479 - val_mse: 10.7479 - val_mae: 1.4887 - lr: 3.2837e-04 - 1s/epoch - 1ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 1s - loss: 12.7387 - mse: 12.7387 - mae: 1.4737 - val_loss: 10.6890 - val_mse: 10.6890 - val_mae: 1.4954 - lr: 3.2837e-04 - 1s/epoch - 1ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 2s - loss: 12.6505 - mse: 12.6505 - mae: 1.4721 - val_loss: 10.6573 - val_mse: 10.6573 - val_mae: 1.5004 - lr: 3.2837e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 1s - loss: 12.6226 - mse: 12.6226 - mae: 1.4660 - val_loss: 10.6198 - val_mse: 10.6198 - val_mae: 1.4851 - lr: 3.2837e-04 - 1s/epoch - 1ms/step\n",
            "Epoch 13/100\n",
            "1000/1000 - 1s - loss: 12.5735 - mse: 12.5735 - mae: 1.4670 - val_loss: 10.5932 - val_mse: 10.5932 - val_mae: 1.5056 - lr: 3.2837e-04 - 1s/epoch - 1ms/step\n",
            "Epoch 14/100\n",
            "1000/1000 - 1s - loss: 12.5199 - mse: 12.5199 - mae: 1.4628 - val_loss: 10.5319 - val_mse: 10.5319 - val_mae: 1.4933 - lr: 3.2837e-04 - 1s/epoch - 1ms/step\n",
            "Epoch 15/100\n",
            "1000/1000 - 1s - loss: 12.4911 - mse: 12.4911 - mae: 1.4612 - val_loss: 10.5078 - val_mse: 10.5078 - val_mae: 1.4824 - lr: 3.2837e-04 - 1s/epoch - 1ms/step\n",
            "Epoch 16/100\n",
            "1000/1000 - 2s - loss: 12.4549 - mse: 12.4549 - mae: 1.4575 - val_loss: 10.4768 - val_mse: 10.4768 - val_mae: 1.4875 - lr: 3.2837e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 17/100\n",
            "1000/1000 - 2s - loss: 12.4597 - mse: 12.4597 - mae: 1.4578 - val_loss: 10.4805 - val_mse: 10.4805 - val_mae: 1.4774 - lr: 3.2837e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 18/100\n",
            "1000/1000 - 2s - loss: 12.4162 - mse: 12.4162 - mae: 1.4565 - val_loss: 10.4218 - val_mse: 10.4218 - val_mae: 1.4694 - lr: 3.2837e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 19/100\n",
            "1000/1000 - 2s - loss: 12.3298 - mse: 12.3298 - mae: 1.4530 - val_loss: 10.4030 - val_mse: 10.4030 - val_mae: 1.4820 - lr: 3.2837e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 20/100\n",
            "1000/1000 - 1s - loss: 12.3859 - mse: 12.3859 - mae: 1.4529 - val_loss: 10.4078 - val_mse: 10.4078 - val_mae: 1.4771 - lr: 3.2837e-04 - 1s/epoch - 1ms/step\n",
            "Epoch 21/100\n",
            "1000/1000 - 2s - loss: 12.3680 - mse: 12.3680 - mae: 1.4517 - val_loss: 10.4824 - val_mse: 10.4824 - val_mae: 1.4775 - lr: 3.2837e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 22/100\n",
            "1000/1000 - 1s - loss: 12.3205 - mse: 12.3205 - mae: 1.4494 - val_loss: 10.4075 - val_mse: 10.4075 - val_mae: 1.4645 - lr: 3.2837e-04 - 1s/epoch - 1ms/step\n",
            "Epoch 23/100\n",
            "1000/1000 - 2s - loss: 12.3193 - mse: 12.3193 - mae: 1.4491 - val_loss: 10.4388 - val_mse: 10.4388 - val_mae: 1.4667 - lr: 3.2837e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 24/100\n",
            "1000/1000 - 1s - loss: 12.2937 - mse: 12.2937 - mae: 1.4451 - val_loss: 10.4109 - val_mse: 10.4109 - val_mae: 1.4751 - lr: 3.2837e-04 - 1s/epoch - 1ms/step\n",
            "Score for fold 1: loss of 10.410881996154785\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 13.3108 - mse: 13.3108 - mae: 1.4761 - val_loss: 6.4328 - val_mse: 6.4328 - val_mae: 1.3668 - lr: 3.2837e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 13.2139 - mse: 13.2139 - mae: 1.4745 - val_loss: 6.4411 - val_mse: 6.4411 - val_mae: 1.3781 - lr: 3.2837e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 1s - loss: 13.1863 - mse: 13.1863 - mae: 1.4708 - val_loss: 6.4433 - val_mse: 6.4433 - val_mae: 1.3725 - lr: 3.2837e-04 - 1s/epoch - 1ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 13.2130 - mse: 13.2130 - mae: 1.4726 - val_loss: 6.4721 - val_mse: 6.4721 - val_mae: 1.4052 - lr: 3.2837e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 13.1101 - mse: 13.1101 - mae: 1.4714 - val_loss: 6.4445 - val_mse: 6.4445 - val_mae: 1.3666 - lr: 3.2837e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 13.1855 - mse: 13.1855 - mae: 1.4688 - val_loss: 6.4587 - val_mse: 6.4587 - val_mae: 1.3841 - lr: 3.2837e-04 - 2s/epoch - 2ms/step\n",
            "Score for fold 2: loss of 6.458719253540039\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 11.5875 - mse: 11.5875 - mae: 1.4443 - val_loss: 12.7777 - val_mse: 12.7777 - val_mae: 1.4713 - lr: 3.2837e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 11.5913 - mse: 11.5913 - mae: 1.4454 - val_loss: 12.7466 - val_mse: 12.7466 - val_mae: 1.4699 - lr: 3.2837e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 11.5357 - mse: 11.5357 - mae: 1.4423 - val_loss: 12.8609 - val_mse: 12.8609 - val_mae: 1.4691 - lr: 3.2837e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 1s - loss: 11.5841 - mse: 11.5841 - mae: 1.4417 - val_loss: 12.7969 - val_mse: 12.7969 - val_mae: 1.4575 - lr: 3.2837e-04 - 1s/epoch - 1ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 11.5579 - mse: 11.5579 - mae: 1.4400 - val_loss: 12.8083 - val_mse: 12.8083 - val_mae: 1.4475 - lr: 3.2837e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 11.5407 - mse: 11.5407 - mae: 1.4423 - val_loss: 12.8009 - val_mse: 12.8009 - val_mae: 1.4752 - lr: 3.2837e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 11.5392 - mse: 11.5392 - mae: 1.4404 - val_loss: 12.8771 - val_mse: 12.8771 - val_mae: 1.4813 - lr: 3.2837e-04 - 2s/epoch - 2ms/step\n",
            "Score for fold 3: loss of 12.877086639404297\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 10.6412 - mse: 10.6412 - mae: 1.4468 - val_loss: 16.3329 - val_mse: 16.3329 - val_mae: 1.4610 - lr: 3.2837e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 10.6389 - mse: 10.6389 - mae: 1.4497 - val_loss: 16.3500 - val_mse: 16.3500 - val_mae: 1.4369 - lr: 3.2837e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 1s - loss: 10.6374 - mse: 10.6374 - mae: 1.4469 - val_loss: 16.4222 - val_mse: 16.4222 - val_mae: 1.4472 - lr: 3.2837e-04 - 1s/epoch - 1ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 10.6120 - mse: 10.6120 - mae: 1.4443 - val_loss: 16.4001 - val_mse: 16.4001 - val_mae: 1.4266 - lr: 3.2837e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 1s - loss: 10.6002 - mse: 10.6002 - mae: 1.4420 - val_loss: 16.4821 - val_mse: 16.4821 - val_mae: 1.4371 - lr: 3.2837e-04 - 1s/epoch - 1ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 10.6171 - mse: 10.6171 - mae: 1.4440 - val_loss: 16.2980 - val_mse: 16.2980 - val_mae: 1.4464 - lr: 3.2837e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 10.5859 - mse: 10.5859 - mae: 1.4421 - val_loss: 16.4914 - val_mse: 16.4914 - val_mae: 1.4450 - lr: 3.2837e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 2s - loss: 10.5947 - mse: 10.5947 - mae: 1.4439 - val_loss: 16.2742 - val_mse: 16.2742 - val_mae: 1.4406 - lr: 3.2837e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 1s - loss: 10.5398 - mse: 10.5398 - mae: 1.4413 - val_loss: 16.2890 - val_mse: 16.2890 - val_mae: 1.4516 - lr: 3.2837e-04 - 1s/epoch - 1ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 2s - loss: 10.5393 - mse: 10.5393 - mae: 1.4390 - val_loss: 16.6089 - val_mse: 16.6089 - val_mae: 1.4263 - lr: 3.2837e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 2s - loss: 10.5707 - mse: 10.5707 - mae: 1.4408 - val_loss: 16.3913 - val_mse: 16.3913 - val_mae: 1.4392 - lr: 3.2837e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 2s - loss: 10.5600 - mse: 10.5600 - mae: 1.4384 - val_loss: 16.3977 - val_mse: 16.3977 - val_mae: 1.4524 - lr: 3.2837e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 13/100\n",
            "1000/1000 - 1s - loss: 10.5537 - mse: 10.5537 - mae: 1.4398 - val_loss: 16.4197 - val_mse: 16.4197 - val_mae: 1.4495 - lr: 3.2837e-04 - 1s/epoch - 1ms/step\n",
            "Score for fold 4: loss of 16.41972541809082\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 11.3505 - mse: 11.3505 - mae: 1.4287 - val_loss: 13.1423 - val_mse: 13.1423 - val_mae: 1.4924 - lr: 3.2837e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 11.3171 - mse: 11.3171 - mae: 1.4270 - val_loss: 13.2061 - val_mse: 13.2061 - val_mae: 1.4666 - lr: 3.2837e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 1s - loss: 11.3202 - mse: 11.3202 - mae: 1.4254 - val_loss: 13.1002 - val_mse: 13.1002 - val_mae: 1.4849 - lr: 3.2837e-04 - 1s/epoch - 1ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 1s - loss: 11.2859 - mse: 11.2859 - mae: 1.4268 - val_loss: 13.1033 - val_mse: 13.1033 - val_mae: 1.4937 - lr: 3.2837e-04 - 1s/epoch - 1ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 11.2736 - mse: 11.2736 - mae: 1.4237 - val_loss: 13.2834 - val_mse: 13.2834 - val_mae: 1.4724 - lr: 3.2837e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 11.3253 - mse: 11.3253 - mae: 1.4229 - val_loss: 13.2054 - val_mse: 13.2054 - val_mae: 1.4823 - lr: 3.2837e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 11.2857 - mse: 11.2857 - mae: 1.4220 - val_loss: 13.1614 - val_mse: 13.1614 - val_mae: 1.4738 - lr: 3.2837e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 2s - loss: 11.2474 - mse: 11.2474 - mae: 1.4248 - val_loss: 13.2487 - val_mse: 13.2487 - val_mae: 1.4602 - lr: 3.2837e-04 - 2s/epoch - 2ms/step\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-10-11 14:32:52,102]\u001b[0m Finished trial#9 resulted in value: 11.884. Current best value is 9.501999999999999 with parameters: {'activation': 'relu', 'num_hidden_layer': 4, 'i': 9, 'learning_rate': 0.00015599216930932077}.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Score for fold 5: loss of 13.248745918273926\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 11s - loss: 13.7353 - mse: 13.7353 - mae: 1.5405 - val_loss: 10.4650 - val_mse: 10.4650 - val_mae: 1.4393 - lr: 1.3502e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 10s - loss: 12.9613 - mse: 12.9613 - mae: 1.4892 - val_loss: 9.6139 - val_mse: 9.6139 - val_mae: 1.4677 - lr: 1.3502e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 11s - loss: 12.7712 - mse: 12.7712 - mae: 1.4726 - val_loss: 9.6472 - val_mse: 9.6472 - val_mae: 1.4610 - lr: 1.3502e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 10s - loss: 12.4782 - mse: 12.4782 - mae: 1.4614 - val_loss: 9.5450 - val_mse: 9.5450 - val_mae: 1.4840 - lr: 1.3502e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 11s - loss: 12.4127 - mse: 12.4127 - mae: 1.4503 - val_loss: 9.6145 - val_mse: 9.6145 - val_mae: 1.4566 - lr: 1.3502e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 10s - loss: 12.2838 - mse: 12.2838 - mae: 1.4424 - val_loss: 9.9968 - val_mse: 9.9968 - val_mae: 1.4536 - lr: 1.3502e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 10s - loss: 12.2400 - mse: 12.2400 - mae: 1.4334 - val_loss: 9.4535 - val_mse: 9.4535 - val_mae: 1.5005 - lr: 1.3502e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 11s - loss: 11.9261 - mse: 11.9261 - mae: 1.4240 - val_loss: 9.6626 - val_mse: 9.6626 - val_mae: 1.4262 - lr: 1.3502e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 11s - loss: 11.6984 - mse: 11.6984 - mae: 1.4148 - val_loss: 9.6808 - val_mse: 9.6808 - val_mae: 1.5105 - lr: 1.3502e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 11s - loss: 11.4519 - mse: 11.4519 - mae: 1.3993 - val_loss: 9.6666 - val_mse: 9.6666 - val_mae: 1.4434 - lr: 1.3502e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 11s - loss: 11.2202 - mse: 11.2202 - mae: 1.3822 - val_loss: 9.8398 - val_mse: 9.8398 - val_mae: 1.5023 - lr: 1.3502e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 10s - loss: 11.0557 - mse: 11.0557 - mae: 1.3682 - val_loss: 11.1309 - val_mse: 11.1309 - val_mae: 1.4937 - lr: 1.3502e-04 - 10s/epoch - 10ms/step\n",
            "Score for fold 1: loss of 11.130881309509277\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 10s - loss: 8.4187 - mse: 8.4187 - mae: 1.3740 - val_loss: 19.6464 - val_mse: 19.6464 - val_mae: 1.3669 - lr: 1.3502e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 10s - loss: 8.2801 - mse: 8.2801 - mae: 1.3558 - val_loss: 19.6264 - val_mse: 19.6264 - val_mae: 1.4347 - lr: 1.3502e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 11s - loss: 7.7808 - mse: 7.7808 - mae: 1.3298 - val_loss: 19.7763 - val_mse: 19.7763 - val_mae: 1.3980 - lr: 1.3502e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 11s - loss: 8.4828 - mse: 8.4828 - mae: 1.3107 - val_loss: 19.5641 - val_mse: 19.5641 - val_mae: 1.4248 - lr: 1.3502e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 11s - loss: 7.0931 - mse: 7.0931 - mae: 1.2860 - val_loss: 20.1264 - val_mse: 20.1264 - val_mae: 1.4238 - lr: 1.3502e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 10s - loss: 7.1868 - mse: 7.1868 - mae: 1.2675 - val_loss: 20.2181 - val_mse: 20.2181 - val_mae: 1.4357 - lr: 1.3502e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 10s - loss: 6.6877 - mse: 6.6877 - mae: 1.2453 - val_loss: 20.2497 - val_mse: 20.2497 - val_mae: 1.4946 - lr: 1.3502e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 11s - loss: 6.3368 - mse: 6.3368 - mae: 1.2252 - val_loss: 19.9889 - val_mse: 19.9889 - val_mae: 1.4625 - lr: 1.3502e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 11s - loss: 6.2119 - mse: 6.2119 - mae: 1.2052 - val_loss: 20.4336 - val_mse: 20.4336 - val_mae: 1.5702 - lr: 1.3502e-04 - 11s/epoch - 11ms/step\n",
            "Score for fold 2: loss of 20.43364143371582\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 11s - loss: 9.2534 - mse: 9.2534 - mae: 1.2796 - val_loss: 7.5442 - val_mse: 7.5442 - val_mae: 1.1687 - lr: 1.3502e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 10s - loss: 8.6379 - mse: 8.6379 - mae: 1.2469 - val_loss: 7.3780 - val_mse: 7.3780 - val_mae: 1.2233 - lr: 1.3502e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 11s - loss: 8.3043 - mse: 8.3043 - mae: 1.2207 - val_loss: 8.8611 - val_mse: 8.8611 - val_mae: 1.2634 - lr: 1.3502e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 11s - loss: 8.0152 - mse: 8.0152 - mae: 1.1995 - val_loss: 7.6968 - val_mse: 7.6968 - val_mae: 1.2750 - lr: 1.3502e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 11s - loss: 7.7467 - mse: 7.7467 - mae: 1.1809 - val_loss: 8.4230 - val_mse: 8.4230 - val_mae: 1.2025 - lr: 1.3502e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 11s - loss: 7.2638 - mse: 7.2638 - mae: 1.1484 - val_loss: 8.1118 - val_mse: 8.1118 - val_mae: 1.2930 - lr: 1.3502e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 11s - loss: 7.1728 - mse: 7.1728 - mae: 1.1334 - val_loss: 8.7209 - val_mse: 8.7209 - val_mae: 1.2356 - lr: 1.3502e-04 - 11s/epoch - 11ms/step\n",
            "Score for fold 3: loss of 8.720909118652344\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 10s - loss: 8.4419 - mse: 8.4419 - mae: 1.1772 - val_loss: 3.6212 - val_mse: 3.6212 - val_mae: 1.0927 - lr: 1.3502e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 11s - loss: 7.8149 - mse: 7.8149 - mae: 1.1534 - val_loss: 4.6802 - val_mse: 4.6802 - val_mae: 1.1953 - lr: 1.3502e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 11s - loss: 7.6830 - mse: 7.6830 - mae: 1.1342 - val_loss: 4.4349 - val_mse: 4.4349 - val_mae: 1.1707 - lr: 1.3502e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 11s - loss: 7.1817 - mse: 7.1817 - mae: 1.1136 - val_loss: 4.1119 - val_mse: 4.1119 - val_mae: 1.1829 - lr: 1.3502e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 11s - loss: 6.9198 - mse: 6.9198 - mae: 1.0947 - val_loss: 4.0802 - val_mse: 4.0802 - val_mae: 1.1982 - lr: 1.3502e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 10s - loss: 6.7731 - mse: 6.7731 - mae: 1.0738 - val_loss: 4.3524 - val_mse: 4.3524 - val_mae: 1.1437 - lr: 1.3502e-04 - 10s/epoch - 10ms/step\n",
            "Score for fold 4: loss of 4.352412700653076\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 11s - loss: 6.9437 - mse: 6.9437 - mae: 1.1113 - val_loss: 3.1700 - val_mse: 3.1700 - val_mae: 1.1189 - lr: 1.3502e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 10s - loss: 6.9833 - mse: 6.9833 - mae: 1.0875 - val_loss: 2.9750 - val_mse: 2.9750 - val_mae: 1.0622 - lr: 1.3502e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 10s - loss: 6.3681 - mse: 6.3681 - mae: 1.0624 - val_loss: 3.5172 - val_mse: 3.5172 - val_mae: 1.0032 - lr: 1.3502e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 11s - loss: 6.1812 - mse: 6.1812 - mae: 1.0520 - val_loss: 3.4593 - val_mse: 3.4593 - val_mae: 1.0562 - lr: 1.3502e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 11s - loss: 6.2037 - mse: 6.2037 - mae: 1.0382 - val_loss: 3.6785 - val_mse: 3.6785 - val_mae: 1.0747 - lr: 1.3502e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 11s - loss: 5.8878 - mse: 5.8878 - mae: 1.0154 - val_loss: 4.0576 - val_mse: 4.0576 - val_mae: 1.2233 - lr: 1.3502e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 11s - loss: 5.3941 - mse: 5.3941 - mae: 0.9958 - val_loss: 3.7728 - val_mse: 3.7728 - val_mae: 1.0715 - lr: 1.3502e-04 - 11s/epoch - 11ms/step\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-10-11 14:40:15,206]\u001b[0m Finished trial#10 resulted in value: 9.680000000000001. Current best value is 9.501999999999999 with parameters: {'activation': 'relu', 'num_hidden_layer': 4, 'i': 9, 'learning_rate': 0.00015599216930932077}.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Score for fold 5: loss of 3.7728357315063477\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 12s - loss: 12.9661 - mse: 12.9661 - mae: 1.5416 - val_loss: 14.0574 - val_mse: 14.0574 - val_mae: 1.5304 - lr: 1.0328e-04 - 12s/epoch - 12ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 11s - loss: 12.0020 - mse: 12.0020 - mae: 1.4869 - val_loss: 14.2554 - val_mse: 14.2554 - val_mae: 1.5184 - lr: 1.0328e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 10s - loss: 11.6907 - mse: 11.6907 - mae: 1.4650 - val_loss: 15.1096 - val_mse: 15.1096 - val_mae: 1.5056 - lr: 1.0328e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 11s - loss: 11.5191 - mse: 11.5191 - mae: 1.4554 - val_loss: 14.4187 - val_mse: 14.4187 - val_mae: 1.4623 - lr: 1.0328e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 11s - loss: 11.4041 - mse: 11.4041 - mae: 1.4432 - val_loss: 14.8457 - val_mse: 14.8457 - val_mae: 1.5355 - lr: 1.0328e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 11s - loss: 11.2291 - mse: 11.2291 - mae: 1.4307 - val_loss: 14.3887 - val_mse: 14.3887 - val_mae: 1.4608 - lr: 1.0328e-04 - 11s/epoch - 11ms/step\n",
            "Score for fold 1: loss of 14.38871955871582\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 11s - loss: 12.6566 - mse: 12.6566 - mae: 1.4441 - val_loss: 8.0173 - val_mse: 8.0173 - val_mae: 1.3869 - lr: 1.0328e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 11s - loss: 12.7719 - mse: 12.7719 - mae: 1.4371 - val_loss: 7.9734 - val_mse: 7.9734 - val_mae: 1.3927 - lr: 1.0328e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 11s - loss: 12.4280 - mse: 12.4280 - mae: 1.4215 - val_loss: 8.1712 - val_mse: 8.1712 - val_mae: 1.3696 - lr: 1.0328e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 11s - loss: 12.1251 - mse: 12.1251 - mae: 1.4146 - val_loss: 8.3873 - val_mse: 8.3873 - val_mae: 1.4218 - lr: 1.0328e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 11s - loss: 11.9088 - mse: 11.9088 - mae: 1.3990 - val_loss: 9.0524 - val_mse: 9.0524 - val_mae: 1.4395 - lr: 1.0328e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 11s - loss: 11.5998 - mse: 11.5998 - mae: 1.3852 - val_loss: 8.0597 - val_mse: 8.0597 - val_mae: 1.4068 - lr: 1.0328e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 11s - loss: 11.4333 - mse: 11.4333 - mae: 1.3716 - val_loss: 8.3052 - val_mse: 8.3052 - val_mae: 1.4653 - lr: 1.0328e-04 - 11s/epoch - 11ms/step\n",
            "Score for fold 2: loss of 8.305216789245605\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 11s - loss: 8.7459 - mse: 8.7459 - mae: 1.3773 - val_loss: 18.5114 - val_mse: 18.5114 - val_mae: 1.3453 - lr: 1.0328e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 11s - loss: 8.7106 - mse: 8.7106 - mae: 1.3657 - val_loss: 18.8143 - val_mse: 18.8143 - val_mae: 1.3381 - lr: 1.0328e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 11s - loss: 8.0449 - mse: 8.0449 - mae: 1.3425 - val_loss: 18.8533 - val_mse: 18.8533 - val_mae: 1.3576 - lr: 1.0328e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 11s - loss: 7.8875 - mse: 7.8875 - mae: 1.3281 - val_loss: 18.6739 - val_mse: 18.6739 - val_mae: 1.3584 - lr: 1.0328e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 10s - loss: 7.7432 - mse: 7.7432 - mae: 1.3106 - val_loss: 18.7563 - val_mse: 18.7563 - val_mae: 1.3248 - lr: 1.0328e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 11s - loss: 7.6671 - mse: 7.6671 - mae: 1.2920 - val_loss: 18.8003 - val_mse: 18.8003 - val_mae: 1.3846 - lr: 1.0328e-04 - 11s/epoch - 11ms/step\n",
            "Score for fold 3: loss of 18.800277709960938\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 11s - loss: 10.7032 - mse: 10.7032 - mae: 1.3243 - val_loss: 5.8069 - val_mse: 5.8069 - val_mae: 1.2401 - lr: 1.0328e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 10s - loss: 10.0222 - mse: 10.0222 - mae: 1.2956 - val_loss: 5.5596 - val_mse: 5.5596 - val_mae: 1.2990 - lr: 1.0328e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 11s - loss: 9.8647 - mse: 9.8647 - mae: 1.2727 - val_loss: 5.7310 - val_mse: 5.7310 - val_mae: 1.3163 - lr: 1.0328e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 11s - loss: 9.9839 - mse: 9.9839 - mae: 1.2541 - val_loss: 6.2062 - val_mse: 6.2062 - val_mae: 1.3186 - lr: 1.0328e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 10s - loss: 9.2360 - mse: 9.2360 - mae: 1.2278 - val_loss: 6.8518 - val_mse: 6.8518 - val_mae: 1.3224 - lr: 1.0328e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 11s - loss: 8.8140 - mse: 8.8140 - mae: 1.2095 - val_loss: 6.0906 - val_mse: 6.0906 - val_mae: 1.2559 - lr: 1.0328e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 11s - loss: 8.6703 - mse: 8.6703 - mae: 1.1976 - val_loss: 6.5072 - val_mse: 6.5072 - val_mae: 1.2829 - lr: 1.0328e-04 - 11s/epoch - 11ms/step\n",
            "Score for fold 4: loss of 6.507236480712891\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 11s - loss: 9.5062 - mse: 9.5062 - mae: 1.2325 - val_loss: 4.4117 - val_mse: 4.4117 - val_mae: 1.2061 - lr: 1.0328e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 11s - loss: 8.6218 - mse: 8.6218 - mae: 1.1958 - val_loss: 4.8438 - val_mse: 4.8438 - val_mae: 1.1577 - lr: 1.0328e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 11s - loss: 8.8972 - mse: 8.8972 - mae: 1.1773 - val_loss: 4.9916 - val_mse: 4.9916 - val_mae: 1.1392 - lr: 1.0328e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 11s - loss: 8.0386 - mse: 8.0386 - mae: 1.1482 - val_loss: 5.1280 - val_mse: 5.1280 - val_mae: 1.2413 - lr: 1.0328e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 11s - loss: 7.7347 - mse: 7.7347 - mae: 1.1311 - val_loss: 5.2905 - val_mse: 5.2905 - val_mae: 1.2267 - lr: 1.0328e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 11s - loss: 7.1961 - mse: 7.1961 - mae: 1.1086 - val_loss: 5.4086 - val_mse: 5.4086 - val_mae: 1.2265 - lr: 1.0328e-04 - 11s/epoch - 11ms/step\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-10-11 14:46:08,970]\u001b[0m Finished trial#11 resulted in value: 10.684000000000001. Current best value is 9.501999999999999 with parameters: {'activation': 'relu', 'num_hidden_layer': 4, 'i': 9, 'learning_rate': 0.00015599216930932077}.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Score for fold 5: loss of 5.408636093139648\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 7s - loss: 12.3199 - mse: 12.3199 - mae: 1.5554 - val_loss: 17.8640 - val_mse: 17.8640 - val_mae: 1.6573 - lr: 1.0950e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 6s - loss: 11.1468 - mse: 11.1468 - mae: 1.4916 - val_loss: 17.1788 - val_mse: 17.1788 - val_mae: 1.5114 - lr: 1.0950e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 6s - loss: 10.9257 - mse: 10.9257 - mae: 1.4719 - val_loss: 17.0331 - val_mse: 17.0331 - val_mae: 1.4834 - lr: 1.0950e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 6s - loss: 10.7457 - mse: 10.7457 - mae: 1.4608 - val_loss: 17.0195 - val_mse: 17.0195 - val_mae: 1.4905 - lr: 1.0950e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 6s - loss: 10.8161 - mse: 10.8161 - mae: 1.4521 - val_loss: 16.9412 - val_mse: 16.9412 - val_mae: 1.5312 - lr: 1.0950e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 6s - loss: 10.5721 - mse: 10.5721 - mae: 1.4418 - val_loss: 16.9123 - val_mse: 16.9123 - val_mae: 1.4945 - lr: 1.0950e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 6s - loss: 10.4131 - mse: 10.4131 - mae: 1.4377 - val_loss: 16.8036 - val_mse: 16.8036 - val_mae: 1.4931 - lr: 1.0950e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 6s - loss: 10.4614 - mse: 10.4614 - mae: 1.4289 - val_loss: 16.8670 - val_mse: 16.8670 - val_mae: 1.4618 - lr: 1.0950e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 6s - loss: 10.2002 - mse: 10.2002 - mae: 1.4227 - val_loss: 16.8124 - val_mse: 16.8124 - val_mae: 1.4463 - lr: 1.0950e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 6s - loss: 10.1873 - mse: 10.1873 - mae: 1.4125 - val_loss: 16.7050 - val_mse: 16.7050 - val_mae: 1.4617 - lr: 1.0950e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 7s - loss: 9.9689 - mse: 9.9689 - mae: 1.4067 - val_loss: 16.7474 - val_mse: 16.7474 - val_mae: 1.4917 - lr: 1.0950e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 6s - loss: 9.8827 - mse: 9.8827 - mae: 1.3993 - val_loss: 16.7195 - val_mse: 16.7195 - val_mae: 1.4900 - lr: 1.0950e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 13/100\n",
            "1000/1000 - 6s - loss: 9.6589 - mse: 9.6589 - mae: 1.3898 - val_loss: 16.8488 - val_mse: 16.8488 - val_mae: 1.4775 - lr: 1.0950e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 14/100\n",
            "1000/1000 - 6s - loss: 9.5835 - mse: 9.5835 - mae: 1.3821 - val_loss: 16.9343 - val_mse: 16.9343 - val_mae: 1.4795 - lr: 1.0950e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 15/100\n",
            "1000/1000 - 6s - loss: 9.4920 - mse: 9.4920 - mae: 1.3770 - val_loss: 16.7296 - val_mse: 16.7296 - val_mae: 1.4799 - lr: 1.0950e-04 - 6s/epoch - 6ms/step\n",
            "Score for fold 1: loss of 16.729602813720703\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 6s - loss: 12.1525 - mse: 12.1525 - mae: 1.4044 - val_loss: 6.6008 - val_mse: 6.6008 - val_mae: 1.3966 - lr: 1.0950e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 6s - loss: 11.8054 - mse: 11.8054 - mae: 1.3968 - val_loss: 6.3526 - val_mse: 6.3526 - val_mae: 1.2927 - lr: 1.0950e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 6s - loss: 11.6356 - mse: 11.6356 - mae: 1.3870 - val_loss: 6.5517 - val_mse: 6.5517 - val_mae: 1.3741 - lr: 1.0950e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 6s - loss: 11.5069 - mse: 11.5069 - mae: 1.3767 - val_loss: 6.6172 - val_mse: 6.6172 - val_mae: 1.3540 - lr: 1.0950e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 7s - loss: 11.3035 - mse: 11.3035 - mae: 1.3632 - val_loss: 7.0370 - val_mse: 7.0370 - val_mae: 1.3152 - lr: 1.0950e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 7s - loss: 11.2281 - mse: 11.2281 - mae: 1.3524 - val_loss: 6.7782 - val_mse: 6.7782 - val_mae: 1.4286 - lr: 1.0950e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 6s - loss: 11.0008 - mse: 11.0008 - mae: 1.3409 - val_loss: 6.5653 - val_mse: 6.5653 - val_mae: 1.3493 - lr: 1.0950e-04 - 6s/epoch - 6ms/step\n",
            "Score for fold 2: loss of 6.565299987792969\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 6s - loss: 9.7510 - mse: 9.7510 - mae: 1.3390 - val_loss: 11.2533 - val_mse: 11.2533 - val_mae: 1.3819 - lr: 1.0950e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 6s - loss: 9.5498 - mse: 9.5498 - mae: 1.3261 - val_loss: 11.4085 - val_mse: 11.4085 - val_mae: 1.3760 - lr: 1.0950e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 6s - loss: 9.2879 - mse: 9.2879 - mae: 1.3125 - val_loss: 11.5036 - val_mse: 11.5036 - val_mae: 1.3850 - lr: 1.0950e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 6s - loss: 9.0147 - mse: 9.0147 - mae: 1.2971 - val_loss: 11.6069 - val_mse: 11.6069 - val_mae: 1.3458 - lr: 1.0950e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 6s - loss: 9.0378 - mse: 9.0378 - mae: 1.2862 - val_loss: 11.8960 - val_mse: 11.8960 - val_mae: 1.3502 - lr: 1.0950e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 6s - loss: 8.7157 - mse: 8.7157 - mae: 1.2732 - val_loss: 11.7188 - val_mse: 11.7188 - val_mae: 1.4075 - lr: 1.0950e-04 - 6s/epoch - 6ms/step\n",
            "Score for fold 3: loss of 11.718806266784668\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 7s - loss: 10.5366 - mse: 10.5366 - mae: 1.3067 - val_loss: 5.6997 - val_mse: 5.6997 - val_mae: 1.2506 - lr: 1.0950e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 6s - loss: 10.2444 - mse: 10.2444 - mae: 1.2896 - val_loss: 5.5049 - val_mse: 5.5049 - val_mae: 1.2352 - lr: 1.0950e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 6s - loss: 9.7718 - mse: 9.7718 - mae: 1.2784 - val_loss: 5.6843 - val_mse: 5.6843 - val_mae: 1.2243 - lr: 1.0950e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 6s - loss: 9.5359 - mse: 9.5359 - mae: 1.2590 - val_loss: 5.7230 - val_mse: 5.7230 - val_mae: 1.3197 - lr: 1.0950e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 6s - loss: 9.2072 - mse: 9.2072 - mae: 1.2488 - val_loss: 6.1509 - val_mse: 6.1509 - val_mae: 1.3210 - lr: 1.0950e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 6s - loss: 9.0263 - mse: 9.0263 - mae: 1.2364 - val_loss: 5.7531 - val_mse: 5.7531 - val_mae: 1.3237 - lr: 1.0950e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 6s - loss: 8.8859 - mse: 8.8859 - mae: 1.2177 - val_loss: 5.9666 - val_mse: 5.9666 - val_mae: 1.2506 - lr: 1.0950e-04 - 6s/epoch - 6ms/step\n",
            "Score for fold 4: loss of 5.966567039489746\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 6s - loss: 8.3791 - mse: 8.3791 - mae: 1.2404 - val_loss: 8.1960 - val_mse: 8.1960 - val_mae: 1.2010 - lr: 1.0950e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 7s - loss: 7.9637 - mse: 7.9637 - mae: 1.2165 - val_loss: 8.2045 - val_mse: 8.2045 - val_mae: 1.2563 - lr: 1.0950e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 7s - loss: 7.9185 - mse: 7.9185 - mae: 1.2077 - val_loss: 8.5374 - val_mse: 8.5374 - val_mae: 1.2430 - lr: 1.0950e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 7s - loss: 7.6049 - mse: 7.6049 - mae: 1.1960 - val_loss: 8.5645 - val_mse: 8.5645 - val_mae: 1.4272 - lr: 1.0950e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 6s - loss: 7.5687 - mse: 7.5687 - mae: 1.1769 - val_loss: 8.6087 - val_mse: 8.6087 - val_mae: 1.2418 - lr: 1.0950e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 7s - loss: 7.2752 - mse: 7.2752 - mae: 1.1620 - val_loss: 9.1352 - val_mse: 9.1352 - val_mae: 1.2677 - lr: 1.0950e-04 - 7s/epoch - 7ms/step\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-10-11 14:50:38,255]\u001b[0m Finished trial#12 resulted in value: 10.026. Current best value is 9.501999999999999 with parameters: {'activation': 'relu', 'num_hidden_layer': 4, 'i': 9, 'learning_rate': 0.00015599216930932077}.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Score for fold 5: loss of 9.135208129882812\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 12s - loss: 13.8575 - mse: 13.8575 - mae: 1.5400 - val_loss: 10.3240 - val_mse: 10.3240 - val_mae: 1.6238 - lr: 2.4802e-04 - 12s/epoch - 12ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 11s - loss: 13.1080 - mse: 13.1080 - mae: 1.4905 - val_loss: 9.9045 - val_mse: 9.9045 - val_mae: 1.4243 - lr: 2.4802e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 11s - loss: 12.8730 - mse: 12.8730 - mae: 1.4724 - val_loss: 9.8358 - val_mse: 9.8358 - val_mae: 1.4628 - lr: 2.4802e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 11s - loss: 12.6352 - mse: 12.6352 - mae: 1.4619 - val_loss: 10.3482 - val_mse: 10.3482 - val_mae: 1.4639 - lr: 2.4802e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 11s - loss: 12.4616 - mse: 12.4616 - mae: 1.4520 - val_loss: 10.1329 - val_mse: 10.1329 - val_mae: 1.4731 - lr: 2.4802e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 10s - loss: 12.4086 - mse: 12.4086 - mae: 1.4445 - val_loss: 9.8871 - val_mse: 9.8871 - val_mae: 1.4204 - lr: 2.4802e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 10s - loss: 12.3027 - mse: 12.3027 - mae: 1.4382 - val_loss: 10.3512 - val_mse: 10.3512 - val_mae: 1.4816 - lr: 2.4802e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 11s - loss: 12.0314 - mse: 12.0314 - mae: 1.4278 - val_loss: 9.8608 - val_mse: 9.8608 - val_mae: 1.4644 - lr: 2.4802e-04 - 11s/epoch - 11ms/step\n",
            "Score for fold 1: loss of 9.860764503479004\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 11s - loss: 11.9368 - mse: 11.9368 - mae: 1.4343 - val_loss: 9.1096 - val_mse: 9.1096 - val_mae: 1.3920 - lr: 2.4802e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 11s - loss: 11.6199 - mse: 11.6199 - mae: 1.4177 - val_loss: 9.0668 - val_mse: 9.0668 - val_mae: 1.3913 - lr: 2.4802e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 11s - loss: 11.2745 - mse: 11.2745 - mae: 1.4015 - val_loss: 9.4711 - val_mse: 9.4711 - val_mae: 1.3878 - lr: 2.4802e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 11s - loss: 11.0756 - mse: 11.0756 - mae: 1.3877 - val_loss: 9.2649 - val_mse: 9.2649 - val_mae: 1.4126 - lr: 2.4802e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 10s - loss: 11.0823 - mse: 11.0823 - mae: 1.3695 - val_loss: 9.3572 - val_mse: 9.3572 - val_mae: 1.4468 - lr: 2.4802e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 11s - loss: 10.6596 - mse: 10.6596 - mae: 1.3605 - val_loss: 9.6675 - val_mse: 9.6675 - val_mae: 1.4481 - lr: 2.4802e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 11s - loss: 10.4131 - mse: 10.4131 - mae: 1.3337 - val_loss: 9.6838 - val_mse: 9.6838 - val_mae: 1.4295 - lr: 2.4802e-04 - 11s/epoch - 11ms/step\n",
            "Score for fold 2: loss of 9.68382740020752\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 10s - loss: 9.7655 - mse: 9.7655 - mae: 1.3652 - val_loss: 12.0289 - val_mse: 12.0289 - val_mae: 1.3264 - lr: 2.4802e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 11s - loss: 9.3250 - mse: 9.3250 - mae: 1.3414 - val_loss: 12.0603 - val_mse: 12.0603 - val_mae: 1.3344 - lr: 2.4802e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 10s - loss: 9.0435 - mse: 9.0435 - mae: 1.3222 - val_loss: 12.3720 - val_mse: 12.3720 - val_mae: 1.3886 - lr: 2.4802e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 11s - loss: 8.7993 - mse: 8.7993 - mae: 1.3039 - val_loss: 12.6665 - val_mse: 12.6665 - val_mae: 1.4833 - lr: 2.4802e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 11s - loss: 8.3084 - mse: 8.3084 - mae: 1.2856 - val_loss: 12.6476 - val_mse: 12.6476 - val_mae: 1.4606 - lr: 2.4802e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 10s - loss: 8.1665 - mse: 8.1665 - mae: 1.2694 - val_loss: 12.4687 - val_mse: 12.4687 - val_mae: 1.3114 - lr: 2.4802e-04 - 10s/epoch - 10ms/step\n",
            "Score for fold 3: loss of 12.468673706054688\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 11s - loss: 9.8410 - mse: 9.8410 - mae: 1.3130 - val_loss: 6.7915 - val_mse: 6.7915 - val_mae: 1.2509 - lr: 2.4802e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 11s - loss: 9.2889 - mse: 9.2889 - mae: 1.2866 - val_loss: 6.5627 - val_mse: 6.5627 - val_mae: 1.1638 - lr: 2.4802e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 11s - loss: 9.1432 - mse: 9.1432 - mae: 1.2635 - val_loss: 6.1980 - val_mse: 6.1980 - val_mae: 1.2296 - lr: 2.4802e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 11s - loss: 8.7587 - mse: 8.7587 - mae: 1.2519 - val_loss: 6.4079 - val_mse: 6.4079 - val_mae: 1.2973 - lr: 2.4802e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 11s - loss: 8.4956 - mse: 8.4956 - mae: 1.2320 - val_loss: 6.0469 - val_mse: 6.0469 - val_mae: 1.2958 - lr: 2.4802e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 10s - loss: 8.3412 - mse: 8.3412 - mae: 1.2210 - val_loss: 6.3318 - val_mse: 6.3318 - val_mae: 1.3321 - lr: 2.4802e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 10s - loss: 8.1222 - mse: 8.1222 - mae: 1.1991 - val_loss: 5.9939 - val_mse: 5.9939 - val_mae: 1.3601 - lr: 2.4802e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 11s - loss: 7.9364 - mse: 7.9364 - mae: 1.1800 - val_loss: 6.7314 - val_mse: 6.7314 - val_mae: 1.4620 - lr: 2.4802e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 11s - loss: 7.5392 - mse: 7.5392 - mae: 1.1646 - val_loss: 7.2781 - val_mse: 7.2781 - val_mae: 1.2607 - lr: 2.4802e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 11s - loss: 7.1952 - mse: 7.1952 - mae: 1.1520 - val_loss: 6.8746 - val_mse: 6.8746 - val_mae: 1.4840 - lr: 2.4802e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 11s - loss: 7.0051 - mse: 7.0051 - mae: 1.1421 - val_loss: 7.8747 - val_mse: 7.8747 - val_mae: 1.4574 - lr: 2.4802e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 10s - loss: 6.8115 - mse: 6.8115 - mae: 1.1305 - val_loss: 5.9884 - val_mse: 5.9884 - val_mae: 1.4212 - lr: 2.4802e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 13/100\n",
            "1000/1000 - 11s - loss: 6.7182 - mse: 6.7182 - mae: 1.1193 - val_loss: 7.4308 - val_mse: 7.4308 - val_mae: 1.2964 - lr: 2.4802e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 14/100\n",
            "1000/1000 - 11s - loss: 6.6953 - mse: 6.6953 - mae: 1.1001 - val_loss: 7.1877 - val_mse: 7.1877 - val_mae: 1.3448 - lr: 2.4802e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 15/100\n",
            "1000/1000 - 10s - loss: 6.3268 - mse: 6.3268 - mae: 1.0868 - val_loss: 7.5107 - val_mse: 7.5107 - val_mae: 1.2644 - lr: 2.4802e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 16/100\n",
            "1000/1000 - 11s - loss: 6.1795 - mse: 6.1795 - mae: 1.0702 - val_loss: 6.9591 - val_mse: 6.9591 - val_mae: 1.3098 - lr: 2.4802e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 17/100\n",
            "1000/1000 - 11s - loss: 6.0080 - mse: 6.0080 - mae: 1.0704 - val_loss: 7.7390 - val_mse: 7.7390 - val_mae: 1.2906 - lr: 2.4802e-04 - 11s/epoch - 11ms/step\n",
            "Score for fold 4: loss of 7.738958358764648\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 11s - loss: 5.7918 - mse: 5.7918 - mae: 1.1542 - val_loss: 9.9234 - val_mse: 9.9234 - val_mae: 1.3426 - lr: 2.4802e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 10s - loss: 5.3298 - mse: 5.3298 - mae: 1.1253 - val_loss: 11.2453 - val_mse: 11.2453 - val_mae: 1.2574 - lr: 2.4802e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 11s - loss: 4.9322 - mse: 4.9322 - mae: 1.1041 - val_loss: 10.3466 - val_mse: 10.3466 - val_mae: 1.1051 - lr: 2.4802e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 10s - loss: 4.9682 - mse: 4.9682 - mae: 1.0892 - val_loss: 9.9610 - val_mse: 9.9610 - val_mae: 1.0942 - lr: 2.4802e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 10s - loss: 4.4268 - mse: 4.4268 - mae: 1.0651 - val_loss: 11.3205 - val_mse: 11.3205 - val_mae: 1.1891 - lr: 2.4802e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 11s - loss: 4.2570 - mse: 4.2570 - mae: 1.0527 - val_loss: 10.6338 - val_mse: 10.6338 - val_mae: 1.1578 - lr: 2.4802e-04 - 11s/epoch - 11ms/step\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-10-11 14:58:36,787]\u001b[0m Finished trial#13 resulted in value: 10.076. Current best value is 9.501999999999999 with parameters: {'activation': 'relu', 'num_hidden_layer': 4, 'i': 9, 'learning_rate': 0.00015599216930932077}.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Score for fold 5: loss of 10.63379192352295\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 7s - loss: 13.1473 - mse: 13.1473 - mae: 1.5445 - val_loss: 14.4538 - val_mse: 14.4538 - val_mae: 1.4819 - lr: 1.7299e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 6s - loss: 12.0058 - mse: 12.0058 - mae: 1.4784 - val_loss: 14.0800 - val_mse: 14.0800 - val_mae: 1.5450 - lr: 1.7299e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 6s - loss: 11.5683 - mse: 11.5683 - mae: 1.4659 - val_loss: 14.0262 - val_mse: 14.0262 - val_mae: 1.5275 - lr: 1.7299e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 6s - loss: 11.4692 - mse: 11.4692 - mae: 1.4530 - val_loss: 14.0343 - val_mse: 14.0343 - val_mae: 1.4615 - lr: 1.7299e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 6s - loss: 11.2882 - mse: 11.2882 - mae: 1.4431 - val_loss: 14.6083 - val_mse: 14.6083 - val_mae: 1.4916 - lr: 1.7299e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 7s - loss: 11.1554 - mse: 11.1554 - mae: 1.4379 - val_loss: 14.0927 - val_mse: 14.0927 - val_mae: 1.5367 - lr: 1.7299e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 7s - loss: 10.9614 - mse: 10.9614 - mae: 1.4243 - val_loss: 14.3161 - val_mse: 14.3161 - val_mae: 1.4379 - lr: 1.7299e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 6s - loss: 10.8474 - mse: 10.8474 - mae: 1.4161 - val_loss: 14.1457 - val_mse: 14.1457 - val_mae: 1.5135 - lr: 1.7299e-04 - 6s/epoch - 6ms/step\n",
            "Score for fold 1: loss of 14.145665168762207\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 6s - loss: 11.9597 - mse: 11.9597 - mae: 1.4264 - val_loss: 9.5891 - val_mse: 9.5891 - val_mae: 1.3849 - lr: 1.7299e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 6s - loss: 11.8437 - mse: 11.8437 - mae: 1.4150 - val_loss: 9.6868 - val_mse: 9.6868 - val_mae: 1.4041 - lr: 1.7299e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 6s - loss: 11.6135 - mse: 11.6135 - mae: 1.4062 - val_loss: 9.7168 - val_mse: 9.7168 - val_mae: 1.3623 - lr: 1.7299e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 6s - loss: 11.4316 - mse: 11.4316 - mae: 1.3948 - val_loss: 9.7832 - val_mse: 9.7832 - val_mae: 1.4153 - lr: 1.7299e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 7s - loss: 11.1097 - mse: 11.1097 - mae: 1.3845 - val_loss: 9.6869 - val_mse: 9.6869 - val_mae: 1.4649 - lr: 1.7299e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 7s - loss: 10.9981 - mse: 10.9981 - mae: 1.3758 - val_loss: 10.0253 - val_mse: 10.0253 - val_mae: 1.4081 - lr: 1.7299e-04 - 7s/epoch - 7ms/step\n",
            "Score for fold 2: loss of 10.025259017944336\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 7s - loss: 11.7525 - mse: 11.7525 - mae: 1.4013 - val_loss: 6.6001 - val_mse: 6.6001 - val_mae: 1.3034 - lr: 1.7299e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 7s - loss: 11.6264 - mse: 11.6264 - mae: 1.3853 - val_loss: 6.6378 - val_mse: 6.6378 - val_mae: 1.3753 - lr: 1.7299e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 6s - loss: 11.4192 - mse: 11.4192 - mae: 1.3756 - val_loss: 6.7199 - val_mse: 6.7199 - val_mae: 1.3575 - lr: 1.7299e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 7s - loss: 11.0399 - mse: 11.0399 - mae: 1.3585 - val_loss: 6.9567 - val_mse: 6.9567 - val_mae: 1.3447 - lr: 1.7299e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 7s - loss: 10.8892 - mse: 10.8892 - mae: 1.3447 - val_loss: 6.8528 - val_mse: 6.8528 - val_mae: 1.3369 - lr: 1.7299e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 7s - loss: 10.6023 - mse: 10.6023 - mae: 1.3288 - val_loss: 7.6659 - val_mse: 7.6659 - val_mae: 1.3067 - lr: 1.7299e-04 - 7s/epoch - 7ms/step\n",
            "Score for fold 3: loss of 7.665889263153076\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 6s - loss: 9.0756 - mse: 9.0756 - mae: 1.3429 - val_loss: 13.4101 - val_mse: 13.4101 - val_mae: 1.3349 - lr: 1.7299e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 6s - loss: 8.7705 - mse: 8.7705 - mae: 1.3246 - val_loss: 14.2926 - val_mse: 14.2926 - val_mae: 1.3476 - lr: 1.7299e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 6s - loss: 8.5217 - mse: 8.5217 - mae: 1.3038 - val_loss: 13.5544 - val_mse: 13.5544 - val_mae: 1.3239 - lr: 1.7299e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 7s - loss: 8.2115 - mse: 8.2115 - mae: 1.2925 - val_loss: 14.0128 - val_mse: 14.0128 - val_mae: 1.4058 - lr: 1.7299e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 6s - loss: 7.6882 - mse: 7.6882 - mae: 1.2700 - val_loss: 13.8845 - val_mse: 13.8845 - val_mae: 1.3297 - lr: 1.7299e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 6s - loss: 7.6963 - mse: 7.6963 - mae: 1.2521 - val_loss: 14.7155 - val_mse: 14.7155 - val_mae: 1.4964 - lr: 1.7299e-04 - 6s/epoch - 6ms/step\n",
            "Score for fold 4: loss of 14.71553897857666\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 7s - loss: 9.3615 - mse: 9.3615 - mae: 1.2803 - val_loss: 7.7472 - val_mse: 7.7472 - val_mae: 1.2765 - lr: 1.7299e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 7s - loss: 8.8399 - mse: 8.8399 - mae: 1.2543 - val_loss: 7.1458 - val_mse: 7.1458 - val_mae: 1.2622 - lr: 1.7299e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 6s - loss: 8.4449 - mse: 8.4449 - mae: 1.2354 - val_loss: 14.0761 - val_mse: 14.0761 - val_mae: 1.3061 - lr: 1.7299e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 6s - loss: 8.3079 - mse: 8.3079 - mae: 1.2157 - val_loss: 7.5172 - val_mse: 7.5172 - val_mae: 1.4110 - lr: 1.7299e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 6s - loss: 7.9958 - mse: 7.9958 - mae: 1.2019 - val_loss: 8.4634 - val_mse: 8.4634 - val_mae: 1.3517 - lr: 1.7299e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 7s - loss: 7.9438 - mse: 7.9438 - mae: 1.1847 - val_loss: 9.3232 - val_mse: 9.3232 - val_mae: 1.3180 - lr: 1.7299e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 7s - loss: 7.5485 - mse: 7.5485 - mae: 1.1712 - val_loss: 7.6757 - val_mse: 7.6757 - val_mae: 1.3393 - lr: 1.7299e-04 - 7s/epoch - 7ms/step\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-10-11 15:02:17,702]\u001b[0m Finished trial#14 resulted in value: 10.85. Current best value is 9.501999999999999 with parameters: {'activation': 'relu', 'num_hidden_layer': 4, 'i': 9, 'learning_rate': 0.00015599216930932077}.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Score for fold 5: loss of 7.675718307495117\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 11s - loss: 13.6193 - mse: 13.6193 - mae: 1.5432 - val_loss: 10.3136 - val_mse: 10.3136 - val_mae: 1.4524 - lr: 1.3935e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 11s - loss: 12.7916 - mse: 12.7916 - mae: 1.4850 - val_loss: 10.1358 - val_mse: 10.1358 - val_mae: 1.4028 - lr: 1.3935e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 11s - loss: 12.8811 - mse: 12.8811 - mae: 1.4770 - val_loss: 9.8785 - val_mse: 9.8785 - val_mae: 1.4618 - lr: 1.3935e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 11s - loss: 12.6408 - mse: 12.6408 - mae: 1.4643 - val_loss: 10.1078 - val_mse: 10.1078 - val_mae: 1.3710 - lr: 1.3935e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 11s - loss: 12.2719 - mse: 12.2719 - mae: 1.4588 - val_loss: 9.9568 - val_mse: 9.9568 - val_mae: 1.4834 - lr: 1.3935e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 11s - loss: 12.2463 - mse: 12.2463 - mae: 1.4485 - val_loss: 9.8261 - val_mse: 9.8261 - val_mae: 1.4887 - lr: 1.3935e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 11s - loss: 12.2867 - mse: 12.2867 - mae: 1.4377 - val_loss: 9.9059 - val_mse: 9.9059 - val_mae: 1.4714 - lr: 1.3935e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 11s - loss: 11.8772 - mse: 11.8772 - mae: 1.4239 - val_loss: 9.8233 - val_mse: 9.8233 - val_mae: 1.4355 - lr: 1.3935e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 11s - loss: 11.8789 - mse: 11.8789 - mae: 1.4147 - val_loss: 10.3712 - val_mse: 10.3712 - val_mae: 1.4462 - lr: 1.3935e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 11s - loss: 11.6630 - mse: 11.6630 - mae: 1.3961 - val_loss: 9.6218 - val_mse: 9.6218 - val_mae: 1.4468 - lr: 1.3935e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 11s - loss: 11.5852 - mse: 11.5852 - mae: 1.3862 - val_loss: 10.0384 - val_mse: 10.0384 - val_mae: 1.4782 - lr: 1.3935e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 11s - loss: 11.2715 - mse: 11.2715 - mae: 1.3713 - val_loss: 9.8343 - val_mse: 9.8343 - val_mae: 1.4640 - lr: 1.3935e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 13/100\n",
            "1000/1000 - 11s - loss: 10.9539 - mse: 10.9539 - mae: 1.3545 - val_loss: 9.8814 - val_mse: 9.8814 - val_mae: 1.4373 - lr: 1.3935e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 14/100\n",
            "1000/1000 - 10s - loss: 10.5570 - mse: 10.5570 - mae: 1.3389 - val_loss: 10.3579 - val_mse: 10.3579 - val_mae: 1.4795 - lr: 1.3935e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 15/100\n",
            "1000/1000 - 11s - loss: 10.2081 - mse: 10.2081 - mae: 1.3136 - val_loss: 10.2337 - val_mse: 10.2337 - val_mae: 1.4860 - lr: 1.3935e-04 - 11s/epoch - 11ms/step\n",
            "Score for fold 1: loss of 10.233705520629883\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 11s - loss: 11.0485 - mse: 11.0485 - mae: 1.3615 - val_loss: 6.7376 - val_mse: 6.7376 - val_mae: 1.3451 - lr: 1.3935e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 11s - loss: 10.4667 - mse: 10.4667 - mae: 1.3321 - val_loss: 6.9043 - val_mse: 6.9043 - val_mae: 1.3043 - lr: 1.3935e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 11s - loss: 10.2903 - mse: 10.2903 - mae: 1.3092 - val_loss: 7.0903 - val_mse: 7.0903 - val_mae: 1.3592 - lr: 1.3935e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 11s - loss: 9.6244 - mse: 9.6244 - mae: 1.2832 - val_loss: 7.3850 - val_mse: 7.3850 - val_mae: 1.3498 - lr: 1.3935e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 11s - loss: 9.4132 - mse: 9.4132 - mae: 1.2620 - val_loss: 7.1757 - val_mse: 7.1757 - val_mae: 1.3060 - lr: 1.3935e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 11s - loss: 9.0455 - mse: 9.0455 - mae: 1.2349 - val_loss: 7.3797 - val_mse: 7.3797 - val_mae: 1.3933 - lr: 1.3935e-04 - 11s/epoch - 11ms/step\n",
            "Score for fold 2: loss of 7.379703521728516\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 11s - loss: 6.5703 - mse: 6.5703 - mae: 1.2584 - val_loss: 17.4605 - val_mse: 17.4605 - val_mae: 1.2284 - lr: 1.3935e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 10s - loss: 6.1939 - mse: 6.1939 - mae: 1.2360 - val_loss: 17.3591 - val_mse: 17.3591 - val_mae: 1.2373 - lr: 1.3935e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 11s - loss: 6.0740 - mse: 6.0740 - mae: 1.2084 - val_loss: 17.7707 - val_mse: 17.7707 - val_mae: 1.2891 - lr: 1.3935e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 11s - loss: 5.4704 - mse: 5.4704 - mae: 1.1799 - val_loss: 18.7414 - val_mse: 18.7414 - val_mae: 1.3040 - lr: 1.3935e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 11s - loss: 5.1788 - mse: 5.1788 - mae: 1.1569 - val_loss: 17.6602 - val_mse: 17.6602 - val_mae: 1.3224 - lr: 1.3935e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 11s - loss: 5.3737 - mse: 5.3737 - mae: 1.1407 - val_loss: 18.9204 - val_mse: 18.9204 - val_mae: 1.3606 - lr: 1.3935e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 11s - loss: 4.8103 - mse: 4.8103 - mae: 1.1231 - val_loss: 19.0102 - val_mse: 19.0102 - val_mae: 1.3263 - lr: 1.3935e-04 - 11s/epoch - 11ms/step\n",
            "Score for fold 3: loss of 19.01024055480957\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 11s - loss: 8.7484 - mse: 8.7484 - mae: 1.1912 - val_loss: 3.2192 - val_mse: 3.2192 - val_mae: 1.0477 - lr: 1.3935e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 11s - loss: 8.1007 - mse: 8.1007 - mae: 1.1600 - val_loss: 3.1877 - val_mse: 3.1877 - val_mae: 1.0531 - lr: 1.3935e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 11s - loss: 8.0951 - mse: 8.0951 - mae: 1.1406 - val_loss: 3.3900 - val_mse: 3.3900 - val_mae: 1.1121 - lr: 1.3935e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 11s - loss: 7.6883 - mse: 7.6883 - mae: 1.1208 - val_loss: 3.2123 - val_mse: 3.2123 - val_mae: 1.0669 - lr: 1.3935e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 11s - loss: 7.1849 - mse: 7.1849 - mae: 1.0916 - val_loss: 3.4359 - val_mse: 3.4359 - val_mae: 1.1621 - lr: 1.3935e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 11s - loss: 6.7744 - mse: 6.7744 - mae: 1.0698 - val_loss: 3.4576 - val_mse: 3.4576 - val_mae: 1.1911 - lr: 1.3935e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 11s - loss: 6.6144 - mse: 6.6144 - mae: 1.0541 - val_loss: 3.5362 - val_mse: 3.5362 - val_mae: 1.1815 - lr: 1.3935e-04 - 11s/epoch - 11ms/step\n",
            "Score for fold 4: loss of 3.536217212677002\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 11s - loss: 6.2647 - mse: 6.2647 - mae: 1.0931 - val_loss: 4.3433 - val_mse: 4.3433 - val_mae: 1.0386 - lr: 1.3935e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 11s - loss: 5.9267 - mse: 5.9267 - mae: 1.0545 - val_loss: 4.6486 - val_mse: 4.6486 - val_mae: 1.0240 - lr: 1.3935e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 11s - loss: 5.7604 - mse: 5.7604 - mae: 1.0300 - val_loss: 4.3086 - val_mse: 4.3086 - val_mae: 1.0813 - lr: 1.3935e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 11s - loss: 5.4246 - mse: 5.4246 - mae: 1.0157 - val_loss: 4.8926 - val_mse: 4.8926 - val_mae: 1.0213 - lr: 1.3935e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 11s - loss: 5.1068 - mse: 5.1068 - mae: 0.9903 - val_loss: 5.3855 - val_mse: 5.3855 - val_mae: 1.0746 - lr: 1.3935e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 11s - loss: 5.1506 - mse: 5.1506 - mae: 0.9776 - val_loss: 6.1838 - val_mse: 6.1838 - val_mae: 1.0731 - lr: 1.3935e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 11s - loss: 5.0880 - mse: 5.0880 - mae: 0.9676 - val_loss: 5.6144 - val_mse: 5.6144 - val_mae: 1.0980 - lr: 1.3935e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 11s - loss: 4.7720 - mse: 4.7720 - mae: 0.9471 - val_loss: 6.1715 - val_mse: 6.1715 - val_mae: 1.0997 - lr: 1.3935e-04 - 11s/epoch - 11ms/step\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-10-11 15:10:13,688]\u001b[0m Finished trial#15 resulted in value: 9.266000000000002. Current best value is 9.266000000000002 with parameters: {'activation': 'relu', 'num_hidden_layer': 4, 'i': 10, 'learning_rate': 0.00013934585348848375}.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Score for fold 5: loss of 6.171487331390381\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 5s - loss: 13.7791 - mse: 13.7791 - mae: 1.5499 - val_loss: 10.1034 - val_mse: 10.1034 - val_mae: 1.5434 - lr: 5.2393e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 4s - loss: 13.2682 - mse: 13.2682 - mae: 1.5028 - val_loss: 9.7541 - val_mse: 9.7541 - val_mae: 1.3775 - lr: 5.2393e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 4s - loss: 12.8678 - mse: 12.8678 - mae: 1.4790 - val_loss: 9.5399 - val_mse: 9.5399 - val_mae: 1.4758 - lr: 5.2393e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 4s - loss: 12.5845 - mse: 12.5845 - mae: 1.4718 - val_loss: 9.5896 - val_mse: 9.5896 - val_mae: 1.4879 - lr: 5.2393e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 5s - loss: 12.5016 - mse: 12.5016 - mae: 1.4604 - val_loss: 10.0545 - val_mse: 10.0545 - val_mae: 1.5234 - lr: 5.2393e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 4s - loss: 12.4478 - mse: 12.4478 - mae: 1.4536 - val_loss: 9.5638 - val_mse: 9.5638 - val_mae: 1.4473 - lr: 5.2393e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 5s - loss: 12.1845 - mse: 12.1845 - mae: 1.4453 - val_loss: 9.7824 - val_mse: 9.7824 - val_mae: 1.4052 - lr: 5.2393e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 4s - loss: 12.2013 - mse: 12.2013 - mae: 1.4451 - val_loss: 9.4482 - val_mse: 9.4482 - val_mae: 1.4680 - lr: 5.2393e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 4s - loss: 12.2368 - mse: 12.2368 - mae: 1.4365 - val_loss: 9.7102 - val_mse: 9.7102 - val_mae: 1.4214 - lr: 5.2393e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 4s - loss: 11.8598 - mse: 11.8598 - mae: 1.4280 - val_loss: 9.5862 - val_mse: 9.5862 - val_mae: 1.4442 - lr: 5.2393e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 4s - loss: 11.7570 - mse: 11.7570 - mae: 1.4181 - val_loss: 9.7480 - val_mse: 9.7480 - val_mae: 1.4448 - lr: 5.2393e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 4s - loss: 11.8144 - mse: 11.8144 - mae: 1.4148 - val_loss: 9.9397 - val_mse: 9.9397 - val_mae: 1.5080 - lr: 5.2393e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 13/100\n",
            "1000/1000 - 4s - loss: 11.5379 - mse: 11.5379 - mae: 1.4084 - val_loss: 9.7463 - val_mse: 9.7463 - val_mae: 1.4806 - lr: 5.2393e-04 - 4s/epoch - 4ms/step\n",
            "Score for fold 1: loss of 9.746298789978027\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 5s - loss: 12.2813 - mse: 12.2813 - mae: 1.4139 - val_loss: 8.2163 - val_mse: 8.2163 - val_mae: 1.4278 - lr: 5.2393e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 4s - loss: 11.7899 - mse: 11.7899 - mae: 1.4039 - val_loss: 8.4087 - val_mse: 8.4087 - val_mae: 1.4025 - lr: 5.2393e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 5s - loss: 11.5069 - mse: 11.5069 - mae: 1.3959 - val_loss: 8.3559 - val_mse: 8.3559 - val_mae: 1.4053 - lr: 5.2393e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 4s - loss: 11.6092 - mse: 11.6092 - mae: 1.3866 - val_loss: 8.7629 - val_mse: 8.7629 - val_mae: 1.3774 - lr: 5.2393e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 4s - loss: 11.2018 - mse: 11.2018 - mae: 1.3755 - val_loss: 8.4108 - val_mse: 8.4108 - val_mae: 1.4153 - lr: 5.2393e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 4s - loss: 11.4498 - mse: 11.4498 - mae: 1.3725 - val_loss: 8.6057 - val_mse: 8.6057 - val_mae: 1.3960 - lr: 5.2393e-04 - 4s/epoch - 4ms/step\n",
            "Score for fold 2: loss of 8.605731964111328\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 4s - loss: 10.9087 - mse: 10.9087 - mae: 1.3809 - val_loss: 9.1262 - val_mse: 9.1262 - val_mae: 1.3340 - lr: 5.2393e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 4s - loss: 10.9411 - mse: 10.9411 - mae: 1.3703 - val_loss: 9.1866 - val_mse: 9.1866 - val_mae: 1.3614 - lr: 5.2393e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 4s - loss: 10.7488 - mse: 10.7488 - mae: 1.3571 - val_loss: 8.9487 - val_mse: 8.9487 - val_mae: 1.3827 - lr: 5.2393e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 4s - loss: 10.4982 - mse: 10.4982 - mae: 1.3504 - val_loss: 9.1633 - val_mse: 9.1633 - val_mae: 1.3662 - lr: 5.2393e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 4s - loss: 10.4393 - mse: 10.4393 - mae: 1.3411 - val_loss: 9.3678 - val_mse: 9.3678 - val_mae: 1.3816 - lr: 5.2393e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 4s - loss: 10.1699 - mse: 10.1699 - mae: 1.3270 - val_loss: 10.5390 - val_mse: 10.5390 - val_mae: 1.3621 - lr: 5.2393e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 4s - loss: 9.9056 - mse: 9.9056 - mae: 1.3134 - val_loss: 9.8052 - val_mse: 9.8052 - val_mae: 1.3686 - lr: 5.2393e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 4s - loss: 9.5559 - mse: 9.5559 - mae: 1.3024 - val_loss: 9.2305 - val_mse: 9.2305 - val_mae: 1.3839 - lr: 5.2393e-04 - 4s/epoch - 4ms/step\n",
            "Score for fold 3: loss of 9.230504989624023\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 4s - loss: 9.7150 - mse: 9.7150 - mae: 1.3349 - val_loss: 9.0572 - val_mse: 9.0572 - val_mae: 1.2874 - lr: 5.2393e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 4s - loss: 9.6392 - mse: 9.6392 - mae: 1.3184 - val_loss: 8.9660 - val_mse: 8.9660 - val_mae: 1.2940 - lr: 5.2393e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 4s - loss: 9.6357 - mse: 9.6357 - mae: 1.3021 - val_loss: 9.1685 - val_mse: 9.1685 - val_mae: 1.2918 - lr: 5.2393e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 5s - loss: 9.1558 - mse: 9.1558 - mae: 1.2878 - val_loss: 9.2981 - val_mse: 9.2981 - val_mae: 1.3062 - lr: 5.2393e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 5s - loss: 9.1377 - mse: 9.1377 - mae: 1.2752 - val_loss: 9.6894 - val_mse: 9.6894 - val_mae: 1.3309 - lr: 5.2393e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 4s - loss: 8.8442 - mse: 8.8442 - mae: 1.2655 - val_loss: 9.5881 - val_mse: 9.5881 - val_mae: 1.3637 - lr: 5.2393e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 4s - loss: 8.7140 - mse: 8.7140 - mae: 1.2488 - val_loss: 9.3773 - val_mse: 9.3773 - val_mae: 1.3523 - lr: 5.2393e-04 - 4s/epoch - 4ms/step\n",
            "Score for fold 4: loss of 9.377344131469727\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 4s - loss: 7.5021 - mse: 7.5021 - mae: 1.2655 - val_loss: 14.0778 - val_mse: 14.0778 - val_mae: 1.2752 - lr: 5.2393e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 4s - loss: 7.5664 - mse: 7.5664 - mae: 1.2505 - val_loss: 14.1518 - val_mse: 14.1518 - val_mae: 1.2499 - lr: 5.2393e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 4s - loss: 7.1914 - mse: 7.1914 - mae: 1.2306 - val_loss: 15.4591 - val_mse: 15.4591 - val_mae: 1.2939 - lr: 5.2393e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 4s - loss: 7.0102 - mse: 7.0102 - mae: 1.2202 - val_loss: 14.5805 - val_mse: 14.5805 - val_mae: 1.2854 - lr: 5.2393e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 4s - loss: 6.7975 - mse: 6.7975 - mae: 1.2047 - val_loss: 14.4817 - val_mse: 14.4817 - val_mae: 1.2819 - lr: 5.2393e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 4s - loss: 6.6679 - mse: 6.6679 - mae: 1.1907 - val_loss: 14.5312 - val_mse: 14.5312 - val_mae: 1.3428 - lr: 5.2393e-04 - 4s/epoch - 4ms/step\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-10-11 15:13:17,410]\u001b[0m Finished trial#16 resulted in value: 10.3. Current best value is 9.266000000000002 with parameters: {'activation': 'relu', 'num_hidden_layer': 4, 'i': 10, 'learning_rate': 0.00013934585348848375}.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Score for fold 5: loss of 14.531240463256836\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 4s - loss: 13.5333 - mse: 13.5333 - mae: 1.5636 - val_loss: 12.6508 - val_mse: 12.6508 - val_mae: 1.5192 - lr: 0.0028 - 4s/epoch - 4ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 4s - loss: 12.6537 - mse: 12.6537 - mae: 1.5134 - val_loss: 13.3939 - val_mse: 13.3939 - val_mae: 1.4974 - lr: 0.0028 - 4s/epoch - 4ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 4s - loss: 12.4605 - mse: 12.4605 - mae: 1.4878 - val_loss: 13.0770 - val_mse: 13.0770 - val_mae: 1.5001 - lr: 0.0028 - 4s/epoch - 4ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 4s - loss: 12.1813 - mse: 12.1813 - mae: 1.4745 - val_loss: 12.0186 - val_mse: 12.0186 - val_mae: 1.5485 - lr: 0.0028 - 4s/epoch - 4ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 4s - loss: 12.1256 - mse: 12.1256 - mae: 1.4697 - val_loss: 12.6967 - val_mse: 12.6967 - val_mae: 1.4534 - lr: 0.0028 - 4s/epoch - 4ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 4s - loss: 12.0550 - mse: 12.0550 - mae: 1.4623 - val_loss: 12.1865 - val_mse: 12.1865 - val_mae: 1.5502 - lr: 0.0028 - 4s/epoch - 4ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 4s - loss: 11.9422 - mse: 11.9422 - mae: 1.4584 - val_loss: 12.8343 - val_mse: 12.8343 - val_mae: 1.4566 - lr: 0.0028 - 4s/epoch - 4ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 4s - loss: 11.9225 - mse: 11.9225 - mae: 1.4558 - val_loss: 12.6910 - val_mse: 12.6910 - val_mae: 1.4944 - lr: 0.0028 - 4s/epoch - 4ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 3s - loss: 11.9071 - mse: 11.9071 - mae: 1.4488 - val_loss: 13.2550 - val_mse: 13.2550 - val_mae: 1.4716 - lr: 0.0028 - 3s/epoch - 3ms/step\n",
            "Score for fold 1: loss of 13.254986763000488\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 4s - loss: 12.1270 - mse: 12.1270 - mae: 1.4374 - val_loss: 9.2127 - val_mse: 9.2127 - val_mae: 1.4370 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 4s - loss: 11.8122 - mse: 11.8122 - mae: 1.4233 - val_loss: 9.2483 - val_mse: 9.2483 - val_mae: 1.4344 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 3s - loss: 11.7700 - mse: 11.7700 - mae: 1.4164 - val_loss: 9.2241 - val_mse: 9.2241 - val_mae: 1.3962 - lr: 0.0010 - 3s/epoch - 3ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 4s - loss: 11.4580 - mse: 11.4580 - mae: 1.4083 - val_loss: 9.5587 - val_mse: 9.5587 - val_mae: 1.4351 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 4s - loss: 11.3932 - mse: 11.3932 - mae: 1.3963 - val_loss: 9.3674 - val_mse: 9.3674 - val_mae: 1.4047 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 4s - loss: 11.3849 - mse: 11.3849 - mae: 1.3942 - val_loss: 9.5204 - val_mse: 9.5204 - val_mae: 1.4481 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Score for fold 2: loss of 9.520448684692383\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 4s - loss: 9.5841 - mse: 9.5841 - mae: 1.3950 - val_loss: 15.6113 - val_mse: 15.6113 - val_mae: 1.4014 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 4s - loss: 9.4529 - mse: 9.4529 - mae: 1.3849 - val_loss: 16.0348 - val_mse: 16.0348 - val_mae: 1.4153 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 3s - loss: 9.1712 - mse: 9.1712 - mae: 1.3785 - val_loss: 16.0351 - val_mse: 16.0351 - val_mae: 1.3982 - lr: 0.0010 - 3s/epoch - 3ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 4s - loss: 9.1334 - mse: 9.1334 - mae: 1.3704 - val_loss: 16.6738 - val_mse: 16.6738 - val_mae: 1.4244 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 3s - loss: 8.8483 - mse: 8.8483 - mae: 1.3610 - val_loss: 17.5561 - val_mse: 17.5561 - val_mae: 1.4269 - lr: 0.0010 - 3s/epoch - 3ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 3s - loss: 8.8311 - mse: 8.8311 - mae: 1.3578 - val_loss: 17.0713 - val_mse: 17.0713 - val_mae: 1.4576 - lr: 0.0010 - 3s/epoch - 3ms/step\n",
            "Score for fold 3: loss of 17.071313858032227\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 4s - loss: 10.8305 - mse: 10.8305 - mae: 1.3760 - val_loss: 9.6833 - val_mse: 9.6833 - val_mae: 1.3877 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 4s - loss: 10.2966 - mse: 10.2966 - mae: 1.3666 - val_loss: 9.0035 - val_mse: 9.0035 - val_mae: 1.3953 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 4s - loss: 10.4083 - mse: 10.4083 - mae: 1.3608 - val_loss: 9.0669 - val_mse: 9.0669 - val_mae: 1.3518 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 4s - loss: 10.2609 - mse: 10.2609 - mae: 1.3480 - val_loss: 9.1902 - val_mse: 9.1902 - val_mae: 1.3700 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 4s - loss: 10.0772 - mse: 10.0772 - mae: 1.3424 - val_loss: 9.2948 - val_mse: 9.2948 - val_mae: 1.3969 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 4s - loss: 10.0546 - mse: 10.0546 - mae: 1.3381 - val_loss: 9.2974 - val_mse: 9.2974 - val_mae: 1.3564 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 4s - loss: 9.8543 - mse: 9.8543 - mae: 1.3317 - val_loss: 9.7212 - val_mse: 9.7212 - val_mae: 1.3970 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Score for fold 4: loss of 9.721163749694824\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 4s - loss: 10.3892 - mse: 10.3892 - mae: 1.3616 - val_loss: 7.4825 - val_mse: 7.4825 - val_mae: 1.2724 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 4s - loss: 10.2045 - mse: 10.2045 - mae: 1.3480 - val_loss: 7.4449 - val_mse: 7.4449 - val_mae: 1.2964 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 4s - loss: 10.0895 - mse: 10.0895 - mae: 1.3357 - val_loss: 7.5916 - val_mse: 7.5916 - val_mae: 1.3414 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 4s - loss: 10.0880 - mse: 10.0880 - mae: 1.3304 - val_loss: 7.7253 - val_mse: 7.7253 - val_mae: 1.3625 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 4s - loss: 9.9419 - mse: 9.9419 - mae: 1.3284 - val_loss: 7.6916 - val_mse: 7.6916 - val_mae: 1.3343 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 4s - loss: 9.7539 - mse: 9.7539 - mae: 1.3177 - val_loss: 7.8254 - val_mse: 7.8254 - val_mae: 1.3599 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 4s - loss: 9.5922 - mse: 9.5922 - mae: 1.3131 - val_loss: 7.8312 - val_mse: 7.8312 - val_mae: 1.3708 - lr: 0.0010 - 4s/epoch - 4ms/step\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-10-11 15:15:27,485]\u001b[0m Finished trial#17 resulted in value: 11.478. Current best value is 9.266000000000002 with parameters: {'activation': 'relu', 'num_hidden_layer': 4, 'i': 10, 'learning_rate': 0.00013934585348848375}.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Score for fold 5: loss of 7.831162929534912\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 12s - loss: 14.8257 - mse: 14.8257 - mae: 1.6307 - val_loss: 12.5413 - val_mse: 12.5413 - val_mae: 1.7159 - lr: 1.9632e-04 - 12s/epoch - 12ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 11s - loss: 14.0096 - mse: 14.0096 - mae: 1.5416 - val_loss: 12.2475 - val_mse: 12.2475 - val_mae: 1.4555 - lr: 1.9632e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 11s - loss: 13.7985 - mse: 13.7985 - mae: 1.5187 - val_loss: 11.7365 - val_mse: 11.7365 - val_mae: 1.5241 - lr: 1.9632e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 11s - loss: 13.7696 - mse: 13.7696 - mae: 1.5207 - val_loss: 11.8661 - val_mse: 11.8661 - val_mae: 1.5884 - lr: 1.9632e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 11s - loss: 13.5913 - mse: 13.5913 - mae: 1.5053 - val_loss: 11.6968 - val_mse: 11.6968 - val_mae: 1.5616 - lr: 1.9632e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 11s - loss: 13.5025 - mse: 13.5025 - mae: 1.4985 - val_loss: 11.5411 - val_mse: 11.5411 - val_mae: 1.4747 - lr: 1.9632e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 11s - loss: 13.4549 - mse: 13.4549 - mae: 1.4934 - val_loss: 11.7927 - val_mse: 11.7927 - val_mae: 1.5836 - lr: 1.9632e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 11s - loss: 13.3477 - mse: 13.3477 - mae: 1.4905 - val_loss: 11.5569 - val_mse: 11.5569 - val_mae: 1.4588 - lr: 1.9632e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 11s - loss: 13.2105 - mse: 13.2105 - mae: 1.4836 - val_loss: 11.5444 - val_mse: 11.5444 - val_mae: 1.4671 - lr: 1.9632e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 11s - loss: 13.1911 - mse: 13.1911 - mae: 1.4787 - val_loss: 11.5620 - val_mse: 11.5620 - val_mae: 1.4974 - lr: 1.9632e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 11s - loss: 13.0979 - mse: 13.0979 - mae: 1.4798 - val_loss: 12.1672 - val_mse: 12.1672 - val_mae: 1.4207 - lr: 1.9632e-04 - 11s/epoch - 11ms/step\n",
            "Score for fold 1: loss of 12.167173385620117\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 11s - loss: 12.6065 - mse: 12.6065 - mae: 1.4855 - val_loss: 13.6543 - val_mse: 13.6543 - val_mae: 1.3908 - lr: 1.9632e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 11s - loss: 12.4309 - mse: 12.4309 - mae: 1.4779 - val_loss: 13.7257 - val_mse: 13.7257 - val_mae: 1.4674 - lr: 1.9632e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 11s - loss: 12.3798 - mse: 12.3798 - mae: 1.4713 - val_loss: 13.8074 - val_mse: 13.8074 - val_mae: 1.4114 - lr: 1.9632e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 11s - loss: 12.3174 - mse: 12.3174 - mae: 1.4700 - val_loss: 13.6864 - val_mse: 13.6864 - val_mae: 1.5507 - lr: 1.9632e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 11s - loss: 12.2666 - mse: 12.2666 - mae: 1.4670 - val_loss: 13.7104 - val_mse: 13.7104 - val_mae: 1.5019 - lr: 1.9632e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 10s - loss: 12.1308 - mse: 12.1308 - mae: 1.4562 - val_loss: 13.6689 - val_mse: 13.6689 - val_mae: 1.4199 - lr: 1.9632e-04 - 10s/epoch - 10ms/step\n",
            "Score for fold 2: loss of 13.66893482208252\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 11s - loss: 10.9534 - mse: 10.9534 - mae: 1.4514 - val_loss: 17.9474 - val_mse: 17.9474 - val_mae: 1.3861 - lr: 1.9632e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 11s - loss: 10.7526 - mse: 10.7526 - mae: 1.4408 - val_loss: 18.1247 - val_mse: 18.1247 - val_mae: 1.4247 - lr: 1.9632e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 11s - loss: 10.5774 - mse: 10.5774 - mae: 1.4313 - val_loss: 17.8453 - val_mse: 17.8453 - val_mae: 1.5159 - lr: 1.9632e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 11s - loss: 10.5363 - mse: 10.5363 - mae: 1.4256 - val_loss: 18.0078 - val_mse: 18.0078 - val_mae: 1.4219 - lr: 1.9632e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 11s - loss: 10.3891 - mse: 10.3891 - mae: 1.4171 - val_loss: 17.9124 - val_mse: 17.9124 - val_mae: 1.5007 - lr: 1.9632e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 11s - loss: 10.2728 - mse: 10.2728 - mae: 1.4090 - val_loss: 17.9514 - val_mse: 17.9514 - val_mae: 1.4861 - lr: 1.9632e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 11s - loss: 10.0229 - mse: 10.0229 - mae: 1.3942 - val_loss: 17.9873 - val_mse: 17.9873 - val_mae: 1.4277 - lr: 1.9632e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 11s - loss: 9.9355 - mse: 9.9355 - mae: 1.3872 - val_loss: 17.7294 - val_mse: 17.7294 - val_mae: 1.5051 - lr: 1.9632e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 11s - loss: 9.7391 - mse: 9.7391 - mae: 1.3729 - val_loss: 17.7835 - val_mse: 17.7835 - val_mae: 1.6037 - lr: 1.9632e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 11s - loss: 9.5668 - mse: 9.5668 - mae: 1.3662 - val_loss: 17.5951 - val_mse: 17.5951 - val_mae: 1.4725 - lr: 1.9632e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 11s - loss: 9.3593 - mse: 9.3593 - mae: 1.3527 - val_loss: 17.8009 - val_mse: 17.8009 - val_mae: 1.4947 - lr: 1.9632e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 11s - loss: 9.2084 - mse: 9.2084 - mae: 1.3476 - val_loss: 17.8281 - val_mse: 17.8281 - val_mae: 1.5784 - lr: 1.9632e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 13/100\n",
            "1000/1000 - 11s - loss: 8.9737 - mse: 8.9737 - mae: 1.3253 - val_loss: 17.7839 - val_mse: 17.7839 - val_mae: 1.4614 - lr: 1.9632e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 14/100\n",
            "1000/1000 - 11s - loss: 8.9045 - mse: 8.9045 - mae: 1.3105 - val_loss: 18.1397 - val_mse: 18.1397 - val_mae: 1.5129 - lr: 1.9632e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 15/100\n",
            "1000/1000 - 11s - loss: 8.7325 - mse: 8.7325 - mae: 1.2932 - val_loss: 17.4933 - val_mse: 17.4933 - val_mae: 1.5957 - lr: 1.9632e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 16/100\n",
            "1000/1000 - 11s - loss: 8.3508 - mse: 8.3508 - mae: 1.2736 - val_loss: 17.5359 - val_mse: 17.5359 - val_mae: 1.5062 - lr: 1.9632e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 17/100\n",
            "1000/1000 - 11s - loss: 8.1102 - mse: 8.1102 - mae: 1.2609 - val_loss: 17.4054 - val_mse: 17.4054 - val_mae: 1.5308 - lr: 1.9632e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 18/100\n",
            "1000/1000 - 11s - loss: 7.9577 - mse: 7.9577 - mae: 1.2410 - val_loss: 17.4681 - val_mse: 17.4681 - val_mae: 1.5430 - lr: 1.9632e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 19/100\n",
            "1000/1000 - 11s - loss: 7.6183 - mse: 7.6183 - mae: 1.2186 - val_loss: 17.4356 - val_mse: 17.4356 - val_mae: 1.5511 - lr: 1.9632e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 20/100\n",
            "1000/1000 - 11s - loss: 7.3474 - mse: 7.3474 - mae: 1.1989 - val_loss: 17.3499 - val_mse: 17.3499 - val_mae: 1.5817 - lr: 1.9632e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 21/100\n",
            "1000/1000 - 11s - loss: 7.1430 - mse: 7.1430 - mae: 1.1814 - val_loss: 17.3095 - val_mse: 17.3095 - val_mae: 1.5791 - lr: 1.9632e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 22/100\n",
            "1000/1000 - 11s - loss: 6.8657 - mse: 6.8657 - mae: 1.1567 - val_loss: 17.2336 - val_mse: 17.2336 - val_mae: 1.5959 - lr: 1.9632e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 23/100\n",
            "1000/1000 - 11s - loss: 6.5443 - mse: 6.5443 - mae: 1.1300 - val_loss: 17.7605 - val_mse: 17.7605 - val_mae: 1.6002 - lr: 1.9632e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 24/100\n",
            "1000/1000 - 11s - loss: 6.5104 - mse: 6.5104 - mae: 1.1039 - val_loss: 17.4852 - val_mse: 17.4852 - val_mae: 1.6420 - lr: 1.9632e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 25/100\n",
            "1000/1000 - 11s - loss: 6.0877 - mse: 6.0877 - mae: 1.0764 - val_loss: 17.4221 - val_mse: 17.4221 - val_mae: 1.6439 - lr: 1.9632e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 26/100\n",
            "1000/1000 - 11s - loss: 5.9047 - mse: 5.9047 - mae: 1.0472 - val_loss: 17.9288 - val_mse: 17.9288 - val_mae: 1.6344 - lr: 1.9632e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 27/100\n",
            "1000/1000 - 11s - loss: 5.5989 - mse: 5.5989 - mae: 1.0212 - val_loss: 17.3869 - val_mse: 17.3869 - val_mae: 1.6436 - lr: 1.9632e-04 - 11s/epoch - 11ms/step\n",
            "Score for fold 3: loss of 17.386919021606445\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 11s - loss: 9.2704 - mse: 9.2704 - mae: 1.1994 - val_loss: 3.2263 - val_mse: 3.2263 - val_mae: 1.0336 - lr: 1.9632e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 11s - loss: 8.7856 - mse: 8.7856 - mae: 1.1539 - val_loss: 3.4838 - val_mse: 3.4838 - val_mae: 1.0788 - lr: 1.9632e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 11s - loss: 8.3903 - mse: 8.3903 - mae: 1.1110 - val_loss: 3.5577 - val_mse: 3.5577 - val_mae: 1.1100 - lr: 1.9632e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 11s - loss: 8.0820 - mse: 8.0820 - mae: 1.0802 - val_loss: 3.5486 - val_mse: 3.5486 - val_mae: 1.1034 - lr: 1.9632e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 11s - loss: 7.7216 - mse: 7.7216 - mae: 1.0414 - val_loss: 3.6324 - val_mse: 3.6324 - val_mae: 1.1096 - lr: 1.9632e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 11s - loss: 7.3960 - mse: 7.3960 - mae: 1.0090 - val_loss: 3.8483 - val_mse: 3.8483 - val_mae: 1.1493 - lr: 1.9632e-04 - 11s/epoch - 11ms/step\n",
            "Score for fold 4: loss of 3.848275899887085\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 11s - loss: 6.6733 - mse: 6.6733 - mae: 1.0409 - val_loss: 5.8986 - val_mse: 5.8986 - val_mae: 0.9517 - lr: 1.9632e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 11s - loss: 6.3022 - mse: 6.3022 - mae: 0.9976 - val_loss: 6.0252 - val_mse: 6.0252 - val_mae: 0.9615 - lr: 1.9632e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 12s - loss: 6.0519 - mse: 6.0519 - mae: 0.9553 - val_loss: 6.1925 - val_mse: 6.1925 - val_mae: 1.0408 - lr: 1.9632e-04 - 12s/epoch - 12ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 11s - loss: 5.7981 - mse: 5.7981 - mae: 0.9273 - val_loss: 6.2481 - val_mse: 6.2481 - val_mae: 1.0331 - lr: 1.9632e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 11s - loss: 5.5619 - mse: 5.5619 - mae: 0.8882 - val_loss: 6.3075 - val_mse: 6.3075 - val_mae: 1.0430 - lr: 1.9632e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 11s - loss: 5.3924 - mse: 5.3924 - mae: 0.8552 - val_loss: 6.3022 - val_mse: 6.3022 - val_mae: 1.0517 - lr: 1.9632e-04 - 11s/epoch - 11ms/step\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-10-11 15:25:50,128]\u001b[0m Finished trial#18 resulted in value: 10.676. Current best value is 9.266000000000002 with parameters: {'activation': 'relu', 'num_hidden_layer': 4, 'i': 10, 'learning_rate': 0.00013934585348848375}.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Score for fold 5: loss of 6.302193641662598\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 3s - loss: 14.9050 - mse: 14.9050 - mae: 1.5594 - val_loss: 9.1192 - val_mse: 9.1192 - val_mae: 1.4632 - lr: 5.0093e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 13.4634 - mse: 13.4634 - mae: 1.5054 - val_loss: 8.5778 - val_mse: 8.5778 - val_mae: 1.5085 - lr: 5.0093e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 13.2442 - mse: 13.2442 - mae: 1.4869 - val_loss: 8.4569 - val_mse: 8.4569 - val_mae: 1.4222 - lr: 5.0093e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 13.0167 - mse: 13.0167 - mae: 1.4729 - val_loss: 8.3133 - val_mse: 8.3133 - val_mae: 1.4377 - lr: 5.0093e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 13.0446 - mse: 13.0446 - mae: 1.4674 - val_loss: 8.2673 - val_mse: 8.2673 - val_mae: 1.4159 - lr: 5.0093e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 12.9691 - mse: 12.9691 - mae: 1.4573 - val_loss: 8.5367 - val_mse: 8.5367 - val_mae: 1.4504 - lr: 5.0093e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 12.7759 - mse: 12.7759 - mae: 1.4518 - val_loss: 8.6156 - val_mse: 8.6156 - val_mae: 1.3991 - lr: 5.0093e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 2s - loss: 12.8072 - mse: 12.8072 - mae: 1.4496 - val_loss: 8.4873 - val_mse: 8.4873 - val_mae: 1.4515 - lr: 5.0093e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 2s - loss: 12.7361 - mse: 12.7361 - mae: 1.4432 - val_loss: 7.8476 - val_mse: 7.8476 - val_mae: 1.4404 - lr: 5.0093e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 2s - loss: 12.6266 - mse: 12.6266 - mae: 1.4411 - val_loss: 8.0027 - val_mse: 8.0027 - val_mae: 1.4476 - lr: 5.0093e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 2s - loss: 12.5370 - mse: 12.5370 - mae: 1.4396 - val_loss: 8.0488 - val_mse: 8.0488 - val_mae: 1.4019 - lr: 5.0093e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 2s - loss: 12.6176 - mse: 12.6176 - mae: 1.4352 - val_loss: 8.0195 - val_mse: 8.0195 - val_mae: 1.4094 - lr: 5.0093e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 13/100\n",
            "1000/1000 - 2s - loss: 12.5138 - mse: 12.5138 - mae: 1.4322 - val_loss: 8.0049 - val_mse: 8.0049 - val_mae: 1.4448 - lr: 5.0093e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 14/100\n",
            "1000/1000 - 2s - loss: 12.5271 - mse: 12.5271 - mae: 1.4260 - val_loss: 7.7348 - val_mse: 7.7348 - val_mae: 1.4702 - lr: 5.0093e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 15/100\n",
            "1000/1000 - 2s - loss: 12.4123 - mse: 12.4123 - mae: 1.4250 - val_loss: 8.5014 - val_mse: 8.5014 - val_mae: 1.4488 - lr: 5.0093e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 16/100\n",
            "1000/1000 - 2s - loss: 12.4058 - mse: 12.4058 - mae: 1.4219 - val_loss: 8.1093 - val_mse: 8.1093 - val_mae: 1.4216 - lr: 5.0093e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 17/100\n",
            "1000/1000 - 2s - loss: 12.2565 - mse: 12.2565 - mae: 1.4149 - val_loss: 8.0567 - val_mse: 8.0567 - val_mae: 1.4353 - lr: 5.0093e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 18/100\n",
            "1000/1000 - 2s - loss: 12.1658 - mse: 12.1658 - mae: 1.4136 - val_loss: 8.2085 - val_mse: 8.2085 - val_mae: 1.4439 - lr: 5.0093e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 19/100\n",
            "1000/1000 - 2s - loss: 12.1807 - mse: 12.1807 - mae: 1.4116 - val_loss: 7.6691 - val_mse: 7.6691 - val_mae: 1.4117 - lr: 5.0093e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 20/100\n",
            "1000/1000 - 2s - loss: 12.1618 - mse: 12.1618 - mae: 1.4096 - val_loss: 7.5404 - val_mse: 7.5404 - val_mae: 1.4322 - lr: 5.0093e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 21/100\n",
            "1000/1000 - 2s - loss: 12.0392 - mse: 12.0392 - mae: 1.4073 - val_loss: 7.6328 - val_mse: 7.6328 - val_mae: 1.4546 - lr: 5.0093e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 22/100\n",
            "1000/1000 - 2s - loss: 11.9640 - mse: 11.9640 - mae: 1.4056 - val_loss: 7.9382 - val_mse: 7.9382 - val_mae: 1.4320 - lr: 5.0093e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 23/100\n",
            "1000/1000 - 2s - loss: 11.9220 - mse: 11.9220 - mae: 1.4051 - val_loss: 7.6032 - val_mse: 7.6032 - val_mae: 1.4565 - lr: 5.0093e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 24/100\n",
            "1000/1000 - 2s - loss: 11.7890 - mse: 11.7890 - mae: 1.3985 - val_loss: 7.9117 - val_mse: 7.9117 - val_mae: 1.4162 - lr: 5.0093e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 25/100\n",
            "1000/1000 - 2s - loss: 11.7940 - mse: 11.7940 - mae: 1.3926 - val_loss: 8.1997 - val_mse: 8.1997 - val_mae: 1.4634 - lr: 5.0093e-04 - 2s/epoch - 2ms/step\n",
            "Score for fold 1: loss of 8.199724197387695\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 10.5853 - mse: 10.5853 - mae: 1.4092 - val_loss: 13.3943 - val_mse: 13.3943 - val_mae: 1.4031 - lr: 5.0093e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 10.3974 - mse: 10.3974 - mae: 1.4019 - val_loss: 13.0323 - val_mse: 13.0323 - val_mae: 1.4198 - lr: 5.0093e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 10.3150 - mse: 10.3150 - mae: 1.3973 - val_loss: 13.0452 - val_mse: 13.0452 - val_mae: 1.4154 - lr: 5.0093e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 10.3294 - mse: 10.3294 - mae: 1.3938 - val_loss: 13.1413 - val_mse: 13.1413 - val_mae: 1.4163 - lr: 5.0093e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 10.2476 - mse: 10.2476 - mae: 1.3905 - val_loss: 13.1443 - val_mse: 13.1443 - val_mae: 1.4326 - lr: 5.0093e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 10.1664 - mse: 10.1664 - mae: 1.3881 - val_loss: 13.3018 - val_mse: 13.3018 - val_mae: 1.3642 - lr: 5.0093e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 10.1050 - mse: 10.1050 - mae: 1.3834 - val_loss: 13.1848 - val_mse: 13.1848 - val_mae: 1.4501 - lr: 5.0093e-04 - 2s/epoch - 2ms/step\n",
            "Score for fold 2: loss of 13.1847505569458\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 11.1793 - mse: 11.1793 - mae: 1.3962 - val_loss: 8.9239 - val_mse: 8.9239 - val_mae: 1.3703 - lr: 5.0093e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 11.0762 - mse: 11.0762 - mae: 1.3879 - val_loss: 8.7684 - val_mse: 8.7684 - val_mae: 1.4253 - lr: 5.0093e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 11.1124 - mse: 11.1124 - mae: 1.3883 - val_loss: 9.0033 - val_mse: 9.0033 - val_mae: 1.3720 - lr: 5.0093e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 11.0295 - mse: 11.0295 - mae: 1.3800 - val_loss: 9.1971 - val_mse: 9.1971 - val_mae: 1.3860 - lr: 5.0093e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 11.0039 - mse: 11.0039 - mae: 1.3767 - val_loss: 9.3385 - val_mse: 9.3385 - val_mae: 1.4658 - lr: 5.0093e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 10.9375 - mse: 10.9375 - mae: 1.3727 - val_loss: 9.3651 - val_mse: 9.3651 - val_mae: 1.4074 - lr: 5.0093e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 10.8000 - mse: 10.8000 - mae: 1.3707 - val_loss: 9.1403 - val_mse: 9.1403 - val_mae: 1.4743 - lr: 5.0093e-04 - 2s/epoch - 2ms/step\n",
            "Score for fold 3: loss of 9.140275955200195\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 11.7002 - mse: 11.7002 - mae: 1.3853 - val_loss: 6.5887 - val_mse: 6.5887 - val_mae: 1.3436 - lr: 5.0093e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 11.2344 - mse: 11.2344 - mae: 1.3719 - val_loss: 6.8664 - val_mse: 6.8664 - val_mae: 1.3424 - lr: 5.0093e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 11.2834 - mse: 11.2834 - mae: 1.3728 - val_loss: 7.1275 - val_mse: 7.1275 - val_mae: 1.3665 - lr: 5.0093e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 11.3018 - mse: 11.3018 - mae: 1.3666 - val_loss: 7.0871 - val_mse: 7.0871 - val_mae: 1.3906 - lr: 5.0093e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 11.2276 - mse: 11.2276 - mae: 1.3643 - val_loss: 6.9778 - val_mse: 6.9778 - val_mae: 1.3720 - lr: 5.0093e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 11.1727 - mse: 11.1727 - mae: 1.3606 - val_loss: 6.9818 - val_mse: 6.9818 - val_mae: 1.3991 - lr: 5.0093e-04 - 2s/epoch - 2ms/step\n",
            "Score for fold 4: loss of 6.981834888458252\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 8.6996 - mse: 8.6996 - mae: 1.3590 - val_loss: 17.0532 - val_mse: 17.0532 - val_mae: 1.3855 - lr: 5.0093e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 8.5529 - mse: 8.5529 - mae: 1.3508 - val_loss: 17.2592 - val_mse: 17.2592 - val_mae: 1.3783 - lr: 5.0093e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 8.4083 - mse: 8.4083 - mae: 1.3486 - val_loss: 17.3701 - val_mse: 17.3701 - val_mae: 1.3877 - lr: 5.0093e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 8.4275 - mse: 8.4275 - mae: 1.3449 - val_loss: 17.4924 - val_mse: 17.4924 - val_mae: 1.4106 - lr: 5.0093e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 8.3828 - mse: 8.3828 - mae: 1.3403 - val_loss: 17.4869 - val_mse: 17.4869 - val_mae: 1.4550 - lr: 5.0093e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 8.2218 - mse: 8.2218 - mae: 1.3349 - val_loss: 17.4581 - val_mse: 17.4581 - val_mae: 1.4314 - lr: 5.0093e-04 - 2s/epoch - 2ms/step\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-10-11 15:27:36,587]\u001b[0m Finished trial#19 resulted in value: 10.992. Current best value is 9.266000000000002 with parameters: {'activation': 'relu', 'num_hidden_layer': 4, 'i': 10, 'learning_rate': 0.00013934585348848375}.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Score for fold 5: loss of 17.45806884765625\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 7s - loss: 12.5904 - mse: 12.5904 - mae: 1.5663 - val_loss: 16.5078 - val_mse: 16.5078 - val_mae: 1.3995 - lr: 1.0888e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 6s - loss: 11.4872 - mse: 11.4872 - mae: 1.5036 - val_loss: 16.3652 - val_mse: 16.3652 - val_mae: 1.3469 - lr: 1.0888e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 6s - loss: 11.2783 - mse: 11.2783 - mae: 1.4841 - val_loss: 15.9750 - val_mse: 15.9750 - val_mae: 1.5006 - lr: 1.0888e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 6s - loss: 10.9303 - mse: 10.9303 - mae: 1.4683 - val_loss: 16.4395 - val_mse: 16.4395 - val_mae: 1.3979 - lr: 1.0888e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 6s - loss: 11.0485 - mse: 11.0485 - mae: 1.4604 - val_loss: 16.0280 - val_mse: 16.0280 - val_mae: 1.4022 - lr: 1.0888e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 7s - loss: 10.8425 - mse: 10.8425 - mae: 1.4492 - val_loss: 16.0759 - val_mse: 16.0759 - val_mae: 1.4177 - lr: 1.0888e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 6s - loss: 10.7931 - mse: 10.7931 - mae: 1.4447 - val_loss: 16.2531 - val_mse: 16.2531 - val_mae: 1.4320 - lr: 1.0888e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 6s - loss: 10.5986 - mse: 10.5986 - mae: 1.4373 - val_loss: 16.0592 - val_mse: 16.0592 - val_mae: 1.4549 - lr: 1.0888e-04 - 6s/epoch - 6ms/step\n",
            "Score for fold 1: loss of 16.059192657470703\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 7s - loss: 12.5457 - mse: 12.5457 - mae: 1.4347 - val_loss: 8.1002 - val_mse: 8.1002 - val_mae: 1.3776 - lr: 1.0888e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 7s - loss: 12.3806 - mse: 12.3806 - mae: 1.4295 - val_loss: 8.1567 - val_mse: 8.1567 - val_mae: 1.4221 - lr: 1.0888e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 7s - loss: 12.2881 - mse: 12.2881 - mae: 1.4241 - val_loss: 8.2178 - val_mse: 8.2178 - val_mae: 1.3836 - lr: 1.0888e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 6s - loss: 12.2576 - mse: 12.2576 - mae: 1.4131 - val_loss: 8.1185 - val_mse: 8.1185 - val_mae: 1.4101 - lr: 1.0888e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 6s - loss: 11.9824 - mse: 11.9824 - mae: 1.4071 - val_loss: 8.4113 - val_mse: 8.4113 - val_mae: 1.4151 - lr: 1.0888e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 6s - loss: 12.0864 - mse: 12.0864 - mae: 1.3978 - val_loss: 8.3051 - val_mse: 8.3051 - val_mae: 1.4099 - lr: 1.0888e-04 - 6s/epoch - 6ms/step\n",
            "Score for fold 2: loss of 8.305143356323242\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 7s - loss: 10.9283 - mse: 10.9283 - mae: 1.3914 - val_loss: 11.8074 - val_mse: 11.8074 - val_mae: 1.4294 - lr: 1.0888e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 7s - loss: 10.8461 - mse: 10.8461 - mae: 1.3832 - val_loss: 11.7651 - val_mse: 11.7651 - val_mae: 1.5088 - lr: 1.0888e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 6s - loss: 10.5610 - mse: 10.5610 - mae: 1.3725 - val_loss: 12.2612 - val_mse: 12.2612 - val_mae: 1.4144 - lr: 1.0888e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 7s - loss: 10.4395 - mse: 10.4395 - mae: 1.3617 - val_loss: 11.8255 - val_mse: 11.8255 - val_mae: 1.4230 - lr: 1.0888e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 7s - loss: 10.2709 - mse: 10.2709 - mae: 1.3503 - val_loss: 11.7393 - val_mse: 11.7393 - val_mae: 1.4754 - lr: 1.0888e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 6s - loss: 10.0718 - mse: 10.0718 - mae: 1.3439 - val_loss: 11.6218 - val_mse: 11.6218 - val_mae: 1.4595 - lr: 1.0888e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 6s - loss: 10.0512 - mse: 10.0512 - mae: 1.3325 - val_loss: 11.9416 - val_mse: 11.9416 - val_mae: 1.4418 - lr: 1.0888e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 7s - loss: 9.8398 - mse: 9.8398 - mae: 1.3250 - val_loss: 12.1436 - val_mse: 12.1436 - val_mae: 1.3854 - lr: 1.0888e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 6s - loss: 9.7192 - mse: 9.7192 - mae: 1.3145 - val_loss: 12.8203 - val_mse: 12.8203 - val_mae: 1.4456 - lr: 1.0888e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 7s - loss: 9.5587 - mse: 9.5587 - mae: 1.3023 - val_loss: 12.2479 - val_mse: 12.2479 - val_mae: 1.5125 - lr: 1.0888e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 7s - loss: 9.3182 - mse: 9.3182 - mae: 1.2916 - val_loss: 11.8300 - val_mse: 11.8300 - val_mae: 1.4719 - lr: 1.0888e-04 - 7s/epoch - 7ms/step\n",
            "Score for fold 3: loss of 11.829975128173828\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 6s - loss: 10.9343 - mse: 10.9343 - mae: 1.3371 - val_loss: 6.5112 - val_mse: 6.5112 - val_mae: 1.2844 - lr: 1.0888e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 6s - loss: 10.4855 - mse: 10.4855 - mae: 1.3187 - val_loss: 6.5942 - val_mse: 6.5942 - val_mae: 1.2991 - lr: 1.0888e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 6s - loss: 10.3618 - mse: 10.3618 - mae: 1.3087 - val_loss: 6.7691 - val_mse: 6.7691 - val_mae: 1.3175 - lr: 1.0888e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 6s - loss: 10.0537 - mse: 10.0537 - mae: 1.2962 - val_loss: 7.0685 - val_mse: 7.0685 - val_mae: 1.3289 - lr: 1.0888e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 6s - loss: 10.0221 - mse: 10.0221 - mae: 1.2847 - val_loss: 7.6062 - val_mse: 7.6062 - val_mae: 1.3226 - lr: 1.0888e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 6s - loss: 10.2333 - mse: 10.2333 - mae: 1.2763 - val_loss: 6.8824 - val_mse: 6.8824 - val_mae: 1.3712 - lr: 1.0888e-04 - 6s/epoch - 6ms/step\n",
            "Score for fold 4: loss of 6.882385730743408\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 6s - loss: 9.0694 - mse: 9.0694 - mae: 1.2993 - val_loss: 9.7957 - val_mse: 9.7957 - val_mae: 1.2722 - lr: 1.0888e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 6s - loss: 8.6914 - mse: 8.6914 - mae: 1.2808 - val_loss: 10.1459 - val_mse: 10.1459 - val_mae: 1.2502 - lr: 1.0888e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 6s - loss: 8.5468 - mse: 8.5468 - mae: 1.2679 - val_loss: 10.3098 - val_mse: 10.3098 - val_mae: 1.3618 - lr: 1.0888e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 6s - loss: 8.2969 - mse: 8.2969 - mae: 1.2552 - val_loss: 10.8762 - val_mse: 10.8762 - val_mae: 1.2679 - lr: 1.0888e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 6s - loss: 8.2891 - mse: 8.2891 - mae: 1.2449 - val_loss: 11.2654 - val_mse: 11.2654 - val_mae: 1.2808 - lr: 1.0888e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 6s - loss: 8.2671 - mse: 8.2671 - mae: 1.2335 - val_loss: 10.8727 - val_mse: 10.8727 - val_mae: 1.2804 - lr: 1.0888e-04 - 6s/epoch - 6ms/step\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-10-11 15:31:42,522]\u001b[0m Finished trial#20 resulted in value: 10.79. Current best value is 9.266000000000002 with parameters: {'activation': 'relu', 'num_hidden_layer': 4, 'i': 10, 'learning_rate': 0.00013934585348848375}.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Score for fold 5: loss of 10.872702598571777\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 12s - loss: 13.9349 - mse: 13.9349 - mae: 1.5390 - val_loss: 9.3996 - val_mse: 9.3996 - val_mae: 1.4822 - lr: 1.5658e-04 - 12s/epoch - 12ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 10s - loss: 13.0918 - mse: 13.0918 - mae: 1.4800 - val_loss: 9.2449 - val_mse: 9.2449 - val_mae: 1.4866 - lr: 1.5658e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 11s - loss: 12.9509 - mse: 12.9509 - mae: 1.4693 - val_loss: 9.1472 - val_mse: 9.1472 - val_mae: 1.5517 - lr: 1.5658e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 11s - loss: 12.7419 - mse: 12.7419 - mae: 1.4610 - val_loss: 9.0748 - val_mse: 9.0748 - val_mae: 1.4568 - lr: 1.5658e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 11s - loss: 12.6719 - mse: 12.6719 - mae: 1.4522 - val_loss: 9.3231 - val_mse: 9.3231 - val_mae: 1.5577 - lr: 1.5658e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 11s - loss: 12.5914 - mse: 12.5914 - mae: 1.4429 - val_loss: 9.2971 - val_mse: 9.2971 - val_mae: 1.4996 - lr: 1.5658e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 11s - loss: 12.4389 - mse: 12.4389 - mae: 1.4309 - val_loss: 9.3084 - val_mse: 9.3084 - val_mae: 1.4642 - lr: 1.5658e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 11s - loss: 12.2727 - mse: 12.2727 - mae: 1.4195 - val_loss: 9.0302 - val_mse: 9.0302 - val_mae: 1.4792 - lr: 1.5658e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 11s - loss: 11.8165 - mse: 11.8165 - mae: 1.4091 - val_loss: 9.1282 - val_mse: 9.1282 - val_mae: 1.4534 - lr: 1.5658e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 11s - loss: 11.8172 - mse: 11.8172 - mae: 1.3983 - val_loss: 9.0014 - val_mse: 9.0014 - val_mae: 1.4639 - lr: 1.5658e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 11s - loss: 11.6503 - mse: 11.6503 - mae: 1.3854 - val_loss: 8.9287 - val_mse: 8.9287 - val_mae: 1.4755 - lr: 1.5658e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 11s - loss: 11.3078 - mse: 11.3078 - mae: 1.3705 - val_loss: 9.2269 - val_mse: 9.2269 - val_mae: 1.4874 - lr: 1.5658e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 13/100\n",
            "1000/1000 - 10s - loss: 10.7673 - mse: 10.7673 - mae: 1.3459 - val_loss: 9.2822 - val_mse: 9.2822 - val_mae: 1.4922 - lr: 1.5658e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 14/100\n",
            "1000/1000 - 11s - loss: 10.6119 - mse: 10.6119 - mae: 1.3288 - val_loss: 9.4619 - val_mse: 9.4619 - val_mae: 1.5362 - lr: 1.5658e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 15/100\n",
            "1000/1000 - 11s - loss: 10.5775 - mse: 10.5775 - mae: 1.3163 - val_loss: 9.2655 - val_mse: 9.2655 - val_mae: 1.4954 - lr: 1.5658e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 16/100\n",
            "1000/1000 - 10s - loss: 10.0570 - mse: 10.0570 - mae: 1.2950 - val_loss: 9.4466 - val_mse: 9.4466 - val_mae: 1.5508 - lr: 1.5658e-04 - 10s/epoch - 10ms/step\n",
            "Score for fold 1: loss of 9.44664192199707\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 11s - loss: 11.0849 - mse: 11.0849 - mae: 1.3507 - val_loss: 7.0640 - val_mse: 7.0640 - val_mae: 1.2776 - lr: 1.5658e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 11s - loss: 10.4095 - mse: 10.4095 - mae: 1.3226 - val_loss: 6.9084 - val_mse: 6.9084 - val_mae: 1.2680 - lr: 1.5658e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 10s - loss: 9.9999 - mse: 9.9999 - mae: 1.2962 - val_loss: 7.4693 - val_mse: 7.4693 - val_mae: 1.3086 - lr: 1.5658e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 11s - loss: 9.9779 - mse: 9.9779 - mae: 1.2811 - val_loss: 7.3519 - val_mse: 7.3519 - val_mae: 1.4088 - lr: 1.5658e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 11s - loss: 9.4072 - mse: 9.4072 - mae: 1.2603 - val_loss: 6.8284 - val_mse: 6.8284 - val_mae: 1.3290 - lr: 1.5658e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 11s - loss: 9.0368 - mse: 9.0368 - mae: 1.2390 - val_loss: 7.2512 - val_mse: 7.2512 - val_mae: 1.3382 - lr: 1.5658e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 11s - loss: 8.8298 - mse: 8.8298 - mae: 1.2252 - val_loss: 6.8305 - val_mse: 6.8305 - val_mae: 1.3381 - lr: 1.5658e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 11s - loss: 8.4663 - mse: 8.4663 - mae: 1.1949 - val_loss: 10.1633 - val_mse: 10.1633 - val_mae: 1.4265 - lr: 1.5658e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 11s - loss: 8.1380 - mse: 8.1380 - mae: 1.1823 - val_loss: 7.4881 - val_mse: 7.4881 - val_mae: 1.4336 - lr: 1.5658e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 11s - loss: 8.1080 - mse: 8.1080 - mae: 1.1633 - val_loss: 7.3868 - val_mse: 7.3868 - val_mae: 1.4033 - lr: 1.5658e-04 - 11s/epoch - 11ms/step\n",
            "Score for fold 2: loss of 7.386775493621826\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 11s - loss: 8.7572 - mse: 8.7572 - mae: 1.2341 - val_loss: 4.6590 - val_mse: 4.6590 - val_mae: 1.1665 - lr: 1.5658e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 11s - loss: 8.2371 - mse: 8.2371 - mae: 1.2026 - val_loss: 4.5722 - val_mse: 4.5722 - val_mae: 1.1620 - lr: 1.5658e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 11s - loss: 7.8309 - mse: 7.8309 - mae: 1.1759 - val_loss: 4.8666 - val_mse: 4.8666 - val_mae: 1.2042 - lr: 1.5658e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 11s - loss: 7.1831 - mse: 7.1831 - mae: 1.1518 - val_loss: 5.1469 - val_mse: 5.1469 - val_mae: 1.1958 - lr: 1.5658e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 11s - loss: 7.1708 - mse: 7.1708 - mae: 1.1320 - val_loss: 4.9233 - val_mse: 4.9233 - val_mae: 1.1682 - lr: 1.5658e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 10s - loss: 6.6359 - mse: 6.6359 - mae: 1.1147 - val_loss: 5.0924 - val_mse: 5.0924 - val_mae: 1.2202 - lr: 1.5658e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 11s - loss: 6.4728 - mse: 6.4728 - mae: 1.0980 - val_loss: 5.2244 - val_mse: 5.2244 - val_mae: 1.2337 - lr: 1.5658e-04 - 11s/epoch - 11ms/step\n",
            "Score for fold 3: loss of 5.224437713623047\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 11s - loss: 6.6747 - mse: 6.6747 - mae: 1.1383 - val_loss: 5.4662 - val_mse: 5.4662 - val_mae: 1.0637 - lr: 1.5658e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 11s - loss: 5.9875 - mse: 5.9875 - mae: 1.1079 - val_loss: 6.0163 - val_mse: 6.0163 - val_mae: 1.1008 - lr: 1.5658e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 11s - loss: 5.7280 - mse: 5.7280 - mae: 1.0889 - val_loss: 6.5944 - val_mse: 6.5944 - val_mae: 1.0951 - lr: 1.5658e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 11s - loss: 5.4648 - mse: 5.4648 - mae: 1.0645 - val_loss: 6.7505 - val_mse: 6.7505 - val_mae: 1.1164 - lr: 1.5658e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 11s - loss: 5.2798 - mse: 5.2798 - mae: 1.0561 - val_loss: 6.6152 - val_mse: 6.6152 - val_mae: 1.1000 - lr: 1.5658e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 10s - loss: 4.9944 - mse: 4.9944 - mae: 1.0264 - val_loss: 6.6070 - val_mse: 6.6070 - val_mae: 1.1740 - lr: 1.5658e-04 - 10s/epoch - 10ms/step\n",
            "Score for fold 4: loss of 6.606982231140137\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 11s - loss: 4.0557 - mse: 4.0557 - mae: 1.0591 - val_loss: 9.8319 - val_mse: 9.8319 - val_mae: 1.0122 - lr: 1.5658e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 11s - loss: 4.0275 - mse: 4.0275 - mae: 1.0381 - val_loss: 10.2324 - val_mse: 10.2324 - val_mae: 0.9710 - lr: 1.5658e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 11s - loss: 3.5630 - mse: 3.5630 - mae: 1.0185 - val_loss: 9.7845 - val_mse: 9.7845 - val_mae: 0.9975 - lr: 1.5658e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 11s - loss: 3.2066 - mse: 3.2066 - mae: 0.9922 - val_loss: 10.5229 - val_mse: 10.5229 - val_mae: 1.1041 - lr: 1.5658e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 11s - loss: 3.1566 - mse: 3.1566 - mae: 0.9818 - val_loss: 11.7853 - val_mse: 11.7853 - val_mae: 1.1352 - lr: 1.5658e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 11s - loss: 3.0272 - mse: 3.0272 - mae: 0.9641 - val_loss: 11.0926 - val_mse: 11.0926 - val_mae: 1.1461 - lr: 1.5658e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 11s - loss: 2.8662 - mse: 2.8662 - mae: 0.9458 - val_loss: 10.9749 - val_mse: 10.9749 - val_mae: 1.0741 - lr: 1.5658e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 11s - loss: 2.6854 - mse: 2.6854 - mae: 0.9341 - val_loss: 12.5021 - val_mse: 12.5021 - val_mae: 1.0894 - lr: 1.5658e-04 - 11s/epoch - 11ms/step\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-10-11 15:40:19,195]\u001b[0m Finished trial#21 resulted in value: 8.234. Current best value is 8.234 with parameters: {'activation': 'relu', 'num_hidden_layer': 4, 'i': 10, 'learning_rate': 0.00015658492778156685}.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Score for fold 5: loss of 12.50208568572998\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 12s - loss: 14.0571 - mse: 14.0571 - mae: 1.5326 - val_loss: 9.6441 - val_mse: 9.6441 - val_mae: 1.5618 - lr: 1.6683e-04 - 12s/epoch - 12ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 11s - loss: 13.0378 - mse: 13.0378 - mae: 1.4784 - val_loss: 9.4540 - val_mse: 9.4540 - val_mae: 1.6036 - lr: 1.6683e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 11s - loss: 12.9007 - mse: 12.9007 - mae: 1.4650 - val_loss: 10.0595 - val_mse: 10.0595 - val_mae: 1.5413 - lr: 1.6683e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 11s - loss: 12.7386 - mse: 12.7386 - mae: 1.4673 - val_loss: 9.8345 - val_mse: 9.8345 - val_mae: 1.4860 - lr: 1.6683e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 11s - loss: 12.5593 - mse: 12.5593 - mae: 1.4510 - val_loss: 9.1953 - val_mse: 9.1953 - val_mae: 1.4403 - lr: 1.6683e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 11s - loss: 12.4424 - mse: 12.4424 - mae: 1.4424 - val_loss: 9.7207 - val_mse: 9.7207 - val_mae: 1.4648 - lr: 1.6683e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 11s - loss: 12.1911 - mse: 12.1911 - mae: 1.4309 - val_loss: 9.3446 - val_mse: 9.3446 - val_mae: 1.5155 - lr: 1.6683e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 11s - loss: 12.2241 - mse: 12.2241 - mae: 1.4205 - val_loss: 9.6866 - val_mse: 9.6866 - val_mae: 1.4762 - lr: 1.6683e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 10s - loss: 11.9899 - mse: 11.9899 - mae: 1.4139 - val_loss: 9.4682 - val_mse: 9.4682 - val_mae: 1.5137 - lr: 1.6683e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 11s - loss: 11.7792 - mse: 11.7792 - mae: 1.4010 - val_loss: 9.6871 - val_mse: 9.6871 - val_mae: 1.4500 - lr: 1.6683e-04 - 11s/epoch - 11ms/step\n",
            "Score for fold 1: loss of 9.687143325805664\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 11s - loss: 10.8088 - mse: 10.8088 - mae: 1.4053 - val_loss: 12.7232 - val_mse: 12.7232 - val_mae: 1.5192 - lr: 1.6683e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 11s - loss: 10.5541 - mse: 10.5541 - mae: 1.3894 - val_loss: 13.4597 - val_mse: 13.4597 - val_mae: 1.4845 - lr: 1.6683e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 11s - loss: 10.4903 - mse: 10.4903 - mae: 1.3734 - val_loss: 13.0826 - val_mse: 13.0826 - val_mae: 1.4592 - lr: 1.6683e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 10s - loss: 9.8649 - mse: 9.8649 - mae: 1.3556 - val_loss: 13.5569 - val_mse: 13.5569 - val_mae: 1.4454 - lr: 1.6683e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 11s - loss: 9.5538 - mse: 9.5538 - mae: 1.3362 - val_loss: 13.3492 - val_mse: 13.3492 - val_mae: 1.3928 - lr: 1.6683e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 11s - loss: 9.4139 - mse: 9.4139 - mae: 1.3204 - val_loss: 12.3486 - val_mse: 12.3486 - val_mae: 1.4429 - lr: 1.6683e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 11s - loss: 9.0452 - mse: 9.0452 - mae: 1.3006 - val_loss: 13.2061 - val_mse: 13.2061 - val_mae: 1.4557 - lr: 1.6683e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 11s - loss: 8.7561 - mse: 8.7561 - mae: 1.2805 - val_loss: 13.1135 - val_mse: 13.1135 - val_mae: 1.4582 - lr: 1.6683e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 11s - loss: 8.4028 - mse: 8.4028 - mae: 1.2620 - val_loss: 13.1713 - val_mse: 13.1713 - val_mae: 1.6063 - lr: 1.6683e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 11s - loss: 7.9887 - mse: 7.9887 - mae: 1.2433 - val_loss: 12.9697 - val_mse: 12.9697 - val_mae: 1.4444 - lr: 1.6683e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 11s - loss: 7.8703 - mse: 7.8703 - mae: 1.2208 - val_loss: 14.0039 - val_mse: 14.0039 - val_mae: 1.4408 - lr: 1.6683e-04 - 11s/epoch - 11ms/step\n",
            "Score for fold 2: loss of 14.003941535949707\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 10s - loss: 9.5857 - mse: 9.5857 - mae: 1.2975 - val_loss: 4.8720 - val_mse: 4.8720 - val_mae: 1.2469 - lr: 1.6683e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 11s - loss: 9.5759 - mse: 9.5759 - mae: 1.2728 - val_loss: 4.9783 - val_mse: 4.9783 - val_mae: 1.2704 - lr: 1.6683e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 11s - loss: 8.9883 - mse: 8.9883 - mae: 1.2473 - val_loss: 5.6083 - val_mse: 5.6083 - val_mae: 1.2538 - lr: 1.6683e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 10s - loss: 8.6804 - mse: 8.6804 - mae: 1.2287 - val_loss: 7.3421 - val_mse: 7.3421 - val_mae: 1.3701 - lr: 1.6683e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 11s - loss: 8.5102 - mse: 8.5102 - mae: 1.2120 - val_loss: 5.5451 - val_mse: 5.5451 - val_mae: 1.3090 - lr: 1.6683e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 11s - loss: 8.0520 - mse: 8.0520 - mae: 1.1900 - val_loss: 5.4652 - val_mse: 5.4652 - val_mae: 1.2267 - lr: 1.6683e-04 - 11s/epoch - 11ms/step\n",
            "Score for fold 3: loss of 5.465224742889404\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 11s - loss: 8.3118 - mse: 8.3118 - mae: 1.2265 - val_loss: 4.4173 - val_mse: 4.4173 - val_mae: 1.1427 - lr: 1.6683e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 11s - loss: 8.0769 - mse: 8.0769 - mae: 1.1986 - val_loss: 4.7020 - val_mse: 4.7020 - val_mae: 1.2235 - lr: 1.6683e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 11s - loss: 7.5023 - mse: 7.5023 - mae: 1.1781 - val_loss: 4.9528 - val_mse: 4.9528 - val_mae: 1.1503 - lr: 1.6683e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 11s - loss: 7.2446 - mse: 7.2446 - mae: 1.1520 - val_loss: 5.0668 - val_mse: 5.0668 - val_mae: 1.1636 - lr: 1.6683e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 11s - loss: 7.0223 - mse: 7.0223 - mae: 1.1313 - val_loss: 5.2080 - val_mse: 5.2080 - val_mae: 1.1943 - lr: 1.6683e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 11s - loss: 6.3687 - mse: 6.3687 - mae: 1.1142 - val_loss: 4.8290 - val_mse: 4.8290 - val_mae: 1.2152 - lr: 1.6683e-04 - 11s/epoch - 11ms/step\n",
            "Score for fold 4: loss of 4.828957557678223\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 11s - loss: 5.5263 - mse: 5.5263 - mae: 1.1407 - val_loss: 9.9916 - val_mse: 9.9916 - val_mae: 1.0164 - lr: 1.6683e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 11s - loss: 5.0923 - mse: 5.0923 - mae: 1.1118 - val_loss: 10.6706 - val_mse: 10.6706 - val_mae: 1.0430 - lr: 1.6683e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 11s - loss: 4.7225 - mse: 4.7225 - mae: 1.0934 - val_loss: 11.5869 - val_mse: 11.5869 - val_mae: 1.0916 - lr: 1.6683e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 11s - loss: 4.6250 - mse: 4.6250 - mae: 1.0883 - val_loss: 11.4188 - val_mse: 11.4188 - val_mae: 1.0970 - lr: 1.6683e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 11s - loss: 4.3624 - mse: 4.3624 - mae: 1.0572 - val_loss: 10.7584 - val_mse: 10.7584 - val_mae: 1.1009 - lr: 1.6683e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 11s - loss: 4.2285 - mse: 4.2285 - mae: 1.0392 - val_loss: 11.9791 - val_mse: 11.9791 - val_mae: 1.1346 - lr: 1.6683e-04 - 11s/epoch - 11ms/step\n",
            "Score for fold 5: loss of 11.979140281677246\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-10-11 15:47:30,182]\u001b[0m Finished trial#22 resulted in value: 9.193999999999999. Current best value is 8.234 with parameters: {'activation': 'relu', 'num_hidden_layer': 4, 'i': 10, 'learning_rate': 0.00015658492778156685}.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 12s - loss: 11.5194 - mse: 11.5194 - mae: 1.5353 - val_loss: 18.6130 - val_mse: 18.6130 - val_mae: 1.4914 - lr: 2.6096e-04 - 12s/epoch - 12ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 10s - loss: 10.7656 - mse: 10.7656 - mae: 1.4850 - val_loss: 18.8768 - val_mse: 18.8768 - val_mae: 1.5199 - lr: 2.6096e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 11s - loss: 10.7605 - mse: 10.7605 - mae: 1.4681 - val_loss: 18.4221 - val_mse: 18.4221 - val_mae: 1.5233 - lr: 2.6096e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 11s - loss: 10.4105 - mse: 10.4105 - mae: 1.4599 - val_loss: 18.3740 - val_mse: 18.3740 - val_mae: 1.4367 - lr: 2.6096e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 11s - loss: 10.0778 - mse: 10.0778 - mae: 1.4453 - val_loss: 18.6725 - val_mse: 18.6725 - val_mae: 1.4904 - lr: 2.6096e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 11s - loss: 10.2279 - mse: 10.2279 - mae: 1.4445 - val_loss: 18.4712 - val_mse: 18.4712 - val_mae: 1.5274 - lr: 2.6096e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 11s - loss: 9.9123 - mse: 9.9123 - mae: 1.4321 - val_loss: 18.3541 - val_mse: 18.3541 - val_mae: 1.5465 - lr: 2.6096e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 11s - loss: 9.7492 - mse: 9.7492 - mae: 1.4202 - val_loss: 18.5129 - val_mse: 18.5129 - val_mae: 1.5436 - lr: 2.6096e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 11s - loss: 9.4072 - mse: 9.4072 - mae: 1.4071 - val_loss: 18.3537 - val_mse: 18.3537 - val_mae: 1.4858 - lr: 2.6096e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 11s - loss: 9.3561 - mse: 9.3561 - mae: 1.3999 - val_loss: 18.1385 - val_mse: 18.1385 - val_mae: 1.5029 - lr: 2.6096e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 11s - loss: 8.9570 - mse: 8.9570 - mae: 1.3824 - val_loss: 19.4337 - val_mse: 19.4337 - val_mae: 1.5474 - lr: 2.6096e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 11s - loss: 8.8543 - mse: 8.8543 - mae: 1.3700 - val_loss: 18.1621 - val_mse: 18.1621 - val_mae: 1.6051 - lr: 2.6096e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 13/100\n",
            "1000/1000 - 11s - loss: 8.3943 - mse: 8.3943 - mae: 1.3520 - val_loss: 19.6213 - val_mse: 19.6213 - val_mae: 1.5836 - lr: 2.6096e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 14/100\n",
            "1000/1000 - 11s - loss: 8.0661 - mse: 8.0661 - mae: 1.3340 - val_loss: 19.2582 - val_mse: 19.2582 - val_mae: 1.5308 - lr: 2.6096e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 15/100\n",
            "1000/1000 - 11s - loss: 8.0963 - mse: 8.0963 - mae: 1.3143 - val_loss: 21.2076 - val_mse: 21.2076 - val_mae: 1.5505 - lr: 2.6096e-04 - 11s/epoch - 11ms/step\n",
            "Score for fold 1: loss of 21.207582473754883\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 12s - loss: 10.1075 - mse: 10.1075 - mae: 1.3549 - val_loss: 13.5515 - val_mse: 13.5515 - val_mae: 1.3141 - lr: 2.6096e-04 - 12s/epoch - 12ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 11s - loss: 9.3480 - mse: 9.3480 - mae: 1.3327 - val_loss: 14.0579 - val_mse: 14.0579 - val_mae: 1.3908 - lr: 2.6096e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 11s - loss: 8.5248 - mse: 8.5248 - mae: 1.3036 - val_loss: 14.1372 - val_mse: 14.1372 - val_mae: 1.3835 - lr: 2.6096e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 11s - loss: 8.2377 - mse: 8.2377 - mae: 1.2818 - val_loss: 14.1715 - val_mse: 14.1715 - val_mae: 1.4131 - lr: 2.6096e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 11s - loss: 8.2434 - mse: 8.2434 - mae: 1.2711 - val_loss: 14.1652 - val_mse: 14.1652 - val_mae: 1.4040 - lr: 2.6096e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 11s - loss: 7.9488 - mse: 7.9488 - mae: 1.2534 - val_loss: 14.3900 - val_mse: 14.3900 - val_mae: 1.4254 - lr: 2.6096e-04 - 11s/epoch - 11ms/step\n",
            "Score for fold 2: loss of 14.3900146484375\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 11s - loss: 10.2600 - mse: 10.2600 - mae: 1.3027 - val_loss: 4.7481 - val_mse: 4.7481 - val_mae: 1.2087 - lr: 2.6096e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 11s - loss: 9.9616 - mse: 9.9616 - mae: 1.2861 - val_loss: 4.7311 - val_mse: 4.7311 - val_mae: 1.2230 - lr: 2.6096e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 11s - loss: 9.4695 - mse: 9.4695 - mae: 1.2686 - val_loss: 5.3726 - val_mse: 5.3726 - val_mae: 1.2013 - lr: 2.6096e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 11s - loss: 8.9554 - mse: 8.9554 - mae: 1.2525 - val_loss: 5.9535 - val_mse: 5.9535 - val_mae: 1.3047 - lr: 2.6096e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 11s - loss: 8.8397 - mse: 8.8397 - mae: 1.2354 - val_loss: 5.9214 - val_mse: 5.9214 - val_mae: 1.2548 - lr: 2.6096e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 11s - loss: 8.3950 - mse: 8.3950 - mae: 1.2173 - val_loss: 5.3230 - val_mse: 5.3230 - val_mae: 1.2620 - lr: 2.6096e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 11s - loss: 8.4894 - mse: 8.4894 - mae: 1.2073 - val_loss: 5.4650 - val_mse: 5.4650 - val_mae: 1.3422 - lr: 2.6096e-04 - 11s/epoch - 11ms/step\n",
            "Score for fold 3: loss of 5.464948654174805\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 11s - loss: 8.9813 - mse: 8.9813 - mae: 1.2385 - val_loss: 4.8403 - val_mse: 4.8403 - val_mae: 1.2198 - lr: 2.6096e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 11s - loss: 8.3682 - mse: 8.3682 - mae: 1.2106 - val_loss: 4.7317 - val_mse: 4.7317 - val_mae: 1.1484 - lr: 2.6096e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 11s - loss: 7.9939 - mse: 7.9939 - mae: 1.1880 - val_loss: 5.1012 - val_mse: 5.1012 - val_mae: 1.2169 - lr: 2.6096e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 11s - loss: 7.5139 - mse: 7.5139 - mae: 1.1771 - val_loss: 4.9258 - val_mse: 4.9258 - val_mae: 1.1964 - lr: 2.6096e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 11s - loss: 7.3709 - mse: 7.3709 - mae: 1.1579 - val_loss: 5.3239 - val_mse: 5.3239 - val_mae: 1.2471 - lr: 2.6096e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 11s - loss: 7.0134 - mse: 7.0134 - mae: 1.1388 - val_loss: 5.1622 - val_mse: 5.1622 - val_mae: 1.2702 - lr: 2.6096e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 11s - loss: 7.0385 - mse: 7.0385 - mae: 1.1239 - val_loss: 5.5273 - val_mse: 5.5273 - val_mae: 1.2213 - lr: 2.6096e-04 - 11s/epoch - 11ms/step\n",
            "Score for fold 4: loss of 5.527294158935547\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 11s - loss: 7.7007 - mse: 7.7007 - mae: 1.1718 - val_loss: 3.4347 - val_mse: 3.4347 - val_mae: 1.0331 - lr: 2.6096e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 11s - loss: 7.2559 - mse: 7.2559 - mae: 1.1444 - val_loss: 3.2379 - val_mse: 3.2379 - val_mae: 1.0727 - lr: 2.6096e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 11s - loss: 7.0209 - mse: 7.0209 - mae: 1.1279 - val_loss: 3.4471 - val_mse: 3.4471 - val_mae: 1.1078 - lr: 2.6096e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 11s - loss: 6.8280 - mse: 6.8280 - mae: 1.1128 - val_loss: 3.5686 - val_mse: 3.5686 - val_mae: 1.0711 - lr: 2.6096e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 11s - loss: 6.6159 - mse: 6.6159 - mae: 1.0891 - val_loss: 3.5042 - val_mse: 3.5042 - val_mae: 1.0825 - lr: 2.6096e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 11s - loss: 6.4915 - mse: 6.4915 - mae: 1.0815 - val_loss: 3.9060 - val_mse: 3.9060 - val_mae: 1.1569 - lr: 2.6096e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 11s - loss: 5.9363 - mse: 5.9363 - mae: 1.0551 - val_loss: 3.5864 - val_mse: 3.5864 - val_mae: 1.1057 - lr: 2.6096e-04 - 11s/epoch - 11ms/step\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-10-11 15:55:20,850]\u001b[0m Finished trial#23 resulted in value: 10.036000000000001. Current best value is 8.234 with parameters: {'activation': 'relu', 'num_hidden_layer': 4, 'i': 10, 'learning_rate': 0.00015658492778156685}.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Score for fold 5: loss of 3.5864202976226807\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 12s - loss: 13.9598 - mse: 13.9598 - mae: 1.5529 - val_loss: 10.2793 - val_mse: 10.2793 - val_mae: 1.4727 - lr: 4.8202e-04 - 12s/epoch - 12ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 11s - loss: 12.8678 - mse: 12.8678 - mae: 1.5046 - val_loss: 9.9329 - val_mse: 9.9329 - val_mae: 1.5050 - lr: 4.8202e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 11s - loss: 12.7807 - mse: 12.7807 - mae: 1.4921 - val_loss: 10.0586 - val_mse: 10.0586 - val_mae: 1.4419 - lr: 4.8202e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 11s - loss: 12.7577 - mse: 12.7577 - mae: 1.4833 - val_loss: 9.7368 - val_mse: 9.7368 - val_mae: 1.4341 - lr: 4.8202e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 11s - loss: 12.4318 - mse: 12.4318 - mae: 1.4679 - val_loss: 9.7150 - val_mse: 9.7150 - val_mae: 1.4469 - lr: 4.8202e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 11s - loss: 12.3486 - mse: 12.3486 - mae: 1.4612 - val_loss: 9.6979 - val_mse: 9.6979 - val_mae: 1.4560 - lr: 4.8202e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 12s - loss: 12.2961 - mse: 12.2961 - mae: 1.4583 - val_loss: 9.7574 - val_mse: 9.7574 - val_mae: 1.4080 - lr: 4.8202e-04 - 12s/epoch - 12ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 11s - loss: 12.2031 - mse: 12.2031 - mae: 1.4488 - val_loss: 9.8275 - val_mse: 9.8275 - val_mae: 1.4373 - lr: 4.8202e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 11s - loss: 11.9562 - mse: 11.9562 - mae: 1.4380 - val_loss: 9.8734 - val_mse: 9.8734 - val_mae: 1.3867 - lr: 4.8202e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 11s - loss: 11.6553 - mse: 11.6553 - mae: 1.4298 - val_loss: 9.8830 - val_mse: 9.8830 - val_mae: 1.4222 - lr: 4.8202e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 11s - loss: 11.9063 - mse: 11.9063 - mae: 1.4201 - val_loss: 10.0071 - val_mse: 10.0071 - val_mae: 1.5130 - lr: 4.8202e-04 - 11s/epoch - 11ms/step\n",
            "Score for fold 1: loss of 10.007101058959961\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 11s - loss: 12.0953 - mse: 12.0953 - mae: 1.4408 - val_loss: 8.1246 - val_mse: 8.1246 - val_mae: 1.3760 - lr: 4.8202e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 11s - loss: 11.7604 - mse: 11.7604 - mae: 1.4196 - val_loss: 8.3554 - val_mse: 8.3554 - val_mae: 1.4031 - lr: 4.8202e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 11s - loss: 11.7137 - mse: 11.7137 - mae: 1.4118 - val_loss: 8.6960 - val_mse: 8.6960 - val_mae: 1.3591 - lr: 4.8202e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 11s - loss: 11.3471 - mse: 11.3471 - mae: 1.4010 - val_loss: 8.4592 - val_mse: 8.4592 - val_mae: 1.3656 - lr: 4.8202e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 10s - loss: 11.0783 - mse: 11.0783 - mae: 1.3893 - val_loss: 8.2590 - val_mse: 8.2590 - val_mae: 1.4195 - lr: 4.8202e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 11s - loss: 10.9364 - mse: 10.9364 - mae: 1.3786 - val_loss: 9.1402 - val_mse: 9.1402 - val_mae: 1.3412 - lr: 4.8202e-04 - 11s/epoch - 11ms/step\n",
            "Score for fold 2: loss of 9.140233039855957\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 11s - loss: 10.1880 - mse: 10.1880 - mae: 1.3969 - val_loss: 12.0430 - val_mse: 12.0430 - val_mae: 1.4279 - lr: 4.8202e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 10s - loss: 9.6989 - mse: 9.6989 - mae: 1.3767 - val_loss: 12.3537 - val_mse: 12.3537 - val_mae: 1.4169 - lr: 4.8202e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 11s - loss: 9.5162 - mse: 9.5162 - mae: 1.3657 - val_loss: 12.3259 - val_mse: 12.3259 - val_mae: 1.4616 - lr: 4.8202e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 11s - loss: 9.3872 - mse: 9.3872 - mae: 1.3610 - val_loss: 12.2661 - val_mse: 12.2661 - val_mae: 1.4236 - lr: 4.8202e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 11s - loss: 9.0409 - mse: 9.0409 - mae: 1.3425 - val_loss: 12.5316 - val_mse: 12.5316 - val_mae: 1.5010 - lr: 4.8202e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 11s - loss: 8.9341 - mse: 8.9341 - mae: 1.3346 - val_loss: 12.9510 - val_mse: 12.9510 - val_mae: 1.4417 - lr: 4.8202e-04 - 11s/epoch - 11ms/step\n",
            "Score for fold 3: loss of 12.950956344604492\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 11s - loss: 10.8648 - mse: 10.8648 - mae: 1.3704 - val_loss: 7.0078 - val_mse: 7.0078 - val_mae: 1.2347 - lr: 4.8202e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 10s - loss: 10.2075 - mse: 10.2075 - mae: 1.3525 - val_loss: 7.1353 - val_mse: 7.1353 - val_mae: 1.3187 - lr: 4.8202e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 11s - loss: 10.1896 - mse: 10.1896 - mae: 1.3268 - val_loss: 7.1031 - val_mse: 7.1031 - val_mae: 1.2394 - lr: 4.8202e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 11s - loss: 9.8460 - mse: 9.8460 - mae: 1.3226 - val_loss: 7.1068 - val_mse: 7.1068 - val_mae: 1.3285 - lr: 4.8202e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 12s - loss: 9.9124 - mse: 9.9124 - mae: 1.3195 - val_loss: 6.9659 - val_mse: 6.9659 - val_mae: 1.3752 - lr: 4.8202e-04 - 12s/epoch - 12ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 11s - loss: 9.8214 - mse: 9.8214 - mae: 1.3006 - val_loss: 6.8655 - val_mse: 6.8655 - val_mae: 1.3906 - lr: 4.8202e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 11s - loss: 9.4452 - mse: 9.4452 - mae: 1.2841 - val_loss: 6.6580 - val_mse: 6.6580 - val_mae: 1.3294 - lr: 4.8202e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 11s - loss: 9.2821 - mse: 9.2821 - mae: 1.2759 - val_loss: 6.8878 - val_mse: 6.8878 - val_mae: 1.3929 - lr: 4.8202e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 11s - loss: 9.1166 - mse: 9.1166 - mae: 1.2610 - val_loss: 6.8337 - val_mse: 6.8337 - val_mae: 1.2813 - lr: 4.8202e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 11s - loss: 8.8662 - mse: 8.8662 - mae: 1.2513 - val_loss: 6.9885 - val_mse: 6.9885 - val_mae: 1.4402 - lr: 4.8202e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 11s - loss: 8.8369 - mse: 8.8369 - mae: 1.2416 - val_loss: 7.9387 - val_mse: 7.9387 - val_mae: 1.4371 - lr: 4.8202e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 10s - loss: 9.5723 - mse: 9.5723 - mae: 1.2334 - val_loss: 7.3357 - val_mse: 7.3357 - val_mae: 1.3886 - lr: 4.8202e-04 - 10s/epoch - 10ms/step\n",
            "Score for fold 4: loss of 7.335695266723633\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 11s - loss: 7.2873 - mse: 7.2873 - mae: 1.2622 - val_loss: 13.4177 - val_mse: 13.4177 - val_mae: 1.2362 - lr: 4.8202e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 11s - loss: 7.2022 - mse: 7.2022 - mae: 1.2597 - val_loss: 13.1241 - val_mse: 13.1241 - val_mae: 1.3528 - lr: 4.8202e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 11s - loss: 6.7691 - mse: 6.7691 - mae: 1.2257 - val_loss: 13.1194 - val_mse: 13.1194 - val_mae: 1.2440 - lr: 4.8202e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 11s - loss: 6.7779 - mse: 6.7779 - mae: 1.2205 - val_loss: 13.2711 - val_mse: 13.2711 - val_mae: 1.2780 - lr: 4.8202e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 11s - loss: 6.3346 - mse: 6.3346 - mae: 1.2007 - val_loss: 13.5791 - val_mse: 13.5791 - val_mae: 1.2248 - lr: 4.8202e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 11s - loss: 6.3065 - mse: 6.3065 - mae: 1.1898 - val_loss: 14.0134 - val_mse: 14.0134 - val_mae: 1.3048 - lr: 4.8202e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 10s - loss: 6.3929 - mse: 6.3929 - mae: 1.1850 - val_loss: 13.6223 - val_mse: 13.6223 - val_mae: 1.3238 - lr: 4.8202e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 11s - loss: 6.0716 - mse: 6.0716 - mae: 1.1760 - val_loss: 13.5178 - val_mse: 13.5178 - val_mae: 1.2882 - lr: 4.8202e-04 - 11s/epoch - 11ms/step\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-10-11 16:03:20,177]\u001b[0m Finished trial#24 resulted in value: 10.591999999999999. Current best value is 8.234 with parameters: {'activation': 'relu', 'num_hidden_layer': 4, 'i': 10, 'learning_rate': 0.00015658492778156685}.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Score for fold 5: loss of 13.517760276794434\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 12s - loss: 13.4402 - mse: 13.4402 - mae: 1.5520 - val_loss: 12.0842 - val_mse: 12.0842 - val_mae: 1.5415 - lr: 2.0144e-04 - 12s/epoch - 12ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 12s - loss: 12.4955 - mse: 12.4955 - mae: 1.4922 - val_loss: 11.9546 - val_mse: 11.9546 - val_mae: 1.4743 - lr: 2.0144e-04 - 12s/epoch - 12ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 11s - loss: 12.3921 - mse: 12.3921 - mae: 1.4833 - val_loss: 11.9720 - val_mse: 11.9720 - val_mae: 1.4305 - lr: 2.0144e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 11s - loss: 12.2254 - mse: 12.2254 - mae: 1.4728 - val_loss: 11.9936 - val_mse: 11.9936 - val_mae: 1.5118 - lr: 2.0144e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 12s - loss: 12.0719 - mse: 12.0719 - mae: 1.4669 - val_loss: 11.8536 - val_mse: 11.8536 - val_mae: 1.4714 - lr: 2.0144e-04 - 12s/epoch - 12ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 11s - loss: 11.7035 - mse: 11.7035 - mae: 1.4506 - val_loss: 11.7523 - val_mse: 11.7523 - val_mae: 1.5116 - lr: 2.0144e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 11s - loss: 11.6121 - mse: 11.6121 - mae: 1.4468 - val_loss: 12.0318 - val_mse: 12.0318 - val_mae: 1.4512 - lr: 2.0144e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 11s - loss: 11.3452 - mse: 11.3452 - mae: 1.4281 - val_loss: 11.3537 - val_mse: 11.3537 - val_mae: 1.4657 - lr: 2.0144e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 11s - loss: 11.0960 - mse: 11.0960 - mae: 1.4170 - val_loss: 11.8287 - val_mse: 11.8287 - val_mae: 1.4745 - lr: 2.0144e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 11s - loss: 10.7485 - mse: 10.7485 - mae: 1.3997 - val_loss: 11.7279 - val_mse: 11.7279 - val_mae: 1.4727 - lr: 2.0144e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 11s - loss: 10.6681 - mse: 10.6681 - mae: 1.3891 - val_loss: 11.6432 - val_mse: 11.6432 - val_mae: 1.4472 - lr: 2.0144e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 10s - loss: 10.4432 - mse: 10.4432 - mae: 1.3783 - val_loss: 12.2679 - val_mse: 12.2679 - val_mae: 1.4511 - lr: 2.0144e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 13/100\n",
            "1000/1000 - 11s - loss: 10.1259 - mse: 10.1259 - mae: 1.3520 - val_loss: 12.3839 - val_mse: 12.3839 - val_mae: 1.3951 - lr: 2.0144e-04 - 11s/epoch - 11ms/step\n",
            "Score for fold 1: loss of 12.383872985839844\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 11s - loss: 10.3520 - mse: 10.3520 - mae: 1.3781 - val_loss: 9.9936 - val_mse: 9.9936 - val_mae: 1.4266 - lr: 2.0144e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 11s - loss: 9.9646 - mse: 9.9646 - mae: 1.3516 - val_loss: 11.1070 - val_mse: 11.1070 - val_mae: 1.4214 - lr: 2.0144e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 11s - loss: 9.6261 - mse: 9.6261 - mae: 1.3364 - val_loss: 10.7374 - val_mse: 10.7374 - val_mae: 1.3776 - lr: 2.0144e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 11s - loss: 9.2444 - mse: 9.2444 - mae: 1.3032 - val_loss: 11.2498 - val_mse: 11.2498 - val_mae: 1.4473 - lr: 2.0144e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 11s - loss: 8.9916 - mse: 8.9916 - mae: 1.2905 - val_loss: 12.5974 - val_mse: 12.5974 - val_mae: 1.3349 - lr: 2.0144e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 11s - loss: 8.8869 - mse: 8.8869 - mae: 1.2704 - val_loss: 11.0587 - val_mse: 11.0587 - val_mae: 1.4080 - lr: 2.0144e-04 - 11s/epoch - 11ms/step\n",
            "Score for fold 2: loss of 11.058684349060059\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 11s - loss: 8.3285 - mse: 8.3285 - mae: 1.3177 - val_loss: 12.1017 - val_mse: 12.1017 - val_mae: 1.1812 - lr: 2.0144e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 11s - loss: 8.1127 - mse: 8.1127 - mae: 1.3004 - val_loss: 12.4916 - val_mse: 12.4916 - val_mae: 1.2077 - lr: 2.0144e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 11s - loss: 7.4528 - mse: 7.4528 - mae: 1.2741 - val_loss: 12.9454 - val_mse: 12.9454 - val_mae: 1.2429 - lr: 2.0144e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 11s - loss: 7.3178 - mse: 7.3178 - mae: 1.2494 - val_loss: 12.8336 - val_mse: 12.8336 - val_mae: 1.2001 - lr: 2.0144e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 11s - loss: 6.9600 - mse: 6.9600 - mae: 1.2287 - val_loss: 13.3059 - val_mse: 13.3059 - val_mae: 1.4732 - lr: 2.0144e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 11s - loss: 6.6376 - mse: 6.6376 - mae: 1.2082 - val_loss: 13.4335 - val_mse: 13.4335 - val_mae: 1.3135 - lr: 2.0144e-04 - 11s/epoch - 11ms/step\n",
            "Score for fold 3: loss of 13.433472633361816\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 11s - loss: 8.7462 - mse: 8.7462 - mae: 1.2465 - val_loss: 5.4405 - val_mse: 5.4405 - val_mae: 1.1034 - lr: 2.0144e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 11s - loss: 8.1108 - mse: 8.1108 - mae: 1.2172 - val_loss: 5.5199 - val_mse: 5.5199 - val_mae: 1.2483 - lr: 2.0144e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 11s - loss: 7.7027 - mse: 7.7027 - mae: 1.2008 - val_loss: 5.4956 - val_mse: 5.4956 - val_mae: 1.1440 - lr: 2.0144e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 11s - loss: 7.3731 - mse: 7.3731 - mae: 1.1702 - val_loss: 5.7428 - val_mse: 5.7428 - val_mae: 1.1604 - lr: 2.0144e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 11s - loss: 7.3398 - mse: 7.3398 - mae: 1.1516 - val_loss: 6.1075 - val_mse: 6.1075 - val_mae: 1.2082 - lr: 2.0144e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 11s - loss: 7.0587 - mse: 7.0587 - mae: 1.1357 - val_loss: 5.8387 - val_mse: 5.8387 - val_mae: 1.2445 - lr: 2.0144e-04 - 11s/epoch - 11ms/step\n",
            "Score for fold 4: loss of 5.8386549949646\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 11s - loss: 7.4795 - mse: 7.4795 - mae: 1.1774 - val_loss: 3.9124 - val_mse: 3.9124 - val_mae: 1.1832 - lr: 2.0144e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 11s - loss: 7.2849 - mse: 7.2849 - mae: 1.1512 - val_loss: 4.5412 - val_mse: 4.5412 - val_mae: 1.1054 - lr: 2.0144e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 11s - loss: 6.8870 - mse: 6.8870 - mae: 1.1319 - val_loss: 3.8756 - val_mse: 3.8756 - val_mae: 1.1132 - lr: 2.0144e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 11s - loss: 6.9335 - mse: 6.9335 - mae: 1.1144 - val_loss: 4.3722 - val_mse: 4.3722 - val_mae: 1.1381 - lr: 2.0144e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 11s - loss: 6.4211 - mse: 6.4211 - mae: 1.1004 - val_loss: 4.1263 - val_mse: 4.1263 - val_mae: 1.1931 - lr: 2.0144e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 11s - loss: 6.3928 - mse: 6.3928 - mae: 1.0943 - val_loss: 5.2523 - val_mse: 5.2523 - val_mae: 1.2432 - lr: 2.0144e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 11s - loss: 6.6341 - mse: 6.6341 - mae: 1.0739 - val_loss: 4.5278 - val_mse: 4.5278 - val_mae: 1.1285 - lr: 2.0144e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 11s - loss: 5.5645 - mse: 5.5645 - mae: 1.0441 - val_loss: 4.4580 - val_mse: 4.4580 - val_mae: 1.2349 - lr: 2.0144e-04 - 11s/epoch - 11ms/step\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-10-11 16:10:39,623]\u001b[0m Finished trial#25 resulted in value: 9.434000000000001. Current best value is 8.234 with parameters: {'activation': 'relu', 'num_hidden_layer': 4, 'i': 10, 'learning_rate': 0.00015658492778156685}.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Score for fold 5: loss of 4.457966327667236\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 8s - loss: 12.7892 - mse: 12.7892 - mae: 1.5485 - val_loss: 15.6729 - val_mse: 15.6729 - val_mae: 1.6240 - lr: 0.0022 - 8s/epoch - 8ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 7s - loss: 11.7977 - mse: 11.7977 - mae: 1.4953 - val_loss: 15.8995 - val_mse: 15.8995 - val_mae: 1.5303 - lr: 0.0022 - 7s/epoch - 7ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 7s - loss: 11.4804 - mse: 11.4804 - mae: 1.4772 - val_loss: 15.6342 - val_mse: 15.6342 - val_mae: 1.4753 - lr: 0.0022 - 7s/epoch - 7ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 7s - loss: 11.3046 - mse: 11.3046 - mae: 1.4598 - val_loss: 15.7743 - val_mse: 15.7743 - val_mae: 1.5414 - lr: 0.0022 - 7s/epoch - 7ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 7s - loss: 11.2070 - mse: 11.2070 - mae: 1.4512 - val_loss: 15.5274 - val_mse: 15.5274 - val_mae: 1.4557 - lr: 0.0022 - 7s/epoch - 7ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 7s - loss: 11.3289 - mse: 11.3289 - mae: 1.4539 - val_loss: 15.3736 - val_mse: 15.3736 - val_mae: 1.5432 - lr: 0.0022 - 7s/epoch - 7ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 7s - loss: 11.1108 - mse: 11.1108 - mae: 1.4627 - val_loss: 15.4485 - val_mse: 15.4485 - val_mae: 1.5099 - lr: 0.0022 - 7s/epoch - 7ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 7s - loss: 11.1891 - mse: 11.1891 - mae: 1.4633 - val_loss: 15.3302 - val_mse: 15.3302 - val_mae: 1.5424 - lr: 0.0022 - 7s/epoch - 7ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 7s - loss: 10.9366 - mse: 10.9366 - mae: 1.4570 - val_loss: 15.3267 - val_mse: 15.3267 - val_mae: 1.5127 - lr: 0.0022 - 7s/epoch - 7ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 7s - loss: 10.9469 - mse: 10.9469 - mae: 1.4535 - val_loss: 15.3150 - val_mse: 15.3150 - val_mae: 1.5659 - lr: 0.0022 - 7s/epoch - 7ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 7s - loss: 10.8260 - mse: 10.8260 - mae: 1.4462 - val_loss: 16.0241 - val_mse: 16.0241 - val_mae: 1.5179 - lr: 0.0022 - 7s/epoch - 7ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 7s - loss: 10.8304 - mse: 10.8304 - mae: 1.4369 - val_loss: 15.5505 - val_mse: 15.5505 - val_mae: 1.5068 - lr: 0.0022 - 7s/epoch - 7ms/step\n",
            "Epoch 13/100\n",
            "1000/1000 - 7s - loss: 10.6949 - mse: 10.6949 - mae: 1.4190 - val_loss: 16.1497 - val_mse: 16.1497 - val_mae: 1.5023 - lr: 0.0022 - 7s/epoch - 7ms/step\n",
            "Epoch 14/100\n",
            "1000/1000 - 7s - loss: 10.3542 - mse: 10.3542 - mae: 1.4141 - val_loss: 15.3059 - val_mse: 15.3059 - val_mae: 1.4662 - lr: 0.0022 - 7s/epoch - 7ms/step\n",
            "Epoch 15/100\n",
            "1000/1000 - 7s - loss: 10.3113 - mse: 10.3113 - mae: 1.4065 - val_loss: 15.5261 - val_mse: 15.5261 - val_mae: 1.5191 - lr: 0.0022 - 7s/epoch - 7ms/step\n",
            "Epoch 16/100\n",
            "1000/1000 - 7s - loss: 10.2078 - mse: 10.2078 - mae: 1.4038 - val_loss: 15.5013 - val_mse: 15.5013 - val_mae: 1.5189 - lr: 0.0022 - 7s/epoch - 7ms/step\n",
            "Epoch 17/100\n",
            "1000/1000 - 7s - loss: 9.9934 - mse: 9.9934 - mae: 1.3956 - val_loss: 15.7228 - val_mse: 15.7228 - val_mae: 1.4716 - lr: 0.0022 - 7s/epoch - 7ms/step\n",
            "Epoch 18/100\n",
            "1000/1000 - 7s - loss: 10.0232 - mse: 10.0232 - mae: 1.3978 - val_loss: 15.6012 - val_mse: 15.6012 - val_mae: 1.4751 - lr: 0.0022 - 7s/epoch - 7ms/step\n",
            "Epoch 19/100\n",
            "1000/1000 - 7s - loss: 9.7503 - mse: 9.7503 - mae: 1.3884 - val_loss: 15.6400 - val_mse: 15.6400 - val_mae: 1.5722 - lr: 0.0022 - 7s/epoch - 7ms/step\n",
            "Score for fold 1: loss of 15.639983177185059\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 7s - loss: 11.1794 - mse: 11.1794 - mae: 1.3940 - val_loss: 8.2916 - val_mse: 8.2916 - val_mae: 1.3489 - lr: 0.0010 - 7s/epoch - 7ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 7s - loss: 10.9073 - mse: 10.9073 - mae: 1.3800 - val_loss: 8.5805 - val_mse: 8.5805 - val_mae: 1.3516 - lr: 0.0010 - 7s/epoch - 7ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 7s - loss: 10.7375 - mse: 10.7375 - mae: 1.3691 - val_loss: 8.5611 - val_mse: 8.5611 - val_mae: 1.3520 - lr: 0.0010 - 7s/epoch - 7ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 7s - loss: 10.5678 - mse: 10.5678 - mae: 1.3574 - val_loss: 8.4791 - val_mse: 8.4791 - val_mae: 1.3611 - lr: 0.0010 - 7s/epoch - 7ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 7s - loss: 10.3286 - mse: 10.3286 - mae: 1.3523 - val_loss: 8.6718 - val_mse: 8.6718 - val_mae: 1.3691 - lr: 0.0010 - 7s/epoch - 7ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 7s - loss: 10.3066 - mse: 10.3066 - mae: 1.3428 - val_loss: 9.1313 - val_mse: 9.1313 - val_mae: 1.4185 - lr: 0.0010 - 7s/epoch - 7ms/step\n",
            "Score for fold 2: loss of 9.131309509277344\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 7s - loss: 10.7406 - mse: 10.7406 - mae: 1.3517 - val_loss: 6.1860 - val_mse: 6.1860 - val_mae: 1.3360 - lr: 0.0010 - 7s/epoch - 7ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 7s - loss: 10.6272 - mse: 10.6272 - mae: 1.3416 - val_loss: 6.1203 - val_mse: 6.1203 - val_mae: 1.3749 - lr: 0.0010 - 7s/epoch - 7ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 7s - loss: 10.6467 - mse: 10.6467 - mae: 1.3318 - val_loss: 6.1258 - val_mse: 6.1258 - val_mae: 1.3274 - lr: 0.0010 - 7s/epoch - 7ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 7s - loss: 10.3500 - mse: 10.3500 - mae: 1.3223 - val_loss: 6.3190 - val_mse: 6.3190 - val_mae: 1.3595 - lr: 0.0010 - 7s/epoch - 7ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 7s - loss: 10.4378 - mse: 10.4378 - mae: 1.3129 - val_loss: 6.5615 - val_mse: 6.5615 - val_mae: 1.3547 - lr: 0.0010 - 7s/epoch - 7ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 7s - loss: 10.1263 - mse: 10.1263 - mae: 1.3060 - val_loss: 6.6420 - val_mse: 6.6420 - val_mae: 1.3796 - lr: 0.0010 - 7s/epoch - 7ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 7s - loss: 9.9893 - mse: 9.9893 - mae: 1.2921 - val_loss: 6.7817 - val_mse: 6.7817 - val_mae: 1.3713 - lr: 0.0010 - 7s/epoch - 7ms/step\n",
            "Score for fold 3: loss of 6.781696796417236\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 7s - loss: 9.8316 - mse: 9.8316 - mae: 1.3226 - val_loss: 6.3992 - val_mse: 6.3992 - val_mae: 1.2883 - lr: 0.0010 - 7s/epoch - 7ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 7s - loss: 9.5510 - mse: 9.5510 - mae: 1.3083 - val_loss: 6.7559 - val_mse: 6.7559 - val_mae: 1.2603 - lr: 0.0010 - 7s/epoch - 7ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 7s - loss: 9.4652 - mse: 9.4652 - mae: 1.2951 - val_loss: 6.8155 - val_mse: 6.8155 - val_mae: 1.2563 - lr: 0.0010 - 7s/epoch - 7ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 7s - loss: 9.3315 - mse: 9.3315 - mae: 1.2855 - val_loss: 6.8319 - val_mse: 6.8319 - val_mae: 1.2879 - lr: 0.0010 - 7s/epoch - 7ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 7s - loss: 9.1580 - mse: 9.1580 - mae: 1.2673 - val_loss: 7.0399 - val_mse: 7.0399 - val_mae: 1.2846 - lr: 0.0010 - 7s/epoch - 7ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 7s - loss: 9.0304 - mse: 9.0304 - mae: 1.2635 - val_loss: 6.9920 - val_mse: 6.9920 - val_mae: 1.2933 - lr: 0.0010 - 7s/epoch - 7ms/step\n",
            "Score for fold 4: loss of 6.992002964019775\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 7s - loss: 7.7726 - mse: 7.7726 - mae: 1.2822 - val_loss: 11.9992 - val_mse: 11.9992 - val_mae: 1.2163 - lr: 0.0010 - 7s/epoch - 7ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 7s - loss: 7.5314 - mse: 7.5314 - mae: 1.2669 - val_loss: 11.7277 - val_mse: 11.7277 - val_mae: 1.2292 - lr: 0.0010 - 7s/epoch - 7ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 7s - loss: 7.3478 - mse: 7.3478 - mae: 1.2508 - val_loss: 12.3741 - val_mse: 12.3741 - val_mae: 1.2473 - lr: 0.0010 - 7s/epoch - 7ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 7s - loss: 7.3426 - mse: 7.3426 - mae: 1.2437 - val_loss: 12.5255 - val_mse: 12.5255 - val_mae: 1.2568 - lr: 0.0010 - 7s/epoch - 7ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 7s - loss: 7.1184 - mse: 7.1184 - mae: 1.2285 - val_loss: 13.4457 - val_mse: 13.4457 - val_mae: 1.2570 - lr: 0.0010 - 7s/epoch - 7ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 7s - loss: 7.0744 - mse: 7.0744 - mae: 1.2195 - val_loss: 13.9708 - val_mse: 13.9708 - val_mae: 1.2793 - lr: 0.0010 - 7s/epoch - 7ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 7s - loss: 6.9057 - mse: 6.9057 - mae: 1.2115 - val_loss: 13.3234 - val_mse: 13.3234 - val_mae: 1.2611 - lr: 0.0010 - 7s/epoch - 7ms/step\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-10-11 16:15:58,502]\u001b[0m Finished trial#26 resulted in value: 10.372000000000002. Current best value is 8.234 with parameters: {'activation': 'relu', 'num_hidden_layer': 4, 'i': 10, 'learning_rate': 0.00015658492778156685}.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Score for fold 5: loss of 13.32341480255127\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 12s - loss: 14.1262 - mse: 14.1262 - mae: 1.6142 - val_loss: 16.0979 - val_mse: 16.0979 - val_mae: 1.5448 - lr: 3.9087e-04 - 12s/epoch - 12ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 11s - loss: 13.5836 - mse: 13.5836 - mae: 1.5409 - val_loss: 15.7683 - val_mse: 15.7683 - val_mae: 1.5957 - lr: 3.9087e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 11s - loss: 13.4635 - mse: 13.4635 - mae: 1.5271 - val_loss: 15.8958 - val_mse: 15.8958 - val_mae: 1.6472 - lr: 3.9087e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 11s - loss: 13.3876 - mse: 13.3876 - mae: 1.5156 - val_loss: 15.3037 - val_mse: 15.3037 - val_mae: 1.6837 - lr: 3.9087e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 11s - loss: 13.1654 - mse: 13.1654 - mae: 1.4955 - val_loss: 15.0082 - val_mse: 15.0082 - val_mae: 1.5732 - lr: 3.9087e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 11s - loss: 13.0660 - mse: 13.0660 - mae: 1.4924 - val_loss: 15.2142 - val_mse: 15.2142 - val_mae: 1.5459 - lr: 3.9087e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 11s - loss: 12.9617 - mse: 12.9617 - mae: 1.4885 - val_loss: 14.9014 - val_mse: 14.9014 - val_mae: 1.6803 - lr: 3.9087e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 11s - loss: 12.8314 - mse: 12.8314 - mae: 1.4765 - val_loss: 15.1023 - val_mse: 15.1023 - val_mae: 1.5128 - lr: 3.9087e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 11s - loss: 12.7724 - mse: 12.7724 - mae: 1.4770 - val_loss: 14.4643 - val_mse: 14.4643 - val_mae: 1.5087 - lr: 3.9087e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 11s - loss: 12.6452 - mse: 12.6452 - mae: 1.4704 - val_loss: 14.5039 - val_mse: 14.5039 - val_mae: 1.4601 - lr: 3.9087e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 10s - loss: 12.6438 - mse: 12.6438 - mae: 1.4727 - val_loss: 14.8901 - val_mse: 14.8901 - val_mae: 1.5537 - lr: 3.9087e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 11s - loss: 12.5813 - mse: 12.5813 - mae: 1.4741 - val_loss: 14.3070 - val_mse: 14.3070 - val_mae: 1.5265 - lr: 3.9087e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 13/100\n",
            "1000/1000 - 11s - loss: 12.4491 - mse: 12.4491 - mae: 1.4589 - val_loss: 14.6285 - val_mse: 14.6285 - val_mae: 1.6417 - lr: 3.9087e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 14/100\n",
            "1000/1000 - 11s - loss: 12.3616 - mse: 12.3616 - mae: 1.4584 - val_loss: 14.2084 - val_mse: 14.2084 - val_mae: 1.5262 - lr: 3.9087e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 15/100\n",
            "1000/1000 - 11s - loss: 12.2978 - mse: 12.2978 - mae: 1.4635 - val_loss: 14.0548 - val_mse: 14.0548 - val_mae: 1.5881 - lr: 3.9087e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 16/100\n",
            "1000/1000 - 11s - loss: 12.3728 - mse: 12.3728 - mae: 1.4592 - val_loss: 14.1319 - val_mse: 14.1319 - val_mae: 1.5249 - lr: 3.9087e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 17/100\n",
            "1000/1000 - 11s - loss: 12.1473 - mse: 12.1473 - mae: 1.4513 - val_loss: 14.5985 - val_mse: 14.5985 - val_mae: 1.4384 - lr: 3.9087e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 18/100\n",
            "1000/1000 - 11s - loss: 12.1104 - mse: 12.1104 - mae: 1.4544 - val_loss: 13.7190 - val_mse: 13.7190 - val_mae: 1.5574 - lr: 3.9087e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 19/100\n",
            "1000/1000 - 11s - loss: 12.0162 - mse: 12.0162 - mae: 1.4459 - val_loss: 13.5982 - val_mse: 13.5982 - val_mae: 1.5233 - lr: 3.9087e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 20/100\n",
            "1000/1000 - 11s - loss: 11.8820 - mse: 11.8820 - mae: 1.4416 - val_loss: 13.4818 - val_mse: 13.4818 - val_mae: 1.5346 - lr: 3.9087e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 21/100\n",
            "1000/1000 - 11s - loss: 12.0675 - mse: 12.0675 - mae: 1.4463 - val_loss: 13.7133 - val_mse: 13.7133 - val_mae: 1.4551 - lr: 3.9087e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 22/100\n",
            "1000/1000 - 11s - loss: 11.8665 - mse: 11.8665 - mae: 1.4423 - val_loss: 13.6938 - val_mse: 13.6938 - val_mae: 1.6605 - lr: 3.9087e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 23/100\n",
            "1000/1000 - 11s - loss: 11.7614 - mse: 11.7614 - mae: 1.4445 - val_loss: 13.6815 - val_mse: 13.6815 - val_mae: 1.4603 - lr: 3.9087e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 24/100\n",
            "1000/1000 - 11s - loss: 11.7472 - mse: 11.7472 - mae: 1.4340 - val_loss: 13.7965 - val_mse: 13.7965 - val_mae: 1.6648 - lr: 3.9087e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 25/100\n",
            "1000/1000 - 11s - loss: 11.8335 - mse: 11.8335 - mae: 1.4336 - val_loss: 13.3180 - val_mse: 13.3180 - val_mae: 1.5668 - lr: 3.9087e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 26/100\n",
            "1000/1000 - 11s - loss: 11.7257 - mse: 11.7257 - mae: 1.4268 - val_loss: 13.4043 - val_mse: 13.4043 - val_mae: 1.5009 - lr: 3.9087e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 27/100\n",
            "1000/1000 - 11s - loss: 11.6839 - mse: 11.6839 - mae: 1.4214 - val_loss: 13.3759 - val_mse: 13.3759 - val_mae: 1.6033 - lr: 3.9087e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 28/100\n",
            "1000/1000 - 11s - loss: 11.4196 - mse: 11.4196 - mae: 1.4115 - val_loss: 13.4491 - val_mse: 13.4491 - val_mae: 1.5985 - lr: 3.9087e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 29/100\n",
            "1000/1000 - 11s - loss: 11.3400 - mse: 11.3400 - mae: 1.4104 - val_loss: 13.2445 - val_mse: 13.2445 - val_mae: 1.5669 - lr: 3.9087e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 30/100\n",
            "1000/1000 - 11s - loss: 11.2893 - mse: 11.2893 - mae: 1.4096 - val_loss: 13.2594 - val_mse: 13.2594 - val_mae: 1.4982 - lr: 3.9087e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 31/100\n",
            "1000/1000 - 11s - loss: 11.2615 - mse: 11.2615 - mae: 1.4024 - val_loss: 13.4248 - val_mse: 13.4248 - val_mae: 1.4725 - lr: 3.9087e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 32/100\n",
            "1000/1000 - 11s - loss: 10.9870 - mse: 10.9870 - mae: 1.3921 - val_loss: 13.0582 - val_mse: 13.0582 - val_mae: 1.5134 - lr: 3.9087e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 33/100\n",
            "1000/1000 - 10s - loss: 10.9780 - mse: 10.9780 - mae: 1.3813 - val_loss: 13.0850 - val_mse: 13.0850 - val_mae: 1.5162 - lr: 3.9087e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 34/100\n",
            "1000/1000 - 12s - loss: 10.7589 - mse: 10.7589 - mae: 1.3782 - val_loss: 13.0196 - val_mse: 13.0196 - val_mae: 1.6128 - lr: 3.9087e-04 - 12s/epoch - 12ms/step\n",
            "Epoch 35/100\n",
            "1000/1000 - 11s - loss: 10.5495 - mse: 10.5495 - mae: 1.3727 - val_loss: 13.5298 - val_mse: 13.5298 - val_mae: 1.4941 - lr: 3.9087e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 36/100\n",
            "1000/1000 - 11s - loss: 10.5762 - mse: 10.5762 - mae: 1.3711 - val_loss: 13.4719 - val_mse: 13.4719 - val_mae: 1.6171 - lr: 3.9087e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 37/100\n",
            "1000/1000 - 11s - loss: 10.3528 - mse: 10.3528 - mae: 1.3553 - val_loss: 13.7097 - val_mse: 13.7097 - val_mae: 1.5414 - lr: 3.9087e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 38/100\n",
            "1000/1000 - 10s - loss: 10.2200 - mse: 10.2200 - mae: 1.3422 - val_loss: 13.2180 - val_mse: 13.2180 - val_mae: 1.5689 - lr: 3.9087e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 39/100\n",
            "1000/1000 - 11s - loss: 9.9187 - mse: 9.9187 - mae: 1.3336 - val_loss: 13.2603 - val_mse: 13.2603 - val_mae: 1.5071 - lr: 3.9087e-04 - 11s/epoch - 11ms/step\n",
            "Score for fold 1: loss of 13.260339736938477\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 11s - loss: 11.1929 - mse: 11.1929 - mae: 1.3845 - val_loss: 8.5087 - val_mse: 8.5087 - val_mae: 1.4088 - lr: 3.9087e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 11s - loss: 11.0028 - mse: 11.0028 - mae: 1.3722 - val_loss: 8.1816 - val_mse: 8.1816 - val_mae: 1.3234 - lr: 3.9087e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 11s - loss: 10.7816 - mse: 10.7816 - mae: 1.3581 - val_loss: 8.5706 - val_mse: 8.5706 - val_mae: 1.3663 - lr: 3.9087e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 11s - loss: 10.5089 - mse: 10.5089 - mae: 1.3401 - val_loss: 8.5156 - val_mse: 8.5156 - val_mae: 1.3622 - lr: 3.9087e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 10s - loss: 10.2520 - mse: 10.2520 - mae: 1.3350 - val_loss: 8.9398 - val_mse: 8.9398 - val_mae: 1.3182 - lr: 3.9087e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 11s - loss: 9.9731 - mse: 9.9731 - mae: 1.3111 - val_loss: 9.0499 - val_mse: 9.0499 - val_mae: 1.3293 - lr: 3.9087e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 11s - loss: 9.7946 - mse: 9.7946 - mae: 1.2991 - val_loss: 8.7007 - val_mse: 8.7007 - val_mae: 1.4631 - lr: 3.9087e-04 - 11s/epoch - 11ms/step\n",
            "Score for fold 2: loss of 8.700677871704102\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 11s - loss: 10.6432 - mse: 10.6432 - mae: 1.3403 - val_loss: 5.4355 - val_mse: 5.4355 - val_mae: 1.3228 - lr: 3.9087e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 11s - loss: 10.3739 - mse: 10.3739 - mae: 1.3218 - val_loss: 6.2904 - val_mse: 6.2904 - val_mae: 1.4695 - lr: 3.9087e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 10s - loss: 10.2078 - mse: 10.2078 - mae: 1.3133 - val_loss: 6.0844 - val_mse: 6.0844 - val_mae: 1.4082 - lr: 3.9087e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 11s - loss: 9.5860 - mse: 9.5860 - mae: 1.2879 - val_loss: 6.2657 - val_mse: 6.2657 - val_mae: 1.2510 - lr: 3.9087e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 11s - loss: 9.5788 - mse: 9.5788 - mae: 1.2783 - val_loss: 5.9201 - val_mse: 5.9201 - val_mae: 1.2559 - lr: 3.9087e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 10s - loss: 9.4132 - mse: 9.4132 - mae: 1.2630 - val_loss: 6.1500 - val_mse: 6.1500 - val_mae: 1.3583 - lr: 3.9087e-04 - 10s/epoch - 10ms/step\n",
            "Score for fold 3: loss of 6.150025844573975\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 11s - loss: 8.7976 - mse: 8.7976 - mae: 1.2935 - val_loss: 8.1649 - val_mse: 8.1649 - val_mae: 1.2169 - lr: 3.9087e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 10s - loss: 8.3189 - mse: 8.3189 - mae: 1.2686 - val_loss: 8.6369 - val_mse: 8.6369 - val_mae: 1.2603 - lr: 3.9087e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 10s - loss: 8.1791 - mse: 8.1791 - mae: 1.2473 - val_loss: 8.7457 - val_mse: 8.7457 - val_mae: 1.2130 - lr: 3.9087e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 11s - loss: 7.7841 - mse: 7.7841 - mae: 1.2358 - val_loss: 8.9180 - val_mse: 8.9180 - val_mae: 1.3110 - lr: 3.9087e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 11s - loss: 7.8334 - mse: 7.8334 - mae: 1.2239 - val_loss: 9.2096 - val_mse: 9.2096 - val_mae: 1.3211 - lr: 3.9087e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 11s - loss: 7.3309 - mse: 7.3309 - mae: 1.2036 - val_loss: 9.1619 - val_mse: 9.1619 - val_mae: 1.2356 - lr: 3.9087e-04 - 11s/epoch - 11ms/step\n",
            "Score for fold 4: loss of 9.161910057067871\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 11s - loss: 6.8036 - mse: 6.8036 - mae: 1.2500 - val_loss: 12.0379 - val_mse: 12.0379 - val_mae: 1.2061 - lr: 3.9087e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 11s - loss: 6.2799 - mse: 6.2799 - mae: 1.2186 - val_loss: 12.2808 - val_mse: 12.2808 - val_mae: 1.1869 - lr: 3.9087e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 10s - loss: 6.1510 - mse: 6.1510 - mae: 1.2105 - val_loss: 13.3409 - val_mse: 13.3409 - val_mae: 1.1504 - lr: 3.9087e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 11s - loss: 5.8126 - mse: 5.8126 - mae: 1.1870 - val_loss: 12.9558 - val_mse: 12.9558 - val_mae: 1.1912 - lr: 3.9087e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 11s - loss: 5.7976 - mse: 5.7976 - mae: 1.1779 - val_loss: 12.7837 - val_mse: 12.7837 - val_mae: 1.2329 - lr: 3.9087e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 10s - loss: 5.5015 - mse: 5.5015 - mae: 1.1587 - val_loss: 13.6580 - val_mse: 13.6580 - val_mae: 1.2619 - lr: 3.9087e-04 - 10s/epoch - 10ms/step\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-10-11 16:27:37,258]\u001b[0m Finished trial#27 resulted in value: 10.185999999999998. Current best value is 8.234 with parameters: {'activation': 'relu', 'num_hidden_layer': 4, 'i': 10, 'learning_rate': 0.00015658492778156685}.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Score for fold 5: loss of 13.657992362976074\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 7s - loss: 13.9561 - mse: 13.9561 - mae: 1.5439 - val_loss: 10.4837 - val_mse: 10.4837 - val_mae: 1.4938 - lr: 1.3600e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 6s - loss: 12.9799 - mse: 12.9799 - mae: 1.4826 - val_loss: 9.8803 - val_mse: 9.8803 - val_mae: 1.5641 - lr: 1.3600e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 6s - loss: 12.6137 - mse: 12.6137 - mae: 1.4613 - val_loss: 9.9379 - val_mse: 9.9379 - val_mae: 1.5163 - lr: 1.3600e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 6s - loss: 12.5597 - mse: 12.5597 - mae: 1.4489 - val_loss: 9.7597 - val_mse: 9.7597 - val_mae: 1.4904 - lr: 1.3600e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 6s - loss: 12.3827 - mse: 12.3827 - mae: 1.4458 - val_loss: 9.7982 - val_mse: 9.7982 - val_mae: 1.4362 - lr: 1.3600e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 6s - loss: 12.2629 - mse: 12.2629 - mae: 1.4403 - val_loss: 9.7438 - val_mse: 9.7438 - val_mae: 1.5003 - lr: 1.3600e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 6s - loss: 12.0671 - mse: 12.0671 - mae: 1.4249 - val_loss: 9.6737 - val_mse: 9.6737 - val_mae: 1.4425 - lr: 1.3600e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 6s - loss: 12.0663 - mse: 12.0663 - mae: 1.4185 - val_loss: 9.7072 - val_mse: 9.7072 - val_mae: 1.4771 - lr: 1.3600e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 6s - loss: 11.8954 - mse: 11.8954 - mae: 1.4125 - val_loss: 9.7437 - val_mse: 9.7437 - val_mae: 1.4658 - lr: 1.3600e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 6s - loss: 11.7980 - mse: 11.7980 - mae: 1.4018 - val_loss: 9.7488 - val_mse: 9.7488 - val_mae: 1.5156 - lr: 1.3600e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 6s - loss: 11.6467 - mse: 11.6467 - mae: 1.3942 - val_loss: 9.6888 - val_mse: 9.6888 - val_mae: 1.5154 - lr: 1.3600e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 6s - loss: 11.5966 - mse: 11.5966 - mae: 1.3868 - val_loss: 9.6888 - val_mse: 9.6888 - val_mae: 1.4756 - lr: 1.3600e-04 - 6s/epoch - 6ms/step\n",
            "Score for fold 1: loss of 9.688800811767578\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 6s - loss: 10.8286 - mse: 10.8286 - mae: 1.3975 - val_loss: 11.9851 - val_mse: 11.9851 - val_mae: 1.4091 - lr: 1.3600e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 6s - loss: 10.8770 - mse: 10.8770 - mae: 1.3913 - val_loss: 12.0117 - val_mse: 12.0117 - val_mae: 1.4187 - lr: 1.3600e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 6s - loss: 10.4852 - mse: 10.4852 - mae: 1.3749 - val_loss: 12.2932 - val_mse: 12.2932 - val_mae: 1.4408 - lr: 1.3600e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 6s - loss: 10.3074 - mse: 10.3074 - mae: 1.3636 - val_loss: 12.2777 - val_mse: 12.2777 - val_mae: 1.4237 - lr: 1.3600e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 6s - loss: 10.1195 - mse: 10.1195 - mae: 1.3506 - val_loss: 12.4777 - val_mse: 12.4777 - val_mae: 1.4945 - lr: 1.3600e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 6s - loss: 10.0745 - mse: 10.0745 - mae: 1.3420 - val_loss: 12.0375 - val_mse: 12.0375 - val_mae: 1.5172 - lr: 1.3600e-04 - 6s/epoch - 6ms/step\n",
            "Score for fold 2: loss of 12.037542343139648\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 6s - loss: 9.2730 - mse: 9.2730 - mae: 1.3660 - val_loss: 14.3834 - val_mse: 14.3834 - val_mae: 1.3195 - lr: 1.3600e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 6s - loss: 9.2034 - mse: 9.2034 - mae: 1.3543 - val_loss: 14.6269 - val_mse: 14.6269 - val_mae: 1.3660 - lr: 1.3600e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 6s - loss: 8.8625 - mse: 8.8625 - mae: 1.3363 - val_loss: 16.5733 - val_mse: 16.5733 - val_mae: 1.3401 - lr: 1.3600e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 6s - loss: 8.7332 - mse: 8.7332 - mae: 1.3279 - val_loss: 14.9922 - val_mse: 14.9922 - val_mae: 1.4036 - lr: 1.3600e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 6s - loss: 8.9122 - mse: 8.9122 - mae: 1.3207 - val_loss: 15.1923 - val_mse: 15.1923 - val_mae: 1.3622 - lr: 1.3600e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 6s - loss: 8.1452 - mse: 8.1452 - mae: 1.2998 - val_loss: 14.9707 - val_mse: 14.9707 - val_mae: 1.3569 - lr: 1.3600e-04 - 6s/epoch - 6ms/step\n",
            "Score for fold 3: loss of 14.970697402954102\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 6s - loss: 9.8170 - mse: 9.8170 - mae: 1.3320 - val_loss: 9.0626 - val_mse: 9.0626 - val_mae: 1.2141 - lr: 1.3600e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 6s - loss: 9.8380 - mse: 9.8380 - mae: 1.3143 - val_loss: 8.8927 - val_mse: 8.8927 - val_mae: 1.2166 - lr: 1.3600e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 6s - loss: 9.3523 - mse: 9.3523 - mae: 1.2958 - val_loss: 8.4539 - val_mse: 8.4539 - val_mae: 1.3643 - lr: 1.3600e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 6s - loss: 9.1753 - mse: 9.1753 - mae: 1.2813 - val_loss: 8.4015 - val_mse: 8.4015 - val_mae: 1.2788 - lr: 1.3600e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 6s - loss: 9.0270 - mse: 9.0270 - mae: 1.2696 - val_loss: 9.5510 - val_mse: 9.5510 - val_mae: 1.3192 - lr: 1.3600e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 6s - loss: 8.7296 - mse: 8.7296 - mae: 1.2512 - val_loss: 8.8588 - val_mse: 8.8588 - val_mae: 1.3535 - lr: 1.3600e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 6s - loss: 8.5184 - mse: 8.5184 - mae: 1.2383 - val_loss: 9.1194 - val_mse: 9.1194 - val_mae: 1.3480 - lr: 1.3600e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 6s - loss: 8.4255 - mse: 8.4255 - mae: 1.2218 - val_loss: 9.1863 - val_mse: 9.1863 - val_mae: 1.3097 - lr: 1.3600e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 6s - loss: 8.0404 - mse: 8.0404 - mae: 1.2059 - val_loss: 9.3259 - val_mse: 9.3259 - val_mae: 1.3114 - lr: 1.3600e-04 - 6s/epoch - 6ms/step\n",
            "Score for fold 4: loss of 9.32593059539795\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 7s - loss: 8.9352 - mse: 8.9352 - mae: 1.2535 - val_loss: 5.6324 - val_mse: 5.6324 - val_mae: 1.1830 - lr: 1.3600e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 6s - loss: 8.7374 - mse: 8.7374 - mae: 1.2331 - val_loss: 5.9987 - val_mse: 5.9987 - val_mae: 1.1434 - lr: 1.3600e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 6s - loss: 8.3891 - mse: 8.3891 - mae: 1.2119 - val_loss: 5.7444 - val_mse: 5.7444 - val_mae: 1.1797 - lr: 1.3600e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 6s - loss: 7.9225 - mse: 7.9225 - mae: 1.1937 - val_loss: 6.0579 - val_mse: 6.0579 - val_mae: 1.1493 - lr: 1.3600e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 6s - loss: 7.8869 - mse: 7.8869 - mae: 1.1800 - val_loss: 6.0711 - val_mse: 6.0711 - val_mae: 1.2298 - lr: 1.3600e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 6s - loss: 7.6806 - mse: 7.6806 - mae: 1.1616 - val_loss: 6.1210 - val_mse: 6.1210 - val_mae: 1.2224 - lr: 1.3600e-04 - 6s/epoch - 6ms/step\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-10-11 16:31:51,992]\u001b[0m Finished trial#28 resulted in value: 10.429999999999998. Current best value is 8.234 with parameters: {'activation': 'relu', 'num_hidden_layer': 4, 'i': 10, 'learning_rate': 0.00015658492778156685}.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Score for fold 5: loss of 6.121025085449219\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 3s - loss: 13.3152 - mse: 13.3152 - mae: 1.5507 - val_loss: 13.8882 - val_mse: 13.8882 - val_mae: 1.4724 - lr: 7.1570e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 12.3314 - mse: 12.3314 - mae: 1.4924 - val_loss: 13.3314 - val_mse: 13.3314 - val_mae: 1.4544 - lr: 7.1570e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 11.9842 - mse: 11.9842 - mae: 1.4725 - val_loss: 13.4205 - val_mse: 13.4205 - val_mae: 1.4647 - lr: 7.1570e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 11.7567 - mse: 11.7567 - mae: 1.4607 - val_loss: 13.8403 - val_mse: 13.8403 - val_mae: 1.4264 - lr: 7.1570e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 11.6991 - mse: 11.6991 - mae: 1.4536 - val_loss: 13.5738 - val_mse: 13.5738 - val_mae: 1.4672 - lr: 7.1570e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 11.6294 - mse: 11.6294 - mae: 1.4530 - val_loss: 13.2090 - val_mse: 13.2090 - val_mae: 1.4458 - lr: 7.1570e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 11.6473 - mse: 11.6473 - mae: 1.4465 - val_loss: 13.2129 - val_mse: 13.2129 - val_mae: 1.4611 - lr: 7.1570e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 2s - loss: 11.4513 - mse: 11.4513 - mae: 1.4383 - val_loss: 13.4883 - val_mse: 13.4883 - val_mae: 1.4561 - lr: 7.1570e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 2s - loss: 11.5761 - mse: 11.5761 - mae: 1.4380 - val_loss: 13.3894 - val_mse: 13.3894 - val_mae: 1.4553 - lr: 7.1570e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 2s - loss: 11.4319 - mse: 11.4319 - mae: 1.4283 - val_loss: 13.9096 - val_mse: 13.9096 - val_mae: 1.4512 - lr: 7.1570e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 2s - loss: 11.5528 - mse: 11.5528 - mae: 1.4290 - val_loss: 13.6355 - val_mse: 13.6355 - val_mae: 1.4793 - lr: 7.1570e-04 - 2s/epoch - 2ms/step\n",
            "Score for fold 1: loss of 13.635477066040039\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 10.1442 - mse: 10.1442 - mae: 1.4210 - val_loss: 18.3101 - val_mse: 18.3101 - val_mae: 1.4513 - lr: 7.1570e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 10.0535 - mse: 10.0535 - mae: 1.4159 - val_loss: 18.4794 - val_mse: 18.4794 - val_mae: 1.4627 - lr: 7.1570e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 9.7628 - mse: 9.7628 - mae: 1.4111 - val_loss: 18.3554 - val_mse: 18.3554 - val_mae: 1.4322 - lr: 7.1570e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 9.6411 - mse: 9.6411 - mae: 1.4082 - val_loss: 18.2673 - val_mse: 18.2673 - val_mae: 1.4785 - lr: 7.1570e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 9.5948 - mse: 9.5948 - mae: 1.4001 - val_loss: 18.3831 - val_mse: 18.3831 - val_mae: 1.4635 - lr: 7.1570e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 9.4861 - mse: 9.4861 - mae: 1.3971 - val_loss: 18.6765 - val_mse: 18.6765 - val_mae: 1.4202 - lr: 7.1570e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 9.4579 - mse: 9.4579 - mae: 1.3933 - val_loss: 18.4572 - val_mse: 18.4572 - val_mae: 1.4635 - lr: 7.1570e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 2s - loss: 9.2234 - mse: 9.2234 - mae: 1.3900 - val_loss: 18.3400 - val_mse: 18.3400 - val_mae: 1.4863 - lr: 7.1570e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 2s - loss: 9.2504 - mse: 9.2504 - mae: 1.3809 - val_loss: 18.2784 - val_mse: 18.2784 - val_mae: 1.5174 - lr: 7.1570e-04 - 2s/epoch - 2ms/step\n",
            "Score for fold 2: loss of 18.27838706970215\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 11.8877 - mse: 11.8877 - mae: 1.4178 - val_loss: 7.7219 - val_mse: 7.7219 - val_mae: 1.3373 - lr: 7.1570e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 11.8185 - mse: 11.8185 - mae: 1.4114 - val_loss: 7.6438 - val_mse: 7.6438 - val_mae: 1.3927 - lr: 7.1570e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 11.7547 - mse: 11.7547 - mae: 1.4079 - val_loss: 7.0611 - val_mse: 7.0611 - val_mae: 1.3530 - lr: 7.1570e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 11.8332 - mse: 11.8332 - mae: 1.4071 - val_loss: 7.3937 - val_mse: 7.3937 - val_mae: 1.3466 - lr: 7.1570e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 11.7099 - mse: 11.7099 - mae: 1.3993 - val_loss: 7.3622 - val_mse: 7.3622 - val_mae: 1.3705 - lr: 7.1570e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 11.6337 - mse: 11.6337 - mae: 1.3956 - val_loss: 8.0536 - val_mse: 8.0536 - val_mae: 1.4215 - lr: 7.1570e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 11.4920 - mse: 11.4920 - mae: 1.3913 - val_loss: 7.2032 - val_mse: 7.2032 - val_mae: 1.4067 - lr: 7.1570e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 2s - loss: 11.5490 - mse: 11.5490 - mae: 1.3885 - val_loss: 7.3162 - val_mse: 7.3162 - val_mae: 1.3986 - lr: 7.1570e-04 - 2s/epoch - 2ms/step\n",
            "Score for fold 3: loss of 7.31619119644165\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 11.6196 - mse: 11.6196 - mae: 1.3930 - val_loss: 6.6926 - val_mse: 6.6926 - val_mae: 1.3601 - lr: 7.1570e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 11.5114 - mse: 11.5114 - mae: 1.3843 - val_loss: 6.8399 - val_mse: 6.8399 - val_mae: 1.3635 - lr: 7.1570e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 11.2654 - mse: 11.2654 - mae: 1.3789 - val_loss: 7.0306 - val_mse: 7.0306 - val_mae: 1.3993 - lr: 7.1570e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 11.1891 - mse: 11.1891 - mae: 1.3786 - val_loss: 7.1217 - val_mse: 7.1217 - val_mae: 1.3713 - lr: 7.1570e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 11.0094 - mse: 11.0094 - mae: 1.3694 - val_loss: 7.1074 - val_mse: 7.1074 - val_mae: 1.3594 - lr: 7.1570e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 11.1021 - mse: 11.1021 - mae: 1.3696 - val_loss: 7.2571 - val_mse: 7.2571 - val_mae: 1.3597 - lr: 7.1570e-04 - 2s/epoch - 2ms/step\n",
            "Score for fold 4: loss of 7.257060527801514\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 10.5808 - mse: 10.5808 - mae: 1.3740 - val_loss: 9.3401 - val_mse: 9.3401 - val_mae: 1.3673 - lr: 7.1570e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 10.6475 - mse: 10.6475 - mae: 1.3686 - val_loss: 9.1013 - val_mse: 9.1013 - val_mae: 1.3414 - lr: 7.1570e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 10.3375 - mse: 10.3375 - mae: 1.3582 - val_loss: 9.3217 - val_mse: 9.3217 - val_mae: 1.3524 - lr: 7.1570e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 10.3421 - mse: 10.3421 - mae: 1.3554 - val_loss: 9.4869 - val_mse: 9.4869 - val_mae: 1.3703 - lr: 7.1570e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 10.2946 - mse: 10.2946 - mae: 1.3508 - val_loss: 9.4423 - val_mse: 9.4423 - val_mae: 1.3794 - lr: 7.1570e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 10.1282 - mse: 10.1282 - mae: 1.3457 - val_loss: 9.4863 - val_mse: 9.4863 - val_mae: 1.3760 - lr: 7.1570e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 10.2098 - mse: 10.2098 - mae: 1.3432 - val_loss: 9.9345 - val_mse: 9.9345 - val_mae: 1.3693 - lr: 7.1570e-04 - 2s/epoch - 2ms/step\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-10-11 16:33:17,726]\u001b[0m Finished trial#29 resulted in value: 11.286. Current best value is 8.234 with parameters: {'activation': 'relu', 'num_hidden_layer': 4, 'i': 10, 'learning_rate': 0.00015658492778156685}.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Score for fold 5: loss of 9.934556007385254\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 5s - loss: 12.7753 - mse: 12.7753 - mae: 1.5505 - val_loss: 16.9699 - val_mse: 16.9699 - val_mae: 1.4808 - lr: 1.0364e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 4s - loss: 11.6164 - mse: 11.6164 - mae: 1.4964 - val_loss: 16.6560 - val_mse: 16.6560 - val_mae: 1.5648 - lr: 1.0364e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 4s - loss: 11.3779 - mse: 11.3779 - mae: 1.4817 - val_loss: 16.4787 - val_mse: 16.4787 - val_mae: 1.4665 - lr: 1.0364e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 4s - loss: 11.1430 - mse: 11.1430 - mae: 1.4653 - val_loss: 16.5541 - val_mse: 16.5541 - val_mae: 1.4813 - lr: 1.0364e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 4s - loss: 10.9628 - mse: 10.9628 - mae: 1.4581 - val_loss: 16.5113 - val_mse: 16.5113 - val_mae: 1.4977 - lr: 1.0364e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 4s - loss: 10.8429 - mse: 10.8429 - mae: 1.4553 - val_loss: 16.5058 - val_mse: 16.5058 - val_mae: 1.4731 - lr: 1.0364e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 4s - loss: 10.8945 - mse: 10.8945 - mae: 1.4461 - val_loss: 16.5427 - val_mse: 16.5427 - val_mae: 1.5682 - lr: 1.0364e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 4s - loss: 10.8261 - mse: 10.8261 - mae: 1.4392 - val_loss: 16.3275 - val_mse: 16.3275 - val_mae: 1.4893 - lr: 1.0364e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 4s - loss: 10.6468 - mse: 10.6468 - mae: 1.4369 - val_loss: 16.6000 - val_mse: 16.6000 - val_mae: 1.4704 - lr: 1.0364e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 4s - loss: 10.6832 - mse: 10.6832 - mae: 1.4328 - val_loss: 16.4673 - val_mse: 16.4673 - val_mae: 1.5041 - lr: 1.0364e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 4s - loss: 10.6315 - mse: 10.6315 - mae: 1.4241 - val_loss: 16.5823 - val_mse: 16.5823 - val_mae: 1.4451 - lr: 1.0364e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 4s - loss: 10.5645 - mse: 10.5645 - mae: 1.4230 - val_loss: 16.3864 - val_mse: 16.3864 - val_mae: 1.5035 - lr: 1.0364e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 13/100\n",
            "1000/1000 - 4s - loss: 10.5144 - mse: 10.5144 - mae: 1.4209 - val_loss: 16.4791 - val_mse: 16.4791 - val_mae: 1.4450 - lr: 1.0364e-04 - 4s/epoch - 4ms/step\n",
            "Score for fold 1: loss of 16.479080200195312\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 4s - loss: 11.8357 - mse: 11.8357 - mae: 1.4330 - val_loss: 11.2577 - val_mse: 11.2577 - val_mae: 1.4902 - lr: 1.0364e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 4s - loss: 11.7939 - mse: 11.7939 - mae: 1.4305 - val_loss: 11.0320 - val_mse: 11.0320 - val_mae: 1.3840 - lr: 1.0364e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 4s - loss: 11.6454 - mse: 11.6454 - mae: 1.4271 - val_loss: 10.9621 - val_mse: 10.9621 - val_mae: 1.4189 - lr: 1.0364e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 4s - loss: 11.6628 - mse: 11.6628 - mae: 1.4203 - val_loss: 11.0014 - val_mse: 11.0014 - val_mae: 1.4193 - lr: 1.0364e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 4s - loss: 11.6533 - mse: 11.6533 - mae: 1.4181 - val_loss: 11.0519 - val_mse: 11.0519 - val_mae: 1.3859 - lr: 1.0364e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 4s - loss: 11.5654 - mse: 11.5654 - mae: 1.4182 - val_loss: 11.0267 - val_mse: 11.0267 - val_mae: 1.4028 - lr: 1.0364e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 4s - loss: 11.4300 - mse: 11.4300 - mae: 1.4082 - val_loss: 10.9618 - val_mse: 10.9618 - val_mae: 1.4396 - lr: 1.0364e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 4s - loss: 11.4533 - mse: 11.4533 - mae: 1.4094 - val_loss: 11.3976 - val_mse: 11.3976 - val_mae: 1.4903 - lr: 1.0364e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 4s - loss: 11.3601 - mse: 11.3601 - mae: 1.4079 - val_loss: 11.1333 - val_mse: 11.1333 - val_mae: 1.3889 - lr: 1.0364e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 4s - loss: 11.3187 - mse: 11.3187 - mae: 1.4004 - val_loss: 11.1704 - val_mse: 11.1704 - val_mae: 1.4151 - lr: 1.0364e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 4s - loss: 11.2462 - mse: 11.2462 - mae: 1.3993 - val_loss: 11.3669 - val_mse: 11.3669 - val_mae: 1.4201 - lr: 1.0364e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 4s - loss: 11.2821 - mse: 11.2821 - mae: 1.3958 - val_loss: 11.2606 - val_mse: 11.2606 - val_mae: 1.4198 - lr: 1.0364e-04 - 4s/epoch - 4ms/step\n",
            "Score for fold 2: loss of 11.26059627532959\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 4s - loss: 11.6722 - mse: 11.6722 - mae: 1.4006 - val_loss: 9.3944 - val_mse: 9.3944 - val_mae: 1.4070 - lr: 1.0364e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 4s - loss: 11.6506 - mse: 11.6506 - mae: 1.3961 - val_loss: 9.3948 - val_mse: 9.3948 - val_mae: 1.3703 - lr: 1.0364e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 4s - loss: 11.6095 - mse: 11.6095 - mae: 1.3908 - val_loss: 9.4968 - val_mse: 9.4968 - val_mae: 1.4378 - lr: 1.0364e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 4s - loss: 11.5633 - mse: 11.5633 - mae: 1.3904 - val_loss: 9.4892 - val_mse: 9.4892 - val_mae: 1.4901 - lr: 1.0364e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 4s - loss: 11.4812 - mse: 11.4812 - mae: 1.3842 - val_loss: 9.5682 - val_mse: 9.5682 - val_mae: 1.4373 - lr: 1.0364e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 4s - loss: 11.3844 - mse: 11.3844 - mae: 1.3858 - val_loss: 9.5102 - val_mse: 9.5102 - val_mae: 1.4170 - lr: 1.0364e-04 - 4s/epoch - 4ms/step\n",
            "Score for fold 3: loss of 9.510184288024902\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 4s - loss: 11.9418 - mse: 11.9418 - mae: 1.3976 - val_loss: 7.2787 - val_mse: 7.2787 - val_mae: 1.3146 - lr: 1.0364e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 4s - loss: 11.8377 - mse: 11.8377 - mae: 1.3938 - val_loss: 7.2459 - val_mse: 7.2459 - val_mae: 1.3117 - lr: 1.0364e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 4s - loss: 11.7792 - mse: 11.7792 - mae: 1.3870 - val_loss: 7.1728 - val_mse: 7.1728 - val_mae: 1.3526 - lr: 1.0364e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 4s - loss: 11.8491 - mse: 11.8491 - mae: 1.3850 - val_loss: 7.2329 - val_mse: 7.2329 - val_mae: 1.3555 - lr: 1.0364e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 4s - loss: 11.6679 - mse: 11.6679 - mae: 1.3851 - val_loss: 7.7321 - val_mse: 7.7321 - val_mae: 1.3544 - lr: 1.0364e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 4s - loss: 11.6098 - mse: 11.6098 - mae: 1.3772 - val_loss: 7.3893 - val_mse: 7.3893 - val_mae: 1.3579 - lr: 1.0364e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 5s - loss: 11.5658 - mse: 11.5658 - mae: 1.3742 - val_loss: 7.3583 - val_mse: 7.3583 - val_mae: 1.4347 - lr: 1.0364e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 4s - loss: 11.5397 - mse: 11.5397 - mae: 1.3688 - val_loss: 7.6881 - val_mse: 7.6881 - val_mae: 1.3853 - lr: 1.0364e-04 - 4s/epoch - 4ms/step\n",
            "Score for fold 4: loss of 7.688099384307861\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 4s - loss: 10.3826 - mse: 10.3826 - mae: 1.3780 - val_loss: 11.8351 - val_mse: 11.8351 - val_mae: 1.3420 - lr: 1.0364e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 5s - loss: 10.4542 - mse: 10.4542 - mae: 1.3708 - val_loss: 11.8822 - val_mse: 11.8822 - val_mae: 1.3521 - lr: 1.0364e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 4s - loss: 10.4136 - mse: 10.4136 - mae: 1.3731 - val_loss: 11.8918 - val_mse: 11.8918 - val_mae: 1.3479 - lr: 1.0364e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 4s - loss: 10.3329 - mse: 10.3329 - mae: 1.3653 - val_loss: 11.8580 - val_mse: 11.8580 - val_mae: 1.4057 - lr: 1.0364e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 4s - loss: 10.3225 - mse: 10.3225 - mae: 1.3636 - val_loss: 11.8788 - val_mse: 11.8788 - val_mae: 1.4010 - lr: 1.0364e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 4s - loss: 10.1649 - mse: 10.1649 - mae: 1.3593 - val_loss: 12.2005 - val_mse: 12.2005 - val_mae: 1.4083 - lr: 1.0364e-04 - 4s/epoch - 4ms/step\n",
            "Score for fold 5: loss of 12.200542449951172\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-10-11 16:36:40,528]\u001b[0m Finished trial#30 resulted in value: 11.428. Current best value is 8.234 with parameters: {'activation': 'relu', 'num_hidden_layer': 4, 'i': 10, 'learning_rate': 0.00015658492778156685}.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 11s - loss: 12.0590 - mse: 12.0590 - mae: 1.5544 - val_loss: 16.5847 - val_mse: 16.5847 - val_mae: 1.3749 - lr: 2.2221e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 11s - loss: 11.2543 - mse: 11.2543 - mae: 1.4910 - val_loss: 16.3711 - val_mse: 16.3711 - val_mae: 1.4084 - lr: 2.2221e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 11s - loss: 11.0339 - mse: 11.0339 - mae: 1.4841 - val_loss: 16.8389 - val_mse: 16.8389 - val_mae: 1.4685 - lr: 2.2221e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 11s - loss: 10.9812 - mse: 10.9812 - mae: 1.4688 - val_loss: 16.5821 - val_mse: 16.5821 - val_mae: 1.4238 - lr: 2.2221e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 11s - loss: 10.6145 - mse: 10.6145 - mae: 1.4565 - val_loss: 16.8710 - val_mse: 16.8710 - val_mae: 1.4106 - lr: 2.2221e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 11s - loss: 10.6440 - mse: 10.6440 - mae: 1.4480 - val_loss: 17.1118 - val_mse: 17.1118 - val_mae: 1.4408 - lr: 2.2221e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 11s - loss: 10.5044 - mse: 10.5044 - mae: 1.4421 - val_loss: 16.4506 - val_mse: 16.4506 - val_mae: 1.4295 - lr: 2.2221e-04 - 11s/epoch - 11ms/step\n",
            "Score for fold 1: loss of 16.450597763061523\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 11s - loss: 11.1868 - mse: 11.1868 - mae: 1.4345 - val_loss: 12.4279 - val_mse: 12.4279 - val_mae: 1.4070 - lr: 2.2221e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 11s - loss: 11.1811 - mse: 11.1811 - mae: 1.4250 - val_loss: 14.1198 - val_mse: 14.1198 - val_mae: 1.4805 - lr: 2.2221e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 11s - loss: 10.6718 - mse: 10.6718 - mae: 1.4098 - val_loss: 12.8019 - val_mse: 12.8019 - val_mae: 1.4777 - lr: 2.2221e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 11s - loss: 10.5744 - mse: 10.5744 - mae: 1.3959 - val_loss: 13.3886 - val_mse: 13.3886 - val_mae: 1.4811 - lr: 2.2221e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 11s - loss: 10.4245 - mse: 10.4245 - mae: 1.3837 - val_loss: 13.9579 - val_mse: 13.9579 - val_mae: 1.4883 - lr: 2.2221e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 11s - loss: 10.1553 - mse: 10.1553 - mae: 1.3638 - val_loss: 12.7806 - val_mse: 12.7806 - val_mae: 1.5342 - lr: 2.2221e-04 - 11s/epoch - 11ms/step\n",
            "Score for fold 2: loss of 12.780604362487793\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 11s - loss: 11.6902 - mse: 11.6902 - mae: 1.3941 - val_loss: 6.5164 - val_mse: 6.5164 - val_mae: 1.2735 - lr: 2.2221e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 11s - loss: 11.1441 - mse: 11.1441 - mae: 1.3644 - val_loss: 6.3391 - val_mse: 6.3391 - val_mae: 1.3270 - lr: 2.2221e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 11s - loss: 12.2596 - mse: 12.2596 - mae: 1.3562 - val_loss: 7.1281 - val_mse: 7.1281 - val_mae: 1.3613 - lr: 2.2221e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 11s - loss: 10.6509 - mse: 10.6509 - mae: 1.3321 - val_loss: 6.5055 - val_mse: 6.5055 - val_mae: 1.2947 - lr: 2.2221e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 11s - loss: 10.2882 - mse: 10.2882 - mae: 1.3136 - val_loss: 6.9587 - val_mse: 6.9587 - val_mae: 1.3404 - lr: 2.2221e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 11s - loss: 10.1804 - mse: 10.1804 - mae: 1.2930 - val_loss: 6.9115 - val_mse: 6.9115 - val_mae: 1.3777 - lr: 2.2221e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 11s - loss: 9.9247 - mse: 9.9247 - mae: 1.2840 - val_loss: 7.0454 - val_mse: 7.0454 - val_mae: 1.3234 - lr: 2.2221e-04 - 11s/epoch - 11ms/step\n",
            "Score for fold 3: loss of 7.045388698577881\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 11s - loss: 10.0552 - mse: 10.0552 - mae: 1.3144 - val_loss: 8.5999 - val_mse: 8.5999 - val_mae: 1.2301 - lr: 2.2221e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 11s - loss: 9.9687 - mse: 9.9687 - mae: 1.2939 - val_loss: 5.7387 - val_mse: 5.7387 - val_mae: 1.3482 - lr: 2.2221e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 10s - loss: 9.2838 - mse: 9.2838 - mae: 1.2699 - val_loss: 6.1374 - val_mse: 6.1374 - val_mae: 1.3411 - lr: 2.2221e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 11s - loss: 8.9854 - mse: 8.9854 - mae: 1.2441 - val_loss: 5.8564 - val_mse: 5.8564 - val_mae: 1.2759 - lr: 2.2221e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 11s - loss: 8.5933 - mse: 8.5933 - mae: 1.2260 - val_loss: 6.0857 - val_mse: 6.0857 - val_mae: 1.2588 - lr: 2.2221e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 11s - loss: 8.3670 - mse: 8.3670 - mae: 1.2173 - val_loss: 6.8438 - val_mse: 6.8438 - val_mae: 1.4038 - lr: 2.2221e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 11s - loss: 7.9500 - mse: 7.9500 - mae: 1.1948 - val_loss: 6.3517 - val_mse: 6.3517 - val_mae: 1.2700 - lr: 2.2221e-04 - 11s/epoch - 11ms/step\n",
            "Score for fold 4: loss of 6.351686477661133\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 11s - loss: 7.9598 - mse: 7.9598 - mae: 1.2226 - val_loss: 7.5350 - val_mse: 7.5350 - val_mae: 1.0935 - lr: 2.2221e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 10s - loss: 7.5638 - mse: 7.5638 - mae: 1.1998 - val_loss: 7.5347 - val_mse: 7.5347 - val_mae: 1.2241 - lr: 2.2221e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 11s - loss: 7.1793 - mse: 7.1793 - mae: 1.1737 - val_loss: 7.3521 - val_mse: 7.3521 - val_mae: 1.2593 - lr: 2.2221e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 11s - loss: 7.9436 - mse: 7.9436 - mae: 1.1611 - val_loss: 7.6509 - val_mse: 7.6509 - val_mae: 1.2097 - lr: 2.2221e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 11s - loss: 6.7033 - mse: 6.7033 - mae: 1.1450 - val_loss: 7.9478 - val_mse: 7.9478 - val_mae: 1.3102 - lr: 2.2221e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 11s - loss: 6.6317 - mse: 6.6317 - mae: 1.1290 - val_loss: 7.9217 - val_mse: 7.9217 - val_mae: 1.2336 - lr: 2.2221e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 11s - loss: 6.2361 - mse: 6.2361 - mae: 1.1142 - val_loss: 8.2688 - val_mse: 8.2688 - val_mae: 1.2674 - lr: 2.2221e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 11s - loss: 6.0660 - mse: 6.0660 - mae: 1.1012 - val_loss: 8.0460 - val_mse: 8.0460 - val_mae: 1.1986 - lr: 2.2221e-04 - 11s/epoch - 11ms/step\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-10-11 16:43:07,188]\u001b[0m Finished trial#31 resulted in value: 10.136. Current best value is 8.234 with parameters: {'activation': 'relu', 'num_hidden_layer': 4, 'i': 10, 'learning_rate': 0.00015658492778156685}.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Score for fold 5: loss of 8.046041488647461\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 11s - loss: 13.0040 - mse: 13.0040 - mae: 1.5344 - val_loss: 13.0584 - val_mse: 13.0584 - val_mae: 1.5394 - lr: 1.9012e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 11s - loss: 12.0885 - mse: 12.0885 - mae: 1.4742 - val_loss: 12.8488 - val_mse: 12.8488 - val_mae: 1.4995 - lr: 1.9012e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 11s - loss: 12.0427 - mse: 12.0427 - mae: 1.4634 - val_loss: 13.0390 - val_mse: 13.0390 - val_mae: 1.4427 - lr: 1.9012e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 11s - loss: 12.0184 - mse: 12.0184 - mae: 1.4574 - val_loss: 13.4253 - val_mse: 13.4253 - val_mae: 1.5802 - lr: 1.9012e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 11s - loss: 11.6658 - mse: 11.6658 - mae: 1.4410 - val_loss: 12.7907 - val_mse: 12.7907 - val_mae: 1.5226 - lr: 1.9012e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 11s - loss: 11.4018 - mse: 11.4018 - mae: 1.4326 - val_loss: 13.1996 - val_mse: 13.1996 - val_mae: 1.5206 - lr: 1.9012e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 11s - loss: 11.4951 - mse: 11.4951 - mae: 1.4236 - val_loss: 12.9769 - val_mse: 12.9769 - val_mae: 1.5803 - lr: 1.9012e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 11s - loss: 10.9409 - mse: 10.9409 - mae: 1.4118 - val_loss: 13.0901 - val_mse: 13.0901 - val_mae: 1.5434 - lr: 1.9012e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 11s - loss: 11.0653 - mse: 11.0653 - mae: 1.4012 - val_loss: 13.4691 - val_mse: 13.4691 - val_mae: 1.4700 - lr: 1.9012e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 11s - loss: 10.7395 - mse: 10.7395 - mae: 1.3897 - val_loss: 12.9754 - val_mse: 12.9754 - val_mae: 1.6248 - lr: 1.9012e-04 - 11s/epoch - 11ms/step\n",
            "Score for fold 1: loss of 12.975350379943848\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 11s - loss: 12.0680 - mse: 12.0680 - mae: 1.4132 - val_loss: 6.7769 - val_mse: 6.7769 - val_mae: 1.3732 - lr: 1.9012e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 11s - loss: 11.9158 - mse: 11.9158 - mae: 1.4001 - val_loss: 6.6912 - val_mse: 6.6912 - val_mae: 1.4140 - lr: 1.9012e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 10s - loss: 11.3861 - mse: 11.3861 - mae: 1.3807 - val_loss: 7.1612 - val_mse: 7.1612 - val_mae: 1.3889 - lr: 1.9012e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 10s - loss: 11.0794 - mse: 11.0794 - mae: 1.3594 - val_loss: 7.2722 - val_mse: 7.2722 - val_mae: 1.3969 - lr: 1.9012e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 10s - loss: 11.2104 - mse: 11.2104 - mae: 1.3518 - val_loss: 7.2566 - val_mse: 7.2566 - val_mae: 1.4049 - lr: 1.9012e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 11s - loss: 10.4596 - mse: 10.4596 - mae: 1.3236 - val_loss: 7.5330 - val_mse: 7.5330 - val_mae: 1.3564 - lr: 1.9012e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 11s - loss: 10.2640 - mse: 10.2640 - mae: 1.3020 - val_loss: 7.6699 - val_mse: 7.6699 - val_mae: 1.4999 - lr: 1.9012e-04 - 11s/epoch - 11ms/step\n",
            "Score for fold 2: loss of 7.669895172119141\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 11s - loss: 10.4187 - mse: 10.4187 - mae: 1.3443 - val_loss: 6.2199 - val_mse: 6.2199 - val_mae: 1.2954 - lr: 1.9012e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 11s - loss: 9.9660 - mse: 9.9660 - mae: 1.3177 - val_loss: 7.0443 - val_mse: 7.0443 - val_mae: 1.3657 - lr: 1.9012e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 11s - loss: 9.8301 - mse: 9.8301 - mae: 1.2985 - val_loss: 6.2690 - val_mse: 6.2690 - val_mae: 1.2824 - lr: 1.9012e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 11s - loss: 9.4554 - mse: 9.4554 - mae: 1.2811 - val_loss: 6.9404 - val_mse: 6.9404 - val_mae: 1.3707 - lr: 1.9012e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 11s - loss: 8.7917 - mse: 8.7917 - mae: 1.2522 - val_loss: 6.8924 - val_mse: 6.8924 - val_mae: 1.4327 - lr: 1.9012e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 11s - loss: 8.6956 - mse: 8.6956 - mae: 1.2418 - val_loss: 6.9939 - val_mse: 6.9939 - val_mae: 1.2958 - lr: 1.9012e-04 - 11s/epoch - 11ms/step\n",
            "Score for fold 3: loss of 6.993863105773926\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 11s - loss: 9.1082 - mse: 9.1082 - mae: 1.2652 - val_loss: 7.3398 - val_mse: 7.3398 - val_mae: 1.2898 - lr: 1.9012e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 11s - loss: 8.2862 - mse: 8.2862 - mae: 1.2418 - val_loss: 7.5887 - val_mse: 7.5887 - val_mae: 1.2697 - lr: 1.9012e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 10s - loss: 8.2017 - mse: 8.2017 - mae: 1.2205 - val_loss: 8.0326 - val_mse: 8.0326 - val_mae: 1.2416 - lr: 1.9012e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 11s - loss: 7.3156 - mse: 7.3156 - mae: 1.1974 - val_loss: 7.5405 - val_mse: 7.5405 - val_mae: 1.2369 - lr: 1.9012e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 11s - loss: 7.2912 - mse: 7.2912 - mae: 1.1728 - val_loss: 7.6179 - val_mse: 7.6179 - val_mae: 1.2457 - lr: 1.9012e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 11s - loss: 6.8404 - mse: 6.8404 - mae: 1.1591 - val_loss: 8.0172 - val_mse: 8.0172 - val_mae: 1.4357 - lr: 1.9012e-04 - 11s/epoch - 11ms/step\n",
            "Score for fold 4: loss of 8.017182350158691\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 11s - loss: 5.9872 - mse: 5.9872 - mae: 1.1958 - val_loss: 10.4303 - val_mse: 10.4303 - val_mae: 1.1087 - lr: 1.9012e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 11s - loss: 5.7346 - mse: 5.7346 - mae: 1.1713 - val_loss: 11.4715 - val_mse: 11.4715 - val_mae: 1.1165 - lr: 1.9012e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 10s - loss: 5.3979 - mse: 5.3979 - mae: 1.1481 - val_loss: 12.5510 - val_mse: 12.5510 - val_mae: 1.2727 - lr: 1.9012e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 11s - loss: 5.4404 - mse: 5.4404 - mae: 1.1300 - val_loss: 11.6360 - val_mse: 11.6360 - val_mae: 1.2042 - lr: 1.9012e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 11s - loss: 5.0549 - mse: 5.0549 - mae: 1.1157 - val_loss: 12.0439 - val_mse: 12.0439 - val_mae: 1.1460 - lr: 1.9012e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 11s - loss: 4.9187 - mse: 4.9187 - mae: 1.1017 - val_loss: 12.5141 - val_mse: 12.5141 - val_mae: 1.1982 - lr: 1.9012e-04 - 11s/epoch - 11ms/step\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-10-11 16:49:33,772]\u001b[0m Finished trial#32 resulted in value: 9.633999999999999. Current best value is 8.234 with parameters: {'activation': 'relu', 'num_hidden_layer': 4, 'i': 10, 'learning_rate': 0.00015658492778156685}.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Score for fold 5: loss of 12.514055252075195\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 12s - loss: 13.6950 - mse: 13.6950 - mae: 1.5431 - val_loss: 11.1332 - val_mse: 11.1332 - val_mae: 1.4336 - lr: 1.3729e-04 - 12s/epoch - 12ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 11s - loss: 12.8493 - mse: 12.8493 - mae: 1.4888 - val_loss: 10.4793 - val_mse: 10.4793 - val_mae: 1.4813 - lr: 1.3729e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 11s - loss: 12.7752 - mse: 12.7752 - mae: 1.4757 - val_loss: 10.4549 - val_mse: 10.4549 - val_mae: 1.4932 - lr: 1.3729e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 11s - loss: 12.2992 - mse: 12.2992 - mae: 1.4622 - val_loss: 10.4749 - val_mse: 10.4749 - val_mae: 1.4940 - lr: 1.3729e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 11s - loss: 12.3583 - mse: 12.3583 - mae: 1.4580 - val_loss: 10.4427 - val_mse: 10.4427 - val_mae: 1.4930 - lr: 1.3729e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 11s - loss: 12.1649 - mse: 12.1649 - mae: 1.4431 - val_loss: 10.2837 - val_mse: 10.2837 - val_mae: 1.5422 - lr: 1.3729e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 11s - loss: 11.9740 - mse: 11.9740 - mae: 1.4376 - val_loss: 10.3757 - val_mse: 10.3757 - val_mae: 1.4545 - lr: 1.3729e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 11s - loss: 11.9017 - mse: 11.9017 - mae: 1.4219 - val_loss: 10.2306 - val_mse: 10.2306 - val_mae: 1.4446 - lr: 1.3729e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 11s - loss: 11.6462 - mse: 11.6462 - mae: 1.4133 - val_loss: 10.5023 - val_mse: 10.5023 - val_mae: 1.4270 - lr: 1.3729e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 11s - loss: 11.2537 - mse: 11.2537 - mae: 1.3985 - val_loss: 10.3578 - val_mse: 10.3578 - val_mae: 1.4539 - lr: 1.3729e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 11s - loss: 11.2290 - mse: 11.2290 - mae: 1.3824 - val_loss: 10.7316 - val_mse: 10.7316 - val_mae: 1.4514 - lr: 1.3729e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 11s - loss: 10.7612 - mse: 10.7612 - mae: 1.3661 - val_loss: 10.7637 - val_mse: 10.7637 - val_mae: 1.4637 - lr: 1.3729e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 13/100\n",
            "1000/1000 - 11s - loss: 10.7147 - mse: 10.7147 - mae: 1.3502 - val_loss: 10.4384 - val_mse: 10.4384 - val_mae: 1.4934 - lr: 1.3729e-04 - 11s/epoch - 11ms/step\n",
            "Score for fold 1: loss of 10.4383544921875\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 11s - loss: 10.7870 - mse: 10.7870 - mae: 1.3706 - val_loss: 9.7225 - val_mse: 9.7225 - val_mae: 1.3330 - lr: 1.3729e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 11s - loss: 10.3778 - mse: 10.3778 - mae: 1.3557 - val_loss: 9.0402 - val_mse: 9.0402 - val_mae: 1.3529 - lr: 1.3729e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 11s - loss: 10.3332 - mse: 10.3332 - mae: 1.3352 - val_loss: 9.5727 - val_mse: 9.5727 - val_mae: 1.4014 - lr: 1.3729e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 10s - loss: 9.7859 - mse: 9.7859 - mae: 1.3129 - val_loss: 9.5459 - val_mse: 9.5459 - val_mae: 1.3641 - lr: 1.3729e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 11s - loss: 9.5081 - mse: 9.5081 - mae: 1.2910 - val_loss: 9.2744 - val_mse: 9.2744 - val_mae: 1.3547 - lr: 1.3729e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 11s - loss: 9.1846 - mse: 9.1846 - mae: 1.2709 - val_loss: 10.5657 - val_mse: 10.5657 - val_mae: 1.3710 - lr: 1.3729e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 11s - loss: 9.0240 - mse: 9.0240 - mae: 1.2510 - val_loss: 9.4760 - val_mse: 9.4760 - val_mae: 1.3251 - lr: 1.3729e-04 - 11s/epoch - 11ms/step\n",
            "Score for fold 2: loss of 9.475973129272461\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 11s - loss: 8.9639 - mse: 8.9639 - mae: 1.2808 - val_loss: 8.9352 - val_mse: 8.9352 - val_mae: 1.2481 - lr: 1.3729e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 10s - loss: 8.5483 - mse: 8.5483 - mae: 1.2542 - val_loss: 9.8435 - val_mse: 9.8435 - val_mae: 1.2486 - lr: 1.3729e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 11s - loss: 8.4661 - mse: 8.4661 - mae: 1.2329 - val_loss: 8.9982 - val_mse: 8.9982 - val_mae: 1.2868 - lr: 1.3729e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 10s - loss: 7.8286 - mse: 7.8286 - mae: 1.2070 - val_loss: 9.3339 - val_mse: 9.3339 - val_mae: 1.2824 - lr: 1.3729e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 11s - loss: 7.4690 - mse: 7.4690 - mae: 1.1855 - val_loss: 9.4424 - val_mse: 9.4424 - val_mae: 1.4377 - lr: 1.3729e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 11s - loss: 7.3442 - mse: 7.3442 - mae: 1.1645 - val_loss: 9.7359 - val_mse: 9.7359 - val_mae: 1.2557 - lr: 1.3729e-04 - 11s/epoch - 11ms/step\n",
            "Score for fold 3: loss of 9.735869407653809\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 11s - loss: 7.0472 - mse: 7.0472 - mae: 1.2174 - val_loss: 11.7522 - val_mse: 11.7522 - val_mae: 1.1251 - lr: 1.3729e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 11s - loss: 6.6154 - mse: 6.6154 - mae: 1.1863 - val_loss: 11.7461 - val_mse: 11.7461 - val_mae: 1.1839 - lr: 1.3729e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 11s - loss: 5.9345 - mse: 5.9345 - mae: 1.1569 - val_loss: 12.1092 - val_mse: 12.1092 - val_mae: 1.2311 - lr: 1.3729e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 11s - loss: 5.8206 - mse: 5.8206 - mae: 1.1353 - val_loss: 12.0782 - val_mse: 12.0782 - val_mae: 1.1915 - lr: 1.3729e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 11s - loss: 5.5137 - mse: 5.5137 - mae: 1.1150 - val_loss: 12.6226 - val_mse: 12.6226 - val_mae: 1.2845 - lr: 1.3729e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 11s - loss: 5.2379 - mse: 5.2379 - mae: 1.0986 - val_loss: 12.5995 - val_mse: 12.5995 - val_mae: 1.2190 - lr: 1.3729e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 11s - loss: 5.1815 - mse: 5.1815 - mae: 1.0859 - val_loss: 12.8933 - val_mse: 12.8933 - val_mae: 1.2383 - lr: 1.3729e-04 - 11s/epoch - 11ms/step\n",
            "Score for fold 4: loss of 12.893326759338379\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 11s - loss: 7.4685 - mse: 7.4685 - mae: 1.1261 - val_loss: 3.3091 - val_mse: 3.3091 - val_mae: 1.0459 - lr: 1.3729e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 11s - loss: 7.1851 - mse: 7.1851 - mae: 1.0999 - val_loss: 4.0580 - val_mse: 4.0580 - val_mae: 1.0845 - lr: 1.3729e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 11s - loss: 6.8711 - mse: 6.8711 - mae: 1.0725 - val_loss: 3.3520 - val_mse: 3.3520 - val_mae: 1.0289 - lr: 1.3729e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 11s - loss: 6.6440 - mse: 6.6440 - mae: 1.0585 - val_loss: 4.3809 - val_mse: 4.3809 - val_mae: 1.1292 - lr: 1.3729e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 11s - loss: 6.3790 - mse: 6.3790 - mae: 1.0431 - val_loss: 3.9529 - val_mse: 3.9529 - val_mae: 1.1718 - lr: 1.3729e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 11s - loss: 6.0423 - mse: 6.0423 - mae: 1.0190 - val_loss: 3.6673 - val_mse: 3.6673 - val_mae: 1.0768 - lr: 1.3729e-04 - 11s/epoch - 11ms/step\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-10-11 16:56:46,319]\u001b[0m Finished trial#33 resulted in value: 9.244000000000002. Current best value is 8.234 with parameters: {'activation': 'relu', 'num_hidden_layer': 4, 'i': 10, 'learning_rate': 0.00015658492778156685}.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Score for fold 5: loss of 3.6673483848571777\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 12s - loss: 10.9373 - mse: 10.9373 - mae: 1.5330 - val_loss: 21.6780 - val_mse: 21.6780 - val_mae: 1.5087 - lr: 1.3917e-04 - 12s/epoch - 12ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 11s - loss: 10.2395 - mse: 10.2395 - mae: 1.4749 - val_loss: 21.4804 - val_mse: 21.4804 - val_mae: 1.4960 - lr: 1.3917e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 11s - loss: 10.0237 - mse: 10.0237 - mae: 1.4608 - val_loss: 21.1921 - val_mse: 21.1921 - val_mae: 1.4970 - lr: 1.3917e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 11s - loss: 9.6676 - mse: 9.6676 - mae: 1.4530 - val_loss: 21.4964 - val_mse: 21.4964 - val_mae: 1.4732 - lr: 1.3917e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 11s - loss: 9.5510 - mse: 9.5510 - mae: 1.4384 - val_loss: 21.5502 - val_mse: 21.5502 - val_mae: 1.5021 - lr: 1.3917e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 11s - loss: 9.3597 - mse: 9.3597 - mae: 1.4298 - val_loss: 21.9680 - val_mse: 21.9680 - val_mae: 1.4391 - lr: 1.3917e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 11s - loss: 9.1426 - mse: 9.1426 - mae: 1.4158 - val_loss: 21.3777 - val_mse: 21.3777 - val_mae: 1.5335 - lr: 1.3917e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 11s - loss: 9.0566 - mse: 9.0566 - mae: 1.4092 - val_loss: 21.2702 - val_mse: 21.2702 - val_mae: 1.6044 - lr: 1.3917e-04 - 11s/epoch - 11ms/step\n",
            "Score for fold 1: loss of 21.27020263671875\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 11s - loss: 12.1692 - mse: 12.1692 - mae: 1.4259 - val_loss: 8.1905 - val_mse: 8.1905 - val_mae: 1.4489 - lr: 1.3917e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 11s - loss: 11.9226 - mse: 11.9226 - mae: 1.4109 - val_loss: 8.0413 - val_mse: 8.0413 - val_mae: 1.4592 - lr: 1.3917e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 11s - loss: 11.6486 - mse: 11.6486 - mae: 1.3961 - val_loss: 8.4166 - val_mse: 8.4166 - val_mae: 1.4568 - lr: 1.3917e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 11s - loss: 11.4234 - mse: 11.4234 - mae: 1.3815 - val_loss: 8.0600 - val_mse: 8.0600 - val_mae: 1.4020 - lr: 1.3917e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 11s - loss: 11.1706 - mse: 11.1706 - mae: 1.3607 - val_loss: 8.2244 - val_mse: 8.2244 - val_mae: 1.4339 - lr: 1.3917e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 11s - loss: 10.7745 - mse: 10.7745 - mae: 1.3425 - val_loss: 8.0741 - val_mse: 8.0741 - val_mae: 1.4922 - lr: 1.3917e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 11s - loss: 10.4806 - mse: 10.4806 - mae: 1.3190 - val_loss: 9.5203 - val_mse: 9.5203 - val_mae: 1.3854 - lr: 1.3917e-04 - 11s/epoch - 11ms/step\n",
            "Score for fold 2: loss of 9.520331382751465\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 11s - loss: 9.9708 - mse: 9.9708 - mae: 1.3402 - val_loss: 10.1359 - val_mse: 10.1359 - val_mae: 1.3426 - lr: 1.3917e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 11s - loss: 9.6572 - mse: 9.6572 - mae: 1.3180 - val_loss: 9.5535 - val_mse: 9.5535 - val_mae: 1.3690 - lr: 1.3917e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 11s - loss: 9.5944 - mse: 9.5944 - mae: 1.2958 - val_loss: 10.2416 - val_mse: 10.2416 - val_mae: 1.3977 - lr: 1.3917e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 11s - loss: 9.3623 - mse: 9.3623 - mae: 1.2769 - val_loss: 10.7969 - val_mse: 10.7969 - val_mae: 1.3805 - lr: 1.3917e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 11s - loss: 8.8278 - mse: 8.8278 - mae: 1.2554 - val_loss: 10.3426 - val_mse: 10.3426 - val_mae: 1.3331 - lr: 1.3917e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 11s - loss: 8.5741 - mse: 8.5741 - mae: 1.2332 - val_loss: 10.7833 - val_mse: 10.7833 - val_mae: 1.3473 - lr: 1.3917e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 11s - loss: 8.1264 - mse: 8.1264 - mae: 1.2124 - val_loss: 10.8859 - val_mse: 10.8859 - val_mae: 1.3920 - lr: 1.3917e-04 - 11s/epoch - 11ms/step\n",
            "Score for fold 3: loss of 10.885862350463867\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 11s - loss: 9.8599 - mse: 9.8599 - mae: 1.2709 - val_loss: 4.6110 - val_mse: 4.6110 - val_mae: 1.1679 - lr: 1.3917e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 11s - loss: 9.2611 - mse: 9.2611 - mae: 1.2433 - val_loss: 4.3039 - val_mse: 4.3039 - val_mae: 1.2221 - lr: 1.3917e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 11s - loss: 8.9470 - mse: 8.9470 - mae: 1.2251 - val_loss: 4.4762 - val_mse: 4.4762 - val_mae: 1.1735 - lr: 1.3917e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 10s - loss: 8.6838 - mse: 8.6838 - mae: 1.2068 - val_loss: 5.2383 - val_mse: 5.2383 - val_mae: 1.1763 - lr: 1.3917e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 11s - loss: 8.2565 - mse: 8.2565 - mae: 1.1763 - val_loss: 4.6774 - val_mse: 4.6774 - val_mae: 1.1877 - lr: 1.3917e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 11s - loss: 8.4002 - mse: 8.4002 - mae: 1.1565 - val_loss: 4.8796 - val_mse: 4.8796 - val_mae: 1.2924 - lr: 1.3917e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 11s - loss: 7.9166 - mse: 7.9166 - mae: 1.1425 - val_loss: 4.8146 - val_mse: 4.8146 - val_mae: 1.2728 - lr: 1.3917e-04 - 11s/epoch - 11ms/step\n",
            "Score for fold 4: loss of 4.814617156982422\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 11s - loss: 7.8661 - mse: 7.8661 - mae: 1.1822 - val_loss: 4.4217 - val_mse: 4.4217 - val_mae: 1.1495 - lr: 1.3917e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 11s - loss: 7.2559 - mse: 7.2559 - mae: 1.1547 - val_loss: 4.9044 - val_mse: 4.9044 - val_mae: 1.1250 - lr: 1.3917e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 11s - loss: 7.2921 - mse: 7.2921 - mae: 1.1291 - val_loss: 4.7976 - val_mse: 4.7976 - val_mae: 1.1856 - lr: 1.3917e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 11s - loss: 7.0697 - mse: 7.0697 - mae: 1.1074 - val_loss: 5.5690 - val_mse: 5.5690 - val_mae: 1.1327 - lr: 1.3917e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 11s - loss: 6.7427 - mse: 6.7427 - mae: 1.0916 - val_loss: 4.9297 - val_mse: 4.9297 - val_mae: 1.1986 - lr: 1.3917e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 11s - loss: 6.3794 - mse: 6.3794 - mae: 1.0775 - val_loss: 5.1856 - val_mse: 5.1856 - val_mae: 1.2567 - lr: 1.3917e-04 - 11s/epoch - 11ms/step\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-10-11 17:03:18,007]\u001b[0m Finished trial#34 resulted in value: 10.336. Current best value is 8.234 with parameters: {'activation': 'relu', 'num_hidden_layer': 4, 'i': 10, 'learning_rate': 0.00015658492778156685}.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Score for fold 5: loss of 5.185617446899414\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 11s - loss: 13.2824 - mse: 13.2824 - mae: 1.6552 - val_loss: 17.4004 - val_mse: 17.4004 - val_mae: 1.6813 - lr: 3.0797e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 11s - loss: 12.8156 - mse: 12.8156 - mae: 1.5778 - val_loss: 17.1940 - val_mse: 17.1940 - val_mae: 1.6395 - lr: 3.0797e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 10s - loss: 12.4520 - mse: 12.4520 - mae: 1.5693 - val_loss: 17.6644 - val_mse: 17.6644 - val_mae: 1.5529 - lr: 3.0797e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 10s - loss: 12.4035 - mse: 12.4035 - mae: 1.5649 - val_loss: 17.2229 - val_mse: 17.2229 - val_mae: 1.5409 - lr: 3.0797e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 10s - loss: 12.3161 - mse: 12.3161 - mae: 1.5560 - val_loss: 17.1259 - val_mse: 17.1259 - val_mae: 1.5705 - lr: 3.0797e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 10s - loss: 12.3094 - mse: 12.3094 - mae: 1.5623 - val_loss: 17.0951 - val_mse: 17.0951 - val_mae: 1.5600 - lr: 3.0797e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 10s - loss: 12.3422 - mse: 12.3422 - mae: 1.5587 - val_loss: 17.1841 - val_mse: 17.1841 - val_mae: 1.5306 - lr: 3.0797e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 10s - loss: 12.3502 - mse: 12.3502 - mae: 1.5632 - val_loss: 17.6459 - val_mse: 17.6459 - val_mae: 1.5194 - lr: 3.0797e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 10s - loss: 12.3371 - mse: 12.3371 - mae: 1.5567 - val_loss: 17.0804 - val_mse: 17.0804 - val_mae: 1.6016 - lr: 3.0797e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 10s - loss: 12.3835 - mse: 12.3835 - mae: 1.5602 - val_loss: 17.7081 - val_mse: 17.7081 - val_mae: 1.6890 - lr: 3.0797e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 10s - loss: 12.4926 - mse: 12.4926 - mae: 1.5602 - val_loss: 16.9204 - val_mse: 16.9204 - val_mae: 1.5568 - lr: 3.0797e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 10s - loss: 12.4308 - mse: 12.4308 - mae: 1.5547 - val_loss: 17.2396 - val_mse: 17.2396 - val_mae: 1.5888 - lr: 3.0797e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 13/100\n",
            "1000/1000 - 10s - loss: 12.2733 - mse: 12.2733 - mae: 1.5550 - val_loss: 17.4041 - val_mse: 17.4041 - val_mae: 1.4996 - lr: 3.0797e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 14/100\n",
            "1000/1000 - 10s - loss: 12.3900 - mse: 12.3900 - mae: 1.5536 - val_loss: 17.4537 - val_mse: 17.4537 - val_mae: 1.5002 - lr: 3.0797e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 15/100\n",
            "1000/1000 - 10s - loss: 12.3191 - mse: 12.3191 - mae: 1.5520 - val_loss: 16.9963 - val_mse: 16.9963 - val_mae: 1.5536 - lr: 3.0797e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 16/100\n",
            "1000/1000 - 10s - loss: 12.3931 - mse: 12.3931 - mae: 1.5577 - val_loss: 16.9932 - val_mse: 16.9932 - val_mae: 1.5805 - lr: 3.0797e-04 - 10s/epoch - 10ms/step\n",
            "Score for fold 1: loss of 16.99325180053711\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 10s - loss: 13.4844 - mse: 13.4844 - mae: 1.5491 - val_loss: 12.3593 - val_mse: 12.3593 - val_mae: 1.4929 - lr: 3.0797e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 10s - loss: 13.4316 - mse: 13.4316 - mae: 1.5433 - val_loss: 12.2565 - val_mse: 12.2565 - val_mae: 1.5324 - lr: 3.0797e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 10s - loss: 13.4643 - mse: 13.4643 - mae: 1.5559 - val_loss: 12.2451 - val_mse: 12.2451 - val_mae: 1.5493 - lr: 3.0797e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 10s - loss: 13.5294 - mse: 13.5294 - mae: 1.5477 - val_loss: 12.6015 - val_mse: 12.6015 - val_mae: 1.4839 - lr: 3.0797e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 10s - loss: 13.4419 - mse: 13.4419 - mae: 1.5523 - val_loss: 12.3634 - val_mse: 12.3634 - val_mae: 1.5695 - lr: 3.0797e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 10s - loss: 13.4880 - mse: 13.4880 - mae: 1.5468 - val_loss: 12.2972 - val_mse: 12.2972 - val_mae: 1.5583 - lr: 3.0797e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 10s - loss: 13.5239 - mse: 13.5239 - mae: 1.5470 - val_loss: 12.2404 - val_mse: 12.2404 - val_mae: 1.5316 - lr: 3.0797e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 11s - loss: 13.5015 - mse: 13.5015 - mae: 1.5471 - val_loss: 12.2445 - val_mse: 12.2445 - val_mae: 1.5543 - lr: 3.0797e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 10s - loss: 13.5146 - mse: 13.5146 - mae: 1.5420 - val_loss: 12.3416 - val_mse: 12.3416 - val_mae: 1.5827 - lr: 3.0797e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 10s - loss: 13.4242 - mse: 13.4242 - mae: 1.5460 - val_loss: 12.2232 - val_mse: 12.2232 - val_mae: 1.5413 - lr: 3.0797e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 10s - loss: 13.5110 - mse: 13.5110 - mae: 1.5479 - val_loss: 12.2388 - val_mse: 12.2388 - val_mae: 1.5890 - lr: 3.0797e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 10s - loss: 13.5022 - mse: 13.5022 - mae: 1.5483 - val_loss: 12.2634 - val_mse: 12.2634 - val_mae: 1.5423 - lr: 3.0797e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 13/100\n",
            "1000/1000 - 10s - loss: 13.5132 - mse: 13.5132 - mae: 1.5502 - val_loss: 12.3927 - val_mse: 12.3927 - val_mae: 1.5016 - lr: 3.0797e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 14/100\n",
            "1000/1000 - 10s - loss: 13.6183 - mse: 13.6183 - mae: 1.5469 - val_loss: 12.2149 - val_mse: 12.2149 - val_mae: 1.5344 - lr: 3.0797e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 15/100\n",
            "1000/1000 - 10s - loss: 13.4938 - mse: 13.4938 - mae: 1.5487 - val_loss: 12.2464 - val_mse: 12.2464 - val_mae: 1.5457 - lr: 3.0797e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 16/100\n",
            "1000/1000 - 10s - loss: 13.4991 - mse: 13.4991 - mae: 1.5467 - val_loss: 12.3835 - val_mse: 12.3835 - val_mae: 1.5477 - lr: 3.0797e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 17/100\n",
            "1000/1000 - 10s - loss: 13.4932 - mse: 13.4932 - mae: 1.5488 - val_loss: 12.2887 - val_mse: 12.2887 - val_mae: 1.5985 - lr: 3.0797e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 18/100\n",
            "1000/1000 - 10s - loss: 13.4954 - mse: 13.4954 - mae: 1.5464 - val_loss: 12.6304 - val_mse: 12.6304 - val_mae: 1.7015 - lr: 3.0797e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 19/100\n",
            "1000/1000 - 10s - loss: 13.4838 - mse: 13.4838 - mae: 1.5425 - val_loss: 12.2709 - val_mse: 12.2709 - val_mae: 1.5218 - lr: 3.0797e-04 - 10s/epoch - 10ms/step\n",
            "Score for fold 2: loss of 12.270936012268066\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 10s - loss: 14.4454 - mse: 14.4454 - mae: 1.5692 - val_loss: 8.6609 - val_mse: 8.6609 - val_mae: 1.4369 - lr: 3.0797e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 10s - loss: 14.3861 - mse: 14.3861 - mae: 1.5683 - val_loss: 8.5949 - val_mse: 8.5949 - val_mae: 1.5461 - lr: 3.0797e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 10s - loss: 14.3910 - mse: 14.3910 - mae: 1.5718 - val_loss: 8.8123 - val_mse: 8.8123 - val_mae: 1.4302 - lr: 3.0797e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 10s - loss: 14.5210 - mse: 14.5210 - mae: 1.5720 - val_loss: 8.7140 - val_mse: 8.7140 - val_mae: 1.5672 - lr: 3.0797e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 10s - loss: 14.3767 - mse: 14.3767 - mae: 1.5686 - val_loss: 8.6900 - val_mse: 8.6900 - val_mae: 1.4457 - lr: 3.0797e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 10s - loss: 14.4206 - mse: 14.4206 - mae: 1.5624 - val_loss: 8.5759 - val_mse: 8.5759 - val_mae: 1.5448 - lr: 3.0797e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 10s - loss: 14.4331 - mse: 14.4331 - mae: 1.5680 - val_loss: 8.7173 - val_mse: 8.7173 - val_mae: 1.4511 - lr: 3.0797e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 10s - loss: 14.4560 - mse: 14.4560 - mae: 1.5680 - val_loss: 8.5712 - val_mse: 8.5712 - val_mae: 1.4996 - lr: 3.0797e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 10s - loss: 14.4381 - mse: 14.4381 - mae: 1.5641 - val_loss: 8.6129 - val_mse: 8.6129 - val_mae: 1.5158 - lr: 3.0797e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 10s - loss: 14.5071 - mse: 14.5071 - mae: 1.5676 - val_loss: 8.7163 - val_mse: 8.7163 - val_mae: 1.4703 - lr: 3.0797e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 10s - loss: 14.4543 - mse: 14.4543 - mae: 1.5685 - val_loss: 8.5867 - val_mse: 8.5867 - val_mae: 1.5080 - lr: 3.0797e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 10s - loss: 14.4173 - mse: 14.4173 - mae: 1.5713 - val_loss: 8.5671 - val_mse: 8.5671 - val_mae: 1.5257 - lr: 3.0797e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 13/100\n",
            "1000/1000 - 10s - loss: 14.4318 - mse: 14.4318 - mae: 1.5662 - val_loss: 8.6340 - val_mse: 8.6340 - val_mae: 1.5529 - lr: 3.0797e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 14/100\n",
            "1000/1000 - 10s - loss: 14.4153 - mse: 14.4153 - mae: 1.5729 - val_loss: 8.6400 - val_mse: 8.6400 - val_mae: 1.4798 - lr: 3.0797e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 15/100\n",
            "1000/1000 - 10s - loss: 14.4265 - mse: 14.4265 - mae: 1.5647 - val_loss: 8.7563 - val_mse: 8.7563 - val_mae: 1.4101 - lr: 3.0797e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 16/100\n",
            "1000/1000 - 10s - loss: 14.4338 - mse: 14.4338 - mae: 1.5647 - val_loss: 8.6764 - val_mse: 8.6764 - val_mae: 1.5773 - lr: 3.0797e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 17/100\n",
            "1000/1000 - 10s - loss: 14.4398 - mse: 14.4398 - mae: 1.5639 - val_loss: 8.6080 - val_mse: 8.6080 - val_mae: 1.4906 - lr: 3.0797e-04 - 10s/epoch - 10ms/step\n",
            "Score for fold 3: loss of 8.608022689819336\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 10s - loss: 14.1522 - mse: 14.1522 - mae: 1.5523 - val_loss: 10.1773 - val_mse: 10.1773 - val_mae: 1.5020 - lr: 3.0797e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 10s - loss: 14.0860 - mse: 14.0860 - mae: 1.5580 - val_loss: 10.0329 - val_mse: 10.0329 - val_mae: 1.4782 - lr: 3.0797e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 10s - loss: 14.0726 - mse: 14.0726 - mae: 1.5587 - val_loss: 10.0734 - val_mse: 10.0734 - val_mae: 1.4978 - lr: 3.0797e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 10s - loss: 14.0149 - mse: 14.0149 - mae: 1.5525 - val_loss: 10.3548 - val_mse: 10.3548 - val_mae: 1.6100 - lr: 3.0797e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 10s - loss: 14.0963 - mse: 14.0963 - mae: 1.5550 - val_loss: 10.2136 - val_mse: 10.2136 - val_mae: 1.5273 - lr: 3.0797e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 10s - loss: 14.0746 - mse: 14.0746 - mae: 1.5534 - val_loss: 10.1868 - val_mse: 10.1868 - val_mae: 1.5717 - lr: 3.0797e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 10s - loss: 14.0682 - mse: 14.0682 - mae: 1.5525 - val_loss: 10.1919 - val_mse: 10.1919 - val_mae: 1.6073 - lr: 3.0797e-04 - 10s/epoch - 10ms/step\n",
            "Score for fold 4: loss of 10.191923141479492\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 10s - loss: 12.3357 - mse: 12.3357 - mae: 1.5408 - val_loss: 18.6126 - val_mse: 18.6126 - val_mae: 1.7259 - lr: 3.0797e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 10s - loss: 12.1398 - mse: 12.1398 - mae: 1.5460 - val_loss: 18.0140 - val_mse: 18.0140 - val_mae: 1.5229 - lr: 3.0797e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 10s - loss: 12.1276 - mse: 12.1276 - mae: 1.5411 - val_loss: 18.0560 - val_mse: 18.0560 - val_mae: 1.6083 - lr: 3.0797e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 10s - loss: 12.0393 - mse: 12.0393 - mae: 1.5405 - val_loss: 17.9512 - val_mse: 17.9512 - val_mae: 1.5778 - lr: 3.0797e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 10s - loss: 12.1185 - mse: 12.1185 - mae: 1.5448 - val_loss: 18.6310 - val_mse: 18.6310 - val_mae: 1.7166 - lr: 3.0797e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 10s - loss: 11.9908 - mse: 11.9908 - mae: 1.5360 - val_loss: 18.3489 - val_mse: 18.3489 - val_mae: 1.6904 - lr: 3.0797e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 10s - loss: 12.0531 - mse: 12.0531 - mae: 1.5422 - val_loss: 17.9588 - val_mse: 17.9588 - val_mae: 1.5271 - lr: 3.0797e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 10s - loss: 12.2044 - mse: 12.2044 - mae: 1.5384 - val_loss: 18.2975 - val_mse: 18.2975 - val_mae: 1.4883 - lr: 3.0797e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 10s - loss: 12.1237 - mse: 12.1237 - mae: 1.5351 - val_loss: 18.1291 - val_mse: 18.1291 - val_mae: 1.6138 - lr: 3.0797e-04 - 10s/epoch - 10ms/step\n",
            "Score for fold 5: loss of 18.129125595092773\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-10-11 17:14:50,047]\u001b[0m Finished trial#35 resulted in value: 13.238. Current best value is 8.234 with parameters: {'activation': 'relu', 'num_hidden_layer': 4, 'i': 10, 'learning_rate': 0.00015658492778156685}.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 7s - loss: 14.5256 - mse: 14.5256 - mae: 1.5461 - val_loss: 10.3424 - val_mse: 10.3424 - val_mae: 1.4763 - lr: 1.5229e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 6s - loss: 13.0659 - mse: 13.0659 - mae: 1.4845 - val_loss: 9.8211 - val_mse: 9.8211 - val_mae: 1.4234 - lr: 1.5229e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 6s - loss: 12.8553 - mse: 12.8553 - mae: 1.4720 - val_loss: 9.8486 - val_mse: 9.8486 - val_mae: 1.4413 - lr: 1.5229e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 6s - loss: 12.7876 - mse: 12.7876 - mae: 1.4593 - val_loss: 8.9646 - val_mse: 8.9646 - val_mae: 1.4807 - lr: 1.5229e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 6s - loss: 12.5790 - mse: 12.5790 - mae: 1.4521 - val_loss: 9.1085 - val_mse: 9.1085 - val_mae: 1.5080 - lr: 1.5229e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 6s - loss: 12.5499 - mse: 12.5499 - mae: 1.4450 - val_loss: 8.9682 - val_mse: 8.9682 - val_mae: 1.4155 - lr: 1.5229e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 6s - loss: 12.3602 - mse: 12.3602 - mae: 1.4379 - val_loss: 9.0945 - val_mse: 9.0945 - val_mae: 1.4637 - lr: 1.5229e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 7s - loss: 12.2500 - mse: 12.2500 - mae: 1.4297 - val_loss: 8.9990 - val_mse: 8.9990 - val_mae: 1.4768 - lr: 1.5229e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 6s - loss: 12.1798 - mse: 12.1798 - mae: 1.4183 - val_loss: 8.8370 - val_mse: 8.8370 - val_mae: 1.4694 - lr: 1.5229e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 6s - loss: 11.9489 - mse: 11.9489 - mae: 1.4054 - val_loss: 9.1510 - val_mse: 9.1510 - val_mae: 1.4666 - lr: 1.5229e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 6s - loss: 11.9387 - mse: 11.9387 - mae: 1.4005 - val_loss: 8.7283 - val_mse: 8.7283 - val_mae: 1.4853 - lr: 1.5229e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 7s - loss: 11.6018 - mse: 11.6018 - mae: 1.3916 - val_loss: 9.5926 - val_mse: 9.5926 - val_mae: 1.4691 - lr: 1.5229e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 13/100\n",
            "1000/1000 - 6s - loss: 11.5236 - mse: 11.5236 - mae: 1.3812 - val_loss: 9.5178 - val_mse: 9.5178 - val_mae: 1.4588 - lr: 1.5229e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 14/100\n",
            "1000/1000 - 6s - loss: 11.2528 - mse: 11.2528 - mae: 1.3683 - val_loss: 8.8685 - val_mse: 8.8685 - val_mae: 1.4780 - lr: 1.5229e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 15/100\n",
            "1000/1000 - 6s - loss: 11.1000 - mse: 11.1000 - mae: 1.3552 - val_loss: 9.2765 - val_mse: 9.2765 - val_mae: 1.4919 - lr: 1.5229e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 16/100\n",
            "1000/1000 - 6s - loss: 10.9456 - mse: 10.9456 - mae: 1.3487 - val_loss: 9.1803 - val_mse: 9.1803 - val_mae: 1.4964 - lr: 1.5229e-04 - 6s/epoch - 6ms/step\n",
            "Score for fold 1: loss of 9.180253982543945\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 6s - loss: 11.4843 - mse: 11.4843 - mae: 1.3871 - val_loss: 7.7995 - val_mse: 7.7995 - val_mae: 1.3247 - lr: 1.5229e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 6s - loss: 11.1832 - mse: 11.1832 - mae: 1.3683 - val_loss: 7.8746 - val_mse: 7.8746 - val_mae: 1.3797 - lr: 1.5229e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 6s - loss: 10.8841 - mse: 10.8841 - mae: 1.3607 - val_loss: 7.8307 - val_mse: 7.8307 - val_mae: 1.3465 - lr: 1.5229e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 6s - loss: 10.6620 - mse: 10.6620 - mae: 1.3401 - val_loss: 8.1929 - val_mse: 8.1929 - val_mae: 1.3183 - lr: 1.5229e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 6s - loss: 10.2244 - mse: 10.2244 - mae: 1.3258 - val_loss: 7.9465 - val_mse: 7.9465 - val_mae: 1.3300 - lr: 1.5229e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 6s - loss: 10.0399 - mse: 10.0399 - mae: 1.3151 - val_loss: 8.1938 - val_mse: 8.1938 - val_mae: 1.3860 - lr: 1.5229e-04 - 6s/epoch - 6ms/step\n",
            "Score for fold 2: loss of 8.193808555603027\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 6s - loss: 10.0010 - mse: 10.0010 - mae: 1.3361 - val_loss: 8.7949 - val_mse: 8.7949 - val_mae: 1.3214 - lr: 1.5229e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 6s - loss: 9.6728 - mse: 9.6728 - mae: 1.3161 - val_loss: 9.3594 - val_mse: 9.3594 - val_mae: 1.2595 - lr: 1.5229e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 6s - loss: 9.7972 - mse: 9.7972 - mae: 1.2979 - val_loss: 8.9085 - val_mse: 8.9085 - val_mae: 1.3030 - lr: 1.5229e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 6s - loss: 9.4108 - mse: 9.4108 - mae: 1.2854 - val_loss: 8.9334 - val_mse: 8.9334 - val_mae: 1.3015 - lr: 1.5229e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 6s - loss: 9.0556 - mse: 9.0556 - mae: 1.2627 - val_loss: 9.0719 - val_mse: 9.0719 - val_mae: 1.3380 - lr: 1.5229e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 6s - loss: 8.8722 - mse: 8.8722 - mae: 1.2511 - val_loss: 9.0623 - val_mse: 9.0623 - val_mae: 1.3107 - lr: 1.5229e-04 - 6s/epoch - 6ms/step\n",
            "Score for fold 3: loss of 9.062335014343262\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 6s - loss: 7.3270 - mse: 7.3270 - mae: 1.2774 - val_loss: 14.1078 - val_mse: 14.1078 - val_mae: 1.2206 - lr: 1.5229e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 6s - loss: 7.1471 - mse: 7.1471 - mae: 1.2587 - val_loss: 14.9945 - val_mse: 14.9945 - val_mae: 1.2253 - lr: 1.5229e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 6s - loss: 6.7954 - mse: 6.7954 - mae: 1.2435 - val_loss: 14.4357 - val_mse: 14.4357 - val_mae: 1.2635 - lr: 1.5229e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 6s - loss: 6.8505 - mse: 6.8505 - mae: 1.2232 - val_loss: 15.0073 - val_mse: 15.0073 - val_mae: 1.2504 - lr: 1.5229e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 6s - loss: 6.4013 - mse: 6.4013 - mae: 1.2073 - val_loss: 14.5845 - val_mse: 14.5845 - val_mae: 1.2923 - lr: 1.5229e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 6s - loss: 6.3153 - mse: 6.3153 - mae: 1.1997 - val_loss: 15.3308 - val_mse: 15.3308 - val_mae: 1.2798 - lr: 1.5229e-04 - 6s/epoch - 6ms/step\n",
            "Score for fold 4: loss of 15.33077621459961\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 6s - loss: 8.4747 - mse: 8.4747 - mae: 1.2303 - val_loss: 6.3903 - val_mse: 6.3903 - val_mae: 1.2226 - lr: 1.5229e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 6s - loss: 7.8813 - mse: 7.8813 - mae: 1.2043 - val_loss: 7.2603 - val_mse: 7.2603 - val_mae: 1.2443 - lr: 1.5229e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 7s - loss: 8.1103 - mse: 8.1103 - mae: 1.1919 - val_loss: 6.4762 - val_mse: 6.4762 - val_mae: 1.2981 - lr: 1.5229e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 7s - loss: 7.5951 - mse: 7.5951 - mae: 1.1761 - val_loss: 6.9260 - val_mse: 6.9260 - val_mae: 1.3455 - lr: 1.5229e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 7s - loss: 6.9891 - mse: 6.9891 - mae: 1.1540 - val_loss: 7.1441 - val_mse: 7.1441 - val_mae: 1.2890 - lr: 1.5229e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 7s - loss: 6.9769 - mse: 6.9769 - mae: 1.1404 - val_loss: 7.1384 - val_mse: 7.1384 - val_mae: 1.2465 - lr: 1.5229e-04 - 7s/epoch - 7ms/step\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-10-11 17:19:13,456]\u001b[0m Finished trial#36 resulted in value: 9.78. Current best value is 8.234 with parameters: {'activation': 'relu', 'num_hidden_layer': 4, 'i': 10, 'learning_rate': 0.00015658492778156685}.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Score for fold 5: loss of 7.13842248916626\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 7s - loss: 13.4311 - mse: 13.4311 - mae: 1.5589 - val_loss: 11.2808 - val_mse: 11.2808 - val_mae: 1.5239 - lr: 0.0016 - 7s/epoch - 7ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 7s - loss: 12.8284 - mse: 12.8284 - mae: 1.5052 - val_loss: 10.9796 - val_mse: 10.9796 - val_mae: 1.5408 - lr: 0.0016 - 7s/epoch - 7ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 6s - loss: 12.4351 - mse: 12.4351 - mae: 1.4897 - val_loss: 11.2884 - val_mse: 11.2884 - val_mae: 1.4872 - lr: 0.0016 - 6s/epoch - 6ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 7s - loss: 12.4112 - mse: 12.4112 - mae: 1.4740 - val_loss: 12.5546 - val_mse: 12.5546 - val_mae: 1.6051 - lr: 0.0016 - 7s/epoch - 7ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 6s - loss: 12.3235 - mse: 12.3235 - mae: 1.4727 - val_loss: 11.5721 - val_mse: 11.5721 - val_mae: 1.4326 - lr: 0.0016 - 6s/epoch - 6ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 6s - loss: 12.3681 - mse: 12.3681 - mae: 1.4609 - val_loss: 11.8715 - val_mse: 11.8715 - val_mae: 1.5660 - lr: 0.0016 - 6s/epoch - 6ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 6s - loss: 12.2709 - mse: 12.2709 - mae: 1.4592 - val_loss: 11.2354 - val_mse: 11.2354 - val_mae: 1.5293 - lr: 0.0016 - 6s/epoch - 6ms/step\n",
            "Score for fold 1: loss of 11.235435485839844\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 6s - loss: 11.8476 - mse: 11.8476 - mae: 1.4493 - val_loss: 11.6151 - val_mse: 11.6151 - val_mae: 1.3972 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 7s - loss: 11.5651 - mse: 11.5651 - mae: 1.4379 - val_loss: 11.6390 - val_mse: 11.6390 - val_mae: 1.4722 - lr: 0.0010 - 7s/epoch - 7ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 7s - loss: 11.4890 - mse: 11.4890 - mae: 1.4327 - val_loss: 11.8062 - val_mse: 11.8062 - val_mae: 1.5007 - lr: 0.0010 - 7s/epoch - 7ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 6s - loss: 11.3830 - mse: 11.3830 - mae: 1.4271 - val_loss: 11.9079 - val_mse: 11.9079 - val_mae: 1.5007 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 6s - loss: 11.2942 - mse: 11.2942 - mae: 1.4222 - val_loss: 11.8106 - val_mse: 11.8106 - val_mae: 1.4124 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 6s - loss: 11.6462 - mse: 11.6462 - mae: 1.4174 - val_loss: 11.8379 - val_mse: 11.8379 - val_mae: 1.4333 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
            "Score for fold 2: loss of 11.837950706481934\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 6s - loss: 9.8510 - mse: 9.8510 - mae: 1.4259 - val_loss: 17.3097 - val_mse: 17.3097 - val_mae: 1.4613 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 6s - loss: 9.2895 - mse: 9.2895 - mae: 1.4156 - val_loss: 17.3128 - val_mse: 17.3128 - val_mae: 1.4281 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 6s - loss: 9.2597 - mse: 9.2597 - mae: 1.4097 - val_loss: 17.4369 - val_mse: 17.4369 - val_mae: 1.4517 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 6s - loss: 9.3589 - mse: 9.3589 - mae: 1.4037 - val_loss: 17.7038 - val_mse: 17.7038 - val_mae: 1.4244 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 7s - loss: 9.0904 - mse: 9.0904 - mae: 1.3957 - val_loss: 17.7353 - val_mse: 17.7353 - val_mae: 1.4647 - lr: 0.0010 - 7s/epoch - 7ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 6s - loss: 9.2628 - mse: 9.2628 - mae: 1.3781 - val_loss: 17.6667 - val_mse: 17.6667 - val_mae: 1.4961 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
            "Score for fold 3: loss of 17.66666603088379\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 6s - loss: 11.5762 - mse: 11.5762 - mae: 1.3978 - val_loss: 6.9787 - val_mse: 6.9787 - val_mae: 1.3930 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 6s - loss: 11.2419 - mse: 11.2419 - mae: 1.3851 - val_loss: 7.1388 - val_mse: 7.1388 - val_mae: 1.4223 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 6s - loss: 11.0989 - mse: 11.0989 - mae: 1.3773 - val_loss: 7.3356 - val_mse: 7.3356 - val_mae: 1.3730 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 6s - loss: 11.0067 - mse: 11.0067 - mae: 1.3685 - val_loss: 7.4905 - val_mse: 7.4905 - val_mae: 1.3647 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 6s - loss: 10.9140 - mse: 10.9140 - mae: 1.3569 - val_loss: 7.2853 - val_mse: 7.2853 - val_mae: 1.3586 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 6s - loss: 11.1685 - mse: 11.1685 - mae: 1.3566 - val_loss: 7.3655 - val_mse: 7.3655 - val_mae: 1.3827 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
            "Score for fold 4: loss of 7.365510940551758\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 6s - loss: 10.7794 - mse: 10.7794 - mae: 1.3732 - val_loss: 7.8480 - val_mse: 7.8480 - val_mae: 1.3208 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 6s - loss: 10.4416 - mse: 10.4416 - mae: 1.3576 - val_loss: 9.0620 - val_mse: 9.0620 - val_mae: 1.3230 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 7s - loss: 10.1096 - mse: 10.1096 - mae: 1.3433 - val_loss: 7.9135 - val_mse: 7.9135 - val_mae: 1.3523 - lr: 0.0010 - 7s/epoch - 7ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 6s - loss: 10.0402 - mse: 10.0402 - mae: 1.3337 - val_loss: 8.1184 - val_mse: 8.1184 - val_mae: 1.3103 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 6s - loss: 9.9975 - mse: 9.9975 - mae: 1.3229 - val_loss: 7.8081 - val_mse: 7.8081 - val_mae: 1.3701 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 6s - loss: 9.7410 - mse: 9.7410 - mae: 1.3125 - val_loss: 8.3924 - val_mse: 8.3924 - val_mae: 1.3751 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 6s - loss: 9.6351 - mse: 9.6351 - mae: 1.3076 - val_loss: 8.0824 - val_mse: 8.0824 - val_mae: 1.3205 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 6s - loss: 9.5532 - mse: 9.5532 - mae: 1.2999 - val_loss: 8.8106 - val_mse: 8.8106 - val_mae: 1.3300 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 6s - loss: 9.3810 - mse: 9.3810 - mae: 1.2902 - val_loss: 9.1944 - val_mse: 9.1944 - val_mae: 1.3431 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 6s - loss: 9.2325 - mse: 9.2325 - mae: 1.2820 - val_loss: 9.7006 - val_mse: 9.7006 - val_mae: 1.3278 - lr: 0.0010 - 6s/epoch - 6ms/step\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-10-11 17:23:04,182]\u001b[0m Finished trial#37 resulted in value: 11.563999999999998. Current best value is 8.234 with parameters: {'activation': 'relu', 'num_hidden_layer': 4, 'i': 10, 'learning_rate': 0.00015658492778156685}.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Score for fold 5: loss of 9.700592994689941\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 12s - loss: 15.0957 - mse: 15.0957 - mae: 1.6361 - val_loss: 11.3692 - val_mse: 11.3692 - val_mae: 1.4509 - lr: 1.1543e-04 - 12s/epoch - 12ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 11s - loss: 14.1460 - mse: 14.1460 - mae: 1.5344 - val_loss: 11.1581 - val_mse: 11.1581 - val_mae: 1.5133 - lr: 1.1543e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 11s - loss: 13.9897 - mse: 13.9897 - mae: 1.5193 - val_loss: 11.4414 - val_mse: 11.4414 - val_mae: 1.4689 - lr: 1.1543e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 11s - loss: 13.8657 - mse: 13.8657 - mae: 1.5047 - val_loss: 11.1671 - val_mse: 11.1671 - val_mae: 1.5107 - lr: 1.1543e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 11s - loss: 13.7754 - mse: 13.7754 - mae: 1.5012 - val_loss: 11.0096 - val_mse: 11.0096 - val_mae: 1.4724 - lr: 1.1543e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 11s - loss: 13.6005 - mse: 13.6005 - mae: 1.4931 - val_loss: 10.8516 - val_mse: 10.8516 - val_mae: 1.4783 - lr: 1.1543e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 11s - loss: 13.5924 - mse: 13.5924 - mae: 1.4893 - val_loss: 11.1312 - val_mse: 11.1312 - val_mae: 1.6252 - lr: 1.1543e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 11s - loss: 13.4412 - mse: 13.4412 - mae: 1.4835 - val_loss: 10.7511 - val_mse: 10.7511 - val_mae: 1.4805 - lr: 1.1543e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 11s - loss: 13.4144 - mse: 13.4144 - mae: 1.4808 - val_loss: 10.6017 - val_mse: 10.6017 - val_mae: 1.4378 - lr: 1.1543e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 11s - loss: 13.2588 - mse: 13.2588 - mae: 1.4741 - val_loss: 10.7547 - val_mse: 10.7547 - val_mae: 1.5206 - lr: 1.1543e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 11s - loss: 13.2199 - mse: 13.2199 - mae: 1.4715 - val_loss: 10.7935 - val_mse: 10.7935 - val_mae: 1.5047 - lr: 1.1543e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 11s - loss: 13.1269 - mse: 13.1269 - mae: 1.4701 - val_loss: 10.6022 - val_mse: 10.6022 - val_mae: 1.4416 - lr: 1.1543e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 13/100\n",
            "1000/1000 - 11s - loss: 13.0885 - mse: 13.0885 - mae: 1.4655 - val_loss: 10.7007 - val_mse: 10.7007 - val_mae: 1.4851 - lr: 1.1543e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 14/100\n",
            "1000/1000 - 11s - loss: 12.9784 - mse: 12.9784 - mae: 1.4595 - val_loss: 10.7138 - val_mse: 10.7138 - val_mae: 1.4895 - lr: 1.1543e-04 - 11s/epoch - 11ms/step\n",
            "Score for fold 1: loss of 10.713783264160156\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 11s - loss: 11.4329 - mse: 11.4329 - mae: 1.4597 - val_loss: 16.8934 - val_mse: 16.8934 - val_mae: 1.4840 - lr: 1.1543e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 11s - loss: 11.2981 - mse: 11.2981 - mae: 1.4537 - val_loss: 16.7958 - val_mse: 16.7958 - val_mae: 1.4684 - lr: 1.1543e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 11s - loss: 11.2009 - mse: 11.2009 - mae: 1.4518 - val_loss: 16.7886 - val_mse: 16.7886 - val_mae: 1.4680 - lr: 1.1543e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 11s - loss: 11.2195 - mse: 11.2195 - mae: 1.4444 - val_loss: 16.7721 - val_mse: 16.7721 - val_mae: 1.4858 - lr: 1.1543e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 10s - loss: 11.0535 - mse: 11.0535 - mae: 1.4397 - val_loss: 16.8322 - val_mse: 16.8322 - val_mae: 1.4431 - lr: 1.1543e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 11s - loss: 10.9914 - mse: 10.9914 - mae: 1.4367 - val_loss: 17.0469 - val_mse: 17.0469 - val_mae: 1.4697 - lr: 1.1543e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 11s - loss: 10.8546 - mse: 10.8546 - mae: 1.4272 - val_loss: 16.8692 - val_mse: 16.8692 - val_mae: 1.4670 - lr: 1.1543e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 11s - loss: 10.7744 - mse: 10.7744 - mae: 1.4223 - val_loss: 16.9276 - val_mse: 16.9276 - val_mae: 1.4760 - lr: 1.1543e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 11s - loss: 10.6966 - mse: 10.6966 - mae: 1.4170 - val_loss: 16.7592 - val_mse: 16.7592 - val_mae: 1.4900 - lr: 1.1543e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 11s - loss: 10.5828 - mse: 10.5828 - mae: 1.4093 - val_loss: 16.8753 - val_mse: 16.8753 - val_mae: 1.4394 - lr: 1.1543e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 11s - loss: 10.4591 - mse: 10.4591 - mae: 1.4042 - val_loss: 16.9947 - val_mse: 16.9947 - val_mae: 1.5155 - lr: 1.1543e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 11s - loss: 10.3071 - mse: 10.3071 - mae: 1.3950 - val_loss: 16.7149 - val_mse: 16.7149 - val_mae: 1.4569 - lr: 1.1543e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 13/100\n",
            "1000/1000 - 11s - loss: 10.2158 - mse: 10.2158 - mae: 1.3886 - val_loss: 16.9029 - val_mse: 16.9029 - val_mae: 1.5525 - lr: 1.1543e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 14/100\n",
            "1000/1000 - 11s - loss: 10.0161 - mse: 10.0161 - mae: 1.3790 - val_loss: 17.0801 - val_mse: 17.0801 - val_mae: 1.4895 - lr: 1.1543e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 15/100\n",
            "1000/1000 - 11s - loss: 9.8773 - mse: 9.8773 - mae: 1.3688 - val_loss: 16.9039 - val_mse: 16.9039 - val_mae: 1.4961 - lr: 1.1543e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 16/100\n",
            "1000/1000 - 11s - loss: 9.7875 - mse: 9.7875 - mae: 1.3615 - val_loss: 16.8449 - val_mse: 16.8449 - val_mae: 1.5105 - lr: 1.1543e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 17/100\n",
            "1000/1000 - 11s - loss: 9.6608 - mse: 9.6608 - mae: 1.3515 - val_loss: 16.9241 - val_mse: 16.9241 - val_mae: 1.5224 - lr: 1.1543e-04 - 11s/epoch - 11ms/step\n",
            "Score for fold 2: loss of 16.924108505249023\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 11s - loss: 10.9013 - mse: 10.9013 - mae: 1.3853 - val_loss: 11.5139 - val_mse: 11.5139 - val_mae: 1.3772 - lr: 1.1543e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 11s - loss: 10.7356 - mse: 10.7356 - mae: 1.3664 - val_loss: 11.6232 - val_mse: 11.6232 - val_mae: 1.3571 - lr: 1.1543e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 11s - loss: 10.5425 - mse: 10.5425 - mae: 1.3548 - val_loss: 11.7052 - val_mse: 11.7052 - val_mae: 1.3782 - lr: 1.1543e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 11s - loss: 10.3962 - mse: 10.3962 - mae: 1.3434 - val_loss: 12.0095 - val_mse: 12.0095 - val_mae: 1.3447 - lr: 1.1543e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 11s - loss: 10.2031 - mse: 10.2031 - mae: 1.3271 - val_loss: 11.7790 - val_mse: 11.7790 - val_mae: 1.3608 - lr: 1.1543e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 11s - loss: 10.0249 - mse: 10.0249 - mae: 1.3153 - val_loss: 11.8513 - val_mse: 11.8513 - val_mae: 1.3986 - lr: 1.1543e-04 - 11s/epoch - 11ms/step\n",
            "Score for fold 3: loss of 11.851283073425293\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 11s - loss: 11.5748 - mse: 11.5748 - mae: 1.3530 - val_loss: 5.2365 - val_mse: 5.2365 - val_mae: 1.2545 - lr: 1.1543e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 11s - loss: 11.3080 - mse: 11.3080 - mae: 1.3271 - val_loss: 5.1770 - val_mse: 5.1770 - val_mae: 1.2344 - lr: 1.1543e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 11s - loss: 11.1169 - mse: 11.1169 - mae: 1.3125 - val_loss: 5.4065 - val_mse: 5.4065 - val_mae: 1.2755 - lr: 1.1543e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 11s - loss: 11.0081 - mse: 11.0081 - mae: 1.2986 - val_loss: 5.4992 - val_mse: 5.4992 - val_mae: 1.2704 - lr: 1.1543e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 11s - loss: 10.6220 - mse: 10.6220 - mae: 1.2832 - val_loss: 5.6199 - val_mse: 5.6199 - val_mae: 1.3250 - lr: 1.1543e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 11s - loss: 10.4463 - mse: 10.4463 - mae: 1.2671 - val_loss: 5.5782 - val_mse: 5.5782 - val_mae: 1.2936 - lr: 1.1543e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 11s - loss: 10.3109 - mse: 10.3109 - mae: 1.2553 - val_loss: 5.7000 - val_mse: 5.7000 - val_mae: 1.3106 - lr: 1.1543e-04 - 11s/epoch - 11ms/step\n",
            "Score for fold 4: loss of 5.700013637542725\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 11s - loss: 9.0637 - mse: 9.0637 - mae: 1.2625 - val_loss: 10.1605 - val_mse: 10.1605 - val_mae: 1.2462 - lr: 1.1543e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 11s - loss: 8.7240 - mse: 8.7240 - mae: 1.2387 - val_loss: 10.3326 - val_mse: 10.3326 - val_mae: 1.2705 - lr: 1.1543e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 11s - loss: 8.5030 - mse: 8.5030 - mae: 1.2186 - val_loss: 10.3964 - val_mse: 10.3964 - val_mae: 1.3159 - lr: 1.1543e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 11s - loss: 8.3027 - mse: 8.3027 - mae: 1.1996 - val_loss: 10.6193 - val_mse: 10.6193 - val_mae: 1.2848 - lr: 1.1543e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 11s - loss: 8.1460 - mse: 8.1460 - mae: 1.1829 - val_loss: 10.7399 - val_mse: 10.7399 - val_mae: 1.3039 - lr: 1.1543e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 11s - loss: 7.8812 - mse: 7.8812 - mae: 1.1583 - val_loss: 10.7498 - val_mse: 10.7498 - val_mae: 1.3369 - lr: 1.1543e-04 - 11s/epoch - 11ms/step\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-10-11 17:32:20,136]\u001b[0m Finished trial#38 resulted in value: 11.186000000000002. Current best value is 8.234 with parameters: {'activation': 'relu', 'num_hidden_layer': 4, 'i': 10, 'learning_rate': 0.00015658492778156685}.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Score for fold 5: loss of 10.749837875366211\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 3s - loss: 12.6387 - mse: 12.6387 - mae: 1.6471 - val_loss: 19.2401 - val_mse: 19.2401 - val_mae: 1.5292 - lr: 2.6512e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 3s - loss: 11.9444 - mse: 11.9444 - mae: 1.5689 - val_loss: 19.2691 - val_mse: 19.2691 - val_mae: 1.5822 - lr: 2.6512e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 3s - loss: 11.9099 - mse: 11.9099 - mae: 1.5684 - val_loss: 19.1523 - val_mse: 19.1523 - val_mae: 1.5181 - lr: 2.6512e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 3s - loss: 11.9368 - mse: 11.9368 - mae: 1.5610 - val_loss: 19.2870 - val_mse: 19.2870 - val_mae: 1.5312 - lr: 2.6512e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 3s - loss: 11.8417 - mse: 11.8417 - mae: 1.5553 - val_loss: 19.0905 - val_mse: 19.0905 - val_mae: 1.5648 - lr: 2.6512e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 3s - loss: 11.8964 - mse: 11.8964 - mae: 1.5531 - val_loss: 19.1822 - val_mse: 19.1822 - val_mae: 1.5641 - lr: 2.6512e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 3s - loss: 11.8439 - mse: 11.8439 - mae: 1.5581 - val_loss: 19.4083 - val_mse: 19.4083 - val_mae: 1.5940 - lr: 2.6512e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 3s - loss: 11.8642 - mse: 11.8642 - mae: 1.5551 - val_loss: 19.1990 - val_mse: 19.1990 - val_mae: 1.4967 - lr: 2.6512e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 3s - loss: 11.8898 - mse: 11.8898 - mae: 1.5541 - val_loss: 19.1170 - val_mse: 19.1170 - val_mae: 1.5247 - lr: 2.6512e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 3s - loss: 11.8090 - mse: 11.8090 - mae: 1.5519 - val_loss: 19.0361 - val_mse: 19.0361 - val_mae: 1.5058 - lr: 2.6512e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 3s - loss: 11.8341 - mse: 11.8341 - mae: 1.5531 - val_loss: 19.0113 - val_mse: 19.0113 - val_mae: 1.5222 - lr: 2.6512e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 3s - loss: 11.8288 - mse: 11.8288 - mae: 1.5492 - val_loss: 19.0744 - val_mse: 19.0744 - val_mae: 1.5253 - lr: 2.6512e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 13/100\n",
            "1000/1000 - 3s - loss: 11.8001 - mse: 11.8001 - mae: 1.5519 - val_loss: 19.0105 - val_mse: 19.0105 - val_mae: 1.5314 - lr: 2.6512e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 14/100\n",
            "1000/1000 - 3s - loss: 11.7924 - mse: 11.7924 - mae: 1.5536 - val_loss: 19.0618 - val_mse: 19.0618 - val_mae: 1.5213 - lr: 2.6512e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 15/100\n",
            "1000/1000 - 3s - loss: 11.8054 - mse: 11.8054 - mae: 1.5523 - val_loss: 19.0282 - val_mse: 19.0282 - val_mae: 1.5307 - lr: 2.6512e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 16/100\n",
            "1000/1000 - 3s - loss: 11.8323 - mse: 11.8323 - mae: 1.5512 - val_loss: 19.1117 - val_mse: 19.1117 - val_mae: 1.5551 - lr: 2.6512e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 17/100\n",
            "1000/1000 - 3s - loss: 11.8004 - mse: 11.8004 - mae: 1.5511 - val_loss: 19.0832 - val_mse: 19.0832 - val_mae: 1.5471 - lr: 2.6512e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 18/100\n",
            "1000/1000 - 3s - loss: 11.7329 - mse: 11.7329 - mae: 1.5495 - val_loss: 19.1235 - val_mse: 19.1235 - val_mae: 1.5356 - lr: 2.6512e-04 - 3s/epoch - 3ms/step\n",
            "Score for fold 1: loss of 19.123523712158203\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 3s - loss: 13.8690 - mse: 13.8690 - mae: 1.5468 - val_loss: 10.4766 - val_mse: 10.4766 - val_mae: 1.5450 - lr: 2.6512e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 3s - loss: 13.8394 - mse: 13.8394 - mae: 1.5410 - val_loss: 10.4957 - val_mse: 10.4957 - val_mae: 1.5579 - lr: 2.6512e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 3s - loss: 13.8588 - mse: 13.8588 - mae: 1.5421 - val_loss: 10.4880 - val_mse: 10.4880 - val_mae: 1.5491 - lr: 2.6512e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 3s - loss: 13.9438 - mse: 13.9438 - mae: 1.5426 - val_loss: 10.6364 - val_mse: 10.6364 - val_mae: 1.6223 - lr: 2.6512e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 3s - loss: 13.8724 - mse: 13.8724 - mae: 1.5421 - val_loss: 10.5146 - val_mse: 10.5146 - val_mae: 1.5586 - lr: 2.6512e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 3s - loss: 13.9452 - mse: 13.9452 - mae: 1.5437 - val_loss: 10.5016 - val_mse: 10.5016 - val_mae: 1.5550 - lr: 2.6512e-04 - 3s/epoch - 3ms/step\n",
            "Score for fold 2: loss of 10.501587867736816\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 3s - loss: 14.1467 - mse: 14.1467 - mae: 1.5575 - val_loss: 9.5352 - val_mse: 9.5352 - val_mae: 1.5567 - lr: 2.6512e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 3s - loss: 14.2001 - mse: 14.2001 - mae: 1.5561 - val_loss: 9.5630 - val_mse: 9.5630 - val_mae: 1.5313 - lr: 2.6512e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 3s - loss: 14.2046 - mse: 14.2046 - mae: 1.5527 - val_loss: 9.4551 - val_mse: 9.4551 - val_mae: 1.5199 - lr: 2.6512e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 3s - loss: 14.1525 - mse: 14.1525 - mae: 1.5554 - val_loss: 9.4646 - val_mse: 9.4646 - val_mae: 1.5355 - lr: 2.6512e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 3s - loss: 14.1790 - mse: 14.1790 - mae: 1.5516 - val_loss: 9.6282 - val_mse: 9.6282 - val_mae: 1.4925 - lr: 2.6512e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 3s - loss: 14.1549 - mse: 14.1549 - mae: 1.5509 - val_loss: 9.5546 - val_mse: 9.5546 - val_mae: 1.4893 - lr: 2.6512e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 3s - loss: 14.1218 - mse: 14.1218 - mae: 1.5542 - val_loss: 9.4544 - val_mse: 9.4544 - val_mae: 1.5625 - lr: 2.6512e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 3s - loss: 14.1793 - mse: 14.1793 - mae: 1.5552 - val_loss: 9.4361 - val_mse: 9.4361 - val_mae: 1.5318 - lr: 2.6512e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 3s - loss: 14.1809 - mse: 14.1809 - mae: 1.5542 - val_loss: 9.4305 - val_mse: 9.4305 - val_mae: 1.5113 - lr: 2.6512e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 3s - loss: 14.1168 - mse: 14.1168 - mae: 1.5531 - val_loss: 9.4753 - val_mse: 9.4753 - val_mae: 1.5313 - lr: 2.6512e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 3s - loss: 14.1619 - mse: 14.1619 - mae: 1.5567 - val_loss: 9.4506 - val_mse: 9.4506 - val_mae: 1.5619 - lr: 2.6512e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 3s - loss: 14.2293 - mse: 14.2293 - mae: 1.5545 - val_loss: 9.4289 - val_mse: 9.4289 - val_mae: 1.5319 - lr: 2.6512e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 13/100\n",
            "1000/1000 - 3s - loss: 14.2153 - mse: 14.2153 - mae: 1.5556 - val_loss: 9.4371 - val_mse: 9.4371 - val_mae: 1.5610 - lr: 2.6512e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 14/100\n",
            "1000/1000 - 3s - loss: 14.2052 - mse: 14.2052 - mae: 1.5569 - val_loss: 9.4104 - val_mse: 9.4104 - val_mae: 1.5599 - lr: 2.6512e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 15/100\n",
            "1000/1000 - 3s - loss: 14.1948 - mse: 14.1948 - mae: 1.5588 - val_loss: 9.4302 - val_mse: 9.4302 - val_mae: 1.5135 - lr: 2.6512e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 16/100\n",
            "1000/1000 - 3s - loss: 14.2309 - mse: 14.2309 - mae: 1.5560 - val_loss: 9.4263 - val_mse: 9.4263 - val_mae: 1.5245 - lr: 2.6512e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 17/100\n",
            "1000/1000 - 3s - loss: 14.1850 - mse: 14.1850 - mae: 1.5594 - val_loss: 9.5906 - val_mse: 9.5906 - val_mae: 1.5041 - lr: 2.6512e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 18/100\n",
            "1000/1000 - 3s - loss: 14.1802 - mse: 14.1802 - mae: 1.5575 - val_loss: 9.4518 - val_mse: 9.4518 - val_mae: 1.5630 - lr: 2.6512e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 19/100\n",
            "1000/1000 - 3s - loss: 14.1783 - mse: 14.1783 - mae: 1.5535 - val_loss: 9.4278 - val_mse: 9.4278 - val_mae: 1.5620 - lr: 2.6512e-04 - 3s/epoch - 3ms/step\n",
            "Score for fold 3: loss of 9.427824020385742\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 3s - loss: 13.7352 - mse: 13.7352 - mae: 1.5404 - val_loss: 11.3653 - val_mse: 11.3653 - val_mae: 1.5799 - lr: 2.6512e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 3s - loss: 13.6440 - mse: 13.6440 - mae: 1.5437 - val_loss: 11.4345 - val_mse: 11.4345 - val_mae: 1.5350 - lr: 2.6512e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 3s - loss: 13.6972 - mse: 13.6972 - mae: 1.5467 - val_loss: 11.4316 - val_mse: 11.4316 - val_mae: 1.5630 - lr: 2.6512e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 3s - loss: 13.6924 - mse: 13.6924 - mae: 1.5407 - val_loss: 11.4481 - val_mse: 11.4481 - val_mae: 1.5531 - lr: 2.6512e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 3s - loss: 13.6455 - mse: 13.6455 - mae: 1.5403 - val_loss: 11.4321 - val_mse: 11.4321 - val_mae: 1.5851 - lr: 2.6512e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 3s - loss: 13.6639 - mse: 13.6639 - mae: 1.5426 - val_loss: 11.4424 - val_mse: 11.4424 - val_mae: 1.5573 - lr: 2.6512e-04 - 3s/epoch - 3ms/step\n",
            "Score for fold 4: loss of 11.442375183105469\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 3s - loss: 12.5579 - mse: 12.5579 - mae: 1.5387 - val_loss: 15.5923 - val_mse: 15.5923 - val_mae: 1.5418 - lr: 2.6512e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 3s - loss: 12.6262 - mse: 12.6262 - mae: 1.5417 - val_loss: 15.7268 - val_mse: 15.7268 - val_mae: 1.5661 - lr: 2.6512e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 3s - loss: 12.7563 - mse: 12.7563 - mae: 1.5422 - val_loss: 15.5877 - val_mse: 15.5877 - val_mae: 1.5512 - lr: 2.6512e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 3s - loss: 12.6523 - mse: 12.6523 - mae: 1.5414 - val_loss: 15.6243 - val_mse: 15.6243 - val_mae: 1.5669 - lr: 2.6512e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 3s - loss: 12.5660 - mse: 12.5660 - mae: 1.5403 - val_loss: 15.6312 - val_mse: 15.6312 - val_mae: 1.5231 - lr: 2.6512e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 3s - loss: 12.6790 - mse: 12.6790 - mae: 1.5411 - val_loss: 15.6269 - val_mse: 15.6269 - val_mae: 1.5605 - lr: 2.6512e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 3s - loss: 12.6601 - mse: 12.6601 - mae: 1.5399 - val_loss: 15.6577 - val_mse: 15.6577 - val_mae: 1.5698 - lr: 2.6512e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 3s - loss: 12.6068 - mse: 12.6068 - mae: 1.5376 - val_loss: 15.6306 - val_mse: 15.6306 - val_mae: 1.5529 - lr: 2.6512e-04 - 3s/epoch - 3ms/step\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-10-11 17:35:02,144]\u001b[0m Finished trial#39 resulted in value: 13.223999999999998. Current best value is 8.234 with parameters: {'activation': 'relu', 'num_hidden_layer': 4, 'i': 10, 'learning_rate': 0.00015658492778156685}.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Score for fold 5: loss of 15.630549430847168\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 7s - loss: 12.6366 - mse: 12.6366 - mae: 1.5384 - val_loss: 15.1109 - val_mse: 15.1109 - val_mae: 1.4547 - lr: 3.8776e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 6s - loss: 11.8285 - mse: 11.8285 - mae: 1.4852 - val_loss: 14.7868 - val_mse: 14.7868 - val_mae: 1.6023 - lr: 3.8776e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 6s - loss: 11.6195 - mse: 11.6195 - mae: 1.4698 - val_loss: 14.8684 - val_mse: 14.8684 - val_mae: 1.4872 - lr: 3.8776e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 6s - loss: 11.4662 - mse: 11.4662 - mae: 1.4572 - val_loss: 14.9169 - val_mse: 14.9169 - val_mae: 1.5001 - lr: 3.8776e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 6s - loss: 11.4006 - mse: 11.4006 - mae: 1.4524 - val_loss: 14.6262 - val_mse: 14.6262 - val_mae: 1.4754 - lr: 3.8776e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 7s - loss: 11.3204 - mse: 11.3204 - mae: 1.4433 - val_loss: 14.6791 - val_mse: 14.6791 - val_mae: 1.4782 - lr: 3.8776e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 6s - loss: 11.0412 - mse: 11.0412 - mae: 1.4337 - val_loss: 14.7075 - val_mse: 14.7075 - val_mae: 1.4916 - lr: 3.8776e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 6s - loss: 10.7050 - mse: 10.7050 - mae: 1.4204 - val_loss: 14.4142 - val_mse: 14.4142 - val_mae: 1.5083 - lr: 3.8776e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 6s - loss: 10.6845 - mse: 10.6845 - mae: 1.4151 - val_loss: 15.0164 - val_mse: 15.0164 - val_mae: 1.5265 - lr: 3.8776e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 6s - loss: 10.6889 - mse: 10.6889 - mae: 1.4098 - val_loss: 14.5647 - val_mse: 14.5647 - val_mae: 1.4499 - lr: 3.8776e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 6s - loss: 10.2343 - mse: 10.2343 - mae: 1.3985 - val_loss: 14.2135 - val_mse: 14.2135 - val_mae: 1.4716 - lr: 3.8776e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 7s - loss: 10.2587 - mse: 10.2587 - mae: 1.3848 - val_loss: 14.8199 - val_mse: 14.8199 - val_mae: 1.4993 - lr: 3.8776e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 13/100\n",
            "1000/1000 - 6s - loss: 9.9164 - mse: 9.9164 - mae: 1.3772 - val_loss: 14.5740 - val_mse: 14.5740 - val_mae: 1.6291 - lr: 3.8776e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 14/100\n",
            "1000/1000 - 6s - loss: 9.8043 - mse: 9.8043 - mae: 1.3587 - val_loss: 14.8076 - val_mse: 14.8076 - val_mae: 1.5102 - lr: 3.8776e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 15/100\n",
            "1000/1000 - 6s - loss: 9.2464 - mse: 9.2464 - mae: 1.3464 - val_loss: 14.9943 - val_mse: 14.9943 - val_mae: 1.4994 - lr: 3.8776e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 16/100\n",
            "1000/1000 - 6s - loss: 9.1079 - mse: 9.1079 - mae: 1.3365 - val_loss: 14.6525 - val_mse: 14.6525 - val_mae: 1.5080 - lr: 3.8776e-04 - 6s/epoch - 6ms/step\n",
            "Score for fold 1: loss of 14.652517318725586\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 6s - loss: 9.4253 - mse: 9.4253 - mae: 1.3813 - val_loss: 14.0302 - val_mse: 14.0302 - val_mae: 1.3636 - lr: 3.8776e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 6s - loss: 9.0505 - mse: 9.0505 - mae: 1.3613 - val_loss: 14.2879 - val_mse: 14.2879 - val_mae: 1.3208 - lr: 3.8776e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 6s - loss: 8.9508 - mse: 8.9508 - mae: 1.3507 - val_loss: 14.9294 - val_mse: 14.9294 - val_mae: 1.3668 - lr: 3.8776e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 7s - loss: 8.3430 - mse: 8.3430 - mae: 1.3229 - val_loss: 14.1431 - val_mse: 14.1431 - val_mae: 1.3566 - lr: 3.8776e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 7s - loss: 8.4313 - mse: 8.4313 - mae: 1.3135 - val_loss: 15.2670 - val_mse: 15.2670 - val_mae: 1.5010 - lr: 3.8776e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 6s - loss: 8.0031 - mse: 8.0031 - mae: 1.3004 - val_loss: 14.3687 - val_mse: 14.3687 - val_mae: 1.3995 - lr: 3.8776e-04 - 6s/epoch - 6ms/step\n",
            "Score for fold 2: loss of 14.368653297424316\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 6s - loss: 10.5513 - mse: 10.5513 - mae: 1.3350 - val_loss: 4.8927 - val_mse: 4.8927 - val_mae: 1.2907 - lr: 3.8776e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 6s - loss: 10.7454 - mse: 10.7454 - mae: 1.3176 - val_loss: 4.8390 - val_mse: 4.8390 - val_mae: 1.3384 - lr: 3.8776e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 6s - loss: 10.0192 - mse: 10.0192 - mae: 1.3066 - val_loss: 5.2304 - val_mse: 5.2304 - val_mae: 1.3181 - lr: 3.8776e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 6s - loss: 9.7120 - mse: 9.7120 - mae: 1.2948 - val_loss: 5.0315 - val_mse: 5.0315 - val_mae: 1.4090 - lr: 3.8776e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 6s - loss: 9.6163 - mse: 9.6163 - mae: 1.2815 - val_loss: 5.5007 - val_mse: 5.5007 - val_mae: 1.2750 - lr: 3.8776e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 6s - loss: 9.2104 - mse: 9.2104 - mae: 1.2676 - val_loss: 5.8995 - val_mse: 5.8995 - val_mae: 1.2858 - lr: 3.8776e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 6s - loss: 9.0362 - mse: 9.0362 - mae: 1.2511 - val_loss: 5.6171 - val_mse: 5.6171 - val_mae: 1.2941 - lr: 3.8776e-04 - 6s/epoch - 6ms/step\n",
            "Score for fold 3: loss of 5.6171135902404785\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 7s - loss: 9.0771 - mse: 9.0771 - mae: 1.2856 - val_loss: 7.0313 - val_mse: 7.0313 - val_mae: 1.1890 - lr: 3.8776e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 6s - loss: 8.3254 - mse: 8.3254 - mae: 1.2603 - val_loss: 7.7828 - val_mse: 7.7828 - val_mae: 1.1575 - lr: 3.8776e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 6s - loss: 8.3815 - mse: 8.3815 - mae: 1.2468 - val_loss: 7.0883 - val_mse: 7.0883 - val_mae: 1.2111 - lr: 3.8776e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 6s - loss: 7.8701 - mse: 7.8701 - mae: 1.2303 - val_loss: 7.4878 - val_mse: 7.4878 - val_mae: 1.2029 - lr: 3.8776e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 6s - loss: 7.8713 - mse: 7.8713 - mae: 1.2201 - val_loss: 7.3503 - val_mse: 7.3503 - val_mae: 1.2115 - lr: 3.8776e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 6s - loss: 7.9389 - mse: 7.9389 - mae: 1.2081 - val_loss: 7.8330 - val_mse: 7.8330 - val_mae: 1.2407 - lr: 3.8776e-04 - 6s/epoch - 6ms/step\n",
            "Score for fold 4: loss of 7.833044528961182\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 6s - loss: 8.2481 - mse: 8.2481 - mae: 1.2315 - val_loss: 4.7125 - val_mse: 4.7125 - val_mae: 1.2035 - lr: 3.8776e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 6s - loss: 7.8485 - mse: 7.8485 - mae: 1.2111 - val_loss: 5.3683 - val_mse: 5.3683 - val_mae: 1.1183 - lr: 3.8776e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 6s - loss: 7.9651 - mse: 7.9651 - mae: 1.1922 - val_loss: 5.9694 - val_mse: 5.9694 - val_mae: 1.1601 - lr: 3.8776e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 7s - loss: 7.6434 - mse: 7.6434 - mae: 1.1763 - val_loss: 5.5409 - val_mse: 5.5409 - val_mae: 1.1920 - lr: 3.8776e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 7s - loss: 7.2800 - mse: 7.2800 - mae: 1.1622 - val_loss: 5.6200 - val_mse: 5.6200 - val_mae: 1.1889 - lr: 3.8776e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 7s - loss: 7.2853 - mse: 7.2853 - mae: 1.1519 - val_loss: 6.2935 - val_mse: 6.2935 - val_mae: 1.2864 - lr: 3.8776e-04 - 7s/epoch - 7ms/step\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-10-11 17:39:32,929]\u001b[0m Finished trial#40 resulted in value: 9.751999999999999. Current best value is 8.234 with parameters: {'activation': 'relu', 'num_hidden_layer': 4, 'i': 10, 'learning_rate': 0.00015658492778156685}.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Score for fold 5: loss of 6.293490409851074\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 12s - loss: 13.9842 - mse: 13.9842 - mae: 1.5498 - val_loss: 9.2737 - val_mse: 9.2737 - val_mae: 1.6268 - lr: 1.8401e-04 - 12s/epoch - 12ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 11s - loss: 13.2248 - mse: 13.2248 - mae: 1.5000 - val_loss: 9.5592 - val_mse: 9.5592 - val_mae: 1.4136 - lr: 1.8401e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 11s - loss: 12.8197 - mse: 12.8197 - mae: 1.4795 - val_loss: 9.3668 - val_mse: 9.3668 - val_mae: 1.4323 - lr: 1.8401e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 10s - loss: 12.6277 - mse: 12.6277 - mae: 1.4687 - val_loss: 9.9754 - val_mse: 9.9754 - val_mae: 1.4053 - lr: 1.8401e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 11s - loss: 12.6139 - mse: 12.6139 - mae: 1.4634 - val_loss: 9.3498 - val_mse: 9.3498 - val_mae: 1.4758 - lr: 1.8401e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 11s - loss: 12.5517 - mse: 12.5517 - mae: 1.4474 - val_loss: 9.3750 - val_mse: 9.3750 - val_mae: 1.5142 - lr: 1.8401e-04 - 11s/epoch - 11ms/step\n",
            "Score for fold 1: loss of 9.375041007995605\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 11s - loss: 12.3698 - mse: 12.3698 - mae: 1.4431 - val_loss: 7.5903 - val_mse: 7.5903 - val_mae: 1.4450 - lr: 1.8401e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 11s - loss: 12.2177 - mse: 12.2177 - mae: 1.4302 - val_loss: 7.9071 - val_mse: 7.9071 - val_mae: 1.4178 - lr: 1.8401e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 11s - loss: 12.1771 - mse: 12.1771 - mae: 1.4167 - val_loss: 8.0194 - val_mse: 8.0194 - val_mae: 1.4571 - lr: 1.8401e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 11s - loss: 12.0104 - mse: 12.0104 - mae: 1.4078 - val_loss: 7.8345 - val_mse: 7.8345 - val_mae: 1.4023 - lr: 1.8401e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 10s - loss: 11.9342 - mse: 11.9342 - mae: 1.3959 - val_loss: 7.8448 - val_mse: 7.8448 - val_mae: 1.4450 - lr: 1.8401e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 11s - loss: 11.4442 - mse: 11.4442 - mae: 1.3789 - val_loss: 8.1627 - val_mse: 8.1627 - val_mae: 1.4205 - lr: 1.8401e-04 - 11s/epoch - 11ms/step\n",
            "Score for fold 2: loss of 8.16273021697998\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 11s - loss: 11.0578 - mse: 11.0578 - mae: 1.3862 - val_loss: 8.4716 - val_mse: 8.4716 - val_mae: 1.4008 - lr: 1.8401e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 11s - loss: 10.8138 - mse: 10.8138 - mae: 1.3625 - val_loss: 8.6251 - val_mse: 8.6251 - val_mae: 1.4481 - lr: 1.8401e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 11s - loss: 10.2995 - mse: 10.2995 - mae: 1.3484 - val_loss: 8.7060 - val_mse: 8.7060 - val_mae: 1.3797 - lr: 1.8401e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 11s - loss: 10.2604 - mse: 10.2604 - mae: 1.3293 - val_loss: 8.6951 - val_mse: 8.6951 - val_mae: 1.4075 - lr: 1.8401e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 11s - loss: 9.9420 - mse: 9.9420 - mae: 1.3103 - val_loss: 8.7487 - val_mse: 8.7487 - val_mae: 1.3984 - lr: 1.8401e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 11s - loss: 9.6524 - mse: 9.6524 - mae: 1.2976 - val_loss: 9.0546 - val_mse: 9.0546 - val_mae: 1.3439 - lr: 1.8401e-04 - 11s/epoch - 11ms/step\n",
            "Score for fold 3: loss of 9.0546293258667\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 11s - loss: 8.5082 - mse: 8.5082 - mae: 1.3214 - val_loss: 12.0526 - val_mse: 12.0526 - val_mae: 1.2592 - lr: 1.8401e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 11s - loss: 8.1112 - mse: 8.1112 - mae: 1.2891 - val_loss: 15.3865 - val_mse: 15.3865 - val_mae: 1.2914 - lr: 1.8401e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 11s - loss: 7.8969 - mse: 7.8969 - mae: 1.2706 - val_loss: 12.8990 - val_mse: 12.8990 - val_mae: 1.3172 - lr: 1.8401e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 10s - loss: 7.7485 - mse: 7.7485 - mae: 1.2539 - val_loss: 12.8261 - val_mse: 12.8261 - val_mae: 1.6005 - lr: 1.8401e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 11s - loss: 7.4914 - mse: 7.4914 - mae: 1.2429 - val_loss: 13.3603 - val_mse: 13.3603 - val_mae: 1.2534 - lr: 1.8401e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 11s - loss: 7.0185 - mse: 7.0185 - mae: 1.2122 - val_loss: 12.2577 - val_mse: 12.2577 - val_mae: 1.3819 - lr: 1.8401e-04 - 11s/epoch - 11ms/step\n",
            "Score for fold 4: loss of 12.257647514343262\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 10s - loss: 7.6025 - mse: 7.6025 - mae: 1.2576 - val_loss: 11.6357 - val_mse: 11.6357 - val_mae: 1.2919 - lr: 1.8401e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 11s - loss: 7.2128 - mse: 7.2128 - mae: 1.2344 - val_loss: 12.0512 - val_mse: 12.0512 - val_mae: 1.2797 - lr: 1.8401e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 11s - loss: 6.4351 - mse: 6.4351 - mae: 1.2019 - val_loss: 11.9987 - val_mse: 11.9987 - val_mae: 1.2826 - lr: 1.8401e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 11s - loss: 6.2136 - mse: 6.2136 - mae: 1.1927 - val_loss: 12.0478 - val_mse: 12.0478 - val_mae: 1.2975 - lr: 1.8401e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 11s - loss: 6.1113 - mse: 6.1113 - mae: 1.1713 - val_loss: 12.4182 - val_mse: 12.4182 - val_mae: 1.1941 - lr: 1.8401e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 11s - loss: 6.0887 - mse: 6.0887 - mae: 1.1622 - val_loss: 12.6573 - val_mse: 12.6573 - val_mae: 1.2365 - lr: 1.8401e-04 - 11s/epoch - 11ms/step\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-10-11 17:45:06,379]\u001b[0m Finished trial#41 resulted in value: 10.302000000000001. Current best value is 8.234 with parameters: {'activation': 'relu', 'num_hidden_layer': 4, 'i': 10, 'learning_rate': 0.00015658492778156685}.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Score for fold 5: loss of 12.657275199890137\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 11s - loss: 14.0023 - mse: 14.0023 - mae: 1.5465 - val_loss: 9.0152 - val_mse: 9.0152 - val_mae: 1.5342 - lr: 2.1512e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 11s - loss: 13.1429 - mse: 13.1429 - mae: 1.4941 - val_loss: 8.9494 - val_mse: 8.9494 - val_mae: 1.4934 - lr: 2.1512e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 11s - loss: 12.8499 - mse: 12.8499 - mae: 1.4738 - val_loss: 9.0985 - val_mse: 9.0985 - val_mae: 1.5402 - lr: 2.1512e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 11s - loss: 12.8060 - mse: 12.8060 - mae: 1.4679 - val_loss: 9.4676 - val_mse: 9.4676 - val_mae: 1.5028 - lr: 2.1512e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 11s - loss: 12.7116 - mse: 12.7116 - mae: 1.4592 - val_loss: 9.1285 - val_mse: 9.1285 - val_mae: 1.4243 - lr: 2.1512e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 11s - loss: 12.4479 - mse: 12.4479 - mae: 1.4444 - val_loss: 9.2285 - val_mse: 9.2285 - val_mae: 1.5250 - lr: 2.1512e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 11s - loss: 12.3584 - mse: 12.3584 - mae: 1.4399 - val_loss: 9.1773 - val_mse: 9.1773 - val_mae: 1.5088 - lr: 2.1512e-04 - 11s/epoch - 11ms/step\n",
            "Score for fold 1: loss of 9.177288055419922\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 11s - loss: 11.3078 - mse: 11.3078 - mae: 1.4459 - val_loss: 12.5334 - val_mse: 12.5334 - val_mae: 1.3659 - lr: 2.1512e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 11s - loss: 11.0342 - mse: 11.0342 - mae: 1.4324 - val_loss: 12.8656 - val_mse: 12.8656 - val_mae: 1.4878 - lr: 2.1512e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 11s - loss: 10.8617 - mse: 10.8617 - mae: 1.4188 - val_loss: 12.7116 - val_mse: 12.7116 - val_mae: 1.4076 - lr: 2.1512e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 11s - loss: 10.5260 - mse: 10.5260 - mae: 1.3984 - val_loss: 13.1759 - val_mse: 13.1759 - val_mae: 1.4514 - lr: 2.1512e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 11s - loss: 10.2747 - mse: 10.2747 - mae: 1.3860 - val_loss: 12.8259 - val_mse: 12.8259 - val_mae: 1.4210 - lr: 2.1512e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 11s - loss: 10.2657 - mse: 10.2657 - mae: 1.3754 - val_loss: 13.7593 - val_mse: 13.7593 - val_mae: 1.6030 - lr: 2.1512e-04 - 11s/epoch - 11ms/step\n",
            "Score for fold 2: loss of 13.759254455566406\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 11s - loss: 10.9913 - mse: 10.9913 - mae: 1.3765 - val_loss: 8.0650 - val_mse: 8.0650 - val_mae: 1.4017 - lr: 2.1512e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 11s - loss: 10.6882 - mse: 10.6882 - mae: 1.3579 - val_loss: 14.2504 - val_mse: 14.2504 - val_mae: 1.4345 - lr: 2.1512e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 11s - loss: 10.3338 - mse: 10.3338 - mae: 1.3302 - val_loss: 9.0895 - val_mse: 9.0895 - val_mae: 1.3294 - lr: 2.1512e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 11s - loss: 9.9991 - mse: 9.9991 - mae: 1.3117 - val_loss: 8.0844 - val_mse: 8.0844 - val_mae: 1.4005 - lr: 2.1512e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 11s - loss: 9.8562 - mse: 9.8562 - mae: 1.2963 - val_loss: 10.2477 - val_mse: 10.2477 - val_mae: 1.3513 - lr: 2.1512e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 11s - loss: 9.5800 - mse: 9.5800 - mae: 1.2765 - val_loss: 8.6564 - val_mse: 8.6564 - val_mae: 1.4427 - lr: 2.1512e-04 - 11s/epoch - 11ms/step\n",
            "Score for fold 3: loss of 8.656437873840332\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 11s - loss: 7.9995 - mse: 7.9995 - mae: 1.3246 - val_loss: 14.9987 - val_mse: 14.9987 - val_mae: 1.3785 - lr: 2.1512e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 11s - loss: 8.2150 - mse: 8.2150 - mae: 1.2989 - val_loss: 15.3655 - val_mse: 15.3655 - val_mae: 1.2257 - lr: 2.1512e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 11s - loss: 7.3088 - mse: 7.3088 - mae: 1.2792 - val_loss: 14.7562 - val_mse: 14.7562 - val_mae: 1.2497 - lr: 2.1512e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 11s - loss: 6.8685 - mse: 6.8685 - mae: 1.2487 - val_loss: 15.7586 - val_mse: 15.7586 - val_mae: 1.3354 - lr: 2.1512e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 11s - loss: 6.6862 - mse: 6.6862 - mae: 1.2418 - val_loss: 15.5968 - val_mse: 15.5968 - val_mae: 1.5214 - lr: 2.1512e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 11s - loss: 6.3723 - mse: 6.3723 - mae: 1.2219 - val_loss: 15.8503 - val_mse: 15.8503 - val_mae: 1.2976 - lr: 2.1512e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 11s - loss: 6.2243 - mse: 6.2243 - mae: 1.2092 - val_loss: 16.0440 - val_mse: 16.0440 - val_mae: 1.2811 - lr: 2.1512e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 11s - loss: 6.1550 - mse: 6.1550 - mae: 1.1858 - val_loss: 15.6756 - val_mse: 15.6756 - val_mae: 1.3538 - lr: 2.1512e-04 - 11s/epoch - 11ms/step\n",
            "Score for fold 4: loss of 15.675566673278809\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 11s - loss: 8.7491 - mse: 8.7491 - mae: 1.2430 - val_loss: 5.0947 - val_mse: 5.0947 - val_mae: 1.0666 - lr: 2.1512e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 11s - loss: 8.1918 - mse: 8.1918 - mae: 1.2183 - val_loss: 5.7748 - val_mse: 5.7748 - val_mae: 1.3585 - lr: 2.1512e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 11s - loss: 7.9380 - mse: 7.9380 - mae: 1.2007 - val_loss: 5.4679 - val_mse: 5.4679 - val_mae: 1.0803 - lr: 2.1512e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 11s - loss: 7.8051 - mse: 7.8051 - mae: 1.1897 - val_loss: 5.5855 - val_mse: 5.5855 - val_mae: 1.1849 - lr: 2.1512e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 11s - loss: 7.5543 - mse: 7.5543 - mae: 1.1639 - val_loss: 5.8182 - val_mse: 5.8182 - val_mae: 1.1264 - lr: 2.1512e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 11s - loss: 6.8911 - mse: 6.8911 - mae: 1.1303 - val_loss: 5.8911 - val_mse: 5.8911 - val_mae: 1.1864 - lr: 2.1512e-04 - 11s/epoch - 11ms/step\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-10-11 17:51:11,713]\u001b[0m Finished trial#42 resulted in value: 10.634. Current best value is 8.234 with parameters: {'activation': 'relu', 'num_hidden_layer': 4, 'i': 10, 'learning_rate': 0.00015658492778156685}.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Score for fold 5: loss of 5.891063690185547\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 12s - loss: 14.3228 - mse: 14.3228 - mae: 1.5493 - val_loss: 8.7368 - val_mse: 8.7368 - val_mae: 1.6312 - lr: 1.6101e-04 - 12s/epoch - 12ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 10s - loss: 13.3852 - mse: 13.3852 - mae: 1.4921 - val_loss: 8.1942 - val_mse: 8.1942 - val_mae: 1.4498 - lr: 1.6101e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 11s - loss: 13.2868 - mse: 13.2868 - mae: 1.4736 - val_loss: 7.9738 - val_mse: 7.9738 - val_mae: 1.4730 - lr: 1.6101e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 11s - loss: 13.1577 - mse: 13.1577 - mae: 1.4688 - val_loss: 7.9384 - val_mse: 7.9384 - val_mae: 1.4538 - lr: 1.6101e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 11s - loss: 12.8756 - mse: 12.8756 - mae: 1.4581 - val_loss: 8.1063 - val_mse: 8.1063 - val_mae: 1.3935 - lr: 1.6101e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 11s - loss: 12.6129 - mse: 12.6129 - mae: 1.4449 - val_loss: 8.0374 - val_mse: 8.0374 - val_mae: 1.4628 - lr: 1.6101e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 11s - loss: 12.6507 - mse: 12.6507 - mae: 1.4382 - val_loss: 7.8741 - val_mse: 7.8741 - val_mae: 1.4548 - lr: 1.6101e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 11s - loss: 12.4636 - mse: 12.4636 - mae: 1.4291 - val_loss: 8.0623 - val_mse: 8.0623 - val_mae: 1.4511 - lr: 1.6101e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 11s - loss: 11.9784 - mse: 11.9784 - mae: 1.4112 - val_loss: 7.8514 - val_mse: 7.8514 - val_mae: 1.4252 - lr: 1.6101e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 10s - loss: 11.9906 - mse: 11.9906 - mae: 1.3998 - val_loss: 8.2867 - val_mse: 8.2867 - val_mae: 1.4880 - lr: 1.6101e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 10s - loss: 11.6160 - mse: 11.6160 - mae: 1.3864 - val_loss: 8.1900 - val_mse: 8.1900 - val_mae: 1.4280 - lr: 1.6101e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 11s - loss: 11.5455 - mse: 11.5455 - mae: 1.3709 - val_loss: 8.1138 - val_mse: 8.1138 - val_mae: 1.4551 - lr: 1.6101e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 13/100\n",
            "1000/1000 - 11s - loss: 11.4203 - mse: 11.4203 - mae: 1.3503 - val_loss: 8.2799 - val_mse: 8.2799 - val_mae: 1.4580 - lr: 1.6101e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 14/100\n",
            "1000/1000 - 11s - loss: 10.7685 - mse: 10.7685 - mae: 1.3289 - val_loss: 8.8000 - val_mse: 8.8000 - val_mae: 1.5116 - lr: 1.6101e-04 - 11s/epoch - 11ms/step\n",
            "Score for fold 1: loss of 8.800013542175293\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 11s - loss: 8.4248 - mse: 8.4248 - mae: 1.3658 - val_loss: 17.7832 - val_mse: 17.7832 - val_mae: 1.4301 - lr: 1.6101e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 11s - loss: 8.1679 - mse: 8.1679 - mae: 1.3481 - val_loss: 17.8072 - val_mse: 17.8072 - val_mae: 1.3259 - lr: 1.6101e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 11s - loss: 7.8661 - mse: 7.8661 - mae: 1.3250 - val_loss: 17.7093 - val_mse: 17.7093 - val_mae: 1.5170 - lr: 1.6101e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 11s - loss: 7.5528 - mse: 7.5528 - mae: 1.3056 - val_loss: 17.8333 - val_mse: 17.8333 - val_mae: 1.4619 - lr: 1.6101e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 11s - loss: 7.2450 - mse: 7.2450 - mae: 1.2795 - val_loss: 17.9460 - val_mse: 17.9460 - val_mae: 1.4343 - lr: 1.6101e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 11s - loss: 6.8064 - mse: 6.8064 - mae: 1.2553 - val_loss: 17.8854 - val_mse: 17.8854 - val_mae: 1.4100 - lr: 1.6101e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 11s - loss: 7.3821 - mse: 7.3821 - mae: 1.2447 - val_loss: 18.0413 - val_mse: 18.0413 - val_mae: 1.4344 - lr: 1.6101e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 11s - loss: 6.2735 - mse: 6.2735 - mae: 1.2120 - val_loss: 18.4039 - val_mse: 18.4039 - val_mae: 1.3672 - lr: 1.6101e-04 - 11s/epoch - 11ms/step\n",
            "Score for fold 2: loss of 18.403879165649414\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 11s - loss: 8.9520 - mse: 8.9520 - mae: 1.2624 - val_loss: 7.5908 - val_mse: 7.5908 - val_mae: 1.1903 - lr: 1.6101e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 11s - loss: 8.5226 - mse: 8.5226 - mae: 1.2326 - val_loss: 8.1962 - val_mse: 8.1962 - val_mae: 1.2259 - lr: 1.6101e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 11s - loss: 8.2568 - mse: 8.2568 - mae: 1.2179 - val_loss: 8.5883 - val_mse: 8.5883 - val_mae: 1.2452 - lr: 1.6101e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 11s - loss: 7.8354 - mse: 7.8354 - mae: 1.1918 - val_loss: 9.1583 - val_mse: 9.1583 - val_mae: 1.2291 - lr: 1.6101e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 11s - loss: 7.6293 - mse: 7.6293 - mae: 1.1735 - val_loss: 9.3146 - val_mse: 9.3146 - val_mae: 1.2353 - lr: 1.6101e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 11s - loss: 7.3030 - mse: 7.3030 - mae: 1.1526 - val_loss: 9.1922 - val_mse: 9.1922 - val_mae: 1.2913 - lr: 1.6101e-04 - 11s/epoch - 11ms/step\n",
            "Score for fold 3: loss of 9.192166328430176\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 11s - loss: 8.4950 - mse: 8.4950 - mae: 1.1938 - val_loss: 4.4790 - val_mse: 4.4790 - val_mae: 1.2648 - lr: 1.6101e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 11s - loss: 8.1170 - mse: 8.1170 - mae: 1.1585 - val_loss: 4.4217 - val_mse: 4.4217 - val_mae: 1.2180 - lr: 1.6101e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 11s - loss: 7.8410 - mse: 7.8410 - mae: 1.1468 - val_loss: 4.1119 - val_mse: 4.1119 - val_mae: 1.2954 - lr: 1.6101e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 11s - loss: 7.3782 - mse: 7.3782 - mae: 1.1219 - val_loss: 4.1930 - val_mse: 4.1930 - val_mae: 1.2495 - lr: 1.6101e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 11s - loss: 6.9887 - mse: 6.9887 - mae: 1.0997 - val_loss: 5.0760 - val_mse: 5.0760 - val_mae: 1.1946 - lr: 1.6101e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 11s - loss: 6.6742 - mse: 6.6742 - mae: 1.0821 - val_loss: 4.7319 - val_mse: 4.7319 - val_mae: 1.1957 - lr: 1.6101e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 10s - loss: 6.7151 - mse: 6.7151 - mae: 1.0672 - val_loss: 4.6924 - val_mse: 4.6924 - val_mae: 1.1850 - lr: 1.6101e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 11s - loss: 6.3966 - mse: 6.3966 - mae: 1.0520 - val_loss: 4.8891 - val_mse: 4.8891 - val_mae: 1.2270 - lr: 1.6101e-04 - 11s/epoch - 11ms/step\n",
            "Score for fold 4: loss of 4.889101028442383\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 11s - loss: 6.7717 - mse: 6.7717 - mae: 1.1075 - val_loss: 4.7652 - val_mse: 4.7652 - val_mae: 1.0876 - lr: 1.6101e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 11s - loss: 5.9558 - mse: 5.9558 - mae: 1.0788 - val_loss: 5.1405 - val_mse: 5.1405 - val_mae: 1.0204 - lr: 1.6101e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 11s - loss: 5.7868 - mse: 5.7868 - mae: 1.0571 - val_loss: 4.8746 - val_mse: 4.8746 - val_mae: 1.0063 - lr: 1.6101e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 11s - loss: 5.3836 - mse: 5.3836 - mae: 1.0407 - val_loss: 5.5367 - val_mse: 5.5367 - val_mae: 1.1192 - lr: 1.6101e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 11s - loss: 5.3579 - mse: 5.3579 - mae: 1.0291 - val_loss: 5.4514 - val_mse: 5.4514 - val_mae: 1.0869 - lr: 1.6101e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 11s - loss: 5.1206 - mse: 5.1206 - mae: 1.0080 - val_loss: 5.1749 - val_mse: 5.1749 - val_mae: 1.0442 - lr: 1.6101e-04 - 11s/epoch - 11ms/step\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-10-11 17:58:54,085]\u001b[0m Finished trial#43 resulted in value: 9.290000000000001. Current best value is 8.234 with parameters: {'activation': 'relu', 'num_hidden_layer': 4, 'i': 10, 'learning_rate': 0.00015658492778156685}.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Score for fold 5: loss of 5.174938678741455\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 3s - loss: 15.0654 - mse: 15.0654 - mae: 1.5873 - val_loss: 11.1845 - val_mse: 11.1845 - val_mae: 1.5032 - lr: 1.2987e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 3s - loss: 13.4644 - mse: 13.4644 - mae: 1.5193 - val_loss: 10.9647 - val_mse: 10.9647 - val_mae: 1.4767 - lr: 1.2987e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 3s - loss: 13.2601 - mse: 13.2601 - mae: 1.4983 - val_loss: 10.6566 - val_mse: 10.6566 - val_mae: 1.5021 - lr: 1.2987e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 3s - loss: 13.0429 - mse: 13.0429 - mae: 1.4924 - val_loss: 10.5419 - val_mse: 10.5419 - val_mae: 1.4636 - lr: 1.2987e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 3s - loss: 12.8993 - mse: 12.8993 - mae: 1.4835 - val_loss: 10.4174 - val_mse: 10.4174 - val_mae: 1.4870 - lr: 1.2987e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 3s - loss: 12.8344 - mse: 12.8344 - mae: 1.4833 - val_loss: 10.2600 - val_mse: 10.2600 - val_mae: 1.4720 - lr: 1.2987e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 3s - loss: 12.7430 - mse: 12.7430 - mae: 1.4738 - val_loss: 10.2144 - val_mse: 10.2144 - val_mae: 1.4666 - lr: 1.2987e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 3s - loss: 12.7320 - mse: 12.7320 - mae: 1.4725 - val_loss: 10.2107 - val_mse: 10.2107 - val_mae: 1.4369 - lr: 1.2987e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 3s - loss: 12.6456 - mse: 12.6456 - mae: 1.4652 - val_loss: 10.1417 - val_mse: 10.1417 - val_mae: 1.4603 - lr: 1.2987e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 3s - loss: 12.5837 - mse: 12.5837 - mae: 1.4664 - val_loss: 10.1013 - val_mse: 10.1013 - val_mae: 1.4745 - lr: 1.2987e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 3s - loss: 12.5580 - mse: 12.5580 - mae: 1.4616 - val_loss: 10.0586 - val_mse: 10.0586 - val_mae: 1.4394 - lr: 1.2987e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 3s - loss: 12.5164 - mse: 12.5164 - mae: 1.4563 - val_loss: 9.9182 - val_mse: 9.9182 - val_mae: 1.4829 - lr: 1.2987e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 13/100\n",
            "1000/1000 - 3s - loss: 12.4410 - mse: 12.4410 - mae: 1.4560 - val_loss: 9.9585 - val_mse: 9.9585 - val_mae: 1.4500 - lr: 1.2987e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 14/100\n",
            "1000/1000 - 3s - loss: 12.4337 - mse: 12.4337 - mae: 1.4552 - val_loss: 10.0024 - val_mse: 10.0024 - val_mae: 1.4415 - lr: 1.2987e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 15/100\n",
            "1000/1000 - 3s - loss: 12.3472 - mse: 12.3472 - mae: 1.4483 - val_loss: 10.0538 - val_mse: 10.0538 - val_mae: 1.4356 - lr: 1.2987e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 16/100\n",
            "1000/1000 - 3s - loss: 12.3596 - mse: 12.3596 - mae: 1.4481 - val_loss: 9.9310 - val_mse: 9.9310 - val_mae: 1.4410 - lr: 1.2987e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 17/100\n",
            "1000/1000 - 3s - loss: 12.3385 - mse: 12.3385 - mae: 1.4473 - val_loss: 9.9136 - val_mse: 9.9136 - val_mae: 1.4347 - lr: 1.2987e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 18/100\n",
            "1000/1000 - 3s - loss: 12.2984 - mse: 12.2984 - mae: 1.4441 - val_loss: 9.9630 - val_mse: 9.9630 - val_mae: 1.4193 - lr: 1.2987e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 19/100\n",
            "1000/1000 - 3s - loss: 12.2850 - mse: 12.2850 - mae: 1.4430 - val_loss: 9.8989 - val_mse: 9.8989 - val_mae: 1.4303 - lr: 1.2987e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 20/100\n",
            "1000/1000 - 3s - loss: 12.2954 - mse: 12.2954 - mae: 1.4414 - val_loss: 9.8436 - val_mse: 9.8436 - val_mae: 1.4582 - lr: 1.2987e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 21/100\n",
            "1000/1000 - 3s - loss: 12.2422 - mse: 12.2422 - mae: 1.4393 - val_loss: 9.8843 - val_mse: 9.8843 - val_mae: 1.4441 - lr: 1.2987e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 22/100\n",
            "1000/1000 - 3s - loss: 12.2401 - mse: 12.2401 - mae: 1.4384 - val_loss: 9.9083 - val_mse: 9.9083 - val_mae: 1.4460 - lr: 1.2987e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 23/100\n",
            "1000/1000 - 3s - loss: 12.2182 - mse: 12.2182 - mae: 1.4383 - val_loss: 9.9493 - val_mse: 9.9493 - val_mae: 1.4226 - lr: 1.2987e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 24/100\n",
            "1000/1000 - 3s - loss: 12.2178 - mse: 12.2178 - mae: 1.4351 - val_loss: 9.8753 - val_mse: 9.8753 - val_mae: 1.4271 - lr: 1.2987e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 25/100\n",
            "1000/1000 - 3s - loss: 12.1515 - mse: 12.1515 - mae: 1.4343 - val_loss: 9.8026 - val_mse: 9.8026 - val_mae: 1.4532 - lr: 1.2987e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 26/100\n",
            "1000/1000 - 3s - loss: 12.1237 - mse: 12.1237 - mae: 1.4319 - val_loss: 9.9541 - val_mse: 9.9541 - val_mae: 1.4319 - lr: 1.2987e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 27/100\n",
            "1000/1000 - 3s - loss: 12.1554 - mse: 12.1554 - mae: 1.4331 - val_loss: 9.8420 - val_mse: 9.8420 - val_mae: 1.4221 - lr: 1.2987e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 28/100\n",
            "1000/1000 - 3s - loss: 12.1031 - mse: 12.1031 - mae: 1.4299 - val_loss: 10.0807 - val_mse: 10.0807 - val_mae: 1.4430 - lr: 1.2987e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 29/100\n",
            "1000/1000 - 3s - loss: 12.1014 - mse: 12.1014 - mae: 1.4299 - val_loss: 9.8480 - val_mse: 9.8480 - val_mae: 1.4232 - lr: 1.2987e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 30/100\n",
            "1000/1000 - 3s - loss: 12.0959 - mse: 12.0959 - mae: 1.4286 - val_loss: 9.8648 - val_mse: 9.8648 - val_mae: 1.4352 - lr: 1.2987e-04 - 3s/epoch - 3ms/step\n",
            "Score for fold 1: loss of 9.864812850952148\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 3s - loss: 12.1141 - mse: 12.1141 - mae: 1.4247 - val_loss: 9.7664 - val_mse: 9.7664 - val_mae: 1.3996 - lr: 1.2987e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 3s - loss: 12.0910 - mse: 12.0910 - mae: 1.4279 - val_loss: 9.7440 - val_mse: 9.7440 - val_mae: 1.3920 - lr: 1.2987e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 3s - loss: 12.0644 - mse: 12.0644 - mae: 1.4231 - val_loss: 9.6871 - val_mse: 9.6871 - val_mae: 1.4522 - lr: 1.2987e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 3s - loss: 12.0498 - mse: 12.0498 - mae: 1.4237 - val_loss: 9.8708 - val_mse: 9.8708 - val_mae: 1.4251 - lr: 1.2987e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 3s - loss: 12.0190 - mse: 12.0190 - mae: 1.4218 - val_loss: 9.9534 - val_mse: 9.9534 - val_mae: 1.4278 - lr: 1.2987e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 3s - loss: 12.0361 - mse: 12.0361 - mae: 1.4223 - val_loss: 9.8887 - val_mse: 9.8887 - val_mae: 1.4008 - lr: 1.2987e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 3s - loss: 11.9978 - mse: 11.9978 - mae: 1.4196 - val_loss: 9.9842 - val_mse: 9.9842 - val_mae: 1.4128 - lr: 1.2987e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 3s - loss: 12.0108 - mse: 12.0108 - mae: 1.4185 - val_loss: 9.7882 - val_mse: 9.7882 - val_mae: 1.4172 - lr: 1.2987e-04 - 3s/epoch - 3ms/step\n",
            "Score for fold 2: loss of 9.788162231445312\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 3s - loss: 11.3782 - mse: 11.3782 - mae: 1.4209 - val_loss: 12.0947 - val_mse: 12.0947 - val_mae: 1.4073 - lr: 1.2987e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 3s - loss: 11.3812 - mse: 11.3812 - mae: 1.4198 - val_loss: 12.1260 - val_mse: 12.1260 - val_mae: 1.4355 - lr: 1.2987e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 3s - loss: 11.3200 - mse: 11.3200 - mae: 1.4174 - val_loss: 12.1496 - val_mse: 12.1496 - val_mae: 1.4400 - lr: 1.2987e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 3s - loss: 11.2925 - mse: 11.2925 - mae: 1.4174 - val_loss: 12.2037 - val_mse: 12.2037 - val_mae: 1.3858 - lr: 1.2987e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 3s - loss: 11.4023 - mse: 11.4023 - mae: 1.4156 - val_loss: 12.1351 - val_mse: 12.1351 - val_mae: 1.4405 - lr: 1.2987e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 3s - loss: 11.3265 - mse: 11.3265 - mae: 1.4139 - val_loss: 12.2328 - val_mse: 12.2328 - val_mae: 1.4657 - lr: 1.2987e-04 - 3s/epoch - 3ms/step\n",
            "Score for fold 3: loss of 12.232847213745117\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 3s - loss: 10.2777 - mse: 10.2777 - mae: 1.4090 - val_loss: 16.2990 - val_mse: 16.2990 - val_mae: 1.4223 - lr: 1.2987e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 3s - loss: 10.2075 - mse: 10.2075 - mae: 1.4074 - val_loss: 16.3421 - val_mse: 16.3421 - val_mae: 1.4670 - lr: 1.2987e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 3s - loss: 10.1880 - mse: 10.1880 - mae: 1.4030 - val_loss: 16.4116 - val_mse: 16.4116 - val_mae: 1.4343 - lr: 1.2987e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 3s - loss: 10.1832 - mse: 10.1832 - mae: 1.4049 - val_loss: 16.3808 - val_mse: 16.3808 - val_mae: 1.4511 - lr: 1.2987e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 3s - loss: 10.1654 - mse: 10.1654 - mae: 1.4029 - val_loss: 16.3475 - val_mse: 16.3475 - val_mae: 1.4501 - lr: 1.2987e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 3s - loss: 10.1593 - mse: 10.1593 - mae: 1.4020 - val_loss: 16.3618 - val_mse: 16.3618 - val_mae: 1.4400 - lr: 1.2987e-04 - 3s/epoch - 3ms/step\n",
            "Score for fold 4: loss of 16.361801147460938\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 3s - loss: 11.8538 - mse: 11.8538 - mae: 1.4196 - val_loss: 9.6957 - val_mse: 9.6957 - val_mae: 1.3665 - lr: 1.2987e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 3s - loss: 11.8258 - mse: 11.8258 - mae: 1.4180 - val_loss: 9.9060 - val_mse: 9.9060 - val_mae: 1.3728 - lr: 1.2987e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 3s - loss: 11.8536 - mse: 11.8536 - mae: 1.4160 - val_loss: 9.7736 - val_mse: 9.7736 - val_mae: 1.4106 - lr: 1.2987e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 3s - loss: 11.7790 - mse: 11.7790 - mae: 1.4141 - val_loss: 9.7602 - val_mse: 9.7602 - val_mae: 1.3965 - lr: 1.2987e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 3s - loss: 11.7426 - mse: 11.7426 - mae: 1.4146 - val_loss: 9.8093 - val_mse: 9.8093 - val_mae: 1.4144 - lr: 1.2987e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 3s - loss: 11.7694 - mse: 11.7694 - mae: 1.4142 - val_loss: 9.7578 - val_mse: 9.7578 - val_mae: 1.3950 - lr: 1.2987e-04 - 3s/epoch - 3ms/step\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-10-11 18:01:28,082]\u001b[0m Finished trial#44 resulted in value: 11.599999999999998. Current best value is 8.234 with parameters: {'activation': 'relu', 'num_hidden_layer': 4, 'i': 10, 'learning_rate': 0.00015658492778156685}.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Score for fold 5: loss of 9.757765769958496\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 12s - loss: 14.2143 - mse: 14.2143 - mae: 1.5491 - val_loss: 9.0466 - val_mse: 9.0466 - val_mae: 1.4156 - lr: 1.0093e-04 - 12s/epoch - 12ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 11s - loss: 13.2587 - mse: 13.2587 - mae: 1.4929 - val_loss: 8.7358 - val_mse: 8.7358 - val_mae: 1.4104 - lr: 1.0093e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 10s - loss: 13.1126 - mse: 13.1126 - mae: 1.4719 - val_loss: 8.5519 - val_mse: 8.5519 - val_mae: 1.4713 - lr: 1.0093e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 11s - loss: 13.0793 - mse: 13.0793 - mae: 1.4620 - val_loss: 8.2208 - val_mse: 8.2208 - val_mae: 1.4996 - lr: 1.0093e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 11s - loss: 12.7535 - mse: 12.7535 - mae: 1.4540 - val_loss: 8.2642 - val_mse: 8.2642 - val_mae: 1.4434 - lr: 1.0093e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 10s - loss: 12.6818 - mse: 12.6818 - mae: 1.4432 - val_loss: 8.2466 - val_mse: 8.2466 - val_mae: 1.4412 - lr: 1.0093e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 11s - loss: 12.4938 - mse: 12.4938 - mae: 1.4340 - val_loss: 8.3308 - val_mse: 8.3308 - val_mae: 1.4477 - lr: 1.0093e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 11s - loss: 12.3070 - mse: 12.3070 - mae: 1.4237 - val_loss: 8.2721 - val_mse: 8.2721 - val_mae: 1.4754 - lr: 1.0093e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 11s - loss: 12.2182 - mse: 12.2182 - mae: 1.4156 - val_loss: 8.4230 - val_mse: 8.4230 - val_mae: 1.4221 - lr: 1.0093e-04 - 11s/epoch - 11ms/step\n",
            "Score for fold 1: loss of 8.423009872436523\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 11s - loss: 10.8347 - mse: 10.8347 - mae: 1.4211 - val_loss: 13.4920 - val_mse: 13.4920 - val_mae: 1.4437 - lr: 1.0093e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 11s - loss: 10.6421 - mse: 10.6421 - mae: 1.4110 - val_loss: 13.2659 - val_mse: 13.2659 - val_mae: 1.3740 - lr: 1.0093e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 11s - loss: 10.3109 - mse: 10.3109 - mae: 1.3923 - val_loss: 13.9791 - val_mse: 13.9791 - val_mae: 1.4005 - lr: 1.0093e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 11s - loss: 10.0946 - mse: 10.0946 - mae: 1.3763 - val_loss: 14.0969 - val_mse: 14.0969 - val_mae: 1.4994 - lr: 1.0093e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 11s - loss: 9.9514 - mse: 9.9514 - mae: 1.3677 - val_loss: 13.8418 - val_mse: 13.8418 - val_mae: 1.4329 - lr: 1.0093e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 11s - loss: 9.5808 - mse: 9.5808 - mae: 1.3481 - val_loss: 14.1208 - val_mse: 14.1208 - val_mae: 1.4133 - lr: 1.0093e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 11s - loss: 9.3950 - mse: 9.3950 - mae: 1.3277 - val_loss: 14.1490 - val_mse: 14.1490 - val_mae: 1.4984 - lr: 1.0093e-04 - 11s/epoch - 11ms/step\n",
            "Score for fold 2: loss of 14.14902114868164\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 11s - loss: 10.5744 - mse: 10.5744 - mae: 1.3493 - val_loss: 8.3851 - val_mse: 8.3851 - val_mae: 1.3478 - lr: 1.0093e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 11s - loss: 10.4634 - mse: 10.4634 - mae: 1.3360 - val_loss: 8.1462 - val_mse: 8.1462 - val_mae: 1.3784 - lr: 1.0093e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 11s - loss: 10.0792 - mse: 10.0792 - mae: 1.3120 - val_loss: 8.6496 - val_mse: 8.6496 - val_mae: 1.4329 - lr: 1.0093e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 11s - loss: 9.7921 - mse: 9.7921 - mae: 1.2914 - val_loss: 8.9644 - val_mse: 8.9644 - val_mae: 1.3186 - lr: 1.0093e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 11s - loss: 9.6138 - mse: 9.6138 - mae: 1.2721 - val_loss: 8.3351 - val_mse: 8.3351 - val_mae: 1.3119 - lr: 1.0093e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 11s - loss: 9.2615 - mse: 9.2615 - mae: 1.2518 - val_loss: 8.3318 - val_mse: 8.3318 - val_mae: 1.3436 - lr: 1.0093e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 11s - loss: 8.9334 - mse: 8.9334 - mae: 1.2284 - val_loss: 8.4931 - val_mse: 8.4931 - val_mae: 1.3256 - lr: 1.0093e-04 - 11s/epoch - 11ms/step\n",
            "Score for fold 3: loss of 8.49312686920166\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 11s - loss: 7.5275 - mse: 7.5275 - mae: 1.2602 - val_loss: 12.9193 - val_mse: 12.9193 - val_mae: 1.1865 - lr: 1.0093e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 11s - loss: 7.1010 - mse: 7.1010 - mae: 1.2357 - val_loss: 13.6245 - val_mse: 13.6245 - val_mae: 1.2677 - lr: 1.0093e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 11s - loss: 7.0422 - mse: 7.0422 - mae: 1.2153 - val_loss: 13.7197 - val_mse: 13.7197 - val_mae: 1.2838 - lr: 1.0093e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 11s - loss: 6.4223 - mse: 6.4223 - mae: 1.1883 - val_loss: 14.1620 - val_mse: 14.1620 - val_mae: 1.2515 - lr: 1.0093e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 11s - loss: 6.4301 - mse: 6.4301 - mae: 1.1654 - val_loss: 14.3796 - val_mse: 14.3796 - val_mae: 1.2742 - lr: 1.0093e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 11s - loss: 5.9735 - mse: 5.9735 - mae: 1.1454 - val_loss: 13.9081 - val_mse: 13.9081 - val_mae: 1.2235 - lr: 1.0093e-04 - 11s/epoch - 11ms/step\n",
            "Score for fold 4: loss of 13.90810775756836\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 11s - loss: 8.5390 - mse: 8.5390 - mae: 1.1812 - val_loss: 4.8183 - val_mse: 4.8183 - val_mae: 1.0877 - lr: 1.0093e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 11s - loss: 7.6882 - mse: 7.6882 - mae: 1.1548 - val_loss: 5.6057 - val_mse: 5.6057 - val_mae: 1.1428 - lr: 1.0093e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 11s - loss: 7.5495 - mse: 7.5495 - mae: 1.1296 - val_loss: 4.7896 - val_mse: 4.7896 - val_mae: 1.1610 - lr: 1.0093e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 11s - loss: 7.2191 - mse: 7.2191 - mae: 1.1072 - val_loss: 5.7759 - val_mse: 5.7759 - val_mae: 1.1713 - lr: 1.0093e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 11s - loss: 7.0811 - mse: 7.0811 - mae: 1.0884 - val_loss: 5.1080 - val_mse: 5.1080 - val_mae: 1.1347 - lr: 1.0093e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 11s - loss: 6.5689 - mse: 6.5689 - mae: 1.0637 - val_loss: 5.2641 - val_mse: 5.2641 - val_mae: 1.1481 - lr: 1.0093e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 11s - loss: 6.5491 - mse: 6.5491 - mae: 1.0496 - val_loss: 7.3246 - val_mse: 7.3246 - val_mae: 1.1549 - lr: 1.0093e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 11s - loss: 6.3662 - mse: 6.3662 - mae: 1.0344 - val_loss: 5.5801 - val_mse: 5.5801 - val_mae: 1.1818 - lr: 1.0093e-04 - 11s/epoch - 11ms/step\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-10-11 18:08:23,740]\u001b[0m Finished trial#45 resulted in value: 10.11. Current best value is 8.234 with parameters: {'activation': 'relu', 'num_hidden_layer': 4, 'i': 10, 'learning_rate': 0.00015658492778156685}.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Score for fold 5: loss of 5.5800557136535645\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 11s - loss: 12.5566 - mse: 12.5566 - mae: 1.6466 - val_loss: 21.3538 - val_mse: 21.3538 - val_mae: 1.5642 - lr: 1.5736e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 10s - loss: 11.5243 - mse: 11.5243 - mae: 1.5727 - val_loss: 21.0016 - val_mse: 21.0016 - val_mae: 1.5900 - lr: 1.5736e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 10s - loss: 11.4936 - mse: 11.4936 - mae: 1.5568 - val_loss: 21.2818 - val_mse: 21.2818 - val_mae: 1.5919 - lr: 1.5736e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 10s - loss: 11.5853 - mse: 11.5853 - mae: 1.5525 - val_loss: 21.0827 - val_mse: 21.0827 - val_mae: 1.5900 - lr: 1.5736e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 10s - loss: 11.3335 - mse: 11.3335 - mae: 1.5479 - val_loss: 22.0633 - val_mse: 22.0633 - val_mae: 1.7403 - lr: 1.5736e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 10s - loss: 11.3805 - mse: 11.3805 - mae: 1.5500 - val_loss: 21.0121 - val_mse: 21.0121 - val_mae: 1.5820 - lr: 1.5736e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 10s - loss: 11.4036 - mse: 11.4036 - mae: 1.5491 - val_loss: 20.9546 - val_mse: 20.9546 - val_mae: 1.5402 - lr: 1.5736e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 10s - loss: 11.4541 - mse: 11.4541 - mae: 1.5496 - val_loss: 21.0184 - val_mse: 21.0184 - val_mae: 1.5490 - lr: 1.5736e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 10s - loss: 11.3239 - mse: 11.3239 - mae: 1.5506 - val_loss: 21.4284 - val_mse: 21.4284 - val_mae: 1.5930 - lr: 1.5736e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 9s - loss: 11.2482 - mse: 11.2482 - mae: 1.5426 - val_loss: 21.0879 - val_mse: 21.0879 - val_mae: 1.5699 - lr: 1.5736e-04 - 9s/epoch - 9ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 10s - loss: 11.2931 - mse: 11.2931 - mae: 1.5451 - val_loss: 20.8234 - val_mse: 20.8234 - val_mae: 1.5390 - lr: 1.5736e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 10s - loss: 11.3040 - mse: 11.3040 - mae: 1.5472 - val_loss: 21.1026 - val_mse: 21.1026 - val_mae: 1.5066 - lr: 1.5736e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 13/100\n",
            "1000/1000 - 10s - loss: 11.3047 - mse: 11.3047 - mae: 1.5419 - val_loss: 20.8467 - val_mse: 20.8467 - val_mae: 1.5641 - lr: 1.5736e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 14/100\n",
            "1000/1000 - 10s - loss: 11.3309 - mse: 11.3309 - mae: 1.5455 - val_loss: 20.9333 - val_mse: 20.9333 - val_mae: 1.5358 - lr: 1.5736e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 15/100\n",
            "1000/1000 - 10s - loss: 11.3193 - mse: 11.3193 - mae: 1.5470 - val_loss: 21.1315 - val_mse: 21.1315 - val_mae: 1.5830 - lr: 1.5736e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 16/100\n",
            "1000/1000 - 10s - loss: 11.3481 - mse: 11.3481 - mae: 1.5412 - val_loss: 20.9482 - val_mse: 20.9482 - val_mae: 1.5784 - lr: 1.5736e-04 - 10s/epoch - 10ms/step\n",
            "Score for fold 1: loss of 20.948150634765625\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 10s - loss: 14.1471 - mse: 14.1471 - mae: 1.5496 - val_loss: 9.6628 - val_mse: 9.6628 - val_mae: 1.5425 - lr: 1.5736e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 10s - loss: 14.1385 - mse: 14.1385 - mae: 1.5556 - val_loss: 9.5820 - val_mse: 9.5820 - val_mae: 1.5184 - lr: 1.5736e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 10s - loss: 14.1130 - mse: 14.1130 - mae: 1.5502 - val_loss: 9.5693 - val_mse: 9.5693 - val_mae: 1.5732 - lr: 1.5736e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 10s - loss: 14.1566 - mse: 14.1566 - mae: 1.5505 - val_loss: 9.6966 - val_mse: 9.6966 - val_mae: 1.6132 - lr: 1.5736e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 10s - loss: 14.1377 - mse: 14.1377 - mae: 1.5537 - val_loss: 9.6029 - val_mse: 9.6029 - val_mae: 1.5618 - lr: 1.5736e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 10s - loss: 14.1113 - mse: 14.1113 - mae: 1.5532 - val_loss: 9.6465 - val_mse: 9.6465 - val_mae: 1.5201 - lr: 1.5736e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 10s - loss: 14.0973 - mse: 14.0973 - mae: 1.5541 - val_loss: 9.5216 - val_mse: 9.5216 - val_mae: 1.5627 - lr: 1.5736e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 10s - loss: 14.1633 - mse: 14.1633 - mae: 1.5519 - val_loss: 9.6395 - val_mse: 9.6395 - val_mae: 1.5960 - lr: 1.5736e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 10s - loss: 14.2436 - mse: 14.2436 - mae: 1.5528 - val_loss: 9.5529 - val_mse: 9.5529 - val_mae: 1.5204 - lr: 1.5736e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 10s - loss: 14.1245 - mse: 14.1245 - mae: 1.5532 - val_loss: 9.6047 - val_mse: 9.6047 - val_mae: 1.5296 - lr: 1.5736e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 10s - loss: 14.2147 - mse: 14.2147 - mae: 1.5553 - val_loss: 9.7171 - val_mse: 9.7171 - val_mae: 1.5748 - lr: 1.5736e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 10s - loss: 14.1759 - mse: 14.1759 - mae: 1.5529 - val_loss: 9.8000 - val_mse: 9.8000 - val_mae: 1.5075 - lr: 1.5736e-04 - 10s/epoch - 10ms/step\n",
            "Score for fold 2: loss of 9.800041198730469\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 10s - loss: 14.0232 - mse: 14.0232 - mae: 1.5618 - val_loss: 10.2734 - val_mse: 10.2734 - val_mae: 1.5018 - lr: 1.5736e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 10s - loss: 14.0812 - mse: 14.0812 - mae: 1.5615 - val_loss: 10.2031 - val_mse: 10.2031 - val_mae: 1.5159 - lr: 1.5736e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 10s - loss: 13.9224 - mse: 13.9224 - mae: 1.5566 - val_loss: 10.3532 - val_mse: 10.3532 - val_mae: 1.4874 - lr: 1.5736e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 10s - loss: 13.9787 - mse: 13.9787 - mae: 1.5620 - val_loss: 10.2526 - val_mse: 10.2526 - val_mae: 1.4904 - lr: 1.5736e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 10s - loss: 13.9966 - mse: 13.9966 - mae: 1.5536 - val_loss: 10.2307 - val_mse: 10.2307 - val_mae: 1.5123 - lr: 1.5736e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 10s - loss: 13.9678 - mse: 13.9678 - mae: 1.5614 - val_loss: 10.2246 - val_mse: 10.2246 - val_mae: 1.5091 - lr: 1.5736e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 10s - loss: 13.9215 - mse: 13.9215 - mae: 1.5574 - val_loss: 10.2076 - val_mse: 10.2076 - val_mae: 1.5391 - lr: 1.5736e-04 - 10s/epoch - 10ms/step\n",
            "Score for fold 3: loss of 10.207595825195312\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 10s - loss: 13.7703 - mse: 13.7703 - mae: 1.5474 - val_loss: 11.1525 - val_mse: 11.1525 - val_mae: 1.5394 - lr: 1.5736e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 10s - loss: 13.8703 - mse: 13.8703 - mae: 1.5515 - val_loss: 11.2910 - val_mse: 11.2910 - val_mae: 1.5324 - lr: 1.5736e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 10s - loss: 13.7969 - mse: 13.7969 - mae: 1.5493 - val_loss: 11.2144 - val_mse: 11.2144 - val_mae: 1.5293 - lr: 1.5736e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 10s - loss: 13.8281 - mse: 13.8281 - mae: 1.5450 - val_loss: 11.7398 - val_mse: 11.7398 - val_mae: 1.5801 - lr: 1.5736e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 10s - loss: 13.8289 - mse: 13.8289 - mae: 1.5463 - val_loss: 11.2390 - val_mse: 11.2390 - val_mae: 1.5788 - lr: 1.5736e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 10s - loss: 13.8029 - mse: 13.8029 - mae: 1.5474 - val_loss: 11.1653 - val_mse: 11.1653 - val_mae: 1.5865 - lr: 1.5736e-04 - 10s/epoch - 10ms/step\n",
            "Score for fold 4: loss of 11.165308952331543\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 10s - loss: 12.9576 - mse: 12.9576 - mae: 1.5387 - val_loss: 14.2771 - val_mse: 14.2771 - val_mae: 1.5791 - lr: 1.5736e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 10s - loss: 13.0112 - mse: 13.0112 - mae: 1.5412 - val_loss: 14.3491 - val_mse: 14.3491 - val_mae: 1.6290 - lr: 1.5736e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 10s - loss: 12.9037 - mse: 12.9037 - mae: 1.5392 - val_loss: 14.3062 - val_mse: 14.3062 - val_mae: 1.5605 - lr: 1.5736e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 10s - loss: 12.9614 - mse: 12.9614 - mae: 1.5424 - val_loss: 14.5232 - val_mse: 14.5232 - val_mae: 1.5191 - lr: 1.5736e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 10s - loss: 12.9604 - mse: 12.9604 - mae: 1.5372 - val_loss: 14.3059 - val_mse: 14.3059 - val_mae: 1.5682 - lr: 1.5736e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 10s - loss: 13.0369 - mse: 13.0369 - mae: 1.5391 - val_loss: 14.3194 - val_mse: 14.3194 - val_mae: 1.5734 - lr: 1.5736e-04 - 10s/epoch - 10ms/step\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-10-11 18:16:25,722]\u001b[0m Finished trial#46 resulted in value: 13.290000000000001. Current best value is 8.234 with parameters: {'activation': 'relu', 'num_hidden_layer': 4, 'i': 10, 'learning_rate': 0.00015658492778156685}.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Score for fold 5: loss of 14.319439888000488\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 12s - loss: 13.6971 - mse: 13.6971 - mae: 1.5533 - val_loss: 10.5323 - val_mse: 10.5323 - val_mae: 1.5607 - lr: 2.9812e-04 - 12s/epoch - 12ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 11s - loss: 12.9475 - mse: 12.9475 - mae: 1.5014 - val_loss: 10.5419 - val_mse: 10.5419 - val_mae: 1.5426 - lr: 2.9812e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 11s - loss: 12.7270 - mse: 12.7270 - mae: 1.4865 - val_loss: 10.5044 - val_mse: 10.5044 - val_mae: 1.5277 - lr: 2.9812e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 11s - loss: 12.1964 - mse: 12.1964 - mae: 1.4749 - val_loss: 10.7141 - val_mse: 10.7141 - val_mae: 1.4188 - lr: 2.9812e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 11s - loss: 12.3750 - mse: 12.3750 - mae: 1.4672 - val_loss: 10.3535 - val_mse: 10.3535 - val_mae: 1.4822 - lr: 2.9812e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 10s - loss: 12.0943 - mse: 12.0943 - mae: 1.4540 - val_loss: 10.5412 - val_mse: 10.5412 - val_mae: 1.5082 - lr: 2.9812e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 11s - loss: 11.8918 - mse: 11.8918 - mae: 1.4459 - val_loss: 10.4334 - val_mse: 10.4334 - val_mae: 1.4032 - lr: 2.9812e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 11s - loss: 11.7513 - mse: 11.7513 - mae: 1.4391 - val_loss: 10.7783 - val_mse: 10.7783 - val_mae: 1.4893 - lr: 2.9812e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 11s - loss: 11.4886 - mse: 11.4886 - mae: 1.4265 - val_loss: 10.5339 - val_mse: 10.5339 - val_mae: 1.5509 - lr: 2.9812e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 11s - loss: 11.3920 - mse: 11.3920 - mae: 1.4191 - val_loss: 10.3102 - val_mse: 10.3102 - val_mae: 1.4445 - lr: 2.9812e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 11s - loss: 10.8721 - mse: 10.8721 - mae: 1.4058 - val_loss: 10.7659 - val_mse: 10.7659 - val_mae: 1.3865 - lr: 2.9812e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 10s - loss: 10.5671 - mse: 10.5671 - mae: 1.3887 - val_loss: 10.4388 - val_mse: 10.4388 - val_mae: 1.4666 - lr: 2.9812e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 13/100\n",
            "1000/1000 - 11s - loss: 10.2246 - mse: 10.2246 - mae: 1.3741 - val_loss: 10.5198 - val_mse: 10.5198 - val_mae: 1.4156 - lr: 2.9812e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 14/100\n",
            "1000/1000 - 11s - loss: 10.3510 - mse: 10.3510 - mae: 1.3655 - val_loss: 10.4647 - val_mse: 10.4647 - val_mae: 1.3978 - lr: 2.9812e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 15/100\n",
            "1000/1000 - 11s - loss: 10.0645 - mse: 10.0645 - mae: 1.3467 - val_loss: 10.8479 - val_mse: 10.8479 - val_mae: 1.4445 - lr: 2.9812e-04 - 11s/epoch - 11ms/step\n",
            "Score for fold 1: loss of 10.847922325134277\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 11s - loss: 10.1609 - mse: 10.1609 - mae: 1.3702 - val_loss: 9.1842 - val_mse: 9.1842 - val_mae: 1.3746 - lr: 2.9812e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 11s - loss: 10.0198 - mse: 10.0198 - mae: 1.3491 - val_loss: 9.4159 - val_mse: 9.4159 - val_mae: 1.3780 - lr: 2.9812e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 11s - loss: 9.9061 - mse: 9.9061 - mae: 1.3387 - val_loss: 9.3759 - val_mse: 9.3759 - val_mae: 1.3252 - lr: 2.9812e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 11s - loss: 9.2672 - mse: 9.2672 - mae: 1.3094 - val_loss: 9.3254 - val_mse: 9.3254 - val_mae: 1.3799 - lr: 2.9812e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 11s - loss: 9.2449 - mse: 9.2449 - mae: 1.2996 - val_loss: 9.6804 - val_mse: 9.6804 - val_mae: 1.4141 - lr: 2.9812e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 11s - loss: 8.9637 - mse: 8.9637 - mae: 1.2807 - val_loss: 10.0676 - val_mse: 10.0676 - val_mae: 1.3602 - lr: 2.9812e-04 - 11s/epoch - 11ms/step\n",
            "Score for fold 2: loss of 10.067625999450684\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 11s - loss: 10.2784 - mse: 10.2784 - mae: 1.3256 - val_loss: 6.6459 - val_mse: 6.6459 - val_mae: 1.3734 - lr: 2.9812e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 11s - loss: 9.5553 - mse: 9.5553 - mae: 1.3120 - val_loss: 5.4234 - val_mse: 5.4234 - val_mae: 1.2060 - lr: 2.9812e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 11s - loss: 9.4549 - mse: 9.4549 - mae: 1.2896 - val_loss: 5.3869 - val_mse: 5.3869 - val_mae: 1.2640 - lr: 2.9812e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 11s - loss: 9.2509 - mse: 9.2509 - mae: 1.2718 - val_loss: 5.6845 - val_mse: 5.6845 - val_mae: 1.4191 - lr: 2.9812e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 11s - loss: 8.8842 - mse: 8.8842 - mae: 1.2543 - val_loss: 6.7617 - val_mse: 6.7617 - val_mae: 1.3516 - lr: 2.9812e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 12s - loss: 8.7362 - mse: 8.7362 - mae: 1.2427 - val_loss: 5.7373 - val_mse: 5.7373 - val_mae: 1.3236 - lr: 2.9812e-04 - 12s/epoch - 12ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 11s - loss: 8.1507 - mse: 8.1507 - mae: 1.2205 - val_loss: 6.5130 - val_mse: 6.5130 - val_mae: 1.3034 - lr: 2.9812e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 11s - loss: 8.0857 - mse: 8.0857 - mae: 1.2068 - val_loss: 5.9308 - val_mse: 5.9308 - val_mae: 1.2918 - lr: 2.9812e-04 - 11s/epoch - 11ms/step\n",
            "Score for fold 3: loss of 5.930802345275879\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 11s - loss: 6.4815 - mse: 6.4815 - mae: 1.2374 - val_loss: 12.6578 - val_mse: 12.6578 - val_mae: 1.3275 - lr: 2.9812e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 11s - loss: 6.4916 - mse: 6.4916 - mae: 1.2146 - val_loss: 12.7205 - val_mse: 12.7205 - val_mae: 1.2062 - lr: 2.9812e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 11s - loss: 5.9111 - mse: 5.9111 - mae: 1.2020 - val_loss: 12.4910 - val_mse: 12.4910 - val_mae: 1.1858 - lr: 2.9812e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 11s - loss: 5.6736 - mse: 5.6736 - mae: 1.1842 - val_loss: 13.6571 - val_mse: 13.6571 - val_mae: 1.3202 - lr: 2.9812e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 11s - loss: 5.6892 - mse: 5.6892 - mae: 1.1767 - val_loss: 13.9798 - val_mse: 13.9798 - val_mae: 1.3424 - lr: 2.9812e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 11s - loss: 5.1964 - mse: 5.1964 - mae: 1.1501 - val_loss: 13.9921 - val_mse: 13.9921 - val_mae: 1.2907 - lr: 2.9812e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 11s - loss: 5.0106 - mse: 5.0106 - mae: 1.1330 - val_loss: 13.9204 - val_mse: 13.9204 - val_mae: 1.3255 - lr: 2.9812e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 11s - loss: 4.9088 - mse: 4.9088 - mae: 1.1279 - val_loss: 14.9361 - val_mse: 14.9361 - val_mae: 1.2227 - lr: 2.9812e-04 - 11s/epoch - 11ms/step\n",
            "Score for fold 4: loss of 14.936107635498047\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 11s - loss: 7.5570 - mse: 7.5570 - mae: 1.1701 - val_loss: 3.8751 - val_mse: 3.8751 - val_mae: 1.0116 - lr: 2.9812e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 11s - loss: 7.2569 - mse: 7.2569 - mae: 1.1432 - val_loss: 4.2456 - val_mse: 4.2456 - val_mae: 1.0414 - lr: 2.9812e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 11s - loss: 6.8181 - mse: 6.8181 - mae: 1.1299 - val_loss: 4.8407 - val_mse: 4.8407 - val_mae: 1.0485 - lr: 2.9812e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 11s - loss: 6.4648 - mse: 6.4648 - mae: 1.1036 - val_loss: 4.0063 - val_mse: 4.0063 - val_mae: 1.1770 - lr: 2.9812e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 11s - loss: 6.2375 - mse: 6.2375 - mae: 1.0953 - val_loss: 4.4441 - val_mse: 4.4441 - val_mae: 1.0968 - lr: 2.9812e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 11s - loss: 6.1034 - mse: 6.1034 - mae: 1.0848 - val_loss: 5.3499 - val_mse: 5.3499 - val_mae: 1.2256 - lr: 2.9812e-04 - 11s/epoch - 11ms/step\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-10-11 18:24:22,814]\u001b[0m Finished trial#47 resulted in value: 9.428. Current best value is 8.234 with parameters: {'activation': 'relu', 'num_hidden_layer': 4, 'i': 10, 'learning_rate': 0.00015658492778156685}.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Score for fold 5: loss of 5.349891185760498\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 7s - loss: 13.8021 - mse: 13.8021 - mae: 1.5558 - val_loss: 10.8913 - val_mse: 10.8913 - val_mae: 1.4498 - lr: 1.5749e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 7s - loss: 12.7726 - mse: 12.7726 - mae: 1.4969 - val_loss: 10.6513 - val_mse: 10.6513 - val_mae: 1.4833 - lr: 1.5749e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 6s - loss: 12.4927 - mse: 12.4927 - mae: 1.4775 - val_loss: 10.7281 - val_mse: 10.7281 - val_mae: 1.3961 - lr: 1.5749e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 6s - loss: 12.4918 - mse: 12.4918 - mae: 1.4687 - val_loss: 10.3924 - val_mse: 10.3924 - val_mae: 1.4860 - lr: 1.5749e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 6s - loss: 12.2248 - mse: 12.2248 - mae: 1.4581 - val_loss: 10.4458 - val_mse: 10.4458 - val_mae: 1.5155 - lr: 1.5749e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 7s - loss: 12.2178 - mse: 12.2178 - mae: 1.4502 - val_loss: 10.6820 - val_mse: 10.6820 - val_mae: 1.3857 - lr: 1.5749e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 6s - loss: 12.0221 - mse: 12.0221 - mae: 1.4419 - val_loss: 10.5147 - val_mse: 10.5147 - val_mae: 1.4074 - lr: 1.5749e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 7s - loss: 11.9490 - mse: 11.9490 - mae: 1.4362 - val_loss: 10.3365 - val_mse: 10.3365 - val_mae: 1.4429 - lr: 1.5749e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 7s - loss: 11.7541 - mse: 11.7541 - mae: 1.4243 - val_loss: 10.2824 - val_mse: 10.2824 - val_mae: 1.4853 - lr: 1.5749e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 6s - loss: 11.7185 - mse: 11.7185 - mae: 1.4138 - val_loss: 10.4163 - val_mse: 10.4163 - val_mae: 1.4191 - lr: 1.5749e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 6s - loss: 11.5103 - mse: 11.5103 - mae: 1.4052 - val_loss: 10.5397 - val_mse: 10.5397 - val_mae: 1.4963 - lr: 1.5749e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 7s - loss: 11.3396 - mse: 11.3396 - mae: 1.3929 - val_loss: 10.5315 - val_mse: 10.5315 - val_mae: 1.4844 - lr: 1.5749e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 13/100\n",
            "1000/1000 - 7s - loss: 11.2041 - mse: 11.2041 - mae: 1.3844 - val_loss: 10.5196 - val_mse: 10.5196 - val_mae: 1.4028 - lr: 1.5749e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 14/100\n",
            "1000/1000 - 6s - loss: 10.7820 - mse: 10.7820 - mae: 1.3742 - val_loss: 10.3928 - val_mse: 10.3928 - val_mae: 1.4341 - lr: 1.5749e-04 - 6s/epoch - 6ms/step\n",
            "Score for fold 1: loss of 10.392816543579102\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 6s - loss: 11.2034 - mse: 11.2034 - mae: 1.3923 - val_loss: 9.1417 - val_mse: 9.1417 - val_mae: 1.3322 - lr: 1.5749e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 7s - loss: 11.1337 - mse: 11.1337 - mae: 1.3809 - val_loss: 9.4037 - val_mse: 9.4037 - val_mae: 1.3770 - lr: 1.5749e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 7s - loss: 10.7876 - mse: 10.7876 - mae: 1.3665 - val_loss: 8.8714 - val_mse: 8.8714 - val_mae: 1.3334 - lr: 1.5749e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 7s - loss: 10.7667 - mse: 10.7667 - mae: 1.3551 - val_loss: 8.8944 - val_mse: 8.8944 - val_mae: 1.4170 - lr: 1.5749e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 7s - loss: 10.5272 - mse: 10.5272 - mae: 1.3390 - val_loss: 9.1288 - val_mse: 9.1288 - val_mae: 1.4090 - lr: 1.5749e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 6s - loss: 10.1942 - mse: 10.1942 - mae: 1.3293 - val_loss: 9.2488 - val_mse: 9.2488 - val_mae: 1.3877 - lr: 1.5749e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 7s - loss: 9.9904 - mse: 9.9904 - mae: 1.3152 - val_loss: 9.3093 - val_mse: 9.3093 - val_mae: 1.3630 - lr: 1.5749e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 6s - loss: 9.6291 - mse: 9.6291 - mae: 1.2943 - val_loss: 9.1714 - val_mse: 9.1714 - val_mae: 1.3563 - lr: 1.5749e-04 - 6s/epoch - 6ms/step\n",
            "Score for fold 2: loss of 9.171377182006836\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 7s - loss: 8.0542 - mse: 8.0542 - mae: 1.3145 - val_loss: 15.5289 - val_mse: 15.5289 - val_mae: 1.3712 - lr: 1.5749e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 6s - loss: 7.7460 - mse: 7.7460 - mae: 1.2932 - val_loss: 16.0927 - val_mse: 16.0927 - val_mae: 1.3410 - lr: 1.5749e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 6s - loss: 7.6997 - mse: 7.6997 - mae: 1.2804 - val_loss: 17.0664 - val_mse: 17.0664 - val_mae: 1.3403 - lr: 1.5749e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 7s - loss: 7.3558 - mse: 7.3558 - mae: 1.2633 - val_loss: 15.7901 - val_mse: 15.7901 - val_mae: 1.4278 - lr: 1.5749e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 7s - loss: 7.4500 - mse: 7.4500 - mae: 1.2496 - val_loss: 16.4800 - val_mse: 16.4800 - val_mae: 1.3381 - lr: 1.5749e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 6s - loss: 7.0728 - mse: 7.0728 - mae: 1.2406 - val_loss: 16.5431 - val_mse: 16.5431 - val_mae: 1.4745 - lr: 1.5749e-04 - 6s/epoch - 6ms/step\n",
            "Score for fold 3: loss of 16.543128967285156\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 7s - loss: 9.0967 - mse: 9.0967 - mae: 1.2801 - val_loss: 7.7176 - val_mse: 7.7176 - val_mae: 1.1924 - lr: 1.5749e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 7s - loss: 8.5878 - mse: 8.5878 - mae: 1.2553 - val_loss: 8.2085 - val_mse: 8.2085 - val_mae: 1.2204 - lr: 1.5749e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 6s - loss: 8.4971 - mse: 8.4971 - mae: 1.2390 - val_loss: 8.1553 - val_mse: 8.1553 - val_mae: 1.2268 - lr: 1.5749e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 6s - loss: 8.2307 - mse: 8.2307 - mae: 1.2247 - val_loss: 8.3787 - val_mse: 8.3787 - val_mae: 1.2852 - lr: 1.5749e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 6s - loss: 7.8506 - mse: 7.8506 - mae: 1.2055 - val_loss: 8.3365 - val_mse: 8.3365 - val_mae: 1.2734 - lr: 1.5749e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 6s - loss: 7.9341 - mse: 7.9341 - mae: 1.1959 - val_loss: 8.0289 - val_mse: 8.0289 - val_mae: 1.2701 - lr: 1.5749e-04 - 6s/epoch - 6ms/step\n",
            "Score for fold 4: loss of 8.0288667678833\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 7s - loss: 8.4958 - mse: 8.4958 - mae: 1.2211 - val_loss: 3.9463 - val_mse: 3.9463 - val_mae: 1.1195 - lr: 1.5749e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 6s - loss: 8.2040 - mse: 8.2040 - mae: 1.1973 - val_loss: 3.8358 - val_mse: 3.8358 - val_mae: 1.1492 - lr: 1.5749e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 7s - loss: 7.8213 - mse: 7.8213 - mae: 1.1775 - val_loss: 4.1133 - val_mse: 4.1133 - val_mae: 1.1957 - lr: 1.5749e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 7s - loss: 7.6200 - mse: 7.6200 - mae: 1.1619 - val_loss: 4.1403 - val_mse: 4.1403 - val_mae: 1.1883 - lr: 1.5749e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 6s - loss: 7.5319 - mse: 7.5319 - mae: 1.1471 - val_loss: 4.4423 - val_mse: 4.4423 - val_mae: 1.1744 - lr: 1.5749e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 6s - loss: 7.2122 - mse: 7.2122 - mae: 1.1271 - val_loss: 4.5243 - val_mse: 4.5243 - val_mae: 1.1975 - lr: 1.5749e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 7s - loss: 6.7970 - mse: 6.7970 - mae: 1.1155 - val_loss: 5.0043 - val_mse: 5.0043 - val_mae: 1.2095 - lr: 1.5749e-04 - 7s/epoch - 7ms/step\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-10-11 18:28:57,737]\u001b[0m Finished trial#48 resulted in value: 9.826. Current best value is 8.234 with parameters: {'activation': 'relu', 'num_hidden_layer': 4, 'i': 10, 'learning_rate': 0.00015658492778156685}.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Score for fold 5: loss of 5.004324913024902\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 12s - loss: 14.1296 - mse: 14.1296 - mae: 1.5480 - val_loss: 8.9651 - val_mse: 8.9651 - val_mae: 1.3923 - lr: 1.1824e-04 - 12s/epoch - 12ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 11s - loss: 13.1944 - mse: 13.1944 - mae: 1.4913 - val_loss: 9.0713 - val_mse: 9.0713 - val_mae: 1.4200 - lr: 1.1824e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 11s - loss: 12.9872 - mse: 12.9872 - mae: 1.4754 - val_loss: 9.7856 - val_mse: 9.7856 - val_mae: 1.5797 - lr: 1.1824e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 11s - loss: 12.8554 - mse: 12.8554 - mae: 1.4602 - val_loss: 9.6737 - val_mse: 9.6737 - val_mae: 1.5412 - lr: 1.1824e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 11s - loss: 12.7190 - mse: 12.7190 - mae: 1.4533 - val_loss: 9.3184 - val_mse: 9.3184 - val_mae: 1.4743 - lr: 1.1824e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 11s - loss: 12.7209 - mse: 12.7209 - mae: 1.4405 - val_loss: 10.2173 - val_mse: 10.2173 - val_mae: 1.4480 - lr: 1.1824e-04 - 11s/epoch - 11ms/step\n",
            "Score for fold 1: loss of 10.217328071594238\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 11s - loss: 12.2472 - mse: 12.2472 - mae: 1.4512 - val_loss: 9.9039 - val_mse: 9.9039 - val_mae: 1.4298 - lr: 1.1824e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 11s - loss: 12.1067 - mse: 12.1067 - mae: 1.4365 - val_loss: 9.8175 - val_mse: 9.8175 - val_mae: 1.4399 - lr: 1.1824e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 11s - loss: 11.9048 - mse: 11.9048 - mae: 1.4220 - val_loss: 9.6838 - val_mse: 9.6838 - val_mae: 1.4004 - lr: 1.1824e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 11s - loss: 11.7506 - mse: 11.7506 - mae: 1.4130 - val_loss: 9.4290 - val_mse: 9.4290 - val_mae: 1.4351 - lr: 1.1824e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 11s - loss: 11.5134 - mse: 11.5134 - mae: 1.4026 - val_loss: 9.8610 - val_mse: 9.8610 - val_mae: 1.4513 - lr: 1.1824e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 11s - loss: 11.5960 - mse: 11.5960 - mae: 1.3890 - val_loss: 9.4428 - val_mse: 9.4428 - val_mae: 1.4870 - lr: 1.1824e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 11s - loss: 10.9796 - mse: 10.9796 - mae: 1.3739 - val_loss: 9.5713 - val_mse: 9.5713 - val_mae: 1.4574 - lr: 1.1824e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 12s - loss: 11.1417 - mse: 11.1417 - mae: 1.3584 - val_loss: 9.6321 - val_mse: 9.6321 - val_mae: 1.3999 - lr: 1.1824e-04 - 12s/epoch - 12ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 11s - loss: 10.5834 - mse: 10.5834 - mae: 1.3413 - val_loss: 9.8510 - val_mse: 9.8510 - val_mae: 1.4013 - lr: 1.1824e-04 - 11s/epoch - 11ms/step\n",
            "Score for fold 2: loss of 9.850970268249512\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 11s - loss: 9.6304 - mse: 9.6304 - mae: 1.3654 - val_loss: 13.2447 - val_mse: 13.2447 - val_mae: 1.3215 - lr: 1.1824e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 11s - loss: 9.2278 - mse: 9.2278 - mae: 1.3422 - val_loss: 13.9222 - val_mse: 13.9222 - val_mae: 1.3392 - lr: 1.1824e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 11s - loss: 8.7208 - mse: 8.7208 - mae: 1.3260 - val_loss: 14.9345 - val_mse: 14.9345 - val_mae: 1.3769 - lr: 1.1824e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 11s - loss: 8.4448 - mse: 8.4448 - mae: 1.3095 - val_loss: 14.7244 - val_mse: 14.7244 - val_mae: 1.3550 - lr: 1.1824e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 11s - loss: 8.2188 - mse: 8.2188 - mae: 1.2836 - val_loss: 14.1006 - val_mse: 14.1006 - val_mae: 1.4185 - lr: 1.1824e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 11s - loss: 8.0065 - mse: 8.0065 - mae: 1.2636 - val_loss: 14.6670 - val_mse: 14.6670 - val_mae: 1.3593 - lr: 1.1824e-04 - 11s/epoch - 11ms/step\n",
            "Score for fold 3: loss of 14.667034149169922\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 11s - loss: 9.5902 - mse: 9.5902 - mae: 1.2911 - val_loss: 7.4244 - val_mse: 7.4244 - val_mae: 1.2510 - lr: 1.1824e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 11s - loss: 9.4144 - mse: 9.4144 - mae: 1.2686 - val_loss: 7.0297 - val_mse: 7.0297 - val_mae: 1.2448 - lr: 1.1824e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 11s - loss: 8.7385 - mse: 8.7385 - mae: 1.2340 - val_loss: 7.7293 - val_mse: 7.7293 - val_mae: 1.2363 - lr: 1.1824e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 11s - loss: 8.3298 - mse: 8.3298 - mae: 1.2168 - val_loss: 7.5746 - val_mse: 7.5746 - val_mae: 1.4411 - lr: 1.1824e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 11s - loss: 8.0792 - mse: 8.0792 - mae: 1.1926 - val_loss: 7.5116 - val_mse: 7.5116 - val_mae: 1.2774 - lr: 1.1824e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 11s - loss: 8.1094 - mse: 8.1094 - mae: 1.1681 - val_loss: 7.8338 - val_mse: 7.8338 - val_mae: 1.2334 - lr: 1.1824e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 11s - loss: 7.5473 - mse: 7.5473 - mae: 1.1517 - val_loss: 7.6931 - val_mse: 7.6931 - val_mae: 1.3494 - lr: 1.1824e-04 - 11s/epoch - 11ms/step\n",
            "Score for fold 4: loss of 7.693087577819824\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 11s - loss: 7.4114 - mse: 7.4114 - mae: 1.1885 - val_loss: 8.5858 - val_mse: 8.5858 - val_mae: 1.1995 - lr: 1.1824e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 11s - loss: 6.7416 - mse: 6.7416 - mae: 1.1579 - val_loss: 10.0263 - val_mse: 10.0263 - val_mae: 1.1802 - lr: 1.1824e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 11s - loss: 6.3794 - mse: 6.3794 - mae: 1.1360 - val_loss: 9.7549 - val_mse: 9.7549 - val_mae: 1.1733 - lr: 1.1824e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 11s - loss: 6.0937 - mse: 6.0937 - mae: 1.1169 - val_loss: 9.3897 - val_mse: 9.3897 - val_mae: 1.2506 - lr: 1.1824e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 11s - loss: 5.8807 - mse: 5.8807 - mae: 1.0992 - val_loss: 9.6438 - val_mse: 9.6438 - val_mae: 1.2219 - lr: 1.1824e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 11s - loss: 5.7000 - mse: 5.7000 - mae: 1.0751 - val_loss: 9.9474 - val_mse: 9.9474 - val_mae: 1.2674 - lr: 1.1824e-04 - 11s/epoch - 11ms/step\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-10-11 18:35:23,606]\u001b[0m Finished trial#49 resulted in value: 10.475999999999999. Current best value is 8.234 with parameters: {'activation': 'relu', 'num_hidden_layer': 4, 'i': 10, 'learning_rate': 0.00015658492778156685}.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Score for fold 5: loss of 9.947393417358398\n"
          ]
        }
      ],
      "source": [
        "model_list=[]\n",
        "history_list=[]\n",
        "study_name = 'NN_study_2'\n",
        "study = optuna.create_study(study_name=study_name, load_if_exists=True)\n",
        "func = lambda trial: objective(trial,training_shuffled1,labelsForTrain_shuffled1)\n",
        "study.optimize(func, n_trials=50,)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_tgpDC4c5U2w",
        "outputId": "d9efa8f4-7a8a-4f67-9215-b2fb19cb96f4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "1250/1250 - 11s - loss: 13.1720 - mse: 13.1720 - mae: 1.5376 - val_loss: 9.7441 - val_mse: 9.7441 - val_mae: 1.5603 - lr: 1.5658e-04 - 11s/epoch - 9ms/step\n",
            "Epoch 2/20\n",
            "1250/1250 - 10s - loss: 12.2971 - mse: 12.2971 - mae: 1.4793 - val_loss: 9.6323 - val_mse: 9.6323 - val_mae: 1.4266 - lr: 1.5658e-04 - 10s/epoch - 8ms/step\n",
            "Epoch 3/20\n",
            "1250/1250 - 10s - loss: 12.1517 - mse: 12.1517 - mae: 1.4647 - val_loss: 9.9542 - val_mse: 9.9542 - val_mae: 1.4973 - lr: 1.5658e-04 - 10s/epoch - 8ms/step\n",
            "Epoch 4/20\n",
            "1250/1250 - 10s - loss: 12.0127 - mse: 12.0127 - mae: 1.4584 - val_loss: 9.6743 - val_mse: 9.6743 - val_mae: 1.4285 - lr: 1.5658e-04 - 10s/epoch - 8ms/step\n",
            "Epoch 5/20\n",
            "1250/1250 - 10s - loss: 11.8618 - mse: 11.8618 - mae: 1.4495 - val_loss: 9.7882 - val_mse: 9.7882 - val_mae: 1.5566 - lr: 1.5658e-04 - 10s/epoch - 8ms/step\n",
            "Epoch 6/20\n",
            "1250/1250 - 10s - loss: 11.7069 - mse: 11.7069 - mae: 1.4416 - val_loss: 9.4614 - val_mse: 9.4614 - val_mae: 1.4420 - lr: 1.5658e-04 - 10s/epoch - 8ms/step\n",
            "Epoch 7/20\n",
            "1250/1250 - 10s - loss: 11.6400 - mse: 11.6400 - mae: 1.4321 - val_loss: 9.7476 - val_mse: 9.7476 - val_mae: 1.4432 - lr: 1.5658e-04 - 10s/epoch - 8ms/step\n",
            "Epoch 8/20\n",
            "1250/1250 - 10s - loss: 11.7602 - mse: 11.7602 - mae: 1.4241 - val_loss: 9.4564 - val_mse: 9.4564 - val_mae: 1.4159 - lr: 1.5658e-04 - 10s/epoch - 8ms/step\n",
            "Epoch 9/20\n",
            "1250/1250 - 10s - loss: 11.0994 - mse: 11.0994 - mae: 1.4142 - val_loss: 9.2867 - val_mse: 9.2867 - val_mae: 1.5374 - lr: 1.5658e-04 - 10s/epoch - 8ms/step\n",
            "Epoch 10/20\n",
            "1250/1250 - 10s - loss: 10.9849 - mse: 10.9849 - mae: 1.4012 - val_loss: 8.8559 - val_mse: 8.8559 - val_mae: 1.4470 - lr: 1.5658e-04 - 10s/epoch - 8ms/step\n",
            "Epoch 11/20\n",
            "1250/1250 - 10s - loss: 10.7836 - mse: 10.7836 - mae: 1.3927 - val_loss: 9.2766 - val_mse: 9.2766 - val_mae: 1.4629 - lr: 1.5658e-04 - 10s/epoch - 8ms/step\n",
            "Epoch 12/20\n",
            "1250/1250 - 10s - loss: 10.3594 - mse: 10.3594 - mae: 1.3731 - val_loss: 9.1171 - val_mse: 9.1171 - val_mae: 1.5330 - lr: 1.5658e-04 - 10s/epoch - 8ms/step\n",
            "Epoch 13/20\n",
            "1250/1250 - 10s - loss: 10.3208 - mse: 10.3208 - mae: 1.3621 - val_loss: 9.4923 - val_mse: 9.4923 - val_mae: 1.5308 - lr: 1.5658e-04 - 10s/epoch - 8ms/step\n",
            "Epoch 14/20\n",
            "1250/1250 - 10s - loss: 10.2229 - mse: 10.2229 - mae: 1.3441 - val_loss: 9.8344 - val_mse: 9.8344 - val_mae: 1.5250 - lr: 1.5658e-04 - 10s/epoch - 8ms/step\n",
            "Epoch 15/20\n",
            "1250/1250 - 10s - loss: 9.7065 - mse: 9.7065 - mae: 1.3265 - val_loss: 9.5461 - val_mse: 9.5461 - val_mae: 1.4663 - lr: 1.5658e-04 - 10s/epoch - 8ms/step\n",
            "Epoch 16/20\n",
            "1250/1250 - 10s - loss: 9.4564 - mse: 9.4564 - mae: 1.3100 - val_loss: 9.3653 - val_mse: 9.3653 - val_mae: 1.4725 - lr: 1.5658e-04 - 10s/epoch - 8ms/step\n",
            "Epoch 17/20\n",
            "1250/1250 - 10s - loss: 9.2760 - mse: 9.2760 - mae: 1.2962 - val_loss: 9.6670 - val_mse: 9.6670 - val_mae: 1.4799 - lr: 1.5658e-04 - 10s/epoch - 8ms/step\n",
            "Epoch 18/20\n",
            "1250/1250 - 10s - loss: 9.0472 - mse: 9.0472 - mae: 1.2745 - val_loss: 9.6714 - val_mse: 9.6714 - val_mae: 1.5040 - lr: 1.5658e-04 - 10s/epoch - 8ms/step\n",
            "Epoch 19/20\n",
            "1250/1250 - 10s - loss: 8.6375 - mse: 8.6375 - mae: 1.2597 - val_loss: 10.0831 - val_mse: 10.0831 - val_mae: 1.6245 - lr: 1.5658e-04 - 10s/epoch - 8ms/step\n",
            "Epoch 20/20\n",
            "1250/1250 - 11s - loss: 8.5825 - mse: 8.5825 - mae: 1.2484 - val_loss: 9.8471 - val_mse: 9.8471 - val_mae: 1.5074 - lr: 1.5658e-04 - 11s/epoch - 8ms/step\n"
          ]
        }
      ],
      "source": [
        "#{'activation': 'relu', 'num_hidden_layer': 4, 'i': 10, 'learning_rate': 0.00015658492778156685}.\n",
        "optimizer = Adam(learning_rate=0.00015658492778156685 ,clipnorm=1.0)\n",
        "model_2 = create_model(activation=\"relu\",num_hidden_layer=4,num_hidden_unit=1024)\n",
        "\n",
        "es = EarlyStopping(monitor='mse', patience=5)\n",
        "reduce_lr = ReduceLROnPlateau(monitor='val_mse', factor=0.2,\n",
        "                              patience=20, min_lr=0.001)\n",
        "model_2.compile(loss='mse',optimizer=optimizer,metrics=['mse','mae'])\n",
        "history = model_2.fit(training,labelsForTrain,\n",
        "                batch_size=64,\n",
        "                epochs=20,\n",
        "                verbose=2,\n",
        "                validation_data=(valing,labelsForVal),\n",
        "                validation_batch_size=64,\n",
        "                callbacks=[es,reduce_lr])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FaO51aiBDYsw",
        "outputId": "d425106b-b30e-4472-8bd4-bd831a0daea0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "313/313 [==============================] - 1s 3ms/step - loss: 11.4872 - mse: 11.4872 - mae: 1.4571\n"
          ]
        }
      ],
      "source": [
        "results_model2 = model_2.evaluate(testing, labelsForTest, batch_size=32)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NmfHslRiGobG"
      },
      "source": [
        "## Shuffle Repetation 3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JbXmpk3DGq1X"
      },
      "outputs": [],
      "source": [
        "# Shuffle the data after train\n",
        "shuffled2 = shuffle(train_df, random_state=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ujyXt7kyGvCA"
      },
      "outputs": [],
      "source": [
        "training_shuffled2,labelsForTrain_shuffled2=process_shuffle_dataset(shuffled2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B_z0MIxxG4ns",
        "outputId": "368b56be-b445-4201-b948-8982d9d7731f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 4s - loss: 16.2831 - mse: 16.2831 - mae: 1.6716 - val_loss: 13.7612 - val_mse: 13.7612 - val_mae: 1.5687 - lr: 1.0540e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 14.2812 - mse: 14.2812 - mae: 1.5151 - val_loss: 13.4955 - val_mse: 13.4955 - val_mae: 1.5279 - lr: 1.0540e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 14.0395 - mse: 14.0395 - mae: 1.5008 - val_loss: 13.3823 - val_mse: 13.3823 - val_mae: 1.5023 - lr: 1.0540e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 13.8651 - mse: 13.8651 - mae: 1.4866 - val_loss: 13.2116 - val_mse: 13.2116 - val_mae: 1.5082 - lr: 1.0540e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 13.7241 - mse: 13.7241 - mae: 1.4773 - val_loss: 13.1127 - val_mse: 13.1127 - val_mae: 1.4944 - lr: 1.0540e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 13.6290 - mse: 13.6290 - mae: 1.4721 - val_loss: 13.0079 - val_mse: 13.0079 - val_mae: 1.5205 - lr: 1.0540e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 13.5394 - mse: 13.5394 - mae: 1.4678 - val_loss: 12.9483 - val_mse: 12.9483 - val_mae: 1.5062 - lr: 1.0540e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 2s - loss: 13.4768 - mse: 13.4768 - mae: 1.4673 - val_loss: 12.8862 - val_mse: 12.8862 - val_mae: 1.5049 - lr: 1.0540e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 2s - loss: 13.4050 - mse: 13.4050 - mae: 1.4621 - val_loss: 12.8816 - val_mse: 12.8816 - val_mae: 1.4917 - lr: 1.0540e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 2s - loss: 13.3434 - mse: 13.3434 - mae: 1.4587 - val_loss: 12.8417 - val_mse: 12.8417 - val_mae: 1.4972 - lr: 1.0540e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 2s - loss: 13.3056 - mse: 13.3056 - mae: 1.4550 - val_loss: 12.8729 - val_mse: 12.8729 - val_mae: 1.5351 - lr: 1.0540e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 2s - loss: 13.2396 - mse: 13.2396 - mae: 1.4546 - val_loss: 12.8185 - val_mse: 12.8185 - val_mae: 1.5052 - lr: 1.0540e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 13/100\n",
            "1000/1000 - 3s - loss: 13.1887 - mse: 13.1887 - mae: 1.4534 - val_loss: 12.7640 - val_mse: 12.7640 - val_mae: 1.4914 - lr: 1.0540e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 14/100\n",
            "1000/1000 - 3s - loss: 13.1458 - mse: 13.1458 - mae: 1.4468 - val_loss: 12.7657 - val_mse: 12.7657 - val_mae: 1.5188 - lr: 1.0540e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 15/100\n",
            "1000/1000 - 3s - loss: 13.1110 - mse: 13.1110 - mae: 1.4482 - val_loss: 12.7171 - val_mse: 12.7171 - val_mae: 1.5105 - lr: 1.0540e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 16/100\n",
            "1000/1000 - 3s - loss: 13.0848 - mse: 13.0848 - mae: 1.4460 - val_loss: 12.7281 - val_mse: 12.7281 - val_mae: 1.4995 - lr: 1.0540e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 17/100\n",
            "1000/1000 - 2s - loss: 13.0273 - mse: 13.0273 - mae: 1.4444 - val_loss: 12.6960 - val_mse: 12.6960 - val_mae: 1.4848 - lr: 1.0540e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 18/100\n",
            "1000/1000 - 2s - loss: 12.9837 - mse: 12.9837 - mae: 1.4429 - val_loss: 12.6567 - val_mse: 12.6567 - val_mae: 1.4915 - lr: 1.0540e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 19/100\n",
            "1000/1000 - 2s - loss: 12.9475 - mse: 12.9475 - mae: 1.4420 - val_loss: 12.6857 - val_mse: 12.6857 - val_mae: 1.5060 - lr: 1.0540e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 20/100\n",
            "1000/1000 - 2s - loss: 12.9211 - mse: 12.9211 - mae: 1.4417 - val_loss: 12.7471 - val_mse: 12.7471 - val_mae: 1.5244 - lr: 1.0540e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 21/100\n",
            "1000/1000 - 2s - loss: 12.8811 - mse: 12.8811 - mae: 1.4408 - val_loss: 12.6760 - val_mse: 12.6760 - val_mae: 1.4880 - lr: 1.0540e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 22/100\n",
            "1000/1000 - 2s - loss: 12.8428 - mse: 12.8428 - mae: 1.4404 - val_loss: 12.6290 - val_mse: 12.6290 - val_mae: 1.4943 - lr: 1.0540e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 23/100\n",
            "1000/1000 - 2s - loss: 12.8136 - mse: 12.8136 - mae: 1.4390 - val_loss: 12.6577 - val_mse: 12.6577 - val_mae: 1.4859 - lr: 1.0540e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 24/100\n",
            "1000/1000 - 2s - loss: 12.7925 - mse: 12.7925 - mae: 1.4352 - val_loss: 12.7658 - val_mse: 12.7658 - val_mae: 1.4679 - lr: 1.0540e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 25/100\n",
            "1000/1000 - 2s - loss: 12.7584 - mse: 12.7584 - mae: 1.4358 - val_loss: 12.6245 - val_mse: 12.6245 - val_mae: 1.4963 - lr: 1.0540e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 26/100\n",
            "1000/1000 - 2s - loss: 12.7264 - mse: 12.7264 - mae: 1.4327 - val_loss: 12.6211 - val_mse: 12.6211 - val_mae: 1.5150 - lr: 1.0540e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 27/100\n",
            "1000/1000 - 2s - loss: 12.6948 - mse: 12.6948 - mae: 1.4343 - val_loss: 12.5946 - val_mse: 12.5946 - val_mae: 1.5071 - lr: 1.0540e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 28/100\n",
            "1000/1000 - 2s - loss: 12.6967 - mse: 12.6967 - mae: 1.4315 - val_loss: 12.6375 - val_mse: 12.6375 - val_mae: 1.4735 - lr: 1.0540e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 29/100\n",
            "1000/1000 - 2s - loss: 12.6684 - mse: 12.6684 - mae: 1.4316 - val_loss: 12.5922 - val_mse: 12.5922 - val_mae: 1.4940 - lr: 1.0540e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 30/100\n",
            "1000/1000 - 2s - loss: 12.6284 - mse: 12.6284 - mae: 1.4327 - val_loss: 12.5912 - val_mse: 12.5912 - val_mae: 1.4888 - lr: 1.0540e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 31/100\n",
            "1000/1000 - 2s - loss: 12.5978 - mse: 12.5978 - mae: 1.4314 - val_loss: 12.5288 - val_mse: 12.5288 - val_mae: 1.4924 - lr: 1.0540e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 32/100\n",
            "1000/1000 - 2s - loss: 12.5666 - mse: 12.5666 - mae: 1.4326 - val_loss: 12.5764 - val_mse: 12.5764 - val_mae: 1.4929 - lr: 1.0540e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 33/100\n",
            "1000/1000 - 2s - loss: 12.5564 - mse: 12.5564 - mae: 1.4287 - val_loss: 12.5365 - val_mse: 12.5365 - val_mae: 1.4893 - lr: 1.0540e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 34/100\n",
            "1000/1000 - 2s - loss: 12.5328 - mse: 12.5328 - mae: 1.4284 - val_loss: 12.5788 - val_mse: 12.5788 - val_mae: 1.5240 - lr: 1.0540e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 35/100\n",
            "1000/1000 - 2s - loss: 12.5000 - mse: 12.5000 - mae: 1.4279 - val_loss: 12.5636 - val_mse: 12.5636 - val_mae: 1.4859 - lr: 1.0540e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 36/100\n",
            "1000/1000 - 2s - loss: 12.4896 - mse: 12.4896 - mae: 1.4240 - val_loss: 12.5966 - val_mse: 12.5966 - val_mae: 1.5051 - lr: 1.0540e-04 - 2s/epoch - 2ms/step\n",
            "Score for fold 1: loss of 12.596553802490234\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 3s - loss: 12.1263 - mse: 12.1263 - mae: 1.4419 - val_loss: 13.8991 - val_mse: 13.8991 - val_mae: 1.4236 - lr: 1.0540e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 3s - loss: 12.0828 - mse: 12.0828 - mae: 1.4401 - val_loss: 13.9299 - val_mse: 13.9299 - val_mae: 1.4643 - lr: 1.0540e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 3s - loss: 12.0704 - mse: 12.0704 - mae: 1.4389 - val_loss: 14.0197 - val_mse: 14.0197 - val_mae: 1.4498 - lr: 1.0540e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 3s - loss: 12.0378 - mse: 12.0378 - mae: 1.4368 - val_loss: 14.0125 - val_mse: 14.0125 - val_mae: 1.4306 - lr: 1.0540e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 3s - loss: 12.0277 - mse: 12.0277 - mae: 1.4347 - val_loss: 13.9441 - val_mse: 13.9441 - val_mae: 1.4272 - lr: 1.0540e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 3s - loss: 12.0053 - mse: 12.0053 - mae: 1.4337 - val_loss: 13.9151 - val_mse: 13.9151 - val_mae: 1.4527 - lr: 1.0540e-04 - 3s/epoch - 3ms/step\n",
            "Score for fold 2: loss of 13.915104866027832\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 3s - loss: 13.0232 - mse: 13.0232 - mae: 1.4481 - val_loss: 9.7065 - val_mse: 9.7065 - val_mae: 1.3875 - lr: 1.0540e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 3s - loss: 12.9868 - mse: 12.9868 - mae: 1.4437 - val_loss: 9.7291 - val_mse: 9.7291 - val_mae: 1.3819 - lr: 1.0540e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 3s - loss: 12.9463 - mse: 12.9463 - mae: 1.4455 - val_loss: 9.7637 - val_mse: 9.7637 - val_mae: 1.3999 - lr: 1.0540e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 3s - loss: 12.9183 - mse: 12.9183 - mae: 1.4456 - val_loss: 9.7731 - val_mse: 9.7731 - val_mae: 1.4183 - lr: 1.0540e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 3s - loss: 12.9046 - mse: 12.9046 - mae: 1.4428 - val_loss: 9.7595 - val_mse: 9.7595 - val_mae: 1.3854 - lr: 1.0540e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 3s - loss: 12.8698 - mse: 12.8698 - mae: 1.4414 - val_loss: 9.7351 - val_mse: 9.7351 - val_mae: 1.4042 - lr: 1.0540e-04 - 3s/epoch - 3ms/step\n",
            "Score for fold 3: loss of 9.735095024108887\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 3s - loss: 11.4541 - mse: 11.4541 - mae: 1.4342 - val_loss: 15.4215 - val_mse: 15.4215 - val_mae: 1.3991 - lr: 1.0540e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 3s - loss: 11.4083 - mse: 11.4083 - mae: 1.4309 - val_loss: 15.4431 - val_mse: 15.4431 - val_mae: 1.4240 - lr: 1.0540e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 3s - loss: 11.3818 - mse: 11.3818 - mae: 1.4336 - val_loss: 15.4851 - val_mse: 15.4851 - val_mae: 1.4446 - lr: 1.0540e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 3s - loss: 11.3487 - mse: 11.3487 - mae: 1.4303 - val_loss: 15.5261 - val_mse: 15.5261 - val_mae: 1.4133 - lr: 1.0540e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 3s - loss: 11.3316 - mse: 11.3316 - mae: 1.4292 - val_loss: 15.5187 - val_mse: 15.5187 - val_mae: 1.4136 - lr: 1.0540e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 3s - loss: 11.3206 - mse: 11.3206 - mae: 1.4278 - val_loss: 15.4838 - val_mse: 15.4838 - val_mae: 1.4317 - lr: 1.0540e-04 - 3s/epoch - 3ms/step\n",
            "Score for fold 4: loss of 15.483806610107422\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 3s - loss: 12.6705 - mse: 12.6705 - mae: 1.4260 - val_loss: 10.0013 - val_mse: 10.0013 - val_mae: 1.4093 - lr: 1.0540e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 3s - loss: 12.6152 - mse: 12.6152 - mae: 1.4250 - val_loss: 10.0739 - val_mse: 10.0739 - val_mae: 1.4641 - lr: 1.0540e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 3s - loss: 12.6023 - mse: 12.6023 - mae: 1.4232 - val_loss: 10.0139 - val_mse: 10.0139 - val_mae: 1.4357 - lr: 1.0540e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 3s - loss: 12.5805 - mse: 12.5805 - mae: 1.4237 - val_loss: 10.2101 - val_mse: 10.2101 - val_mae: 1.4353 - lr: 1.0540e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 3s - loss: 12.5658 - mse: 12.5658 - mae: 1.4209 - val_loss: 10.0734 - val_mse: 10.0734 - val_mae: 1.4733 - lr: 1.0540e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 3s - loss: 12.5387 - mse: 12.5387 - mae: 1.4216 - val_loss: 10.1083 - val_mse: 10.1083 - val_mae: 1.4392 - lr: 1.0540e-04 - 3s/epoch - 3ms/step\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-10-11 22:17:43,504]\u001b[0m Finished trial#0 resulted in value: 12.37. Current best value is 12.37 with parameters: {'activation': 'tanh', 'num_hidden_layer': 4, 'i': 7, 'learning_rate': 0.00010539545644462869}.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Score for fold 5: loss of 10.108320236206055\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 3s - loss: 14.6742 - mse: 14.6742 - mae: 1.5844 - val_loss: 12.1411 - val_mse: 12.1411 - val_mae: 1.6000 - lr: 0.0020 - 3s/epoch - 3ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 14.2220 - mse: 14.2220 - mae: 1.5287 - val_loss: 11.8277 - val_mse: 11.8277 - val_mae: 1.5437 - lr: 0.0020 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 14.0141 - mse: 14.0141 - mae: 1.5139 - val_loss: 11.9414 - val_mse: 11.9414 - val_mae: 1.4797 - lr: 0.0020 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 13.8487 - mse: 13.8487 - mae: 1.5085 - val_loss: 11.7798 - val_mse: 11.7798 - val_mae: 1.4641 - lr: 0.0020 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 13.7673 - mse: 13.7673 - mae: 1.5019 - val_loss: 11.6329 - val_mse: 11.6329 - val_mae: 1.5723 - lr: 0.0020 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 13.6783 - mse: 13.6783 - mae: 1.4978 - val_loss: 11.5985 - val_mse: 11.5985 - val_mae: 1.4954 - lr: 0.0020 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 13.6318 - mse: 13.6318 - mae: 1.4891 - val_loss: 11.7202 - val_mse: 11.7202 - val_mae: 1.4193 - lr: 0.0020 - 2s/epoch - 2ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 2s - loss: 13.6253 - mse: 13.6253 - mae: 1.4880 - val_loss: 11.6484 - val_mse: 11.6484 - val_mae: 1.4466 - lr: 0.0020 - 2s/epoch - 2ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 2s - loss: 13.5262 - mse: 13.5262 - mae: 1.4844 - val_loss: 11.7395 - val_mse: 11.7395 - val_mae: 1.4284 - lr: 0.0020 - 2s/epoch - 2ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 2s - loss: 13.4675 - mse: 13.4675 - mae: 1.4842 - val_loss: 11.5282 - val_mse: 11.5282 - val_mae: 1.4669 - lr: 0.0020 - 2s/epoch - 2ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 2s - loss: 13.4422 - mse: 13.4422 - mae: 1.4773 - val_loss: 11.8661 - val_mse: 11.8661 - val_mae: 1.5468 - lr: 0.0020 - 2s/epoch - 2ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 2s - loss: 13.3736 - mse: 13.3736 - mae: 1.4755 - val_loss: 11.3140 - val_mse: 11.3140 - val_mae: 1.4807 - lr: 0.0020 - 2s/epoch - 2ms/step\n",
            "Epoch 13/100\n",
            "1000/1000 - 2s - loss: 13.2974 - mse: 13.2974 - mae: 1.4816 - val_loss: 11.4240 - val_mse: 11.4240 - val_mae: 1.4716 - lr: 0.0020 - 2s/epoch - 2ms/step\n",
            "Epoch 14/100\n",
            "1000/1000 - 2s - loss: 13.2899 - mse: 13.2899 - mae: 1.4777 - val_loss: 11.4526 - val_mse: 11.4526 - val_mae: 1.5106 - lr: 0.0020 - 2s/epoch - 2ms/step\n",
            "Epoch 15/100\n",
            "1000/1000 - 2s - loss: 13.2458 - mse: 13.2458 - mae: 1.4731 - val_loss: 11.4749 - val_mse: 11.4749 - val_mae: 1.4791 - lr: 0.0020 - 2s/epoch - 2ms/step\n",
            "Epoch 16/100\n",
            "1000/1000 - 2s - loss: 13.1834 - mse: 13.1834 - mae: 1.4758 - val_loss: 11.2680 - val_mse: 11.2680 - val_mae: 1.4813 - lr: 0.0020 - 2s/epoch - 2ms/step\n",
            "Epoch 17/100\n",
            "1000/1000 - 2s - loss: 13.0927 - mse: 13.0927 - mae: 1.4747 - val_loss: 11.3006 - val_mse: 11.3006 - val_mae: 1.4785 - lr: 0.0020 - 2s/epoch - 2ms/step\n",
            "Epoch 18/100\n",
            "1000/1000 - 2s - loss: 13.0782 - mse: 13.0782 - mae: 1.4700 - val_loss: 11.3658 - val_mse: 11.3658 - val_mae: 1.4289 - lr: 0.0020 - 2s/epoch - 2ms/step\n",
            "Epoch 19/100\n",
            "1000/1000 - 2s - loss: 13.0279 - mse: 13.0279 - mae: 1.4692 - val_loss: 11.4687 - val_mse: 11.4687 - val_mae: 1.5249 - lr: 0.0020 - 2s/epoch - 2ms/step\n",
            "Epoch 20/100\n",
            "1000/1000 - 2s - loss: 12.9858 - mse: 12.9858 - mae: 1.4667 - val_loss: 11.3589 - val_mse: 11.3589 - val_mae: 1.4651 - lr: 0.0020 - 2s/epoch - 2ms/step\n",
            "Epoch 21/100\n",
            "1000/1000 - 2s - loss: 12.9146 - mse: 12.9146 - mae: 1.4696 - val_loss: 11.2499 - val_mse: 11.2499 - val_mae: 1.5384 - lr: 0.0020 - 2s/epoch - 2ms/step\n",
            "Epoch 22/100\n",
            "1000/1000 - 2s - loss: 12.8952 - mse: 12.8952 - mae: 1.4684 - val_loss: 11.4852 - val_mse: 11.4852 - val_mae: 1.5545 - lr: 0.0020 - 2s/epoch - 2ms/step\n",
            "Epoch 23/100\n",
            "1000/1000 - 2s - loss: 12.8998 - mse: 12.8998 - mae: 1.4679 - val_loss: 11.2421 - val_mse: 11.2421 - val_mae: 1.4826 - lr: 0.0020 - 2s/epoch - 2ms/step\n",
            "Epoch 24/100\n",
            "1000/1000 - 2s - loss: 12.8209 - mse: 12.8209 - mae: 1.4671 - val_loss: 11.3024 - val_mse: 11.3024 - val_mae: 1.4909 - lr: 0.0020 - 2s/epoch - 2ms/step\n",
            "Epoch 25/100\n",
            "1000/1000 - 2s - loss: 12.8032 - mse: 12.8032 - mae: 1.4688 - val_loss: 11.5218 - val_mse: 11.5218 - val_mae: 1.5037 - lr: 0.0020 - 2s/epoch - 2ms/step\n",
            "Epoch 26/100\n",
            "1000/1000 - 2s - loss: 12.7658 - mse: 12.7658 - mae: 1.4678 - val_loss: 11.1368 - val_mse: 11.1368 - val_mae: 1.5118 - lr: 0.0020 - 2s/epoch - 2ms/step\n",
            "Epoch 27/100\n",
            "1000/1000 - 2s - loss: 12.7299 - mse: 12.7299 - mae: 1.4636 - val_loss: 11.3790 - val_mse: 11.3790 - val_mae: 1.5451 - lr: 0.0020 - 2s/epoch - 2ms/step\n",
            "Epoch 28/100\n",
            "1000/1000 - 2s - loss: 12.7555 - mse: 12.7555 - mae: 1.4676 - val_loss: 11.3763 - val_mse: 11.3763 - val_mae: 1.4845 - lr: 0.0020 - 2s/epoch - 2ms/step\n",
            "Epoch 29/100\n",
            "1000/1000 - 2s - loss: 12.6831 - mse: 12.6831 - mae: 1.4635 - val_loss: 11.2470 - val_mse: 11.2470 - val_mae: 1.4921 - lr: 0.0020 - 2s/epoch - 2ms/step\n",
            "Epoch 30/100\n",
            "1000/1000 - 2s - loss: 12.6629 - mse: 12.6629 - mae: 1.4679 - val_loss: 11.2598 - val_mse: 11.2598 - val_mae: 1.4939 - lr: 0.0020 - 2s/epoch - 2ms/step\n",
            "Epoch 31/100\n",
            "1000/1000 - 2s - loss: 12.5780 - mse: 12.5780 - mae: 1.4603 - val_loss: 11.3762 - val_mse: 11.3762 - val_mae: 1.4887 - lr: 0.0020 - 2s/epoch - 2ms/step\n",
            "Score for fold 1: loss of 11.376184463500977\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 10.6461 - mse: 10.6461 - mae: 1.4458 - val_loss: 18.4160 - val_mse: 18.4160 - val_mae: 1.4673 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 10.5641 - mse: 10.5641 - mae: 1.4439 - val_loss: 18.4772 - val_mse: 18.4772 - val_mae: 1.4452 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 10.5409 - mse: 10.5409 - mae: 1.4394 - val_loss: 18.4359 - val_mse: 18.4359 - val_mae: 1.4420 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 10.5021 - mse: 10.5021 - mae: 1.4376 - val_loss: 18.4604 - val_mse: 18.4604 - val_mae: 1.4181 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 10.4802 - mse: 10.4802 - mae: 1.4399 - val_loss: 18.4788 - val_mse: 18.4788 - val_mae: 1.4463 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 10.4478 - mse: 10.4478 - mae: 1.4374 - val_loss: 18.4969 - val_mse: 18.4969 - val_mae: 1.4499 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Score for fold 2: loss of 18.496952056884766\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 11.8652 - mse: 11.8652 - mae: 1.4388 - val_loss: 12.8632 - val_mse: 12.8632 - val_mae: 1.4474 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 11.8494 - mse: 11.8494 - mae: 1.4347 - val_loss: 12.9512 - val_mse: 12.9512 - val_mae: 1.4461 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 11.7893 - mse: 11.7893 - mae: 1.4310 - val_loss: 13.0740 - val_mse: 13.0740 - val_mae: 1.5110 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 11.7857 - mse: 11.7857 - mae: 1.4304 - val_loss: 13.0283 - val_mse: 13.0283 - val_mae: 1.4349 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 11.7330 - mse: 11.7330 - mae: 1.4310 - val_loss: 13.0786 - val_mse: 13.0786 - val_mae: 1.4957 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 11.7335 - mse: 11.7335 - mae: 1.4307 - val_loss: 13.1087 - val_mse: 13.1087 - val_mae: 1.4335 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Score for fold 3: loss of 13.10873794555664\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 12.9845 - mse: 12.9845 - mae: 1.4515 - val_loss: 7.9630 - val_mse: 7.9630 - val_mae: 1.4251 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 12.9332 - mse: 12.9332 - mae: 1.4448 - val_loss: 8.0273 - val_mse: 8.0273 - val_mae: 1.4110 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 12.9053 - mse: 12.9053 - mae: 1.4447 - val_loss: 8.0420 - val_mse: 8.0420 - val_mae: 1.4089 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 12.8690 - mse: 12.8690 - mae: 1.4464 - val_loss: 8.1286 - val_mse: 8.1286 - val_mae: 1.3992 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 12.8827 - mse: 12.8827 - mae: 1.4418 - val_loss: 8.2079 - val_mse: 8.2079 - val_mae: 1.3922 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 12.8545 - mse: 12.8545 - mae: 1.4448 - val_loss: 8.1201 - val_mse: 8.1201 - val_mae: 1.3910 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Score for fold 4: loss of 8.12009048461914\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 12.4433 - mse: 12.4433 - mae: 1.4383 - val_loss: 9.7618 - val_mse: 9.7618 - val_mae: 1.4897 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 12.4070 - mse: 12.4070 - mae: 1.4370 - val_loss: 9.8651 - val_mse: 9.8651 - val_mae: 1.4233 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 12.3672 - mse: 12.3672 - mae: 1.4344 - val_loss: 9.9322 - val_mse: 9.9322 - val_mae: 1.4213 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 12.3815 - mse: 12.3815 - mae: 1.4331 - val_loss: 9.8595 - val_mse: 9.8595 - val_mae: 1.4493 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 12.3227 - mse: 12.3227 - mae: 1.4333 - val_loss: 10.2180 - val_mse: 10.2180 - val_mae: 1.4916 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 12.3533 - mse: 12.3533 - mae: 1.4349 - val_loss: 9.9913 - val_mse: 9.9913 - val_mae: 1.4477 - lr: 0.0010 - 2s/epoch - 2ms/step\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-10-11 22:19:34,854]\u001b[0m Finished trial#1 resulted in value: 12.22. Current best value is 12.22 with parameters: {'activation': 'tanh', 'num_hidden_layer': 2, 'i': 9, 'learning_rate': 0.001992883472921733}.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Score for fold 5: loss of 9.991300582885742\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 3s - loss: 14.6556 - mse: 14.6556 - mae: 1.5869 - val_loss: 15.8398 - val_mse: 15.8398 - val_mae: 1.5594 - lr: 8.4888e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 13.5968 - mse: 13.5968 - mae: 1.5098 - val_loss: 15.3157 - val_mse: 15.3157 - val_mae: 1.5310 - lr: 8.4888e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 13.2791 - mse: 13.2791 - mae: 1.4986 - val_loss: 15.0925 - val_mse: 15.0925 - val_mae: 1.6050 - lr: 8.4888e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 13.0659 - mse: 13.0659 - mae: 1.4880 - val_loss: 14.7805 - val_mse: 14.7805 - val_mae: 1.5834 - lr: 8.4888e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 12.9085 - mse: 12.9085 - mae: 1.4829 - val_loss: 14.6636 - val_mse: 14.6636 - val_mae: 1.5162 - lr: 8.4888e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 12.7846 - mse: 12.7846 - mae: 1.4721 - val_loss: 14.5570 - val_mse: 14.5570 - val_mae: 1.4938 - lr: 8.4888e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 12.6923 - mse: 12.6923 - mae: 1.4666 - val_loss: 14.5441 - val_mse: 14.5441 - val_mae: 1.5214 - lr: 8.4888e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 2s - loss: 12.5516 - mse: 12.5516 - mae: 1.4623 - val_loss: 14.6163 - val_mse: 14.6163 - val_mae: 1.5548 - lr: 8.4888e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 2s - loss: 12.5273 - mse: 12.5273 - mae: 1.4590 - val_loss: 14.3015 - val_mse: 14.3015 - val_mae: 1.4867 - lr: 8.4888e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 2s - loss: 12.4138 - mse: 12.4138 - mae: 1.4503 - val_loss: 14.2556 - val_mse: 14.2556 - val_mae: 1.4896 - lr: 8.4888e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 2s - loss: 12.3225 - mse: 12.3225 - mae: 1.4467 - val_loss: 14.4005 - val_mse: 14.4005 - val_mae: 1.5156 - lr: 8.4888e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 2s - loss: 12.2621 - mse: 12.2621 - mae: 1.4429 - val_loss: 14.5444 - val_mse: 14.5444 - val_mae: 1.5243 - lr: 8.4888e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 13/100\n",
            "1000/1000 - 2s - loss: 12.2329 - mse: 12.2329 - mae: 1.4456 - val_loss: 14.3612 - val_mse: 14.3612 - val_mae: 1.5570 - lr: 8.4888e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 14/100\n",
            "1000/1000 - 2s - loss: 12.1270 - mse: 12.1270 - mae: 1.4368 - val_loss: 14.2551 - val_mse: 14.2551 - val_mae: 1.4792 - lr: 8.4888e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 15/100\n",
            "1000/1000 - 2s - loss: 12.1179 - mse: 12.1179 - mae: 1.4344 - val_loss: 14.1572 - val_mse: 14.1572 - val_mae: 1.5143 - lr: 8.4888e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 16/100\n",
            "1000/1000 - 2s - loss: 12.0665 - mse: 12.0665 - mae: 1.4350 - val_loss: 14.0458 - val_mse: 14.0458 - val_mae: 1.4817 - lr: 8.4888e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 17/100\n",
            "1000/1000 - 2s - loss: 11.9922 - mse: 11.9922 - mae: 1.4292 - val_loss: 14.0502 - val_mse: 14.0502 - val_mae: 1.5491 - lr: 8.4888e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 18/100\n",
            "1000/1000 - 2s - loss: 11.9129 - mse: 11.9129 - mae: 1.4266 - val_loss: 14.1371 - val_mse: 14.1371 - val_mae: 1.5152 - lr: 8.4888e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 19/100\n",
            "1000/1000 - 2s - loss: 11.8769 - mse: 11.8769 - mae: 1.4241 - val_loss: 13.9461 - val_mse: 13.9461 - val_mae: 1.5015 - lr: 8.4888e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 20/100\n",
            "1000/1000 - 2s - loss: 11.8426 - mse: 11.8426 - mae: 1.4209 - val_loss: 13.9516 - val_mse: 13.9516 - val_mae: 1.4798 - lr: 8.4888e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 21/100\n",
            "1000/1000 - 2s - loss: 11.7584 - mse: 11.7584 - mae: 1.4199 - val_loss: 13.9440 - val_mse: 13.9440 - val_mae: 1.5828 - lr: 8.4888e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 22/100\n",
            "1000/1000 - 2s - loss: 11.6594 - mse: 11.6594 - mae: 1.4145 - val_loss: 13.8988 - val_mse: 13.8988 - val_mae: 1.5421 - lr: 8.4888e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 23/100\n",
            "1000/1000 - 2s - loss: 11.5941 - mse: 11.5941 - mae: 1.4137 - val_loss: 13.9343 - val_mse: 13.9343 - val_mae: 1.5113 - lr: 8.4888e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 24/100\n",
            "1000/1000 - 2s - loss: 11.5768 - mse: 11.5768 - mae: 1.4114 - val_loss: 14.0583 - val_mse: 14.0583 - val_mae: 1.4906 - lr: 8.4888e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 25/100\n",
            "1000/1000 - 2s - loss: 11.5241 - mse: 11.5241 - mae: 1.4088 - val_loss: 13.8190 - val_mse: 13.8190 - val_mae: 1.5323 - lr: 8.4888e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 26/100\n",
            "1000/1000 - 2s - loss: 11.4295 - mse: 11.4295 - mae: 1.4078 - val_loss: 13.8536 - val_mse: 13.8536 - val_mae: 1.5350 - lr: 8.4888e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 27/100\n",
            "1000/1000 - 2s - loss: 11.4013 - mse: 11.4013 - mae: 1.4035 - val_loss: 13.9827 - val_mse: 13.9827 - val_mae: 1.5749 - lr: 8.4888e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 28/100\n",
            "1000/1000 - 2s - loss: 11.3978 - mse: 11.3978 - mae: 1.4072 - val_loss: 13.8548 - val_mse: 13.8548 - val_mae: 1.5000 - lr: 8.4888e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 29/100\n",
            "1000/1000 - 2s - loss: 11.3183 - mse: 11.3183 - mae: 1.4018 - val_loss: 13.7865 - val_mse: 13.7865 - val_mae: 1.5177 - lr: 8.4888e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 30/100\n",
            "1000/1000 - 2s - loss: 11.2137 - mse: 11.2137 - mae: 1.3961 - val_loss: 13.8839 - val_mse: 13.8839 - val_mae: 1.5254 - lr: 8.4888e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 31/100\n",
            "1000/1000 - 2s - loss: 11.1624 - mse: 11.1624 - mae: 1.3979 - val_loss: 14.0217 - val_mse: 14.0217 - val_mae: 1.5848 - lr: 8.4888e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 32/100\n",
            "1000/1000 - 2s - loss: 11.1414 - mse: 11.1414 - mae: 1.3923 - val_loss: 13.7995 - val_mse: 13.7995 - val_mae: 1.5264 - lr: 8.4888e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 33/100\n",
            "1000/1000 - 2s - loss: 11.0625 - mse: 11.0625 - mae: 1.3912 - val_loss: 14.0157 - val_mse: 14.0157 - val_mae: 1.5641 - lr: 8.4888e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 34/100\n",
            "1000/1000 - 2s - loss: 11.0684 - mse: 11.0684 - mae: 1.3892 - val_loss: 13.7963 - val_mse: 13.7963 - val_mae: 1.5499 - lr: 8.4888e-04 - 2s/epoch - 2ms/step\n",
            "Score for fold 1: loss of 13.796272277832031\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 10.2014 - mse: 10.2014 - mae: 1.4235 - val_loss: 17.5486 - val_mse: 17.5486 - val_mae: 1.4353 - lr: 8.4888e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 10.1315 - mse: 10.1315 - mae: 1.4190 - val_loss: 17.8549 - val_mse: 17.8549 - val_mae: 1.4519 - lr: 8.4888e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 9.9651 - mse: 9.9651 - mae: 1.4112 - val_loss: 17.7570 - val_mse: 17.7570 - val_mae: 1.4739 - lr: 8.4888e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 9.9213 - mse: 9.9213 - mae: 1.4077 - val_loss: 18.2560 - val_mse: 18.2560 - val_mae: 1.4182 - lr: 8.4888e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 9.8685 - mse: 9.8685 - mae: 1.4057 - val_loss: 17.8095 - val_mse: 17.8095 - val_mae: 1.4279 - lr: 8.4888e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 9.7841 - mse: 9.7841 - mae: 1.4009 - val_loss: 17.6941 - val_mse: 17.6941 - val_mae: 1.4120 - lr: 8.4888e-04 - 2s/epoch - 2ms/step\n",
            "Score for fold 2: loss of 17.694149017333984\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 11.9327 - mse: 11.9327 - mae: 1.4208 - val_loss: 9.1118 - val_mse: 9.1118 - val_mae: 1.3763 - lr: 8.4888e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 11.7548 - mse: 11.7548 - mae: 1.4145 - val_loss: 9.2123 - val_mse: 9.2123 - val_mae: 1.3633 - lr: 8.4888e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 11.6494 - mse: 11.6494 - mae: 1.4085 - val_loss: 9.3611 - val_mse: 9.3611 - val_mae: 1.3815 - lr: 8.4888e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 11.6148 - mse: 11.6148 - mae: 1.4038 - val_loss: 9.3982 - val_mse: 9.3982 - val_mae: 1.4171 - lr: 8.4888e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 11.5471 - mse: 11.5471 - mae: 1.4056 - val_loss: 9.5170 - val_mse: 9.5170 - val_mae: 1.4197 - lr: 8.4888e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 11.4721 - mse: 11.4721 - mae: 1.3977 - val_loss: 9.4166 - val_mse: 9.4166 - val_mae: 1.3888 - lr: 8.4888e-04 - 2s/epoch - 2ms/step\n",
            "Score for fold 3: loss of 9.416566848754883\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 12.1235 - mse: 12.1235 - mae: 1.4146 - val_loss: 6.9004 - val_mse: 6.9004 - val_mae: 1.3346 - lr: 8.4888e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 11.9796 - mse: 11.9796 - mae: 1.4069 - val_loss: 6.9214 - val_mse: 6.9214 - val_mae: 1.3421 - lr: 8.4888e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 11.9162 - mse: 11.9162 - mae: 1.4023 - val_loss: 7.1228 - val_mse: 7.1228 - val_mae: 1.3306 - lr: 8.4888e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 11.8899 - mse: 11.8899 - mae: 1.3957 - val_loss: 7.0890 - val_mse: 7.0890 - val_mae: 1.3710 - lr: 8.4888e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 11.7806 - mse: 11.7806 - mae: 1.3936 - val_loss: 7.1065 - val_mse: 7.1065 - val_mae: 1.4006 - lr: 8.4888e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 11.7992 - mse: 11.7992 - mae: 1.3953 - val_loss: 7.5035 - val_mse: 7.5035 - val_mae: 1.4187 - lr: 8.4888e-04 - 2s/epoch - 2ms/step\n",
            "Score for fold 4: loss of 7.503513336181641\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 11.2303 - mse: 11.2303 - mae: 1.3963 - val_loss: 9.5400 - val_mse: 9.5400 - val_mae: 1.4153 - lr: 8.4888e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 11.0546 - mse: 11.0546 - mae: 1.3894 - val_loss: 9.7645 - val_mse: 9.7645 - val_mae: 1.3763 - lr: 8.4888e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 11.0716 - mse: 11.0716 - mae: 1.3875 - val_loss: 9.6249 - val_mse: 9.6249 - val_mae: 1.3774 - lr: 8.4888e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 11.0046 - mse: 11.0046 - mae: 1.3801 - val_loss: 9.7147 - val_mse: 9.7147 - val_mae: 1.4114 - lr: 8.4888e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 10.9177 - mse: 10.9177 - mae: 1.3758 - val_loss: 9.7233 - val_mse: 9.7233 - val_mae: 1.3959 - lr: 8.4888e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 10.8518 - mse: 10.8518 - mae: 1.3744 - val_loss: 9.8072 - val_mse: 9.8072 - val_mae: 1.3995 - lr: 8.4888e-04 - 2s/epoch - 2ms/step\n",
            "Score for fold 5: loss of 9.80717658996582\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-10-11 22:21:28,746]\u001b[0m Finished trial#2 resulted in value: 11.644000000000002. Current best value is 11.644000000000002 with parameters: {'activation': 'tanh', 'num_hidden_layer': 4, 'i': 6, 'learning_rate': 0.0008488756176057718}.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 14.4142 - mse: 14.4142 - mae: 1.5924 - val_loss: 14.8240 - val_mse: 14.8240 - val_mae: 1.6339 - lr: 0.0088 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 13.7881 - mse: 13.7881 - mae: 1.5473 - val_loss: 14.1742 - val_mse: 14.1742 - val_mae: 1.5566 - lr: 0.0088 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 13.5704 - mse: 13.5704 - mae: 1.5424 - val_loss: 13.9719 - val_mse: 13.9719 - val_mae: 1.4923 - lr: 0.0088 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 13.3815 - mse: 13.3815 - mae: 1.5245 - val_loss: 13.9490 - val_mse: 13.9490 - val_mae: 1.5444 - lr: 0.0088 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 13.3416 - mse: 13.3416 - mae: 1.5248 - val_loss: 13.9318 - val_mse: 13.9318 - val_mae: 1.4923 - lr: 0.0088 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 13.2247 - mse: 13.2247 - mae: 1.5124 - val_loss: 13.8079 - val_mse: 13.8079 - val_mae: 1.5795 - lr: 0.0088 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 13.1513 - mse: 13.1513 - mae: 1.5140 - val_loss: 13.6125 - val_mse: 13.6125 - val_mae: 1.4698 - lr: 0.0088 - 2s/epoch - 2ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 2s - loss: 13.1372 - mse: 13.1372 - mae: 1.5080 - val_loss: 13.5914 - val_mse: 13.5914 - val_mae: 1.4855 - lr: 0.0088 - 2s/epoch - 2ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 2s - loss: 13.0703 - mse: 13.0703 - mae: 1.5065 - val_loss: 13.6292 - val_mse: 13.6292 - val_mae: 1.5290 - lr: 0.0088 - 2s/epoch - 2ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 2s - loss: 13.0126 - mse: 13.0126 - mae: 1.5081 - val_loss: 13.6403 - val_mse: 13.6403 - val_mae: 1.5182 - lr: 0.0088 - 2s/epoch - 2ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 2s - loss: 13.0392 - mse: 13.0392 - mae: 1.5052 - val_loss: 13.5034 - val_mse: 13.5034 - val_mae: 1.4999 - lr: 0.0088 - 2s/epoch - 2ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 2s - loss: 12.9509 - mse: 12.9509 - mae: 1.5040 - val_loss: 13.6567 - val_mse: 13.6567 - val_mae: 1.4939 - lr: 0.0088 - 2s/epoch - 2ms/step\n",
            "Epoch 13/100\n",
            "1000/1000 - 2s - loss: 13.0601 - mse: 13.0601 - mae: 1.5131 - val_loss: 13.4031 - val_mse: 13.4031 - val_mae: 1.5841 - lr: 0.0088 - 2s/epoch - 2ms/step\n",
            "Epoch 14/100\n",
            "1000/1000 - 2s - loss: 13.0047 - mse: 13.0047 - mae: 1.5042 - val_loss: 13.3441 - val_mse: 13.3441 - val_mae: 1.5444 - lr: 0.0088 - 2s/epoch - 2ms/step\n",
            "Epoch 15/100\n",
            "1000/1000 - 2s - loss: 12.9670 - mse: 12.9670 - mae: 1.5076 - val_loss: 13.5872 - val_mse: 13.5872 - val_mae: 1.4936 - lr: 0.0088 - 2s/epoch - 2ms/step\n",
            "Epoch 16/100\n",
            "1000/1000 - 2s - loss: 12.9722 - mse: 12.9722 - mae: 1.5021 - val_loss: 13.2315 - val_mse: 13.2315 - val_mae: 1.4885 - lr: 0.0088 - 2s/epoch - 2ms/step\n",
            "Epoch 17/100\n",
            "1000/1000 - 2s - loss: 12.9178 - mse: 12.9178 - mae: 1.5025 - val_loss: 13.4511 - val_mse: 13.4511 - val_mae: 1.5388 - lr: 0.0088 - 2s/epoch - 2ms/step\n",
            "Epoch 18/100\n",
            "1000/1000 - 2s - loss: 12.8866 - mse: 12.8866 - mae: 1.5008 - val_loss: 13.2413 - val_mse: 13.2413 - val_mae: 1.5073 - lr: 0.0088 - 2s/epoch - 2ms/step\n",
            "Epoch 19/100\n",
            "1000/1000 - 2s - loss: 12.8639 - mse: 12.8639 - mae: 1.5039 - val_loss: 13.2319 - val_mse: 13.2319 - val_mae: 1.5453 - lr: 0.0088 - 2s/epoch - 2ms/step\n",
            "Epoch 20/100\n",
            "1000/1000 - 2s - loss: 12.8382 - mse: 12.8382 - mae: 1.4960 - val_loss: 13.1522 - val_mse: 13.1522 - val_mae: 1.5140 - lr: 0.0088 - 2s/epoch - 2ms/step\n",
            "Epoch 21/100\n",
            "1000/1000 - 2s - loss: 12.8281 - mse: 12.8281 - mae: 1.5002 - val_loss: 13.0733 - val_mse: 13.0733 - val_mae: 1.4940 - lr: 0.0088 - 2s/epoch - 2ms/step\n",
            "Epoch 22/100\n",
            "1000/1000 - 2s - loss: 12.9432 - mse: 12.9432 - mae: 1.5107 - val_loss: 13.0900 - val_mse: 13.0900 - val_mae: 1.5152 - lr: 0.0088 - 2s/epoch - 2ms/step\n",
            "Epoch 23/100\n",
            "1000/1000 - 2s - loss: 12.7741 - mse: 12.7741 - mae: 1.5104 - val_loss: 13.3521 - val_mse: 13.3521 - val_mae: 1.4592 - lr: 0.0088 - 2s/epoch - 2ms/step\n",
            "Epoch 24/100\n",
            "1000/1000 - 2s - loss: 12.8123 - mse: 12.8123 - mae: 1.4997 - val_loss: 13.3835 - val_mse: 13.3835 - val_mae: 1.4508 - lr: 0.0088 - 2s/epoch - 2ms/step\n",
            "Epoch 25/100\n",
            "1000/1000 - 2s - loss: 12.7681 - mse: 12.7681 - mae: 1.5028 - val_loss: 13.3347 - val_mse: 13.3347 - val_mae: 1.5498 - lr: 0.0088 - 2s/epoch - 2ms/step\n",
            "Epoch 26/100\n",
            "1000/1000 - 2s - loss: 12.7992 - mse: 12.7992 - mae: 1.5045 - val_loss: 13.4303 - val_mse: 13.4303 - val_mae: 1.5669 - lr: 0.0088 - 2s/epoch - 2ms/step\n",
            "Score for fold 1: loss of 13.430323600769043\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 12.9175 - mse: 12.9175 - mae: 1.4709 - val_loss: 11.2688 - val_mse: 11.2688 - val_mae: 1.4424 - lr: 0.0018 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 12.8058 - mse: 12.8058 - mae: 1.4619 - val_loss: 11.2589 - val_mse: 11.2589 - val_mae: 1.4613 - lr: 0.0018 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 12.7054 - mse: 12.7054 - mae: 1.4612 - val_loss: 11.3291 - val_mse: 11.3291 - val_mae: 1.4576 - lr: 0.0018 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 12.6606 - mse: 12.6606 - mae: 1.4586 - val_loss: 11.2148 - val_mse: 11.2148 - val_mae: 1.4538 - lr: 0.0018 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 12.6527 - mse: 12.6527 - mae: 1.4586 - val_loss: 11.2823 - val_mse: 11.2823 - val_mae: 1.4553 - lr: 0.0018 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 12.6454 - mse: 12.6454 - mae: 1.4582 - val_loss: 11.2602 - val_mse: 11.2602 - val_mae: 1.4518 - lr: 0.0018 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 12.6024 - mse: 12.6024 - mae: 1.4577 - val_loss: 11.2196 - val_mse: 11.2196 - val_mae: 1.4920 - lr: 0.0018 - 2s/epoch - 2ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 2s - loss: 12.5752 - mse: 12.5752 - mae: 1.4556 - val_loss: 11.2686 - val_mse: 11.2686 - val_mae: 1.4518 - lr: 0.0018 - 2s/epoch - 2ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 2s - loss: 12.5655 - mse: 12.5655 - mae: 1.4557 - val_loss: 11.2855 - val_mse: 11.2855 - val_mae: 1.4779 - lr: 0.0018 - 2s/epoch - 2ms/step\n",
            "Score for fold 2: loss of 11.285504341125488\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 12.3342 - mse: 12.3342 - mae: 1.4505 - val_loss: 11.9992 - val_mse: 11.9992 - val_mae: 1.4531 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 12.2746 - mse: 12.2746 - mae: 1.4501 - val_loss: 12.0465 - val_mse: 12.0465 - val_mae: 1.4717 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 12.2574 - mse: 12.2574 - mae: 1.4494 - val_loss: 12.0876 - val_mse: 12.0876 - val_mae: 1.4669 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 12.2547 - mse: 12.2547 - mae: 1.4494 - val_loss: 12.1211 - val_mse: 12.1211 - val_mae: 1.4732 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 12.2431 - mse: 12.2431 - mae: 1.4488 - val_loss: 12.1587 - val_mse: 12.1587 - val_mae: 1.4791 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 12.2347 - mse: 12.2347 - mae: 1.4489 - val_loss: 12.2202 - val_mse: 12.2202 - val_mae: 1.4514 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Score for fold 3: loss of 12.22022533416748\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 12.8205 - mse: 12.8205 - mae: 1.4597 - val_loss: 9.7671 - val_mse: 9.7671 - val_mae: 1.4149 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 12.8064 - mse: 12.8064 - mae: 1.4576 - val_loss: 9.8212 - val_mse: 9.8212 - val_mae: 1.4403 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 12.7840 - mse: 12.7840 - mae: 1.4567 - val_loss: 9.8289 - val_mse: 9.8289 - val_mae: 1.4287 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 12.7780 - mse: 12.7780 - mae: 1.4551 - val_loss: 9.8152 - val_mse: 9.8152 - val_mae: 1.4111 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 12.7805 - mse: 12.7805 - mae: 1.4570 - val_loss: 9.8216 - val_mse: 9.8216 - val_mae: 1.4132 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 12.7765 - mse: 12.7765 - mae: 1.4532 - val_loss: 9.8233 - val_mse: 9.8233 - val_mae: 1.4176 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Score for fold 4: loss of 9.82331371307373\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 11.2924 - mse: 11.2924 - mae: 1.4511 - val_loss: 15.8614 - val_mse: 15.8614 - val_mae: 1.4241 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 11.2632 - mse: 11.2632 - mae: 1.4487 - val_loss: 15.7737 - val_mse: 15.7737 - val_mae: 1.4612 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 11.2837 - mse: 11.2837 - mae: 1.4460 - val_loss: 15.8658 - val_mse: 15.8658 - val_mae: 1.4529 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 11.2479 - mse: 11.2479 - mae: 1.4476 - val_loss: 15.8852 - val_mse: 15.8852 - val_mae: 1.4514 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 11.2295 - mse: 11.2295 - mae: 1.4454 - val_loss: 15.9201 - val_mse: 15.9201 - val_mae: 1.4437 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 11.2210 - mse: 11.2210 - mae: 1.4485 - val_loss: 15.9455 - val_mse: 15.9455 - val_mae: 1.4686 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 11.2181 - mse: 11.2181 - mae: 1.4481 - val_loss: 15.9463 - val_mse: 15.9463 - val_mae: 1.4555 - lr: 0.0010 - 2s/epoch - 2ms/step\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-10-11 22:23:06,210]\u001b[0m Finished trial#3 resulted in value: 12.541999999999998. Current best value is 11.644000000000002 with parameters: {'activation': 'tanh', 'num_hidden_layer': 4, 'i': 6, 'learning_rate': 0.0008488756176057718}.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Score for fold 5: loss of 15.946349143981934\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 3s - loss: 15.0082 - mse: 15.0082 - mae: 1.6173 - val_loss: 12.8325 - val_mse: 12.8325 - val_mae: 1.5189 - lr: 0.0052 - 3s/epoch - 3ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 14.0751 - mse: 14.0751 - mae: 1.5424 - val_loss: 12.6974 - val_mse: 12.6974 - val_mae: 1.5269 - lr: 0.0052 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 13.7899 - mse: 13.7899 - mae: 1.5352 - val_loss: 12.8142 - val_mse: 12.8142 - val_mae: 1.5552 - lr: 0.0052 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 13.5733 - mse: 13.5733 - mae: 1.5183 - val_loss: 12.5775 - val_mse: 12.5775 - val_mae: 1.5095 - lr: 0.0052 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 13.5319 - mse: 13.5319 - mae: 1.5246 - val_loss: 12.7708 - val_mse: 12.7708 - val_mae: 1.5250 - lr: 0.0052 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 13.4540 - mse: 13.4540 - mae: 1.5220 - val_loss: 12.4324 - val_mse: 12.4324 - val_mae: 1.5139 - lr: 0.0052 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 13.3813 - mse: 13.3813 - mae: 1.5187 - val_loss: 12.2900 - val_mse: 12.2900 - val_mae: 1.5162 - lr: 0.0052 - 2s/epoch - 2ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 2s - loss: 13.2104 - mse: 13.2104 - mae: 1.5118 - val_loss: 12.6927 - val_mse: 12.6927 - val_mae: 1.7026 - lr: 0.0052 - 2s/epoch - 2ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 2s - loss: 13.1425 - mse: 13.1425 - mae: 1.5145 - val_loss: 12.5726 - val_mse: 12.5726 - val_mae: 1.5212 - lr: 0.0052 - 2s/epoch - 2ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 2s - loss: 13.1366 - mse: 13.1366 - mae: 1.5178 - val_loss: 12.3183 - val_mse: 12.3183 - val_mae: 1.5018 - lr: 0.0052 - 2s/epoch - 2ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 2s - loss: 13.0643 - mse: 13.0643 - mae: 1.5107 - val_loss: 12.7901 - val_mse: 12.7901 - val_mae: 1.4745 - lr: 0.0052 - 2s/epoch - 2ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 2s - loss: 13.0008 - mse: 13.0008 - mae: 1.5139 - val_loss: 12.5714 - val_mse: 12.5714 - val_mae: 1.5731 - lr: 0.0052 - 2s/epoch - 2ms/step\n",
            "Score for fold 1: loss of 12.571364402770996\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 12.6242 - mse: 12.6242 - mae: 1.4788 - val_loss: 11.9907 - val_mse: 11.9907 - val_mae: 1.4461 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 12.2489 - mse: 12.2489 - mae: 1.4549 - val_loss: 12.0022 - val_mse: 12.0022 - val_mae: 1.4699 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 12.0606 - mse: 12.0606 - mae: 1.4373 - val_loss: 12.1249 - val_mse: 12.1249 - val_mae: 1.5169 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 11.9449 - mse: 11.9449 - mae: 1.4313 - val_loss: 12.1856 - val_mse: 12.1856 - val_mae: 1.5220 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 11.8107 - mse: 11.8107 - mae: 1.4328 - val_loss: 12.0821 - val_mse: 12.0821 - val_mae: 1.4779 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 11.7722 - mse: 11.7722 - mae: 1.4233 - val_loss: 12.1093 - val_mse: 12.1093 - val_mae: 1.4903 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Score for fold 2: loss of 12.109333992004395\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 10.3142 - mse: 10.3142 - mae: 1.4335 - val_loss: 17.6769 - val_mse: 17.6769 - val_mae: 1.4293 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 10.2758 - mse: 10.2758 - mae: 1.4203 - val_loss: 17.6911 - val_mse: 17.6911 - val_mae: 1.4039 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 10.1608 - mse: 10.1608 - mae: 1.4133 - val_loss: 17.6582 - val_mse: 17.6582 - val_mae: 1.4608 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 10.0533 - mse: 10.0533 - mae: 1.4106 - val_loss: 17.7713 - val_mse: 17.7713 - val_mae: 1.4921 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 10.0330 - mse: 10.0330 - mae: 1.4083 - val_loss: 17.6966 - val_mse: 17.6966 - val_mae: 1.4116 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 9.9830 - mse: 9.9830 - mae: 1.3989 - val_loss: 17.9531 - val_mse: 17.9531 - val_mae: 1.4333 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 9.9928 - mse: 9.9928 - mae: 1.4006 - val_loss: 17.8460 - val_mse: 17.8460 - val_mae: 1.4144 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 2s - loss: 9.8999 - mse: 9.8999 - mae: 1.3956 - val_loss: 17.8027 - val_mse: 17.8027 - val_mae: 1.4546 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Score for fold 3: loss of 17.802696228027344\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 11.9955 - mse: 11.9955 - mae: 1.4105 - val_loss: 9.3924 - val_mse: 9.3924 - val_mae: 1.3882 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 11.8873 - mse: 11.8873 - mae: 1.4000 - val_loss: 9.5915 - val_mse: 9.5915 - val_mae: 1.4484 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 11.8265 - mse: 11.8265 - mae: 1.4064 - val_loss: 9.5462 - val_mse: 9.5462 - val_mae: 1.4511 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 11.7377 - mse: 11.7377 - mae: 1.3945 - val_loss: 9.7385 - val_mse: 9.7385 - val_mae: 1.4057 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 11.6710 - mse: 11.6710 - mae: 1.3980 - val_loss: 9.8350 - val_mse: 9.8350 - val_mae: 1.4296 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 11.6172 - mse: 11.6172 - mae: 1.3920 - val_loss: 9.7880 - val_mse: 9.7880 - val_mae: 1.4440 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Score for fold 4: loss of 9.788028717041016\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 12.1509 - mse: 12.1509 - mae: 1.4096 - val_loss: 7.5894 - val_mse: 7.5894 - val_mae: 1.3587 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 12.0352 - mse: 12.0352 - mae: 1.4030 - val_loss: 7.7425 - val_mse: 7.7425 - val_mae: 1.3740 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 11.9559 - mse: 11.9559 - mae: 1.4025 - val_loss: 7.9191 - val_mse: 7.9191 - val_mae: 1.3624 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 11.8643 - mse: 11.8643 - mae: 1.4001 - val_loss: 7.8173 - val_mse: 7.8173 - val_mae: 1.3646 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 11.8331 - mse: 11.8331 - mae: 1.3930 - val_loss: 7.9761 - val_mse: 7.9761 - val_mae: 1.3456 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 11.7554 - mse: 11.7554 - mae: 1.3881 - val_loss: 8.1644 - val_mse: 8.1644 - val_mae: 1.4104 - lr: 0.0010 - 2s/epoch - 2ms/step\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-10-11 22:24:23,739]\u001b[0m Finished trial#4 resulted in value: 12.086000000000002. Current best value is 11.644000000000002 with parameters: {'activation': 'tanh', 'num_hidden_layer': 4, 'i': 6, 'learning_rate': 0.0008488756176057718}.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Score for fold 5: loss of 8.164427757263184\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 3s - loss: 13.1108 - mse: 13.1108 - mae: 1.5588 - val_loss: 12.5076 - val_mse: 12.5076 - val_mae: 1.5373 - lr: 0.0028 - 3s/epoch - 3ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 12.8299 - mse: 12.8299 - mae: 1.5199 - val_loss: 12.2964 - val_mse: 12.2964 - val_mae: 1.5299 - lr: 0.0028 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 12.5329 - mse: 12.5329 - mae: 1.5041 - val_loss: 12.0900 - val_mse: 12.0900 - val_mae: 1.5295 - lr: 0.0028 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 12.4115 - mse: 12.4115 - mae: 1.4979 - val_loss: 11.9111 - val_mse: 11.9111 - val_mae: 1.5871 - lr: 0.0028 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 12.2472 - mse: 12.2472 - mae: 1.4961 - val_loss: 11.9476 - val_mse: 11.9476 - val_mae: 1.5994 - lr: 0.0028 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 12.3054 - mse: 12.3054 - mae: 1.4825 - val_loss: 11.8506 - val_mse: 11.8506 - val_mae: 1.5558 - lr: 0.0028 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 12.2068 - mse: 12.2068 - mae: 1.4883 - val_loss: 11.8901 - val_mse: 11.8901 - val_mae: 1.4462 - lr: 0.0028 - 2s/epoch - 2ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 2s - loss: 12.2725 - mse: 12.2725 - mae: 1.4844 - val_loss: 11.8166 - val_mse: 11.8166 - val_mae: 1.5073 - lr: 0.0028 - 2s/epoch - 2ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 2s - loss: 12.1397 - mse: 12.1397 - mae: 1.4773 - val_loss: 11.9050 - val_mse: 11.9050 - val_mae: 1.5118 - lr: 0.0028 - 2s/epoch - 2ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 2s - loss: 11.9918 - mse: 11.9918 - mae: 1.4763 - val_loss: 11.6987 - val_mse: 11.6987 - val_mae: 1.4510 - lr: 0.0028 - 2s/epoch - 2ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 2s - loss: 12.0804 - mse: 12.0804 - mae: 1.4705 - val_loss: 11.6812 - val_mse: 11.6812 - val_mae: 1.4987 - lr: 0.0028 - 2s/epoch - 2ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 2s - loss: 12.1166 - mse: 12.1166 - mae: 1.4685 - val_loss: 11.7553 - val_mse: 11.7553 - val_mae: 1.5278 - lr: 0.0028 - 2s/epoch - 2ms/step\n",
            "Epoch 13/100\n",
            "1000/1000 - 2s - loss: 12.1575 - mse: 12.1575 - mae: 1.4664 - val_loss: 11.8380 - val_mse: 11.8380 - val_mae: 1.4479 - lr: 0.0028 - 2s/epoch - 2ms/step\n",
            "Epoch 14/100\n",
            "1000/1000 - 2s - loss: 12.0336 - mse: 12.0336 - mae: 1.4672 - val_loss: 11.6337 - val_mse: 11.6337 - val_mae: 1.5291 - lr: 0.0028 - 2s/epoch - 2ms/step\n",
            "Epoch 15/100\n",
            "1000/1000 - 2s - loss: 12.0670 - mse: 12.0670 - mae: 1.4670 - val_loss: 11.8569 - val_mse: 11.8569 - val_mae: 1.5360 - lr: 0.0028 - 2s/epoch - 2ms/step\n",
            "Epoch 16/100\n",
            "1000/1000 - 2s - loss: 12.0748 - mse: 12.0748 - mae: 1.4648 - val_loss: 11.5004 - val_mse: 11.5004 - val_mae: 1.4565 - lr: 0.0028 - 2s/epoch - 2ms/step\n",
            "Epoch 17/100\n",
            "1000/1000 - 2s - loss: 12.1255 - mse: 12.1255 - mae: 1.4663 - val_loss: 11.7419 - val_mse: 11.7419 - val_mae: 1.6490 - lr: 0.0028 - 2s/epoch - 2ms/step\n",
            "Epoch 18/100\n",
            "1000/1000 - 2s - loss: 11.8178 - mse: 11.8178 - mae: 1.4655 - val_loss: 11.5932 - val_mse: 11.5932 - val_mae: 1.5150 - lr: 0.0028 - 2s/epoch - 2ms/step\n",
            "Epoch 19/100\n",
            "1000/1000 - 2s - loss: 11.7089 - mse: 11.7089 - mae: 1.4605 - val_loss: 11.6549 - val_mse: 11.6549 - val_mae: 1.5002 - lr: 0.0028 - 2s/epoch - 2ms/step\n",
            "Epoch 20/100\n",
            "1000/1000 - 2s - loss: 11.7067 - mse: 11.7067 - mae: 1.4570 - val_loss: 11.6369 - val_mse: 11.6369 - val_mae: 1.4555 - lr: 0.0028 - 2s/epoch - 2ms/step\n",
            "Epoch 21/100\n",
            "1000/1000 - 2s - loss: 11.6311 - mse: 11.6311 - mae: 1.4559 - val_loss: 11.7049 - val_mse: 11.7049 - val_mae: 1.5543 - lr: 0.0028 - 2s/epoch - 2ms/step\n",
            "Score for fold 1: loss of 11.704946517944336\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 11.2390 - mse: 11.2390 - mae: 1.4380 - val_loss: 12.0568 - val_mse: 12.0568 - val_mae: 1.4234 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 11.0164 - mse: 11.0164 - mae: 1.4319 - val_loss: 12.1047 - val_mse: 12.1047 - val_mae: 1.4076 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 11.0977 - mse: 11.0977 - mae: 1.4309 - val_loss: 12.0990 - val_mse: 12.0990 - val_mae: 1.3707 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 11.1569 - mse: 11.1569 - mae: 1.4333 - val_loss: 12.1174 - val_mse: 12.1174 - val_mae: 1.3873 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 11.0452 - mse: 11.0452 - mae: 1.4291 - val_loss: 12.0774 - val_mse: 12.0774 - val_mae: 1.4128 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 11.0391 - mse: 11.0391 - mae: 1.4265 - val_loss: 12.1191 - val_mse: 12.1191 - val_mae: 1.4939 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Score for fold 2: loss of 12.119138717651367\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 11.8489 - mse: 11.8489 - mae: 1.4144 - val_loss: 8.7596 - val_mse: 8.7596 - val_mae: 1.4160 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 11.7876 - mse: 11.7876 - mae: 1.4139 - val_loss: 8.6702 - val_mse: 8.6702 - val_mae: 1.4664 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 11.7274 - mse: 11.7274 - mae: 1.4148 - val_loss: 8.9995 - val_mse: 8.9995 - val_mae: 1.4415 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 11.6504 - mse: 11.6504 - mae: 1.4115 - val_loss: 8.7110 - val_mse: 8.7110 - val_mae: 1.4915 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 11.6621 - mse: 11.6621 - mae: 1.4075 - val_loss: 9.3256 - val_mse: 9.3256 - val_mae: 1.4686 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 11.6341 - mse: 11.6341 - mae: 1.4041 - val_loss: 8.7321 - val_mse: 8.7321 - val_mae: 1.4574 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 11.5038 - mse: 11.5038 - mae: 1.4033 - val_loss: 9.2080 - val_mse: 9.2080 - val_mae: 1.4552 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Score for fold 3: loss of 9.207983016967773\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 10.2912 - mse: 10.2912 - mae: 1.4260 - val_loss: 14.2015 - val_mse: 14.2015 - val_mae: 1.3957 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 10.3649 - mse: 10.3649 - mae: 1.4263 - val_loss: 14.2301 - val_mse: 14.2301 - val_mae: 1.4228 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 10.0926 - mse: 10.0926 - mae: 1.4176 - val_loss: 14.1330 - val_mse: 14.1330 - val_mae: 1.4090 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 10.1398 - mse: 10.1398 - mae: 1.4174 - val_loss: 14.4063 - val_mse: 14.4063 - val_mae: 1.4185 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 10.1079 - mse: 10.1079 - mae: 1.4148 - val_loss: 14.4175 - val_mse: 14.4175 - val_mae: 1.4415 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 9.9788 - mse: 9.9788 - mae: 1.4137 - val_loss: 14.3092 - val_mse: 14.3092 - val_mae: 1.4113 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 10.0416 - mse: 10.0416 - mae: 1.4152 - val_loss: 14.4894 - val_mse: 14.4894 - val_mae: 1.4579 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 2s - loss: 10.1000 - mse: 10.1000 - mae: 1.4148 - val_loss: 14.3570 - val_mse: 14.3570 - val_mae: 1.4089 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Score for fold 4: loss of 14.356956481933594\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 11.5695 - mse: 11.5695 - mae: 1.4145 - val_loss: 9.1728 - val_mse: 9.1728 - val_mae: 1.4267 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 11.2055 - mse: 11.2055 - mae: 1.4049 - val_loss: 9.2253 - val_mse: 9.2253 - val_mae: 1.4407 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 11.2406 - mse: 11.2406 - mae: 1.4055 - val_loss: 9.4086 - val_mse: 9.4086 - val_mae: 1.4177 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 11.2885 - mse: 11.2885 - mae: 1.4059 - val_loss: 9.5003 - val_mse: 9.5003 - val_mae: 1.4423 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 11.1427 - mse: 11.1427 - mae: 1.4054 - val_loss: 9.5196 - val_mse: 9.5196 - val_mae: 1.4387 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 11.1883 - mse: 11.1883 - mae: 1.4008 - val_loss: 9.4508 - val_mse: 9.4508 - val_mae: 1.4197 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Score for fold 5: loss of 9.450791358947754\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-10-11 22:26:02,972]\u001b[0m Finished trial#5 resulted in value: 11.368. Current best value is 11.368 with parameters: {'activation': 'relu', 'num_hidden_layer': 2, 'i': 9, 'learning_rate': 0.0028041147717882187}.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 4s - loss: 12.5191 - mse: 12.5191 - mae: 1.5709 - val_loss: 16.6346 - val_mse: 16.6346 - val_mae: 1.7456 - lr: 0.0073 - 4s/epoch - 4ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 3s - loss: 11.8498 - mse: 11.8498 - mae: 1.5134 - val_loss: 14.9285 - val_mse: 14.9285 - val_mae: 1.5124 - lr: 0.0073 - 3s/epoch - 3ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 3s - loss: 11.9486 - mse: 11.9486 - mae: 1.5164 - val_loss: 15.1636 - val_mse: 15.1636 - val_mae: 1.4964 - lr: 0.0073 - 3s/epoch - 3ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 4s - loss: 11.6897 - mse: 11.6897 - mae: 1.5096 - val_loss: 15.0004 - val_mse: 15.0004 - val_mae: 1.5118 - lr: 0.0073 - 4s/epoch - 4ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 3s - loss: 11.6804 - mse: 11.6804 - mae: 1.4994 - val_loss: 15.1072 - val_mse: 15.1072 - val_mae: 1.5718 - lr: 0.0073 - 3s/epoch - 3ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 4s - loss: 11.5262 - mse: 11.5262 - mae: 1.4937 - val_loss: 15.6482 - val_mse: 15.6482 - val_mae: 1.4980 - lr: 0.0073 - 4s/epoch - 4ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 4s - loss: 11.5312 - mse: 11.5312 - mae: 1.4983 - val_loss: 14.9030 - val_mse: 14.9030 - val_mae: 1.5767 - lr: 0.0073 - 4s/epoch - 4ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 4s - loss: 11.4086 - mse: 11.4086 - mae: 1.4898 - val_loss: 14.9541 - val_mse: 14.9541 - val_mae: 1.4944 - lr: 0.0073 - 4s/epoch - 4ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 4s - loss: 11.2698 - mse: 11.2698 - mae: 1.4892 - val_loss: 15.1932 - val_mse: 15.1932 - val_mae: 1.5821 - lr: 0.0073 - 4s/epoch - 4ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 4s - loss: 11.3689 - mse: 11.3689 - mae: 1.4865 - val_loss: 16.1852 - val_mse: 16.1852 - val_mae: 1.7177 - lr: 0.0073 - 4s/epoch - 4ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 4s - loss: 11.7270 - mse: 11.7270 - mae: 1.4863 - val_loss: 14.9865 - val_mse: 14.9865 - val_mae: 1.5275 - lr: 0.0073 - 4s/epoch - 4ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 4s - loss: 11.4042 - mse: 11.4042 - mae: 1.4787 - val_loss: 14.9361 - val_mse: 14.9361 - val_mae: 1.5143 - lr: 0.0073 - 4s/epoch - 4ms/step\n",
            "Score for fold 1: loss of 14.936150550842285\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 4s - loss: 10.2680 - mse: 10.2680 - mae: 1.4564 - val_loss: 15.5798 - val_mse: 15.5798 - val_mae: 1.4504 - lr: 0.0015 - 4s/epoch - 4ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 4s - loss: 10.1499 - mse: 10.1499 - mae: 1.4462 - val_loss: 15.5112 - val_mse: 15.5112 - val_mae: 1.4340 - lr: 0.0015 - 4s/epoch - 4ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 4s - loss: 10.0611 - mse: 10.0611 - mae: 1.4398 - val_loss: 15.5534 - val_mse: 15.5534 - val_mae: 1.4095 - lr: 0.0015 - 4s/epoch - 4ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 4s - loss: 9.9646 - mse: 9.9646 - mae: 1.4333 - val_loss: 15.4767 - val_mse: 15.4767 - val_mae: 1.4359 - lr: 0.0015 - 4s/epoch - 4ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 4s - loss: 9.8201 - mse: 9.8201 - mae: 1.4290 - val_loss: 15.5704 - val_mse: 15.5704 - val_mae: 1.4810 - lr: 0.0015 - 4s/epoch - 4ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 4s - loss: 9.7840 - mse: 9.7840 - mae: 1.4261 - val_loss: 15.5116 - val_mse: 15.5116 - val_mae: 1.4567 - lr: 0.0015 - 4s/epoch - 4ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 4s - loss: 9.6369 - mse: 9.6369 - mae: 1.4196 - val_loss: 15.6295 - val_mse: 15.6295 - val_mae: 1.4818 - lr: 0.0015 - 4s/epoch - 4ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 4s - loss: 9.6687 - mse: 9.6687 - mae: 1.4170 - val_loss: 15.6969 - val_mse: 15.6969 - val_mae: 1.4324 - lr: 0.0015 - 4s/epoch - 4ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 4s - loss: 9.5592 - mse: 9.5592 - mae: 1.4140 - val_loss: 15.6114 - val_mse: 15.6114 - val_mae: 1.4223 - lr: 0.0015 - 4s/epoch - 4ms/step\n",
            "Score for fold 2: loss of 15.611449241638184\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 4s - loss: 11.5263 - mse: 11.5263 - mae: 1.4251 - val_loss: 7.5386 - val_mse: 7.5386 - val_mae: 1.3437 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 4s - loss: 11.4748 - mse: 11.4748 - mae: 1.4184 - val_loss: 7.5384 - val_mse: 7.5384 - val_mae: 1.3868 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 4s - loss: 11.3438 - mse: 11.3438 - mae: 1.4155 - val_loss: 7.5267 - val_mse: 7.5267 - val_mae: 1.3752 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 4s - loss: 11.2291 - mse: 11.2291 - mae: 1.4120 - val_loss: 7.6764 - val_mse: 7.6764 - val_mae: 1.3889 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 4s - loss: 11.2110 - mse: 11.2110 - mae: 1.4049 - val_loss: 7.8499 - val_mse: 7.8499 - val_mae: 1.3725 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 4s - loss: 11.1321 - mse: 11.1321 - mae: 1.4053 - val_loss: 7.6546 - val_mse: 7.6546 - val_mae: 1.3979 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 4s - loss: 11.0945 - mse: 11.0945 - mae: 1.4010 - val_loss: 7.7486 - val_mse: 7.7486 - val_mae: 1.3885 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 4s - loss: 11.1092 - mse: 11.1092 - mae: 1.3969 - val_loss: 7.8455 - val_mse: 7.8455 - val_mae: 1.4192 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Score for fold 3: loss of 7.845460414886475\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 4s - loss: 11.1664 - mse: 11.1664 - mae: 1.3995 - val_loss: 7.5420 - val_mse: 7.5420 - val_mae: 1.3681 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 4s - loss: 11.0027 - mse: 11.0027 - mae: 1.3895 - val_loss: 7.5510 - val_mse: 7.5510 - val_mae: 1.3926 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 4s - loss: 10.9589 - mse: 10.9589 - mae: 1.3886 - val_loss: 7.4601 - val_mse: 7.4601 - val_mae: 1.3774 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 4s - loss: 10.9386 - mse: 10.9386 - mae: 1.3853 - val_loss: 7.5566 - val_mse: 7.5566 - val_mae: 1.4023 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 4s - loss: 10.8301 - mse: 10.8301 - mae: 1.3817 - val_loss: 7.5195 - val_mse: 7.5195 - val_mae: 1.4014 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 4s - loss: 10.6636 - mse: 10.6636 - mae: 1.3760 - val_loss: 7.7433 - val_mse: 7.7433 - val_mae: 1.3808 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 4s - loss: 10.6296 - mse: 10.6296 - mae: 1.3736 - val_loss: 7.8236 - val_mse: 7.8236 - val_mae: 1.3802 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 4s - loss: 10.6563 - mse: 10.6563 - mae: 1.3707 - val_loss: 7.6596 - val_mse: 7.6596 - val_mae: 1.4182 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Score for fold 4: loss of 7.659559726715088\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 4s - loss: 10.5037 - mse: 10.5037 - mae: 1.3790 - val_loss: 7.9091 - val_mse: 7.9091 - val_mae: 1.3520 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 4s - loss: 10.4189 - mse: 10.4189 - mae: 1.3739 - val_loss: 8.0858 - val_mse: 8.0858 - val_mae: 1.3466 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 4s - loss: 10.2723 - mse: 10.2723 - mae: 1.3692 - val_loss: 8.3052 - val_mse: 8.3052 - val_mae: 1.3522 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 4s - loss: 10.2124 - mse: 10.2124 - mae: 1.3658 - val_loss: 8.5822 - val_mse: 8.5822 - val_mae: 1.3768 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 4s - loss: 10.3182 - mse: 10.3182 - mae: 1.3634 - val_loss: 8.1446 - val_mse: 8.1446 - val_mae: 1.3931 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 4s - loss: 10.0782 - mse: 10.0782 - mae: 1.3556 - val_loss: 8.1731 - val_mse: 8.1731 - val_mae: 1.3781 - lr: 0.0010 - 4s/epoch - 4ms/step\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-10-11 22:28:43,909]\u001b[0m Finished trial#6 resulted in value: 10.846. Current best value is 10.846 with parameters: {'activation': 'relu', 'num_hidden_layer': 3, 'i': 9, 'learning_rate': 0.007308147233672993}.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Score for fold 5: loss of 8.173078536987305\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 4s - loss: 12.3988 - mse: 12.3988 - mae: 1.5913 - val_loss: 21.2109 - val_mse: 21.2109 - val_mae: 1.5962 - lr: 5.7615e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 4s - loss: 11.6736 - mse: 11.6736 - mae: 1.5162 - val_loss: 21.3681 - val_mse: 21.3681 - val_mae: 1.5682 - lr: 5.7615e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 4s - loss: 11.6114 - mse: 11.6114 - mae: 1.5033 - val_loss: 21.1458 - val_mse: 21.1458 - val_mae: 1.5151 - lr: 5.7615e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 4s - loss: 11.4593 - mse: 11.4593 - mae: 1.4973 - val_loss: 20.9595 - val_mse: 20.9595 - val_mae: 1.5139 - lr: 5.7615e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 4s - loss: 11.3486 - mse: 11.3486 - mae: 1.4855 - val_loss: 20.7164 - val_mse: 20.7164 - val_mae: 1.5237 - lr: 5.7615e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 4s - loss: 11.1894 - mse: 11.1894 - mae: 1.4798 - val_loss: 21.0084 - val_mse: 21.0084 - val_mae: 1.5240 - lr: 5.7615e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 4s - loss: 11.1206 - mse: 11.1206 - mae: 1.4769 - val_loss: 20.8812 - val_mse: 20.8812 - val_mae: 1.4474 - lr: 5.7615e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 4s - loss: 11.0146 - mse: 11.0146 - mae: 1.4657 - val_loss: 20.6227 - val_mse: 20.6227 - val_mae: 1.5465 - lr: 5.7615e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 4s - loss: 10.9047 - mse: 10.9047 - mae: 1.4605 - val_loss: 20.5181 - val_mse: 20.5181 - val_mae: 1.5372 - lr: 5.7615e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 4s - loss: 10.8102 - mse: 10.8102 - mae: 1.4583 - val_loss: 20.5104 - val_mse: 20.5104 - val_mae: 1.5017 - lr: 5.7615e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 4s - loss: 10.6963 - mse: 10.6963 - mae: 1.4562 - val_loss: 20.5907 - val_mse: 20.5907 - val_mae: 1.4932 - lr: 5.7615e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 4s - loss: 10.6459 - mse: 10.6459 - mae: 1.4452 - val_loss: 20.4809 - val_mse: 20.4809 - val_mae: 1.4977 - lr: 5.7615e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 13/100\n",
            "1000/1000 - 4s - loss: 10.5403 - mse: 10.5403 - mae: 1.4393 - val_loss: 20.3252 - val_mse: 20.3252 - val_mae: 1.5018 - lr: 5.7615e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 14/100\n",
            "1000/1000 - 4s - loss: 10.3875 - mse: 10.3875 - mae: 1.4406 - val_loss: 20.2962 - val_mse: 20.2962 - val_mae: 1.5518 - lr: 5.7615e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 15/100\n",
            "1000/1000 - 4s - loss: 10.3171 - mse: 10.3171 - mae: 1.4336 - val_loss: 20.2599 - val_mse: 20.2599 - val_mae: 1.5295 - lr: 5.7615e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 16/100\n",
            "1000/1000 - 4s - loss: 10.1503 - mse: 10.1503 - mae: 1.4305 - val_loss: 20.2932 - val_mse: 20.2932 - val_mae: 1.5628 - lr: 5.7615e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 17/100\n",
            "1000/1000 - 4s - loss: 10.0362 - mse: 10.0362 - mae: 1.4264 - val_loss: 20.2294 - val_mse: 20.2294 - val_mae: 1.5994 - lr: 5.7615e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 18/100\n",
            "1000/1000 - 4s - loss: 9.9426 - mse: 9.9426 - mae: 1.4211 - val_loss: 20.3572 - val_mse: 20.3572 - val_mae: 1.6373 - lr: 5.7615e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 19/100\n",
            "1000/1000 - 4s - loss: 9.8910 - mse: 9.8910 - mae: 1.4178 - val_loss: 20.3673 - val_mse: 20.3673 - val_mae: 1.5753 - lr: 5.7615e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 20/100\n",
            "1000/1000 - 4s - loss: 9.7074 - mse: 9.7074 - mae: 1.4176 - val_loss: 20.2737 - val_mse: 20.2737 - val_mae: 1.5649 - lr: 5.7615e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 21/100\n",
            "1000/1000 - 4s - loss: 9.6624 - mse: 9.6624 - mae: 1.4109 - val_loss: 20.1078 - val_mse: 20.1078 - val_mae: 1.5555 - lr: 5.7615e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 22/100\n",
            "1000/1000 - 4s - loss: 9.5323 - mse: 9.5323 - mae: 1.4073 - val_loss: 20.1710 - val_mse: 20.1710 - val_mae: 1.6021 - lr: 5.7615e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 23/100\n",
            "1000/1000 - 4s - loss: 9.4183 - mse: 9.4183 - mae: 1.4068 - val_loss: 20.0974 - val_mse: 20.0974 - val_mae: 1.5697 - lr: 5.7615e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 24/100\n",
            "1000/1000 - 4s - loss: 9.2890 - mse: 9.2890 - mae: 1.3979 - val_loss: 20.1552 - val_mse: 20.1552 - val_mae: 1.6005 - lr: 5.7615e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 25/100\n",
            "1000/1000 - 4s - loss: 9.1295 - mse: 9.1295 - mae: 1.3876 - val_loss: 20.1353 - val_mse: 20.1353 - val_mae: 1.5866 - lr: 5.7615e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 26/100\n",
            "1000/1000 - 4s - loss: 9.0797 - mse: 9.0797 - mae: 1.3896 - val_loss: 19.9814 - val_mse: 19.9814 - val_mae: 1.5839 - lr: 5.7615e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 27/100\n",
            "1000/1000 - 4s - loss: 8.9564 - mse: 8.9564 - mae: 1.3845 - val_loss: 20.0804 - val_mse: 20.0804 - val_mae: 1.6292 - lr: 5.7615e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 28/100\n",
            "1000/1000 - 4s - loss: 8.8495 - mse: 8.8495 - mae: 1.3848 - val_loss: 20.3467 - val_mse: 20.3467 - val_mae: 1.6054 - lr: 5.7615e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 29/100\n",
            "1000/1000 - 4s - loss: 8.6931 - mse: 8.6931 - mae: 1.3791 - val_loss: 20.1048 - val_mse: 20.1048 - val_mae: 1.6557 - lr: 5.7615e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 30/100\n",
            "1000/1000 - 4s - loss: 8.5375 - mse: 8.5375 - mae: 1.3714 - val_loss: 20.3668 - val_mse: 20.3668 - val_mae: 1.6701 - lr: 5.7615e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 31/100\n",
            "1000/1000 - 4s - loss: 8.5677 - mse: 8.5677 - mae: 1.3712 - val_loss: 20.2191 - val_mse: 20.2191 - val_mae: 1.6638 - lr: 5.7615e-04 - 4s/epoch - 4ms/step\n",
            "Score for fold 1: loss of 20.219079971313477\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 4s - loss: 11.5469 - mse: 11.5469 - mae: 1.4480 - val_loss: 8.4058 - val_mse: 8.4058 - val_mae: 1.3626 - lr: 5.7615e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 4s - loss: 11.3960 - mse: 11.3960 - mae: 1.4351 - val_loss: 8.5409 - val_mse: 8.5409 - val_mae: 1.3744 - lr: 5.7615e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 4s - loss: 11.1418 - mse: 11.1418 - mae: 1.4206 - val_loss: 8.6206 - val_mse: 8.6206 - val_mae: 1.4223 - lr: 5.7615e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 4s - loss: 11.0568 - mse: 11.0568 - mae: 1.4104 - val_loss: 8.7886 - val_mse: 8.7886 - val_mae: 1.4002 - lr: 5.7615e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 4s - loss: 11.0226 - mse: 11.0226 - mae: 1.4052 - val_loss: 8.9622 - val_mse: 8.9622 - val_mae: 1.4901 - lr: 5.7615e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 4s - loss: 10.8212 - mse: 10.8212 - mae: 1.4018 - val_loss: 8.7869 - val_mse: 8.7869 - val_mae: 1.4364 - lr: 5.7615e-04 - 4s/epoch - 4ms/step\n",
            "Score for fold 2: loss of 8.78689193725586\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 4s - loss: 10.6247 - mse: 10.6247 - mae: 1.4165 - val_loss: 9.4009 - val_mse: 9.4009 - val_mae: 1.3959 - lr: 5.7615e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 4s - loss: 10.5428 - mse: 10.5428 - mae: 1.4023 - val_loss: 9.3589 - val_mse: 9.3589 - val_mae: 1.3611 - lr: 5.7615e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 4s - loss: 10.4025 - mse: 10.4025 - mae: 1.3914 - val_loss: 9.4277 - val_mse: 9.4277 - val_mae: 1.3640 - lr: 5.7615e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 4s - loss: 10.2725 - mse: 10.2725 - mae: 1.3865 - val_loss: 9.6167 - val_mse: 9.6167 - val_mae: 1.4160 - lr: 5.7615e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 4s - loss: 10.1600 - mse: 10.1600 - mae: 1.3791 - val_loss: 9.6178 - val_mse: 9.6178 - val_mae: 1.4210 - lr: 5.7615e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 4s - loss: 9.9725 - mse: 9.9725 - mae: 1.3674 - val_loss: 9.6975 - val_mse: 9.6975 - val_mae: 1.4534 - lr: 5.7615e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 4s - loss: 9.9062 - mse: 9.9062 - mae: 1.3654 - val_loss: 9.6051 - val_mse: 9.6051 - val_mae: 1.4386 - lr: 5.7615e-04 - 4s/epoch - 4ms/step\n",
            "Score for fold 3: loss of 9.605119705200195\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 4s - loss: 10.8249 - mse: 10.8249 - mae: 1.4045 - val_loss: 5.7664 - val_mse: 5.7664 - val_mae: 1.3499 - lr: 5.7615e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 4s - loss: 10.6500 - mse: 10.6500 - mae: 1.3823 - val_loss: 5.8557 - val_mse: 5.8557 - val_mae: 1.3491 - lr: 5.7615e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 4s - loss: 10.5540 - mse: 10.5540 - mae: 1.3716 - val_loss: 5.9748 - val_mse: 5.9748 - val_mae: 1.3821 - lr: 5.7615e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 4s - loss: 10.3566 - mse: 10.3566 - mae: 1.3655 - val_loss: 6.2530 - val_mse: 6.2530 - val_mae: 1.3548 - lr: 5.7615e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 4s - loss: 10.2987 - mse: 10.2987 - mae: 1.3601 - val_loss: 6.2108 - val_mse: 6.2108 - val_mae: 1.3891 - lr: 5.7615e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 4s - loss: 10.2320 - mse: 10.2320 - mae: 1.3578 - val_loss: 6.2391 - val_mse: 6.2391 - val_mae: 1.4207 - lr: 5.7615e-04 - 4s/epoch - 4ms/step\n",
            "Score for fold 4: loss of 6.239072322845459\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 4s - loss: 9.7009 - mse: 9.7009 - mae: 1.3775 - val_loss: 8.1019 - val_mse: 8.1019 - val_mae: 1.3496 - lr: 5.7615e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 4s - loss: 9.5117 - mse: 9.5117 - mae: 1.3588 - val_loss: 8.2555 - val_mse: 8.2555 - val_mae: 1.3264 - lr: 5.7615e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 4s - loss: 9.3772 - mse: 9.3772 - mae: 1.3428 - val_loss: 8.6109 - val_mse: 8.6109 - val_mae: 1.3658 - lr: 5.7615e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 4s - loss: 9.2483 - mse: 9.2483 - mae: 1.3371 - val_loss: 8.6035 - val_mse: 8.6035 - val_mae: 1.3859 - lr: 5.7615e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 4s - loss: 9.1868 - mse: 9.1868 - mae: 1.3315 - val_loss: 8.4360 - val_mse: 8.4360 - val_mae: 1.3883 - lr: 5.7615e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 4s - loss: 9.0179 - mse: 9.0179 - mae: 1.3197 - val_loss: 8.7645 - val_mse: 8.7645 - val_mae: 1.4372 - lr: 5.7615e-04 - 4s/epoch - 4ms/step\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-10-11 22:32:14,651]\u001b[0m Finished trial#7 resulted in value: 10.724. Current best value is 10.724 with parameters: {'activation': 'tanh', 'num_hidden_layer': 3, 'i': 9, 'learning_rate': 0.0005761450508066512}.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Score for fold 5: loss of 8.764511108398438\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 18.7318 - mse: 18.7318 - mae: 1.8162 - val_loss: 9.7623 - val_mse: 9.7623 - val_mae: 1.5296 - lr: 1.2060e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 15.3911 - mse: 15.3911 - mae: 1.5443 - val_loss: 9.3919 - val_mse: 9.3919 - val_mae: 1.5077 - lr: 1.2060e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 15.0739 - mse: 15.0739 - mae: 1.5286 - val_loss: 9.2416 - val_mse: 9.2416 - val_mae: 1.5059 - lr: 1.2060e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 14.9178 - mse: 14.9178 - mae: 1.5256 - val_loss: 9.1604 - val_mse: 9.1604 - val_mae: 1.4917 - lr: 1.2060e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 14.7908 - mse: 14.7908 - mae: 1.5163 - val_loss: 9.0903 - val_mse: 9.0903 - val_mae: 1.5007 - lr: 1.2060e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 14.7146 - mse: 14.7146 - mae: 1.5112 - val_loss: 9.0508 - val_mse: 9.0508 - val_mae: 1.4962 - lr: 1.2060e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 14.6358 - mse: 14.6358 - mae: 1.5061 - val_loss: 9.0483 - val_mse: 9.0483 - val_mae: 1.4883 - lr: 1.2060e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 2s - loss: 14.5726 - mse: 14.5726 - mae: 1.5018 - val_loss: 8.9586 - val_mse: 8.9586 - val_mae: 1.4817 - lr: 1.2060e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 2s - loss: 14.5032 - mse: 14.5032 - mae: 1.4992 - val_loss: 8.9753 - val_mse: 8.9753 - val_mae: 1.4817 - lr: 1.2060e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 2s - loss: 14.4479 - mse: 14.4479 - mae: 1.4958 - val_loss: 8.9202 - val_mse: 8.9202 - val_mae: 1.4661 - lr: 1.2060e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 2s - loss: 14.4108 - mse: 14.4108 - mae: 1.4912 - val_loss: 8.8819 - val_mse: 8.8819 - val_mae: 1.4768 - lr: 1.2060e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 2s - loss: 14.3685 - mse: 14.3685 - mae: 1.4867 - val_loss: 8.8596 - val_mse: 8.8596 - val_mae: 1.4806 - lr: 1.2060e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 13/100\n",
            "1000/1000 - 2s - loss: 14.3211 - mse: 14.3211 - mae: 1.4887 - val_loss: 8.8420 - val_mse: 8.8420 - val_mae: 1.4534 - lr: 1.2060e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 14/100\n",
            "1000/1000 - 2s - loss: 14.2908 - mse: 14.2908 - mae: 1.4824 - val_loss: 8.8571 - val_mse: 8.8571 - val_mae: 1.4594 - lr: 1.2060e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 15/100\n",
            "1000/1000 - 2s - loss: 14.2641 - mse: 14.2641 - mae: 1.4787 - val_loss: 8.8313 - val_mse: 8.8313 - val_mae: 1.4924 - lr: 1.2060e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 16/100\n",
            "1000/1000 - 2s - loss: 14.2242 - mse: 14.2242 - mae: 1.4807 - val_loss: 8.7759 - val_mse: 8.7759 - val_mae: 1.4864 - lr: 1.2060e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 17/100\n",
            "1000/1000 - 2s - loss: 14.1987 - mse: 14.1987 - mae: 1.4811 - val_loss: 8.7859 - val_mse: 8.7859 - val_mae: 1.4635 - lr: 1.2060e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 18/100\n",
            "1000/1000 - 2s - loss: 14.1756 - mse: 14.1756 - mae: 1.4775 - val_loss: 8.7437 - val_mse: 8.7437 - val_mae: 1.4540 - lr: 1.2060e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 19/100\n",
            "1000/1000 - 2s - loss: 14.1505 - mse: 14.1505 - mae: 1.4736 - val_loss: 8.7238 - val_mse: 8.7238 - val_mae: 1.4676 - lr: 1.2060e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 20/100\n",
            "1000/1000 - 2s - loss: 14.1309 - mse: 14.1309 - mae: 1.4737 - val_loss: 8.7221 - val_mse: 8.7221 - val_mae: 1.4540 - lr: 1.2060e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 21/100\n",
            "1000/1000 - 2s - loss: 14.1017 - mse: 14.1017 - mae: 1.4724 - val_loss: 8.7231 - val_mse: 8.7231 - val_mae: 1.4716 - lr: 1.2060e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 22/100\n",
            "1000/1000 - 2s - loss: 14.0763 - mse: 14.0763 - mae: 1.4725 - val_loss: 8.7381 - val_mse: 8.7381 - val_mae: 1.4556 - lr: 1.2060e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 23/100\n",
            "1000/1000 - 2s - loss: 14.0657 - mse: 14.0657 - mae: 1.4685 - val_loss: 8.6673 - val_mse: 8.6673 - val_mae: 1.4568 - lr: 1.2060e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 24/100\n",
            "1000/1000 - 2s - loss: 14.0436 - mse: 14.0436 - mae: 1.4671 - val_loss: 8.6424 - val_mse: 8.6424 - val_mae: 1.4497 - lr: 1.2060e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 25/100\n",
            "1000/1000 - 2s - loss: 14.0312 - mse: 14.0312 - mae: 1.4654 - val_loss: 8.6292 - val_mse: 8.6292 - val_mae: 1.4507 - lr: 1.2060e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 26/100\n",
            "1000/1000 - 2s - loss: 14.0178 - mse: 14.0178 - mae: 1.4650 - val_loss: 8.6256 - val_mse: 8.6256 - val_mae: 1.4625 - lr: 1.2060e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 27/100\n",
            "1000/1000 - 2s - loss: 13.9920 - mse: 13.9920 - mae: 1.4638 - val_loss: 8.6339 - val_mse: 8.6339 - val_mae: 1.4544 - lr: 1.2060e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 28/100\n",
            "1000/1000 - 2s - loss: 13.9867 - mse: 13.9867 - mae: 1.4657 - val_loss: 8.6330 - val_mse: 8.6330 - val_mae: 1.4724 - lr: 1.2060e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 29/100\n",
            "1000/1000 - 2s - loss: 13.9647 - mse: 13.9647 - mae: 1.4635 - val_loss: 8.6542 - val_mse: 8.6542 - val_mae: 1.4604 - lr: 1.2060e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 30/100\n",
            "1000/1000 - 2s - loss: 13.9520 - mse: 13.9520 - mae: 1.4610 - val_loss: 8.5806 - val_mse: 8.5806 - val_mae: 1.4559 - lr: 1.2060e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 31/100\n",
            "1000/1000 - 2s - loss: 13.9385 - mse: 13.9385 - mae: 1.4627 - val_loss: 8.6125 - val_mse: 8.6125 - val_mae: 1.4581 - lr: 1.2060e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 32/100\n",
            "1000/1000 - 2s - loss: 13.9102 - mse: 13.9102 - mae: 1.4626 - val_loss: 8.5835 - val_mse: 8.5835 - val_mae: 1.4656 - lr: 1.2060e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 33/100\n",
            "1000/1000 - 2s - loss: 13.8986 - mse: 13.8986 - mae: 1.4568 - val_loss: 8.5650 - val_mse: 8.5650 - val_mae: 1.4431 - lr: 1.2060e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 34/100\n",
            "1000/1000 - 2s - loss: 13.8836 - mse: 13.8836 - mae: 1.4570 - val_loss: 8.5600 - val_mse: 8.5600 - val_mae: 1.4493 - lr: 1.2060e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 35/100\n",
            "1000/1000 - 2s - loss: 13.8586 - mse: 13.8586 - mae: 1.4562 - val_loss: 8.5632 - val_mse: 8.5632 - val_mae: 1.4711 - lr: 1.2060e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 36/100\n",
            "1000/1000 - 2s - loss: 13.8444 - mse: 13.8444 - mae: 1.4585 - val_loss: 8.5672 - val_mse: 8.5672 - val_mae: 1.4477 - lr: 1.2060e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 37/100\n",
            "1000/1000 - 2s - loss: 13.8349 - mse: 13.8349 - mae: 1.4548 - val_loss: 8.5230 - val_mse: 8.5230 - val_mae: 1.4504 - lr: 1.2060e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 38/100\n",
            "1000/1000 - 2s - loss: 13.8270 - mse: 13.8270 - mae: 1.4560 - val_loss: 8.4788 - val_mse: 8.4788 - val_mae: 1.4466 - lr: 1.2060e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 39/100\n",
            "1000/1000 - 2s - loss: 13.8002 - mse: 13.8002 - mae: 1.4552 - val_loss: 8.5134 - val_mse: 8.5134 - val_mae: 1.4485 - lr: 1.2060e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 40/100\n",
            "1000/1000 - 2s - loss: 13.7965 - mse: 13.7965 - mae: 1.4537 - val_loss: 8.4847 - val_mse: 8.4847 - val_mae: 1.4510 - lr: 1.2060e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 41/100\n",
            "1000/1000 - 2s - loss: 13.7751 - mse: 13.7751 - mae: 1.4515 - val_loss: 8.4912 - val_mse: 8.4912 - val_mae: 1.4522 - lr: 1.2060e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 42/100\n",
            "1000/1000 - 2s - loss: 13.7542 - mse: 13.7542 - mae: 1.4529 - val_loss: 8.4807 - val_mse: 8.4807 - val_mae: 1.4415 - lr: 1.2060e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 43/100\n",
            "1000/1000 - 2s - loss: 13.7456 - mse: 13.7456 - mae: 1.4502 - val_loss: 8.4709 - val_mse: 8.4709 - val_mae: 1.4418 - lr: 1.2060e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 44/100\n",
            "1000/1000 - 2s - loss: 13.7400 - mse: 13.7400 - mae: 1.4487 - val_loss: 8.5207 - val_mse: 8.5207 - val_mae: 1.4724 - lr: 1.2060e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 45/100\n",
            "1000/1000 - 2s - loss: 13.7269 - mse: 13.7269 - mae: 1.4513 - val_loss: 8.4690 - val_mse: 8.4690 - val_mae: 1.4347 - lr: 1.2060e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 46/100\n",
            "1000/1000 - 2s - loss: 13.7183 - mse: 13.7183 - mae: 1.4450 - val_loss: 8.4390 - val_mse: 8.4390 - val_mae: 1.4421 - lr: 1.2060e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 47/100\n",
            "1000/1000 - 2s - loss: 13.7002 - mse: 13.7002 - mae: 1.4501 - val_loss: 8.5141 - val_mse: 8.5141 - val_mae: 1.4373 - lr: 1.2060e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 48/100\n",
            "1000/1000 - 2s - loss: 13.6857 - mse: 13.6857 - mae: 1.4473 - val_loss: 8.4618 - val_mse: 8.4618 - val_mae: 1.4410 - lr: 1.2060e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 49/100\n",
            "1000/1000 - 2s - loss: 13.6545 - mse: 13.6545 - mae: 1.4475 - val_loss: 8.4688 - val_mse: 8.4688 - val_mae: 1.4296 - lr: 1.2060e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 50/100\n",
            "1000/1000 - 2s - loss: 13.6548 - mse: 13.6548 - mae: 1.4451 - val_loss: 8.4848 - val_mse: 8.4848 - val_mae: 1.4436 - lr: 1.2060e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 51/100\n",
            "1000/1000 - 2s - loss: 13.6550 - mse: 13.6550 - mae: 1.4435 - val_loss: 8.3903 - val_mse: 8.3903 - val_mae: 1.4436 - lr: 1.2060e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 52/100\n",
            "1000/1000 - 2s - loss: 13.6282 - mse: 13.6282 - mae: 1.4434 - val_loss: 8.4057 - val_mse: 8.4057 - val_mae: 1.4357 - lr: 1.2060e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 53/100\n",
            "1000/1000 - 2s - loss: 13.6231 - mse: 13.6231 - mae: 1.4433 - val_loss: 8.4115 - val_mse: 8.4115 - val_mae: 1.4622 - lr: 1.2060e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 54/100\n",
            "1000/1000 - 2s - loss: 13.6244 - mse: 13.6244 - mae: 1.4440 - val_loss: 8.3957 - val_mse: 8.3957 - val_mae: 1.4402 - lr: 1.2060e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 55/100\n",
            "1000/1000 - 2s - loss: 13.6078 - mse: 13.6078 - mae: 1.4428 - val_loss: 8.3721 - val_mse: 8.3721 - val_mae: 1.4501 - lr: 1.2060e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 56/100\n",
            "1000/1000 - 2s - loss: 13.5803 - mse: 13.5803 - mae: 1.4431 - val_loss: 8.3822 - val_mse: 8.3822 - val_mae: 1.4554 - lr: 1.2060e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 57/100\n",
            "1000/1000 - 2s - loss: 13.5789 - mse: 13.5789 - mae: 1.4425 - val_loss: 8.3490 - val_mse: 8.3490 - val_mae: 1.4391 - lr: 1.2060e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 58/100\n",
            "1000/1000 - 2s - loss: 13.5676 - mse: 13.5676 - mae: 1.4410 - val_loss: 8.4020 - val_mse: 8.4020 - val_mae: 1.4404 - lr: 1.2060e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 59/100\n",
            "1000/1000 - 2s - loss: 13.5509 - mse: 13.5509 - mae: 1.4397 - val_loss: 8.4451 - val_mse: 8.4451 - val_mae: 1.4675 - lr: 1.2060e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 60/100\n",
            "1000/1000 - 2s - loss: 13.5481 - mse: 13.5481 - mae: 1.4425 - val_loss: 8.4239 - val_mse: 8.4239 - val_mae: 1.4395 - lr: 1.2060e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 61/100\n",
            "1000/1000 - 2s - loss: 13.5320 - mse: 13.5320 - mae: 1.4406 - val_loss: 8.3732 - val_mse: 8.3732 - val_mae: 1.4465 - lr: 1.2060e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 62/100\n",
            "1000/1000 - 2s - loss: 13.5327 - mse: 13.5327 - mae: 1.4395 - val_loss: 8.3133 - val_mse: 8.3133 - val_mae: 1.4475 - lr: 1.2060e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 63/100\n",
            "1000/1000 - 2s - loss: 13.5034 - mse: 13.5034 - mae: 1.4394 - val_loss: 8.3537 - val_mse: 8.3537 - val_mae: 1.4370 - lr: 1.2060e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 64/100\n",
            "1000/1000 - 2s - loss: 13.5246 - mse: 13.5246 - mae: 1.4395 - val_loss: 8.3632 - val_mse: 8.3632 - val_mae: 1.4499 - lr: 1.2060e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 65/100\n",
            "1000/1000 - 2s - loss: 13.5004 - mse: 13.5004 - mae: 1.4394 - val_loss: 8.3375 - val_mse: 8.3375 - val_mae: 1.4374 - lr: 1.2060e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 66/100\n",
            "1000/1000 - 2s - loss: 13.4949 - mse: 13.4949 - mae: 1.4385 - val_loss: 8.3204 - val_mse: 8.3204 - val_mae: 1.4459 - lr: 1.2060e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 67/100\n",
            "1000/1000 - 2s - loss: 13.4774 - mse: 13.4774 - mae: 1.4387 - val_loss: 8.3040 - val_mse: 8.3040 - val_mae: 1.4263 - lr: 1.2060e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 68/100\n",
            "1000/1000 - 2s - loss: 13.4590 - mse: 13.4590 - mae: 1.4352 - val_loss: 8.3383 - val_mse: 8.3383 - val_mae: 1.4462 - lr: 1.2060e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 69/100\n",
            "1000/1000 - 2s - loss: 13.4554 - mse: 13.4554 - mae: 1.4370 - val_loss: 8.3440 - val_mse: 8.3440 - val_mae: 1.4323 - lr: 1.2060e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 70/100\n",
            "1000/1000 - 2s - loss: 13.4644 - mse: 13.4644 - mae: 1.4348 - val_loss: 8.3081 - val_mse: 8.3081 - val_mae: 1.4441 - lr: 1.2060e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 71/100\n",
            "1000/1000 - 2s - loss: 13.4499 - mse: 13.4499 - mae: 1.4343 - val_loss: 8.3212 - val_mse: 8.3212 - val_mae: 1.4462 - lr: 1.2060e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 72/100\n",
            "1000/1000 - 2s - loss: 13.4355 - mse: 13.4355 - mae: 1.4373 - val_loss: 8.3377 - val_mse: 8.3377 - val_mae: 1.4410 - lr: 1.2060e-04 - 2s/epoch - 2ms/step\n",
            "Score for fold 1: loss of 8.337699890136719\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 10.2597 - mse: 10.2597 - mae: 1.4318 - val_loss: 20.9728 - val_mse: 20.9728 - val_mae: 1.4482 - lr: 1.2060e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 10.2482 - mse: 10.2482 - mae: 1.4345 - val_loss: 20.9599 - val_mse: 20.9599 - val_mae: 1.4453 - lr: 1.2060e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 10.2178 - mse: 10.2178 - mae: 1.4315 - val_loss: 20.9646 - val_mse: 20.9646 - val_mae: 1.4483 - lr: 1.2060e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 10.2444 - mse: 10.2444 - mae: 1.4290 - val_loss: 20.9627 - val_mse: 20.9627 - val_mae: 1.4529 - lr: 1.2060e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 10.1936 - mse: 10.1936 - mae: 1.4299 - val_loss: 20.9466 - val_mse: 20.9466 - val_mae: 1.4502 - lr: 1.2060e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 10.1982 - mse: 10.1982 - mae: 1.4297 - val_loss: 21.0311 - val_mse: 21.0311 - val_mae: 1.4625 - lr: 1.2060e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 10.1984 - mse: 10.1984 - mae: 1.4311 - val_loss: 20.9488 - val_mse: 20.9488 - val_mae: 1.4474 - lr: 1.2060e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 2s - loss: 10.1714 - mse: 10.1714 - mae: 1.4292 - val_loss: 20.9888 - val_mse: 20.9888 - val_mae: 1.4452 - lr: 1.2060e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 2s - loss: 10.1702 - mse: 10.1702 - mae: 1.4276 - val_loss: 20.9851 - val_mse: 20.9851 - val_mae: 1.4638 - lr: 1.2060e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 2s - loss: 10.1637 - mse: 10.1637 - mae: 1.4281 - val_loss: 20.9631 - val_mse: 20.9631 - val_mae: 1.4573 - lr: 1.2060e-04 - 2s/epoch - 2ms/step\n",
            "Score for fold 2: loss of 20.96312713623047\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 13.1780 - mse: 13.1780 - mae: 1.4384 - val_loss: 8.8165 - val_mse: 8.8165 - val_mae: 1.4179 - lr: 1.2060e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 13.1641 - mse: 13.1641 - mae: 1.4376 - val_loss: 8.8524 - val_mse: 8.8524 - val_mae: 1.4402 - lr: 1.2060e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 13.1447 - mse: 13.1447 - mae: 1.4362 - val_loss: 8.8252 - val_mse: 8.8252 - val_mae: 1.4163 - lr: 1.2060e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 13.1404 - mse: 13.1404 - mae: 1.4385 - val_loss: 8.8237 - val_mse: 8.8237 - val_mae: 1.4159 - lr: 1.2060e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 13.1347 - mse: 13.1347 - mae: 1.4367 - val_loss: 8.8421 - val_mse: 8.8421 - val_mae: 1.4155 - lr: 1.2060e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 13.1349 - mse: 13.1349 - mae: 1.4345 - val_loss: 8.8389 - val_mse: 8.8389 - val_mae: 1.4425 - lr: 1.2060e-04 - 2s/epoch - 2ms/step\n",
            "Score for fold 3: loss of 8.838912963867188\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 12.6288 - mse: 12.6288 - mae: 1.4324 - val_loss: 10.8341 - val_mse: 10.8341 - val_mae: 1.4243 - lr: 1.2060e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 12.6244 - mse: 12.6244 - mae: 1.4293 - val_loss: 10.8347 - val_mse: 10.8347 - val_mae: 1.4370 - lr: 1.2060e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 12.6169 - mse: 12.6169 - mae: 1.4308 - val_loss: 10.8720 - val_mse: 10.8720 - val_mae: 1.4182 - lr: 1.2060e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 12.5977 - mse: 12.5977 - mae: 1.4273 - val_loss: 10.8849 - val_mse: 10.8849 - val_mae: 1.4449 - lr: 1.2060e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 12.5879 - mse: 12.5879 - mae: 1.4299 - val_loss: 10.9108 - val_mse: 10.9108 - val_mae: 1.4282 - lr: 1.2060e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 12.5900 - mse: 12.5900 - mae: 1.4270 - val_loss: 10.9262 - val_mse: 10.9262 - val_mae: 1.4389 - lr: 1.2060e-04 - 2s/epoch - 2ms/step\n",
            "Score for fold 4: loss of 10.926224708557129\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 12.1602 - mse: 12.1602 - mae: 1.4317 - val_loss: 12.6814 - val_mse: 12.6814 - val_mae: 1.4455 - lr: 1.2060e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 12.1584 - mse: 12.1584 - mae: 1.4282 - val_loss: 12.6916 - val_mse: 12.6916 - val_mae: 1.4335 - lr: 1.2060e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 12.1360 - mse: 12.1360 - mae: 1.4284 - val_loss: 12.6621 - val_mse: 12.6621 - val_mae: 1.4226 - lr: 1.2060e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 12.1403 - mse: 12.1403 - mae: 1.4323 - val_loss: 12.7026 - val_mse: 12.7026 - val_mae: 1.4322 - lr: 1.2060e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 12.1183 - mse: 12.1183 - mae: 1.4278 - val_loss: 12.6801 - val_mse: 12.6801 - val_mae: 1.4281 - lr: 1.2060e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 12.1205 - mse: 12.1205 - mae: 1.4274 - val_loss: 12.6954 - val_mse: 12.6954 - val_mae: 1.4412 - lr: 1.2060e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 12.1190 - mse: 12.1190 - mae: 1.4295 - val_loss: 12.6855 - val_mse: 12.6855 - val_mae: 1.4292 - lr: 1.2060e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 2s - loss: 12.0931 - mse: 12.0931 - mae: 1.4269 - val_loss: 12.7746 - val_mse: 12.7746 - val_mae: 1.4416 - lr: 1.2060e-04 - 2s/epoch - 2ms/step\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-10-11 22:35:24,850]\u001b[0m Finished trial#8 resulted in value: 12.368. Current best value is 10.724 with parameters: {'activation': 'tanh', 'num_hidden_layer': 3, 'i': 9, 'learning_rate': 0.0005761450508066512}.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Score for fold 5: loss of 12.774550437927246\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 3s - loss: 14.4745 - mse: 14.4745 - mae: 1.6097 - val_loss: 15.0505 - val_mse: 15.0505 - val_mae: 1.5244 - lr: 0.0058 - 3s/epoch - 3ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 13.9671 - mse: 13.9671 - mae: 1.5748 - val_loss: 14.2725 - val_mse: 14.2725 - val_mae: 1.5468 - lr: 0.0058 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 13.7733 - mse: 13.7733 - mae: 1.5675 - val_loss: 14.4188 - val_mse: 14.4188 - val_mae: 1.5142 - lr: 0.0058 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 13.6467 - mse: 13.6467 - mae: 1.5578 - val_loss: 14.0162 - val_mse: 14.0162 - val_mae: 1.5586 - lr: 0.0058 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 13.5321 - mse: 13.5321 - mae: 1.5490 - val_loss: 13.7353 - val_mse: 13.7353 - val_mae: 1.5577 - lr: 0.0058 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 13.3970 - mse: 13.3970 - mae: 1.5487 - val_loss: 13.8630 - val_mse: 13.8630 - val_mae: 1.5736 - lr: 0.0058 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 13.3356 - mse: 13.3356 - mae: 1.5404 - val_loss: 13.7090 - val_mse: 13.7090 - val_mae: 1.5282 - lr: 0.0058 - 2s/epoch - 2ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 2s - loss: 13.2548 - mse: 13.2548 - mae: 1.5382 - val_loss: 13.5921 - val_mse: 13.5921 - val_mae: 1.5438 - lr: 0.0058 - 2s/epoch - 2ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 2s - loss: 13.2007 - mse: 13.2007 - mae: 1.5354 - val_loss: 13.4877 - val_mse: 13.4877 - val_mae: 1.5825 - lr: 0.0058 - 2s/epoch - 2ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 2s - loss: 13.0742 - mse: 13.0742 - mae: 1.5405 - val_loss: 13.9381 - val_mse: 13.9381 - val_mae: 1.6015 - lr: 0.0058 - 2s/epoch - 2ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 2s - loss: 13.0462 - mse: 13.0462 - mae: 1.5322 - val_loss: 13.4673 - val_mse: 13.4673 - val_mae: 1.5774 - lr: 0.0058 - 2s/epoch - 2ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 2s - loss: 13.0622 - mse: 13.0622 - mae: 1.5339 - val_loss: 13.4466 - val_mse: 13.4466 - val_mae: 1.5091 - lr: 0.0058 - 2s/epoch - 2ms/step\n",
            "Epoch 13/100\n",
            "1000/1000 - 2s - loss: 12.9584 - mse: 12.9584 - mae: 1.5382 - val_loss: 13.7249 - val_mse: 13.7249 - val_mae: 1.6855 - lr: 0.0058 - 2s/epoch - 2ms/step\n",
            "Epoch 14/100\n",
            "1000/1000 - 2s - loss: 12.9519 - mse: 12.9519 - mae: 1.5431 - val_loss: 13.3416 - val_mse: 13.3416 - val_mae: 1.6057 - lr: 0.0058 - 2s/epoch - 2ms/step\n",
            "Epoch 15/100\n",
            "1000/1000 - 2s - loss: 12.8934 - mse: 12.8934 - mae: 1.5357 - val_loss: 13.2658 - val_mse: 13.2658 - val_mae: 1.5218 - lr: 0.0058 - 2s/epoch - 2ms/step\n",
            "Epoch 16/100\n",
            "1000/1000 - 2s - loss: 12.8341 - mse: 12.8341 - mae: 1.5374 - val_loss: 13.6254 - val_mse: 13.6254 - val_mae: 1.6431 - lr: 0.0058 - 2s/epoch - 2ms/step\n",
            "Epoch 17/100\n",
            "1000/1000 - 2s - loss: 12.8449 - mse: 12.8449 - mae: 1.5341 - val_loss: 13.3315 - val_mse: 13.3315 - val_mae: 1.5358 - lr: 0.0058 - 2s/epoch - 2ms/step\n",
            "Epoch 18/100\n",
            "1000/1000 - 2s - loss: 12.8340 - mse: 12.8340 - mae: 1.5396 - val_loss: 13.2361 - val_mse: 13.2361 - val_mae: 1.5922 - lr: 0.0058 - 2s/epoch - 2ms/step\n",
            "Epoch 19/100\n",
            "1000/1000 - 2s - loss: 12.8404 - mse: 12.8404 - mae: 1.5291 - val_loss: 13.5023 - val_mse: 13.5023 - val_mae: 1.6488 - lr: 0.0058 - 2s/epoch - 2ms/step\n",
            "Epoch 20/100\n",
            "1000/1000 - 2s - loss: 12.7419 - mse: 12.7419 - mae: 1.5223 - val_loss: 13.2883 - val_mse: 13.2883 - val_mae: 1.6181 - lr: 0.0058 - 2s/epoch - 2ms/step\n",
            "Epoch 21/100\n",
            "1000/1000 - 2s - loss: 12.7409 - mse: 12.7409 - mae: 1.5276 - val_loss: 13.8635 - val_mse: 13.8635 - val_mae: 1.6693 - lr: 0.0058 - 2s/epoch - 2ms/step\n",
            "Epoch 22/100\n",
            "1000/1000 - 2s - loss: 12.7325 - mse: 12.7325 - mae: 1.5271 - val_loss: 13.2128 - val_mse: 13.2128 - val_mae: 1.6016 - lr: 0.0058 - 2s/epoch - 2ms/step\n",
            "Epoch 23/100\n",
            "1000/1000 - 2s - loss: 12.7168 - mse: 12.7168 - mae: 1.5330 - val_loss: 13.2941 - val_mse: 13.2941 - val_mae: 1.5108 - lr: 0.0058 - 2s/epoch - 2ms/step\n",
            "Epoch 24/100\n",
            "1000/1000 - 2s - loss: 12.6630 - mse: 12.6630 - mae: 1.5249 - val_loss: 13.2121 - val_mse: 13.2121 - val_mae: 1.5451 - lr: 0.0058 - 2s/epoch - 2ms/step\n",
            "Epoch 25/100\n",
            "1000/1000 - 2s - loss: 12.6651 - mse: 12.6651 - mae: 1.5395 - val_loss: 13.9242 - val_mse: 13.9242 - val_mae: 1.7013 - lr: 0.0058 - 2s/epoch - 2ms/step\n",
            "Epoch 26/100\n",
            "1000/1000 - 2s - loss: 12.6636 - mse: 12.6636 - mae: 1.5346 - val_loss: 13.1840 - val_mse: 13.1840 - val_mae: 1.5560 - lr: 0.0058 - 2s/epoch - 2ms/step\n",
            "Epoch 27/100\n",
            "1000/1000 - 2s - loss: 12.6448 - mse: 12.6448 - mae: 1.5318 - val_loss: 13.2253 - val_mse: 13.2253 - val_mae: 1.5750 - lr: 0.0058 - 2s/epoch - 2ms/step\n",
            "Epoch 28/100\n",
            "1000/1000 - 2s - loss: 12.6615 - mse: 12.6615 - mae: 1.5319 - val_loss: 13.6497 - val_mse: 13.6497 - val_mae: 1.6194 - lr: 0.0058 - 2s/epoch - 2ms/step\n",
            "Epoch 29/100\n",
            "1000/1000 - 2s - loss: 12.6505 - mse: 12.6505 - mae: 1.5369 - val_loss: 13.2921 - val_mse: 13.2921 - val_mae: 1.5990 - lr: 0.0058 - 2s/epoch - 2ms/step\n",
            "Epoch 30/100\n",
            "1000/1000 - 2s - loss: 12.5221 - mse: 12.5221 - mae: 1.5368 - val_loss: 13.0991 - val_mse: 13.0991 - val_mae: 1.6602 - lr: 0.0058 - 2s/epoch - 2ms/step\n",
            "Epoch 31/100\n",
            "1000/1000 - 2s - loss: 12.5560 - mse: 12.5560 - mae: 1.5340 - val_loss: 13.0475 - val_mse: 13.0475 - val_mae: 1.6077 - lr: 0.0058 - 2s/epoch - 2ms/step\n",
            "Epoch 32/100\n",
            "1000/1000 - 2s - loss: 12.5047 - mse: 12.5047 - mae: 1.5361 - val_loss: 13.0077 - val_mse: 13.0077 - val_mae: 1.5735 - lr: 0.0058 - 2s/epoch - 2ms/step\n",
            "Epoch 33/100\n",
            "1000/1000 - 2s - loss: 12.4874 - mse: 12.4874 - mae: 1.5364 - val_loss: 13.3224 - val_mse: 13.3224 - val_mae: 1.6502 - lr: 0.0058 - 2s/epoch - 2ms/step\n",
            "Epoch 34/100\n",
            "1000/1000 - 2s - loss: 12.5219 - mse: 12.5219 - mae: 1.5365 - val_loss: 12.9464 - val_mse: 12.9464 - val_mae: 1.5454 - lr: 0.0058 - 2s/epoch - 2ms/step\n",
            "Epoch 35/100\n",
            "1000/1000 - 2s - loss: 12.4364 - mse: 12.4364 - mae: 1.5258 - val_loss: 12.9852 - val_mse: 12.9852 - val_mae: 1.6251 - lr: 0.0058 - 2s/epoch - 2ms/step\n",
            "Epoch 36/100\n",
            "1000/1000 - 2s - loss: 12.4416 - mse: 12.4416 - mae: 1.5259 - val_loss: 13.1403 - val_mse: 13.1403 - val_mae: 1.6499 - lr: 0.0058 - 2s/epoch - 2ms/step\n",
            "Epoch 37/100\n",
            "1000/1000 - 2s - loss: 12.4174 - mse: 12.4174 - mae: 1.5335 - val_loss: 13.0037 - val_mse: 13.0037 - val_mae: 1.5906 - lr: 0.0058 - 2s/epoch - 2ms/step\n",
            "Epoch 38/100\n",
            "1000/1000 - 2s - loss: 12.3732 - mse: 12.3732 - mae: 1.5326 - val_loss: 12.9921 - val_mse: 12.9921 - val_mae: 1.5606 - lr: 0.0058 - 2s/epoch - 2ms/step\n",
            "Epoch 39/100\n",
            "1000/1000 - 2s - loss: 12.4024 - mse: 12.4024 - mae: 1.5272 - val_loss: 13.2957 - val_mse: 13.2957 - val_mae: 1.6641 - lr: 0.0058 - 2s/epoch - 2ms/step\n",
            "Score for fold 1: loss of 13.295649528503418\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 10.2577 - mse: 10.2577 - mae: 1.4466 - val_loss: 19.0015 - val_mse: 19.0015 - val_mae: 1.4753 - lr: 0.0012 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 10.1275 - mse: 10.1275 - mae: 1.4370 - val_loss: 18.9531 - val_mse: 18.9531 - val_mae: 1.4454 - lr: 0.0012 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 10.0950 - mse: 10.0950 - mae: 1.4322 - val_loss: 18.8052 - val_mse: 18.8052 - val_mae: 1.4742 - lr: 0.0012 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 10.0488 - mse: 10.0488 - mae: 1.4289 - val_loss: 18.8514 - val_mse: 18.8514 - val_mae: 1.4797 - lr: 0.0012 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 10.0357 - mse: 10.0357 - mae: 1.4248 - val_loss: 18.8006 - val_mse: 18.8006 - val_mae: 1.4845 - lr: 0.0012 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 9.9962 - mse: 9.9962 - mae: 1.4303 - val_loss: 18.8583 - val_mse: 18.8583 - val_mae: 1.4789 - lr: 0.0012 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 9.9875 - mse: 9.9875 - mae: 1.4268 - val_loss: 18.8318 - val_mse: 18.8318 - val_mae: 1.4662 - lr: 0.0012 - 2s/epoch - 2ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 2s - loss: 9.9537 - mse: 9.9537 - mae: 1.4275 - val_loss: 19.1289 - val_mse: 19.1289 - val_mae: 1.5269 - lr: 0.0012 - 2s/epoch - 2ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 2s - loss: 9.9463 - mse: 9.9463 - mae: 1.4225 - val_loss: 18.9111 - val_mse: 18.9111 - val_mae: 1.4796 - lr: 0.0012 - 2s/epoch - 2ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 2s - loss: 9.9013 - mse: 9.9013 - mae: 1.4217 - val_loss: 18.8949 - val_mse: 18.8949 - val_mae: 1.5174 - lr: 0.0012 - 2s/epoch - 2ms/step\n",
            "Score for fold 2: loss of 18.894941329956055\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 12.4503 - mse: 12.4503 - mae: 1.4433 - val_loss: 8.5902 - val_mse: 8.5902 - val_mae: 1.4018 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 12.4194 - mse: 12.4194 - mae: 1.4388 - val_loss: 8.5714 - val_mse: 8.5714 - val_mae: 1.4047 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 12.3809 - mse: 12.3809 - mae: 1.4348 - val_loss: 8.7272 - val_mse: 8.7272 - val_mae: 1.4114 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 12.3430 - mse: 12.3430 - mae: 1.4346 - val_loss: 8.7561 - val_mse: 8.7561 - val_mae: 1.4101 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 12.3247 - mse: 12.3247 - mae: 1.4325 - val_loss: 8.7992 - val_mse: 8.7992 - val_mae: 1.4593 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 12.2758 - mse: 12.2758 - mae: 1.4322 - val_loss: 8.7900 - val_mse: 8.7900 - val_mae: 1.4060 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 12.2620 - mse: 12.2620 - mae: 1.4319 - val_loss: 8.8554 - val_mse: 8.8554 - val_mae: 1.4321 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Score for fold 3: loss of 8.855391502380371\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 12.4797 - mse: 12.4797 - mae: 1.4514 - val_loss: 7.8861 - val_mse: 7.8861 - val_mae: 1.3582 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 12.4118 - mse: 12.4118 - mae: 1.4489 - val_loss: 7.9638 - val_mse: 7.9638 - val_mae: 1.3464 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 12.4126 - mse: 12.4126 - mae: 1.4474 - val_loss: 8.0569 - val_mse: 8.0569 - val_mae: 1.3786 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 12.3532 - mse: 12.3532 - mae: 1.4450 - val_loss: 7.9963 - val_mse: 7.9963 - val_mae: 1.3686 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 12.3558 - mse: 12.3558 - mae: 1.4445 - val_loss: 8.1374 - val_mse: 8.1374 - val_mae: 1.3952 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 12.3626 - mse: 12.3626 - mae: 1.4442 - val_loss: 8.0941 - val_mse: 8.0941 - val_mae: 1.3588 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Score for fold 4: loss of 8.094085693359375\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 11.6457 - mse: 11.6457 - mae: 1.4393 - val_loss: 10.7692 - val_mse: 10.7692 - val_mae: 1.3939 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 11.6134 - mse: 11.6134 - mae: 1.4380 - val_loss: 10.8509 - val_mse: 10.8509 - val_mae: 1.4040 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 11.5919 - mse: 11.5919 - mae: 1.4379 - val_loss: 10.9655 - val_mse: 10.9655 - val_mae: 1.4121 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 11.5759 - mse: 11.5759 - mae: 1.4376 - val_loss: 10.9367 - val_mse: 10.9367 - val_mae: 1.4114 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 11.5376 - mse: 11.5376 - mae: 1.4370 - val_loss: 11.0244 - val_mse: 11.0244 - val_mae: 1.4363 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 11.5138 - mse: 11.5138 - mae: 1.4359 - val_loss: 11.0888 - val_mse: 11.0888 - val_mae: 1.4114 - lr: 0.0010 - 2s/epoch - 2ms/step\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-10-11 22:37:40,932]\u001b[0m Finished trial#9 resulted in value: 12.046000000000001. Current best value is 10.724 with parameters: {'activation': 'tanh', 'num_hidden_layer': 3, 'i': 9, 'learning_rate': 0.0005761450508066512}.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Score for fold 5: loss of 11.088836669921875\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 7s - loss: 14.2997 - mse: 14.2997 - mae: 1.6571 - val_loss: 13.8052 - val_mse: 13.8052 - val_mae: 1.5271 - lr: 3.8320e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 6s - loss: 13.3440 - mse: 13.3440 - mae: 1.5747 - val_loss: 13.8754 - val_mse: 13.8754 - val_mae: 1.4620 - lr: 3.8320e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 6s - loss: 13.2811 - mse: 13.2811 - mae: 1.5593 - val_loss: 13.6751 - val_mse: 13.6751 - val_mae: 1.5268 - lr: 3.8320e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 6s - loss: 13.2233 - mse: 13.2233 - mae: 1.5593 - val_loss: 13.7602 - val_mse: 13.7602 - val_mae: 1.5991 - lr: 3.8320e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 6s - loss: 13.3566 - mse: 13.3566 - mae: 1.5610 - val_loss: 13.7640 - val_mse: 13.7640 - val_mae: 1.5524 - lr: 3.8320e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 6s - loss: 13.2422 - mse: 13.2422 - mae: 1.5518 - val_loss: 13.7835 - val_mse: 13.7835 - val_mae: 1.4846 - lr: 3.8320e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 6s - loss: 13.1041 - mse: 13.1041 - mae: 1.5560 - val_loss: 13.8230 - val_mse: 13.8230 - val_mae: 1.5046 - lr: 3.8320e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 6s - loss: 13.1853 - mse: 13.1853 - mae: 1.5544 - val_loss: 13.8792 - val_mse: 13.8792 - val_mae: 1.4783 - lr: 3.8320e-04 - 6s/epoch - 6ms/step\n",
            "Score for fold 1: loss of 13.879220008850098\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 6s - loss: 11.7910 - mse: 11.7910 - mae: 1.5529 - val_loss: 19.7831 - val_mse: 19.7831 - val_mae: 1.5048 - lr: 3.8320e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 6s - loss: 11.7384 - mse: 11.7384 - mae: 1.5606 - val_loss: 19.3174 - val_mse: 19.3174 - val_mae: 1.6420 - lr: 3.8320e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 6s - loss: 11.7676 - mse: 11.7676 - mae: 1.5541 - val_loss: 19.2939 - val_mse: 19.2939 - val_mae: 1.6175 - lr: 3.8320e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 6s - loss: 11.7589 - mse: 11.7589 - mae: 1.5625 - val_loss: 19.3343 - val_mse: 19.3343 - val_mae: 1.6212 - lr: 3.8320e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 6s - loss: 11.7792 - mse: 11.7792 - mae: 1.5626 - val_loss: 19.4917 - val_mse: 19.4917 - val_mae: 1.5408 - lr: 3.8320e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 6s - loss: 11.8789 - mse: 11.8789 - mae: 1.5547 - val_loss: 19.3976 - val_mse: 19.3976 - val_mae: 1.6075 - lr: 3.8320e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 6s - loss: 11.7963 - mse: 11.7963 - mae: 1.5570 - val_loss: 20.1092 - val_mse: 20.1092 - val_mae: 1.4853 - lr: 3.8320e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 6s - loss: 11.8020 - mse: 11.8020 - mae: 1.5613 - val_loss: 19.4974 - val_mse: 19.4974 - val_mae: 1.5456 - lr: 3.8320e-04 - 6s/epoch - 6ms/step\n",
            "Score for fold 2: loss of 19.497364044189453\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 6s - loss: 13.6980 - mse: 13.6980 - mae: 1.5476 - val_loss: 11.8355 - val_mse: 11.8355 - val_mae: 1.6631 - lr: 3.8320e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 6s - loss: 13.7547 - mse: 13.7547 - mae: 1.5471 - val_loss: 11.6832 - val_mse: 11.6832 - val_mae: 1.5849 - lr: 3.8320e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 3/100\n",
            "Epoch 4/100\n",
            "1000/1000 - 6s - loss: 13.6425 - mse: 13.6425 - mae: 1.5419 - val_loss: 11.9031 - val_mse: 11.9031 - val_mae: 1.4787 - lr: 3.8320e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 6s - loss: 13.7248 - mse: 13.7248 - mae: 1.5488 - val_loss: 11.8131 - val_mse: 11.8131 - val_mae: 1.6308 - lr: 3.8320e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 6s - loss: 13.7841 - mse: 13.7841 - mae: 1.5455 - val_loss: 11.7599 - val_mse: 11.7599 - val_mae: 1.6350 - lr: 3.8320e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 6s - loss: 13.7580 - mse: 13.7580 - mae: 1.5455 - val_loss: 12.4234 - val_mse: 12.4234 - val_mae: 1.8584 - lr: 3.8320e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 6s - loss: 13.7194 - mse: 13.7194 - mae: 1.5544 - val_loss: 11.6676 - val_mse: 11.6676 - val_mae: 1.5473 - lr: 3.8320e-04 - 6s/epoch - 6ms/step\n",
            "Score for fold 3: loss of 11.66761302947998\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 6s - loss: 14.0726 - mse: 14.0726 - mae: 1.5673 - val_loss: 10.2924 - val_mse: 10.2924 - val_mae: 1.6374 - lr: 3.8320e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 6s - loss: 14.1159 - mse: 14.1159 - mae: 1.5735 - val_loss: 10.3143 - val_mse: 10.3143 - val_mae: 1.4413 - lr: 3.8320e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 6s - loss: 14.1055 - mse: 14.1055 - mae: 1.5605 - val_loss: 10.4561 - val_mse: 10.4561 - val_mae: 1.4364 - lr: 3.8320e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 6s - loss: 14.1255 - mse: 14.1255 - mae: 1.5663 - val_loss: 10.2416 - val_mse: 10.2416 - val_mae: 1.5182 - lr: 3.8320e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 6s - loss: 14.1040 - mse: 14.1040 - mae: 1.5624 - val_loss: 10.1215 - val_mse: 10.1215 - val_mae: 1.5322 - lr: 3.8320e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 6s - loss: 14.0018 - mse: 14.0018 - mae: 1.5600 - val_loss: 10.1003 - val_mse: 10.1003 - val_mae: 1.4924 - lr: 3.8320e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 6s - loss: 14.0611 - mse: 14.0611 - mae: 1.5718 - val_loss: 10.2859 - val_mse: 10.2859 - val_mae: 1.4840 - lr: 3.8320e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 7s - loss: 13.9940 - mse: 13.9940 - mae: 1.5601 - val_loss: 10.2860 - val_mse: 10.2860 - val_mae: 1.4374 - lr: 3.8320e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 7s - loss: 14.1609 - mse: 14.1609 - mae: 1.5672 - val_loss: 10.3195 - val_mse: 10.3195 - val_mae: 1.4287 - lr: 3.8320e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 7s - loss: 14.2583 - mse: 14.2583 - mae: 1.5708 - val_loss: 10.3272 - val_mse: 10.3272 - val_mae: 1.4365 - lr: 3.8320e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 7s - loss: 14.2245 - mse: 14.2245 - mae: 1.5642 - val_loss: 10.0860 - val_mse: 10.0860 - val_mae: 1.5159 - lr: 3.8320e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 7s - loss: 14.1715 - mse: 14.1715 - mae: 1.5692 - val_loss: 10.5564 - val_mse: 10.5564 - val_mae: 1.5180 - lr: 3.8320e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 13/100\n",
            "1000/1000 - 7s - loss: 14.0891 - mse: 14.0891 - mae: 1.5640 - val_loss: 10.0802 - val_mse: 10.0802 - val_mae: 1.5632 - lr: 3.8320e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 14/100\n",
            "1000/1000 - 7s - loss: 14.2112 - mse: 14.2112 - mae: 1.5624 - val_loss: 10.4295 - val_mse: 10.4295 - val_mae: 1.6418 - lr: 3.8320e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 15/100\n",
            "1000/1000 - 7s - loss: 13.9723 - mse: 13.9723 - mae: 1.5618 - val_loss: 10.4766 - val_mse: 10.4766 - val_mae: 1.5562 - lr: 3.8320e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 16/100\n",
            "1000/1000 - 6s - loss: 14.1321 - mse: 14.1321 - mae: 1.5617 - val_loss: 10.2015 - val_mse: 10.2015 - val_mae: 1.5447 - lr: 3.8320e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 17/100\n",
            "1000/1000 - 6s - loss: 14.0476 - mse: 14.0476 - mae: 1.5680 - val_loss: 10.2645 - val_mse: 10.2645 - val_mae: 1.5425 - lr: 3.8320e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 18/100\n",
            "1000/1000 - 6s - loss: 14.2166 - mse: 14.2166 - mae: 1.5643 - val_loss: 10.1797 - val_mse: 10.1797 - val_mae: 1.4658 - lr: 3.8320e-04 - 6s/epoch - 6ms/step\n",
            "Score for fold 4: loss of 10.179678916931152\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 6s - loss: 13.7199 - mse: 13.7199 - mae: 1.5498 - val_loss: 11.2445 - val_mse: 11.2445 - val_mae: 1.6091 - lr: 3.8320e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 6s - loss: 13.8366 - mse: 13.8366 - mae: 1.5521 - val_loss: 11.1323 - val_mse: 11.1323 - val_mae: 1.5153 - lr: 3.8320e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 6s - loss: 13.8589 - mse: 13.8589 - mae: 1.5582 - val_loss: 11.4956 - val_mse: 11.4956 - val_mae: 1.5896 - lr: 3.8320e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 7s - loss: 13.8299 - mse: 13.8299 - mae: 1.5525 - val_loss: 11.1537 - val_mse: 11.1537 - val_mae: 1.5099 - lr: 3.8320e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 6s - loss: 13.8469 - mse: 13.8469 - mae: 1.5623 - val_loss: 11.3444 - val_mse: 11.3444 - val_mae: 1.5887 - lr: 3.8320e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 6s - loss: 13.8249 - mse: 13.8249 - mae: 1.5596 - val_loss: 11.2911 - val_mse: 11.2911 - val_mae: 1.5501 - lr: 3.8320e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 6s - loss: 13.7861 - mse: 13.7861 - mae: 1.5508 - val_loss: 11.1846 - val_mse: 11.1846 - val_mae: 1.5496 - lr: 3.8320e-04 - 6s/epoch - 6ms/step\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-10-11 22:42:59,675]\u001b[0m Finished trial#10 resulted in value: 13.282. Current best value is 10.724 with parameters: {'activation': 'tanh', 'num_hidden_layer': 3, 'i': 9, 'learning_rate': 0.0005761450508066512}.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Score for fold 5: loss of 11.184584617614746\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 7s - loss: 13.8483 - mse: 13.8483 - mae: 1.5426 - val_loss: 10.0836 - val_mse: 10.0836 - val_mae: 1.4272 - lr: 3.9719e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 7s - loss: 12.7967 - mse: 12.7967 - mae: 1.4836 - val_loss: 10.2644 - val_mse: 10.2644 - val_mae: 1.5058 - lr: 3.9719e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 7s - loss: 12.6408 - mse: 12.6408 - mae: 1.4698 - val_loss: 10.2081 - val_mse: 10.2081 - val_mae: 1.4357 - lr: 3.9719e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 7s - loss: 12.4594 - mse: 12.4594 - mae: 1.4602 - val_loss: 9.9736 - val_mse: 9.9736 - val_mae: 1.4318 - lr: 3.9719e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 7s - loss: 12.4192 - mse: 12.4192 - mae: 1.4483 - val_loss: 9.9160 - val_mse: 9.9160 - val_mae: 1.4459 - lr: 3.9719e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 7s - loss: 12.2877 - mse: 12.2877 - mae: 1.4457 - val_loss: 9.8469 - val_mse: 9.8469 - val_mae: 1.4553 - lr: 3.9719e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 7s - loss: 12.2787 - mse: 12.2787 - mae: 1.4374 - val_loss: 9.8011 - val_mse: 9.8011 - val_mae: 1.4440 - lr: 3.9719e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 7s - loss: 12.0698 - mse: 12.0698 - mae: 1.4300 - val_loss: 9.3932 - val_mse: 9.3932 - val_mae: 1.4448 - lr: 3.9719e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 7s - loss: 12.0037 - mse: 12.0037 - mae: 1.4272 - val_loss: 9.9162 - val_mse: 9.9162 - val_mae: 1.5494 - lr: 3.9719e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 7s - loss: 11.7574 - mse: 11.7574 - mae: 1.4179 - val_loss: 9.8966 - val_mse: 9.8966 - val_mae: 1.4862 - lr: 3.9719e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 7s - loss: 11.7956 - mse: 11.7956 - mae: 1.4180 - val_loss: 9.6003 - val_mse: 9.6003 - val_mae: 1.4315 - lr: 3.9719e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 7s - loss: 11.6979 - mse: 11.6979 - mae: 1.4039 - val_loss: 10.0832 - val_mse: 10.0832 - val_mae: 1.3913 - lr: 3.9719e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 13/100\n",
            "1000/1000 - 7s - loss: 11.5164 - mse: 11.5164 - mae: 1.3986 - val_loss: 9.7196 - val_mse: 9.7196 - val_mae: 1.4887 - lr: 3.9719e-04 - 7s/epoch - 7ms/step\n",
            "Score for fold 1: loss of 9.719639778137207\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 7s - loss: 10.7885 - mse: 10.7885 - mae: 1.4039 - val_loss: 12.6761 - val_mse: 12.6761 - val_mae: 1.3340 - lr: 3.9719e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 7s - loss: 10.3162 - mse: 10.3162 - mae: 1.3862 - val_loss: 12.7913 - val_mse: 12.7913 - val_mae: 1.3902 - lr: 3.9719e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 7s - loss: 10.1942 - mse: 10.1942 - mae: 1.3777 - val_loss: 13.8221 - val_mse: 13.8221 - val_mae: 1.5088 - lr: 3.9719e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 7s - loss: 10.2428 - mse: 10.2428 - mae: 1.3704 - val_loss: 13.7015 - val_mse: 13.7015 - val_mae: 1.4169 - lr: 3.9719e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 7s - loss: 10.0518 - mse: 10.0518 - mae: 1.3609 - val_loss: 12.6148 - val_mse: 12.6148 - val_mae: 1.4028 - lr: 3.9719e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 7s - loss: 9.6197 - mse: 9.6197 - mae: 1.3434 - val_loss: 13.1043 - val_mse: 13.1043 - val_mae: 1.3944 - lr: 3.9719e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 7s - loss: 9.5515 - mse: 9.5515 - mae: 1.3374 - val_loss: 12.9476 - val_mse: 12.9476 - val_mae: 1.3781 - lr: 3.9719e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 7s - loss: 9.4802 - mse: 9.4802 - mae: 1.3309 - val_loss: 13.1337 - val_mse: 13.1337 - val_mae: 1.4356 - lr: 3.9719e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 7s - loss: 9.2264 - mse: 9.2264 - mae: 1.3144 - val_loss: 13.1080 - val_mse: 13.1080 - val_mae: 1.4710 - lr: 3.9719e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 7s - loss: 9.0501 - mse: 9.0501 - mae: 1.2987 - val_loss: 12.9572 - val_mse: 12.9572 - val_mae: 1.4247 - lr: 3.9719e-04 - 7s/epoch - 7ms/step\n",
            "Score for fold 2: loss of 12.957232475280762\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 7s - loss: 10.5506 - mse: 10.5506 - mae: 1.3352 - val_loss: 7.6383 - val_mse: 7.6383 - val_mae: 1.3075 - lr: 3.9719e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 7s - loss: 10.2260 - mse: 10.2260 - mae: 1.3189 - val_loss: 7.5231 - val_mse: 7.5231 - val_mae: 1.2923 - lr: 3.9719e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 7s - loss: 10.1592 - mse: 10.1592 - mae: 1.3048 - val_loss: 7.9025 - val_mse: 7.9025 - val_mae: 1.2830 - lr: 3.9719e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 7s - loss: 9.7959 - mse: 9.7959 - mae: 1.2872 - val_loss: 7.5950 - val_mse: 7.5950 - val_mae: 1.3657 - lr: 3.9719e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 7s - loss: 9.8637 - mse: 9.8637 - mae: 1.2768 - val_loss: 7.5126 - val_mse: 7.5126 - val_mae: 1.3305 - lr: 3.9719e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 7s - loss: 9.4202 - mse: 9.4202 - mae: 1.2590 - val_loss: 8.0826 - val_mse: 8.0826 - val_mae: 1.3694 - lr: 3.9719e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 7s - loss: 9.1589 - mse: 9.1589 - mae: 1.2384 - val_loss: 8.1332 - val_mse: 8.1332 - val_mae: 1.3275 - lr: 3.9719e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 7s - loss: 9.0344 - mse: 9.0344 - mae: 1.2253 - val_loss: 7.7665 - val_mse: 7.7665 - val_mae: 1.3902 - lr: 3.9719e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 7s - loss: 8.9574 - mse: 8.9574 - mae: 1.2104 - val_loss: 8.2253 - val_mse: 8.2253 - val_mae: 1.3580 - lr: 3.9719e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 7s - loss: 8.6429 - mse: 8.6429 - mae: 1.1972 - val_loss: 8.2734 - val_mse: 8.2734 - val_mae: 1.3997 - lr: 3.9719e-04 - 7s/epoch - 7ms/step\n",
            "Score for fold 3: loss of 8.273444175720215\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 7s - loss: 8.9369 - mse: 8.9369 - mae: 1.2371 - val_loss: 5.8135 - val_mse: 5.8135 - val_mae: 1.1931 - lr: 3.9719e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 7s - loss: 8.7749 - mse: 8.7749 - mae: 1.2122 - val_loss: 6.0225 - val_mse: 6.0225 - val_mae: 1.2080 - lr: 3.9719e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 7s - loss: 8.6833 - mse: 8.6833 - mae: 1.1945 - val_loss: 5.6932 - val_mse: 5.6932 - val_mae: 1.1974 - lr: 3.9719e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 7s - loss: 8.3583 - mse: 8.3583 - mae: 1.1724 - val_loss: 5.8172 - val_mse: 5.8172 - val_mae: 1.2431 - lr: 3.9719e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 7s - loss: 8.1434 - mse: 8.1434 - mae: 1.1539 - val_loss: 5.8857 - val_mse: 5.8857 - val_mae: 1.2731 - lr: 3.9719e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 7s - loss: 7.8475 - mse: 7.8475 - mae: 1.1343 - val_loss: 6.0804 - val_mse: 6.0804 - val_mae: 1.2311 - lr: 3.9719e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 7s - loss: 7.6815 - mse: 7.6815 - mae: 1.1160 - val_loss: 6.4552 - val_mse: 6.4552 - val_mae: 1.2208 - lr: 3.9719e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 7s - loss: 7.4727 - mse: 7.4727 - mae: 1.0952 - val_loss: 6.2044 - val_mse: 6.2044 - val_mae: 1.2792 - lr: 3.9719e-04 - 7s/epoch - 7ms/step\n",
            "Score for fold 4: loss of 6.204372406005859\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 7s - loss: 6.5990 - mse: 6.5990 - mae: 1.1462 - val_loss: 10.8460 - val_mse: 10.8460 - val_mae: 1.0585 - lr: 3.9719e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 7s - loss: 5.9622 - mse: 5.9622 - mae: 1.1183 - val_loss: 11.2794 - val_mse: 11.2794 - val_mae: 1.1192 - lr: 3.9719e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 7s - loss: 5.6384 - mse: 5.6384 - mae: 1.0899 - val_loss: 11.5080 - val_mse: 11.5080 - val_mae: 1.1052 - lr: 3.9719e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 7s - loss: 6.0050 - mse: 6.0050 - mae: 1.0742 - val_loss: 11.4104 - val_mse: 11.4104 - val_mae: 1.1351 - lr: 3.9719e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 7s - loss: 5.2713 - mse: 5.2713 - mae: 1.0527 - val_loss: 11.6265 - val_mse: 11.6265 - val_mae: 1.1445 - lr: 3.9719e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 7s - loss: 5.1689 - mse: 5.1689 - mae: 1.0336 - val_loss: 11.4667 - val_mse: 11.4667 - val_mae: 1.1478 - lr: 3.9719e-04 - 7s/epoch - 7ms/step\n",
            "Score for fold 5: loss of 11.466736793518066\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-10-11 22:48:34,476]\u001b[0m Finished trial#11 resulted in value: 9.724. Current best value is 9.724 with parameters: {'activation': 'relu', 'num_hidden_layer': 3, 'i': 10, 'learning_rate': 0.00039718555663465806}.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 7s - loss: 12.0913 - mse: 12.0913 - mae: 1.5370 - val_loss: 17.3414 - val_mse: 17.3414 - val_mae: 1.5099 - lr: 3.0070e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 7s - loss: 11.0817 - mse: 11.0817 - mae: 1.4780 - val_loss: 17.0543 - val_mse: 17.0543 - val_mae: 1.6238 - lr: 3.0070e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 7s - loss: 10.9397 - mse: 10.9397 - mae: 1.4701 - val_loss: 17.0770 - val_mse: 17.0770 - val_mae: 1.4666 - lr: 3.0070e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 7s - loss: 10.8067 - mse: 10.8067 - mae: 1.4563 - val_loss: 17.3962 - val_mse: 17.3962 - val_mae: 1.4277 - lr: 3.0070e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 7s - loss: 10.7517 - mse: 10.7517 - mae: 1.4484 - val_loss: 16.8718 - val_mse: 16.8718 - val_mae: 1.4632 - lr: 3.0070e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 7s - loss: 10.7184 - mse: 10.7184 - mae: 1.4438 - val_loss: 17.0400 - val_mse: 17.0400 - val_mae: 1.4860 - lr: 3.0070e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 7s - loss: 10.4800 - mse: 10.4800 - mae: 1.4302 - val_loss: 16.8948 - val_mse: 16.8948 - val_mae: 1.4275 - lr: 3.0070e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 7s - loss: 10.3289 - mse: 10.3289 - mae: 1.4252 - val_loss: 16.7541 - val_mse: 16.7541 - val_mae: 1.5180 - lr: 3.0070e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 7s - loss: 10.2918 - mse: 10.2918 - mae: 1.4225 - val_loss: 16.8539 - val_mse: 16.8539 - val_mae: 1.5132 - lr: 3.0070e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 7s - loss: 10.1990 - mse: 10.1990 - mae: 1.4117 - val_loss: 16.8366 - val_mse: 16.8366 - val_mae: 1.4419 - lr: 3.0070e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 7s - loss: 10.0086 - mse: 10.0086 - mae: 1.4030 - val_loss: 16.7207 - val_mse: 16.7207 - val_mae: 1.4915 - lr: 3.0070e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 7s - loss: 9.8824 - mse: 9.8824 - mae: 1.3990 - val_loss: 16.6368 - val_mse: 16.6368 - val_mae: 1.4659 - lr: 3.0070e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 13/100\n",
            "1000/1000 - 7s - loss: 9.6278 - mse: 9.6278 - mae: 1.3909 - val_loss: 16.7054 - val_mse: 16.7054 - val_mae: 1.4702 - lr: 3.0070e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 14/100\n",
            "1000/1000 - 7s - loss: 9.6131 - mse: 9.6131 - mae: 1.3801 - val_loss: 16.7675 - val_mse: 16.7675 - val_mae: 1.4654 - lr: 3.0070e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 15/100\n",
            "1000/1000 - 7s - loss: 9.5156 - mse: 9.5156 - mae: 1.3691 - val_loss: 16.9664 - val_mse: 16.9664 - val_mae: 1.4309 - lr: 3.0070e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 16/100\n",
            "1000/1000 - 7s - loss: 9.2588 - mse: 9.2588 - mae: 1.3595 - val_loss: 16.7689 - val_mse: 16.7689 - val_mae: 1.4671 - lr: 3.0070e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 17/100\n",
            "1000/1000 - 7s - loss: 9.1348 - mse: 9.1348 - mae: 1.3539 - val_loss: 16.8892 - val_mse: 16.8892 - val_mae: 1.5313 - lr: 3.0070e-04 - 7s/epoch - 7ms/step\n",
            "Score for fold 1: loss of 16.889211654663086\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 7s - loss: 11.3707 - mse: 11.3707 - mae: 1.3813 - val_loss: 7.5479 - val_mse: 7.5479 - val_mae: 1.4023 - lr: 3.0070e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 7s - loss: 11.2874 - mse: 11.2874 - mae: 1.3699 - val_loss: 7.7046 - val_mse: 7.7046 - val_mae: 1.3438 - lr: 3.0070e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 7s - loss: 10.9068 - mse: 10.9068 - mae: 1.3577 - val_loss: 7.9639 - val_mse: 7.9639 - val_mae: 1.3694 - lr: 3.0070e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 7s - loss: 10.8738 - mse: 10.8738 - mae: 1.3439 - val_loss: 7.9310 - val_mse: 7.9310 - val_mae: 1.3986 - lr: 3.0070e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 7s - loss: 10.9353 - mse: 10.9353 - mae: 1.3369 - val_loss: 7.8945 - val_mse: 7.8945 - val_mae: 1.3521 - lr: 3.0070e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 7s - loss: 10.6550 - mse: 10.6550 - mae: 1.3209 - val_loss: 7.8355 - val_mse: 7.8355 - val_mae: 1.3868 - lr: 3.0070e-04 - 7s/epoch - 7ms/step\n",
            "Score for fold 2: loss of 7.835513591766357\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 7s - loss: 10.3022 - mse: 10.3022 - mae: 1.3418 - val_loss: 9.2893 - val_mse: 9.2893 - val_mae: 1.2517 - lr: 3.0070e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 7s - loss: 10.1454 - mse: 10.1454 - mae: 1.3231 - val_loss: 9.0220 - val_mse: 9.0220 - val_mae: 1.2666 - lr: 3.0070e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 7s - loss: 10.1124 - mse: 10.1124 - mae: 1.3117 - val_loss: 9.3062 - val_mse: 9.3062 - val_mae: 1.2841 - lr: 3.0070e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 7s - loss: 9.6595 - mse: 9.6595 - mae: 1.2949 - val_loss: 9.6466 - val_mse: 9.6466 - val_mae: 1.3162 - lr: 3.0070e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 7s - loss: 9.3840 - mse: 9.3840 - mae: 1.2839 - val_loss: 10.0711 - val_mse: 10.0711 - val_mae: 1.2939 - lr: 3.0070e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 7s - loss: 9.3340 - mse: 9.3340 - mae: 1.2681 - val_loss: 9.3872 - val_mse: 9.3872 - val_mae: 1.3005 - lr: 3.0070e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 7s - loss: 9.3603 - mse: 9.3603 - mae: 1.2535 - val_loss: 10.0258 - val_mse: 10.0258 - val_mae: 1.3383 - lr: 3.0070e-04 - 7s/epoch - 7ms/step\n",
            "Score for fold 3: loss of 10.02584457397461\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 7s - loss: 9.9245 - mse: 9.9245 - mae: 1.2752 - val_loss: 5.5415 - val_mse: 5.5415 - val_mae: 1.1736 - lr: 3.0070e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 7s - loss: 9.5138 - mse: 9.5138 - mae: 1.2552 - val_loss: 5.5470 - val_mse: 5.5470 - val_mae: 1.2045 - lr: 3.0070e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 7s - loss: 9.2066 - mse: 9.2066 - mae: 1.2330 - val_loss: 5.9372 - val_mse: 5.9372 - val_mae: 1.2650 - lr: 3.0070e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 7s - loss: 9.3222 - mse: 9.3222 - mae: 1.2226 - val_loss: 5.5216 - val_mse: 5.5216 - val_mae: 1.2393 - lr: 3.0070e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 7s - loss: 8.9025 - mse: 8.9025 - mae: 1.2004 - val_loss: 6.0709 - val_mse: 6.0709 - val_mae: 1.2933 - lr: 3.0070e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 7s - loss: 8.6641 - mse: 8.6641 - mae: 1.1825 - val_loss: 6.0319 - val_mse: 6.0319 - val_mae: 1.2808 - lr: 3.0070e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 7s - loss: 8.5482 - mse: 8.5482 - mae: 1.1703 - val_loss: 6.0224 - val_mse: 6.0224 - val_mae: 1.2790 - lr: 3.0070e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 7s - loss: 8.2829 - mse: 8.2829 - mae: 1.1537 - val_loss: 5.9555 - val_mse: 5.9555 - val_mae: 1.2728 - lr: 3.0070e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 7s - loss: 8.4294 - mse: 8.4294 - mae: 1.1383 - val_loss: 6.4132 - val_mse: 6.4132 - val_mae: 1.3104 - lr: 3.0070e-04 - 7s/epoch - 7ms/step\n",
            "Score for fold 4: loss of 6.41318941116333\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 7s - loss: 7.8152 - mse: 7.8152 - mae: 1.1797 - val_loss: 8.1296 - val_mse: 8.1296 - val_mae: 1.1278 - lr: 3.0070e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 7s - loss: 7.2909 - mse: 7.2909 - mae: 1.1490 - val_loss: 8.3845 - val_mse: 8.3845 - val_mae: 1.1820 - lr: 3.0070e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 7s - loss: 7.1799 - mse: 7.1799 - mae: 1.1344 - val_loss: 8.5207 - val_mse: 8.5207 - val_mae: 1.1751 - lr: 3.0070e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 7s - loss: 7.1840 - mse: 7.1840 - mae: 1.1112 - val_loss: 8.3488 - val_mse: 8.3488 - val_mae: 1.1597 - lr: 3.0070e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 7s - loss: 6.8468 - mse: 6.8468 - mae: 1.0922 - val_loss: 8.8147 - val_mse: 8.8147 - val_mae: 1.1987 - lr: 3.0070e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 7s - loss: 6.6786 - mse: 6.6786 - mae: 1.0759 - val_loss: 9.3222 - val_mse: 9.3222 - val_mae: 1.2182 - lr: 3.0070e-04 - 7s/epoch - 7ms/step\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-10-11 22:53:56,388]\u001b[0m Finished trial#12 resulted in value: 10.098. Current best value is 9.724 with parameters: {'activation': 'relu', 'num_hidden_layer': 3, 'i': 10, 'learning_rate': 0.00039718555663465806}.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Score for fold 5: loss of 9.322178840637207\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 7s - loss: 13.4928 - mse: 13.4928 - mae: 1.5441 - val_loss: 11.9836 - val_mse: 11.9836 - val_mae: 1.4120 - lr: 2.6062e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 7s - loss: 12.5029 - mse: 12.5029 - mae: 1.4975 - val_loss: 11.6954 - val_mse: 11.6954 - val_mae: 1.3857 - lr: 2.6062e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 7s - loss: 12.1599 - mse: 12.1599 - mae: 1.4733 - val_loss: 11.3686 - val_mse: 11.3686 - val_mae: 1.4429 - lr: 2.6062e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 7s - loss: 12.1133 - mse: 12.1133 - mae: 1.4618 - val_loss: 11.4251 - val_mse: 11.4251 - val_mae: 1.4705 - lr: 2.6062e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 7s - loss: 12.1086 - mse: 12.1086 - mae: 1.4557 - val_loss: 11.5376 - val_mse: 11.5376 - val_mae: 1.4750 - lr: 2.6062e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 7s - loss: 11.9602 - mse: 11.9602 - mae: 1.4476 - val_loss: 11.4504 - val_mse: 11.4504 - val_mae: 1.4964 - lr: 2.6062e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 7s - loss: 11.8085 - mse: 11.8085 - mae: 1.4431 - val_loss: 11.4133 - val_mse: 11.4133 - val_mae: 1.4607 - lr: 2.6062e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 7s - loss: 11.5883 - mse: 11.5883 - mae: 1.4304 - val_loss: 11.4944 - val_mse: 11.4944 - val_mae: 1.4202 - lr: 2.6062e-04 - 7s/epoch - 7ms/step\n",
            "Score for fold 1: loss of 11.494417190551758\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 7s - loss: 12.3895 - mse: 12.3895 - mae: 1.4342 - val_loss: 9.3296 - val_mse: 9.3296 - val_mae: 1.4192 - lr: 2.6062e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 7s - loss: 12.1631 - mse: 12.1631 - mae: 1.4265 - val_loss: 9.1122 - val_mse: 9.1122 - val_mae: 1.3876 - lr: 2.6062e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 8s - loss: 11.9998 - mse: 11.9998 - mae: 1.4193 - val_loss: 9.2034 - val_mse: 9.2034 - val_mae: 1.4551 - lr: 2.6062e-04 - 8s/epoch - 8ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 8s - loss: 11.8727 - mse: 11.8727 - mae: 1.4136 - val_loss: 9.0996 - val_mse: 9.0996 - val_mae: 1.4846 - lr: 2.6062e-04 - 8s/epoch - 8ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 7s - loss: 11.8889 - mse: 11.8889 - mae: 1.4044 - val_loss: 9.1525 - val_mse: 9.1525 - val_mae: 1.4443 - lr: 2.6062e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 7s - loss: 11.4986 - mse: 11.4986 - mae: 1.3974 - val_loss: 9.1983 - val_mse: 9.1983 - val_mae: 1.4510 - lr: 2.6062e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 8s - loss: 11.4644 - mse: 11.4644 - mae: 1.3918 - val_loss: 9.3044 - val_mse: 9.3044 - val_mae: 1.4272 - lr: 2.6062e-04 - 8s/epoch - 8ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 7s - loss: 11.2181 - mse: 11.2181 - mae: 1.3790 - val_loss: 9.2773 - val_mse: 9.2773 - val_mae: 1.3563 - lr: 2.6062e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 7s - loss: 11.1840 - mse: 11.1840 - mae: 1.3717 - val_loss: 9.2803 - val_mse: 9.2803 - val_mae: 1.4596 - lr: 2.6062e-04 - 7s/epoch - 7ms/step\n",
            "Score for fold 2: loss of 9.280348777770996\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 7s - loss: 9.0475 - mse: 9.0475 - mae: 1.3671 - val_loss: 16.7784 - val_mse: 16.7784 - val_mae: 1.3513 - lr: 2.6062e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 7s - loss: 8.8291 - mse: 8.8291 - mae: 1.3558 - val_loss: 16.9236 - val_mse: 16.9236 - val_mae: 1.4667 - lr: 2.6062e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 7s - loss: 8.7728 - mse: 8.7728 - mae: 1.3443 - val_loss: 16.7597 - val_mse: 16.7597 - val_mae: 1.4401 - lr: 2.6062e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 7s - loss: 8.4941 - mse: 8.4941 - mae: 1.3342 - val_loss: 16.9672 - val_mse: 16.9672 - val_mae: 1.3921 - lr: 2.6062e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 8s - loss: 8.4335 - mse: 8.4335 - mae: 1.3250 - val_loss: 17.0313 - val_mse: 17.0313 - val_mae: 1.4187 - lr: 2.6062e-04 - 8s/epoch - 8ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 7s - loss: 8.2220 - mse: 8.2220 - mae: 1.3096 - val_loss: 17.0521 - val_mse: 17.0521 - val_mae: 1.4185 - lr: 2.6062e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 8s - loss: 8.1869 - mse: 8.1869 - mae: 1.3027 - val_loss: 17.4104 - val_mse: 17.4104 - val_mae: 1.4585 - lr: 2.6062e-04 - 8s/epoch - 8ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 8s - loss: 7.9422 - mse: 7.9422 - mae: 1.2868 - val_loss: 17.1030 - val_mse: 17.1030 - val_mae: 1.4318 - lr: 2.6062e-04 - 8s/epoch - 8ms/step\n",
            "Score for fold 3: loss of 17.10304832458496\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 8s - loss: 10.4672 - mse: 10.4672 - mae: 1.3289 - val_loss: 7.1653 - val_mse: 7.1653 - val_mae: 1.2785 - lr: 2.6062e-04 - 8s/epoch - 8ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 7s - loss: 10.2655 - mse: 10.2655 - mae: 1.3079 - val_loss: 7.9460 - val_mse: 7.9460 - val_mae: 1.2590 - lr: 2.6062e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 7s - loss: 9.9626 - mse: 9.9626 - mae: 1.2975 - val_loss: 7.1977 - val_mse: 7.1977 - val_mae: 1.2807 - lr: 2.6062e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 7s - loss: 9.8514 - mse: 9.8514 - mae: 1.2817 - val_loss: 7.9077 - val_mse: 7.9077 - val_mae: 1.2792 - lr: 2.6062e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 8s - loss: 9.5682 - mse: 9.5682 - mae: 1.2642 - val_loss: 7.6788 - val_mse: 7.6788 - val_mae: 1.3125 - lr: 2.6062e-04 - 8s/epoch - 8ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 8s - loss: 9.3490 - mse: 9.3490 - mae: 1.2486 - val_loss: 7.8380 - val_mse: 7.8380 - val_mae: 1.3635 - lr: 2.6062e-04 - 8s/epoch - 8ms/step\n",
            "Score for fold 4: loss of 7.837974548339844\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 7s - loss: 9.5002 - mse: 9.5002 - mae: 1.2719 - val_loss: 7.2824 - val_mse: 7.2824 - val_mae: 1.2237 - lr: 2.6062e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 7s - loss: 9.0517 - mse: 9.0517 - mae: 1.2525 - val_loss: 7.2763 - val_mse: 7.2763 - val_mae: 1.2080 - lr: 2.6062e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 7s - loss: 8.9045 - mse: 8.9045 - mae: 1.2318 - val_loss: 7.8147 - val_mse: 7.8147 - val_mae: 1.2275 - lr: 2.6062e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 7s - loss: 8.8416 - mse: 8.8416 - mae: 1.2164 - val_loss: 8.0648 - val_mse: 8.0648 - val_mae: 1.2186 - lr: 2.6062e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 7s - loss: 8.5378 - mse: 8.5378 - mae: 1.2006 - val_loss: 7.9577 - val_mse: 7.9577 - val_mae: 1.2521 - lr: 2.6062e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 7s - loss: 8.5891 - mse: 8.5891 - mae: 1.1901 - val_loss: 8.1445 - val_mse: 8.1445 - val_mae: 1.2362 - lr: 2.6062e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 7s - loss: 8.2376 - mse: 8.2376 - mae: 1.1728 - val_loss: 8.0427 - val_mse: 8.0427 - val_mae: 1.2965 - lr: 2.6062e-04 - 7s/epoch - 7ms/step\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-10-11 22:58:42,854]\u001b[0m Finished trial#13 resulted in value: 10.750000000000002. Current best value is 9.724 with parameters: {'activation': 'relu', 'num_hidden_layer': 3, 'i': 10, 'learning_rate': 0.00039718555663465806}.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Score for fold 5: loss of 8.042750358581543\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 8s - loss: 11.5989 - mse: 11.5989 - mae: 1.5505 - val_loss: 19.7539 - val_mse: 19.7539 - val_mae: 1.4702 - lr: 2.0189e-04 - 8s/epoch - 8ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 7s - loss: 10.6436 - mse: 10.6436 - mae: 1.4914 - val_loss: 19.0696 - val_mse: 19.0696 - val_mae: 1.5153 - lr: 2.0189e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 7s - loss: 10.3819 - mse: 10.3819 - mae: 1.4735 - val_loss: 19.7410 - val_mse: 19.7410 - val_mae: 1.6089 - lr: 2.0189e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 7s - loss: 10.1387 - mse: 10.1387 - mae: 1.4546 - val_loss: 20.1698 - val_mse: 20.1698 - val_mae: 1.4895 - lr: 2.0189e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 7s - loss: 10.1243 - mse: 10.1243 - mae: 1.4485 - val_loss: 20.0382 - val_mse: 20.0382 - val_mae: 1.5828 - lr: 2.0189e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 7s - loss: 9.9469 - mse: 9.9469 - mae: 1.4409 - val_loss: 19.7602 - val_mse: 19.7602 - val_mae: 1.4579 - lr: 2.0189e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 7s - loss: 9.8738 - mse: 9.8738 - mae: 1.4355 - val_loss: 20.1262 - val_mse: 20.1262 - val_mae: 1.4815 - lr: 2.0189e-04 - 7s/epoch - 7ms/step\n",
            "Score for fold 1: loss of 20.126258850097656\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 7s - loss: 12.1444 - mse: 12.1444 - mae: 1.4289 - val_loss: 10.0711 - val_mse: 10.0711 - val_mae: 1.4437 - lr: 2.0189e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 7s - loss: 12.0056 - mse: 12.0056 - mae: 1.4180 - val_loss: 10.1929 - val_mse: 10.1929 - val_mae: 1.5118 - lr: 2.0189e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 8s - loss: 11.8908 - mse: 11.8908 - mae: 1.4143 - val_loss: 10.5194 - val_mse: 10.5194 - val_mae: 1.4321 - lr: 2.0189e-04 - 8s/epoch - 8ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 8s - loss: 11.7940 - mse: 11.7940 - mae: 1.4061 - val_loss: 10.1006 - val_mse: 10.1006 - val_mae: 1.4351 - lr: 2.0189e-04 - 8s/epoch - 8ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 8s - loss: 11.5135 - mse: 11.5135 - mae: 1.3984 - val_loss: 10.3266 - val_mse: 10.3266 - val_mae: 1.4994 - lr: 2.0189e-04 - 8s/epoch - 8ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 8s - loss: 11.4961 - mse: 11.4961 - mae: 1.3982 - val_loss: 10.1995 - val_mse: 10.1995 - val_mae: 1.4979 - lr: 2.0189e-04 - 8s/epoch - 8ms/step\n",
            "Score for fold 2: loss of 10.199470520019531\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 8s - loss: 12.1161 - mse: 12.1161 - mae: 1.4096 - val_loss: 8.0310 - val_mse: 8.0310 - val_mae: 1.3869 - lr: 2.0189e-04 - 8s/epoch - 8ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 7s - loss: 12.1012 - mse: 12.1012 - mae: 1.4001 - val_loss: 7.7758 - val_mse: 7.7758 - val_mae: 1.3970 - lr: 2.0189e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 8s - loss: 11.8259 - mse: 11.8259 - mae: 1.3937 - val_loss: 7.3958 - val_mse: 7.3958 - val_mae: 1.4692 - lr: 2.0189e-04 - 8s/epoch - 8ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 8s - loss: 11.6569 - mse: 11.6569 - mae: 1.3865 - val_loss: 7.3953 - val_mse: 7.3953 - val_mae: 1.3919 - lr: 2.0189e-04 - 8s/epoch - 8ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 8s - loss: 11.4476 - mse: 11.4476 - mae: 1.3756 - val_loss: 7.6251 - val_mse: 7.6251 - val_mae: 1.3807 - lr: 2.0189e-04 - 8s/epoch - 8ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 8s - loss: 11.4309 - mse: 11.4309 - mae: 1.3667 - val_loss: 7.6700 - val_mse: 7.6700 - val_mae: 1.3720 - lr: 2.0189e-04 - 8s/epoch - 8ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 8s - loss: 11.1809 - mse: 11.1809 - mae: 1.3584 - val_loss: 7.5444 - val_mse: 7.5444 - val_mae: 1.4081 - lr: 2.0189e-04 - 8s/epoch - 8ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 7s - loss: 11.0447 - mse: 11.0447 - mae: 1.3509 - val_loss: 7.7652 - val_mse: 7.7652 - val_mae: 1.4131 - lr: 2.0189e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 7s - loss: 10.9572 - mse: 10.9572 - mae: 1.3403 - val_loss: 7.7131 - val_mse: 7.7131 - val_mae: 1.4217 - lr: 2.0189e-04 - 7s/epoch - 7ms/step\n",
            "Score for fold 3: loss of 7.713090896606445\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 7s - loss: 10.5989 - mse: 10.5989 - mae: 1.3608 - val_loss: 8.9324 - val_mse: 8.9324 - val_mae: 1.3284 - lr: 2.0189e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 7s - loss: 10.4586 - mse: 10.4586 - mae: 1.3454 - val_loss: 9.2128 - val_mse: 9.2128 - val_mae: 1.3960 - lr: 2.0189e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 7s - loss: 10.3103 - mse: 10.3103 - mae: 1.3343 - val_loss: 9.0109 - val_mse: 9.0109 - val_mae: 1.3276 - lr: 2.0189e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 7s - loss: 10.1085 - mse: 10.1085 - mae: 1.3232 - val_loss: 8.5912 - val_mse: 8.5912 - val_mae: 1.3940 - lr: 2.0189e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 7s - loss: 9.9515 - mse: 9.9515 - mae: 1.3105 - val_loss: 8.9766 - val_mse: 8.9766 - val_mae: 1.3141 - lr: 2.0189e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 7s - loss: 9.8393 - mse: 9.8393 - mae: 1.2994 - val_loss: 8.9087 - val_mse: 8.9087 - val_mae: 1.3406 - lr: 2.0189e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 7s - loss: 9.6062 - mse: 9.6062 - mae: 1.2891 - val_loss: 10.0009 - val_mse: 10.0009 - val_mae: 1.3642 - lr: 2.0189e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 7s - loss: 9.5999 - mse: 9.5999 - mae: 1.2783 - val_loss: 9.7203 - val_mse: 9.7203 - val_mae: 1.4260 - lr: 2.0189e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 7s - loss: 9.4209 - mse: 9.4209 - mae: 1.2686 - val_loss: 9.3417 - val_mse: 9.3417 - val_mae: 1.4155 - lr: 2.0189e-04 - 7s/epoch - 7ms/step\n",
            "Score for fold 4: loss of 9.341711044311523\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 7s - loss: 9.5955 - mse: 9.5955 - mae: 1.2998 - val_loss: 8.3918 - val_mse: 8.3918 - val_mae: 1.2227 - lr: 2.0189e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 7s - loss: 9.4872 - mse: 9.4872 - mae: 1.2838 - val_loss: 8.4740 - val_mse: 8.4740 - val_mae: 1.2365 - lr: 2.0189e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 7s - loss: 9.1944 - mse: 9.1944 - mae: 1.2700 - val_loss: 9.0143 - val_mse: 9.0143 - val_mae: 1.2475 - lr: 2.0189e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 7s - loss: 8.8517 - mse: 8.8517 - mae: 1.2550 - val_loss: 8.4921 - val_mse: 8.4921 - val_mae: 1.2451 - lr: 2.0189e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 7s - loss: 8.8568 - mse: 8.8568 - mae: 1.2418 - val_loss: 8.7991 - val_mse: 8.7991 - val_mae: 1.2957 - lr: 2.0189e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 7s - loss: 8.8270 - mse: 8.8270 - mae: 1.2344 - val_loss: 8.8146 - val_mse: 8.8146 - val_mae: 1.2743 - lr: 2.0189e-04 - 7s/epoch - 7ms/step\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-10-11 23:03:21,495]\u001b[0m Finished trial#14 resulted in value: 11.238. Current best value is 9.724 with parameters: {'activation': 'relu', 'num_hidden_layer': 3, 'i': 10, 'learning_rate': 0.00039718555663465806}.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Score for fold 5: loss of 8.814615249633789\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 8s - loss: 12.3069 - mse: 12.3069 - mae: 1.5454 - val_loss: 16.0906 - val_mse: 16.0906 - val_mae: 1.5319 - lr: 0.0011 - 8s/epoch - 8ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 7s - loss: 11.7615 - mse: 11.7615 - mae: 1.5042 - val_loss: 15.8760 - val_mse: 15.8760 - val_mae: 1.4565 - lr: 0.0011 - 7s/epoch - 7ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 7s - loss: 11.6042 - mse: 11.6042 - mae: 1.4817 - val_loss: 15.8607 - val_mse: 15.8607 - val_mae: 1.4839 - lr: 0.0011 - 7s/epoch - 7ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 7s - loss: 11.2543 - mse: 11.2543 - mae: 1.4731 - val_loss: 15.8211 - val_mse: 15.8211 - val_mae: 1.4774 - lr: 0.0011 - 7s/epoch - 7ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 7s - loss: 11.0621 - mse: 11.0621 - mae: 1.4585 - val_loss: 15.6817 - val_mse: 15.6817 - val_mae: 1.4825 - lr: 0.0011 - 7s/epoch - 7ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 7s - loss: 10.9959 - mse: 10.9959 - mae: 1.4562 - val_loss: 15.5139 - val_mse: 15.5139 - val_mae: 1.4679 - lr: 0.0011 - 7s/epoch - 7ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 7s - loss: 11.0221 - mse: 11.0221 - mae: 1.4503 - val_loss: 16.1323 - val_mse: 16.1323 - val_mae: 1.4294 - lr: 0.0011 - 7s/epoch - 7ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 7s - loss: 10.7944 - mse: 10.7944 - mae: 1.4390 - val_loss: 15.9049 - val_mse: 15.9049 - val_mae: 1.5179 - lr: 0.0011 - 7s/epoch - 7ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 7s - loss: 10.7651 - mse: 10.7651 - mae: 1.4346 - val_loss: 15.7615 - val_mse: 15.7615 - val_mae: 1.4413 - lr: 0.0011 - 7s/epoch - 7ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 7s - loss: 10.5653 - mse: 10.5653 - mae: 1.4308 - val_loss: 16.8116 - val_mse: 16.8116 - val_mae: 1.4938 - lr: 0.0011 - 7s/epoch - 7ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 7s - loss: 10.4878 - mse: 10.4878 - mae: 1.4326 - val_loss: 15.7943 - val_mse: 15.7943 - val_mae: 1.4629 - lr: 0.0011 - 7s/epoch - 7ms/step\n",
            "Score for fold 1: loss of 15.79428482055664\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 7s - loss: 11.8559 - mse: 11.8559 - mae: 1.4395 - val_loss: 10.2490 - val_mse: 10.2490 - val_mae: 1.4483 - lr: 0.0010 - 7s/epoch - 7ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 7s - loss: 11.5409 - mse: 11.5409 - mae: 1.4285 - val_loss: 10.4616 - val_mse: 10.4616 - val_mae: 1.4318 - lr: 0.0010 - 7s/epoch - 7ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 7s - loss: 11.6201 - mse: 11.6201 - mae: 1.4252 - val_loss: 10.7243 - val_mse: 10.7243 - val_mae: 1.4137 - lr: 0.0010 - 7s/epoch - 7ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 7s - loss: 11.2648 - mse: 11.2648 - mae: 1.4164 - val_loss: 10.5166 - val_mse: 10.5166 - val_mae: 1.4340 - lr: 0.0010 - 7s/epoch - 7ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 7s - loss: 11.2494 - mse: 11.2494 - mae: 1.4085 - val_loss: 11.5620 - val_mse: 11.5620 - val_mae: 1.4323 - lr: 0.0010 - 7s/epoch - 7ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 7s - loss: 11.2315 - mse: 11.2315 - mae: 1.4034 - val_loss: 10.2691 - val_mse: 10.2691 - val_mae: 1.4677 - lr: 0.0010 - 7s/epoch - 7ms/step\n",
            "Score for fold 2: loss of 10.26909351348877\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 7s - loss: 11.5065 - mse: 11.5065 - mae: 1.4065 - val_loss: 8.9779 - val_mse: 8.9779 - val_mae: 1.4301 - lr: 0.0010 - 7s/epoch - 7ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 7s - loss: 11.2941 - mse: 11.2941 - mae: 1.3921 - val_loss: 8.8808 - val_mse: 8.8808 - val_mae: 1.3778 - lr: 0.0010 - 7s/epoch - 7ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 7s - loss: 11.1354 - mse: 11.1354 - mae: 1.3827 - val_loss: 8.8322 - val_mse: 8.8322 - val_mae: 1.4145 - lr: 0.0010 - 7s/epoch - 7ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 7s - loss: 10.9266 - mse: 10.9266 - mae: 1.3726 - val_loss: 9.1772 - val_mse: 9.1772 - val_mae: 1.3898 - lr: 0.0010 - 7s/epoch - 7ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 7s - loss: 10.9705 - mse: 10.9705 - mae: 1.3636 - val_loss: 9.2057 - val_mse: 9.2057 - val_mae: 1.4026 - lr: 0.0010 - 7s/epoch - 7ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 7s - loss: 10.6472 - mse: 10.6472 - mae: 1.3527 - val_loss: 9.5064 - val_mse: 9.5064 - val_mae: 1.4277 - lr: 0.0010 - 7s/epoch - 7ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 7s - loss: 10.4134 - mse: 10.4134 - mae: 1.3432 - val_loss: 9.2367 - val_mse: 9.2367 - val_mae: 1.4409 - lr: 0.0010 - 7s/epoch - 7ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 7s - loss: 10.3436 - mse: 10.3436 - mae: 1.3362 - val_loss: 9.5213 - val_mse: 9.5213 - val_mae: 1.4082 - lr: 0.0010 - 7s/epoch - 7ms/step\n",
            "Score for fold 3: loss of 9.521306991577148\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 7s - loss: 9.8030 - mse: 9.8030 - mae: 1.3565 - val_loss: 11.7946 - val_mse: 11.7946 - val_mae: 1.3345 - lr: 0.0010 - 7s/epoch - 7ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 7s - loss: 9.3702 - mse: 9.3702 - mae: 1.3399 - val_loss: 12.4479 - val_mse: 12.4479 - val_mae: 1.3080 - lr: 0.0010 - 7s/epoch - 7ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 7s - loss: 9.1413 - mse: 9.1413 - mae: 1.3219 - val_loss: 12.6689 - val_mse: 12.6689 - val_mae: 1.3307 - lr: 0.0010 - 7s/epoch - 7ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 7s - loss: 8.9284 - mse: 8.9284 - mae: 1.3108 - val_loss: 12.5112 - val_mse: 12.5112 - val_mae: 1.3761 - lr: 0.0010 - 7s/epoch - 7ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 7s - loss: 8.6826 - mse: 8.6826 - mae: 1.2968 - val_loss: 12.3863 - val_mse: 12.3863 - val_mae: 1.3895 - lr: 0.0010 - 7s/epoch - 7ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 7s - loss: 8.7709 - mse: 8.7709 - mae: 1.2887 - val_loss: 13.3108 - val_mse: 13.3108 - val_mae: 1.4139 - lr: 0.0010 - 7s/epoch - 7ms/step\n",
            "Score for fold 4: loss of 13.310850143432617\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 7s - loss: 10.4167 - mse: 10.4167 - mae: 1.3098 - val_loss: 6.5201 - val_mse: 6.5201 - val_mae: 1.2888 - lr: 0.0010 - 7s/epoch - 7ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 7s - loss: 10.1536 - mse: 10.1536 - mae: 1.2959 - val_loss: 6.5477 - val_mse: 6.5477 - val_mae: 1.3125 - lr: 0.0010 - 7s/epoch - 7ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 7s - loss: 9.6951 - mse: 9.6951 - mae: 1.2830 - val_loss: 6.7385 - val_mse: 6.7385 - val_mae: 1.3218 - lr: 0.0010 - 7s/epoch - 7ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 7s - loss: 9.6451 - mse: 9.6451 - mae: 1.2673 - val_loss: 6.9424 - val_mse: 6.9424 - val_mae: 1.3225 - lr: 0.0010 - 7s/epoch - 7ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 7s - loss: 9.4919 - mse: 9.4919 - mae: 1.2503 - val_loss: 6.9349 - val_mse: 6.9349 - val_mae: 1.2878 - lr: 0.0010 - 7s/epoch - 7ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 7s - loss: 9.3070 - mse: 9.3070 - mae: 1.2439 - val_loss: 6.9414 - val_mse: 6.9414 - val_mae: 1.3448 - lr: 0.0010 - 7s/epoch - 7ms/step\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-10-11 23:07:52,490]\u001b[0m Finished trial#15 resulted in value: 11.166. Current best value is 9.724 with parameters: {'activation': 'relu', 'num_hidden_layer': 3, 'i': 10, 'learning_rate': 0.00039718555663465806}.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Score for fold 5: loss of 6.941422462463379\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 4s - loss: 14.8685 - mse: 14.8685 - mae: 1.6320 - val_loss: 10.1382 - val_mse: 10.1382 - val_mae: 1.5387 - lr: 2.0218e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 3s - loss: 14.1195 - mse: 14.1195 - mae: 1.5663 - val_loss: 10.1718 - val_mse: 10.1718 - val_mae: 1.5936 - lr: 2.0218e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 3s - loss: 14.2861 - mse: 14.2861 - mae: 1.5618 - val_loss: 10.4775 - val_mse: 10.4775 - val_mae: 1.6202 - lr: 2.0218e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 4s - loss: 14.1233 - mse: 14.1233 - mae: 1.5641 - val_loss: 10.2652 - val_mse: 10.2652 - val_mae: 1.5100 - lr: 2.0218e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 4s - loss: 14.1672 - mse: 14.1672 - mae: 1.5581 - val_loss: 10.0865 - val_mse: 10.0865 - val_mae: 1.5759 - lr: 2.0218e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 4s - loss: 14.0922 - mse: 14.0922 - mae: 1.5570 - val_loss: 10.0239 - val_mse: 10.0239 - val_mae: 1.5620 - lr: 2.0218e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 4s - loss: 14.1116 - mse: 14.1116 - mae: 1.5536 - val_loss: 10.1192 - val_mse: 10.1192 - val_mae: 1.5223 - lr: 2.0218e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 4s - loss: 14.0322 - mse: 14.0322 - mae: 1.5540 - val_loss: 9.9944 - val_mse: 9.9944 - val_mae: 1.5332 - lr: 2.0218e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 4s - loss: 14.0457 - mse: 14.0457 - mae: 1.5540 - val_loss: 10.0331 - val_mse: 10.0331 - val_mae: 1.5102 - lr: 2.0218e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 4s - loss: 14.1320 - mse: 14.1320 - mae: 1.5522 - val_loss: 10.0224 - val_mse: 10.0224 - val_mae: 1.5495 - lr: 2.0218e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 4s - loss: 14.0287 - mse: 14.0287 - mae: 1.5555 - val_loss: 10.0491 - val_mse: 10.0491 - val_mae: 1.5352 - lr: 2.0218e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 4s - loss: 14.0376 - mse: 14.0376 - mae: 1.5483 - val_loss: 10.0301 - val_mse: 10.0301 - val_mae: 1.5698 - lr: 2.0218e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 13/100\n",
            "1000/1000 - 4s - loss: 14.0647 - mse: 14.0647 - mae: 1.5532 - val_loss: 10.0982 - val_mse: 10.0982 - val_mae: 1.5739 - lr: 2.0218e-04 - 4s/epoch - 4ms/step\n",
            "Score for fold 1: loss of 10.098174095153809\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 3s - loss: 13.2015 - mse: 13.2015 - mae: 1.5549 - val_loss: 13.3988 - val_mse: 13.3988 - val_mae: 1.4947 - lr: 2.0218e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 4s - loss: 13.1746 - mse: 13.1746 - mae: 1.5536 - val_loss: 13.5439 - val_mse: 13.5439 - val_mae: 1.5695 - lr: 2.0218e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 4s - loss: 13.2438 - mse: 13.2438 - mae: 1.5528 - val_loss: 13.4958 - val_mse: 13.4958 - val_mae: 1.5525 - lr: 2.0218e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 4s - loss: 13.0936 - mse: 13.0936 - mae: 1.5506 - val_loss: 13.3912 - val_mse: 13.3912 - val_mae: 1.5554 - lr: 2.0218e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 4s - loss: 13.1710 - mse: 13.1710 - mae: 1.5504 - val_loss: 13.5169 - val_mse: 13.5169 - val_mae: 1.5370 - lr: 2.0218e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 4s - loss: 13.1695 - mse: 13.1695 - mae: 1.5512 - val_loss: 13.4141 - val_mse: 13.4141 - val_mae: 1.5344 - lr: 2.0218e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 4s - loss: 13.1500 - mse: 13.1500 - mae: 1.5485 - val_loss: 13.4351 - val_mse: 13.4351 - val_mae: 1.5153 - lr: 2.0218e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 4s - loss: 13.1894 - mse: 13.1894 - mae: 1.5490 - val_loss: 13.5018 - val_mse: 13.5018 - val_mae: 1.5635 - lr: 2.0218e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 4s - loss: 13.1597 - mse: 13.1597 - mae: 1.5539 - val_loss: 13.4280 - val_mse: 13.4280 - val_mae: 1.5154 - lr: 2.0218e-04 - 4s/epoch - 4ms/step\n",
            "Score for fold 2: loss of 13.428010940551758\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 4s - loss: 13.1483 - mse: 13.1483 - mae: 1.5462 - val_loss: 13.6642 - val_mse: 13.6642 - val_mae: 1.5488 - lr: 2.0218e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 4s - loss: 13.1171 - mse: 13.1171 - mae: 1.5415 - val_loss: 13.7859 - val_mse: 13.7859 - val_mae: 1.5552 - lr: 2.0218e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 4s - loss: 13.1971 - mse: 13.1971 - mae: 1.5423 - val_loss: 13.6553 - val_mse: 13.6553 - val_mae: 1.5629 - lr: 2.0218e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 4s - loss: 13.1632 - mse: 13.1632 - mae: 1.5499 - val_loss: 13.9257 - val_mse: 13.9257 - val_mae: 1.5796 - lr: 2.0218e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 4s - loss: 13.2531 - mse: 13.2531 - mae: 1.5436 - val_loss: 13.6933 - val_mse: 13.6933 - val_mae: 1.5739 - lr: 2.0218e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 4s - loss: 13.1869 - mse: 13.1869 - mae: 1.5477 - val_loss: 13.6879 - val_mse: 13.6879 - val_mae: 1.5562 - lr: 2.0218e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 4s - loss: 13.1818 - mse: 13.1818 - mae: 1.5414 - val_loss: 13.6926 - val_mse: 13.6926 - val_mae: 1.5538 - lr: 2.0218e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 4s - loss: 13.2313 - mse: 13.2313 - mae: 1.5448 - val_loss: 13.6772 - val_mse: 13.6772 - val_mae: 1.5716 - lr: 2.0218e-04 - 4s/epoch - 4ms/step\n",
            "Score for fold 3: loss of 13.677138328552246\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 4s - loss: 13.9792 - mse: 13.9792 - mae: 1.5502 - val_loss: 10.3253 - val_mse: 10.3253 - val_mae: 1.5466 - lr: 2.0218e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 4s - loss: 13.9615 - mse: 13.9615 - mae: 1.5516 - val_loss: 10.4653 - val_mse: 10.4653 - val_mae: 1.5183 - lr: 2.0218e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 4s - loss: 13.9252 - mse: 13.9252 - mae: 1.5450 - val_loss: 10.3360 - val_mse: 10.3360 - val_mae: 1.5321 - lr: 2.0218e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 4s - loss: 13.9296 - mse: 13.9296 - mae: 1.5475 - val_loss: 10.2952 - val_mse: 10.2952 - val_mae: 1.5382 - lr: 2.0218e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 4s - loss: 13.9899 - mse: 13.9899 - mae: 1.5472 - val_loss: 10.3094 - val_mse: 10.3094 - val_mae: 1.5628 - lr: 2.0218e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 4s - loss: 13.9433 - mse: 13.9433 - mae: 1.5479 - val_loss: 10.4765 - val_mse: 10.4765 - val_mae: 1.5341 - lr: 2.0218e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 4s - loss: 13.9989 - mse: 13.9989 - mae: 1.5517 - val_loss: 10.3619 - val_mse: 10.3619 - val_mae: 1.5455 - lr: 2.0218e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 4s - loss: 13.9475 - mse: 13.9475 - mae: 1.5471 - val_loss: 10.4362 - val_mse: 10.4362 - val_mae: 1.5503 - lr: 2.0218e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 4s - loss: 14.0000 - mse: 14.0000 - mae: 1.5510 - val_loss: 10.3409 - val_mse: 10.3409 - val_mae: 1.5468 - lr: 2.0218e-04 - 4s/epoch - 4ms/step\n",
            "Score for fold 4: loss of 10.340882301330566\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 3s - loss: 11.9016 - mse: 11.9016 - mae: 1.5406 - val_loss: 18.5776 - val_mse: 18.5776 - val_mae: 1.5745 - lr: 2.0218e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 4s - loss: 11.8591 - mse: 11.8591 - mae: 1.5433 - val_loss: 18.5499 - val_mse: 18.5499 - val_mae: 1.5397 - lr: 2.0218e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 4s - loss: 11.8773 - mse: 11.8773 - mae: 1.5398 - val_loss: 18.5582 - val_mse: 18.5582 - val_mae: 1.5392 - lr: 2.0218e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 4s - loss: 11.8552 - mse: 11.8552 - mae: 1.5407 - val_loss: 18.6103 - val_mse: 18.6103 - val_mae: 1.5864 - lr: 2.0218e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 4s - loss: 11.8671 - mse: 11.8671 - mae: 1.5451 - val_loss: 18.6007 - val_mse: 18.6007 - val_mae: 1.5293 - lr: 2.0218e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 4s - loss: 11.8730 - mse: 11.8730 - mae: 1.5417 - val_loss: 18.6384 - val_mse: 18.6384 - val_mae: 1.5192 - lr: 2.0218e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 4s - loss: 11.8383 - mse: 11.8383 - mae: 1.5420 - val_loss: 18.6122 - val_mse: 18.6122 - val_mae: 1.5946 - lr: 2.0218e-04 - 4s/epoch - 4ms/step\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-10-11 23:10:41,261]\u001b[0m Finished trial#16 resulted in value: 13.232. Current best value is 9.724 with parameters: {'activation': 'relu', 'num_hidden_layer': 3, 'i': 10, 'learning_rate': 0.00039718555663465806}.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Score for fold 5: loss of 18.61220932006836\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 8s - loss: 13.3150 - mse: 13.3150 - mae: 1.5358 - val_loss: 11.9191 - val_mse: 11.9191 - val_mae: 1.6111 - lr: 4.3366e-04 - 8s/epoch - 8ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 7s - loss: 12.6480 - mse: 12.6480 - mae: 1.4822 - val_loss: 11.2214 - val_mse: 11.2214 - val_mae: 1.6009 - lr: 4.3366e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 7s - loss: 12.4330 - mse: 12.4330 - mae: 1.4691 - val_loss: 11.3969 - val_mse: 11.3969 - val_mae: 1.5577 - lr: 4.3366e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 7s - loss: 12.2555 - mse: 12.2555 - mae: 1.4552 - val_loss: 11.2681 - val_mse: 11.2681 - val_mae: 1.6122 - lr: 4.3366e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 7s - loss: 12.0436 - mse: 12.0436 - mae: 1.4457 - val_loss: 11.1758 - val_mse: 11.1758 - val_mae: 1.5554 - lr: 4.3366e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 7s - loss: 11.9790 - mse: 11.9790 - mae: 1.4386 - val_loss: 10.9703 - val_mse: 10.9703 - val_mae: 1.4758 - lr: 4.3366e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 7s - loss: 11.7949 - mse: 11.7949 - mae: 1.4288 - val_loss: 11.1346 - val_mse: 11.1346 - val_mae: 1.4772 - lr: 4.3366e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 7s - loss: 11.7267 - mse: 11.7267 - mae: 1.4257 - val_loss: 10.9935 - val_mse: 10.9935 - val_mae: 1.4867 - lr: 4.3366e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 7s - loss: 11.4765 - mse: 11.4765 - mae: 1.4160 - val_loss: 11.1954 - val_mse: 11.1954 - val_mae: 1.5496 - lr: 4.3366e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 7s - loss: 11.4972 - mse: 11.4972 - mae: 1.4114 - val_loss: 10.7359 - val_mse: 10.7359 - val_mae: 1.4935 - lr: 4.3366e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 7s - loss: 11.3282 - mse: 11.3282 - mae: 1.4057 - val_loss: 10.7667 - val_mse: 10.7667 - val_mae: 1.4608 - lr: 4.3366e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 7s - loss: 11.0652 - mse: 11.0652 - mae: 1.3924 - val_loss: 11.2677 - val_mse: 11.2677 - val_mae: 1.5264 - lr: 4.3366e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 13/100\n",
            "1000/1000 - 7s - loss: 10.9718 - mse: 10.9718 - mae: 1.3873 - val_loss: 12.1626 - val_mse: 12.1626 - val_mae: 1.4668 - lr: 4.3366e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 14/100\n",
            "1000/1000 - 7s - loss: 10.7222 - mse: 10.7222 - mae: 1.3794 - val_loss: 12.1950 - val_mse: 12.1950 - val_mae: 1.5175 - lr: 4.3366e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 15/100\n",
            "1000/1000 - 7s - loss: 10.6206 - mse: 10.6206 - mae: 1.3688 - val_loss: 12.0255 - val_mse: 12.0255 - val_mae: 1.4859 - lr: 4.3366e-04 - 7s/epoch - 7ms/step\n",
            "Score for fold 1: loss of 12.025511741638184\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 7s - loss: 11.4531 - mse: 11.4531 - mae: 1.3944 - val_loss: 7.4566 - val_mse: 7.4566 - val_mae: 1.3236 - lr: 4.3366e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 7s - loss: 11.4147 - mse: 11.4147 - mae: 1.3843 - val_loss: 7.3621 - val_mse: 7.3621 - val_mae: 1.4076 - lr: 4.3366e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 7s - loss: 11.2998 - mse: 11.2998 - mae: 1.3762 - val_loss: 7.4906 - val_mse: 7.4906 - val_mae: 1.3880 - lr: 4.3366e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 7s - loss: 10.9735 - mse: 10.9735 - mae: 1.3617 - val_loss: 7.6326 - val_mse: 7.6326 - val_mae: 1.4263 - lr: 4.3366e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 7s - loss: 10.9162 - mse: 10.9162 - mae: 1.3536 - val_loss: 7.5979 - val_mse: 7.5979 - val_mae: 1.4180 - lr: 4.3366e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 7s - loss: 10.6907 - mse: 10.6907 - mae: 1.3382 - val_loss: 7.4817 - val_mse: 7.4817 - val_mae: 1.3845 - lr: 4.3366e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 7s - loss: 10.4839 - mse: 10.4839 - mae: 1.3260 - val_loss: 7.6953 - val_mse: 7.6953 - val_mae: 1.3660 - lr: 4.3366e-04 - 7s/epoch - 7ms/step\n",
            "Score for fold 2: loss of 7.695320129394531\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 7s - loss: 10.4074 - mse: 10.4074 - mae: 1.3454 - val_loss: 8.2355 - val_mse: 8.2355 - val_mae: 1.3385 - lr: 4.3366e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 7s - loss: 10.2635 - mse: 10.2635 - mae: 1.3330 - val_loss: 7.9102 - val_mse: 7.9102 - val_mae: 1.3212 - lr: 4.3366e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 7s - loss: 9.6978 - mse: 9.6978 - mae: 1.3098 - val_loss: 8.4912 - val_mse: 8.4912 - val_mae: 1.3296 - lr: 4.3366e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 7s - loss: 9.6910 - mse: 9.6910 - mae: 1.2981 - val_loss: 8.4406 - val_mse: 8.4406 - val_mae: 1.3011 - lr: 4.3366e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 7s - loss: 9.2741 - mse: 9.2741 - mae: 1.2829 - val_loss: 8.4247 - val_mse: 8.4247 - val_mae: 1.3557 - lr: 4.3366e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 7s - loss: 9.2535 - mse: 9.2535 - mae: 1.2735 - val_loss: 8.6680 - val_mse: 8.6680 - val_mae: 1.3288 - lr: 4.3366e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 7s - loss: 8.9139 - mse: 8.9139 - mae: 1.2544 - val_loss: 8.7897 - val_mse: 8.7897 - val_mae: 1.3798 - lr: 4.3366e-04 - 7s/epoch - 7ms/step\n",
            "Score for fold 3: loss of 8.789741516113281\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 7s - loss: 9.2426 - mse: 9.2426 - mae: 1.2878 - val_loss: 7.3154 - val_mse: 7.3154 - val_mae: 1.2411 - lr: 4.3366e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 7s - loss: 9.0041 - mse: 9.0041 - mae: 1.2603 - val_loss: 7.3825 - val_mse: 7.3825 - val_mae: 1.2596 - lr: 4.3366e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 7s - loss: 8.6992 - mse: 8.6992 - mae: 1.2465 - val_loss: 7.6627 - val_mse: 7.6627 - val_mae: 1.2396 - lr: 4.3366e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 7s - loss: 8.4033 - mse: 8.4033 - mae: 1.2267 - val_loss: 7.6104 - val_mse: 7.6104 - val_mae: 1.2245 - lr: 4.3366e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 7s - loss: 8.1709 - mse: 8.1709 - mae: 1.2095 - val_loss: 7.7188 - val_mse: 7.7188 - val_mae: 1.2777 - lr: 4.3366e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 7s - loss: 8.1718 - mse: 8.1718 - mae: 1.1972 - val_loss: 8.0143 - val_mse: 8.0143 - val_mae: 1.2839 - lr: 4.3366e-04 - 7s/epoch - 7ms/step\n",
            "Score for fold 4: loss of 8.014254570007324\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 7s - loss: 6.6523 - mse: 6.6523 - mae: 1.2120 - val_loss: 13.5649 - val_mse: 13.5649 - val_mae: 1.1609 - lr: 4.3366e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 7s - loss: 6.5320 - mse: 6.5320 - mae: 1.1850 - val_loss: 13.4514 - val_mse: 13.4514 - val_mae: 1.1787 - lr: 4.3366e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 7s - loss: 6.2682 - mse: 6.2682 - mae: 1.1676 - val_loss: 13.7686 - val_mse: 13.7686 - val_mae: 1.2110 - lr: 4.3366e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 7s - loss: 6.2769 - mse: 6.2769 - mae: 1.1540 - val_loss: 13.9623 - val_mse: 13.9623 - val_mae: 1.2049 - lr: 4.3366e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 7s - loss: 5.7438 - mse: 5.7438 - mae: 1.1299 - val_loss: 13.7880 - val_mse: 13.7880 - val_mae: 1.2106 - lr: 4.3366e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 7s - loss: 5.5823 - mse: 5.5823 - mae: 1.1100 - val_loss: 14.0733 - val_mse: 14.0733 - val_mae: 1.2291 - lr: 4.3366e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 7s - loss: 5.3857 - mse: 5.3857 - mae: 1.0953 - val_loss: 14.2013 - val_mse: 14.2013 - val_mae: 1.2240 - lr: 4.3366e-04 - 7s/epoch - 7ms/step\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-10-11 23:15:53,452]\u001b[0m Finished trial#17 resulted in value: 10.146. Current best value is 9.724 with parameters: {'activation': 'relu', 'num_hidden_layer': 3, 'i': 10, 'learning_rate': 0.00039718555663465806}.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Score for fold 5: loss of 14.20132064819336\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 3s - loss: 14.3537 - mse: 14.3537 - mae: 1.5516 - val_loss: 8.4542 - val_mse: 8.4542 - val_mae: 1.4381 - lr: 8.8754e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 13.4878 - mse: 13.4878 - mae: 1.5042 - val_loss: 8.2092 - val_mse: 8.2092 - val_mae: 1.4651 - lr: 8.8754e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 13.2132 - mse: 13.2132 - mae: 1.4911 - val_loss: 7.8461 - val_mse: 7.8461 - val_mae: 1.3708 - lr: 8.8754e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 13.0891 - mse: 13.0891 - mae: 1.4759 - val_loss: 7.4803 - val_mse: 7.4803 - val_mae: 1.4117 - lr: 8.8754e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 13.0753 - mse: 13.0753 - mae: 1.4706 - val_loss: 7.6142 - val_mse: 7.6142 - val_mae: 1.4302 - lr: 8.8754e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 12.9008 - mse: 12.9008 - mae: 1.4660 - val_loss: 7.6426 - val_mse: 7.6426 - val_mae: 1.4262 - lr: 8.8754e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 12.9219 - mse: 12.9219 - mae: 1.4601 - val_loss: 7.5396 - val_mse: 7.5396 - val_mae: 1.4008 - lr: 8.8754e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 2s - loss: 12.7325 - mse: 12.7325 - mae: 1.4567 - val_loss: 7.3800 - val_mse: 7.3800 - val_mae: 1.3977 - lr: 8.8754e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 2s - loss: 12.7819 - mse: 12.7819 - mae: 1.4529 - val_loss: 7.5024 - val_mse: 7.5024 - val_mae: 1.4135 - lr: 8.8754e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 2s - loss: 12.5946 - mse: 12.5946 - mae: 1.4483 - val_loss: 7.3769 - val_mse: 7.3769 - val_mae: 1.3771 - lr: 8.8754e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 2s - loss: 12.6600 - mse: 12.6600 - mae: 1.4432 - val_loss: 7.6839 - val_mse: 7.6839 - val_mae: 1.4197 - lr: 8.8754e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 2s - loss: 12.5993 - mse: 12.5993 - mae: 1.4390 - val_loss: 7.4840 - val_mse: 7.4840 - val_mae: 1.4406 - lr: 8.8754e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 13/100\n",
            "1000/1000 - 2s - loss: 12.4970 - mse: 12.4970 - mae: 1.4325 - val_loss: 7.6134 - val_mse: 7.6134 - val_mae: 1.5116 - lr: 8.8754e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 14/100\n",
            "1000/1000 - 2s - loss: 12.3219 - mse: 12.3219 - mae: 1.4297 - val_loss: 7.6820 - val_mse: 7.6820 - val_mae: 1.3996 - lr: 8.8754e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 15/100\n",
            "1000/1000 - 2s - loss: 12.3826 - mse: 12.3826 - mae: 1.4300 - val_loss: 8.2317 - val_mse: 8.2317 - val_mae: 1.4697 - lr: 8.8754e-04 - 2s/epoch - 2ms/step\n",
            "Score for fold 1: loss of 8.231729507446289\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 11.8169 - mse: 11.8169 - mae: 1.4296 - val_loss: 10.0814 - val_mse: 10.0814 - val_mae: 1.3905 - lr: 8.8754e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 11.4913 - mse: 11.4913 - mae: 1.4205 - val_loss: 10.2024 - val_mse: 10.2024 - val_mae: 1.4213 - lr: 8.8754e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 11.3274 - mse: 11.3274 - mae: 1.4154 - val_loss: 10.0970 - val_mse: 10.0970 - val_mae: 1.4313 - lr: 8.8754e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 11.2115 - mse: 11.2115 - mae: 1.4085 - val_loss: 10.0990 - val_mse: 10.0990 - val_mae: 1.3934 - lr: 8.8754e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 11.2965 - mse: 11.2965 - mae: 1.4014 - val_loss: 10.3914 - val_mse: 10.3914 - val_mae: 1.4914 - lr: 8.8754e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 11.1461 - mse: 11.1461 - mae: 1.3992 - val_loss: 10.2760 - val_mse: 10.2760 - val_mae: 1.4587 - lr: 8.8754e-04 - 2s/epoch - 2ms/step\n",
            "Score for fold 2: loss of 10.276004791259766\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 9.0737 - mse: 9.0737 - mae: 1.3955 - val_loss: 17.9898 - val_mse: 17.9898 - val_mae: 1.4259 - lr: 8.8754e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 9.1149 - mse: 9.1149 - mae: 1.3888 - val_loss: 17.9634 - val_mse: 17.9634 - val_mae: 1.4383 - lr: 8.8754e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 8.8992 - mse: 8.8992 - mae: 1.3818 - val_loss: 18.2188 - val_mse: 18.2188 - val_mae: 1.4303 - lr: 8.8754e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 8.8546 - mse: 8.8546 - mae: 1.3765 - val_loss: 18.3912 - val_mse: 18.3912 - val_mae: 1.4123 - lr: 8.8754e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 9.0789 - mse: 9.0789 - mae: 1.3723 - val_loss: 18.1936 - val_mse: 18.1936 - val_mae: 1.4811 - lr: 8.8754e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 9.0512 - mse: 9.0512 - mae: 1.3655 - val_loss: 18.6778 - val_mse: 18.6778 - val_mae: 1.4536 - lr: 8.8754e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 8.4810 - mse: 8.4810 - mae: 1.3599 - val_loss: 18.4399 - val_mse: 18.4399 - val_mae: 1.4415 - lr: 8.8754e-04 - 2s/epoch - 2ms/step\n",
            "Score for fold 3: loss of 18.439863204956055\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 10.8665 - mse: 10.8665 - mae: 1.3811 - val_loss: 8.9749 - val_mse: 8.9749 - val_mae: 1.4054 - lr: 8.8754e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 11.0644 - mse: 11.0644 - mae: 1.3776 - val_loss: 8.9463 - val_mse: 8.9463 - val_mae: 1.3504 - lr: 8.8754e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 10.7841 - mse: 10.7841 - mae: 1.3707 - val_loss: 8.8096 - val_mse: 8.8096 - val_mae: 1.3821 - lr: 8.8754e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 10.6504 - mse: 10.6504 - mae: 1.3619 - val_loss: 8.9859 - val_mse: 8.9859 - val_mae: 1.3730 - lr: 8.8754e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 10.5445 - mse: 10.5445 - mae: 1.3570 - val_loss: 9.1018 - val_mse: 9.1018 - val_mae: 1.3784 - lr: 8.8754e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 10.5725 - mse: 10.5725 - mae: 1.3523 - val_loss: 9.2371 - val_mse: 9.2371 - val_mae: 1.3839 - lr: 8.8754e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 10.6367 - mse: 10.6367 - mae: 1.3447 - val_loss: 8.9764 - val_mse: 8.9764 - val_mae: 1.3878 - lr: 8.8754e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 2s - loss: 10.3699 - mse: 10.3699 - mae: 1.3418 - val_loss: 8.9820 - val_mse: 8.9820 - val_mae: 1.4093 - lr: 8.8754e-04 - 2s/epoch - 2ms/step\n",
            "Score for fold 4: loss of 8.981972694396973\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 10.2120 - mse: 10.2120 - mae: 1.3527 - val_loss: 10.4052 - val_mse: 10.4052 - val_mae: 1.3480 - lr: 8.8754e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 10.0800 - mse: 10.0800 - mae: 1.3414 - val_loss: 9.9738 - val_mse: 9.9738 - val_mae: 1.3484 - lr: 8.8754e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 10.0314 - mse: 10.0314 - mae: 1.3347 - val_loss: 10.1845 - val_mse: 10.1845 - val_mae: 1.3668 - lr: 8.8754e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 9.8589 - mse: 9.8589 - mae: 1.3254 - val_loss: 13.2041 - val_mse: 13.2041 - val_mae: 1.3956 - lr: 8.8754e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 9.7704 - mse: 9.7704 - mae: 1.3262 - val_loss: 11.8126 - val_mse: 11.8126 - val_mae: 1.3908 - lr: 8.8754e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 9.7403 - mse: 9.7403 - mae: 1.3197 - val_loss: 12.5178 - val_mse: 12.5178 - val_mae: 1.4103 - lr: 8.8754e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 9.5820 - mse: 9.5820 - mae: 1.3136 - val_loss: 11.1083 - val_mse: 11.1083 - val_mae: 1.4026 - lr: 8.8754e-04 - 2s/epoch - 2ms/step\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-10-11 23:17:29,815]\u001b[0m Finished trial#18 resulted in value: 11.408000000000001. Current best value is 9.724 with parameters: {'activation': 'relu', 'num_hidden_layer': 3, 'i': 10, 'learning_rate': 0.00039718555663465806}.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Score for fold 5: loss of 11.10831069946289\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 13s - loss: 12.3157 - mse: 12.3157 - mae: 1.5288 - val_loss: 15.3074 - val_mse: 15.3074 - val_mae: 1.4989 - lr: 2.7563e-04 - 13s/epoch - 13ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 12s - loss: 11.6539 - mse: 11.6539 - mae: 1.4801 - val_loss: 16.3721 - val_mse: 16.3721 - val_mae: 1.4989 - lr: 2.7563e-04 - 12s/epoch - 12ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 12s - loss: 11.4304 - mse: 11.4304 - mae: 1.4597 - val_loss: 15.4940 - val_mse: 15.4940 - val_mae: 1.4711 - lr: 2.7563e-04 - 12s/epoch - 12ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 12s - loss: 11.5916 - mse: 11.5916 - mae: 1.4570 - val_loss: 15.9884 - val_mse: 15.9884 - val_mae: 1.4899 - lr: 2.7563e-04 - 12s/epoch - 12ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 12s - loss: 11.2835 - mse: 11.2835 - mae: 1.4452 - val_loss: 15.6288 - val_mse: 15.6288 - val_mae: 1.5562 - lr: 2.7563e-04 - 12s/epoch - 12ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 12s - loss: 11.0421 - mse: 11.0421 - mae: 1.4353 - val_loss: 15.0039 - val_mse: 15.0039 - val_mae: 1.5329 - lr: 2.7563e-04 - 12s/epoch - 12ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 12s - loss: 10.8289 - mse: 10.8289 - mae: 1.4231 - val_loss: 15.0692 - val_mse: 15.0692 - val_mae: 1.4881 - lr: 2.7563e-04 - 12s/epoch - 12ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 12s - loss: 10.9609 - mse: 10.9609 - mae: 1.4237 - val_loss: 14.5493 - val_mse: 14.5493 - val_mae: 1.4521 - lr: 2.7563e-04 - 12s/epoch - 12ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 12s - loss: 10.5340 - mse: 10.5340 - mae: 1.4068 - val_loss: 14.7823 - val_mse: 14.7823 - val_mae: 1.4655 - lr: 2.7563e-04 - 12s/epoch - 12ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 12s - loss: 10.2779 - mse: 10.2779 - mae: 1.4020 - val_loss: 15.2193 - val_mse: 15.2193 - val_mae: 1.4971 - lr: 2.7563e-04 - 12s/epoch - 12ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 12s - loss: 10.2197 - mse: 10.2197 - mae: 1.3844 - val_loss: 14.5433 - val_mse: 14.5433 - val_mae: 1.5751 - lr: 2.7563e-04 - 12s/epoch - 12ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 12s - loss: 9.8387 - mse: 9.8387 - mae: 1.3747 - val_loss: 15.1669 - val_mse: 15.1669 - val_mae: 1.4952 - lr: 2.7563e-04 - 12s/epoch - 12ms/step\n",
            "Epoch 13/100\n",
            "1000/1000 - 12s - loss: 9.5399 - mse: 9.5399 - mae: 1.3565 - val_loss: 14.5949 - val_mse: 14.5949 - val_mae: 1.4893 - lr: 2.7563e-04 - 12s/epoch - 12ms/step\n",
            "Epoch 14/100\n",
            "1000/1000 - 12s - loss: 9.4110 - mse: 9.4110 - mae: 1.3434 - val_loss: 14.7333 - val_mse: 14.7333 - val_mae: 1.5223 - lr: 2.7563e-04 - 12s/epoch - 12ms/step\n",
            "Epoch 15/100\n",
            "1000/1000 - 12s - loss: 9.2365 - mse: 9.2365 - mae: 1.3255 - val_loss: 14.5626 - val_mse: 14.5626 - val_mae: 1.4963 - lr: 2.7563e-04 - 12s/epoch - 12ms/step\n",
            "Epoch 16/100\n",
            "1000/1000 - 12s - loss: 8.8650 - mse: 8.8650 - mae: 1.3061 - val_loss: 14.8200 - val_mse: 14.8200 - val_mae: 1.5243 - lr: 2.7563e-04 - 12s/epoch - 12ms/step\n",
            "Score for fold 1: loss of 14.820032119750977\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 12s - loss: 11.1432 - mse: 11.1432 - mae: 1.3812 - val_loss: 6.3351 - val_mse: 6.3351 - val_mae: 1.2265 - lr: 2.7563e-04 - 12s/epoch - 12ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 13s - loss: 10.5664 - mse: 10.5664 - mae: 1.3486 - val_loss: 6.7705 - val_mse: 6.7705 - val_mae: 1.4380 - lr: 2.7563e-04 - 13s/epoch - 13ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 13s - loss: 10.3305 - mse: 10.3305 - mae: 1.3385 - val_loss: 6.6295 - val_mse: 6.6295 - val_mae: 1.2913 - lr: 2.7563e-04 - 13s/epoch - 13ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 14s - loss: 9.7969 - mse: 9.7969 - mae: 1.3107 - val_loss: 7.0951 - val_mse: 7.0951 - val_mae: 1.3096 - lr: 2.7563e-04 - 14s/epoch - 14ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 14s - loss: 9.7108 - mse: 9.7108 - mae: 1.3043 - val_loss: 7.1022 - val_mse: 7.1022 - val_mae: 1.2990 - lr: 2.7563e-04 - 14s/epoch - 14ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 14s - loss: 9.6310 - mse: 9.6310 - mae: 1.2845 - val_loss: 7.0193 - val_mse: 7.0193 - val_mae: 1.3075 - lr: 2.7563e-04 - 14s/epoch - 14ms/step\n",
            "Score for fold 2: loss of 7.019312381744385\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 13s - loss: 7.4084 - mse: 7.4084 - mae: 1.3081 - val_loss: 15.8214 - val_mse: 15.8214 - val_mae: 1.2969 - lr: 2.7563e-04 - 13s/epoch - 13ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 13s - loss: 7.1247 - mse: 7.1247 - mae: 1.2893 - val_loss: 15.1940 - val_mse: 15.1940 - val_mae: 1.3564 - lr: 2.7563e-04 - 13s/epoch - 13ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 13s - loss: 6.8028 - mse: 6.8028 - mae: 1.2611 - val_loss: 15.3390 - val_mse: 15.3390 - val_mae: 1.2033 - lr: 2.7563e-04 - 13s/epoch - 13ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 13s - loss: 6.5396 - mse: 6.5396 - mae: 1.2464 - val_loss: 15.3052 - val_mse: 15.3052 - val_mae: 1.3534 - lr: 2.7563e-04 - 13s/epoch - 13ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 14s - loss: 6.3392 - mse: 6.3392 - mae: 1.2281 - val_loss: 16.2725 - val_mse: 16.2725 - val_mae: 1.2510 - lr: 2.7563e-04 - 14s/epoch - 14ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 12s - loss: 6.0579 - mse: 6.0579 - mae: 1.2075 - val_loss: 16.1348 - val_mse: 16.1348 - val_mae: 1.3859 - lr: 2.7563e-04 - 12s/epoch - 12ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 12s - loss: 5.9744 - mse: 5.9744 - mae: 1.1936 - val_loss: 16.0729 - val_mse: 16.0729 - val_mae: 1.4024 - lr: 2.7563e-04 - 12s/epoch - 12ms/step\n",
            "Score for fold 3: loss of 16.07294464111328\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 12s - loss: 9.0927 - mse: 9.0927 - mae: 1.2459 - val_loss: 3.9146 - val_mse: 3.9146 - val_mae: 1.1802 - lr: 2.7563e-04 - 12s/epoch - 12ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 16s - loss: 8.6783 - mse: 8.6783 - mae: 1.2236 - val_loss: 3.8659 - val_mse: 3.8659 - val_mae: 1.1648 - lr: 2.7563e-04 - 16s/epoch - 16ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 18s - loss: 8.4280 - mse: 8.4280 - mae: 1.2024 - val_loss: 3.9553 - val_mse: 3.9553 - val_mae: 1.1241 - lr: 2.7563e-04 - 18s/epoch - 18ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 19s - loss: 7.8328 - mse: 7.8328 - mae: 1.1854 - val_loss: 3.9765 - val_mse: 3.9765 - val_mae: 1.2183 - lr: 2.7563e-04 - 19s/epoch - 19ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 20s - loss: 7.5975 - mse: 7.5975 - mae: 1.1683 - val_loss: 4.0809 - val_mse: 4.0809 - val_mae: 1.1646 - lr: 2.7563e-04 - 20s/epoch - 20ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 20s - loss: 7.1682 - mse: 7.1682 - mae: 1.1480 - val_loss: 4.4637 - val_mse: 4.4637 - val_mae: 1.4014 - lr: 2.7563e-04 - 20s/epoch - 20ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 23s - loss: 7.3738 - mse: 7.3738 - mae: 1.1469 - val_loss: 4.5812 - val_mse: 4.5812 - val_mae: 1.1918 - lr: 2.7563e-04 - 23s/epoch - 23ms/step\n",
            "Score for fold 4: loss of 4.581204414367676\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 29s - loss: 7.2660 - mse: 7.2660 - mae: 1.1716 - val_loss: 3.2436 - val_mse: 3.2436 - val_mae: 1.1227 - lr: 2.7563e-04 - 29s/epoch - 29ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 30s - loss: 6.9330 - mse: 6.9330 - mae: 1.1472 - val_loss: 3.4653 - val_mse: 3.4653 - val_mae: 1.1168 - lr: 2.7563e-04 - 30s/epoch - 30ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 24s - loss: 6.6202 - mse: 6.6202 - mae: 1.1258 - val_loss: 3.6560 - val_mse: 3.6560 - val_mae: 1.1766 - lr: 2.7563e-04 - 24s/epoch - 24ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 20s - loss: 6.5444 - mse: 6.5444 - mae: 1.1207 - val_loss: 3.3857 - val_mse: 3.3857 - val_mae: 1.1694 - lr: 2.7563e-04 - 20s/epoch - 20ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 18s - loss: 6.0885 - mse: 6.0885 - mae: 1.0988 - val_loss: 3.8766 - val_mse: 3.8766 - val_mae: 1.1004 - lr: 2.7563e-04 - 18s/epoch - 18ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 18s - loss: 5.6591 - mse: 5.6591 - mae: 1.0812 - val_loss: 3.9069 - val_mse: 3.9069 - val_mae: 1.2598 - lr: 2.7563e-04 - 18s/epoch - 18ms/step\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-10-11 23:28:10,519]\u001b[0m Finished trial#19 resulted in value: 9.279999999999998. Current best value is 9.279999999999998 with parameters: {'activation': 'relu', 'num_hidden_layer': 4, 'i': 10, 'learning_rate': 0.00027563299450396555}.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Score for fold 5: loss of 3.9069457054138184\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 5s - loss: 13.3238 - mse: 13.3238 - mae: 1.6471 - val_loss: 16.0544 - val_mse: 16.0544 - val_mae: 1.5747 - lr: 1.5704e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 5s - loss: 12.7165 - mse: 12.7165 - mae: 1.5627 - val_loss: 15.9274 - val_mse: 15.9274 - val_mae: 1.5500 - lr: 1.5704e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 5s - loss: 12.7465 - mse: 12.7465 - mae: 1.5598 - val_loss: 15.8256 - val_mse: 15.8256 - val_mae: 1.5952 - lr: 1.5704e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 5s - loss: 12.6178 - mse: 12.6178 - mae: 1.5585 - val_loss: 15.7939 - val_mse: 15.7939 - val_mae: 1.5166 - lr: 1.5704e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 5s - loss: 12.7783 - mse: 12.7783 - mae: 1.5586 - val_loss: 16.0268 - val_mse: 16.0268 - val_mae: 1.5360 - lr: 1.5704e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 5s - loss: 12.7696 - mse: 12.7696 - mae: 1.5557 - val_loss: 15.8833 - val_mse: 15.8833 - val_mae: 1.5420 - lr: 1.5704e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 4s - loss: 12.7138 - mse: 12.7138 - mae: 1.5578 - val_loss: 15.8187 - val_mse: 15.8187 - val_mae: 1.5427 - lr: 1.5704e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 4s - loss: 12.6124 - mse: 12.6124 - mae: 1.5477 - val_loss: 15.7969 - val_mse: 15.7969 - val_mae: 1.5302 - lr: 1.5704e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 4s - loss: 12.5895 - mse: 12.5895 - mae: 1.5511 - val_loss: 15.8038 - val_mse: 15.8038 - val_mae: 1.5439 - lr: 1.5704e-04 - 4s/epoch - 4ms/step\n",
            "Score for fold 1: loss of 15.803844451904297\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 4s - loss: 13.6793 - mse: 13.6793 - mae: 1.5562 - val_loss: 11.3255 - val_mse: 11.3255 - val_mae: 1.5329 - lr: 1.5704e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 4s - loss: 13.7677 - mse: 13.7677 - mae: 1.5592 - val_loss: 11.4264 - val_mse: 11.4264 - val_mae: 1.4911 - lr: 1.5704e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 4s - loss: 13.7541 - mse: 13.7541 - mae: 1.5554 - val_loss: 11.3858 - val_mse: 11.3858 - val_mae: 1.5179 - lr: 1.5704e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 4s - loss: 13.7873 - mse: 13.7873 - mae: 1.5617 - val_loss: 11.3607 - val_mse: 11.3607 - val_mae: 1.5413 - lr: 1.5704e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 4s - loss: 13.7982 - mse: 13.7982 - mae: 1.5579 - val_loss: 11.3948 - val_mse: 11.3948 - val_mae: 1.5443 - lr: 1.5704e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 4s - loss: 13.6932 - mse: 13.6932 - mae: 1.5555 - val_loss: 11.3028 - val_mse: 11.3028 - val_mae: 1.5232 - lr: 1.5704e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 4s - loss: 13.7652 - mse: 13.7652 - mae: 1.5553 - val_loss: 11.4033 - val_mse: 11.4033 - val_mae: 1.5056 - lr: 1.5704e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 4s - loss: 13.7358 - mse: 13.7358 - mae: 1.5557 - val_loss: 11.3279 - val_mse: 11.3279 - val_mae: 1.5347 - lr: 1.5704e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 4s - loss: 13.7678 - mse: 13.7678 - mae: 1.5523 - val_loss: 11.3614 - val_mse: 11.3614 - val_mae: 1.5381 - lr: 1.5704e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 4s - loss: 13.6411 - mse: 13.6411 - mae: 1.5529 - val_loss: 11.3940 - val_mse: 11.3940 - val_mae: 1.5490 - lr: 1.5704e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 4s - loss: 13.6843 - mse: 13.6843 - mae: 1.5544 - val_loss: 11.3980 - val_mse: 11.3980 - val_mae: 1.4886 - lr: 1.5704e-04 - 4s/epoch - 4ms/step\n",
            "Score for fold 2: loss of 11.397985458374023\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 4s - loss: 13.8289 - mse: 13.8289 - mae: 1.5459 - val_loss: 10.7453 - val_mse: 10.7453 - val_mae: 1.5585 - lr: 1.5704e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 4s - loss: 13.8249 - mse: 13.8249 - mae: 1.5465 - val_loss: 10.6034 - val_mse: 10.6034 - val_mae: 1.5517 - lr: 1.5704e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 4s - loss: 13.8933 - mse: 13.8933 - mae: 1.5459 - val_loss: 10.6616 - val_mse: 10.6616 - val_mae: 1.5495 - lr: 1.5704e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 4s - loss: 13.8427 - mse: 13.8427 - mae: 1.5436 - val_loss: 10.6664 - val_mse: 10.6664 - val_mae: 1.5314 - lr: 1.5704e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 4s - loss: 13.8955 - mse: 13.8955 - mae: 1.5458 - val_loss: 10.7443 - val_mse: 10.7443 - val_mae: 1.5507 - lr: 1.5704e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 4s - loss: 13.8453 - mse: 13.8453 - mae: 1.5426 - val_loss: 10.7593 - val_mse: 10.7593 - val_mae: 1.5796 - lr: 1.5704e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 4s - loss: 13.8345 - mse: 13.8345 - mae: 1.5431 - val_loss: 10.7368 - val_mse: 10.7368 - val_mae: 1.5545 - lr: 1.5704e-04 - 4s/epoch - 4ms/step\n",
            "Score for fold 3: loss of 10.736820220947266\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 4s - loss: 13.7925 - mse: 13.7925 - mae: 1.5362 - val_loss: 11.2129 - val_mse: 11.2129 - val_mae: 1.5581 - lr: 1.5704e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 4s - loss: 13.7412 - mse: 13.7412 - mae: 1.5363 - val_loss: 11.1436 - val_mse: 11.1436 - val_mae: 1.6063 - lr: 1.5704e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 4s - loss: 13.7435 - mse: 13.7435 - mae: 1.5381 - val_loss: 11.2011 - val_mse: 11.2011 - val_mae: 1.5946 - lr: 1.5704e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 4s - loss: 13.8009 - mse: 13.8009 - mae: 1.5341 - val_loss: 11.1140 - val_mse: 11.1140 - val_mae: 1.5679 - lr: 1.5704e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 4s - loss: 13.7544 - mse: 13.7544 - mae: 1.5383 - val_loss: 11.2520 - val_mse: 11.2520 - val_mae: 1.6445 - lr: 1.5704e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 5s - loss: 13.6838 - mse: 13.6838 - mae: 1.5359 - val_loss: 11.2000 - val_mse: 11.2000 - val_mae: 1.5746 - lr: 1.5704e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 5s - loss: 13.7407 - mse: 13.7407 - mae: 1.5358 - val_loss: 11.1658 - val_mse: 11.1658 - val_mae: 1.5586 - lr: 1.5704e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 4s - loss: 13.7695 - mse: 13.7695 - mae: 1.5335 - val_loss: 11.1707 - val_mse: 11.1707 - val_mae: 1.5743 - lr: 1.5704e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 4s - loss: 13.7923 - mse: 13.7923 - mae: 1.5326 - val_loss: 11.2245 - val_mse: 11.2245 - val_mae: 1.5684 - lr: 1.5704e-04 - 4s/epoch - 4ms/step\n",
            "Score for fold 4: loss of 11.224481582641602\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 4s - loss: 12.2867 - mse: 12.2867 - mae: 1.5462 - val_loss: 17.0734 - val_mse: 17.0734 - val_mae: 1.5517 - lr: 1.5704e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 3s - loss: 12.2826 - mse: 12.2826 - mae: 1.5469 - val_loss: 17.2756 - val_mse: 17.2756 - val_mae: 1.5633 - lr: 1.5704e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 4s - loss: 12.2803 - mse: 12.2803 - mae: 1.5482 - val_loss: 17.1383 - val_mse: 17.1383 - val_mae: 1.5602 - lr: 1.5704e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 3s - loss: 12.2442 - mse: 12.2442 - mae: 1.5504 - val_loss: 17.3377 - val_mse: 17.3377 - val_mae: 1.5569 - lr: 1.5704e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 4s - loss: 12.2191 - mse: 12.2191 - mae: 1.5459 - val_loss: 17.4183 - val_mse: 17.4183 - val_mae: 1.5497 - lr: 1.5704e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 4s - loss: 12.2436 - mse: 12.2436 - mae: 1.5481 - val_loss: 17.3208 - val_mse: 17.3208 - val_mae: 1.5593 - lr: 1.5704e-04 - 4s/epoch - 4ms/step\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-10-11 23:31:09,834]\u001b[0m Finished trial#20 resulted in value: 13.296000000000001. Current best value is 9.279999999999998 with parameters: {'activation': 'relu', 'num_hidden_layer': 4, 'i': 10, 'learning_rate': 0.00027563299450396555}.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Score for fold 5: loss of 17.320833206176758\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 12s - loss: 13.3893 - mse: 13.3893 - mae: 1.5419 - val_loss: 12.0364 - val_mse: 12.0364 - val_mae: 1.4995 - lr: 2.9897e-04 - 12s/epoch - 12ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 12s - loss: 12.6422 - mse: 12.6422 - mae: 1.4913 - val_loss: 11.7278 - val_mse: 11.7278 - val_mae: 1.4724 - lr: 2.9897e-04 - 12s/epoch - 12ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 12s - loss: 12.2237 - mse: 12.2237 - mae: 1.4792 - val_loss: 12.2354 - val_mse: 12.2354 - val_mae: 1.4396 - lr: 2.9897e-04 - 12s/epoch - 12ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 11s - loss: 12.1396 - mse: 12.1396 - mae: 1.4652 - val_loss: 12.5591 - val_mse: 12.5591 - val_mae: 1.5120 - lr: 2.9897e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 11s - loss: 12.0762 - mse: 12.0762 - mae: 1.4590 - val_loss: 11.8649 - val_mse: 11.8649 - val_mae: 1.4898 - lr: 2.9897e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 11s - loss: 11.7098 - mse: 11.7098 - mae: 1.4448 - val_loss: 11.8760 - val_mse: 11.8760 - val_mae: 1.5007 - lr: 2.9897e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 11s - loss: 11.6620 - mse: 11.6620 - mae: 1.4371 - val_loss: 12.3154 - val_mse: 12.3154 - val_mae: 1.5240 - lr: 2.9897e-04 - 11s/epoch - 11ms/step\n",
            "Score for fold 1: loss of 12.315412521362305\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 11s - loss: 12.5016 - mse: 12.5016 - mae: 1.4518 - val_loss: 7.8853 - val_mse: 7.8853 - val_mae: 1.4453 - lr: 2.9897e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 11s - loss: 12.2797 - mse: 12.2797 - mae: 1.4382 - val_loss: 8.7620 - val_mse: 8.7620 - val_mae: 1.5870 - lr: 2.9897e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 11s - loss: 12.1044 - mse: 12.1044 - mae: 1.4232 - val_loss: 8.2636 - val_mse: 8.2636 - val_mae: 1.3281 - lr: 2.9897e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 11s - loss: 11.6455 - mse: 11.6455 - mae: 1.4123 - val_loss: 8.2640 - val_mse: 8.2640 - val_mae: 1.3904 - lr: 2.9897e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 11s - loss: 11.4375 - mse: 11.4375 - mae: 1.3973 - val_loss: 8.0557 - val_mse: 8.0557 - val_mae: 1.4113 - lr: 2.9897e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 14s - loss: 11.4322 - mse: 11.4322 - mae: 1.3855 - val_loss: 7.8653 - val_mse: 7.8653 - val_mae: 1.3941 - lr: 2.9897e-04 - 14s/epoch - 14ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 17s - loss: 11.1676 - mse: 11.1676 - mae: 1.3642 - val_loss: 8.0355 - val_mse: 8.0355 - val_mae: 1.4665 - lr: 2.9897e-04 - 17s/epoch - 17ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 17s - loss: 10.8339 - mse: 10.8339 - mae: 1.3521 - val_loss: 8.3066 - val_mse: 8.3066 - val_mae: 1.3813 - lr: 2.9897e-04 - 17s/epoch - 17ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 18s - loss: 10.7559 - mse: 10.7559 - mae: 1.3372 - val_loss: 8.4088 - val_mse: 8.4088 - val_mae: 1.3981 - lr: 2.9897e-04 - 18s/epoch - 18ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 18s - loss: 10.2932 - mse: 10.2932 - mae: 1.3174 - val_loss: 8.4357 - val_mse: 8.4357 - val_mae: 1.4869 - lr: 2.9897e-04 - 18s/epoch - 18ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 18s - loss: 9.8305 - mse: 9.8305 - mae: 1.3032 - val_loss: 8.1392 - val_mse: 8.1392 - val_mae: 1.4795 - lr: 2.9897e-04 - 18s/epoch - 18ms/step\n",
            "Score for fold 2: loss of 8.139213562011719\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 17s - loss: 10.6139 - mse: 10.6139 - mae: 1.3480 - val_loss: 7.4318 - val_mse: 7.4318 - val_mae: 1.2733 - lr: 2.9897e-04 - 17s/epoch - 17ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 16s - loss: 9.7508 - mse: 9.7508 - mae: 1.3175 - val_loss: 7.3974 - val_mse: 7.3974 - val_mae: 1.3387 - lr: 2.9897e-04 - 16s/epoch - 16ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 19s - loss: 9.5546 - mse: 9.5546 - mae: 1.3017 - val_loss: 7.6100 - val_mse: 7.6100 - val_mae: 1.2953 - lr: 2.9897e-04 - 19s/epoch - 19ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 22s - loss: 9.2915 - mse: 9.2915 - mae: 1.2845 - val_loss: 7.5138 - val_mse: 7.5138 - val_mae: 1.4471 - lr: 2.9897e-04 - 22s/epoch - 22ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 20s - loss: 9.0447 - mse: 9.0447 - mae: 1.2744 - val_loss: 7.5719 - val_mse: 7.5719 - val_mae: 1.3441 - lr: 2.9897e-04 - 20s/epoch - 20ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 24s - loss: 8.7020 - mse: 8.7020 - mae: 1.2600 - val_loss: 7.6381 - val_mse: 7.6381 - val_mae: 1.3945 - lr: 2.9897e-04 - 24s/epoch - 24ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 21s - loss: 8.3570 - mse: 8.3570 - mae: 1.2418 - val_loss: 8.0412 - val_mse: 8.0412 - val_mae: 1.3072 - lr: 2.9897e-04 - 21s/epoch - 21ms/step\n",
            "Score for fold 3: loss of 8.041231155395508\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 20s - loss: 7.0978 - mse: 7.0978 - mae: 1.2654 - val_loss: 12.8538 - val_mse: 12.8538 - val_mae: 1.2213 - lr: 2.9897e-04 - 20s/epoch - 20ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 19s - loss: 6.6462 - mse: 6.6462 - mae: 1.2495 - val_loss: 13.2449 - val_mse: 13.2449 - val_mae: 1.3489 - lr: 2.9897e-04 - 19s/epoch - 19ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 19s - loss: 6.4633 - mse: 6.4633 - mae: 1.2306 - val_loss: 13.6654 - val_mse: 13.6654 - val_mae: 1.2449 - lr: 2.9897e-04 - 19s/epoch - 19ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 14s - loss: 6.4981 - mse: 6.4981 - mae: 1.2127 - val_loss: 13.8046 - val_mse: 13.8046 - val_mae: 1.3254 - lr: 2.9897e-04 - 14s/epoch - 14ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 21s - loss: 6.3167 - mse: 6.3167 - mae: 1.2055 - val_loss: 13.8133 - val_mse: 13.8133 - val_mae: 1.3260 - lr: 2.9897e-04 - 21s/epoch - 21ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 20s - loss: 5.8495 - mse: 5.8495 - mae: 1.1819 - val_loss: 14.4942 - val_mse: 14.4942 - val_mae: 1.2777 - lr: 2.9897e-04 - 20s/epoch - 20ms/step\n",
            "Score for fold 4: loss of 14.494231224060059\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 18s - loss: 8.0746 - mse: 8.0746 - mae: 1.2244 - val_loss: 6.1827 - val_mse: 6.1827 - val_mae: 1.1279 - lr: 2.9897e-04 - 18s/epoch - 18ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 17s - loss: 7.4692 - mse: 7.4692 - mae: 1.2019 - val_loss: 6.2989 - val_mse: 6.2989 - val_mae: 1.1893 - lr: 2.9897e-04 - 17s/epoch - 17ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 19s - loss: 7.1467 - mse: 7.1467 - mae: 1.1870 - val_loss: 8.3606 - val_mse: 8.3606 - val_mae: 1.1441 - lr: 2.9897e-04 - 19s/epoch - 19ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 15s - loss: 6.8893 - mse: 6.8893 - mae: 1.1648 - val_loss: 7.0168 - val_mse: 7.0168 - val_mae: 1.1341 - lr: 2.9897e-04 - 15s/epoch - 15ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 21s - loss: 6.5648 - mse: 6.5648 - mae: 1.1539 - val_loss: 6.9946 - val_mse: 6.9946 - val_mae: 1.3045 - lr: 2.9897e-04 - 21s/epoch - 21ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 20s - loss: 6.5023 - mse: 6.5023 - mae: 1.1483 - val_loss: 7.3526 - val_mse: 7.3526 - val_mae: 1.2903 - lr: 2.9897e-04 - 20s/epoch - 20ms/step\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-10-11 23:41:23,959]\u001b[0m Finished trial#21 resulted in value: 10.068000000000001. Current best value is 9.279999999999998 with parameters: {'activation': 'relu', 'num_hidden_layer': 4, 'i': 10, 'learning_rate': 0.00027563299450396555}.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Score for fold 5: loss of 7.352581024169922\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 21s - loss: 14.5320 - mse: 14.5320 - mae: 1.5675 - val_loss: 9.6635 - val_mse: 9.6635 - val_mae: 1.4423 - lr: 5.4122e-04 - 21s/epoch - 21ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 18s - loss: 13.2079 - mse: 13.2079 - mae: 1.5002 - val_loss: 9.4897 - val_mse: 9.4897 - val_mae: 1.4624 - lr: 5.4122e-04 - 18s/epoch - 18ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 21s - loss: 12.9358 - mse: 12.9358 - mae: 1.4885 - val_loss: 9.9669 - val_mse: 9.9669 - val_mae: 1.4605 - lr: 5.4122e-04 - 21s/epoch - 21ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 18s - loss: 12.8402 - mse: 12.8402 - mae: 1.4755 - val_loss: 9.8105 - val_mse: 9.8105 - val_mae: 1.4752 - lr: 5.4122e-04 - 18s/epoch - 18ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 16s - loss: 12.5906 - mse: 12.5906 - mae: 1.4646 - val_loss: 9.8462 - val_mse: 9.8462 - val_mae: 1.5181 - lr: 5.4122e-04 - 16s/epoch - 16ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 14s - loss: 12.4484 - mse: 12.4484 - mae: 1.4563 - val_loss: 10.7473 - val_mse: 10.7473 - val_mae: 1.4643 - lr: 5.4122e-04 - 14s/epoch - 14ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 18s - loss: 12.3999 - mse: 12.3999 - mae: 1.4547 - val_loss: 9.5237 - val_mse: 9.5237 - val_mae: 1.5204 - lr: 5.4122e-04 - 18s/epoch - 18ms/step\n",
            "Score for fold 1: loss of 9.523707389831543\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 14s - loss: 9.6797 - mse: 9.6797 - mae: 1.4441 - val_loss: 19.8258 - val_mse: 19.8258 - val_mae: 1.4656 - lr: 5.4122e-04 - 14s/epoch - 14ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 12s - loss: 9.5807 - mse: 9.5807 - mae: 1.4383 - val_loss: 19.6625 - val_mse: 19.6625 - val_mae: 1.5285 - lr: 5.4122e-04 - 12s/epoch - 12ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 12s - loss: 9.3912 - mse: 9.3912 - mae: 1.4297 - val_loss: 19.7202 - val_mse: 19.7202 - val_mae: 1.4533 - lr: 5.4122e-04 - 12s/epoch - 12ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 12s - loss: 9.2956 - mse: 9.2956 - mae: 1.4161 - val_loss: 19.6501 - val_mse: 19.6501 - val_mae: 1.5224 - lr: 5.4122e-04 - 12s/epoch - 12ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 12s - loss: 9.1113 - mse: 9.1113 - mae: 1.4120 - val_loss: 20.5608 - val_mse: 20.5608 - val_mae: 1.5512 - lr: 5.4122e-04 - 12s/epoch - 12ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 11s - loss: 8.9860 - mse: 8.9860 - mae: 1.4019 - val_loss: 20.6016 - val_mse: 20.6016 - val_mae: 1.4929 - lr: 5.4122e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 11s - loss: 8.9360 - mse: 8.9360 - mae: 1.3952 - val_loss: 23.0573 - val_mse: 23.0573 - val_mae: 1.4492 - lr: 5.4122e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 11s - loss: 8.7718 - mse: 8.7718 - mae: 1.3905 - val_loss: 20.5116 - val_mse: 20.5116 - val_mae: 1.5592 - lr: 5.4122e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 11s - loss: 8.5689 - mse: 8.5689 - mae: 1.3767 - val_loss: 19.7295 - val_mse: 19.7295 - val_mae: 1.4520 - lr: 5.4122e-04 - 11s/epoch - 11ms/step\n",
            "Score for fold 2: loss of 19.729516983032227\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 11s - loss: 11.4382 - mse: 11.4382 - mae: 1.4151 - val_loss: 8.9587 - val_mse: 8.9587 - val_mae: 1.3096 - lr: 5.4122e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 13s - loss: 11.1780 - mse: 11.1780 - mae: 1.3994 - val_loss: 8.9816 - val_mse: 8.9816 - val_mae: 1.3813 - lr: 5.4122e-04 - 13s/epoch - 13ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 17s - loss: 11.0686 - mse: 11.0686 - mae: 1.3903 - val_loss: 9.4773 - val_mse: 9.4773 - val_mae: 1.4088 - lr: 5.4122e-04 - 17s/epoch - 17ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 17s - loss: 10.8430 - mse: 10.8430 - mae: 1.3811 - val_loss: 9.3022 - val_mse: 9.3022 - val_mae: 1.4332 - lr: 5.4122e-04 - 17s/epoch - 17ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 18s - loss: 10.3833 - mse: 10.3833 - mae: 1.3652 - val_loss: 9.5016 - val_mse: 9.5016 - val_mae: 1.3862 - lr: 5.4122e-04 - 18s/epoch - 18ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 18s - loss: 10.2533 - mse: 10.2533 - mae: 1.3562 - val_loss: 9.5335 - val_mse: 9.5335 - val_mae: 1.4043 - lr: 5.4122e-04 - 18s/epoch - 18ms/step\n",
            "Score for fold 3: loss of 9.533452033996582\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 19s - loss: 10.4343 - mse: 10.4343 - mae: 1.3723 - val_loss: 10.3713 - val_mse: 10.3713 - val_mae: 1.3781 - lr: 5.4122e-04 - 19s/epoch - 19ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 18s - loss: 9.8360 - mse: 9.8360 - mae: 1.3593 - val_loss: 10.3486 - val_mse: 10.3486 - val_mae: 1.3831 - lr: 5.4122e-04 - 18s/epoch - 18ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 18s - loss: 9.4785 - mse: 9.4785 - mae: 1.3414 - val_loss: 10.7421 - val_mse: 10.7421 - val_mae: 1.3841 - lr: 5.4122e-04 - 18s/epoch - 18ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 18s - loss: 9.6005 - mse: 9.6005 - mae: 1.3320 - val_loss: 10.7951 - val_mse: 10.7951 - val_mae: 1.3838 - lr: 5.4122e-04 - 18s/epoch - 18ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 18s - loss: 9.2280 - mse: 9.2280 - mae: 1.3197 - val_loss: 10.7303 - val_mse: 10.7303 - val_mae: 1.3665 - lr: 5.4122e-04 - 18s/epoch - 18ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 18s - loss: 9.0385 - mse: 9.0385 - mae: 1.3110 - val_loss: 10.7152 - val_mse: 10.7152 - val_mae: 1.4021 - lr: 5.4122e-04 - 18s/epoch - 18ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 18s - loss: 8.8535 - mse: 8.8535 - mae: 1.3035 - val_loss: 11.2191 - val_mse: 11.2191 - val_mae: 1.3390 - lr: 5.4122e-04 - 18s/epoch - 18ms/step\n",
            "Score for fold 4: loss of 11.219085693359375\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 18s - loss: 10.2380 - mse: 10.2380 - mae: 1.3288 - val_loss: 4.9817 - val_mse: 4.9817 - val_mae: 1.2486 - lr: 5.4122e-04 - 18s/epoch - 18ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 18s - loss: 10.2198 - mse: 10.2198 - mae: 1.3207 - val_loss: 5.2615 - val_mse: 5.2615 - val_mae: 1.2660 - lr: 5.4122e-04 - 18s/epoch - 18ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 18s - loss: 10.0112 - mse: 10.0112 - mae: 1.3033 - val_loss: 5.0829 - val_mse: 5.0829 - val_mae: 1.2980 - lr: 5.4122e-04 - 18s/epoch - 18ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 19s - loss: 9.8096 - mse: 9.8096 - mae: 1.2986 - val_loss: 5.8664 - val_mse: 5.8664 - val_mae: 1.2970 - lr: 5.4122e-04 - 19s/epoch - 19ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 18s - loss: 9.4601 - mse: 9.4601 - mae: 1.2825 - val_loss: 5.4465 - val_mse: 5.4465 - val_mae: 1.3211 - lr: 5.4122e-04 - 18s/epoch - 18ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 18s - loss: 9.2917 - mse: 9.2917 - mae: 1.2709 - val_loss: 5.5139 - val_mse: 5.5139 - val_mae: 1.3190 - lr: 5.4122e-04 - 18s/epoch - 18ms/step\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-10-11 23:50:55,825]\u001b[0m Finished trial#22 resulted in value: 11.102. Current best value is 9.279999999999998 with parameters: {'activation': 'relu', 'num_hidden_layer': 4, 'i': 10, 'learning_rate': 0.00027563299450396555}.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Score for fold 5: loss of 5.51386833190918\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 9s - loss: 13.7499 - mse: 13.7499 - mae: 1.5446 - val_loss: 9.9719 - val_mse: 9.9719 - val_mae: 1.3740 - lr: 2.6908e-04 - 9s/epoch - 9ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 9s - loss: 12.8882 - mse: 12.8882 - mae: 1.4850 - val_loss: 10.0860 - val_mse: 10.0860 - val_mae: 1.4003 - lr: 2.6908e-04 - 9s/epoch - 9ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 9s - loss: 12.6962 - mse: 12.6962 - mae: 1.4742 - val_loss: 9.7512 - val_mse: 9.7512 - val_mae: 1.4708 - lr: 2.6908e-04 - 9s/epoch - 9ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 9s - loss: 12.5141 - mse: 12.5141 - mae: 1.4665 - val_loss: 9.8771 - val_mse: 9.8771 - val_mae: 1.4460 - lr: 2.6908e-04 - 9s/epoch - 9ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 8s - loss: 12.4977 - mse: 12.4977 - mae: 1.4596 - val_loss: 9.8355 - val_mse: 9.8355 - val_mae: 1.4429 - lr: 2.6908e-04 - 8s/epoch - 8ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 7s - loss: 12.2526 - mse: 12.2526 - mae: 1.4489 - val_loss: 9.9042 - val_mse: 9.9042 - val_mae: 1.4843 - lr: 2.6908e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 7s - loss: 12.2111 - mse: 12.2111 - mae: 1.4384 - val_loss: 9.6546 - val_mse: 9.6546 - val_mae: 1.4519 - lr: 2.6908e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 10s - loss: 12.0983 - mse: 12.0983 - mae: 1.4373 - val_loss: 9.6465 - val_mse: 9.6465 - val_mae: 1.4281 - lr: 2.6908e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 9s - loss: 12.0242 - mse: 12.0242 - mae: 1.4246 - val_loss: 10.2174 - val_mse: 10.2174 - val_mae: 1.4685 - lr: 2.6908e-04 - 9s/epoch - 9ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 9s - loss: 12.0248 - mse: 12.0248 - mae: 1.4102 - val_loss: 9.5312 - val_mse: 9.5312 - val_mae: 1.4247 - lr: 2.6908e-04 - 9s/epoch - 9ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 9s - loss: 11.4471 - mse: 11.4471 - mae: 1.4009 - val_loss: 9.6480 - val_mse: 9.6480 - val_mae: 1.4456 - lr: 2.6908e-04 - 9s/epoch - 9ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 9s - loss: 11.2497 - mse: 11.2497 - mae: 1.3887 - val_loss: 9.5917 - val_mse: 9.5917 - val_mae: 1.4929 - lr: 2.6908e-04 - 9s/epoch - 9ms/step\n",
            "Epoch 13/100\n",
            "1000/1000 - 9s - loss: 11.3219 - mse: 11.3219 - mae: 1.3782 - val_loss: 9.5706 - val_mse: 9.5706 - val_mae: 1.4355 - lr: 2.6908e-04 - 9s/epoch - 9ms/step\n",
            "Epoch 14/100\n",
            "1000/1000 - 9s - loss: 10.9161 - mse: 10.9161 - mae: 1.3721 - val_loss: 11.9578 - val_mse: 11.9578 - val_mae: 1.4832 - lr: 2.6908e-04 - 9s/epoch - 9ms/step\n",
            "Epoch 15/100\n",
            "1000/1000 - 9s - loss: 10.6190 - mse: 10.6190 - mae: 1.3476 - val_loss: 9.7073 - val_mse: 9.7073 - val_mae: 1.4368 - lr: 2.6908e-04 - 9s/epoch - 9ms/step\n",
            "Score for fold 1: loss of 9.707305908203125\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 10s - loss: 10.0275 - mse: 10.0275 - mae: 1.3798 - val_loss: 11.4005 - val_mse: 11.4005 - val_mae: 1.4223 - lr: 2.6908e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 11s - loss: 9.9381 - mse: 9.9381 - mae: 1.3639 - val_loss: 11.3272 - val_mse: 11.3272 - val_mae: 1.3771 - lr: 2.6908e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 11s - loss: 9.4660 - mse: 9.4660 - mae: 1.3456 - val_loss: 11.5660 - val_mse: 11.5660 - val_mae: 1.3998 - lr: 2.6908e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 11s - loss: 9.3810 - mse: 9.3810 - mae: 1.3303 - val_loss: 11.3176 - val_mse: 11.3176 - val_mae: 1.4539 - lr: 2.6908e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 11s - loss: 9.2042 - mse: 9.2042 - mae: 1.3201 - val_loss: 11.5628 - val_mse: 11.5628 - val_mae: 1.3557 - lr: 2.6908e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 11s - loss: 8.8116 - mse: 8.8116 - mae: 1.2987 - val_loss: 12.3249 - val_mse: 12.3249 - val_mae: 1.3493 - lr: 2.6908e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 11s - loss: 8.5317 - mse: 8.5317 - mae: 1.2757 - val_loss: 12.1965 - val_mse: 12.1965 - val_mae: 1.4353 - lr: 2.6908e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 11s - loss: 8.4956 - mse: 8.4956 - mae: 1.2661 - val_loss: 12.3207 - val_mse: 12.3207 - val_mae: 1.4711 - lr: 2.6908e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 12s - loss: 8.3786 - mse: 8.3786 - mae: 1.2507 - val_loss: 12.3795 - val_mse: 12.3795 - val_mae: 1.3504 - lr: 2.6908e-04 - 12s/epoch - 12ms/step\n",
            "Score for fold 2: loss of 12.379484176635742\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 11s - loss: 10.0001 - mse: 10.0001 - mae: 1.3115 - val_loss: 5.5675 - val_mse: 5.5675 - val_mae: 1.2429 - lr: 2.6908e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 11s - loss: 9.3736 - mse: 9.3736 - mae: 1.2850 - val_loss: 6.0272 - val_mse: 6.0272 - val_mae: 1.2062 - lr: 2.6908e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 12s - loss: 9.1630 - mse: 9.1630 - mae: 1.2641 - val_loss: 5.9303 - val_mse: 5.9303 - val_mae: 1.3119 - lr: 2.6908e-04 - 12s/epoch - 12ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 12s - loss: 8.9256 - mse: 8.9256 - mae: 1.2520 - val_loss: 5.7881 - val_mse: 5.7881 - val_mae: 1.2634 - lr: 2.6908e-04 - 12s/epoch - 12ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 12s - loss: 8.4428 - mse: 8.4428 - mae: 1.2353 - val_loss: 6.3874 - val_mse: 6.3874 - val_mae: 1.2210 - lr: 2.6908e-04 - 12s/epoch - 12ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 11s - loss: 8.3953 - mse: 8.3953 - mae: 1.2138 - val_loss: 6.0978 - val_mse: 6.0978 - val_mae: 1.3463 - lr: 2.6908e-04 - 11s/epoch - 11ms/step\n",
            "Score for fold 3: loss of 6.097800254821777\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 11s - loss: 6.7019 - mse: 6.7019 - mae: 1.2371 - val_loss: 12.7535 - val_mse: 12.7535 - val_mae: 1.1690 - lr: 2.6908e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 12s - loss: 6.2942 - mse: 6.2942 - mae: 1.2202 - val_loss: 12.8445 - val_mse: 12.8445 - val_mae: 1.2964 - lr: 2.6908e-04 - 12s/epoch - 12ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 12s - loss: 6.1668 - mse: 6.1668 - mae: 1.1936 - val_loss: 13.4682 - val_mse: 13.4682 - val_mae: 1.1522 - lr: 2.6908e-04 - 12s/epoch - 12ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 12s - loss: 5.9209 - mse: 5.9209 - mae: 1.1831 - val_loss: 12.6288 - val_mse: 12.6288 - val_mae: 1.2543 - lr: 2.6908e-04 - 12s/epoch - 12ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 12s - loss: 5.6605 - mse: 5.6605 - mae: 1.1682 - val_loss: 13.3643 - val_mse: 13.3643 - val_mae: 1.2236 - lr: 2.6908e-04 - 12s/epoch - 12ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 12s - loss: 5.3340 - mse: 5.3340 - mae: 1.1448 - val_loss: 14.3247 - val_mse: 14.3247 - val_mae: 1.3147 - lr: 2.6908e-04 - 12s/epoch - 12ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 12s - loss: 5.3957 - mse: 5.3957 - mae: 1.1375 - val_loss: 13.8269 - val_mse: 13.8269 - val_mae: 1.2789 - lr: 2.6908e-04 - 12s/epoch - 12ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 12s - loss: 5.0882 - mse: 5.0882 - mae: 1.1262 - val_loss: 13.6562 - val_mse: 13.6562 - val_mae: 1.2349 - lr: 2.6908e-04 - 12s/epoch - 12ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 12s - loss: 4.9748 - mse: 4.9748 - mae: 1.1136 - val_loss: 14.3827 - val_mse: 14.3827 - val_mae: 1.2749 - lr: 2.6908e-04 - 12s/epoch - 12ms/step\n",
            "Score for fold 4: loss of 14.382668495178223\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 12s - loss: 7.6415 - mse: 7.6415 - mae: 1.1641 - val_loss: 3.2886 - val_mse: 3.2886 - val_mae: 1.0922 - lr: 2.6908e-04 - 12s/epoch - 12ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 12s - loss: 7.2365 - mse: 7.2365 - mae: 1.1398 - val_loss: 3.9693 - val_mse: 3.9693 - val_mae: 1.0642 - lr: 2.6908e-04 - 12s/epoch - 12ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 11s - loss: 6.7504 - mse: 6.7504 - mae: 1.1269 - val_loss: 3.8859 - val_mse: 3.8859 - val_mae: 1.1542 - lr: 2.6908e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 11s - loss: 6.6667 - mse: 6.6667 - mae: 1.1067 - val_loss: 4.0283 - val_mse: 4.0283 - val_mae: 1.2877 - lr: 2.6908e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 10s - loss: 6.4232 - mse: 6.4232 - mae: 1.0994 - val_loss: 3.6840 - val_mse: 3.6840 - val_mae: 1.0580 - lr: 2.6908e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 10s - loss: 6.0965 - mse: 6.0965 - mae: 1.0853 - val_loss: 4.2286 - val_mse: 4.2286 - val_mae: 1.1436 - lr: 2.6908e-04 - 10s/epoch - 10ms/step\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-10-11 23:58:57,084]\u001b[0m Finished trial#23 resulted in value: 9.360000000000003. Current best value is 9.279999999999998 with parameters: {'activation': 'relu', 'num_hidden_layer': 4, 'i': 10, 'learning_rate': 0.00027563299450396555}.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Score for fold 5: loss of 4.22859001159668\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 10s - loss: 13.9262 - mse: 13.9262 - mae: 1.5549 - val_loss: 10.2322 - val_mse: 10.2322 - val_mae: 1.4584 - lr: 1.5117e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 9s - loss: 12.8794 - mse: 12.8794 - mae: 1.4919 - val_loss: 10.4115 - val_mse: 10.4115 - val_mae: 1.4567 - lr: 1.5117e-04 - 9s/epoch - 9ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 9s - loss: 12.5371 - mse: 12.5371 - mae: 1.4723 - val_loss: 10.4178 - val_mse: 10.4178 - val_mae: 1.5007 - lr: 1.5117e-04 - 9s/epoch - 9ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 9s - loss: 12.4496 - mse: 12.4496 - mae: 1.4642 - val_loss: 10.3646 - val_mse: 10.3646 - val_mae: 1.4901 - lr: 1.5117e-04 - 9s/epoch - 9ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 9s - loss: 12.3634 - mse: 12.3634 - mae: 1.4577 - val_loss: 10.3591 - val_mse: 10.3591 - val_mae: 1.4028 - lr: 1.5117e-04 - 9s/epoch - 9ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 9s - loss: 12.3726 - mse: 12.3726 - mae: 1.4466 - val_loss: 10.5019 - val_mse: 10.5019 - val_mae: 1.4374 - lr: 1.5117e-04 - 9s/epoch - 9ms/step\n",
            "Score for fold 1: loss of 10.501873016357422\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 9s - loss: 12.6603 - mse: 12.6603 - mae: 1.4513 - val_loss: 8.2973 - val_mse: 8.2973 - val_mae: 1.4213 - lr: 1.5117e-04 - 9s/epoch - 9ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 8s - loss: 12.4375 - mse: 12.4375 - mae: 1.4410 - val_loss: 8.5550 - val_mse: 8.5550 - val_mae: 1.4682 - lr: 1.5117e-04 - 8s/epoch - 8ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 8s - loss: 12.3924 - mse: 12.3924 - mae: 1.4280 - val_loss: 8.5336 - val_mse: 8.5336 - val_mae: 1.4166 - lr: 1.5117e-04 - 8s/epoch - 8ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 8s - loss: 12.2319 - mse: 12.2319 - mae: 1.4198 - val_loss: 8.4359 - val_mse: 8.4359 - val_mae: 1.4059 - lr: 1.5117e-04 - 8s/epoch - 8ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 8s - loss: 11.9930 - mse: 11.9930 - mae: 1.4122 - val_loss: 8.7219 - val_mse: 8.7219 - val_mae: 1.4239 - lr: 1.5117e-04 - 8s/epoch - 8ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 8s - loss: 11.8660 - mse: 11.8660 - mae: 1.4041 - val_loss: 8.6146 - val_mse: 8.6146 - val_mae: 1.4047 - lr: 1.5117e-04 - 8s/epoch - 8ms/step\n",
            "Score for fold 2: loss of 8.614638328552246\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 7s - loss: 11.8416 - mse: 11.8416 - mae: 1.4171 - val_loss: 7.8798 - val_mse: 7.8798 - val_mae: 1.4319 - lr: 1.5117e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 7s - loss: 11.8488 - mse: 11.8488 - mae: 1.4077 - val_loss: 7.9733 - val_mse: 7.9733 - val_mae: 1.3910 - lr: 1.5117e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 7s - loss: 11.6553 - mse: 11.6553 - mae: 1.3954 - val_loss: 7.8766 - val_mse: 7.8766 - val_mae: 1.3577 - lr: 1.5117e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 8s - loss: 11.3567 - mse: 11.3567 - mae: 1.3813 - val_loss: 7.8835 - val_mse: 7.8835 - val_mae: 1.4447 - lr: 1.5117e-04 - 8s/epoch - 8ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 8s - loss: 11.2453 - mse: 11.2453 - mae: 1.3691 - val_loss: 8.0699 - val_mse: 8.0699 - val_mae: 1.3816 - lr: 1.5117e-04 - 8s/epoch - 8ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 7s - loss: 11.1429 - mse: 11.1429 - mae: 1.3620 - val_loss: 7.8385 - val_mse: 7.8385 - val_mae: 1.4541 - lr: 1.5117e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 7s - loss: 10.7895 - mse: 10.7895 - mae: 1.3486 - val_loss: 8.6033 - val_mse: 8.6033 - val_mae: 1.4499 - lr: 1.5117e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 7s - loss: 10.5525 - mse: 10.5525 - mae: 1.3306 - val_loss: 8.0768 - val_mse: 8.0768 - val_mae: 1.4632 - lr: 1.5117e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 8s - loss: 10.3269 - mse: 10.3269 - mae: 1.3191 - val_loss: 8.2022 - val_mse: 8.2022 - val_mae: 1.4234 - lr: 1.5117e-04 - 8s/epoch - 8ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 7s - loss: 10.4636 - mse: 10.4636 - mae: 1.3107 - val_loss: 8.3170 - val_mse: 8.3170 - val_mae: 1.5004 - lr: 1.5117e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 6s - loss: 10.3574 - mse: 10.3574 - mae: 1.2951 - val_loss: 8.3769 - val_mse: 8.3769 - val_mae: 1.3877 - lr: 1.5117e-04 - 6s/epoch - 6ms/step\n",
            "Score for fold 3: loss of 8.37692928314209\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 6s - loss: 9.5734 - mse: 9.5734 - mae: 1.3189 - val_loss: 9.6862 - val_mse: 9.6862 - val_mae: 1.3103 - lr: 1.5117e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 6s - loss: 9.2820 - mse: 9.2820 - mae: 1.3059 - val_loss: 10.0753 - val_mse: 10.0753 - val_mae: 1.2813 - lr: 1.5117e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 6s - loss: 9.0378 - mse: 9.0378 - mae: 1.2834 - val_loss: 10.0259 - val_mse: 10.0259 - val_mae: 1.3373 - lr: 1.5117e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 6s - loss: 8.8875 - mse: 8.8875 - mae: 1.2760 - val_loss: 10.3505 - val_mse: 10.3505 - val_mae: 1.3380 - lr: 1.5117e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 6s - loss: 8.6528 - mse: 8.6528 - mae: 1.2578 - val_loss: 10.2514 - val_mse: 10.2514 - val_mae: 1.3498 - lr: 1.5117e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 6s - loss: 8.3505 - mse: 8.3505 - mae: 1.2380 - val_loss: 10.7040 - val_mse: 10.7040 - val_mae: 1.4010 - lr: 1.5117e-04 - 6s/epoch - 6ms/step\n",
            "Score for fold 4: loss of 10.703971862792969\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 6s - loss: 7.0805 - mse: 7.0805 - mae: 1.2689 - val_loss: 14.1281 - val_mse: 14.1281 - val_mae: 1.2145 - lr: 1.5117e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 6s - loss: 7.2953 - mse: 7.2953 - mae: 1.2513 - val_loss: 14.6761 - val_mse: 14.6761 - val_mae: 1.2523 - lr: 1.5117e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 6s - loss: 6.6513 - mse: 6.6513 - mae: 1.2256 - val_loss: 14.5242 - val_mse: 14.5242 - val_mae: 1.2403 - lr: 1.5117e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 6s - loss: 6.8676 - mse: 6.8676 - mae: 1.2188 - val_loss: 14.5969 - val_mse: 14.5969 - val_mae: 1.2750 - lr: 1.5117e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 6s - loss: 6.2484 - mse: 6.2484 - mae: 1.2004 - val_loss: 14.9496 - val_mse: 14.9496 - val_mae: 1.2527 - lr: 1.5117e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 6s - loss: 6.1165 - mse: 6.1165 - mae: 1.1844 - val_loss: 15.5533 - val_mse: 15.5533 - val_mae: 1.2658 - lr: 1.5117e-04 - 6s/epoch - 6ms/step\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-10-12 00:03:21,816]\u001b[0m Finished trial#24 resulted in value: 10.748. Current best value is 9.279999999999998 with parameters: {'activation': 'relu', 'num_hidden_layer': 4, 'i': 10, 'learning_rate': 0.00027563299450396555}.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Score for fold 5: loss of 15.553332328796387\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 7s - loss: 14.0886 - mse: 14.0886 - mae: 1.5581 - val_loss: 9.7980 - val_mse: 9.7980 - val_mae: 1.6335 - lr: 6.2954e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 6s - loss: 13.1733 - mse: 13.1733 - mae: 1.5150 - val_loss: 9.3406 - val_mse: 9.3406 - val_mae: 1.4231 - lr: 6.2954e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 6s - loss: 13.1147 - mse: 13.1147 - mae: 1.4943 - val_loss: 9.1672 - val_mse: 9.1672 - val_mae: 1.4083 - lr: 6.2954e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 6s - loss: 12.9275 - mse: 12.9275 - mae: 1.4860 - val_loss: 9.4839 - val_mse: 9.4839 - val_mae: 1.4165 - lr: 6.2954e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 6s - loss: 12.8152 - mse: 12.8152 - mae: 1.4716 - val_loss: 9.5607 - val_mse: 9.5607 - val_mae: 1.4753 - lr: 6.2954e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 6s - loss: 12.6059 - mse: 12.6059 - mae: 1.4645 - val_loss: 9.2715 - val_mse: 9.2715 - val_mae: 1.4599 - lr: 6.2954e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 6s - loss: 12.8789 - mse: 12.8789 - mae: 1.4604 - val_loss: 8.8913 - val_mse: 8.8913 - val_mae: 1.4580 - lr: 6.2954e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 6s - loss: 12.5210 - mse: 12.5210 - mae: 1.4513 - val_loss: 9.1026 - val_mse: 9.1026 - val_mae: 1.4245 - lr: 6.2954e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 6s - loss: 12.3652 - mse: 12.3652 - mae: 1.4440 - val_loss: 8.8802 - val_mse: 8.8802 - val_mae: 1.4687 - lr: 6.2954e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 6s - loss: 11.9851 - mse: 11.9851 - mae: 1.4315 - val_loss: 9.5286 - val_mse: 9.5286 - val_mae: 1.4286 - lr: 6.2954e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 6s - loss: 11.8660 - mse: 11.8660 - mae: 1.4252 - val_loss: 9.3841 - val_mse: 9.3841 - val_mae: 1.4892 - lr: 6.2954e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 6s - loss: 11.7326 - mse: 11.7326 - mae: 1.4148 - val_loss: 9.0984 - val_mse: 9.0984 - val_mae: 1.4974 - lr: 6.2954e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 13/100\n",
            "1000/1000 - 6s - loss: 11.6297 - mse: 11.6297 - mae: 1.4041 - val_loss: 9.2985 - val_mse: 9.2985 - val_mae: 1.5439 - lr: 6.2954e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 14/100\n",
            "1000/1000 - 6s - loss: 11.3188 - mse: 11.3188 - mae: 1.3938 - val_loss: 9.5753 - val_mse: 9.5753 - val_mae: 1.4771 - lr: 6.2954e-04 - 6s/epoch - 6ms/step\n",
            "Score for fold 1: loss of 9.57529354095459\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 6s - loss: 11.3041 - mse: 11.3041 - mae: 1.4002 - val_loss: 9.7564 - val_mse: 9.7564 - val_mae: 1.4422 - lr: 6.2954e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 6s - loss: 11.0770 - mse: 11.0770 - mae: 1.3873 - val_loss: 9.7728 - val_mse: 9.7728 - val_mae: 1.4034 - lr: 6.2954e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 6s - loss: 10.7245 - mse: 10.7245 - mae: 1.3773 - val_loss: 9.6410 - val_mse: 9.6410 - val_mae: 1.4700 - lr: 6.2954e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 6s - loss: 10.3576 - mse: 10.3576 - mae: 1.3601 - val_loss: 10.2071 - val_mse: 10.2071 - val_mae: 1.4195 - lr: 6.2954e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 6s - loss: 10.3836 - mse: 10.3836 - mae: 1.3524 - val_loss: 12.0598 - val_mse: 12.0598 - val_mae: 1.4377 - lr: 6.2954e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 6s - loss: 10.2537 - mse: 10.2537 - mae: 1.3439 - val_loss: 12.4347 - val_mse: 12.4347 - val_mae: 1.4353 - lr: 6.2954e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 6s - loss: 10.1420 - mse: 10.1420 - mae: 1.3393 - val_loss: 12.4461 - val_mse: 12.4461 - val_mae: 1.4310 - lr: 6.2954e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 6s - loss: 9.8863 - mse: 9.8863 - mae: 1.3329 - val_loss: 10.6179 - val_mse: 10.6179 - val_mae: 1.4810 - lr: 6.2954e-04 - 6s/epoch - 6ms/step\n",
            "Score for fold 2: loss of 10.617870330810547\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 6s - loss: 9.8619 - mse: 9.8619 - mae: 1.3489 - val_loss: 11.0030 - val_mse: 11.0030 - val_mae: 1.3599 - lr: 6.2954e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 6s - loss: 9.5529 - mse: 9.5529 - mae: 1.3436 - val_loss: 11.4177 - val_mse: 11.4177 - val_mae: 1.3599 - lr: 6.2954e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 6s - loss: 9.0978 - mse: 9.0978 - mae: 1.3326 - val_loss: 11.4665 - val_mse: 11.4665 - val_mae: 1.5144 - lr: 6.2954e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 6s - loss: 9.0084 - mse: 9.0084 - mae: 1.3220 - val_loss: 11.5000 - val_mse: 11.5000 - val_mae: 1.3620 - lr: 6.2954e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 6s - loss: 8.7865 - mse: 8.7865 - mae: 1.3161 - val_loss: 11.9342 - val_mse: 11.9342 - val_mae: 1.3806 - lr: 6.2954e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 6s - loss: 8.8427 - mse: 8.8427 - mae: 1.2995 - val_loss: 11.8446 - val_mse: 11.8446 - val_mae: 1.3898 - lr: 6.2954e-04 - 6s/epoch - 6ms/step\n",
            "Score for fold 3: loss of 11.844596862792969\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 6s - loss: 7.9603 - mse: 7.9603 - mae: 1.3255 - val_loss: 14.5100 - val_mse: 14.5100 - val_mae: 1.3310 - lr: 6.2954e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 6s - loss: 7.5805 - mse: 7.5805 - mae: 1.3162 - val_loss: 15.8698 - val_mse: 15.8698 - val_mae: 1.2626 - lr: 6.2954e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 6s - loss: 7.4781 - mse: 7.4781 - mae: 1.3007 - val_loss: 14.4203 - val_mse: 14.4203 - val_mae: 1.2632 - lr: 6.2954e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 6s - loss: 7.5791 - mse: 7.5791 - mae: 1.2949 - val_loss: 15.1635 - val_mse: 15.1635 - val_mae: 1.2926 - lr: 6.2954e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 6s - loss: 7.1849 - mse: 7.1849 - mae: 1.2873 - val_loss: 15.1498 - val_mse: 15.1498 - val_mae: 1.4304 - lr: 6.2954e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 6s - loss: 6.9731 - mse: 6.9731 - mae: 1.2746 - val_loss: 15.0825 - val_mse: 15.0825 - val_mae: 1.3104 - lr: 6.2954e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 6s - loss: 7.0302 - mse: 7.0302 - mae: 1.2645 - val_loss: 15.1097 - val_mse: 15.1097 - val_mae: 1.3224 - lr: 6.2954e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 6s - loss: 6.7678 - mse: 6.7678 - mae: 1.2541 - val_loss: 15.2745 - val_mse: 15.2745 - val_mae: 1.4017 - lr: 6.2954e-04 - 6s/epoch - 6ms/step\n",
            "Score for fold 4: loss of 15.274528503417969\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 6s - loss: 9.5079 - mse: 9.5079 - mae: 1.2984 - val_loss: 4.7776 - val_mse: 4.7776 - val_mae: 1.2293 - lr: 6.2954e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 6s - loss: 9.2179 - mse: 9.2179 - mae: 1.2837 - val_loss: 4.3331 - val_mse: 4.3331 - val_mae: 1.1970 - lr: 6.2954e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 6s - loss: 9.0821 - mse: 9.0821 - mae: 1.2641 - val_loss: 4.1967 - val_mse: 4.1967 - val_mae: 1.1853 - lr: 6.2954e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 6s - loss: 8.9575 - mse: 8.9575 - mae: 1.2583 - val_loss: 4.5815 - val_mse: 4.5815 - val_mae: 1.2535 - lr: 6.2954e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 6s - loss: 8.5059 - mse: 8.5059 - mae: 1.2452 - val_loss: 4.9223 - val_mse: 4.9223 - val_mae: 1.2855 - lr: 6.2954e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 6s - loss: 8.4348 - mse: 8.4348 - mae: 1.2407 - val_loss: 5.0629 - val_mse: 5.0629 - val_mae: 1.1827 - lr: 6.2954e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 6s - loss: 8.1715 - mse: 8.1715 - mae: 1.2239 - val_loss: 5.0153 - val_mse: 5.0153 - val_mae: 1.2067 - lr: 6.2954e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 6s - loss: 8.0662 - mse: 8.0662 - mae: 1.2152 - val_loss: 4.8670 - val_mse: 4.8670 - val_mae: 1.2261 - lr: 6.2954e-04 - 6s/epoch - 6ms/step\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-10-12 00:07:54,290]\u001b[0m Finished trial#25 resulted in value: 10.436. Current best value is 9.279999999999998 with parameters: {'activation': 'relu', 'num_hidden_layer': 4, 'i': 10, 'learning_rate': 0.00027563299450396555}.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Score for fold 5: loss of 4.8669915199279785\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 7s - loss: 13.6361 - mse: 13.6361 - mae: 1.5596 - val_loss: 12.7812 - val_mse: 12.7812 - val_mae: 1.5197 - lr: 0.0015 - 7s/epoch - 7ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 6s - loss: 12.5782 - mse: 12.5782 - mae: 1.5174 - val_loss: 12.6141 - val_mse: 12.6141 - val_mae: 1.5159 - lr: 0.0015 - 6s/epoch - 6ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 6s - loss: 12.3880 - mse: 12.3880 - mae: 1.5027 - val_loss: 12.6189 - val_mse: 12.6189 - val_mae: 1.4789 - lr: 0.0015 - 6s/epoch - 6ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 6s - loss: 12.3839 - mse: 12.3839 - mae: 1.4948 - val_loss: 12.9179 - val_mse: 12.9179 - val_mae: 1.5770 - lr: 0.0015 - 6s/epoch - 6ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 6s - loss: 12.1460 - mse: 12.1460 - mae: 1.4725 - val_loss: 12.4610 - val_mse: 12.4610 - val_mae: 1.4811 - lr: 0.0015 - 6s/epoch - 6ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 6s - loss: 12.1752 - mse: 12.1752 - mae: 1.4761 - val_loss: 12.1842 - val_mse: 12.1842 - val_mae: 1.4526 - lr: 0.0015 - 6s/epoch - 6ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 6s - loss: 12.1243 - mse: 12.1243 - mae: 1.4617 - val_loss: 12.3731 - val_mse: 12.3731 - val_mae: 1.5715 - lr: 0.0015 - 6s/epoch - 6ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 6s - loss: 12.3336 - mse: 12.3336 - mae: 1.4592 - val_loss: 12.2452 - val_mse: 12.2452 - val_mae: 1.4811 - lr: 0.0015 - 6s/epoch - 6ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 6s - loss: 11.8965 - mse: 11.8965 - mae: 1.4475 - val_loss: 12.4510 - val_mse: 12.4510 - val_mae: 1.4289 - lr: 0.0015 - 6s/epoch - 6ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 6s - loss: 11.5401 - mse: 11.5401 - mae: 1.4427 - val_loss: 12.2068 - val_mse: 12.2068 - val_mae: 1.4516 - lr: 0.0015 - 6s/epoch - 6ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 6s - loss: 11.6919 - mse: 11.6919 - mae: 1.4429 - val_loss: 12.1944 - val_mse: 12.1944 - val_mae: 1.4573 - lr: 0.0015 - 6s/epoch - 6ms/step\n",
            "Score for fold 1: loss of 12.194378852844238\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 6s - loss: 10.0864 - mse: 10.0864 - mae: 1.4297 - val_loss: 16.3337 - val_mse: 16.3337 - val_mae: 1.4317 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 6s - loss: 9.8973 - mse: 9.8973 - mae: 1.4172 - val_loss: 16.1522 - val_mse: 16.1522 - val_mae: 1.4786 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 6s - loss: 9.7051 - mse: 9.7051 - mae: 1.4093 - val_loss: 16.8690 - val_mse: 16.8690 - val_mae: 1.4847 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 6s - loss: 9.5455 - mse: 9.5455 - mae: 1.4059 - val_loss: 16.5045 - val_mse: 16.5045 - val_mae: 1.4338 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 6s - loss: 9.5406 - mse: 9.5406 - mae: 1.4004 - val_loss: 16.5405 - val_mse: 16.5405 - val_mae: 1.4198 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 6s - loss: 9.6932 - mse: 9.6932 - mae: 1.3933 - val_loss: 16.6670 - val_mse: 16.6670 - val_mae: 1.4701 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 6s - loss: 9.2891 - mse: 9.2891 - mae: 1.3898 - val_loss: 16.6686 - val_mse: 16.6686 - val_mae: 1.4366 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
            "Score for fold 2: loss of 16.66857147216797\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 6s - loss: 10.7960 - mse: 10.7960 - mae: 1.4013 - val_loss: 11.5752 - val_mse: 11.5752 - val_mae: 1.4274 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 6s - loss: 10.7027 - mse: 10.7027 - mae: 1.3987 - val_loss: 10.5222 - val_mse: 10.5222 - val_mae: 1.4143 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 6s - loss: 10.5118 - mse: 10.5118 - mae: 1.3884 - val_loss: 11.3915 - val_mse: 11.3915 - val_mae: 1.4052 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 6s - loss: 10.5297 - mse: 10.5297 - mae: 1.3798 - val_loss: 11.3667 - val_mse: 11.3667 - val_mae: 1.4216 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 6s - loss: 10.2216 - mse: 10.2216 - mae: 1.3724 - val_loss: 10.7172 - val_mse: 10.7172 - val_mae: 1.4170 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 6s - loss: 10.3161 - mse: 10.3161 - mae: 1.3682 - val_loss: 11.0948 - val_mse: 11.0948 - val_mae: 1.4170 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 6s - loss: 10.0825 - mse: 10.0825 - mae: 1.3569 - val_loss: 11.0728 - val_mse: 11.0728 - val_mae: 1.4240 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
            "Score for fold 3: loss of 11.072829246520996\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 6s - loss: 11.1743 - mse: 11.1743 - mae: 1.3814 - val_loss: 8.3208 - val_mse: 8.3208 - val_mae: 1.3242 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 6s - loss: 11.0321 - mse: 11.0321 - mae: 1.3719 - val_loss: 8.7413 - val_mse: 8.7413 - val_mae: 1.3541 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 6s - loss: 10.3416 - mse: 10.3416 - mae: 1.3580 - val_loss: 8.7991 - val_mse: 8.7991 - val_mae: 1.3866 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 6s - loss: 10.6556 - mse: 10.6556 - mae: 1.3536 - val_loss: 9.0078 - val_mse: 9.0078 - val_mae: 1.3323 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 6s - loss: 10.2782 - mse: 10.2782 - mae: 1.3442 - val_loss: 9.1743 - val_mse: 9.1743 - val_mae: 1.3786 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 6s - loss: 9.9683 - mse: 9.9683 - mae: 1.3364 - val_loss: 8.7098 - val_mse: 8.7098 - val_mae: 1.3780 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
            "Score for fold 4: loss of 8.70980453491211\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 6s - loss: 10.7736 - mse: 10.7736 - mae: 1.3613 - val_loss: 5.7935 - val_mse: 5.7935 - val_mae: 1.2869 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 6s - loss: 10.4750 - mse: 10.4750 - mae: 1.3475 - val_loss: 5.9720 - val_mse: 5.9720 - val_mae: 1.2976 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 6s - loss: 10.4465 - mse: 10.4465 - mae: 1.3354 - val_loss: 6.1243 - val_mse: 6.1243 - val_mae: 1.3398 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 6s - loss: 10.4423 - mse: 10.4423 - mae: 1.3276 - val_loss: 5.9798 - val_mse: 5.9798 - val_mae: 1.3020 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 6s - loss: 10.1338 - mse: 10.1338 - mae: 1.3201 - val_loss: 6.0460 - val_mse: 6.0460 - val_mae: 1.2966 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 6s - loss: 10.0553 - mse: 10.0553 - mae: 1.3103 - val_loss: 6.0248 - val_mse: 6.0248 - val_mae: 1.3110 - lr: 0.0010 - 6s/epoch - 6ms/step\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-10-12 00:11:43,553]\u001b[0m Finished trial#26 resulted in value: 10.931999999999999. Current best value is 9.279999999999998 with parameters: {'activation': 'relu', 'num_hidden_layer': 4, 'i': 10, 'learning_rate': 0.00027563299450396555}.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Score for fold 5: loss of 6.024753570556641\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 12s - loss: 11.7373 - mse: 11.7373 - mae: 1.5562 - val_loss: 18.5083 - val_mse: 18.5083 - val_mae: 1.4562 - lr: 4.1226e-04 - 12s/epoch - 12ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 12s - loss: 10.8937 - mse: 10.8937 - mae: 1.4933 - val_loss: 18.8778 - val_mse: 18.8778 - val_mae: 1.4594 - lr: 4.1226e-04 - 12s/epoch - 12ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 12s - loss: 10.7914 - mse: 10.7914 - mae: 1.4863 - val_loss: 18.3324 - val_mse: 18.3324 - val_mae: 1.5040 - lr: 4.1226e-04 - 12s/epoch - 12ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 12s - loss: 10.7204 - mse: 10.7204 - mae: 1.4746 - val_loss: 18.6262 - val_mse: 18.6262 - val_mae: 1.4588 - lr: 4.1226e-04 - 12s/epoch - 12ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 12s - loss: 10.4898 - mse: 10.4898 - mae: 1.4648 - val_loss: 18.7472 - val_mse: 18.7472 - val_mae: 1.4579 - lr: 4.1226e-04 - 12s/epoch - 12ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 12s - loss: 10.4232 - mse: 10.4232 - mae: 1.4532 - val_loss: 19.1108 - val_mse: 19.1108 - val_mae: 1.5186 - lr: 4.1226e-04 - 12s/epoch - 12ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 12s - loss: 10.2004 - mse: 10.2004 - mae: 1.4469 - val_loss: 18.8305 - val_mse: 18.8305 - val_mae: 1.4271 - lr: 4.1226e-04 - 12s/epoch - 12ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 12s - loss: 9.8339 - mse: 9.8339 - mae: 1.4370 - val_loss: 18.4534 - val_mse: 18.4534 - val_mae: 1.4958 - lr: 4.1226e-04 - 12s/epoch - 12ms/step\n",
            "Score for fold 1: loss of 18.453433990478516\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 12s - loss: 11.7045 - mse: 11.7045 - mae: 1.4407 - val_loss: 10.3765 - val_mse: 10.3765 - val_mae: 1.4771 - lr: 4.1226e-04 - 12s/epoch - 12ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 12s - loss: 11.4015 - mse: 11.4015 - mae: 1.4264 - val_loss: 10.5806 - val_mse: 10.5806 - val_mae: 1.4452 - lr: 4.1226e-04 - 12s/epoch - 12ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 12s - loss: 11.4727 - mse: 11.4727 - mae: 1.4162 - val_loss: 10.7165 - val_mse: 10.7165 - val_mae: 1.4240 - lr: 4.1226e-04 - 12s/epoch - 12ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 12s - loss: 10.9746 - mse: 10.9746 - mae: 1.4042 - val_loss: 10.8362 - val_mse: 10.8362 - val_mae: 1.4617 - lr: 4.1226e-04 - 12s/epoch - 12ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 12s - loss: 11.1225 - mse: 11.1225 - mae: 1.3950 - val_loss: 12.8986 - val_mse: 12.8986 - val_mae: 1.4364 - lr: 4.1226e-04 - 12s/epoch - 12ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 12s - loss: 10.5376 - mse: 10.5376 - mae: 1.3809 - val_loss: 11.2233 - val_mse: 11.2233 - val_mae: 1.4823 - lr: 4.1226e-04 - 12s/epoch - 12ms/step\n",
            "Score for fold 2: loss of 11.223261833190918\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 12s - loss: 10.3748 - mse: 10.3748 - mae: 1.3930 - val_loss: 12.0893 - val_mse: 12.0893 - val_mae: 1.4152 - lr: 4.1226e-04 - 12s/epoch - 12ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 12s - loss: 10.0577 - mse: 10.0577 - mae: 1.3783 - val_loss: 12.0572 - val_mse: 12.0572 - val_mae: 1.3925 - lr: 4.1226e-04 - 12s/epoch - 12ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 12s - loss: 9.8083 - mse: 9.8083 - mae: 1.3597 - val_loss: 12.1304 - val_mse: 12.1304 - val_mae: 1.4347 - lr: 4.1226e-04 - 12s/epoch - 12ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 12s - loss: 9.5829 - mse: 9.5829 - mae: 1.3479 - val_loss: 12.0658 - val_mse: 12.0658 - val_mae: 1.3730 - lr: 4.1226e-04 - 12s/epoch - 12ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 13s - loss: 9.3165 - mse: 9.3165 - mae: 1.3333 - val_loss: 12.4694 - val_mse: 12.4694 - val_mae: 1.4336 - lr: 4.1226e-04 - 13s/epoch - 13ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 13s - loss: 9.4796 - mse: 9.4796 - mae: 1.3241 - val_loss: 11.9660 - val_mse: 11.9660 - val_mae: 1.4725 - lr: 4.1226e-04 - 13s/epoch - 13ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 12s - loss: 10.0986 - mse: 10.0986 - mae: 1.3142 - val_loss: 12.3171 - val_mse: 12.3171 - val_mae: 1.4492 - lr: 4.1226e-04 - 12s/epoch - 12ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 13s - loss: 8.8458 - mse: 8.8458 - mae: 1.3008 - val_loss: 12.5515 - val_mse: 12.5515 - val_mae: 1.4519 - lr: 4.1226e-04 - 13s/epoch - 13ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 13s - loss: 8.9137 - mse: 8.9137 - mae: 1.2922 - val_loss: 12.7266 - val_mse: 12.7266 - val_mae: 1.4094 - lr: 4.1226e-04 - 13s/epoch - 13ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 13s - loss: 8.6299 - mse: 8.6299 - mae: 1.2797 - val_loss: 12.9421 - val_mse: 12.9421 - val_mae: 1.4346 - lr: 4.1226e-04 - 13s/epoch - 13ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 13s - loss: 8.5063 - mse: 8.5063 - mae: 1.2657 - val_loss: 12.7365 - val_mse: 12.7365 - val_mae: 1.4530 - lr: 4.1226e-04 - 13s/epoch - 13ms/step\n",
            "Score for fold 3: loss of 12.736513137817383\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 13s - loss: 10.4526 - mse: 10.4526 - mae: 1.3295 - val_loss: 4.9977 - val_mse: 4.9977 - val_mae: 1.2931 - lr: 4.1226e-04 - 13s/epoch - 13ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 13s - loss: 10.2413 - mse: 10.2413 - mae: 1.3102 - val_loss: 5.0640 - val_mse: 5.0640 - val_mae: 1.2872 - lr: 4.1226e-04 - 13s/epoch - 13ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 13s - loss: 9.8478 - mse: 9.8478 - mae: 1.2931 - val_loss: 4.9454 - val_mse: 4.9454 - val_mae: 1.2611 - lr: 4.1226e-04 - 13s/epoch - 13ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 13s - loss: 9.5989 - mse: 9.5989 - mae: 1.2837 - val_loss: 5.0215 - val_mse: 5.0215 - val_mae: 1.2844 - lr: 4.1226e-04 - 13s/epoch - 13ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 13s - loss: 9.5752 - mse: 9.5752 - mae: 1.2708 - val_loss: 5.2488 - val_mse: 5.2488 - val_mae: 1.2872 - lr: 4.1226e-04 - 13s/epoch - 13ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 13s - loss: 8.9746 - mse: 8.9746 - mae: 1.2622 - val_loss: 5.4571 - val_mse: 5.4571 - val_mae: 1.3200 - lr: 4.1226e-04 - 13s/epoch - 13ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 13s - loss: 8.9383 - mse: 8.9383 - mae: 1.2441 - val_loss: 5.5042 - val_mse: 5.5042 - val_mae: 1.3021 - lr: 4.1226e-04 - 13s/epoch - 13ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 12s - loss: 8.7195 - mse: 8.7195 - mae: 1.2319 - val_loss: 5.6501 - val_mse: 5.6501 - val_mae: 1.4040 - lr: 4.1226e-04 - 12s/epoch - 12ms/step\n",
            "Score for fold 4: loss of 5.650110721588135\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 12s - loss: 8.8984 - mse: 8.8984 - mae: 1.2721 - val_loss: 5.2814 - val_mse: 5.2814 - val_mae: 1.1637 - lr: 4.1226e-04 - 12s/epoch - 12ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 12s - loss: 8.4795 - mse: 8.4795 - mae: 1.2508 - val_loss: 6.1753 - val_mse: 6.1753 - val_mae: 1.3374 - lr: 4.1226e-04 - 12s/epoch - 12ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 12s - loss: 8.1458 - mse: 8.1458 - mae: 1.2337 - val_loss: 5.7620 - val_mse: 5.7620 - val_mae: 1.2030 - lr: 4.1226e-04 - 12s/epoch - 12ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 12s - loss: 8.0065 - mse: 8.0065 - mae: 1.2199 - val_loss: 5.6444 - val_mse: 5.6444 - val_mae: 1.2282 - lr: 4.1226e-04 - 12s/epoch - 12ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 12s - loss: 7.9583 - mse: 7.9583 - mae: 1.2107 - val_loss: 5.4973 - val_mse: 5.4973 - val_mae: 1.1749 - lr: 4.1226e-04 - 12s/epoch - 12ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 12s - loss: 7.6684 - mse: 7.6684 - mae: 1.2004 - val_loss: 5.9629 - val_mse: 5.9629 - val_mae: 1.2520 - lr: 4.1226e-04 - 12s/epoch - 12ms/step\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-10-12 00:19:56,883]\u001b[0m Finished trial#27 resulted in value: 10.804. Current best value is 9.279999999999998 with parameters: {'activation': 'relu', 'num_hidden_layer': 4, 'i': 10, 'learning_rate': 0.00027563299450396555}.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Score for fold 5: loss of 5.962930202484131\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 7s - loss: 13.9747 - mse: 13.9747 - mae: 1.5506 - val_loss: 9.4390 - val_mse: 9.4390 - val_mae: 1.3886 - lr: 2.2387e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 6s - loss: 13.2664 - mse: 13.2664 - mae: 1.4931 - val_loss: 9.5596 - val_mse: 9.5596 - val_mae: 1.5374 - lr: 2.2387e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 6s - loss: 13.0672 - mse: 13.0672 - mae: 1.4772 - val_loss: 8.4278 - val_mse: 8.4278 - val_mae: 1.3965 - lr: 2.2387e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 6s - loss: 12.7404 - mse: 12.7404 - mae: 1.4661 - val_loss: 9.0365 - val_mse: 9.0365 - val_mae: 1.4181 - lr: 2.2387e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 6s - loss: 12.8548 - mse: 12.8548 - mae: 1.4609 - val_loss: 8.6348 - val_mse: 8.6348 - val_mae: 1.4718 - lr: 2.2387e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 6s - loss: 12.7339 - mse: 12.7339 - mae: 1.4467 - val_loss: 8.4387 - val_mse: 8.4387 - val_mae: 1.4623 - lr: 2.2387e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 6s - loss: 12.4700 - mse: 12.4700 - mae: 1.4372 - val_loss: 9.1978 - val_mse: 9.1978 - val_mae: 1.4709 - lr: 2.2387e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 7s - loss: 12.3049 - mse: 12.3049 - mae: 1.4301 - val_loss: 8.7709 - val_mse: 8.7709 - val_mae: 1.5846 - lr: 2.2387e-04 - 7s/epoch - 7ms/step\n",
            "Score for fold 1: loss of 8.770920753479004\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 6s - loss: 11.5904 - mse: 11.5904 - mae: 1.4325 - val_loss: 11.7211 - val_mse: 11.7211 - val_mae: 1.4050 - lr: 2.2387e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 6s - loss: 11.5220 - mse: 11.5220 - mae: 1.4241 - val_loss: 12.0095 - val_mse: 12.0095 - val_mae: 1.4460 - lr: 2.2387e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 6s - loss: 11.0312 - mse: 11.0312 - mae: 1.4078 - val_loss: 11.6129 - val_mse: 11.6129 - val_mae: 1.3950 - lr: 2.2387e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 6s - loss: 10.7515 - mse: 10.7515 - mae: 1.3970 - val_loss: 11.6503 - val_mse: 11.6503 - val_mae: 1.4067 - lr: 2.2387e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 6s - loss: 10.5796 - mse: 10.5796 - mae: 1.3840 - val_loss: 11.5592 - val_mse: 11.5592 - val_mae: 1.3728 - lr: 2.2387e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 6s - loss: 10.3460 - mse: 10.3460 - mae: 1.3722 - val_loss: 12.1985 - val_mse: 12.1985 - val_mae: 1.4562 - lr: 2.2387e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 6s - loss: 10.0713 - mse: 10.0713 - mae: 1.3595 - val_loss: 11.8305 - val_mse: 11.8305 - val_mae: 1.4696 - lr: 2.2387e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 6s - loss: 9.8835 - mse: 9.8835 - mae: 1.3433 - val_loss: 11.7896 - val_mse: 11.7896 - val_mae: 1.4545 - lr: 2.2387e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 6s - loss: 9.5851 - mse: 9.5851 - mae: 1.3289 - val_loss: 12.2478 - val_mse: 12.2478 - val_mae: 1.3951 - lr: 2.2387e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 6s - loss: 9.4909 - mse: 9.4909 - mae: 1.3131 - val_loss: 12.1418 - val_mse: 12.1418 - val_mae: 1.4356 - lr: 2.2387e-04 - 6s/epoch - 6ms/step\n",
            "Score for fold 2: loss of 12.141777038574219\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 6s - loss: 10.5420 - mse: 10.5420 - mae: 1.3441 - val_loss: 8.9667 - val_mse: 8.9667 - val_mae: 1.3218 - lr: 2.2387e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 6s - loss: 10.1909 - mse: 10.1909 - mae: 1.3275 - val_loss: 8.9563 - val_mse: 8.9563 - val_mae: 1.3939 - lr: 2.2387e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 6s - loss: 9.9400 - mse: 9.9400 - mae: 1.3118 - val_loss: 8.9501 - val_mse: 8.9501 - val_mae: 1.3647 - lr: 2.2387e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 6s - loss: 9.4412 - mse: 9.4412 - mae: 1.2994 - val_loss: 8.9339 - val_mse: 8.9339 - val_mae: 1.3466 - lr: 2.2387e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 6s - loss: 9.4103 - mse: 9.4103 - mae: 1.2783 - val_loss: 8.9575 - val_mse: 8.9575 - val_mae: 1.3813 - lr: 2.2387e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 6s - loss: 9.0437 - mse: 9.0437 - mae: 1.2623 - val_loss: 9.4348 - val_mse: 9.4348 - val_mae: 1.3891 - lr: 2.2387e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 6s - loss: 8.6897 - mse: 8.6897 - mae: 1.2466 - val_loss: 9.1131 - val_mse: 9.1131 - val_mae: 1.4288 - lr: 2.2387e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 6s - loss: 8.5349 - mse: 8.5349 - mae: 1.2368 - val_loss: 9.0726 - val_mse: 9.0726 - val_mae: 1.4065 - lr: 2.2387e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 6s - loss: 8.5336 - mse: 8.5336 - mae: 1.2238 - val_loss: 10.0005 - val_mse: 10.0005 - val_mae: 1.4890 - lr: 2.2387e-04 - 6s/epoch - 6ms/step\n",
            "Score for fold 3: loss of 10.000506401062012\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 6s - loss: 7.4417 - mse: 7.4417 - mae: 1.2665 - val_loss: 13.4416 - val_mse: 13.4416 - val_mae: 1.2614 - lr: 2.2387e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 6s - loss: 7.3184 - mse: 7.3184 - mae: 1.2390 - val_loss: 13.6684 - val_mse: 13.6684 - val_mae: 1.2372 - lr: 2.2387e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 6s - loss: 6.8393 - mse: 6.8393 - mae: 1.2251 - val_loss: 13.3201 - val_mse: 13.3201 - val_mae: 1.4363 - lr: 2.2387e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 6s - loss: 6.5594 - mse: 6.5594 - mae: 1.2043 - val_loss: 14.2154 - val_mse: 14.2154 - val_mae: 1.2655 - lr: 2.2387e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 6s - loss: 6.4101 - mse: 6.4101 - mae: 1.1949 - val_loss: 14.0367 - val_mse: 14.0367 - val_mae: 1.2598 - lr: 2.2387e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 6s - loss: 6.0701 - mse: 6.0701 - mae: 1.1778 - val_loss: 14.6287 - val_mse: 14.6287 - val_mae: 1.2713 - lr: 2.2387e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 6s - loss: 5.9171 - mse: 5.9171 - mae: 1.1651 - val_loss: 13.9355 - val_mse: 13.9355 - val_mae: 1.3239 - lr: 2.2387e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 6s - loss: 5.5458 - mse: 5.5458 - mae: 1.1436 - val_loss: 13.9716 - val_mse: 13.9716 - val_mae: 1.3171 - lr: 2.2387e-04 - 6s/epoch - 6ms/step\n",
            "Score for fold 4: loss of 13.971555709838867\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 6s - loss: 7.8548 - mse: 7.8548 - mae: 1.2021 - val_loss: 5.0916 - val_mse: 5.0916 - val_mae: 1.1179 - lr: 2.2387e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 6s - loss: 7.6287 - mse: 7.6287 - mae: 1.1822 - val_loss: 4.7695 - val_mse: 4.7695 - val_mae: 1.1068 - lr: 2.2387e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 6s - loss: 7.3591 - mse: 7.3591 - mae: 1.1657 - val_loss: 5.2636 - val_mse: 5.2636 - val_mae: 1.2407 - lr: 2.2387e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 6s - loss: 7.0396 - mse: 7.0396 - mae: 1.1486 - val_loss: 5.3200 - val_mse: 5.3200 - val_mae: 1.1860 - lr: 2.2387e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 6s - loss: 6.9440 - mse: 6.9440 - mae: 1.1393 - val_loss: 5.4560 - val_mse: 5.4560 - val_mae: 1.1250 - lr: 2.2387e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 6s - loss: 6.8299 - mse: 6.8299 - mae: 1.1226 - val_loss: 5.3369 - val_mse: 5.3369 - val_mae: 1.1505 - lr: 2.2387e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 6s - loss: 6.3728 - mse: 6.3728 - mae: 1.1111 - val_loss: 5.6446 - val_mse: 5.6446 - val_mae: 1.1690 - lr: 2.2387e-04 - 6s/epoch - 6ms/step\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-10-12 00:24:30,440]\u001b[0m Finished trial#28 resulted in value: 10.104000000000001. Current best value is 9.279999999999998 with parameters: {'activation': 'relu', 'num_hidden_layer': 4, 'i': 10, 'learning_rate': 0.00027563299450396555}.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Score for fold 5: loss of 5.644578456878662\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 4s - loss: 15.8054 - mse: 15.8054 - mae: 1.6785 - val_loss: 10.0875 - val_mse: 10.0875 - val_mae: 1.5742 - lr: 1.0485e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 4s - loss: 14.4265 - mse: 14.4265 - mae: 1.5663 - val_loss: 9.5471 - val_mse: 9.5471 - val_mae: 1.5665 - lr: 1.0485e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 4s - loss: 14.2722 - mse: 14.2722 - mae: 1.5560 - val_loss: 9.3421 - val_mse: 9.3421 - val_mae: 1.5269 - lr: 1.0485e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 4s - loss: 14.3782 - mse: 14.3782 - mae: 1.5627 - val_loss: 9.3584 - val_mse: 9.3584 - val_mae: 1.5125 - lr: 1.0485e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 4s - loss: 14.3535 - mse: 14.3535 - mae: 1.5558 - val_loss: 9.4756 - val_mse: 9.4756 - val_mae: 1.5471 - lr: 1.0485e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 4s - loss: 14.2658 - mse: 14.2658 - mae: 1.5587 - val_loss: 9.3315 - val_mse: 9.3315 - val_mae: 1.5061 - lr: 1.0485e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 4s - loss: 14.2565 - mse: 14.2565 - mae: 1.5519 - val_loss: 9.2850 - val_mse: 9.2850 - val_mae: 1.5255 - lr: 1.0485e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 4s - loss: 14.2603 - mse: 14.2603 - mae: 1.5606 - val_loss: 9.3583 - val_mse: 9.3583 - val_mae: 1.5234 - lr: 1.0485e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 4s - loss: 14.3323 - mse: 14.3323 - mae: 1.5579 - val_loss: 9.3090 - val_mse: 9.3090 - val_mae: 1.5398 - lr: 1.0485e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 4s - loss: 14.2758 - mse: 14.2758 - mae: 1.5580 - val_loss: 9.4097 - val_mse: 9.4097 - val_mae: 1.5877 - lr: 1.0485e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 4s - loss: 14.2718 - mse: 14.2718 - mae: 1.5591 - val_loss: 9.2949 - val_mse: 9.2949 - val_mae: 1.4806 - lr: 1.0485e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 4s - loss: 14.2046 - mse: 14.2046 - mae: 1.5579 - val_loss: 9.3864 - val_mse: 9.3864 - val_mae: 1.5115 - lr: 1.0485e-04 - 4s/epoch - 4ms/step\n",
            "Score for fold 1: loss of 9.38642692565918\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 4s - loss: 13.9340 - mse: 13.9340 - mae: 1.5455 - val_loss: 10.5525 - val_mse: 10.5525 - val_mae: 1.5950 - lr: 1.0485e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 4s - loss: 13.8773 - mse: 13.8773 - mae: 1.5499 - val_loss: 10.7549 - val_mse: 10.7549 - val_mae: 1.5095 - lr: 1.0485e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 4s - loss: 13.9505 - mse: 13.9505 - mae: 1.5468 - val_loss: 10.6648 - val_mse: 10.6648 - val_mae: 1.5791 - lr: 1.0485e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 4s - loss: 13.8902 - mse: 13.8902 - mae: 1.5490 - val_loss: 10.6145 - val_mse: 10.6145 - val_mae: 1.5294 - lr: 1.0485e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 4s - loss: 13.9115 - mse: 13.9115 - mae: 1.5458 - val_loss: 10.6765 - val_mse: 10.6765 - val_mae: 1.5152 - lr: 1.0485e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 4s - loss: 13.9046 - mse: 13.9046 - mae: 1.5484 - val_loss: 10.6659 - val_mse: 10.6659 - val_mae: 1.5200 - lr: 1.0485e-04 - 4s/epoch - 4ms/step\n",
            "Score for fold 2: loss of 10.665872573852539\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 4s - loss: 12.2214 - mse: 12.2214 - mae: 1.5512 - val_loss: 17.5092 - val_mse: 17.5092 - val_mae: 1.5422 - lr: 1.0485e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 4s - loss: 12.1782 - mse: 12.1782 - mae: 1.5521 - val_loss: 17.5011 - val_mse: 17.5011 - val_mae: 1.5434 - lr: 1.0485e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 4s - loss: 12.1562 - mse: 12.1562 - mae: 1.5488 - val_loss: 17.6495 - val_mse: 17.6495 - val_mae: 1.5185 - lr: 1.0485e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 4s - loss: 12.1080 - mse: 12.1080 - mae: 1.5520 - val_loss: 17.5612 - val_mse: 17.5612 - val_mae: 1.5194 - lr: 1.0485e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 4s - loss: 12.1240 - mse: 12.1240 - mae: 1.5496 - val_loss: 17.5200 - val_mse: 17.5200 - val_mae: 1.5567 - lr: 1.0485e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 4s - loss: 12.1368 - mse: 12.1368 - mae: 1.5522 - val_loss: 17.5761 - val_mse: 17.5761 - val_mae: 1.5465 - lr: 1.0485e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 4s - loss: 12.1464 - mse: 12.1464 - mae: 1.5492 - val_loss: 17.5349 - val_mse: 17.5349 - val_mae: 1.5305 - lr: 1.0485e-04 - 4s/epoch - 4ms/step\n",
            "Score for fold 3: loss of 17.53492546081543\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 4s - loss: 12.1702 - mse: 12.1702 - mae: 1.5294 - val_loss: 17.5964 - val_mse: 17.5964 - val_mae: 1.5918 - lr: 1.0485e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 4s - loss: 12.1644 - mse: 12.1644 - mae: 1.5300 - val_loss: 17.5672 - val_mse: 17.5672 - val_mae: 1.5980 - lr: 1.0485e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 4s - loss: 12.1236 - mse: 12.1236 - mae: 1.5261 - val_loss: 17.6100 - val_mse: 17.6100 - val_mae: 1.5867 - lr: 1.0485e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 4s - loss: 12.1623 - mse: 12.1623 - mae: 1.5300 - val_loss: 17.5466 - val_mse: 17.5466 - val_mae: 1.5836 - lr: 1.0485e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 4s - loss: 12.1482 - mse: 12.1482 - mae: 1.5244 - val_loss: 17.5712 - val_mse: 17.5712 - val_mae: 1.6195 - lr: 1.0485e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 4s - loss: 12.1628 - mse: 12.1628 - mae: 1.5269 - val_loss: 17.6768 - val_mse: 17.6768 - val_mae: 1.6614 - lr: 1.0485e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 4s - loss: 12.1812 - mse: 12.1812 - mae: 1.5308 - val_loss: 17.5139 - val_mse: 17.5139 - val_mae: 1.6256 - lr: 1.0485e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 4s - loss: 12.0977 - mse: 12.0977 - mae: 1.5281 - val_loss: 17.4801 - val_mse: 17.4801 - val_mae: 1.6104 - lr: 1.0485e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 4s - loss: 12.1409 - mse: 12.1409 - mae: 1.5322 - val_loss: 17.5856 - val_mse: 17.5856 - val_mae: 1.5913 - lr: 1.0485e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 4s - loss: 12.0838 - mse: 12.0838 - mae: 1.5281 - val_loss: 17.6218 - val_mse: 17.6218 - val_mae: 1.6162 - lr: 1.0485e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 4s - loss: 12.0616 - mse: 12.0616 - mae: 1.5290 - val_loss: 17.5629 - val_mse: 17.5629 - val_mae: 1.5638 - lr: 1.0485e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 4s - loss: 12.1569 - mse: 12.1569 - mae: 1.5278 - val_loss: 17.5685 - val_mse: 17.5685 - val_mae: 1.5833 - lr: 1.0485e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 13/100\n",
            "1000/1000 - 4s - loss: 12.1452 - mse: 12.1452 - mae: 1.5260 - val_loss: 17.5531 - val_mse: 17.5531 - val_mae: 1.6268 - lr: 1.0485e-04 - 4s/epoch - 4ms/step\n",
            "Score for fold 4: loss of 17.553096771240234\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 4s - loss: 13.7337 - mse: 13.7337 - mae: 1.5479 - val_loss: 11.1491 - val_mse: 11.1491 - val_mae: 1.5764 - lr: 1.0485e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 4s - loss: 13.7643 - mse: 13.7643 - mae: 1.5465 - val_loss: 11.1110 - val_mse: 11.1110 - val_mae: 1.5614 - lr: 1.0485e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 4s - loss: 13.7686 - mse: 13.7686 - mae: 1.5456 - val_loss: 11.1657 - val_mse: 11.1657 - val_mae: 1.5425 - lr: 1.0485e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 4s - loss: 13.8023 - mse: 13.8023 - mae: 1.5466 - val_loss: 11.2227 - val_mse: 11.2227 - val_mae: 1.5460 - lr: 1.0485e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 4s - loss: 13.7547 - mse: 13.7547 - mae: 1.5469 - val_loss: 11.2065 - val_mse: 11.2065 - val_mae: 1.5204 - lr: 1.0485e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 4s - loss: 13.7742 - mse: 13.7742 - mae: 1.5440 - val_loss: 11.0804 - val_mse: 11.0804 - val_mae: 1.5300 - lr: 1.0485e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 4s - loss: 13.7796 - mse: 13.7796 - mae: 1.5457 - val_loss: 11.1202 - val_mse: 11.1202 - val_mae: 1.5358 - lr: 1.0485e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 4s - loss: 13.8127 - mse: 13.8127 - mae: 1.5477 - val_loss: 11.2275 - val_mse: 11.2275 - val_mae: 1.5854 - lr: 1.0485e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 4s - loss: 13.7698 - mse: 13.7698 - mae: 1.5482 - val_loss: 11.1000 - val_mse: 11.1000 - val_mae: 1.5450 - lr: 1.0485e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 4s - loss: 13.7657 - mse: 13.7657 - mae: 1.5433 - val_loss: 11.0454 - val_mse: 11.0454 - val_mae: 1.5419 - lr: 1.0485e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 4s - loss: 13.7827 - mse: 13.7827 - mae: 1.5449 - val_loss: 11.0826 - val_mse: 11.0826 - val_mae: 1.5304 - lr: 1.0485e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 4s - loss: 13.7799 - mse: 13.7799 - mae: 1.5465 - val_loss: 11.0335 - val_mse: 11.0335 - val_mae: 1.5258 - lr: 1.0485e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 13/100\n",
            "1000/1000 - 4s - loss: 13.7853 - mse: 13.7853 - mae: 1.5438 - val_loss: 11.0479 - val_mse: 11.0479 - val_mae: 1.5551 - lr: 1.0485e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 14/100\n",
            "1000/1000 - 3s - loss: 13.7872 - mse: 13.7872 - mae: 1.5457 - val_loss: 11.0503 - val_mse: 11.0503 - val_mae: 1.5332 - lr: 1.0485e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 15/100\n",
            "1000/1000 - 3s - loss: 13.7804 - mse: 13.7804 - mae: 1.5453 - val_loss: 11.0550 - val_mse: 11.0550 - val_mae: 1.5233 - lr: 1.0485e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 16/100\n",
            "1000/1000 - 3s - loss: 13.7435 - mse: 13.7435 - mae: 1.5437 - val_loss: 11.0896 - val_mse: 11.0896 - val_mae: 1.5318 - lr: 1.0485e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 17/100\n",
            "1000/1000 - 3s - loss: 13.7742 - mse: 13.7742 - mae: 1.5483 - val_loss: 11.1310 - val_mse: 11.1310 - val_mae: 1.5439 - lr: 1.0485e-04 - 3s/epoch - 3ms/step\n",
            "Score for fold 5: loss of 11.13105297088623\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-10-12 00:27:52,269]\u001b[0m Finished trial#29 resulted in value: 13.254. Current best value is 9.279999999999998 with parameters: {'activation': 'relu', 'num_hidden_layer': 4, 'i': 10, 'learning_rate': 0.00027563299450396555}.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 3s - loss: 15.1344 - mse: 15.1344 - mae: 1.6040 - val_loss: 11.1277 - val_mse: 11.1277 - val_mae: 1.5232 - lr: 1.5141e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 13.5317 - mse: 13.5317 - mae: 1.5200 - val_loss: 10.7840 - val_mse: 10.7840 - val_mae: 1.5162 - lr: 1.5141e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 13.1113 - mse: 13.1113 - mae: 1.5017 - val_loss: 10.5624 - val_mse: 10.5624 - val_mae: 1.4592 - lr: 1.5141e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 12.9215 - mse: 12.9215 - mae: 1.4915 - val_loss: 10.2884 - val_mse: 10.2884 - val_mae: 1.5249 - lr: 1.5141e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 12.7032 - mse: 12.7032 - mae: 1.4751 - val_loss: 10.3604 - val_mse: 10.3604 - val_mae: 1.4361 - lr: 1.5141e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 12.6670 - mse: 12.6670 - mae: 1.4693 - val_loss: 10.1678 - val_mse: 10.1678 - val_mae: 1.4772 - lr: 1.5141e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 12.5844 - mse: 12.5844 - mae: 1.4614 - val_loss: 10.0955 - val_mse: 10.0955 - val_mae: 1.4422 - lr: 1.5141e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 2s - loss: 12.4642 - mse: 12.4642 - mae: 1.4562 - val_loss: 10.1754 - val_mse: 10.1754 - val_mae: 1.4835 - lr: 1.5141e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 2s - loss: 12.4339 - mse: 12.4339 - mae: 1.4547 - val_loss: 10.0294 - val_mse: 10.0294 - val_mae: 1.4835 - lr: 1.5141e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 2s - loss: 12.3883 - mse: 12.3883 - mae: 1.4484 - val_loss: 10.0332 - val_mse: 10.0332 - val_mae: 1.4881 - lr: 1.5141e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 2s - loss: 12.3517 - mse: 12.3517 - mae: 1.4484 - val_loss: 10.1241 - val_mse: 10.1241 - val_mae: 1.4273 - lr: 1.5141e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 2s - loss: 12.3002 - mse: 12.3002 - mae: 1.4436 - val_loss: 9.9603 - val_mse: 9.9603 - val_mae: 1.4558 - lr: 1.5141e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 13/100\n",
            "1000/1000 - 2s - loss: 12.2535 - mse: 12.2535 - mae: 1.4392 - val_loss: 10.0074 - val_mse: 10.0074 - val_mae: 1.4577 - lr: 1.5141e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 14/100\n",
            "1000/1000 - 2s - loss: 12.1777 - mse: 12.1777 - mae: 1.4365 - val_loss: 9.9524 - val_mse: 9.9524 - val_mae: 1.4644 - lr: 1.5141e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 15/100\n",
            "1000/1000 - 2s - loss: 12.1383 - mse: 12.1383 - mae: 1.4303 - val_loss: 10.0211 - val_mse: 10.0211 - val_mae: 1.4551 - lr: 1.5141e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 16/100\n",
            "1000/1000 - 2s - loss: 12.1143 - mse: 12.1143 - mae: 1.4293 - val_loss: 10.0394 - val_mse: 10.0394 - val_mae: 1.4377 - lr: 1.5141e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 17/100\n",
            "1000/1000 - 2s - loss: 12.0919 - mse: 12.0919 - mae: 1.4307 - val_loss: 10.0155 - val_mse: 10.0155 - val_mae: 1.4659 - lr: 1.5141e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 18/100\n",
            "1000/1000 - 2s - loss: 12.0763 - mse: 12.0763 - mae: 1.4275 - val_loss: 9.9980 - val_mse: 9.9980 - val_mae: 1.4780 - lr: 1.5141e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 19/100\n",
            "1000/1000 - 2s - loss: 12.0195 - mse: 12.0195 - mae: 1.4218 - val_loss: 10.1237 - val_mse: 10.1237 - val_mae: 1.6086 - lr: 1.5141e-04 - 2s/epoch - 2ms/step\n",
            "Score for fold 1: loss of 10.123701095581055\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 12.1805 - mse: 12.1805 - mae: 1.4267 - val_loss: 8.9789 - val_mse: 8.9789 - val_mae: 1.3857 - lr: 1.5141e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 12.1313 - mse: 12.1313 - mae: 1.4273 - val_loss: 8.9816 - val_mse: 8.9816 - val_mae: 1.4617 - lr: 1.5141e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 12.0239 - mse: 12.0239 - mae: 1.4187 - val_loss: 8.9951 - val_mse: 8.9951 - val_mae: 1.3943 - lr: 1.5141e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 11.9534 - mse: 11.9534 - mae: 1.4117 - val_loss: 8.9876 - val_mse: 8.9876 - val_mae: 1.4397 - lr: 1.5141e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 11.9435 - mse: 11.9435 - mae: 1.4118 - val_loss: 9.1116 - val_mse: 9.1116 - val_mae: 1.4278 - lr: 1.5141e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 11.8607 - mse: 11.8607 - mae: 1.4122 - val_loss: 9.2058 - val_mse: 9.2058 - val_mae: 1.4862 - lr: 1.5141e-04 - 2s/epoch - 2ms/step\n",
            "Score for fold 2: loss of 9.205774307250977\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 11.0859 - mse: 11.0859 - mae: 1.4139 - val_loss: 12.1161 - val_mse: 12.1161 - val_mae: 1.4217 - lr: 1.5141e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 11.0995 - mse: 11.0995 - mae: 1.4107 - val_loss: 12.3396 - val_mse: 12.3396 - val_mae: 1.4366 - lr: 1.5141e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 10.9501 - mse: 10.9501 - mae: 1.4071 - val_loss: 12.5941 - val_mse: 12.5941 - val_mae: 1.4066 - lr: 1.5141e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 10.9104 - mse: 10.9104 - mae: 1.4045 - val_loss: 12.4002 - val_mse: 12.4002 - val_mae: 1.4054 - lr: 1.5141e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 11.0330 - mse: 11.0330 - mae: 1.4068 - val_loss: 12.3870 - val_mse: 12.3870 - val_mae: 1.4128 - lr: 1.5141e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 10.8738 - mse: 10.8738 - mae: 1.4018 - val_loss: 12.5499 - val_mse: 12.5499 - val_mae: 1.4257 - lr: 1.5141e-04 - 2s/epoch - 2ms/step\n",
            "Score for fold 3: loss of 12.549901962280273\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 9.6387 - mse: 9.6387 - mae: 1.3994 - val_loss: 17.8355 - val_mse: 17.8355 - val_mae: 1.4311 - lr: 1.5141e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 9.4519 - mse: 9.4519 - mae: 1.3932 - val_loss: 17.7713 - val_mse: 17.7713 - val_mae: 1.4199 - lr: 1.5141e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 9.3597 - mse: 9.3597 - mae: 1.3892 - val_loss: 18.1760 - val_mse: 18.1760 - val_mae: 1.4484 - lr: 1.5141e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 9.3063 - mse: 9.3063 - mae: 1.3861 - val_loss: 17.7188 - val_mse: 17.7188 - val_mae: 1.4213 - lr: 1.5141e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 9.2064 - mse: 9.2064 - mae: 1.3843 - val_loss: 17.9466 - val_mse: 17.9466 - val_mae: 1.4196 - lr: 1.5141e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 9.1706 - mse: 9.1706 - mae: 1.3857 - val_loss: 18.0058 - val_mse: 18.0058 - val_mae: 1.4553 - lr: 1.5141e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 9.1493 - mse: 9.1493 - mae: 1.3806 - val_loss: 18.2160 - val_mse: 18.2160 - val_mae: 1.4759 - lr: 1.5141e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 2s - loss: 9.0738 - mse: 9.0738 - mae: 1.3737 - val_loss: 17.7410 - val_mse: 17.7410 - val_mae: 1.4903 - lr: 1.5141e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 2s - loss: 9.0632 - mse: 9.0632 - mae: 1.3754 - val_loss: 18.3813 - val_mse: 18.3813 - val_mae: 1.4232 - lr: 1.5141e-04 - 2s/epoch - 2ms/step\n",
            "Score for fold 4: loss of 18.381322860717773\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 11.7159 - mse: 11.7159 - mae: 1.4064 - val_loss: 6.9728 - val_mse: 6.9728 - val_mae: 1.3414 - lr: 1.5141e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 11.7291 - mse: 11.7291 - mae: 1.4031 - val_loss: 7.2088 - val_mse: 7.2088 - val_mae: 1.3415 - lr: 1.5141e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 11.6301 - mse: 11.6301 - mae: 1.3989 - val_loss: 7.1465 - val_mse: 7.1465 - val_mae: 1.3062 - lr: 1.5141e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 11.4339 - mse: 11.4339 - mae: 1.3936 - val_loss: 7.2052 - val_mse: 7.2052 - val_mae: 1.3779 - lr: 1.5141e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 11.5700 - mse: 11.5700 - mae: 1.3920 - val_loss: 7.2223 - val_mse: 7.2223 - val_mae: 1.3781 - lr: 1.5141e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 11.5245 - mse: 11.5245 - mae: 1.3920 - val_loss: 7.2901 - val_mse: 7.2901 - val_mae: 1.3677 - lr: 1.5141e-04 - 2s/epoch - 2ms/step\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-10-12 00:29:50,897]\u001b[0m Finished trial#30 resulted in value: 11.51. Current best value is 9.279999999999998 with parameters: {'activation': 'relu', 'num_hidden_layer': 4, 'i': 10, 'learning_rate': 0.00027563299450396555}.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Score for fold 5: loss of 7.290090560913086\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 12s - loss: 12.7223 - mse: 12.7223 - mae: 1.5432 - val_loss: 13.7858 - val_mse: 13.7858 - val_mae: 1.4869 - lr: 3.1084e-04 - 12s/epoch - 12ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 11s - loss: 11.9277 - mse: 11.9277 - mae: 1.4908 - val_loss: 14.0784 - val_mse: 14.0784 - val_mae: 1.5018 - lr: 3.1084e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 11s - loss: 11.8157 - mse: 11.8157 - mae: 1.4808 - val_loss: 13.8694 - val_mse: 13.8694 - val_mae: 1.5111 - lr: 3.1084e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 11s - loss: 11.5802 - mse: 11.5802 - mae: 1.4640 - val_loss: 14.3408 - val_mse: 14.3408 - val_mae: 1.4833 - lr: 3.1084e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 11s - loss: 11.4807 - mse: 11.4807 - mae: 1.4620 - val_loss: 14.4752 - val_mse: 14.4752 - val_mae: 1.5498 - lr: 3.1084e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 11s - loss: 11.3316 - mse: 11.3316 - mae: 1.4468 - val_loss: 14.1292 - val_mse: 14.1292 - val_mae: 1.4833 - lr: 3.1084e-04 - 11s/epoch - 11ms/step\n",
            "Score for fold 1: loss of 14.129181861877441\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 11s - loss: 12.8836 - mse: 12.8836 - mae: 1.4556 - val_loss: 7.3471 - val_mse: 7.3471 - val_mae: 1.4023 - lr: 3.1084e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 11s - loss: 12.6680 - mse: 12.6680 - mae: 1.4424 - val_loss: 7.4511 - val_mse: 7.4511 - val_mae: 1.4527 - lr: 3.1084e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 11s - loss: 12.2022 - mse: 12.2022 - mae: 1.4316 - val_loss: 7.4190 - val_mse: 7.4190 - val_mae: 1.4397 - lr: 3.1084e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 11s - loss: 12.2556 - mse: 12.2556 - mae: 1.4243 - val_loss: 7.5473 - val_mse: 7.5473 - val_mae: 1.4084 - lr: 3.1084e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 11s - loss: 12.0330 - mse: 12.0330 - mae: 1.4125 - val_loss: 8.0700 - val_mse: 8.0700 - val_mae: 1.4071 - lr: 3.1084e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 11s - loss: 11.6159 - mse: 11.6159 - mae: 1.4014 - val_loss: 8.0384 - val_mse: 8.0384 - val_mae: 1.4300 - lr: 3.1084e-04 - 11s/epoch - 11ms/step\n",
            "Score for fold 2: loss of 8.038384437561035\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 11s - loss: 11.4790 - mse: 11.4790 - mae: 1.4085 - val_loss: 7.8279 - val_mse: 7.8279 - val_mae: 1.3740 - lr: 3.1084e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 11s - loss: 11.3367 - mse: 11.3367 - mae: 1.3948 - val_loss: 8.5401 - val_mse: 8.5401 - val_mae: 1.4769 - lr: 3.1084e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 12s - loss: 11.3651 - mse: 11.3651 - mae: 1.3799 - val_loss: 8.5433 - val_mse: 8.5433 - val_mae: 1.4223 - lr: 3.1084e-04 - 12s/epoch - 12ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 12s - loss: 11.0109 - mse: 11.0109 - mae: 1.3703 - val_loss: 8.4815 - val_mse: 8.4815 - val_mae: 1.4348 - lr: 3.1084e-04 - 12s/epoch - 12ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 12s - loss: 10.5531 - mse: 10.5531 - mae: 1.3536 - val_loss: 8.4902 - val_mse: 8.4902 - val_mae: 1.3714 - lr: 3.1084e-04 - 12s/epoch - 12ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 13s - loss: 10.3542 - mse: 10.3542 - mae: 1.3379 - val_loss: 8.1712 - val_mse: 8.1712 - val_mae: 1.3989 - lr: 3.1084e-04 - 13s/epoch - 13ms/step\n",
            "Score for fold 3: loss of 8.171211242675781\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 13s - loss: 10.0505 - mse: 10.0505 - mae: 1.3592 - val_loss: 8.9183 - val_mse: 8.9183 - val_mae: 1.3353 - lr: 3.1084e-04 - 13s/epoch - 13ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 13s - loss: 10.6316 - mse: 10.6316 - mae: 1.3480 - val_loss: 9.3937 - val_mse: 9.3937 - val_mae: 1.3091 - lr: 3.1084e-04 - 13s/epoch - 13ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 12s - loss: 9.6884 - mse: 9.6884 - mae: 1.3295 - val_loss: 9.6079 - val_mse: 9.6079 - val_mae: 1.3876 - lr: 3.1084e-04 - 12s/epoch - 12ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 12s - loss: 9.2369 - mse: 9.2369 - mae: 1.3205 - val_loss: 9.7491 - val_mse: 9.7491 - val_mae: 1.3164 - lr: 3.1084e-04 - 12s/epoch - 12ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 12s - loss: 9.1628 - mse: 9.1628 - mae: 1.3006 - val_loss: 9.6610 - val_mse: 9.6610 - val_mae: 1.3600 - lr: 3.1084e-04 - 12s/epoch - 12ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 12s - loss: 8.7356 - mse: 8.7356 - mae: 1.2781 - val_loss: 9.6986 - val_mse: 9.6986 - val_mae: 1.4546 - lr: 3.1084e-04 - 12s/epoch - 12ms/step\n",
            "Score for fold 4: loss of 9.698580741882324\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 12s - loss: 8.1277 - mse: 8.1277 - mae: 1.3126 - val_loss: 13.2620 - val_mse: 13.2620 - val_mae: 1.2420 - lr: 3.1084e-04 - 12s/epoch - 12ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 12s - loss: 7.5468 - mse: 7.5468 - mae: 1.2891 - val_loss: 13.9623 - val_mse: 13.9623 - val_mae: 1.3669 - lr: 3.1084e-04 - 12s/epoch - 12ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 12s - loss: 7.7950 - mse: 7.7950 - mae: 1.2785 - val_loss: 13.8903 - val_mse: 13.8903 - val_mae: 1.2488 - lr: 3.1084e-04 - 12s/epoch - 12ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 12s - loss: 7.0751 - mse: 7.0751 - mae: 1.2623 - val_loss: 13.8546 - val_mse: 13.8546 - val_mae: 1.3316 - lr: 3.1084e-04 - 12s/epoch - 12ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 12s - loss: 6.8513 - mse: 6.8513 - mae: 1.2348 - val_loss: 14.0439 - val_mse: 14.0439 - val_mae: 1.2789 - lr: 3.1084e-04 - 12s/epoch - 12ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 12s - loss: 6.6392 - mse: 6.6392 - mae: 1.2226 - val_loss: 14.5182 - val_mse: 14.5182 - val_mae: 1.2701 - lr: 3.1084e-04 - 12s/epoch - 12ms/step\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-10-12 00:35:56,168]\u001b[0m Finished trial#31 resulted in value: 10.912. Current best value is 9.279999999999998 with parameters: {'activation': 'relu', 'num_hidden_layer': 4, 'i': 10, 'learning_rate': 0.00027563299450396555}.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Score for fold 5: loss of 14.518179893493652\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 12s - loss: 13.8432 - mse: 13.8432 - mae: 1.5529 - val_loss: 9.5413 - val_mse: 9.5413 - val_mae: 1.4845 - lr: 4.0024e-04 - 12s/epoch - 12ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 11s - loss: 13.1491 - mse: 13.1491 - mae: 1.4947 - val_loss: 9.5829 - val_mse: 9.5829 - val_mae: 1.5204 - lr: 4.0024e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 11s - loss: 13.2301 - mse: 13.2301 - mae: 1.4823 - val_loss: 9.3008 - val_mse: 9.3008 - val_mae: 1.4384 - lr: 4.0024e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 11s - loss: 13.1569 - mse: 13.1569 - mae: 1.4736 - val_loss: 9.0056 - val_mse: 9.0056 - val_mae: 1.5115 - lr: 4.0024e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 11s - loss: 12.7440 - mse: 12.7440 - mae: 1.4600 - val_loss: 8.8424 - val_mse: 8.8424 - val_mae: 1.4812 - lr: 4.0024e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 11s - loss: 12.8069 - mse: 12.8069 - mae: 1.4568 - val_loss: 9.2235 - val_mse: 9.2235 - val_mae: 1.5280 - lr: 4.0024e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 11s - loss: 12.4346 - mse: 12.4346 - mae: 1.4393 - val_loss: 9.8138 - val_mse: 9.8138 - val_mae: 1.6422 - lr: 4.0024e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 11s - loss: 12.6257 - mse: 12.6257 - mae: 1.4449 - val_loss: 8.8709 - val_mse: 8.8709 - val_mae: 1.4495 - lr: 4.0024e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 11s - loss: 12.3868 - mse: 12.3868 - mae: 1.4295 - val_loss: 9.3465 - val_mse: 9.3465 - val_mae: 1.4721 - lr: 4.0024e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 12s - loss: 12.3447 - mse: 12.3447 - mae: 1.4220 - val_loss: 9.7238 - val_mse: 9.7238 - val_mae: 1.5668 - lr: 4.0024e-04 - 12s/epoch - 12ms/step\n",
            "Score for fold 1: loss of 9.723848342895508\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 11s - loss: 9.8605 - mse: 9.8605 - mae: 1.4211 - val_loss: 18.2661 - val_mse: 18.2661 - val_mae: 1.4692 - lr: 4.0024e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 11s - loss: 9.4258 - mse: 9.4258 - mae: 1.4054 - val_loss: 17.9920 - val_mse: 17.9920 - val_mae: 1.4564 - lr: 4.0024e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 11s - loss: 9.0668 - mse: 9.0668 - mae: 1.3920 - val_loss: 18.0894 - val_mse: 18.0894 - val_mae: 1.4585 - lr: 4.0024e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 11s - loss: 8.7239 - mse: 8.7239 - mae: 1.3811 - val_loss: 18.1972 - val_mse: 18.1972 - val_mae: 1.4283 - lr: 4.0024e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 12s - loss: 8.9930 - mse: 8.9930 - mae: 1.3738 - val_loss: 18.1741 - val_mse: 18.1741 - val_mae: 1.3935 - lr: 4.0024e-04 - 12s/epoch - 12ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 12s - loss: 8.5747 - mse: 8.5747 - mae: 1.3602 - val_loss: 18.4735 - val_mse: 18.4735 - val_mae: 1.5525 - lr: 4.0024e-04 - 12s/epoch - 12ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 11s - loss: 8.1923 - mse: 8.1923 - mae: 1.3456 - val_loss: 18.7531 - val_mse: 18.7531 - val_mae: 1.4676 - lr: 4.0024e-04 - 11s/epoch - 11ms/step\n",
            "Score for fold 2: loss of 18.753074645996094\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 11s - loss: 11.1840 - mse: 11.1840 - mae: 1.3831 - val_loss: 7.8436 - val_mse: 7.8436 - val_mae: 1.3024 - lr: 4.0024e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 11s - loss: 10.6129 - mse: 10.6129 - mae: 1.3744 - val_loss: 7.5952 - val_mse: 7.5952 - val_mae: 1.3683 - lr: 4.0024e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 11s - loss: 10.3848 - mse: 10.3848 - mae: 1.3615 - val_loss: 7.4962 - val_mse: 7.4962 - val_mae: 1.3630 - lr: 4.0024e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 11s - loss: 10.0791 - mse: 10.0791 - mae: 1.3413 - val_loss: 8.1490 - val_mse: 8.1490 - val_mae: 1.4068 - lr: 4.0024e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 11s - loss: 10.1742 - mse: 10.1742 - mae: 1.3325 - val_loss: 7.4872 - val_mse: 7.4872 - val_mae: 1.3364 - lr: 4.0024e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 11s - loss: 9.7256 - mse: 9.7256 - mae: 1.3199 - val_loss: 8.0661 - val_mse: 8.0661 - val_mae: 1.3729 - lr: 4.0024e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 11s - loss: 9.5933 - mse: 9.5933 - mae: 1.3092 - val_loss: 8.0797 - val_mse: 8.0797 - val_mae: 1.3275 - lr: 4.0024e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 11s - loss: 9.1669 - mse: 9.1669 - mae: 1.2921 - val_loss: 7.8965 - val_mse: 7.8965 - val_mae: 1.3702 - lr: 4.0024e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 11s - loss: 9.1669 - mse: 9.1669 - mae: 1.2796 - val_loss: 8.5139 - val_mse: 8.5139 - val_mae: 1.3639 - lr: 4.0024e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 11s - loss: 9.0089 - mse: 9.0089 - mae: 1.2678 - val_loss: 8.0567 - val_mse: 8.0567 - val_mae: 1.3652 - lr: 4.0024e-04 - 11s/epoch - 11ms/step\n",
            "Score for fold 3: loss of 8.056672096252441\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 11s - loss: 9.2025 - mse: 9.2025 - mae: 1.3031 - val_loss: 8.7566 - val_mse: 8.7566 - val_mae: 1.2997 - lr: 4.0024e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 11s - loss: 8.7431 - mse: 8.7431 - mae: 1.2878 - val_loss: 7.8118 - val_mse: 7.8118 - val_mae: 1.3059 - lr: 4.0024e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 11s - loss: 8.7404 - mse: 8.7404 - mae: 1.2691 - val_loss: 6.9733 - val_mse: 6.9733 - val_mae: 1.2776 - lr: 4.0024e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 12s - loss: 8.5464 - mse: 8.5464 - mae: 1.2613 - val_loss: 7.1963 - val_mse: 7.1963 - val_mae: 1.2834 - lr: 4.0024e-04 - 12s/epoch - 12ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 12s - loss: 8.1307 - mse: 8.1307 - mae: 1.2349 - val_loss: 7.7593 - val_mse: 7.7593 - val_mae: 1.3275 - lr: 4.0024e-04 - 12s/epoch - 12ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 12s - loss: 7.9237 - mse: 7.9237 - mae: 1.2230 - val_loss: 7.6069 - val_mse: 7.6069 - val_mae: 1.2524 - lr: 4.0024e-04 - 12s/epoch - 12ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 12s - loss: 7.6675 - mse: 7.6675 - mae: 1.2179 - val_loss: 8.5765 - val_mse: 8.5765 - val_mae: 1.3086 - lr: 4.0024e-04 - 12s/epoch - 12ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 12s - loss: 7.4400 - mse: 7.4400 - mae: 1.2000 - val_loss: 8.0005 - val_mse: 8.0005 - val_mae: 1.4682 - lr: 4.0024e-04 - 12s/epoch - 12ms/step\n",
            "Score for fold 4: loss of 8.000471115112305\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 12s - loss: 7.8827 - mse: 7.8827 - mae: 1.2444 - val_loss: 6.5519 - val_mse: 6.5519 - val_mae: 1.2104 - lr: 4.0024e-04 - 12s/epoch - 12ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 13s - loss: 7.3940 - mse: 7.3940 - mae: 1.2265 - val_loss: 5.9671 - val_mse: 5.9671 - val_mae: 1.1259 - lr: 4.0024e-04 - 13s/epoch - 13ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 13s - loss: 7.2948 - mse: 7.2948 - mae: 1.1977 - val_loss: 7.5542 - val_mse: 7.5542 - val_mae: 1.1919 - lr: 4.0024e-04 - 13s/epoch - 13ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 13s - loss: 7.0130 - mse: 7.0130 - mae: 1.1848 - val_loss: 7.3108 - val_mse: 7.3108 - val_mae: 1.2184 - lr: 4.0024e-04 - 13s/epoch - 13ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 13s - loss: 6.9983 - mse: 6.9983 - mae: 1.1772 - val_loss: 6.8857 - val_mse: 6.8857 - val_mae: 1.1786 - lr: 4.0024e-04 - 13s/epoch - 13ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 13s - loss: 6.6729 - mse: 6.6729 - mae: 1.1582 - val_loss: 7.1473 - val_mse: 7.1473 - val_mae: 1.1989 - lr: 4.0024e-04 - 13s/epoch - 13ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 13s - loss: 6.5223 - mse: 6.5223 - mae: 1.1505 - val_loss: 7.7680 - val_mse: 7.7680 - val_mae: 1.3170 - lr: 4.0024e-04 - 13s/epoch - 13ms/step\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-10-12 00:44:17,075]\u001b[0m Finished trial#32 resulted in value: 10.459999999999999. Current best value is 9.279999999999998 with parameters: {'activation': 'relu', 'num_hidden_layer': 4, 'i': 10, 'learning_rate': 0.00027563299450396555}.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Score for fold 5: loss of 7.768030166625977\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 14s - loss: 13.8511 - mse: 13.8511 - mae: 1.5579 - val_loss: 11.3656 - val_mse: 11.3656 - val_mae: 1.4326 - lr: 7.0148e-04 - 14s/epoch - 14ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 13s - loss: 12.9990 - mse: 12.9990 - mae: 1.5132 - val_loss: 10.9308 - val_mse: 10.9308 - val_mae: 1.6113 - lr: 7.0148e-04 - 13s/epoch - 13ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 13s - loss: 12.7785 - mse: 12.7785 - mae: 1.4915 - val_loss: 10.9500 - val_mse: 10.9500 - val_mae: 1.4465 - lr: 7.0148e-04 - 13s/epoch - 13ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 13s - loss: 12.7689 - mse: 12.7689 - mae: 1.4827 - val_loss: 10.5132 - val_mse: 10.5132 - val_mae: 1.4912 - lr: 7.0148e-04 - 13s/epoch - 13ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 13s - loss: 12.4446 - mse: 12.4446 - mae: 1.4711 - val_loss: 10.5763 - val_mse: 10.5763 - val_mae: 1.4221 - lr: 7.0148e-04 - 13s/epoch - 13ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 13s - loss: 12.4742 - mse: 12.4742 - mae: 1.4655 - val_loss: 10.5141 - val_mse: 10.5141 - val_mae: 1.4431 - lr: 7.0148e-04 - 13s/epoch - 13ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 13s - loss: 12.3690 - mse: 12.3690 - mae: 1.4621 - val_loss: 10.5454 - val_mse: 10.5454 - val_mae: 1.5055 - lr: 7.0148e-04 - 13s/epoch - 13ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 12s - loss: 12.1532 - mse: 12.1532 - mae: 1.4503 - val_loss: 10.9035 - val_mse: 10.9035 - val_mae: 1.4689 - lr: 7.0148e-04 - 12s/epoch - 12ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 13s - loss: 11.9628 - mse: 11.9628 - mae: 1.4480 - val_loss: 10.6301 - val_mse: 10.6301 - val_mae: 1.4385 - lr: 7.0148e-04 - 13s/epoch - 13ms/step\n",
            "Score for fold 1: loss of 10.630066871643066\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 13s - loss: 12.8719 - mse: 12.8719 - mae: 1.4404 - val_loss: 7.8393 - val_mse: 7.8393 - val_mae: 1.4939 - lr: 7.0148e-04 - 13s/epoch - 13ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 13s - loss: 12.2842 - mse: 12.2842 - mae: 1.4353 - val_loss: 7.7344 - val_mse: 7.7344 - val_mae: 1.4625 - lr: 7.0148e-04 - 13s/epoch - 13ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 13s - loss: 12.2482 - mse: 12.2482 - mae: 1.4248 - val_loss: 7.7268 - val_mse: 7.7268 - val_mae: 1.4846 - lr: 7.0148e-04 - 13s/epoch - 13ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 13s - loss: 12.2468 - mse: 12.2468 - mae: 1.4220 - val_loss: 8.2706 - val_mse: 8.2706 - val_mae: 1.4119 - lr: 7.0148e-04 - 13s/epoch - 13ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 13s - loss: 11.9682 - mse: 11.9682 - mae: 1.4113 - val_loss: 7.9916 - val_mse: 7.9916 - val_mae: 1.4198 - lr: 7.0148e-04 - 13s/epoch - 13ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 14s - loss: 12.0066 - mse: 12.0066 - mae: 1.4148 - val_loss: 7.9259 - val_mse: 7.9259 - val_mae: 1.5029 - lr: 7.0148e-04 - 14s/epoch - 14ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 14s - loss: 11.5941 - mse: 11.5941 - mae: 1.4000 - val_loss: 8.1311 - val_mse: 8.1311 - val_mae: 1.5181 - lr: 7.0148e-04 - 14s/epoch - 14ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 14s - loss: 11.7056 - mse: 11.7056 - mae: 1.3908 - val_loss: 7.8687 - val_mse: 7.8687 - val_mae: 1.4346 - lr: 7.0148e-04 - 14s/epoch - 14ms/step\n",
            "Score for fold 2: loss of 7.868719100952148\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 13s - loss: 10.7080 - mse: 10.7080 - mae: 1.4062 - val_loss: 12.3480 - val_mse: 12.3480 - val_mae: 1.3685 - lr: 7.0148e-04 - 13s/epoch - 13ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 13s - loss: 10.4810 - mse: 10.4810 - mae: 1.3888 - val_loss: 11.3694 - val_mse: 11.3694 - val_mae: 1.3897 - lr: 7.0148e-04 - 13s/epoch - 13ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 13s - loss: 10.1966 - mse: 10.1966 - mae: 1.3779 - val_loss: 11.5063 - val_mse: 11.5063 - val_mae: 1.3816 - lr: 7.0148e-04 - 13s/epoch - 13ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 13s - loss: 10.0672 - mse: 10.0672 - mae: 1.3618 - val_loss: 11.9956 - val_mse: 11.9956 - val_mae: 1.4983 - lr: 7.0148e-04 - 13s/epoch - 13ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 15s - loss: 9.8976 - mse: 9.8976 - mae: 1.3530 - val_loss: 11.7116 - val_mse: 11.7116 - val_mae: 1.4605 - lr: 7.0148e-04 - 15s/epoch - 15ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 13s - loss: 10.0830 - mse: 10.0830 - mae: 1.3398 - val_loss: 11.7148 - val_mse: 11.7148 - val_mae: 1.4547 - lr: 7.0148e-04 - 13s/epoch - 13ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 12s - loss: 9.3582 - mse: 9.3582 - mae: 1.3233 - val_loss: 12.3389 - val_mse: 12.3389 - val_mae: 1.4171 - lr: 7.0148e-04 - 12s/epoch - 12ms/step\n",
            "Score for fold 3: loss of 12.338857650756836\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 12s - loss: 10.2440 - mse: 10.2440 - mae: 1.3677 - val_loss: 9.1200 - val_mse: 9.1200 - val_mae: 1.3943 - lr: 7.0148e-04 - 12s/epoch - 12ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 12s - loss: 9.8306 - mse: 9.8306 - mae: 1.3544 - val_loss: 9.6308 - val_mse: 9.6308 - val_mae: 1.2972 - lr: 7.0148e-04 - 12s/epoch - 12ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 12s - loss: 9.5770 - mse: 9.5770 - mae: 1.3399 - val_loss: 9.5860 - val_mse: 9.5860 - val_mae: 1.3732 - lr: 7.0148e-04 - 12s/epoch - 12ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 11s - loss: 9.3287 - mse: 9.3287 - mae: 1.3219 - val_loss: 9.1930 - val_mse: 9.1930 - val_mae: 1.3237 - lr: 7.0148e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 12s - loss: 9.3024 - mse: 9.3024 - mae: 1.3103 - val_loss: 9.6560 - val_mse: 9.6560 - val_mae: 1.4021 - lr: 7.0148e-04 - 12s/epoch - 12ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 17s - loss: 9.6128 - mse: 9.6128 - mae: 1.3005 - val_loss: 10.3857 - val_mse: 10.3857 - val_mae: 1.3877 - lr: 7.0148e-04 - 17s/epoch - 17ms/step\n",
            "Score for fold 4: loss of 10.385735511779785\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 17s - loss: 8.3272 - mse: 8.3272 - mae: 1.3150 - val_loss: 13.3713 - val_mse: 13.3713 - val_mae: 1.2602 - lr: 7.0148e-04 - 17s/epoch - 17ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 13s - loss: 7.7383 - mse: 7.7383 - mae: 1.2936 - val_loss: 13.7201 - val_mse: 13.7201 - val_mae: 1.3348 - lr: 7.0148e-04 - 13s/epoch - 13ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 21s - loss: 7.7184 - mse: 7.7184 - mae: 1.2755 - val_loss: 13.8242 - val_mse: 13.8242 - val_mae: 1.2842 - lr: 7.0148e-04 - 21s/epoch - 21ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 19s - loss: 7.3443 - mse: 7.3443 - mae: 1.2600 - val_loss: 13.3374 - val_mse: 13.3374 - val_mae: 1.2906 - lr: 7.0148e-04 - 19s/epoch - 19ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 17s - loss: 7.1806 - mse: 7.1806 - mae: 1.2510 - val_loss: 13.7027 - val_mse: 13.7027 - val_mae: 1.3318 - lr: 7.0148e-04 - 17s/epoch - 17ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 19s - loss: 7.3211 - mse: 7.3211 - mae: 1.2410 - val_loss: 14.8513 - val_mse: 14.8513 - val_mae: 1.3506 - lr: 7.0148e-04 - 19s/epoch - 19ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 19s - loss: 7.0540 - mse: 7.0540 - mae: 1.2319 - val_loss: 13.5687 - val_mse: 13.5687 - val_mae: 1.3764 - lr: 7.0148e-04 - 19s/epoch - 19ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 21s - loss: 6.8623 - mse: 6.8623 - mae: 1.2230 - val_loss: 15.7096 - val_mse: 15.7096 - val_mae: 1.3882 - lr: 7.0148e-04 - 21s/epoch - 21ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 20s - loss: 6.7222 - mse: 6.7222 - mae: 1.2114 - val_loss: 18.6506 - val_mse: 18.6506 - val_mae: 1.3120 - lr: 7.0148e-04 - 20s/epoch - 20ms/step\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-10-12 00:53:44,262]\u001b[0m Finished trial#33 resulted in value: 11.976. Current best value is 9.279999999999998 with parameters: {'activation': 'relu', 'num_hidden_layer': 4, 'i': 10, 'learning_rate': 0.00027563299450396555}.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Score for fold 5: loss of 18.650611877441406\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 10s - loss: 13.8965 - mse: 13.8965 - mae: 1.5573 - val_loss: 9.5700 - val_mse: 9.5700 - val_mae: 1.4432 - lr: 3.2121e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 9s - loss: 13.1468 - mse: 13.1468 - mae: 1.4978 - val_loss: 9.5093 - val_mse: 9.5093 - val_mae: 1.4672 - lr: 3.2121e-04 - 9s/epoch - 9ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 9s - loss: 12.9473 - mse: 12.9473 - mae: 1.4881 - val_loss: 9.4421 - val_mse: 9.4421 - val_mae: 1.4401 - lr: 3.2121e-04 - 9s/epoch - 9ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 10s - loss: 12.5814 - mse: 12.5814 - mae: 1.4731 - val_loss: 9.4058 - val_mse: 9.4058 - val_mae: 1.4792 - lr: 3.2121e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 10s - loss: 12.6600 - mse: 12.6600 - mae: 1.4672 - val_loss: 9.2746 - val_mse: 9.2746 - val_mae: 1.4012 - lr: 3.2121e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 10s - loss: 12.6992 - mse: 12.6992 - mae: 1.4627 - val_loss: 9.2527 - val_mse: 9.2527 - val_mae: 1.4204 - lr: 3.2121e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 9s - loss: 12.3087 - mse: 12.3087 - mae: 1.4475 - val_loss: 9.3587 - val_mse: 9.3587 - val_mae: 1.4032 - lr: 3.2121e-04 - 9s/epoch - 9ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 10s - loss: 11.9993 - mse: 11.9993 - mae: 1.4339 - val_loss: 9.3922 - val_mse: 9.3922 - val_mae: 1.4663 - lr: 3.2121e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 9s - loss: 12.0747 - mse: 12.0747 - mae: 1.4352 - val_loss: 9.3968 - val_mse: 9.3968 - val_mae: 1.4222 - lr: 3.2121e-04 - 9s/epoch - 9ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 10s - loss: 12.0343 - mse: 12.0343 - mae: 1.4220 - val_loss: 9.3350 - val_mse: 9.3350 - val_mae: 1.3783 - lr: 3.2121e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 10s - loss: 11.4606 - mse: 11.4606 - mae: 1.4029 - val_loss: 9.2947 - val_mse: 9.2947 - val_mae: 1.4286 - lr: 3.2121e-04 - 10s/epoch - 10ms/step\n",
            "Score for fold 1: loss of 9.294672966003418\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 9s - loss: 11.8970 - mse: 11.8970 - mae: 1.4177 - val_loss: 8.1033 - val_mse: 8.1033 - val_mae: 1.4052 - lr: 3.2121e-04 - 9s/epoch - 9ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 10s - loss: 11.8018 - mse: 11.8018 - mae: 1.4049 - val_loss: 8.5219 - val_mse: 8.5219 - val_mae: 1.4062 - lr: 3.2121e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 9s - loss: 11.3189 - mse: 11.3189 - mae: 1.3898 - val_loss: 8.5215 - val_mse: 8.5215 - val_mae: 1.3736 - lr: 3.2121e-04 - 9s/epoch - 9ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 10s - loss: 11.1192 - mse: 11.1192 - mae: 1.3768 - val_loss: 8.3113 - val_mse: 8.3113 - val_mae: 1.4105 - lr: 3.2121e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 10s - loss: 10.7880 - mse: 10.7880 - mae: 1.3649 - val_loss: 10.3492 - val_mse: 10.3492 - val_mae: 1.3805 - lr: 3.2121e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 10s - loss: 10.4988 - mse: 10.4988 - mae: 1.3449 - val_loss: 9.1547 - val_mse: 9.1547 - val_mae: 1.3307 - lr: 3.2121e-04 - 10s/epoch - 10ms/step\n",
            "Score for fold 2: loss of 9.1547212600708\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 10s - loss: 9.6946 - mse: 9.6946 - mae: 1.3551 - val_loss: 11.8005 - val_mse: 11.8005 - val_mae: 1.3561 - lr: 3.2121e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 9s - loss: 9.3578 - mse: 9.3578 - mae: 1.3339 - val_loss: 12.6852 - val_mse: 12.6852 - val_mae: 1.3817 - lr: 3.2121e-04 - 9s/epoch - 9ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 10s - loss: 9.0744 - mse: 9.0744 - mae: 1.3174 - val_loss: 12.4718 - val_mse: 12.4718 - val_mae: 1.3720 - lr: 3.2121e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 10s - loss: 8.7175 - mse: 8.7175 - mae: 1.3093 - val_loss: 14.2668 - val_mse: 14.2668 - val_mae: 1.4165 - lr: 3.2121e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 10s - loss: 8.3235 - mse: 8.3235 - mae: 1.2885 - val_loss: 12.4752 - val_mse: 12.4752 - val_mae: 1.3417 - lr: 3.2121e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 10s - loss: 8.3070 - mse: 8.3070 - mae: 1.2760 - val_loss: 13.4314 - val_mse: 13.4314 - val_mae: 1.3724 - lr: 3.2121e-04 - 10s/epoch - 10ms/step\n",
            "Score for fold 3: loss of 13.431384086608887\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 10s - loss: 8.1586 - mse: 8.1586 - mae: 1.3076 - val_loss: 13.1116 - val_mse: 13.1116 - val_mae: 1.4221 - lr: 3.2121e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 10s - loss: 8.0746 - mse: 8.0746 - mae: 1.2923 - val_loss: 13.2126 - val_mse: 13.2126 - val_mae: 1.2736 - lr: 3.2121e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 9s - loss: 7.5858 - mse: 7.5858 - mae: 1.2716 - val_loss: 13.5000 - val_mse: 13.5000 - val_mae: 1.3611 - lr: 3.2121e-04 - 9s/epoch - 9ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 10s - loss: 7.4980 - mse: 7.4980 - mae: 1.2618 - val_loss: 13.5307 - val_mse: 13.5307 - val_mae: 1.2750 - lr: 3.2121e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 10s - loss: 7.3476 - mse: 7.3476 - mae: 1.2492 - val_loss: 13.7181 - val_mse: 13.7181 - val_mae: 1.2924 - lr: 3.2121e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 9s - loss: 6.8713 - mse: 6.8713 - mae: 1.2317 - val_loss: 14.2216 - val_mse: 14.2216 - val_mae: 1.3399 - lr: 3.2121e-04 - 9s/epoch - 9ms/step\n",
            "Score for fold 4: loss of 14.22155475616455\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 10s - loss: 8.8676 - mse: 8.8676 - mae: 1.2636 - val_loss: 5.9575 - val_mse: 5.9575 - val_mae: 1.2533 - lr: 3.2121e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 10s - loss: 8.4835 - mse: 8.4835 - mae: 1.2412 - val_loss: 5.5971 - val_mse: 5.5971 - val_mae: 1.2320 - lr: 3.2121e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 10s - loss: 8.2894 - mse: 8.2894 - mae: 1.2240 - val_loss: 6.2044 - val_mse: 6.2044 - val_mae: 1.1733 - lr: 3.2121e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 10s - loss: 8.1498 - mse: 8.1498 - mae: 1.2106 - val_loss: 6.1902 - val_mse: 6.1902 - val_mae: 1.2200 - lr: 3.2121e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 10s - loss: 7.8072 - mse: 7.8072 - mae: 1.1901 - val_loss: 6.1464 - val_mse: 6.1464 - val_mae: 1.2434 - lr: 3.2121e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 11s - loss: 7.4424 - mse: 7.4424 - mae: 1.1797 - val_loss: 6.2671 - val_mse: 6.2671 - val_mae: 1.3355 - lr: 3.2121e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 10s - loss: 7.3036 - mse: 7.3036 - mae: 1.1674 - val_loss: 6.6065 - val_mse: 6.6065 - val_mae: 1.2682 - lr: 3.2121e-04 - 10s/epoch - 10ms/step\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-10-12 00:59:46,450]\u001b[0m Finished trial#34 resulted in value: 10.54. Current best value is 9.279999999999998 with parameters: {'activation': 'relu', 'num_hidden_layer': 4, 'i': 10, 'learning_rate': 0.00027563299450396555}.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Score for fold 5: loss of 6.606506824493408\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 22s - loss: 13.8875 - mse: 13.8875 - mae: 1.5439 - val_loss: 10.2110 - val_mse: 10.2110 - val_mae: 1.5005 - lr: 2.3446e-04 - 22s/epoch - 22ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 21s - loss: 13.0069 - mse: 13.0069 - mae: 1.4963 - val_loss: 9.8218 - val_mse: 9.8218 - val_mae: 1.4901 - lr: 2.3446e-04 - 21s/epoch - 21ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 18s - loss: 12.8543 - mse: 12.8543 - mae: 1.4825 - val_loss: 9.9897 - val_mse: 9.9897 - val_mae: 1.4285 - lr: 2.3446e-04 - 18s/epoch - 18ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 17s - loss: 12.9909 - mse: 12.9909 - mae: 1.4706 - val_loss: 9.3677 - val_mse: 9.3677 - val_mae: 1.4855 - lr: 2.3446e-04 - 17s/epoch - 17ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 19s - loss: 12.6957 - mse: 12.6957 - mae: 1.4579 - val_loss: 9.9275 - val_mse: 9.9275 - val_mae: 1.4880 - lr: 2.3446e-04 - 19s/epoch - 19ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 18s - loss: 12.4146 - mse: 12.4146 - mae: 1.4469 - val_loss: 9.5285 - val_mse: 9.5285 - val_mae: 1.4829 - lr: 2.3446e-04 - 18s/epoch - 18ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 17s - loss: 12.4048 - mse: 12.4048 - mae: 1.4378 - val_loss: 9.4961 - val_mse: 9.4961 - val_mae: 1.5834 - lr: 2.3446e-04 - 17s/epoch - 17ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 16s - loss: 12.1703 - mse: 12.1703 - mae: 1.4282 - val_loss: 9.1811 - val_mse: 9.1811 - val_mae: 1.4844 - lr: 2.3446e-04 - 16s/epoch - 16ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 15s - loss: 11.9429 - mse: 11.9429 - mae: 1.4196 - val_loss: 9.5354 - val_mse: 9.5354 - val_mae: 1.5012 - lr: 2.3446e-04 - 15s/epoch - 15ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 15s - loss: 11.7269 - mse: 11.7269 - mae: 1.4039 - val_loss: 9.9750 - val_mse: 9.9750 - val_mae: 1.3849 - lr: 2.3446e-04 - 15s/epoch - 15ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 16s - loss: 11.2783 - mse: 11.2783 - mae: 1.3914 - val_loss: 8.8405 - val_mse: 8.8405 - val_mae: 1.5618 - lr: 2.3446e-04 - 16s/epoch - 16ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 17s - loss: 11.0328 - mse: 11.0328 - mae: 1.3805 - val_loss: 9.1782 - val_mse: 9.1782 - val_mae: 1.5410 - lr: 2.3446e-04 - 17s/epoch - 17ms/step\n",
            "Epoch 13/100\n",
            "1000/1000 - 18s - loss: 11.0178 - mse: 11.0178 - mae: 1.3634 - val_loss: 9.5216 - val_mse: 9.5216 - val_mae: 1.4386 - lr: 2.3446e-04 - 18s/epoch - 18ms/step\n",
            "Epoch 14/100\n",
            "1000/1000 - 20s - loss: 10.6413 - mse: 10.6413 - mae: 1.3405 - val_loss: 10.1380 - val_mse: 10.1380 - val_mae: 1.4461 - lr: 2.3446e-04 - 20s/epoch - 20ms/step\n",
            "Epoch 15/100\n",
            "1000/1000 - 22s - loss: 10.3190 - mse: 10.3190 - mae: 1.3191 - val_loss: 10.3157 - val_mse: 10.3157 - val_mae: 1.5517 - lr: 2.3446e-04 - 22s/epoch - 22ms/step\n",
            "Epoch 16/100\n",
            "1000/1000 - 21s - loss: 9.8601 - mse: 9.8601 - mae: 1.3061 - val_loss: 10.1176 - val_mse: 10.1176 - val_mae: 1.4895 - lr: 2.3446e-04 - 21s/epoch - 21ms/step\n",
            "Score for fold 1: loss of 10.117591857910156\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 19s - loss: 10.7531 - mse: 10.7531 - mae: 1.3591 - val_loss: 6.7389 - val_mse: 6.7389 - val_mae: 1.2467 - lr: 2.3446e-04 - 19s/epoch - 19ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 17s - loss: 10.5726 - mse: 10.5726 - mae: 1.3418 - val_loss: 6.6709 - val_mse: 6.6709 - val_mae: 1.3145 - lr: 2.3446e-04 - 17s/epoch - 17ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 17s - loss: 10.2331 - mse: 10.2331 - mae: 1.3204 - val_loss: 6.9110 - val_mse: 6.9110 - val_mae: 1.4518 - lr: 2.3446e-04 - 17s/epoch - 17ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 16s - loss: 9.7655 - mse: 9.7655 - mae: 1.3033 - val_loss: 6.8617 - val_mse: 6.8617 - val_mae: 1.4786 - lr: 2.3446e-04 - 16s/epoch - 16ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 16s - loss: 9.3754 - mse: 9.3754 - mae: 1.2836 - val_loss: 6.9228 - val_mse: 6.9228 - val_mae: 1.3124 - lr: 2.3446e-04 - 16s/epoch - 16ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 15s - loss: 9.4002 - mse: 9.4002 - mae: 1.2704 - val_loss: 7.1793 - val_mse: 7.1793 - val_mae: 1.5042 - lr: 2.3446e-04 - 15s/epoch - 15ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 16s - loss: 9.1610 - mse: 9.1610 - mae: 1.2493 - val_loss: 7.3510 - val_mse: 7.3510 - val_mae: 1.3344 - lr: 2.3446e-04 - 16s/epoch - 16ms/step\n",
            "Score for fold 2: loss of 7.351038455963135\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 15s - loss: 9.4697 - mse: 9.4697 - mae: 1.2959 - val_loss: 5.2321 - val_mse: 5.2321 - val_mae: 1.2615 - lr: 2.3446e-04 - 15s/epoch - 15ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 15s - loss: 8.8368 - mse: 8.8368 - mae: 1.2686 - val_loss: 5.0851 - val_mse: 5.0851 - val_mae: 1.2930 - lr: 2.3446e-04 - 15s/epoch - 15ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 15s - loss: 8.8038 - mse: 8.8038 - mae: 1.2581 - val_loss: 5.6305 - val_mse: 5.6305 - val_mae: 1.2439 - lr: 2.3446e-04 - 15s/epoch - 15ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 15s - loss: 8.3916 - mse: 8.3916 - mae: 1.2325 - val_loss: 6.3985 - val_mse: 6.3985 - val_mae: 1.2266 - lr: 2.3446e-04 - 15s/epoch - 15ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 15s - loss: 8.1671 - mse: 8.1671 - mae: 1.2225 - val_loss: 6.5817 - val_mse: 6.5817 - val_mae: 1.5024 - lr: 2.3446e-04 - 15s/epoch - 15ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 14s - loss: 7.4356 - mse: 7.4356 - mae: 1.1956 - val_loss: 5.9608 - val_mse: 5.9608 - val_mae: 1.2107 - lr: 2.3446e-04 - 14s/epoch - 14ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 14s - loss: 7.4388 - mse: 7.4388 - mae: 1.1805 - val_loss: 6.2301 - val_mse: 6.2301 - val_mae: 1.2756 - lr: 2.3446e-04 - 14s/epoch - 14ms/step\n",
            "Score for fold 3: loss of 6.230116844177246\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 14s - loss: 6.5919 - mse: 6.5919 - mae: 1.2097 - val_loss: 8.3284 - val_mse: 8.3284 - val_mae: 1.1318 - lr: 2.3446e-04 - 14s/epoch - 14ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 14s - loss: 6.2690 - mse: 6.2690 - mae: 1.1849 - val_loss: 8.3443 - val_mse: 8.3443 - val_mae: 1.1238 - lr: 2.3446e-04 - 14s/epoch - 14ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 14s - loss: 6.0194 - mse: 6.0194 - mae: 1.1688 - val_loss: 8.6023 - val_mse: 8.6023 - val_mae: 1.1516 - lr: 2.3446e-04 - 14s/epoch - 14ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 13s - loss: 5.5696 - mse: 5.5696 - mae: 1.1446 - val_loss: 8.7353 - val_mse: 8.7353 - val_mae: 1.2033 - lr: 2.3446e-04 - 13s/epoch - 13ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 13s - loss: 5.3837 - mse: 5.3837 - mae: 1.1286 - val_loss: 9.6024 - val_mse: 9.6024 - val_mae: 1.1681 - lr: 2.3446e-04 - 13s/epoch - 13ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 13s - loss: 4.9527 - mse: 4.9527 - mae: 1.1154 - val_loss: 11.8021 - val_mse: 11.8021 - val_mae: 1.1950 - lr: 2.3446e-04 - 13s/epoch - 13ms/step\n",
            "Score for fold 4: loss of 11.80214786529541\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 13s - loss: 5.6708 - mse: 5.6708 - mae: 1.1492 - val_loss: 8.4320 - val_mse: 8.4320 - val_mae: 1.0918 - lr: 2.3446e-04 - 13s/epoch - 13ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 13s - loss: 4.8924 - mse: 4.8924 - mae: 1.1147 - val_loss: 9.1530 - val_mse: 9.1530 - val_mae: 1.1353 - lr: 2.3446e-04 - 13s/epoch - 13ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 13s - loss: 4.6605 - mse: 4.6605 - mae: 1.0946 - val_loss: 9.6510 - val_mse: 9.6510 - val_mae: 1.0348 - lr: 2.3446e-04 - 13s/epoch - 13ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 13s - loss: 4.4480 - mse: 4.4480 - mae: 1.0832 - val_loss: 9.5595 - val_mse: 9.5595 - val_mae: 1.3018 - lr: 2.3446e-04 - 13s/epoch - 13ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 13s - loss: 4.5061 - mse: 4.5061 - mae: 1.0737 - val_loss: 10.3825 - val_mse: 10.3825 - val_mae: 1.1302 - lr: 2.3446e-04 - 13s/epoch - 13ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 13s - loss: 4.1231 - mse: 4.1231 - mae: 1.0480 - val_loss: 10.0755 - val_mse: 10.0755 - val_mae: 1.1488 - lr: 2.3446e-04 - 13s/epoch - 13ms/step\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-10-12 01:11:05,330]\u001b[0m Finished trial#35 resulted in value: 9.116. Current best value is 9.116 with parameters: {'activation': 'relu', 'num_hidden_layer': 4, 'i': 10, 'learning_rate': 0.0002344581502453532}.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Score for fold 5: loss of 10.07546615600586\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 7s - loss: 14.2043 - mse: 14.2043 - mae: 1.5532 - val_loss: 9.5222 - val_mse: 9.5222 - val_mae: 1.4511 - lr: 1.1885e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 6s - loss: 13.1321 - mse: 13.1321 - mae: 1.4943 - val_loss: 10.0177 - val_mse: 10.0177 - val_mae: 1.4610 - lr: 1.1885e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 6s - loss: 12.8442 - mse: 12.8442 - mae: 1.4741 - val_loss: 10.2209 - val_mse: 10.2209 - val_mae: 1.4319 - lr: 1.1885e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 6s - loss: 12.7135 - mse: 12.7135 - mae: 1.4607 - val_loss: 10.4562 - val_mse: 10.4562 - val_mae: 1.4940 - lr: 1.1885e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 6s - loss: 12.6082 - mse: 12.6082 - mae: 1.4552 - val_loss: 10.5390 - val_mse: 10.5390 - val_mae: 1.4355 - lr: 1.1885e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 6s - loss: 12.4280 - mse: 12.4280 - mae: 1.4470 - val_loss: 9.5870 - val_mse: 9.5870 - val_mae: 1.5240 - lr: 1.1885e-04 - 6s/epoch - 6ms/step\n",
            "Score for fold 1: loss of 9.586999893188477\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 6s - loss: 9.9499 - mse: 9.9499 - mae: 1.4372 - val_loss: 19.7663 - val_mse: 19.7663 - val_mae: 1.4523 - lr: 1.1885e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 6s - loss: 9.9047 - mse: 9.9047 - mae: 1.4320 - val_loss: 19.4178 - val_mse: 19.4178 - val_mae: 1.4252 - lr: 1.1885e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 6s - loss: 9.7119 - mse: 9.7119 - mae: 1.4204 - val_loss: 19.8629 - val_mse: 19.8629 - val_mae: 1.4618 - lr: 1.1885e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 6s - loss: 9.6900 - mse: 9.6900 - mae: 1.4123 - val_loss: 19.5168 - val_mse: 19.5168 - val_mae: 1.4961 - lr: 1.1885e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 6s - loss: 9.5291 - mse: 9.5291 - mae: 1.4048 - val_loss: 19.3853 - val_mse: 19.3853 - val_mae: 1.4771 - lr: 1.1885e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 7s - loss: 9.4623 - mse: 9.4623 - mae: 1.3950 - val_loss: 19.3856 - val_mse: 19.3856 - val_mae: 1.4962 - lr: 1.1885e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 8s - loss: 9.2872 - mse: 9.2872 - mae: 1.3894 - val_loss: 19.3426 - val_mse: 19.3426 - val_mae: 1.4997 - lr: 1.1885e-04 - 8s/epoch - 8ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 10s - loss: 8.9804 - mse: 8.9804 - mae: 1.3767 - val_loss: 19.5691 - val_mse: 19.5691 - val_mae: 1.4966 - lr: 1.1885e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 9s - loss: 8.9932 - mse: 8.9932 - mae: 1.3691 - val_loss: 19.6175 - val_mse: 19.6175 - val_mae: 1.4942 - lr: 1.1885e-04 - 9s/epoch - 9ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 10s - loss: 8.7818 - mse: 8.7818 - mae: 1.3629 - val_loss: 19.6288 - val_mse: 19.6288 - val_mae: 1.4638 - lr: 1.1885e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 8s - loss: 8.5458 - mse: 8.5458 - mae: 1.3532 - val_loss: 19.9025 - val_mse: 19.9025 - val_mae: 1.5298 - lr: 1.1885e-04 - 8s/epoch - 8ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 7s - loss: 8.4046 - mse: 8.4046 - mae: 1.3393 - val_loss: 19.4227 - val_mse: 19.4227 - val_mae: 1.4637 - lr: 1.1885e-04 - 7s/epoch - 7ms/step\n",
            "Score for fold 2: loss of 19.4227237701416\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 7s - loss: 11.4520 - mse: 11.4520 - mae: 1.3830 - val_loss: 7.9046 - val_mse: 7.9046 - val_mae: 1.3394 - lr: 1.1885e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 8s - loss: 11.0748 - mse: 11.0748 - mae: 1.3668 - val_loss: 7.8145 - val_mse: 7.8145 - val_mae: 1.3485 - lr: 1.1885e-04 - 8s/epoch - 8ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 8s - loss: 10.7401 - mse: 10.7401 - mae: 1.3524 - val_loss: 7.6618 - val_mse: 7.6618 - val_mae: 1.3339 - lr: 1.1885e-04 - 8s/epoch - 8ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 8s - loss: 10.6918 - mse: 10.6918 - mae: 1.3424 - val_loss: 7.6747 - val_mse: 7.6747 - val_mae: 1.3808 - lr: 1.1885e-04 - 8s/epoch - 8ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 9s - loss: 10.3290 - mse: 10.3290 - mae: 1.3288 - val_loss: 8.0952 - val_mse: 8.0952 - val_mae: 1.3365 - lr: 1.1885e-04 - 9s/epoch - 9ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 9s - loss: 10.3439 - mse: 10.3439 - mae: 1.3193 - val_loss: 7.9152 - val_mse: 7.9152 - val_mae: 1.3591 - lr: 1.1885e-04 - 9s/epoch - 9ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 8s - loss: 10.1544 - mse: 10.1544 - mae: 1.3090 - val_loss: 8.0349 - val_mse: 8.0349 - val_mae: 1.4358 - lr: 1.1885e-04 - 8s/epoch - 8ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 7s - loss: 10.0184 - mse: 10.0184 - mae: 1.2986 - val_loss: 8.2304 - val_mse: 8.2304 - val_mae: 1.3767 - lr: 1.1885e-04 - 7s/epoch - 7ms/step\n",
            "Score for fold 3: loss of 8.230436325073242\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 7s - loss: 10.2887 - mse: 10.2887 - mae: 1.3248 - val_loss: 7.7562 - val_mse: 7.7562 - val_mae: 1.2492 - lr: 1.1885e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 6s - loss: 9.8404 - mse: 9.8404 - mae: 1.3021 - val_loss: 7.6883 - val_mse: 7.6883 - val_mae: 1.2916 - lr: 1.1885e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 6s - loss: 9.5145 - mse: 9.5145 - mae: 1.2843 - val_loss: 7.4121 - val_mse: 7.4121 - val_mae: 1.2834 - lr: 1.1885e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 6s - loss: 9.6263 - mse: 9.6263 - mae: 1.2824 - val_loss: 7.8345 - val_mse: 7.8345 - val_mae: 1.2929 - lr: 1.1885e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 6s - loss: 9.3335 - mse: 9.3335 - mae: 1.2627 - val_loss: 7.5718 - val_mse: 7.5718 - val_mae: 1.3681 - lr: 1.1885e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 7s - loss: 8.8852 - mse: 8.8852 - mae: 1.2480 - val_loss: 8.0990 - val_mse: 8.0990 - val_mae: 1.2978 - lr: 1.1885e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 6s - loss: 8.7351 - mse: 8.7351 - mae: 1.2360 - val_loss: 7.8625 - val_mse: 7.8625 - val_mae: 1.3473 - lr: 1.1885e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 6s - loss: 8.6300 - mse: 8.6300 - mae: 1.2260 - val_loss: 8.0846 - val_mse: 8.0846 - val_mae: 1.3882 - lr: 1.1885e-04 - 6s/epoch - 6ms/step\n",
            "Score for fold 4: loss of 8.084558486938477\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 6s - loss: 8.8153 - mse: 8.8153 - mae: 1.2650 - val_loss: 6.8599 - val_mse: 6.8599 - val_mae: 1.1768 - lr: 1.1885e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 6s - loss: 8.8650 - mse: 8.8650 - mae: 1.2399 - val_loss: 7.1612 - val_mse: 7.1612 - val_mae: 1.1801 - lr: 1.1885e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 6s - loss: 8.3013 - mse: 8.3013 - mae: 1.2330 - val_loss: 7.3656 - val_mse: 7.3656 - val_mae: 1.2044 - lr: 1.1885e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 6s - loss: 8.0598 - mse: 8.0598 - mae: 1.2098 - val_loss: 7.3970 - val_mse: 7.3970 - val_mae: 1.2100 - lr: 1.1885e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 6s - loss: 7.6620 - mse: 7.6620 - mae: 1.1832 - val_loss: 7.7237 - val_mse: 7.7237 - val_mae: 1.3246 - lr: 1.1885e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 6s - loss: 7.5312 - mse: 7.5312 - mae: 1.1767 - val_loss: 8.3111 - val_mse: 8.3111 - val_mae: 1.2344 - lr: 1.1885e-04 - 6s/epoch - 6ms/step\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-10-12 01:15:53,557]\u001b[0m Finished trial#36 resulted in value: 10.726. Current best value is 9.116 with parameters: {'activation': 'relu', 'num_hidden_layer': 4, 'i': 10, 'learning_rate': 0.0002344581502453532}.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Score for fold 5: loss of 8.311134338378906\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 3s - loss: 14.0248 - mse: 14.0248 - mae: 1.5711 - val_loss: 14.0415 - val_mse: 14.0415 - val_mae: 1.5596 - lr: 1.8331e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 3s - loss: 12.6264 - mse: 12.6264 - mae: 1.5070 - val_loss: 13.7379 - val_mse: 13.7379 - val_mae: 1.4890 - lr: 1.8331e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 3s - loss: 12.3544 - mse: 12.3544 - mae: 1.4889 - val_loss: 13.5637 - val_mse: 13.5637 - val_mae: 1.5044 - lr: 1.8331e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 3s - loss: 12.1526 - mse: 12.1526 - mae: 1.4819 - val_loss: 13.3454 - val_mse: 13.3454 - val_mae: 1.4923 - lr: 1.8331e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 3s - loss: 12.0628 - mse: 12.0628 - mae: 1.4729 - val_loss: 13.2930 - val_mse: 13.2930 - val_mae: 1.4973 - lr: 1.8331e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 3s - loss: 11.9098 - mse: 11.9098 - mae: 1.4688 - val_loss: 13.1766 - val_mse: 13.1766 - val_mae: 1.4619 - lr: 1.8331e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 3s - loss: 11.8546 - mse: 11.8546 - mae: 1.4621 - val_loss: 13.0966 - val_mse: 13.0966 - val_mae: 1.4892 - lr: 1.8331e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 3s - loss: 11.7958 - mse: 11.7958 - mae: 1.4597 - val_loss: 13.0938 - val_mse: 13.0938 - val_mae: 1.4629 - lr: 1.8331e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 3s - loss: 11.8253 - mse: 11.8253 - mae: 1.4555 - val_loss: 12.9758 - val_mse: 12.9758 - val_mae: 1.4671 - lr: 1.8331e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 3s - loss: 11.7019 - mse: 11.7019 - mae: 1.4547 - val_loss: 12.9956 - val_mse: 12.9956 - val_mae: 1.4629 - lr: 1.8331e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 3s - loss: 11.6790 - mse: 11.6790 - mae: 1.4464 - val_loss: 12.9695 - val_mse: 12.9695 - val_mae: 1.4767 - lr: 1.8331e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 3s - loss: 11.5874 - mse: 11.5874 - mae: 1.4464 - val_loss: 12.9498 - val_mse: 12.9498 - val_mae: 1.4654 - lr: 1.8331e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 13/100\n",
            "1000/1000 - 3s - loss: 11.5948 - mse: 11.5948 - mae: 1.4428 - val_loss: 12.8853 - val_mse: 12.8853 - val_mae: 1.4714 - lr: 1.8331e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 14/100\n",
            "1000/1000 - 3s - loss: 11.5941 - mse: 11.5941 - mae: 1.4394 - val_loss: 12.8756 - val_mse: 12.8756 - val_mae: 1.4978 - lr: 1.8331e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 15/100\n",
            "1000/1000 - 3s - loss: 11.5065 - mse: 11.5065 - mae: 1.4393 - val_loss: 12.9804 - val_mse: 12.9804 - val_mae: 1.4379 - lr: 1.8331e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 16/100\n",
            "1000/1000 - 3s - loss: 11.5013 - mse: 11.5013 - mae: 1.4378 - val_loss: 12.8404 - val_mse: 12.8404 - val_mae: 1.4386 - lr: 1.8331e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 17/100\n",
            "1000/1000 - 3s - loss: 11.5463 - mse: 11.5463 - mae: 1.4355 - val_loss: 12.8085 - val_mse: 12.8085 - val_mae: 1.4636 - lr: 1.8331e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 18/100\n",
            "1000/1000 - 3s - loss: 11.4345 - mse: 11.4345 - mae: 1.4350 - val_loss: 12.8176 - val_mse: 12.8176 - val_mae: 1.4603 - lr: 1.8331e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 19/100\n",
            "1000/1000 - 3s - loss: 11.4271 - mse: 11.4271 - mae: 1.4310 - val_loss: 12.8188 - val_mse: 12.8188 - val_mae: 1.4830 - lr: 1.8331e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 20/100\n",
            "1000/1000 - 3s - loss: 11.4126 - mse: 11.4126 - mae: 1.4297 - val_loss: 12.8283 - val_mse: 12.8283 - val_mae: 1.4443 - lr: 1.8331e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 21/100\n",
            "1000/1000 - 3s - loss: 11.4008 - mse: 11.4008 - mae: 1.4281 - val_loss: 12.8282 - val_mse: 12.8282 - val_mae: 1.4332 - lr: 1.8331e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 22/100\n",
            "1000/1000 - 3s - loss: 11.3652 - mse: 11.3652 - mae: 1.4287 - val_loss: 12.7709 - val_mse: 12.7709 - val_mae: 1.4457 - lr: 1.8331e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 23/100\n",
            "1000/1000 - 3s - loss: 11.3592 - mse: 11.3592 - mae: 1.4276 - val_loss: 12.8098 - val_mse: 12.8098 - val_mae: 1.4332 - lr: 1.8331e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 24/100\n",
            "1000/1000 - 3s - loss: 11.3372 - mse: 11.3372 - mae: 1.4272 - val_loss: 12.7459 - val_mse: 12.7459 - val_mae: 1.4475 - lr: 1.8331e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 25/100\n",
            "1000/1000 - 3s - loss: 11.3290 - mse: 11.3290 - mae: 1.4237 - val_loss: 12.7697 - val_mse: 12.7697 - val_mae: 1.4803 - lr: 1.8331e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 26/100\n",
            "1000/1000 - 3s - loss: 11.2657 - mse: 11.2657 - mae: 1.4230 - val_loss: 12.7688 - val_mse: 12.7688 - val_mae: 1.4811 - lr: 1.8331e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 27/100\n",
            "1000/1000 - 3s - loss: 11.2481 - mse: 11.2481 - mae: 1.4212 - val_loss: 12.7305 - val_mse: 12.7305 - val_mae: 1.4650 - lr: 1.8331e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 28/100\n",
            "1000/1000 - 3s - loss: 11.2735 - mse: 11.2735 - mae: 1.4215 - val_loss: 12.7133 - val_mse: 12.7133 - val_mae: 1.4836 - lr: 1.8331e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 29/100\n",
            "1000/1000 - 3s - loss: 11.2270 - mse: 11.2270 - mae: 1.4184 - val_loss: 12.7830 - val_mse: 12.7830 - val_mae: 1.4679 - lr: 1.8331e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 30/100\n",
            "1000/1000 - 3s - loss: 11.1681 - mse: 11.1681 - mae: 1.4178 - val_loss: 12.8160 - val_mse: 12.8160 - val_mae: 1.4665 - lr: 1.8331e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 31/100\n",
            "1000/1000 - 3s - loss: 11.2217 - mse: 11.2217 - mae: 1.4137 - val_loss: 12.7683 - val_mse: 12.7683 - val_mae: 1.4575 - lr: 1.8331e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 32/100\n",
            "1000/1000 - 3s - loss: 11.2224 - mse: 11.2224 - mae: 1.4164 - val_loss: 12.7330 - val_mse: 12.7330 - val_mae: 1.4593 - lr: 1.8331e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 33/100\n",
            "1000/1000 - 3s - loss: 11.1773 - mse: 11.1773 - mae: 1.4144 - val_loss: 12.8572 - val_mse: 12.8572 - val_mae: 1.4543 - lr: 1.8331e-04 - 3s/epoch - 3ms/step\n",
            "Score for fold 1: loss of 12.857245445251465\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 3s - loss: 12.1899 - mse: 12.1899 - mae: 1.4167 - val_loss: 8.6127 - val_mse: 8.6127 - val_mae: 1.4194 - lr: 1.8331e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 3s - loss: 12.1967 - mse: 12.1967 - mae: 1.4174 - val_loss: 8.6358 - val_mse: 8.6358 - val_mae: 1.4020 - lr: 1.8331e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 3s - loss: 12.1768 - mse: 12.1768 - mae: 1.4132 - val_loss: 8.5770 - val_mse: 8.5770 - val_mae: 1.4491 - lr: 1.8331e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 3s - loss: 12.1718 - mse: 12.1718 - mae: 1.4139 - val_loss: 8.5829 - val_mse: 8.5829 - val_mae: 1.4409 - lr: 1.8331e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 3s - loss: 12.1837 - mse: 12.1837 - mae: 1.4099 - val_loss: 8.6412 - val_mse: 8.6412 - val_mae: 1.4475 - lr: 1.8331e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 3s - loss: 12.1485 - mse: 12.1485 - mae: 1.4116 - val_loss: 8.6277 - val_mse: 8.6277 - val_mae: 1.4315 - lr: 1.8331e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 3s - loss: 12.0733 - mse: 12.0733 - mae: 1.4090 - val_loss: 8.7284 - val_mse: 8.7284 - val_mae: 1.4280 - lr: 1.8331e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 3s - loss: 12.0675 - mse: 12.0675 - mae: 1.4065 - val_loss: 8.7171 - val_mse: 8.7171 - val_mae: 1.4535 - lr: 1.8331e-04 - 3s/epoch - 3ms/step\n",
            "Score for fold 2: loss of 8.717121124267578\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 3s - loss: 11.9367 - mse: 11.9367 - mae: 1.4243 - val_loss: 9.2093 - val_mse: 9.2093 - val_mae: 1.4099 - lr: 1.8331e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 4s - loss: 11.8791 - mse: 11.8791 - mae: 1.4227 - val_loss: 9.3218 - val_mse: 9.3218 - val_mae: 1.3759 - lr: 1.8331e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 4s - loss: 11.8861 - mse: 11.8861 - mae: 1.4167 - val_loss: 9.2543 - val_mse: 9.2543 - val_mae: 1.4259 - lr: 1.8331e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 4s - loss: 11.8905 - mse: 11.8905 - mae: 1.4176 - val_loss: 9.3715 - val_mse: 9.3715 - val_mae: 1.4084 - lr: 1.8331e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 4s - loss: 11.8133 - mse: 11.8133 - mae: 1.4176 - val_loss: 9.3591 - val_mse: 9.3591 - val_mae: 1.3727 - lr: 1.8331e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 4s - loss: 11.8452 - mse: 11.8452 - mae: 1.4118 - val_loss: 9.3504 - val_mse: 9.3504 - val_mae: 1.4278 - lr: 1.8331e-04 - 4s/epoch - 4ms/step\n",
            "Score for fold 3: loss of 9.350391387939453\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 4s - loss: 9.3426 - mse: 9.3426 - mae: 1.4087 - val_loss: 19.1729 - val_mse: 19.1729 - val_mae: 1.4215 - lr: 1.8331e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 4s - loss: 9.3287 - mse: 9.3287 - mae: 1.4054 - val_loss: 19.1628 - val_mse: 19.1628 - val_mae: 1.4439 - lr: 1.8331e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 5s - loss: 9.3119 - mse: 9.3119 - mae: 1.4027 - val_loss: 19.1510 - val_mse: 19.1510 - val_mae: 1.3996 - lr: 1.8331e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 5s - loss: 9.3437 - mse: 9.3437 - mae: 1.4040 - val_loss: 19.1183 - val_mse: 19.1183 - val_mae: 1.4405 - lr: 1.8331e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 4s - loss: 9.2703 - mse: 9.2703 - mae: 1.4025 - val_loss: 19.2776 - val_mse: 19.2776 - val_mae: 1.4380 - lr: 1.8331e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 4s - loss: 9.3036 - mse: 9.3036 - mae: 1.3986 - val_loss: 19.1971 - val_mse: 19.1971 - val_mae: 1.4588 - lr: 1.8331e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 4s - loss: 9.2341 - mse: 9.2341 - mae: 1.3975 - val_loss: 19.2118 - val_mse: 19.2118 - val_mae: 1.4382 - lr: 1.8331e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 4s - loss: 9.2219 - mse: 9.2219 - mae: 1.3985 - val_loss: 19.2233 - val_mse: 19.2233 - val_mae: 1.4052 - lr: 1.8331e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 4s - loss: 9.2456 - mse: 9.2456 - mae: 1.3972 - val_loss: 19.1879 - val_mse: 19.1879 - val_mae: 1.4902 - lr: 1.8331e-04 - 4s/epoch - 4ms/step\n",
            "Score for fold 4: loss of 19.187868118286133\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 4s - loss: 12.2163 - mse: 12.2163 - mae: 1.4110 - val_loss: 7.4485 - val_mse: 7.4485 - val_mae: 1.3844 - lr: 1.8331e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 4s - loss: 12.1998 - mse: 12.1998 - mae: 1.4121 - val_loss: 7.2726 - val_mse: 7.2726 - val_mae: 1.3848 - lr: 1.8331e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 4s - loss: 12.1564 - mse: 12.1564 - mae: 1.4087 - val_loss: 7.3537 - val_mse: 7.3537 - val_mae: 1.3909 - lr: 1.8331e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 4s - loss: 12.1570 - mse: 12.1570 - mae: 1.4075 - val_loss: 7.4298 - val_mse: 7.4298 - val_mae: 1.3954 - lr: 1.8331e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 4s - loss: 12.1536 - mse: 12.1536 - mae: 1.4064 - val_loss: 7.2646 - val_mse: 7.2646 - val_mae: 1.3852 - lr: 1.8331e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 4s - loss: 12.1008 - mse: 12.1008 - mae: 1.4035 - val_loss: 7.6851 - val_mse: 7.6851 - val_mae: 1.3985 - lr: 1.8331e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 4s - loss: 12.1141 - mse: 12.1141 - mae: 1.4049 - val_loss: 7.2500 - val_mse: 7.2500 - val_mae: 1.4034 - lr: 1.8331e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 4s - loss: 12.1007 - mse: 12.1007 - mae: 1.4022 - val_loss: 7.5978 - val_mse: 7.5978 - val_mae: 1.4197 - lr: 1.8331e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 4s - loss: 12.0677 - mse: 12.0677 - mae: 1.4025 - val_loss: 7.5134 - val_mse: 7.5134 - val_mae: 1.3608 - lr: 1.8331e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 4s - loss: 12.0774 - mse: 12.0774 - mae: 1.3996 - val_loss: 7.6998 - val_mse: 7.6998 - val_mae: 1.3981 - lr: 1.8331e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 3s - loss: 12.0153 - mse: 12.0153 - mae: 1.3990 - val_loss: 7.3615 - val_mse: 7.3615 - val_mae: 1.3857 - lr: 1.8331e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 3s - loss: 12.0224 - mse: 12.0224 - mae: 1.3991 - val_loss: 7.3826 - val_mse: 7.3826 - val_mae: 1.4253 - lr: 1.8331e-04 - 3s/epoch - 3ms/step\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-10-12 01:19:38,441]\u001b[0m Finished trial#37 resulted in value: 11.500000000000002. Current best value is 9.116 with parameters: {'activation': 'relu', 'num_hidden_layer': 4, 'i': 10, 'learning_rate': 0.0002344581502453532}.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Score for fold 5: loss of 7.382609844207764\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 9s - loss: 11.9292 - mse: 11.9292 - mae: 1.5421 - val_loss: 17.8167 - val_mse: 17.8167 - val_mae: 1.4464 - lr: 4.9710e-04 - 9s/epoch - 9ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 8s - loss: 11.0941 - mse: 11.0941 - mae: 1.4909 - val_loss: 17.2042 - val_mse: 17.2042 - val_mae: 1.4443 - lr: 4.9710e-04 - 8s/epoch - 8ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 10s - loss: 10.9105 - mse: 10.9105 - mae: 1.4732 - val_loss: 17.5275 - val_mse: 17.5275 - val_mae: 1.4613 - lr: 4.9710e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 10s - loss: 10.8087 - mse: 10.8087 - mae: 1.4644 - val_loss: 17.0879 - val_mse: 17.0879 - val_mae: 1.4584 - lr: 4.9710e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 9s - loss: 10.7176 - mse: 10.7176 - mae: 1.4592 - val_loss: 17.1820 - val_mse: 17.1820 - val_mae: 1.5329 - lr: 4.9710e-04 - 9s/epoch - 9ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 9s - loss: 10.6474 - mse: 10.6474 - mae: 1.4517 - val_loss: 16.9734 - val_mse: 16.9734 - val_mae: 1.4901 - lr: 4.9710e-04 - 9s/epoch - 9ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 9s - loss: 10.4460 - mse: 10.4460 - mae: 1.4398 - val_loss: 16.8748 - val_mse: 16.8748 - val_mae: 1.5495 - lr: 4.9710e-04 - 9s/epoch - 9ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 8s - loss: 10.1219 - mse: 10.1219 - mae: 1.4262 - val_loss: 17.3419 - val_mse: 17.3419 - val_mae: 1.4824 - lr: 4.9710e-04 - 8s/epoch - 8ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 8s - loss: 10.1553 - mse: 10.1553 - mae: 1.4185 - val_loss: 16.9955 - val_mse: 16.9955 - val_mae: 1.4759 - lr: 4.9710e-04 - 8s/epoch - 8ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 9s - loss: 9.7659 - mse: 9.7659 - mae: 1.4040 - val_loss: 17.2081 - val_mse: 17.2081 - val_mae: 1.5048 - lr: 4.9710e-04 - 9s/epoch - 9ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 9s - loss: 9.5339 - mse: 9.5339 - mae: 1.3928 - val_loss: 16.8351 - val_mse: 16.8351 - val_mae: 1.4774 - lr: 4.9710e-04 - 9s/epoch - 9ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 8s - loss: 9.5308 - mse: 9.5308 - mae: 1.3849 - val_loss: 17.2506 - val_mse: 17.2506 - val_mae: 1.4509 - lr: 4.9710e-04 - 8s/epoch - 8ms/step\n",
            "Epoch 13/100\n",
            "1000/1000 - 9s - loss: 9.5311 - mse: 9.5311 - mae: 1.3781 - val_loss: 17.4515 - val_mse: 17.4515 - val_mae: 1.5089 - lr: 4.9710e-04 - 9s/epoch - 9ms/step\n",
            "Epoch 14/100\n",
            "1000/1000 - 8s - loss: 9.3099 - mse: 9.3099 - mae: 1.3581 - val_loss: 17.4793 - val_mse: 17.4793 - val_mae: 1.5479 - lr: 4.9710e-04 - 8s/epoch - 8ms/step\n",
            "Epoch 15/100\n",
            "1000/1000 - 7s - loss: 8.9234 - mse: 8.9234 - mae: 1.3513 - val_loss: 17.7231 - val_mse: 17.7231 - val_mae: 1.4510 - lr: 4.9710e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 16/100\n",
            "1000/1000 - 7s - loss: 8.6801 - mse: 8.6801 - mae: 1.3404 - val_loss: 17.8947 - val_mse: 17.8947 - val_mae: 1.5890 - lr: 4.9710e-04 - 7s/epoch - 7ms/step\n",
            "Score for fold 1: loss of 17.894718170166016\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 7s - loss: 11.7414 - mse: 11.7414 - mae: 1.3897 - val_loss: 6.4346 - val_mse: 6.4346 - val_mae: 1.2529 - lr: 4.9710e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 8s - loss: 11.3270 - mse: 11.3270 - mae: 1.3731 - val_loss: 6.5697 - val_mse: 6.5697 - val_mae: 1.3884 - lr: 4.9710e-04 - 8s/epoch - 8ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 8s - loss: 11.0891 - mse: 11.0891 - mae: 1.3619 - val_loss: 6.6256 - val_mse: 6.6256 - val_mae: 1.3907 - lr: 4.9710e-04 - 8s/epoch - 8ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 8s - loss: 10.8219 - mse: 10.8219 - mae: 1.3447 - val_loss: 6.6662 - val_mse: 6.6662 - val_mae: 1.3860 - lr: 4.9710e-04 - 8s/epoch - 8ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 8s - loss: 10.5661 - mse: 10.5661 - mae: 1.3278 - val_loss: 6.7614 - val_mse: 6.7614 - val_mae: 1.3852 - lr: 4.9710e-04 - 8s/epoch - 8ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 8s - loss: 10.3008 - mse: 10.3008 - mae: 1.3170 - val_loss: 6.7524 - val_mse: 6.7524 - val_mae: 1.3614 - lr: 4.9710e-04 - 8s/epoch - 8ms/step\n",
            "Score for fold 2: loss of 6.752439975738525\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 8s - loss: 8.6086 - mse: 8.6086 - mae: 1.3258 - val_loss: 13.7211 - val_mse: 13.7211 - val_mae: 1.4657 - lr: 4.9710e-04 - 8s/epoch - 8ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 8s - loss: 8.3066 - mse: 8.3066 - mae: 1.3126 - val_loss: 13.3638 - val_mse: 13.3638 - val_mae: 1.4133 - lr: 4.9710e-04 - 8s/epoch - 8ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 7s - loss: 7.9429 - mse: 7.9429 - mae: 1.2902 - val_loss: 14.4146 - val_mse: 14.4146 - val_mae: 1.4298 - lr: 4.9710e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 8s - loss: 8.0063 - mse: 8.0063 - mae: 1.2799 - val_loss: 13.9359 - val_mse: 13.9359 - val_mae: 1.3685 - lr: 4.9710e-04 - 8s/epoch - 8ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 8s - loss: 8.1957 - mse: 8.1957 - mae: 1.2644 - val_loss: 13.8274 - val_mse: 13.8274 - val_mae: 1.3689 - lr: 4.9710e-04 - 8s/epoch - 8ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 8s - loss: 7.5251 - mse: 7.5251 - mae: 1.2536 - val_loss: 14.6825 - val_mse: 14.6825 - val_mae: 1.4921 - lr: 4.9710e-04 - 8s/epoch - 8ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 8s - loss: 7.5563 - mse: 7.5563 - mae: 1.2427 - val_loss: 14.1621 - val_mse: 14.1621 - val_mae: 1.4889 - lr: 4.9710e-04 - 8s/epoch - 8ms/step\n",
            "Score for fold 3: loss of 14.162103652954102\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 8s - loss: 9.3510 - mse: 9.3510 - mae: 1.2900 - val_loss: 6.2661 - val_mse: 6.2661 - val_mae: 1.1893 - lr: 4.9710e-04 - 8s/epoch - 8ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 9s - loss: 9.1467 - mse: 9.1467 - mae: 1.2747 - val_loss: 6.3592 - val_mse: 6.3592 - val_mae: 1.2178 - lr: 4.9710e-04 - 9s/epoch - 9ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 8s - loss: 9.0782 - mse: 9.0782 - mae: 1.2615 - val_loss: 6.6896 - val_mse: 6.6896 - val_mae: 1.2920 - lr: 4.9710e-04 - 8s/epoch - 8ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 8s - loss: 8.6640 - mse: 8.6640 - mae: 1.2473 - val_loss: 6.4779 - val_mse: 6.4779 - val_mae: 1.1799 - lr: 4.9710e-04 - 8s/epoch - 8ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 7s - loss: 8.6509 - mse: 8.6509 - mae: 1.2353 - val_loss: 6.4677 - val_mse: 6.4677 - val_mae: 1.2802 - lr: 4.9710e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 8s - loss: 8.2271 - mse: 8.2271 - mae: 1.2239 - val_loss: 6.7387 - val_mse: 6.7387 - val_mae: 1.2680 - lr: 4.9710e-04 - 8s/epoch - 8ms/step\n",
            "Score for fold 4: loss of 6.738681793212891\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 8s - loss: 8.9934 - mse: 8.9934 - mae: 1.2471 - val_loss: 4.4199 - val_mse: 4.4199 - val_mae: 1.1958 - lr: 4.9710e-04 - 8s/epoch - 8ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 7s - loss: 8.4822 - mse: 8.4822 - mae: 1.2264 - val_loss: 5.2024 - val_mse: 5.2024 - val_mae: 1.2244 - lr: 4.9710e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 7s - loss: 8.4219 - mse: 8.4219 - mae: 1.2135 - val_loss: 4.9440 - val_mse: 4.9440 - val_mae: 1.1560 - lr: 4.9710e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 6s - loss: 8.2863 - mse: 8.2863 - mae: 1.2103 - val_loss: 4.9418 - val_mse: 4.9418 - val_mae: 1.2266 - lr: 4.9710e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 6s - loss: 8.0170 - mse: 8.0170 - mae: 1.1958 - val_loss: 4.9632 - val_mse: 4.9632 - val_mae: 1.1807 - lr: 4.9710e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 6s - loss: 7.7521 - mse: 7.7521 - mae: 1.1728 - val_loss: 5.9606 - val_mse: 5.9606 - val_mae: 1.2616 - lr: 4.9710e-04 - 6s/epoch - 6ms/step\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-10-12 01:25:12,916]\u001b[0m Finished trial#38 resulted in value: 10.3. Current best value is 9.116 with parameters: {'activation': 'relu', 'num_hidden_layer': 4, 'i': 10, 'learning_rate': 0.0002344581502453532}.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Score for fold 5: loss of 5.9605865478515625\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 10s - loss: 12.5873 - mse: 12.5873 - mae: 1.5287 - val_loss: 14.8275 - val_mse: 14.8275 - val_mae: 1.5786 - lr: 2.5321e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 8s - loss: 11.8222 - mse: 11.8222 - mae: 1.4768 - val_loss: 14.1700 - val_mse: 14.1700 - val_mae: 1.4838 - lr: 2.5321e-04 - 8s/epoch - 8ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 8s - loss: 11.5840 - mse: 11.5840 - mae: 1.4588 - val_loss: 13.8743 - val_mse: 13.8743 - val_mae: 1.5680 - lr: 2.5321e-04 - 8s/epoch - 8ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 7s - loss: 11.5911 - mse: 11.5911 - mae: 1.4504 - val_loss: 13.7851 - val_mse: 13.7851 - val_mae: 1.4663 - lr: 2.5321e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 8s - loss: 11.5106 - mse: 11.5106 - mae: 1.4445 - val_loss: 13.6635 - val_mse: 13.6635 - val_mae: 1.4781 - lr: 2.5321e-04 - 8s/epoch - 8ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 7s - loss: 11.3417 - mse: 11.3417 - mae: 1.4317 - val_loss: 13.6337 - val_mse: 13.6337 - val_mae: 1.4506 - lr: 2.5321e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 7s - loss: 11.2127 - mse: 11.2127 - mae: 1.4245 - val_loss: 13.6576 - val_mse: 13.6576 - val_mae: 1.5861 - lr: 2.5321e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 7s - loss: 11.2006 - mse: 11.2006 - mae: 1.4176 - val_loss: 13.3959 - val_mse: 13.3959 - val_mae: 1.4984 - lr: 2.5321e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 8s - loss: 11.0901 - mse: 11.0901 - mae: 1.4133 - val_loss: 13.6034 - val_mse: 13.6034 - val_mae: 1.4822 - lr: 2.5321e-04 - 8s/epoch - 8ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 8s - loss: 10.9194 - mse: 10.9194 - mae: 1.4084 - val_loss: 13.4739 - val_mse: 13.4739 - val_mae: 1.5053 - lr: 2.5321e-04 - 8s/epoch - 8ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 7s - loss: 10.5891 - mse: 10.5891 - mae: 1.3972 - val_loss: 15.1304 - val_mse: 15.1304 - val_mae: 1.5816 - lr: 2.5321e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 7s - loss: 10.6222 - mse: 10.6222 - mae: 1.3931 - val_loss: 13.4540 - val_mse: 13.4540 - val_mae: 1.4884 - lr: 2.5321e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 13/100\n",
            "1000/1000 - 7s - loss: 10.4758 - mse: 10.4758 - mae: 1.3810 - val_loss: 13.3768 - val_mse: 13.3768 - val_mae: 1.5853 - lr: 2.5321e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 14/100\n",
            "1000/1000 - 7s - loss: 10.4287 - mse: 10.4287 - mae: 1.3744 - val_loss: 14.6759 - val_mse: 14.6759 - val_mae: 1.5622 - lr: 2.5321e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 15/100\n",
            "1000/1000 - 8s - loss: 10.2187 - mse: 10.2187 - mae: 1.3667 - val_loss: 14.9843 - val_mse: 14.9843 - val_mae: 1.5655 - lr: 2.5321e-04 - 8s/epoch - 8ms/step\n",
            "Epoch 16/100\n",
            "1000/1000 - 9s - loss: 10.0938 - mse: 10.0938 - mae: 1.3597 - val_loss: 13.3352 - val_mse: 13.3352 - val_mae: 1.4774 - lr: 2.5321e-04 - 9s/epoch - 9ms/step\n",
            "Epoch 17/100\n",
            "1000/1000 - 9s - loss: 10.0216 - mse: 10.0216 - mae: 1.3504 - val_loss: 13.3678 - val_mse: 13.3678 - val_mae: 1.5100 - lr: 2.5321e-04 - 9s/epoch - 9ms/step\n",
            "Epoch 18/100\n",
            "1000/1000 - 8s - loss: 9.8391 - mse: 9.8391 - mae: 1.3376 - val_loss: 13.9249 - val_mse: 13.9249 - val_mae: 1.5491 - lr: 2.5321e-04 - 8s/epoch - 8ms/step\n",
            "Epoch 19/100\n",
            "1000/1000 - 8s - loss: 9.6525 - mse: 9.6525 - mae: 1.3304 - val_loss: 13.7321 - val_mse: 13.7321 - val_mae: 1.5096 - lr: 2.5321e-04 - 8s/epoch - 8ms/step\n",
            "Epoch 20/100\n",
            "1000/1000 - 9s - loss: 9.3782 - mse: 9.3782 - mae: 1.3182 - val_loss: 14.0182 - val_mse: 14.0182 - val_mae: 1.4941 - lr: 2.5321e-04 - 9s/epoch - 9ms/step\n",
            "Epoch 21/100\n",
            "1000/1000 - 9s - loss: 9.3709 - mse: 9.3709 - mae: 1.3123 - val_loss: 14.0239 - val_mse: 14.0239 - val_mae: 1.5100 - lr: 2.5321e-04 - 9s/epoch - 9ms/step\n",
            "Score for fold 1: loss of 14.023865699768066\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 9s - loss: 11.3108 - mse: 11.3108 - mae: 1.3571 - val_loss: 5.9948 - val_mse: 5.9948 - val_mae: 1.2940 - lr: 2.5321e-04 - 9s/epoch - 9ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 9s - loss: 11.2936 - mse: 11.2936 - mae: 1.3433 - val_loss: 6.2762 - val_mse: 6.2762 - val_mae: 1.3123 - lr: 2.5321e-04 - 9s/epoch - 9ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 9s - loss: 10.9943 - mse: 10.9943 - mae: 1.3339 - val_loss: 6.1840 - val_mse: 6.1840 - val_mae: 1.3196 - lr: 2.5321e-04 - 9s/epoch - 9ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 9s - loss: 10.8496 - mse: 10.8496 - mae: 1.3175 - val_loss: 6.4930 - val_mse: 6.4930 - val_mae: 1.2986 - lr: 2.5321e-04 - 9s/epoch - 9ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 9s - loss: 10.6197 - mse: 10.6197 - mae: 1.3056 - val_loss: 6.4809 - val_mse: 6.4809 - val_mae: 1.3259 - lr: 2.5321e-04 - 9s/epoch - 9ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 9s - loss: 10.4408 - mse: 10.4408 - mae: 1.2919 - val_loss: 6.2145 - val_mse: 6.2145 - val_mae: 1.3744 - lr: 2.5321e-04 - 9s/epoch - 9ms/step\n",
            "Score for fold 2: loss of 6.214478492736816\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 10s - loss: 10.9499 - mse: 10.9499 - mae: 1.3145 - val_loss: 5.2410 - val_mse: 5.2410 - val_mae: 1.2785 - lr: 2.5321e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 10s - loss: 10.2734 - mse: 10.2734 - mae: 1.2950 - val_loss: 5.4157 - val_mse: 5.4157 - val_mae: 1.2400 - lr: 2.5321e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 10s - loss: 10.1078 - mse: 10.1078 - mae: 1.2762 - val_loss: 5.3807 - val_mse: 5.3807 - val_mae: 1.2297 - lr: 2.5321e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 10s - loss: 9.9722 - mse: 9.9722 - mae: 1.2639 - val_loss: 5.4939 - val_mse: 5.4939 - val_mae: 1.2574 - lr: 2.5321e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 10s - loss: 9.7687 - mse: 9.7687 - mae: 1.2491 - val_loss: 5.4457 - val_mse: 5.4457 - val_mae: 1.3286 - lr: 2.5321e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 10s - loss: 9.5834 - mse: 9.5834 - mae: 1.2375 - val_loss: 5.4474 - val_mse: 5.4474 - val_mae: 1.2743 - lr: 2.5321e-04 - 10s/epoch - 10ms/step\n",
            "Score for fold 3: loss of 5.4473772048950195\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 10s - loss: 6.6189 - mse: 6.6189 - mae: 1.2442 - val_loss: 16.7674 - val_mse: 16.7674 - val_mae: 1.1876 - lr: 2.5321e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 10s - loss: 6.3298 - mse: 6.3298 - mae: 1.2229 - val_loss: 16.7651 - val_mse: 16.7651 - val_mae: 1.2549 - lr: 2.5321e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 10s - loss: 6.1749 - mse: 6.1749 - mae: 1.2061 - val_loss: 18.7786 - val_mse: 18.7786 - val_mae: 1.3064 - lr: 2.5321e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 10s - loss: 6.0369 - mse: 6.0369 - mae: 1.1884 - val_loss: 17.2303 - val_mse: 17.2303 - val_mae: 1.2402 - lr: 2.5321e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 10s - loss: 5.8121 - mse: 5.8121 - mae: 1.1719 - val_loss: 17.9986 - val_mse: 17.9986 - val_mae: 1.3153 - lr: 2.5321e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 10s - loss: 5.6909 - mse: 5.6909 - mae: 1.1599 - val_loss: 17.7033 - val_mse: 17.7033 - val_mae: 1.2598 - lr: 2.5321e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 10s - loss: 5.4516 - mse: 5.4516 - mae: 1.1377 - val_loss: 17.6100 - val_mse: 17.6100 - val_mae: 1.3671 - lr: 2.5321e-04 - 10s/epoch - 10ms/step\n",
            "Score for fold 4: loss of 17.6099796295166\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 10s - loss: 8.6747 - mse: 8.6747 - mae: 1.1831 - val_loss: 4.6229 - val_mse: 4.6229 - val_mae: 1.0874 - lr: 2.5321e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 10s - loss: 8.4897 - mse: 8.4897 - mae: 1.1615 - val_loss: 4.8395 - val_mse: 4.8395 - val_mae: 1.1163 - lr: 2.5321e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 10s - loss: 8.3416 - mse: 8.3416 - mae: 1.1373 - val_loss: 4.7513 - val_mse: 4.7513 - val_mae: 1.1260 - lr: 2.5321e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 10s - loss: 8.0178 - mse: 8.0178 - mae: 1.1230 - val_loss: 5.0758 - val_mse: 5.0758 - val_mae: 1.1495 - lr: 2.5321e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 10s - loss: 7.8871 - mse: 7.8871 - mae: 1.1044 - val_loss: 5.0333 - val_mse: 5.0333 - val_mae: 1.1817 - lr: 2.5321e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 10s - loss: 7.8473 - mse: 7.8473 - mae: 1.0901 - val_loss: 4.9275 - val_mse: 4.9275 - val_mae: 1.1576 - lr: 2.5321e-04 - 10s/epoch - 10ms/step\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-10-12 01:32:13,283]\u001b[0m Finished trial#39 resulted in value: 9.644. Current best value is 9.116 with parameters: {'activation': 'relu', 'num_hidden_layer': 4, 'i': 10, 'learning_rate': 0.0002344581502453532}.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Score for fold 5: loss of 4.927478790283203\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 3s - loss: 15.9473 - mse: 15.9473 - mae: 1.6496 - val_loss: 11.4240 - val_mse: 11.4240 - val_mae: 1.5214 - lr: 2.4453e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 14.3079 - mse: 14.3079 - mae: 1.5284 - val_loss: 11.2689 - val_mse: 11.2689 - val_mae: 1.5492 - lr: 2.4453e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 14.1261 - mse: 14.1261 - mae: 1.5208 - val_loss: 11.1674 - val_mse: 11.1674 - val_mae: 1.5430 - lr: 2.4453e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 14.0052 - mse: 14.0052 - mae: 1.5101 - val_loss: 11.0783 - val_mse: 11.0783 - val_mae: 1.5125 - lr: 2.4453e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 13.8900 - mse: 13.8900 - mae: 1.5058 - val_loss: 11.1431 - val_mse: 11.1431 - val_mae: 1.5131 - lr: 2.4453e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 13.8151 - mse: 13.8151 - mae: 1.4946 - val_loss: 10.9871 - val_mse: 10.9871 - val_mae: 1.4780 - lr: 2.4453e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 13.7679 - mse: 13.7679 - mae: 1.4892 - val_loss: 10.9279 - val_mse: 10.9279 - val_mae: 1.5357 - lr: 2.4453e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 2s - loss: 13.7189 - mse: 13.7189 - mae: 1.4916 - val_loss: 10.8678 - val_mse: 10.8678 - val_mae: 1.4913 - lr: 2.4453e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 2s - loss: 13.6504 - mse: 13.6504 - mae: 1.4862 - val_loss: 10.8351 - val_mse: 10.8351 - val_mae: 1.4704 - lr: 2.4453e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 2s - loss: 13.6406 - mse: 13.6406 - mae: 1.4821 - val_loss: 10.8277 - val_mse: 10.8277 - val_mae: 1.4650 - lr: 2.4453e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 2s - loss: 13.6162 - mse: 13.6162 - mae: 1.4796 - val_loss: 10.7080 - val_mse: 10.7080 - val_mae: 1.4855 - lr: 2.4453e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 2s - loss: 13.5859 - mse: 13.5859 - mae: 1.4754 - val_loss: 10.6698 - val_mse: 10.6698 - val_mae: 1.4908 - lr: 2.4453e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 13/100\n",
            "1000/1000 - 2s - loss: 13.5537 - mse: 13.5537 - mae: 1.4754 - val_loss: 10.6081 - val_mse: 10.6081 - val_mae: 1.4839 - lr: 2.4453e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 14/100\n",
            "1000/1000 - 2s - loss: 13.5142 - mse: 13.5142 - mae: 1.4719 - val_loss: 10.6339 - val_mse: 10.6339 - val_mae: 1.4631 - lr: 2.4453e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 15/100\n",
            "1000/1000 - 2s - loss: 13.4775 - mse: 13.4775 - mae: 1.4695 - val_loss: 10.5905 - val_mse: 10.5905 - val_mae: 1.4988 - lr: 2.4453e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 16/100\n",
            "1000/1000 - 2s - loss: 13.4331 - mse: 13.4331 - mae: 1.4687 - val_loss: 10.5382 - val_mse: 10.5382 - val_mae: 1.4678 - lr: 2.4453e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 17/100\n",
            "1000/1000 - 2s - loss: 13.4400 - mse: 13.4400 - mae: 1.4661 - val_loss: 10.5301 - val_mse: 10.5301 - val_mae: 1.4869 - lr: 2.4453e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 18/100\n",
            "1000/1000 - 2s - loss: 13.4147 - mse: 13.4147 - mae: 1.4651 - val_loss: 10.4350 - val_mse: 10.4350 - val_mae: 1.4740 - lr: 2.4453e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 19/100\n",
            "1000/1000 - 2s - loss: 13.3768 - mse: 13.3768 - mae: 1.4608 - val_loss: 10.4794 - val_mse: 10.4794 - val_mae: 1.4492 - lr: 2.4453e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 20/100\n",
            "1000/1000 - 2s - loss: 13.3167 - mse: 13.3167 - mae: 1.4597 - val_loss: 10.5087 - val_mse: 10.5087 - val_mae: 1.4600 - lr: 2.4453e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 21/100\n",
            "1000/1000 - 2s - loss: 13.3346 - mse: 13.3346 - mae: 1.4595 - val_loss: 10.4369 - val_mse: 10.4369 - val_mae: 1.4876 - lr: 2.4453e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 22/100\n",
            "1000/1000 - 2s - loss: 13.3103 - mse: 13.3103 - mae: 1.4567 - val_loss: 10.4918 - val_mse: 10.4918 - val_mae: 1.4609 - lr: 2.4453e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 23/100\n",
            "1000/1000 - 2s - loss: 13.2861 - mse: 13.2861 - mae: 1.4562 - val_loss: 10.4449 - val_mse: 10.4449 - val_mae: 1.4686 - lr: 2.4453e-04 - 2s/epoch - 2ms/step\n",
            "Score for fold 1: loss of 10.444896697998047\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 14.0451 - mse: 14.0451 - mae: 1.4743 - val_loss: 7.3113 - val_mse: 7.3113 - val_mae: 1.3860 - lr: 2.4453e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 14.0204 - mse: 14.0204 - mae: 1.4729 - val_loss: 7.2960 - val_mse: 7.2960 - val_mae: 1.4227 - lr: 2.4453e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 13.9880 - mse: 13.9880 - mae: 1.4723 - val_loss: 7.2678 - val_mse: 7.2678 - val_mae: 1.3842 - lr: 2.4453e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 13.9817 - mse: 13.9817 - mae: 1.4695 - val_loss: 7.2555 - val_mse: 7.2555 - val_mae: 1.3678 - lr: 2.4453e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 13.9441 - mse: 13.9441 - mae: 1.4683 - val_loss: 7.2414 - val_mse: 7.2414 - val_mae: 1.3933 - lr: 2.4453e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 13.9089 - mse: 13.9089 - mae: 1.4665 - val_loss: 7.2743 - val_mse: 7.2743 - val_mae: 1.3766 - lr: 2.4453e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 13.8744 - mse: 13.8744 - mae: 1.4675 - val_loss: 7.2596 - val_mse: 7.2596 - val_mae: 1.3686 - lr: 2.4453e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 2s - loss: 13.8600 - mse: 13.8600 - mae: 1.4639 - val_loss: 7.2554 - val_mse: 7.2554 - val_mae: 1.3760 - lr: 2.4453e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 2s - loss: 13.8412 - mse: 13.8412 - mae: 1.4640 - val_loss: 7.2160 - val_mse: 7.2160 - val_mae: 1.3964 - lr: 2.4453e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 2s - loss: 13.8059 - mse: 13.8059 - mae: 1.4642 - val_loss: 7.2232 - val_mse: 7.2232 - val_mae: 1.3993 - lr: 2.4453e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 2s - loss: 13.7923 - mse: 13.7923 - mae: 1.4636 - val_loss: 7.2036 - val_mse: 7.2036 - val_mae: 1.3936 - lr: 2.4453e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 2s - loss: 13.7975 - mse: 13.7975 - mae: 1.4620 - val_loss: 7.2290 - val_mse: 7.2290 - val_mae: 1.3742 - lr: 2.4453e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 13/100\n",
            "1000/1000 - 2s - loss: 13.7867 - mse: 13.7867 - mae: 1.4588 - val_loss: 7.2342 - val_mse: 7.2342 - val_mae: 1.3962 - lr: 2.4453e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 14/100\n",
            "1000/1000 - 2s - loss: 13.7398 - mse: 13.7398 - mae: 1.4604 - val_loss: 7.2743 - val_mse: 7.2743 - val_mae: 1.3909 - lr: 2.4453e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 15/100\n",
            "1000/1000 - 2s - loss: 13.7205 - mse: 13.7205 - mae: 1.4610 - val_loss: 7.2593 - val_mse: 7.2593 - val_mae: 1.4331 - lr: 2.4453e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 16/100\n",
            "1000/1000 - 2s - loss: 13.7158 - mse: 13.7158 - mae: 1.4598 - val_loss: 7.2479 - val_mse: 7.2479 - val_mae: 1.3886 - lr: 2.4453e-04 - 2s/epoch - 2ms/step\n",
            "Score for fold 2: loss of 7.2479071617126465\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 12.9965 - mse: 12.9965 - mae: 1.4441 - val_loss: 10.1243 - val_mse: 10.1243 - val_mae: 1.4344 - lr: 2.4453e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 12.9921 - mse: 12.9921 - mae: 1.4420 - val_loss: 10.2228 - val_mse: 10.2228 - val_mae: 1.4410 - lr: 2.4453e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 12.9625 - mse: 12.9625 - mae: 1.4406 - val_loss: 10.1591 - val_mse: 10.1591 - val_mae: 1.4408 - lr: 2.4453e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 12.9335 - mse: 12.9335 - mae: 1.4401 - val_loss: 10.1730 - val_mse: 10.1730 - val_mae: 1.4402 - lr: 2.4453e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 12.8952 - mse: 12.8952 - mae: 1.4391 - val_loss: 10.2189 - val_mse: 10.2189 - val_mae: 1.4480 - lr: 2.4453e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 12.8849 - mse: 12.8849 - mae: 1.4396 - val_loss: 10.1845 - val_mse: 10.1845 - val_mae: 1.4378 - lr: 2.4453e-04 - 2s/epoch - 2ms/step\n",
            "Score for fold 3: loss of 10.184453964233398\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 9.8385 - mse: 9.8385 - mae: 1.4243 - val_loss: 22.3278 - val_mse: 22.3278 - val_mae: 1.4748 - lr: 2.4453e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 9.8416 - mse: 9.8416 - mae: 1.4224 - val_loss: 22.4509 - val_mse: 22.4509 - val_mae: 1.4758 - lr: 2.4453e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 9.7931 - mse: 9.7931 - mae: 1.4190 - val_loss: 22.3044 - val_mse: 22.3044 - val_mae: 1.4871 - lr: 2.4453e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 9.7820 - mse: 9.7820 - mae: 1.4216 - val_loss: 22.3297 - val_mse: 22.3297 - val_mae: 1.5017 - lr: 2.4453e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 9.7701 - mse: 9.7701 - mae: 1.4192 - val_loss: 22.2935 - val_mse: 22.2935 - val_mae: 1.4972 - lr: 2.4453e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 9.7570 - mse: 9.7570 - mae: 1.4185 - val_loss: 22.3908 - val_mse: 22.3908 - val_mae: 1.4724 - lr: 2.4453e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 9.7513 - mse: 9.7513 - mae: 1.4160 - val_loss: 22.3286 - val_mse: 22.3286 - val_mae: 1.4734 - lr: 2.4453e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 2s - loss: 9.7590 - mse: 9.7590 - mae: 1.4169 - val_loss: 22.3028 - val_mse: 22.3028 - val_mae: 1.5055 - lr: 2.4453e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 2s - loss: 9.7301 - mse: 9.7301 - mae: 1.4180 - val_loss: 22.3310 - val_mse: 22.3310 - val_mae: 1.5032 - lr: 2.4453e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 2s - loss: 9.7226 - mse: 9.7226 - mae: 1.4187 - val_loss: 22.3147 - val_mse: 22.3147 - val_mae: 1.4803 - lr: 2.4453e-04 - 2s/epoch - 2ms/step\n",
            "Score for fold 4: loss of 22.314687728881836\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 12.2763 - mse: 12.2763 - mae: 1.4331 - val_loss: 12.1128 - val_mse: 12.1128 - val_mae: 1.4475 - lr: 2.4453e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 12.2366 - mse: 12.2366 - mae: 1.4332 - val_loss: 12.1728 - val_mse: 12.1728 - val_mae: 1.4349 - lr: 2.4453e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 12.2450 - mse: 12.2450 - mae: 1.4314 - val_loss: 12.2751 - val_mse: 12.2751 - val_mae: 1.4929 - lr: 2.4453e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 12.2226 - mse: 12.2226 - mae: 1.4322 - val_loss: 12.3581 - val_mse: 12.3581 - val_mae: 1.4269 - lr: 2.4453e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 12.2436 - mse: 12.2436 - mae: 1.4265 - val_loss: 12.2565 - val_mse: 12.2565 - val_mae: 1.4661 - lr: 2.4453e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 12.2099 - mse: 12.2099 - mae: 1.4300 - val_loss: 12.3292 - val_mse: 12.3292 - val_mae: 1.4284 - lr: 2.4453e-04 - 2s/epoch - 2ms/step\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-10-12 01:34:43,295]\u001b[0m Finished trial#40 resulted in value: 12.501999999999999. Current best value is 9.116 with parameters: {'activation': 'relu', 'num_hidden_layer': 4, 'i': 10, 'learning_rate': 0.0002344581502453532}.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Score for fold 5: loss of 12.329204559326172\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 9s - loss: 11.9007 - mse: 11.9007 - mae: 1.5400 - val_loss: 18.1548 - val_mse: 18.1548 - val_mae: 1.5620 - lr: 2.5249e-04 - 9s/epoch - 9ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 8s - loss: 10.9106 - mse: 10.9106 - mae: 1.4755 - val_loss: 17.9435 - val_mse: 17.9435 - val_mae: 1.5327 - lr: 2.5249e-04 - 8s/epoch - 8ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 8s - loss: 10.5295 - mse: 10.5295 - mae: 1.4591 - val_loss: 18.3231 - val_mse: 18.3231 - val_mae: 1.5301 - lr: 2.5249e-04 - 8s/epoch - 8ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 8s - loss: 10.5402 - mse: 10.5402 - mae: 1.4519 - val_loss: 18.0433 - val_mse: 18.0433 - val_mae: 1.4473 - lr: 2.5249e-04 - 8s/epoch - 8ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 8s - loss: 10.3839 - mse: 10.3839 - mae: 1.4412 - val_loss: 18.2409 - val_mse: 18.2409 - val_mae: 1.4156 - lr: 2.5249e-04 - 8s/epoch - 8ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 8s - loss: 10.3112 - mse: 10.3112 - mae: 1.4379 - val_loss: 18.0255 - val_mse: 18.0255 - val_mae: 1.4564 - lr: 2.5249e-04 - 8s/epoch - 8ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 8s - loss: 10.2338 - mse: 10.2338 - mae: 1.4311 - val_loss: 18.1528 - val_mse: 18.1528 - val_mae: 1.4570 - lr: 2.5249e-04 - 8s/epoch - 8ms/step\n",
            "Score for fold 1: loss of 18.152849197387695\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 8s - loss: 12.2559 - mse: 12.2559 - mae: 1.4436 - val_loss: 8.8916 - val_mse: 8.8916 - val_mae: 1.4122 - lr: 2.5249e-04 - 8s/epoch - 8ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 8s - loss: 12.4266 - mse: 12.4266 - mae: 1.4409 - val_loss: 8.9273 - val_mse: 8.9273 - val_mae: 1.4395 - lr: 2.5249e-04 - 8s/epoch - 8ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 8s - loss: 12.1762 - mse: 12.1762 - mae: 1.4359 - val_loss: 8.9243 - val_mse: 8.9243 - val_mae: 1.3981 - lr: 2.5249e-04 - 8s/epoch - 8ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 8s - loss: 11.9056 - mse: 11.9056 - mae: 1.4209 - val_loss: 8.9155 - val_mse: 8.9155 - val_mae: 1.4087 - lr: 2.5249e-04 - 8s/epoch - 8ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 8s - loss: 11.7597 - mse: 11.7597 - mae: 1.4178 - val_loss: 9.0954 - val_mse: 9.0954 - val_mae: 1.3878 - lr: 2.5249e-04 - 8s/epoch - 8ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 8s - loss: 11.6486 - mse: 11.6486 - mae: 1.4074 - val_loss: 9.1616 - val_mse: 9.1616 - val_mae: 1.3904 - lr: 2.5249e-04 - 8s/epoch - 8ms/step\n",
            "Score for fold 2: loss of 9.161599159240723\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 8s - loss: 11.0419 - mse: 11.0419 - mae: 1.4018 - val_loss: 11.9274 - val_mse: 11.9274 - val_mae: 1.3806 - lr: 2.5249e-04 - 8s/epoch - 8ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 8s - loss: 10.9119 - mse: 10.9119 - mae: 1.3951 - val_loss: 11.9573 - val_mse: 11.9573 - val_mae: 1.4170 - lr: 2.5249e-04 - 8s/epoch - 8ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 8s - loss: 10.6685 - mse: 10.6685 - mae: 1.3817 - val_loss: 11.4957 - val_mse: 11.4957 - val_mae: 1.4008 - lr: 2.5249e-04 - 8s/epoch - 8ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 8s - loss: 10.3982 - mse: 10.3982 - mae: 1.3756 - val_loss: 11.8723 - val_mse: 11.8723 - val_mae: 1.4473 - lr: 2.5249e-04 - 8s/epoch - 8ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 8s - loss: 10.4318 - mse: 10.4318 - mae: 1.3650 - val_loss: 11.6348 - val_mse: 11.6348 - val_mae: 1.3792 - lr: 2.5249e-04 - 8s/epoch - 8ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 8s - loss: 10.2749 - mse: 10.2749 - mae: 1.3576 - val_loss: 12.3601 - val_mse: 12.3601 - val_mae: 1.4730 - lr: 2.5249e-04 - 8s/epoch - 8ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 8s - loss: 10.3539 - mse: 10.3539 - mae: 1.3468 - val_loss: 11.7374 - val_mse: 11.7374 - val_mae: 1.4362 - lr: 2.5249e-04 - 8s/epoch - 8ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 8s - loss: 10.0915 - mse: 10.0915 - mae: 1.3354 - val_loss: 11.6214 - val_mse: 11.6214 - val_mae: 1.4390 - lr: 2.5249e-04 - 8s/epoch - 8ms/step\n",
            "Score for fold 3: loss of 11.621450424194336\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 8s - loss: 10.5019 - mse: 10.5019 - mae: 1.3516 - val_loss: 9.3964 - val_mse: 9.3964 - val_mae: 1.3465 - lr: 2.5249e-04 - 8s/epoch - 8ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 8s - loss: 10.1922 - mse: 10.1922 - mae: 1.3334 - val_loss: 9.3738 - val_mse: 9.3738 - val_mae: 1.3123 - lr: 2.5249e-04 - 8s/epoch - 8ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 7s - loss: 9.9509 - mse: 9.9509 - mae: 1.3216 - val_loss: 9.3896 - val_mse: 9.3896 - val_mae: 1.3687 - lr: 2.5249e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 7s - loss: 9.7987 - mse: 9.7987 - mae: 1.3071 - val_loss: 9.7478 - val_mse: 9.7478 - val_mae: 1.4187 - lr: 2.5249e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 7s - loss: 9.8628 - mse: 9.8628 - mae: 1.2986 - val_loss: 9.6370 - val_mse: 9.6370 - val_mae: 1.4191 - lr: 2.5249e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 7s - loss: 9.4944 - mse: 9.4944 - mae: 1.2857 - val_loss: 10.1389 - val_mse: 10.1389 - val_mae: 1.3914 - lr: 2.5249e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 7s - loss: 9.3723 - mse: 9.3723 - mae: 1.2732 - val_loss: 10.2784 - val_mse: 10.2784 - val_mae: 1.3709 - lr: 2.5249e-04 - 7s/epoch - 7ms/step\n",
            "Score for fold 4: loss of 10.278428077697754\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 7s - loss: 10.5613 - mse: 10.5613 - mae: 1.3103 - val_loss: 5.5164 - val_mse: 5.5164 - val_mae: 1.2441 - lr: 2.5249e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 7s - loss: 10.0974 - mse: 10.0974 - mae: 1.2897 - val_loss: 5.6748 - val_mse: 5.6748 - val_mae: 1.2249 - lr: 2.5249e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 7s - loss: 10.0450 - mse: 10.0450 - mae: 1.2776 - val_loss: 5.8439 - val_mse: 5.8439 - val_mae: 1.2457 - lr: 2.5249e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 7s - loss: 9.8472 - mse: 9.8472 - mae: 1.2576 - val_loss: 5.7669 - val_mse: 5.7669 - val_mae: 1.2789 - lr: 2.5249e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 7s - loss: 9.5588 - mse: 9.5588 - mae: 1.2450 - val_loss: 5.9859 - val_mse: 5.9859 - val_mae: 1.2971 - lr: 2.5249e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 7s - loss: 9.4263 - mse: 9.4263 - mae: 1.2339 - val_loss: 5.8621 - val_mse: 5.8621 - val_mae: 1.2546 - lr: 2.5249e-04 - 7s/epoch - 7ms/step\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-10-12 01:39:15,333]\u001b[0m Finished trial#41 resulted in value: 11.014. Current best value is 9.116 with parameters: {'activation': 'relu', 'num_hidden_layer': 4, 'i': 10, 'learning_rate': 0.0002344581502453532}.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Score for fold 5: loss of 5.862143516540527\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 8s - loss: 13.5010 - mse: 13.5010 - mae: 1.5412 - val_loss: 11.2026 - val_mse: 11.2026 - val_mae: 1.5139 - lr: 3.5997e-04 - 8s/epoch - 8ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 7s - loss: 12.5817 - mse: 12.5817 - mae: 1.4840 - val_loss: 11.3770 - val_mse: 11.3770 - val_mae: 1.4254 - lr: 3.5997e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 7s - loss: 12.4203 - mse: 12.4203 - mae: 1.4661 - val_loss: 11.3700 - val_mse: 11.3700 - val_mae: 1.4866 - lr: 3.5997e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 7s - loss: 12.3372 - mse: 12.3372 - mae: 1.4550 - val_loss: 11.0246 - val_mse: 11.0246 - val_mae: 1.5081 - lr: 3.5997e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 7s - loss: 12.1732 - mse: 12.1732 - mae: 1.4456 - val_loss: 11.3209 - val_mse: 11.3209 - val_mae: 1.4785 - lr: 3.5997e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 7s - loss: 12.1862 - mse: 12.1862 - mae: 1.4429 - val_loss: 11.2773 - val_mse: 11.2773 - val_mae: 1.5426 - lr: 3.5997e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 7s - loss: 11.9680 - mse: 11.9680 - mae: 1.4366 - val_loss: 11.2804 - val_mse: 11.2804 - val_mae: 1.4685 - lr: 3.5997e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 7s - loss: 11.9231 - mse: 11.9231 - mae: 1.4307 - val_loss: 10.7350 - val_mse: 10.7350 - val_mae: 1.4419 - lr: 3.5997e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 7s - loss: 11.7491 - mse: 11.7491 - mae: 1.4244 - val_loss: 10.7125 - val_mse: 10.7125 - val_mae: 1.3915 - lr: 3.5997e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 7s - loss: 11.6419 - mse: 11.6419 - mae: 1.4149 - val_loss: 11.1508 - val_mse: 11.1508 - val_mae: 1.4609 - lr: 3.5997e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 7s - loss: 11.5649 - mse: 11.5649 - mae: 1.4060 - val_loss: 10.7205 - val_mse: 10.7205 - val_mae: 1.4518 - lr: 3.5997e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 7s - loss: 11.3870 - mse: 11.3870 - mae: 1.3983 - val_loss: 10.9541 - val_mse: 10.9541 - val_mae: 1.4117 - lr: 3.5997e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 13/100\n",
            "1000/1000 - 7s - loss: 11.1766 - mse: 11.1766 - mae: 1.3898 - val_loss: 10.7861 - val_mse: 10.7861 - val_mae: 1.4213 - lr: 3.5997e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 14/100\n",
            "1000/1000 - 7s - loss: 11.0092 - mse: 11.0092 - mae: 1.3815 - val_loss: 11.3205 - val_mse: 11.3205 - val_mae: 1.5071 - lr: 3.5997e-04 - 7s/epoch - 7ms/step\n",
            "Score for fold 1: loss of 11.320521354675293\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 7s - loss: 11.2176 - mse: 11.2176 - mae: 1.3936 - val_loss: 10.1202 - val_mse: 10.1202 - val_mae: 1.4068 - lr: 3.5997e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 7s - loss: 11.1498 - mse: 11.1498 - mae: 1.3806 - val_loss: 9.9512 - val_mse: 9.9512 - val_mae: 1.4174 - lr: 3.5997e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 7s - loss: 10.7981 - mse: 10.7981 - mae: 1.3708 - val_loss: 10.3123 - val_mse: 10.3123 - val_mae: 1.3785 - lr: 3.5997e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 7s - loss: 10.7122 - mse: 10.7122 - mae: 1.3533 - val_loss: 9.8381 - val_mse: 9.8381 - val_mae: 1.4309 - lr: 3.5997e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 7s - loss: 10.6104 - mse: 10.6104 - mae: 1.3482 - val_loss: 10.3530 - val_mse: 10.3530 - val_mae: 1.4075 - lr: 3.5997e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 7s - loss: 10.3119 - mse: 10.3119 - mae: 1.3333 - val_loss: 10.2732 - val_mse: 10.2732 - val_mae: 1.4248 - lr: 3.5997e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 7s - loss: 10.1287 - mse: 10.1287 - mae: 1.3236 - val_loss: 10.2075 - val_mse: 10.2075 - val_mae: 1.4361 - lr: 3.5997e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 7s - loss: 9.9714 - mse: 9.9714 - mae: 1.3075 - val_loss: 10.4604 - val_mse: 10.4604 - val_mae: 1.4361 - lr: 3.5997e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 7s - loss: 9.8741 - mse: 9.8741 - mae: 1.2935 - val_loss: 10.3789 - val_mse: 10.3789 - val_mae: 1.4010 - lr: 3.5997e-04 - 7s/epoch - 7ms/step\n",
            "Score for fold 2: loss of 10.378861427307129\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 7s - loss: 8.2483 - mse: 8.2483 - mae: 1.3211 - val_loss: 16.5845 - val_mse: 16.5845 - val_mae: 1.3229 - lr: 3.5997e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 7s - loss: 7.9360 - mse: 7.9360 - mae: 1.3036 - val_loss: 16.8643 - val_mse: 16.8643 - val_mae: 1.3129 - lr: 3.5997e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 7s - loss: 7.9679 - mse: 7.9679 - mae: 1.2925 - val_loss: 16.7945 - val_mse: 16.7945 - val_mae: 1.3313 - lr: 3.5997e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 7s - loss: 7.4204 - mse: 7.4204 - mae: 1.2707 - val_loss: 16.7539 - val_mse: 16.7539 - val_mae: 1.3472 - lr: 3.5997e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 7s - loss: 7.2565 - mse: 7.2565 - mae: 1.2568 - val_loss: 16.5525 - val_mse: 16.5525 - val_mae: 1.3632 - lr: 3.5997e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 7s - loss: 7.2596 - mse: 7.2596 - mae: 1.2390 - val_loss: 16.9603 - val_mse: 16.9603 - val_mae: 1.3480 - lr: 3.5997e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 7s - loss: 6.9932 - mse: 6.9932 - mae: 1.2193 - val_loss: 16.8292 - val_mse: 16.8292 - val_mae: 1.3860 - lr: 3.5997e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 7s - loss: 6.7033 - mse: 6.7033 - mae: 1.2006 - val_loss: 17.1101 - val_mse: 17.1101 - val_mae: 1.3707 - lr: 3.5997e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 7s - loss: 6.5672 - mse: 6.5672 - mae: 1.1887 - val_loss: 16.9471 - val_mse: 16.9471 - val_mae: 1.4009 - lr: 3.5997e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 7s - loss: 6.2985 - mse: 6.2985 - mae: 1.1753 - val_loss: 16.9732 - val_mse: 16.9732 - val_mae: 1.3996 - lr: 3.5997e-04 - 7s/epoch - 7ms/step\n",
            "Score for fold 3: loss of 16.973194122314453\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 7s - loss: 8.9120 - mse: 8.9120 - mae: 1.2282 - val_loss: 7.1864 - val_mse: 7.1864 - val_mae: 1.1524 - lr: 3.5997e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 7s - loss: 8.8020 - mse: 8.8020 - mae: 1.2062 - val_loss: 6.3642 - val_mse: 6.3642 - val_mae: 1.1768 - lr: 3.5997e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 7s - loss: 8.4349 - mse: 8.4349 - mae: 1.1831 - val_loss: 6.7872 - val_mse: 6.7872 - val_mae: 1.2153 - lr: 3.5997e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 7s - loss: 8.1546 - mse: 8.1546 - mae: 1.1669 - val_loss: 7.0641 - val_mse: 7.0641 - val_mae: 1.2136 - lr: 3.5997e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 7s - loss: 7.9419 - mse: 7.9419 - mae: 1.1413 - val_loss: 6.9360 - val_mse: 6.9360 - val_mae: 1.1997 - lr: 3.5997e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 7s - loss: 7.7834 - mse: 7.7834 - mae: 1.1241 - val_loss: 7.4622 - val_mse: 7.4622 - val_mae: 1.2154 - lr: 3.5997e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 8s - loss: 7.5462 - mse: 7.5462 - mae: 1.1026 - val_loss: 7.5089 - val_mse: 7.5089 - val_mae: 1.2564 - lr: 3.5997e-04 - 8s/epoch - 8ms/step\n",
            "Score for fold 4: loss of 7.508919715881348\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 7s - loss: 8.3220 - mse: 8.3220 - mae: 1.1498 - val_loss: 3.9607 - val_mse: 3.9607 - val_mae: 1.0537 - lr: 3.5997e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 7s - loss: 8.1099 - mse: 8.1099 - mae: 1.1210 - val_loss: 4.0364 - val_mse: 4.0364 - val_mae: 1.0735 - lr: 3.5997e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 7s - loss: 7.8214 - mse: 7.8214 - mae: 1.0954 - val_loss: 4.0394 - val_mse: 4.0394 - val_mae: 1.0765 - lr: 3.5997e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 7s - loss: 8.1864 - mse: 8.1864 - mae: 1.0803 - val_loss: 4.2472 - val_mse: 4.2472 - val_mae: 1.1110 - lr: 3.5997e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 7s - loss: 7.4969 - mse: 7.4969 - mae: 1.0590 - val_loss: 4.1086 - val_mse: 4.1086 - val_mae: 1.1037 - lr: 3.5997e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 7s - loss: 7.1824 - mse: 7.1824 - mae: 1.0401 - val_loss: 4.4130 - val_mse: 4.4130 - val_mae: 1.1254 - lr: 3.5997e-04 - 7s/epoch - 7ms/step\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-10-12 01:44:50,393]\u001b[0m Finished trial#42 resulted in value: 10.118. Current best value is 9.116 with parameters: {'activation': 'relu', 'num_hidden_layer': 4, 'i': 10, 'learning_rate': 0.0002344581502453532}.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Score for fold 5: loss of 4.412988662719727\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 7s - loss: 14.1385 - mse: 14.1385 - mae: 1.5519 - val_loss: 9.4514 - val_mse: 9.4514 - val_mae: 1.4489 - lr: 1.7411e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 7s - loss: 13.0271 - mse: 13.0271 - mae: 1.4963 - val_loss: 9.7091 - val_mse: 9.7091 - val_mae: 1.4539 - lr: 1.7411e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 7s - loss: 12.7821 - mse: 12.7821 - mae: 1.4772 - val_loss: 9.4487 - val_mse: 9.4487 - val_mae: 1.4775 - lr: 1.7411e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 7s - loss: 12.7814 - mse: 12.7814 - mae: 1.4622 - val_loss: 10.2067 - val_mse: 10.2067 - val_mae: 1.3999 - lr: 1.7411e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 7s - loss: 12.5699 - mse: 12.5699 - mae: 1.4564 - val_loss: 9.4773 - val_mse: 9.4773 - val_mae: 1.4651 - lr: 1.7411e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 7s - loss: 12.4146 - mse: 12.4146 - mae: 1.4521 - val_loss: 9.4750 - val_mse: 9.4750 - val_mae: 1.4564 - lr: 1.7411e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 7s - loss: 12.4002 - mse: 12.4002 - mae: 1.4435 - val_loss: 9.6916 - val_mse: 9.6916 - val_mae: 1.5193 - lr: 1.7411e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 7s - loss: 12.4150 - mse: 12.4150 - mae: 1.4399 - val_loss: 9.4436 - val_mse: 9.4436 - val_mae: 1.4419 - lr: 1.7411e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 7s - loss: 12.1113 - mse: 12.1113 - mae: 1.4319 - val_loss: 9.6489 - val_mse: 9.6489 - val_mae: 1.4904 - lr: 1.7411e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 7s - loss: 12.1395 - mse: 12.1395 - mae: 1.4305 - val_loss: 9.2916 - val_mse: 9.2916 - val_mae: 1.3720 - lr: 1.7411e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 7s - loss: 11.9634 - mse: 11.9634 - mae: 1.4187 - val_loss: 9.0830 - val_mse: 9.0830 - val_mae: 1.4064 - lr: 1.7411e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 7s - loss: 11.9237 - mse: 11.9237 - mae: 1.4135 - val_loss: 9.5972 - val_mse: 9.5972 - val_mae: 1.5110 - lr: 1.7411e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 13/100\n",
            "1000/1000 - 7s - loss: 11.8242 - mse: 11.8242 - mae: 1.4098 - val_loss: 9.4569 - val_mse: 9.4569 - val_mae: 1.4649 - lr: 1.7411e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 14/100\n",
            "1000/1000 - 7s - loss: 11.5994 - mse: 11.5994 - mae: 1.3985 - val_loss: 9.8435 - val_mse: 9.8435 - val_mae: 1.4214 - lr: 1.7411e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 15/100\n",
            "1000/1000 - 7s - loss: 11.4324 - mse: 11.4324 - mae: 1.3919 - val_loss: 9.1976 - val_mse: 9.1976 - val_mae: 1.4379 - lr: 1.7411e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 16/100\n",
            "1000/1000 - 7s - loss: 11.4442 - mse: 11.4442 - mae: 1.3918 - val_loss: 9.9904 - val_mse: 9.9904 - val_mae: 1.4203 - lr: 1.7411e-04 - 7s/epoch - 7ms/step\n",
            "Score for fold 1: loss of 9.990416526794434\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 7s - loss: 11.2767 - mse: 11.2767 - mae: 1.3936 - val_loss: 9.5897 - val_mse: 9.5897 - val_mae: 1.4584 - lr: 1.7411e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 7s - loss: 11.0827 - mse: 11.0827 - mae: 1.3801 - val_loss: 9.8710 - val_mse: 9.8710 - val_mae: 1.3789 - lr: 1.7411e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 7s - loss: 10.9062 - mse: 10.9062 - mae: 1.3742 - val_loss: 9.6905 - val_mse: 9.6905 - val_mae: 1.4674 - lr: 1.7411e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 7s - loss: 10.9323 - mse: 10.9323 - mae: 1.3660 - val_loss: 10.3781 - val_mse: 10.3781 - val_mae: 1.4727 - lr: 1.7411e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 7s - loss: 10.7459 - mse: 10.7459 - mae: 1.3587 - val_loss: 10.2927 - val_mse: 10.2927 - val_mae: 1.4126 - lr: 1.7411e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 7s - loss: 10.5418 - mse: 10.5418 - mae: 1.3497 - val_loss: 10.0835 - val_mse: 10.0835 - val_mae: 1.3696 - lr: 1.7411e-04 - 7s/epoch - 7ms/step\n",
            "Score for fold 2: loss of 10.08344841003418\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 7s - loss: 8.2329 - mse: 8.2329 - mae: 1.3570 - val_loss: 19.3971 - val_mse: 19.3971 - val_mae: 1.3661 - lr: 1.7411e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 7s - loss: 8.1880 - mse: 8.1880 - mae: 1.3473 - val_loss: 18.8013 - val_mse: 18.8013 - val_mae: 1.3443 - lr: 1.7411e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 7s - loss: 8.0134 - mse: 8.0134 - mae: 1.3377 - val_loss: 19.1408 - val_mse: 19.1408 - val_mae: 1.3576 - lr: 1.7411e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 7s - loss: 7.9606 - mse: 7.9606 - mae: 1.3276 - val_loss: 19.1429 - val_mse: 19.1429 - val_mae: 1.4059 - lr: 1.7411e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 7s - loss: 7.7058 - mse: 7.7058 - mae: 1.3170 - val_loss: 19.6748 - val_mse: 19.6748 - val_mae: 1.4075 - lr: 1.7411e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 7s - loss: 7.7059 - mse: 7.7059 - mae: 1.3085 - val_loss: 19.0527 - val_mse: 19.0527 - val_mae: 1.3824 - lr: 1.7411e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 7s - loss: 7.4179 - mse: 7.4179 - mae: 1.2940 - val_loss: 19.1066 - val_mse: 19.1066 - val_mae: 1.4534 - lr: 1.7411e-04 - 7s/epoch - 7ms/step\n",
            "Score for fold 3: loss of 19.106582641601562\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 7s - loss: 10.2836 - mse: 10.2836 - mae: 1.3204 - val_loss: 8.6306 - val_mse: 8.6306 - val_mae: 1.2832 - lr: 1.7411e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 7s - loss: 10.0524 - mse: 10.0524 - mae: 1.3068 - val_loss: 8.3446 - val_mse: 8.3446 - val_mae: 1.2993 - lr: 1.7411e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 7s - loss: 9.7357 - mse: 9.7357 - mae: 1.2916 - val_loss: 8.5137 - val_mse: 8.5137 - val_mae: 1.3269 - lr: 1.7411e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 7s - loss: 9.6172 - mse: 9.6172 - mae: 1.2805 - val_loss: 8.7603 - val_mse: 8.7603 - val_mae: 1.2735 - lr: 1.7411e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 7s - loss: 9.5881 - mse: 9.5881 - mae: 1.2724 - val_loss: 9.4173 - val_mse: 9.4173 - val_mae: 1.3511 - lr: 1.7411e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 7s - loss: 9.4041 - mse: 9.4041 - mae: 1.2617 - val_loss: 8.7936 - val_mse: 8.7936 - val_mae: 1.3148 - lr: 1.7411e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 7s - loss: 9.3125 - mse: 9.3125 - mae: 1.2508 - val_loss: 8.7119 - val_mse: 8.7119 - val_mae: 1.3349 - lr: 1.7411e-04 - 7s/epoch - 7ms/step\n",
            "Score for fold 4: loss of 8.711861610412598\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 7s - loss: 10.0012 - mse: 10.0012 - mae: 1.2700 - val_loss: 4.7124 - val_mse: 4.7124 - val_mae: 1.1864 - lr: 1.7411e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 7s - loss: 9.9483 - mse: 9.9483 - mae: 1.2585 - val_loss: 5.1280 - val_mse: 5.1280 - val_mae: 1.2159 - lr: 1.7411e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 7s - loss: 9.9287 - mse: 9.9287 - mae: 1.2445 - val_loss: 5.1260 - val_mse: 5.1260 - val_mae: 1.2719 - lr: 1.7411e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 7s - loss: 9.5592 - mse: 9.5592 - mae: 1.2321 - val_loss: 5.1210 - val_mse: 5.1210 - val_mae: 1.2246 - lr: 1.7411e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 7s - loss: 9.3677 - mse: 9.3677 - mae: 1.2191 - val_loss: 5.3697 - val_mse: 5.3697 - val_mae: 1.3085 - lr: 1.7411e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 7s - loss: 9.4343 - mse: 9.4343 - mae: 1.2080 - val_loss: 5.2653 - val_mse: 5.2653 - val_mae: 1.2365 - lr: 1.7411e-04 - 7s/epoch - 7ms/step\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-10-12 01:49:49,057]\u001b[0m Finished trial#43 resulted in value: 10.632. Current best value is 9.116 with parameters: {'activation': 'relu', 'num_hidden_layer': 4, 'i': 10, 'learning_rate': 0.0002344581502453532}.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Score for fold 5: loss of 5.265312194824219\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 8s - loss: 13.0324 - mse: 13.0324 - mae: 1.5421 - val_loss: 12.8240 - val_mse: 12.8240 - val_mae: 1.5050 - lr: 7.5762e-04 - 8s/epoch - 8ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 7s - loss: 12.2145 - mse: 12.2145 - mae: 1.4793 - val_loss: 13.0426 - val_mse: 13.0426 - val_mae: 1.4701 - lr: 7.5762e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 7s - loss: 12.0647 - mse: 12.0647 - mae: 1.4727 - val_loss: 12.9166 - val_mse: 12.9166 - val_mae: 1.5424 - lr: 7.5762e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 7s - loss: 11.9093 - mse: 11.9093 - mae: 1.4576 - val_loss: 12.6709 - val_mse: 12.6709 - val_mae: 1.5436 - lr: 7.5762e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 7s - loss: 11.8713 - mse: 11.8713 - mae: 1.4515 - val_loss: 12.4960 - val_mse: 12.4960 - val_mae: 1.5278 - lr: 7.5762e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 7s - loss: 11.9377 - mse: 11.9377 - mae: 1.4462 - val_loss: 12.5339 - val_mse: 12.5339 - val_mae: 1.4756 - lr: 7.5762e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 7s - loss: 11.5536 - mse: 11.5536 - mae: 1.4353 - val_loss: 12.4205 - val_mse: 12.4205 - val_mae: 1.4659 - lr: 7.5762e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 7s - loss: 11.5560 - mse: 11.5560 - mae: 1.4318 - val_loss: 12.6379 - val_mse: 12.6379 - val_mae: 1.4385 - lr: 7.5762e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 7s - loss: 11.4606 - mse: 11.4606 - mae: 1.4213 - val_loss: 12.6149 - val_mse: 12.6149 - val_mae: 1.4258 - lr: 7.5762e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 7s - loss: 11.2443 - mse: 11.2443 - mae: 1.4160 - val_loss: 12.7936 - val_mse: 12.7936 - val_mae: 1.4268 - lr: 7.5762e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 7s - loss: 11.2096 - mse: 11.2096 - mae: 1.4129 - val_loss: 13.2043 - val_mse: 13.2043 - val_mae: 1.5405 - lr: 7.5762e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 7s - loss: 11.1233 - mse: 11.1233 - mae: 1.4033 - val_loss: 12.5883 - val_mse: 12.5883 - val_mae: 1.4905 - lr: 7.5762e-04 - 7s/epoch - 7ms/step\n",
            "Score for fold 1: loss of 12.588325500488281\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 7s - loss: 12.1391 - mse: 12.1391 - mae: 1.4181 - val_loss: 8.3011 - val_mse: 8.3011 - val_mae: 1.4401 - lr: 7.5762e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 7s - loss: 11.8678 - mse: 11.8678 - mae: 1.4110 - val_loss: 8.4615 - val_mse: 8.4615 - val_mae: 1.4319 - lr: 7.5762e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 7s - loss: 11.9463 - mse: 11.9463 - mae: 1.4016 - val_loss: 8.2833 - val_mse: 8.2833 - val_mae: 1.4144 - lr: 7.5762e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 7s - loss: 11.5186 - mse: 11.5186 - mae: 1.3954 - val_loss: 8.7077 - val_mse: 8.7077 - val_mae: 1.4268 - lr: 7.5762e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 7s - loss: 11.5674 - mse: 11.5674 - mae: 1.3896 - val_loss: 8.4243 - val_mse: 8.4243 - val_mae: 1.4489 - lr: 7.5762e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 7s - loss: 11.2694 - mse: 11.2694 - mae: 1.3824 - val_loss: 8.4934 - val_mse: 8.4934 - val_mae: 1.4640 - lr: 7.5762e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 7s - loss: 11.1351 - mse: 11.1351 - mae: 1.3697 - val_loss: 8.7007 - val_mse: 8.7007 - val_mae: 1.4318 - lr: 7.5762e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 7s - loss: 10.9909 - mse: 10.9909 - mae: 1.3641 - val_loss: 8.6433 - val_mse: 8.6433 - val_mae: 1.4193 - lr: 7.5762e-04 - 7s/epoch - 7ms/step\n",
            "Score for fold 2: loss of 8.643331527709961\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 7s - loss: 11.6896 - mse: 11.6896 - mae: 1.3850 - val_loss: 6.5026 - val_mse: 6.5026 - val_mae: 1.3381 - lr: 7.5762e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 7s - loss: 11.0902 - mse: 11.0902 - mae: 1.3673 - val_loss: 6.5191 - val_mse: 6.5191 - val_mae: 1.3344 - lr: 7.5762e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 7s - loss: 11.0847 - mse: 11.0847 - mae: 1.3611 - val_loss: 6.6335 - val_mse: 6.6335 - val_mae: 1.3314 - lr: 7.5762e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 7s - loss: 10.8017 - mse: 10.8017 - mae: 1.3416 - val_loss: 6.2912 - val_mse: 6.2912 - val_mae: 1.3368 - lr: 7.5762e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 7s - loss: 10.7437 - mse: 10.7437 - mae: 1.3306 - val_loss: 6.5831 - val_mse: 6.5831 - val_mae: 1.3740 - lr: 7.5762e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 7s - loss: 10.4764 - mse: 10.4764 - mae: 1.3179 - val_loss: 6.7158 - val_mse: 6.7158 - val_mae: 1.3879 - lr: 7.5762e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 7s - loss: 10.1865 - mse: 10.1865 - mae: 1.3059 - val_loss: 7.0628 - val_mse: 7.0628 - val_mae: 1.3530 - lr: 7.5762e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 7s - loss: 10.1621 - mse: 10.1621 - mae: 1.2917 - val_loss: 7.1232 - val_mse: 7.1232 - val_mae: 1.3668 - lr: 7.5762e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 7s - loss: 9.8965 - mse: 9.8965 - mae: 1.2757 - val_loss: 6.8020 - val_mse: 6.8020 - val_mae: 1.3424 - lr: 7.5762e-04 - 7s/epoch - 7ms/step\n",
            "Score for fold 3: loss of 6.801971912384033\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 7s - loss: 9.6221 - mse: 9.6221 - mae: 1.3068 - val_loss: 8.9524 - val_mse: 8.9524 - val_mae: 1.2600 - lr: 7.5762e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 7s - loss: 9.0360 - mse: 9.0360 - mae: 1.2834 - val_loss: 9.9027 - val_mse: 9.9027 - val_mae: 1.2749 - lr: 7.5762e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 7s - loss: 8.9816 - mse: 8.9816 - mae: 1.2695 - val_loss: 9.1613 - val_mse: 9.1613 - val_mae: 1.2812 - lr: 7.5762e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 7s - loss: 8.5914 - mse: 8.5914 - mae: 1.2480 - val_loss: 9.2767 - val_mse: 9.2767 - val_mae: 1.3490 - lr: 7.5762e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 7s - loss: 8.4464 - mse: 8.4464 - mae: 1.2311 - val_loss: 9.4542 - val_mse: 9.4542 - val_mae: 1.3023 - lr: 7.5762e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 7s - loss: 8.2702 - mse: 8.2702 - mae: 1.2205 - val_loss: 9.8922 - val_mse: 9.8922 - val_mae: 1.3301 - lr: 7.5762e-04 - 7s/epoch - 7ms/step\n",
            "Score for fold 4: loss of 9.892221450805664\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 7s - loss: 7.2210 - mse: 7.2210 - mae: 1.2409 - val_loss: 13.4926 - val_mse: 13.4926 - val_mae: 1.2009 - lr: 7.5762e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 7s - loss: 6.8762 - mse: 6.8762 - mae: 1.2125 - val_loss: 13.9800 - val_mse: 13.9800 - val_mae: 1.2290 - lr: 7.5762e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 7s - loss: 6.8136 - mse: 6.8136 - mae: 1.1953 - val_loss: 14.3704 - val_mse: 14.3704 - val_mae: 1.2579 - lr: 7.5762e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 7s - loss: 6.5663 - mse: 6.5663 - mae: 1.1777 - val_loss: 14.5612 - val_mse: 14.5612 - val_mae: 1.2654 - lr: 7.5762e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 7s - loss: 6.2673 - mse: 6.2673 - mae: 1.1584 - val_loss: 14.2684 - val_mse: 14.2684 - val_mae: 1.2914 - lr: 7.5762e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 7s - loss: 6.1381 - mse: 6.1381 - mae: 1.1457 - val_loss: 15.0314 - val_mse: 15.0314 - val_mae: 1.2641 - lr: 7.5762e-04 - 7s/epoch - 7ms/step\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-10-12 01:54:43,670]\u001b[0m Finished trial#44 resulted in value: 10.59. Current best value is 9.116 with parameters: {'activation': 'relu', 'num_hidden_layer': 4, 'i': 10, 'learning_rate': 0.0002344581502453532}.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Score for fold 5: loss of 15.031402587890625\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 5s - loss: 11.5446 - mse: 11.5446 - mae: 1.5358 - val_loss: 22.1518 - val_mse: 22.1518 - val_mae: 1.6635 - lr: 1.2845e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 4s - loss: 10.3592 - mse: 10.3592 - mae: 1.4786 - val_loss: 21.6615 - val_mse: 21.6615 - val_mae: 1.5613 - lr: 1.2845e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 4s - loss: 10.0492 - mse: 10.0492 - mae: 1.4606 - val_loss: 21.2568 - val_mse: 21.2568 - val_mae: 1.5874 - lr: 1.2845e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 4s - loss: 9.9521 - mse: 9.9521 - mae: 1.4515 - val_loss: 21.1563 - val_mse: 21.1563 - val_mae: 1.5343 - lr: 1.2845e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 4s - loss: 9.8434 - mse: 9.8434 - mae: 1.4440 - val_loss: 21.0359 - val_mse: 21.0359 - val_mae: 1.5179 - lr: 1.2845e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 4s - loss: 9.6591 - mse: 9.6591 - mae: 1.4342 - val_loss: 20.9425 - val_mse: 20.9425 - val_mae: 1.5450 - lr: 1.2845e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 4s - loss: 9.6182 - mse: 9.6182 - mae: 1.4269 - val_loss: 20.9743 - val_mse: 20.9743 - val_mae: 1.5563 - lr: 1.2845e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 4s - loss: 9.6718 - mse: 9.6718 - mae: 1.4259 - val_loss: 20.9944 - val_mse: 20.9944 - val_mae: 1.5140 - lr: 1.2845e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 4s - loss: 9.4923 - mse: 9.4923 - mae: 1.4198 - val_loss: 20.8800 - val_mse: 20.8800 - val_mae: 1.5198 - lr: 1.2845e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 4s - loss: 9.3936 - mse: 9.3936 - mae: 1.4120 - val_loss: 20.7967 - val_mse: 20.7967 - val_mae: 1.5462 - lr: 1.2845e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 4s - loss: 9.4260 - mse: 9.4260 - mae: 1.4122 - val_loss: 20.7873 - val_mse: 20.7873 - val_mae: 1.5594 - lr: 1.2845e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 4s - loss: 9.3697 - mse: 9.3697 - mae: 1.4077 - val_loss: 20.8912 - val_mse: 20.8912 - val_mae: 1.4859 - lr: 1.2845e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 13/100\n",
            "1000/1000 - 4s - loss: 9.2793 - mse: 9.2793 - mae: 1.4026 - val_loss: 20.9322 - val_mse: 20.9322 - val_mae: 1.5719 - lr: 1.2845e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 14/100\n",
            "1000/1000 - 4s - loss: 9.3169 - mse: 9.3169 - mae: 1.3995 - val_loss: 21.0736 - val_mse: 21.0736 - val_mae: 1.4701 - lr: 1.2845e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 15/100\n",
            "1000/1000 - 4s - loss: 9.2407 - mse: 9.2407 - mae: 1.3990 - val_loss: 20.6831 - val_mse: 20.6831 - val_mae: 1.5002 - lr: 1.2845e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 16/100\n",
            "1000/1000 - 4s - loss: 9.2746 - mse: 9.2746 - mae: 1.3943 - val_loss: 20.8674 - val_mse: 20.8674 - val_mae: 1.5562 - lr: 1.2845e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 17/100\n",
            "1000/1000 - 4s - loss: 9.1564 - mse: 9.1564 - mae: 1.3912 - val_loss: 20.6514 - val_mse: 20.6514 - val_mae: 1.6093 - lr: 1.2845e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 18/100\n",
            "1000/1000 - 4s - loss: 9.1275 - mse: 9.1275 - mae: 1.3860 - val_loss: 20.6471 - val_mse: 20.6471 - val_mae: 1.4926 - lr: 1.2845e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 19/100\n",
            "1000/1000 - 4s - loss: 9.0317 - mse: 9.0317 - mae: 1.3826 - val_loss: 20.5801 - val_mse: 20.5801 - val_mae: 1.5343 - lr: 1.2845e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 20/100\n",
            "1000/1000 - 4s - loss: 9.1490 - mse: 9.1490 - mae: 1.3815 - val_loss: 20.6804 - val_mse: 20.6804 - val_mae: 1.5045 - lr: 1.2845e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 21/100\n",
            "1000/1000 - 4s - loss: 8.9093 - mse: 8.9093 - mae: 1.3772 - val_loss: 20.7778 - val_mse: 20.7778 - val_mae: 1.4965 - lr: 1.2845e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 22/100\n",
            "1000/1000 - 4s - loss: 9.0437 - mse: 9.0437 - mae: 1.3747 - val_loss: 20.6514 - val_mse: 20.6514 - val_mae: 1.5005 - lr: 1.2845e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 23/100\n",
            "1000/1000 - 4s - loss: 8.9150 - mse: 8.9150 - mae: 1.3729 - val_loss: 20.6691 - val_mse: 20.6691 - val_mae: 1.4792 - lr: 1.2845e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 24/100\n",
            "1000/1000 - 4s - loss: 8.9691 - mse: 8.9691 - mae: 1.3697 - val_loss: 20.5749 - val_mse: 20.5749 - val_mae: 1.4860 - lr: 1.2845e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 25/100\n",
            "1000/1000 - 4s - loss: 8.7694 - mse: 8.7694 - mae: 1.3616 - val_loss: 20.4807 - val_mse: 20.4807 - val_mae: 1.5096 - lr: 1.2845e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 26/100\n",
            "1000/1000 - 4s - loss: 8.8414 - mse: 8.8414 - mae: 1.3600 - val_loss: 20.4883 - val_mse: 20.4883 - val_mae: 1.4937 - lr: 1.2845e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 27/100\n",
            "1000/1000 - 4s - loss: 8.6615 - mse: 8.6615 - mae: 1.3558 - val_loss: 20.4550 - val_mse: 20.4550 - val_mae: 1.5333 - lr: 1.2845e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 28/100\n",
            "1000/1000 - 4s - loss: 8.6889 - mse: 8.6889 - mae: 1.3527 - val_loss: 20.5778 - val_mse: 20.5778 - val_mae: 1.5198 - lr: 1.2845e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 29/100\n",
            "1000/1000 - 4s - loss: 8.5262 - mse: 8.5262 - mae: 1.3485 - val_loss: 20.5234 - val_mse: 20.5234 - val_mae: 1.5245 - lr: 1.2845e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 30/100\n",
            "1000/1000 - 4s - loss: 8.5804 - mse: 8.5804 - mae: 1.3432 - val_loss: 20.6483 - val_mse: 20.6483 - val_mae: 1.4979 - lr: 1.2845e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 31/100\n",
            "1000/1000 - 4s - loss: 8.4136 - mse: 8.4136 - mae: 1.3428 - val_loss: 20.6740 - val_mse: 20.6740 - val_mae: 1.5414 - lr: 1.2845e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 32/100\n",
            "1000/1000 - 4s - loss: 8.3787 - mse: 8.3787 - mae: 1.3377 - val_loss: 20.4392 - val_mse: 20.4392 - val_mae: 1.5350 - lr: 1.2845e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 33/100\n",
            "1000/1000 - 4s - loss: 8.3962 - mse: 8.3962 - mae: 1.3378 - val_loss: 20.4938 - val_mse: 20.4938 - val_mae: 1.5770 - lr: 1.2845e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 34/100\n",
            "1000/1000 - 4s - loss: 8.2237 - mse: 8.2237 - mae: 1.3302 - val_loss: 20.5128 - val_mse: 20.5128 - val_mae: 1.5995 - lr: 1.2845e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 35/100\n",
            "1000/1000 - 4s - loss: 8.2060 - mse: 8.2060 - mae: 1.3278 - val_loss: 20.6305 - val_mse: 20.6305 - val_mae: 1.5432 - lr: 1.2845e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 36/100\n",
            "1000/1000 - 4s - loss: 8.3305 - mse: 8.3305 - mae: 1.3254 - val_loss: 20.7469 - val_mse: 20.7469 - val_mae: 1.4912 - lr: 1.2845e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 37/100\n",
            "1000/1000 - 4s - loss: 8.1238 - mse: 8.1238 - mae: 1.3208 - val_loss: 20.9097 - val_mse: 20.9097 - val_mae: 1.4811 - lr: 1.2845e-04 - 4s/epoch - 4ms/step\n",
            "Score for fold 1: loss of 20.90968132019043\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 4s - loss: 11.0332 - mse: 11.0332 - mae: 1.3705 - val_loss: 9.2241 - val_mse: 9.2241 - val_mae: 1.3900 - lr: 1.2845e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 4s - loss: 11.0086 - mse: 11.0086 - mae: 1.3671 - val_loss: 8.9302 - val_mse: 8.9302 - val_mae: 1.3492 - lr: 1.2845e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 4s - loss: 10.8304 - mse: 10.8304 - mae: 1.3581 - val_loss: 9.0063 - val_mse: 9.0063 - val_mae: 1.3722 - lr: 1.2845e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 4s - loss: 10.7438 - mse: 10.7438 - mae: 1.3522 - val_loss: 9.6408 - val_mse: 9.6408 - val_mae: 1.3573 - lr: 1.2845e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 4s - loss: 10.7005 - mse: 10.7005 - mae: 1.3519 - val_loss: 9.6962 - val_mse: 9.6962 - val_mae: 1.3910 - lr: 1.2845e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 4s - loss: 10.6594 - mse: 10.6594 - mae: 1.3431 - val_loss: 9.2668 - val_mse: 9.2668 - val_mae: 1.3430 - lr: 1.2845e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 4s - loss: 10.6865 - mse: 10.6865 - mae: 1.3427 - val_loss: 9.8694 - val_mse: 9.8694 - val_mae: 1.3707 - lr: 1.2845e-04 - 4s/epoch - 4ms/step\n",
            "Score for fold 2: loss of 9.869357109069824\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 4s - loss: 10.6530 - mse: 10.6530 - mae: 1.3563 - val_loss: 9.8832 - val_mse: 9.8832 - val_mae: 1.3184 - lr: 1.2845e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 4s - loss: 10.4023 - mse: 10.4023 - mae: 1.3447 - val_loss: 9.7623 - val_mse: 9.7623 - val_mae: 1.3095 - lr: 1.2845e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 4s - loss: 10.5999 - mse: 10.5999 - mae: 1.3423 - val_loss: 9.8267 - val_mse: 9.8267 - val_mae: 1.4165 - lr: 1.2845e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 4s - loss: 10.3518 - mse: 10.3518 - mae: 1.3389 - val_loss: 9.7619 - val_mse: 9.7619 - val_mae: 1.3849 - lr: 1.2845e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 4s - loss: 10.1582 - mse: 10.1582 - mae: 1.3293 - val_loss: 10.0491 - val_mse: 10.0491 - val_mae: 1.2982 - lr: 1.2845e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 4s - loss: 10.0141 - mse: 10.0141 - mae: 1.3254 - val_loss: 10.1418 - val_mse: 10.1418 - val_mae: 1.3657 - lr: 1.2845e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 4s - loss: 10.1670 - mse: 10.1670 - mae: 1.3237 - val_loss: 10.0692 - val_mse: 10.0692 - val_mae: 1.3682 - lr: 1.2845e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 4s - loss: 9.8288 - mse: 9.8288 - mae: 1.3140 - val_loss: 9.9774 - val_mse: 9.9774 - val_mae: 1.3927 - lr: 1.2845e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 4s - loss: 10.0286 - mse: 10.0286 - mae: 1.3127 - val_loss: 10.0316 - val_mse: 10.0316 - val_mae: 1.3846 - lr: 1.2845e-04 - 4s/epoch - 4ms/step\n",
            "Score for fold 3: loss of 10.031633377075195\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 4s - loss: 10.7267 - mse: 10.7267 - mae: 1.3360 - val_loss: 6.7155 - val_mse: 6.7155 - val_mae: 1.2655 - lr: 1.2845e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 4s - loss: 10.5590 - mse: 10.5590 - mae: 1.3239 - val_loss: 6.7687 - val_mse: 6.7687 - val_mae: 1.2727 - lr: 1.2845e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 4s - loss: 10.5014 - mse: 10.5014 - mae: 1.3186 - val_loss: 7.2858 - val_mse: 7.2858 - val_mae: 1.2693 - lr: 1.2845e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 4s - loss: 10.3319 - mse: 10.3319 - mae: 1.3128 - val_loss: 7.2743 - val_mse: 7.2743 - val_mae: 1.3105 - lr: 1.2845e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 4s - loss: 10.3692 - mse: 10.3692 - mae: 1.3121 - val_loss: 7.1417 - val_mse: 7.1417 - val_mae: 1.3006 - lr: 1.2845e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 4s - loss: 10.3056 - mse: 10.3056 - mae: 1.3046 - val_loss: 7.6495 - val_mse: 7.6495 - val_mae: 1.2993 - lr: 1.2845e-04 - 4s/epoch - 4ms/step\n",
            "Score for fold 4: loss of 7.649472236633301\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 4s - loss: 10.7626 - mse: 10.7626 - mae: 1.3261 - val_loss: 5.0921 - val_mse: 5.0921 - val_mae: 1.2451 - lr: 1.2845e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 4s - loss: 10.7328 - mse: 10.7328 - mae: 1.3216 - val_loss: 5.3308 - val_mse: 5.3308 - val_mae: 1.2340 - lr: 1.2845e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 4s - loss: 10.6020 - mse: 10.6020 - mae: 1.3128 - val_loss: 5.4836 - val_mse: 5.4836 - val_mae: 1.2055 - lr: 1.2845e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 4s - loss: 10.5470 - mse: 10.5470 - mae: 1.3062 - val_loss: 5.1993 - val_mse: 5.1993 - val_mae: 1.2208 - lr: 1.2845e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 4s - loss: 10.4882 - mse: 10.4882 - mae: 1.3006 - val_loss: 5.2745 - val_mse: 5.2745 - val_mae: 1.2448 - lr: 1.2845e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 4s - loss: 10.2904 - mse: 10.2904 - mae: 1.2951 - val_loss: 5.4730 - val_mse: 5.4730 - val_mae: 1.2873 - lr: 1.2845e-04 - 4s/epoch - 4ms/step\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-10-12 01:59:27,193]\u001b[0m Finished trial#45 resulted in value: 10.786. Current best value is 9.116 with parameters: {'activation': 'relu', 'num_hidden_layer': 4, 'i': 10, 'learning_rate': 0.0002344581502453532}.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Score for fold 5: loss of 5.472958564758301\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 8s - loss: 16.1201 - mse: 16.1201 - mae: 1.7068 - val_loss: 10.4482 - val_mse: 10.4482 - val_mae: 1.4437 - lr: 0.0029 - 8s/epoch - 8ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 7s - loss: 14.9778 - mse: 14.9778 - mae: 1.6467 - val_loss: 10.6804 - val_mse: 10.6804 - val_mae: 1.6576 - lr: 0.0029 - 7s/epoch - 7ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 7s - loss: 14.7143 - mse: 14.7143 - mae: 1.6437 - val_loss: 10.0094 - val_mse: 10.0094 - val_mae: 1.4692 - lr: 0.0029 - 7s/epoch - 7ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 7s - loss: 14.4997 - mse: 14.4997 - mae: 1.6299 - val_loss: 9.6595 - val_mse: 9.6595 - val_mae: 1.4442 - lr: 0.0029 - 7s/epoch - 7ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 7s - loss: 14.2318 - mse: 14.2318 - mae: 1.6356 - val_loss: 9.8405 - val_mse: 9.8405 - val_mae: 1.6514 - lr: 0.0029 - 7s/epoch - 7ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 7s - loss: 14.0837 - mse: 14.0837 - mae: 1.5999 - val_loss: 10.1263 - val_mse: 10.1263 - val_mae: 1.6597 - lr: 0.0029 - 7s/epoch - 7ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 7s - loss: 14.0440 - mse: 14.0440 - mae: 1.6100 - val_loss: 9.5704 - val_mse: 9.5704 - val_mae: 1.6304 - lr: 0.0029 - 7s/epoch - 7ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 7s - loss: 13.9657 - mse: 13.9657 - mae: 1.6104 - val_loss: 9.7750 - val_mse: 9.7750 - val_mae: 1.4300 - lr: 0.0029 - 7s/epoch - 7ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 7s - loss: 14.0033 - mse: 14.0033 - mae: 1.6223 - val_loss: 9.6898 - val_mse: 9.6898 - val_mae: 1.7663 - lr: 0.0029 - 7s/epoch - 7ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 7s - loss: 14.0771 - mse: 14.0771 - mae: 1.6222 - val_loss: 9.8276 - val_mse: 9.8276 - val_mae: 1.7772 - lr: 0.0029 - 7s/epoch - 7ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 7s - loss: 14.0330 - mse: 14.0330 - mae: 1.6165 - val_loss: 9.2862 - val_mse: 9.2862 - val_mae: 1.5278 - lr: 0.0029 - 7s/epoch - 7ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 7s - loss: 14.1045 - mse: 14.1045 - mae: 1.6420 - val_loss: 9.3435 - val_mse: 9.3435 - val_mae: 1.6681 - lr: 0.0029 - 7s/epoch - 7ms/step\n",
            "Epoch 13/100\n",
            "1000/1000 - 7s - loss: 13.9262 - mse: 13.9262 - mae: 1.6108 - val_loss: 9.1333 - val_mse: 9.1333 - val_mae: 1.5002 - lr: 0.0029 - 7s/epoch - 7ms/step\n",
            "Epoch 14/100\n",
            "1000/1000 - 7s - loss: 13.8762 - mse: 13.8762 - mae: 1.6162 - val_loss: 9.2433 - val_mse: 9.2433 - val_mae: 1.5752 - lr: 0.0029 - 7s/epoch - 7ms/step\n",
            "Epoch 15/100\n",
            "1000/1000 - 7s - loss: 13.7911 - mse: 13.7911 - mae: 1.5905 - val_loss: 10.4291 - val_mse: 10.4291 - val_mae: 1.5382 - lr: 0.0029 - 7s/epoch - 7ms/step\n",
            "Epoch 16/100\n",
            "1000/1000 - 7s - loss: 13.7930 - mse: 13.7930 - mae: 1.6015 - val_loss: 9.2166 - val_mse: 9.2166 - val_mae: 1.4708 - lr: 0.0029 - 7s/epoch - 7ms/step\n",
            "Epoch 17/100\n",
            "1000/1000 - 7s - loss: 13.9020 - mse: 13.9020 - mae: 1.6177 - val_loss: 9.2094 - val_mse: 9.2094 - val_mae: 1.5090 - lr: 0.0029 - 7s/epoch - 7ms/step\n",
            "Epoch 18/100\n",
            "1000/1000 - 7s - loss: 13.7280 - mse: 13.7280 - mae: 1.5880 - val_loss: 9.5773 - val_mse: 9.5773 - val_mae: 1.4224 - lr: 0.0029 - 7s/epoch - 7ms/step\n",
            "Score for fold 1: loss of 9.577310562133789\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 7s - loss: 12.0287 - mse: 12.0287 - mae: 1.4879 - val_loss: 13.9254 - val_mse: 13.9254 - val_mae: 1.6111 - lr: 0.0010 - 7s/epoch - 7ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 7s - loss: 11.8350 - mse: 11.8350 - mae: 1.4788 - val_loss: 14.0302 - val_mse: 14.0302 - val_mae: 1.4572 - lr: 0.0010 - 7s/epoch - 7ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 7s - loss: 11.6405 - mse: 11.6405 - mae: 1.4687 - val_loss: 13.9315 - val_mse: 13.9315 - val_mae: 1.4845 - lr: 0.0010 - 7s/epoch - 7ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 7s - loss: 11.5864 - mse: 11.5864 - mae: 1.4667 - val_loss: 13.8490 - val_mse: 13.8490 - val_mae: 1.4894 - lr: 0.0010 - 7s/epoch - 7ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 7s - loss: 11.5290 - mse: 11.5290 - mae: 1.4742 - val_loss: 13.7649 - val_mse: 13.7649 - val_mae: 1.6148 - lr: 0.0010 - 7s/epoch - 7ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 7s - loss: 11.4601 - mse: 11.4601 - mae: 1.4617 - val_loss: 13.6221 - val_mse: 13.6221 - val_mae: 1.6487 - lr: 0.0010 - 7s/epoch - 7ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 7s - loss: 11.3533 - mse: 11.3533 - mae: 1.4590 - val_loss: 13.9568 - val_mse: 13.9568 - val_mae: 1.5264 - lr: 0.0010 - 7s/epoch - 7ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 7s - loss: 11.3058 - mse: 11.3058 - mae: 1.4529 - val_loss: 14.0382 - val_mse: 14.0382 - val_mae: 1.5245 - lr: 0.0010 - 7s/epoch - 7ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 7s - loss: 11.2249 - mse: 11.2249 - mae: 1.4535 - val_loss: 14.1652 - val_mse: 14.1652 - val_mae: 1.4912 - lr: 0.0010 - 7s/epoch - 7ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 7s - loss: 11.2393 - mse: 11.2393 - mae: 1.4544 - val_loss: 14.1127 - val_mse: 14.1127 - val_mae: 1.5188 - lr: 0.0010 - 7s/epoch - 7ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 7s - loss: 11.1266 - mse: 11.1266 - mae: 1.4480 - val_loss: 13.6996 - val_mse: 13.6996 - val_mae: 1.5404 - lr: 0.0010 - 7s/epoch - 7ms/step\n",
            "Score for fold 2: loss of 13.699567794799805\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 7s - loss: 11.3893 - mse: 11.3893 - mae: 1.4727 - val_loss: 12.6885 - val_mse: 12.6885 - val_mae: 1.4315 - lr: 0.0010 - 7s/epoch - 7ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 7s - loss: 11.3343 - mse: 11.3343 - mae: 1.4784 - val_loss: 13.1994 - val_mse: 13.1994 - val_mae: 1.4034 - lr: 0.0010 - 7s/epoch - 7ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 7s - loss: 11.2392 - mse: 11.2392 - mae: 1.4688 - val_loss: 12.8342 - val_mse: 12.8342 - val_mae: 1.4477 - lr: 0.0010 - 7s/epoch - 7ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 7s - loss: 11.1106 - mse: 11.1106 - mae: 1.4611 - val_loss: 12.8398 - val_mse: 12.8398 - val_mae: 1.5816 - lr: 0.0010 - 7s/epoch - 7ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 7s - loss: 11.0063 - mse: 11.0063 - mae: 1.4544 - val_loss: 13.1568 - val_mse: 13.1568 - val_mae: 1.6441 - lr: 0.0010 - 7s/epoch - 7ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 7s - loss: 10.9690 - mse: 10.9690 - mae: 1.4505 - val_loss: 12.8964 - val_mse: 12.8964 - val_mae: 1.4353 - lr: 0.0010 - 7s/epoch - 7ms/step\n",
            "Score for fold 3: loss of 12.896410942077637\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 7s - loss: 12.2625 - mse: 12.2625 - mae: 1.4721 - val_loss: 7.6051 - val_mse: 7.6051 - val_mae: 1.3760 - lr: 0.0010 - 7s/epoch - 7ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 7s - loss: 12.3068 - mse: 12.3068 - mae: 1.4748 - val_loss: 8.0561 - val_mse: 8.0561 - val_mae: 1.3943 - lr: 0.0010 - 7s/epoch - 7ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 7s - loss: 12.2113 - mse: 12.2113 - mae: 1.4630 - val_loss: 7.7742 - val_mse: 7.7742 - val_mae: 1.4693 - lr: 0.0010 - 7s/epoch - 7ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 7s - loss: 12.1696 - mse: 12.1696 - mae: 1.4698 - val_loss: 7.7856 - val_mse: 7.7856 - val_mae: 1.5087 - lr: 0.0010 - 7s/epoch - 7ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 7s - loss: 12.1151 - mse: 12.1151 - mae: 1.4686 - val_loss: 8.1498 - val_mse: 8.1498 - val_mae: 1.6312 - lr: 0.0010 - 7s/epoch - 7ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 7s - loss: 12.1992 - mse: 12.1992 - mae: 1.4714 - val_loss: 7.9568 - val_mse: 7.9568 - val_mae: 1.3410 - lr: 0.0010 - 7s/epoch - 7ms/step\n",
            "Score for fold 4: loss of 7.9568376541137695\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 7s - loss: 10.2348 - mse: 10.2348 - mae: 1.4725 - val_loss: 15.2809 - val_mse: 15.2809 - val_mae: 1.3822 - lr: 0.0010 - 7s/epoch - 7ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 7s - loss: 10.0969 - mse: 10.0969 - mae: 1.4546 - val_loss: 15.8876 - val_mse: 15.8876 - val_mae: 1.4103 - lr: 0.0010 - 7s/epoch - 7ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 7s - loss: 10.0238 - mse: 10.0238 - mae: 1.4573 - val_loss: 15.5539 - val_mse: 15.5539 - val_mae: 1.3405 - lr: 0.0010 - 7s/epoch - 7ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 7s - loss: 9.9501 - mse: 9.9501 - mae: 1.4566 - val_loss: 15.3562 - val_mse: 15.3562 - val_mae: 1.3871 - lr: 0.0010 - 7s/epoch - 7ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 7s - loss: 9.9496 - mse: 9.9496 - mae: 1.4469 - val_loss: 15.6340 - val_mse: 15.6340 - val_mae: 1.3600 - lr: 0.0010 - 7s/epoch - 7ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 7s - loss: 9.8580 - mse: 9.8580 - mae: 1.4373 - val_loss: 15.6204 - val_mse: 15.6204 - val_mae: 1.3593 - lr: 0.0010 - 7s/epoch - 7ms/step\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-10-12 02:05:03,549]\u001b[0m Finished trial#46 resulted in value: 11.952. Current best value is 9.116 with parameters: {'activation': 'relu', 'num_hidden_layer': 4, 'i': 10, 'learning_rate': 0.0002344581502453532}.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Score for fold 5: loss of 15.620367050170898\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 3s - loss: 12.2370 - mse: 12.2370 - mae: 1.5359 - val_loss: 18.8355 - val_mse: 18.8355 - val_mae: 1.5772 - lr: 4.8438e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 11.1066 - mse: 11.1066 - mae: 1.4790 - val_loss: 18.7296 - val_mse: 18.7296 - val_mae: 1.5492 - lr: 4.8438e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 10.8446 - mse: 10.8446 - mae: 1.4643 - val_loss: 18.4709 - val_mse: 18.4709 - val_mae: 1.5705 - lr: 4.8438e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 10.6963 - mse: 10.6963 - mae: 1.4575 - val_loss: 18.5899 - val_mse: 18.5899 - val_mae: 1.5657 - lr: 4.8438e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 10.6244 - mse: 10.6244 - mae: 1.4481 - val_loss: 18.3177 - val_mse: 18.3177 - val_mae: 1.5313 - lr: 4.8438e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 10.5286 - mse: 10.5286 - mae: 1.4452 - val_loss: 18.2040 - val_mse: 18.2040 - val_mae: 1.6018 - lr: 4.8438e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 10.4392 - mse: 10.4392 - mae: 1.4432 - val_loss: 18.0915 - val_mse: 18.0915 - val_mae: 1.5225 - lr: 4.8438e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 2s - loss: 10.3771 - mse: 10.3771 - mae: 1.4360 - val_loss: 18.1660 - val_mse: 18.1660 - val_mae: 1.5943 - lr: 4.8438e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 2s - loss: 10.3517 - mse: 10.3517 - mae: 1.4331 - val_loss: 18.0525 - val_mse: 18.0525 - val_mae: 1.5555 - lr: 4.8438e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 2s - loss: 10.2955 - mse: 10.2955 - mae: 1.4274 - val_loss: 18.5382 - val_mse: 18.5382 - val_mae: 1.5218 - lr: 4.8438e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 2s - loss: 10.2973 - mse: 10.2973 - mae: 1.4281 - val_loss: 18.1413 - val_mse: 18.1413 - val_mae: 1.5321 - lr: 4.8438e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 2s - loss: 10.2237 - mse: 10.2237 - mae: 1.4273 - val_loss: 18.2523 - val_mse: 18.2523 - val_mae: 1.5344 - lr: 4.8438e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 13/100\n",
            "1000/1000 - 2s - loss: 10.2445 - mse: 10.2445 - mae: 1.4263 - val_loss: 17.9891 - val_mse: 17.9891 - val_mae: 1.5797 - lr: 4.8438e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 14/100\n",
            "1000/1000 - 2s - loss: 10.1847 - mse: 10.1847 - mae: 1.4242 - val_loss: 18.0589 - val_mse: 18.0589 - val_mae: 1.5436 - lr: 4.8438e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 15/100\n",
            "1000/1000 - 2s - loss: 10.1924 - mse: 10.1924 - mae: 1.4199 - val_loss: 18.0667 - val_mse: 18.0667 - val_mae: 1.5651 - lr: 4.8438e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 16/100\n",
            "1000/1000 - 2s - loss: 10.1858 - mse: 10.1858 - mae: 1.4206 - val_loss: 18.2006 - val_mse: 18.2006 - val_mae: 1.5015 - lr: 4.8438e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 17/100\n",
            "1000/1000 - 2s - loss: 10.1337 - mse: 10.1337 - mae: 1.4194 - val_loss: 17.9685 - val_mse: 17.9685 - val_mae: 1.5274 - lr: 4.8438e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 18/100\n",
            "1000/1000 - 2s - loss: 10.1250 - mse: 10.1250 - mae: 1.4195 - val_loss: 18.1683 - val_mse: 18.1683 - val_mae: 1.5615 - lr: 4.8438e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 19/100\n",
            "1000/1000 - 2s - loss: 10.0975 - mse: 10.0975 - mae: 1.4170 - val_loss: 17.9265 - val_mse: 17.9265 - val_mae: 1.5699 - lr: 4.8438e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 20/100\n",
            "1000/1000 - 2s - loss: 10.0430 - mse: 10.0430 - mae: 1.4136 - val_loss: 18.2251 - val_mse: 18.2251 - val_mae: 1.5275 - lr: 4.8438e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 21/100\n",
            "1000/1000 - 2s - loss: 10.0322 - mse: 10.0322 - mae: 1.4141 - val_loss: 17.9704 - val_mse: 17.9704 - val_mae: 1.5227 - lr: 4.8438e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 22/100\n",
            "1000/1000 - 2s - loss: 10.0214 - mse: 10.0214 - mae: 1.4100 - val_loss: 18.0758 - val_mse: 18.0758 - val_mae: 1.5264 - lr: 4.8438e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 23/100\n",
            "1000/1000 - 2s - loss: 9.9394 - mse: 9.9394 - mae: 1.4109 - val_loss: 17.9817 - val_mse: 17.9817 - val_mae: 1.5508 - lr: 4.8438e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 24/100\n",
            "1000/1000 - 2s - loss: 10.0002 - mse: 10.0002 - mae: 1.4109 - val_loss: 17.8446 - val_mse: 17.8446 - val_mae: 1.5436 - lr: 4.8438e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 25/100\n",
            "1000/1000 - 2s - loss: 9.9856 - mse: 9.9856 - mae: 1.4110 - val_loss: 17.9045 - val_mse: 17.9045 - val_mae: 1.4951 - lr: 4.8438e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 26/100\n",
            "1000/1000 - 2s - loss: 9.9589 - mse: 9.9589 - mae: 1.4092 - val_loss: 17.8710 - val_mse: 17.8710 - val_mae: 1.4979 - lr: 4.8438e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 27/100\n",
            "1000/1000 - 2s - loss: 9.8946 - mse: 9.8946 - mae: 1.4077 - val_loss: 17.9619 - val_mse: 17.9619 - val_mae: 1.5164 - lr: 4.8438e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 28/100\n",
            "1000/1000 - 2s - loss: 9.9028 - mse: 9.9028 - mae: 1.4053 - val_loss: 17.9875 - val_mse: 17.9875 - val_mae: 1.5462 - lr: 4.8438e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 29/100\n",
            "1000/1000 - 2s - loss: 9.8886 - mse: 9.8886 - mae: 1.4074 - val_loss: 17.9268 - val_mse: 17.9268 - val_mae: 1.5186 - lr: 4.8438e-04 - 2s/epoch - 2ms/step\n",
            "Score for fold 1: loss of 17.92677116394043\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 11.9520 - mse: 11.9520 - mae: 1.4383 - val_loss: 9.5287 - val_mse: 9.5287 - val_mae: 1.4230 - lr: 4.8438e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 11.9240 - mse: 11.9240 - mae: 1.4360 - val_loss: 9.7222 - val_mse: 9.7222 - val_mae: 1.4228 - lr: 4.8438e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 11.8868 - mse: 11.8868 - mae: 1.4335 - val_loss: 9.5252 - val_mse: 9.5252 - val_mae: 1.4225 - lr: 4.8438e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 11.9263 - mse: 11.9263 - mae: 1.4323 - val_loss: 9.6476 - val_mse: 9.6476 - val_mae: 1.4226 - lr: 4.8438e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 11.7596 - mse: 11.7596 - mae: 1.4278 - val_loss: 9.6315 - val_mse: 9.6315 - val_mae: 1.4361 - lr: 4.8438e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 11.8423 - mse: 11.8423 - mae: 1.4283 - val_loss: 9.6179 - val_mse: 9.6179 - val_mae: 1.4103 - lr: 4.8438e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 11.7901 - mse: 11.7901 - mae: 1.4291 - val_loss: 9.7983 - val_mse: 9.7983 - val_mae: 1.4342 - lr: 4.8438e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 2s - loss: 11.7507 - mse: 11.7507 - mae: 1.4279 - val_loss: 9.6784 - val_mse: 9.6784 - val_mae: 1.4449 - lr: 4.8438e-04 - 2s/epoch - 2ms/step\n",
            "Score for fold 2: loss of 9.678353309631348\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 11.5579 - mse: 11.5579 - mae: 1.4279 - val_loss: 10.6652 - val_mse: 10.6652 - val_mae: 1.4633 - lr: 4.8438e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 11.5400 - mse: 11.5400 - mae: 1.4263 - val_loss: 10.5438 - val_mse: 10.5438 - val_mae: 1.4308 - lr: 4.8438e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 11.4537 - mse: 11.4537 - mae: 1.4237 - val_loss: 10.6306 - val_mse: 10.6306 - val_mae: 1.4361 - lr: 4.8438e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 11.4913 - mse: 11.4913 - mae: 1.4249 - val_loss: 10.5514 - val_mse: 10.5514 - val_mae: 1.4043 - lr: 4.8438e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 11.4498 - mse: 11.4498 - mae: 1.4186 - val_loss: 10.6469 - val_mse: 10.6469 - val_mae: 1.4061 - lr: 4.8438e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 11.4371 - mse: 11.4371 - mae: 1.4190 - val_loss: 10.7065 - val_mse: 10.7065 - val_mae: 1.4345 - lr: 4.8438e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 11.3884 - mse: 11.3884 - mae: 1.4173 - val_loss: 10.7816 - val_mse: 10.7816 - val_mae: 1.4137 - lr: 4.8438e-04 - 2s/epoch - 2ms/step\n",
            "Score for fold 3: loss of 10.781609535217285\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 11.8079 - mse: 11.8079 - mae: 1.4253 - val_loss: 9.2631 - val_mse: 9.2631 - val_mae: 1.3668 - lr: 4.8438e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 11.7512 - mse: 11.7512 - mae: 1.4229 - val_loss: 9.2495 - val_mse: 9.2495 - val_mae: 1.3498 - lr: 4.8438e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 11.6726 - mse: 11.6726 - mae: 1.4231 - val_loss: 9.1766 - val_mse: 9.1766 - val_mae: 1.3845 - lr: 4.8438e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 11.6983 - mse: 11.6983 - mae: 1.4216 - val_loss: 9.1187 - val_mse: 9.1187 - val_mae: 1.4076 - lr: 4.8438e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 11.6257 - mse: 11.6257 - mae: 1.4182 - val_loss: 9.1302 - val_mse: 9.1302 - val_mae: 1.4166 - lr: 4.8438e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 11.6795 - mse: 11.6795 - mae: 1.4185 - val_loss: 9.2566 - val_mse: 9.2566 - val_mae: 1.4226 - lr: 4.8438e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 11.5944 - mse: 11.5944 - mae: 1.4189 - val_loss: 9.1669 - val_mse: 9.1669 - val_mae: 1.4036 - lr: 4.8438e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 2s - loss: 11.6390 - mse: 11.6390 - mae: 1.4184 - val_loss: 9.4361 - val_mse: 9.4361 - val_mae: 1.4415 - lr: 4.8438e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 2s - loss: 11.6526 - mse: 11.6526 - mae: 1.4137 - val_loss: 9.2092 - val_mse: 9.2092 - val_mae: 1.3854 - lr: 4.8438e-04 - 2s/epoch - 2ms/step\n",
            "Score for fold 4: loss of 9.20923900604248\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 11.4429 - mse: 11.4429 - mae: 1.4221 - val_loss: 9.6998 - val_mse: 9.6998 - val_mae: 1.3781 - lr: 4.8438e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 11.3798 - mse: 11.3798 - mae: 1.4233 - val_loss: 9.8058 - val_mse: 9.8058 - val_mae: 1.3752 - lr: 4.8438e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 11.3803 - mse: 11.3803 - mae: 1.4189 - val_loss: 9.8055 - val_mse: 9.8055 - val_mae: 1.4522 - lr: 4.8438e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 11.4220 - mse: 11.4220 - mae: 1.4200 - val_loss: 9.9162 - val_mse: 9.9162 - val_mae: 1.3688 - lr: 4.8438e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 11.3574 - mse: 11.3574 - mae: 1.4199 - val_loss: 9.8834 - val_mse: 9.8834 - val_mae: 1.4222 - lr: 4.8438e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 11.3080 - mse: 11.3080 - mae: 1.4149 - val_loss: 9.8232 - val_mse: 9.8232 - val_mae: 1.3847 - lr: 4.8438e-04 - 2s/epoch - 2ms/step\n",
            "Score for fold 5: loss of 9.823190689086914\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-10-12 02:07:14,996]\u001b[0m Finished trial#47 resulted in value: 11.484. Current best value is 9.116 with parameters: {'activation': 'relu', 'num_hidden_layer': 4, 'i': 10, 'learning_rate': 0.0002344581502453532}.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 7s - loss: 15.1250 - mse: 15.1250 - mae: 1.6561 - val_loss: 10.6410 - val_mse: 10.6410 - val_mae: 1.6263 - lr: 2.4115e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 7s - loss: 14.3781 - mse: 14.3781 - mae: 1.5767 - val_loss: 10.5822 - val_mse: 10.5822 - val_mae: 1.5558 - lr: 2.4115e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 7s - loss: 14.0949 - mse: 14.0949 - mae: 1.5634 - val_loss: 10.5630 - val_mse: 10.5630 - val_mae: 1.4762 - lr: 2.4115e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 7s - loss: 14.0415 - mse: 14.0415 - mae: 1.5590 - val_loss: 10.6368 - val_mse: 10.6368 - val_mae: 1.4908 - lr: 2.4115e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 6s - loss: 14.0555 - mse: 14.0555 - mae: 1.5453 - val_loss: 10.5627 - val_mse: 10.5627 - val_mae: 1.5537 - lr: 2.4115e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 6s - loss: 14.0405 - mse: 14.0405 - mae: 1.5547 - val_loss: 10.6293 - val_mse: 10.6293 - val_mae: 1.6050 - lr: 2.4115e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 6s - loss: 14.0909 - mse: 14.0909 - mae: 1.5448 - val_loss: 10.5612 - val_mse: 10.5612 - val_mae: 1.5997 - lr: 2.4115e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 6s - loss: 14.0403 - mse: 14.0403 - mae: 1.5532 - val_loss: 10.6339 - val_mse: 10.6339 - val_mae: 1.5038 - lr: 2.4115e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 6s - loss: 14.0412 - mse: 14.0412 - mae: 1.5463 - val_loss: 10.6497 - val_mse: 10.6497 - val_mae: 1.5418 - lr: 2.4115e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 6s - loss: 13.9866 - mse: 13.9866 - mae: 1.5430 - val_loss: 10.6458 - val_mse: 10.6458 - val_mae: 1.5569 - lr: 2.4115e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 6s - loss: 13.9788 - mse: 13.9788 - mae: 1.5477 - val_loss: 10.5133 - val_mse: 10.5133 - val_mae: 1.5984 - lr: 2.4115e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 6s - loss: 13.9171 - mse: 13.9171 - mae: 1.5489 - val_loss: 10.7117 - val_mse: 10.7117 - val_mae: 1.4865 - lr: 2.4115e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 13/100\n",
            "1000/1000 - 6s - loss: 13.9948 - mse: 13.9948 - mae: 1.5498 - val_loss: 10.6603 - val_mse: 10.6603 - val_mae: 1.5751 - lr: 2.4115e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 14/100\n",
            "1000/1000 - 6s - loss: 13.9487 - mse: 13.9487 - mae: 1.5426 - val_loss: 10.8607 - val_mse: 10.8607 - val_mae: 1.5599 - lr: 2.4115e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 15/100\n",
            "1000/1000 - 6s - loss: 13.9803 - mse: 13.9803 - mae: 1.5475 - val_loss: 10.8099 - val_mse: 10.8099 - val_mae: 1.6225 - lr: 2.4115e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 16/100\n",
            "1000/1000 - 6s - loss: 13.9580 - mse: 13.9580 - mae: 1.5488 - val_loss: 10.9029 - val_mse: 10.9029 - val_mae: 1.6249 - lr: 2.4115e-04 - 6s/epoch - 6ms/step\n",
            "Score for fold 1: loss of 10.902881622314453\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 6s - loss: 11.9136 - mse: 11.9136 - mae: 1.5385 - val_loss: 18.7827 - val_mse: 18.7827 - val_mae: 1.5604 - lr: 2.4115e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 6s - loss: 12.0013 - mse: 12.0013 - mae: 1.5437 - val_loss: 18.6933 - val_mse: 18.6933 - val_mae: 1.6103 - lr: 2.4115e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 6s - loss: 11.9021 - mse: 11.9021 - mae: 1.5389 - val_loss: 18.6894 - val_mse: 18.6894 - val_mae: 1.5590 - lr: 2.4115e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 7s - loss: 11.9452 - mse: 11.9452 - mae: 1.5469 - val_loss: 18.6924 - val_mse: 18.6924 - val_mae: 1.5886 - lr: 2.4115e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 6s - loss: 11.8876 - mse: 11.8876 - mae: 1.5413 - val_loss: 18.6658 - val_mse: 18.6658 - val_mae: 1.5946 - lr: 2.4115e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 7s - loss: 11.9869 - mse: 11.9869 - mae: 1.5523 - val_loss: 18.7283 - val_mse: 18.7283 - val_mae: 1.5441 - lr: 2.4115e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 7s - loss: 11.9658 - mse: 11.9658 - mae: 1.5466 - val_loss: 18.9426 - val_mse: 18.9426 - val_mae: 1.6194 - lr: 2.4115e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 6s - loss: 11.8848 - mse: 11.8848 - mae: 1.5454 - val_loss: 18.7008 - val_mse: 18.7008 - val_mae: 1.6227 - lr: 2.4115e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 6s - loss: 11.9558 - mse: 11.9558 - mae: 1.5452 - val_loss: 18.6334 - val_mse: 18.6334 - val_mae: 1.6105 - lr: 2.4115e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 7s - loss: 11.8047 - mse: 11.8047 - mae: 1.5392 - val_loss: 18.7305 - val_mse: 18.7305 - val_mae: 1.5739 - lr: 2.4115e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 6s - loss: 11.9738 - mse: 11.9738 - mae: 1.5497 - val_loss: 18.9673 - val_mse: 18.9673 - val_mae: 1.7868 - lr: 2.4115e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 6s - loss: 11.9954 - mse: 11.9954 - mae: 1.5495 - val_loss: 18.7013 - val_mse: 18.7013 - val_mae: 1.6181 - lr: 2.4115e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 13/100\n",
            "1000/1000 - 6s - loss: 12.0193 - mse: 12.0193 - mae: 1.5438 - val_loss: 18.7583 - val_mse: 18.7583 - val_mae: 1.5655 - lr: 2.4115e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 14/100\n",
            "1000/1000 - 6s - loss: 11.9093 - mse: 11.9093 - mae: 1.5430 - val_loss: 18.7608 - val_mse: 18.7608 - val_mae: 1.5587 - lr: 2.4115e-04 - 6s/epoch - 6ms/step\n",
            "Score for fold 2: loss of 18.760807037353516\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 6s - loss: 14.2161 - mse: 14.2161 - mae: 1.5657 - val_loss: 9.9218 - val_mse: 9.9218 - val_mae: 1.5133 - lr: 2.4115e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 6s - loss: 14.1288 - mse: 14.1288 - mae: 1.5578 - val_loss: 9.9456 - val_mse: 9.9456 - val_mae: 1.5018 - lr: 2.4115e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 7s - loss: 14.0069 - mse: 14.0069 - mae: 1.5546 - val_loss: 9.9173 - val_mse: 9.9173 - val_mae: 1.4943 - lr: 2.4115e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 6s - loss: 14.1894 - mse: 14.1894 - mae: 1.5609 - val_loss: 10.0357 - val_mse: 10.0357 - val_mae: 1.6006 - lr: 2.4115e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 6s - loss: 14.1545 - mse: 14.1545 - mae: 1.5616 - val_loss: 9.9597 - val_mse: 9.9597 - val_mae: 1.4760 - lr: 2.4115e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 6s - loss: 14.1363 - mse: 14.1363 - mae: 1.5606 - val_loss: 10.0525 - val_mse: 10.0525 - val_mae: 1.6174 - lr: 2.4115e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 6s - loss: 14.1306 - mse: 14.1306 - mae: 1.5625 - val_loss: 9.9841 - val_mse: 9.9841 - val_mae: 1.5662 - lr: 2.4115e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 6s - loss: 14.1635 - mse: 14.1635 - mae: 1.5581 - val_loss: 9.9528 - val_mse: 9.9528 - val_mae: 1.4954 - lr: 2.4115e-04 - 6s/epoch - 6ms/step\n",
            "Score for fold 3: loss of 9.952837944030762\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 6s - loss: 13.8547 - mse: 13.8547 - mae: 1.5689 - val_loss: 11.1813 - val_mse: 11.1813 - val_mae: 1.5373 - lr: 2.4115e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 6s - loss: 13.8666 - mse: 13.8666 - mae: 1.5657 - val_loss: 11.1955 - val_mse: 11.1955 - val_mae: 1.5036 - lr: 2.4115e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 6s - loss: 13.7748 - mse: 13.7748 - mae: 1.5735 - val_loss: 11.3693 - val_mse: 11.3693 - val_mae: 1.6120 - lr: 2.4115e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 7s - loss: 13.7514 - mse: 13.7514 - mae: 1.5669 - val_loss: 11.2308 - val_mse: 11.2308 - val_mae: 1.4528 - lr: 2.4115e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 7s - loss: 13.9010 - mse: 13.9010 - mae: 1.5642 - val_loss: 11.1117 - val_mse: 11.1117 - val_mae: 1.5307 - lr: 2.4115e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 7s - loss: 13.7211 - mse: 13.7211 - mae: 1.5722 - val_loss: 11.3067 - val_mse: 11.3067 - val_mae: 1.4350 - lr: 2.4115e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 7s - loss: 13.8635 - mse: 13.8635 - mae: 1.5720 - val_loss: 11.3602 - val_mse: 11.3602 - val_mae: 1.5707 - lr: 2.4115e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 6s - loss: 13.8159 - mse: 13.8159 - mae: 1.5618 - val_loss: 11.1248 - val_mse: 11.1248 - val_mae: 1.4938 - lr: 2.4115e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 6s - loss: 13.9278 - mse: 13.9278 - mae: 1.5697 - val_loss: 11.3132 - val_mse: 11.3132 - val_mae: 1.4234 - lr: 2.4115e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 6s - loss: 13.7695 - mse: 13.7695 - mae: 1.5603 - val_loss: 11.2811 - val_mse: 11.2811 - val_mae: 1.4498 - lr: 2.4115e-04 - 6s/epoch - 6ms/step\n",
            "Score for fold 4: loss of 11.281071662902832\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 7s - loss: 12.8465 - mse: 12.8465 - mae: 1.5532 - val_loss: 15.7355 - val_mse: 15.7355 - val_mae: 1.5514 - lr: 2.4115e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 7s - loss: 12.7274 - mse: 12.7274 - mae: 1.5474 - val_loss: 15.7583 - val_mse: 15.7583 - val_mae: 1.5401 - lr: 2.4115e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 7s - loss: 12.6487 - mse: 12.6487 - mae: 1.5517 - val_loss: 15.9323 - val_mse: 15.9323 - val_mae: 1.5507 - lr: 2.4115e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 7s - loss: 12.7054 - mse: 12.7054 - mae: 1.5502 - val_loss: 15.7347 - val_mse: 15.7347 - val_mae: 1.5450 - lr: 2.4115e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 7s - loss: 12.6707 - mse: 12.6707 - mae: 1.5451 - val_loss: 16.0120 - val_mse: 16.0120 - val_mae: 1.5011 - lr: 2.4115e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 7s - loss: 12.7037 - mse: 12.7037 - mae: 1.5431 - val_loss: 16.1300 - val_mse: 16.1300 - val_mae: 1.4933 - lr: 2.4115e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 6s - loss: 12.5756 - mse: 12.5756 - mae: 1.5536 - val_loss: 15.6898 - val_mse: 15.6898 - val_mae: 1.5786 - lr: 2.4115e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 6s - loss: 12.6198 - mse: 12.6198 - mae: 1.5439 - val_loss: 15.7851 - val_mse: 15.7851 - val_mae: 1.5640 - lr: 2.4115e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 6s - loss: 12.7464 - mse: 12.7464 - mae: 1.5526 - val_loss: 15.7587 - val_mse: 15.7587 - val_mae: 1.5508 - lr: 2.4115e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 6s - loss: 12.7056 - mse: 12.7056 - mae: 1.5503 - val_loss: 15.9041 - val_mse: 15.9041 - val_mae: 1.4839 - lr: 2.4115e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 7s - loss: 12.5806 - mse: 12.5806 - mae: 1.5448 - val_loss: 15.7151 - val_mse: 15.7151 - val_mae: 1.5672 - lr: 2.4115e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 7s - loss: 12.6150 - mse: 12.6150 - mae: 1.5471 - val_loss: 15.6919 - val_mse: 15.6919 - val_mae: 1.5698 - lr: 2.4115e-04 - 7s/epoch - 7ms/step\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-10-12 02:13:52,101]\u001b[0m Finished trial#48 resulted in value: 13.315999999999999. Current best value is 9.116 with parameters: {'activation': 'relu', 'num_hidden_layer': 4, 'i': 10, 'learning_rate': 0.0002344581502453532}.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Score for fold 5: loss of 15.691909790039062\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 12s - loss: 11.8801 - mse: 11.8801 - mae: 1.5398 - val_loss: 17.3052 - val_mse: 17.3052 - val_mae: 1.4935 - lr: 3.4389e-04 - 12s/epoch - 12ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 12s - loss: 11.4699 - mse: 11.4699 - mae: 1.4943 - val_loss: 17.0599 - val_mse: 17.0599 - val_mae: 1.5047 - lr: 3.4389e-04 - 12s/epoch - 12ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 12s - loss: 11.2533 - mse: 11.2533 - mae: 1.4826 - val_loss: 16.9006 - val_mse: 16.9006 - val_mae: 1.4583 - lr: 3.4389e-04 - 12s/epoch - 12ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 12s - loss: 10.9404 - mse: 10.9404 - mae: 1.4651 - val_loss: 16.4307 - val_mse: 16.4307 - val_mae: 1.4914 - lr: 3.4389e-04 - 12s/epoch - 12ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 11s - loss: 10.9455 - mse: 10.9455 - mae: 1.4596 - val_loss: 16.4642 - val_mse: 16.4642 - val_mae: 1.4633 - lr: 3.4389e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 11s - loss: 10.8032 - mse: 10.8032 - mae: 1.4502 - val_loss: 16.4923 - val_mse: 16.4923 - val_mae: 1.4413 - lr: 3.4389e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 12s - loss: 10.6332 - mse: 10.6332 - mae: 1.4428 - val_loss: 16.4398 - val_mse: 16.4398 - val_mae: 1.5140 - lr: 3.4389e-04 - 12s/epoch - 12ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 12s - loss: 10.4639 - mse: 10.4639 - mae: 1.4391 - val_loss: 16.3380 - val_mse: 16.3380 - val_mae: 1.4951 - lr: 3.4389e-04 - 12s/epoch - 12ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 12s - loss: 10.1677 - mse: 10.1677 - mae: 1.4209 - val_loss: 16.1916 - val_mse: 16.1916 - val_mae: 1.4645 - lr: 3.4389e-04 - 12s/epoch - 12ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 12s - loss: 10.1115 - mse: 10.1115 - mae: 1.4125 - val_loss: 16.3660 - val_mse: 16.3660 - val_mae: 1.4433 - lr: 3.4389e-04 - 12s/epoch - 12ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 12s - loss: 10.0777 - mse: 10.0777 - mae: 1.4001 - val_loss: 16.5000 - val_mse: 16.5000 - val_mae: 1.4296 - lr: 3.4389e-04 - 12s/epoch - 12ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 12s - loss: 9.9187 - mse: 9.9187 - mae: 1.3916 - val_loss: 16.4437 - val_mse: 16.4437 - val_mae: 1.5117 - lr: 3.4389e-04 - 12s/epoch - 12ms/step\n",
            "Epoch 13/100\n",
            "1000/1000 - 12s - loss: 9.4883 - mse: 9.4883 - mae: 1.3757 - val_loss: 16.6090 - val_mse: 16.6090 - val_mae: 1.5579 - lr: 3.4389e-04 - 12s/epoch - 12ms/step\n",
            "Epoch 14/100\n",
            "1000/1000 - 12s - loss: 9.1710 - mse: 9.1710 - mae: 1.3600 - val_loss: 16.8228 - val_mse: 16.8228 - val_mae: 1.4461 - lr: 3.4389e-04 - 12s/epoch - 12ms/step\n",
            "Score for fold 1: loss of 16.822776794433594\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 11s - loss: 10.9768 - mse: 10.9768 - mae: 1.3979 - val_loss: 10.1589 - val_mse: 10.1589 - val_mae: 1.3468 - lr: 3.4389e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 12s - loss: 10.5612 - mse: 10.5612 - mae: 1.3857 - val_loss: 9.9126 - val_mse: 9.9126 - val_mae: 1.3414 - lr: 3.4389e-04 - 12s/epoch - 12ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 12s - loss: 10.2007 - mse: 10.2007 - mae: 1.3654 - val_loss: 9.8037 - val_mse: 9.8037 - val_mae: 1.4137 - lr: 3.4389e-04 - 12s/epoch - 12ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 11s - loss: 9.9865 - mse: 9.9865 - mae: 1.3474 - val_loss: 10.2566 - val_mse: 10.2566 - val_mae: 1.3919 - lr: 3.4389e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 11s - loss: 9.9346 - mse: 9.9346 - mae: 1.3332 - val_loss: 10.2436 - val_mse: 10.2436 - val_mae: 1.3503 - lr: 3.4389e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 12s - loss: 9.6386 - mse: 9.6386 - mae: 1.3205 - val_loss: 10.7364 - val_mse: 10.7364 - val_mae: 1.3449 - lr: 3.4389e-04 - 12s/epoch - 12ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 11s - loss: 9.2345 - mse: 9.2345 - mae: 1.3031 - val_loss: 10.3754 - val_mse: 10.3754 - val_mae: 1.4044 - lr: 3.4389e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 11s - loss: 9.1608 - mse: 9.1608 - mae: 1.2919 - val_loss: 10.8828 - val_mse: 10.8828 - val_mae: 1.5133 - lr: 3.4389e-04 - 11s/epoch - 11ms/step\n",
            "Score for fold 2: loss of 10.882835388183594\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 11s - loss: 9.4754 - mse: 9.4754 - mae: 1.3216 - val_loss: 10.0335 - val_mse: 10.0335 - val_mae: 1.2667 - lr: 3.4389e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 11s - loss: 8.9241 - mse: 8.9241 - mae: 1.2919 - val_loss: 10.2096 - val_mse: 10.2096 - val_mae: 1.3106 - lr: 3.4389e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 11s - loss: 8.7955 - mse: 8.7955 - mae: 1.2904 - val_loss: 10.5748 - val_mse: 10.5748 - val_mae: 1.3231 - lr: 3.4389e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 11s - loss: 8.4025 - mse: 8.4025 - mae: 1.2721 - val_loss: 10.6648 - val_mse: 10.6648 - val_mae: 1.3958 - lr: 3.4389e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 11s - loss: 8.2742 - mse: 8.2742 - mae: 1.2574 - val_loss: 10.4800 - val_mse: 10.4800 - val_mae: 1.3063 - lr: 3.4389e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 11s - loss: 7.8064 - mse: 7.8064 - mae: 1.2420 - val_loss: 10.8879 - val_mse: 10.8879 - val_mae: 1.2636 - lr: 3.4389e-04 - 11s/epoch - 11ms/step\n",
            "Score for fold 3: loss of 10.887893676757812\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 12s - loss: 9.5353 - mse: 9.5353 - mae: 1.2669 - val_loss: 4.7592 - val_mse: 4.7592 - val_mae: 1.1726 - lr: 3.4389e-04 - 12s/epoch - 12ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 12s - loss: 9.1790 - mse: 9.1790 - mae: 1.2459 - val_loss: 4.9633 - val_mse: 4.9633 - val_mae: 1.2349 - lr: 3.4389e-04 - 12s/epoch - 12ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 12s - loss: 8.9832 - mse: 8.9832 - mae: 1.2378 - val_loss: 5.5676 - val_mse: 5.5676 - val_mae: 1.2864 - lr: 3.4389e-04 - 12s/epoch - 12ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 12s - loss: 8.6329 - mse: 8.6329 - mae: 1.2160 - val_loss: 4.8149 - val_mse: 4.8149 - val_mae: 1.2356 - lr: 3.4389e-04 - 12s/epoch - 12ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 12s - loss: 8.5394 - mse: 8.5394 - mae: 1.2014 - val_loss: 5.1943 - val_mse: 5.1943 - val_mae: 1.2175 - lr: 3.4389e-04 - 12s/epoch - 12ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 12s - loss: 8.2182 - mse: 8.2182 - mae: 1.1890 - val_loss: 5.3444 - val_mse: 5.3444 - val_mae: 1.2604 - lr: 3.4389e-04 - 12s/epoch - 12ms/step\n",
            "Score for fold 4: loss of 5.3444390296936035\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 12s - loss: 8.3598 - mse: 8.3598 - mae: 1.2234 - val_loss: 4.8999 - val_mse: 4.8999 - val_mae: 1.1962 - lr: 3.4389e-04 - 12s/epoch - 12ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 12s - loss: 8.0976 - mse: 8.0976 - mae: 1.2080 - val_loss: 5.5709 - val_mse: 5.5709 - val_mae: 1.1290 - lr: 3.4389e-04 - 12s/epoch - 12ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 12s - loss: 7.8755 - mse: 7.8755 - mae: 1.1955 - val_loss: 6.8082 - val_mse: 6.8082 - val_mae: 1.2059 - lr: 3.4389e-04 - 12s/epoch - 12ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 12s - loss: 7.8236 - mse: 7.8236 - mae: 1.1779 - val_loss: 5.2709 - val_mse: 5.2709 - val_mae: 1.2584 - lr: 3.4389e-04 - 12s/epoch - 12ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 12s - loss: 7.4916 - mse: 7.4916 - mae: 1.1634 - val_loss: 5.4649 - val_mse: 5.4649 - val_mae: 1.2101 - lr: 3.4389e-04 - 12s/epoch - 12ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 12s - loss: 7.3585 - mse: 7.3585 - mae: 1.1557 - val_loss: 5.5654 - val_mse: 5.5654 - val_mae: 1.3508 - lr: 3.4389e-04 - 12s/epoch - 12ms/step\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-10-12 02:21:49,064]\u001b[0m Finished trial#49 resulted in value: 9.900000000000002. Current best value is 9.116 with parameters: {'activation': 'relu', 'num_hidden_layer': 4, 'i': 10, 'learning_rate': 0.0002344581502453532}.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Score for fold 5: loss of 5.565433025360107\n"
          ]
        }
      ],
      "source": [
        "model_list=[]\n",
        "history_list=[]\n",
        "study_name = 'NN_study_3'\n",
        "study = optuna.create_study(study_name=study_name, load_if_exists=True)\n",
        "func = lambda trial: objective(trial,training_shuffled2,labelsForTrain_shuffled2)\n",
        "study.optimize(func, n_trials=50,)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UqBj5WCO_2P6",
        "outputId": "517ece84-3d3a-444c-9f2b-bc2d5c7f41ef"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "1250/1250 - 11s - loss: 13.1419 - mse: 13.1419 - mae: 1.5370 - val_loss: 10.2413 - val_mse: 10.2413 - val_mae: 1.5060 - lr: 2.3446e-04 - 11s/epoch - 9ms/step\n",
            "Epoch 2/20\n",
            "1250/1250 - 10s - loss: 12.4282 - mse: 12.4282 - mae: 1.4893 - val_loss: 9.8529 - val_mse: 9.8529 - val_mae: 1.6123 - lr: 2.3446e-04 - 10s/epoch - 8ms/step\n",
            "Epoch 3/20\n",
            "1250/1250 - 10s - loss: 12.1646 - mse: 12.1646 - mae: 1.4766 - val_loss: 9.1604 - val_mse: 9.1604 - val_mae: 1.4807 - lr: 2.3446e-04 - 10s/epoch - 8ms/step\n",
            "Epoch 4/20\n",
            "1250/1250 - 10s - loss: 12.0066 - mse: 12.0066 - mae: 1.4636 - val_loss: 10.1957 - val_mse: 10.1957 - val_mae: 1.4743 - lr: 2.3446e-04 - 10s/epoch - 8ms/step\n",
            "Epoch 5/20\n",
            "1250/1250 - 10s - loss: 11.9720 - mse: 11.9720 - mae: 1.4554 - val_loss: 9.0260 - val_mse: 9.0260 - val_mae: 1.4675 - lr: 2.3446e-04 - 10s/epoch - 8ms/step\n",
            "Epoch 6/20\n",
            "1250/1250 - 10s - loss: 11.7660 - mse: 11.7660 - mae: 1.4424 - val_loss: 9.7267 - val_mse: 9.7267 - val_mae: 1.4576 - lr: 2.3446e-04 - 10s/epoch - 8ms/step\n",
            "Epoch 7/20\n",
            "1250/1250 - 10s - loss: 11.5622 - mse: 11.5622 - mae: 1.4373 - val_loss: 9.4200 - val_mse: 9.4200 - val_mae: 1.4874 - lr: 2.3446e-04 - 10s/epoch - 8ms/step\n",
            "Epoch 8/20\n",
            "1250/1250 - 10s - loss: 11.3568 - mse: 11.3568 - mae: 1.4273 - val_loss: 9.1225 - val_mse: 9.1225 - val_mae: 1.5552 - lr: 2.3446e-04 - 10s/epoch - 8ms/step\n",
            "Epoch 9/20\n",
            "1250/1250 - 10s - loss: 10.9840 - mse: 10.9840 - mae: 1.4157 - val_loss: 9.0278 - val_mse: 9.0278 - val_mae: 1.5072 - lr: 2.3446e-04 - 10s/epoch - 8ms/step\n",
            "Epoch 10/20\n",
            "1250/1250 - 10s - loss: 11.0157 - mse: 11.0157 - mae: 1.4047 - val_loss: 8.8614 - val_mse: 8.8614 - val_mae: 1.4845 - lr: 2.3446e-04 - 10s/epoch - 8ms/step\n",
            "Epoch 11/20\n",
            "1250/1250 - 10s - loss: 10.7150 - mse: 10.7150 - mae: 1.3941 - val_loss: 9.7109 - val_mse: 9.7109 - val_mae: 1.4740 - lr: 2.3446e-04 - 10s/epoch - 8ms/step\n",
            "Epoch 12/20\n",
            "1250/1250 - 10s - loss: 10.4707 - mse: 10.4707 - mae: 1.3865 - val_loss: 10.1400 - val_mse: 10.1400 - val_mae: 1.5182 - lr: 2.3446e-04 - 10s/epoch - 8ms/step\n",
            "Epoch 13/20\n",
            "1250/1250 - 10s - loss: 10.3052 - mse: 10.3052 - mae: 1.3775 - val_loss: 9.6144 - val_mse: 9.6144 - val_mae: 1.5797 - lr: 2.3446e-04 - 10s/epoch - 8ms/step\n",
            "Epoch 14/20\n",
            "1250/1250 - 10s - loss: 10.1206 - mse: 10.1206 - mae: 1.3595 - val_loss: 9.4414 - val_mse: 9.4414 - val_mae: 1.5018 - lr: 2.3446e-04 - 10s/epoch - 8ms/step\n",
            "Epoch 15/20\n",
            "1250/1250 - 10s - loss: 10.0996 - mse: 10.0996 - mae: 1.3454 - val_loss: 9.1222 - val_mse: 9.1222 - val_mae: 1.5016 - lr: 2.3446e-04 - 10s/epoch - 8ms/step\n",
            "Epoch 16/20\n",
            "1250/1250 - 10s - loss: 9.5527 - mse: 9.5527 - mae: 1.3268 - val_loss: 9.7223 - val_mse: 9.7223 - val_mae: 1.5239 - lr: 2.3446e-04 - 10s/epoch - 8ms/step\n",
            "Epoch 17/20\n",
            "1250/1250 - 10s - loss: 9.3952 - mse: 9.3952 - mae: 1.3132 - val_loss: 9.4549 - val_mse: 9.4549 - val_mae: 1.6828 - lr: 2.3446e-04 - 10s/epoch - 8ms/step\n",
            "Epoch 18/20\n",
            "1250/1250 - 10s - loss: 9.0403 - mse: 9.0403 - mae: 1.3023 - val_loss: 9.2816 - val_mse: 9.2816 - val_mae: 1.5681 - lr: 2.3446e-04 - 10s/epoch - 8ms/step\n",
            "Epoch 19/20\n",
            "1250/1250 - 10s - loss: 9.0020 - mse: 9.0020 - mae: 1.2959 - val_loss: 10.4088 - val_mse: 10.4088 - val_mae: 1.4542 - lr: 2.3446e-04 - 10s/epoch - 8ms/step\n",
            "Epoch 20/20\n",
            "1250/1250 - 10s - loss: 8.8067 - mse: 8.8067 - mae: 1.2857 - val_loss: 9.5989 - val_mse: 9.5989 - val_mae: 1.6087 - lr: 2.3446e-04 - 10s/epoch - 8ms/step\n"
          ]
        }
      ],
      "source": [
        "#{'activation': 'relu', 'num_hidden_layer': 4, 'i': 10, 'learning_rate': 0.0002344581502453532}.\n",
        "optimizer = Adam(learning_rate=0.0002344581502453532 ,clipnorm=1.0)\n",
        "model_3 = create_model(activation=\"relu\",num_hidden_layer=4,num_hidden_unit=1024)\n",
        "\n",
        "es = EarlyStopping(monitor='mse', patience=5)\n",
        "reduce_lr = ReduceLROnPlateau(monitor='val_mse', factor=0.2,\n",
        "                              patience=20, min_lr=0.001)\n",
        "model_3.compile(loss='mse',optimizer=optimizer,metrics=['mse','mae'])\n",
        "history = model_3.fit(training,labelsForTrain,\n",
        "                batch_size=64,\n",
        "                epochs=20,\n",
        "                verbose=2,\n",
        "                validation_data=(valing,labelsForVal),\n",
        "                validation_batch_size=64,\n",
        "                callbacks=[es,reduce_lr])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P873ZE1UCcDx",
        "outputId": "ffc37792-dafe-4506-ca96-331e51e52651"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "313/313 [==============================] - 1s 3ms/step - loss: 12.7887 - mse: 12.7887 - mae: 1.5975\n"
          ]
        }
      ],
      "source": [
        "results_model3 = model_3.evaluate(testing, labelsForTest, batch_size=32)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x5qEUSduCg37"
      },
      "source": [
        "## Shuffle Repetation 4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NdMGB-QGCk7r"
      },
      "outputs": [],
      "source": [
        "# Shuffle the data after train\n",
        "shuffled3 = shuffle(train_df, random_state=3)\n",
        "training_shuffled3,labelsForTrain_shuffled3=process_shuffle_dataset(shuffled3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pOGCiNHdCpn9",
        "outputId": "fb096794-34ea-4fe1-b983-6b8f626916a9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 3s - loss: 14.5177 - mse: 14.5177 - mae: 1.5928 - val_loss: 12.4316 - val_mse: 12.4316 - val_mae: 1.4930 - lr: 1.8369e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 13.1106 - mse: 13.1106 - mae: 1.5187 - val_loss: 12.0437 - val_mse: 12.0437 - val_mae: 1.4482 - lr: 1.8369e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 12.8143 - mse: 12.8143 - mae: 1.5012 - val_loss: 11.9883 - val_mse: 11.9883 - val_mae: 1.4621 - lr: 1.8369e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 12.5671 - mse: 12.5671 - mae: 1.4891 - val_loss: 11.8943 - val_mse: 11.8943 - val_mae: 1.5048 - lr: 1.8369e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 12.3520 - mse: 12.3520 - mae: 1.4801 - val_loss: 11.9016 - val_mse: 11.9016 - val_mae: 1.4663 - lr: 1.8369e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 12.2087 - mse: 12.2087 - mae: 1.4757 - val_loss: 11.9011 - val_mse: 11.9011 - val_mae: 1.4375 - lr: 1.8369e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 12.1460 - mse: 12.1460 - mae: 1.4651 - val_loss: 11.6970 - val_mse: 11.6970 - val_mae: 1.4786 - lr: 1.8369e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 2s - loss: 12.0613 - mse: 12.0613 - mae: 1.4630 - val_loss: 12.0372 - val_mse: 12.0372 - val_mae: 1.4810 - lr: 1.8369e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 2s - loss: 12.0092 - mse: 12.0092 - mae: 1.4589 - val_loss: 11.9115 - val_mse: 11.9115 - val_mae: 1.4670 - lr: 1.8369e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 2s - loss: 12.0095 - mse: 12.0095 - mae: 1.4565 - val_loss: 11.6435 - val_mse: 11.6435 - val_mae: 1.4719 - lr: 1.8369e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 2s - loss: 11.9022 - mse: 11.9022 - mae: 1.4511 - val_loss: 11.9254 - val_mse: 11.9254 - val_mae: 1.4258 - lr: 1.8369e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 2s - loss: 11.8558 - mse: 11.8558 - mae: 1.4489 - val_loss: 12.3072 - val_mse: 12.3072 - val_mae: 1.4252 - lr: 1.8369e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 13/100\n",
            "1000/1000 - 2s - loss: 11.8927 - mse: 11.8927 - mae: 1.4470 - val_loss: 11.7769 - val_mse: 11.7769 - val_mae: 1.4504 - lr: 1.8369e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 14/100\n",
            "1000/1000 - 2s - loss: 11.8079 - mse: 11.8079 - mae: 1.4424 - val_loss: 11.9207 - val_mse: 11.9207 - val_mae: 1.4476 - lr: 1.8369e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 15/100\n",
            "1000/1000 - 2s - loss: 11.8302 - mse: 11.8302 - mae: 1.4424 - val_loss: 11.8335 - val_mse: 11.8335 - val_mae: 1.4512 - lr: 1.8369e-04 - 2s/epoch - 2ms/step\n",
            "Score for fold 1: loss of 11.833491325378418\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 12.5360 - mse: 12.5360 - mae: 1.4376 - val_loss: 8.8145 - val_mse: 8.8145 - val_mae: 1.4072 - lr: 1.8369e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 12.5397 - mse: 12.5397 - mae: 1.4352 - val_loss: 8.7242 - val_mse: 8.7242 - val_mae: 1.4238 - lr: 1.8369e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 12.4921 - mse: 12.4921 - mae: 1.4351 - val_loss: 8.6557 - val_mse: 8.6557 - val_mae: 1.4550 - lr: 1.8369e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 12.4426 - mse: 12.4426 - mae: 1.4315 - val_loss: 8.6816 - val_mse: 8.6816 - val_mae: 1.5018 - lr: 1.8369e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 12.4112 - mse: 12.4112 - mae: 1.4291 - val_loss: 8.6905 - val_mse: 8.6905 - val_mae: 1.4263 - lr: 1.8369e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 12.3164 - mse: 12.3164 - mae: 1.4286 - val_loss: 8.6451 - val_mse: 8.6451 - val_mae: 1.4284 - lr: 1.8369e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 12.2373 - mse: 12.2373 - mae: 1.4249 - val_loss: 8.6978 - val_mse: 8.6978 - val_mae: 1.4217 - lr: 1.8369e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 2s - loss: 12.3342 - mse: 12.3342 - mae: 1.4264 - val_loss: 8.7016 - val_mse: 8.7016 - val_mae: 1.3906 - lr: 1.8369e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 2s - loss: 12.2460 - mse: 12.2460 - mae: 1.4180 - val_loss: 8.7530 - val_mse: 8.7530 - val_mae: 1.4370 - lr: 1.8369e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 2s - loss: 12.2625 - mse: 12.2625 - mae: 1.4224 - val_loss: 8.6677 - val_mse: 8.6677 - val_mae: 1.4283 - lr: 1.8369e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 2s - loss: 12.2000 - mse: 12.2000 - mae: 1.4179 - val_loss: 8.6820 - val_mse: 8.6820 - val_mae: 1.4267 - lr: 1.8369e-04 - 2s/epoch - 2ms/step\n",
            "Score for fold 2: loss of 8.681947708129883\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 10.5304 - mse: 10.5304 - mae: 1.4196 - val_loss: 15.5784 - val_mse: 15.5784 - val_mae: 1.4418 - lr: 1.8369e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 10.4585 - mse: 10.4585 - mae: 1.4176 - val_loss: 15.6351 - val_mse: 15.6351 - val_mae: 1.3996 - lr: 1.8369e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 10.3238 - mse: 10.3238 - mae: 1.4182 - val_loss: 15.6671 - val_mse: 15.6671 - val_mae: 1.4445 - lr: 1.8369e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 10.3634 - mse: 10.3634 - mae: 1.4183 - val_loss: 15.7617 - val_mse: 15.7617 - val_mae: 1.4373 - lr: 1.8369e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 10.3522 - mse: 10.3522 - mae: 1.4118 - val_loss: 15.9584 - val_mse: 15.9584 - val_mae: 1.4570 - lr: 1.8369e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 10.3220 - mse: 10.3220 - mae: 1.4105 - val_loss: 15.7199 - val_mse: 15.7199 - val_mae: 1.4128 - lr: 1.8369e-04 - 2s/epoch - 2ms/step\n",
            "Score for fold 3: loss of 15.719951629638672\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 12.1190 - mse: 12.1190 - mae: 1.4214 - val_loss: 8.5310 - val_mse: 8.5310 - val_mae: 1.3966 - lr: 1.8369e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 12.0827 - mse: 12.0827 - mae: 1.4177 - val_loss: 8.6748 - val_mse: 8.6748 - val_mae: 1.4020 - lr: 1.8369e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 12.1385 - mse: 12.1385 - mae: 1.4180 - val_loss: 8.7300 - val_mse: 8.7300 - val_mae: 1.4258 - lr: 1.8369e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 12.0571 - mse: 12.0571 - mae: 1.4167 - val_loss: 8.7238 - val_mse: 8.7238 - val_mae: 1.3867 - lr: 1.8369e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 11.9989 - mse: 11.9989 - mae: 1.4134 - val_loss: 8.7235 - val_mse: 8.7235 - val_mae: 1.3969 - lr: 1.8369e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 11.9462 - mse: 11.9462 - mae: 1.4109 - val_loss: 8.6640 - val_mse: 8.6640 - val_mae: 1.4350 - lr: 1.8369e-04 - 2s/epoch - 2ms/step\n",
            "Score for fold 4: loss of 8.664026260375977\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 10.8879 - mse: 10.8879 - mae: 1.4142 - val_loss: 12.8264 - val_mse: 12.8264 - val_mae: 1.4387 - lr: 1.8369e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 10.8826 - mse: 10.8826 - mae: 1.4054 - val_loss: 12.8510 - val_mse: 12.8510 - val_mae: 1.4462 - lr: 1.8369e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 10.8251 - mse: 10.8251 - mae: 1.4082 - val_loss: 13.0173 - val_mse: 13.0173 - val_mae: 1.4022 - lr: 1.8369e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 10.8523 - mse: 10.8523 - mae: 1.4033 - val_loss: 13.1009 - val_mse: 13.1009 - val_mae: 1.3972 - lr: 1.8369e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 10.7257 - mse: 10.7257 - mae: 1.4021 - val_loss: 13.0109 - val_mse: 13.0109 - val_mae: 1.4196 - lr: 1.8369e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 10.6435 - mse: 10.6435 - mae: 1.4004 - val_loss: 13.0732 - val_mse: 13.0732 - val_mae: 1.4408 - lr: 1.8369e-04 - 2s/epoch - 2ms/step\n",
            "Score for fold 5: loss of 13.07322883605957\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-10-12 02:36:13,349]\u001b[0m Finished trial#0 resulted in value: 11.592. Current best value is 11.592 with parameters: {'activation': 'relu', 'num_hidden_layer': 3, 'i': 7, 'learning_rate': 0.00018369073169150139}.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 6s - loss: 13.6484 - mse: 13.6484 - mae: 1.6678 - val_loss: 19.2139 - val_mse: 19.2139 - val_mae: 1.6074 - lr: 0.0037 - 6s/epoch - 6ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 5s - loss: 12.5718 - mse: 12.5718 - mae: 1.5947 - val_loss: 18.6021 - val_mse: 18.6021 - val_mae: 1.5334 - lr: 0.0037 - 5s/epoch - 5ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 5s - loss: 12.2756 - mse: 12.2756 - mae: 1.5704 - val_loss: 18.1239 - val_mse: 18.1239 - val_mae: 1.4676 - lr: 0.0037 - 5s/epoch - 5ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 5s - loss: 12.1104 - mse: 12.1104 - mae: 1.5852 - val_loss: 19.0319 - val_mse: 19.0319 - val_mae: 1.6420 - lr: 0.0037 - 5s/epoch - 5ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 5s - loss: 12.0666 - mse: 12.0666 - mae: 1.5802 - val_loss: 18.0740 - val_mse: 18.0740 - val_mae: 1.4659 - lr: 0.0037 - 5s/epoch - 5ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 5s - loss: 11.9297 - mse: 11.9297 - mae: 1.5754 - val_loss: 18.0285 - val_mse: 18.0285 - val_mae: 1.7074 - lr: 0.0037 - 5s/epoch - 5ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 5s - loss: 11.8834 - mse: 11.8834 - mae: 1.5652 - val_loss: 18.9921 - val_mse: 18.9921 - val_mae: 1.4490 - lr: 0.0037 - 5s/epoch - 5ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 5s - loss: 11.7382 - mse: 11.7382 - mae: 1.5720 - val_loss: 17.7224 - val_mse: 17.7224 - val_mae: 1.5611 - lr: 0.0037 - 5s/epoch - 5ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 5s - loss: 11.7950 - mse: 11.7950 - mae: 1.5703 - val_loss: 19.0742 - val_mse: 19.0742 - val_mae: 1.5645 - lr: 0.0037 - 5s/epoch - 5ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 5s - loss: 11.8208 - mse: 11.8208 - mae: 1.5689 - val_loss: 17.5544 - val_mse: 17.5544 - val_mae: 1.4766 - lr: 0.0037 - 5s/epoch - 5ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 5s - loss: 11.6264 - mse: 11.6264 - mae: 1.5666 - val_loss: 17.5237 - val_mse: 17.5237 - val_mae: 1.5602 - lr: 0.0037 - 5s/epoch - 5ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 5s - loss: 11.7144 - mse: 11.7144 - mae: 1.5778 - val_loss: 17.7915 - val_mse: 17.7915 - val_mae: 1.4220 - lr: 0.0037 - 5s/epoch - 5ms/step\n",
            "Epoch 13/100\n",
            "1000/1000 - 5s - loss: 11.7170 - mse: 11.7170 - mae: 1.5659 - val_loss: 17.8589 - val_mse: 17.8589 - val_mae: 1.4280 - lr: 0.0037 - 5s/epoch - 5ms/step\n",
            "Epoch 14/100\n",
            "1000/1000 - 5s - loss: 11.7571 - mse: 11.7571 - mae: 1.5788 - val_loss: 18.1370 - val_mse: 18.1370 - val_mae: 1.4196 - lr: 0.0037 - 5s/epoch - 5ms/step\n",
            "Epoch 15/100\n",
            "1000/1000 - 4s - loss: 11.5420 - mse: 11.5420 - mae: 1.5611 - val_loss: 18.2350 - val_mse: 18.2350 - val_mae: 1.4737 - lr: 0.0037 - 4s/epoch - 4ms/step\n",
            "Epoch 16/100\n",
            "1000/1000 - 4s - loss: 11.5552 - mse: 11.5552 - mae: 1.5598 - val_loss: 18.2763 - val_mse: 18.2763 - val_mae: 1.4114 - lr: 0.0037 - 4s/epoch - 4ms/step\n",
            "Score for fold 1: loss of 18.276329040527344\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 5s - loss: 13.1419 - mse: 13.1419 - mae: 1.4702 - val_loss: 8.5450 - val_mse: 8.5450 - val_mae: 1.4041 - lr: 0.0010 - 5s/epoch - 5ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 5s - loss: 12.7713 - mse: 12.7713 - mae: 1.4503 - val_loss: 8.6255 - val_mse: 8.6255 - val_mae: 1.4012 - lr: 0.0010 - 5s/epoch - 5ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 5s - loss: 12.6533 - mse: 12.6533 - mae: 1.4482 - val_loss: 8.5236 - val_mse: 8.5236 - val_mae: 1.4107 - lr: 0.0010 - 5s/epoch - 5ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 5s - loss: 12.5035 - mse: 12.5035 - mae: 1.4427 - val_loss: 8.6150 - val_mse: 8.6150 - val_mae: 1.4620 - lr: 0.0010 - 5s/epoch - 5ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 5s - loss: 12.2961 - mse: 12.2961 - mae: 1.4307 - val_loss: 8.7389 - val_mse: 8.7389 - val_mae: 1.4864 - lr: 0.0010 - 5s/epoch - 5ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 5s - loss: 12.2085 - mse: 12.2085 - mae: 1.4280 - val_loss: 8.5083 - val_mse: 8.5083 - val_mae: 1.5553 - lr: 0.0010 - 5s/epoch - 5ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 5s - loss: 12.0970 - mse: 12.0970 - mae: 1.4259 - val_loss: 8.8224 - val_mse: 8.8224 - val_mae: 1.4170 - lr: 0.0010 - 5s/epoch - 5ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 5s - loss: 12.0098 - mse: 12.0098 - mae: 1.4223 - val_loss: 8.6687 - val_mse: 8.6687 - val_mae: 1.5137 - lr: 0.0010 - 5s/epoch - 5ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 5s - loss: 11.9526 - mse: 11.9526 - mae: 1.4233 - val_loss: 8.9076 - val_mse: 8.9076 - val_mae: 1.4288 - lr: 0.0010 - 5s/epoch - 5ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 5s - loss: 11.8736 - mse: 11.8736 - mae: 1.4202 - val_loss: 8.8988 - val_mse: 8.8988 - val_mae: 1.5411 - lr: 0.0010 - 5s/epoch - 5ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 5s - loss: 11.8200 - mse: 11.8200 - mae: 1.4259 - val_loss: 8.8976 - val_mse: 8.8976 - val_mae: 1.4270 - lr: 0.0010 - 5s/epoch - 5ms/step\n",
            "Score for fold 2: loss of 8.897639274597168\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 5s - loss: 10.6087 - mse: 10.6087 - mae: 1.4314 - val_loss: 13.4960 - val_mse: 13.4960 - val_mae: 1.4262 - lr: 0.0010 - 5s/epoch - 5ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 5s - loss: 10.4536 - mse: 10.4536 - mae: 1.4224 - val_loss: 13.5910 - val_mse: 13.5910 - val_mae: 1.3933 - lr: 0.0010 - 5s/epoch - 5ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 4s - loss: 10.3666 - mse: 10.3666 - mae: 1.4149 - val_loss: 13.8421 - val_mse: 13.8421 - val_mae: 1.4938 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 4s - loss: 10.2477 - mse: 10.2477 - mae: 1.4105 - val_loss: 14.0693 - val_mse: 14.0693 - val_mae: 1.4477 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 4s - loss: 10.1681 - mse: 10.1681 - mae: 1.4071 - val_loss: 14.0136 - val_mse: 14.0136 - val_mae: 1.6255 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 4s - loss: 10.0630 - mse: 10.0630 - mae: 1.4045 - val_loss: 14.3483 - val_mse: 14.3483 - val_mae: 1.5283 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Score for fold 3: loss of 14.34830093383789\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 5s - loss: 10.9516 - mse: 10.9516 - mae: 1.4201 - val_loss: 11.0211 - val_mse: 11.0211 - val_mae: 1.3198 - lr: 0.0010 - 5s/epoch - 5ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 5s - loss: 10.7164 - mse: 10.7164 - mae: 1.4117 - val_loss: 11.0302 - val_mse: 11.0302 - val_mae: 1.3687 - lr: 0.0010 - 5s/epoch - 5ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 5s - loss: 10.4980 - mse: 10.4980 - mae: 1.3988 - val_loss: 11.1387 - val_mse: 11.1387 - val_mae: 1.3611 - lr: 0.0010 - 5s/epoch - 5ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 5s - loss: 10.4869 - mse: 10.4869 - mae: 1.3961 - val_loss: 11.3650 - val_mse: 11.3650 - val_mae: 1.3547 - lr: 0.0010 - 5s/epoch - 5ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 5s - loss: 10.3741 - mse: 10.3741 - mae: 1.3980 - val_loss: 11.3445 - val_mse: 11.3445 - val_mae: 1.4155 - lr: 0.0010 - 5s/epoch - 5ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 5s - loss: 10.1374 - mse: 10.1374 - mae: 1.3830 - val_loss: 11.4644 - val_mse: 11.4644 - val_mae: 1.5326 - lr: 0.0010 - 5s/epoch - 5ms/step\n",
            "Score for fold 4: loss of 11.464385986328125\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 5s - loss: 11.5812 - mse: 11.5812 - mae: 1.4202 - val_loss: 5.6751 - val_mse: 5.6751 - val_mae: 1.3068 - lr: 0.0010 - 5s/epoch - 5ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 5s - loss: 11.4664 - mse: 11.4664 - mae: 1.4060 - val_loss: 5.9901 - val_mse: 5.9901 - val_mae: 1.4572 - lr: 0.0010 - 5s/epoch - 5ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 5s - loss: 11.3706 - mse: 11.3706 - mae: 1.3998 - val_loss: 5.9045 - val_mse: 5.9045 - val_mae: 1.3849 - lr: 0.0010 - 5s/epoch - 5ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 5s - loss: 11.2208 - mse: 11.2208 - mae: 1.4017 - val_loss: 5.9185 - val_mse: 5.9185 - val_mae: 1.3592 - lr: 0.0010 - 5s/epoch - 5ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 5s - loss: 11.0658 - mse: 11.0658 - mae: 1.3889 - val_loss: 6.1039 - val_mse: 6.1039 - val_mae: 1.2808 - lr: 0.0010 - 5s/epoch - 5ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 4s - loss: 10.9707 - mse: 10.9707 - mae: 1.3840 - val_loss: 5.9733 - val_mse: 5.9733 - val_mae: 1.4023 - lr: 0.0010 - 4s/epoch - 4ms/step\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-10-12 02:39:48,200]\u001b[0m Finished trial#1 resulted in value: 11.792. Current best value is 11.592 with parameters: {'activation': 'relu', 'num_hidden_layer': 3, 'i': 7, 'learning_rate': 0.00018369073169150139}.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Score for fold 5: loss of 5.973310470581055\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 13.2759 - mse: 13.2759 - mae: 1.5887 - val_loss: 19.7385 - val_mse: 19.7385 - val_mae: 1.5228 - lr: 0.0021 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 12.1874 - mse: 12.1874 - mae: 1.5098 - val_loss: 19.4972 - val_mse: 19.4972 - val_mae: 1.4833 - lr: 0.0021 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 11.8884 - mse: 11.8884 - mae: 1.4930 - val_loss: 19.3158 - val_mse: 19.3158 - val_mae: 1.5234 - lr: 0.0021 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 11.7860 - mse: 11.7860 - mae: 1.4889 - val_loss: 19.4344 - val_mse: 19.4344 - val_mae: 1.5093 - lr: 0.0021 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 11.7095 - mse: 11.7095 - mae: 1.4882 - val_loss: 19.3072 - val_mse: 19.3072 - val_mae: 1.4928 - lr: 0.0021 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 11.6063 - mse: 11.6063 - mae: 1.4850 - val_loss: 19.1270 - val_mse: 19.1270 - val_mae: 1.4805 - lr: 0.0021 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 11.5473 - mse: 11.5473 - mae: 1.4819 - val_loss: 19.0955 - val_mse: 19.0955 - val_mae: 1.4372 - lr: 0.0021 - 2s/epoch - 2ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 2s - loss: 11.4760 - mse: 11.4760 - mae: 1.4750 - val_loss: 19.0384 - val_mse: 19.0384 - val_mae: 1.4622 - lr: 0.0021 - 2s/epoch - 2ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 2s - loss: 11.4317 - mse: 11.4317 - mae: 1.4760 - val_loss: 19.0662 - val_mse: 19.0662 - val_mae: 1.4643 - lr: 0.0021 - 2s/epoch - 2ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 2s - loss: 11.3896 - mse: 11.3896 - mae: 1.4736 - val_loss: 18.9876 - val_mse: 18.9876 - val_mae: 1.5348 - lr: 0.0021 - 2s/epoch - 2ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 2s - loss: 11.3458 - mse: 11.3458 - mae: 1.4729 - val_loss: 18.9483 - val_mse: 18.9483 - val_mae: 1.4587 - lr: 0.0021 - 2s/epoch - 2ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 2s - loss: 11.3200 - mse: 11.3200 - mae: 1.4715 - val_loss: 18.9375 - val_mse: 18.9375 - val_mae: 1.4672 - lr: 0.0021 - 2s/epoch - 2ms/step\n",
            "Epoch 13/100\n",
            "1000/1000 - 2s - loss: 11.2731 - mse: 11.2731 - mae: 1.4703 - val_loss: 19.0496 - val_mse: 19.0496 - val_mae: 1.5015 - lr: 0.0021 - 2s/epoch - 2ms/step\n",
            "Epoch 14/100\n",
            "1000/1000 - 2s - loss: 11.2721 - mse: 11.2721 - mae: 1.4756 - val_loss: 19.0366 - val_mse: 19.0366 - val_mae: 1.5154 - lr: 0.0021 - 2s/epoch - 2ms/step\n",
            "Epoch 15/100\n",
            "1000/1000 - 2s - loss: 11.2260 - mse: 11.2260 - mae: 1.4699 - val_loss: 18.9256 - val_mse: 18.9256 - val_mae: 1.4677 - lr: 0.0021 - 2s/epoch - 2ms/step\n",
            "Epoch 16/100\n",
            "1000/1000 - 2s - loss: 11.2043 - mse: 11.2043 - mae: 1.4685 - val_loss: 18.9048 - val_mse: 18.9048 - val_mae: 1.4908 - lr: 0.0021 - 2s/epoch - 2ms/step\n",
            "Epoch 17/100\n",
            "1000/1000 - 2s - loss: 11.1981 - mse: 11.1981 - mae: 1.4725 - val_loss: 18.9393 - val_mse: 18.9393 - val_mae: 1.4808 - lr: 0.0021 - 2s/epoch - 2ms/step\n",
            "Epoch 18/100\n",
            "1000/1000 - 2s - loss: 11.1661 - mse: 11.1661 - mae: 1.4620 - val_loss: 18.9161 - val_mse: 18.9161 - val_mae: 1.4461 - lr: 0.0021 - 2s/epoch - 2ms/step\n",
            "Epoch 19/100\n",
            "1000/1000 - 2s - loss: 11.1493 - mse: 11.1493 - mae: 1.4694 - val_loss: 18.7842 - val_mse: 18.7842 - val_mae: 1.4990 - lr: 0.0021 - 2s/epoch - 2ms/step\n",
            "Epoch 20/100\n",
            "1000/1000 - 2s - loss: 11.1014 - mse: 11.1014 - mae: 1.4640 - val_loss: 18.7625 - val_mse: 18.7625 - val_mae: 1.4545 - lr: 0.0021 - 2s/epoch - 2ms/step\n",
            "Epoch 21/100\n",
            "1000/1000 - 2s - loss: 11.0620 - mse: 11.0620 - mae: 1.4686 - val_loss: 18.9406 - val_mse: 18.9406 - val_mae: 1.4576 - lr: 0.0021 - 2s/epoch - 2ms/step\n",
            "Epoch 22/100\n",
            "1000/1000 - 2s - loss: 11.0427 - mse: 11.0427 - mae: 1.4641 - val_loss: 19.0005 - val_mse: 19.0005 - val_mae: 1.4895 - lr: 0.0021 - 2s/epoch - 2ms/step\n",
            "Epoch 23/100\n",
            "1000/1000 - 2s - loss: 11.0300 - mse: 11.0300 - mae: 1.4644 - val_loss: 18.9986 - val_mse: 18.9986 - val_mae: 1.5132 - lr: 0.0021 - 2s/epoch - 2ms/step\n",
            "Epoch 24/100\n",
            "1000/1000 - 2s - loss: 11.0131 - mse: 11.0131 - mae: 1.4674 - val_loss: 19.3203 - val_mse: 19.3203 - val_mae: 1.5594 - lr: 0.0021 - 2s/epoch - 2ms/step\n",
            "Epoch 25/100\n",
            "1000/1000 - 2s - loss: 11.0165 - mse: 11.0165 - mae: 1.4630 - val_loss: 18.9575 - val_mse: 18.9575 - val_mae: 1.4542 - lr: 0.0021 - 2s/epoch - 2ms/step\n",
            "Score for fold 1: loss of 18.95752716064453\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 12.0735 - mse: 12.0735 - mae: 1.4412 - val_loss: 13.8850 - val_mse: 13.8850 - val_mae: 1.4852 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 12.0152 - mse: 12.0152 - mae: 1.4402 - val_loss: 14.0148 - val_mse: 14.0148 - val_mae: 1.4715 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 12.0170 - mse: 12.0170 - mae: 1.4363 - val_loss: 14.0600 - val_mse: 14.0600 - val_mae: 1.4704 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 11.9848 - mse: 11.9848 - mae: 1.4328 - val_loss: 13.9472 - val_mse: 13.9472 - val_mae: 1.4811 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 11.9806 - mse: 11.9806 - mae: 1.4361 - val_loss: 13.9594 - val_mse: 13.9594 - val_mae: 1.4770 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 11.9513 - mse: 11.9513 - mae: 1.4366 - val_loss: 13.9572 - val_mse: 13.9572 - val_mae: 1.4614 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Score for fold 2: loss of 13.957201957702637\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 12.9160 - mse: 12.9160 - mae: 1.4475 - val_loss: 10.0301 - val_mse: 10.0301 - val_mae: 1.4360 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 12.8603 - mse: 12.8603 - mae: 1.4454 - val_loss: 10.0626 - val_mse: 10.0626 - val_mae: 1.4394 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 12.8478 - mse: 12.8478 - mae: 1.4439 - val_loss: 10.0637 - val_mse: 10.0637 - val_mae: 1.4401 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 12.8142 - mse: 12.8142 - mae: 1.4440 - val_loss: 10.0701 - val_mse: 10.0701 - val_mae: 1.4497 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 12.8028 - mse: 12.8028 - mae: 1.4467 - val_loss: 10.0986 - val_mse: 10.0986 - val_mae: 1.4692 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 12.7873 - mse: 12.7873 - mae: 1.4448 - val_loss: 10.2484 - val_mse: 10.2484 - val_mae: 1.4500 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Score for fold 3: loss of 10.24841594696045\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 12.5248 - mse: 12.5248 - mae: 1.4516 - val_loss: 11.1455 - val_mse: 11.1455 - val_mae: 1.4306 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 12.4789 - mse: 12.4789 - mae: 1.4501 - val_loss: 11.1358 - val_mse: 11.1358 - val_mae: 1.4459 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 12.4826 - mse: 12.4826 - mae: 1.4454 - val_loss: 11.1275 - val_mse: 11.1275 - val_mae: 1.4514 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 12.4569 - mse: 12.4569 - mae: 1.4513 - val_loss: 11.1740 - val_mse: 11.1740 - val_mae: 1.4070 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 12.4322 - mse: 12.4322 - mae: 1.4485 - val_loss: 11.2468 - val_mse: 11.2468 - val_mae: 1.4088 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 12.4316 - mse: 12.4316 - mae: 1.4464 - val_loss: 11.2323 - val_mse: 11.2323 - val_mae: 1.4144 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 12.4182 - mse: 12.4182 - mae: 1.4479 - val_loss: 11.2384 - val_mse: 11.2384 - val_mae: 1.4466 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 2s - loss: 12.4014 - mse: 12.4014 - mae: 1.4475 - val_loss: 11.2673 - val_mse: 11.2673 - val_mae: 1.4431 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Score for fold 4: loss of 11.267351150512695\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 13.2289 - mse: 13.2289 - mae: 1.4529 - val_loss: 7.8893 - val_mse: 7.8893 - val_mae: 1.4390 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 13.2301 - mse: 13.2301 - mae: 1.4498 - val_loss: 7.9060 - val_mse: 7.9060 - val_mae: 1.4640 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 13.1825 - mse: 13.1825 - mae: 1.4508 - val_loss: 7.9068 - val_mse: 7.9068 - val_mae: 1.4120 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 13.1671 - mse: 13.1671 - mae: 1.4499 - val_loss: 7.9710 - val_mse: 7.9710 - val_mae: 1.4590 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 13.1616 - mse: 13.1616 - mae: 1.4482 - val_loss: 8.0272 - val_mse: 8.0272 - val_mae: 1.4915 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 13.1243 - mse: 13.1243 - mae: 1.4504 - val_loss: 7.9463 - val_mse: 7.9463 - val_mae: 1.4244 - lr: 0.0010 - 2s/epoch - 2ms/step\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-10-12 02:41:25,113]\u001b[0m Finished trial#2 resulted in value: 12.478. Current best value is 11.592 with parameters: {'activation': 'relu', 'num_hidden_layer': 3, 'i': 7, 'learning_rate': 0.00018369073169150139}.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Score for fold 5: loss of 7.946262836456299\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 14.1025 - mse: 14.1025 - mae: 1.5806 - val_loss: 15.3525 - val_mse: 15.3525 - val_mae: 1.5701 - lr: 0.0057 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 13.3967 - mse: 13.3967 - mae: 1.5330 - val_loss: 15.3347 - val_mse: 15.3347 - val_mae: 1.5628 - lr: 0.0057 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 13.2648 - mse: 13.2648 - mae: 1.5182 - val_loss: 15.0387 - val_mse: 15.0387 - val_mae: 1.4874 - lr: 0.0057 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 13.0754 - mse: 13.0754 - mae: 1.5063 - val_loss: 15.2306 - val_mse: 15.2306 - val_mae: 1.5397 - lr: 0.0057 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 13.0883 - mse: 13.0883 - mae: 1.5036 - val_loss: 14.7304 - val_mse: 14.7304 - val_mae: 1.5013 - lr: 0.0057 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 12.9214 - mse: 12.9214 - mae: 1.5013 - val_loss: 14.7029 - val_mse: 14.7029 - val_mae: 1.5142 - lr: 0.0057 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 12.8869 - mse: 12.8869 - mae: 1.4999 - val_loss: 14.7053 - val_mse: 14.7053 - val_mae: 1.4515 - lr: 0.0057 - 2s/epoch - 2ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 2s - loss: 12.8820 - mse: 12.8820 - mae: 1.4979 - val_loss: 14.5375 - val_mse: 14.5375 - val_mae: 1.4883 - lr: 0.0057 - 2s/epoch - 2ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 2s - loss: 12.8068 - mse: 12.8068 - mae: 1.5000 - val_loss: 14.4087 - val_mse: 14.4087 - val_mae: 1.5663 - lr: 0.0057 - 2s/epoch - 2ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 2s - loss: 12.7642 - mse: 12.7642 - mae: 1.4951 - val_loss: 14.0920 - val_mse: 14.0920 - val_mae: 1.5155 - lr: 0.0057 - 2s/epoch - 2ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 2s - loss: 12.7474 - mse: 12.7474 - mae: 1.4982 - val_loss: 14.1579 - val_mse: 14.1579 - val_mae: 1.5452 - lr: 0.0057 - 2s/epoch - 2ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 2s - loss: 12.6859 - mse: 12.6859 - mae: 1.4954 - val_loss: 14.0959 - val_mse: 14.0959 - val_mae: 1.5194 - lr: 0.0057 - 2s/epoch - 2ms/step\n",
            "Epoch 13/100\n",
            "1000/1000 - 2s - loss: 12.6502 - mse: 12.6502 - mae: 1.4924 - val_loss: 13.9890 - val_mse: 13.9890 - val_mae: 1.4982 - lr: 0.0057 - 2s/epoch - 2ms/step\n",
            "Epoch 14/100\n",
            "1000/1000 - 2s - loss: 12.6657 - mse: 12.6657 - mae: 1.4966 - val_loss: 14.0020 - val_mse: 14.0020 - val_mae: 1.4997 - lr: 0.0057 - 2s/epoch - 2ms/step\n",
            "Epoch 15/100\n",
            "1000/1000 - 2s - loss: 12.6071 - mse: 12.6071 - mae: 1.4972 - val_loss: 14.0302 - val_mse: 14.0302 - val_mae: 1.5141 - lr: 0.0057 - 2s/epoch - 2ms/step\n",
            "Epoch 16/100\n",
            "1000/1000 - 2s - loss: 12.6166 - mse: 12.6166 - mae: 1.4972 - val_loss: 13.8095 - val_mse: 13.8095 - val_mae: 1.4924 - lr: 0.0057 - 2s/epoch - 2ms/step\n",
            "Epoch 17/100\n",
            "1000/1000 - 2s - loss: 12.6029 - mse: 12.6029 - mae: 1.4929 - val_loss: 14.1031 - val_mse: 14.1031 - val_mae: 1.5547 - lr: 0.0057 - 2s/epoch - 2ms/step\n",
            "Epoch 18/100\n",
            "1000/1000 - 2s - loss: 12.5877 - mse: 12.5877 - mae: 1.4989 - val_loss: 14.1700 - val_mse: 14.1700 - val_mae: 1.5067 - lr: 0.0057 - 2s/epoch - 2ms/step\n",
            "Epoch 19/100\n",
            "1000/1000 - 2s - loss: 12.5846 - mse: 12.5846 - mae: 1.4968 - val_loss: 13.8059 - val_mse: 13.8059 - val_mae: 1.5008 - lr: 0.0057 - 2s/epoch - 2ms/step\n",
            "Epoch 20/100\n",
            "1000/1000 - 2s - loss: 12.5393 - mse: 12.5393 - mae: 1.4990 - val_loss: 13.6970 - val_mse: 13.6970 - val_mae: 1.5346 - lr: 0.0057 - 2s/epoch - 2ms/step\n",
            "Epoch 21/100\n",
            "1000/1000 - 2s - loss: 12.5054 - mse: 12.5054 - mae: 1.4975 - val_loss: 13.9127 - val_mse: 13.9127 - val_mae: 1.5478 - lr: 0.0057 - 2s/epoch - 2ms/step\n",
            "Epoch 22/100\n",
            "1000/1000 - 2s - loss: 12.5463 - mse: 12.5463 - mae: 1.4991 - val_loss: 13.6556 - val_mse: 13.6556 - val_mae: 1.4841 - lr: 0.0057 - 2s/epoch - 2ms/step\n",
            "Epoch 23/100\n",
            "1000/1000 - 2s - loss: 12.5402 - mse: 12.5402 - mae: 1.4937 - val_loss: 13.6766 - val_mse: 13.6766 - val_mae: 1.5439 - lr: 0.0057 - 2s/epoch - 2ms/step\n",
            "Epoch 24/100\n",
            "1000/1000 - 2s - loss: 12.5506 - mse: 12.5506 - mae: 1.4959 - val_loss: 13.5698 - val_mse: 13.5698 - val_mae: 1.5941 - lr: 0.0057 - 2s/epoch - 2ms/step\n",
            "Epoch 25/100\n",
            "1000/1000 - 2s - loss: 12.5426 - mse: 12.5426 - mae: 1.4931 - val_loss: 13.4456 - val_mse: 13.4456 - val_mae: 1.5503 - lr: 0.0057 - 2s/epoch - 2ms/step\n",
            "Epoch 26/100\n",
            "1000/1000 - 2s - loss: 12.4382 - mse: 12.4382 - mae: 1.4953 - val_loss: 13.6941 - val_mse: 13.6941 - val_mae: 1.4707 - lr: 0.0057 - 2s/epoch - 2ms/step\n",
            "Epoch 27/100\n",
            "1000/1000 - 2s - loss: 12.3988 - mse: 12.3988 - mae: 1.4883 - val_loss: 13.7060 - val_mse: 13.7060 - val_mae: 1.5084 - lr: 0.0057 - 2s/epoch - 2ms/step\n",
            "Epoch 28/100\n",
            "1000/1000 - 2s - loss: 12.4992 - mse: 12.4992 - mae: 1.4899 - val_loss: 13.8678 - val_mse: 13.8678 - val_mae: 1.5793 - lr: 0.0057 - 2s/epoch - 2ms/step\n",
            "Epoch 29/100\n",
            "1000/1000 - 2s - loss: 12.4459 - mse: 12.4459 - mae: 1.4950 - val_loss: 13.8500 - val_mse: 13.8500 - val_mae: 1.4994 - lr: 0.0057 - 2s/epoch - 2ms/step\n",
            "Epoch 30/100\n",
            "1000/1000 - 2s - loss: 12.4139 - mse: 12.4139 - mae: 1.4876 - val_loss: 13.3710 - val_mse: 13.3710 - val_mae: 1.5284 - lr: 0.0057 - 2s/epoch - 2ms/step\n",
            "Epoch 31/100\n",
            "1000/1000 - 2s - loss: 12.3534 - mse: 12.3534 - mae: 1.4938 - val_loss: 13.6384 - val_mse: 13.6384 - val_mae: 1.5547 - lr: 0.0057 - 2s/epoch - 2ms/step\n",
            "Epoch 32/100\n",
            "1000/1000 - 2s - loss: 12.4191 - mse: 12.4191 - mae: 1.4892 - val_loss: 13.5615 - val_mse: 13.5615 - val_mae: 1.5356 - lr: 0.0057 - 2s/epoch - 2ms/step\n",
            "Epoch 33/100\n",
            "1000/1000 - 2s - loss: 12.4095 - mse: 12.4095 - mae: 1.4946 - val_loss: 13.6966 - val_mse: 13.6966 - val_mae: 1.4957 - lr: 0.0057 - 2s/epoch - 2ms/step\n",
            "Epoch 34/100\n",
            "1000/1000 - 2s - loss: 12.4053 - mse: 12.4053 - mae: 1.4911 - val_loss: 13.6212 - val_mse: 13.6212 - val_mae: 1.5183 - lr: 0.0057 - 2s/epoch - 2ms/step\n",
            "Epoch 35/100\n",
            "1000/1000 - 2s - loss: 12.3645 - mse: 12.3645 - mae: 1.4919 - val_loss: 13.5592 - val_mse: 13.5592 - val_mae: 1.4983 - lr: 0.0057 - 2s/epoch - 2ms/step\n",
            "Score for fold 1: loss of 13.559194564819336\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 12.7222 - mse: 12.7222 - mae: 1.4627 - val_loss: 10.4895 - val_mse: 10.4895 - val_mae: 1.4458 - lr: 0.0011 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 12.5488 - mse: 12.5488 - mae: 1.4566 - val_loss: 10.4722 - val_mse: 10.4722 - val_mae: 1.4440 - lr: 0.0011 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 12.5264 - mse: 12.5264 - mae: 1.4531 - val_loss: 10.5166 - val_mse: 10.5166 - val_mae: 1.4378 - lr: 0.0011 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 12.4857 - mse: 12.4857 - mae: 1.4525 - val_loss: 10.4911 - val_mse: 10.4911 - val_mae: 1.4484 - lr: 0.0011 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 12.4273 - mse: 12.4273 - mae: 1.4498 - val_loss: 10.5319 - val_mse: 10.5319 - val_mae: 1.4700 - lr: 0.0011 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 12.4092 - mse: 12.4092 - mae: 1.4493 - val_loss: 10.4619 - val_mse: 10.4619 - val_mae: 1.4554 - lr: 0.0011 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 12.3788 - mse: 12.3788 - mae: 1.4519 - val_loss: 10.4836 - val_mse: 10.4836 - val_mae: 1.4473 - lr: 0.0011 - 2s/epoch - 2ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 2s - loss: 12.3689 - mse: 12.3689 - mae: 1.4499 - val_loss: 10.5132 - val_mse: 10.5132 - val_mae: 1.4446 - lr: 0.0011 - 2s/epoch - 2ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 2s - loss: 12.3635 - mse: 12.3635 - mae: 1.4484 - val_loss: 10.5260 - val_mse: 10.5260 - val_mae: 1.4354 - lr: 0.0011 - 2s/epoch - 2ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 2s - loss: 12.3524 - mse: 12.3524 - mae: 1.4497 - val_loss: 10.5488 - val_mse: 10.5488 - val_mae: 1.4600 - lr: 0.0011 - 2s/epoch - 2ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 2s - loss: 12.3233 - mse: 12.3233 - mae: 1.4500 - val_loss: 10.5166 - val_mse: 10.5166 - val_mae: 1.4553 - lr: 0.0011 - 2s/epoch - 2ms/step\n",
            "Score for fold 2: loss of 10.51661205291748\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 10.5685 - mse: 10.5685 - mae: 1.4524 - val_loss: 17.3586 - val_mse: 17.3586 - val_mae: 1.4212 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 10.5634 - mse: 10.5634 - mae: 1.4496 - val_loss: 17.3944 - val_mse: 17.3944 - val_mae: 1.4465 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 10.5413 - mse: 10.5413 - mae: 1.4499 - val_loss: 17.4478 - val_mse: 17.4478 - val_mae: 1.4311 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 10.5236 - mse: 10.5236 - mae: 1.4515 - val_loss: 17.4557 - val_mse: 17.4557 - val_mae: 1.4295 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 10.5272 - mse: 10.5272 - mae: 1.4498 - val_loss: 17.4826 - val_mse: 17.4826 - val_mae: 1.4320 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 10.5090 - mse: 10.5090 - mae: 1.4492 - val_loss: 17.5308 - val_mse: 17.5308 - val_mae: 1.4655 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Score for fold 3: loss of 17.530784606933594\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 12.2200 - mse: 12.2200 - mae: 1.4345 - val_loss: 10.7549 - val_mse: 10.7549 - val_mae: 1.4511 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 12.1673 - mse: 12.1673 - mae: 1.4310 - val_loss: 10.7326 - val_mse: 10.7326 - val_mae: 1.4954 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 12.1678 - mse: 12.1678 - mae: 1.4327 - val_loss: 10.8562 - val_mse: 10.8562 - val_mae: 1.4886 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 12.1642 - mse: 12.1642 - mae: 1.4306 - val_loss: 10.8017 - val_mse: 10.8017 - val_mae: 1.4961 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 12.1459 - mse: 12.1459 - mae: 1.4296 - val_loss: 10.9003 - val_mse: 10.9003 - val_mae: 1.4691 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 12.1060 - mse: 12.1060 - mae: 1.4301 - val_loss: 10.9378 - val_mse: 10.9378 - val_mae: 1.4738 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 12.1073 - mse: 12.1073 - mae: 1.4284 - val_loss: 10.9084 - val_mse: 10.9084 - val_mae: 1.4976 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Score for fold 4: loss of 10.908391952514648\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 12.6514 - mse: 12.6514 - mae: 1.4510 - val_loss: 8.7730 - val_mse: 8.7730 - val_mae: 1.4100 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 12.6293 - mse: 12.6293 - mae: 1.4479 - val_loss: 8.8102 - val_mse: 8.8102 - val_mae: 1.4032 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 12.5821 - mse: 12.5821 - mae: 1.4485 - val_loss: 8.8783 - val_mse: 8.8783 - val_mae: 1.4196 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 12.5668 - mse: 12.5668 - mae: 1.4473 - val_loss: 8.8515 - val_mse: 8.8515 - val_mae: 1.4232 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 12.5795 - mse: 12.5795 - mae: 1.4467 - val_loss: 8.9391 - val_mse: 8.9391 - val_mae: 1.4086 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 12.5622 - mse: 12.5622 - mae: 1.4464 - val_loss: 8.9596 - val_mse: 8.9596 - val_mae: 1.4177 - lr: 0.0010 - 2s/epoch - 2ms/step\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-10-12 02:43:27,665]\u001b[0m Finished trial#3 resulted in value: 12.296. Current best value is 11.592 with parameters: {'activation': 'relu', 'num_hidden_layer': 3, 'i': 7, 'learning_rate': 0.00018369073169150139}.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Score for fold 5: loss of 8.959619522094727\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 4s - loss: 14.7102 - mse: 14.7102 - mae: 1.5965 - val_loss: 13.0179 - val_mse: 13.0179 - val_mae: 1.4755 - lr: 8.4115e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 3s - loss: 13.8555 - mse: 13.8555 - mae: 1.5245 - val_loss: 12.6775 - val_mse: 12.6775 - val_mae: 1.4707 - lr: 8.4115e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 3s - loss: 13.6395 - mse: 13.6395 - mae: 1.5149 - val_loss: 12.5611 - val_mse: 12.5611 - val_mae: 1.5423 - lr: 8.4115e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 3s - loss: 13.5171 - mse: 13.5171 - mae: 1.5045 - val_loss: 12.4062 - val_mse: 12.4062 - val_mae: 1.4988 - lr: 8.4115e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 3s - loss: 13.3222 - mse: 13.3222 - mae: 1.4932 - val_loss: 12.3638 - val_mse: 12.3638 - val_mae: 1.5148 - lr: 8.4115e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 3s - loss: 13.1844 - mse: 13.1844 - mae: 1.4881 - val_loss: 12.3509 - val_mse: 12.3509 - val_mae: 1.5781 - lr: 8.4115e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 3s - loss: 13.0682 - mse: 13.0682 - mae: 1.4844 - val_loss: 12.2441 - val_mse: 12.2441 - val_mae: 1.4951 - lr: 8.4115e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 3s - loss: 12.9555 - mse: 12.9555 - mae: 1.4773 - val_loss: 12.1603 - val_mse: 12.1603 - val_mae: 1.5062 - lr: 8.4115e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 3s - loss: 12.8339 - mse: 12.8339 - mae: 1.4715 - val_loss: 12.2836 - val_mse: 12.2836 - val_mae: 1.5048 - lr: 8.4115e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 3s - loss: 12.7649 - mse: 12.7649 - mae: 1.4654 - val_loss: 12.1645 - val_mse: 12.1645 - val_mae: 1.5174 - lr: 8.4115e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 3s - loss: 12.6730 - mse: 12.6730 - mae: 1.4672 - val_loss: 12.1886 - val_mse: 12.1886 - val_mae: 1.5239 - lr: 8.4115e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 3s - loss: 12.5296 - mse: 12.5296 - mae: 1.4598 - val_loss: 12.0202 - val_mse: 12.0202 - val_mae: 1.5050 - lr: 8.4115e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 13/100\n",
            "1000/1000 - 3s - loss: 12.4797 - mse: 12.4797 - mae: 1.4557 - val_loss: 12.0539 - val_mse: 12.0539 - val_mae: 1.5689 - lr: 8.4115e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 14/100\n",
            "1000/1000 - 3s - loss: 12.3450 - mse: 12.3450 - mae: 1.4537 - val_loss: 12.0853 - val_mse: 12.0853 - val_mae: 1.5519 - lr: 8.4115e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 15/100\n",
            "1000/1000 - 3s - loss: 12.2482 - mse: 12.2482 - mae: 1.4493 - val_loss: 11.9959 - val_mse: 11.9959 - val_mae: 1.4778 - lr: 8.4115e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 16/100\n",
            "1000/1000 - 3s - loss: 12.1272 - mse: 12.1272 - mae: 1.4430 - val_loss: 12.0684 - val_mse: 12.0684 - val_mae: 1.5632 - lr: 8.4115e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 17/100\n",
            "1000/1000 - 3s - loss: 12.0422 - mse: 12.0422 - mae: 1.4457 - val_loss: 11.8584 - val_mse: 11.8584 - val_mae: 1.5091 - lr: 8.4115e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 18/100\n",
            "1000/1000 - 3s - loss: 11.9436 - mse: 11.9436 - mae: 1.4378 - val_loss: 11.9456 - val_mse: 11.9456 - val_mae: 1.5085 - lr: 8.4115e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 19/100\n",
            "1000/1000 - 3s - loss: 11.8228 - mse: 11.8228 - mae: 1.4315 - val_loss: 11.9254 - val_mse: 11.9254 - val_mae: 1.5597 - lr: 8.4115e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 20/100\n",
            "1000/1000 - 3s - loss: 11.6880 - mse: 11.6880 - mae: 1.4311 - val_loss: 11.9621 - val_mse: 11.9621 - val_mae: 1.5980 - lr: 8.4115e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 21/100\n",
            "1000/1000 - 3s - loss: 11.6017 - mse: 11.6017 - mae: 1.4284 - val_loss: 11.9775 - val_mse: 11.9775 - val_mae: 1.5150 - lr: 8.4115e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 22/100\n",
            "1000/1000 - 3s - loss: 11.5107 - mse: 11.5107 - mae: 1.4239 - val_loss: 11.9436 - val_mse: 11.9436 - val_mae: 1.5524 - lr: 8.4115e-04 - 3s/epoch - 3ms/step\n",
            "Score for fold 1: loss of 11.943578720092773\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 3s - loss: 11.7058 - mse: 11.7058 - mae: 1.4486 - val_loss: 11.2626 - val_mse: 11.2626 - val_mae: 1.4512 - lr: 8.4115e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 3s - loss: 11.6190 - mse: 11.6190 - mae: 1.4361 - val_loss: 11.1259 - val_mse: 11.1259 - val_mae: 1.4401 - lr: 8.4115e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 3s - loss: 11.5069 - mse: 11.5069 - mae: 1.4296 - val_loss: 11.4627 - val_mse: 11.4627 - val_mae: 1.4545 - lr: 8.4115e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 3s - loss: 11.4082 - mse: 11.4082 - mae: 1.4271 - val_loss: 11.4033 - val_mse: 11.4033 - val_mae: 1.4694 - lr: 8.4115e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 3s - loss: 11.3042 - mse: 11.3042 - mae: 1.4159 - val_loss: 11.3521 - val_mse: 11.3521 - val_mae: 1.4847 - lr: 8.4115e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 3s - loss: 11.2114 - mse: 11.2114 - mae: 1.4128 - val_loss: 11.4537 - val_mse: 11.4537 - val_mae: 1.4936 - lr: 8.4115e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 3s - loss: 11.1420 - mse: 11.1420 - mae: 1.4117 - val_loss: 11.5380 - val_mse: 11.5380 - val_mae: 1.4836 - lr: 8.4115e-04 - 3s/epoch - 3ms/step\n",
            "Score for fold 2: loss of 11.53798770904541\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 3s - loss: 10.0005 - mse: 10.0005 - mae: 1.4308 - val_loss: 15.9468 - val_mse: 15.9468 - val_mae: 1.4209 - lr: 8.4115e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 3s - loss: 9.8529 - mse: 9.8529 - mae: 1.4172 - val_loss: 16.1530 - val_mse: 16.1530 - val_mae: 1.4347 - lr: 8.4115e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 3s - loss: 9.7169 - mse: 9.7169 - mae: 1.4059 - val_loss: 16.0743 - val_mse: 16.0743 - val_mae: 1.4344 - lr: 8.4115e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 3s - loss: 9.6347 - mse: 9.6347 - mae: 1.4007 - val_loss: 16.1335 - val_mse: 16.1335 - val_mae: 1.4605 - lr: 8.4115e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 3s - loss: 9.5119 - mse: 9.5119 - mae: 1.3991 - val_loss: 16.1413 - val_mse: 16.1413 - val_mae: 1.4931 - lr: 8.4115e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 3s - loss: 9.4216 - mse: 9.4216 - mae: 1.3928 - val_loss: 16.2498 - val_mse: 16.2498 - val_mae: 1.4664 - lr: 8.4115e-04 - 3s/epoch - 3ms/step\n",
            "Score for fold 3: loss of 16.249774932861328\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 3s - loss: 11.7490 - mse: 11.7490 - mae: 1.4247 - val_loss: 7.0531 - val_mse: 7.0531 - val_mae: 1.3888 - lr: 8.4115e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 3s - loss: 11.5400 - mse: 11.5400 - mae: 1.4150 - val_loss: 6.9398 - val_mse: 6.9398 - val_mae: 1.3725 - lr: 8.4115e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 3s - loss: 11.4522 - mse: 11.4522 - mae: 1.4087 - val_loss: 7.1252 - val_mse: 7.1252 - val_mae: 1.3755 - lr: 8.4115e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 3s - loss: 11.3451 - mse: 11.3451 - mae: 1.4014 - val_loss: 7.2518 - val_mse: 7.2518 - val_mae: 1.4111 - lr: 8.4115e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 3s - loss: 11.2457 - mse: 11.2457 - mae: 1.3955 - val_loss: 7.3762 - val_mse: 7.3762 - val_mae: 1.4309 - lr: 8.4115e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 3s - loss: 11.1281 - mse: 11.1281 - mae: 1.3925 - val_loss: 7.3457 - val_mse: 7.3457 - val_mae: 1.4628 - lr: 8.4115e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 3s - loss: 11.0152 - mse: 11.0152 - mae: 1.3849 - val_loss: 7.4162 - val_mse: 7.4162 - val_mae: 1.4145 - lr: 8.4115e-04 - 3s/epoch - 3ms/step\n",
            "Score for fold 4: loss of 7.416232109069824\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 3s - loss: 10.4869 - mse: 10.4869 - mae: 1.4158 - val_loss: 9.5191 - val_mse: 9.5191 - val_mae: 1.3235 - lr: 8.4115e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 3s - loss: 10.2754 - mse: 10.2754 - mae: 1.4007 - val_loss: 9.6227 - val_mse: 9.6227 - val_mae: 1.3946 - lr: 8.4115e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 3s - loss: 10.1780 - mse: 10.1780 - mae: 1.3915 - val_loss: 9.7537 - val_mse: 9.7537 - val_mae: 1.4030 - lr: 8.4115e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 3s - loss: 10.0933 - mse: 10.0933 - mae: 1.3849 - val_loss: 9.6672 - val_mse: 9.6672 - val_mae: 1.3903 - lr: 8.4115e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 3s - loss: 9.9653 - mse: 9.9653 - mae: 1.3774 - val_loss: 9.7179 - val_mse: 9.7179 - val_mae: 1.4004 - lr: 8.4115e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 3s - loss: 9.8863 - mse: 9.8863 - mae: 1.3773 - val_loss: 9.6745 - val_mse: 9.6745 - val_mae: 1.4021 - lr: 8.4115e-04 - 3s/epoch - 3ms/step\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-10-12 02:45:54,808]\u001b[0m Finished trial#4 resulted in value: 11.364. Current best value is 11.364 with parameters: {'activation': 'tanh', 'num_hidden_layer': 3, 'i': 8, 'learning_rate': 0.0008411476405498753}.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Score for fold 5: loss of 9.674471855163574\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 5s - loss: 14.8905 - mse: 14.8905 - mae: 1.6094 - val_loss: 13.1460 - val_mse: 13.1460 - val_mae: 1.5368 - lr: 0.0014 - 5s/epoch - 5ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 4s - loss: 14.0755 - mse: 14.0755 - mae: 1.5419 - val_loss: 13.3032 - val_mse: 13.3032 - val_mae: 1.5248 - lr: 0.0014 - 4s/epoch - 4ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 4s - loss: 13.9214 - mse: 13.9214 - mae: 1.5273 - val_loss: 12.6706 - val_mse: 12.6706 - val_mae: 1.5005 - lr: 0.0014 - 4s/epoch - 4ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 4s - loss: 13.6757 - mse: 13.6757 - mae: 1.5091 - val_loss: 12.4272 - val_mse: 12.4272 - val_mae: 1.5115 - lr: 0.0014 - 4s/epoch - 4ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 4s - loss: 13.4188 - mse: 13.4188 - mae: 1.4945 - val_loss: 12.7529 - val_mse: 12.7529 - val_mae: 1.4745 - lr: 0.0014 - 4s/epoch - 4ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 4s - loss: 13.3361 - mse: 13.3361 - mae: 1.5005 - val_loss: 12.4436 - val_mse: 12.4436 - val_mae: 1.4575 - lr: 0.0014 - 4s/epoch - 4ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 4s - loss: 13.2185 - mse: 13.2185 - mae: 1.4855 - val_loss: 12.2262 - val_mse: 12.2262 - val_mae: 1.6688 - lr: 0.0014 - 4s/epoch - 4ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 4s - loss: 13.1347 - mse: 13.1347 - mae: 1.4835 - val_loss: 12.0094 - val_mse: 12.0094 - val_mae: 1.5293 - lr: 0.0014 - 4s/epoch - 4ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 4s - loss: 12.9654 - mse: 12.9654 - mae: 1.4782 - val_loss: 11.9245 - val_mse: 11.9245 - val_mae: 1.5421 - lr: 0.0014 - 4s/epoch - 4ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 4s - loss: 12.9743 - mse: 12.9743 - mae: 1.4872 - val_loss: 12.1445 - val_mse: 12.1445 - val_mae: 1.4611 - lr: 0.0014 - 4s/epoch - 4ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 4s - loss: 12.7829 - mse: 12.7829 - mae: 1.4759 - val_loss: 12.1790 - val_mse: 12.1790 - val_mae: 1.4384 - lr: 0.0014 - 4s/epoch - 4ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 4s - loss: 12.9138 - mse: 12.9138 - mae: 1.4723 - val_loss: 11.9065 - val_mse: 11.9065 - val_mae: 1.6092 - lr: 0.0014 - 4s/epoch - 4ms/step\n",
            "Epoch 13/100\n",
            "1000/1000 - 4s - loss: 12.7347 - mse: 12.7347 - mae: 1.4785 - val_loss: 11.9562 - val_mse: 11.9562 - val_mae: 1.4578 - lr: 0.0014 - 4s/epoch - 4ms/step\n",
            "Epoch 14/100\n",
            "1000/1000 - 4s - loss: 12.6195 - mse: 12.6195 - mae: 1.4639 - val_loss: 11.7474 - val_mse: 11.7474 - val_mae: 1.4591 - lr: 0.0014 - 4s/epoch - 4ms/step\n",
            "Epoch 15/100\n",
            "1000/1000 - 4s - loss: 12.6604 - mse: 12.6604 - mae: 1.4732 - val_loss: 12.0870 - val_mse: 12.0870 - val_mae: 1.4836 - lr: 0.0014 - 4s/epoch - 4ms/step\n",
            "Epoch 16/100\n",
            "1000/1000 - 4s - loss: 12.4749 - mse: 12.4749 - mae: 1.4641 - val_loss: 11.7985 - val_mse: 11.7985 - val_mae: 1.5550 - lr: 0.0014 - 4s/epoch - 4ms/step\n",
            "Epoch 17/100\n",
            "1000/1000 - 4s - loss: 12.4104 - mse: 12.4104 - mae: 1.4597 - val_loss: 12.1888 - val_mse: 12.1888 - val_mae: 1.5467 - lr: 0.0014 - 4s/epoch - 4ms/step\n",
            "Epoch 18/100\n",
            "1000/1000 - 4s - loss: 12.3409 - mse: 12.3409 - mae: 1.4585 - val_loss: 11.9104 - val_mse: 11.9104 - val_mae: 1.5865 - lr: 0.0014 - 4s/epoch - 4ms/step\n",
            "Epoch 19/100\n",
            "1000/1000 - 4s - loss: 12.2455 - mse: 12.2455 - mae: 1.4503 - val_loss: 11.6123 - val_mse: 11.6123 - val_mae: 1.5652 - lr: 0.0014 - 4s/epoch - 4ms/step\n",
            "Epoch 20/100\n",
            "1000/1000 - 4s - loss: 12.3404 - mse: 12.3404 - mae: 1.4521 - val_loss: 11.7295 - val_mse: 11.7295 - val_mae: 1.5588 - lr: 0.0014 - 4s/epoch - 4ms/step\n",
            "Epoch 21/100\n",
            "1000/1000 - 4s - loss: 12.1027 - mse: 12.1027 - mae: 1.4432 - val_loss: 11.4577 - val_mse: 11.4577 - val_mae: 1.5694 - lr: 0.0014 - 4s/epoch - 4ms/step\n",
            "Epoch 22/100\n",
            "1000/1000 - 4s - loss: 12.0457 - mse: 12.0457 - mae: 1.4397 - val_loss: 11.7519 - val_mse: 11.7519 - val_mae: 1.5570 - lr: 0.0014 - 4s/epoch - 4ms/step\n",
            "Epoch 23/100\n",
            "1000/1000 - 4s - loss: 11.9348 - mse: 11.9348 - mae: 1.4407 - val_loss: 11.8149 - val_mse: 11.8149 - val_mae: 1.4273 - lr: 0.0014 - 4s/epoch - 4ms/step\n",
            "Epoch 24/100\n",
            "1000/1000 - 4s - loss: 11.9531 - mse: 11.9531 - mae: 1.4331 - val_loss: 11.5603 - val_mse: 11.5603 - val_mae: 1.5306 - lr: 0.0014 - 4s/epoch - 4ms/step\n",
            "Epoch 25/100\n",
            "1000/1000 - 4s - loss: 11.9373 - mse: 11.9373 - mae: 1.4373 - val_loss: 11.5511 - val_mse: 11.5511 - val_mae: 1.5392 - lr: 0.0014 - 4s/epoch - 4ms/step\n",
            "Epoch 26/100\n",
            "1000/1000 - 4s - loss: 11.7419 - mse: 11.7419 - mae: 1.4266 - val_loss: 11.7788 - val_mse: 11.7788 - val_mae: 1.4578 - lr: 0.0014 - 4s/epoch - 4ms/step\n",
            "Score for fold 1: loss of 11.778829574584961\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 4s - loss: 11.4624 - mse: 11.4624 - mae: 1.4291 - val_loss: 12.4545 - val_mse: 12.4545 - val_mae: 1.5359 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 4s - loss: 11.1600 - mse: 11.1600 - mae: 1.4163 - val_loss: 12.2518 - val_mse: 12.2518 - val_mae: 1.5767 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 4s - loss: 11.1041 - mse: 11.1041 - mae: 1.4082 - val_loss: 12.5561 - val_mse: 12.5561 - val_mae: 1.3294 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 4s - loss: 10.9040 - mse: 10.9040 - mae: 1.4040 - val_loss: 12.3045 - val_mse: 12.3045 - val_mae: 1.4345 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 4s - loss: 10.7278 - mse: 10.7278 - mae: 1.4056 - val_loss: 12.4468 - val_mse: 12.4468 - val_mae: 1.4402 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 4s - loss: 10.6781 - mse: 10.6781 - mae: 1.3978 - val_loss: 12.2173 - val_mse: 12.2173 - val_mae: 1.4088 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 4s - loss: 10.5373 - mse: 10.5373 - mae: 1.3858 - val_loss: 12.6468 - val_mse: 12.6468 - val_mae: 1.4765 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 4s - loss: 10.3352 - mse: 10.3352 - mae: 1.3879 - val_loss: 12.5388 - val_mse: 12.5388 - val_mae: 1.3948 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 4s - loss: 10.2929 - mse: 10.2929 - mae: 1.3794 - val_loss: 12.3479 - val_mse: 12.3479 - val_mae: 1.4169 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 4s - loss: 10.1936 - mse: 10.1936 - mae: 1.3724 - val_loss: 12.6044 - val_mse: 12.6044 - val_mae: 1.4545 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 4s - loss: 9.9392 - mse: 9.9392 - mae: 1.3632 - val_loss: 12.5263 - val_mse: 12.5263 - val_mae: 1.4882 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Score for fold 2: loss of 12.526299476623535\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 4s - loss: 10.9775 - mse: 10.9775 - mae: 1.3941 - val_loss: 8.7642 - val_mse: 8.7642 - val_mae: 1.3629 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 4s - loss: 10.8476 - mse: 10.8476 - mae: 1.3886 - val_loss: 9.0971 - val_mse: 9.0971 - val_mae: 1.3669 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 4s - loss: 10.6131 - mse: 10.6131 - mae: 1.3891 - val_loss: 8.9423 - val_mse: 8.9423 - val_mae: 1.3292 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 4s - loss: 10.4480 - mse: 10.4480 - mae: 1.3805 - val_loss: 9.3672 - val_mse: 9.3672 - val_mae: 1.3502 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 4s - loss: 10.2808 - mse: 10.2808 - mae: 1.3733 - val_loss: 8.9617 - val_mse: 8.9617 - val_mae: 1.3331 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 4s - loss: 10.0634 - mse: 10.0634 - mae: 1.3583 - val_loss: 9.5514 - val_mse: 9.5514 - val_mae: 1.3955 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Score for fold 3: loss of 9.55136775970459\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 4s - loss: 9.0867 - mse: 9.0867 - mae: 1.3715 - val_loss: 14.1063 - val_mse: 14.1063 - val_mae: 1.2855 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 4s - loss: 8.8783 - mse: 8.8783 - mae: 1.3652 - val_loss: 13.5165 - val_mse: 13.5165 - val_mae: 1.3096 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 4s - loss: 8.5511 - mse: 8.5511 - mae: 1.3477 - val_loss: 13.7997 - val_mse: 13.7997 - val_mae: 1.3001 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 4s - loss: 8.3805 - mse: 8.3805 - mae: 1.3411 - val_loss: 13.9614 - val_mse: 13.9614 - val_mae: 1.3811 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 4s - loss: 8.0667 - mse: 8.0667 - mae: 1.3324 - val_loss: 13.9189 - val_mse: 13.9189 - val_mae: 1.4896 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 4s - loss: 8.1412 - mse: 8.1412 - mae: 1.3278 - val_loss: 14.4742 - val_mse: 14.4742 - val_mae: 1.3939 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 4s - loss: 7.8795 - mse: 7.8795 - mae: 1.3099 - val_loss: 14.5047 - val_mse: 14.5047 - val_mae: 1.3802 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Score for fold 4: loss of 14.504731178283691\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 4s - loss: 10.0299 - mse: 10.0299 - mae: 1.3410 - val_loss: 6.2694 - val_mse: 6.2694 - val_mae: 1.2848 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 4s - loss: 9.8295 - mse: 9.8295 - mae: 1.3320 - val_loss: 6.2559 - val_mse: 6.2559 - val_mae: 1.2751 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 4s - loss: 9.6784 - mse: 9.6784 - mae: 1.3261 - val_loss: 6.6988 - val_mse: 6.6988 - val_mae: 1.2993 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 4s - loss: 9.5082 - mse: 9.5082 - mae: 1.3121 - val_loss: 6.6211 - val_mse: 6.6211 - val_mae: 1.2942 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 4s - loss: 9.1351 - mse: 9.1351 - mae: 1.3031 - val_loss: 6.8670 - val_mse: 6.8670 - val_mae: 1.2816 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 4s - loss: 9.0662 - mse: 9.0662 - mae: 1.2973 - val_loss: 6.5854 - val_mse: 6.5854 - val_mae: 1.3777 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 4s - loss: 8.7511 - mse: 8.7511 - mae: 1.2806 - val_loss: 7.3266 - val_mse: 7.3266 - val_mae: 1.4169 - lr: 0.0010 - 4s/epoch - 4ms/step\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-10-12 02:49:34,395]\u001b[0m Finished trial#5 resulted in value: 11.138. Current best value is 11.138 with parameters: {'activation': 'tanh', 'num_hidden_layer': 4, 'i': 8, 'learning_rate': 0.0013987125513590854}.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Score for fold 5: loss of 7.326644420623779\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 22.6130 - mse: 22.6130 - mae: 2.1254 - val_loss: 11.3366 - val_mse: 11.3366 - val_mae: 1.6062 - lr: 1.1812e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 17.2573 - mse: 17.2573 - mae: 1.5480 - val_loss: 9.0861 - val_mse: 9.0861 - val_mae: 1.5143 - lr: 1.1812e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 16.1546 - mse: 16.1546 - mae: 1.5589 - val_loss: 8.7613 - val_mse: 8.7613 - val_mae: 1.5092 - lr: 1.1812e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 15.8288 - mse: 15.8288 - mae: 1.5477 - val_loss: 8.6102 - val_mse: 8.6102 - val_mae: 1.5036 - lr: 1.1812e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 15.6549 - mse: 15.6549 - mae: 1.5412 - val_loss: 8.5287 - val_mse: 8.5287 - val_mae: 1.4921 - lr: 1.1812e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 15.5365 - mse: 15.5365 - mae: 1.5338 - val_loss: 8.4619 - val_mse: 8.4619 - val_mae: 1.4924 - lr: 1.1812e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 15.4381 - mse: 15.4381 - mae: 1.5283 - val_loss: 8.4034 - val_mse: 8.4034 - val_mae: 1.4783 - lr: 1.1812e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 2s - loss: 15.3641 - mse: 15.3641 - mae: 1.5239 - val_loss: 8.3569 - val_mse: 8.3569 - val_mae: 1.4807 - lr: 1.1812e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 2s - loss: 15.2833 - mse: 15.2833 - mae: 1.5214 - val_loss: 8.3409 - val_mse: 8.3409 - val_mae: 1.4727 - lr: 1.1812e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 2s - loss: 15.2200 - mse: 15.2200 - mae: 1.5169 - val_loss: 8.2861 - val_mse: 8.2861 - val_mae: 1.4766 - lr: 1.1812e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 2s - loss: 15.1607 - mse: 15.1607 - mae: 1.5120 - val_loss: 8.2507 - val_mse: 8.2507 - val_mae: 1.4638 - lr: 1.1812e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 2s - loss: 15.1062 - mse: 15.1062 - mae: 1.5081 - val_loss: 8.2289 - val_mse: 8.2289 - val_mae: 1.4629 - lr: 1.1812e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 13/100\n",
            "1000/1000 - 2s - loss: 15.0612 - mse: 15.0612 - mae: 1.5085 - val_loss: 8.1949 - val_mse: 8.1949 - val_mae: 1.4584 - lr: 1.1812e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 14/100\n",
            "1000/1000 - 2s - loss: 15.0117 - mse: 15.0117 - mae: 1.4998 - val_loss: 8.1699 - val_mse: 8.1699 - val_mae: 1.4703 - lr: 1.1812e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 15/100\n",
            "1000/1000 - 2s - loss: 14.9682 - mse: 14.9682 - mae: 1.5011 - val_loss: 8.1531 - val_mse: 8.1531 - val_mae: 1.4644 - lr: 1.1812e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 16/100\n",
            "1000/1000 - 2s - loss: 14.9320 - mse: 14.9320 - mae: 1.4994 - val_loss: 8.1355 - val_mse: 8.1355 - val_mae: 1.4586 - lr: 1.1812e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 17/100\n",
            "1000/1000 - 2s - loss: 14.9007 - mse: 14.9007 - mae: 1.4973 - val_loss: 8.1213 - val_mse: 8.1213 - val_mae: 1.4589 - lr: 1.1812e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 18/100\n",
            "1000/1000 - 2s - loss: 14.8667 - mse: 14.8667 - mae: 1.4908 - val_loss: 8.0972 - val_mse: 8.0972 - val_mae: 1.4538 - lr: 1.1812e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 19/100\n",
            "1000/1000 - 2s - loss: 14.8191 - mse: 14.8191 - mae: 1.4924 - val_loss: 8.0993 - val_mse: 8.0993 - val_mae: 1.4580 - lr: 1.1812e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 20/100\n",
            "1000/1000 - 2s - loss: 14.8055 - mse: 14.8055 - mae: 1.4887 - val_loss: 8.0833 - val_mse: 8.0833 - val_mae: 1.4511 - lr: 1.1812e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 21/100\n",
            "1000/1000 - 2s - loss: 14.7690 - mse: 14.7690 - mae: 1.4888 - val_loss: 8.0589 - val_mse: 8.0589 - val_mae: 1.4561 - lr: 1.1812e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 22/100\n",
            "1000/1000 - 2s - loss: 14.7451 - mse: 14.7451 - mae: 1.4883 - val_loss: 8.0356 - val_mse: 8.0356 - val_mae: 1.4467 - lr: 1.1812e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 23/100\n",
            "1000/1000 - 2s - loss: 14.7111 - mse: 14.7111 - mae: 1.4858 - val_loss: 8.0318 - val_mse: 8.0318 - val_mae: 1.4453 - lr: 1.1812e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 24/100\n",
            "1000/1000 - 2s - loss: 14.6827 - mse: 14.6827 - mae: 1.4816 - val_loss: 8.0064 - val_mse: 8.0064 - val_mae: 1.4582 - lr: 1.1812e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 25/100\n",
            "1000/1000 - 2s - loss: 14.6444 - mse: 14.6444 - mae: 1.4847 - val_loss: 8.0250 - val_mse: 8.0250 - val_mae: 1.4477 - lr: 1.1812e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 26/100\n",
            "1000/1000 - 2s - loss: 14.6240 - mse: 14.6240 - mae: 1.4816 - val_loss: 8.0182 - val_mse: 8.0182 - val_mae: 1.4448 - lr: 1.1812e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 27/100\n",
            "1000/1000 - 2s - loss: 14.6007 - mse: 14.6007 - mae: 1.4789 - val_loss: 7.9839 - val_mse: 7.9839 - val_mae: 1.4449 - lr: 1.1812e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 28/100\n",
            "1000/1000 - 2s - loss: 14.5795 - mse: 14.5795 - mae: 1.4802 - val_loss: 7.9977 - val_mse: 7.9977 - val_mae: 1.4448 - lr: 1.1812e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 29/100\n",
            "1000/1000 - 2s - loss: 14.5615 - mse: 14.5615 - mae: 1.4773 - val_loss: 7.9787 - val_mse: 7.9787 - val_mae: 1.4406 - lr: 1.1812e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 30/100\n",
            "1000/1000 - 2s - loss: 14.5501 - mse: 14.5501 - mae: 1.4747 - val_loss: 7.9589 - val_mse: 7.9589 - val_mae: 1.4403 - lr: 1.1812e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 31/100\n",
            "1000/1000 - 2s - loss: 14.5179 - mse: 14.5179 - mae: 1.4770 - val_loss: 7.9489 - val_mse: 7.9489 - val_mae: 1.4441 - lr: 1.1812e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 32/100\n",
            "1000/1000 - 2s - loss: 14.5001 - mse: 14.5001 - mae: 1.4742 - val_loss: 7.9461 - val_mse: 7.9461 - val_mae: 1.4444 - lr: 1.1812e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 33/100\n",
            "1000/1000 - 2s - loss: 14.4807 - mse: 14.4807 - mae: 1.4739 - val_loss: 7.9425 - val_mse: 7.9425 - val_mae: 1.4477 - lr: 1.1812e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 34/100\n",
            "1000/1000 - 2s - loss: 14.4627 - mse: 14.4627 - mae: 1.4709 - val_loss: 7.9225 - val_mse: 7.9225 - val_mae: 1.4477 - lr: 1.1812e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 35/100\n",
            "1000/1000 - 2s - loss: 14.4449 - mse: 14.4449 - mae: 1.4712 - val_loss: 7.9227 - val_mse: 7.9227 - val_mae: 1.4344 - lr: 1.1812e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 36/100\n",
            "1000/1000 - 2s - loss: 14.4209 - mse: 14.4209 - mae: 1.4691 - val_loss: 7.9039 - val_mse: 7.9039 - val_mae: 1.4454 - lr: 1.1812e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 37/100\n",
            "1000/1000 - 2s - loss: 14.3918 - mse: 14.3918 - mae: 1.4700 - val_loss: 7.8932 - val_mse: 7.8932 - val_mae: 1.4401 - lr: 1.1812e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 38/100\n",
            "1000/1000 - 2s - loss: 14.3768 - mse: 14.3768 - mae: 1.4707 - val_loss: 7.9023 - val_mse: 7.9023 - val_mae: 1.4336 - lr: 1.1812e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 39/100\n",
            "1000/1000 - 2s - loss: 14.3637 - mse: 14.3637 - mae: 1.4690 - val_loss: 7.8824 - val_mse: 7.8824 - val_mae: 1.4479 - lr: 1.1812e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 40/100\n",
            "1000/1000 - 2s - loss: 14.3604 - mse: 14.3604 - mae: 1.4695 - val_loss: 7.8872 - val_mse: 7.8872 - val_mae: 1.4436 - lr: 1.1812e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 41/100\n",
            "1000/1000 - 2s - loss: 14.3416 - mse: 14.3416 - mae: 1.4690 - val_loss: 7.8739 - val_mse: 7.8739 - val_mae: 1.4376 - lr: 1.1812e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 42/100\n",
            "1000/1000 - 2s - loss: 14.3257 - mse: 14.3257 - mae: 1.4666 - val_loss: 7.8646 - val_mse: 7.8646 - val_mae: 1.4358 - lr: 1.1812e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 43/100\n",
            "1000/1000 - 2s - loss: 14.3053 - mse: 14.3053 - mae: 1.4652 - val_loss: 7.8600 - val_mse: 7.8600 - val_mae: 1.4324 - lr: 1.1812e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 44/100\n",
            "1000/1000 - 2s - loss: 14.2917 - mse: 14.2917 - mae: 1.4651 - val_loss: 7.8794 - val_mse: 7.8794 - val_mae: 1.4226 - lr: 1.1812e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 45/100\n",
            "1000/1000 - 2s - loss: 14.2801 - mse: 14.2801 - mae: 1.4628 - val_loss: 7.8461 - val_mse: 7.8461 - val_mae: 1.4454 - lr: 1.1812e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 46/100\n",
            "1000/1000 - 2s - loss: 14.2622 - mse: 14.2622 - mae: 1.4650 - val_loss: 7.8364 - val_mse: 7.8364 - val_mae: 1.4409 - lr: 1.1812e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 47/100\n",
            "1000/1000 - 2s - loss: 14.2500 - mse: 14.2500 - mae: 1.4644 - val_loss: 7.8398 - val_mse: 7.8398 - val_mae: 1.4284 - lr: 1.1812e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 48/100\n",
            "1000/1000 - 2s - loss: 14.2389 - mse: 14.2389 - mae: 1.4624 - val_loss: 7.8273 - val_mse: 7.8273 - val_mae: 1.4409 - lr: 1.1812e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 49/100\n",
            "1000/1000 - 2s - loss: 14.2247 - mse: 14.2247 - mae: 1.4630 - val_loss: 7.8256 - val_mse: 7.8256 - val_mae: 1.4333 - lr: 1.1812e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 50/100\n",
            "1000/1000 - 2s - loss: 14.2117 - mse: 14.2117 - mae: 1.4638 - val_loss: 7.8207 - val_mse: 7.8207 - val_mae: 1.4313 - lr: 1.1812e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 51/100\n",
            "1000/1000 - 2s - loss: 14.2141 - mse: 14.2141 - mae: 1.4597 - val_loss: 7.8121 - val_mse: 7.8121 - val_mae: 1.4346 - lr: 1.1812e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 52/100\n",
            "1000/1000 - 2s - loss: 14.1905 - mse: 14.1905 - mae: 1.4610 - val_loss: 7.8020 - val_mse: 7.8020 - val_mae: 1.4275 - lr: 1.1812e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 53/100\n",
            "1000/1000 - 2s - loss: 14.1891 - mse: 14.1891 - mae: 1.4611 - val_loss: 7.7884 - val_mse: 7.7884 - val_mae: 1.4356 - lr: 1.1812e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 54/100\n",
            "1000/1000 - 2s - loss: 14.1587 - mse: 14.1587 - mae: 1.4602 - val_loss: 7.7959 - val_mse: 7.7959 - val_mae: 1.4333 - lr: 1.1812e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 55/100\n",
            "1000/1000 - 2s - loss: 14.1449 - mse: 14.1449 - mae: 1.4603 - val_loss: 7.7876 - val_mse: 7.7876 - val_mae: 1.4304 - lr: 1.1812e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 56/100\n",
            "1000/1000 - 2s - loss: 14.1458 - mse: 14.1458 - mae: 1.4584 - val_loss: 7.7857 - val_mse: 7.7857 - val_mae: 1.4392 - lr: 1.1812e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 57/100\n",
            "1000/1000 - 2s - loss: 14.1278 - mse: 14.1278 - mae: 1.4597 - val_loss: 7.7801 - val_mse: 7.7801 - val_mae: 1.4249 - lr: 1.1812e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 58/100\n",
            "1000/1000 - 2s - loss: 14.1169 - mse: 14.1169 - mae: 1.4577 - val_loss: 7.7695 - val_mse: 7.7695 - val_mae: 1.4397 - lr: 1.1812e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 59/100\n",
            "1000/1000 - 2s - loss: 14.0954 - mse: 14.0954 - mae: 1.4599 - val_loss: 7.7657 - val_mse: 7.7657 - val_mae: 1.4305 - lr: 1.1812e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 60/100\n",
            "1000/1000 - 2s - loss: 14.0892 - mse: 14.0892 - mae: 1.4573 - val_loss: 7.7792 - val_mse: 7.7792 - val_mae: 1.4221 - lr: 1.1812e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 61/100\n",
            "1000/1000 - 2s - loss: 14.0879 - mse: 14.0879 - mae: 1.4558 - val_loss: 7.7708 - val_mse: 7.7708 - val_mae: 1.4380 - lr: 1.1812e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 62/100\n",
            "1000/1000 - 2s - loss: 14.0592 - mse: 14.0592 - mae: 1.4583 - val_loss: 7.7614 - val_mse: 7.7614 - val_mae: 1.4403 - lr: 1.1812e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 63/100\n",
            "1000/1000 - 2s - loss: 14.0571 - mse: 14.0571 - mae: 1.4563 - val_loss: 7.7463 - val_mse: 7.7463 - val_mae: 1.4242 - lr: 1.1812e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 64/100\n",
            "1000/1000 - 2s - loss: 14.0421 - mse: 14.0421 - mae: 1.4560 - val_loss: 7.7413 - val_mse: 7.7413 - val_mae: 1.4315 - lr: 1.1812e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 65/100\n",
            "1000/1000 - 2s - loss: 14.0378 - mse: 14.0378 - mae: 1.4562 - val_loss: 7.7332 - val_mse: 7.7332 - val_mae: 1.4319 - lr: 1.1812e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 66/100\n",
            "1000/1000 - 2s - loss: 14.0270 - mse: 14.0270 - mae: 1.4528 - val_loss: 7.7240 - val_mse: 7.7240 - val_mae: 1.4323 - lr: 1.1812e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 67/100\n",
            "1000/1000 - 2s - loss: 14.0112 - mse: 14.0112 - mae: 1.4554 - val_loss: 7.7187 - val_mse: 7.7187 - val_mae: 1.4385 - lr: 1.1812e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 68/100\n",
            "1000/1000 - 2s - loss: 14.0154 - mse: 14.0154 - mae: 1.4543 - val_loss: 7.7150 - val_mse: 7.7150 - val_mae: 1.4313 - lr: 1.1812e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 69/100\n",
            "1000/1000 - 2s - loss: 13.9901 - mse: 13.9901 - mae: 1.4561 - val_loss: 7.7188 - val_mse: 7.7188 - val_mae: 1.4219 - lr: 1.1812e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 70/100\n",
            "1000/1000 - 2s - loss: 13.9928 - mse: 13.9928 - mae: 1.4522 - val_loss: 7.7095 - val_mse: 7.7095 - val_mae: 1.4332 - lr: 1.1812e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 71/100\n",
            "1000/1000 - 2s - loss: 13.9786 - mse: 13.9786 - mae: 1.4515 - val_loss: 7.7189 - val_mse: 7.7189 - val_mae: 1.4260 - lr: 1.1812e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 72/100\n",
            "1000/1000 - 2s - loss: 13.9737 - mse: 13.9737 - mae: 1.4497 - val_loss: 7.7215 - val_mse: 7.7215 - val_mae: 1.4331 - lr: 1.1812e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 73/100\n",
            "1000/1000 - 2s - loss: 13.9699 - mse: 13.9699 - mae: 1.4536 - val_loss: 7.7170 - val_mse: 7.7170 - val_mae: 1.4288 - lr: 1.1812e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 74/100\n",
            "1000/1000 - 2s - loss: 13.9581 - mse: 13.9581 - mae: 1.4482 - val_loss: 7.7114 - val_mse: 7.7114 - val_mae: 1.4362 - lr: 1.1812e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 75/100\n",
            "1000/1000 - 2s - loss: 13.9485 - mse: 13.9485 - mae: 1.4531 - val_loss: 7.7106 - val_mse: 7.7106 - val_mae: 1.4246 - lr: 1.1812e-04 - 2s/epoch - 2ms/step\n",
            "Score for fold 1: loss of 7.710590362548828\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 13.5465 - mse: 13.5465 - mae: 1.4597 - val_loss: 9.2330 - val_mse: 9.2330 - val_mae: 1.4070 - lr: 1.1812e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 13.5443 - mse: 13.5443 - mae: 1.4609 - val_loss: 9.2347 - val_mse: 9.2347 - val_mae: 1.3957 - lr: 1.1812e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 13.5321 - mse: 13.5321 - mae: 1.4611 - val_loss: 9.2573 - val_mse: 9.2573 - val_mae: 1.3942 - lr: 1.1812e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 13.5286 - mse: 13.5286 - mae: 1.4588 - val_loss: 9.2534 - val_mse: 9.2534 - val_mae: 1.4006 - lr: 1.1812e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 13.5175 - mse: 13.5175 - mae: 1.4593 - val_loss: 9.2520 - val_mse: 9.2520 - val_mae: 1.3993 - lr: 1.1812e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 13.5017 - mse: 13.5017 - mae: 1.4581 - val_loss: 9.2499 - val_mse: 9.2499 - val_mae: 1.4046 - lr: 1.1812e-04 - 2s/epoch - 2ms/step\n",
            "Score for fold 2: loss of 9.249860763549805\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 12.4659 - mse: 12.4659 - mae: 1.4296 - val_loss: 13.4492 - val_mse: 13.4492 - val_mae: 1.5008 - lr: 1.1812e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 12.4698 - mse: 12.4698 - mae: 1.4264 - val_loss: 13.4544 - val_mse: 13.4544 - val_mae: 1.5018 - lr: 1.1812e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 12.4505 - mse: 12.4505 - mae: 1.4272 - val_loss: 13.5081 - val_mse: 13.5081 - val_mae: 1.5099 - lr: 1.1812e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 12.4508 - mse: 12.4508 - mae: 1.4269 - val_loss: 13.4726 - val_mse: 13.4726 - val_mae: 1.4975 - lr: 1.1812e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 12.4434 - mse: 12.4434 - mae: 1.4262 - val_loss: 13.4642 - val_mse: 13.4642 - val_mae: 1.5089 - lr: 1.1812e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 12.4312 - mse: 12.4312 - mae: 1.4261 - val_loss: 13.4851 - val_mse: 13.4851 - val_mae: 1.5032 - lr: 1.1812e-04 - 2s/epoch - 2ms/step\n",
            "Score for fold 3: loss of 13.485084533691406\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 12.7265 - mse: 12.7265 - mae: 1.4449 - val_loss: 12.2647 - val_mse: 12.2647 - val_mae: 1.4313 - lr: 1.1812e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 12.7197 - mse: 12.7197 - mae: 1.4437 - val_loss: 12.2390 - val_mse: 12.2390 - val_mae: 1.4348 - lr: 1.1812e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 12.7063 - mse: 12.7063 - mae: 1.4434 - val_loss: 12.2861 - val_mse: 12.2861 - val_mae: 1.4386 - lr: 1.1812e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 12.7028 - mse: 12.7028 - mae: 1.4410 - val_loss: 12.2753 - val_mse: 12.2753 - val_mae: 1.4479 - lr: 1.1812e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 12.7048 - mse: 12.7048 - mae: 1.4429 - val_loss: 12.2750 - val_mse: 12.2750 - val_mae: 1.4609 - lr: 1.1812e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 12.6957 - mse: 12.6957 - mae: 1.4432 - val_loss: 12.2865 - val_mse: 12.2865 - val_mae: 1.4509 - lr: 1.1812e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 12.6848 - mse: 12.6848 - mae: 1.4394 - val_loss: 12.2972 - val_mse: 12.2972 - val_mae: 1.4498 - lr: 1.1812e-04 - 2s/epoch - 2ms/step\n",
            "Score for fold 4: loss of 12.297210693359375\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 10.6044 - mse: 10.6044 - mae: 1.4375 - val_loss: 20.6194 - val_mse: 20.6194 - val_mae: 1.4464 - lr: 1.1812e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 10.5640 - mse: 10.5640 - mae: 1.4363 - val_loss: 20.6312 - val_mse: 20.6312 - val_mae: 1.4607 - lr: 1.1812e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 10.5519 - mse: 10.5519 - mae: 1.4365 - val_loss: 20.6440 - val_mse: 20.6440 - val_mae: 1.4528 - lr: 1.1812e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 10.5452 - mse: 10.5452 - mae: 1.4365 - val_loss: 20.6422 - val_mse: 20.6422 - val_mae: 1.4568 - lr: 1.1812e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 10.5313 - mse: 10.5313 - mae: 1.4390 - val_loss: 20.6572 - val_mse: 20.6572 - val_mae: 1.4470 - lr: 1.1812e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 10.5268 - mse: 10.5268 - mae: 1.4384 - val_loss: 20.6846 - val_mse: 20.6846 - val_mae: 1.4421 - lr: 1.1812e-04 - 2s/epoch - 2ms/step\n",
            "Score for fold 5: loss of 20.68462371826172\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-10-12 02:52:32,928]\u001b[0m Finished trial#6 resulted in value: 12.686. Current best value is 11.138 with parameters: {'activation': 'tanh', 'num_hidden_layer': 4, 'i': 8, 'learning_rate': 0.0013987125513590854}.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 5s - loss: 14.6152 - mse: 14.6152 - mae: 1.5751 - val_loss: 9.6344 - val_mse: 9.6344 - val_mae: 1.5234 - lr: 1.2664e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 4s - loss: 13.3862 - mse: 13.3862 - mae: 1.5109 - val_loss: 9.5729 - val_mse: 9.5729 - val_mae: 1.4400 - lr: 1.2664e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 4s - loss: 13.0573 - mse: 13.0573 - mae: 1.4919 - val_loss: 9.2540 - val_mse: 9.2540 - val_mae: 1.4473 - lr: 1.2664e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 4s - loss: 12.8870 - mse: 12.8870 - mae: 1.4783 - val_loss: 9.2165 - val_mse: 9.2165 - val_mae: 1.3859 - lr: 1.2664e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 4s - loss: 12.7482 - mse: 12.7482 - mae: 1.4670 - val_loss: 9.1013 - val_mse: 9.1013 - val_mae: 1.4856 - lr: 1.2664e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 4s - loss: 12.6486 - mse: 12.6486 - mae: 1.4580 - val_loss: 8.9624 - val_mse: 8.9624 - val_mae: 1.5189 - lr: 1.2664e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 4s - loss: 12.6428 - mse: 12.6428 - mae: 1.4548 - val_loss: 9.1191 - val_mse: 9.1191 - val_mae: 1.4063 - lr: 1.2664e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 4s - loss: 12.5640 - mse: 12.5640 - mae: 1.4498 - val_loss: 9.1304 - val_mse: 9.1304 - val_mae: 1.4079 - lr: 1.2664e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 4s - loss: 12.4601 - mse: 12.4601 - mae: 1.4400 - val_loss: 9.0950 - val_mse: 9.0950 - val_mae: 1.4154 - lr: 1.2664e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 4s - loss: 12.4348 - mse: 12.4348 - mae: 1.4417 - val_loss: 9.0374 - val_mse: 9.0374 - val_mae: 1.3827 - lr: 1.2664e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 4s - loss: 12.2514 - mse: 12.2514 - mae: 1.4292 - val_loss: 9.1575 - val_mse: 9.1575 - val_mae: 1.3804 - lr: 1.2664e-04 - 4s/epoch - 4ms/step\n",
            "Score for fold 1: loss of 9.157523155212402\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 4s - loss: 12.4617 - mse: 12.4617 - mae: 1.4355 - val_loss: 8.1619 - val_mse: 8.1619 - val_mae: 1.4187 - lr: 1.2664e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 4s - loss: 12.4037 - mse: 12.4037 - mae: 1.4392 - val_loss: 8.0583 - val_mse: 8.0583 - val_mae: 1.3796 - lr: 1.2664e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 4s - loss: 12.3177 - mse: 12.3177 - mae: 1.4295 - val_loss: 8.2991 - val_mse: 8.2991 - val_mae: 1.3836 - lr: 1.2664e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 4s - loss: 12.3233 - mse: 12.3233 - mae: 1.4238 - val_loss: 7.9931 - val_mse: 7.9931 - val_mae: 1.3733 - lr: 1.2664e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 4s - loss: 12.1842 - mse: 12.1842 - mae: 1.4166 - val_loss: 8.1531 - val_mse: 8.1531 - val_mae: 1.3814 - lr: 1.2664e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 4s - loss: 12.0789 - mse: 12.0789 - mae: 1.4186 - val_loss: 8.2314 - val_mse: 8.2314 - val_mae: 1.4520 - lr: 1.2664e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 4s - loss: 11.8818 - mse: 11.8818 - mae: 1.4064 - val_loss: 8.1861 - val_mse: 8.1861 - val_mae: 1.3612 - lr: 1.2664e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 4s - loss: 11.9739 - mse: 11.9739 - mae: 1.4082 - val_loss: 8.0867 - val_mse: 8.0867 - val_mae: 1.4590 - lr: 1.2664e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 4s - loss: 11.9456 - mse: 11.9456 - mae: 1.4011 - val_loss: 8.2067 - val_mse: 8.2067 - val_mae: 1.4084 - lr: 1.2664e-04 - 4s/epoch - 4ms/step\n",
            "Score for fold 2: loss of 8.206670761108398\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 4s - loss: 11.7846 - mse: 11.7846 - mae: 1.4020 - val_loss: 8.1840 - val_mse: 8.1840 - val_mae: 1.3739 - lr: 1.2664e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 4s - loss: 11.5845 - mse: 11.5845 - mae: 1.3949 - val_loss: 8.3392 - val_mse: 8.3392 - val_mae: 1.3840 - lr: 1.2664e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 4s - loss: 11.8881 - mse: 11.8881 - mae: 1.3924 - val_loss: 8.1453 - val_mse: 8.1453 - val_mae: 1.3977 - lr: 1.2664e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 4s - loss: 11.5983 - mse: 11.5983 - mae: 1.3834 - val_loss: 8.2364 - val_mse: 8.2364 - val_mae: 1.3938 - lr: 1.2664e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 4s - loss: 11.3281 - mse: 11.3281 - mae: 1.3744 - val_loss: 8.2591 - val_mse: 8.2591 - val_mae: 1.3883 - lr: 1.2664e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 4s - loss: 11.3477 - mse: 11.3477 - mae: 1.3727 - val_loss: 8.3955 - val_mse: 8.3955 - val_mae: 1.4475 - lr: 1.2664e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 4s - loss: 11.3132 - mse: 11.3132 - mae: 1.3673 - val_loss: 8.3319 - val_mse: 8.3319 - val_mae: 1.4301 - lr: 1.2664e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 4s - loss: 11.2488 - mse: 11.2488 - mae: 1.3622 - val_loss: 8.3443 - val_mse: 8.3443 - val_mae: 1.4359 - lr: 1.2664e-04 - 4s/epoch - 4ms/step\n",
            "Score for fold 3: loss of 8.344329833984375\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 4s - loss: 8.9296 - mse: 8.9296 - mae: 1.3667 - val_loss: 17.3048 - val_mse: 17.3048 - val_mae: 1.3608 - lr: 1.2664e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 4s - loss: 8.8007 - mse: 8.8007 - mae: 1.3591 - val_loss: 17.3699 - val_mse: 17.3699 - val_mae: 1.4159 - lr: 1.2664e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 4s - loss: 8.7646 - mse: 8.7646 - mae: 1.3533 - val_loss: 17.5034 - val_mse: 17.5034 - val_mae: 1.4063 - lr: 1.2664e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 4s - loss: 8.5152 - mse: 8.5152 - mae: 1.3435 - val_loss: 17.3701 - val_mse: 17.3701 - val_mae: 1.4218 - lr: 1.2664e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 4s - loss: 8.4750 - mse: 8.4750 - mae: 1.3424 - val_loss: 17.5998 - val_mse: 17.5998 - val_mae: 1.3890 - lr: 1.2664e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 4s - loss: 8.3109 - mse: 8.3109 - mae: 1.3313 - val_loss: 17.6257 - val_mse: 17.6257 - val_mae: 1.3849 - lr: 1.2664e-04 - 4s/epoch - 4ms/step\n",
            "Score for fold 4: loss of 17.62566566467285\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 4s - loss: 9.7651 - mse: 9.7651 - mae: 1.3523 - val_loss: 11.3984 - val_mse: 11.3984 - val_mae: 1.3837 - lr: 1.2664e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 4s - loss: 9.7301 - mse: 9.7301 - mae: 1.3485 - val_loss: 11.6594 - val_mse: 11.6594 - val_mae: 1.3088 - lr: 1.2664e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 4s - loss: 9.5447 - mse: 9.5447 - mae: 1.3420 - val_loss: 11.6416 - val_mse: 11.6416 - val_mae: 1.3487 - lr: 1.2664e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 4s - loss: 9.4234 - mse: 9.4234 - mae: 1.3360 - val_loss: 12.1849 - val_mse: 12.1849 - val_mae: 1.3532 - lr: 1.2664e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 4s - loss: 9.3280 - mse: 9.3280 - mae: 1.3270 - val_loss: 12.3460 - val_mse: 12.3460 - val_mae: 1.3611 - lr: 1.2664e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 4s - loss: 9.2094 - mse: 9.2094 - mae: 1.3221 - val_loss: 13.5671 - val_mse: 13.5671 - val_mae: 1.3546 - lr: 1.2664e-04 - 4s/epoch - 4ms/step\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-10-12 02:55:06,355]\u001b[0m Finished trial#7 resulted in value: 11.382000000000001. Current best value is 11.138 with parameters: {'activation': 'tanh', 'num_hidden_layer': 4, 'i': 8, 'learning_rate': 0.0013987125513590854}.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Score for fold 5: loss of 13.567083358764648\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 7s - loss: 13.7893 - mse: 13.7893 - mae: 1.5455 - val_loss: 10.7730 - val_mse: 10.7730 - val_mae: 1.4602 - lr: 5.8017e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 6s - loss: 13.3086 - mse: 13.3086 - mae: 1.5004 - val_loss: 8.5162 - val_mse: 8.5162 - val_mae: 1.5539 - lr: 5.8017e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 6s - loss: 13.3077 - mse: 13.3077 - mae: 1.4963 - val_loss: 8.1720 - val_mse: 8.1720 - val_mae: 1.4862 - lr: 5.8017e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 6s - loss: 13.2316 - mse: 13.2316 - mae: 1.4773 - val_loss: 8.7756 - val_mse: 8.7756 - val_mae: 1.4725 - lr: 5.8017e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 6s - loss: 12.8988 - mse: 12.8988 - mae: 1.4674 - val_loss: 8.1259 - val_mse: 8.1259 - val_mae: 1.4206 - lr: 5.8017e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 6s - loss: 12.8368 - mse: 12.8368 - mae: 1.4598 - val_loss: 8.0409 - val_mse: 8.0409 - val_mae: 1.5097 - lr: 5.8017e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 6s - loss: 12.8071 - mse: 12.8071 - mae: 1.4559 - val_loss: 8.6672 - val_mse: 8.6672 - val_mae: 1.4888 - lr: 5.8017e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 6s - loss: 12.5650 - mse: 12.5650 - mae: 1.4445 - val_loss: 8.3078 - val_mse: 8.3078 - val_mae: 1.4392 - lr: 5.8017e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 6s - loss: 12.5429 - mse: 12.5429 - mae: 1.4460 - val_loss: 8.0654 - val_mse: 8.0654 - val_mae: 1.4234 - lr: 5.8017e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 6s - loss: 12.1962 - mse: 12.1962 - mae: 1.4243 - val_loss: 8.3175 - val_mse: 8.3175 - val_mae: 1.4146 - lr: 5.8017e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 6s - loss: 12.0317 - mse: 12.0317 - mae: 1.4151 - val_loss: 8.4469 - val_mse: 8.4469 - val_mae: 1.4639 - lr: 5.8017e-04 - 6s/epoch - 6ms/step\n",
            "Score for fold 1: loss of 8.446858406066895\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 6s - loss: 11.2964 - mse: 11.2964 - mae: 1.4216 - val_loss: 11.1409 - val_mse: 11.1409 - val_mae: 1.3825 - lr: 5.8017e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 6s - loss: 11.2011 - mse: 11.2011 - mae: 1.4137 - val_loss: 11.1881 - val_mse: 11.1881 - val_mae: 1.4680 - lr: 5.8017e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 6s - loss: 11.0468 - mse: 11.0468 - mae: 1.4007 - val_loss: 11.7441 - val_mse: 11.7441 - val_mae: 1.5684 - lr: 5.8017e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 6s - loss: 10.8338 - mse: 10.8338 - mae: 1.3998 - val_loss: 11.3088 - val_mse: 11.3088 - val_mae: 1.4207 - lr: 5.8017e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 6s - loss: 10.3617 - mse: 10.3617 - mae: 1.3781 - val_loss: 11.4497 - val_mse: 11.4497 - val_mae: 1.4559 - lr: 5.8017e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 6s - loss: 10.3424 - mse: 10.3424 - mae: 1.3744 - val_loss: 12.0157 - val_mse: 12.0157 - val_mae: 1.4179 - lr: 5.8017e-04 - 6s/epoch - 6ms/step\n",
            "Score for fold 2: loss of 12.015682220458984\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 6s - loss: 10.8819 - mse: 10.8819 - mae: 1.3950 - val_loss: 9.8345 - val_mse: 9.8345 - val_mae: 1.3277 - lr: 5.8017e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 6s - loss: 10.7684 - mse: 10.7684 - mae: 1.3822 - val_loss: 9.7981 - val_mse: 9.7981 - val_mae: 1.4048 - lr: 5.8017e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 6s - loss: 10.4526 - mse: 10.4526 - mae: 1.3678 - val_loss: 9.8953 - val_mse: 9.8953 - val_mae: 1.4289 - lr: 5.8017e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 6s - loss: 10.2204 - mse: 10.2204 - mae: 1.3561 - val_loss: 10.1926 - val_mse: 10.1926 - val_mae: 1.3405 - lr: 5.8017e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 6s - loss: 9.9089 - mse: 9.9089 - mae: 1.3482 - val_loss: 9.6738 - val_mse: 9.6738 - val_mae: 1.4117 - lr: 5.8017e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 6s - loss: 9.6847 - mse: 9.6847 - mae: 1.3295 - val_loss: 11.3112 - val_mse: 11.3112 - val_mae: 1.4098 - lr: 5.8017e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 6s - loss: 9.8515 - mse: 9.8515 - mae: 1.3253 - val_loss: 10.4782 - val_mse: 10.4782 - val_mae: 1.4736 - lr: 5.8017e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 6s - loss: 9.5400 - mse: 9.5400 - mae: 1.3166 - val_loss: 10.4807 - val_mse: 10.4807 - val_mae: 1.3470 - lr: 5.8017e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 6s - loss: 9.2318 - mse: 9.2318 - mae: 1.3032 - val_loss: 10.0686 - val_mse: 10.0686 - val_mae: 1.5281 - lr: 5.8017e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 6s - loss: 8.9456 - mse: 8.9456 - mae: 1.2909 - val_loss: 9.6020 - val_mse: 9.6020 - val_mae: 1.5294 - lr: 5.8017e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 6s - loss: 8.8879 - mse: 8.8879 - mae: 1.2826 - val_loss: 9.6540 - val_mse: 9.6540 - val_mae: 1.4404 - lr: 5.8017e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 6s - loss: 8.7657 - mse: 8.7657 - mae: 1.2748 - val_loss: 10.2109 - val_mse: 10.2109 - val_mae: 1.3874 - lr: 5.8017e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 13/100\n",
            "1000/1000 - 6s - loss: 8.7381 - mse: 8.7381 - mae: 1.2746 - val_loss: 10.4537 - val_mse: 10.4537 - val_mae: 1.3784 - lr: 5.8017e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 14/100\n",
            "1000/1000 - 6s - loss: 8.4095 - mse: 8.4095 - mae: 1.2596 - val_loss: 10.3201 - val_mse: 10.3201 - val_mae: 1.4810 - lr: 5.8017e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 15/100\n",
            "1000/1000 - 6s - loss: 8.1840 - mse: 8.1840 - mae: 1.2466 - val_loss: 10.9460 - val_mse: 10.9460 - val_mae: 1.3755 - lr: 5.8017e-04 - 6s/epoch - 6ms/step\n",
            "Score for fold 3: loss of 10.94596004486084\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 6s - loss: 9.6106 - mse: 9.6106 - mae: 1.3222 - val_loss: 7.8133 - val_mse: 7.8133 - val_mae: 1.4075 - lr: 5.8017e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 6s - loss: 9.0211 - mse: 9.0211 - mae: 1.2994 - val_loss: 7.5659 - val_mse: 7.5659 - val_mae: 1.2807 - lr: 5.8017e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 6s - loss: 8.4566 - mse: 8.4566 - mae: 1.2770 - val_loss: 7.9321 - val_mse: 7.9321 - val_mae: 1.2775 - lr: 5.8017e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 6s - loss: 8.2696 - mse: 8.2696 - mae: 1.2670 - val_loss: 7.3753 - val_mse: 7.3753 - val_mae: 1.2521 - lr: 5.8017e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 6s - loss: 7.9695 - mse: 7.9695 - mae: 1.2554 - val_loss: 8.8260 - val_mse: 8.8260 - val_mae: 1.2612 - lr: 5.8017e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 6s - loss: 8.1280 - mse: 8.1280 - mae: 1.2502 - val_loss: 8.2289 - val_mse: 8.2289 - val_mae: 1.2516 - lr: 5.8017e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 6s - loss: 7.8576 - mse: 7.8576 - mae: 1.2469 - val_loss: 7.8716 - val_mse: 7.8716 - val_mae: 1.3095 - lr: 5.8017e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 6s - loss: 7.6132 - mse: 7.6132 - mae: 1.2276 - val_loss: 7.9044 - val_mse: 7.9044 - val_mae: 1.2390 - lr: 5.8017e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 6s - loss: 7.6277 - mse: 7.6277 - mae: 1.2248 - val_loss: 7.6877 - val_mse: 7.6877 - val_mae: 1.2688 - lr: 5.8017e-04 - 6s/epoch - 6ms/step\n",
            "Score for fold 4: loss of 7.6876983642578125\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 6s - loss: 6.8301 - mse: 6.8301 - mae: 1.2502 - val_loss: 11.2742 - val_mse: 11.2742 - val_mae: 1.1901 - lr: 5.8017e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 6s - loss: 6.5086 - mse: 6.5086 - mae: 1.2345 - val_loss: 11.5714 - val_mse: 11.5714 - val_mae: 1.1781 - lr: 5.8017e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 6s - loss: 6.3383 - mse: 6.3383 - mae: 1.2159 - val_loss: 11.6826 - val_mse: 11.6826 - val_mae: 1.1873 - lr: 5.8017e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 6s - loss: 6.1988 - mse: 6.1988 - mae: 1.2075 - val_loss: 11.8251 - val_mse: 11.8251 - val_mae: 1.3668 - lr: 5.8017e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 6s - loss: 5.9672 - mse: 5.9672 - mae: 1.1993 - val_loss: 12.5436 - val_mse: 12.5436 - val_mae: 1.2885 - lr: 5.8017e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 6s - loss: 6.0698 - mse: 6.0698 - mae: 1.1886 - val_loss: 12.6963 - val_mse: 12.6963 - val_mae: 1.2908 - lr: 5.8017e-04 - 6s/epoch - 6ms/step\n",
            "Score for fold 5: loss of 12.696256637573242\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-10-12 03:00:05,400]\u001b[0m Finished trial#8 resulted in value: 10.362. Current best value is 10.362 with parameters: {'activation': 'relu', 'num_hidden_layer': 4, 'i': 9, 'learning_rate': 0.000580166332999716}.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 4s - loss: 14.9097 - mse: 14.9097 - mae: 1.5871 - val_loss: 12.4442 - val_mse: 12.4442 - val_mae: 1.5714 - lr: 3.4138e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 4s - loss: 14.0471 - mse: 14.0471 - mae: 1.5126 - val_loss: 12.0723 - val_mse: 12.0723 - val_mae: 1.5107 - lr: 3.4138e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 4s - loss: 13.8795 - mse: 13.8795 - mae: 1.5101 - val_loss: 12.1327 - val_mse: 12.1327 - val_mae: 1.4625 - lr: 3.4138e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 4s - loss: 13.7261 - mse: 13.7261 - mae: 1.4948 - val_loss: 11.9038 - val_mse: 11.9038 - val_mae: 1.4671 - lr: 3.4138e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 4s - loss: 13.5594 - mse: 13.5594 - mae: 1.4834 - val_loss: 11.9862 - val_mse: 11.9862 - val_mae: 1.4823 - lr: 3.4138e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 4s - loss: 13.4349 - mse: 13.4349 - mae: 1.4798 - val_loss: 11.8833 - val_mse: 11.8833 - val_mae: 1.4958 - lr: 3.4138e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 4s - loss: 13.2945 - mse: 13.2945 - mae: 1.4721 - val_loss: 11.5340 - val_mse: 11.5340 - val_mae: 1.5419 - lr: 3.4138e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 4s - loss: 13.2362 - mse: 13.2362 - mae: 1.4691 - val_loss: 11.5930 - val_mse: 11.5930 - val_mae: 1.5685 - lr: 3.4138e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 4s - loss: 13.1157 - mse: 13.1157 - mae: 1.4626 - val_loss: 11.5191 - val_mse: 11.5191 - val_mae: 1.5020 - lr: 3.4138e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 4s - loss: 13.0015 - mse: 13.0015 - mae: 1.4557 - val_loss: 11.8032 - val_mse: 11.8032 - val_mae: 1.5070 - lr: 3.4138e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 4s - loss: 12.9850 - mse: 12.9850 - mae: 1.4541 - val_loss: 11.5467 - val_mse: 11.5467 - val_mae: 1.5393 - lr: 3.4138e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 4s - loss: 12.8164 - mse: 12.8164 - mae: 1.4494 - val_loss: 11.6673 - val_mse: 11.6673 - val_mae: 1.4746 - lr: 3.4138e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 13/100\n",
            "1000/1000 - 4s - loss: 12.7207 - mse: 12.7207 - mae: 1.4431 - val_loss: 11.5332 - val_mse: 11.5332 - val_mae: 1.5260 - lr: 3.4138e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 14/100\n",
            "1000/1000 - 4s - loss: 12.6188 - mse: 12.6188 - mae: 1.4404 - val_loss: 11.4302 - val_mse: 11.4302 - val_mae: 1.4852 - lr: 3.4138e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 15/100\n",
            "1000/1000 - 4s - loss: 12.5111 - mse: 12.5111 - mae: 1.4390 - val_loss: 11.5523 - val_mse: 11.5523 - val_mae: 1.4740 - lr: 3.4138e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 16/100\n",
            "1000/1000 - 4s - loss: 12.4850 - mse: 12.4850 - mae: 1.4306 - val_loss: 11.6036 - val_mse: 11.6036 - val_mae: 1.5043 - lr: 3.4138e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 17/100\n",
            "1000/1000 - 4s - loss: 12.3896 - mse: 12.3896 - mae: 1.4264 - val_loss: 11.3668 - val_mse: 11.3668 - val_mae: 1.4882 - lr: 3.4138e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 18/100\n",
            "1000/1000 - 4s - loss: 12.2285 - mse: 12.2285 - mae: 1.4206 - val_loss: 11.5332 - val_mse: 11.5332 - val_mae: 1.5041 - lr: 3.4138e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 19/100\n",
            "1000/1000 - 4s - loss: 12.1510 - mse: 12.1510 - mae: 1.4151 - val_loss: 11.3311 - val_mse: 11.3311 - val_mae: 1.5553 - lr: 3.4138e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 20/100\n",
            "1000/1000 - 4s - loss: 12.0399 - mse: 12.0399 - mae: 1.4092 - val_loss: 11.5030 - val_mse: 11.5030 - val_mae: 1.5394 - lr: 3.4138e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 21/100\n",
            "1000/1000 - 4s - loss: 11.9228 - mse: 11.9228 - mae: 1.4075 - val_loss: 11.5951 - val_mse: 11.5951 - val_mae: 1.4997 - lr: 3.4138e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 22/100\n",
            "1000/1000 - 4s - loss: 11.8706 - mse: 11.8706 - mae: 1.3970 - val_loss: 11.3926 - val_mse: 11.3926 - val_mae: 1.5388 - lr: 3.4138e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 23/100\n",
            "1000/1000 - 4s - loss: 11.7640 - mse: 11.7640 - mae: 1.3949 - val_loss: 11.5017 - val_mse: 11.5017 - val_mae: 1.5190 - lr: 3.4138e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 24/100\n",
            "1000/1000 - 4s - loss: 11.6045 - mse: 11.6045 - mae: 1.3883 - val_loss: 11.5225 - val_mse: 11.5225 - val_mae: 1.6158 - lr: 3.4138e-04 - 4s/epoch - 4ms/step\n",
            "Score for fold 1: loss of 11.52245044708252\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 4s - loss: 10.3155 - mse: 10.3155 - mae: 1.4188 - val_loss: 16.4642 - val_mse: 16.4642 - val_mae: 1.4070 - lr: 3.4138e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 4s - loss: 10.1325 - mse: 10.1325 - mae: 1.4047 - val_loss: 16.4873 - val_mse: 16.4873 - val_mae: 1.4170 - lr: 3.4138e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 4s - loss: 9.9570 - mse: 9.9570 - mae: 1.3951 - val_loss: 16.5807 - val_mse: 16.5807 - val_mae: 1.4494 - lr: 3.4138e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 4s - loss: 9.8330 - mse: 9.8330 - mae: 1.3867 - val_loss: 16.8729 - val_mse: 16.8729 - val_mae: 1.5348 - lr: 3.4138e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 4s - loss: 9.6450 - mse: 9.6450 - mae: 1.3737 - val_loss: 16.7756 - val_mse: 16.7756 - val_mae: 1.4867 - lr: 3.4138e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 4s - loss: 9.5328 - mse: 9.5328 - mae: 1.3681 - val_loss: 16.7868 - val_mse: 16.7868 - val_mae: 1.4826 - lr: 3.4138e-04 - 4s/epoch - 4ms/step\n",
            "Score for fold 2: loss of 16.78676986694336\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 4s - loss: 11.2753 - mse: 11.2753 - mae: 1.3917 - val_loss: 9.5350 - val_mse: 9.5350 - val_mae: 1.3208 - lr: 3.4138e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 4s - loss: 11.1197 - mse: 11.1197 - mae: 1.3768 - val_loss: 9.6070 - val_mse: 9.6070 - val_mae: 1.3665 - lr: 3.4138e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 4s - loss: 10.9961 - mse: 10.9961 - mae: 1.3657 - val_loss: 9.7121 - val_mse: 9.7121 - val_mae: 1.3739 - lr: 3.4138e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 4s - loss: 10.8270 - mse: 10.8270 - mae: 1.3534 - val_loss: 9.7438 - val_mse: 9.7438 - val_mae: 1.3794 - lr: 3.4138e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 4s - loss: 10.7173 - mse: 10.7173 - mae: 1.3478 - val_loss: 9.9774 - val_mse: 9.9774 - val_mae: 1.4443 - lr: 3.4138e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 4s - loss: 10.6001 - mse: 10.6001 - mae: 1.3409 - val_loss: 9.8955 - val_mse: 9.8955 - val_mae: 1.4218 - lr: 3.4138e-04 - 4s/epoch - 4ms/step\n",
            "Score for fold 3: loss of 9.895522117614746\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 4s - loss: 10.0270 - mse: 10.0270 - mae: 1.3556 - val_loss: 11.8329 - val_mse: 11.8329 - val_mae: 1.3596 - lr: 3.4138e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 4s - loss: 9.7890 - mse: 9.7890 - mae: 1.3349 - val_loss: 12.1720 - val_mse: 12.1720 - val_mae: 1.3423 - lr: 3.4138e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 4s - loss: 9.6427 - mse: 9.6427 - mae: 1.3301 - val_loss: 12.2367 - val_mse: 12.2367 - val_mae: 1.3549 - lr: 3.4138e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 4s - loss: 9.5102 - mse: 9.5102 - mae: 1.3219 - val_loss: 12.3299 - val_mse: 12.3299 - val_mae: 1.3481 - lr: 3.4138e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 4s - loss: 9.3695 - mse: 9.3695 - mae: 1.3107 - val_loss: 12.5397 - val_mse: 12.5397 - val_mae: 1.3784 - lr: 3.4138e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 4s - loss: 9.2509 - mse: 9.2509 - mae: 1.3005 - val_loss: 12.2534 - val_mse: 12.2534 - val_mae: 1.3996 - lr: 3.4138e-04 - 4s/epoch - 4ms/step\n",
            "Score for fold 4: loss of 12.253363609313965\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 4s - loss: 11.0736 - mse: 11.0736 - mae: 1.3375 - val_loss: 4.7660 - val_mse: 4.7660 - val_mae: 1.2291 - lr: 3.4138e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 4s - loss: 10.8572 - mse: 10.8572 - mae: 1.3230 - val_loss: 4.9607 - val_mse: 4.9607 - val_mae: 1.2587 - lr: 3.4138e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 4s - loss: 10.7168 - mse: 10.7168 - mae: 1.3040 - val_loss: 4.9669 - val_mse: 4.9669 - val_mae: 1.2632 - lr: 3.4138e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 4s - loss: 10.5065 - mse: 10.5065 - mae: 1.2974 - val_loss: 5.1615 - val_mse: 5.1615 - val_mae: 1.2926 - lr: 3.4138e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 4s - loss: 10.3541 - mse: 10.3541 - mae: 1.2875 - val_loss: 5.3080 - val_mse: 5.3080 - val_mae: 1.3150 - lr: 3.4138e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 4s - loss: 10.2492 - mse: 10.2492 - mae: 1.2771 - val_loss: 5.3986 - val_mse: 5.3986 - val_mae: 1.3262 - lr: 3.4138e-04 - 4s/epoch - 4ms/step\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-10-12 03:03:04,164]\u001b[0m Finished trial#9 resulted in value: 11.172. Current best value is 10.362 with parameters: {'activation': 'relu', 'num_hidden_layer': 4, 'i': 9, 'learning_rate': 0.000580166332999716}.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Score for fold 5: loss of 5.398619651794434\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 11s - loss: 15.7222 - mse: 15.7222 - mae: 1.6662 - val_loss: 10.5798 - val_mse: 10.5798 - val_mae: 1.5629 - lr: 5.3749e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 10s - loss: 14.2237 - mse: 14.2237 - mae: 1.5726 - val_loss: 11.1534 - val_mse: 11.1534 - val_mae: 1.4749 - lr: 5.3749e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 10s - loss: 14.1172 - mse: 14.1172 - mae: 1.5702 - val_loss: 10.3463 - val_mse: 10.3463 - val_mae: 1.5548 - lr: 5.3749e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 10s - loss: 14.1000 - mse: 14.1000 - mae: 1.5724 - val_loss: 10.7968 - val_mse: 10.7968 - val_mae: 1.4670 - lr: 5.3749e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 10s - loss: 14.3704 - mse: 14.3704 - mae: 1.5594 - val_loss: 10.4278 - val_mse: 10.4278 - val_mae: 1.4614 - lr: 5.3749e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 10s - loss: 14.1765 - mse: 14.1765 - mae: 1.5571 - val_loss: 10.6219 - val_mse: 10.6219 - val_mae: 1.4647 - lr: 5.3749e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 10s - loss: 14.2391 - mse: 14.2391 - mae: 1.5653 - val_loss: 10.3999 - val_mse: 10.3999 - val_mae: 1.4901 - lr: 5.3749e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 10s - loss: 14.2197 - mse: 14.2197 - mae: 1.5643 - val_loss: 10.2738 - val_mse: 10.2738 - val_mae: 1.5055 - lr: 5.3749e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 10s - loss: 14.2156 - mse: 14.2156 - mae: 1.5762 - val_loss: 10.2822 - val_mse: 10.2822 - val_mae: 1.6199 - lr: 5.3749e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 10s - loss: 14.1340 - mse: 14.1340 - mae: 1.5594 - val_loss: 10.5022 - val_mse: 10.5022 - val_mae: 1.5275 - lr: 5.3749e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 10s - loss: 14.2189 - mse: 14.2189 - mae: 1.5642 - val_loss: 10.4020 - val_mse: 10.4020 - val_mae: 1.6262 - lr: 5.3749e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 10s - loss: 14.0793 - mse: 14.0793 - mae: 1.5625 - val_loss: 10.2636 - val_mse: 10.2636 - val_mae: 1.5110 - lr: 5.3749e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 13/100\n",
            "1000/1000 - 11s - loss: 14.0346 - mse: 14.0346 - mae: 1.5607 - val_loss: 10.3353 - val_mse: 10.3353 - val_mae: 1.5138 - lr: 5.3749e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 14/100\n",
            "1000/1000 - 11s - loss: 14.1162 - mse: 14.1162 - mae: 1.5607 - val_loss: 10.2496 - val_mse: 10.2496 - val_mae: 1.5967 - lr: 5.3749e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 15/100\n",
            "1000/1000 - 11s - loss: 14.1612 - mse: 14.1612 - mae: 1.5581 - val_loss: 10.3280 - val_mse: 10.3280 - val_mae: 1.5154 - lr: 5.3749e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 16/100\n",
            "1000/1000 - 11s - loss: 14.0917 - mse: 14.0917 - mae: 1.5607 - val_loss: 11.0310 - val_mse: 11.0310 - val_mae: 1.5485 - lr: 5.3749e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 17/100\n",
            "1000/1000 - 11s - loss: 14.0998 - mse: 14.0998 - mae: 1.5649 - val_loss: 10.3423 - val_mse: 10.3423 - val_mae: 1.4977 - lr: 5.3749e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 18/100\n",
            "1000/1000 - 11s - loss: 14.0807 - mse: 14.0807 - mae: 1.5568 - val_loss: 10.3662 - val_mse: 10.3662 - val_mae: 1.4861 - lr: 5.3749e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 19/100\n",
            "1000/1000 - 11s - loss: 14.0866 - mse: 14.0866 - mae: 1.5589 - val_loss: 10.2046 - val_mse: 10.2046 - val_mae: 1.5814 - lr: 5.3749e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 20/100\n",
            "1000/1000 - 11s - loss: 14.1179 - mse: 14.1179 - mae: 1.5661 - val_loss: 10.3809 - val_mse: 10.3809 - val_mae: 1.4582 - lr: 5.3749e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 21/100\n",
            "1000/1000 - 11s - loss: 14.2221 - mse: 14.2221 - mae: 1.5667 - val_loss: 10.4172 - val_mse: 10.4172 - val_mae: 1.6419 - lr: 5.3749e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 22/100\n",
            "1000/1000 - 11s - loss: 14.1848 - mse: 14.1848 - mae: 1.5627 - val_loss: 10.1946 - val_mse: 10.1946 - val_mae: 1.5601 - lr: 5.3749e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 23/100\n",
            "1000/1000 - 10s - loss: 14.0600 - mse: 14.0600 - mae: 1.5624 - val_loss: 10.3026 - val_mse: 10.3026 - val_mae: 1.6153 - lr: 5.3749e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 24/100\n",
            "1000/1000 - 10s - loss: 14.2405 - mse: 14.2405 - mae: 1.5612 - val_loss: 10.1959 - val_mse: 10.1959 - val_mae: 1.6282 - lr: 5.3749e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 25/100\n",
            "1000/1000 - 10s - loss: 14.0737 - mse: 14.0737 - mae: 1.5521 - val_loss: 10.3373 - val_mse: 10.3373 - val_mae: 1.6243 - lr: 5.3749e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 26/100\n",
            "1000/1000 - 10s - loss: 14.1037 - mse: 14.1037 - mae: 1.5678 - val_loss: 10.2528 - val_mse: 10.2528 - val_mae: 1.6053 - lr: 5.3749e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 27/100\n",
            "1000/1000 - 10s - loss: 14.1946 - mse: 14.1946 - mae: 1.5593 - val_loss: 10.2707 - val_mse: 10.2707 - val_mae: 1.5432 - lr: 5.3749e-04 - 10s/epoch - 10ms/step\n",
            "Score for fold 1: loss of 10.270722389221191\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 11s - loss: 13.9820 - mse: 13.9820 - mae: 1.5684 - val_loss: 10.3542 - val_mse: 10.3542 - val_mae: 1.6779 - lr: 5.3749e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 13s - loss: 14.1203 - mse: 14.1203 - mae: 1.5671 - val_loss: 10.2165 - val_mse: 10.2165 - val_mae: 1.5686 - lr: 5.3749e-04 - 13s/epoch - 13ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 13s - loss: 14.0620 - mse: 14.0620 - mae: 1.5609 - val_loss: 10.4147 - val_mse: 10.4147 - val_mae: 1.4875 - lr: 5.3749e-04 - 13s/epoch - 13ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 13s - loss: 14.1239 - mse: 14.1239 - mae: 1.5633 - val_loss: 11.5144 - val_mse: 11.5144 - val_mae: 1.4919 - lr: 5.3749e-04 - 13s/epoch - 13ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 14s - loss: 14.2197 - mse: 14.2197 - mae: 1.5717 - val_loss: 10.6136 - val_mse: 10.6136 - val_mae: 1.5053 - lr: 5.3749e-04 - 14s/epoch - 14ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 13s - loss: 14.1863 - mse: 14.1863 - mae: 1.5700 - val_loss: 10.1913 - val_mse: 10.1913 - val_mae: 1.5373 - lr: 5.3749e-04 - 13s/epoch - 13ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 13s - loss: 14.1079 - mse: 14.1079 - mae: 1.5655 - val_loss: 10.2230 - val_mse: 10.2230 - val_mae: 1.5943 - lr: 5.3749e-04 - 13s/epoch - 13ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 13s - loss: 13.9733 - mse: 13.9733 - mae: 1.5583 - val_loss: 11.0855 - val_mse: 11.0855 - val_mae: 1.4649 - lr: 5.3749e-04 - 13s/epoch - 13ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 13s - loss: 14.1438 - mse: 14.1438 - mae: 1.5718 - val_loss: 10.4482 - val_mse: 10.4482 - val_mae: 1.7197 - lr: 5.3749e-04 - 13s/epoch - 13ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 14s - loss: 13.9981 - mse: 13.9981 - mae: 1.5667 - val_loss: 10.3946 - val_mse: 10.3946 - val_mae: 1.5759 - lr: 5.3749e-04 - 14s/epoch - 14ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 13s - loss: 14.1596 - mse: 14.1596 - mae: 1.5560 - val_loss: 10.6155 - val_mse: 10.6155 - val_mae: 1.5282 - lr: 5.3749e-04 - 13s/epoch - 13ms/step\n",
            "Score for fold 2: loss of 10.615449905395508\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 13s - loss: 11.3940 - mse: 11.3940 - mae: 1.5527 - val_loss: 21.4983 - val_mse: 21.4983 - val_mae: 1.5489 - lr: 5.3749e-04 - 13s/epoch - 13ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 14s - loss: 11.3720 - mse: 11.3720 - mae: 1.5555 - val_loss: 22.0382 - val_mse: 22.0382 - val_mae: 1.7927 - lr: 5.3749e-04 - 14s/epoch - 14ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 14s - loss: 11.2814 - mse: 11.2814 - mae: 1.5502 - val_loss: 22.0677 - val_mse: 22.0677 - val_mae: 1.6141 - lr: 5.3749e-04 - 14s/epoch - 14ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 13s - loss: 11.3201 - mse: 11.3201 - mae: 1.5507 - val_loss: 21.2117 - val_mse: 21.2117 - val_mae: 1.5375 - lr: 5.3749e-04 - 13s/epoch - 13ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 13s - loss: 11.3306 - mse: 11.3306 - mae: 1.5435 - val_loss: 21.4494 - val_mse: 21.4494 - val_mae: 1.5803 - lr: 5.3749e-04 - 13s/epoch - 13ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 14s - loss: 11.2711 - mse: 11.2711 - mae: 1.5539 - val_loss: 21.3623 - val_mse: 21.3623 - val_mae: 1.5500 - lr: 5.3749e-04 - 14s/epoch - 14ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 13s - loss: 11.4901 - mse: 11.4901 - mae: 1.5533 - val_loss: 21.2539 - val_mse: 21.2539 - val_mae: 1.5345 - lr: 5.3749e-04 - 13s/epoch - 13ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 13s - loss: 11.3000 - mse: 11.3000 - mae: 1.5530 - val_loss: 21.4056 - val_mse: 21.4056 - val_mae: 1.5396 - lr: 5.3749e-04 - 13s/epoch - 13ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 13s - loss: 11.4576 - mse: 11.4576 - mae: 1.5615 - val_loss: 21.4530 - val_mse: 21.4530 - val_mae: 1.5970 - lr: 5.3749e-04 - 13s/epoch - 13ms/step\n",
            "Score for fold 3: loss of 21.453014373779297\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 13s - loss: 12.9739 - mse: 12.9739 - mae: 1.5619 - val_loss: 15.0577 - val_mse: 15.0577 - val_mae: 1.5515 - lr: 5.3749e-04 - 13s/epoch - 13ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 11s - loss: 12.9516 - mse: 12.9516 - mae: 1.5609 - val_loss: 15.0894 - val_mse: 15.0894 - val_mae: 1.5222 - lr: 5.3749e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 13s - loss: 13.1272 - mse: 13.1272 - mae: 1.5623 - val_loss: 15.2401 - val_mse: 15.2401 - val_mae: 1.5204 - lr: 5.3749e-04 - 13s/epoch - 13ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 11s - loss: 13.0281 - mse: 13.0281 - mae: 1.5561 - val_loss: 15.0600 - val_mse: 15.0600 - val_mae: 1.5532 - lr: 5.3749e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 13s - loss: 12.8303 - mse: 12.8303 - mae: 1.5546 - val_loss: 15.3329 - val_mse: 15.3329 - val_mae: 1.6592 - lr: 5.3749e-04 - 13s/epoch - 13ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 14s - loss: 12.9367 - mse: 12.9367 - mae: 1.5598 - val_loss: 15.0599 - val_mse: 15.0599 - val_mae: 1.5878 - lr: 5.3749e-04 - 14s/epoch - 14ms/step\n",
            "Score for fold 4: loss of 15.059901237487793\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 15s - loss: 14.5385 - mse: 14.5385 - mae: 1.5589 - val_loss: 9.4468 - val_mse: 9.4468 - val_mae: 1.5757 - lr: 5.3749e-04 - 15s/epoch - 15ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 15s - loss: 14.4305 - mse: 14.4305 - mae: 1.5659 - val_loss: 9.2633 - val_mse: 9.2633 - val_mae: 1.5641 - lr: 5.3749e-04 - 15s/epoch - 15ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 15s - loss: 14.3256 - mse: 14.3256 - mae: 1.5642 - val_loss: 11.1516 - val_mse: 11.1516 - val_mae: 1.8427 - lr: 5.3749e-04 - 15s/epoch - 15ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 15s - loss: 14.3417 - mse: 14.3417 - mae: 1.5593 - val_loss: 9.1382 - val_mse: 9.1382 - val_mae: 1.5242 - lr: 5.3749e-04 - 15s/epoch - 15ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 13s - loss: 14.4190 - mse: 14.4190 - mae: 1.5585 - val_loss: 9.1467 - val_mse: 9.1467 - val_mae: 1.5199 - lr: 5.3749e-04 - 13s/epoch - 13ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 11s - loss: 14.3572 - mse: 14.3572 - mae: 1.5738 - val_loss: 9.2603 - val_mse: 9.2603 - val_mae: 1.4769 - lr: 5.3749e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 11s - loss: 14.3366 - mse: 14.3366 - mae: 1.5651 - val_loss: 9.1105 - val_mse: 9.1105 - val_mae: 1.5559 - lr: 5.3749e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 11s - loss: 14.3489 - mse: 14.3489 - mae: 1.5667 - val_loss: 9.3643 - val_mse: 9.3643 - val_mae: 1.6120 - lr: 5.3749e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 11s - loss: 14.4223 - mse: 14.4223 - mae: 1.5691 - val_loss: 9.3384 - val_mse: 9.3384 - val_mae: 1.5936 - lr: 5.3749e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 11s - loss: 14.3699 - mse: 14.3699 - mae: 1.5647 - val_loss: 9.1293 - val_mse: 9.1293 - val_mae: 1.5399 - lr: 5.3749e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 13s - loss: 14.3511 - mse: 14.3511 - mae: 1.5568 - val_loss: 9.0982 - val_mse: 9.0982 - val_mae: 1.5095 - lr: 5.3749e-04 - 13s/epoch - 13ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 13s - loss: 14.4371 - mse: 14.4371 - mae: 1.5618 - val_loss: 9.5170 - val_mse: 9.5170 - val_mae: 1.6010 - lr: 5.3749e-04 - 13s/epoch - 13ms/step\n",
            "Epoch 13/100\n",
            "1000/1000 - 13s - loss: 14.3997 - mse: 14.3997 - mae: 1.5580 - val_loss: 9.2083 - val_mse: 9.2083 - val_mae: 1.4674 - lr: 5.3749e-04 - 13s/epoch - 13ms/step\n",
            "Epoch 14/100\n",
            "1000/1000 - 13s - loss: 14.3208 - mse: 14.3208 - mae: 1.5622 - val_loss: 9.1268 - val_mse: 9.1268 - val_mae: 1.5118 - lr: 5.3749e-04 - 13s/epoch - 13ms/step\n",
            "Epoch 15/100\n",
            "1000/1000 - 12s - loss: 14.3296 - mse: 14.3296 - mae: 1.5622 - val_loss: 9.1794 - val_mse: 9.1794 - val_mae: 1.5505 - lr: 5.3749e-04 - 12s/epoch - 12ms/step\n",
            "Epoch 16/100\n",
            "1000/1000 - 10s - loss: 14.2560 - mse: 14.2560 - mae: 1.5650 - val_loss: 9.1464 - val_mse: 9.1464 - val_mae: 1.5472 - lr: 5.3749e-04 - 10s/epoch - 10ms/step\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-10-12 03:16:53,792]\u001b[0m Finished trial#10 resulted in value: 13.310000000000002. Current best value is 10.362 with parameters: {'activation': 'relu', 'num_hidden_layer': 4, 'i': 9, 'learning_rate': 0.000580166332999716}.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Score for fold 5: loss of 9.14638614654541\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 12s - loss: 14.7501 - mse: 14.7501 - mae: 1.5792 - val_loss: 8.8322 - val_mse: 8.8322 - val_mae: 1.4696 - lr: 0.0016 - 12s/epoch - 12ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 11s - loss: 13.6342 - mse: 13.6342 - mae: 1.5443 - val_loss: 8.0779 - val_mse: 8.0779 - val_mae: 1.4518 - lr: 0.0016 - 11s/epoch - 11ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 11s - loss: 13.5356 - mse: 13.5356 - mae: 1.5274 - val_loss: 7.8406 - val_mse: 7.8406 - val_mae: 1.5104 - lr: 0.0016 - 11s/epoch - 11ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 11s - loss: 13.4881 - mse: 13.4881 - mae: 1.5167 - val_loss: 7.9537 - val_mse: 7.9537 - val_mae: 1.4387 - lr: 0.0016 - 11s/epoch - 11ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 11s - loss: 13.3782 - mse: 13.3782 - mae: 1.5006 - val_loss: 8.6100 - val_mse: 8.6100 - val_mae: 1.5018 - lr: 0.0016 - 11s/epoch - 11ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 11s - loss: 13.1135 - mse: 13.1135 - mae: 1.4877 - val_loss: 7.9450 - val_mse: 7.9450 - val_mae: 1.4919 - lr: 0.0016 - 11s/epoch - 11ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 11s - loss: 13.0013 - mse: 13.0013 - mae: 1.4834 - val_loss: 8.0364 - val_mse: 8.0364 - val_mae: 1.4339 - lr: 0.0016 - 11s/epoch - 11ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 11s - loss: 13.0396 - mse: 13.0396 - mae: 1.4749 - val_loss: 8.2277 - val_mse: 8.2277 - val_mae: 1.3680 - lr: 0.0016 - 11s/epoch - 11ms/step\n",
            "Score for fold 1: loss of 8.227713584899902\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 12s - loss: 11.0673 - mse: 11.0673 - mae: 1.4363 - val_loss: 14.3852 - val_mse: 14.3852 - val_mae: 1.5609 - lr: 0.0010 - 12s/epoch - 12ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 12s - loss: 11.0186 - mse: 11.0186 - mae: 1.4286 - val_loss: 14.3921 - val_mse: 14.3921 - val_mae: 1.4734 - lr: 0.0010 - 12s/epoch - 12ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 12s - loss: 10.6242 - mse: 10.6242 - mae: 1.4205 - val_loss: 14.3343 - val_mse: 14.3343 - val_mae: 1.4768 - lr: 0.0010 - 12s/epoch - 12ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 12s - loss: 10.4957 - mse: 10.4957 - mae: 1.4123 - val_loss: 14.4164 - val_mse: 14.4164 - val_mae: 1.4781 - lr: 0.0010 - 12s/epoch - 12ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 12s - loss: 10.5010 - mse: 10.5010 - mae: 1.4049 - val_loss: 14.4272 - val_mse: 14.4272 - val_mae: 1.4822 - lr: 0.0010 - 12s/epoch - 12ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 12s - loss: 10.2746 - mse: 10.2746 - mae: 1.4031 - val_loss: 14.4676 - val_mse: 14.4676 - val_mae: 1.4519 - lr: 0.0010 - 12s/epoch - 12ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 12s - loss: 10.2337 - mse: 10.2337 - mae: 1.3978 - val_loss: 14.4610 - val_mse: 14.4610 - val_mae: 1.5307 - lr: 0.0010 - 12s/epoch - 12ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 12s - loss: 10.0390 - mse: 10.0390 - mae: 1.3892 - val_loss: 14.4829 - val_mse: 14.4829 - val_mae: 1.5147 - lr: 0.0010 - 12s/epoch - 12ms/step\n",
            "Score for fold 2: loss of 14.482952117919922\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 11s - loss: 11.7728 - mse: 11.7728 - mae: 1.4192 - val_loss: 7.3871 - val_mse: 7.3871 - val_mae: 1.3649 - lr: 0.0010 - 11s/epoch - 11ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 11s - loss: 11.5888 - mse: 11.5888 - mae: 1.4075 - val_loss: 7.6187 - val_mse: 7.6187 - val_mae: 1.3763 - lr: 0.0010 - 11s/epoch - 11ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 11s - loss: 11.3660 - mse: 11.3660 - mae: 1.3932 - val_loss: 7.7799 - val_mse: 7.7799 - val_mae: 1.4228 - lr: 0.0010 - 11s/epoch - 11ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 11s - loss: 11.2810 - mse: 11.2810 - mae: 1.3864 - val_loss: 7.7015 - val_mse: 7.7015 - val_mae: 1.3723 - lr: 0.0010 - 11s/epoch - 11ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 11s - loss: 10.9736 - mse: 10.9736 - mae: 1.3795 - val_loss: 8.5035 - val_mse: 8.5035 - val_mae: 1.4084 - lr: 0.0010 - 11s/epoch - 11ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 11s - loss: 10.9848 - mse: 10.9848 - mae: 1.3720 - val_loss: 7.7983 - val_mse: 7.7983 - val_mae: 1.4441 - lr: 0.0010 - 11s/epoch - 11ms/step\n",
            "Score for fold 3: loss of 7.798262119293213\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 11s - loss: 8.6109 - mse: 8.6109 - mae: 1.3838 - val_loss: 17.2462 - val_mse: 17.2462 - val_mae: 1.3542 - lr: 0.0010 - 11s/epoch - 11ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 11s - loss: 8.3680 - mse: 8.3680 - mae: 1.3659 - val_loss: 17.5839 - val_mse: 17.5839 - val_mae: 1.3893 - lr: 0.0010 - 11s/epoch - 11ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 11s - loss: 8.5384 - mse: 8.5384 - mae: 1.3616 - val_loss: 17.8412 - val_mse: 17.8412 - val_mae: 1.3939 - lr: 0.0010 - 11s/epoch - 11ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 11s - loss: 8.3029 - mse: 8.3029 - mae: 1.3516 - val_loss: 17.8425 - val_mse: 17.8425 - val_mae: 1.4637 - lr: 0.0010 - 11s/epoch - 11ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 11s - loss: 8.0321 - mse: 8.0321 - mae: 1.3470 - val_loss: 17.7699 - val_mse: 17.7699 - val_mae: 1.3567 - lr: 0.0010 - 11s/epoch - 11ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 11s - loss: 7.6565 - mse: 7.6565 - mae: 1.3298 - val_loss: 18.7758 - val_mse: 18.7758 - val_mae: 1.3707 - lr: 0.0010 - 11s/epoch - 11ms/step\n",
            "Score for fold 4: loss of 18.775842666625977\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 11s - loss: 10.7103 - mse: 10.7103 - mae: 1.3483 - val_loss: 6.1882 - val_mse: 6.1882 - val_mae: 1.3131 - lr: 0.0010 - 11s/epoch - 11ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 11s - loss: 10.4403 - mse: 10.4403 - mae: 1.3413 - val_loss: 6.4006 - val_mse: 6.4006 - val_mae: 1.3201 - lr: 0.0010 - 11s/epoch - 11ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 11s - loss: 10.5960 - mse: 10.5960 - mae: 1.3351 - val_loss: 6.4996 - val_mse: 6.4996 - val_mae: 1.3331 - lr: 0.0010 - 11s/epoch - 11ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 11s - loss: 10.0497 - mse: 10.0497 - mae: 1.3150 - val_loss: 6.4515 - val_mse: 6.4515 - val_mae: 1.3232 - lr: 0.0010 - 11s/epoch - 11ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 11s - loss: 10.0036 - mse: 10.0036 - mae: 1.3087 - val_loss: 6.7339 - val_mse: 6.7339 - val_mae: 1.3147 - lr: 0.0010 - 11s/epoch - 11ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 11s - loss: 9.8763 - mse: 9.8763 - mae: 1.2997 - val_loss: 6.7203 - val_mse: 6.7203 - val_mae: 1.3591 - lr: 0.0010 - 11s/epoch - 11ms/step\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-10-12 03:23:31,952]\u001b[0m Finished trial#11 resulted in value: 11.202000000000002. Current best value is 10.362 with parameters: {'activation': 'relu', 'num_hidden_layer': 4, 'i': 9, 'learning_rate': 0.000580166332999716}.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Score for fold 5: loss of 6.720324516296387\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 7s - loss: 13.6555 - mse: 13.6555 - mae: 1.5537 - val_loss: 13.1057 - val_mse: 13.1057 - val_mae: 1.4611 - lr: 0.0011 - 7s/epoch - 7ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 6s - loss: 12.6754 - mse: 12.6754 - mae: 1.4999 - val_loss: 11.4797 - val_mse: 11.4797 - val_mae: 1.5299 - lr: 0.0011 - 6s/epoch - 6ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 6s - loss: 12.6582 - mse: 12.6582 - mae: 1.4866 - val_loss: 11.6719 - val_mse: 11.6719 - val_mae: 1.4986 - lr: 0.0011 - 6s/epoch - 6ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 6s - loss: 12.4294 - mse: 12.4294 - mae: 1.4742 - val_loss: 12.4283 - val_mse: 12.4283 - val_mae: 1.5751 - lr: 0.0011 - 6s/epoch - 6ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 6s - loss: 12.3659 - mse: 12.3659 - mae: 1.4649 - val_loss: 11.2871 - val_mse: 11.2871 - val_mae: 1.4252 - lr: 0.0011 - 6s/epoch - 6ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 6s - loss: 12.2081 - mse: 12.2081 - mae: 1.4584 - val_loss: 11.8322 - val_mse: 11.8322 - val_mae: 1.5945 - lr: 0.0011 - 6s/epoch - 6ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 7s - loss: 12.0062 - mse: 12.0062 - mae: 1.4486 - val_loss: 11.0467 - val_mse: 11.0467 - val_mae: 1.5526 - lr: 0.0011 - 7s/epoch - 7ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 6s - loss: 11.8537 - mse: 11.8537 - mae: 1.4421 - val_loss: 11.6169 - val_mse: 11.6169 - val_mae: 1.4836 - lr: 0.0011 - 6s/epoch - 6ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 6s - loss: 11.6576 - mse: 11.6576 - mae: 1.4279 - val_loss: 11.1723 - val_mse: 11.1723 - val_mae: 1.4508 - lr: 0.0011 - 6s/epoch - 6ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 6s - loss: 11.5545 - mse: 11.5545 - mae: 1.4261 - val_loss: 11.2876 - val_mse: 11.2876 - val_mae: 1.5623 - lr: 0.0011 - 6s/epoch - 6ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 6s - loss: 11.3257 - mse: 11.3257 - mae: 1.4181 - val_loss: 11.0270 - val_mse: 11.0270 - val_mae: 1.4894 - lr: 0.0011 - 6s/epoch - 6ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 6s - loss: 11.1862 - mse: 11.1862 - mae: 1.4162 - val_loss: 11.2118 - val_mse: 11.2118 - val_mae: 1.4768 - lr: 0.0011 - 6s/epoch - 6ms/step\n",
            "Epoch 13/100\n",
            "1000/1000 - 6s - loss: 11.2171 - mse: 11.2171 - mae: 1.4080 - val_loss: 11.5542 - val_mse: 11.5542 - val_mae: 1.4854 - lr: 0.0011 - 6s/epoch - 6ms/step\n",
            "Epoch 14/100\n",
            "1000/1000 - 6s - loss: 10.9657 - mse: 10.9657 - mae: 1.4054 - val_loss: 11.2034 - val_mse: 11.2034 - val_mae: 1.4533 - lr: 0.0011 - 6s/epoch - 6ms/step\n",
            "Epoch 15/100\n",
            "1000/1000 - 6s - loss: 10.8566 - mse: 10.8566 - mae: 1.4063 - val_loss: 11.5293 - val_mse: 11.5293 - val_mae: 1.4556 - lr: 0.0011 - 6s/epoch - 6ms/step\n",
            "Epoch 16/100\n",
            "1000/1000 - 6s - loss: 10.8962 - mse: 10.8962 - mae: 1.3972 - val_loss: 10.9565 - val_mse: 10.9565 - val_mae: 1.4586 - lr: 0.0011 - 6s/epoch - 6ms/step\n",
            "Epoch 17/100\n",
            "1000/1000 - 6s - loss: 10.7116 - mse: 10.7116 - mae: 1.3842 - val_loss: 11.3225 - val_mse: 11.3225 - val_mae: 1.5012 - lr: 0.0011 - 6s/epoch - 6ms/step\n",
            "Epoch 18/100\n",
            "1000/1000 - 6s - loss: 10.4137 - mse: 10.4137 - mae: 1.3769 - val_loss: 11.3311 - val_mse: 11.3311 - val_mae: 1.4607 - lr: 0.0011 - 6s/epoch - 6ms/step\n",
            "Epoch 19/100\n",
            "1000/1000 - 6s - loss: 10.6223 - mse: 10.6223 - mae: 1.3711 - val_loss: 11.4581 - val_mse: 11.4581 - val_mae: 1.5226 - lr: 0.0011 - 6s/epoch - 6ms/step\n",
            "Epoch 20/100\n",
            "1000/1000 - 6s - loss: 10.2520 - mse: 10.2520 - mae: 1.3560 - val_loss: 11.2662 - val_mse: 11.2662 - val_mae: 1.4554 - lr: 0.0011 - 6s/epoch - 6ms/step\n",
            "Epoch 21/100\n",
            "1000/1000 - 6s - loss: 9.9886 - mse: 9.9886 - mae: 1.3484 - val_loss: 11.7516 - val_mse: 11.7516 - val_mae: 1.5236 - lr: 0.0011 - 6s/epoch - 6ms/step\n",
            "Score for fold 1: loss of 11.751575469970703\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 6s - loss: 10.8783 - mse: 10.8783 - mae: 1.3838 - val_loss: 8.7250 - val_mse: 8.7250 - val_mae: 1.3234 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 6s - loss: 10.5609 - mse: 10.5609 - mae: 1.3692 - val_loss: 8.8624 - val_mse: 8.8624 - val_mae: 1.3039 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 6s - loss: 10.3314 - mse: 10.3314 - mae: 1.3590 - val_loss: 8.8259 - val_mse: 8.8259 - val_mae: 1.3400 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 6s - loss: 10.2080 - mse: 10.2080 - mae: 1.3460 - val_loss: 8.9137 - val_mse: 8.9137 - val_mae: 1.3188 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 6s - loss: 9.7765 - mse: 9.7765 - mae: 1.3334 - val_loss: 9.1462 - val_mse: 9.1462 - val_mae: 1.3376 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 6s - loss: 9.6464 - mse: 9.6464 - mae: 1.3211 - val_loss: 9.4662 - val_mse: 9.4662 - val_mae: 1.4247 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
            "Score for fold 2: loss of 9.466181755065918\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 6s - loss: 10.4526 - mse: 10.4526 - mae: 1.3383 - val_loss: 7.1687 - val_mse: 7.1687 - val_mae: 1.2916 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 6s - loss: 10.2307 - mse: 10.2307 - mae: 1.3237 - val_loss: 7.8663 - val_mse: 7.8663 - val_mae: 1.2808 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 6s - loss: 10.0658 - mse: 10.0658 - mae: 1.3117 - val_loss: 7.1077 - val_mse: 7.1077 - val_mae: 1.2713 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 6s - loss: 9.7196 - mse: 9.7196 - mae: 1.2997 - val_loss: 7.0618 - val_mse: 7.0618 - val_mae: 1.3417 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 6s - loss: 9.7616 - mse: 9.7616 - mae: 1.2881 - val_loss: 7.4096 - val_mse: 7.4096 - val_mae: 1.3424 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 6s - loss: 9.4927 - mse: 9.4927 - mae: 1.2799 - val_loss: 7.2550 - val_mse: 7.2550 - val_mae: 1.3093 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 6s - loss: 9.3060 - mse: 9.3060 - mae: 1.2665 - val_loss: 7.5072 - val_mse: 7.5072 - val_mae: 1.2774 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 6s - loss: 9.2679 - mse: 9.2679 - mae: 1.2644 - val_loss: 8.1042 - val_mse: 8.1042 - val_mae: 1.3924 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 6s - loss: 9.0705 - mse: 9.0705 - mae: 1.2515 - val_loss: 7.7010 - val_mse: 7.7010 - val_mae: 1.3752 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
            "Score for fold 3: loss of 7.700957775115967\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 7s - loss: 7.6222 - mse: 7.6222 - mae: 1.2815 - val_loss: 14.6344 - val_mse: 14.6344 - val_mae: 1.2685 - lr: 0.0010 - 7s/epoch - 7ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 6s - loss: 7.1131 - mse: 7.1131 - mae: 1.2659 - val_loss: 15.1492 - val_mse: 15.1492 - val_mae: 1.2565 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 6s - loss: 6.9889 - mse: 6.9889 - mae: 1.2557 - val_loss: 14.7594 - val_mse: 14.7594 - val_mae: 1.3229 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 6s - loss: 6.6468 - mse: 6.6468 - mae: 1.2388 - val_loss: 15.1864 - val_mse: 15.1864 - val_mae: 1.3009 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 6s - loss: 6.7863 - mse: 6.7863 - mae: 1.2340 - val_loss: 15.4207 - val_mse: 15.4207 - val_mae: 1.3238 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 6s - loss: 6.5724 - mse: 6.5724 - mae: 1.2190 - val_loss: 15.5050 - val_mse: 15.5050 - val_mae: 1.2791 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
            "Score for fold 4: loss of 15.504983901977539\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 6s - loss: 9.1252 - mse: 9.1252 - mae: 1.2476 - val_loss: 5.5481 - val_mse: 5.5481 - val_mae: 1.1816 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 6s - loss: 8.7015 - mse: 8.7015 - mae: 1.2317 - val_loss: 5.3355 - val_mse: 5.3355 - val_mae: 1.1851 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 7s - loss: 8.6786 - mse: 8.6786 - mae: 1.2229 - val_loss: 5.6801 - val_mse: 5.6801 - val_mae: 1.1981 - lr: 0.0010 - 7s/epoch - 7ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 7s - loss: 8.4678 - mse: 8.4678 - mae: 1.2116 - val_loss: 5.8254 - val_mse: 5.8254 - val_mae: 1.2062 - lr: 0.0010 - 7s/epoch - 7ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 7s - loss: 8.2425 - mse: 8.2425 - mae: 1.1988 - val_loss: 5.5901 - val_mse: 5.5901 - val_mae: 1.2672 - lr: 0.0010 - 7s/epoch - 7ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 6s - loss: 8.1180 - mse: 8.1180 - mae: 1.1918 - val_loss: 5.6790 - val_mse: 5.6790 - val_mae: 1.2294 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 6s - loss: 7.9391 - mse: 7.9391 - mae: 1.1860 - val_loss: 5.6909 - val_mse: 5.6909 - val_mae: 1.2324 - lr: 0.0010 - 6s/epoch - 6ms/step\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-10-12 03:28:48,726]\u001b[0m Finished trial#12 resulted in value: 10.022. Current best value is 10.022 with parameters: {'activation': 'relu', 'num_hidden_layer': 4, 'i': 9, 'learning_rate': 0.0011355512574150934}.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Score for fold 5: loss of 5.690929889678955\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 7s - loss: 13.0009 - mse: 13.0009 - mae: 1.5475 - val_loss: 13.3894 - val_mse: 13.3894 - val_mae: 1.4719 - lr: 3.6677e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 6s - loss: 12.0530 - mse: 12.0530 - mae: 1.4851 - val_loss: 13.5518 - val_mse: 13.5518 - val_mae: 1.4728 - lr: 3.6677e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 6s - loss: 11.8329 - mse: 11.8329 - mae: 1.4721 - val_loss: 13.6854 - val_mse: 13.6854 - val_mae: 1.4691 - lr: 3.6677e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 6s - loss: 11.8447 - mse: 11.8447 - mae: 1.4657 - val_loss: 13.4143 - val_mse: 13.4143 - val_mae: 1.4569 - lr: 3.6677e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 7s - loss: 11.5019 - mse: 11.5019 - mae: 1.4509 - val_loss: 13.5967 - val_mse: 13.5967 - val_mae: 1.4891 - lr: 3.6677e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 6s - loss: 11.5495 - mse: 11.5495 - mae: 1.4431 - val_loss: 13.3061 - val_mse: 13.3061 - val_mae: 1.4375 - lr: 3.6677e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 6s - loss: 11.5009 - mse: 11.5009 - mae: 1.4401 - val_loss: 13.2520 - val_mse: 13.2520 - val_mae: 1.4331 - lr: 3.6677e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 6s - loss: 11.2309 - mse: 11.2309 - mae: 1.4270 - val_loss: 13.5775 - val_mse: 13.5775 - val_mae: 1.4607 - lr: 3.6677e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 6s - loss: 11.0417 - mse: 11.0417 - mae: 1.4141 - val_loss: 13.3186 - val_mse: 13.3186 - val_mae: 1.4277 - lr: 3.6677e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 6s - loss: 10.7756 - mse: 10.7756 - mae: 1.4104 - val_loss: 13.3575 - val_mse: 13.3575 - val_mae: 1.4565 - lr: 3.6677e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 6s - loss: 10.7120 - mse: 10.7120 - mae: 1.3954 - val_loss: 13.4558 - val_mse: 13.4558 - val_mae: 1.5519 - lr: 3.6677e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 6s - loss: 10.3119 - mse: 10.3119 - mae: 1.3853 - val_loss: 13.6078 - val_mse: 13.6078 - val_mae: 1.4501 - lr: 3.6677e-04 - 6s/epoch - 6ms/step\n",
            "Score for fold 1: loss of 13.607845306396484\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 6s - loss: 9.9431 - mse: 9.9431 - mae: 1.4097 - val_loss: 13.6534 - val_mse: 13.6534 - val_mae: 1.3428 - lr: 3.6677e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 6s - loss: 9.9147 - mse: 9.9147 - mae: 1.3963 - val_loss: 13.6904 - val_mse: 13.6904 - val_mae: 1.3879 - lr: 3.6677e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 6s - loss: 9.3589 - mse: 9.3589 - mae: 1.3796 - val_loss: 14.0303 - val_mse: 14.0303 - val_mae: 1.3986 - lr: 3.6677e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 6s - loss: 9.5073 - mse: 9.5073 - mae: 1.3668 - val_loss: 14.1224 - val_mse: 14.1224 - val_mae: 1.3698 - lr: 3.6677e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 6s - loss: 9.0976 - mse: 9.0976 - mae: 1.3552 - val_loss: 14.5311 - val_mse: 14.5311 - val_mae: 1.4810 - lr: 3.6677e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 6s - loss: 8.9614 - mse: 8.9614 - mae: 1.3391 - val_loss: 14.3669 - val_mse: 14.3669 - val_mae: 1.4729 - lr: 3.6677e-04 - 6s/epoch - 6ms/step\n",
            "Score for fold 2: loss of 14.366887092590332\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 6s - loss: 10.4925 - mse: 10.4925 - mae: 1.3582 - val_loss: 7.3272 - val_mse: 7.3272 - val_mae: 1.2702 - lr: 3.6677e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 6s - loss: 10.1847 - mse: 10.1847 - mae: 1.3396 - val_loss: 7.4508 - val_mse: 7.4508 - val_mae: 1.3309 - lr: 3.6677e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 6s - loss: 9.9414 - mse: 9.9414 - mae: 1.3229 - val_loss: 7.5117 - val_mse: 7.5117 - val_mae: 1.3437 - lr: 3.6677e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 6s - loss: 9.8252 - mse: 9.8252 - mae: 1.3119 - val_loss: 7.9605 - val_mse: 7.9605 - val_mae: 1.3448 - lr: 3.6677e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 6s - loss: 9.4696 - mse: 9.4696 - mae: 1.2944 - val_loss: 7.6501 - val_mse: 7.6501 - val_mae: 1.3197 - lr: 3.6677e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 6s - loss: 9.5114 - mse: 9.5114 - mae: 1.2778 - val_loss: 8.4472 - val_mse: 8.4472 - val_mae: 1.4077 - lr: 3.6677e-04 - 6s/epoch - 6ms/step\n",
            "Score for fold 3: loss of 8.447175025939941\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 6s - loss: 9.4407 - mse: 9.4407 - mae: 1.3116 - val_loss: 9.2506 - val_mse: 9.2506 - val_mae: 1.3395 - lr: 3.6677e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 6s - loss: 9.2372 - mse: 9.2372 - mae: 1.2898 - val_loss: 6.9525 - val_mse: 6.9525 - val_mae: 1.2177 - lr: 3.6677e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 6s - loss: 8.6831 - mse: 8.6831 - mae: 1.2721 - val_loss: 7.2582 - val_mse: 7.2582 - val_mae: 1.2911 - lr: 3.6677e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 6s - loss: 8.7738 - mse: 8.7738 - mae: 1.2688 - val_loss: 8.0099 - val_mse: 8.0099 - val_mae: 1.3312 - lr: 3.6677e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 6s - loss: 8.3972 - mse: 8.3972 - mae: 1.2482 - val_loss: 7.5534 - val_mse: 7.5534 - val_mae: 1.3273 - lr: 3.6677e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 6s - loss: 8.2106 - mse: 8.2106 - mae: 1.2333 - val_loss: 8.0993 - val_mse: 8.0993 - val_mae: 1.3493 - lr: 3.6677e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 6s - loss: 8.0720 - mse: 8.0720 - mae: 1.2253 - val_loss: 8.4329 - val_mse: 8.4329 - val_mae: 1.4110 - lr: 3.6677e-04 - 6s/epoch - 6ms/step\n",
            "Score for fold 4: loss of 8.432889938354492\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 6s - loss: 8.7559 - mse: 8.7559 - mae: 1.2460 - val_loss: 4.9687 - val_mse: 4.9687 - val_mae: 1.1795 - lr: 3.6677e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 6s - loss: 8.1076 - mse: 8.1076 - mae: 1.2279 - val_loss: 5.8246 - val_mse: 5.8246 - val_mae: 1.3012 - lr: 3.6677e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 6s - loss: 7.8095 - mse: 7.8095 - mae: 1.2123 - val_loss: 5.7399 - val_mse: 5.7399 - val_mae: 1.2113 - lr: 3.6677e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 6s - loss: 7.8495 - mse: 7.8495 - mae: 1.1995 - val_loss: 6.4350 - val_mse: 6.4350 - val_mae: 1.2160 - lr: 3.6677e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 6s - loss: 7.1839 - mse: 7.1839 - mae: 1.1819 - val_loss: 5.6905 - val_mse: 5.6905 - val_mae: 1.2156 - lr: 3.6677e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 6s - loss: 7.1705 - mse: 7.1705 - mae: 1.1806 - val_loss: 6.6106 - val_mse: 6.6106 - val_mae: 1.3565 - lr: 3.6677e-04 - 6s/epoch - 6ms/step\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-10-12 03:32:43,967]\u001b[0m Finished trial#13 resulted in value: 10.293999999999999. Current best value is 10.022 with parameters: {'activation': 'relu', 'num_hidden_layer': 4, 'i': 9, 'learning_rate': 0.0011355512574150934}.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Score for fold 5: loss of 6.6105546951293945\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 5s - loss: 14.1133 - mse: 14.1133 - mae: 1.5483 - val_loss: 9.7154 - val_mse: 9.7154 - val_mae: 1.4371 - lr: 2.9966e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 4s - loss: 13.0947 - mse: 13.0947 - mae: 1.4930 - val_loss: 9.3833 - val_mse: 9.3833 - val_mae: 1.4596 - lr: 2.9966e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 4s - loss: 12.8020 - mse: 12.8020 - mae: 1.4731 - val_loss: 9.2166 - val_mse: 9.2166 - val_mae: 1.4546 - lr: 2.9966e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 4s - loss: 12.8005 - mse: 12.8005 - mae: 1.4655 - val_loss: 9.2541 - val_mse: 9.2541 - val_mae: 1.4831 - lr: 2.9966e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 4s - loss: 12.6988 - mse: 12.6988 - mae: 1.4588 - val_loss: 9.1018 - val_mse: 9.1018 - val_mae: 1.4243 - lr: 2.9966e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 4s - loss: 12.5622 - mse: 12.5622 - mae: 1.4530 - val_loss: 9.0777 - val_mse: 9.0777 - val_mae: 1.4214 - lr: 2.9966e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 4s - loss: 12.5879 - mse: 12.5879 - mae: 1.4467 - val_loss: 9.1292 - val_mse: 9.1292 - val_mae: 1.4644 - lr: 2.9966e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 4s - loss: 12.5188 - mse: 12.5188 - mae: 1.4384 - val_loss: 9.1257 - val_mse: 9.1257 - val_mae: 1.4327 - lr: 2.9966e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 4s - loss: 12.3125 - mse: 12.3125 - mae: 1.4345 - val_loss: 9.1563 - val_mse: 9.1563 - val_mae: 1.5082 - lr: 2.9966e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 4s - loss: 12.1252 - mse: 12.1252 - mae: 1.4286 - val_loss: 8.9680 - val_mse: 8.9680 - val_mae: 1.3967 - lr: 2.9966e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 4s - loss: 12.2893 - mse: 12.2893 - mae: 1.4248 - val_loss: 9.0669 - val_mse: 9.0669 - val_mae: 1.4404 - lr: 2.9966e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 4s - loss: 12.1084 - mse: 12.1084 - mae: 1.4198 - val_loss: 8.9748 - val_mse: 8.9748 - val_mae: 1.4467 - lr: 2.9966e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 13/100\n",
            "1000/1000 - 4s - loss: 11.9465 - mse: 11.9465 - mae: 1.4119 - val_loss: 9.2600 - val_mse: 9.2600 - val_mae: 1.4277 - lr: 2.9966e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 14/100\n",
            "1000/1000 - 4s - loss: 11.8176 - mse: 11.8176 - mae: 1.4049 - val_loss: 9.1353 - val_mse: 9.1353 - val_mae: 1.4481 - lr: 2.9966e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 15/100\n",
            "1000/1000 - 4s - loss: 11.8572 - mse: 11.8572 - mae: 1.3988 - val_loss: 9.1047 - val_mse: 9.1047 - val_mae: 1.4616 - lr: 2.9966e-04 - 4s/epoch - 4ms/step\n",
            "Score for fold 1: loss of 9.104653358459473\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 4s - loss: 11.9924 - mse: 11.9924 - mae: 1.4104 - val_loss: 8.1770 - val_mse: 8.1770 - val_mae: 1.3610 - lr: 2.9966e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 4s - loss: 11.7822 - mse: 11.7822 - mae: 1.4035 - val_loss: 8.0862 - val_mse: 8.0862 - val_mae: 1.4387 - lr: 2.9966e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 4s - loss: 11.7408 - mse: 11.7408 - mae: 1.3968 - val_loss: 8.1596 - val_mse: 8.1596 - val_mae: 1.4094 - lr: 2.9966e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 4s - loss: 11.3812 - mse: 11.3812 - mae: 1.3846 - val_loss: 8.2748 - val_mse: 8.2748 - val_mae: 1.3623 - lr: 2.9966e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 4s - loss: 11.5934 - mse: 11.5934 - mae: 1.3812 - val_loss: 8.1022 - val_mse: 8.1022 - val_mae: 1.3662 - lr: 2.9966e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 4s - loss: 11.5797 - mse: 11.5797 - mae: 1.3752 - val_loss: 8.1679 - val_mse: 8.1679 - val_mae: 1.4414 - lr: 2.9966e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 4s - loss: 11.1053 - mse: 11.1053 - mae: 1.3659 - val_loss: 8.6861 - val_mse: 8.6861 - val_mae: 1.3773 - lr: 2.9966e-04 - 4s/epoch - 4ms/step\n",
            "Score for fold 2: loss of 8.68612003326416\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 4s - loss: 10.8759 - mse: 10.8759 - mae: 1.3706 - val_loss: 9.0475 - val_mse: 9.0475 - val_mae: 1.3876 - lr: 2.9966e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 4s - loss: 10.5671 - mse: 10.5671 - mae: 1.3618 - val_loss: 9.1576 - val_mse: 9.1576 - val_mae: 1.3477 - lr: 2.9966e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 4s - loss: 10.5029 - mse: 10.5029 - mae: 1.3493 - val_loss: 10.3303 - val_mse: 10.3303 - val_mae: 1.3890 - lr: 2.9966e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 4s - loss: 10.4980 - mse: 10.4980 - mae: 1.3471 - val_loss: 9.7985 - val_mse: 9.7985 - val_mae: 1.3651 - lr: 2.9966e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 4s - loss: 10.2700 - mse: 10.2700 - mae: 1.3327 - val_loss: 9.3923 - val_mse: 9.3923 - val_mae: 1.4043 - lr: 2.9966e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 4s - loss: 10.0313 - mse: 10.0313 - mae: 1.3279 - val_loss: 9.3698 - val_mse: 9.3698 - val_mae: 1.3736 - lr: 2.9966e-04 - 4s/epoch - 4ms/step\n",
            "Score for fold 3: loss of 9.369791030883789\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 4s - loss: 8.5028 - mse: 8.5028 - mae: 1.3397 - val_loss: 16.1450 - val_mse: 16.1450 - val_mae: 1.2838 - lr: 2.9966e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 4s - loss: 8.2915 - mse: 8.2915 - mae: 1.3266 - val_loss: 16.1477 - val_mse: 16.1477 - val_mae: 1.3513 - lr: 2.9966e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 4s - loss: 8.0591 - mse: 8.0591 - mae: 1.3119 - val_loss: 15.9600 - val_mse: 15.9600 - val_mae: 1.3473 - lr: 2.9966e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 4s - loss: 8.1432 - mse: 8.1432 - mae: 1.3053 - val_loss: 16.0698 - val_mse: 16.0698 - val_mae: 1.3104 - lr: 2.9966e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 4s - loss: 7.9782 - mse: 7.9782 - mae: 1.2941 - val_loss: 16.3065 - val_mse: 16.3065 - val_mae: 1.3237 - lr: 2.9966e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 4s - loss: 7.8840 - mse: 7.8840 - mae: 1.2856 - val_loss: 16.4058 - val_mse: 16.4058 - val_mae: 1.3210 - lr: 2.9966e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 4s - loss: 7.7772 - mse: 7.7772 - mae: 1.2770 - val_loss: 16.4837 - val_mse: 16.4837 - val_mae: 1.3521 - lr: 2.9966e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 4s - loss: 7.4897 - mse: 7.4897 - mae: 1.2656 - val_loss: 16.3678 - val_mse: 16.3678 - val_mae: 1.3795 - lr: 2.9966e-04 - 4s/epoch - 4ms/step\n",
            "Score for fold 4: loss of 16.367843627929688\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 4s - loss: 9.1433 - mse: 9.1433 - mae: 1.2907 - val_loss: 10.0061 - val_mse: 10.0061 - val_mae: 1.2763 - lr: 2.9966e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 4s - loss: 9.0361 - mse: 9.0361 - mae: 1.2786 - val_loss: 9.2886 - val_mse: 9.2886 - val_mae: 1.2448 - lr: 2.9966e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 4s - loss: 8.9110 - mse: 8.9110 - mae: 1.2674 - val_loss: 9.4800 - val_mse: 9.4800 - val_mae: 1.2540 - lr: 2.9966e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 4s - loss: 8.6535 - mse: 8.6535 - mae: 1.2559 - val_loss: 9.5296 - val_mse: 9.5296 - val_mae: 1.3095 - lr: 2.9966e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 4s - loss: 8.7181 - mse: 8.7181 - mae: 1.2480 - val_loss: 10.3164 - val_mse: 10.3164 - val_mae: 1.2970 - lr: 2.9966e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 4s - loss: 8.4460 - mse: 8.4460 - mae: 1.2357 - val_loss: 10.5146 - val_mse: 10.5146 - val_mae: 1.3171 - lr: 2.9966e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 4s - loss: 8.3489 - mse: 8.3489 - mae: 1.2266 - val_loss: 9.7197 - val_mse: 9.7197 - val_mae: 1.3004 - lr: 2.9966e-04 - 4s/epoch - 4ms/step\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-10-12 03:35:57,513]\u001b[0m Finished trial#14 resulted in value: 10.65. Current best value is 10.022 with parameters: {'activation': 'relu', 'num_hidden_layer': 4, 'i': 9, 'learning_rate': 0.0011355512574150934}.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Score for fold 5: loss of 9.719738006591797\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 7s - loss: 15.1717 - mse: 15.1717 - mae: 1.6486 - val_loss: 10.2070 - val_mse: 10.2070 - val_mae: 1.5217 - lr: 8.5251e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 6s - loss: 14.2688 - mse: 14.2688 - mae: 1.5720 - val_loss: 10.3730 - val_mse: 10.3730 - val_mae: 1.6442 - lr: 8.5251e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 6s - loss: 14.4040 - mse: 14.4040 - mae: 1.5751 - val_loss: 10.2637 - val_mse: 10.2637 - val_mae: 1.4786 - lr: 8.5251e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 6s - loss: 14.1245 - mse: 14.1245 - mae: 1.5684 - val_loss: 10.3270 - val_mse: 10.3270 - val_mae: 1.6343 - lr: 8.5251e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 6s - loss: 14.4458 - mse: 14.4458 - mae: 1.5749 - val_loss: 9.9947 - val_mse: 9.9947 - val_mae: 1.5473 - lr: 8.5251e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 6s - loss: 14.2533 - mse: 14.2533 - mae: 1.5665 - val_loss: 10.8202 - val_mse: 10.8202 - val_mae: 1.6119 - lr: 8.5251e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 6s - loss: 14.2513 - mse: 14.2513 - mae: 1.5626 - val_loss: 10.1284 - val_mse: 10.1284 - val_mae: 1.6445 - lr: 8.5251e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 6s - loss: 14.1005 - mse: 14.1005 - mae: 1.5639 - val_loss: 10.8227 - val_mse: 10.8227 - val_mae: 1.6723 - lr: 8.5251e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 6s - loss: 14.2306 - mse: 14.2306 - mae: 1.5623 - val_loss: 10.0849 - val_mse: 10.0849 - val_mae: 1.4922 - lr: 8.5251e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 6s - loss: 14.1768 - mse: 14.1768 - mae: 1.5678 - val_loss: 9.9970 - val_mse: 9.9970 - val_mae: 1.4797 - lr: 8.5251e-04 - 6s/epoch - 6ms/step\n",
            "Score for fold 1: loss of 9.996983528137207\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 6s - loss: 13.8636 - mse: 13.8636 - mae: 1.5654 - val_loss: 11.6217 - val_mse: 11.6217 - val_mae: 1.4580 - lr: 8.5251e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 6s - loss: 13.7993 - mse: 13.7993 - mae: 1.5699 - val_loss: 12.5155 - val_mse: 12.5155 - val_mae: 1.7024 - lr: 8.5251e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 6s - loss: 13.8154 - mse: 13.8154 - mae: 1.5701 - val_loss: 11.7197 - val_mse: 11.7197 - val_mae: 1.4661 - lr: 8.5251e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 6s - loss: 13.8178 - mse: 13.8178 - mae: 1.5694 - val_loss: 11.8892 - val_mse: 11.8892 - val_mae: 1.6508 - lr: 8.5251e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 6s - loss: 13.6707 - mse: 13.6707 - mae: 1.5600 - val_loss: 11.5013 - val_mse: 11.5013 - val_mae: 1.4946 - lr: 8.5251e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 6s - loss: 13.7969 - mse: 13.7969 - mae: 1.5724 - val_loss: 12.1946 - val_mse: 12.1946 - val_mae: 1.6589 - lr: 8.5251e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 6s - loss: 13.8068 - mse: 13.8068 - mae: 1.5678 - val_loss: 11.4908 - val_mse: 11.4908 - val_mae: 1.5678 - lr: 8.5251e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 6s - loss: 13.8390 - mse: 13.8390 - mae: 1.5614 - val_loss: 11.4642 - val_mse: 11.4642 - val_mae: 1.5104 - lr: 8.5251e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 6s - loss: 13.9558 - mse: 13.9558 - mae: 1.5706 - val_loss: 11.6812 - val_mse: 11.6812 - val_mae: 1.4898 - lr: 8.5251e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 6s - loss: 13.7775 - mse: 13.7775 - mae: 1.5647 - val_loss: 11.6684 - val_mse: 11.6684 - val_mae: 1.5402 - lr: 8.5251e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 6s - loss: 13.7139 - mse: 13.7139 - mae: 1.5674 - val_loss: 11.5813 - val_mse: 11.5813 - val_mae: 1.5290 - lr: 8.5251e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 6s - loss: 13.7818 - mse: 13.7818 - mae: 1.5646 - val_loss: 11.5783 - val_mse: 11.5783 - val_mae: 1.5046 - lr: 8.5251e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 13/100\n",
            "1000/1000 - 6s - loss: 13.7742 - mse: 13.7742 - mae: 1.5668 - val_loss: 11.4947 - val_mse: 11.4947 - val_mae: 1.4781 - lr: 8.5251e-04 - 6s/epoch - 6ms/step\n",
            "Score for fold 2: loss of 11.49473762512207\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 6s - loss: 14.2599 - mse: 14.2599 - mae: 1.5529 - val_loss: 10.1238 - val_mse: 10.1238 - val_mae: 1.6820 - lr: 8.5251e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 6s - loss: 14.3260 - mse: 14.3260 - mae: 1.5611 - val_loss: 9.9728 - val_mse: 9.9728 - val_mae: 1.5959 - lr: 8.5251e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 6s - loss: 14.1543 - mse: 14.1543 - mae: 1.5576 - val_loss: 10.0077 - val_mse: 10.0077 - val_mae: 1.5425 - lr: 8.5251e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 6s - loss: 14.1536 - mse: 14.1536 - mae: 1.5615 - val_loss: 10.0821 - val_mse: 10.0821 - val_mae: 1.5101 - lr: 8.5251e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 6s - loss: 14.2126 - mse: 14.2126 - mae: 1.5559 - val_loss: 10.0100 - val_mse: 10.0100 - val_mae: 1.5822 - lr: 8.5251e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 6s - loss: 14.2309 - mse: 14.2309 - mae: 1.5569 - val_loss: 10.1092 - val_mse: 10.1092 - val_mae: 1.4959 - lr: 8.5251e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 6s - loss: 14.2813 - mse: 14.2813 - mae: 1.5612 - val_loss: 9.8991 - val_mse: 9.8991 - val_mae: 1.5592 - lr: 8.5251e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 6s - loss: 14.2365 - mse: 14.2365 - mae: 1.5577 - val_loss: 9.9745 - val_mse: 9.9745 - val_mae: 1.5515 - lr: 8.5251e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 6s - loss: 14.2092 - mse: 14.2092 - mae: 1.5592 - val_loss: 10.1416 - val_mse: 10.1416 - val_mae: 1.6463 - lr: 8.5251e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 6s - loss: 14.2121 - mse: 14.2121 - mae: 1.5630 - val_loss: 9.9286 - val_mse: 9.9286 - val_mae: 1.6289 - lr: 8.5251e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 6s - loss: 14.1893 - mse: 14.1893 - mae: 1.5548 - val_loss: 10.0197 - val_mse: 10.0197 - val_mae: 1.6389 - lr: 8.5251e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 6s - loss: 14.2573 - mse: 14.2573 - mae: 1.5639 - val_loss: 10.2139 - val_mse: 10.2139 - val_mae: 1.4833 - lr: 8.5251e-04 - 6s/epoch - 6ms/step\n",
            "Score for fold 3: loss of 10.213896751403809\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 6s - loss: 11.2821 - mse: 11.2821 - mae: 1.5492 - val_loss: 21.9539 - val_mse: 21.9539 - val_mae: 1.5361 - lr: 8.5251e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 6s - loss: 11.1574 - mse: 11.1574 - mae: 1.5448 - val_loss: 21.9709 - val_mse: 21.9709 - val_mae: 1.6382 - lr: 8.5251e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 6s - loss: 11.1900 - mse: 11.1900 - mae: 1.5486 - val_loss: 21.7529 - val_mse: 21.7529 - val_mae: 1.6133 - lr: 8.5251e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 6s - loss: 11.3494 - mse: 11.3494 - mae: 1.5465 - val_loss: 21.8031 - val_mse: 21.8031 - val_mae: 1.6181 - lr: 8.5251e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 6s - loss: 11.1259 - mse: 11.1259 - mae: 1.5468 - val_loss: 21.8963 - val_mse: 21.8963 - val_mae: 1.6773 - lr: 8.5251e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 6s - loss: 11.3053 - mse: 11.3053 - mae: 1.5514 - val_loss: 21.8203 - val_mse: 21.8203 - val_mae: 1.6205 - lr: 8.5251e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 6s - loss: 11.3346 - mse: 11.3346 - mae: 1.5472 - val_loss: 22.2339 - val_mse: 22.2339 - val_mae: 1.5053 - lr: 8.5251e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 6s - loss: 11.3293 - mse: 11.3293 - mae: 1.5528 - val_loss: 21.8884 - val_mse: 21.8884 - val_mae: 1.5655 - lr: 8.5251e-04 - 6s/epoch - 6ms/step\n",
            "Score for fold 4: loss of 21.888442993164062\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 6s - loss: 13.3273 - mse: 13.3273 - mae: 1.5545 - val_loss: 12.7719 - val_mse: 12.7719 - val_mae: 1.5170 - lr: 8.5251e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 6s - loss: 13.3486 - mse: 13.3486 - mae: 1.5582 - val_loss: 12.8526 - val_mse: 12.8526 - val_mae: 1.5110 - lr: 8.5251e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 6s - loss: 13.4842 - mse: 13.4842 - mae: 1.5598 - val_loss: 12.7535 - val_mse: 12.7535 - val_mae: 1.5503 - lr: 8.5251e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 6s - loss: 13.6551 - mse: 13.6551 - mae: 1.5634 - val_loss: 12.8054 - val_mse: 12.8054 - val_mae: 1.5050 - lr: 8.5251e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 6s - loss: 13.6016 - mse: 13.6016 - mae: 1.5579 - val_loss: 12.7692 - val_mse: 12.7692 - val_mae: 1.5064 - lr: 8.5251e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 6s - loss: 13.5016 - mse: 13.5016 - mae: 1.5690 - val_loss: 13.1847 - val_mse: 13.1847 - val_mae: 1.6619 - lr: 8.5251e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 6s - loss: 13.5983 - mse: 13.5983 - mae: 1.5637 - val_loss: 12.8518 - val_mse: 12.8518 - val_mae: 1.6032 - lr: 8.5251e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 6s - loss: 13.4957 - mse: 13.4957 - mae: 1.5650 - val_loss: 12.7996 - val_mse: 12.7996 - val_mae: 1.5326 - lr: 8.5251e-04 - 6s/epoch - 6ms/step\n",
            "Score for fold 5: loss of 12.799551010131836\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-10-12 03:41:01,696]\u001b[0m Finished trial#15 resulted in value: 13.278. Current best value is 10.022 with parameters: {'activation': 'relu', 'num_hidden_layer': 4, 'i': 9, 'learning_rate': 0.0011355512574150934}.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 8s - loss: 14.1186 - mse: 14.1186 - mae: 1.5601 - val_loss: 9.6267 - val_mse: 9.6267 - val_mae: 1.4148 - lr: 2.7354e-04 - 8s/epoch - 8ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 7s - loss: 12.9684 - mse: 12.9684 - mae: 1.4967 - val_loss: 9.5777 - val_mse: 9.5777 - val_mae: 1.4787 - lr: 2.7354e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 7s - loss: 12.7170 - mse: 12.7170 - mae: 1.4771 - val_loss: 9.4339 - val_mse: 9.4339 - val_mae: 1.4647 - lr: 2.7354e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 7s - loss: 12.6678 - mse: 12.6678 - mae: 1.4690 - val_loss: 9.4627 - val_mse: 9.4627 - val_mae: 1.3933 - lr: 2.7354e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 7s - loss: 12.5296 - mse: 12.5296 - mae: 1.4586 - val_loss: 9.4497 - val_mse: 9.4497 - val_mae: 1.4109 - lr: 2.7354e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 7s - loss: 12.3625 - mse: 12.3625 - mae: 1.4505 - val_loss: 9.4172 - val_mse: 9.4172 - val_mae: 1.5097 - lr: 2.7354e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 7s - loss: 12.2637 - mse: 12.2637 - mae: 1.4398 - val_loss: 9.3496 - val_mse: 9.3496 - val_mae: 1.4070 - lr: 2.7354e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 7s - loss: 12.2596 - mse: 12.2596 - mae: 1.4366 - val_loss: 9.4333 - val_mse: 9.4333 - val_mae: 1.4283 - lr: 2.7354e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 7s - loss: 11.9970 - mse: 11.9970 - mae: 1.4340 - val_loss: 9.4242 - val_mse: 9.4242 - val_mae: 1.3912 - lr: 2.7354e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 7s - loss: 12.0564 - mse: 12.0564 - mae: 1.4235 - val_loss: 9.3126 - val_mse: 9.3126 - val_mae: 1.4537 - lr: 2.7354e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 7s - loss: 11.6556 - mse: 11.6556 - mae: 1.4145 - val_loss: 9.3867 - val_mse: 9.3867 - val_mae: 1.3975 - lr: 2.7354e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 7s - loss: 11.6063 - mse: 11.6063 - mae: 1.4058 - val_loss: 9.5684 - val_mse: 9.5684 - val_mae: 1.3835 - lr: 2.7354e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 13/100\n",
            "1000/1000 - 7s - loss: 11.5049 - mse: 11.5049 - mae: 1.4009 - val_loss: 9.4931 - val_mse: 9.4931 - val_mae: 1.4477 - lr: 2.7354e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 14/100\n",
            "1000/1000 - 7s - loss: 11.3379 - mse: 11.3379 - mae: 1.3942 - val_loss: 9.4622 - val_mse: 9.4622 - val_mae: 1.4240 - lr: 2.7354e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 15/100\n",
            "1000/1000 - 7s - loss: 11.1562 - mse: 11.1562 - mae: 1.3812 - val_loss: 9.3944 - val_mse: 9.3944 - val_mae: 1.4324 - lr: 2.7354e-04 - 7s/epoch - 7ms/step\n",
            "Score for fold 1: loss of 9.394414901733398\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 7s - loss: 10.0145 - mse: 10.0145 - mae: 1.3863 - val_loss: 14.7306 - val_mse: 14.7306 - val_mae: 1.3786 - lr: 2.7354e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 7s - loss: 9.6699 - mse: 9.6699 - mae: 1.3709 - val_loss: 15.5597 - val_mse: 15.5597 - val_mae: 1.4566 - lr: 2.7354e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 7s - loss: 9.5935 - mse: 9.5935 - mae: 1.3682 - val_loss: 15.2845 - val_mse: 15.2845 - val_mae: 1.3714 - lr: 2.7354e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 7s - loss: 9.2445 - mse: 9.2445 - mae: 1.3556 - val_loss: 15.2415 - val_mse: 15.2415 - val_mae: 1.4076 - lr: 2.7354e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 7s - loss: 9.0668 - mse: 9.0668 - mae: 1.3392 - val_loss: 15.3467 - val_mse: 15.3467 - val_mae: 1.4020 - lr: 2.7354e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 7s - loss: 8.8982 - mse: 8.8982 - mae: 1.3309 - val_loss: 15.0694 - val_mse: 15.0694 - val_mae: 1.3969 - lr: 2.7354e-04 - 7s/epoch - 7ms/step\n",
            "Score for fold 2: loss of 15.069376945495605\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 7s - loss: 10.8139 - mse: 10.8139 - mae: 1.3532 - val_loss: 8.4036 - val_mse: 8.4036 - val_mae: 1.3302 - lr: 2.7354e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 7s - loss: 10.6993 - mse: 10.6993 - mae: 1.3424 - val_loss: 8.5072 - val_mse: 8.5072 - val_mae: 1.3299 - lr: 2.7354e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 7s - loss: 10.1524 - mse: 10.1524 - mae: 1.3239 - val_loss: 8.9323 - val_mse: 8.9323 - val_mae: 1.3762 - lr: 2.7354e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 7s - loss: 9.9916 - mse: 9.9916 - mae: 1.3118 - val_loss: 8.9063 - val_mse: 8.9063 - val_mae: 1.3677 - lr: 2.7354e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 7s - loss: 9.5975 - mse: 9.5975 - mae: 1.2979 - val_loss: 8.8329 - val_mse: 8.8329 - val_mae: 1.3124 - lr: 2.7354e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 7s - loss: 9.9214 - mse: 9.9214 - mae: 1.2894 - val_loss: 8.8407 - val_mse: 8.8407 - val_mae: 1.3446 - lr: 2.7354e-04 - 7s/epoch - 7ms/step\n",
            "Score for fold 3: loss of 8.840755462646484\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 7s - loss: 9.0177 - mse: 9.0177 - mae: 1.2934 - val_loss: 11.4839 - val_mse: 11.4839 - val_mae: 1.2881 - lr: 2.7354e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 7s - loss: 8.7629 - mse: 8.7629 - mae: 1.2757 - val_loss: 11.4009 - val_mse: 11.4009 - val_mae: 1.3121 - lr: 2.7354e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 7s - loss: 8.3971 - mse: 8.3971 - mae: 1.2614 - val_loss: 11.3008 - val_mse: 11.3008 - val_mae: 1.3440 - lr: 2.7354e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 7s - loss: 8.1917 - mse: 8.1917 - mae: 1.2421 - val_loss: 11.5898 - val_mse: 11.5898 - val_mae: 1.3090 - lr: 2.7354e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 7s - loss: 8.1087 - mse: 8.1087 - mae: 1.2284 - val_loss: 12.0158 - val_mse: 12.0158 - val_mae: 1.2964 - lr: 2.7354e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 7s - loss: 7.8515 - mse: 7.8515 - mae: 1.2127 - val_loss: 12.3459 - val_mse: 12.3459 - val_mae: 1.3808 - lr: 2.7354e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 7s - loss: 7.8288 - mse: 7.8288 - mae: 1.1988 - val_loss: 11.8201 - val_mse: 11.8201 - val_mae: 1.3288 - lr: 2.7354e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 7s - loss: 7.5175 - mse: 7.5175 - mae: 1.1869 - val_loss: 11.8522 - val_mse: 11.8522 - val_mae: 1.3978 - lr: 2.7354e-04 - 7s/epoch - 7ms/step\n",
            "Score for fold 4: loss of 11.852160453796387\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 7s - loss: 9.2060 - mse: 9.2060 - mae: 1.2276 - val_loss: 5.0188 - val_mse: 5.0188 - val_mae: 1.1660 - lr: 2.7354e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 7s - loss: 8.9625 - mse: 8.9625 - mae: 1.2070 - val_loss: 4.8643 - val_mse: 4.8643 - val_mae: 1.2151 - lr: 2.7354e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 7s - loss: 8.7940 - mse: 8.7940 - mae: 1.1902 - val_loss: 5.3022 - val_mse: 5.3022 - val_mae: 1.1820 - lr: 2.7354e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 7s - loss: 8.6147 - mse: 8.6147 - mae: 1.1707 - val_loss: 5.8635 - val_mse: 5.8635 - val_mae: 1.1708 - lr: 2.7354e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 7s - loss: 8.3046 - mse: 8.3046 - mae: 1.1556 - val_loss: 5.6090 - val_mse: 5.6090 - val_mae: 1.2324 - lr: 2.7354e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 7s - loss: 8.2115 - mse: 8.2115 - mae: 1.1392 - val_loss: 5.5362 - val_mse: 5.5362 - val_mae: 1.2359 - lr: 2.7354e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 7s - loss: 8.1353 - mse: 8.1353 - mae: 1.1247 - val_loss: 6.4453 - val_mse: 6.4453 - val_mae: 1.2400 - lr: 2.7354e-04 - 7s/epoch - 7ms/step\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-10-12 03:46:04,485]\u001b[0m Finished trial#16 resulted in value: 10.32. Current best value is 10.022 with parameters: {'activation': 'relu', 'num_hidden_layer': 4, 'i': 9, 'learning_rate': 0.0011355512574150934}.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Score for fold 5: loss of 6.445250034332275\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 7s - loss: 14.0064 - mse: 14.0064 - mae: 1.5803 - val_loss: 10.2023 - val_mse: 10.2023 - val_mae: 1.4579 - lr: 0.0033 - 7s/epoch - 7ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 6s - loss: 13.4877 - mse: 13.4877 - mae: 1.5305 - val_loss: 10.8268 - val_mse: 10.8268 - val_mae: 1.4695 - lr: 0.0033 - 6s/epoch - 6ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 6s - loss: 13.3867 - mse: 13.3867 - mae: 1.5248 - val_loss: 9.7067 - val_mse: 9.7067 - val_mae: 1.5594 - lr: 0.0033 - 6s/epoch - 6ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 6s - loss: 13.0975 - mse: 13.0975 - mae: 1.5253 - val_loss: 9.5326 - val_mse: 9.5326 - val_mae: 1.4844 - lr: 0.0033 - 6s/epoch - 6ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 6s - loss: 13.1297 - mse: 13.1297 - mae: 1.5101 - val_loss: 9.6265 - val_mse: 9.6265 - val_mae: 1.6113 - lr: 0.0033 - 6s/epoch - 6ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 6s - loss: 13.0711 - mse: 13.0711 - mae: 1.5226 - val_loss: 10.0967 - val_mse: 10.0967 - val_mae: 1.5594 - lr: 0.0033 - 6s/epoch - 6ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 6s - loss: 13.2136 - mse: 13.2136 - mae: 1.5199 - val_loss: 9.8075 - val_mse: 9.8075 - val_mae: 1.5246 - lr: 0.0033 - 6s/epoch - 6ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 6s - loss: 13.3587 - mse: 13.3587 - mae: 1.5321 - val_loss: 9.8988 - val_mse: 9.8988 - val_mae: 1.5095 - lr: 0.0033 - 6s/epoch - 6ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 6s - loss: 13.2282 - mse: 13.2282 - mae: 1.5358 - val_loss: 10.1324 - val_mse: 10.1324 - val_mae: 1.6289 - lr: 0.0033 - 6s/epoch - 6ms/step\n",
            "Score for fold 1: loss of 10.132423400878906\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 6s - loss: 9.4314 - mse: 9.4314 - mae: 1.5094 - val_loss: 22.2818 - val_mse: 22.2818 - val_mae: 1.4703 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 6s - loss: 9.1060 - mse: 9.1060 - mae: 1.4651 - val_loss: 22.0462 - val_mse: 22.0462 - val_mae: 1.4641 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 6s - loss: 9.0424 - mse: 9.0424 - mae: 1.4554 - val_loss: 22.2270 - val_mse: 22.2270 - val_mae: 1.4645 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 6s - loss: 8.8481 - mse: 8.8481 - mae: 1.4489 - val_loss: 22.2072 - val_mse: 22.2072 - val_mae: 1.4905 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 6s - loss: 8.7215 - mse: 8.7215 - mae: 1.4453 - val_loss: 21.9950 - val_mse: 21.9950 - val_mae: 1.5309 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 6s - loss: 8.7824 - mse: 8.7824 - mae: 1.4423 - val_loss: 22.0468 - val_mse: 22.0468 - val_mae: 1.4878 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 6s - loss: 8.7716 - mse: 8.7716 - mae: 1.4379 - val_loss: 22.2082 - val_mse: 22.2082 - val_mae: 1.5061 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 6s - loss: 8.5817 - mse: 8.5817 - mae: 1.4362 - val_loss: 22.4872 - val_mse: 22.4872 - val_mae: 1.4679 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 6s - loss: 8.4568 - mse: 8.4568 - mae: 1.4232 - val_loss: 23.5880 - val_mse: 23.5880 - val_mae: 1.4902 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 6s - loss: 8.7765 - mse: 8.7765 - mae: 1.4284 - val_loss: 22.6616 - val_mse: 22.6616 - val_mae: 1.5683 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
            "Score for fold 2: loss of 22.661579132080078\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 6s - loss: 11.8317 - mse: 11.8317 - mae: 1.4426 - val_loss: 8.6673 - val_mse: 8.6673 - val_mae: 1.4196 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 6s - loss: 11.7330 - mse: 11.7330 - mae: 1.4405 - val_loss: 8.5473 - val_mse: 8.5473 - val_mae: 1.4310 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 6s - loss: 11.6928 - mse: 11.6928 - mae: 1.4389 - val_loss: 8.8207 - val_mse: 8.8207 - val_mae: 1.4467 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 6s - loss: 11.5460 - mse: 11.5460 - mae: 1.4370 - val_loss: 8.6223 - val_mse: 8.6223 - val_mae: 1.4242 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 6s - loss: 11.4471 - mse: 11.4471 - mae: 1.4333 - val_loss: 8.8554 - val_mse: 8.8554 - val_mae: 1.4590 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 6s - loss: 11.3311 - mse: 11.3311 - mae: 1.4224 - val_loss: 9.2740 - val_mse: 9.2740 - val_mae: 1.4237 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 7s - loss: 11.1698 - mse: 11.1698 - mae: 1.4125 - val_loss: 8.7830 - val_mse: 8.7830 - val_mae: 1.4482 - lr: 0.0010 - 7s/epoch - 7ms/step\n",
            "Score for fold 3: loss of 8.782950401306152\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 6s - loss: 11.4296 - mse: 11.4296 - mae: 1.4303 - val_loss: 8.1860 - val_mse: 8.1860 - val_mae: 1.3661 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 6s - loss: 11.3876 - mse: 11.3876 - mae: 1.4257 - val_loss: 8.0235 - val_mse: 8.0235 - val_mae: 1.3445 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 6s - loss: 11.1942 - mse: 11.1942 - mae: 1.4188 - val_loss: 9.4733 - val_mse: 9.4733 - val_mae: 1.3967 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 6s - loss: 11.2151 - mse: 11.2151 - mae: 1.4117 - val_loss: 8.3859 - val_mse: 8.3859 - val_mae: 1.4074 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 6s - loss: 11.0094 - mse: 11.0094 - mae: 1.4028 - val_loss: 8.5678 - val_mse: 8.5678 - val_mae: 1.4093 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 6s - loss: 10.9505 - mse: 10.9505 - mae: 1.4015 - val_loss: 8.8492 - val_mse: 8.8492 - val_mae: 1.4106 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 6s - loss: 10.7641 - mse: 10.7641 - mae: 1.3967 - val_loss: 8.1251 - val_mse: 8.1251 - val_mae: 1.3941 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
            "Score for fold 4: loss of 8.125072479248047\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 6s - loss: 11.0639 - mse: 11.0639 - mae: 1.3983 - val_loss: 7.2875 - val_mse: 7.2875 - val_mae: 1.3703 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 6s - loss: 11.1731 - mse: 11.1731 - mae: 1.3891 - val_loss: 7.5051 - val_mse: 7.5051 - val_mae: 1.3903 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 6s - loss: 10.9172 - mse: 10.9172 - mae: 1.3865 - val_loss: 7.3731 - val_mse: 7.3731 - val_mae: 1.4149 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 6s - loss: 10.8532 - mse: 10.8532 - mae: 1.3796 - val_loss: 7.8498 - val_mse: 7.8498 - val_mae: 1.3922 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 6s - loss: 10.7520 - mse: 10.7520 - mae: 1.3701 - val_loss: 7.6375 - val_mse: 7.6375 - val_mae: 1.3779 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 6s - loss: 10.5505 - mse: 10.5505 - mae: 1.3650 - val_loss: 7.8280 - val_mse: 7.8280 - val_mae: 1.4216 - lr: 0.0010 - 6s/epoch - 6ms/step\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-10-12 03:50:16,079]\u001b[0m Finished trial#17 resulted in value: 11.506. Current best value is 10.022 with parameters: {'activation': 'relu', 'num_hidden_layer': 4, 'i': 9, 'learning_rate': 0.0011355512574150934}.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Score for fold 5: loss of 7.827991008758545\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 3s - loss: 12.9323 - mse: 12.9323 - mae: 1.5441 - val_loss: 13.7402 - val_mse: 13.7402 - val_mae: 1.5517 - lr: 4.4098e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 12.0839 - mse: 12.0839 - mae: 1.4898 - val_loss: 14.4949 - val_mse: 14.4949 - val_mae: 1.4476 - lr: 4.4098e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 11.8811 - mse: 11.8811 - mae: 1.4664 - val_loss: 14.3821 - val_mse: 14.3821 - val_mae: 1.5220 - lr: 4.4098e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 11.7046 - mse: 11.7046 - mae: 1.4576 - val_loss: 14.6483 - val_mse: 14.6483 - val_mae: 1.5294 - lr: 4.4098e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 3s - loss: 11.5763 - mse: 11.5763 - mae: 1.4535 - val_loss: 13.7827 - val_mse: 13.7827 - val_mae: 1.5622 - lr: 4.4098e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 3s - loss: 11.5318 - mse: 11.5318 - mae: 1.4463 - val_loss: 14.1082 - val_mse: 14.1082 - val_mae: 1.4760 - lr: 4.4098e-04 - 3s/epoch - 3ms/step\n",
            "Score for fold 1: loss of 14.108202934265137\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 3s - loss: 10.9255 - mse: 10.9255 - mae: 1.4531 - val_loss: 15.8945 - val_mse: 15.8945 - val_mae: 1.4639 - lr: 4.4098e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 3s - loss: 10.7664 - mse: 10.7664 - mae: 1.4436 - val_loss: 15.9039 - val_mse: 15.9039 - val_mae: 1.4199 - lr: 4.4098e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 3s - loss: 10.5520 - mse: 10.5520 - mae: 1.4362 - val_loss: 15.9457 - val_mse: 15.9457 - val_mae: 1.4438 - lr: 4.4098e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 3s - loss: 10.6069 - mse: 10.6069 - mae: 1.4310 - val_loss: 15.9677 - val_mse: 15.9677 - val_mae: 1.4349 - lr: 4.4098e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 3s - loss: 10.4278 - mse: 10.4278 - mae: 1.4246 - val_loss: 15.9526 - val_mse: 15.9526 - val_mae: 1.4378 - lr: 4.4098e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 3s - loss: 10.3076 - mse: 10.3076 - mae: 1.4187 - val_loss: 15.8613 - val_mse: 15.8613 - val_mae: 1.4692 - lr: 4.4098e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 3s - loss: 10.2539 - mse: 10.2539 - mae: 1.4167 - val_loss: 15.8851 - val_mse: 15.8851 - val_mae: 1.4472 - lr: 4.4098e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 3s - loss: 10.0009 - mse: 10.0009 - mae: 1.4080 - val_loss: 16.0273 - val_mse: 16.0273 - val_mae: 1.4169 - lr: 4.4098e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 3s - loss: 9.9987 - mse: 9.9987 - mae: 1.4041 - val_loss: 15.9538 - val_mse: 15.9538 - val_mae: 1.4858 - lr: 4.4098e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 3s - loss: 9.9609 - mse: 9.9609 - mae: 1.4025 - val_loss: 15.8364 - val_mse: 15.8364 - val_mae: 1.5177 - lr: 4.4098e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 3s - loss: 9.8639 - mse: 9.8639 - mae: 1.3957 - val_loss: 16.0275 - val_mse: 16.0275 - val_mae: 1.4515 - lr: 4.4098e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 3s - loss: 9.7025 - mse: 9.7025 - mae: 1.3876 - val_loss: 16.0168 - val_mse: 16.0168 - val_mae: 1.4675 - lr: 4.4098e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 13/100\n",
            "1000/1000 - 3s - loss: 9.6263 - mse: 9.6263 - mae: 1.3828 - val_loss: 16.0237 - val_mse: 16.0237 - val_mae: 1.5381 - lr: 4.4098e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 14/100\n",
            "1000/1000 - 3s - loss: 9.3861 - mse: 9.3861 - mae: 1.3768 - val_loss: 16.0611 - val_mse: 16.0611 - val_mae: 1.4425 - lr: 4.4098e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 15/100\n",
            "1000/1000 - 3s - loss: 9.3889 - mse: 9.3889 - mae: 1.3742 - val_loss: 16.3071 - val_mse: 16.3071 - val_mae: 1.4295 - lr: 4.4098e-04 - 3s/epoch - 3ms/step\n",
            "Score for fold 2: loss of 16.307048797607422\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 3s - loss: 11.7626 - mse: 11.7626 - mae: 1.4043 - val_loss: 7.6939 - val_mse: 7.6939 - val_mae: 1.3956 - lr: 4.4098e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 3s - loss: 11.2762 - mse: 11.2762 - mae: 1.3965 - val_loss: 7.9120 - val_mse: 7.9120 - val_mae: 1.3856 - lr: 4.4098e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 3s - loss: 11.2630 - mse: 11.2630 - mae: 1.3865 - val_loss: 7.5643 - val_mse: 7.5643 - val_mae: 1.4138 - lr: 4.4098e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 3s - loss: 11.3285 - mse: 11.3285 - mae: 1.3836 - val_loss: 7.9760 - val_mse: 7.9760 - val_mae: 1.4520 - lr: 4.4098e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 3s - loss: 11.1364 - mse: 11.1364 - mae: 1.3789 - val_loss: 7.8564 - val_mse: 7.8564 - val_mae: 1.3695 - lr: 4.4098e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 3s - loss: 11.0928 - mse: 11.0928 - mae: 1.3742 - val_loss: 7.9915 - val_mse: 7.9915 - val_mae: 1.3842 - lr: 4.4098e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 3s - loss: 10.8919 - mse: 10.8919 - mae: 1.3654 - val_loss: 8.0681 - val_mse: 8.0681 - val_mae: 1.3914 - lr: 4.4098e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 3s - loss: 10.8359 - mse: 10.8359 - mae: 1.3669 - val_loss: 8.1955 - val_mse: 8.1955 - val_mae: 1.4659 - lr: 4.4098e-04 - 3s/epoch - 3ms/step\n",
            "Score for fold 3: loss of 8.195539474487305\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 3s - loss: 9.8876 - mse: 9.8876 - mae: 1.3734 - val_loss: 11.8593 - val_mse: 11.8593 - val_mae: 1.4380 - lr: 4.4098e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 3s - loss: 9.8433 - mse: 9.8433 - mae: 1.3652 - val_loss: 11.7754 - val_mse: 11.7754 - val_mae: 1.4276 - lr: 4.4098e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 3s - loss: 9.7306 - mse: 9.7306 - mae: 1.3611 - val_loss: 12.7018 - val_mse: 12.7018 - val_mae: 1.3654 - lr: 4.4098e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 3s - loss: 9.3474 - mse: 9.3474 - mae: 1.3518 - val_loss: 12.5446 - val_mse: 12.5446 - val_mae: 1.4370 - lr: 4.4098e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 3s - loss: 9.3683 - mse: 9.3683 - mae: 1.3453 - val_loss: 11.7803 - val_mse: 11.7803 - val_mae: 1.4140 - lr: 4.4098e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 3s - loss: 9.2536 - mse: 9.2536 - mae: 1.3407 - val_loss: 12.7393 - val_mse: 12.7393 - val_mae: 1.3981 - lr: 4.4098e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 3s - loss: 9.2536 - mse: 9.2536 - mae: 1.3395 - val_loss: 12.3243 - val_mse: 12.3243 - val_mae: 1.4043 - lr: 4.4098e-04 - 3s/epoch - 3ms/step\n",
            "Score for fold 4: loss of 12.324335098266602\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 3s - loss: 11.0278 - mse: 11.0278 - mae: 1.3681 - val_loss: 5.7493 - val_mse: 5.7493 - val_mae: 1.2981 - lr: 4.4098e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 3s - loss: 10.6366 - mse: 10.6366 - mae: 1.3546 - val_loss: 6.8470 - val_mse: 6.8470 - val_mae: 1.3337 - lr: 4.4098e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 3s - loss: 10.9034 - mse: 10.9034 - mae: 1.3495 - val_loss: 6.3465 - val_mse: 6.3465 - val_mae: 1.3740 - lr: 4.4098e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 3s - loss: 10.4069 - mse: 10.4069 - mae: 1.3383 - val_loss: 5.9176 - val_mse: 5.9176 - val_mae: 1.2863 - lr: 4.4098e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 3s - loss: 10.2053 - mse: 10.2053 - mae: 1.3317 - val_loss: 5.9951 - val_mse: 5.9951 - val_mae: 1.3806 - lr: 4.4098e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 3s - loss: 10.1448 - mse: 10.1448 - mae: 1.3297 - val_loss: 6.1599 - val_mse: 6.1599 - val_mae: 1.3613 - lr: 4.4098e-04 - 3s/epoch - 3ms/step\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-10-12 03:52:17,228]\u001b[0m Finished trial#18 resulted in value: 11.419999999999998. Current best value is 10.022 with parameters: {'activation': 'relu', 'num_hidden_layer': 4, 'i': 9, 'learning_rate': 0.0011355512574150934}.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Score for fold 5: loss of 6.159909248352051\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 9s - loss: 13.0655 - mse: 13.0655 - mae: 1.6636 - val_loss: 19.8529 - val_mse: 19.8529 - val_mae: 1.6016 - lr: 0.0011 - 9s/epoch - 9ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 9s - loss: 12.0597 - mse: 12.0597 - mae: 1.5831 - val_loss: 19.4340 - val_mse: 19.4340 - val_mae: 1.4950 - lr: 0.0011 - 9s/epoch - 9ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 9s - loss: 12.0766 - mse: 12.0766 - mae: 1.5783 - val_loss: 19.4040 - val_mse: 19.4040 - val_mae: 1.5495 - lr: 0.0011 - 9s/epoch - 9ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 9s - loss: 12.0129 - mse: 12.0129 - mae: 1.5820 - val_loss: 19.8691 - val_mse: 19.8691 - val_mae: 1.5982 - lr: 0.0011 - 9s/epoch - 9ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 8s - loss: 12.2655 - mse: 12.2655 - mae: 1.5782 - val_loss: 19.3895 - val_mse: 19.3895 - val_mae: 1.5806 - lr: 0.0011 - 8s/epoch - 8ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 8s - loss: 12.0456 - mse: 12.0456 - mae: 1.5738 - val_loss: 20.3271 - val_mse: 20.3271 - val_mae: 1.6088 - lr: 0.0011 - 8s/epoch - 8ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 9s - loss: 12.0742 - mse: 12.0742 - mae: 1.5724 - val_loss: 19.5020 - val_mse: 19.5020 - val_mae: 1.4889 - lr: 0.0011 - 9s/epoch - 9ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 8s - loss: 11.8830 - mse: 11.8830 - mae: 1.5693 - val_loss: 19.4897 - val_mse: 19.4897 - val_mae: 1.5619 - lr: 0.0011 - 8s/epoch - 8ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 8s - loss: 12.0498 - mse: 12.0498 - mae: 1.5703 - val_loss: 19.5003 - val_mse: 19.5003 - val_mae: 1.6160 - lr: 0.0011 - 8s/epoch - 8ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 8s - loss: 12.1278 - mse: 12.1278 - mae: 1.5801 - val_loss: 19.9147 - val_mse: 19.9147 - val_mae: 1.6245 - lr: 0.0011 - 8s/epoch - 8ms/step\n",
            "Score for fold 1: loss of 19.91474723815918\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 9s - loss: 13.7260 - mse: 13.7260 - mae: 1.5620 - val_loss: 12.5267 - val_mse: 12.5267 - val_mae: 1.5646 - lr: 0.0010 - 9s/epoch - 9ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 8s - loss: 13.6379 - mse: 13.6379 - mae: 1.5640 - val_loss: 12.6790 - val_mse: 12.6790 - val_mae: 1.6131 - lr: 0.0010 - 8s/epoch - 8ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 8s - loss: 13.6921 - mse: 13.6921 - mae: 1.5615 - val_loss: 12.2993 - val_mse: 12.2993 - val_mae: 1.5358 - lr: 0.0010 - 8s/epoch - 8ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 8s - loss: 13.7224 - mse: 13.7224 - mae: 1.5690 - val_loss: 12.5290 - val_mse: 12.5290 - val_mae: 1.5989 - lr: 0.0010 - 8s/epoch - 8ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 9s - loss: 13.5069 - mse: 13.5069 - mae: 1.5613 - val_loss: 12.3573 - val_mse: 12.3573 - val_mae: 1.6211 - lr: 0.0010 - 9s/epoch - 9ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 9s - loss: 13.6253 - mse: 13.6253 - mae: 1.5673 - val_loss: 12.2630 - val_mse: 12.2630 - val_mae: 1.6010 - lr: 0.0010 - 9s/epoch - 9ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 9s - loss: 13.7103 - mse: 13.7103 - mae: 1.5683 - val_loss: 12.3073 - val_mse: 12.3073 - val_mae: 1.6200 - lr: 0.0010 - 9s/epoch - 9ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 9s - loss: 13.6875 - mse: 13.6875 - mae: 1.5667 - val_loss: 12.4945 - val_mse: 12.4945 - val_mae: 1.5608 - lr: 0.0010 - 9s/epoch - 9ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 9s - loss: 13.6566 - mse: 13.6566 - mae: 1.5627 - val_loss: 12.6663 - val_mse: 12.6663 - val_mae: 1.5746 - lr: 0.0010 - 9s/epoch - 9ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 9s - loss: 13.6271 - mse: 13.6271 - mae: 1.5595 - val_loss: 12.7768 - val_mse: 12.7768 - val_mae: 1.5128 - lr: 0.0010 - 9s/epoch - 9ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 9s - loss: 13.5106 - mse: 13.5106 - mae: 1.5550 - val_loss: 12.2355 - val_mse: 12.2355 - val_mae: 1.5997 - lr: 0.0010 - 9s/epoch - 9ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 9s - loss: 13.4769 - mse: 13.4769 - mae: 1.5567 - val_loss: 12.4171 - val_mse: 12.4171 - val_mae: 1.5790 - lr: 0.0010 - 9s/epoch - 9ms/step\n",
            "Epoch 13/100\n",
            "1000/1000 - 9s - loss: 13.6085 - mse: 13.6085 - mae: 1.5612 - val_loss: 12.6601 - val_mse: 12.6601 - val_mae: 1.6091 - lr: 0.0010 - 9s/epoch - 9ms/step\n",
            "Epoch 14/100\n",
            "1000/1000 - 9s - loss: 13.6226 - mse: 13.6226 - mae: 1.5621 - val_loss: 12.5337 - val_mse: 12.5337 - val_mae: 1.5601 - lr: 0.0010 - 9s/epoch - 9ms/step\n",
            "Epoch 15/100\n",
            "1000/1000 - 9s - loss: 13.6452 - mse: 13.6452 - mae: 1.5592 - val_loss: 12.6615 - val_mse: 12.6615 - val_mae: 1.6298 - lr: 0.0010 - 9s/epoch - 9ms/step\n",
            "Epoch 16/100\n",
            "1000/1000 - 9s - loss: 13.6050 - mse: 13.6050 - mae: 1.5642 - val_loss: 12.4019 - val_mse: 12.4019 - val_mae: 1.5476 - lr: 0.0010 - 9s/epoch - 9ms/step\n",
            "Score for fold 2: loss of 12.401860237121582\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 9s - loss: 13.7559 - mse: 13.7559 - mae: 1.5730 - val_loss: 12.1193 - val_mse: 12.1193 - val_mae: 1.4842 - lr: 0.0010 - 9s/epoch - 9ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 9s - loss: 13.6277 - mse: 13.6277 - mae: 1.5731 - val_loss: 12.9839 - val_mse: 12.9839 - val_mae: 1.8604 - lr: 0.0010 - 9s/epoch - 9ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 9s - loss: 13.6049 - mse: 13.6049 - mae: 1.5779 - val_loss: 12.0075 - val_mse: 12.0075 - val_mae: 1.5409 - lr: 0.0010 - 9s/epoch - 9ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 9s - loss: 13.7552 - mse: 13.7552 - mae: 1.5710 - val_loss: 12.3386 - val_mse: 12.3386 - val_mae: 1.5088 - lr: 0.0010 - 9s/epoch - 9ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 9s - loss: 13.7767 - mse: 13.7767 - mae: 1.5646 - val_loss: 12.0701 - val_mse: 12.0701 - val_mae: 1.5413 - lr: 0.0010 - 9s/epoch - 9ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 9s - loss: 13.5961 - mse: 13.5961 - mae: 1.5666 - val_loss: 12.0203 - val_mse: 12.0203 - val_mae: 1.5795 - lr: 0.0010 - 9s/epoch - 9ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 9s - loss: 13.6213 - mse: 13.6213 - mae: 1.5772 - val_loss: 12.0266 - val_mse: 12.0266 - val_mae: 1.5583 - lr: 0.0010 - 9s/epoch - 9ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 9s - loss: 13.8322 - mse: 13.8322 - mae: 1.5632 - val_loss: 12.2438 - val_mse: 12.2438 - val_mae: 1.4656 - lr: 0.0010 - 9s/epoch - 9ms/step\n",
            "Score for fold 3: loss of 12.24382495880127\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 9s - loss: 13.5832 - mse: 13.5832 - mae: 1.5540 - val_loss: 13.0361 - val_mse: 13.0361 - val_mae: 1.5812 - lr: 0.0010 - 9s/epoch - 9ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 9s - loss: 13.4067 - mse: 13.4067 - mae: 1.5578 - val_loss: 12.9737 - val_mse: 12.9737 - val_mae: 1.5763 - lr: 0.0010 - 9s/epoch - 9ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 8s - loss: 13.5308 - mse: 13.5308 - mae: 1.5603 - val_loss: 14.4849 - val_mse: 14.4849 - val_mae: 1.7646 - lr: 0.0010 - 8s/epoch - 8ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 9s - loss: 13.6154 - mse: 13.6154 - mae: 1.5543 - val_loss: 13.3531 - val_mse: 13.3531 - val_mae: 1.6461 - lr: 0.0010 - 9s/epoch - 9ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 9s - loss: 13.4309 - mse: 13.4309 - mae: 1.5564 - val_loss: 13.0927 - val_mse: 13.0927 - val_mae: 1.5624 - lr: 0.0010 - 9s/epoch - 9ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 10s - loss: 13.5001 - mse: 13.5001 - mae: 1.5560 - val_loss: 12.9738 - val_mse: 12.9738 - val_mae: 1.5915 - lr: 0.0010 - 10s/epoch - 10ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 10s - loss: 13.5860 - mse: 13.5860 - mae: 1.5630 - val_loss: 12.7348 - val_mse: 12.7348 - val_mae: 1.5344 - lr: 0.0010 - 10s/epoch - 10ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 9s - loss: 13.4770 - mse: 13.4770 - mae: 1.5586 - val_loss: 12.8852 - val_mse: 12.8852 - val_mae: 1.5485 - lr: 0.0010 - 9s/epoch - 9ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 10s - loss: 13.7064 - mse: 13.7064 - mae: 1.5643 - val_loss: 12.6740 - val_mse: 12.6740 - val_mae: 1.6113 - lr: 0.0010 - 10s/epoch - 10ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 11s - loss: 13.4706 - mse: 13.4706 - mae: 1.5600 - val_loss: 13.0257 - val_mse: 13.0257 - val_mae: 1.5966 - lr: 0.0010 - 11s/epoch - 11ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 11s - loss: 13.4005 - mse: 13.4005 - mae: 1.5579 - val_loss: 13.5952 - val_mse: 13.5952 - val_mae: 1.6416 - lr: 0.0010 - 11s/epoch - 11ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 10s - loss: 13.5146 - mse: 13.5146 - mae: 1.5557 - val_loss: 12.7697 - val_mse: 12.7697 - val_mae: 1.5701 - lr: 0.0010 - 10s/epoch - 10ms/step\n",
            "Epoch 13/100\n",
            "1000/1000 - 10s - loss: 13.4756 - mse: 13.4756 - mae: 1.5631 - val_loss: 12.9725 - val_mse: 12.9725 - val_mae: 1.6451 - lr: 0.0010 - 10s/epoch - 10ms/step\n",
            "Epoch 14/100\n",
            "1000/1000 - 10s - loss: 13.5718 - mse: 13.5718 - mae: 1.5556 - val_loss: 12.7519 - val_mse: 12.7519 - val_mae: 1.5384 - lr: 0.0010 - 10s/epoch - 10ms/step\n",
            "Score for fold 4: loss of 12.751906394958496\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 8s - loss: 14.4274 - mse: 14.4274 - mae: 1.5803 - val_loss: 9.5419 - val_mse: 9.5419 - val_mae: 1.5100 - lr: 0.0010 - 8s/epoch - 8ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 8s - loss: 14.4208 - mse: 14.4208 - mae: 1.5790 - val_loss: 9.4905 - val_mse: 9.4905 - val_mae: 1.5294 - lr: 0.0010 - 8s/epoch - 8ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 8s - loss: 14.3790 - mse: 14.3790 - mae: 1.5769 - val_loss: 9.8105 - val_mse: 9.8105 - val_mae: 1.5589 - lr: 0.0010 - 8s/epoch - 8ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 8s - loss: 14.3835 - mse: 14.3835 - mae: 1.5791 - val_loss: 9.9596 - val_mse: 9.9596 - val_mae: 1.4553 - lr: 0.0010 - 8s/epoch - 8ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 8s - loss: 14.2978 - mse: 14.2978 - mae: 1.5756 - val_loss: 9.6858 - val_mse: 9.6858 - val_mae: 1.5467 - lr: 0.0010 - 8s/epoch - 8ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 8s - loss: 14.2996 - mse: 14.2996 - mae: 1.5737 - val_loss: 9.6867 - val_mse: 9.6867 - val_mae: 1.4596 - lr: 0.0010 - 8s/epoch - 8ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 8s - loss: 14.3691 - mse: 14.3691 - mae: 1.5752 - val_loss: 9.5944 - val_mse: 9.5944 - val_mae: 1.5284 - lr: 0.0010 - 8s/epoch - 8ms/step\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-10-12 04:00:32,547]\u001b[0m Finished trial#19 resulted in value: 13.378. Current best value is 10.022 with parameters: {'activation': 'relu', 'num_hidden_layer': 4, 'i': 9, 'learning_rate': 0.0011355512574150934}.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Score for fold 5: loss of 9.594411849975586\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 4s - loss: 14.3519 - mse: 14.3519 - mae: 1.5810 - val_loss: 12.5661 - val_mse: 12.5661 - val_mae: 1.4943 - lr: 1.8892e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 3s - loss: 12.7982 - mse: 12.7982 - mae: 1.5084 - val_loss: 12.3468 - val_mse: 12.3468 - val_mae: 1.4673 - lr: 1.8892e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 3s - loss: 12.4639 - mse: 12.4639 - mae: 1.4916 - val_loss: 12.0912 - val_mse: 12.0912 - val_mae: 1.4989 - lr: 1.8892e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 3s - loss: 12.2695 - mse: 12.2695 - mae: 1.4778 - val_loss: 12.0789 - val_mse: 12.0789 - val_mae: 1.4580 - lr: 1.8892e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 3s - loss: 12.2473 - mse: 12.2473 - mae: 1.4703 - val_loss: 12.0812 - val_mse: 12.0812 - val_mae: 1.4008 - lr: 1.8892e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 3s - loss: 12.1229 - mse: 12.1229 - mae: 1.4601 - val_loss: 12.1064 - val_mse: 12.1064 - val_mae: 1.4479 - lr: 1.8892e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 3s - loss: 12.0913 - mse: 12.0913 - mae: 1.4604 - val_loss: 11.8408 - val_mse: 11.8408 - val_mae: 1.4724 - lr: 1.8892e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 3s - loss: 12.0480 - mse: 12.0480 - mae: 1.4533 - val_loss: 11.7920 - val_mse: 11.7920 - val_mae: 1.5234 - lr: 1.8892e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 3s - loss: 11.9082 - mse: 11.9082 - mae: 1.4505 - val_loss: 12.1180 - val_mse: 12.1180 - val_mae: 1.4503 - lr: 1.8892e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 3s - loss: 11.9223 - mse: 11.9223 - mae: 1.4442 - val_loss: 11.8229 - val_mse: 11.8229 - val_mae: 1.4695 - lr: 1.8892e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 3s - loss: 11.7560 - mse: 11.7560 - mae: 1.4399 - val_loss: 12.0639 - val_mse: 12.0639 - val_mae: 1.4606 - lr: 1.8892e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 3s - loss: 11.8119 - mse: 11.8119 - mae: 1.4356 - val_loss: 12.0627 - val_mse: 12.0627 - val_mae: 1.4598 - lr: 1.8892e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 13/100\n",
            "1000/1000 - 3s - loss: 11.7678 - mse: 11.7678 - mae: 1.4359 - val_loss: 12.0503 - val_mse: 12.0503 - val_mae: 1.4442 - lr: 1.8892e-04 - 3s/epoch - 3ms/step\n",
            "Score for fold 1: loss of 12.050262451171875\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 3s - loss: 12.6216 - mse: 12.6216 - mae: 1.4437 - val_loss: 8.0649 - val_mse: 8.0649 - val_mae: 1.4384 - lr: 1.8892e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 3s - loss: 12.6279 - mse: 12.6279 - mae: 1.4403 - val_loss: 8.1444 - val_mse: 8.1444 - val_mae: 1.4227 - lr: 1.8892e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 3s - loss: 12.4915 - mse: 12.4915 - mae: 1.4283 - val_loss: 8.1754 - val_mse: 8.1754 - val_mae: 1.4379 - lr: 1.8892e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 3s - loss: 12.4902 - mse: 12.4902 - mae: 1.4296 - val_loss: 8.4929 - val_mse: 8.4929 - val_mae: 1.3920 - lr: 1.8892e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 3s - loss: 12.3645 - mse: 12.3645 - mae: 1.4236 - val_loss: 8.1103 - val_mse: 8.1103 - val_mae: 1.4600 - lr: 1.8892e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 3s - loss: 12.3447 - mse: 12.3447 - mae: 1.4247 - val_loss: 8.1861 - val_mse: 8.1861 - val_mae: 1.4698 - lr: 1.8892e-04 - 3s/epoch - 3ms/step\n",
            "Score for fold 2: loss of 8.186120986938477\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 3s - loss: 11.7988 - mse: 11.7988 - mae: 1.4283 - val_loss: 10.1033 - val_mse: 10.1033 - val_mae: 1.4311 - lr: 1.8892e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 3s - loss: 11.7203 - mse: 11.7203 - mae: 1.4237 - val_loss: 10.0777 - val_mse: 10.0777 - val_mae: 1.4482 - lr: 1.8892e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 3s - loss: 11.6952 - mse: 11.6952 - mae: 1.4233 - val_loss: 10.1613 - val_mse: 10.1613 - val_mae: 1.4223 - lr: 1.8892e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 3s - loss: 11.6181 - mse: 11.6181 - mae: 1.4152 - val_loss: 10.4040 - val_mse: 10.4040 - val_mae: 1.3592 - lr: 1.8892e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 3s - loss: 11.5241 - mse: 11.5241 - mae: 1.4144 - val_loss: 10.3072 - val_mse: 10.3072 - val_mae: 1.3893 - lr: 1.8892e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 3s - loss: 11.3953 - mse: 11.3953 - mae: 1.4084 - val_loss: 10.2837 - val_mse: 10.2837 - val_mae: 1.4191 - lr: 1.8892e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 3s - loss: 11.3273 - mse: 11.3273 - mae: 1.4034 - val_loss: 10.3699 - val_mse: 10.3699 - val_mae: 1.4011 - lr: 1.8892e-04 - 3s/epoch - 3ms/step\n",
            "Score for fold 3: loss of 10.369857788085938\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 3s - loss: 9.4117 - mse: 9.4117 - mae: 1.4029 - val_loss: 17.7857 - val_mse: 17.7857 - val_mae: 1.4583 - lr: 1.8892e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 3s - loss: 9.2349 - mse: 9.2349 - mae: 1.3985 - val_loss: 18.2416 - val_mse: 18.2416 - val_mae: 1.3861 - lr: 1.8892e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 3s - loss: 9.2197 - mse: 9.2197 - mae: 1.3926 - val_loss: 18.3757 - val_mse: 18.3757 - val_mae: 1.4055 - lr: 1.8892e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 3s - loss: 9.1501 - mse: 9.1501 - mae: 1.3901 - val_loss: 17.9826 - val_mse: 17.9826 - val_mae: 1.4113 - lr: 1.8892e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 3s - loss: 9.0614 - mse: 9.0614 - mae: 1.3835 - val_loss: 18.2018 - val_mse: 18.2018 - val_mae: 1.4284 - lr: 1.8892e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 3s - loss: 9.0337 - mse: 9.0337 - mae: 1.3832 - val_loss: 18.1382 - val_mse: 18.1382 - val_mae: 1.4354 - lr: 1.8892e-04 - 3s/epoch - 3ms/step\n",
            "Score for fold 4: loss of 18.138187408447266\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 3s - loss: 11.4878 - mse: 11.4878 - mae: 1.3954 - val_loss: 8.5477 - val_mse: 8.5477 - val_mae: 1.3615 - lr: 1.8892e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 3s - loss: 11.2160 - mse: 11.2160 - mae: 1.3914 - val_loss: 8.7191 - val_mse: 8.7191 - val_mae: 1.4084 - lr: 1.8892e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 3s - loss: 11.1242 - mse: 11.1242 - mae: 1.3843 - val_loss: 8.5528 - val_mse: 8.5528 - val_mae: 1.3647 - lr: 1.8892e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 3s - loss: 11.1304 - mse: 11.1304 - mae: 1.3842 - val_loss: 8.7545 - val_mse: 8.7545 - val_mae: 1.4827 - lr: 1.8892e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 3s - loss: 11.0557 - mse: 11.0557 - mae: 1.3786 - val_loss: 8.7815 - val_mse: 8.7815 - val_mae: 1.4058 - lr: 1.8892e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 3s - loss: 10.9625 - mse: 10.9625 - mae: 1.3761 - val_loss: 8.6810 - val_mse: 8.6810 - val_mae: 1.4044 - lr: 1.8892e-04 - 3s/epoch - 3ms/step\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-10-12 04:02:18,550]\u001b[0m Finished trial#20 resulted in value: 11.486. Current best value is 10.022 with parameters: {'activation': 'relu', 'num_hidden_layer': 4, 'i': 9, 'learning_rate': 0.0011355512574150934}.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Score for fold 5: loss of 8.680994033813477\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 11s - loss: 11.8192 - mse: 11.8192 - mae: 1.5398 - val_loss: 17.7035 - val_mse: 17.7035 - val_mae: 1.4684 - lr: 2.3927e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 10s - loss: 11.0390 - mse: 11.0390 - mae: 1.4833 - val_loss: 17.4359 - val_mse: 17.4359 - val_mae: 1.4794 - lr: 2.3927e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 10s - loss: 10.8703 - mse: 10.8703 - mae: 1.4655 - val_loss: 17.1888 - val_mse: 17.1888 - val_mae: 1.4594 - lr: 2.3927e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 9s - loss: 10.6458 - mse: 10.6458 - mae: 1.4556 - val_loss: 17.4403 - val_mse: 17.4403 - val_mae: 1.4085 - lr: 2.3927e-04 - 9s/epoch - 9ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 9s - loss: 10.6051 - mse: 10.6051 - mae: 1.4471 - val_loss: 17.1723 - val_mse: 17.1723 - val_mae: 1.4292 - lr: 2.3927e-04 - 9s/epoch - 9ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 9s - loss: 10.5442 - mse: 10.5442 - mae: 1.4424 - val_loss: 17.0708 - val_mse: 17.0708 - val_mae: 1.4939 - lr: 2.3927e-04 - 9s/epoch - 9ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 8s - loss: 10.3897 - mse: 10.3897 - mae: 1.4319 - val_loss: 17.1118 - val_mse: 17.1118 - val_mae: 1.5023 - lr: 2.3927e-04 - 8s/epoch - 8ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 8s - loss: 10.3536 - mse: 10.3536 - mae: 1.4261 - val_loss: 17.0383 - val_mse: 17.0383 - val_mae: 1.4343 - lr: 2.3927e-04 - 8s/epoch - 8ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 8s - loss: 10.1130 - mse: 10.1130 - mae: 1.4182 - val_loss: 17.9600 - val_mse: 17.9600 - val_mae: 1.5036 - lr: 2.3927e-04 - 8s/epoch - 8ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 8s - loss: 10.1615 - mse: 10.1615 - mae: 1.4167 - val_loss: 16.9958 - val_mse: 16.9958 - val_mae: 1.4318 - lr: 2.3927e-04 - 8s/epoch - 8ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 8s - loss: 9.9165 - mse: 9.9165 - mae: 1.4062 - val_loss: 17.0582 - val_mse: 17.0582 - val_mae: 1.4189 - lr: 2.3927e-04 - 8s/epoch - 8ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 8s - loss: 9.8705 - mse: 9.8705 - mae: 1.3987 - val_loss: 17.0824 - val_mse: 17.0824 - val_mae: 1.4435 - lr: 2.3927e-04 - 8s/epoch - 8ms/step\n",
            "Epoch 13/100\n",
            "1000/1000 - 8s - loss: 9.7661 - mse: 9.7661 - mae: 1.3934 - val_loss: 17.0944 - val_mse: 17.0944 - val_mae: 1.4743 - lr: 2.3927e-04 - 8s/epoch - 8ms/step\n",
            "Epoch 14/100\n",
            "1000/1000 - 8s - loss: 9.4877 - mse: 9.4877 - mae: 1.3842 - val_loss: 17.2837 - val_mse: 17.2837 - val_mae: 1.4310 - lr: 2.3927e-04 - 8s/epoch - 8ms/step\n",
            "Epoch 15/100\n",
            "1000/1000 - 8s - loss: 9.4258 - mse: 9.4258 - mae: 1.3776 - val_loss: 17.0161 - val_mse: 17.0161 - val_mae: 1.4728 - lr: 2.3927e-04 - 8s/epoch - 8ms/step\n",
            "Score for fold 1: loss of 17.016136169433594\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 8s - loss: 11.5221 - mse: 11.5221 - mae: 1.3938 - val_loss: 8.1682 - val_mse: 8.1682 - val_mae: 1.3994 - lr: 2.3927e-04 - 8s/epoch - 8ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 8s - loss: 11.3625 - mse: 11.3625 - mae: 1.3871 - val_loss: 8.5816 - val_mse: 8.5816 - val_mae: 1.3607 - lr: 2.3927e-04 - 8s/epoch - 8ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 8s - loss: 11.2132 - mse: 11.2132 - mae: 1.3749 - val_loss: 8.9720 - val_mse: 8.9720 - val_mae: 1.3793 - lr: 2.3927e-04 - 8s/epoch - 8ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 10s - loss: 11.0036 - mse: 11.0036 - mae: 1.3655 - val_loss: 9.0553 - val_mse: 9.0553 - val_mae: 1.3449 - lr: 2.3927e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 10s - loss: 10.8068 - mse: 10.8068 - mae: 1.3531 - val_loss: 8.6598 - val_mse: 8.6598 - val_mae: 1.3928 - lr: 2.3927e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 10s - loss: 10.6918 - mse: 10.6918 - mae: 1.3421 - val_loss: 8.7022 - val_mse: 8.7022 - val_mae: 1.3926 - lr: 2.3927e-04 - 10s/epoch - 10ms/step\n",
            "Score for fold 2: loss of 8.702202796936035\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 11s - loss: 11.0839 - mse: 11.0839 - mae: 1.3539 - val_loss: 6.4743 - val_mse: 6.4743 - val_mae: 1.3260 - lr: 2.3927e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 11s - loss: 10.9195 - mse: 10.9195 - mae: 1.3408 - val_loss: 7.4572 - val_mse: 7.4572 - val_mae: 1.3275 - lr: 2.3927e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 10s - loss: 10.8164 - mse: 10.8164 - mae: 1.3306 - val_loss: 7.2696 - val_mse: 7.2696 - val_mae: 1.3804 - lr: 2.3927e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 10s - loss: 10.7060 - mse: 10.7060 - mae: 1.3189 - val_loss: 6.7261 - val_mse: 6.7261 - val_mae: 1.3327 - lr: 2.3927e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 10s - loss: 10.4045 - mse: 10.4045 - mae: 1.3056 - val_loss: 7.2611 - val_mse: 7.2611 - val_mae: 1.3573 - lr: 2.3927e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 10s - loss: 10.2691 - mse: 10.2691 - mae: 1.2903 - val_loss: 7.4441 - val_mse: 7.4441 - val_mae: 1.3535 - lr: 2.3927e-04 - 10s/epoch - 10ms/step\n",
            "Score for fold 3: loss of 7.444084644317627\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 10s - loss: 9.8416 - mse: 9.8416 - mae: 1.3119 - val_loss: 9.4074 - val_mse: 9.4074 - val_mae: 1.3555 - lr: 2.3927e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 9s - loss: 9.3252 - mse: 9.3252 - mae: 1.2900 - val_loss: 10.2691 - val_mse: 10.2691 - val_mae: 1.2587 - lr: 2.3927e-04 - 9s/epoch - 9ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 9s - loss: 9.1889 - mse: 9.1889 - mae: 1.2774 - val_loss: 10.3237 - val_mse: 10.3237 - val_mae: 1.2668 - lr: 2.3927e-04 - 9s/epoch - 9ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 9s - loss: 8.9448 - mse: 8.9448 - mae: 1.2633 - val_loss: 10.0727 - val_mse: 10.0727 - val_mae: 1.2887 - lr: 2.3927e-04 - 9s/epoch - 9ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 9s - loss: 8.7015 - mse: 8.7015 - mae: 1.2454 - val_loss: 9.8991 - val_mse: 9.8991 - val_mae: 1.4002 - lr: 2.3927e-04 - 9s/epoch - 9ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 9s - loss: 8.6187 - mse: 8.6187 - mae: 1.2331 - val_loss: 9.8368 - val_mse: 9.8368 - val_mae: 1.3284 - lr: 2.3927e-04 - 9s/epoch - 9ms/step\n",
            "Score for fold 4: loss of 9.836836814880371\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 8s - loss: 8.9734 - mse: 8.9734 - mae: 1.2574 - val_loss: 8.2508 - val_mse: 8.2508 - val_mae: 1.1925 - lr: 2.3927e-04 - 8s/epoch - 8ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 8s - loss: 8.9789 - mse: 8.9789 - mae: 1.2388 - val_loss: 8.5162 - val_mse: 8.5162 - val_mae: 1.1927 - lr: 2.3927e-04 - 8s/epoch - 8ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 8s - loss: 8.4768 - mse: 8.4768 - mae: 1.2231 - val_loss: 8.5225 - val_mse: 8.5225 - val_mae: 1.1930 - lr: 2.3927e-04 - 8s/epoch - 8ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 8s - loss: 8.3277 - mse: 8.3277 - mae: 1.2089 - val_loss: 8.6802 - val_mse: 8.6802 - val_mae: 1.3046 - lr: 2.3927e-04 - 8s/epoch - 8ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 8s - loss: 8.1896 - mse: 8.1896 - mae: 1.1923 - val_loss: 8.4123 - val_mse: 8.4123 - val_mae: 1.2727 - lr: 2.3927e-04 - 8s/epoch - 8ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 8s - loss: 8.0028 - mse: 8.0028 - mae: 1.1766 - val_loss: 9.1737 - val_mse: 9.1737 - val_mae: 1.2632 - lr: 2.3927e-04 - 8s/epoch - 8ms/step\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-10-12 04:08:15,385]\u001b[0m Finished trial#21 resulted in value: 10.434000000000001. Current best value is 10.022 with parameters: {'activation': 'relu', 'num_hidden_layer': 4, 'i': 9, 'learning_rate': 0.0011355512574150934}.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Score for fold 5: loss of 9.17370891571045\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 6s - loss: 14.0767 - mse: 14.0767 - mae: 1.5331 - val_loss: 9.2669 - val_mse: 9.2669 - val_mae: 1.4475 - lr: 7.1118e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 5s - loss: 13.2098 - mse: 13.2098 - mae: 1.4841 - val_loss: 9.4039 - val_mse: 9.4039 - val_mae: 1.4225 - lr: 7.1118e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 5s - loss: 13.0561 - mse: 13.0561 - mae: 1.4734 - val_loss: 9.0714 - val_mse: 9.0714 - val_mae: 1.4559 - lr: 7.1118e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 5s - loss: 12.8058 - mse: 12.8058 - mae: 1.4594 - val_loss: 8.8748 - val_mse: 8.8748 - val_mae: 1.4609 - lr: 7.1118e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 5s - loss: 12.7932 - mse: 12.7932 - mae: 1.4512 - val_loss: 8.8555 - val_mse: 8.8555 - val_mae: 1.5119 - lr: 7.1118e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 5s - loss: 12.6729 - mse: 12.6729 - mae: 1.4448 - val_loss: 8.8097 - val_mse: 8.8097 - val_mae: 1.4504 - lr: 7.1118e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 5s - loss: 12.2957 - mse: 12.2957 - mae: 1.4329 - val_loss: 8.3787 - val_mse: 8.3787 - val_mae: 1.4843 - lr: 7.1118e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 5s - loss: 12.4762 - mse: 12.4762 - mae: 1.4348 - val_loss: 8.7482 - val_mse: 8.7482 - val_mae: 1.4784 - lr: 7.1118e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 5s - loss: 12.1824 - mse: 12.1824 - mae: 1.4245 - val_loss: 8.6500 - val_mse: 8.6500 - val_mae: 1.4640 - lr: 7.1118e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 4s - loss: 12.2103 - mse: 12.2103 - mae: 1.4251 - val_loss: 8.5362 - val_mse: 8.5362 - val_mae: 1.4454 - lr: 7.1118e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 4s - loss: 12.0474 - mse: 12.0474 - mae: 1.4108 - val_loss: 8.5875 - val_mse: 8.5875 - val_mae: 1.4875 - lr: 7.1118e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 4s - loss: 12.1906 - mse: 12.1906 - mae: 1.4068 - val_loss: 8.4577 - val_mse: 8.4577 - val_mae: 1.3961 - lr: 7.1118e-04 - 4s/epoch - 4ms/step\n",
            "Score for fold 1: loss of 8.457674026489258\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 5s - loss: 11.5797 - mse: 11.5797 - mae: 1.4216 - val_loss: 9.7821 - val_mse: 9.7821 - val_mae: 1.3309 - lr: 7.1118e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 4s - loss: 11.3371 - mse: 11.3371 - mae: 1.4105 - val_loss: 9.8877 - val_mse: 9.8877 - val_mae: 1.3577 - lr: 7.1118e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 4s - loss: 11.0883 - mse: 11.0883 - mae: 1.4017 - val_loss: 9.9167 - val_mse: 9.9167 - val_mae: 1.3968 - lr: 7.1118e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 4s - loss: 11.0951 - mse: 11.0951 - mae: 1.3978 - val_loss: 9.9029 - val_mse: 9.9029 - val_mae: 1.4403 - lr: 7.1118e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 5s - loss: 11.0755 - mse: 11.0755 - mae: 1.3895 - val_loss: 10.1237 - val_mse: 10.1237 - val_mae: 1.4144 - lr: 7.1118e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 4s - loss: 10.6784 - mse: 10.6784 - mae: 1.3798 - val_loss: 10.3755 - val_mse: 10.3755 - val_mae: 1.4272 - lr: 7.1118e-04 - 4s/epoch - 4ms/step\n",
            "Score for fold 2: loss of 10.375503540039062\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 4s - loss: 8.8839 - mse: 8.8839 - mae: 1.3875 - val_loss: 18.3882 - val_mse: 18.3882 - val_mae: 1.3753 - lr: 7.1118e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 4s - loss: 8.5660 - mse: 8.5660 - mae: 1.3767 - val_loss: 18.0748 - val_mse: 18.0748 - val_mae: 1.3743 - lr: 7.1118e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 4s - loss: 8.2884 - mse: 8.2884 - mae: 1.3612 - val_loss: 18.2900 - val_mse: 18.2900 - val_mae: 1.4051 - lr: 7.1118e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 5s - loss: 8.3861 - mse: 8.3861 - mae: 1.3550 - val_loss: 18.2765 - val_mse: 18.2765 - val_mae: 1.4008 - lr: 7.1118e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 4s - loss: 8.2918 - mse: 8.2918 - mae: 1.3420 - val_loss: 19.2816 - val_mse: 19.2816 - val_mae: 1.4004 - lr: 7.1118e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 5s - loss: 7.9229 - mse: 7.9229 - mae: 1.3324 - val_loss: 18.6961 - val_mse: 18.6961 - val_mae: 1.4367 - lr: 7.1118e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 4s - loss: 7.8000 - mse: 7.8000 - mae: 1.3216 - val_loss: 18.5757 - val_mse: 18.5757 - val_mae: 1.4206 - lr: 7.1118e-04 - 4s/epoch - 4ms/step\n",
            "Score for fold 3: loss of 18.57567596435547\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 4s - loss: 10.1436 - mse: 10.1436 - mae: 1.3380 - val_loss: 8.4568 - val_mse: 8.4568 - val_mae: 1.2969 - lr: 7.1118e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 4s - loss: 9.9896 - mse: 9.9896 - mae: 1.3242 - val_loss: 8.7251 - val_mse: 8.7251 - val_mae: 1.3364 - lr: 7.1118e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 4s - loss: 9.7725 - mse: 9.7725 - mae: 1.3115 - val_loss: 9.4595 - val_mse: 9.4595 - val_mae: 1.3683 - lr: 7.1118e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 4s - loss: 9.5842 - mse: 9.5842 - mae: 1.3034 - val_loss: 9.1827 - val_mse: 9.1827 - val_mae: 1.3572 - lr: 7.1118e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 4s - loss: 9.3754 - mse: 9.3754 - mae: 1.2877 - val_loss: 9.1616 - val_mse: 9.1616 - val_mae: 1.3517 - lr: 7.1118e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 4s - loss: 9.2365 - mse: 9.2365 - mae: 1.2745 - val_loss: 8.9423 - val_mse: 8.9423 - val_mae: 1.3862 - lr: 7.1118e-04 - 4s/epoch - 4ms/step\n",
            "Score for fold 4: loss of 8.942261695861816\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 4s - loss: 10.2092 - mse: 10.2092 - mae: 1.2996 - val_loss: 6.3621 - val_mse: 6.3621 - val_mae: 1.2683 - lr: 7.1118e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 4s - loss: 9.5795 - mse: 9.5795 - mae: 1.2788 - val_loss: 6.9623 - val_mse: 6.9623 - val_mae: 1.2758 - lr: 7.1118e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 4s - loss: 9.2023 - mse: 9.2023 - mae: 1.2606 - val_loss: 7.3675 - val_mse: 7.3675 - val_mae: 1.2644 - lr: 7.1118e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 4s - loss: 9.0964 - mse: 9.0964 - mae: 1.2448 - val_loss: 7.5802 - val_mse: 7.5802 - val_mae: 1.2600 - lr: 7.1118e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 4s - loss: 8.8717 - mse: 8.8717 - mae: 1.2264 - val_loss: 7.0861 - val_mse: 7.0861 - val_mae: 1.2756 - lr: 7.1118e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 4s - loss: 8.7101 - mse: 8.7101 - mae: 1.2152 - val_loss: 7.0990 - val_mse: 7.0990 - val_mae: 1.3118 - lr: 7.1118e-04 - 4s/epoch - 4ms/step\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-10-12 04:11:08,676]\u001b[0m Finished trial#22 resulted in value: 10.692. Current best value is 10.022 with parameters: {'activation': 'relu', 'num_hidden_layer': 4, 'i': 9, 'learning_rate': 0.0011355512574150934}.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Score for fold 5: loss of 7.098965167999268\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 3s - loss: 14.4364 - mse: 14.4364 - mae: 1.5651 - val_loss: 9.1403 - val_mse: 9.1403 - val_mae: 1.4844 - lr: 3.5321e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 3s - loss: 13.5348 - mse: 13.5348 - mae: 1.5103 - val_loss: 8.9316 - val_mse: 8.9316 - val_mae: 1.4921 - lr: 3.5321e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 3s - loss: 13.1840 - mse: 13.1840 - mae: 1.4968 - val_loss: 8.8481 - val_mse: 8.8481 - val_mae: 1.4651 - lr: 3.5321e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 3s - loss: 13.0653 - mse: 13.0653 - mae: 1.4869 - val_loss: 8.5277 - val_mse: 8.5277 - val_mae: 1.4165 - lr: 3.5321e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 3s - loss: 12.9419 - mse: 12.9419 - mae: 1.4793 - val_loss: 8.5317 - val_mse: 8.5317 - val_mae: 1.4648 - lr: 3.5321e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 3s - loss: 12.8057 - mse: 12.8057 - mae: 1.4739 - val_loss: 8.5286 - val_mse: 8.5286 - val_mae: 1.4652 - lr: 3.5321e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 3s - loss: 12.7428 - mse: 12.7428 - mae: 1.4672 - val_loss: 8.4307 - val_mse: 8.4307 - val_mae: 1.4062 - lr: 3.5321e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 3s - loss: 12.8224 - mse: 12.8224 - mae: 1.4651 - val_loss: 8.4382 - val_mse: 8.4382 - val_mae: 1.4100 - lr: 3.5321e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 3s - loss: 12.6537 - mse: 12.6537 - mae: 1.4615 - val_loss: 8.4447 - val_mse: 8.4447 - val_mae: 1.4210 - lr: 3.5321e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 3s - loss: 12.6266 - mse: 12.6266 - mae: 1.4642 - val_loss: 8.4056 - val_mse: 8.4056 - val_mae: 1.4343 - lr: 3.5321e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 3s - loss: 12.6433 - mse: 12.6433 - mae: 1.4592 - val_loss: 8.4466 - val_mse: 8.4466 - val_mae: 1.4127 - lr: 3.5321e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 3s - loss: 12.6225 - mse: 12.6225 - mae: 1.4554 - val_loss: 8.5723 - val_mse: 8.5723 - val_mae: 1.4632 - lr: 3.5321e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 13/100\n",
            "1000/1000 - 3s - loss: 12.5095 - mse: 12.5095 - mae: 1.4531 - val_loss: 8.7711 - val_mse: 8.7711 - val_mae: 1.4873 - lr: 3.5321e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 14/100\n",
            "1000/1000 - 3s - loss: 12.4739 - mse: 12.4739 - mae: 1.4534 - val_loss: 8.4135 - val_mse: 8.4135 - val_mae: 1.4065 - lr: 3.5321e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 15/100\n",
            "1000/1000 - 3s - loss: 12.4986 - mse: 12.4986 - mae: 1.4452 - val_loss: 8.5117 - val_mse: 8.5117 - val_mae: 1.3993 - lr: 3.5321e-04 - 3s/epoch - 3ms/step\n",
            "Score for fold 1: loss of 8.511720657348633\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 3s - loss: 10.5924 - mse: 10.5924 - mae: 1.4309 - val_loss: 16.2950 - val_mse: 16.2950 - val_mae: 1.4824 - lr: 3.5321e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 3s - loss: 10.5334 - mse: 10.5334 - mae: 1.4258 - val_loss: 16.1309 - val_mse: 16.1309 - val_mae: 1.5073 - lr: 3.5321e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 3s - loss: 10.4435 - mse: 10.4435 - mae: 1.4275 - val_loss: 16.3904 - val_mse: 16.3904 - val_mae: 1.4376 - lr: 3.5321e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 3s - loss: 10.5656 - mse: 10.5656 - mae: 1.4240 - val_loss: 16.2845 - val_mse: 16.2845 - val_mae: 1.4446 - lr: 3.5321e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 3s - loss: 10.4735 - mse: 10.4735 - mae: 1.4218 - val_loss: 16.1445 - val_mse: 16.1445 - val_mae: 1.4478 - lr: 3.5321e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 3s - loss: 10.4143 - mse: 10.4143 - mae: 1.4186 - val_loss: 16.2936 - val_mse: 16.2936 - val_mae: 1.4903 - lr: 3.5321e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 3s - loss: 10.3550 - mse: 10.3550 - mae: 1.4192 - val_loss: 16.4834 - val_mse: 16.4834 - val_mae: 1.4546 - lr: 3.5321e-04 - 3s/epoch - 3ms/step\n",
            "Score for fold 2: loss of 16.48342514038086\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 3s - loss: 12.1398 - mse: 12.1398 - mae: 1.4362 - val_loss: 9.3434 - val_mse: 9.3434 - val_mae: 1.4337 - lr: 3.5321e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 3s - loss: 12.0630 - mse: 12.0630 - mae: 1.4334 - val_loss: 9.2893 - val_mse: 9.2893 - val_mae: 1.4075 - lr: 3.5321e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 3s - loss: 12.1094 - mse: 12.1094 - mae: 1.4298 - val_loss: 9.2585 - val_mse: 9.2585 - val_mae: 1.4258 - lr: 3.5321e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 3s - loss: 12.0954 - mse: 12.0954 - mae: 1.4312 - val_loss: 9.4640 - val_mse: 9.4640 - val_mae: 1.4801 - lr: 3.5321e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 3s - loss: 12.0123 - mse: 12.0123 - mae: 1.4248 - val_loss: 9.4083 - val_mse: 9.4083 - val_mae: 1.4307 - lr: 3.5321e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 3s - loss: 11.9316 - mse: 11.9316 - mae: 1.4287 - val_loss: 9.3658 - val_mse: 9.3658 - val_mae: 1.3922 - lr: 3.5321e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 3s - loss: 11.9514 - mse: 11.9514 - mae: 1.4256 - val_loss: 9.2934 - val_mse: 9.2934 - val_mae: 1.4258 - lr: 3.5321e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 3s - loss: 11.9354 - mse: 11.9354 - mae: 1.4237 - val_loss: 9.3078 - val_mse: 9.3078 - val_mae: 1.4607 - lr: 3.5321e-04 - 3s/epoch - 3ms/step\n",
            "Score for fold 3: loss of 9.307821273803711\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 3s - loss: 10.7889 - mse: 10.7889 - mae: 1.4245 - val_loss: 13.7738 - val_mse: 13.7738 - val_mae: 1.4295 - lr: 3.5321e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 3s - loss: 10.7818 - mse: 10.7818 - mae: 1.4170 - val_loss: 13.9640 - val_mse: 13.9640 - val_mae: 1.4011 - lr: 3.5321e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 3s - loss: 10.7171 - mse: 10.7171 - mae: 1.4171 - val_loss: 13.8677 - val_mse: 13.8677 - val_mae: 1.4256 - lr: 3.5321e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 3s - loss: 10.7271 - mse: 10.7271 - mae: 1.4162 - val_loss: 14.0051 - val_mse: 14.0051 - val_mae: 1.4356 - lr: 3.5321e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 3s - loss: 10.7689 - mse: 10.7689 - mae: 1.4163 - val_loss: 13.9253 - val_mse: 13.9253 - val_mae: 1.4334 - lr: 3.5321e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 3s - loss: 10.6452 - mse: 10.6452 - mae: 1.4126 - val_loss: 14.0645 - val_mse: 14.0645 - val_mae: 1.4198 - lr: 3.5321e-04 - 3s/epoch - 3ms/step\n",
            "Score for fold 4: loss of 14.064452171325684\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 3s - loss: 11.7307 - mse: 11.7307 - mae: 1.4207 - val_loss: 9.6320 - val_mse: 9.6320 - val_mae: 1.4303 - lr: 3.5321e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 3s - loss: 11.6537 - mse: 11.6537 - mae: 1.4147 - val_loss: 10.5884 - val_mse: 10.5884 - val_mae: 1.4317 - lr: 3.5321e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 3s - loss: 11.7000 - mse: 11.7000 - mae: 1.4158 - val_loss: 9.6762 - val_mse: 9.6762 - val_mae: 1.4074 - lr: 3.5321e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 3s - loss: 11.6601 - mse: 11.6601 - mae: 1.4152 - val_loss: 9.7951 - val_mse: 9.7951 - val_mae: 1.3994 - lr: 3.5321e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 3s - loss: 11.6385 - mse: 11.6385 - mae: 1.4146 - val_loss: 9.5776 - val_mse: 9.5776 - val_mae: 1.4130 - lr: 3.5321e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 3s - loss: 11.5740 - mse: 11.5740 - mae: 1.4128 - val_loss: 9.5703 - val_mse: 9.5703 - val_mae: 1.3873 - lr: 3.5321e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 3s - loss: 11.5718 - mse: 11.5718 - mae: 1.4103 - val_loss: 9.7141 - val_mse: 9.7141 - val_mae: 1.4751 - lr: 3.5321e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 3s - loss: 11.5193 - mse: 11.5193 - mae: 1.4071 - val_loss: 9.8026 - val_mse: 9.8026 - val_mae: 1.4282 - lr: 3.5321e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 3s - loss: 11.5277 - mse: 11.5277 - mae: 1.4071 - val_loss: 9.6297 - val_mse: 9.6297 - val_mae: 1.3863 - lr: 3.5321e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 3s - loss: 11.5517 - mse: 11.5517 - mae: 1.4072 - val_loss: 9.7518 - val_mse: 9.7518 - val_mae: 1.4266 - lr: 3.5321e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 3s - loss: 11.4969 - mse: 11.4969 - mae: 1.4091 - val_loss: 9.9190 - val_mse: 9.9190 - val_mae: 1.4032 - lr: 3.5321e-04 - 3s/epoch - 3ms/step\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-10-12 04:13:24,377]\u001b[0m Finished trial#23 resulted in value: 11.656000000000002. Current best value is 10.022 with parameters: {'activation': 'relu', 'num_hidden_layer': 4, 'i': 9, 'learning_rate': 0.0011355512574150934}.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Score for fold 5: loss of 9.918974876403809\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 7s - loss: 14.0094 - mse: 14.0094 - mae: 1.5563 - val_loss: 10.2502 - val_mse: 10.2502 - val_mae: 1.4521 - lr: 1.7314e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 6s - loss: 12.9020 - mse: 12.9020 - mae: 1.4945 - val_loss: 10.0153 - val_mse: 10.0153 - val_mae: 1.4475 - lr: 1.7314e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 6s - loss: 12.7088 - mse: 12.7088 - mae: 1.4722 - val_loss: 10.0933 - val_mse: 10.0933 - val_mae: 1.4112 - lr: 1.7314e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 6s - loss: 12.4663 - mse: 12.4663 - mae: 1.4637 - val_loss: 9.9216 - val_mse: 9.9216 - val_mae: 1.5390 - lr: 1.7314e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 6s - loss: 12.4614 - mse: 12.4614 - mae: 1.4639 - val_loss: 10.0041 - val_mse: 10.0041 - val_mae: 1.4062 - lr: 1.7314e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 6s - loss: 12.3864 - mse: 12.3864 - mae: 1.4537 - val_loss: 9.9141 - val_mse: 9.9141 - val_mae: 1.4182 - lr: 1.7314e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 6s - loss: 11.9729 - mse: 11.9729 - mae: 1.4395 - val_loss: 9.8694 - val_mse: 9.8694 - val_mae: 1.4678 - lr: 1.7314e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 6s - loss: 12.0823 - mse: 12.0823 - mae: 1.4344 - val_loss: 9.9767 - val_mse: 9.9767 - val_mae: 1.4620 - lr: 1.7314e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 6s - loss: 11.9336 - mse: 11.9336 - mae: 1.4236 - val_loss: 9.8911 - val_mse: 9.8911 - val_mae: 1.4492 - lr: 1.7314e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 6s - loss: 11.6689 - mse: 11.6689 - mae: 1.4129 - val_loss: 10.4193 - val_mse: 10.4193 - val_mae: 1.4574 - lr: 1.7314e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 6s - loss: 11.6066 - mse: 11.6066 - mae: 1.4063 - val_loss: 10.4701 - val_mse: 10.4701 - val_mae: 1.5212 - lr: 1.7314e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 6s - loss: 11.3685 - mse: 11.3685 - mae: 1.3873 - val_loss: 10.0757 - val_mse: 10.0757 - val_mae: 1.5029 - lr: 1.7314e-04 - 6s/epoch - 6ms/step\n",
            "Score for fold 1: loss of 10.075678825378418\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 6s - loss: 11.0164 - mse: 11.0164 - mae: 1.3959 - val_loss: 10.8804 - val_mse: 10.8804 - val_mae: 1.4560 - lr: 1.7314e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 6s - loss: 10.7993 - mse: 10.7993 - mae: 1.3842 - val_loss: 10.8148 - val_mse: 10.8148 - val_mae: 1.3964 - lr: 1.7314e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 6s - loss: 10.5999 - mse: 10.5999 - mae: 1.3700 - val_loss: 10.9989 - val_mse: 10.9989 - val_mae: 1.4351 - lr: 1.7314e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 6s - loss: 10.2015 - mse: 10.2015 - mae: 1.3513 - val_loss: 11.2209 - val_mse: 11.2209 - val_mae: 1.4701 - lr: 1.7314e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 6s - loss: 10.1568 - mse: 10.1568 - mae: 1.3404 - val_loss: 10.9878 - val_mse: 10.9878 - val_mae: 1.4080 - lr: 1.7314e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 6s - loss: 9.8235 - mse: 9.8235 - mae: 1.3256 - val_loss: 11.5666 - val_mse: 11.5666 - val_mae: 1.4082 - lr: 1.7314e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 6s - loss: 9.5844 - mse: 9.5844 - mae: 1.3092 - val_loss: 11.4702 - val_mse: 11.4702 - val_mae: 1.4032 - lr: 1.7314e-04 - 6s/epoch - 6ms/step\n",
            "Score for fold 2: loss of 11.470233917236328\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 6s - loss: 10.0663 - mse: 10.0663 - mae: 1.3484 - val_loss: 10.7561 - val_mse: 10.7561 - val_mae: 1.3005 - lr: 1.7314e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 6s - loss: 9.7198 - mse: 9.7198 - mae: 1.3249 - val_loss: 9.7828 - val_mse: 9.7828 - val_mae: 1.3116 - lr: 1.7314e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 6s - loss: 9.2593 - mse: 9.2593 - mae: 1.3062 - val_loss: 10.0657 - val_mse: 10.0657 - val_mae: 1.3232 - lr: 1.7314e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 6s - loss: 9.0988 - mse: 9.0988 - mae: 1.2930 - val_loss: 10.1371 - val_mse: 10.1371 - val_mae: 1.3435 - lr: 1.7314e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 6s - loss: 8.8544 - mse: 8.8544 - mae: 1.2773 - val_loss: 10.1957 - val_mse: 10.1957 - val_mae: 1.3417 - lr: 1.7314e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 7s - loss: 8.7391 - mse: 8.7391 - mae: 1.2645 - val_loss: 10.4118 - val_mse: 10.4118 - val_mae: 1.2953 - lr: 1.7314e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 7s - loss: 8.2381 - mse: 8.2381 - mae: 1.2411 - val_loss: 10.9822 - val_mse: 10.9822 - val_mae: 1.4537 - lr: 1.7314e-04 - 7s/epoch - 7ms/step\n",
            "Score for fold 3: loss of 10.982189178466797\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 6s - loss: 8.0138 - mse: 8.0138 - mae: 1.2853 - val_loss: 13.0556 - val_mse: 13.0556 - val_mae: 1.2618 - lr: 1.7314e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 6s - loss: 7.5556 - mse: 7.5556 - mae: 1.2519 - val_loss: 13.3479 - val_mse: 13.3479 - val_mae: 1.1975 - lr: 1.7314e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 7s - loss: 7.3150 - mse: 7.3150 - mae: 1.2407 - val_loss: 13.0194 - val_mse: 13.0194 - val_mae: 1.2698 - lr: 1.7314e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 7s - loss: 6.8801 - mse: 6.8801 - mae: 1.2182 - val_loss: 13.5901 - val_mse: 13.5901 - val_mae: 1.2321 - lr: 1.7314e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 7s - loss: 7.0508 - mse: 7.0508 - mae: 1.2092 - val_loss: 13.4410 - val_mse: 13.4410 - val_mae: 1.3444 - lr: 1.7314e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 7s - loss: 6.6469 - mse: 6.6469 - mae: 1.1881 - val_loss: 13.6368 - val_mse: 13.6368 - val_mae: 1.3416 - lr: 1.7314e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 7s - loss: 6.3337 - mse: 6.3337 - mae: 1.1752 - val_loss: 13.8157 - val_mse: 13.8157 - val_mae: 1.2969 - lr: 1.7314e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 7s - loss: 6.1867 - mse: 6.1867 - mae: 1.1582 - val_loss: 13.9039 - val_mse: 13.9039 - val_mae: 1.3561 - lr: 1.7314e-04 - 7s/epoch - 7ms/step\n",
            "Score for fold 4: loss of 13.903889656066895\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 7s - loss: 8.7271 - mse: 8.7271 - mae: 1.2132 - val_loss: 3.9740 - val_mse: 3.9740 - val_mae: 1.1095 - lr: 1.7314e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 7s - loss: 8.4228 - mse: 8.4228 - mae: 1.1930 - val_loss: 4.4547 - val_mse: 4.4547 - val_mae: 1.1021 - lr: 1.7314e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 7s - loss: 8.0572 - mse: 8.0572 - mae: 1.1725 - val_loss: 3.9191 - val_mse: 3.9191 - val_mae: 1.1425 - lr: 1.7314e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 7s - loss: 7.8105 - mse: 7.8105 - mae: 1.1498 - val_loss: 4.3602 - val_mse: 4.3602 - val_mae: 1.1741 - lr: 1.7314e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 7s - loss: 7.8574 - mse: 7.8574 - mae: 1.1395 - val_loss: 4.4150 - val_mse: 4.4150 - val_mae: 1.1633 - lr: 1.7314e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 7s - loss: 7.0387 - mse: 7.0387 - mae: 1.1197 - val_loss: 4.5432 - val_mse: 4.5432 - val_mae: 1.2091 - lr: 1.7314e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 7s - loss: 7.0377 - mse: 7.0377 - mae: 1.1119 - val_loss: 4.8990 - val_mse: 4.8990 - val_mae: 1.2004 - lr: 1.7314e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 6s - loss: 7.4902 - mse: 7.4902 - mae: 1.0956 - val_loss: 4.5822 - val_mse: 4.5822 - val_mae: 1.1689 - lr: 1.7314e-04 - 6s/epoch - 6ms/step\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-10-12 04:18:01,309]\u001b[0m Finished trial#24 resulted in value: 10.202. Current best value is 10.022 with parameters: {'activation': 'relu', 'num_hidden_layer': 4, 'i': 9, 'learning_rate': 0.0011355512574150934}.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Score for fold 5: loss of 4.582176685333252\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 7s - loss: 14.5822 - mse: 14.5822 - mae: 1.5625 - val_loss: 8.9172 - val_mse: 8.9172 - val_mae: 1.5056 - lr: 1.0734e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 6s - loss: 13.3723 - mse: 13.3723 - mae: 1.4983 - val_loss: 8.4132 - val_mse: 8.4132 - val_mae: 1.4587 - lr: 1.0734e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 6s - loss: 13.0985 - mse: 13.0985 - mae: 1.4777 - val_loss: 8.4988 - val_mse: 8.4988 - val_mae: 1.5632 - lr: 1.0734e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 6s - loss: 12.9296 - mse: 12.9296 - mae: 1.4670 - val_loss: 8.3133 - val_mse: 8.3133 - val_mae: 1.4512 - lr: 1.0734e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 7s - loss: 12.8517 - mse: 12.8517 - mae: 1.4584 - val_loss: 8.3151 - val_mse: 8.3151 - val_mae: 1.4028 - lr: 1.0734e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 7s - loss: 12.7569 - mse: 12.7569 - mae: 1.4499 - val_loss: 8.1725 - val_mse: 8.1725 - val_mae: 1.4629 - lr: 1.0734e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 7s - loss: 12.7103 - mse: 12.7103 - mae: 1.4419 - val_loss: 8.2263 - val_mse: 8.2263 - val_mae: 1.4686 - lr: 1.0734e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 7s - loss: 12.6435 - mse: 12.6435 - mae: 1.4384 - val_loss: 8.1730 - val_mse: 8.1730 - val_mae: 1.4439 - lr: 1.0734e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 6s - loss: 12.4259 - mse: 12.4259 - mae: 1.4303 - val_loss: 8.2440 - val_mse: 8.2440 - val_mae: 1.4698 - lr: 1.0734e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 6s - loss: 12.3024 - mse: 12.3024 - mae: 1.4264 - val_loss: 8.2587 - val_mse: 8.2587 - val_mae: 1.5042 - lr: 1.0734e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 6s - loss: 12.3177 - mse: 12.3177 - mae: 1.4161 - val_loss: 8.1015 - val_mse: 8.1015 - val_mae: 1.4794 - lr: 1.0734e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 7s - loss: 12.1941 - mse: 12.1941 - mae: 1.4092 - val_loss: 8.1468 - val_mse: 8.1468 - val_mae: 1.4600 - lr: 1.0734e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 13/100\n",
            "1000/1000 - 7s - loss: 11.9772 - mse: 11.9772 - mae: 1.4032 - val_loss: 8.6377 - val_mse: 8.6377 - val_mae: 1.4504 - lr: 1.0734e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 14/100\n",
            "1000/1000 - 7s - loss: 11.8794 - mse: 11.8794 - mae: 1.3917 - val_loss: 8.2038 - val_mse: 8.2038 - val_mae: 1.4388 - lr: 1.0734e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 15/100\n",
            "1000/1000 - 7s - loss: 11.7001 - mse: 11.7001 - mae: 1.3861 - val_loss: 8.3605 - val_mse: 8.3605 - val_mae: 1.4091 - lr: 1.0734e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 16/100\n",
            "1000/1000 - 7s - loss: 11.7942 - mse: 11.7942 - mae: 1.3770 - val_loss: 8.0155 - val_mse: 8.0155 - val_mae: 1.4739 - lr: 1.0734e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 17/100\n",
            "1000/1000 - 7s - loss: 11.5661 - mse: 11.5661 - mae: 1.3671 - val_loss: 7.8775 - val_mse: 7.8775 - val_mae: 1.3892 - lr: 1.0734e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 18/100\n",
            "1000/1000 - 6s - loss: 11.3185 - mse: 11.3185 - mae: 1.3602 - val_loss: 8.1150 - val_mse: 8.1150 - val_mae: 1.4444 - lr: 1.0734e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 19/100\n",
            "1000/1000 - 6s - loss: 11.2465 - mse: 11.2465 - mae: 1.3501 - val_loss: 7.8532 - val_mse: 7.8532 - val_mae: 1.4143 - lr: 1.0734e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 20/100\n",
            "1000/1000 - 6s - loss: 11.0895 - mse: 11.0895 - mae: 1.3430 - val_loss: 8.0290 - val_mse: 8.0290 - val_mae: 1.5157 - lr: 1.0734e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 21/100\n",
            "1000/1000 - 6s - loss: 10.9277 - mse: 10.9277 - mae: 1.3306 - val_loss: 7.8294 - val_mse: 7.8294 - val_mae: 1.4605 - lr: 1.0734e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 22/100\n",
            "1000/1000 - 7s - loss: 10.7607 - mse: 10.7607 - mae: 1.3221 - val_loss: 8.5914 - val_mse: 8.5914 - val_mae: 1.5043 - lr: 1.0734e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 23/100\n",
            "1000/1000 - 7s - loss: 10.6763 - mse: 10.6763 - mae: 1.3094 - val_loss: 8.1355 - val_mse: 8.1355 - val_mae: 1.5111 - lr: 1.0734e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 24/100\n",
            "1000/1000 - 7s - loss: 10.5009 - mse: 10.5009 - mae: 1.3015 - val_loss: 8.6336 - val_mse: 8.6336 - val_mae: 1.4481 - lr: 1.0734e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 25/100\n",
            "1000/1000 - 7s - loss: 10.1592 - mse: 10.1592 - mae: 1.2882 - val_loss: 8.4743 - val_mse: 8.4743 - val_mae: 1.4996 - lr: 1.0734e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 26/100\n",
            "1000/1000 - 7s - loss: 9.9272 - mse: 9.9272 - mae: 1.2773 - val_loss: 8.5293 - val_mse: 8.5293 - val_mae: 1.4565 - lr: 1.0734e-04 - 7s/epoch - 7ms/step\n",
            "Score for fold 1: loss of 8.529319763183594\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 7s - loss: 10.4553 - mse: 10.4553 - mae: 1.3295 - val_loss: 8.2228 - val_mse: 8.2228 - val_mae: 1.3031 - lr: 1.0734e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 7s - loss: 10.1603 - mse: 10.1603 - mae: 1.3121 - val_loss: 7.2768 - val_mse: 7.2768 - val_mae: 1.2758 - lr: 1.0734e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 7s - loss: 9.9763 - mse: 9.9763 - mae: 1.2954 - val_loss: 7.1348 - val_mse: 7.1348 - val_mae: 1.3144 - lr: 1.0734e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 7s - loss: 9.7282 - mse: 9.7282 - mae: 1.2837 - val_loss: 10.3605 - val_mse: 10.3605 - val_mae: 1.3007 - lr: 1.0734e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 6s - loss: 9.3639 - mse: 9.3639 - mae: 1.2685 - val_loss: 8.4572 - val_mse: 8.4572 - val_mae: 1.3134 - lr: 1.0734e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 6s - loss: 9.1883 - mse: 9.1883 - mae: 1.2550 - val_loss: 7.4392 - val_mse: 7.4392 - val_mae: 1.3374 - lr: 1.0734e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 6s - loss: 9.1016 - mse: 9.1016 - mae: 1.2455 - val_loss: 8.3123 - val_mse: 8.3123 - val_mae: 1.4400 - lr: 1.0734e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 6s - loss: 8.8927 - mse: 8.8927 - mae: 1.2287 - val_loss: 9.8831 - val_mse: 9.8831 - val_mae: 1.3257 - lr: 1.0734e-04 - 6s/epoch - 6ms/step\n",
            "Score for fold 2: loss of 9.883112907409668\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 6s - loss: 6.9212 - mse: 6.9212 - mae: 1.2653 - val_loss: 15.9898 - val_mse: 15.9898 - val_mae: 1.2050 - lr: 1.0734e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 6s - loss: 6.6030 - mse: 6.6030 - mae: 1.2441 - val_loss: 16.9916 - val_mse: 16.9916 - val_mae: 1.2926 - lr: 1.0734e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 6s - loss: 6.3683 - mse: 6.3683 - mae: 1.2250 - val_loss: 16.8875 - val_mse: 16.8875 - val_mae: 1.3548 - lr: 1.0734e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 6s - loss: 5.9219 - mse: 5.9219 - mae: 1.2079 - val_loss: 17.3358 - val_mse: 17.3358 - val_mae: 1.2559 - lr: 1.0734e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 6s - loss: 5.8086 - mse: 5.8086 - mae: 1.1920 - val_loss: 17.2890 - val_mse: 17.2890 - val_mae: 1.2510 - lr: 1.0734e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 6s - loss: 5.6497 - mse: 5.6497 - mae: 1.1854 - val_loss: 17.4125 - val_mse: 17.4125 - val_mae: 1.2986 - lr: 1.0734e-04 - 6s/epoch - 6ms/step\n",
            "Score for fold 3: loss of 17.4124698638916\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 6s - loss: 9.3964 - mse: 9.3964 - mae: 1.2210 - val_loss: 4.1502 - val_mse: 4.1502 - val_mae: 1.1884 - lr: 1.0734e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 6s - loss: 9.0622 - mse: 9.0622 - mae: 1.1968 - val_loss: 4.1845 - val_mse: 4.1845 - val_mae: 1.1186 - lr: 1.0734e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 6s - loss: 8.4539 - mse: 8.4539 - mae: 1.1851 - val_loss: 4.3813 - val_mse: 4.3813 - val_mae: 1.1529 - lr: 1.0734e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 6s - loss: 8.1684 - mse: 8.1684 - mae: 1.1703 - val_loss: 4.2102 - val_mse: 4.2102 - val_mae: 1.1395 - lr: 1.0734e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 6s - loss: 8.1984 - mse: 8.1984 - mae: 1.1563 - val_loss: 4.5569 - val_mse: 4.5569 - val_mae: 1.2603 - lr: 1.0734e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 6s - loss: 7.7618 - mse: 7.7618 - mae: 1.1431 - val_loss: 4.6606 - val_mse: 4.6606 - val_mae: 1.1995 - lr: 1.0734e-04 - 6s/epoch - 6ms/step\n",
            "Score for fold 4: loss of 4.6606364250183105\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 6s - loss: 7.3389 - mse: 7.3389 - mae: 1.1737 - val_loss: 7.1170 - val_mse: 7.1170 - val_mae: 1.0597 - lr: 1.0734e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 6s - loss: 6.9362 - mse: 6.9362 - mae: 1.1432 - val_loss: 7.6409 - val_mse: 7.6409 - val_mae: 1.1002 - lr: 1.0734e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 6s - loss: 6.6757 - mse: 6.6757 - mae: 1.1335 - val_loss: 7.1075 - val_mse: 7.1075 - val_mae: 1.1947 - lr: 1.0734e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 6s - loss: 6.3741 - mse: 6.3741 - mae: 1.1152 - val_loss: 7.6770 - val_mse: 7.6770 - val_mae: 1.1518 - lr: 1.0734e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 6s - loss: 6.2022 - mse: 6.2022 - mae: 1.1059 - val_loss: 7.6803 - val_mse: 7.6803 - val_mae: 1.1389 - lr: 1.0734e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 6s - loss: 5.8979 - mse: 5.8979 - mae: 1.0862 - val_loss: 8.0020 - val_mse: 8.0020 - val_mae: 1.1357 - lr: 1.0734e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 6s - loss: 6.0092 - mse: 6.0092 - mae: 1.0795 - val_loss: 7.6842 - val_mse: 7.6842 - val_mae: 1.1637 - lr: 1.0734e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 6s - loss: 5.9196 - mse: 5.9196 - mae: 1.0687 - val_loss: 7.9682 - val_mse: 7.9682 - val_mae: 1.1619 - lr: 1.0734e-04 - 6s/epoch - 6ms/step\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-10-12 04:23:54,820]\u001b[0m Finished trial#25 resulted in value: 9.690000000000001. Current best value is 9.690000000000001 with parameters: {'activation': 'relu', 'num_hidden_layer': 4, 'i': 9, 'learning_rate': 0.00010734230668397356}.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Score for fold 5: loss of 7.96821928024292\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 7s - loss: 11.4723 - mse: 11.4723 - mae: 1.5358 - val_loss: 20.6788 - val_mse: 20.6788 - val_mae: 1.5833 - lr: 1.2055e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 6s - loss: 10.5016 - mse: 10.5016 - mae: 1.4870 - val_loss: 20.2835 - val_mse: 20.2835 - val_mae: 1.5116 - lr: 1.2055e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 6s - loss: 10.1819 - mse: 10.1819 - mae: 1.4656 - val_loss: 20.2857 - val_mse: 20.2857 - val_mae: 1.4405 - lr: 1.2055e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 6s - loss: 10.0326 - mse: 10.0326 - mae: 1.4549 - val_loss: 20.2221 - val_mse: 20.2221 - val_mae: 1.5313 - lr: 1.2055e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 6s - loss: 9.9014 - mse: 9.9014 - mae: 1.4457 - val_loss: 19.9376 - val_mse: 19.9376 - val_mae: 1.4584 - lr: 1.2055e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 6s - loss: 9.8577 - mse: 9.8577 - mae: 1.4346 - val_loss: 20.1498 - val_mse: 20.1498 - val_mae: 1.4410 - lr: 1.2055e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 6s - loss: 9.7500 - mse: 9.7500 - mae: 1.4286 - val_loss: 20.1651 - val_mse: 20.1651 - val_mae: 1.4827 - lr: 1.2055e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 6s - loss: 9.6036 - mse: 9.6036 - mae: 1.4193 - val_loss: 20.3258 - val_mse: 20.3258 - val_mae: 1.4760 - lr: 1.2055e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 6s - loss: 9.5441 - mse: 9.5441 - mae: 1.4126 - val_loss: 20.2281 - val_mse: 20.2281 - val_mae: 1.4672 - lr: 1.2055e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 6s - loss: 9.4882 - mse: 9.4882 - mae: 1.4071 - val_loss: 20.2224 - val_mse: 20.2224 - val_mae: 1.4957 - lr: 1.2055e-04 - 6s/epoch - 6ms/step\n",
            "Score for fold 1: loss of 20.222373962402344\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 6s - loss: 12.1976 - mse: 12.1976 - mae: 1.4205 - val_loss: 9.0472 - val_mse: 9.0472 - val_mae: 1.3988 - lr: 1.2055e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 6s - loss: 11.9233 - mse: 11.9233 - mae: 1.4116 - val_loss: 9.2778 - val_mse: 9.2778 - val_mae: 1.3903 - lr: 1.2055e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 6s - loss: 11.9341 - mse: 11.9341 - mae: 1.4031 - val_loss: 9.1316 - val_mse: 9.1316 - val_mae: 1.5302 - lr: 1.2055e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 6s - loss: 11.8287 - mse: 11.8287 - mae: 1.3962 - val_loss: 8.7490 - val_mse: 8.7490 - val_mae: 1.4973 - lr: 1.2055e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 6s - loss: 11.5496 - mse: 11.5496 - mae: 1.3864 - val_loss: 8.7519 - val_mse: 8.7519 - val_mae: 1.4651 - lr: 1.2055e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 6s - loss: 11.2833 - mse: 11.2833 - mae: 1.3709 - val_loss: 9.0894 - val_mse: 9.0894 - val_mae: 1.4569 - lr: 1.2055e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 6s - loss: 11.0478 - mse: 11.0478 - mae: 1.3575 - val_loss: 8.8534 - val_mse: 8.8534 - val_mae: 1.4472 - lr: 1.2055e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 6s - loss: 11.0223 - mse: 11.0223 - mae: 1.3514 - val_loss: 8.7644 - val_mse: 8.7644 - val_mae: 1.4573 - lr: 1.2055e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 6s - loss: 10.7642 - mse: 10.7642 - mae: 1.3369 - val_loss: 8.9972 - val_mse: 8.9972 - val_mae: 1.4787 - lr: 1.2055e-04 - 6s/epoch - 6ms/step\n",
            "Score for fold 2: loss of 8.997177124023438\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 6s - loss: 11.3341 - mse: 11.3341 - mae: 1.3669 - val_loss: 8.0412 - val_mse: 8.0412 - val_mae: 1.3465 - lr: 1.2055e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 6s - loss: 10.9200 - mse: 10.9200 - mae: 1.3513 - val_loss: 7.7406 - val_mse: 7.7406 - val_mae: 1.3526 - lr: 1.2055e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 6s - loss: 10.6226 - mse: 10.6226 - mae: 1.3387 - val_loss: 7.7716 - val_mse: 7.7716 - val_mae: 1.3766 - lr: 1.2055e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 6s - loss: 10.6625 - mse: 10.6625 - mae: 1.3253 - val_loss: 7.9334 - val_mse: 7.9334 - val_mae: 1.3320 - lr: 1.2055e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 6s - loss: 10.2568 - mse: 10.2568 - mae: 1.3086 - val_loss: 8.1402 - val_mse: 8.1402 - val_mae: 1.3644 - lr: 1.2055e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 6s - loss: 10.0409 - mse: 10.0409 - mae: 1.2963 - val_loss: 8.1095 - val_mse: 8.1095 - val_mae: 1.4049 - lr: 1.2055e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 6s - loss: 9.8464 - mse: 9.8464 - mae: 1.2860 - val_loss: 8.2086 - val_mse: 8.2086 - val_mae: 1.3819 - lr: 1.2055e-04 - 6s/epoch - 6ms/step\n",
            "Score for fold 3: loss of 8.208553314208984\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 6s - loss: 9.9052 - mse: 9.9052 - mae: 1.3066 - val_loss: 8.7779 - val_mse: 8.7779 - val_mae: 1.2472 - lr: 1.2055e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 6s - loss: 9.4938 - mse: 9.4938 - mae: 1.2840 - val_loss: 9.2758 - val_mse: 9.2758 - val_mae: 1.2616 - lr: 1.2055e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 6s - loss: 9.0896 - mse: 9.0896 - mae: 1.2690 - val_loss: 9.1225 - val_mse: 9.1225 - val_mae: 1.3006 - lr: 1.2055e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 6s - loss: 9.1253 - mse: 9.1253 - mae: 1.2593 - val_loss: 8.8995 - val_mse: 8.8995 - val_mae: 1.3586 - lr: 1.2055e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 6s - loss: 8.4758 - mse: 8.4758 - mae: 1.2413 - val_loss: 9.4036 - val_mse: 9.4036 - val_mae: 1.2883 - lr: 1.2055e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 6s - loss: 8.8690 - mse: 8.8690 - mae: 1.2269 - val_loss: 9.1699 - val_mse: 9.1699 - val_mae: 1.3189 - lr: 1.2055e-04 - 6s/epoch - 6ms/step\n",
            "Score for fold 4: loss of 9.169883728027344\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 6s - loss: 9.3110 - mse: 9.3110 - mae: 1.2644 - val_loss: 6.2428 - val_mse: 6.2428 - val_mae: 1.1888 - lr: 1.2055e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 6s - loss: 8.7374 - mse: 8.7374 - mae: 1.2505 - val_loss: 6.6298 - val_mse: 6.6298 - val_mae: 1.1988 - lr: 1.2055e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 6s - loss: 8.6025 - mse: 8.6025 - mae: 1.2375 - val_loss: 6.9524 - val_mse: 6.9524 - val_mae: 1.1775 - lr: 1.2055e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 6s - loss: 8.2877 - mse: 8.2877 - mae: 1.2174 - val_loss: 7.6862 - val_mse: 7.6862 - val_mae: 1.2195 - lr: 1.2055e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 6s - loss: 8.1240 - mse: 8.1240 - mae: 1.2016 - val_loss: 6.8411 - val_mse: 6.8411 - val_mae: 1.2008 - lr: 1.2055e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 6s - loss: 7.7342 - mse: 7.7342 - mae: 1.1796 - val_loss: 6.9241 - val_mse: 6.9241 - val_mae: 1.2924 - lr: 1.2055e-04 - 6s/epoch - 6ms/step\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-10-12 04:27:55,138]\u001b[0m Finished trial#26 resulted in value: 10.704. Current best value is 9.690000000000001 with parameters: {'activation': 'relu', 'num_hidden_layer': 4, 'i': 9, 'learning_rate': 0.00010734230668397356}.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Score for fold 5: loss of 6.9241437911987305\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 4s - loss: 14.0358 - mse: 14.0358 - mae: 1.5559 - val_loss: 9.7441 - val_mse: 9.7441 - val_mae: 1.4217 - lr: 0.0021 - 4s/epoch - 4ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 4s - loss: 13.5661 - mse: 13.5661 - mae: 1.5164 - val_loss: 8.7613 - val_mse: 8.7613 - val_mae: 1.4211 - lr: 0.0021 - 4s/epoch - 4ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 4s - loss: 13.5275 - mse: 13.5275 - mae: 1.4966 - val_loss: 8.2690 - val_mse: 8.2690 - val_mae: 1.4335 - lr: 0.0021 - 4s/epoch - 4ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 4s - loss: 13.1394 - mse: 13.1394 - mae: 1.4846 - val_loss: 7.9387 - val_mse: 7.9387 - val_mae: 1.5558 - lr: 0.0021 - 4s/epoch - 4ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 4s - loss: 13.0111 - mse: 13.0111 - mae: 1.4766 - val_loss: 8.8854 - val_mse: 8.8854 - val_mae: 1.4621 - lr: 0.0021 - 4s/epoch - 4ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 4s - loss: 12.9335 - mse: 12.9335 - mae: 1.4698 - val_loss: 8.5248 - val_mse: 8.5248 - val_mae: 1.4918 - lr: 0.0021 - 4s/epoch - 4ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 4s - loss: 12.7720 - mse: 12.7720 - mae: 1.4624 - val_loss: 9.9007 - val_mse: 9.9007 - val_mae: 1.4701 - lr: 0.0021 - 4s/epoch - 4ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 4s - loss: 12.7607 - mse: 12.7607 - mae: 1.4590 - val_loss: 8.8071 - val_mse: 8.8071 - val_mae: 1.4714 - lr: 0.0021 - 4s/epoch - 4ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 4s - loss: 12.5181 - mse: 12.5181 - mae: 1.4624 - val_loss: 7.6773 - val_mse: 7.6773 - val_mae: 1.4094 - lr: 0.0021 - 4s/epoch - 4ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 4s - loss: 12.7096 - mse: 12.7096 - mae: 1.4547 - val_loss: 7.9765 - val_mse: 7.9765 - val_mae: 1.4642 - lr: 0.0021 - 4s/epoch - 4ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 4s - loss: 12.3103 - mse: 12.3103 - mae: 1.4408 - val_loss: 8.5107 - val_mse: 8.5107 - val_mae: 1.4522 - lr: 0.0021 - 4s/epoch - 4ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 4s - loss: 12.3817 - mse: 12.3817 - mae: 1.4382 - val_loss: 7.9988 - val_mse: 7.9988 - val_mae: 1.4492 - lr: 0.0021 - 4s/epoch - 4ms/step\n",
            "Epoch 13/100\n",
            "1000/1000 - 4s - loss: 12.3173 - mse: 12.3173 - mae: 1.4331 - val_loss: 7.6302 - val_mse: 7.6302 - val_mae: 1.4235 - lr: 0.0021 - 4s/epoch - 4ms/step\n",
            "Epoch 14/100\n",
            "1000/1000 - 4s - loss: 12.0247 - mse: 12.0247 - mae: 1.4213 - val_loss: 8.1701 - val_mse: 8.1701 - val_mae: 1.4234 - lr: 0.0021 - 4s/epoch - 4ms/step\n",
            "Epoch 15/100\n",
            "1000/1000 - 4s - loss: 11.9289 - mse: 11.9289 - mae: 1.4189 - val_loss: 8.6974 - val_mse: 8.6974 - val_mae: 1.4387 - lr: 0.0021 - 4s/epoch - 4ms/step\n",
            "Epoch 16/100\n",
            "1000/1000 - 4s - loss: 11.8968 - mse: 11.8968 - mae: 1.4155 - val_loss: 8.5915 - val_mse: 8.5915 - val_mae: 1.4287 - lr: 0.0021 - 4s/epoch - 4ms/step\n",
            "Epoch 17/100\n",
            "1000/1000 - 4s - loss: 11.7296 - mse: 11.7296 - mae: 1.4111 - val_loss: 9.6997 - val_mse: 9.6997 - val_mae: 1.4773 - lr: 0.0021 - 4s/epoch - 4ms/step\n",
            "Epoch 18/100\n",
            "1000/1000 - 4s - loss: 11.6426 - mse: 11.6426 - mae: 1.4062 - val_loss: 9.2213 - val_mse: 9.2213 - val_mae: 1.4313 - lr: 0.0021 - 4s/epoch - 4ms/step\n",
            "Score for fold 1: loss of 9.22128677368164\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 4s - loss: 11.9483 - mse: 11.9483 - mae: 1.4005 - val_loss: 6.8364 - val_mse: 6.8364 - val_mae: 1.4004 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 4s - loss: 11.3422 - mse: 11.3422 - mae: 1.3858 - val_loss: 6.8929 - val_mse: 6.8929 - val_mae: 1.3557 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 4s - loss: 11.2223 - mse: 11.2223 - mae: 1.3755 - val_loss: 7.0034 - val_mse: 7.0034 - val_mae: 1.3546 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 4s - loss: 10.9959 - mse: 10.9959 - mae: 1.3657 - val_loss: 7.2222 - val_mse: 7.2222 - val_mae: 1.3384 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 4s - loss: 11.6751 - mse: 11.6751 - mae: 1.3638 - val_loss: 7.0992 - val_mse: 7.0992 - val_mae: 1.3689 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 4s - loss: 10.6902 - mse: 10.6902 - mae: 1.3568 - val_loss: 6.8616 - val_mse: 6.8616 - val_mae: 1.3566 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Score for fold 2: loss of 6.861634731292725\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 4s - loss: 8.2396 - mse: 8.2396 - mae: 1.3615 - val_loss: 16.8182 - val_mse: 16.8182 - val_mae: 1.3526 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 4s - loss: 8.1356 - mse: 8.1356 - mae: 1.3513 - val_loss: 16.6133 - val_mse: 16.6133 - val_mae: 1.3649 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 4s - loss: 7.9789 - mse: 7.9789 - mae: 1.3436 - val_loss: 16.8950 - val_mse: 16.8950 - val_mae: 1.3471 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 4s - loss: 7.7353 - mse: 7.7353 - mae: 1.3316 - val_loss: 16.9708 - val_mse: 16.9708 - val_mae: 1.3594 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 4s - loss: 7.5458 - mse: 7.5458 - mae: 1.3238 - val_loss: 16.8996 - val_mse: 16.8996 - val_mae: 1.3955 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 4s - loss: 7.4973 - mse: 7.4973 - mae: 1.3180 - val_loss: 17.2372 - val_mse: 17.2372 - val_mae: 1.3747 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 4s - loss: 7.5513 - mse: 7.5513 - mae: 1.3158 - val_loss: 17.1148 - val_mse: 17.1148 - val_mae: 1.3493 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Score for fold 3: loss of 17.11484718322754\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 4s - loss: 9.6672 - mse: 9.6672 - mae: 1.3355 - val_loss: 10.4316 - val_mse: 10.4316 - val_mae: 1.3359 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 4s - loss: 8.9271 - mse: 8.9271 - mae: 1.3171 - val_loss: 10.6350 - val_mse: 10.6350 - val_mae: 1.3278 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 4s - loss: 8.8322 - mse: 8.8322 - mae: 1.3079 - val_loss: 10.9304 - val_mse: 10.9304 - val_mae: 1.3519 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 4s - loss: 8.7222 - mse: 8.7222 - mae: 1.3052 - val_loss: 11.1542 - val_mse: 11.1542 - val_mae: 1.3507 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 4s - loss: 8.6845 - mse: 8.6845 - mae: 1.2974 - val_loss: 11.0326 - val_mse: 11.0326 - val_mae: 1.3162 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 4s - loss: 8.6607 - mse: 8.6607 - mae: 1.2906 - val_loss: 11.1747 - val_mse: 11.1747 - val_mae: 1.3612 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Score for fold 4: loss of 11.174654006958008\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 4s - loss: 9.4308 - mse: 9.4308 - mae: 1.3099 - val_loss: 7.1716 - val_mse: 7.1716 - val_mae: 1.3374 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 4s - loss: 9.2724 - mse: 9.2724 - mae: 1.2981 - val_loss: 7.4809 - val_mse: 7.4809 - val_mae: 1.2831 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 4s - loss: 9.2602 - mse: 9.2602 - mae: 1.2906 - val_loss: 7.5151 - val_mse: 7.5151 - val_mae: 1.3188 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 4s - loss: 9.1268 - mse: 9.1268 - mae: 1.2838 - val_loss: 7.6461 - val_mse: 7.6461 - val_mae: 1.3293 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 4s - loss: 9.0290 - mse: 9.0290 - mae: 1.2758 - val_loss: 7.5975 - val_mse: 7.5975 - val_mae: 1.3763 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 4s - loss: 8.9648 - mse: 8.9648 - mae: 1.2768 - val_loss: 8.0321 - val_mse: 8.0321 - val_mae: 1.3538 - lr: 0.0010 - 4s/epoch - 4ms/step\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-10-12 04:30:38,137]\u001b[0m Finished trial#27 resulted in value: 10.478. Current best value is 9.690000000000001 with parameters: {'activation': 'relu', 'num_hidden_layer': 4, 'i': 9, 'learning_rate': 0.00010734230668397356}.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Score for fold 5: loss of 8.03214168548584\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 7s - loss: 13.7400 - mse: 13.7400 - mae: 1.6545 - val_loss: 15.3959 - val_mse: 15.3959 - val_mae: 1.5777 - lr: 1.0061e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 6s - loss: 13.0261 - mse: 13.0261 - mae: 1.5584 - val_loss: 15.3200 - val_mse: 15.3200 - val_mae: 1.5364 - lr: 1.0061e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 6s - loss: 12.7311 - mse: 12.7311 - mae: 1.5510 - val_loss: 15.2042 - val_mse: 15.2042 - val_mae: 1.5818 - lr: 1.0061e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 6s - loss: 12.9153 - mse: 12.9153 - mae: 1.5511 - val_loss: 15.3227 - val_mse: 15.3227 - val_mae: 1.5505 - lr: 1.0061e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 6s - loss: 12.8917 - mse: 12.8917 - mae: 1.5481 - val_loss: 15.2203 - val_mse: 15.2203 - val_mae: 1.5488 - lr: 1.0061e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 6s - loss: 12.8657 - mse: 12.8657 - mae: 1.5459 - val_loss: 15.1165 - val_mse: 15.1165 - val_mae: 1.5969 - lr: 1.0061e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 6s - loss: 12.9630 - mse: 12.9630 - mae: 1.5462 - val_loss: 15.4198 - val_mse: 15.4198 - val_mae: 1.6337 - lr: 1.0061e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 6s - loss: 12.8299 - mse: 12.8299 - mae: 1.5473 - val_loss: 15.1845 - val_mse: 15.1845 - val_mae: 1.5622 - lr: 1.0061e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 6s - loss: 12.8282 - mse: 12.8282 - mae: 1.5426 - val_loss: 15.1831 - val_mse: 15.1831 - val_mae: 1.5548 - lr: 1.0061e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 6s - loss: 12.7916 - mse: 12.7916 - mae: 1.5430 - val_loss: 15.1459 - val_mse: 15.1459 - val_mae: 1.5589 - lr: 1.0061e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 6s - loss: 12.9459 - mse: 12.9459 - mae: 1.5495 - val_loss: 15.3067 - val_mse: 15.3067 - val_mae: 1.5425 - lr: 1.0061e-04 - 6s/epoch - 6ms/step\n",
            "Score for fold 1: loss of 15.306736946105957\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 7s - loss: 14.4035 - mse: 14.4035 - mae: 1.5575 - val_loss: 8.6393 - val_mse: 8.6393 - val_mae: 1.5115 - lr: 1.0061e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 7s - loss: 14.4046 - mse: 14.4046 - mae: 1.5567 - val_loss: 8.6549 - val_mse: 8.6549 - val_mae: 1.5064 - lr: 1.0061e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 7s - loss: 14.3383 - mse: 14.3383 - mae: 1.5541 - val_loss: 8.5830 - val_mse: 8.5830 - val_mae: 1.5493 - lr: 1.0061e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 7s - loss: 14.4248 - mse: 14.4248 - mae: 1.5592 - val_loss: 8.8197 - val_mse: 8.8197 - val_mae: 1.4935 - lr: 1.0061e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 7s - loss: 14.3530 - mse: 14.3530 - mae: 1.5570 - val_loss: 8.6382 - val_mse: 8.6382 - val_mae: 1.4989 - lr: 1.0061e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 7s - loss: 14.4173 - mse: 14.4173 - mae: 1.5537 - val_loss: 8.5934 - val_mse: 8.5934 - val_mae: 1.5389 - lr: 1.0061e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 7s - loss: 14.3779 - mse: 14.3779 - mae: 1.5618 - val_loss: 8.6489 - val_mse: 8.6489 - val_mae: 1.5161 - lr: 1.0061e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 7s - loss: 14.3127 - mse: 14.3127 - mae: 1.5539 - val_loss: 8.8184 - val_mse: 8.8184 - val_mae: 1.5580 - lr: 1.0061e-04 - 7s/epoch - 7ms/step\n",
            "Score for fold 2: loss of 8.818365097045898\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 7s - loss: 12.3273 - mse: 12.3273 - mae: 1.5527 - val_loss: 16.8788 - val_mse: 16.8788 - val_mae: 1.5316 - lr: 1.0061e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 7s - loss: 12.3064 - mse: 12.3064 - mae: 1.5497 - val_loss: 16.8753 - val_mse: 16.8753 - val_mae: 1.5501 - lr: 1.0061e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 7s - loss: 12.3241 - mse: 12.3241 - mae: 1.5511 - val_loss: 16.8706 - val_mse: 16.8706 - val_mae: 1.5320 - lr: 1.0061e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 6s - loss: 12.2791 - mse: 12.2791 - mae: 1.5491 - val_loss: 16.8576 - val_mse: 16.8576 - val_mae: 1.5556 - lr: 1.0061e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 6s - loss: 12.2922 - mse: 12.2922 - mae: 1.5494 - val_loss: 16.8767 - val_mse: 16.8767 - val_mae: 1.5369 - lr: 1.0061e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 6s - loss: 12.3226 - mse: 12.3226 - mae: 1.5507 - val_loss: 16.9336 - val_mse: 16.9336 - val_mae: 1.5062 - lr: 1.0061e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 6s - loss: 12.2896 - mse: 12.2896 - mae: 1.5478 - val_loss: 16.8139 - val_mse: 16.8139 - val_mae: 1.5364 - lr: 1.0061e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 6s - loss: 12.2655 - mse: 12.2655 - mae: 1.5490 - val_loss: 16.9127 - val_mse: 16.9127 - val_mae: 1.5706 - lr: 1.0061e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 6s - loss: 12.3333 - mse: 12.3333 - mae: 1.5476 - val_loss: 16.8892 - val_mse: 16.8892 - val_mae: 1.5675 - lr: 1.0061e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 6s - loss: 12.2787 - mse: 12.2787 - mae: 1.5474 - val_loss: 16.8638 - val_mse: 16.8638 - val_mae: 1.5376 - lr: 1.0061e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 6s - loss: 12.2864 - mse: 12.2864 - mae: 1.5502 - val_loss: 16.8162 - val_mse: 16.8162 - val_mae: 1.5606 - lr: 1.0061e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 6s - loss: 12.2961 - mse: 12.2961 - mae: 1.5491 - val_loss: 16.9039 - val_mse: 16.9039 - val_mae: 1.5217 - lr: 1.0061e-04 - 6s/epoch - 6ms/step\n",
            "Score for fold 3: loss of 16.903850555419922\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 6s - loss: 12.9902 - mse: 12.9902 - mae: 1.5420 - val_loss: 14.0122 - val_mse: 14.0122 - val_mae: 1.5468 - lr: 1.0061e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 6s - loss: 13.0312 - mse: 13.0312 - mae: 1.5416 - val_loss: 13.9212 - val_mse: 13.9212 - val_mae: 1.5451 - lr: 1.0061e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 6s - loss: 12.9837 - mse: 12.9837 - mae: 1.5399 - val_loss: 13.9996 - val_mse: 13.9996 - val_mae: 1.5817 - lr: 1.0061e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 6s - loss: 13.1023 - mse: 13.1023 - mae: 1.5427 - val_loss: 14.0423 - val_mse: 14.0423 - val_mae: 1.5812 - lr: 1.0061e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 6s - loss: 13.0557 - mse: 13.0557 - mae: 1.5434 - val_loss: 14.1301 - val_mse: 14.1301 - val_mae: 1.5690 - lr: 1.0061e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 6s - loss: 13.1356 - mse: 13.1356 - mae: 1.5418 - val_loss: 14.0433 - val_mse: 14.0433 - val_mae: 1.5805 - lr: 1.0061e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 6s - loss: 13.0729 - mse: 13.0729 - mae: 1.5382 - val_loss: 13.9495 - val_mse: 13.9495 - val_mae: 1.5369 - lr: 1.0061e-04 - 6s/epoch - 6ms/step\n",
            "Score for fold 4: loss of 13.94946002960205\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 6s - loss: 13.6479 - mse: 13.6479 - mae: 1.5434 - val_loss: 11.4216 - val_mse: 11.4216 - val_mae: 1.5430 - lr: 1.0061e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 6s - loss: 13.6719 - mse: 13.6719 - mae: 1.5447 - val_loss: 11.3766 - val_mse: 11.3766 - val_mae: 1.5469 - lr: 1.0061e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 6s - loss: 13.6953 - mse: 13.6953 - mae: 1.5461 - val_loss: 11.5659 - val_mse: 11.5659 - val_mae: 1.5266 - lr: 1.0061e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 7s - loss: 13.6814 - mse: 13.6814 - mae: 1.5421 - val_loss: 11.3879 - val_mse: 11.3879 - val_mae: 1.5388 - lr: 1.0061e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 7s - loss: 13.7147 - mse: 13.7147 - mae: 1.5471 - val_loss: 11.4270 - val_mse: 11.4270 - val_mae: 1.5317 - lr: 1.0061e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 7s - loss: 13.5922 - mse: 13.5922 - mae: 1.5427 - val_loss: 11.6340 - val_mse: 11.6340 - val_mae: 1.5427 - lr: 1.0061e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 7s - loss: 13.6088 - mse: 13.6088 - mae: 1.5397 - val_loss: 11.3965 - val_mse: 11.3965 - val_mae: 1.5611 - lr: 1.0061e-04 - 7s/epoch - 7ms/step\n",
            "Score for fold 5: loss of 11.396496772766113\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-10-12 04:35:30,539]\u001b[0m Finished trial#28 resulted in value: 13.276000000000002. Current best value is 9.690000000000001 with parameters: {'activation': 'relu', 'num_hidden_layer': 4, 'i': 9, 'learning_rate': 0.00010734230668397356}.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 5s - loss: 14.0260 - mse: 14.0260 - mae: 1.5454 - val_loss: 9.5144 - val_mse: 9.5144 - val_mae: 1.5371 - lr: 0.0011 - 5s/epoch - 5ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 4s - loss: 13.0431 - mse: 13.0431 - mae: 1.5065 - val_loss: 8.8804 - val_mse: 8.8804 - val_mae: 1.4901 - lr: 0.0011 - 4s/epoch - 4ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 4s - loss: 13.0381 - mse: 13.0381 - mae: 1.4918 - val_loss: 9.0809 - val_mse: 9.0809 - val_mae: 1.4380 - lr: 0.0011 - 4s/epoch - 4ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 4s - loss: 13.0215 - mse: 13.0215 - mae: 1.4862 - val_loss: 9.1575 - val_mse: 9.1575 - val_mae: 1.4245 - lr: 0.0011 - 4s/epoch - 4ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 4s - loss: 12.7368 - mse: 12.7368 - mae: 1.4776 - val_loss: 8.8513 - val_mse: 8.8513 - val_mae: 1.4292 - lr: 0.0011 - 4s/epoch - 4ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 4s - loss: 12.7980 - mse: 12.7980 - mae: 1.4674 - val_loss: 8.7708 - val_mse: 8.7708 - val_mae: 1.4339 - lr: 0.0011 - 4s/epoch - 4ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 4s - loss: 12.7736 - mse: 12.7736 - mae: 1.4635 - val_loss: 8.7273 - val_mse: 8.7273 - val_mae: 1.4676 - lr: 0.0011 - 4s/epoch - 4ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 4s - loss: 12.6944 - mse: 12.6944 - mae: 1.4564 - val_loss: 8.9954 - val_mse: 8.9954 - val_mae: 1.4549 - lr: 0.0011 - 4s/epoch - 4ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 4s - loss: 12.4970 - mse: 12.4970 - mae: 1.4482 - val_loss: 8.7117 - val_mse: 8.7117 - val_mae: 1.4565 - lr: 0.0011 - 4s/epoch - 4ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 4s - loss: 12.0900 - mse: 12.0900 - mae: 1.4395 - val_loss: 8.7553 - val_mse: 8.7553 - val_mae: 1.3815 - lr: 0.0011 - 4s/epoch - 4ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 4s - loss: 11.8427 - mse: 11.8427 - mae: 1.4300 - val_loss: 9.1052 - val_mse: 9.1052 - val_mae: 1.3887 - lr: 0.0011 - 4s/epoch - 4ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 4s - loss: 11.9603 - mse: 11.9603 - mae: 1.4257 - val_loss: 8.9997 - val_mse: 8.9997 - val_mae: 1.4934 - lr: 0.0011 - 4s/epoch - 4ms/step\n",
            "Epoch 13/100\n",
            "1000/1000 - 4s - loss: 11.6509 - mse: 11.6509 - mae: 1.4163 - val_loss: 9.1091 - val_mse: 9.1091 - val_mae: 1.4306 - lr: 0.0011 - 4s/epoch - 4ms/step\n",
            "Epoch 14/100\n",
            "1000/1000 - 4s - loss: 11.9235 - mse: 11.9235 - mae: 1.4202 - val_loss: 9.1013 - val_mse: 9.1013 - val_mae: 1.4321 - lr: 0.0011 - 4s/epoch - 4ms/step\n",
            "Score for fold 1: loss of 9.101319313049316\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 4s - loss: 11.2928 - mse: 11.2928 - mae: 1.4221 - val_loss: 10.2251 - val_mse: 10.2251 - val_mae: 1.4011 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 4s - loss: 11.0444 - mse: 11.0444 - mae: 1.4120 - val_loss: 9.9218 - val_mse: 9.9218 - val_mae: 1.4339 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 4s - loss: 11.0851 - mse: 11.0851 - mae: 1.4023 - val_loss: 10.2126 - val_mse: 10.2126 - val_mae: 1.4780 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 4s - loss: 10.8339 - mse: 10.8339 - mae: 1.3971 - val_loss: 10.3793 - val_mse: 10.3793 - val_mae: 1.4105 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 4s - loss: 10.8462 - mse: 10.8462 - mae: 1.3915 - val_loss: 10.5700 - val_mse: 10.5700 - val_mae: 1.4270 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 4s - loss: 10.5102 - mse: 10.5102 - mae: 1.3798 - val_loss: 10.9057 - val_mse: 10.9057 - val_mae: 1.4669 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 4s - loss: 10.4162 - mse: 10.4162 - mae: 1.3757 - val_loss: 10.5212 - val_mse: 10.5212 - val_mae: 1.4801 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Score for fold 2: loss of 10.52121353149414\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 4s - loss: 9.9392 - mse: 9.9392 - mae: 1.3820 - val_loss: 12.5284 - val_mse: 12.5284 - val_mae: 1.4490 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 4s - loss: 10.0391 - mse: 10.0391 - mae: 1.3828 - val_loss: 12.5621 - val_mse: 12.5621 - val_mae: 1.4097 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 4s - loss: 9.5850 - mse: 9.5850 - mae: 1.3682 - val_loss: 12.5444 - val_mse: 12.5444 - val_mae: 1.4091 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 4s - loss: 9.4012 - mse: 9.4012 - mae: 1.3661 - val_loss: 12.8029 - val_mse: 12.8029 - val_mae: 1.4278 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 4s - loss: 9.4289 - mse: 9.4289 - mae: 1.3556 - val_loss: 12.7518 - val_mse: 12.7518 - val_mae: 1.4236 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 4s - loss: 9.3590 - mse: 9.3590 - mae: 1.3501 - val_loss: 12.7504 - val_mse: 12.7504 - val_mae: 1.3955 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Score for fold 3: loss of 12.750397682189941\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 4s - loss: 10.9125 - mse: 10.9125 - mae: 1.3745 - val_loss: 6.9686 - val_mse: 6.9686 - val_mae: 1.3688 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 4s - loss: 10.9976 - mse: 10.9976 - mae: 1.3654 - val_loss: 6.7163 - val_mse: 6.7163 - val_mae: 1.2762 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 4s - loss: 10.4369 - mse: 10.4369 - mae: 1.3515 - val_loss: 7.2207 - val_mse: 7.2207 - val_mae: 1.2966 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 4s - loss: 10.3878 - mse: 10.3878 - mae: 1.3406 - val_loss: 7.1781 - val_mse: 7.1781 - val_mae: 1.3803 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 4s - loss: 10.2234 - mse: 10.2234 - mae: 1.3350 - val_loss: 6.9652 - val_mse: 6.9652 - val_mae: 1.3014 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 4s - loss: 10.2263 - mse: 10.2263 - mae: 1.3250 - val_loss: 7.1005 - val_mse: 7.1005 - val_mae: 1.3246 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 4s - loss: 10.0924 - mse: 10.0924 - mae: 1.3185 - val_loss: 7.1542 - val_mse: 7.1542 - val_mae: 1.3837 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Score for fold 4: loss of 7.154160976409912\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 4s - loss: 8.7199 - mse: 8.7199 - mae: 1.3376 - val_loss: 13.1888 - val_mse: 13.1888 - val_mae: 1.2919 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 4s - loss: 8.4333 - mse: 8.4333 - mae: 1.3237 - val_loss: 13.9755 - val_mse: 13.9755 - val_mae: 1.3475 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 4s - loss: 8.2574 - mse: 8.2574 - mae: 1.3186 - val_loss: 13.8567 - val_mse: 13.8567 - val_mae: 1.3488 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 4s - loss: 8.1928 - mse: 8.1928 - mae: 1.3103 - val_loss: 13.8005 - val_mse: 13.8005 - val_mae: 1.3422 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 4s - loss: 8.1001 - mse: 8.1001 - mae: 1.3096 - val_loss: 14.7276 - val_mse: 14.7276 - val_mae: 1.3190 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 4s - loss: 7.8935 - mse: 7.8935 - mae: 1.2968 - val_loss: 14.0749 - val_mse: 14.0749 - val_mae: 1.3962 - lr: 0.0010 - 4s/epoch - 4ms/step\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-10-12 04:38:19,211]\u001b[0m Finished trial#29 resulted in value: 10.718. Current best value is 9.690000000000001 with parameters: {'activation': 'relu', 'num_hidden_layer': 4, 'i': 9, 'learning_rate': 0.00010734230668397356}.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Score for fold 5: loss of 14.074930191040039\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 7s - loss: 13.9832 - mse: 13.9832 - mae: 1.5475 - val_loss: 9.6863 - val_mse: 9.6863 - val_mae: 1.4634 - lr: 1.5529e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 6s - loss: 13.1161 - mse: 13.1161 - mae: 1.4896 - val_loss: 9.1708 - val_mse: 9.1708 - val_mae: 1.4823 - lr: 1.5529e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 6s - loss: 12.9407 - mse: 12.9407 - mae: 1.4676 - val_loss: 9.0242 - val_mse: 9.0242 - val_mae: 1.4739 - lr: 1.5529e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 6s - loss: 12.7192 - mse: 12.7192 - mae: 1.4613 - val_loss: 9.0620 - val_mse: 9.0620 - val_mae: 1.5046 - lr: 1.5529e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 6s - loss: 12.5402 - mse: 12.5402 - mae: 1.4485 - val_loss: 9.3716 - val_mse: 9.3716 - val_mae: 1.4667 - lr: 1.5529e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 6s - loss: 12.5862 - mse: 12.5862 - mae: 1.4432 - val_loss: 9.0068 - val_mse: 9.0068 - val_mae: 1.5225 - lr: 1.5529e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 6s - loss: 12.3927 - mse: 12.3927 - mae: 1.4373 - val_loss: 8.9381 - val_mse: 8.9381 - val_mae: 1.4700 - lr: 1.5529e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 6s - loss: 12.1753 - mse: 12.1753 - mae: 1.4273 - val_loss: 8.8171 - val_mse: 8.8171 - val_mae: 1.4887 - lr: 1.5529e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 6s - loss: 12.1265 - mse: 12.1265 - mae: 1.4138 - val_loss: 9.1156 - val_mse: 9.1156 - val_mae: 1.5020 - lr: 1.5529e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 7s - loss: 11.9001 - mse: 11.9001 - mae: 1.4091 - val_loss: 8.8130 - val_mse: 8.8130 - val_mae: 1.5272 - lr: 1.5529e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 8s - loss: 11.7449 - mse: 11.7449 - mae: 1.3989 - val_loss: 9.1616 - val_mse: 9.1616 - val_mae: 1.5260 - lr: 1.5529e-04 - 8s/epoch - 8ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 8s - loss: 11.6002 - mse: 11.6002 - mae: 1.3879 - val_loss: 10.3694 - val_mse: 10.3694 - val_mae: 1.5798 - lr: 1.5529e-04 - 8s/epoch - 8ms/step\n",
            "Epoch 13/100\n",
            "1000/1000 - 8s - loss: 11.5213 - mse: 11.5213 - mae: 1.3757 - val_loss: 9.2504 - val_mse: 9.2504 - val_mae: 1.4698 - lr: 1.5529e-04 - 8s/epoch - 8ms/step\n",
            "Epoch 14/100\n",
            "1000/1000 - 8s - loss: 11.4499 - mse: 11.4499 - mae: 1.3667 - val_loss: 9.0866 - val_mse: 9.0866 - val_mae: 1.4327 - lr: 1.5529e-04 - 8s/epoch - 8ms/step\n",
            "Epoch 15/100\n",
            "1000/1000 - 8s - loss: 11.1161 - mse: 11.1161 - mae: 1.3497 - val_loss: 11.1977 - val_mse: 11.1977 - val_mae: 1.5884 - lr: 1.5529e-04 - 8s/epoch - 8ms/step\n",
            "Score for fold 1: loss of 11.19775104522705\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 8s - loss: 11.1891 - mse: 11.1891 - mae: 1.3797 - val_loss: 7.9061 - val_mse: 7.9061 - val_mae: 1.3488 - lr: 1.5529e-04 - 8s/epoch - 8ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 7s - loss: 11.2707 - mse: 11.2707 - mae: 1.3689 - val_loss: 8.0290 - val_mse: 8.0290 - val_mae: 1.4311 - lr: 1.5529e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 7s - loss: 11.1326 - mse: 11.1326 - mae: 1.3556 - val_loss: 7.9740 - val_mse: 7.9740 - val_mae: 1.4106 - lr: 1.5529e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 7s - loss: 10.6659 - mse: 10.6659 - mae: 1.3459 - val_loss: 8.0656 - val_mse: 8.0656 - val_mae: 1.4144 - lr: 1.5529e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 6s - loss: 10.6351 - mse: 10.6351 - mae: 1.3236 - val_loss: 9.4494 - val_mse: 9.4494 - val_mae: 1.5441 - lr: 1.5529e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 6s - loss: 10.1564 - mse: 10.1564 - mae: 1.3139 - val_loss: 8.2955 - val_mse: 8.2955 - val_mae: 1.3887 - lr: 1.5529e-04 - 6s/epoch - 6ms/step\n",
            "Score for fold 2: loss of 8.295488357543945\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 6s - loss: 8.8832 - mse: 8.8832 - mae: 1.3373 - val_loss: 14.0274 - val_mse: 14.0274 - val_mae: 1.2659 - lr: 1.5529e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 6s - loss: 8.4921 - mse: 8.4921 - mae: 1.3142 - val_loss: 13.7976 - val_mse: 13.7976 - val_mae: 1.3593 - lr: 1.5529e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 6s - loss: 8.3821 - mse: 8.3821 - mae: 1.2950 - val_loss: 15.5010 - val_mse: 15.5010 - val_mae: 1.3472 - lr: 1.5529e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 6s - loss: 8.1989 - mse: 8.1989 - mae: 1.2792 - val_loss: 14.9188 - val_mse: 14.9188 - val_mae: 1.3761 - lr: 1.5529e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 6s - loss: 7.9163 - mse: 7.9163 - mae: 1.2656 - val_loss: 14.6197 - val_mse: 14.6197 - val_mae: 1.2936 - lr: 1.5529e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 6s - loss: 7.4734 - mse: 7.4734 - mae: 1.2486 - val_loss: 14.3421 - val_mse: 14.3421 - val_mae: 1.3393 - lr: 1.5529e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 6s - loss: 7.2495 - mse: 7.2495 - mae: 1.2335 - val_loss: 15.1706 - val_mse: 15.1706 - val_mae: 1.3803 - lr: 1.5529e-04 - 6s/epoch - 6ms/step\n",
            "Score for fold 3: loss of 15.17062759399414\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 6s - loss: 8.6845 - mse: 8.6845 - mae: 1.2623 - val_loss: 8.9608 - val_mse: 8.9608 - val_mae: 1.1851 - lr: 1.5529e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 6s - loss: 8.3300 - mse: 8.3300 - mae: 1.2446 - val_loss: 9.6428 - val_mse: 9.6428 - val_mae: 1.2350 - lr: 1.5529e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 6s - loss: 8.3852 - mse: 8.3852 - mae: 1.2240 - val_loss: 9.8615 - val_mse: 9.8615 - val_mae: 1.2311 - lr: 1.5529e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 6s - loss: 7.6738 - mse: 7.6738 - mae: 1.2036 - val_loss: 9.6533 - val_mse: 9.6533 - val_mae: 1.2841 - lr: 1.5529e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 6s - loss: 7.2364 - mse: 7.2364 - mae: 1.1825 - val_loss: 9.7976 - val_mse: 9.7976 - val_mae: 1.3286 - lr: 1.5529e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 6s - loss: 7.5493 - mse: 7.5493 - mae: 1.1770 - val_loss: 10.3462 - val_mse: 10.3462 - val_mae: 1.2262 - lr: 1.5529e-04 - 6s/epoch - 6ms/step\n",
            "Score for fold 4: loss of 10.346189498901367\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 6s - loss: 8.1009 - mse: 8.1009 - mae: 1.2147 - val_loss: 6.9613 - val_mse: 6.9613 - val_mae: 1.1093 - lr: 1.5529e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 6s - loss: 7.7435 - mse: 7.7435 - mae: 1.1906 - val_loss: 7.6834 - val_mse: 7.6834 - val_mae: 1.1822 - lr: 1.5529e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 6s - loss: 7.3596 - mse: 7.3596 - mae: 1.1690 - val_loss: 7.1626 - val_mse: 7.1626 - val_mae: 1.1613 - lr: 1.5529e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 6s - loss: 7.1816 - mse: 7.1816 - mae: 1.1531 - val_loss: 8.0170 - val_mse: 8.0170 - val_mae: 1.1625 - lr: 1.5529e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 6s - loss: 6.7034 - mse: 6.7034 - mae: 1.1379 - val_loss: 7.7517 - val_mse: 7.7517 - val_mae: 1.2254 - lr: 1.5529e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 6s - loss: 6.6131 - mse: 6.6131 - mae: 1.1253 - val_loss: 9.1530 - val_mse: 9.1530 - val_mae: 1.2705 - lr: 1.5529e-04 - 6s/epoch - 6ms/step\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-10-12 04:42:45,835]\u001b[0m Finished trial#30 resulted in value: 10.834. Current best value is 9.690000000000001 with parameters: {'activation': 'relu', 'num_hidden_layer': 4, 'i': 9, 'learning_rate': 0.00010734230668397356}.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Score for fold 5: loss of 9.153019905090332\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 7s - loss: 13.9836 - mse: 13.9836 - mae: 1.5497 - val_loss: 10.1184 - val_mse: 10.1184 - val_mae: 1.5780 - lr: 1.8821e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 6s - loss: 13.0305 - mse: 13.0305 - mae: 1.4895 - val_loss: 9.2117 - val_mse: 9.2117 - val_mae: 1.5847 - lr: 1.8821e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 6s - loss: 12.8033 - mse: 12.8033 - mae: 1.4723 - val_loss: 9.0314 - val_mse: 9.0314 - val_mae: 1.4071 - lr: 1.8821e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 6s - loss: 12.8085 - mse: 12.8085 - mae: 1.4608 - val_loss: 8.9265 - val_mse: 8.9265 - val_mae: 1.4150 - lr: 1.8821e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 6s - loss: 12.5358 - mse: 12.5358 - mae: 1.4526 - val_loss: 8.8031 - val_mse: 8.8031 - val_mae: 1.4574 - lr: 1.8821e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 6s - loss: 12.6476 - mse: 12.6476 - mae: 1.4474 - val_loss: 8.9699 - val_mse: 8.9699 - val_mae: 1.4313 - lr: 1.8821e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 6s - loss: 12.2987 - mse: 12.2987 - mae: 1.4353 - val_loss: 9.1419 - val_mse: 9.1419 - val_mae: 1.4360 - lr: 1.8821e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 6s - loss: 12.2577 - mse: 12.2577 - mae: 1.4288 - val_loss: 8.9117 - val_mse: 8.9117 - val_mae: 1.4459 - lr: 1.8821e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 6s - loss: 12.2574 - mse: 12.2574 - mae: 1.4193 - val_loss: 8.7950 - val_mse: 8.7950 - val_mae: 1.3942 - lr: 1.8821e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 6s - loss: 11.9204 - mse: 11.9204 - mae: 1.4109 - val_loss: 8.7676 - val_mse: 8.7676 - val_mae: 1.4545 - lr: 1.8821e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 6s - loss: 11.6647 - mse: 11.6647 - mae: 1.3965 - val_loss: 9.0137 - val_mse: 9.0137 - val_mae: 1.4285 - lr: 1.8821e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 6s - loss: 11.6512 - mse: 11.6512 - mae: 1.3867 - val_loss: 9.1076 - val_mse: 9.1076 - val_mae: 1.4957 - lr: 1.8821e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 13/100\n",
            "1000/1000 - 6s - loss: 11.3363 - mse: 11.3363 - mae: 1.3749 - val_loss: 9.2091 - val_mse: 9.2091 - val_mae: 1.4297 - lr: 1.8821e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 14/100\n",
            "1000/1000 - 6s - loss: 11.0556 - mse: 11.0556 - mae: 1.3626 - val_loss: 9.0382 - val_mse: 9.0382 - val_mae: 1.4172 - lr: 1.8821e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 15/100\n",
            "1000/1000 - 6s - loss: 10.9266 - mse: 10.9266 - mae: 1.3462 - val_loss: 9.0048 - val_mse: 9.0048 - val_mae: 1.4467 - lr: 1.8821e-04 - 6s/epoch - 6ms/step\n",
            "Score for fold 1: loss of 9.00479793548584\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 6s - loss: 10.6874 - mse: 10.6874 - mae: 1.3705 - val_loss: 9.4264 - val_mse: 9.4264 - val_mae: 1.3223 - lr: 1.8821e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 6s - loss: 10.7686 - mse: 10.7686 - mae: 1.3609 - val_loss: 9.3065 - val_mse: 9.3065 - val_mae: 1.4211 - lr: 1.8821e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 6s - loss: 10.5314 - mse: 10.5314 - mae: 1.3461 - val_loss: 10.1331 - val_mse: 10.1331 - val_mae: 1.4174 - lr: 1.8821e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 6s - loss: 10.0001 - mse: 10.0001 - mae: 1.3239 - val_loss: 9.5384 - val_mse: 9.5384 - val_mae: 1.4259 - lr: 1.8821e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 6s - loss: 9.9647 - mse: 9.9647 - mae: 1.3160 - val_loss: 9.7544 - val_mse: 9.7544 - val_mae: 1.4322 - lr: 1.8821e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 6s - loss: 9.6663 - mse: 9.6663 - mae: 1.2983 - val_loss: 10.1441 - val_mse: 10.1441 - val_mae: 1.3907 - lr: 1.8821e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 6s - loss: 9.5110 - mse: 9.5110 - mae: 1.2793 - val_loss: 10.1877 - val_mse: 10.1877 - val_mae: 1.4281 - lr: 1.8821e-04 - 6s/epoch - 6ms/step\n",
            "Score for fold 2: loss of 10.187676429748535\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 6s - loss: 9.9639 - mse: 9.9639 - mae: 1.3158 - val_loss: 7.3982 - val_mse: 7.3982 - val_mae: 1.2816 - lr: 1.8821e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 6s - loss: 9.6235 - mse: 9.6235 - mae: 1.2975 - val_loss: 7.1544 - val_mse: 7.1544 - val_mae: 1.3610 - lr: 1.8821e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 6s - loss: 9.3577 - mse: 9.3577 - mae: 1.2766 - val_loss: 7.8601 - val_mse: 7.8601 - val_mae: 1.3192 - lr: 1.8821e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 6s - loss: 8.9123 - mse: 8.9123 - mae: 1.2615 - val_loss: 7.8058 - val_mse: 7.8058 - val_mae: 1.2538 - lr: 1.8821e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 6s - loss: 8.7860 - mse: 8.7860 - mae: 1.2492 - val_loss: 7.9486 - val_mse: 7.9486 - val_mae: 1.2957 - lr: 1.8821e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 6s - loss: 8.4398 - mse: 8.4398 - mae: 1.2271 - val_loss: 8.3876 - val_mse: 8.3876 - val_mae: 1.3297 - lr: 1.8821e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 6s - loss: 8.5358 - mse: 8.5358 - mae: 1.2233 - val_loss: 8.4829 - val_mse: 8.4829 - val_mae: 1.3206 - lr: 1.8821e-04 - 6s/epoch - 6ms/step\n",
            "Score for fold 3: loss of 8.482884407043457\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 6s - loss: 6.3362 - mse: 6.3362 - mae: 1.2402 - val_loss: 16.3875 - val_mse: 16.3875 - val_mae: 1.2765 - lr: 1.8821e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 6s - loss: 6.0661 - mse: 6.0661 - mae: 1.2145 - val_loss: 15.8394 - val_mse: 15.8394 - val_mae: 1.2530 - lr: 1.8821e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 6s - loss: 5.7082 - mse: 5.7082 - mae: 1.1914 - val_loss: 17.1430 - val_mse: 17.1430 - val_mae: 1.2846 - lr: 1.8821e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 6s - loss: 5.6316 - mse: 5.6316 - mae: 1.1751 - val_loss: 16.5464 - val_mse: 16.5464 - val_mae: 1.2419 - lr: 1.8821e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 6s - loss: 5.2877 - mse: 5.2877 - mae: 1.1642 - val_loss: 18.6763 - val_mse: 18.6763 - val_mae: 1.3028 - lr: 1.8821e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 6s - loss: 6.2207 - mse: 6.2207 - mae: 1.1577 - val_loss: 16.7052 - val_mse: 16.7052 - val_mae: 1.3327 - lr: 1.8821e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 6s - loss: 4.9178 - mse: 4.9178 - mae: 1.1359 - val_loss: 16.5975 - val_mse: 16.5975 - val_mae: 1.3527 - lr: 1.8821e-04 - 6s/epoch - 6ms/step\n",
            "Score for fold 4: loss of 16.597454071044922\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 6s - loss: 8.5657 - mse: 8.5657 - mae: 1.2020 - val_loss: 3.7799 - val_mse: 3.7799 - val_mae: 1.0693 - lr: 1.8821e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 6s - loss: 8.1415 - mse: 8.1415 - mae: 1.1749 - val_loss: 3.8358 - val_mse: 3.8358 - val_mae: 1.1102 - lr: 1.8821e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 6s - loss: 7.8682 - mse: 7.8682 - mae: 1.1622 - val_loss: 4.4649 - val_mse: 4.4649 - val_mae: 1.1202 - lr: 1.8821e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 6s - loss: 7.4334 - mse: 7.4334 - mae: 1.1433 - val_loss: 4.2142 - val_mse: 4.2142 - val_mae: 1.1831 - lr: 1.8821e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 6s - loss: 7.2781 - mse: 7.2781 - mae: 1.1318 - val_loss: 4.4086 - val_mse: 4.4086 - val_mae: 1.1292 - lr: 1.8821e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 6s - loss: 6.9373 - mse: 6.9373 - mae: 1.1140 - val_loss: 4.4175 - val_mse: 4.4175 - val_mae: 1.1688 - lr: 1.8821e-04 - 6s/epoch - 6ms/step\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-10-12 04:47:10,274]\u001b[0m Finished trial#31 resulted in value: 9.738. Current best value is 9.690000000000001 with parameters: {'activation': 'relu', 'num_hidden_layer': 4, 'i': 9, 'learning_rate': 0.00010734230668397356}.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Score for fold 5: loss of 4.41749906539917\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 7s - loss: 13.5635 - mse: 13.5635 - mae: 1.5434 - val_loss: 11.6412 - val_mse: 11.6412 - val_mae: 1.5258 - lr: 2.1548e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 6s - loss: 12.5178 - mse: 12.5178 - mae: 1.4905 - val_loss: 11.4880 - val_mse: 11.4880 - val_mae: 1.4946 - lr: 2.1548e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 6s - loss: 12.2676 - mse: 12.2676 - mae: 1.4689 - val_loss: 11.4981 - val_mse: 11.4981 - val_mae: 1.4920 - lr: 2.1548e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 6s - loss: 12.3316 - mse: 12.3316 - mae: 1.4630 - val_loss: 11.8158 - val_mse: 11.8158 - val_mae: 1.4062 - lr: 2.1548e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 6s - loss: 12.1401 - mse: 12.1401 - mae: 1.4559 - val_loss: 11.3919 - val_mse: 11.3919 - val_mae: 1.4442 - lr: 2.1548e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 6s - loss: 11.9218 - mse: 11.9218 - mae: 1.4394 - val_loss: 11.8452 - val_mse: 11.8452 - val_mae: 1.4900 - lr: 2.1548e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 6s - loss: 11.8396 - mse: 11.8396 - mae: 1.4405 - val_loss: 11.2920 - val_mse: 11.2920 - val_mae: 1.4232 - lr: 2.1548e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 6s - loss: 11.6538 - mse: 11.6538 - mae: 1.4266 - val_loss: 11.2753 - val_mse: 11.2753 - val_mae: 1.4202 - lr: 2.1548e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 6s - loss: 11.4067 - mse: 11.4067 - mae: 1.4111 - val_loss: 11.8886 - val_mse: 11.8886 - val_mae: 1.4767 - lr: 2.1548e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 6s - loss: 11.2065 - mse: 11.2065 - mae: 1.4042 - val_loss: 11.1672 - val_mse: 11.1672 - val_mae: 1.4784 - lr: 2.1548e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 6s - loss: 10.9106 - mse: 10.9106 - mae: 1.3916 - val_loss: 11.3070 - val_mse: 11.3070 - val_mae: 1.4563 - lr: 2.1548e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 6s - loss: 10.9413 - mse: 10.9413 - mae: 1.3834 - val_loss: 11.4067 - val_mse: 11.4067 - val_mae: 1.4614 - lr: 2.1548e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 13/100\n",
            "1000/1000 - 6s - loss: 10.6517 - mse: 10.6517 - mae: 1.3668 - val_loss: 11.1199 - val_mse: 11.1199 - val_mae: 1.4887 - lr: 2.1548e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 14/100\n",
            "1000/1000 - 6s - loss: 10.5331 - mse: 10.5331 - mae: 1.3539 - val_loss: 11.3828 - val_mse: 11.3828 - val_mae: 1.5437 - lr: 2.1548e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 15/100\n",
            "1000/1000 - 6s - loss: 10.1551 - mse: 10.1551 - mae: 1.3414 - val_loss: 11.9161 - val_mse: 11.9161 - val_mae: 1.4518 - lr: 2.1548e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 16/100\n",
            "1000/1000 - 6s - loss: 9.7812 - mse: 9.7812 - mae: 1.3254 - val_loss: 12.7262 - val_mse: 12.7262 - val_mae: 1.5540 - lr: 2.1548e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 17/100\n",
            "1000/1000 - 6s - loss: 9.7964 - mse: 9.7964 - mae: 1.3127 - val_loss: 12.5131 - val_mse: 12.5131 - val_mae: 1.4619 - lr: 2.1548e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 18/100\n",
            "1000/1000 - 6s - loss: 9.4448 - mse: 9.4448 - mae: 1.2941 - val_loss: 12.0556 - val_mse: 12.0556 - val_mae: 1.5277 - lr: 2.1548e-04 - 6s/epoch - 6ms/step\n",
            "Score for fold 1: loss of 12.055580139160156\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 6s - loss: 7.9728 - mse: 7.9728 - mae: 1.3460 - val_loss: 17.7414 - val_mse: 17.7414 - val_mae: 1.2903 - lr: 2.1548e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 6s - loss: 7.8404 - mse: 7.8404 - mae: 1.3305 - val_loss: 18.3972 - val_mse: 18.3972 - val_mae: 1.2786 - lr: 2.1548e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 6s - loss: 7.6706 - mse: 7.6706 - mae: 1.3134 - val_loss: 18.2379 - val_mse: 18.2379 - val_mae: 1.3397 - lr: 2.1548e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 6s - loss: 7.3470 - mse: 7.3470 - mae: 1.3006 - val_loss: 18.3386 - val_mse: 18.3386 - val_mae: 1.3249 - lr: 2.1548e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 6s - loss: 6.9395 - mse: 6.9395 - mae: 1.2813 - val_loss: 18.5644 - val_mse: 18.5644 - val_mae: 1.3426 - lr: 2.1548e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 6s - loss: 6.5798 - mse: 6.5798 - mae: 1.2647 - val_loss: 19.3841 - val_mse: 19.3841 - val_mae: 1.3491 - lr: 2.1548e-04 - 6s/epoch - 6ms/step\n",
            "Score for fold 2: loss of 19.38408088684082\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 6s - loss: 9.6764 - mse: 9.6764 - mae: 1.2977 - val_loss: 6.3959 - val_mse: 6.3959 - val_mae: 1.2504 - lr: 2.1548e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 6s - loss: 9.4153 - mse: 9.4153 - mae: 1.2795 - val_loss: 6.8684 - val_mse: 6.8684 - val_mae: 1.2719 - lr: 2.1548e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 6s - loss: 9.2307 - mse: 9.2307 - mae: 1.2539 - val_loss: 7.9863 - val_mse: 7.9863 - val_mae: 1.3330 - lr: 2.1548e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 6s - loss: 8.7274 - mse: 8.7274 - mae: 1.2368 - val_loss: 7.7251 - val_mse: 7.7251 - val_mae: 1.2941 - lr: 2.1548e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 6s - loss: 8.5177 - mse: 8.5177 - mae: 1.2246 - val_loss: 7.3569 - val_mse: 7.3569 - val_mae: 1.2776 - lr: 2.1548e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 6s - loss: 8.3546 - mse: 8.3546 - mae: 1.1998 - val_loss: 7.6477 - val_mse: 7.6477 - val_mae: 1.3388 - lr: 2.1548e-04 - 6s/epoch - 6ms/step\n",
            "Score for fold 3: loss of 7.647726535797119\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 6s - loss: 9.6116 - mse: 9.6116 - mae: 1.2459 - val_loss: 4.4859 - val_mse: 4.4859 - val_mae: 1.1225 - lr: 2.1548e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 6s - loss: 9.0566 - mse: 9.0566 - mae: 1.2218 - val_loss: 4.1347 - val_mse: 4.1347 - val_mae: 1.1836 - lr: 2.1548e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 6s - loss: 8.5128 - mse: 8.5128 - mae: 1.2113 - val_loss: 4.4789 - val_mse: 4.4789 - val_mae: 1.3551 - lr: 2.1548e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 6s - loss: 8.2018 - mse: 8.2018 - mae: 1.1827 - val_loss: 4.3492 - val_mse: 4.3492 - val_mae: 1.1834 - lr: 2.1548e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 6s - loss: 8.0921 - mse: 8.0921 - mae: 1.1781 - val_loss: 5.0646 - val_mse: 5.0646 - val_mae: 1.1626 - lr: 2.1548e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 6s - loss: 7.8680 - mse: 7.8680 - mae: 1.1559 - val_loss: 4.8480 - val_mse: 4.8480 - val_mae: 1.2015 - lr: 2.1548e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 6s - loss: 7.6171 - mse: 7.6171 - mae: 1.1458 - val_loss: 5.2924 - val_mse: 5.2924 - val_mae: 1.1934 - lr: 2.1548e-04 - 6s/epoch - 6ms/step\n",
            "Score for fold 4: loss of 5.292437553405762\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 6s - loss: 7.8059 - mse: 7.8059 - mae: 1.1767 - val_loss: 4.7324 - val_mse: 4.7324 - val_mae: 1.1149 - lr: 2.1548e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 6s - loss: 7.4700 - mse: 7.4700 - mae: 1.1485 - val_loss: 4.2608 - val_mse: 4.2608 - val_mae: 1.1342 - lr: 2.1548e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 6s - loss: 7.2122 - mse: 7.2122 - mae: 1.1320 - val_loss: 3.6013 - val_mse: 3.6013 - val_mae: 1.1841 - lr: 2.1548e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 6s - loss: 7.0318 - mse: 7.0318 - mae: 1.1223 - val_loss: 4.3286 - val_mse: 4.3286 - val_mae: 1.1755 - lr: 2.1548e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 6s - loss: 6.9116 - mse: 6.9116 - mae: 1.1044 - val_loss: 3.9693 - val_mse: 3.9693 - val_mae: 1.1491 - lr: 2.1548e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 6s - loss: 6.5242 - mse: 6.5242 - mae: 1.0914 - val_loss: 4.1017 - val_mse: 4.1017 - val_mae: 1.1343 - lr: 2.1548e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 6s - loss: 6.8183 - mse: 6.8183 - mae: 1.0825 - val_loss: 4.3918 - val_mse: 4.3918 - val_mae: 1.1714 - lr: 2.1548e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 6s - loss: 6.3563 - mse: 6.3563 - mae: 1.0674 - val_loss: 4.8220 - val_mse: 4.8220 - val_mae: 1.2045 - lr: 2.1548e-04 - 6s/epoch - 6ms/step\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-10-12 04:51:49,148]\u001b[0m Finished trial#32 resulted in value: 9.84. Current best value is 9.690000000000001 with parameters: {'activation': 'relu', 'num_hidden_layer': 4, 'i': 9, 'learning_rate': 0.00010734230668397356}.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Score for fold 5: loss of 4.8220014572143555\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 7s - loss: 13.1376 - mse: 13.1376 - mae: 1.5320 - val_loss: 13.4323 - val_mse: 13.4323 - val_mae: 1.4917 - lr: 2.1149e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 6s - loss: 12.1257 - mse: 12.1257 - mae: 1.4795 - val_loss: 14.0332 - val_mse: 14.0332 - val_mae: 1.5175 - lr: 2.1149e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 6s - loss: 11.8378 - mse: 11.8378 - mae: 1.4603 - val_loss: 13.2728 - val_mse: 13.2728 - val_mae: 1.4232 - lr: 2.1149e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 6s - loss: 11.8320 - mse: 11.8320 - mae: 1.4517 - val_loss: 13.1881 - val_mse: 13.1881 - val_mae: 1.5125 - lr: 2.1149e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 6s - loss: 11.6573 - mse: 11.6573 - mae: 1.4472 - val_loss: 13.2631 - val_mse: 13.2631 - val_mae: 1.4144 - lr: 2.1149e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 6s - loss: 11.5645 - mse: 11.5645 - mae: 1.4394 - val_loss: 12.9580 - val_mse: 12.9580 - val_mae: 1.4746 - lr: 2.1149e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 6s - loss: 11.2890 - mse: 11.2890 - mae: 1.4312 - val_loss: 13.2038 - val_mse: 13.2038 - val_mae: 1.4808 - lr: 2.1149e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 6s - loss: 11.2480 - mse: 11.2480 - mae: 1.4252 - val_loss: 13.0030 - val_mse: 13.0030 - val_mae: 1.4922 - lr: 2.1149e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 6s - loss: 11.0776 - mse: 11.0776 - mae: 1.4125 - val_loss: 12.8709 - val_mse: 12.8709 - val_mae: 1.4762 - lr: 2.1149e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 6s - loss: 10.9233 - mse: 10.9233 - mae: 1.4004 - val_loss: 12.8064 - val_mse: 12.8064 - val_mae: 1.4596 - lr: 2.1149e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 6s - loss: 10.7607 - mse: 10.7607 - mae: 1.3881 - val_loss: 13.8579 - val_mse: 13.8579 - val_mae: 1.4351 - lr: 2.1149e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 6s - loss: 10.4967 - mse: 10.4967 - mae: 1.3730 - val_loss: 13.5711 - val_mse: 13.5711 - val_mae: 1.4634 - lr: 2.1149e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 13/100\n",
            "1000/1000 - 6s - loss: 10.3617 - mse: 10.3617 - mae: 1.3631 - val_loss: 13.9949 - val_mse: 13.9949 - val_mae: 1.4388 - lr: 2.1149e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 14/100\n",
            "1000/1000 - 6s - loss: 10.0072 - mse: 10.0072 - mae: 1.3520 - val_loss: 12.7907 - val_mse: 12.7907 - val_mae: 1.4909 - lr: 2.1149e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 15/100\n",
            "1000/1000 - 6s - loss: 9.8431 - mse: 9.8431 - mae: 1.3339 - val_loss: 13.0416 - val_mse: 13.0416 - val_mae: 1.5132 - lr: 2.1149e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 16/100\n",
            "1000/1000 - 6s - loss: 9.9101 - mse: 9.9101 - mae: 1.3207 - val_loss: 13.2207 - val_mse: 13.2207 - val_mae: 1.5752 - lr: 2.1149e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 17/100\n",
            "1000/1000 - 6s - loss: 9.6007 - mse: 9.6007 - mae: 1.3147 - val_loss: 14.0665 - val_mse: 14.0665 - val_mae: 1.4869 - lr: 2.1149e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 18/100\n",
            "1000/1000 - 6s - loss: 9.3066 - mse: 9.3066 - mae: 1.2998 - val_loss: 12.7071 - val_mse: 12.7071 - val_mae: 1.4874 - lr: 2.1149e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 19/100\n",
            "1000/1000 - 6s - loss: 8.9884 - mse: 8.9884 - mae: 1.2857 - val_loss: 13.6367 - val_mse: 13.6367 - val_mae: 1.5058 - lr: 2.1149e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 20/100\n",
            "1000/1000 - 6s - loss: 8.6989 - mse: 8.6989 - mae: 1.2723 - val_loss: 13.8723 - val_mse: 13.8723 - val_mae: 1.5408 - lr: 2.1149e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 21/100\n",
            "1000/1000 - 6s - loss: 8.6641 - mse: 8.6641 - mae: 1.2560 - val_loss: 14.3851 - val_mse: 14.3851 - val_mae: 1.5558 - lr: 2.1149e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 22/100\n",
            "1000/1000 - 6s - loss: 8.3268 - mse: 8.3268 - mae: 1.2404 - val_loss: 13.2351 - val_mse: 13.2351 - val_mae: 1.5919 - lr: 2.1149e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 23/100\n",
            "1000/1000 - 6s - loss: 8.0664 - mse: 8.0664 - mae: 1.2232 - val_loss: 14.4104 - val_mse: 14.4104 - val_mae: 1.5407 - lr: 2.1149e-04 - 6s/epoch - 6ms/step\n",
            "Score for fold 1: loss of 14.410367012023926\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 6s - loss: 9.7700 - mse: 9.7700 - mae: 1.3031 - val_loss: 7.2071 - val_mse: 7.2071 - val_mae: 1.2556 - lr: 2.1149e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 6s - loss: 9.5185 - mse: 9.5185 - mae: 1.2935 - val_loss: 7.5994 - val_mse: 7.5994 - val_mae: 1.2654 - lr: 2.1149e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 6s - loss: 9.2688 - mse: 9.2688 - mae: 1.2664 - val_loss: 7.5675 - val_mse: 7.5675 - val_mae: 1.2431 - lr: 2.1149e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 6s - loss: 8.9949 - mse: 8.9949 - mae: 1.2505 - val_loss: 7.8095 - val_mse: 7.8095 - val_mae: 1.3119 - lr: 2.1149e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 6s - loss: 8.6623 - mse: 8.6623 - mae: 1.2400 - val_loss: 8.0107 - val_mse: 8.0107 - val_mae: 1.4189 - lr: 2.1149e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 6s - loss: 8.5303 - mse: 8.5303 - mae: 1.2214 - val_loss: 8.4741 - val_mse: 8.4741 - val_mae: 1.4215 - lr: 2.1149e-04 - 6s/epoch - 6ms/step\n",
            "Score for fold 2: loss of 8.474133491516113\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 6s - loss: 9.2607 - mse: 9.2607 - mae: 1.2682 - val_loss: 4.7001 - val_mse: 4.7001 - val_mae: 1.1927 - lr: 2.1149e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 6s - loss: 8.9654 - mse: 8.9654 - mae: 1.2418 - val_loss: 5.9451 - val_mse: 5.9451 - val_mae: 1.4228 - lr: 2.1149e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 6s - loss: 8.6738 - mse: 8.6738 - mae: 1.2302 - val_loss: 7.7972 - val_mse: 7.7972 - val_mae: 1.2223 - lr: 2.1149e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 6s - loss: 8.4753 - mse: 8.4753 - mae: 1.2148 - val_loss: 4.8471 - val_mse: 4.8471 - val_mae: 1.2568 - lr: 2.1149e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 6s - loss: 8.1054 - mse: 8.1054 - mae: 1.1949 - val_loss: 5.0091 - val_mse: 5.0091 - val_mae: 1.2151 - lr: 2.1149e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 6s - loss: 7.8918 - mse: 7.8918 - mae: 1.1791 - val_loss: 5.2557 - val_mse: 5.2557 - val_mae: 1.1996 - lr: 2.1149e-04 - 6s/epoch - 6ms/step\n",
            "Score for fold 3: loss of 5.255712985992432\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 6s - loss: 6.7323 - mse: 6.7323 - mae: 1.2055 - val_loss: 10.8103 - val_mse: 10.8103 - val_mae: 1.1540 - lr: 2.1149e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 6s - loss: 5.8720 - mse: 5.8720 - mae: 1.1701 - val_loss: 11.1452 - val_mse: 11.1452 - val_mae: 1.1346 - lr: 2.1149e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 6s - loss: 6.6195 - mse: 6.6195 - mae: 1.1632 - val_loss: 11.1140 - val_mse: 11.1140 - val_mae: 1.1342 - lr: 2.1149e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 6s - loss: 5.6474 - mse: 5.6474 - mae: 1.1409 - val_loss: 11.7830 - val_mse: 11.7830 - val_mae: 1.1446 - lr: 2.1149e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 6s - loss: 5.4630 - mse: 5.4630 - mae: 1.1300 - val_loss: 11.5643 - val_mse: 11.5643 - val_mae: 1.1690 - lr: 2.1149e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 6s - loss: 5.4397 - mse: 5.4397 - mae: 1.1117 - val_loss: 11.8757 - val_mse: 11.8757 - val_mae: 1.2022 - lr: 2.1149e-04 - 6s/epoch - 6ms/step\n",
            "Score for fold 4: loss of 11.875727653503418\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 6s - loss: 7.3117 - mse: 7.3117 - mae: 1.1448 - val_loss: 5.2808 - val_mse: 5.2808 - val_mae: 1.0396 - lr: 2.1149e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 6s - loss: 6.6046 - mse: 6.6046 - mae: 1.1258 - val_loss: 5.4679 - val_mse: 5.4679 - val_mae: 1.1467 - lr: 2.1149e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 6s - loss: 6.0699 - mse: 6.0699 - mae: 1.1084 - val_loss: 5.6892 - val_mse: 5.6892 - val_mae: 1.0695 - lr: 2.1149e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 6s - loss: 6.0594 - mse: 6.0594 - mae: 1.0898 - val_loss: 5.8320 - val_mse: 5.8320 - val_mae: 1.1092 - lr: 2.1149e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 6s - loss: 5.8872 - mse: 5.8872 - mae: 1.0830 - val_loss: 5.7217 - val_mse: 5.7217 - val_mae: 1.1407 - lr: 2.1149e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 6s - loss: 5.5706 - mse: 5.5706 - mae: 1.0646 - val_loss: 5.9627 - val_mse: 5.9627 - val_mae: 1.1702 - lr: 2.1149e-04 - 6s/epoch - 6ms/step\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-10-12 04:56:40,419]\u001b[0m Finished trial#33 resulted in value: 9.196000000000002. Current best value is 9.196000000000002 with parameters: {'activation': 'relu', 'num_hidden_layer': 4, 'i': 9, 'learning_rate': 0.00021148832678344413}.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Score for fold 5: loss of 5.962705135345459\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 12s - loss: 14.0885 - mse: 14.0885 - mae: 1.5465 - val_loss: 9.8681 - val_mse: 9.8681 - val_mae: 1.4508 - lr: 1.9279e-04 - 12s/epoch - 12ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 11s - loss: 12.9660 - mse: 12.9660 - mae: 1.4892 - val_loss: 10.4044 - val_mse: 10.4044 - val_mae: 1.4934 - lr: 1.9279e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 11s - loss: 13.0259 - mse: 13.0259 - mae: 1.4783 - val_loss: 9.6801 - val_mse: 9.6801 - val_mae: 1.4690 - lr: 1.9279e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 12s - loss: 12.5638 - mse: 12.5638 - mae: 1.4654 - val_loss: 9.2825 - val_mse: 9.2825 - val_mae: 1.4802 - lr: 1.9279e-04 - 12s/epoch - 12ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 12s - loss: 12.5703 - mse: 12.5703 - mae: 1.4570 - val_loss: 9.5276 - val_mse: 9.5276 - val_mae: 1.3867 - lr: 1.9279e-04 - 12s/epoch - 12ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 12s - loss: 12.3235 - mse: 12.3235 - mae: 1.4443 - val_loss: 9.4169 - val_mse: 9.4169 - val_mae: 1.4721 - lr: 1.9279e-04 - 12s/epoch - 12ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 12s - loss: 12.0158 - mse: 12.0158 - mae: 1.4328 - val_loss: 9.8520 - val_mse: 9.8520 - val_mae: 1.4546 - lr: 1.9279e-04 - 12s/epoch - 12ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 11s - loss: 12.0258 - mse: 12.0258 - mae: 1.4264 - val_loss: 9.2024 - val_mse: 9.2024 - val_mae: 1.4768 - lr: 1.9279e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 11s - loss: 11.9380 - mse: 11.9380 - mae: 1.4178 - val_loss: 10.0795 - val_mse: 10.0795 - val_mae: 1.5305 - lr: 1.9279e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 11s - loss: 11.4621 - mse: 11.4621 - mae: 1.3978 - val_loss: 9.3029 - val_mse: 9.3029 - val_mae: 1.5051 - lr: 1.9279e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 11s - loss: 11.2198 - mse: 11.2198 - mae: 1.3796 - val_loss: 9.3395 - val_mse: 9.3395 - val_mae: 1.5776 - lr: 1.9279e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 11s - loss: 10.9545 - mse: 10.9545 - mae: 1.3664 - val_loss: 10.4851 - val_mse: 10.4851 - val_mae: 1.4619 - lr: 1.9279e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 13/100\n",
            "1000/1000 - 11s - loss: 10.7862 - mse: 10.7862 - mae: 1.3516 - val_loss: 9.3512 - val_mse: 9.3512 - val_mae: 1.4971 - lr: 1.9279e-04 - 11s/epoch - 11ms/step\n",
            "Score for fold 1: loss of 9.35124683380127\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 12s - loss: 10.6695 - mse: 10.6695 - mae: 1.3782 - val_loss: 9.9424 - val_mse: 9.9424 - val_mae: 1.3770 - lr: 1.9279e-04 - 12s/epoch - 12ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 12s - loss: 10.0435 - mse: 10.0435 - mae: 1.3498 - val_loss: 9.8326 - val_mse: 9.8326 - val_mae: 1.4740 - lr: 1.9279e-04 - 12s/epoch - 12ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 12s - loss: 9.7649 - mse: 9.7649 - mae: 1.3336 - val_loss: 11.4822 - val_mse: 11.4822 - val_mae: 1.3954 - lr: 1.9279e-04 - 12s/epoch - 12ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 12s - loss: 9.5591 - mse: 9.5591 - mae: 1.3126 - val_loss: 11.1248 - val_mse: 11.1248 - val_mae: 1.4275 - lr: 1.9279e-04 - 12s/epoch - 12ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 12s - loss: 9.3438 - mse: 9.3438 - mae: 1.2982 - val_loss: 9.9319 - val_mse: 9.9319 - val_mae: 1.3677 - lr: 1.9279e-04 - 12s/epoch - 12ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 11s - loss: 9.2216 - mse: 9.2216 - mae: 1.2739 - val_loss: 10.6206 - val_mse: 10.6206 - val_mae: 1.4310 - lr: 1.9279e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 11s - loss: 8.8387 - mse: 8.8387 - mae: 1.2640 - val_loss: 10.2211 - val_mse: 10.2211 - val_mae: 1.4535 - lr: 1.9279e-04 - 11s/epoch - 11ms/step\n",
            "Score for fold 2: loss of 10.22105884552002\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 11s - loss: 10.0403 - mse: 10.0403 - mae: 1.3079 - val_loss: 5.2271 - val_mse: 5.2271 - val_mae: 1.2017 - lr: 1.9279e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 11s - loss: 9.5472 - mse: 9.5472 - mae: 1.2805 - val_loss: 5.2066 - val_mse: 5.2066 - val_mae: 1.2160 - lr: 1.9279e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 12s - loss: 9.1926 - mse: 9.1926 - mae: 1.2589 - val_loss: 5.1084 - val_mse: 5.1084 - val_mae: 1.3098 - lr: 1.9279e-04 - 12s/epoch - 12ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 11s - loss: 8.8385 - mse: 8.8385 - mae: 1.2397 - val_loss: 5.3001 - val_mse: 5.3001 - val_mae: 1.3397 - lr: 1.9279e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 11s - loss: 8.4309 - mse: 8.4309 - mae: 1.2147 - val_loss: 6.2710 - val_mse: 6.2710 - val_mae: 1.3050 - lr: 1.9279e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 11s - loss: 8.1234 - mse: 8.1234 - mae: 1.1932 - val_loss: 5.5213 - val_mse: 5.5213 - val_mae: 1.2705 - lr: 1.9279e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 12s - loss: 7.9766 - mse: 7.9766 - mae: 1.1801 - val_loss: 5.7849 - val_mse: 5.7849 - val_mae: 1.3399 - lr: 1.9279e-04 - 12s/epoch - 12ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 12s - loss: 7.6700 - mse: 7.6700 - mae: 1.1654 - val_loss: 6.1337 - val_mse: 6.1337 - val_mae: 1.4259 - lr: 1.9279e-04 - 12s/epoch - 12ms/step\n",
            "Score for fold 3: loss of 6.1336541175842285\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 12s - loss: 7.6294 - mse: 7.6294 - mae: 1.2133 - val_loss: 7.0171 - val_mse: 7.0171 - val_mae: 1.1014 - lr: 1.9279e-04 - 12s/epoch - 12ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 12s - loss: 7.2700 - mse: 7.2700 - mae: 1.1769 - val_loss: 6.9806 - val_mse: 6.9806 - val_mae: 1.1498 - lr: 1.9279e-04 - 12s/epoch - 12ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 12s - loss: 6.7229 - mse: 6.7229 - mae: 1.1543 - val_loss: 6.8969 - val_mse: 6.8969 - val_mae: 1.1414 - lr: 1.9279e-04 - 12s/epoch - 12ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 11s - loss: 6.6876 - mse: 6.6876 - mae: 1.1406 - val_loss: 7.5277 - val_mse: 7.5277 - val_mae: 1.1805 - lr: 1.9279e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 11s - loss: 6.4457 - mse: 6.4457 - mae: 1.1243 - val_loss: 7.4183 - val_mse: 7.4183 - val_mae: 1.1858 - lr: 1.9279e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 11s - loss: 6.3468 - mse: 6.3468 - mae: 1.0991 - val_loss: 6.8909 - val_mse: 6.8909 - val_mae: 1.2412 - lr: 1.9279e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 11s - loss: 5.7328 - mse: 5.7328 - mae: 1.0793 - val_loss: 7.8261 - val_mse: 7.8261 - val_mae: 1.1836 - lr: 1.9279e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 11s - loss: 5.8166 - mse: 5.8166 - mae: 1.0663 - val_loss: 7.9539 - val_mse: 7.9539 - val_mae: 1.3311 - lr: 1.9279e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 11s - loss: 5.7043 - mse: 5.7043 - mae: 1.0423 - val_loss: 7.8177 - val_mse: 7.8177 - val_mae: 1.1964 - lr: 1.9279e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 11s - loss: 5.3408 - mse: 5.3408 - mae: 1.0246 - val_loss: 7.7768 - val_mse: 7.7768 - val_mae: 1.2276 - lr: 1.9279e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 11s - loss: 5.2307 - mse: 5.2307 - mae: 1.0085 - val_loss: 8.5912 - val_mse: 8.5912 - val_mae: 1.2462 - lr: 1.9279e-04 - 11s/epoch - 11ms/step\n",
            "Score for fold 4: loss of 8.591214179992676\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 11s - loss: 4.9753 - mse: 4.9753 - mae: 1.0764 - val_loss: 8.5679 - val_mse: 8.5679 - val_mae: 0.9837 - lr: 1.9279e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 11s - loss: 4.7210 - mse: 4.7210 - mae: 1.0445 - val_loss: 9.4355 - val_mse: 9.4355 - val_mae: 1.0345 - lr: 1.9279e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 11s - loss: 4.2400 - mse: 4.2400 - mae: 1.0274 - val_loss: 8.9798 - val_mse: 8.9798 - val_mae: 1.0318 - lr: 1.9279e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 11s - loss: 4.2175 - mse: 4.2175 - mae: 1.0045 - val_loss: 9.5817 - val_mse: 9.5817 - val_mae: 1.0413 - lr: 1.9279e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 11s - loss: 3.7347 - mse: 3.7347 - mae: 0.9852 - val_loss: 9.6919 - val_mse: 9.6919 - val_mae: 1.0621 - lr: 1.9279e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 11s - loss: 3.7472 - mse: 3.7472 - mae: 0.9684 - val_loss: 9.6921 - val_mse: 9.6921 - val_mae: 1.0872 - lr: 1.9279e-04 - 11s/epoch - 11ms/step\n",
            "Score for fold 5: loss of 9.692108154296875\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-10-12 05:05:25,740]\u001b[0m Finished trial#34 resulted in value: 8.796. Current best value is 8.796 with parameters: {'activation': 'relu', 'num_hidden_layer': 4, 'i': 10, 'learning_rate': 0.00019278929576381387}.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 8s - loss: 13.2380 - mse: 13.2380 - mae: 1.5406 - val_loss: 12.4866 - val_mse: 12.4866 - val_mae: 1.5248 - lr: 1.5100e-04 - 8s/epoch - 8ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 7s - loss: 12.4373 - mse: 12.4373 - mae: 1.4909 - val_loss: 12.0688 - val_mse: 12.0688 - val_mae: 1.4623 - lr: 1.5100e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 7s - loss: 12.1739 - mse: 12.1739 - mae: 1.4714 - val_loss: 11.9533 - val_mse: 11.9533 - val_mae: 1.4441 - lr: 1.5100e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 7s - loss: 12.0873 - mse: 12.0873 - mae: 1.4579 - val_loss: 11.8674 - val_mse: 11.8674 - val_mae: 1.4411 - lr: 1.5100e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 7s - loss: 12.0183 - mse: 12.0183 - mae: 1.4502 - val_loss: 11.9862 - val_mse: 11.9862 - val_mae: 1.5304 - lr: 1.5100e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 7s - loss: 11.8387 - mse: 11.8387 - mae: 1.4441 - val_loss: 11.6308 - val_mse: 11.6308 - val_mae: 1.4760 - lr: 1.5100e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 7s - loss: 11.8178 - mse: 11.8178 - mae: 1.4358 - val_loss: 11.7092 - val_mse: 11.7092 - val_mae: 1.4281 - lr: 1.5100e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 7s - loss: 11.7079 - mse: 11.7079 - mae: 1.4325 - val_loss: 11.7738 - val_mse: 11.7738 - val_mae: 1.4237 - lr: 1.5100e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 7s - loss: 11.6907 - mse: 11.6907 - mae: 1.4235 - val_loss: 12.0893 - val_mse: 12.0893 - val_mae: 1.4013 - lr: 1.5100e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 7s - loss: 11.6147 - mse: 11.6147 - mae: 1.4216 - val_loss: 11.9565 - val_mse: 11.9565 - val_mae: 1.4710 - lr: 1.5100e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 7s - loss: 11.4869 - mse: 11.4869 - mae: 1.4154 - val_loss: 11.5763 - val_mse: 11.5763 - val_mae: 1.4631 - lr: 1.5100e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 7s - loss: 11.3807 - mse: 11.3807 - mae: 1.4084 - val_loss: 11.9835 - val_mse: 11.9835 - val_mae: 1.4802 - lr: 1.5100e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 13/100\n",
            "1000/1000 - 7s - loss: 11.2623 - mse: 11.2623 - mae: 1.4034 - val_loss: 11.7157 - val_mse: 11.7157 - val_mae: 1.3979 - lr: 1.5100e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 14/100\n",
            "1000/1000 - 7s - loss: 11.1515 - mse: 11.1515 - mae: 1.3986 - val_loss: 12.1982 - val_mse: 12.1982 - val_mae: 1.4997 - lr: 1.5100e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 15/100\n",
            "1000/1000 - 7s - loss: 11.0723 - mse: 11.0723 - mae: 1.3904 - val_loss: 12.0702 - val_mse: 12.0702 - val_mae: 1.4390 - lr: 1.5100e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 16/100\n",
            "1000/1000 - 7s - loss: 11.0207 - mse: 11.0207 - mae: 1.3855 - val_loss: 11.8863 - val_mse: 11.8863 - val_mae: 1.4708 - lr: 1.5100e-04 - 7s/epoch - 7ms/step\n",
            "Score for fold 1: loss of 11.88631534576416\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 7s - loss: 11.5866 - mse: 11.5866 - mae: 1.3974 - val_loss: 8.8479 - val_mse: 8.8479 - val_mae: 1.4255 - lr: 1.5100e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 7s - loss: 11.4615 - mse: 11.4615 - mae: 1.3856 - val_loss: 8.6052 - val_mse: 8.6052 - val_mae: 1.4229 - lr: 1.5100e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 7s - loss: 11.3256 - mse: 11.3256 - mae: 1.3795 - val_loss: 9.2736 - val_mse: 9.2736 - val_mae: 1.3907 - lr: 1.5100e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 7s - loss: 11.1669 - mse: 11.1669 - mae: 1.3707 - val_loss: 8.7562 - val_mse: 8.7562 - val_mae: 1.4360 - lr: 1.5100e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 7s - loss: 11.0864 - mse: 11.0864 - mae: 1.3653 - val_loss: 9.0644 - val_mse: 9.0644 - val_mae: 1.3678 - lr: 1.5100e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 7s - loss: 10.9085 - mse: 10.9085 - mae: 1.3564 - val_loss: 9.1388 - val_mse: 9.1388 - val_mae: 1.3997 - lr: 1.5100e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 7s - loss: 10.7951 - mse: 10.7951 - mae: 1.3487 - val_loss: 9.3137 - val_mse: 9.3137 - val_mae: 1.4681 - lr: 1.5100e-04 - 7s/epoch - 7ms/step\n",
            "Score for fold 2: loss of 9.313733100891113\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 7s - loss: 10.8387 - mse: 10.8387 - mae: 1.3722 - val_loss: 8.8690 - val_mse: 8.8690 - val_mae: 1.3309 - lr: 1.5100e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 7s - loss: 10.7406 - mse: 10.7406 - mae: 1.3607 - val_loss: 9.7620 - val_mse: 9.7620 - val_mae: 1.3684 - lr: 1.5100e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 7s - loss: 10.4439 - mse: 10.4439 - mae: 1.3508 - val_loss: 9.1158 - val_mse: 9.1158 - val_mae: 1.3235 - lr: 1.5100e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 7s - loss: 10.4891 - mse: 10.4891 - mae: 1.3417 - val_loss: 9.2024 - val_mse: 9.2024 - val_mae: 1.3637 - lr: 1.5100e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 7s - loss: 10.2407 - mse: 10.2407 - mae: 1.3317 - val_loss: 9.4640 - val_mse: 9.4640 - val_mae: 1.3524 - lr: 1.5100e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 7s - loss: 10.1660 - mse: 10.1660 - mae: 1.3244 - val_loss: 9.4240 - val_mse: 9.4240 - val_mae: 1.3988 - lr: 1.5100e-04 - 7s/epoch - 7ms/step\n",
            "Score for fold 3: loss of 9.423981666564941\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 7s - loss: 8.4151 - mse: 8.4151 - mae: 1.3291 - val_loss: 16.1003 - val_mse: 16.1003 - val_mae: 1.3078 - lr: 1.5100e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 7s - loss: 8.4523 - mse: 8.4523 - mae: 1.3213 - val_loss: 15.5609 - val_mse: 15.5609 - val_mae: 1.3273 - lr: 1.5100e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 7s - loss: 8.3294 - mse: 8.3294 - mae: 1.3108 - val_loss: 16.0100 - val_mse: 16.0100 - val_mae: 1.2775 - lr: 1.5100e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 7s - loss: 8.0858 - mse: 8.0858 - mae: 1.3013 - val_loss: 16.1076 - val_mse: 16.1076 - val_mae: 1.2929 - lr: 1.5100e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 7s - loss: 7.8547 - mse: 7.8547 - mae: 1.2918 - val_loss: 16.1185 - val_mse: 16.1185 - val_mae: 1.3765 - lr: 1.5100e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 7s - loss: 7.8221 - mse: 7.8221 - mae: 1.2837 - val_loss: 16.3043 - val_mse: 16.3043 - val_mae: 1.3278 - lr: 1.5100e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 7s - loss: 7.6853 - mse: 7.6853 - mae: 1.2715 - val_loss: 16.2413 - val_mse: 16.2413 - val_mae: 1.3502 - lr: 1.5100e-04 - 7s/epoch - 7ms/step\n",
            "Score for fold 4: loss of 16.241252899169922\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 7s - loss: 10.0298 - mse: 10.0298 - mae: 1.2873 - val_loss: 5.9514 - val_mse: 5.9514 - val_mae: 1.2198 - lr: 1.5100e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 7s - loss: 10.0551 - mse: 10.0551 - mae: 1.2799 - val_loss: 5.8782 - val_mse: 5.8782 - val_mae: 1.2452 - lr: 1.5100e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 7s - loss: 10.0396 - mse: 10.0396 - mae: 1.2670 - val_loss: 6.0290 - val_mse: 6.0290 - val_mae: 1.2564 - lr: 1.5100e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 7s - loss: 9.5971 - mse: 9.5971 - mae: 1.2546 - val_loss: 6.0388 - val_mse: 6.0388 - val_mae: 1.2777 - lr: 1.5100e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 7s - loss: 9.5306 - mse: 9.5306 - mae: 1.2397 - val_loss: 6.3961 - val_mse: 6.3961 - val_mae: 1.2649 - lr: 1.5100e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 7s - loss: 9.2602 - mse: 9.2602 - mae: 1.2315 - val_loss: 6.3618 - val_mse: 6.3618 - val_mae: 1.2818 - lr: 1.5100e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 7s - loss: 9.4232 - mse: 9.4232 - mae: 1.2247 - val_loss: 6.2734 - val_mse: 6.2734 - val_mae: 1.2940 - lr: 1.5100e-04 - 7s/epoch - 7ms/step\n",
            "Score for fold 5: loss of 6.273426532745361\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-10-12 05:10:26,435]\u001b[0m Finished trial#35 resulted in value: 10.626. Current best value is 8.796 with parameters: {'activation': 'relu', 'num_hidden_layer': 4, 'i': 10, 'learning_rate': 0.00019278929576381387}.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 12s - loss: 13.6647 - mse: 13.6647 - mae: 1.5401 - val_loss: 10.8821 - val_mse: 10.8821 - val_mae: 1.4706 - lr: 2.1363e-04 - 12s/epoch - 12ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 11s - loss: 12.8238 - mse: 12.8238 - mae: 1.4859 - val_loss: 10.8673 - val_mse: 10.8673 - val_mae: 1.4509 - lr: 2.1363e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 11s - loss: 12.5671 - mse: 12.5671 - mae: 1.4741 - val_loss: 10.5016 - val_mse: 10.5016 - val_mae: 1.3919 - lr: 2.1363e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 11s - loss: 12.5267 - mse: 12.5267 - mae: 1.4654 - val_loss: 10.4584 - val_mse: 10.4584 - val_mae: 1.4588 - lr: 2.1363e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 11s - loss: 12.4202 - mse: 12.4202 - mae: 1.4560 - val_loss: 10.1274 - val_mse: 10.1274 - val_mae: 1.4559 - lr: 2.1363e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 11s - loss: 12.1938 - mse: 12.1938 - mae: 1.4481 - val_loss: 10.5337 - val_mse: 10.5337 - val_mae: 1.4505 - lr: 2.1363e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 11s - loss: 12.0674 - mse: 12.0674 - mae: 1.4359 - val_loss: 10.0715 - val_mse: 10.0715 - val_mae: 1.3964 - lr: 2.1363e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 11s - loss: 11.7484 - mse: 11.7484 - mae: 1.4256 - val_loss: 10.0579 - val_mse: 10.0579 - val_mae: 1.4616 - lr: 2.1363e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 11s - loss: 11.7105 - mse: 11.7105 - mae: 1.4188 - val_loss: 10.4860 - val_mse: 10.4860 - val_mae: 1.4784 - lr: 2.1363e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 11s - loss: 11.4377 - mse: 11.4377 - mae: 1.4044 - val_loss: 10.7506 - val_mse: 10.7506 - val_mae: 1.4566 - lr: 2.1363e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 11s - loss: 11.0777 - mse: 11.0777 - mae: 1.3894 - val_loss: 10.0084 - val_mse: 10.0084 - val_mae: 1.4247 - lr: 2.1363e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 11s - loss: 10.8089 - mse: 10.8089 - mae: 1.3687 - val_loss: 10.6971 - val_mse: 10.6971 - val_mae: 1.4646 - lr: 2.1363e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 13/100\n",
            "1000/1000 - 11s - loss: 10.7559 - mse: 10.7559 - mae: 1.3698 - val_loss: 10.1526 - val_mse: 10.1526 - val_mae: 1.4911 - lr: 2.1363e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 14/100\n",
            "1000/1000 - 12s - loss: 10.2134 - mse: 10.2134 - mae: 1.3409 - val_loss: 10.4690 - val_mse: 10.4690 - val_mae: 1.4420 - lr: 2.1363e-04 - 12s/epoch - 12ms/step\n",
            "Epoch 15/100\n",
            "1000/1000 - 11s - loss: 10.0172 - mse: 10.0172 - mae: 1.3220 - val_loss: 10.5774 - val_mse: 10.5774 - val_mae: 1.5530 - lr: 2.1363e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 16/100\n",
            "1000/1000 - 11s - loss: 9.6166 - mse: 9.6166 - mae: 1.3060 - val_loss: 10.2273 - val_mse: 10.2273 - val_mae: 1.4568 - lr: 2.1363e-04 - 11s/epoch - 11ms/step\n",
            "Score for fold 1: loss of 10.227278709411621\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 11s - loss: 10.6469 - mse: 10.6469 - mae: 1.3512 - val_loss: 6.6702 - val_mse: 6.6702 - val_mae: 1.3285 - lr: 2.1363e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 11s - loss: 10.2485 - mse: 10.2485 - mae: 1.3263 - val_loss: 7.0916 - val_mse: 7.0916 - val_mae: 1.3365 - lr: 2.1363e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 11s - loss: 9.7343 - mse: 9.7343 - mae: 1.3057 - val_loss: 6.7978 - val_mse: 6.7978 - val_mae: 1.3140 - lr: 2.1363e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 11s - loss: 9.3811 - mse: 9.3811 - mae: 1.2834 - val_loss: 7.9048 - val_mse: 7.9048 - val_mae: 1.4402 - lr: 2.1363e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 11s - loss: 9.2068 - mse: 9.2068 - mae: 1.2681 - val_loss: 8.4202 - val_mse: 8.4202 - val_mae: 1.3564 - lr: 2.1363e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 11s - loss: 9.2497 - mse: 9.2497 - mae: 1.2504 - val_loss: 8.3939 - val_mse: 8.3939 - val_mae: 1.4616 - lr: 2.1363e-04 - 11s/epoch - 11ms/step\n",
            "Score for fold 2: loss of 8.39389419555664\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 11s - loss: 9.1192 - mse: 9.1192 - mae: 1.2883 - val_loss: 7.7675 - val_mse: 7.7675 - val_mae: 1.2636 - lr: 2.1363e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 11s - loss: 8.8193 - mse: 8.8193 - mae: 1.2582 - val_loss: 8.0193 - val_mse: 8.0193 - val_mae: 1.2201 - lr: 2.1363e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 11s - loss: 8.4354 - mse: 8.4354 - mae: 1.2415 - val_loss: 8.2104 - val_mse: 8.2104 - val_mae: 1.2288 - lr: 2.1363e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 11s - loss: 7.8694 - mse: 7.8694 - mae: 1.2151 - val_loss: 7.6253 - val_mse: 7.6253 - val_mae: 1.2566 - lr: 2.1363e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 11s - loss: 7.7129 - mse: 7.7129 - mae: 1.1999 - val_loss: 8.6657 - val_mse: 8.6657 - val_mae: 1.2652 - lr: 2.1363e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 11s - loss: 7.1636 - mse: 7.1636 - mae: 1.1813 - val_loss: 8.5024 - val_mse: 8.5024 - val_mae: 1.3647 - lr: 2.1363e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 11s - loss: 7.1966 - mse: 7.1966 - mae: 1.1758 - val_loss: 8.3839 - val_mse: 8.3839 - val_mae: 1.4863 - lr: 2.1363e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 11s - loss: 6.7243 - mse: 6.7243 - mae: 1.1571 - val_loss: 8.6642 - val_mse: 8.6642 - val_mae: 1.2642 - lr: 2.1363e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 11s - loss: 6.7145 - mse: 6.7145 - mae: 1.1452 - val_loss: 8.1749 - val_mse: 8.1749 - val_mae: 1.3707 - lr: 2.1363e-04 - 11s/epoch - 11ms/step\n",
            "Score for fold 3: loss of 8.174847602844238\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 11s - loss: 5.8518 - mse: 5.8518 - mae: 1.1856 - val_loss: 12.1988 - val_mse: 12.1988 - val_mae: 1.1825 - lr: 2.1363e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 11s - loss: 5.4811 - mse: 5.4811 - mae: 1.1604 - val_loss: 12.3324 - val_mse: 12.3324 - val_mae: 1.2540 - lr: 2.1363e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 11s - loss: 5.2844 - mse: 5.2844 - mae: 1.1330 - val_loss: 12.9672 - val_mse: 12.9672 - val_mae: 1.2081 - lr: 2.1363e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 11s - loss: 5.0226 - mse: 5.0226 - mae: 1.1218 - val_loss: 13.3756 - val_mse: 13.3756 - val_mae: 1.1874 - lr: 2.1363e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 11s - loss: 4.4919 - mse: 4.4919 - mae: 1.0945 - val_loss: 13.4432 - val_mse: 13.4432 - val_mae: 1.2323 - lr: 2.1363e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 11s - loss: 4.6408 - mse: 4.6408 - mae: 1.0873 - val_loss: 14.7612 - val_mse: 14.7612 - val_mae: 1.1596 - lr: 2.1363e-04 - 11s/epoch - 11ms/step\n",
            "Score for fold 4: loss of 14.761218070983887\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 11s - loss: 7.2239 - mse: 7.2239 - mae: 1.1358 - val_loss: 2.9405 - val_mse: 2.9405 - val_mae: 0.9621 - lr: 2.1363e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 11s - loss: 6.6243 - mse: 6.6243 - mae: 1.1013 - val_loss: 3.0957 - val_mse: 3.0957 - val_mae: 1.0199 - lr: 2.1363e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 11s - loss: 6.3596 - mse: 6.3596 - mae: 1.0791 - val_loss: 3.5371 - val_mse: 3.5371 - val_mae: 1.0931 - lr: 2.1363e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 11s - loss: 6.2465 - mse: 6.2465 - mae: 1.0638 - val_loss: 3.6823 - val_mse: 3.6823 - val_mae: 1.1731 - lr: 2.1363e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 11s - loss: 5.7511 - mse: 5.7511 - mae: 1.0524 - val_loss: 3.7310 - val_mse: 3.7310 - val_mae: 1.1559 - lr: 2.1363e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 11s - loss: 5.6391 - mse: 5.6391 - mae: 1.0332 - val_loss: 3.6449 - val_mse: 3.6449 - val_mae: 1.0932 - lr: 2.1363e-04 - 11s/epoch - 11ms/step\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-10-12 05:18:43,443]\u001b[0m Finished trial#36 resulted in value: 9.038. Current best value is 8.796 with parameters: {'activation': 'relu', 'num_hidden_layer': 4, 'i': 10, 'learning_rate': 0.00019278929576381387}.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Score for fold 5: loss of 3.6448988914489746\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 12s - loss: 12.7184 - mse: 12.7184 - mae: 1.5335 - val_loss: 14.5851 - val_mse: 14.5851 - val_mae: 1.4728 - lr: 1.1036e-04 - 12s/epoch - 12ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 11s - loss: 11.7045 - mse: 11.7045 - mae: 1.4751 - val_loss: 14.4542 - val_mse: 14.4542 - val_mae: 1.5293 - lr: 1.1036e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 11s - loss: 11.6203 - mse: 11.6203 - mae: 1.4585 - val_loss: 14.3890 - val_mse: 14.3890 - val_mae: 1.4659 - lr: 1.1036e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 11s - loss: 11.4966 - mse: 11.4966 - mae: 1.4528 - val_loss: 14.6489 - val_mse: 14.6489 - val_mae: 1.4964 - lr: 1.1036e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 11s - loss: 11.2635 - mse: 11.2635 - mae: 1.4385 - val_loss: 14.5563 - val_mse: 14.5563 - val_mae: 1.4663 - lr: 1.1036e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 11s - loss: 11.1540 - mse: 11.1540 - mae: 1.4325 - val_loss: 14.2716 - val_mse: 14.2716 - val_mae: 1.4941 - lr: 1.1036e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 11s - loss: 11.0576 - mse: 11.0576 - mae: 1.4194 - val_loss: 14.3352 - val_mse: 14.3352 - val_mae: 1.4681 - lr: 1.1036e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 11s - loss: 10.8710 - mse: 10.8710 - mae: 1.4118 - val_loss: 14.5308 - val_mse: 14.5308 - val_mae: 1.5144 - lr: 1.1036e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 11s - loss: 10.6211 - mse: 10.6211 - mae: 1.3988 - val_loss: 14.8448 - val_mse: 14.8448 - val_mae: 1.4889 - lr: 1.1036e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 11s - loss: 10.6622 - mse: 10.6622 - mae: 1.3929 - val_loss: 14.7454 - val_mse: 14.7454 - val_mae: 1.4745 - lr: 1.1036e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 11s - loss: 10.3419 - mse: 10.3419 - mae: 1.3739 - val_loss: 14.6900 - val_mse: 14.6900 - val_mae: 1.5635 - lr: 1.1036e-04 - 11s/epoch - 11ms/step\n",
            "Score for fold 1: loss of 14.689953804016113\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 11s - loss: 10.7542 - mse: 10.7542 - mae: 1.3902 - val_loss: 12.0101 - val_mse: 12.0101 - val_mae: 1.4703 - lr: 1.1036e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 11s - loss: 10.6080 - mse: 10.6080 - mae: 1.3789 - val_loss: 12.3126 - val_mse: 12.3126 - val_mae: 1.4022 - lr: 1.1036e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 11s - loss: 10.2972 - mse: 10.2972 - mae: 1.3606 - val_loss: 12.1575 - val_mse: 12.1575 - val_mae: 1.4438 - lr: 1.1036e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 11s - loss: 10.3423 - mse: 10.3423 - mae: 1.3469 - val_loss: 12.4323 - val_mse: 12.4323 - val_mae: 1.4353 - lr: 1.1036e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 11s - loss: 9.7452 - mse: 9.7452 - mae: 1.3273 - val_loss: 12.4918 - val_mse: 12.4918 - val_mae: 1.4527 - lr: 1.1036e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 11s - loss: 10.2559 - mse: 10.2559 - mae: 1.3104 - val_loss: 12.3994 - val_mse: 12.3994 - val_mae: 1.4719 - lr: 1.1036e-04 - 11s/epoch - 11ms/step\n",
            "Score for fold 2: loss of 12.39944076538086\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 11s - loss: 9.1708 - mse: 9.1708 - mae: 1.3380 - val_loss: 13.3663 - val_mse: 13.3663 - val_mae: 1.2631 - lr: 1.1036e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 11s - loss: 8.9235 - mse: 8.9235 - mae: 1.3159 - val_loss: 13.3338 - val_mse: 13.3338 - val_mae: 1.3225 - lr: 1.1036e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 12s - loss: 8.7977 - mse: 8.7977 - mae: 1.2988 - val_loss: 13.3892 - val_mse: 13.3892 - val_mae: 1.3698 - lr: 1.1036e-04 - 12s/epoch - 12ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 13s - loss: 8.3247 - mse: 8.3247 - mae: 1.2758 - val_loss: 14.1604 - val_mse: 14.1604 - val_mae: 1.3348 - lr: 1.1036e-04 - 13s/epoch - 13ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 13s - loss: 8.1024 - mse: 8.1024 - mae: 1.2554 - val_loss: 14.8463 - val_mse: 14.8463 - val_mae: 1.3670 - lr: 1.1036e-04 - 13s/epoch - 13ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 13s - loss: 7.7610 - mse: 7.7610 - mae: 1.2305 - val_loss: 13.7089 - val_mse: 13.7089 - val_mae: 1.3686 - lr: 1.1036e-04 - 13s/epoch - 13ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 14s - loss: 7.2936 - mse: 7.2936 - mae: 1.2061 - val_loss: 13.7943 - val_mse: 13.7943 - val_mae: 1.3519 - lr: 1.1036e-04 - 14s/epoch - 14ms/step\n",
            "Score for fold 3: loss of 13.794265747070312\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 13s - loss: 9.4726 - mse: 9.4726 - mae: 1.2616 - val_loss: 5.3255 - val_mse: 5.3255 - val_mae: 1.1944 - lr: 1.1036e-04 - 13s/epoch - 13ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 13s - loss: 9.0006 - mse: 9.0006 - mae: 1.2265 - val_loss: 5.6997 - val_mse: 5.6997 - val_mae: 1.1614 - lr: 1.1036e-04 - 13s/epoch - 13ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 12s - loss: 8.8029 - mse: 8.8029 - mae: 1.2076 - val_loss: 5.5404 - val_mse: 5.5404 - val_mae: 1.1930 - lr: 1.1036e-04 - 12s/epoch - 12ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 13s - loss: 8.3237 - mse: 8.3237 - mae: 1.1812 - val_loss: 5.8215 - val_mse: 5.8215 - val_mae: 1.2130 - lr: 1.1036e-04 - 13s/epoch - 13ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 13s - loss: 7.9160 - mse: 7.9160 - mae: 1.1621 - val_loss: 5.9859 - val_mse: 5.9859 - val_mae: 1.2952 - lr: 1.1036e-04 - 13s/epoch - 13ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 13s - loss: 7.8417 - mse: 7.8417 - mae: 1.1438 - val_loss: 6.3716 - val_mse: 6.3716 - val_mae: 1.3802 - lr: 1.1036e-04 - 13s/epoch - 13ms/step\n",
            "Score for fold 4: loss of 6.371642112731934\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 13s - loss: 8.1152 - mse: 8.1152 - mae: 1.1754 - val_loss: 3.1320 - val_mse: 3.1320 - val_mae: 1.0827 - lr: 1.1036e-04 - 13s/epoch - 13ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 13s - loss: 7.8088 - mse: 7.8088 - mae: 1.1547 - val_loss: 4.5160 - val_mse: 4.5160 - val_mae: 1.0706 - lr: 1.1036e-04 - 13s/epoch - 13ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 12s - loss: 7.6839 - mse: 7.6839 - mae: 1.1338 - val_loss: 3.8041 - val_mse: 3.8041 - val_mae: 1.1777 - lr: 1.1036e-04 - 12s/epoch - 12ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 11s - loss: 7.5357 - mse: 7.5357 - mae: 1.1105 - val_loss: 4.0134 - val_mse: 4.0134 - val_mae: 1.1248 - lr: 1.1036e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 11s - loss: 7.2648 - mse: 7.2648 - mae: 1.0886 - val_loss: 3.7891 - val_mse: 3.7891 - val_mae: 1.1500 - lr: 1.1036e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 11s - loss: 6.8650 - mse: 6.8650 - mae: 1.0751 - val_loss: 3.8614 - val_mse: 3.8614 - val_mae: 1.1320 - lr: 1.1036e-04 - 11s/epoch - 11ms/step\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-10-12 05:26:02,592]\u001b[0m Finished trial#37 resulted in value: 10.221999999999998. Current best value is 8.796 with parameters: {'activation': 'relu', 'num_hidden_layer': 4, 'i': 10, 'learning_rate': 0.00019278929576381387}.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Score for fold 5: loss of 3.8613860607147217\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 12s - loss: 11.6911 - mse: 11.6911 - mae: 1.5423 - val_loss: 17.5400 - val_mse: 17.5400 - val_mae: 1.5055 - lr: 2.4641e-04 - 12s/epoch - 12ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 11s - loss: 11.1300 - mse: 11.1300 - mae: 1.4847 - val_loss: 17.5284 - val_mse: 17.5284 - val_mae: 1.5945 - lr: 2.4641e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 11s - loss: 10.8991 - mse: 10.8991 - mae: 1.4678 - val_loss: 17.2187 - val_mse: 17.2187 - val_mae: 1.5658 - lr: 2.4641e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 11s - loss: 10.6638 - mse: 10.6638 - mae: 1.4619 - val_loss: 17.4384 - val_mse: 17.4384 - val_mae: 1.4998 - lr: 2.4641e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 11s - loss: 10.6230 - mse: 10.6230 - mae: 1.4543 - val_loss: 17.3419 - val_mse: 17.3419 - val_mae: 1.5140 - lr: 2.4641e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 11s - loss: 10.4603 - mse: 10.4603 - mae: 1.4414 - val_loss: 17.2826 - val_mse: 17.2826 - val_mae: 1.5045 - lr: 2.4641e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 11s - loss: 10.3488 - mse: 10.3488 - mae: 1.4325 - val_loss: 17.2690 - val_mse: 17.2690 - val_mae: 1.4655 - lr: 2.4641e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 11s - loss: 10.0471 - mse: 10.0471 - mae: 1.4196 - val_loss: 17.1067 - val_mse: 17.1067 - val_mae: 1.4611 - lr: 2.4641e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 11s - loss: 9.7966 - mse: 9.7966 - mae: 1.4115 - val_loss: 17.4291 - val_mse: 17.4291 - val_mae: 1.4452 - lr: 2.4641e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 11s - loss: 9.7919 - mse: 9.7919 - mae: 1.3977 - val_loss: 17.5601 - val_mse: 17.5601 - val_mae: 1.5024 - lr: 2.4641e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 11s - loss: 9.3689 - mse: 9.3689 - mae: 1.3843 - val_loss: 17.0891 - val_mse: 17.0891 - val_mae: 1.4885 - lr: 2.4641e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 11s - loss: 9.1513 - mse: 9.1513 - mae: 1.3710 - val_loss: 17.4352 - val_mse: 17.4352 - val_mae: 1.5582 - lr: 2.4641e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 13/100\n",
            "1000/1000 - 11s - loss: 9.0806 - mse: 9.0806 - mae: 1.3551 - val_loss: 17.3231 - val_mse: 17.3231 - val_mae: 1.4496 - lr: 2.4641e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 14/100\n",
            "1000/1000 - 11s - loss: 8.8588 - mse: 8.8588 - mae: 1.3365 - val_loss: 17.6262 - val_mse: 17.6262 - val_mae: 1.5643 - lr: 2.4641e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 15/100\n",
            "1000/1000 - 11s - loss: 8.6176 - mse: 8.6176 - mae: 1.3224 - val_loss: 17.2426 - val_mse: 17.2426 - val_mae: 1.5032 - lr: 2.4641e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 16/100\n",
            "1000/1000 - 11s - loss: 8.1815 - mse: 8.1815 - mae: 1.2993 - val_loss: 17.0618 - val_mse: 17.0618 - val_mae: 1.5268 - lr: 2.4641e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 17/100\n",
            "1000/1000 - 11s - loss: 7.9386 - mse: 7.9386 - mae: 1.2827 - val_loss: 17.0822 - val_mse: 17.0822 - val_mae: 1.4920 - lr: 2.4641e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 18/100\n",
            "1000/1000 - 11s - loss: 7.6035 - mse: 7.6035 - mae: 1.2596 - val_loss: 17.0393 - val_mse: 17.0393 - val_mae: 1.5468 - lr: 2.4641e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 19/100\n",
            "1000/1000 - 11s - loss: 7.4583 - mse: 7.4583 - mae: 1.2538 - val_loss: 17.5511 - val_mse: 17.5511 - val_mae: 1.6296 - lr: 2.4641e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 20/100\n",
            "1000/1000 - 11s - loss: 7.0671 - mse: 7.0671 - mae: 1.2315 - val_loss: 17.4064 - val_mse: 17.4064 - val_mae: 1.5144 - lr: 2.4641e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 21/100\n",
            "1000/1000 - 11s - loss: 6.7349 - mse: 6.7349 - mae: 1.2096 - val_loss: 17.3774 - val_mse: 17.3774 - val_mae: 1.5725 - lr: 2.4641e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 22/100\n",
            "1000/1000 - 11s - loss: 6.6761 - mse: 6.6761 - mae: 1.2040 - val_loss: 18.6895 - val_mse: 18.6895 - val_mae: 1.4848 - lr: 2.4641e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 23/100\n",
            "1000/1000 - 11s - loss: 6.4465 - mse: 6.4465 - mae: 1.1882 - val_loss: 18.0354 - val_mse: 18.0354 - val_mae: 1.4786 - lr: 2.4641e-04 - 11s/epoch - 11ms/step\n",
            "Score for fold 1: loss of 18.035354614257812\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 11s - loss: 9.8274 - mse: 9.8274 - mae: 1.2950 - val_loss: 4.9827 - val_mse: 4.9827 - val_mae: 1.1773 - lr: 2.4641e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 11s - loss: 9.3832 - mse: 9.3832 - mae: 1.2667 - val_loss: 5.5609 - val_mse: 5.5609 - val_mae: 1.2864 - lr: 2.4641e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 11s - loss: 8.8454 - mse: 8.8454 - mae: 1.2394 - val_loss: 5.5603 - val_mse: 5.5603 - val_mae: 1.3132 - lr: 2.4641e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 11s - loss: 8.6105 - mse: 8.6105 - mae: 1.2198 - val_loss: 5.9905 - val_mse: 5.9905 - val_mae: 1.2123 - lr: 2.4641e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 11s - loss: 8.1868 - mse: 8.1868 - mae: 1.2019 - val_loss: 6.1473 - val_mse: 6.1473 - val_mae: 1.2314 - lr: 2.4641e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 11s - loss: 8.0249 - mse: 8.0249 - mae: 1.1900 - val_loss: 6.0039 - val_mse: 6.0039 - val_mae: 1.2713 - lr: 2.4641e-04 - 11s/epoch - 11ms/step\n",
            "Score for fold 2: loss of 6.003863334655762\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 11s - loss: 7.9245 - mse: 7.9245 - mae: 1.2347 - val_loss: 6.6472 - val_mse: 6.6472 - val_mae: 1.3381 - lr: 2.4641e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 11s - loss: 7.6120 - mse: 7.6120 - mae: 1.1874 - val_loss: 6.3083 - val_mse: 6.3083 - val_mae: 1.3479 - lr: 2.4641e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 11s - loss: 7.4138 - mse: 7.4138 - mae: 1.1754 - val_loss: 6.5661 - val_mse: 6.5661 - val_mae: 1.1909 - lr: 2.4641e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 11s - loss: 7.0047 - mse: 7.0047 - mae: 1.1496 - val_loss: 6.7656 - val_mse: 6.7656 - val_mae: 1.2279 - lr: 2.4641e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 11s - loss: 6.7074 - mse: 6.7074 - mae: 1.1271 - val_loss: 7.1587 - val_mse: 7.1587 - val_mae: 1.1908 - lr: 2.4641e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 11s - loss: 6.5444 - mse: 6.5444 - mae: 1.1137 - val_loss: 7.2218 - val_mse: 7.2218 - val_mae: 1.1875 - lr: 2.4641e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 11s - loss: 6.3024 - mse: 6.3024 - mae: 1.0935 - val_loss: 7.0262 - val_mse: 7.0262 - val_mae: 1.2494 - lr: 2.4641e-04 - 11s/epoch - 11ms/step\n",
            "Score for fold 3: loss of 7.026232719421387\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 11s - loss: 7.4382 - mse: 7.4382 - mae: 1.1402 - val_loss: 3.0019 - val_mse: 3.0019 - val_mae: 1.0992 - lr: 2.4641e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 11s - loss: 7.2351 - mse: 7.2351 - mae: 1.1084 - val_loss: 3.1104 - val_mse: 3.1104 - val_mae: 1.1332 - lr: 2.4641e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 11s - loss: 6.5334 - mse: 6.5334 - mae: 1.0861 - val_loss: 4.1545 - val_mse: 4.1545 - val_mae: 1.1279 - lr: 2.4641e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 11s - loss: 6.4083 - mse: 6.4083 - mae: 1.0710 - val_loss: 3.5889 - val_mse: 3.5889 - val_mae: 1.2399 - lr: 2.4641e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 11s - loss: 6.1661 - mse: 6.1661 - mae: 1.0578 - val_loss: 3.6487 - val_mse: 3.6487 - val_mae: 1.1954 - lr: 2.4641e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 11s - loss: 6.2717 - mse: 6.2717 - mae: 1.0394 - val_loss: 3.7174 - val_mse: 3.7174 - val_mae: 1.1379 - lr: 2.4641e-04 - 11s/epoch - 11ms/step\n",
            "Score for fold 4: loss of 3.7173752784729004\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 11s - loss: 5.7068 - mse: 5.7068 - mae: 1.0797 - val_loss: 5.4670 - val_mse: 5.4670 - val_mae: 0.9778 - lr: 2.4641e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 11s - loss: 5.3528 - mse: 5.3528 - mae: 1.0451 - val_loss: 5.7779 - val_mse: 5.7779 - val_mae: 0.9988 - lr: 2.4641e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 11s - loss: 4.8959 - mse: 4.8959 - mae: 1.0250 - val_loss: 5.9685 - val_mse: 5.9685 - val_mae: 1.0980 - lr: 2.4641e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 11s - loss: 4.6715 - mse: 4.6715 - mae: 1.0085 - val_loss: 5.8158 - val_mse: 5.8158 - val_mae: 1.0159 - lr: 2.4641e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 11s - loss: 4.7207 - mse: 4.7207 - mae: 0.9986 - val_loss: 6.5841 - val_mse: 6.5841 - val_mae: 1.0097 - lr: 2.4641e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 11s - loss: 4.4908 - mse: 4.4908 - mae: 0.9853 - val_loss: 6.5431 - val_mse: 6.5431 - val_mae: 1.0889 - lr: 2.4641e-04 - 11s/epoch - 11ms/step\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-10-12 05:35:10,289]\u001b[0m Finished trial#38 resulted in value: 8.266. Current best value is 8.266 with parameters: {'activation': 'relu', 'num_hidden_layer': 4, 'i': 10, 'learning_rate': 0.0002464065452818332}.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Score for fold 5: loss of 6.5430908203125\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 12s - loss: 15.1008 - mse: 15.1008 - mae: 1.6249 - val_loss: 11.5963 - val_mse: 11.5963 - val_mae: 1.5401 - lr: 2.5052e-04 - 12s/epoch - 12ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 11s - loss: 14.3233 - mse: 14.3233 - mae: 1.5434 - val_loss: 12.2921 - val_mse: 12.2921 - val_mae: 1.5537 - lr: 2.5052e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 11s - loss: 14.2024 - mse: 14.2024 - mae: 1.5317 - val_loss: 11.6124 - val_mse: 11.6124 - val_mae: 1.5419 - lr: 2.5052e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 11s - loss: 14.0410 - mse: 14.0410 - mae: 1.5119 - val_loss: 11.5773 - val_mse: 11.5773 - val_mae: 1.4668 - lr: 2.5052e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 11s - loss: 13.9665 - mse: 13.9665 - mae: 1.5053 - val_loss: 11.5609 - val_mse: 11.5609 - val_mae: 1.5006 - lr: 2.5052e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 11s - loss: 13.8644 - mse: 13.8644 - mae: 1.5039 - val_loss: 11.0636 - val_mse: 11.0636 - val_mae: 1.4923 - lr: 2.5052e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 11s - loss: 13.7912 - mse: 13.7912 - mae: 1.4975 - val_loss: 11.0132 - val_mse: 11.0132 - val_mae: 1.5470 - lr: 2.5052e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 11s - loss: 13.6621 - mse: 13.6621 - mae: 1.4926 - val_loss: 10.9053 - val_mse: 10.9053 - val_mae: 1.4726 - lr: 2.5052e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 11s - loss: 13.5799 - mse: 13.5799 - mae: 1.4844 - val_loss: 10.9152 - val_mse: 10.9152 - val_mae: 1.4420 - lr: 2.5052e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 11s - loss: 13.4380 - mse: 13.4380 - mae: 1.4826 - val_loss: 10.9037 - val_mse: 10.9037 - val_mae: 1.4146 - lr: 2.5052e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 11s - loss: 13.4196 - mse: 13.4196 - mae: 1.4730 - val_loss: 11.0751 - val_mse: 11.0751 - val_mae: 1.5851 - lr: 2.5052e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 11s - loss: 13.3471 - mse: 13.3471 - mae: 1.4658 - val_loss: 10.9429 - val_mse: 10.9429 - val_mae: 1.6362 - lr: 2.5052e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 13/100\n",
            "1000/1000 - 11s - loss: 13.2592 - mse: 13.2592 - mae: 1.4659 - val_loss: 10.9674 - val_mse: 10.9674 - val_mae: 1.4552 - lr: 2.5052e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 14/100\n",
            "1000/1000 - 11s - loss: 13.1477 - mse: 13.1477 - mae: 1.4609 - val_loss: 10.6544 - val_mse: 10.6544 - val_mae: 1.4828 - lr: 2.5052e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 15/100\n",
            "1000/1000 - 11s - loss: 13.0494 - mse: 13.0494 - mae: 1.4520 - val_loss: 10.8812 - val_mse: 10.8812 - val_mae: 1.5499 - lr: 2.5052e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 16/100\n",
            "1000/1000 - 11s - loss: 12.8908 - mse: 12.8908 - mae: 1.4466 - val_loss: 10.7406 - val_mse: 10.7406 - val_mae: 1.5053 - lr: 2.5052e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 17/100\n",
            "1000/1000 - 11s - loss: 12.7773 - mse: 12.7773 - mae: 1.4382 - val_loss: 10.5814 - val_mse: 10.5814 - val_mae: 1.5948 - lr: 2.5052e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 18/100\n",
            "1000/1000 - 11s - loss: 12.6726 - mse: 12.6726 - mae: 1.4320 - val_loss: 10.7226 - val_mse: 10.7226 - val_mae: 1.5229 - lr: 2.5052e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 19/100\n",
            "1000/1000 - 11s - loss: 12.4553 - mse: 12.4553 - mae: 1.4208 - val_loss: 11.0056 - val_mse: 11.0056 - val_mae: 1.5150 - lr: 2.5052e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 20/100\n",
            "1000/1000 - 12s - loss: 12.3918 - mse: 12.3918 - mae: 1.4180 - val_loss: 10.5269 - val_mse: 10.5269 - val_mae: 1.5006 - lr: 2.5052e-04 - 12s/epoch - 12ms/step\n",
            "Epoch 21/100\n",
            "1000/1000 - 11s - loss: 12.1852 - mse: 12.1852 - mae: 1.4089 - val_loss: 10.4354 - val_mse: 10.4354 - val_mae: 1.4854 - lr: 2.5052e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 22/100\n",
            "1000/1000 - 11s - loss: 12.0240 - mse: 12.0240 - mae: 1.3976 - val_loss: 10.5201 - val_mse: 10.5201 - val_mae: 1.5364 - lr: 2.5052e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 23/100\n",
            "1000/1000 - 11s - loss: 11.8129 - mse: 11.8129 - mae: 1.3829 - val_loss: 10.2007 - val_mse: 10.2007 - val_mae: 1.5221 - lr: 2.5052e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 24/100\n",
            "1000/1000 - 11s - loss: 11.6128 - mse: 11.6128 - mae: 1.3708 - val_loss: 10.6228 - val_mse: 10.6228 - val_mae: 1.5049 - lr: 2.5052e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 25/100\n",
            "1000/1000 - 11s - loss: 11.5341 - mse: 11.5341 - mae: 1.3604 - val_loss: 10.3149 - val_mse: 10.3149 - val_mae: 1.5331 - lr: 2.5052e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 26/100\n",
            "1000/1000 - 11s - loss: 11.2221 - mse: 11.2221 - mae: 1.3395 - val_loss: 10.8328 - val_mse: 10.8328 - val_mae: 1.5572 - lr: 2.5052e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 27/100\n",
            "1000/1000 - 11s - loss: 11.0568 - mse: 11.0568 - mae: 1.3283 - val_loss: 10.6440 - val_mse: 10.6440 - val_mae: 1.4950 - lr: 2.5052e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 28/100\n",
            "1000/1000 - 11s - loss: 10.7693 - mse: 10.7693 - mae: 1.3087 - val_loss: 10.5883 - val_mse: 10.5883 - val_mae: 1.5241 - lr: 2.5052e-04 - 11s/epoch - 11ms/step\n",
            "Score for fold 1: loss of 10.588313102722168\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 11s - loss: 11.1085 - mse: 11.1085 - mae: 1.3612 - val_loss: 8.9066 - val_mse: 8.9066 - val_mae: 1.2995 - lr: 2.5052e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 11s - loss: 10.8103 - mse: 10.8103 - mae: 1.3376 - val_loss: 8.9791 - val_mse: 8.9791 - val_mae: 1.3456 - lr: 2.5052e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 11s - loss: 10.3555 - mse: 10.3555 - mae: 1.3063 - val_loss: 9.3126 - val_mse: 9.3126 - val_mae: 1.4578 - lr: 2.5052e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 11s - loss: 9.9911 - mse: 9.9911 - mae: 1.2819 - val_loss: 9.1136 - val_mse: 9.1136 - val_mae: 1.3889 - lr: 2.5052e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 11s - loss: 9.9150 - mse: 9.9150 - mae: 1.2583 - val_loss: 9.1645 - val_mse: 9.1645 - val_mae: 1.3695 - lr: 2.5052e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 11s - loss: 9.3735 - mse: 9.3735 - mae: 1.2247 - val_loss: 9.3017 - val_mse: 9.3017 - val_mae: 1.4335 - lr: 2.5052e-04 - 11s/epoch - 11ms/step\n",
            "Score for fold 2: loss of 9.301751136779785\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 11s - loss: 9.6695 - mse: 9.6695 - mae: 1.2744 - val_loss: 7.6320 - val_mse: 7.6320 - val_mae: 1.1850 - lr: 2.5052e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 11s - loss: 9.1310 - mse: 9.1310 - mae: 1.2352 - val_loss: 7.5944 - val_mse: 7.5944 - val_mae: 1.2172 - lr: 2.5052e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 11s - loss: 8.7282 - mse: 8.7282 - mae: 1.1986 - val_loss: 7.8578 - val_mse: 7.8578 - val_mae: 1.2406 - lr: 2.5052e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 11s - loss: 8.2625 - mse: 8.2625 - mae: 1.1600 - val_loss: 7.9461 - val_mse: 7.9461 - val_mae: 1.2293 - lr: 2.5052e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 12s - loss: 7.8413 - mse: 7.8413 - mae: 1.1246 - val_loss: 8.2413 - val_mse: 8.2413 - val_mae: 1.4312 - lr: 2.5052e-04 - 12s/epoch - 12ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 11s - loss: 7.5451 - mse: 7.5451 - mae: 1.0923 - val_loss: 7.9007 - val_mse: 7.9007 - val_mae: 1.2527 - lr: 2.5052e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 11s - loss: 7.1946 - mse: 7.1946 - mae: 1.0521 - val_loss: 7.9150 - val_mse: 7.9150 - val_mae: 1.2928 - lr: 2.5052e-04 - 11s/epoch - 11ms/step\n",
            "Score for fold 3: loss of 7.914989948272705\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 11s - loss: 7.8101 - mse: 7.8101 - mae: 1.1180 - val_loss: 4.8645 - val_mse: 4.8645 - val_mae: 1.0029 - lr: 2.5052e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 11s - loss: 7.3346 - mse: 7.3346 - mae: 1.0652 - val_loss: 5.0354 - val_mse: 5.0354 - val_mae: 1.0669 - lr: 2.5052e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 11s - loss: 6.9377 - mse: 6.9377 - mae: 1.0192 - val_loss: 5.0659 - val_mse: 5.0659 - val_mae: 1.0584 - lr: 2.5052e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 11s - loss: 6.6221 - mse: 6.6221 - mae: 0.9799 - val_loss: 5.1225 - val_mse: 5.1225 - val_mae: 1.0669 - lr: 2.5052e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 11s - loss: 6.3035 - mse: 6.3035 - mae: 0.9449 - val_loss: 5.3296 - val_mse: 5.3296 - val_mae: 1.0892 - lr: 2.5052e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 12s - loss: 6.0662 - mse: 6.0662 - mae: 0.9108 - val_loss: 5.4532 - val_mse: 5.4532 - val_mae: 1.0999 - lr: 2.5052e-04 - 12s/epoch - 12ms/step\n",
            "Score for fold 4: loss of 5.453213691711426\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 11s - loss: 4.3463 - mse: 4.3463 - mae: 0.9579 - val_loss: 11.3915 - val_mse: 11.3915 - val_mae: 0.8878 - lr: 2.5052e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 11s - loss: 4.0526 - mse: 4.0526 - mae: 0.9062 - val_loss: 11.4600 - val_mse: 11.4600 - val_mae: 0.9270 - lr: 2.5052e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 11s - loss: 3.7993 - mse: 3.7993 - mae: 0.8705 - val_loss: 11.5243 - val_mse: 11.5243 - val_mae: 0.9218 - lr: 2.5052e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 11s - loss: 3.5370 - mse: 3.5370 - mae: 0.8373 - val_loss: 11.7158 - val_mse: 11.7158 - val_mae: 0.9865 - lr: 2.5052e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 12s - loss: 3.3828 - mse: 3.3828 - mae: 0.8129 - val_loss: 11.7922 - val_mse: 11.7922 - val_mae: 0.9765 - lr: 2.5052e-04 - 12s/epoch - 12ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 11s - loss: 3.2172 - mse: 3.2172 - mae: 0.7801 - val_loss: 11.8082 - val_mse: 11.8082 - val_mae: 0.9900 - lr: 2.5052e-04 - 11s/epoch - 11ms/step\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-10-12 05:45:17,888]\u001b[0m Finished trial#39 resulted in value: 9.012. Current best value is 8.266 with parameters: {'activation': 'relu', 'num_hidden_layer': 4, 'i': 10, 'learning_rate': 0.0002464065452818332}.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Score for fold 5: loss of 11.808243751525879\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 8s - loss: 15.3814 - mse: 15.3814 - mae: 1.6313 - val_loss: 8.9172 - val_mse: 8.9172 - val_mae: 1.4724 - lr: 4.3095e-04 - 8s/epoch - 8ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 7s - loss: 14.7442 - mse: 14.7442 - mae: 1.5412 - val_loss: 9.0893 - val_mse: 9.0893 - val_mae: 1.4322 - lr: 4.3095e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 7s - loss: 14.6579 - mse: 14.6579 - mae: 1.5264 - val_loss: 8.9837 - val_mse: 8.9837 - val_mae: 1.5138 - lr: 4.3095e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 7s - loss: 14.5460 - mse: 14.5460 - mae: 1.5142 - val_loss: 8.7892 - val_mse: 8.7892 - val_mae: 1.5135 - lr: 4.3095e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 7s - loss: 14.3706 - mse: 14.3706 - mae: 1.5114 - val_loss: 8.6351 - val_mse: 8.6351 - val_mae: 1.4669 - lr: 4.3095e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 7s - loss: 14.2337 - mse: 14.2337 - mae: 1.5004 - val_loss: 8.6008 - val_mse: 8.6008 - val_mae: 1.5171 - lr: 4.3095e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 7s - loss: 14.1101 - mse: 14.1101 - mae: 1.4964 - val_loss: 8.7560 - val_mse: 8.7560 - val_mae: 1.5422 - lr: 4.3095e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 7s - loss: 14.0368 - mse: 14.0368 - mae: 1.4942 - val_loss: 8.6105 - val_mse: 8.6105 - val_mae: 1.5654 - lr: 4.3095e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 7s - loss: 13.9648 - mse: 13.9648 - mae: 1.4853 - val_loss: 8.5887 - val_mse: 8.5887 - val_mae: 1.5146 - lr: 4.3095e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 7s - loss: 13.8190 - mse: 13.8190 - mae: 1.4828 - val_loss: 8.6876 - val_mse: 8.6876 - val_mae: 1.4570 - lr: 4.3095e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 7s - loss: 13.6619 - mse: 13.6619 - mae: 1.4762 - val_loss: 8.7089 - val_mse: 8.7089 - val_mae: 1.5151 - lr: 4.3095e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 7s - loss: 13.6158 - mse: 13.6158 - mae: 1.4758 - val_loss: 8.6168 - val_mse: 8.6168 - val_mae: 1.5098 - lr: 4.3095e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 13/100\n",
            "1000/1000 - 7s - loss: 13.3938 - mse: 13.3938 - mae: 1.4662 - val_loss: 8.7731 - val_mse: 8.7731 - val_mae: 1.4967 - lr: 4.3095e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 14/100\n",
            "1000/1000 - 7s - loss: 13.3201 - mse: 13.3201 - mae: 1.4628 - val_loss: 8.5854 - val_mse: 8.5854 - val_mae: 1.4717 - lr: 4.3095e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 15/100\n",
            "1000/1000 - 7s - loss: 13.2038 - mse: 13.2038 - mae: 1.4564 - val_loss: 8.6350 - val_mse: 8.6350 - val_mae: 1.4249 - lr: 4.3095e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 16/100\n",
            "1000/1000 - 7s - loss: 13.0410 - mse: 13.0410 - mae: 1.4531 - val_loss: 8.6517 - val_mse: 8.6517 - val_mae: 1.4918 - lr: 4.3095e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 17/100\n",
            "1000/1000 - 7s - loss: 12.9114 - mse: 12.9114 - mae: 1.4522 - val_loss: 8.6739 - val_mse: 8.6739 - val_mae: 1.4689 - lr: 4.3095e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 18/100\n",
            "1000/1000 - 7s - loss: 12.8065 - mse: 12.8065 - mae: 1.4422 - val_loss: 8.5948 - val_mse: 8.5948 - val_mae: 1.5335 - lr: 4.3095e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 19/100\n",
            "1000/1000 - 7s - loss: 12.6617 - mse: 12.6617 - mae: 1.4380 - val_loss: 8.9121 - val_mse: 8.9121 - val_mae: 1.5576 - lr: 4.3095e-04 - 7s/epoch - 7ms/step\n",
            "Score for fold 1: loss of 8.912083625793457\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 7s - loss: 12.2486 - mse: 12.2486 - mae: 1.4568 - val_loss: 10.7207 - val_mse: 10.7207 - val_mae: 1.3991 - lr: 4.3095e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 7s - loss: 12.0762 - mse: 12.0762 - mae: 1.4442 - val_loss: 10.8250 - val_mse: 10.8250 - val_mae: 1.4547 - lr: 4.3095e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 7s - loss: 11.9883 - mse: 11.9883 - mae: 1.4377 - val_loss: 11.4921 - val_mse: 11.4921 - val_mae: 1.5191 - lr: 4.3095e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 7s - loss: 11.8171 - mse: 11.8171 - mae: 1.4293 - val_loss: 10.8901 - val_mse: 10.8901 - val_mae: 1.4360 - lr: 4.3095e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 7s - loss: 11.7325 - mse: 11.7325 - mae: 1.4234 - val_loss: 11.0575 - val_mse: 11.0575 - val_mae: 1.4732 - lr: 4.3095e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 7s - loss: 11.6635 - mse: 11.6635 - mae: 1.4233 - val_loss: 10.4839 - val_mse: 10.4839 - val_mae: 1.4831 - lr: 4.3095e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 7s - loss: 11.4689 - mse: 11.4689 - mae: 1.4159 - val_loss: 10.8682 - val_mse: 10.8682 - val_mae: 1.4798 - lr: 4.3095e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 7s - loss: 11.3352 - mse: 11.3352 - mae: 1.4140 - val_loss: 11.0525 - val_mse: 11.0525 - val_mae: 1.4835 - lr: 4.3095e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 7s - loss: 11.2881 - mse: 11.2881 - mae: 1.4034 - val_loss: 11.0613 - val_mse: 11.0613 - val_mae: 1.5340 - lr: 4.3095e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 7s - loss: 11.1892 - mse: 11.1892 - mae: 1.3995 - val_loss: 10.7770 - val_mse: 10.7770 - val_mae: 1.5502 - lr: 4.3095e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 7s - loss: 11.0484 - mse: 11.0484 - mae: 1.3959 - val_loss: 10.6691 - val_mse: 10.6691 - val_mae: 1.5121 - lr: 4.3095e-04 - 7s/epoch - 7ms/step\n",
            "Score for fold 2: loss of 10.669053077697754\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 7s - loss: 12.1541 - mse: 12.1541 - mae: 1.4431 - val_loss: 6.1915 - val_mse: 6.1915 - val_mae: 1.3440 - lr: 4.3095e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 7s - loss: 11.9739 - mse: 11.9739 - mae: 1.4262 - val_loss: 6.3379 - val_mse: 6.3379 - val_mae: 1.3764 - lr: 4.3095e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 7s - loss: 11.8516 - mse: 11.8516 - mae: 1.4129 - val_loss: 6.3772 - val_mse: 6.3772 - val_mae: 1.4078 - lr: 4.3095e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 7s - loss: 11.6655 - mse: 11.6655 - mae: 1.4065 - val_loss: 6.3305 - val_mse: 6.3305 - val_mae: 1.4185 - lr: 4.3095e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 7s - loss: 11.4988 - mse: 11.4988 - mae: 1.3985 - val_loss: 6.5527 - val_mse: 6.5527 - val_mae: 1.4086 - lr: 4.3095e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 7s - loss: 11.3998 - mse: 11.3998 - mae: 1.3922 - val_loss: 6.5677 - val_mse: 6.5677 - val_mae: 1.4853 - lr: 4.3095e-04 - 7s/epoch - 7ms/step\n",
            "Score for fold 3: loss of 6.567701816558838\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 7s - loss: 9.4641 - mse: 9.4641 - mae: 1.4028 - val_loss: 14.2366 - val_mse: 14.2366 - val_mae: 1.3968 - lr: 4.3095e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 7s - loss: 9.2403 - mse: 9.2403 - mae: 1.3856 - val_loss: 14.5502 - val_mse: 14.5502 - val_mae: 1.4312 - lr: 4.3095e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 7s - loss: 9.0568 - mse: 9.0568 - mae: 1.3734 - val_loss: 14.7201 - val_mse: 14.7201 - val_mae: 1.4368 - lr: 4.3095e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 7s - loss: 9.0206 - mse: 9.0206 - mae: 1.3637 - val_loss: 14.8630 - val_mse: 14.8630 - val_mae: 1.5783 - lr: 4.3095e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 7s - loss: 8.7994 - mse: 8.7994 - mae: 1.3552 - val_loss: 15.1154 - val_mse: 15.1154 - val_mae: 1.5335 - lr: 4.3095e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 7s - loss: 8.6943 - mse: 8.6943 - mae: 1.3517 - val_loss: 15.0629 - val_mse: 15.0629 - val_mae: 1.5005 - lr: 4.3095e-04 - 7s/epoch - 7ms/step\n",
            "Score for fold 4: loss of 15.062938690185547\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 7s - loss: 8.7996 - mse: 8.7996 - mae: 1.3870 - val_loss: 14.4670 - val_mse: 14.4670 - val_mae: 1.3606 - lr: 4.3095e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 7s - loss: 8.6273 - mse: 8.6273 - mae: 1.3764 - val_loss: 14.7612 - val_mse: 14.7612 - val_mae: 1.3766 - lr: 4.3095e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 7s - loss: 8.4762 - mse: 8.4762 - mae: 1.3620 - val_loss: 14.8904 - val_mse: 14.8904 - val_mae: 1.3964 - lr: 4.3095e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 7s - loss: 8.2932 - mse: 8.2932 - mae: 1.3535 - val_loss: 14.8928 - val_mse: 14.8928 - val_mae: 1.3977 - lr: 4.3095e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 7s - loss: 8.2356 - mse: 8.2356 - mae: 1.3452 - val_loss: 15.0686 - val_mse: 15.0686 - val_mae: 1.4675 - lr: 4.3095e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 7s - loss: 8.0392 - mse: 8.0392 - mae: 1.3350 - val_loss: 14.9644 - val_mse: 14.9644 - val_mae: 1.4311 - lr: 4.3095e-04 - 7s/epoch - 7ms/step\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-10-12 05:50:55,531]\u001b[0m Finished trial#40 resulted in value: 11.234. Current best value is 8.266 with parameters: {'activation': 'relu', 'num_hidden_layer': 4, 'i': 10, 'learning_rate': 0.0002464065452818332}.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Score for fold 5: loss of 14.964362144470215\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 12s - loss: 15.3212 - mse: 15.3212 - mae: 1.6385 - val_loss: 10.6046 - val_mse: 10.6046 - val_mae: 1.4591 - lr: 2.4724e-04 - 12s/epoch - 12ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 11s - loss: 14.6062 - mse: 14.6062 - mae: 1.5547 - val_loss: 10.9037 - val_mse: 10.9037 - val_mae: 1.7564 - lr: 2.4724e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 11s - loss: 14.4455 - mse: 14.4455 - mae: 1.5364 - val_loss: 10.4097 - val_mse: 10.4097 - val_mae: 1.5174 - lr: 2.4724e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 11s - loss: 14.2719 - mse: 14.2719 - mae: 1.5223 - val_loss: 9.9933 - val_mse: 9.9933 - val_mae: 1.4702 - lr: 2.4724e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 11s - loss: 14.1930 - mse: 14.1930 - mae: 1.5182 - val_loss: 9.9573 - val_mse: 9.9573 - val_mae: 1.5972 - lr: 2.4724e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 11s - loss: 14.0272 - mse: 14.0272 - mae: 1.5100 - val_loss: 9.9359 - val_mse: 9.9359 - val_mae: 1.4356 - lr: 2.4724e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 11s - loss: 13.9522 - mse: 13.9522 - mae: 1.5076 - val_loss: 9.9711 - val_mse: 9.9711 - val_mae: 1.4933 - lr: 2.4724e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 11s - loss: 13.8944 - mse: 13.8944 - mae: 1.4977 - val_loss: 9.9893 - val_mse: 9.9893 - val_mae: 1.5735 - lr: 2.4724e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 11s - loss: 13.7481 - mse: 13.7481 - mae: 1.4905 - val_loss: 10.1366 - val_mse: 10.1366 - val_mae: 1.5678 - lr: 2.4724e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 11s - loss: 13.7055 - mse: 13.7055 - mae: 1.4895 - val_loss: 9.8576 - val_mse: 9.8576 - val_mae: 1.3877 - lr: 2.4724e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 11s - loss: 13.5957 - mse: 13.5957 - mae: 1.4844 - val_loss: 9.8056 - val_mse: 9.8056 - val_mae: 1.5269 - lr: 2.4724e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 11s - loss: 13.5484 - mse: 13.5484 - mae: 1.4791 - val_loss: 9.5836 - val_mse: 9.5836 - val_mae: 1.4121 - lr: 2.4724e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 13/100\n",
            "1000/1000 - 11s - loss: 13.5267 - mse: 13.5267 - mae: 1.4760 - val_loss: 9.7188 - val_mse: 9.7188 - val_mae: 1.3727 - lr: 2.4724e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 14/100\n",
            "1000/1000 - 11s - loss: 13.3938 - mse: 13.3938 - mae: 1.4700 - val_loss: 9.6975 - val_mse: 9.6975 - val_mae: 1.4077 - lr: 2.4724e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 15/100\n",
            "1000/1000 - 11s - loss: 13.3034 - mse: 13.3034 - mae: 1.4675 - val_loss: 9.4600 - val_mse: 9.4600 - val_mae: 1.4214 - lr: 2.4724e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 16/100\n",
            "1000/1000 - 11s - loss: 13.2458 - mse: 13.2458 - mae: 1.4547 - val_loss: 9.3918 - val_mse: 9.3918 - val_mae: 1.4406 - lr: 2.4724e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 17/100\n",
            "1000/1000 - 11s - loss: 13.1339 - mse: 13.1339 - mae: 1.4544 - val_loss: 9.3910 - val_mse: 9.3910 - val_mae: 1.5328 - lr: 2.4724e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 18/100\n",
            "1000/1000 - 12s - loss: 13.0542 - mse: 13.0542 - mae: 1.4526 - val_loss: 9.3554 - val_mse: 9.3554 - val_mae: 1.4766 - lr: 2.4724e-04 - 12s/epoch - 12ms/step\n",
            "Epoch 19/100\n",
            "1000/1000 - 12s - loss: 12.9854 - mse: 12.9854 - mae: 1.4505 - val_loss: 9.4077 - val_mse: 9.4077 - val_mae: 1.4101 - lr: 2.4724e-04 - 12s/epoch - 12ms/step\n",
            "Epoch 20/100\n",
            "1000/1000 - 11s - loss: 12.8706 - mse: 12.8706 - mae: 1.4370 - val_loss: 9.4466 - val_mse: 9.4466 - val_mae: 1.5864 - lr: 2.4724e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 21/100\n",
            "1000/1000 - 11s - loss: 12.7576 - mse: 12.7576 - mae: 1.4356 - val_loss: 9.2348 - val_mse: 9.2348 - val_mae: 1.4222 - lr: 2.4724e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 22/100\n",
            "1000/1000 - 11s - loss: 12.5987 - mse: 12.5987 - mae: 1.4284 - val_loss: 9.3702 - val_mse: 9.3702 - val_mae: 1.4535 - lr: 2.4724e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 23/100\n",
            "1000/1000 - 11s - loss: 12.5408 - mse: 12.5408 - mae: 1.4241 - val_loss: 9.5409 - val_mse: 9.5409 - val_mae: 1.5357 - lr: 2.4724e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 24/100\n",
            "1000/1000 - 11s - loss: 12.4031 - mse: 12.4031 - mae: 1.4131 - val_loss: 9.2400 - val_mse: 9.2400 - val_mae: 1.4719 - lr: 2.4724e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 25/100\n",
            "1000/1000 - 11s - loss: 12.1273 - mse: 12.1273 - mae: 1.4042 - val_loss: 9.4206 - val_mse: 9.4206 - val_mae: 1.4890 - lr: 2.4724e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 26/100\n",
            "1000/1000 - 11s - loss: 12.0835 - mse: 12.0835 - mae: 1.4000 - val_loss: 9.7929 - val_mse: 9.7929 - val_mae: 1.5617 - lr: 2.4724e-04 - 11s/epoch - 11ms/step\n",
            "Score for fold 1: loss of 9.792925834655762\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 11s - loss: 10.2860 - mse: 10.2860 - mae: 1.4128 - val_loss: 17.0416 - val_mse: 17.0416 - val_mae: 1.4401 - lr: 2.4724e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 11s - loss: 10.0693 - mse: 10.0693 - mae: 1.3979 - val_loss: 16.5686 - val_mse: 16.5686 - val_mae: 1.4706 - lr: 2.4724e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 11s - loss: 9.8382 - mse: 9.8382 - mae: 1.3864 - val_loss: 16.7044 - val_mse: 16.7044 - val_mae: 1.4149 - lr: 2.4724e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 11s - loss: 9.6551 - mse: 9.6551 - mae: 1.3706 - val_loss: 16.8570 - val_mse: 16.8570 - val_mae: 1.4405 - lr: 2.4724e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 11s - loss: 9.4521 - mse: 9.4521 - mae: 1.3525 - val_loss: 16.5604 - val_mse: 16.5604 - val_mae: 1.4717 - lr: 2.4724e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 11s - loss: 9.2824 - mse: 9.2824 - mae: 1.3430 - val_loss: 16.6708 - val_mse: 16.6708 - val_mae: 1.4082 - lr: 2.4724e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 11s - loss: 9.0455 - mse: 9.0455 - mae: 1.3238 - val_loss: 17.0778 - val_mse: 17.0778 - val_mae: 1.4478 - lr: 2.4724e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 11s - loss: 8.7389 - mse: 8.7389 - mae: 1.3019 - val_loss: 16.9708 - val_mse: 16.9708 - val_mae: 1.4580 - lr: 2.4724e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 11s - loss: 8.5272 - mse: 8.5272 - mae: 1.2829 - val_loss: 16.7322 - val_mse: 16.7322 - val_mae: 1.4451 - lr: 2.4724e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 11s - loss: 8.0565 - mse: 8.0565 - mae: 1.2567 - val_loss: 17.4041 - val_mse: 17.4041 - val_mae: 1.4477 - lr: 2.4724e-04 - 11s/epoch - 11ms/step\n",
            "Score for fold 2: loss of 17.404071807861328\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 11s - loss: 8.9661 - mse: 8.9661 - mae: 1.3006 - val_loss: 12.3986 - val_mse: 12.3986 - val_mae: 1.2774 - lr: 2.4724e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 11s - loss: 8.6161 - mse: 8.6161 - mae: 1.2713 - val_loss: 12.4865 - val_mse: 12.4865 - val_mae: 1.2867 - lr: 2.4724e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 11s - loss: 8.2907 - mse: 8.2907 - mae: 1.2408 - val_loss: 12.7481 - val_mse: 12.7481 - val_mae: 1.2859 - lr: 2.4724e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 11s - loss: 7.8836 - mse: 7.8836 - mae: 1.2083 - val_loss: 12.8357 - val_mse: 12.8357 - val_mae: 1.3630 - lr: 2.4724e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 11s - loss: 7.6145 - mse: 7.6145 - mae: 1.1759 - val_loss: 12.4657 - val_mse: 12.4657 - val_mae: 1.3213 - lr: 2.4724e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 11s - loss: 7.2847 - mse: 7.2847 - mae: 1.1466 - val_loss: 12.8189 - val_mse: 12.8189 - val_mae: 1.3535 - lr: 2.4724e-04 - 11s/epoch - 11ms/step\n",
            "Score for fold 3: loss of 12.818848609924316\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 11s - loss: 9.0330 - mse: 9.0330 - mae: 1.2047 - val_loss: 4.8561 - val_mse: 4.8561 - val_mae: 1.0747 - lr: 2.4724e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 11s - loss: 8.5524 - mse: 8.5524 - mae: 1.1546 - val_loss: 5.0127 - val_mse: 5.0127 - val_mae: 1.1335 - lr: 2.4724e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 11s - loss: 8.2315 - mse: 8.2315 - mae: 1.1255 - val_loss: 5.1065 - val_mse: 5.1065 - val_mae: 1.1529 - lr: 2.4724e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 11s - loss: 7.8944 - mse: 7.8944 - mae: 1.0923 - val_loss: 5.1430 - val_mse: 5.1430 - val_mae: 1.1435 - lr: 2.4724e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 11s - loss: 7.5297 - mse: 7.5297 - mae: 1.0497 - val_loss: 5.1179 - val_mse: 5.1179 - val_mae: 1.1821 - lr: 2.4724e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 11s - loss: 7.2320 - mse: 7.2320 - mae: 1.0173 - val_loss: 5.1734 - val_mse: 5.1734 - val_mae: 1.2341 - lr: 2.4724e-04 - 11s/epoch - 11ms/step\n",
            "Score for fold 4: loss of 5.173357963562012\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 11s - loss: 7.5818 - mse: 7.5818 - mae: 1.0716 - val_loss: 3.9742 - val_mse: 3.9742 - val_mae: 0.9724 - lr: 2.4724e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 11s - loss: 7.0825 - mse: 7.0825 - mae: 1.0267 - val_loss: 4.1159 - val_mse: 4.1159 - val_mae: 1.0244 - lr: 2.4724e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 11s - loss: 6.7274 - mse: 6.7274 - mae: 0.9802 - val_loss: 4.1611 - val_mse: 4.1611 - val_mae: 0.9969 - lr: 2.4724e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 12s - loss: 6.4934 - mse: 6.4934 - mae: 0.9468 - val_loss: 4.3335 - val_mse: 4.3335 - val_mae: 1.0162 - lr: 2.4724e-04 - 12s/epoch - 12ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 12s - loss: 6.2197 - mse: 6.2197 - mae: 0.9159 - val_loss: 4.5414 - val_mse: 4.5414 - val_mae: 1.0495 - lr: 2.4724e-04 - 12s/epoch - 12ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 11s - loss: 6.0590 - mse: 6.0590 - mae: 0.8895 - val_loss: 4.6908 - val_mse: 4.6908 - val_mae: 1.0866 - lr: 2.4724e-04 - 11s/epoch - 11ms/step\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-10-12 06:01:16,722]\u001b[0m Finished trial#41 resulted in value: 9.974. Current best value is 8.266 with parameters: {'activation': 'relu', 'num_hidden_layer': 4, 'i': 10, 'learning_rate': 0.0002464065452818332}.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Score for fold 5: loss of 4.690849781036377\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 12s - loss: 21.1809 - mse: 21.1809 - mae: 2.1227 - val_loss: 10.2541 - val_mse: 10.2541 - val_mae: 1.7340 - lr: 0.0095 - 12s/epoch - 12ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 11s - loss: 17.0819 - mse: 17.0819 - mae: 1.9437 - val_loss: 9.3554 - val_mse: 9.3554 - val_mae: 1.6059 - lr: 0.0095 - 11s/epoch - 11ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 11s - loss: 16.8358 - mse: 16.8358 - mae: 1.9584 - val_loss: 9.9356 - val_mse: 9.9356 - val_mae: 1.7696 - lr: 0.0095 - 11s/epoch - 11ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 11s - loss: 16.8928 - mse: 16.8928 - mae: 2.0077 - val_loss: 9.7863 - val_mse: 9.7863 - val_mae: 2.0086 - lr: 0.0095 - 11s/epoch - 11ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 12s - loss: 16.8000 - mse: 16.8000 - mae: 1.9857 - val_loss: 9.3447 - val_mse: 9.3447 - val_mae: 1.5789 - lr: 0.0095 - 12s/epoch - 12ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 11s - loss: 16.5932 - mse: 16.5932 - mae: 1.9839 - val_loss: 9.7507 - val_mse: 9.7507 - val_mae: 1.4796 - lr: 0.0095 - 11s/epoch - 11ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 11s - loss: 16.6439 - mse: 16.6439 - mae: 1.9802 - val_loss: 14.4661 - val_mse: 14.4661 - val_mae: 2.5355 - lr: 0.0095 - 11s/epoch - 11ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 11s - loss: 16.5856 - mse: 16.5856 - mae: 1.9589 - val_loss: 10.1067 - val_mse: 10.1067 - val_mae: 1.5910 - lr: 0.0095 - 11s/epoch - 11ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 12s - loss: 16.7154 - mse: 16.7154 - mae: 2.0011 - val_loss: 9.9592 - val_mse: 9.9592 - val_mae: 1.7420 - lr: 0.0095 - 12s/epoch - 12ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 11s - loss: 16.6239 - mse: 16.6239 - mae: 1.9850 - val_loss: 10.0813 - val_mse: 10.0813 - val_mae: 1.5787 - lr: 0.0095 - 11s/epoch - 11ms/step\n",
            "Score for fold 1: loss of 10.081339836120605\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 11s - loss: 14.4584 - mse: 14.4584 - mae: 1.6662 - val_loss: 12.1954 - val_mse: 12.1954 - val_mae: 1.5082 - lr: 0.0019 - 11s/epoch - 11ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 11s - loss: 14.1938 - mse: 14.1938 - mae: 1.6396 - val_loss: 11.3128 - val_mse: 11.3128 - val_mae: 1.7479 - lr: 0.0019 - 11s/epoch - 11ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 11s - loss: 14.0329 - mse: 14.0329 - mae: 1.6361 - val_loss: 11.4936 - val_mse: 11.4936 - val_mae: 1.5293 - lr: 0.0019 - 11s/epoch - 11ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 11s - loss: 13.8795 - mse: 13.8795 - mae: 1.6176 - val_loss: 11.0795 - val_mse: 11.0795 - val_mae: 1.5775 - lr: 0.0019 - 11s/epoch - 11ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 11s - loss: 13.8204 - mse: 13.8204 - mae: 1.6153 - val_loss: 11.3616 - val_mse: 11.3616 - val_mae: 1.8183 - lr: 0.0019 - 11s/epoch - 11ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 11s - loss: 13.5552 - mse: 13.5552 - mae: 1.5951 - val_loss: 11.1313 - val_mse: 11.1313 - val_mae: 1.4839 - lr: 0.0019 - 11s/epoch - 11ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 11s - loss: 13.4660 - mse: 13.4660 - mae: 1.5776 - val_loss: 10.9780 - val_mse: 10.9780 - val_mae: 1.4562 - lr: 0.0019 - 11s/epoch - 11ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 12s - loss: 13.4507 - mse: 13.4507 - mae: 1.5985 - val_loss: 10.7435 - val_mse: 10.7435 - val_mae: 1.4637 - lr: 0.0019 - 12s/epoch - 12ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 12s - loss: 13.3918 - mse: 13.3918 - mae: 1.5901 - val_loss: 10.9260 - val_mse: 10.9260 - val_mae: 1.5688 - lr: 0.0019 - 12s/epoch - 12ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 12s - loss: 13.3067 - mse: 13.3067 - mae: 1.5713 - val_loss: 10.6613 - val_mse: 10.6613 - val_mae: 1.6043 - lr: 0.0019 - 12s/epoch - 12ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 11s - loss: 13.2701 - mse: 13.2701 - mae: 1.5833 - val_loss: 10.8172 - val_mse: 10.8172 - val_mae: 1.4869 - lr: 0.0019 - 11s/epoch - 11ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 11s - loss: 13.2727 - mse: 13.2727 - mae: 1.5902 - val_loss: 10.6975 - val_mse: 10.6975 - val_mae: 1.5004 - lr: 0.0019 - 11s/epoch - 11ms/step\n",
            "Epoch 13/100\n",
            "1000/1000 - 11s - loss: 13.2172 - mse: 13.2172 - mae: 1.5797 - val_loss: 11.2802 - val_mse: 11.2802 - val_mae: 1.4702 - lr: 0.0019 - 11s/epoch - 11ms/step\n",
            "Epoch 14/100\n",
            "1000/1000 - 12s - loss: 13.2457 - mse: 13.2457 - mae: 1.5831 - val_loss: 10.8332 - val_mse: 10.8332 - val_mae: 1.5139 - lr: 0.0019 - 12s/epoch - 12ms/step\n",
            "Epoch 15/100\n",
            "1000/1000 - 11s - loss: 13.2368 - mse: 13.2368 - mae: 1.5875 - val_loss: 10.8123 - val_mse: 10.8123 - val_mae: 1.5543 - lr: 0.0019 - 11s/epoch - 11ms/step\n",
            "Score for fold 2: loss of 10.812308311462402\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 11s - loss: 10.6835 - mse: 10.6835 - mae: 1.5149 - val_loss: 19.4141 - val_mse: 19.4141 - val_mae: 1.5224 - lr: 0.0010 - 11s/epoch - 11ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 11s - loss: 10.5597 - mse: 10.5597 - mae: 1.4995 - val_loss: 19.5781 - val_mse: 19.5781 - val_mae: 1.4999 - lr: 0.0010 - 11s/epoch - 11ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 11s - loss: 10.4702 - mse: 10.4702 - mae: 1.4992 - val_loss: 19.7446 - val_mse: 19.7446 - val_mae: 1.4583 - lr: 0.0010 - 11s/epoch - 11ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 11s - loss: 10.3927 - mse: 10.3927 - mae: 1.4905 - val_loss: 19.6589 - val_mse: 19.6589 - val_mae: 1.4983 - lr: 0.0010 - 11s/epoch - 11ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 12s - loss: 10.3767 - mse: 10.3767 - mae: 1.4892 - val_loss: 19.7923 - val_mse: 19.7923 - val_mae: 1.5644 - lr: 0.0010 - 12s/epoch - 12ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 11s - loss: 10.3146 - mse: 10.3146 - mae: 1.4904 - val_loss: 19.8030 - val_mse: 19.8030 - val_mae: 1.4667 - lr: 0.0010 - 11s/epoch - 11ms/step\n",
            "Score for fold 3: loss of 19.8029842376709\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 11s - loss: 12.7652 - mse: 12.7652 - mae: 1.5051 - val_loss: 9.6385 - val_mse: 9.6385 - val_mae: 1.5552 - lr: 0.0010 - 11s/epoch - 11ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 11s - loss: 12.6786 - mse: 12.6786 - mae: 1.5041 - val_loss: 9.8596 - val_mse: 9.8596 - val_mae: 1.4255 - lr: 0.0010 - 11s/epoch - 11ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 11s - loss: 12.6272 - mse: 12.6272 - mae: 1.4930 - val_loss: 9.8781 - val_mse: 9.8781 - val_mae: 1.4816 - lr: 0.0010 - 11s/epoch - 11ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 11s - loss: 12.5650 - mse: 12.5650 - mae: 1.4914 - val_loss: 9.8032 - val_mse: 9.8032 - val_mae: 1.5572 - lr: 0.0010 - 11s/epoch - 11ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 11s - loss: 12.4220 - mse: 12.4220 - mae: 1.4845 - val_loss: 10.0030 - val_mse: 10.0030 - val_mae: 1.4541 - lr: 0.0010 - 11s/epoch - 11ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 11s - loss: 12.4344 - mse: 12.4344 - mae: 1.4863 - val_loss: 9.9430 - val_mse: 9.9430 - val_mae: 1.5724 - lr: 0.0010 - 11s/epoch - 11ms/step\n",
            "Score for fold 4: loss of 9.942994117736816\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 12s - loss: 11.4439 - mse: 11.4439 - mae: 1.4855 - val_loss: 13.9284 - val_mse: 13.9284 - val_mae: 1.6353 - lr: 0.0010 - 12s/epoch - 12ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 11s - loss: 11.3616 - mse: 11.3616 - mae: 1.4800 - val_loss: 13.9090 - val_mse: 13.9090 - val_mae: 1.6142 - lr: 0.0010 - 11s/epoch - 11ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 11s - loss: 11.3043 - mse: 11.3043 - mae: 1.4857 - val_loss: 14.0549 - val_mse: 14.0549 - val_mae: 1.5503 - lr: 0.0010 - 11s/epoch - 11ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 11s - loss: 11.2061 - mse: 11.2061 - mae: 1.4687 - val_loss: 14.2464 - val_mse: 14.2464 - val_mae: 1.6817 - lr: 0.0010 - 11s/epoch - 11ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 11s - loss: 11.1097 - mse: 11.1097 - mae: 1.4715 - val_loss: 14.1046 - val_mse: 14.1046 - val_mae: 1.5578 - lr: 0.0010 - 11s/epoch - 11ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 11s - loss: 11.0986 - mse: 11.0986 - mae: 1.4663 - val_loss: 14.4835 - val_mse: 14.4835 - val_mae: 1.4398 - lr: 0.0010 - 11s/epoch - 11ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 11s - loss: 11.0890 - mse: 11.0890 - mae: 1.4681 - val_loss: 14.2093 - val_mse: 14.2093 - val_mae: 1.6458 - lr: 0.0010 - 11s/epoch - 11ms/step\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-10-12 06:09:49,470]\u001b[0m Finished trial#42 resulted in value: 12.968. Current best value is 8.266 with parameters: {'activation': 'relu', 'num_hidden_layer': 4, 'i': 10, 'learning_rate': 0.0002464065452818332}.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Score for fold 5: loss of 14.209283828735352\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 13s - loss: 14.9972 - mse: 14.9972 - mae: 1.6196 - val_loss: 12.4411 - val_mse: 12.4411 - val_mae: 1.6165 - lr: 2.1725e-04 - 13s/epoch - 13ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 12s - loss: 14.4039 - mse: 14.4039 - mae: 1.5370 - val_loss: 11.3146 - val_mse: 11.3146 - val_mae: 1.5732 - lr: 2.1725e-04 - 12s/epoch - 12ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 12s - loss: 14.1979 - mse: 14.1979 - mae: 1.5232 - val_loss: 10.9608 - val_mse: 10.9608 - val_mae: 1.4894 - lr: 2.1725e-04 - 12s/epoch - 12ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 12s - loss: 14.0696 - mse: 14.0696 - mae: 1.5121 - val_loss: 10.8472 - val_mse: 10.8472 - val_mae: 1.4751 - lr: 2.1725e-04 - 12s/epoch - 12ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 12s - loss: 13.9147 - mse: 13.9147 - mae: 1.5082 - val_loss: 10.7765 - val_mse: 10.7765 - val_mae: 1.5072 - lr: 2.1725e-04 - 12s/epoch - 12ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 11s - loss: 13.8080 - mse: 13.8080 - mae: 1.4988 - val_loss: 10.5464 - val_mse: 10.5464 - val_mae: 1.4875 - lr: 2.1725e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 12s - loss: 13.7180 - mse: 13.7180 - mae: 1.4955 - val_loss: 10.7749 - val_mse: 10.7749 - val_mae: 1.5394 - lr: 2.1725e-04 - 12s/epoch - 12ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 12s - loss: 13.6316 - mse: 13.6316 - mae: 1.4923 - val_loss: 10.5790 - val_mse: 10.5790 - val_mae: 1.5130 - lr: 2.1725e-04 - 12s/epoch - 12ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 12s - loss: 13.5081 - mse: 13.5081 - mae: 1.4835 - val_loss: 10.3883 - val_mse: 10.3883 - val_mae: 1.4641 - lr: 2.1725e-04 - 12s/epoch - 12ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 12s - loss: 13.4803 - mse: 13.4803 - mae: 1.4844 - val_loss: 10.6520 - val_mse: 10.6520 - val_mae: 1.5627 - lr: 2.1725e-04 - 12s/epoch - 12ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 12s - loss: 13.4134 - mse: 13.4134 - mae: 1.4737 - val_loss: 10.4991 - val_mse: 10.4991 - val_mae: 1.5546 - lr: 2.1725e-04 - 12s/epoch - 12ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 12s - loss: 13.2962 - mse: 13.2962 - mae: 1.4685 - val_loss: 10.5096 - val_mse: 10.5096 - val_mae: 1.5915 - lr: 2.1725e-04 - 12s/epoch - 12ms/step\n",
            "Epoch 13/100\n",
            "1000/1000 - 12s - loss: 13.1128 - mse: 13.1128 - mae: 1.4670 - val_loss: 10.7358 - val_mse: 10.7358 - val_mae: 1.5617 - lr: 2.1725e-04 - 12s/epoch - 12ms/step\n",
            "Epoch 14/100\n",
            "1000/1000 - 12s - loss: 13.2090 - mse: 13.2090 - mae: 1.4620 - val_loss: 10.3669 - val_mse: 10.3669 - val_mae: 1.4691 - lr: 2.1725e-04 - 12s/epoch - 12ms/step\n",
            "Epoch 15/100\n",
            "1000/1000 - 12s - loss: 12.9388 - mse: 12.9388 - mae: 1.4558 - val_loss: 10.2422 - val_mse: 10.2422 - val_mae: 1.5436 - lr: 2.1725e-04 - 12s/epoch - 12ms/step\n",
            "Epoch 16/100\n",
            "1000/1000 - 12s - loss: 12.8555 - mse: 12.8555 - mae: 1.4556 - val_loss: 10.3356 - val_mse: 10.3356 - val_mae: 1.4965 - lr: 2.1725e-04 - 12s/epoch - 12ms/step\n",
            "Epoch 17/100\n",
            "1000/1000 - 12s - loss: 12.7849 - mse: 12.7849 - mae: 1.4414 - val_loss: 10.1762 - val_mse: 10.1762 - val_mae: 1.5436 - lr: 2.1725e-04 - 12s/epoch - 12ms/step\n",
            "Epoch 18/100\n",
            "1000/1000 - 11s - loss: 12.6695 - mse: 12.6695 - mae: 1.4409 - val_loss: 10.5915 - val_mse: 10.5915 - val_mae: 1.6518 - lr: 2.1725e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 19/100\n",
            "1000/1000 - 12s - loss: 12.4798 - mse: 12.4798 - mae: 1.4288 - val_loss: 10.2970 - val_mse: 10.2970 - val_mae: 1.5735 - lr: 2.1725e-04 - 12s/epoch - 12ms/step\n",
            "Epoch 20/100\n",
            "1000/1000 - 12s - loss: 12.3084 - mse: 12.3084 - mae: 1.4209 - val_loss: 10.2104 - val_mse: 10.2104 - val_mae: 1.4572 - lr: 2.1725e-04 - 12s/epoch - 12ms/step\n",
            "Epoch 21/100\n",
            "1000/1000 - 12s - loss: 12.2316 - mse: 12.2316 - mae: 1.4141 - val_loss: 10.3418 - val_mse: 10.3418 - val_mae: 1.4470 - lr: 2.1725e-04 - 12s/epoch - 12ms/step\n",
            "Epoch 22/100\n",
            "1000/1000 - 12s - loss: 12.1342 - mse: 12.1342 - mae: 1.4032 - val_loss: 10.2405 - val_mse: 10.2405 - val_mae: 1.5561 - lr: 2.1725e-04 - 12s/epoch - 12ms/step\n",
            "Score for fold 1: loss of 10.2405424118042\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 12s - loss: 11.0210 - mse: 11.0210 - mae: 1.4160 - val_loss: 14.7987 - val_mse: 14.7987 - val_mae: 1.4184 - lr: 2.1725e-04 - 12s/epoch - 12ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 11s - loss: 10.7929 - mse: 10.7929 - mae: 1.4006 - val_loss: 14.6250 - val_mse: 14.6250 - val_mae: 1.4688 - lr: 2.1725e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 12s - loss: 10.6664 - mse: 10.6664 - mae: 1.3914 - val_loss: 14.5399 - val_mse: 14.5399 - val_mae: 1.5017 - lr: 2.1725e-04 - 12s/epoch - 12ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 12s - loss: 10.5675 - mse: 10.5675 - mae: 1.3762 - val_loss: 14.6891 - val_mse: 14.6891 - val_mae: 1.5599 - lr: 2.1725e-04 - 12s/epoch - 12ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 12s - loss: 10.3448 - mse: 10.3448 - mae: 1.3630 - val_loss: 14.8265 - val_mse: 14.8265 - val_mae: 1.5326 - lr: 2.1725e-04 - 12s/epoch - 12ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 12s - loss: 10.2041 - mse: 10.2041 - mae: 1.3463 - val_loss: 15.0144 - val_mse: 15.0144 - val_mae: 1.4839 - lr: 2.1725e-04 - 12s/epoch - 12ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 12s - loss: 9.9650 - mse: 9.9650 - mae: 1.3281 - val_loss: 15.2461 - val_mse: 15.2461 - val_mae: 1.4603 - lr: 2.1725e-04 - 12s/epoch - 12ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 12s - loss: 9.7292 - mse: 9.7292 - mae: 1.3156 - val_loss: 15.8465 - val_mse: 15.8465 - val_mae: 1.5572 - lr: 2.1725e-04 - 12s/epoch - 12ms/step\n",
            "Score for fold 2: loss of 15.846517562866211\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 12s - loss: 8.5072 - mse: 8.5072 - mae: 1.3458 - val_loss: 18.8731 - val_mse: 18.8731 - val_mae: 1.3416 - lr: 2.1725e-04 - 12s/epoch - 12ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 12s - loss: 8.2863 - mse: 8.2863 - mae: 1.3243 - val_loss: 19.3249 - val_mse: 19.3249 - val_mae: 1.3630 - lr: 2.1725e-04 - 12s/epoch - 12ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 12s - loss: 7.9575 - mse: 7.9575 - mae: 1.3010 - val_loss: 19.1374 - val_mse: 19.1374 - val_mae: 1.3817 - lr: 2.1725e-04 - 12s/epoch - 12ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 11s - loss: 7.6188 - mse: 7.6188 - mae: 1.2803 - val_loss: 19.3901 - val_mse: 19.3901 - val_mae: 1.4031 - lr: 2.1725e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 11s - loss: 7.4053 - mse: 7.4053 - mae: 1.2616 - val_loss: 19.3485 - val_mse: 19.3485 - val_mae: 1.3894 - lr: 2.1725e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 11s - loss: 7.0623 - mse: 7.0623 - mae: 1.2319 - val_loss: 19.5382 - val_mse: 19.5382 - val_mae: 1.4540 - lr: 2.1725e-04 - 11s/epoch - 11ms/step\n",
            "Score for fold 3: loss of 19.53822898864746\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 12s - loss: 10.5138 - mse: 10.5138 - mae: 1.2843 - val_loss: 4.7490 - val_mse: 4.7490 - val_mae: 1.2209 - lr: 2.1725e-04 - 12s/epoch - 12ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 11s - loss: 10.1468 - mse: 10.1468 - mae: 1.2522 - val_loss: 4.8662 - val_mse: 4.8662 - val_mae: 1.1934 - lr: 2.1725e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 11s - loss: 9.8585 - mse: 9.8585 - mae: 1.2236 - val_loss: 4.9203 - val_mse: 4.9203 - val_mae: 1.2247 - lr: 2.1725e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 12s - loss: 9.3878 - mse: 9.3878 - mae: 1.1940 - val_loss: 5.1895 - val_mse: 5.1895 - val_mae: 1.2297 - lr: 2.1725e-04 - 12s/epoch - 12ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 11s - loss: 9.0833 - mse: 9.0833 - mae: 1.1687 - val_loss: 5.1449 - val_mse: 5.1449 - val_mae: 1.2804 - lr: 2.1725e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 11s - loss: 8.7313 - mse: 8.7313 - mae: 1.1383 - val_loss: 5.4010 - val_mse: 5.4010 - val_mae: 1.3307 - lr: 2.1725e-04 - 11s/epoch - 11ms/step\n",
            "Score for fold 4: loss of 5.401019096374512\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 12s - loss: 8.9428 - mse: 8.9428 - mae: 1.1803 - val_loss: 3.9883 - val_mse: 3.9883 - val_mae: 1.1170 - lr: 2.1725e-04 - 12s/epoch - 12ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 11s - loss: 8.6468 - mse: 8.6468 - mae: 1.1415 - val_loss: 3.8002 - val_mse: 3.8002 - val_mae: 1.1047 - lr: 2.1725e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 12s - loss: 8.2162 - mse: 8.2162 - mae: 1.1012 - val_loss: 4.0439 - val_mse: 4.0439 - val_mae: 1.1479 - lr: 2.1725e-04 - 12s/epoch - 12ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 12s - loss: 7.9612 - mse: 7.9612 - mae: 1.0655 - val_loss: 4.0271 - val_mse: 4.0271 - val_mae: 1.0971 - lr: 2.1725e-04 - 12s/epoch - 12ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 12s - loss: 7.5847 - mse: 7.5847 - mae: 1.0351 - val_loss: 3.9460 - val_mse: 3.9460 - val_mae: 1.1633 - lr: 2.1725e-04 - 12s/epoch - 12ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 12s - loss: 7.2635 - mse: 7.2635 - mae: 1.0004 - val_loss: 3.9850 - val_mse: 3.9850 - val_mae: 1.1520 - lr: 2.1725e-04 - 12s/epoch - 12ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 11s - loss: 7.0508 - mse: 7.0508 - mae: 0.9701 - val_loss: 4.2662 - val_mse: 4.2662 - val_mae: 1.1941 - lr: 2.1725e-04 - 11s/epoch - 11ms/step\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-10-12 06:19:33,392]\u001b[0m Finished trial#43 resulted in value: 11.059999999999999. Current best value is 8.266 with parameters: {'activation': 'relu', 'num_hidden_layer': 4, 'i': 10, 'learning_rate': 0.0002464065452818332}.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Score for fold 5: loss of 4.266174793243408\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 12s - loss: 15.3913 - mse: 15.3913 - mae: 1.6232 - val_loss: 9.9671 - val_mse: 9.9671 - val_mae: 1.6087 - lr: 1.3947e-04 - 12s/epoch - 12ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 11s - loss: 14.4520 - mse: 14.4520 - mae: 1.5375 - val_loss: 9.8889 - val_mse: 9.8889 - val_mae: 1.5614 - lr: 1.3947e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 12s - loss: 14.2810 - mse: 14.2810 - mae: 1.5171 - val_loss: 9.7023 - val_mse: 9.7023 - val_mae: 1.4733 - lr: 1.3947e-04 - 12s/epoch - 12ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 12s - loss: 14.0595 - mse: 14.0595 - mae: 1.5076 - val_loss: 9.6380 - val_mse: 9.6380 - val_mae: 1.5613 - lr: 1.3947e-04 - 12s/epoch - 12ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 11s - loss: 14.0533 - mse: 14.0533 - mae: 1.4980 - val_loss: 9.6366 - val_mse: 9.6366 - val_mae: 1.5313 - lr: 1.3947e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 11s - loss: 13.9297 - mse: 13.9297 - mae: 1.4921 - val_loss: 9.4820 - val_mse: 9.4820 - val_mae: 1.4997 - lr: 1.3947e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 11s - loss: 13.8695 - mse: 13.8695 - mae: 1.4888 - val_loss: 9.5277 - val_mse: 9.5277 - val_mae: 1.5122 - lr: 1.3947e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 11s - loss: 13.8137 - mse: 13.8137 - mae: 1.4818 - val_loss: 9.6187 - val_mse: 9.6187 - val_mae: 1.5760 - lr: 1.3947e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 11s - loss: 13.7146 - mse: 13.7146 - mae: 1.4777 - val_loss: 9.7618 - val_mse: 9.7618 - val_mae: 1.5169 - lr: 1.3947e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 12s - loss: 13.6788 - mse: 13.6788 - mae: 1.4749 - val_loss: 9.4333 - val_mse: 9.4333 - val_mae: 1.5201 - lr: 1.3947e-04 - 12s/epoch - 12ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 12s - loss: 13.6223 - mse: 13.6223 - mae: 1.4745 - val_loss: 9.5479 - val_mse: 9.5479 - val_mae: 1.4822 - lr: 1.3947e-04 - 12s/epoch - 12ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 12s - loss: 13.5472 - mse: 13.5472 - mae: 1.4647 - val_loss: 9.3249 - val_mse: 9.3249 - val_mae: 1.5102 - lr: 1.3947e-04 - 12s/epoch - 12ms/step\n",
            "Epoch 13/100\n",
            "1000/1000 - 11s - loss: 13.4416 - mse: 13.4416 - mae: 1.4591 - val_loss: 9.4273 - val_mse: 9.4273 - val_mae: 1.5181 - lr: 1.3947e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 14/100\n",
            "1000/1000 - 12s - loss: 13.4055 - mse: 13.4055 - mae: 1.4530 - val_loss: 9.3743 - val_mse: 9.3743 - val_mae: 1.4724 - lr: 1.3947e-04 - 12s/epoch - 12ms/step\n",
            "Epoch 15/100\n",
            "1000/1000 - 12s - loss: 13.3610 - mse: 13.3610 - mae: 1.4554 - val_loss: 9.3789 - val_mse: 9.3789 - val_mae: 1.4819 - lr: 1.3947e-04 - 12s/epoch - 12ms/step\n",
            "Epoch 16/100\n",
            "1000/1000 - 12s - loss: 13.2650 - mse: 13.2650 - mae: 1.4493 - val_loss: 9.3007 - val_mse: 9.3007 - val_mae: 1.4281 - lr: 1.3947e-04 - 12s/epoch - 12ms/step\n",
            "Epoch 17/100\n",
            "1000/1000 - 11s - loss: 13.1326 - mse: 13.1326 - mae: 1.4395 - val_loss: 9.4717 - val_mse: 9.4717 - val_mae: 1.5131 - lr: 1.3947e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 18/100\n",
            "1000/1000 - 12s - loss: 13.0925 - mse: 13.0925 - mae: 1.4354 - val_loss: 9.2074 - val_mse: 9.2074 - val_mae: 1.5410 - lr: 1.3947e-04 - 12s/epoch - 12ms/step\n",
            "Epoch 19/100\n",
            "1000/1000 - 12s - loss: 12.9714 - mse: 12.9714 - mae: 1.4273 - val_loss: 9.4668 - val_mse: 9.4668 - val_mae: 1.4614 - lr: 1.3947e-04 - 12s/epoch - 12ms/step\n",
            "Epoch 20/100\n",
            "1000/1000 - 12s - loss: 12.8736 - mse: 12.8736 - mae: 1.4219 - val_loss: 9.2755 - val_mse: 9.2755 - val_mae: 1.4997 - lr: 1.3947e-04 - 12s/epoch - 12ms/step\n",
            "Epoch 21/100\n",
            "1000/1000 - 12s - loss: 12.7348 - mse: 12.7348 - mae: 1.4141 - val_loss: 9.2157 - val_mse: 9.2157 - val_mae: 1.4531 - lr: 1.3947e-04 - 12s/epoch - 12ms/step\n",
            "Epoch 22/100\n",
            "1000/1000 - 12s - loss: 12.6441 - mse: 12.6441 - mae: 1.4070 - val_loss: 9.2521 - val_mse: 9.2521 - val_mae: 1.5186 - lr: 1.3947e-04 - 12s/epoch - 12ms/step\n",
            "Epoch 23/100\n",
            "1000/1000 - 12s - loss: 12.5754 - mse: 12.5754 - mae: 1.3977 - val_loss: 9.2108 - val_mse: 9.2108 - val_mae: 1.5258 - lr: 1.3947e-04 - 12s/epoch - 12ms/step\n",
            "Score for fold 1: loss of 9.210836410522461\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 12s - loss: 12.7438 - mse: 12.7438 - mae: 1.4203 - val_loss: 8.0305 - val_mse: 8.0305 - val_mae: 1.4433 - lr: 1.3947e-04 - 12s/epoch - 12ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 12s - loss: 12.5699 - mse: 12.5699 - mae: 1.4074 - val_loss: 8.3731 - val_mse: 8.3731 - val_mae: 1.4480 - lr: 1.3947e-04 - 12s/epoch - 12ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 12s - loss: 12.4030 - mse: 12.4030 - mae: 1.3984 - val_loss: 8.3576 - val_mse: 8.3576 - val_mae: 1.4233 - lr: 1.3947e-04 - 12s/epoch - 12ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 11s - loss: 12.2507 - mse: 12.2507 - mae: 1.3848 - val_loss: 8.5416 - val_mse: 8.5416 - val_mae: 1.4174 - lr: 1.3947e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 11s - loss: 12.0581 - mse: 12.0581 - mae: 1.3775 - val_loss: 8.1919 - val_mse: 8.1919 - val_mae: 1.4243 - lr: 1.3947e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 12s - loss: 11.8969 - mse: 11.8969 - mae: 1.3662 - val_loss: 8.4852 - val_mse: 8.4852 - val_mae: 1.4241 - lr: 1.3947e-04 - 12s/epoch - 12ms/step\n",
            "Score for fold 2: loss of 8.485167503356934\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 12s - loss: 11.5786 - mse: 11.5786 - mae: 1.3879 - val_loss: 9.0742 - val_mse: 9.0742 - val_mae: 1.3242 - lr: 1.3947e-04 - 12s/epoch - 12ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 12s - loss: 11.3431 - mse: 11.3431 - mae: 1.3654 - val_loss: 9.3806 - val_mse: 9.3806 - val_mae: 1.3939 - lr: 1.3947e-04 - 12s/epoch - 12ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 12s - loss: 11.1336 - mse: 11.1336 - mae: 1.3554 - val_loss: 9.3165 - val_mse: 9.3165 - val_mae: 1.3629 - lr: 1.3947e-04 - 12s/epoch - 12ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 11s - loss: 10.9715 - mse: 10.9715 - mae: 1.3364 - val_loss: 9.4036 - val_mse: 9.4036 - val_mae: 1.3752 - lr: 1.3947e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 11s - loss: 10.8735 - mse: 10.8735 - mae: 1.3260 - val_loss: 9.3643 - val_mse: 9.3643 - val_mae: 1.4074 - lr: 1.3947e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 11s - loss: 10.4991 - mse: 10.4991 - mae: 1.3110 - val_loss: 9.3581 - val_mse: 9.3581 - val_mae: 1.4294 - lr: 1.3947e-04 - 11s/epoch - 11ms/step\n",
            "Score for fold 3: loss of 9.358081817626953\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 12s - loss: 9.7761 - mse: 9.7761 - mae: 1.3314 - val_loss: 12.9240 - val_mse: 12.9240 - val_mae: 1.2938 - lr: 1.3947e-04 - 12s/epoch - 12ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 11s - loss: 9.5163 - mse: 9.5163 - mae: 1.3120 - val_loss: 12.2480 - val_mse: 12.2480 - val_mae: 1.3070 - lr: 1.3947e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 12s - loss: 9.2877 - mse: 9.2877 - mae: 1.2916 - val_loss: 12.9504 - val_mse: 12.9504 - val_mae: 1.3384 - lr: 1.3947e-04 - 12s/epoch - 12ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 12s - loss: 8.9979 - mse: 8.9979 - mae: 1.2733 - val_loss: 13.0275 - val_mse: 13.0275 - val_mae: 1.3292 - lr: 1.3947e-04 - 12s/epoch - 12ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 11s - loss: 8.8577 - mse: 8.8577 - mae: 1.2565 - val_loss: 13.2839 - val_mse: 13.2839 - val_mae: 1.3385 - lr: 1.3947e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 11s - loss: 8.5911 - mse: 8.5911 - mae: 1.2378 - val_loss: 13.3308 - val_mse: 13.3308 - val_mae: 1.3788 - lr: 1.3947e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 11s - loss: 8.3900 - mse: 8.3900 - mae: 1.2176 - val_loss: 13.3907 - val_mse: 13.3907 - val_mae: 1.4149 - lr: 1.3947e-04 - 11s/epoch - 11ms/step\n",
            "Score for fold 4: loss of 13.390676498413086\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 12s - loss: 7.7984 - mse: 7.7984 - mae: 1.2597 - val_loss: 14.7114 - val_mse: 14.7114 - val_mae: 1.1707 - lr: 1.3947e-04 - 12s/epoch - 12ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 11s - loss: 7.3876 - mse: 7.3876 - mae: 1.2274 - val_loss: 14.8127 - val_mse: 14.8127 - val_mae: 1.2431 - lr: 1.3947e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 11s - loss: 7.0212 - mse: 7.0212 - mae: 1.2008 - val_loss: 14.8131 - val_mse: 14.8131 - val_mae: 1.2089 - lr: 1.3947e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 11s - loss: 6.8654 - mse: 6.8654 - mae: 1.1802 - val_loss: 14.7802 - val_mse: 14.7802 - val_mae: 1.2673 - lr: 1.3947e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 11s - loss: 6.4691 - mse: 6.4691 - mae: 1.1538 - val_loss: 14.8286 - val_mse: 14.8286 - val_mae: 1.2679 - lr: 1.3947e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 11s - loss: 6.2150 - mse: 6.2150 - mae: 1.1291 - val_loss: 14.9167 - val_mse: 14.9167 - val_mae: 1.2950 - lr: 1.3947e-04 - 11s/epoch - 11ms/step\n",
            "Score for fold 5: loss of 14.916735649108887\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-10-12 06:28:56,870]\u001b[0m Finished trial#44 resulted in value: 11.074000000000002. Current best value is 8.266 with parameters: {'activation': 'relu', 'num_hidden_layer': 4, 'i': 10, 'learning_rate': 0.0002464065452818332}.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 12s - loss: 12.3149 - mse: 12.3149 - mae: 1.6188 - val_loss: 22.5135 - val_mse: 22.5135 - val_mae: 1.6124 - lr: 2.9793e-04 - 12s/epoch - 12ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 11s - loss: 11.7871 - mse: 11.7871 - mae: 1.5434 - val_loss: 22.4610 - val_mse: 22.4610 - val_mae: 1.5347 - lr: 2.9793e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 11s - loss: 11.6203 - mse: 11.6203 - mae: 1.5256 - val_loss: 22.4285 - val_mse: 22.4285 - val_mae: 1.5632 - lr: 2.9793e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 11s - loss: 11.4381 - mse: 11.4381 - mae: 1.5079 - val_loss: 22.6121 - val_mse: 22.6121 - val_mae: 1.5511 - lr: 2.9793e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 11s - loss: 11.4374 - mse: 11.4374 - mae: 1.5006 - val_loss: 22.2302 - val_mse: 22.2302 - val_mae: 1.5372 - lr: 2.9793e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 12s - loss: 11.2525 - mse: 11.2525 - mae: 1.4915 - val_loss: 22.1007 - val_mse: 22.1007 - val_mae: 1.5649 - lr: 2.9793e-04 - 12s/epoch - 12ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 12s - loss: 11.1374 - mse: 11.1374 - mae: 1.4821 - val_loss: 21.9692 - val_mse: 21.9692 - val_mae: 1.5134 - lr: 2.9793e-04 - 12s/epoch - 12ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 12s - loss: 11.0790 - mse: 11.0790 - mae: 1.4790 - val_loss: 22.1250 - val_mse: 22.1250 - val_mae: 1.5334 - lr: 2.9793e-04 - 12s/epoch - 12ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 12s - loss: 10.9904 - mse: 10.9904 - mae: 1.4728 - val_loss: 21.8736 - val_mse: 21.8736 - val_mae: 1.5427 - lr: 2.9793e-04 - 12s/epoch - 12ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 12s - loss: 10.8827 - mse: 10.8827 - mae: 1.4683 - val_loss: 21.5439 - val_mse: 21.5439 - val_mae: 1.5529 - lr: 2.9793e-04 - 12s/epoch - 12ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 12s - loss: 10.8491 - mse: 10.8491 - mae: 1.4634 - val_loss: 21.6499 - val_mse: 21.6499 - val_mae: 1.6343 - lr: 2.9793e-04 - 12s/epoch - 12ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 12s - loss: 10.7919 - mse: 10.7919 - mae: 1.4633 - val_loss: 21.5682 - val_mse: 21.5682 - val_mae: 1.5304 - lr: 2.9793e-04 - 12s/epoch - 12ms/step\n",
            "Epoch 13/100\n",
            "1000/1000 - 12s - loss: 10.7426 - mse: 10.7426 - mae: 1.4591 - val_loss: 21.3038 - val_mse: 21.3038 - val_mae: 1.5435 - lr: 2.9793e-04 - 12s/epoch - 12ms/step\n",
            "Epoch 14/100\n",
            "1000/1000 - 12s - loss: 10.6804 - mse: 10.6804 - mae: 1.4546 - val_loss: 21.3103 - val_mse: 21.3103 - val_mae: 1.4826 - lr: 2.9793e-04 - 12s/epoch - 12ms/step\n",
            "Epoch 15/100\n",
            "1000/1000 - 12s - loss: 10.6290 - mse: 10.6290 - mae: 1.4531 - val_loss: 21.3808 - val_mse: 21.3808 - val_mae: 1.5601 - lr: 2.9793e-04 - 12s/epoch - 12ms/step\n",
            "Epoch 16/100\n",
            "1000/1000 - 12s - loss: 10.5663 - mse: 10.5663 - mae: 1.4527 - val_loss: 21.7944 - val_mse: 21.7944 - val_mae: 1.5763 - lr: 2.9793e-04 - 12s/epoch - 12ms/step\n",
            "Epoch 17/100\n",
            "1000/1000 - 11s - loss: 10.4919 - mse: 10.4919 - mae: 1.4422 - val_loss: 21.7179 - val_mse: 21.7179 - val_mae: 1.4893 - lr: 2.9793e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 18/100\n",
            "1000/1000 - 11s - loss: 10.4138 - mse: 10.4138 - mae: 1.4421 - val_loss: 21.1315 - val_mse: 21.1315 - val_mae: 1.6170 - lr: 2.9793e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 19/100\n",
            "1000/1000 - 11s - loss: 10.2938 - mse: 10.2938 - mae: 1.4431 - val_loss: 21.5137 - val_mse: 21.5137 - val_mae: 1.4879 - lr: 2.9793e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 20/100\n",
            "1000/1000 - 11s - loss: 10.3029 - mse: 10.3029 - mae: 1.4319 - val_loss: 21.1536 - val_mse: 21.1536 - val_mae: 1.5546 - lr: 2.9793e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 21/100\n",
            "1000/1000 - 11s - loss: 10.1379 - mse: 10.1379 - mae: 1.4319 - val_loss: 21.2442 - val_mse: 21.2442 - val_mae: 1.5681 - lr: 2.9793e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 22/100\n",
            "1000/1000 - 11s - loss: 10.0243 - mse: 10.0243 - mae: 1.4201 - val_loss: 21.2535 - val_mse: 21.2535 - val_mae: 1.5616 - lr: 2.9793e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 23/100\n",
            "1000/1000 - 11s - loss: 10.0679 - mse: 10.0679 - mae: 1.4204 - val_loss: 21.3253 - val_mse: 21.3253 - val_mae: 1.5307 - lr: 2.9793e-04 - 11s/epoch - 11ms/step\n",
            "Score for fold 1: loss of 21.32529067993164\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 11s - loss: 12.3528 - mse: 12.3528 - mae: 1.4503 - val_loss: 11.3760 - val_mse: 11.3760 - val_mae: 1.4642 - lr: 2.9793e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 11s - loss: 12.0796 - mse: 12.0796 - mae: 1.4393 - val_loss: 11.5920 - val_mse: 11.5920 - val_mae: 1.3970 - lr: 2.9793e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 11s - loss: 11.9568 - mse: 11.9568 - mae: 1.4372 - val_loss: 11.8328 - val_mse: 11.8328 - val_mae: 1.4383 - lr: 2.9793e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 11s - loss: 11.7267 - mse: 11.7267 - mae: 1.4249 - val_loss: 11.5938 - val_mse: 11.5938 - val_mae: 1.5830 - lr: 2.9793e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 11s - loss: 11.6031 - mse: 11.6031 - mae: 1.4188 - val_loss: 11.6654 - val_mse: 11.6654 - val_mae: 1.4105 - lr: 2.9793e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 11s - loss: 11.4110 - mse: 11.4110 - mae: 1.4034 - val_loss: 12.1615 - val_mse: 12.1615 - val_mae: 1.4211 - lr: 2.9793e-04 - 11s/epoch - 11ms/step\n",
            "Score for fold 2: loss of 12.16154670715332\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 11s - loss: 12.1304 - mse: 12.1304 - mae: 1.4220 - val_loss: 9.1668 - val_mse: 9.1668 - val_mae: 1.3330 - lr: 2.9793e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 11s - loss: 11.7271 - mse: 11.7271 - mae: 1.4082 - val_loss: 8.9131 - val_mse: 8.9131 - val_mae: 1.3692 - lr: 2.9793e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 11s - loss: 11.5791 - mse: 11.5791 - mae: 1.3885 - val_loss: 8.9292 - val_mse: 8.9292 - val_mae: 1.4107 - lr: 2.9793e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 11s - loss: 11.1996 - mse: 11.1996 - mae: 1.3685 - val_loss: 9.0367 - val_mse: 9.0367 - val_mae: 1.4092 - lr: 2.9793e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 11s - loss: 11.0472 - mse: 11.0472 - mae: 1.3538 - val_loss: 9.2400 - val_mse: 9.2400 - val_mae: 1.4200 - lr: 2.9793e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 11s - loss: 10.6714 - mse: 10.6714 - mae: 1.3389 - val_loss: 9.1626 - val_mse: 9.1626 - val_mae: 1.5138 - lr: 2.9793e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 12s - loss: 10.3706 - mse: 10.3706 - mae: 1.3173 - val_loss: 9.2466 - val_mse: 9.2466 - val_mae: 1.4328 - lr: 2.9793e-04 - 12s/epoch - 12ms/step\n",
            "Score for fold 3: loss of 9.24659538269043\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 11s - loss: 10.7135 - mse: 10.7135 - mae: 1.3461 - val_loss: 7.1565 - val_mse: 7.1565 - val_mae: 1.2498 - lr: 2.9793e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 11s - loss: 10.3912 - mse: 10.3912 - mae: 1.3185 - val_loss: 7.2419 - val_mse: 7.2419 - val_mae: 1.3708 - lr: 2.9793e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 11s - loss: 10.0196 - mse: 10.0196 - mae: 1.2906 - val_loss: 7.2957 - val_mse: 7.2957 - val_mae: 1.3339 - lr: 2.9793e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 11s - loss: 9.9062 - mse: 9.9062 - mae: 1.2638 - val_loss: 7.4890 - val_mse: 7.4890 - val_mae: 1.2984 - lr: 2.9793e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 11s - loss: 9.3820 - mse: 9.3820 - mae: 1.2378 - val_loss: 7.5352 - val_mse: 7.5352 - val_mae: 1.3788 - lr: 2.9793e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 11s - loss: 8.9980 - mse: 8.9980 - mae: 1.2110 - val_loss: 7.7773 - val_mse: 7.7773 - val_mae: 1.3584 - lr: 2.9793e-04 - 11s/epoch - 11ms/step\n",
            "Score for fold 4: loss of 7.777263164520264\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 12s - loss: 9.4420 - mse: 9.4420 - mae: 1.2519 - val_loss: 5.4978 - val_mse: 5.4978 - val_mae: 1.1996 - lr: 2.9793e-04 - 12s/epoch - 12ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 12s - loss: 9.0216 - mse: 9.0216 - mae: 1.2080 - val_loss: 5.6397 - val_mse: 5.6397 - val_mae: 1.1606 - lr: 2.9793e-04 - 12s/epoch - 12ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 12s - loss: 8.6757 - mse: 8.6757 - mae: 1.1744 - val_loss: 5.5785 - val_mse: 5.5785 - val_mae: 1.2070 - lr: 2.9793e-04 - 12s/epoch - 12ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 12s - loss: 8.3590 - mse: 8.3590 - mae: 1.1456 - val_loss: 5.8132 - val_mse: 5.8132 - val_mae: 1.2715 - lr: 2.9793e-04 - 12s/epoch - 12ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 12s - loss: 8.0264 - mse: 8.0264 - mae: 1.1134 - val_loss: 5.6801 - val_mse: 5.6801 - val_mae: 1.2582 - lr: 2.9793e-04 - 12s/epoch - 12ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 11s - loss: 7.7060 - mse: 7.7060 - mae: 1.0819 - val_loss: 5.7868 - val_mse: 5.7868 - val_mae: 1.2402 - lr: 2.9793e-04 - 11s/epoch - 11ms/step\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-10-12 06:38:16,873]\u001b[0m Finished trial#45 resulted in value: 11.261999999999999. Current best value is 8.266 with parameters: {'activation': 'relu', 'num_hidden_layer': 4, 'i': 10, 'learning_rate': 0.0002464065452818332}.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Score for fold 5: loss of 5.786838054656982\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 12s - loss: 13.1509 - mse: 13.1509 - mae: 1.5350 - val_loss: 12.0627 - val_mse: 12.0627 - val_mae: 1.5284 - lr: 4.9106e-04 - 12s/epoch - 12ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 11s - loss: 12.4539 - mse: 12.4539 - mae: 1.4883 - val_loss: 12.7682 - val_mse: 12.7682 - val_mae: 1.5012 - lr: 4.9106e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 11s - loss: 12.2432 - mse: 12.2432 - mae: 1.4716 - val_loss: 11.8075 - val_mse: 11.8075 - val_mae: 1.5668 - lr: 4.9106e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 11s - loss: 12.2357 - mse: 12.2357 - mae: 1.4598 - val_loss: 11.7945 - val_mse: 11.7945 - val_mae: 1.4575 - lr: 4.9106e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 11s - loss: 12.1858 - mse: 12.1858 - mae: 1.4521 - val_loss: 11.9146 - val_mse: 11.9146 - val_mae: 1.5727 - lr: 4.9106e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 11s - loss: 12.0570 - mse: 12.0570 - mae: 1.4422 - val_loss: 12.0047 - val_mse: 12.0047 - val_mae: 1.5962 - lr: 4.9106e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 11s - loss: 11.8275 - mse: 11.8275 - mae: 1.4404 - val_loss: 12.0622 - val_mse: 12.0622 - val_mae: 1.4941 - lr: 4.9106e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 11s - loss: 11.5157 - mse: 11.5157 - mae: 1.4309 - val_loss: 11.6347 - val_mse: 11.6347 - val_mae: 1.6167 - lr: 4.9106e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 11s - loss: 11.4716 - mse: 11.4716 - mae: 1.4237 - val_loss: 12.2275 - val_mse: 12.2275 - val_mae: 1.5293 - lr: 4.9106e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 12s - loss: 11.3335 - mse: 11.3335 - mae: 1.4154 - val_loss: 12.0488 - val_mse: 12.0488 - val_mae: 1.5386 - lr: 4.9106e-04 - 12s/epoch - 12ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 11s - loss: 11.1061 - mse: 11.1061 - mae: 1.4058 - val_loss: 12.0434 - val_mse: 12.0434 - val_mae: 1.4462 - lr: 4.9106e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 11s - loss: 10.9648 - mse: 10.9648 - mae: 1.3920 - val_loss: 12.4931 - val_mse: 12.4931 - val_mae: 1.5562 - lr: 4.9106e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 13/100\n",
            "1000/1000 - 11s - loss: 10.9055 - mse: 10.9055 - mae: 1.3880 - val_loss: 11.8682 - val_mse: 11.8682 - val_mae: 1.5014 - lr: 4.9106e-04 - 11s/epoch - 11ms/step\n",
            "Score for fold 1: loss of 11.868245124816895\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 12s - loss: 11.9967 - mse: 11.9967 - mae: 1.4288 - val_loss: 7.7594 - val_mse: 7.7594 - val_mae: 1.4064 - lr: 4.9106e-04 - 12s/epoch - 12ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 11s - loss: 11.5635 - mse: 11.5635 - mae: 1.4105 - val_loss: 7.9307 - val_mse: 7.9307 - val_mae: 1.3258 - lr: 4.9106e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 12s - loss: 11.7842 - mse: 11.7842 - mae: 1.3996 - val_loss: 8.1475 - val_mse: 8.1475 - val_mae: 1.3736 - lr: 4.9106e-04 - 12s/epoch - 12ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 12s - loss: 11.1657 - mse: 11.1657 - mae: 1.3894 - val_loss: 8.2921 - val_mse: 8.2921 - val_mae: 1.3507 - lr: 4.9106e-04 - 12s/epoch - 12ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 11s - loss: 11.0185 - mse: 11.0185 - mae: 1.3851 - val_loss: 8.5093 - val_mse: 8.5093 - val_mae: 1.3436 - lr: 4.9106e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 11s - loss: 10.9230 - mse: 10.9230 - mae: 1.3716 - val_loss: 8.6477 - val_mse: 8.6477 - val_mae: 1.3703 - lr: 4.9106e-04 - 11s/epoch - 11ms/step\n",
            "Score for fold 2: loss of 8.647665023803711\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 11s - loss: 11.2814 - mse: 11.2814 - mae: 1.3845 - val_loss: 6.0933 - val_mse: 6.0933 - val_mae: 1.3246 - lr: 4.9106e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 11s - loss: 10.8447 - mse: 10.8447 - mae: 1.3730 - val_loss: 6.3183 - val_mse: 6.3183 - val_mae: 1.3760 - lr: 4.9106e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 11s - loss: 10.7687 - mse: 10.7687 - mae: 1.3576 - val_loss: 6.6273 - val_mse: 6.6273 - val_mae: 1.3409 - lr: 4.9106e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 11s - loss: 10.4699 - mse: 10.4699 - mae: 1.3437 - val_loss: 6.3906 - val_mse: 6.3906 - val_mae: 1.3809 - lr: 4.9106e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 11s - loss: 10.4746 - mse: 10.4746 - mae: 1.3381 - val_loss: 6.8020 - val_mse: 6.8020 - val_mae: 1.3162 - lr: 4.9106e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 11s - loss: 10.3045 - mse: 10.3045 - mae: 1.3246 - val_loss: 6.8746 - val_mse: 6.8746 - val_mae: 1.3614 - lr: 4.9106e-04 - 11s/epoch - 11ms/step\n",
            "Score for fold 3: loss of 6.874612808227539\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 11s - loss: 9.8880 - mse: 9.8880 - mae: 1.3472 - val_loss: 8.3588 - val_mse: 8.3588 - val_mae: 1.1887 - lr: 4.9106e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 11s - loss: 9.5989 - mse: 9.5989 - mae: 1.3242 - val_loss: 8.2486 - val_mse: 8.2486 - val_mae: 1.2860 - lr: 4.9106e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 11s - loss: 9.1065 - mse: 9.1065 - mae: 1.3108 - val_loss: 8.5892 - val_mse: 8.5892 - val_mae: 1.2369 - lr: 4.9106e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 11s - loss: 9.0188 - mse: 9.0188 - mae: 1.3040 - val_loss: 8.6549 - val_mse: 8.6549 - val_mae: 1.2370 - lr: 4.9106e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 11s - loss: 8.7938 - mse: 8.7938 - mae: 1.2840 - val_loss: 8.8510 - val_mse: 8.8510 - val_mae: 1.3137 - lr: 4.9106e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 11s - loss: 8.5902 - mse: 8.5902 - mae: 1.2756 - val_loss: 9.0092 - val_mse: 9.0092 - val_mae: 1.3238 - lr: 4.9106e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 11s - loss: 8.5527 - mse: 8.5527 - mae: 1.2701 - val_loss: 8.7540 - val_mse: 8.7540 - val_mae: 1.2893 - lr: 4.9106e-04 - 11s/epoch - 11ms/step\n",
            "Score for fold 4: loss of 8.754009246826172\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 11s - loss: 6.6609 - mse: 6.6609 - mae: 1.2786 - val_loss: 15.0917 - val_mse: 15.0917 - val_mae: 1.2565 - lr: 4.9106e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 11s - loss: 6.6444 - mse: 6.6444 - mae: 1.2599 - val_loss: 15.5913 - val_mse: 15.5913 - val_mae: 1.3464 - lr: 4.9106e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 11s - loss: 6.2444 - mse: 6.2444 - mae: 1.2455 - val_loss: 15.9653 - val_mse: 15.9653 - val_mae: 1.3047 - lr: 4.9106e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 11s - loss: 6.1055 - mse: 6.1055 - mae: 1.2281 - val_loss: 15.8085 - val_mse: 15.8085 - val_mae: 1.3413 - lr: 4.9106e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 11s - loss: 5.8387 - mse: 5.8387 - mae: 1.2079 - val_loss: 16.1135 - val_mse: 16.1135 - val_mae: 1.2679 - lr: 4.9106e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 11s - loss: 5.8487 - mse: 5.8487 - mae: 1.2015 - val_loss: 15.6315 - val_mse: 15.6315 - val_mae: 1.3433 - lr: 4.9106e-04 - 11s/epoch - 11ms/step\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-10-12 06:45:38,461]\u001b[0m Finished trial#46 resulted in value: 10.354000000000001. Current best value is 8.266 with parameters: {'activation': 'relu', 'num_hidden_layer': 4, 'i': 10, 'learning_rate': 0.0002464065452818332}.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Score for fold 5: loss of 15.631474494934082\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 12s - loss: 15.3111 - mse: 15.3111 - mae: 1.6615 - val_loss: 12.5972 - val_mse: 12.5972 - val_mae: 1.5618 - lr: 6.3044e-04 - 12s/epoch - 12ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 12s - loss: 14.7899 - mse: 14.7899 - mae: 1.5790 - val_loss: 12.3795 - val_mse: 12.3795 - val_mae: 1.5100 - lr: 6.3044e-04 - 12s/epoch - 12ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 11s - loss: 14.3979 - mse: 14.3979 - mae: 1.5497 - val_loss: 11.8879 - val_mse: 11.8879 - val_mae: 1.4559 - lr: 6.3044e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 11s - loss: 14.0441 - mse: 14.0441 - mae: 1.5308 - val_loss: 12.1338 - val_mse: 12.1338 - val_mae: 1.5905 - lr: 6.3044e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 11s - loss: 13.9454 - mse: 13.9454 - mae: 1.5240 - val_loss: 11.7064 - val_mse: 11.7064 - val_mae: 1.4559 - lr: 6.3044e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 11s - loss: 13.6472 - mse: 13.6472 - mae: 1.5072 - val_loss: 11.5148 - val_mse: 11.5148 - val_mae: 1.5040 - lr: 6.3044e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 11s - loss: 13.3785 - mse: 13.3785 - mae: 1.5053 - val_loss: 11.4129 - val_mse: 11.4129 - val_mae: 1.5154 - lr: 6.3044e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 11s - loss: 13.4760 - mse: 13.4760 - mae: 1.5144 - val_loss: 11.1584 - val_mse: 11.1584 - val_mae: 1.4421 - lr: 6.3044e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 11s - loss: 13.4330 - mse: 13.4330 - mae: 1.5181 - val_loss: 11.2325 - val_mse: 11.2325 - val_mae: 1.5312 - lr: 6.3044e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 11s - loss: 13.1551 - mse: 13.1551 - mae: 1.5015 - val_loss: 11.5560 - val_mse: 11.5560 - val_mae: 1.4360 - lr: 6.3044e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 11s - loss: 13.1935 - mse: 13.1935 - mae: 1.4974 - val_loss: 11.3363 - val_mse: 11.3363 - val_mae: 1.4702 - lr: 6.3044e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 11s - loss: 13.0414 - mse: 13.0414 - mae: 1.5077 - val_loss: 11.1345 - val_mse: 11.1345 - val_mae: 1.6120 - lr: 6.3044e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 13/100\n",
            "1000/1000 - 11s - loss: 12.9960 - mse: 12.9960 - mae: 1.5058 - val_loss: 11.6304 - val_mse: 11.6304 - val_mae: 1.8123 - lr: 6.3044e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 14/100\n",
            "1000/1000 - 11s - loss: 13.0717 - mse: 13.0717 - mae: 1.5043 - val_loss: 11.3577 - val_mse: 11.3577 - val_mae: 1.6007 - lr: 6.3044e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 15/100\n",
            "1000/1000 - 11s - loss: 12.9524 - mse: 12.9524 - mae: 1.4935 - val_loss: 11.4563 - val_mse: 11.4563 - val_mae: 1.5088 - lr: 6.3044e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 16/100\n",
            "1000/1000 - 11s - loss: 12.9013 - mse: 12.9013 - mae: 1.4861 - val_loss: 11.3473 - val_mse: 11.3473 - val_mae: 1.7725 - lr: 6.3044e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 17/100\n",
            "1000/1000 - 11s - loss: 12.8223 - mse: 12.8223 - mae: 1.4912 - val_loss: 10.9412 - val_mse: 10.9412 - val_mae: 1.5568 - lr: 6.3044e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 18/100\n",
            "1000/1000 - 11s - loss: 12.8244 - mse: 12.8244 - mae: 1.4952 - val_loss: 11.0084 - val_mse: 11.0084 - val_mae: 1.5143 - lr: 6.3044e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 19/100\n",
            "1000/1000 - 11s - loss: 12.6811 - mse: 12.6811 - mae: 1.4850 - val_loss: 11.4208 - val_mse: 11.4208 - val_mae: 1.3996 - lr: 6.3044e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 20/100\n",
            "1000/1000 - 11s - loss: 12.6408 - mse: 12.6408 - mae: 1.4858 - val_loss: 10.8064 - val_mse: 10.8064 - val_mae: 1.3951 - lr: 6.3044e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 21/100\n",
            "1000/1000 - 11s - loss: 12.6181 - mse: 12.6181 - mae: 1.4849 - val_loss: 10.9674 - val_mse: 10.9674 - val_mae: 1.4171 - lr: 6.3044e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 22/100\n",
            "1000/1000 - 11s - loss: 12.5282 - mse: 12.5282 - mae: 1.4860 - val_loss: 11.2625 - val_mse: 11.2625 - val_mae: 1.5941 - lr: 6.3044e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 23/100\n",
            "1000/1000 - 11s - loss: 12.4716 - mse: 12.4716 - mae: 1.4759 - val_loss: 11.0455 - val_mse: 11.0455 - val_mae: 1.3847 - lr: 6.3044e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 24/100\n",
            "1000/1000 - 11s - loss: 12.3893 - mse: 12.3893 - mae: 1.4800 - val_loss: 11.4045 - val_mse: 11.4045 - val_mae: 1.4653 - lr: 6.3044e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 25/100\n",
            "1000/1000 - 11s - loss: 12.4024 - mse: 12.4024 - mae: 1.4789 - val_loss: 11.0450 - val_mse: 11.0450 - val_mae: 1.6737 - lr: 6.3044e-04 - 11s/epoch - 11ms/step\n",
            "Score for fold 1: loss of 11.045014381408691\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 11s - loss: 11.6422 - mse: 11.6422 - mae: 1.4807 - val_loss: 14.5255 - val_mse: 14.5255 - val_mae: 1.5427 - lr: 6.3044e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 11s - loss: 11.4470 - mse: 11.4470 - mae: 1.4734 - val_loss: 15.0978 - val_mse: 15.0978 - val_mae: 1.5518 - lr: 6.3044e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 11s - loss: 11.4119 - mse: 11.4119 - mae: 1.4767 - val_loss: 14.4324 - val_mse: 14.4324 - val_mae: 1.4471 - lr: 6.3044e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 11s - loss: 11.2809 - mse: 11.2809 - mae: 1.4674 - val_loss: 14.5831 - val_mse: 14.5831 - val_mae: 1.5376 - lr: 6.3044e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 12s - loss: 11.3683 - mse: 11.3683 - mae: 1.4663 - val_loss: 14.4608 - val_mse: 14.4608 - val_mae: 1.4892 - lr: 6.3044e-04 - 12s/epoch - 12ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 11s - loss: 11.1369 - mse: 11.1369 - mae: 1.4660 - val_loss: 14.6203 - val_mse: 14.6203 - val_mae: 1.4754 - lr: 6.3044e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 11s - loss: 11.1202 - mse: 11.1202 - mae: 1.4678 - val_loss: 14.3712 - val_mse: 14.3712 - val_mae: 1.6150 - lr: 6.3044e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 11s - loss: 11.0863 - mse: 11.0863 - mae: 1.4585 - val_loss: 14.4768 - val_mse: 14.4768 - val_mae: 1.7133 - lr: 6.3044e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 11s - loss: 10.9366 - mse: 10.9366 - mae: 1.4520 - val_loss: 14.7767 - val_mse: 14.7767 - val_mae: 1.5492 - lr: 6.3044e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 11s - loss: 10.9448 - mse: 10.9448 - mae: 1.4652 - val_loss: 14.8267 - val_mse: 14.8267 - val_mae: 1.5971 - lr: 6.3044e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 11s - loss: 10.8620 - mse: 10.8620 - mae: 1.4512 - val_loss: 14.4540 - val_mse: 14.4540 - val_mae: 1.5688 - lr: 6.3044e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 11s - loss: 10.8865 - mse: 10.8865 - mae: 1.4458 - val_loss: 14.9136 - val_mse: 14.9136 - val_mae: 1.4507 - lr: 6.3044e-04 - 11s/epoch - 11ms/step\n",
            "Score for fold 2: loss of 14.913619995117188\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 11s - loss: 13.0872 - mse: 13.0872 - mae: 1.4820 - val_loss: 6.6168 - val_mse: 6.6168 - val_mae: 1.4183 - lr: 6.3044e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 11s - loss: 12.7066 - mse: 12.7066 - mae: 1.4674 - val_loss: 6.7832 - val_mse: 6.7832 - val_mae: 1.4446 - lr: 6.3044e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 11s - loss: 12.7004 - mse: 12.7004 - mae: 1.4714 - val_loss: 7.6059 - val_mse: 7.6059 - val_mae: 1.9431 - lr: 6.3044e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 11s - loss: 12.6568 - mse: 12.6568 - mae: 1.4751 - val_loss: 6.5773 - val_mse: 6.5773 - val_mae: 1.3290 - lr: 6.3044e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 11s - loss: 12.3470 - mse: 12.3470 - mae: 1.4537 - val_loss: 6.5585 - val_mse: 6.5585 - val_mae: 1.4496 - lr: 6.3044e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 11s - loss: 12.2870 - mse: 12.2870 - mae: 1.4670 - val_loss: 6.8904 - val_mse: 6.8904 - val_mae: 1.3976 - lr: 6.3044e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 11s - loss: 12.4458 - mse: 12.4458 - mae: 1.4584 - val_loss: 7.2498 - val_mse: 7.2498 - val_mae: 1.3263 - lr: 6.3044e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 11s - loss: 12.3890 - mse: 12.3890 - mae: 1.4562 - val_loss: 6.6270 - val_mse: 6.6270 - val_mae: 1.3971 - lr: 6.3044e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 11s - loss: 12.2806 - mse: 12.2806 - mae: 1.4629 - val_loss: 6.9666 - val_mse: 6.9666 - val_mae: 1.3763 - lr: 6.3044e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 11s - loss: 12.0268 - mse: 12.0268 - mae: 1.4495 - val_loss: 7.0400 - val_mse: 7.0400 - val_mae: 1.4885 - lr: 6.3044e-04 - 11s/epoch - 11ms/step\n",
            "Score for fold 3: loss of 7.039966106414795\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 11s - loss: 10.2660 - mse: 10.2660 - mae: 1.4497 - val_loss: 15.2125 - val_mse: 15.2125 - val_mae: 1.6507 - lr: 6.3044e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 11s - loss: 10.1527 - mse: 10.1527 - mae: 1.4504 - val_loss: 14.8819 - val_mse: 14.8819 - val_mae: 1.4448 - lr: 6.3044e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 11s - loss: 9.8931 - mse: 9.8931 - mae: 1.4410 - val_loss: 15.0255 - val_mse: 15.0255 - val_mae: 1.5073 - lr: 6.3044e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 11s - loss: 9.8848 - mse: 9.8848 - mae: 1.4340 - val_loss: 15.1936 - val_mse: 15.1936 - val_mae: 1.3355 - lr: 6.3044e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 11s - loss: 9.7819 - mse: 9.7819 - mae: 1.4355 - val_loss: 15.1925 - val_mse: 15.1925 - val_mae: 1.3838 - lr: 6.3044e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 11s - loss: 9.6583 - mse: 9.6583 - mae: 1.4316 - val_loss: 15.4312 - val_mse: 15.4312 - val_mae: 1.5788 - lr: 6.3044e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 11s - loss: 9.5660 - mse: 9.5660 - mae: 1.4330 - val_loss: 15.2541 - val_mse: 15.2541 - val_mae: 1.4074 - lr: 6.3044e-04 - 11s/epoch - 11ms/step\n",
            "Score for fold 4: loss of 15.254075050354004\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 11s - loss: 10.7941 - mse: 10.7941 - mae: 1.4345 - val_loss: 10.4099 - val_mse: 10.4099 - val_mae: 1.3552 - lr: 6.3044e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 12s - loss: 10.6881 - mse: 10.6881 - mae: 1.4303 - val_loss: 10.3546 - val_mse: 10.3546 - val_mae: 1.4284 - lr: 6.3044e-04 - 12s/epoch - 12ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 12s - loss: 10.6376 - mse: 10.6376 - mae: 1.4357 - val_loss: 11.1235 - val_mse: 11.1235 - val_mae: 1.3630 - lr: 6.3044e-04 - 12s/epoch - 12ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 12s - loss: 10.5122 - mse: 10.5122 - mae: 1.4213 - val_loss: 10.8064 - val_mse: 10.8064 - val_mae: 1.5697 - lr: 6.3044e-04 - 12s/epoch - 12ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 12s - loss: 10.3897 - mse: 10.3897 - mae: 1.4136 - val_loss: 10.9472 - val_mse: 10.9472 - val_mae: 1.3379 - lr: 6.3044e-04 - 12s/epoch - 12ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 12s - loss: 10.4361 - mse: 10.4361 - mae: 1.4121 - val_loss: 10.9231 - val_mse: 10.9231 - val_mae: 1.4414 - lr: 6.3044e-04 - 12s/epoch - 12ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 11s - loss: 10.1879 - mse: 10.1879 - mae: 1.4052 - val_loss: 10.6706 - val_mse: 10.6706 - val_mae: 1.3491 - lr: 6.3044e-04 - 11s/epoch - 11ms/step\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-10-12 06:57:20,149]\u001b[0m Finished trial#47 resulted in value: 11.784. Current best value is 8.266 with parameters: {'activation': 'relu', 'num_hidden_layer': 4, 'i': 10, 'learning_rate': 0.0002464065452818332}.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Score for fold 5: loss of 10.670561790466309\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 11s - loss: 15.4290 - mse: 15.4290 - mae: 1.6550 - val_loss: 10.8208 - val_mse: 10.8208 - val_mae: 1.5993 - lr: 3.7868e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 11s - loss: 14.4516 - mse: 14.4516 - mae: 1.5684 - val_loss: 12.0496 - val_mse: 12.0496 - val_mae: 1.4946 - lr: 3.7868e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 10s - loss: 14.2287 - mse: 14.2287 - mae: 1.5605 - val_loss: 10.6954 - val_mse: 10.6954 - val_mae: 1.5377 - lr: 3.7868e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 10s - loss: 14.1319 - mse: 14.1319 - mae: 1.5598 - val_loss: 10.5473 - val_mse: 10.5473 - val_mae: 1.5444 - lr: 3.7868e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 11s - loss: 14.1493 - mse: 14.1493 - mae: 1.5557 - val_loss: 10.5791 - val_mse: 10.5791 - val_mae: 1.5221 - lr: 3.7868e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 10s - loss: 14.0637 - mse: 14.0637 - mae: 1.5523 - val_loss: 10.4550 - val_mse: 10.4550 - val_mae: 1.5886 - lr: 3.7868e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 10s - loss: 14.0399 - mse: 14.0399 - mae: 1.5468 - val_loss: 10.5982 - val_mse: 10.5982 - val_mae: 1.5343 - lr: 3.7868e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 10s - loss: 14.1820 - mse: 14.1820 - mae: 1.5548 - val_loss: 10.1858 - val_mse: 10.1858 - val_mae: 1.5790 - lr: 3.7868e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 11s - loss: 14.0345 - mse: 14.0345 - mae: 1.5534 - val_loss: 10.3056 - val_mse: 10.3056 - val_mae: 1.5738 - lr: 3.7868e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 11s - loss: 14.0695 - mse: 14.0695 - mae: 1.5519 - val_loss: 10.2773 - val_mse: 10.2773 - val_mae: 1.6104 - lr: 3.7868e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 10s - loss: 14.0026 - mse: 14.0026 - mae: 1.5495 - val_loss: 10.3053 - val_mse: 10.3053 - val_mae: 1.5725 - lr: 3.7868e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 11s - loss: 14.0175 - mse: 14.0175 - mae: 1.5467 - val_loss: 10.2631 - val_mse: 10.2631 - val_mae: 1.6087 - lr: 3.7868e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 13/100\n",
            "1000/1000 - 11s - loss: 14.1130 - mse: 14.1130 - mae: 1.5520 - val_loss: 10.5229 - val_mse: 10.5229 - val_mae: 1.5350 - lr: 3.7868e-04 - 11s/epoch - 11ms/step\n",
            "Score for fold 1: loss of 10.52293586730957\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 10s - loss: 13.2331 - mse: 13.2331 - mae: 1.5526 - val_loss: 13.4795 - val_mse: 13.4795 - val_mae: 1.5720 - lr: 3.7868e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 10s - loss: 13.2464 - mse: 13.2464 - mae: 1.5570 - val_loss: 13.4302 - val_mse: 13.4302 - val_mae: 1.5579 - lr: 3.7868e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 10s - loss: 13.3027 - mse: 13.3027 - mae: 1.5540 - val_loss: 13.4137 - val_mse: 13.4137 - val_mae: 1.5153 - lr: 3.7868e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 10s - loss: 13.2739 - mse: 13.2739 - mae: 1.5523 - val_loss: 13.5505 - val_mse: 13.5505 - val_mae: 1.6202 - lr: 3.7868e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 10s - loss: 13.2830 - mse: 13.2830 - mae: 1.5535 - val_loss: 13.4360 - val_mse: 13.4360 - val_mae: 1.5163 - lr: 3.7868e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 10s - loss: 13.2613 - mse: 13.2613 - mae: 1.5508 - val_loss: 14.6064 - val_mse: 14.6064 - val_mae: 1.7306 - lr: 3.7868e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 10s - loss: 13.3037 - mse: 13.3037 - mae: 1.5515 - val_loss: 13.3963 - val_mse: 13.3963 - val_mae: 1.5407 - lr: 3.7868e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 10s - loss: 13.2788 - mse: 13.2788 - mae: 1.5504 - val_loss: 13.5904 - val_mse: 13.5904 - val_mae: 1.6545 - lr: 3.7868e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 10s - loss: 13.4093 - mse: 13.4093 - mae: 1.5498 - val_loss: 13.4271 - val_mse: 13.4271 - val_mae: 1.5366 - lr: 3.7868e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 10s - loss: 13.2816 - mse: 13.2816 - mae: 1.5511 - val_loss: 13.5526 - val_mse: 13.5526 - val_mae: 1.6184 - lr: 3.7868e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 10s - loss: 13.3369 - mse: 13.3369 - mae: 1.5619 - val_loss: 13.4150 - val_mse: 13.4150 - val_mae: 1.5261 - lr: 3.7868e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 11s - loss: 13.2775 - mse: 13.2775 - mae: 1.5505 - val_loss: 13.5314 - val_mse: 13.5314 - val_mae: 1.6301 - lr: 3.7868e-04 - 11s/epoch - 11ms/step\n",
            "Score for fold 2: loss of 13.531414031982422\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 10s - loss: 12.0457 - mse: 12.0457 - mae: 1.5595 - val_loss: 18.3909 - val_mse: 18.3909 - val_mae: 1.5555 - lr: 3.7868e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 10s - loss: 12.0305 - mse: 12.0305 - mae: 1.5668 - val_loss: 18.3590 - val_mse: 18.3590 - val_mae: 1.5286 - lr: 3.7868e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 10s - loss: 11.9825 - mse: 11.9825 - mae: 1.5533 - val_loss: 18.5228 - val_mse: 18.5228 - val_mae: 1.5696 - lr: 3.7868e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 10s - loss: 12.0537 - mse: 12.0537 - mae: 1.5611 - val_loss: 18.3938 - val_mse: 18.3938 - val_mae: 1.5445 - lr: 3.7868e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 10s - loss: 12.0065 - mse: 12.0065 - mae: 1.5613 - val_loss: 18.4237 - val_mse: 18.4237 - val_mae: 1.4599 - lr: 3.7868e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 10s - loss: 12.0567 - mse: 12.0567 - mae: 1.5596 - val_loss: 18.5750 - val_mse: 18.5750 - val_mae: 1.5992 - lr: 3.7868e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 10s - loss: 11.9785 - mse: 11.9785 - mae: 1.5593 - val_loss: 18.3813 - val_mse: 18.3813 - val_mae: 1.5686 - lr: 3.7868e-04 - 10s/epoch - 10ms/step\n",
            "Score for fold 3: loss of 18.38127899169922\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 10s - loss: 13.3153 - mse: 13.3153 - mae: 1.5526 - val_loss: 13.3639 - val_mse: 13.3639 - val_mae: 1.6063 - lr: 3.7868e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 10s - loss: 13.4074 - mse: 13.4074 - mae: 1.5495 - val_loss: 13.4693 - val_mse: 13.4693 - val_mae: 1.6055 - lr: 3.7868e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 10s - loss: 13.3316 - mse: 13.3316 - mae: 1.5492 - val_loss: 13.6127 - val_mse: 13.6127 - val_mae: 1.6598 - lr: 3.7868e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 10s - loss: 13.2936 - mse: 13.2936 - mae: 1.5544 - val_loss: 13.4590 - val_mse: 13.4590 - val_mae: 1.6371 - lr: 3.7868e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 10s - loss: 13.2787 - mse: 13.2787 - mae: 1.5474 - val_loss: 13.3116 - val_mse: 13.3116 - val_mae: 1.5966 - lr: 3.7868e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 10s - loss: 13.3453 - mse: 13.3453 - mae: 1.5554 - val_loss: 13.6889 - val_mse: 13.6889 - val_mae: 1.7423 - lr: 3.7868e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 10s - loss: 13.2676 - mse: 13.2676 - mae: 1.5522 - val_loss: 13.3165 - val_mse: 13.3165 - val_mae: 1.5772 - lr: 3.7868e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 10s - loss: 13.3400 - mse: 13.3400 - mae: 1.5450 - val_loss: 13.3653 - val_mse: 13.3653 - val_mae: 1.6035 - lr: 3.7868e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 10s - loss: 13.2207 - mse: 13.2207 - mae: 1.5480 - val_loss: 13.4988 - val_mse: 13.4988 - val_mae: 1.4841 - lr: 3.7868e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 10s - loss: 13.3898 - mse: 13.3898 - mae: 1.5516 - val_loss: 13.6023 - val_mse: 13.6023 - val_mae: 1.7093 - lr: 3.7868e-04 - 10s/epoch - 10ms/step\n",
            "Score for fold 4: loss of 13.602339744567871\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 10s - loss: 13.9920 - mse: 13.9920 - mae: 1.5653 - val_loss: 10.8138 - val_mse: 10.8138 - val_mae: 1.4569 - lr: 3.7868e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 10s - loss: 13.9453 - mse: 13.9453 - mae: 1.5584 - val_loss: 10.6564 - val_mse: 10.6564 - val_mae: 1.5010 - lr: 3.7868e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 10s - loss: 13.9627 - mse: 13.9627 - mae: 1.5599 - val_loss: 10.7556 - val_mse: 10.7556 - val_mae: 1.4702 - lr: 3.7868e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 10s - loss: 13.9317 - mse: 13.9317 - mae: 1.5665 - val_loss: 10.5878 - val_mse: 10.5878 - val_mae: 1.5058 - lr: 3.7868e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 10s - loss: 13.9746 - mse: 13.9746 - mae: 1.5625 - val_loss: 10.6097 - val_mse: 10.6097 - val_mae: 1.5103 - lr: 3.7868e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 10s - loss: 13.9559 - mse: 13.9559 - mae: 1.5543 - val_loss: 10.6248 - val_mse: 10.6248 - val_mae: 1.5464 - lr: 3.7868e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 10s - loss: 14.0024 - mse: 14.0024 - mae: 1.5677 - val_loss: 10.9901 - val_mse: 10.9901 - val_mae: 1.6406 - lr: 3.7868e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 10s - loss: 13.9928 - mse: 13.9928 - mae: 1.5602 - val_loss: 10.6231 - val_mse: 10.6231 - val_mae: 1.5564 - lr: 3.7868e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 10s - loss: 13.9377 - mse: 13.9377 - mae: 1.5667 - val_loss: 12.6318 - val_mse: 12.6318 - val_mae: 1.8809 - lr: 3.7868e-04 - 10s/epoch - 10ms/step\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-10-12 07:06:18,331]\u001b[0m Finished trial#48 resulted in value: 13.732. Current best value is 8.266 with parameters: {'activation': 'relu', 'num_hidden_layer': 4, 'i': 10, 'learning_rate': 0.0002464065452818332}.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Score for fold 5: loss of 12.631828308105469\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 12s - loss: 13.3603 - mse: 13.3603 - mae: 1.5490 - val_loss: 12.4909 - val_mse: 12.4909 - val_mae: 1.4486 - lr: 2.9387e-04 - 12s/epoch - 12ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 11s - loss: 12.5014 - mse: 12.5014 - mae: 1.4860 - val_loss: 12.4329 - val_mse: 12.4329 - val_mae: 1.4620 - lr: 2.9387e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 11s - loss: 12.0809 - mse: 12.0809 - mae: 1.4733 - val_loss: 12.0866 - val_mse: 12.0866 - val_mae: 1.4710 - lr: 2.9387e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 11s - loss: 12.1029 - mse: 12.1029 - mae: 1.4633 - val_loss: 12.3661 - val_mse: 12.3661 - val_mae: 1.4549 - lr: 2.9387e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 11s - loss: 11.9464 - mse: 11.9464 - mae: 1.4503 - val_loss: 11.8880 - val_mse: 11.8880 - val_mae: 1.5303 - lr: 2.9387e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 11s - loss: 11.7829 - mse: 11.7829 - mae: 1.4430 - val_loss: 11.8376 - val_mse: 11.8376 - val_mae: 1.4565 - lr: 2.9387e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 11s - loss: 11.4519 - mse: 11.4519 - mae: 1.4313 - val_loss: 11.9358 - val_mse: 11.9358 - val_mae: 1.5055 - lr: 2.9387e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 11s - loss: 11.4322 - mse: 11.4322 - mae: 1.4247 - val_loss: 11.7622 - val_mse: 11.7622 - val_mae: 1.4638 - lr: 2.9387e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 11s - loss: 11.3090 - mse: 11.3090 - mae: 1.4154 - val_loss: 11.8328 - val_mse: 11.8328 - val_mae: 1.4388 - lr: 2.9387e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 11s - loss: 11.1002 - mse: 11.1002 - mae: 1.3965 - val_loss: 12.0704 - val_mse: 12.0704 - val_mae: 1.5356 - lr: 2.9387e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 11s - loss: 10.8278 - mse: 10.8278 - mae: 1.3860 - val_loss: 12.0337 - val_mse: 12.0337 - val_mae: 1.5081 - lr: 2.9387e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 11s - loss: 10.5075 - mse: 10.5075 - mae: 1.3747 - val_loss: 12.0670 - val_mse: 12.0670 - val_mae: 1.4148 - lr: 2.9387e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 13/100\n",
            "1000/1000 - 11s - loss: 10.4999 - mse: 10.4999 - mae: 1.3543 - val_loss: 12.0118 - val_mse: 12.0118 - val_mae: 1.5068 - lr: 2.9387e-04 - 11s/epoch - 11ms/step\n",
            "Score for fold 1: loss of 12.011762619018555\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 11s - loss: 10.5562 - mse: 10.5562 - mae: 1.3929 - val_loss: 11.4848 - val_mse: 11.4848 - val_mae: 1.4308 - lr: 2.9387e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 11s - loss: 9.9618 - mse: 9.9618 - mae: 1.3681 - val_loss: 11.0079 - val_mse: 11.0079 - val_mae: 1.3287 - lr: 2.9387e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 11s - loss: 9.6351 - mse: 9.6351 - mae: 1.3531 - val_loss: 11.2584 - val_mse: 11.2584 - val_mae: 1.3723 - lr: 2.9387e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 11s - loss: 9.4802 - mse: 9.4802 - mae: 1.3378 - val_loss: 11.1216 - val_mse: 11.1216 - val_mae: 1.3913 - lr: 2.9387e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 11s - loss: 9.5532 - mse: 9.5532 - mae: 1.3239 - val_loss: 11.6126 - val_mse: 11.6126 - val_mae: 1.4016 - lr: 2.9387e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 11s - loss: 9.0452 - mse: 9.0452 - mae: 1.3056 - val_loss: 12.2999 - val_mse: 12.2999 - val_mae: 1.3450 - lr: 2.9387e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 11s - loss: 8.7870 - mse: 8.7870 - mae: 1.2871 - val_loss: 11.6379 - val_mse: 11.6379 - val_mae: 1.4908 - lr: 2.9387e-04 - 11s/epoch - 11ms/step\n",
            "Score for fold 2: loss of 11.637857437133789\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 11s - loss: 10.2917 - mse: 10.2917 - mae: 1.3316 - val_loss: 7.7412 - val_mse: 7.7412 - val_mae: 1.2734 - lr: 2.9387e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 11s - loss: 9.9036 - mse: 9.9036 - mae: 1.3092 - val_loss: 6.0430 - val_mse: 6.0430 - val_mae: 1.2526 - lr: 2.9387e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 11s - loss: 9.6812 - mse: 9.6812 - mae: 1.2845 - val_loss: 6.5443 - val_mse: 6.5443 - val_mae: 1.1957 - lr: 2.9387e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 11s - loss: 9.0208 - mse: 9.0208 - mae: 1.2675 - val_loss: 7.0079 - val_mse: 7.0079 - val_mae: 1.2092 - lr: 2.9387e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 11s - loss: 9.0002 - mse: 9.0002 - mae: 1.2530 - val_loss: 7.7719 - val_mse: 7.7719 - val_mae: 1.2604 - lr: 2.9387e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 11s - loss: 8.4349 - mse: 8.4349 - mae: 1.2492 - val_loss: 6.3590 - val_mse: 6.3590 - val_mae: 1.2209 - lr: 2.9387e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 11s - loss: 8.6458 - mse: 8.6458 - mae: 1.2335 - val_loss: 6.6590 - val_mse: 6.6590 - val_mae: 1.3274 - lr: 2.9387e-04 - 11s/epoch - 11ms/step\n",
            "Score for fold 3: loss of 6.659045219421387\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 11s - loss: 9.0741 - mse: 9.0741 - mae: 1.2589 - val_loss: 4.3996 - val_mse: 4.3996 - val_mae: 1.1369 - lr: 2.9387e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 12s - loss: 8.7446 - mse: 8.7446 - mae: 1.2325 - val_loss: 4.4693 - val_mse: 4.4693 - val_mae: 1.2857 - lr: 2.9387e-04 - 12s/epoch - 12ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 12s - loss: 8.1592 - mse: 8.1592 - mae: 1.2101 - val_loss: 4.6043 - val_mse: 4.6043 - val_mae: 1.1675 - lr: 2.9387e-04 - 12s/epoch - 12ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 11s - loss: 7.9954 - mse: 7.9954 - mae: 1.1952 - val_loss: 5.0135 - val_mse: 5.0135 - val_mae: 1.1765 - lr: 2.9387e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 11s - loss: 8.0448 - mse: 8.0448 - mae: 1.1820 - val_loss: 4.8052 - val_mse: 4.8052 - val_mae: 1.2360 - lr: 2.9387e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 11s - loss: 7.6044 - mse: 7.6044 - mae: 1.1622 - val_loss: 5.0761 - val_mse: 5.0761 - val_mae: 1.2105 - lr: 2.9387e-04 - 11s/epoch - 11ms/step\n",
            "Score for fold 4: loss of 5.076127529144287\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 11s - loss: 6.3521 - mse: 6.3521 - mae: 1.1957 - val_loss: 10.5154 - val_mse: 10.5154 - val_mae: 1.0760 - lr: 2.9387e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 11s - loss: 5.6269 - mse: 5.6269 - mae: 1.1607 - val_loss: 10.9514 - val_mse: 10.9514 - val_mae: 1.0936 - lr: 2.9387e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 11s - loss: 5.7150 - mse: 5.7150 - mae: 1.1462 - val_loss: 10.5850 - val_mse: 10.5850 - val_mae: 1.1449 - lr: 2.9387e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 11s - loss: 5.2919 - mse: 5.2919 - mae: 1.1275 - val_loss: 11.5889 - val_mse: 11.5889 - val_mae: 1.2386 - lr: 2.9387e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 11s - loss: 4.9911 - mse: 4.9911 - mae: 1.1112 - val_loss: 12.8347 - val_mse: 12.8347 - val_mae: 1.2507 - lr: 2.9387e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 11s - loss: 4.8385 - mse: 4.8385 - mae: 1.0950 - val_loss: 11.3584 - val_mse: 11.3584 - val_mae: 1.2742 - lr: 2.9387e-04 - 11s/epoch - 11ms/step\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-10-12 07:13:49,785]\u001b[0m Finished trial#49 resulted in value: 9.35. Current best value is 8.266 with parameters: {'activation': 'relu', 'num_hidden_layer': 4, 'i': 10, 'learning_rate': 0.0002464065452818332}.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Score for fold 5: loss of 11.35837173461914\n"
          ]
        }
      ],
      "source": [
        "model_list=[]\n",
        "history_list=[]\n",
        "study_name = 'NN_study_4'\n",
        "study = optuna.create_study(study_name=study_name, load_if_exists=True)\n",
        "func = lambda trial: objective(trial,training_shuffled3,labelsForTrain_shuffled3)\n",
        "study.optimize(func, n_trials=50,)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RxAKqk4RyJ39",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7107b849-efb1-4647-d2cf-a206f6fc298a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "1250/1250 - 10s - loss: 13.0072 - mse: 13.0072 - mae: 1.5320 - val_loss: 10.1108 - val_mse: 10.1108 - val_mae: 1.4740 - lr: 2.3446e-04 - 10s/epoch - 8ms/step\n",
            "Epoch 2/20\n",
            "1250/1250 - 10s - loss: 12.3459 - mse: 12.3459 - mae: 1.4875 - val_loss: 9.1224 - val_mse: 9.1224 - val_mae: 1.5955 - lr: 2.3446e-04 - 10s/epoch - 8ms/step\n",
            "Epoch 3/20\n",
            "1250/1250 - 10s - loss: 12.0455 - mse: 12.0455 - mae: 1.4725 - val_loss: 10.2879 - val_mse: 10.2879 - val_mae: 1.4975 - lr: 2.3446e-04 - 10s/epoch - 8ms/step\n",
            "Epoch 4/20\n",
            "1250/1250 - 10s - loss: 11.9728 - mse: 11.9728 - mae: 1.4627 - val_loss: 9.4100 - val_mse: 9.4100 - val_mae: 1.4249 - lr: 2.3446e-04 - 10s/epoch - 8ms/step\n",
            "Epoch 5/20\n",
            "1250/1250 - 10s - loss: 11.8766 - mse: 11.8766 - mae: 1.4530 - val_loss: 9.2896 - val_mse: 9.2896 - val_mae: 1.4845 - lr: 2.3446e-04 - 10s/epoch - 8ms/step\n",
            "Epoch 6/20\n",
            "1250/1250 - 10s - loss: 11.7795 - mse: 11.7795 - mae: 1.4432 - val_loss: 9.7825 - val_mse: 9.7825 - val_mae: 1.4486 - lr: 2.3446e-04 - 10s/epoch - 8ms/step\n",
            "Epoch 7/20\n",
            "1250/1250 - 10s - loss: 11.5378 - mse: 11.5378 - mae: 1.4345 - val_loss: 8.9566 - val_mse: 8.9566 - val_mae: 1.5397 - lr: 2.3446e-04 - 10s/epoch - 8ms/step\n",
            "Epoch 8/20\n",
            "1250/1250 - 10s - loss: 11.3308 - mse: 11.3308 - mae: 1.4206 - val_loss: 9.9403 - val_mse: 9.9403 - val_mae: 1.5055 - lr: 2.3446e-04 - 10s/epoch - 8ms/step\n",
            "Epoch 9/20\n",
            "1250/1250 - 10s - loss: 11.2584 - mse: 11.2584 - mae: 1.4159 - val_loss: 9.8499 - val_mse: 9.8499 - val_mae: 1.5024 - lr: 2.3446e-04 - 10s/epoch - 8ms/step\n",
            "Epoch 10/20\n",
            "1250/1250 - 10s - loss: 11.0296 - mse: 11.0296 - mae: 1.4055 - val_loss: 8.7314 - val_mse: 8.7314 - val_mae: 1.4546 - lr: 2.3446e-04 - 10s/epoch - 8ms/step\n",
            "Epoch 11/20\n",
            "1250/1250 - 10s - loss: 10.5340 - mse: 10.5340 - mae: 1.3879 - val_loss: 8.9400 - val_mse: 8.9400 - val_mae: 1.5104 - lr: 2.3446e-04 - 10s/epoch - 8ms/step\n",
            "Epoch 12/20\n",
            "1250/1250 - 10s - loss: 10.3794 - mse: 10.3794 - mae: 1.3766 - val_loss: 9.9915 - val_mse: 9.9915 - val_mae: 1.5271 - lr: 2.3446e-04 - 10s/epoch - 8ms/step\n",
            "Epoch 13/20\n",
            "1250/1250 - 10s - loss: 10.4894 - mse: 10.4894 - mae: 1.3698 - val_loss: 8.9731 - val_mse: 8.9731 - val_mae: 1.5039 - lr: 2.3446e-04 - 10s/epoch - 8ms/step\n",
            "Epoch 14/20\n",
            "1250/1250 - 10s - loss: 10.0670 - mse: 10.0670 - mae: 1.3551 - val_loss: 9.5120 - val_mse: 9.5120 - val_mae: 1.4678 - lr: 2.3446e-04 - 10s/epoch - 8ms/step\n",
            "Epoch 15/20\n",
            "1250/1250 - 10s - loss: 9.9494 - mse: 9.9494 - mae: 1.3445 - val_loss: 8.8507 - val_mse: 8.8507 - val_mae: 1.5058 - lr: 2.3446e-04 - 10s/epoch - 8ms/step\n",
            "Epoch 16/20\n",
            "1250/1250 - 11s - loss: 9.6564 - mse: 9.6564 - mae: 1.3242 - val_loss: 9.9842 - val_mse: 9.9842 - val_mae: 1.5007 - lr: 2.3446e-04 - 11s/epoch - 9ms/step\n",
            "Epoch 17/20\n",
            "1250/1250 - 11s - loss: 9.3659 - mse: 9.3659 - mae: 1.3225 - val_loss: 10.1685 - val_mse: 10.1685 - val_mae: 1.5326 - lr: 2.3446e-04 - 11s/epoch - 9ms/step\n",
            "Epoch 18/20\n",
            "1250/1250 - 11s - loss: 9.1927 - mse: 9.1927 - mae: 1.2980 - val_loss: 9.8951 - val_mse: 9.8951 - val_mae: 1.5288 - lr: 2.3446e-04 - 11s/epoch - 9ms/step\n",
            "Epoch 19/20\n",
            "1250/1250 - 10s - loss: 9.0941 - mse: 9.0941 - mae: 1.2875 - val_loss: 11.1294 - val_mse: 11.1294 - val_mae: 1.6593 - lr: 2.3446e-04 - 10s/epoch - 8ms/step\n",
            "Epoch 20/20\n",
            "1250/1250 - 10s - loss: 8.5960 - mse: 8.5960 - mae: 1.2758 - val_loss: 9.5740 - val_mse: 9.5740 - val_mae: 1.5909 - lr: 2.3446e-04 - 10s/epoch - 8ms/step\n"
          ]
        }
      ],
      "source": [
        "#{'activation': 'relu', 'num_hidden_layer': 4, 'i': 10, 'learning_rate': 0.0002344581502453532}.\n",
        "optimizer = Adam(learning_rate=0.0002344581502453532 ,clipnorm=1.0)\n",
        "model_4 = create_model(activation=\"relu\",num_hidden_layer=4,num_hidden_unit=1024)\n",
        "\n",
        "es = EarlyStopping(monitor='mse', patience=5)\n",
        "reduce_lr = ReduceLROnPlateau(monitor='val_mse', factor=0.2,\n",
        "                              patience=20, min_lr=0.001)\n",
        "model_4.compile(loss='mse',optimizer=optimizer,metrics=['mse','mae'])\n",
        "history = model_4.fit(training,labelsForTrain,\n",
        "                batch_size=64,\n",
        "                epochs=20,\n",
        "                verbose=2,\n",
        "                validation_data=(valing,labelsForVal),\n",
        "                validation_batch_size=64,\n",
        "                callbacks=[es,reduce_lr])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6h-GPMVRyPru"
      },
      "outputs": [],
      "source": [
        "results_model4 = model_4.evaluate(testing, labelsForTest, batch_size=32)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dV7ly-XIyTWR"
      },
      "source": [
        "## Shuffle Repetation 5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MJ-DVmmlyYDk"
      },
      "outputs": [],
      "source": [
        "# Shuffle the data after train\n",
        "shuffled4 = shuffle(train_df, random_state=4)\n",
        "training_shuffled4,labelsForTrain_shuffled4=process_shuffle_dataset(shuffled4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "54_G8WmeyesV",
        "outputId": "c8fc954c-f573-4382-8e44-09797110ddd1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 13.0544 - mse: 13.0544 - mae: 1.5487 - val_loss: 13.4013 - val_mse: 13.4013 - val_mae: 1.4425 - lr: 0.0032 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 1s - loss: 12.1720 - mse: 12.1720 - mae: 1.4975 - val_loss: 13.3141 - val_mse: 13.3141 - val_mae: 1.4376 - lr: 0.0032 - 1s/epoch - 1ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 1s - loss: 12.1458 - mse: 12.1458 - mae: 1.4826 - val_loss: 13.1327 - val_mse: 13.1327 - val_mae: 1.4581 - lr: 0.0032 - 1s/epoch - 1ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 1s - loss: 11.8623 - mse: 11.8623 - mae: 1.4758 - val_loss: 13.1570 - val_mse: 13.1570 - val_mae: 1.4560 - lr: 0.0032 - 1s/epoch - 1ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 1s - loss: 12.0117 - mse: 12.0117 - mae: 1.4720 - val_loss: 13.2969 - val_mse: 13.2969 - val_mae: 1.5034 - lr: 0.0032 - 1s/epoch - 1ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 1s - loss: 11.8001 - mse: 11.8001 - mae: 1.4748 - val_loss: 12.8407 - val_mse: 12.8407 - val_mae: 1.4486 - lr: 0.0032 - 1s/epoch - 1ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 1s - loss: 11.6769 - mse: 11.6769 - mae: 1.4685 - val_loss: 12.8307 - val_mse: 12.8307 - val_mae: 1.4721 - lr: 0.0032 - 1s/epoch - 1ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 1s - loss: 11.5341 - mse: 11.5341 - mae: 1.4571 - val_loss: 13.2141 - val_mse: 13.2141 - val_mae: 1.4825 - lr: 0.0032 - 1s/epoch - 1ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 1s - loss: 11.5726 - mse: 11.5726 - mae: 1.4502 - val_loss: 13.0685 - val_mse: 13.0685 - val_mae: 1.4644 - lr: 0.0032 - 1s/epoch - 1ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 1s - loss: 11.4326 - mse: 11.4326 - mae: 1.4462 - val_loss: 13.3217 - val_mse: 13.3217 - val_mae: 1.4666 - lr: 0.0032 - 1s/epoch - 1ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 1s - loss: 11.2067 - mse: 11.2067 - mae: 1.4381 - val_loss: 13.1094 - val_mse: 13.1094 - val_mae: 1.4333 - lr: 0.0032 - 1s/epoch - 1ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 1s - loss: 11.1626 - mse: 11.1626 - mae: 1.4294 - val_loss: 13.0598 - val_mse: 13.0598 - val_mae: 1.4790 - lr: 0.0032 - 1s/epoch - 1ms/step\n",
            "Score for fold 1: loss of 13.05981731414795\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 1s - loss: 12.0278 - mse: 12.0278 - mae: 1.4230 - val_loss: 8.1146 - val_mse: 8.1146 - val_mae: 1.3907 - lr: 0.0010 - 1s/epoch - 1ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 1s - loss: 11.9559 - mse: 11.9559 - mae: 1.4164 - val_loss: 7.8869 - val_mse: 7.8869 - val_mae: 1.3931 - lr: 0.0010 - 1s/epoch - 1ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 1s - loss: 11.8533 - mse: 11.8533 - mae: 1.4138 - val_loss: 7.9022 - val_mse: 7.9022 - val_mae: 1.4017 - lr: 0.0010 - 1s/epoch - 1ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 1s - loss: 11.7318 - mse: 11.7318 - mae: 1.4093 - val_loss: 7.9167 - val_mse: 7.9167 - val_mae: 1.3935 - lr: 0.0010 - 1s/epoch - 1ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 1s - loss: 11.6718 - mse: 11.6718 - mae: 1.4068 - val_loss: 8.0105 - val_mse: 8.0105 - val_mae: 1.3833 - lr: 0.0010 - 1s/epoch - 1ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 1s - loss: 11.5900 - mse: 11.5900 - mae: 1.4043 - val_loss: 7.9174 - val_mse: 7.9174 - val_mae: 1.3958 - lr: 0.0010 - 1s/epoch - 1ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 1s - loss: 11.5159 - mse: 11.5159 - mae: 1.3990 - val_loss: 7.9903 - val_mse: 7.9903 - val_mae: 1.3841 - lr: 0.0010 - 1s/epoch - 1ms/step\n",
            "Score for fold 2: loss of 7.990260601043701\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 1s - loss: 9.9075 - mse: 9.9075 - mae: 1.4061 - val_loss: 14.4287 - val_mse: 14.4287 - val_mae: 1.4018 - lr: 0.0010 - 1s/epoch - 1ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 1s - loss: 9.8699 - mse: 9.8699 - mae: 1.4015 - val_loss: 14.4723 - val_mse: 14.4723 - val_mae: 1.3933 - lr: 0.0010 - 1s/epoch - 1ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 1s - loss: 9.7189 - mse: 9.7189 - mae: 1.3980 - val_loss: 14.5368 - val_mse: 14.5368 - val_mae: 1.3879 - lr: 0.0010 - 1s/epoch - 1ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 1s - loss: 9.6859 - mse: 9.6859 - mae: 1.3980 - val_loss: 14.5351 - val_mse: 14.5351 - val_mae: 1.3687 - lr: 0.0010 - 1s/epoch - 1ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 1s - loss: 9.6050 - mse: 9.6050 - mae: 1.3919 - val_loss: 14.6838 - val_mse: 14.6838 - val_mae: 1.3858 - lr: 0.0010 - 1s/epoch - 1ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 1s - loss: 9.5992 - mse: 9.5992 - mae: 1.3881 - val_loss: 14.6553 - val_mse: 14.6553 - val_mae: 1.3907 - lr: 0.0010 - 1s/epoch - 1ms/step\n",
            "Score for fold 3: loss of 14.655265808105469\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 1s - loss: 10.7659 - mse: 10.7659 - mae: 1.3919 - val_loss: 9.7417 - val_mse: 9.7417 - val_mae: 1.4090 - lr: 0.0010 - 1s/epoch - 1ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 1s - loss: 10.7258 - mse: 10.7258 - mae: 1.3878 - val_loss: 9.8892 - val_mse: 9.8892 - val_mae: 1.3908 - lr: 0.0010 - 1s/epoch - 1ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 1s - loss: 10.6679 - mse: 10.6679 - mae: 1.3868 - val_loss: 10.2278 - val_mse: 10.2278 - val_mae: 1.3868 - lr: 0.0010 - 1s/epoch - 1ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 1s - loss: 10.6273 - mse: 10.6273 - mae: 1.3810 - val_loss: 10.2743 - val_mse: 10.2743 - val_mae: 1.4004 - lr: 0.0010 - 1s/epoch - 1ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 1s - loss: 10.5816 - mse: 10.5816 - mae: 1.3816 - val_loss: 10.6533 - val_mse: 10.6533 - val_mae: 1.3997 - lr: 0.0010 - 1s/epoch - 1ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 1s - loss: 10.4704 - mse: 10.4704 - mae: 1.3756 - val_loss: 10.5589 - val_mse: 10.5589 - val_mae: 1.3972 - lr: 0.0010 - 1s/epoch - 1ms/step\n",
            "Score for fold 4: loss of 10.558897018432617\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 1s - loss: 10.8317 - mse: 10.8317 - mae: 1.3896 - val_loss: 9.1783 - val_mse: 9.1783 - val_mae: 1.3858 - lr: 0.0010 - 1s/epoch - 1ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 1s - loss: 10.7842 - mse: 10.7842 - mae: 1.3853 - val_loss: 9.3817 - val_mse: 9.3817 - val_mae: 1.3874 - lr: 0.0010 - 1s/epoch - 1ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 1s - loss: 10.7979 - mse: 10.7979 - mae: 1.3820 - val_loss: 9.6880 - val_mse: 9.6880 - val_mae: 1.3855 - lr: 0.0010 - 1s/epoch - 1ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 1s - loss: 10.6638 - mse: 10.6638 - mae: 1.3825 - val_loss: 9.3037 - val_mse: 9.3037 - val_mae: 1.3812 - lr: 0.0010 - 1s/epoch - 1ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 1s - loss: 10.6428 - mse: 10.6428 - mae: 1.3754 - val_loss: 9.6486 - val_mse: 9.6486 - val_mae: 1.4227 - lr: 0.0010 - 1s/epoch - 1ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 1s - loss: 10.5977 - mse: 10.5977 - mae: 1.3753 - val_loss: 9.6832 - val_mse: 9.6832 - val_mae: 1.4253 - lr: 0.0010 - 1s/epoch - 1ms/step\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-10-12 07:28:31,493]\u001b[0m Finished trial#0 resulted in value: 11.190000000000001. Current best value is 11.190000000000001 with parameters: {'activation': 'relu', 'num_hidden_layer': 3, 'i': 6, 'learning_rate': 0.003169692767892019}.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Score for fold 5: loss of 9.683155059814453\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 13.9120 - mse: 13.9120 - mae: 1.5929 - val_loss: 12.1817 - val_mse: 12.1817 - val_mae: 1.5640 - lr: 0.0096 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 13.2943 - mse: 13.2943 - mae: 1.5495 - val_loss: 12.0714 - val_mse: 12.0714 - val_mae: 1.5963 - lr: 0.0096 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 1s - loss: 12.8891 - mse: 12.8891 - mae: 1.5357 - val_loss: 12.2091 - val_mse: 12.2091 - val_mae: 1.4487 - lr: 0.0096 - 1s/epoch - 1ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 1s - loss: 12.7675 - mse: 12.7675 - mae: 1.5291 - val_loss: 12.3396 - val_mse: 12.3396 - val_mae: 1.4174 - lr: 0.0096 - 1s/epoch - 1ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 12.7592 - mse: 12.7592 - mae: 1.5269 - val_loss: 12.2607 - val_mse: 12.2607 - val_mae: 1.4205 - lr: 0.0096 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 12.5859 - mse: 12.5859 - mae: 1.5183 - val_loss: 11.7219 - val_mse: 11.7219 - val_mae: 1.5241 - lr: 0.0096 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 12.5969 - mse: 12.5969 - mae: 1.5215 - val_loss: 11.8091 - val_mse: 11.8091 - val_mae: 1.4455 - lr: 0.0096 - 2s/epoch - 2ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 2s - loss: 12.7581 - mse: 12.7581 - mae: 1.5163 - val_loss: 12.2844 - val_mse: 12.2844 - val_mae: 1.3987 - lr: 0.0096 - 2s/epoch - 2ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 2s - loss: 12.7262 - mse: 12.7262 - mae: 1.5148 - val_loss: 11.6374 - val_mse: 11.6374 - val_mae: 1.5546 - lr: 0.0096 - 2s/epoch - 2ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 2s - loss: 12.3180 - mse: 12.3180 - mae: 1.5066 - val_loss: 12.3022 - val_mse: 12.3022 - val_mae: 1.4437 - lr: 0.0096 - 2s/epoch - 2ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 2s - loss: 12.7497 - mse: 12.7497 - mae: 1.5116 - val_loss: 11.6944 - val_mse: 11.6944 - val_mae: 1.4488 - lr: 0.0096 - 2s/epoch - 2ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 2s - loss: 12.4660 - mse: 12.4660 - mae: 1.5149 - val_loss: 11.9349 - val_mse: 11.9349 - val_mae: 1.4812 - lr: 0.0096 - 2s/epoch - 2ms/step\n",
            "Epoch 13/100\n",
            "1000/1000 - 2s - loss: 12.4705 - mse: 12.4705 - mae: 1.5097 - val_loss: 11.7461 - val_mse: 11.7461 - val_mae: 1.4531 - lr: 0.0096 - 2s/epoch - 2ms/step\n",
            "Epoch 14/100\n",
            "1000/1000 - 1s - loss: 12.4902 - mse: 12.4902 - mae: 1.5061 - val_loss: 12.0079 - val_mse: 12.0079 - val_mae: 1.5847 - lr: 0.0096 - 1s/epoch - 1ms/step\n",
            "Score for fold 1: loss of 12.007950782775879\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 12.1255 - mse: 12.1255 - mae: 1.4655 - val_loss: 10.5869 - val_mse: 10.5869 - val_mae: 1.4696 - lr: 0.0019 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 1s - loss: 12.0570 - mse: 12.0570 - mae: 1.4627 - val_loss: 10.2223 - val_mse: 10.2223 - val_mae: 1.4264 - lr: 0.0019 - 1s/epoch - 1ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 1s - loss: 12.0089 - mse: 12.0089 - mae: 1.4578 - val_loss: 10.2255 - val_mse: 10.2255 - val_mae: 1.4815 - lr: 0.0019 - 1s/epoch - 1ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 11.8181 - mse: 11.8181 - mae: 1.4553 - val_loss: 10.2171 - val_mse: 10.2171 - val_mae: 1.4699 - lr: 0.0019 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 1s - loss: 11.8331 - mse: 11.8331 - mae: 1.4561 - val_loss: 10.3108 - val_mse: 10.3108 - val_mae: 1.4457 - lr: 0.0019 - 1s/epoch - 1ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 11.7905 - mse: 11.7905 - mae: 1.4522 - val_loss: 10.2841 - val_mse: 10.2841 - val_mae: 1.4806 - lr: 0.0019 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 11.7955 - mse: 11.7955 - mae: 1.4489 - val_loss: 10.6144 - val_mse: 10.6144 - val_mae: 1.4238 - lr: 0.0019 - 2s/epoch - 2ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 2s - loss: 11.6610 - mse: 11.6610 - mae: 1.4490 - val_loss: 10.2756 - val_mse: 10.2756 - val_mae: 1.5043 - lr: 0.0019 - 2s/epoch - 2ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 2s - loss: 11.7654 - mse: 11.7654 - mae: 1.4477 - val_loss: 10.4800 - val_mse: 10.4800 - val_mae: 1.4239 - lr: 0.0019 - 2s/epoch - 2ms/step\n",
            "Score for fold 2: loss of 10.4800443649292\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 12.1312 - mse: 12.1312 - mae: 1.4398 - val_loss: 7.7770 - val_mse: 7.7770 - val_mae: 1.4421 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 12.1260 - mse: 12.1260 - mae: 1.4353 - val_loss: 7.7503 - val_mse: 7.7503 - val_mae: 1.4414 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 12.0903 - mse: 12.0903 - mae: 1.4371 - val_loss: 7.8349 - val_mse: 7.8349 - val_mae: 1.4283 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 1s - loss: 12.0599 - mse: 12.0599 - mae: 1.4321 - val_loss: 7.8670 - val_mse: 7.8670 - val_mae: 1.4328 - lr: 0.0010 - 1s/epoch - 1ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 12.0263 - mse: 12.0263 - mae: 1.4310 - val_loss: 7.8875 - val_mse: 7.8875 - val_mae: 1.4230 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 12.0073 - mse: 12.0073 - mae: 1.4279 - val_loss: 7.8594 - val_mse: 7.8594 - val_mae: 1.4354 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 1s - loss: 11.9714 - mse: 11.9714 - mae: 1.4307 - val_loss: 7.9116 - val_mse: 7.9116 - val_mae: 1.4572 - lr: 0.0010 - 1s/epoch - 1ms/step\n",
            "Score for fold 3: loss of 7.911574363708496\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 9.5465 - mse: 9.5465 - mae: 1.4279 - val_loss: 17.6431 - val_mse: 17.6431 - val_mae: 1.4563 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 1s - loss: 9.4986 - mse: 9.4986 - mae: 1.4259 - val_loss: 17.5939 - val_mse: 17.5939 - val_mae: 1.4460 - lr: 0.0010 - 1s/epoch - 1ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 1s - loss: 9.4472 - mse: 9.4472 - mae: 1.4208 - val_loss: 17.8788 - val_mse: 17.8788 - val_mae: 1.4484 - lr: 0.0010 - 1s/epoch - 1ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 9.4121 - mse: 9.4121 - mae: 1.4211 - val_loss: 17.7782 - val_mse: 17.7782 - val_mae: 1.4343 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 9.3891 - mse: 9.3891 - mae: 1.4173 - val_loss: 17.8801 - val_mse: 17.8801 - val_mae: 1.4232 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 9.4568 - mse: 9.4568 - mae: 1.4169 - val_loss: 18.0571 - val_mse: 18.0571 - val_mae: 1.4639 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 1s - loss: 9.3859 - mse: 9.3859 - mae: 1.4175 - val_loss: 18.3011 - val_mse: 18.3011 - val_mae: 1.4447 - lr: 0.0010 - 1s/epoch - 1ms/step\n",
            "Score for fold 4: loss of 18.301076889038086\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 11.5222 - mse: 11.5222 - mae: 1.4258 - val_loss: 9.3139 - val_mse: 9.3139 - val_mae: 1.4123 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 11.3949 - mse: 11.3949 - mae: 1.4215 - val_loss: 9.6465 - val_mse: 9.6465 - val_mae: 1.4116 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 1s - loss: 11.4210 - mse: 11.4210 - mae: 1.4206 - val_loss: 9.5143 - val_mse: 9.5143 - val_mae: 1.4324 - lr: 0.0010 - 1s/epoch - 1ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 11.4524 - mse: 11.4524 - mae: 1.4213 - val_loss: 9.4839 - val_mse: 9.4839 - val_mae: 1.4457 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 1s - loss: 11.3546 - mse: 11.3546 - mae: 1.4190 - val_loss: 9.6677 - val_mse: 9.6677 - val_mae: 1.4466 - lr: 0.0010 - 1s/epoch - 1ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 11.3577 - mse: 11.3577 - mae: 1.4202 - val_loss: 9.9022 - val_mse: 9.9022 - val_mae: 1.4306 - lr: 0.0010 - 2s/epoch - 2ms/step\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-10-12 07:29:41,229]\u001b[0m Finished trial#1 resulted in value: 11.72. Current best value is 11.190000000000001 with parameters: {'activation': 'relu', 'num_hidden_layer': 3, 'i': 6, 'learning_rate': 0.003169692767892019}.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Score for fold 5: loss of 9.902189254760742\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 5s - loss: 16.2432 - mse: 16.2432 - mae: 1.7168 - val_loss: 11.0240 - val_mse: 11.0240 - val_mae: 1.8250 - lr: 0.0029 - 5s/epoch - 5ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 4s - loss: 14.9802 - mse: 14.9802 - mae: 1.6234 - val_loss: 10.3844 - val_mse: 10.3844 - val_mae: 1.5598 - lr: 0.0029 - 4s/epoch - 4ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 4s - loss: 14.6159 - mse: 14.6159 - mae: 1.6144 - val_loss: 10.5185 - val_mse: 10.5185 - val_mae: 1.5683 - lr: 0.0029 - 4s/epoch - 4ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 3s - loss: 14.6373 - mse: 14.6373 - mae: 1.6152 - val_loss: 12.1060 - val_mse: 12.1060 - val_mae: 1.8012 - lr: 0.0029 - 3s/epoch - 3ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 3s - loss: 14.6801 - mse: 14.6801 - mae: 1.6053 - val_loss: 10.4707 - val_mse: 10.4707 - val_mae: 1.5456 - lr: 0.0029 - 3s/epoch - 3ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 3s - loss: 14.7063 - mse: 14.7063 - mae: 1.6189 - val_loss: 10.4042 - val_mse: 10.4042 - val_mae: 1.5519 - lr: 0.0029 - 3s/epoch - 3ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 3s - loss: 14.5105 - mse: 14.5105 - mae: 1.6018 - val_loss: 11.3877 - val_mse: 11.3877 - val_mae: 1.7110 - lr: 0.0029 - 3s/epoch - 3ms/step\n",
            "Score for fold 1: loss of 11.387738227844238\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 3s - loss: 14.0415 - mse: 14.0415 - mae: 1.5587 - val_loss: 10.5975 - val_mse: 10.5975 - val_mae: 1.5635 - lr: 0.0010 - 3s/epoch - 3ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 3s - loss: 14.0772 - mse: 14.0772 - mae: 1.5620 - val_loss: 10.4658 - val_mse: 10.4658 - val_mae: 1.6114 - lr: 0.0010 - 3s/epoch - 3ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 3s - loss: 14.0474 - mse: 14.0474 - mae: 1.5613 - val_loss: 10.5440 - val_mse: 10.5440 - val_mae: 1.5200 - lr: 0.0010 - 3s/epoch - 3ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 3s - loss: 14.1931 - mse: 14.1931 - mae: 1.5624 - val_loss: 10.4487 - val_mse: 10.4487 - val_mae: 1.5296 - lr: 0.0010 - 3s/epoch - 3ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 3s - loss: 14.1242 - mse: 14.1242 - mae: 1.5686 - val_loss: 10.3692 - val_mse: 10.3692 - val_mae: 1.5666 - lr: 0.0010 - 3s/epoch - 3ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 3s - loss: 14.2554 - mse: 14.2554 - mae: 1.5650 - val_loss: 10.2166 - val_mse: 10.2166 - val_mae: 1.5137 - lr: 0.0010 - 3s/epoch - 3ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 3s - loss: 14.0657 - mse: 14.0657 - mae: 1.5595 - val_loss: 11.6169 - val_mse: 11.6169 - val_mae: 1.6646 - lr: 0.0010 - 3s/epoch - 3ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 3s - loss: 13.9550 - mse: 13.9550 - mae: 1.5635 - val_loss: 11.3181 - val_mse: 11.3181 - val_mae: 1.5797 - lr: 0.0010 - 3s/epoch - 3ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 3s - loss: 14.1999 - mse: 14.1999 - mae: 1.5621 - val_loss: 10.3944 - val_mse: 10.3944 - val_mae: 1.5553 - lr: 0.0010 - 3s/epoch - 3ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 3s - loss: 14.1772 - mse: 14.1772 - mae: 1.5658 - val_loss: 10.2795 - val_mse: 10.2795 - val_mae: 1.6108 - lr: 0.0010 - 3s/epoch - 3ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 3s - loss: 14.1073 - mse: 14.1073 - mae: 1.5630 - val_loss: 11.1296 - val_mse: 11.1296 - val_mae: 1.5573 - lr: 0.0010 - 3s/epoch - 3ms/step\n",
            "Score for fold 2: loss of 11.129591941833496\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 3s - loss: 13.1909 - mse: 13.1909 - mae: 1.5541 - val_loss: 14.0984 - val_mse: 14.0984 - val_mae: 1.5363 - lr: 0.0010 - 3s/epoch - 3ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 3s - loss: 13.1813 - mse: 13.1813 - mae: 1.5628 - val_loss: 13.8317 - val_mse: 13.8317 - val_mae: 1.5638 - lr: 0.0010 - 3s/epoch - 3ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 3s - loss: 13.1292 - mse: 13.1292 - mae: 1.5531 - val_loss: 14.0419 - val_mse: 14.0419 - val_mae: 1.5734 - lr: 0.0010 - 3s/epoch - 3ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 3s - loss: 12.9792 - mse: 12.9792 - mae: 1.5511 - val_loss: 13.8210 - val_mse: 13.8210 - val_mae: 1.5786 - lr: 0.0010 - 3s/epoch - 3ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 3s - loss: 13.2782 - mse: 13.2782 - mae: 1.5585 - val_loss: 13.8854 - val_mse: 13.8854 - val_mae: 1.5502 - lr: 0.0010 - 3s/epoch - 3ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 3s - loss: 13.1937 - mse: 13.1937 - mae: 1.5550 - val_loss: 14.0964 - val_mse: 14.0964 - val_mae: 1.5895 - lr: 0.0010 - 3s/epoch - 3ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 3s - loss: 13.1663 - mse: 13.1663 - mae: 1.5593 - val_loss: 14.2288 - val_mse: 14.2288 - val_mae: 1.5984 - lr: 0.0010 - 3s/epoch - 3ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 3s - loss: 13.1849 - mse: 13.1849 - mae: 1.5537 - val_loss: 13.9475 - val_mse: 13.9475 - val_mae: 1.5603 - lr: 0.0010 - 3s/epoch - 3ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 3s - loss: 13.2908 - mse: 13.2908 - mae: 1.5534 - val_loss: 14.1230 - val_mse: 14.1230 - val_mae: 1.5968 - lr: 0.0010 - 3s/epoch - 3ms/step\n",
            "Score for fold 3: loss of 14.123041152954102\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 3s - loss: 13.9767 - mse: 13.9767 - mae: 1.5673 - val_loss: 10.3693 - val_mse: 10.3693 - val_mae: 1.5375 - lr: 0.0010 - 3s/epoch - 3ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 3s - loss: 14.0020 - mse: 14.0020 - mae: 1.5545 - val_loss: 11.2383 - val_mse: 11.2383 - val_mae: 1.4900 - lr: 0.0010 - 3s/epoch - 3ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 3s - loss: 13.9895 - mse: 13.9895 - mae: 1.5641 - val_loss: 11.1947 - val_mse: 11.1947 - val_mae: 1.5205 - lr: 0.0010 - 3s/epoch - 3ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 3s - loss: 13.9937 - mse: 13.9937 - mae: 1.5646 - val_loss: 10.7247 - val_mse: 10.7247 - val_mae: 1.5273 - lr: 0.0010 - 3s/epoch - 3ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 3s - loss: 14.0339 - mse: 14.0339 - mae: 1.5693 - val_loss: 10.5353 - val_mse: 10.5353 - val_mae: 1.5113 - lr: 0.0010 - 3s/epoch - 3ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 3s - loss: 13.9857 - mse: 13.9857 - mae: 1.5652 - val_loss: 10.7207 - val_mse: 10.7207 - val_mae: 1.5231 - lr: 0.0010 - 3s/epoch - 3ms/step\n",
            "Score for fold 4: loss of 10.720708847045898\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 4s - loss: 11.3642 - mse: 11.3642 - mae: 1.5575 - val_loss: 21.1378 - val_mse: 21.1378 - val_mae: 1.5164 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 3s - loss: 11.6188 - mse: 11.6188 - mae: 1.5577 - val_loss: 21.0649 - val_mse: 21.0649 - val_mae: 1.5269 - lr: 0.0010 - 3s/epoch - 3ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 3s - loss: 11.4211 - mse: 11.4211 - mae: 1.5598 - val_loss: 21.1872 - val_mse: 21.1872 - val_mae: 1.5504 - lr: 0.0010 - 3s/epoch - 3ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 3s - loss: 11.3696 - mse: 11.3696 - mae: 1.5583 - val_loss: 21.7161 - val_mse: 21.7161 - val_mae: 1.6503 - lr: 0.0010 - 3s/epoch - 3ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 3s - loss: 11.4426 - mse: 11.4426 - mae: 1.5531 - val_loss: 21.1827 - val_mse: 21.1827 - val_mae: 1.5875 - lr: 0.0010 - 3s/epoch - 3ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 3s - loss: 11.4475 - mse: 11.4475 - mae: 1.5636 - val_loss: 21.2245 - val_mse: 21.2245 - val_mae: 1.6053 - lr: 0.0010 - 3s/epoch - 3ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 3s - loss: 11.5723 - mse: 11.5723 - mae: 1.5619 - val_loss: 21.1868 - val_mse: 21.1868 - val_mae: 1.5829 - lr: 0.0010 - 3s/epoch - 3ms/step\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-10-12 07:32:04,439]\u001b[0m Finished trial#2 resulted in value: 13.709999999999999. Current best value is 11.190000000000001 with parameters: {'activation': 'relu', 'num_hidden_layer': 3, 'i': 6, 'learning_rate': 0.003169692767892019}.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Score for fold 5: loss of 21.18683624267578\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 5s - loss: 14.7928 - mse: 14.7928 - mae: 1.6543 - val_loss: 13.4959 - val_mse: 13.4959 - val_mae: 1.7183 - lr: 1.3807e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 4s - loss: 13.5857 - mse: 13.5857 - mae: 1.5710 - val_loss: 13.0020 - val_mse: 13.0020 - val_mae: 1.5652 - lr: 1.3807e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 4s - loss: 13.6375 - mse: 13.6375 - mae: 1.5625 - val_loss: 13.0130 - val_mse: 13.0130 - val_mae: 1.5282 - lr: 1.3807e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 4s - loss: 13.5001 - mse: 13.5001 - mae: 1.5568 - val_loss: 12.8967 - val_mse: 12.8967 - val_mae: 1.5000 - lr: 1.3807e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 4s - loss: 13.5281 - mse: 13.5281 - mae: 1.5501 - val_loss: 12.7708 - val_mse: 12.7708 - val_mae: 1.5600 - lr: 1.3807e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 4s - loss: 13.3998 - mse: 13.3998 - mae: 1.5507 - val_loss: 12.8179 - val_mse: 12.8179 - val_mae: 1.5598 - lr: 1.3807e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 4s - loss: 13.4067 - mse: 13.4067 - mae: 1.5469 - val_loss: 12.7381 - val_mse: 12.7381 - val_mae: 1.5791 - lr: 1.3807e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 4s - loss: 13.4999 - mse: 13.4999 - mae: 1.5520 - val_loss: 12.8697 - val_mse: 12.8697 - val_mae: 1.5676 - lr: 1.3807e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 4s - loss: 13.3798 - mse: 13.3798 - mae: 1.5442 - val_loss: 12.7346 - val_mse: 12.7346 - val_mae: 1.5760 - lr: 1.3807e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 4s - loss: 13.4149 - mse: 13.4149 - mae: 1.5455 - val_loss: 12.6307 - val_mse: 12.6307 - val_mae: 1.5580 - lr: 1.3807e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 4s - loss: 13.3626 - mse: 13.3626 - mae: 1.5461 - val_loss: 12.7539 - val_mse: 12.7539 - val_mae: 1.5613 - lr: 1.3807e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 4s - loss: 13.4017 - mse: 13.4017 - mae: 1.5457 - val_loss: 12.7461 - val_mse: 12.7461 - val_mae: 1.5888 - lr: 1.3807e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 13/100\n",
            "1000/1000 - 4s - loss: 13.3133 - mse: 13.3133 - mae: 1.5429 - val_loss: 12.7026 - val_mse: 12.7026 - val_mae: 1.5159 - lr: 1.3807e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 14/100\n",
            "1000/1000 - 4s - loss: 13.3666 - mse: 13.3666 - mae: 1.5488 - val_loss: 12.9200 - val_mse: 12.9200 - val_mae: 1.5926 - lr: 1.3807e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 15/100\n",
            "1000/1000 - 4s - loss: 13.3941 - mse: 13.3941 - mae: 1.5412 - val_loss: 12.6106 - val_mse: 12.6106 - val_mae: 1.5653 - lr: 1.3807e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 16/100\n",
            "1000/1000 - 4s - loss: 13.4288 - mse: 13.4288 - mae: 1.5437 - val_loss: 12.6316 - val_mse: 12.6316 - val_mae: 1.5521 - lr: 1.3807e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 17/100\n",
            "1000/1000 - 4s - loss: 13.3562 - mse: 13.3562 - mae: 1.5469 - val_loss: 12.6137 - val_mse: 12.6137 - val_mae: 1.5344 - lr: 1.3807e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 18/100\n",
            "1000/1000 - 4s - loss: 13.3164 - mse: 13.3164 - mae: 1.5414 - val_loss: 12.7007 - val_mse: 12.7007 - val_mae: 1.5634 - lr: 1.3807e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 19/100\n",
            "1000/1000 - 4s - loss: 13.4135 - mse: 13.4135 - mae: 1.5436 - val_loss: 12.5846 - val_mse: 12.5846 - val_mae: 1.5325 - lr: 1.3807e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 20/100\n",
            "1000/1000 - 4s - loss: 13.3660 - mse: 13.3660 - mae: 1.5454 - val_loss: 12.7670 - val_mse: 12.7670 - val_mae: 1.5364 - lr: 1.3807e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 21/100\n",
            "1000/1000 - 4s - loss: 13.3563 - mse: 13.3563 - mae: 1.5461 - val_loss: 12.6420 - val_mse: 12.6420 - val_mae: 1.5259 - lr: 1.3807e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 22/100\n",
            "1000/1000 - 4s - loss: 13.3506 - mse: 13.3506 - mae: 1.5401 - val_loss: 12.6478 - val_mse: 12.6478 - val_mae: 1.5820 - lr: 1.3807e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 23/100\n",
            "1000/1000 - 4s - loss: 13.3039 - mse: 13.3039 - mae: 1.5469 - val_loss: 12.6683 - val_mse: 12.6683 - val_mae: 1.5505 - lr: 1.3807e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 24/100\n",
            "1000/1000 - 5s - loss: 13.2822 - mse: 13.2822 - mae: 1.5435 - val_loss: 12.5481 - val_mse: 12.5481 - val_mae: 1.5456 - lr: 1.3807e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 25/100\n",
            "1000/1000 - 5s - loss: 13.3471 - mse: 13.3471 - mae: 1.5409 - val_loss: 12.6779 - val_mse: 12.6779 - val_mae: 1.5495 - lr: 1.3807e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 26/100\n",
            "1000/1000 - 5s - loss: 13.3865 - mse: 13.3865 - mae: 1.5437 - val_loss: 12.6311 - val_mse: 12.6311 - val_mae: 1.5750 - lr: 1.3807e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 27/100\n",
            "1000/1000 - 5s - loss: 13.3189 - mse: 13.3189 - mae: 1.5448 - val_loss: 12.6284 - val_mse: 12.6284 - val_mae: 1.5402 - lr: 1.3807e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 28/100\n",
            "1000/1000 - 5s - loss: 13.2966 - mse: 13.2966 - mae: 1.5464 - val_loss: 12.5976 - val_mse: 12.5976 - val_mae: 1.5374 - lr: 1.3807e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 29/100\n",
            "1000/1000 - 5s - loss: 13.3367 - mse: 13.3367 - mae: 1.5432 - val_loss: 12.6356 - val_mse: 12.6356 - val_mae: 1.5315 - lr: 1.3807e-04 - 5s/epoch - 5ms/step\n",
            "Score for fold 1: loss of 12.635584831237793\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 5s - loss: 13.3623 - mse: 13.3623 - mae: 1.5429 - val_loss: 12.7646 - val_mse: 12.7646 - val_mae: 1.5521 - lr: 1.3807e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 5s - loss: 13.3692 - mse: 13.3692 - mae: 1.5444 - val_loss: 12.7671 - val_mse: 12.7671 - val_mae: 1.5173 - lr: 1.3807e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 5s - loss: 13.4075 - mse: 13.4075 - mae: 1.5423 - val_loss: 12.7531 - val_mse: 12.7531 - val_mae: 1.5774 - lr: 1.3807e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 5s - loss: 13.3435 - mse: 13.3435 - mae: 1.5479 - val_loss: 12.8120 - val_mse: 12.8120 - val_mae: 1.5379 - lr: 1.3807e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 5s - loss: 13.3324 - mse: 13.3324 - mae: 1.5431 - val_loss: 12.7942 - val_mse: 12.7942 - val_mae: 1.5520 - lr: 1.3807e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 5s - loss: 13.4181 - mse: 13.4181 - mae: 1.5456 - val_loss: 12.8169 - val_mse: 12.8169 - val_mae: 1.5468 - lr: 1.3807e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 5s - loss: 13.3534 - mse: 13.3534 - mae: 1.5457 - val_loss: 12.8686 - val_mse: 12.8686 - val_mae: 1.5487 - lr: 1.3807e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 5s - loss: 13.3686 - mse: 13.3686 - mae: 1.5422 - val_loss: 12.7638 - val_mse: 12.7638 - val_mae: 1.5370 - lr: 1.3807e-04 - 5s/epoch - 5ms/step\n",
            "Score for fold 2: loss of 12.763847351074219\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 5s - loss: 11.1277 - mse: 11.1277 - mae: 1.5319 - val_loss: 21.6478 - val_mse: 21.6478 - val_mae: 1.5819 - lr: 1.3807e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 5s - loss: 11.1475 - mse: 11.1475 - mae: 1.5332 - val_loss: 21.8054 - val_mse: 21.8054 - val_mae: 1.6689 - lr: 1.3807e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 5s - loss: 11.0882 - mse: 11.0882 - mae: 1.5284 - val_loss: 21.6526 - val_mse: 21.6526 - val_mae: 1.5737 - lr: 1.3807e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 5s - loss: 11.0886 - mse: 11.0886 - mae: 1.5305 - val_loss: 21.6872 - val_mse: 21.6872 - val_mae: 1.5993 - lr: 1.3807e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 5s - loss: 11.1632 - mse: 11.1632 - mae: 1.5311 - val_loss: 21.6248 - val_mse: 21.6248 - val_mae: 1.5819 - lr: 1.3807e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 5s - loss: 11.1030 - mse: 11.1030 - mae: 1.5348 - val_loss: 21.7280 - val_mse: 21.7280 - val_mae: 1.5693 - lr: 1.3807e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 5s - loss: 11.0990 - mse: 11.0990 - mae: 1.5320 - val_loss: 21.6723 - val_mse: 21.6723 - val_mae: 1.5816 - lr: 1.3807e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 5s - loss: 11.0879 - mse: 11.0879 - mae: 1.5331 - val_loss: 21.6890 - val_mse: 21.6890 - val_mae: 1.5860 - lr: 1.3807e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 5s - loss: 11.1307 - mse: 11.1307 - mae: 1.5349 - val_loss: 21.6982 - val_mse: 21.6982 - val_mae: 1.5717 - lr: 1.3807e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 5s - loss: 11.1256 - mse: 11.1256 - mae: 1.5289 - val_loss: 21.6821 - val_mse: 21.6821 - val_mae: 1.5813 - lr: 1.3807e-04 - 5s/epoch - 5ms/step\n",
            "Score for fold 3: loss of 21.682151794433594\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 5s - loss: 14.1808 - mse: 14.1808 - mae: 1.5573 - val_loss: 9.4467 - val_mse: 9.4467 - val_mae: 1.5443 - lr: 1.3807e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 5s - loss: 14.1622 - mse: 14.1622 - mae: 1.5543 - val_loss: 9.4508 - val_mse: 9.4508 - val_mae: 1.5120 - lr: 1.3807e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 5s - loss: 14.1985 - mse: 14.1985 - mae: 1.5523 - val_loss: 9.4475 - val_mse: 9.4475 - val_mae: 1.5378 - lr: 1.3807e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 5s - loss: 14.2033 - mse: 14.2033 - mae: 1.5558 - val_loss: 9.4406 - val_mse: 9.4406 - val_mae: 1.5290 - lr: 1.3807e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 5s - loss: 14.1875 - mse: 14.1875 - mae: 1.5509 - val_loss: 9.4226 - val_mse: 9.4226 - val_mae: 1.4968 - lr: 1.3807e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 5s - loss: 14.1988 - mse: 14.1988 - mae: 1.5551 - val_loss: 9.4371 - val_mse: 9.4371 - val_mae: 1.5109 - lr: 1.3807e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 5s - loss: 14.2723 - mse: 14.2723 - mae: 1.5538 - val_loss: 9.4781 - val_mse: 9.4781 - val_mae: 1.5527 - lr: 1.3807e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 5s - loss: 14.2299 - mse: 14.2299 - mae: 1.5538 - val_loss: 9.4439 - val_mse: 9.4439 - val_mae: 1.4954 - lr: 1.3807e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 5s - loss: 14.1836 - mse: 14.1836 - mae: 1.5609 - val_loss: 9.4707 - val_mse: 9.4707 - val_mae: 1.5163 - lr: 1.3807e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 6s - loss: 14.2434 - mse: 14.2434 - mae: 1.5588 - val_loss: 9.7357 - val_mse: 9.7357 - val_mae: 1.5879 - lr: 1.3807e-04 - 6s/epoch - 6ms/step\n",
            "Score for fold 4: loss of 9.735735893249512\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 6s - loss: 14.1966 - mse: 14.1966 - mae: 1.5606 - val_loss: 9.7437 - val_mse: 9.7437 - val_mae: 1.5762 - lr: 1.3807e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 6s - loss: 14.1683 - mse: 14.1683 - mae: 1.5559 - val_loss: 9.5936 - val_mse: 9.5936 - val_mae: 1.5056 - lr: 1.3807e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 5s - loss: 14.1818 - mse: 14.1818 - mae: 1.5588 - val_loss: 9.7605 - val_mse: 9.7605 - val_mae: 1.4929 - lr: 1.3807e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 6s - loss: 14.2003 - mse: 14.2003 - mae: 1.5538 - val_loss: 9.6277 - val_mse: 9.6277 - val_mae: 1.5465 - lr: 1.3807e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 5s - loss: 14.1001 - mse: 14.1001 - mae: 1.5590 - val_loss: 9.6095 - val_mse: 9.6095 - val_mae: 1.5154 - lr: 1.3807e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 5s - loss: 14.1574 - mse: 14.1574 - mae: 1.5489 - val_loss: 9.7395 - val_mse: 9.7395 - val_mae: 1.6035 - lr: 1.3807e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 5s - loss: 14.1992 - mse: 14.1992 - mae: 1.5616 - val_loss: 9.6322 - val_mse: 9.6322 - val_mae: 1.4807 - lr: 1.3807e-04 - 5s/epoch - 5ms/step\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-10-12 07:37:20,260]\u001b[0m Finished trial#3 resulted in value: 13.290000000000001. Current best value is 11.190000000000001 with parameters: {'activation': 'relu', 'num_hidden_layer': 3, 'i': 6, 'learning_rate': 0.003169692767892019}.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Score for fold 5: loss of 9.632157325744629\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 3s - loss: 14.9727 - mse: 14.9727 - mae: 1.6299 - val_loss: 11.8359 - val_mse: 11.8359 - val_mae: 1.5299 - lr: 9.3380e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 14.0394 - mse: 14.0394 - mae: 1.5704 - val_loss: 11.6484 - val_mse: 11.6484 - val_mae: 1.5056 - lr: 9.3380e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 14.0960 - mse: 14.0960 - mae: 1.5641 - val_loss: 10.2936 - val_mse: 10.2936 - val_mae: 1.5995 - lr: 9.3380e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 14.0495 - mse: 14.0495 - mae: 1.5566 - val_loss: 10.7409 - val_mse: 10.7409 - val_mae: 1.4810 - lr: 9.3380e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 14.0417 - mse: 14.0417 - mae: 1.5607 - val_loss: 10.6504 - val_mse: 10.6504 - val_mae: 1.5782 - lr: 9.3380e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 13.9066 - mse: 13.9066 - mae: 1.5560 - val_loss: 10.7095 - val_mse: 10.7095 - val_mae: 1.4957 - lr: 9.3380e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 14.1069 - mse: 14.1069 - mae: 1.5594 - val_loss: 10.2380 - val_mse: 10.2380 - val_mae: 1.6017 - lr: 9.3380e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 2s - loss: 13.9764 - mse: 13.9764 - mae: 1.5546 - val_loss: 10.1452 - val_mse: 10.1452 - val_mae: 1.5319 - lr: 9.3380e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 2s - loss: 13.9310 - mse: 13.9310 - mae: 1.5489 - val_loss: 10.4629 - val_mse: 10.4629 - val_mae: 1.5353 - lr: 9.3380e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 2s - loss: 13.9805 - mse: 13.9805 - mae: 1.5496 - val_loss: 10.3399 - val_mse: 10.3399 - val_mae: 1.5943 - lr: 9.3380e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 2s - loss: 13.9582 - mse: 13.9582 - mae: 1.5582 - val_loss: 10.6586 - val_mse: 10.6586 - val_mae: 1.4779 - lr: 9.3380e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 2s - loss: 13.9359 - mse: 13.9359 - mae: 1.5487 - val_loss: 10.2240 - val_mse: 10.2240 - val_mae: 1.6009 - lr: 9.3380e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 13/100\n",
            "1000/1000 - 2s - loss: 14.0350 - mse: 14.0350 - mae: 1.5549 - val_loss: 10.2419 - val_mse: 10.2419 - val_mae: 1.5796 - lr: 9.3380e-04 - 2s/epoch - 2ms/step\n",
            "Score for fold 1: loss of 10.241887092590332\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 13.9005 - mse: 13.9005 - mae: 1.5591 - val_loss: 10.5420 - val_mse: 10.5420 - val_mae: 1.5542 - lr: 9.3380e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 14.0325 - mse: 14.0325 - mae: 1.5600 - val_loss: 10.6396 - val_mse: 10.6396 - val_mae: 1.6114 - lr: 9.3380e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 13.9392 - mse: 13.9392 - mae: 1.5549 - val_loss: 10.5937 - val_mse: 10.5937 - val_mae: 1.5497 - lr: 9.3380e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 13.9390 - mse: 13.9390 - mae: 1.5576 - val_loss: 10.6084 - val_mse: 10.6084 - val_mae: 1.5591 - lr: 9.3380e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 13.8899 - mse: 13.8899 - mae: 1.5587 - val_loss: 10.6988 - val_mse: 10.6988 - val_mae: 1.5954 - lr: 9.3380e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 13.9384 - mse: 13.9384 - mae: 1.5570 - val_loss: 10.8649 - val_mse: 10.8649 - val_mae: 1.5493 - lr: 9.3380e-04 - 2s/epoch - 2ms/step\n",
            "Score for fold 2: loss of 10.864917755126953\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 13.9776 - mse: 13.9776 - mae: 1.5686 - val_loss: 10.3604 - val_mse: 10.3604 - val_mae: 1.4649 - lr: 9.3380e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 14.0060 - mse: 14.0060 - mae: 1.5652 - val_loss: 10.3887 - val_mse: 10.3887 - val_mae: 1.5311 - lr: 9.3380e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 13.9156 - mse: 13.9156 - mae: 1.5713 - val_loss: 10.5272 - val_mse: 10.5272 - val_mae: 1.5414 - lr: 9.3380e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 14.0718 - mse: 14.0718 - mae: 1.5706 - val_loss: 10.3595 - val_mse: 10.3595 - val_mae: 1.4665 - lr: 9.3380e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 13.9237 - mse: 13.9237 - mae: 1.5678 - val_loss: 10.5002 - val_mse: 10.5002 - val_mae: 1.5293 - lr: 9.3380e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 13.9588 - mse: 13.9588 - mae: 1.5620 - val_loss: 10.5554 - val_mse: 10.5554 - val_mae: 1.5555 - lr: 9.3380e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 13.9058 - mse: 13.9058 - mae: 1.5693 - val_loss: 10.4274 - val_mse: 10.4274 - val_mae: 1.5035 - lr: 9.3380e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 2s - loss: 13.9582 - mse: 13.9582 - mae: 1.5640 - val_loss: 10.3575 - val_mse: 10.3575 - val_mae: 1.4883 - lr: 9.3380e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 2s - loss: 14.0868 - mse: 14.0868 - mae: 1.5714 - val_loss: 10.3849 - val_mse: 10.3849 - val_mae: 1.4836 - lr: 9.3380e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 2s - loss: 14.0317 - mse: 14.0317 - mae: 1.5628 - val_loss: 10.4949 - val_mse: 10.4949 - val_mae: 1.5374 - lr: 9.3380e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 2s - loss: 14.0158 - mse: 14.0158 - mae: 1.5739 - val_loss: 10.7053 - val_mse: 10.7053 - val_mae: 1.5821 - lr: 9.3380e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 2s - loss: 13.9418 - mse: 13.9418 - mae: 1.5716 - val_loss: 10.4147 - val_mse: 10.4147 - val_mae: 1.4168 - lr: 9.3380e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 13/100\n",
            "1000/1000 - 2s - loss: 13.9995 - mse: 13.9995 - mae: 1.5679 - val_loss: 11.2177 - val_mse: 11.2177 - val_mae: 1.5985 - lr: 9.3380e-04 - 2s/epoch - 2ms/step\n",
            "Score for fold 3: loss of 11.2177095413208\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 11.3804 - mse: 11.3804 - mae: 1.5486 - val_loss: 21.7593 - val_mse: 21.7593 - val_mae: 1.5959 - lr: 9.3380e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 11.2648 - mse: 11.2648 - mae: 1.5417 - val_loss: 21.9179 - val_mse: 21.9179 - val_mae: 1.6329 - lr: 9.3380e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 11.1955 - mse: 11.1955 - mae: 1.5433 - val_loss: 21.4879 - val_mse: 21.4879 - val_mae: 1.5866 - lr: 9.3380e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 11.2943 - mse: 11.2943 - mae: 1.5488 - val_loss: 21.8188 - val_mse: 21.8188 - val_mae: 1.5815 - lr: 9.3380e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 11.2081 - mse: 11.2081 - mae: 1.5391 - val_loss: 21.6138 - val_mse: 21.6138 - val_mae: 1.5748 - lr: 9.3380e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 11.3405 - mse: 11.3405 - mae: 1.5403 - val_loss: 21.7526 - val_mse: 21.7526 - val_mae: 1.6843 - lr: 9.3380e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 11.2704 - mse: 11.2704 - mae: 1.5399 - val_loss: 21.4749 - val_mse: 21.4749 - val_mae: 1.5553 - lr: 9.3380e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 2s - loss: 11.2158 - mse: 11.2158 - mae: 1.5449 - val_loss: 21.6579 - val_mse: 21.6579 - val_mae: 1.6142 - lr: 9.3380e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 2s - loss: 11.3404 - mse: 11.3404 - mae: 1.5417 - val_loss: 21.8360 - val_mse: 21.8360 - val_mae: 1.6078 - lr: 9.3380e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 2s - loss: 11.3539 - mse: 11.3539 - mae: 1.5417 - val_loss: 21.4706 - val_mse: 21.4706 - val_mae: 1.5620 - lr: 9.3380e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 2s - loss: 11.3091 - mse: 11.3091 - mae: 1.5431 - val_loss: 21.5466 - val_mse: 21.5466 - val_mae: 1.5168 - lr: 9.3380e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 2s - loss: 11.3059 - mse: 11.3059 - mae: 1.5435 - val_loss: 21.4541 - val_mse: 21.4541 - val_mae: 1.6026 - lr: 9.3380e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 13/100\n",
            "1000/1000 - 2s - loss: 11.3002 - mse: 11.3002 - mae: 1.5426 - val_loss: 21.9322 - val_mse: 21.9322 - val_mae: 1.7079 - lr: 9.3380e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 14/100\n",
            "1000/1000 - 2s - loss: 11.2920 - mse: 11.2920 - mae: 1.5474 - val_loss: 21.6535 - val_mse: 21.6535 - val_mae: 1.6071 - lr: 9.3380e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 15/100\n",
            "1000/1000 - 2s - loss: 11.3981 - mse: 11.3981 - mae: 1.5400 - val_loss: 21.3651 - val_mse: 21.3651 - val_mae: 1.5425 - lr: 9.3380e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 16/100\n",
            "1000/1000 - 2s - loss: 11.2533 - mse: 11.2533 - mae: 1.5505 - val_loss: 21.5392 - val_mse: 21.5392 - val_mae: 1.5299 - lr: 9.3380e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 17/100\n",
            "1000/1000 - 2s - loss: 11.3170 - mse: 11.3170 - mae: 1.5413 - val_loss: 22.1454 - val_mse: 22.1454 - val_mae: 1.7683 - lr: 9.3380e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 18/100\n",
            "1000/1000 - 2s - loss: 11.4849 - mse: 11.4849 - mae: 1.5513 - val_loss: 21.6097 - val_mse: 21.6097 - val_mae: 1.6032 - lr: 9.3380e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 19/100\n",
            "1000/1000 - 2s - loss: 11.2468 - mse: 11.2468 - mae: 1.5397 - val_loss: 21.5380 - val_mse: 21.5380 - val_mae: 1.5313 - lr: 9.3380e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 20/100\n",
            "1000/1000 - 2s - loss: 11.1998 - mse: 11.1998 - mae: 1.5408 - val_loss: 21.4276 - val_mse: 21.4276 - val_mae: 1.5471 - lr: 9.3380e-04 - 2s/epoch - 2ms/step\n",
            "Score for fold 4: loss of 21.427589416503906\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 13.1679 - mse: 13.1679 - mae: 1.5509 - val_loss: 14.7309 - val_mse: 14.7309 - val_mae: 1.8770 - lr: 9.3380e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 13.2716 - mse: 13.2716 - mae: 1.5470 - val_loss: 13.3865 - val_mse: 13.3865 - val_mae: 1.5844 - lr: 9.3380e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 13.2878 - mse: 13.2878 - mae: 1.5514 - val_loss: 13.3881 - val_mse: 13.3881 - val_mae: 1.5344 - lr: 9.3380e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 13.3839 - mse: 13.3839 - mae: 1.5513 - val_loss: 13.4017 - val_mse: 13.4017 - val_mae: 1.5263 - lr: 9.3380e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 13.2563 - mse: 13.2563 - mae: 1.5538 - val_loss: 13.4647 - val_mse: 13.4647 - val_mae: 1.5136 - lr: 9.3380e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 13.3279 - mse: 13.3279 - mae: 1.5422 - val_loss: 13.5292 - val_mse: 13.5292 - val_mae: 1.5717 - lr: 9.3380e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 13.3629 - mse: 13.3629 - mae: 1.5479 - val_loss: 13.6169 - val_mse: 13.6169 - val_mae: 1.4963 - lr: 9.3380e-04 - 2s/epoch - 2ms/step\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-10-12 07:39:19,177]\u001b[0m Finished trial#4 resulted in value: 13.474. Current best value is 11.190000000000001 with parameters: {'activation': 'relu', 'num_hidden_layer': 3, 'i': 6, 'learning_rate': 0.003169692767892019}.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Score for fold 5: loss of 13.616878509521484\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 6s - loss: 13.1444 - mse: 13.1444 - mae: 1.5473 - val_loss: 12.9526 - val_mse: 12.9526 - val_mae: 1.5727 - lr: 9.8081e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 5s - loss: 12.3716 - mse: 12.3716 - mae: 1.4938 - val_loss: 12.9428 - val_mse: 12.9428 - val_mae: 1.4738 - lr: 9.8081e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 5s - loss: 12.1773 - mse: 12.1773 - mae: 1.4829 - val_loss: 12.4108 - val_mse: 12.4108 - val_mae: 1.4775 - lr: 9.8081e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 5s - loss: 12.0417 - mse: 12.0417 - mae: 1.4668 - val_loss: 12.3155 - val_mse: 12.3155 - val_mae: 1.4213 - lr: 9.8081e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 5s - loss: 11.8064 - mse: 11.8064 - mae: 1.4598 - val_loss: 12.3365 - val_mse: 12.3365 - val_mae: 1.4533 - lr: 9.8081e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 5s - loss: 11.9048 - mse: 11.9048 - mae: 1.4546 - val_loss: 12.3829 - val_mse: 12.3829 - val_mae: 1.4821 - lr: 9.8081e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 5s - loss: 11.6937 - mse: 11.6937 - mae: 1.4496 - val_loss: 12.0556 - val_mse: 12.0556 - val_mae: 1.4506 - lr: 9.8081e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 5s - loss: 11.6294 - mse: 11.6294 - mae: 1.4394 - val_loss: 12.3611 - val_mse: 12.3611 - val_mae: 1.4338 - lr: 9.8081e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 5s - loss: 11.4739 - mse: 11.4739 - mae: 1.4355 - val_loss: 12.1768 - val_mse: 12.1768 - val_mae: 1.4636 - lr: 9.8081e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 5s - loss: 11.5495 - mse: 11.5495 - mae: 1.4312 - val_loss: 12.0931 - val_mse: 12.0931 - val_mae: 1.4315 - lr: 9.8081e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 5s - loss: 11.3157 - mse: 11.3157 - mae: 1.4215 - val_loss: 12.3434 - val_mse: 12.3434 - val_mae: 1.4137 - lr: 9.8081e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 5s - loss: 11.1314 - mse: 11.1314 - mae: 1.4197 - val_loss: 11.9651 - val_mse: 11.9651 - val_mae: 1.4457 - lr: 9.8081e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 13/100\n",
            "1000/1000 - 5s - loss: 11.0604 - mse: 11.0604 - mae: 1.4197 - val_loss: 11.9693 - val_mse: 11.9693 - val_mae: 1.4710 - lr: 9.8081e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 14/100\n",
            "1000/1000 - 5s - loss: 10.8821 - mse: 10.8821 - mae: 1.4146 - val_loss: 12.0993 - val_mse: 12.0993 - val_mae: 1.5075 - lr: 9.8081e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 15/100\n",
            "1000/1000 - 5s - loss: 10.8017 - mse: 10.8017 - mae: 1.4093 - val_loss: 12.6002 - val_mse: 12.6002 - val_mae: 1.4544 - lr: 9.8081e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 16/100\n",
            "1000/1000 - 5s - loss: 10.9090 - mse: 10.9090 - mae: 1.4063 - val_loss: 12.1309 - val_mse: 12.1309 - val_mae: 1.4468 - lr: 9.8081e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 17/100\n",
            "1000/1000 - 5s - loss: 10.5105 - mse: 10.5105 - mae: 1.4010 - val_loss: 12.1239 - val_mse: 12.1239 - val_mae: 1.4409 - lr: 9.8081e-04 - 5s/epoch - 5ms/step\n",
            "Score for fold 1: loss of 12.123885154724121\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 5s - loss: 11.7383 - mse: 11.7383 - mae: 1.4171 - val_loss: 7.9450 - val_mse: 7.9450 - val_mae: 1.3735 - lr: 9.8081e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 5s - loss: 11.5563 - mse: 11.5563 - mae: 1.4043 - val_loss: 8.0516 - val_mse: 8.0516 - val_mae: 1.3868 - lr: 9.8081e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 5s - loss: 11.3558 - mse: 11.3558 - mae: 1.3932 - val_loss: 8.1190 - val_mse: 8.1190 - val_mae: 1.3881 - lr: 9.8081e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 5s - loss: 11.1746 - mse: 11.1746 - mae: 1.3843 - val_loss: 8.1728 - val_mse: 8.1728 - val_mae: 1.3753 - lr: 9.8081e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 5s - loss: 10.9994 - mse: 10.9994 - mae: 1.3764 - val_loss: 8.5230 - val_mse: 8.5230 - val_mae: 1.4222 - lr: 9.8081e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 5s - loss: 10.9627 - mse: 10.9627 - mae: 1.3652 - val_loss: 8.7037 - val_mse: 8.7037 - val_mae: 1.3705 - lr: 9.8081e-04 - 5s/epoch - 5ms/step\n",
            "Score for fold 2: loss of 8.703703880310059\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 5s - loss: 9.4118 - mse: 9.4118 - mae: 1.3717 - val_loss: 15.8037 - val_mse: 15.8037 - val_mae: 1.3352 - lr: 9.8081e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 5s - loss: 8.8803 - mse: 8.8803 - mae: 1.3529 - val_loss: 15.2421 - val_mse: 15.2421 - val_mae: 1.4016 - lr: 9.8081e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 5s - loss: 8.9555 - mse: 8.9555 - mae: 1.3444 - val_loss: 18.5003 - val_mse: 18.5003 - val_mae: 1.4006 - lr: 9.8081e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 5s - loss: 8.6551 - mse: 8.6551 - mae: 1.3341 - val_loss: 16.4820 - val_mse: 16.4820 - val_mae: 1.3795 - lr: 9.8081e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 5s - loss: 8.3238 - mse: 8.3238 - mae: 1.3203 - val_loss: 15.7231 - val_mse: 15.7231 - val_mae: 1.4276 - lr: 9.8081e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 6s - loss: 8.2750 - mse: 8.2750 - mae: 1.3047 - val_loss: 15.9165 - val_mse: 15.9165 - val_mae: 1.4018 - lr: 9.8081e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 6s - loss: 8.2334 - mse: 8.2334 - mae: 1.2991 - val_loss: 16.8560 - val_mse: 16.8560 - val_mae: 1.4112 - lr: 9.8081e-04 - 6s/epoch - 6ms/step\n",
            "Score for fold 3: loss of 16.855998992919922\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 6s - loss: 10.1387 - mse: 10.1387 - mae: 1.3246 - val_loss: 9.9995 - val_mse: 9.9995 - val_mae: 1.2579 - lr: 9.8081e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 6s - loss: 9.5763 - mse: 9.5763 - mae: 1.3046 - val_loss: 8.6097 - val_mse: 8.6097 - val_mae: 1.3601 - lr: 9.8081e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 6s - loss: 9.5771 - mse: 9.5771 - mae: 1.2895 - val_loss: 8.6555 - val_mse: 8.6555 - val_mae: 1.2854 - lr: 9.8081e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 6s - loss: 9.1435 - mse: 9.1435 - mae: 1.2755 - val_loss: 8.4376 - val_mse: 8.4376 - val_mae: 1.2864 - lr: 9.8081e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 6s - loss: 9.1202 - mse: 9.1202 - mae: 1.2639 - val_loss: 9.0117 - val_mse: 9.0117 - val_mae: 1.3000 - lr: 9.8081e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 6s - loss: 8.8811 - mse: 8.8811 - mae: 1.2467 - val_loss: 9.0248 - val_mse: 9.0248 - val_mae: 1.3053 - lr: 9.8081e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 6s - loss: 8.7827 - mse: 8.7827 - mae: 1.2346 - val_loss: 9.2282 - val_mse: 9.2282 - val_mae: 1.3138 - lr: 9.8081e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 6s - loss: 8.4847 - mse: 8.4847 - mae: 1.2207 - val_loss: 8.9178 - val_mse: 8.9178 - val_mae: 1.3517 - lr: 9.8081e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 6s - loss: 8.4400 - mse: 8.4400 - mae: 1.2114 - val_loss: 9.7503 - val_mse: 9.7503 - val_mae: 1.3572 - lr: 9.8081e-04 - 6s/epoch - 6ms/step\n",
            "Score for fold 4: loss of 9.750340461730957\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 5s - loss: 9.2077 - mse: 9.2077 - mae: 1.2535 - val_loss: 6.4109 - val_mse: 6.4109 - val_mae: 1.2388 - lr: 9.8081e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 5s - loss: 8.6265 - mse: 8.6265 - mae: 1.2297 - val_loss: 6.3868 - val_mse: 6.3868 - val_mae: 1.1756 - lr: 9.8081e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 5s - loss: 8.5277 - mse: 8.5277 - mae: 1.2114 - val_loss: 6.1588 - val_mse: 6.1588 - val_mae: 1.2496 - lr: 9.8081e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 5s - loss: 8.2838 - mse: 8.2838 - mae: 1.1947 - val_loss: 6.4766 - val_mse: 6.4766 - val_mae: 1.2177 - lr: 9.8081e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 5s - loss: 8.3941 - mse: 8.3941 - mae: 1.1829 - val_loss: 6.5068 - val_mse: 6.5068 - val_mae: 1.2544 - lr: 9.8081e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 5s - loss: 7.8700 - mse: 7.8700 - mae: 1.1654 - val_loss: 6.5391 - val_mse: 6.5391 - val_mae: 1.2597 - lr: 9.8081e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 5s - loss: 7.9732 - mse: 7.9732 - mae: 1.1530 - val_loss: 6.8195 - val_mse: 6.8195 - val_mae: 1.2539 - lr: 9.8081e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 5s - loss: 7.5058 - mse: 7.5058 - mae: 1.1362 - val_loss: 7.1124 - val_mse: 7.1124 - val_mae: 1.2568 - lr: 9.8081e-04 - 5s/epoch - 5ms/step\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-10-12 07:43:36,003]\u001b[0m Finished trial#5 resulted in value: 10.908. Current best value is 10.908 with parameters: {'activation': 'relu', 'num_hidden_layer': 3, 'i': 10, 'learning_rate': 0.000980808748427148}.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Score for fold 5: loss of 7.112378120422363\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 3s - loss: 14.5126 - mse: 14.5126 - mae: 1.5702 - val_loss: 12.4376 - val_mse: 12.4376 - val_mae: 1.5432 - lr: 1.1601e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 3s - loss: 12.8598 - mse: 12.8598 - mae: 1.5095 - val_loss: 11.9287 - val_mse: 11.9287 - val_mae: 1.4853 - lr: 1.1601e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 3s - loss: 12.5959 - mse: 12.5959 - mae: 1.4933 - val_loss: 11.2158 - val_mse: 11.2158 - val_mae: 1.4977 - lr: 1.1601e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 3s - loss: 12.4253 - mse: 12.4253 - mae: 1.4770 - val_loss: 10.9983 - val_mse: 10.9983 - val_mae: 1.4645 - lr: 1.1601e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 3s - loss: 12.3179 - mse: 12.3179 - mae: 1.4674 - val_loss: 10.8208 - val_mse: 10.8208 - val_mae: 1.4863 - lr: 1.1601e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 3s - loss: 12.3405 - mse: 12.3405 - mae: 1.4595 - val_loss: 10.8400 - val_mse: 10.8400 - val_mae: 1.4395 - lr: 1.1601e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 3s - loss: 12.1798 - mse: 12.1798 - mae: 1.4522 - val_loss: 10.7676 - val_mse: 10.7676 - val_mae: 1.4851 - lr: 1.1601e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 3s - loss: 12.0393 - mse: 12.0393 - mae: 1.4467 - val_loss: 10.9725 - val_mse: 10.9725 - val_mae: 1.4447 - lr: 1.1601e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 3s - loss: 12.0750 - mse: 12.0750 - mae: 1.4386 - val_loss: 10.6991 - val_mse: 10.6991 - val_mae: 1.5010 - lr: 1.1601e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 3s - loss: 11.9204 - mse: 11.9204 - mae: 1.4367 - val_loss: 10.9533 - val_mse: 10.9533 - val_mae: 1.4355 - lr: 1.1601e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 3s - loss: 11.8660 - mse: 11.8660 - mae: 1.4331 - val_loss: 10.8568 - val_mse: 10.8568 - val_mae: 1.4236 - lr: 1.1601e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 3s - loss: 11.8488 - mse: 11.8488 - mae: 1.4274 - val_loss: 10.7546 - val_mse: 10.7546 - val_mae: 1.4565 - lr: 1.1601e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 13/100\n",
            "1000/1000 - 3s - loss: 11.7564 - mse: 11.7564 - mae: 1.4248 - val_loss: 10.6611 - val_mse: 10.6611 - val_mae: 1.4906 - lr: 1.1601e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 14/100\n",
            "1000/1000 - 3s - loss: 11.6376 - mse: 11.6376 - mae: 1.4228 - val_loss: 11.2517 - val_mse: 11.2517 - val_mae: 1.4751 - lr: 1.1601e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 15/100\n",
            "1000/1000 - 3s - loss: 11.5924 - mse: 11.5924 - mae: 1.4147 - val_loss: 10.9509 - val_mse: 10.9509 - val_mae: 1.5518 - lr: 1.1601e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 16/100\n",
            "1000/1000 - 3s - loss: 11.5790 - mse: 11.5790 - mae: 1.4170 - val_loss: 10.6141 - val_mse: 10.6141 - val_mae: 1.4348 - lr: 1.1601e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 17/100\n",
            "1000/1000 - 3s - loss: 11.4398 - mse: 11.4398 - mae: 1.4113 - val_loss: 10.9123 - val_mse: 10.9123 - val_mae: 1.5053 - lr: 1.1601e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 18/100\n",
            "1000/1000 - 3s - loss: 11.3541 - mse: 11.3541 - mae: 1.4019 - val_loss: 10.8145 - val_mse: 10.8145 - val_mae: 1.4728 - lr: 1.1601e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 19/100\n",
            "1000/1000 - 3s - loss: 11.3213 - mse: 11.3213 - mae: 1.3997 - val_loss: 10.9780 - val_mse: 10.9780 - val_mae: 1.4851 - lr: 1.1601e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 20/100\n",
            "1000/1000 - 3s - loss: 11.2535 - mse: 11.2535 - mae: 1.3985 - val_loss: 10.9918 - val_mse: 10.9918 - val_mae: 1.4779 - lr: 1.1601e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 21/100\n",
            "1000/1000 - 3s - loss: 11.1223 - mse: 11.1223 - mae: 1.3926 - val_loss: 10.6684 - val_mse: 10.6684 - val_mae: 1.4352 - lr: 1.1601e-04 - 3s/epoch - 3ms/step\n",
            "Score for fold 1: loss of 10.66842269897461\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 3s - loss: 12.0117 - mse: 12.0117 - mae: 1.4048 - val_loss: 7.1083 - val_mse: 7.1083 - val_mae: 1.4155 - lr: 1.1601e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 3s - loss: 11.9306 - mse: 11.9306 - mae: 1.4008 - val_loss: 7.1483 - val_mse: 7.1483 - val_mae: 1.3762 - lr: 1.1601e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 3s - loss: 11.8630 - mse: 11.8630 - mae: 1.3952 - val_loss: 7.3711 - val_mse: 7.3711 - val_mae: 1.4136 - lr: 1.1601e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 3s - loss: 11.8025 - mse: 11.8025 - mae: 1.3928 - val_loss: 7.5457 - val_mse: 7.5457 - val_mae: 1.3441 - lr: 1.1601e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 3s - loss: 11.6186 - mse: 11.6186 - mae: 1.3898 - val_loss: 7.3199 - val_mse: 7.3199 - val_mae: 1.4200 - lr: 1.1601e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 3s - loss: 11.6551 - mse: 11.6551 - mae: 1.3857 - val_loss: 7.3283 - val_mse: 7.3283 - val_mae: 1.4504 - lr: 1.1601e-04 - 3s/epoch - 3ms/step\n",
            "Score for fold 2: loss of 7.328291416168213\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 3s - loss: 10.8524 - mse: 10.8524 - mae: 1.3855 - val_loss: 9.8542 - val_mse: 9.8542 - val_mae: 1.3937 - lr: 1.1601e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 3s - loss: 10.7985 - mse: 10.7985 - mae: 1.3790 - val_loss: 10.1348 - val_mse: 10.1348 - val_mae: 1.4117 - lr: 1.1601e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 3s - loss: 10.5979 - mse: 10.5979 - mae: 1.3720 - val_loss: 10.2445 - val_mse: 10.2445 - val_mae: 1.4008 - lr: 1.1601e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 3s - loss: 10.6368 - mse: 10.6368 - mae: 1.3695 - val_loss: 10.2127 - val_mse: 10.2127 - val_mae: 1.3693 - lr: 1.1601e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 3s - loss: 10.4370 - mse: 10.4370 - mae: 1.3617 - val_loss: 10.2767 - val_mse: 10.2767 - val_mae: 1.4328 - lr: 1.1601e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 3s - loss: 10.4508 - mse: 10.4508 - mae: 1.3572 - val_loss: 10.5588 - val_mse: 10.5588 - val_mae: 1.3983 - lr: 1.1601e-04 - 3s/epoch - 3ms/step\n",
            "Score for fold 3: loss of 10.558769226074219\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 3s - loss: 10.8712 - mse: 10.8712 - mae: 1.3784 - val_loss: 8.6957 - val_mse: 8.6957 - val_mae: 1.3098 - lr: 1.1601e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 3s - loss: 10.6860 - mse: 10.6860 - mae: 1.3696 - val_loss: 8.6268 - val_mse: 8.6268 - val_mae: 1.3923 - lr: 1.1601e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 3s - loss: 10.5715 - mse: 10.5715 - mae: 1.3589 - val_loss: 8.8904 - val_mse: 8.8904 - val_mae: 1.4094 - lr: 1.1601e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 3s - loss: 10.4819 - mse: 10.4819 - mae: 1.3561 - val_loss: 9.2612 - val_mse: 9.2612 - val_mae: 1.3512 - lr: 1.1601e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 3s - loss: 10.3434 - mse: 10.3434 - mae: 1.3505 - val_loss: 8.5921 - val_mse: 8.5921 - val_mae: 1.4127 - lr: 1.1601e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 3s - loss: 10.2357 - mse: 10.2357 - mae: 1.3397 - val_loss: 9.1942 - val_mse: 9.1942 - val_mae: 1.3700 - lr: 1.1601e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 3s - loss: 10.1609 - mse: 10.1609 - mae: 1.3354 - val_loss: 8.9966 - val_mse: 8.9966 - val_mae: 1.3601 - lr: 1.1601e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 3s - loss: 10.0276 - mse: 10.0276 - mae: 1.3313 - val_loss: 9.5656 - val_mse: 9.5656 - val_mae: 1.3582 - lr: 1.1601e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 3s - loss: 9.9071 - mse: 9.9071 - mae: 1.3236 - val_loss: 9.1233 - val_mse: 9.1233 - val_mae: 1.4157 - lr: 1.1601e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 3s - loss: 9.6751 - mse: 9.6751 - mae: 1.3147 - val_loss: 8.7499 - val_mse: 8.7499 - val_mae: 1.3724 - lr: 1.1601e-04 - 3s/epoch - 3ms/step\n",
            "Score for fold 4: loss of 8.749885559082031\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 3s - loss: 8.0139 - mse: 8.0139 - mae: 1.3300 - val_loss: 16.3633 - val_mse: 16.3633 - val_mae: 1.2659 - lr: 1.1601e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 3s - loss: 8.0556 - mse: 8.0556 - mae: 1.3201 - val_loss: 15.9595 - val_mse: 15.9595 - val_mae: 1.3564 - lr: 1.1601e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 3s - loss: 7.7346 - mse: 7.7346 - mae: 1.3094 - val_loss: 16.0954 - val_mse: 16.0954 - val_mae: 1.3480 - lr: 1.1601e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 3s - loss: 7.7471 - mse: 7.7471 - mae: 1.3076 - val_loss: 16.4654 - val_mse: 16.4654 - val_mae: 1.3278 - lr: 1.1601e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 3s - loss: 7.5684 - mse: 7.5684 - mae: 1.2943 - val_loss: 16.6784 - val_mse: 16.6784 - val_mae: 1.3391 - lr: 1.1601e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 3s - loss: 7.4469 - mse: 7.4469 - mae: 1.2907 - val_loss: 17.0732 - val_mse: 17.0732 - val_mae: 1.3494 - lr: 1.1601e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 3s - loss: 7.3151 - mse: 7.3151 - mae: 1.2812 - val_loss: 16.6327 - val_mse: 16.6327 - val_mae: 1.4020 - lr: 1.1601e-04 - 3s/epoch - 3ms/step\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-10-12 07:45:52,084]\u001b[0m Finished trial#6 resulted in value: 10.788. Current best value is 10.788 with parameters: {'activation': 'relu', 'num_hidden_layer': 4, 'i': 8, 'learning_rate': 0.0001160126182319962}.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Score for fold 5: loss of 16.632678985595703\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 13.3543 - mse: 13.3543 - mae: 1.5898 - val_loss: 19.6020 - val_mse: 19.6020 - val_mae: 1.5446 - lr: 0.0024 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 1s - loss: 12.4899 - mse: 12.4899 - mae: 1.5268 - val_loss: 19.0049 - val_mse: 19.0049 - val_mae: 1.5429 - lr: 0.0024 - 1s/epoch - 1ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 1s - loss: 12.2732 - mse: 12.2732 - mae: 1.5121 - val_loss: 18.9293 - val_mse: 18.9293 - val_mae: 1.5596 - lr: 0.0024 - 1s/epoch - 1ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 1s - loss: 12.0200 - mse: 12.0200 - mae: 1.4993 - val_loss: 18.9244 - val_mse: 18.9244 - val_mae: 1.5115 - lr: 0.0024 - 1s/epoch - 1ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 1s - loss: 11.8526 - mse: 11.8526 - mae: 1.4879 - val_loss: 18.5121 - val_mse: 18.5121 - val_mae: 1.6210 - lr: 0.0024 - 1s/epoch - 1ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 1s - loss: 11.7587 - mse: 11.7587 - mae: 1.4810 - val_loss: 18.5949 - val_mse: 18.5949 - val_mae: 1.5185 - lr: 0.0024 - 1s/epoch - 1ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 1s - loss: 11.5692 - mse: 11.5692 - mae: 1.4712 - val_loss: 18.4925 - val_mse: 18.4925 - val_mae: 1.5754 - lr: 0.0024 - 1s/epoch - 1ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 1s - loss: 11.5587 - mse: 11.5587 - mae: 1.4709 - val_loss: 18.5988 - val_mse: 18.5988 - val_mae: 1.5233 - lr: 0.0024 - 1s/epoch - 1ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 1s - loss: 11.4299 - mse: 11.4299 - mae: 1.4630 - val_loss: 18.5854 - val_mse: 18.5854 - val_mae: 1.5105 - lr: 0.0024 - 1s/epoch - 1ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 1s - loss: 11.3763 - mse: 11.3763 - mae: 1.4603 - val_loss: 18.2989 - val_mse: 18.2989 - val_mae: 1.6002 - lr: 0.0024 - 1s/epoch - 1ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 1s - loss: 11.2934 - mse: 11.2934 - mae: 1.4569 - val_loss: 18.1195 - val_mse: 18.1195 - val_mae: 1.5210 - lr: 0.0024 - 1s/epoch - 1ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 1s - loss: 11.2374 - mse: 11.2374 - mae: 1.4588 - val_loss: 18.2789 - val_mse: 18.2789 - val_mae: 1.6294 - lr: 0.0024 - 1s/epoch - 1ms/step\n",
            "Epoch 13/100\n",
            "1000/1000 - 1s - loss: 11.1309 - mse: 11.1309 - mae: 1.4530 - val_loss: 18.4208 - val_mse: 18.4208 - val_mae: 1.4444 - lr: 0.0024 - 1s/epoch - 1ms/step\n",
            "Epoch 14/100\n",
            "1000/1000 - 1s - loss: 11.1220 - mse: 11.1220 - mae: 1.4532 - val_loss: 18.3079 - val_mse: 18.3079 - val_mae: 1.5405 - lr: 0.0024 - 1s/epoch - 1ms/step\n",
            "Epoch 15/100\n",
            "1000/1000 - 1s - loss: 11.1140 - mse: 11.1140 - mae: 1.4503 - val_loss: 18.3317 - val_mse: 18.3317 - val_mae: 1.5638 - lr: 0.0024 - 1s/epoch - 1ms/step\n",
            "Epoch 16/100\n",
            "1000/1000 - 1s - loss: 10.9623 - mse: 10.9623 - mae: 1.4476 - val_loss: 18.3276 - val_mse: 18.3276 - val_mae: 1.4447 - lr: 0.0024 - 1s/epoch - 1ms/step\n",
            "Score for fold 1: loss of 18.327577590942383\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 1s - loss: 10.7695 - mse: 10.7695 - mae: 1.4226 - val_loss: 17.4558 - val_mse: 17.4558 - val_mae: 1.4817 - lr: 0.0010 - 1s/epoch - 1ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 1s - loss: 10.5624 - mse: 10.5624 - mae: 1.4147 - val_loss: 17.5970 - val_mse: 17.5970 - val_mae: 1.4405 - lr: 0.0010 - 1s/epoch - 1ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 1s - loss: 10.5319 - mse: 10.5319 - mae: 1.4088 - val_loss: 17.4697 - val_mse: 17.4697 - val_mae: 1.4289 - lr: 0.0010 - 1s/epoch - 1ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 1s - loss: 10.4216 - mse: 10.4216 - mae: 1.4047 - val_loss: 17.4790 - val_mse: 17.4790 - val_mae: 1.4215 - lr: 0.0010 - 1s/epoch - 1ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 1s - loss: 10.3330 - mse: 10.3330 - mae: 1.3989 - val_loss: 17.6351 - val_mse: 17.6351 - val_mae: 1.4116 - lr: 0.0010 - 1s/epoch - 1ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 1s - loss: 10.3426 - mse: 10.3426 - mae: 1.3979 - val_loss: 17.3280 - val_mse: 17.3280 - val_mae: 1.4470 - lr: 0.0010 - 1s/epoch - 1ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 1s - loss: 10.2326 - mse: 10.2326 - mae: 1.3952 - val_loss: 17.5102 - val_mse: 17.5102 - val_mae: 1.4717 - lr: 0.0010 - 1s/epoch - 1ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 1s - loss: 10.2075 - mse: 10.2075 - mae: 1.3922 - val_loss: 17.6469 - val_mse: 17.6469 - val_mae: 1.4866 - lr: 0.0010 - 1s/epoch - 1ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 1s - loss: 10.1419 - mse: 10.1419 - mae: 1.3910 - val_loss: 17.7068 - val_mse: 17.7068 - val_mae: 1.4772 - lr: 0.0010 - 1s/epoch - 1ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 1s - loss: 10.1311 - mse: 10.1311 - mae: 1.3851 - val_loss: 17.7176 - val_mse: 17.7176 - val_mae: 1.5506 - lr: 0.0010 - 1s/epoch - 1ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 1s - loss: 10.1089 - mse: 10.1089 - mae: 1.3876 - val_loss: 17.5834 - val_mse: 17.5834 - val_mae: 1.4677 - lr: 0.0010 - 1s/epoch - 1ms/step\n",
            "Score for fold 2: loss of 17.583425521850586\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 12.6748 - mse: 12.6748 - mae: 1.4182 - val_loss: 7.4839 - val_mse: 7.4839 - val_mae: 1.3983 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 1s - loss: 12.6106 - mse: 12.6106 - mae: 1.4146 - val_loss: 7.5206 - val_mse: 7.5206 - val_mae: 1.3200 - lr: 0.0010 - 1s/epoch - 1ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 1s - loss: 12.4808 - mse: 12.4808 - mae: 1.4101 - val_loss: 7.6058 - val_mse: 7.6058 - val_mae: 1.3523 - lr: 0.0010 - 1s/epoch - 1ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 1s - loss: 12.4428 - mse: 12.4428 - mae: 1.4083 - val_loss: 7.4726 - val_mse: 7.4726 - val_mae: 1.4546 - lr: 0.0010 - 1s/epoch - 1ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 1s - loss: 12.3440 - mse: 12.3440 - mae: 1.4058 - val_loss: 7.4774 - val_mse: 7.4774 - val_mae: 1.4214 - lr: 0.0010 - 1s/epoch - 1ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 1s - loss: 12.3018 - mse: 12.3018 - mae: 1.4011 - val_loss: 7.6914 - val_mse: 7.6914 - val_mae: 1.3424 - lr: 0.0010 - 1s/epoch - 1ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 1s - loss: 12.2269 - mse: 12.2269 - mae: 1.4000 - val_loss: 7.4838 - val_mse: 7.4838 - val_mae: 1.4094 - lr: 0.0010 - 1s/epoch - 1ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 1s - loss: 12.1030 - mse: 12.1030 - mae: 1.3975 - val_loss: 7.5285 - val_mse: 7.5285 - val_mae: 1.3745 - lr: 0.0010 - 1s/epoch - 1ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 1s - loss: 12.1046 - mse: 12.1046 - mae: 1.3900 - val_loss: 7.7939 - val_mse: 7.7939 - val_mae: 1.3843 - lr: 0.0010 - 1s/epoch - 1ms/step\n",
            "Score for fold 3: loss of 7.7939133644104\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 1s - loss: 12.1719 - mse: 12.1719 - mae: 1.4069 - val_loss: 7.3116 - val_mse: 7.3116 - val_mae: 1.3320 - lr: 0.0010 - 1s/epoch - 1ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 1s - loss: 12.1993 - mse: 12.1993 - mae: 1.4029 - val_loss: 7.2322 - val_mse: 7.2322 - val_mae: 1.3550 - lr: 0.0010 - 1s/epoch - 1ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 1s - loss: 12.0543 - mse: 12.0543 - mae: 1.3997 - val_loss: 7.4007 - val_mse: 7.4007 - val_mae: 1.3674 - lr: 0.0010 - 1s/epoch - 1ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 1s - loss: 11.9976 - mse: 11.9976 - mae: 1.4003 - val_loss: 7.5686 - val_mse: 7.5686 - val_mae: 1.3232 - lr: 0.0010 - 1s/epoch - 1ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 1s - loss: 11.9564 - mse: 11.9564 - mae: 1.3970 - val_loss: 7.5039 - val_mse: 7.5039 - val_mae: 1.3444 - lr: 0.0010 - 1s/epoch - 1ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 1s - loss: 11.8983 - mse: 11.8983 - mae: 1.3937 - val_loss: 7.3923 - val_mse: 7.3923 - val_mae: 1.3683 - lr: 0.0010 - 1s/epoch - 1ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 11.8652 - mse: 11.8652 - mae: 1.3914 - val_loss: 7.6706 - val_mse: 7.6706 - val_mae: 1.3505 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Score for fold 4: loss of 7.670568466186523\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 11.7218 - mse: 11.7218 - mae: 1.4007 - val_loss: 8.1153 - val_mse: 8.1153 - val_mae: 1.2970 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 1s - loss: 11.6105 - mse: 11.6105 - mae: 1.3968 - val_loss: 8.0755 - val_mse: 8.0755 - val_mae: 1.3143 - lr: 0.0010 - 1s/epoch - 1ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 1s - loss: 11.5629 - mse: 11.5629 - mae: 1.3879 - val_loss: 8.1687 - val_mse: 8.1687 - val_mae: 1.3731 - lr: 0.0010 - 1s/epoch - 1ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 1s - loss: 11.4801 - mse: 11.4801 - mae: 1.3890 - val_loss: 8.1896 - val_mse: 8.1896 - val_mae: 1.4165 - lr: 0.0010 - 1s/epoch - 1ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 1s - loss: 11.5543 - mse: 11.5543 - mae: 1.3883 - val_loss: 8.4160 - val_mse: 8.4160 - val_mae: 1.2918 - lr: 0.0010 - 1s/epoch - 1ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 1s - loss: 11.3739 - mse: 11.3739 - mae: 1.3784 - val_loss: 8.3974 - val_mse: 8.3974 - val_mae: 1.3470 - lr: 0.0010 - 1s/epoch - 1ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 1s - loss: 11.3509 - mse: 11.3509 - mae: 1.3841 - val_loss: 8.5717 - val_mse: 8.5717 - val_mae: 1.3696 - lr: 0.0010 - 1s/epoch - 1ms/step\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-10-12 07:47:10,108]\u001b[0m Finished trial#7 resulted in value: 11.988. Current best value is 10.788 with parameters: {'activation': 'relu', 'num_hidden_layer': 4, 'i': 8, 'learning_rate': 0.0001160126182319962}.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Score for fold 5: loss of 8.571711540222168\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 5s - loss: 16.0234 - mse: 16.0234 - mae: 1.6770 - val_loss: 9.4073 - val_mse: 9.4073 - val_mae: 1.5191 - lr: 0.0014 - 5s/epoch - 5ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 4s - loss: 14.8764 - mse: 14.8764 - mae: 1.6118 - val_loss: 9.7430 - val_mse: 9.7430 - val_mae: 1.6229 - lr: 0.0014 - 4s/epoch - 4ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 4s - loss: 14.6214 - mse: 14.6214 - mae: 1.5868 - val_loss: 9.1127 - val_mse: 9.1127 - val_mae: 1.4756 - lr: 0.0014 - 4s/epoch - 4ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 4s - loss: 14.6043 - mse: 14.6043 - mae: 1.5948 - val_loss: 9.1891 - val_mse: 9.1891 - val_mae: 1.4519 - lr: 0.0014 - 4s/epoch - 4ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 4s - loss: 14.4419 - mse: 14.4419 - mae: 1.5817 - val_loss: 9.3237 - val_mse: 9.3237 - val_mae: 1.4521 - lr: 0.0014 - 4s/epoch - 4ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 4s - loss: 14.7196 - mse: 14.7196 - mae: 1.5875 - val_loss: 9.0965 - val_mse: 9.0965 - val_mae: 1.4931 - lr: 0.0014 - 4s/epoch - 4ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 4s - loss: 14.4775 - mse: 14.4775 - mae: 1.5839 - val_loss: 9.0670 - val_mse: 9.0670 - val_mae: 1.5785 - lr: 0.0014 - 4s/epoch - 4ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 4s - loss: 14.7734 - mse: 14.7734 - mae: 1.5862 - val_loss: 9.1195 - val_mse: 9.1195 - val_mae: 1.4506 - lr: 0.0014 - 4s/epoch - 4ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 4s - loss: 14.4521 - mse: 14.4521 - mae: 1.5765 - val_loss: 9.3196 - val_mse: 9.3196 - val_mae: 1.4656 - lr: 0.0014 - 4s/epoch - 4ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 4s - loss: 14.6265 - mse: 14.6265 - mae: 1.5938 - val_loss: 9.7379 - val_mse: 9.7379 - val_mae: 1.6256 - lr: 0.0014 - 4s/epoch - 4ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 5s - loss: 14.5213 - mse: 14.5213 - mae: 1.5834 - val_loss: 9.4248 - val_mse: 9.4248 - val_mae: 1.6187 - lr: 0.0014 - 5s/epoch - 5ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 5s - loss: 14.5138 - mse: 14.5138 - mae: 1.5787 - val_loss: 9.1165 - val_mse: 9.1165 - val_mae: 1.5431 - lr: 0.0014 - 5s/epoch - 5ms/step\n",
            "Score for fold 1: loss of 9.116470336914062\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 5s - loss: 13.7716 - mse: 13.7716 - mae: 1.5568 - val_loss: 11.5894 - val_mse: 11.5894 - val_mae: 1.5160 - lr: 0.0010 - 5s/epoch - 5ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 5s - loss: 13.9028 - mse: 13.9028 - mae: 1.5597 - val_loss: 11.4866 - val_mse: 11.4866 - val_mae: 1.5580 - lr: 0.0010 - 5s/epoch - 5ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 5s - loss: 13.8584 - mse: 13.8584 - mae: 1.5588 - val_loss: 13.1733 - val_mse: 13.1733 - val_mae: 1.4988 - lr: 0.0010 - 5s/epoch - 5ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 5s - loss: 13.8357 - mse: 13.8357 - mae: 1.5690 - val_loss: 11.8524 - val_mse: 11.8524 - val_mae: 1.4998 - lr: 0.0010 - 5s/epoch - 5ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 5s - loss: 13.7786 - mse: 13.7786 - mae: 1.5627 - val_loss: 11.2703 - val_mse: 11.2703 - val_mae: 1.5564 - lr: 0.0010 - 5s/epoch - 5ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 5s - loss: 13.8781 - mse: 13.8781 - mae: 1.5599 - val_loss: 11.4321 - val_mse: 11.4321 - val_mae: 1.5513 - lr: 0.0010 - 5s/epoch - 5ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 5s - loss: 13.8356 - mse: 13.8356 - mae: 1.5592 - val_loss: 11.3796 - val_mse: 11.3796 - val_mae: 1.6118 - lr: 0.0010 - 5s/epoch - 5ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 5s - loss: 14.0067 - mse: 14.0067 - mae: 1.5656 - val_loss: 11.2976 - val_mse: 11.2976 - val_mae: 1.5741 - lr: 0.0010 - 5s/epoch - 5ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 5s - loss: 13.8452 - mse: 13.8452 - mae: 1.5596 - val_loss: 11.3298 - val_mse: 11.3298 - val_mae: 1.6169 - lr: 0.0010 - 5s/epoch - 5ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 5s - loss: 14.1421 - mse: 14.1421 - mae: 1.5686 - val_loss: 11.3714 - val_mse: 11.3714 - val_mae: 1.5143 - lr: 0.0010 - 5s/epoch - 5ms/step\n",
            "Score for fold 2: loss of 11.371397018432617\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 4s - loss: 12.7739 - mse: 12.7739 - mae: 1.5561 - val_loss: 15.8997 - val_mse: 15.8997 - val_mae: 1.4916 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 4s - loss: 12.7689 - mse: 12.7689 - mae: 1.5560 - val_loss: 17.1620 - val_mse: 17.1620 - val_mae: 1.9188 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 5s - loss: 12.8172 - mse: 12.8172 - mae: 1.5581 - val_loss: 15.8709 - val_mse: 15.8709 - val_mae: 1.5022 - lr: 0.0010 - 5s/epoch - 5ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 4s - loss: 12.9153 - mse: 12.9153 - mae: 1.5600 - val_loss: 16.0106 - val_mse: 16.0106 - val_mae: 1.5476 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 4s - loss: 12.7029 - mse: 12.7029 - mae: 1.5614 - val_loss: 15.9835 - val_mse: 15.9835 - val_mae: 1.5412 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 4s - loss: 12.8001 - mse: 12.8001 - mae: 1.5544 - val_loss: 15.8810 - val_mse: 15.8810 - val_mae: 1.5820 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 5s - loss: 12.8602 - mse: 12.8602 - mae: 1.5506 - val_loss: 16.2427 - val_mse: 16.2427 - val_mae: 1.6871 - lr: 0.0010 - 5s/epoch - 5ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 5s - loss: 12.8162 - mse: 12.8162 - mae: 1.5528 - val_loss: 15.6116 - val_mse: 15.6116 - val_mae: 1.5597 - lr: 0.0010 - 5s/epoch - 5ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 5s - loss: 12.8245 - mse: 12.8245 - mae: 1.5590 - val_loss: 15.7689 - val_mse: 15.7689 - val_mae: 1.6278 - lr: 0.0010 - 5s/epoch - 5ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 5s - loss: 12.9825 - mse: 12.9825 - mae: 1.5591 - val_loss: 15.7296 - val_mse: 15.7296 - val_mae: 1.5434 - lr: 0.0010 - 5s/epoch - 5ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 4s - loss: 12.7665 - mse: 12.7665 - mae: 1.5533 - val_loss: 16.1740 - val_mse: 16.1740 - val_mae: 1.4846 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 4s - loss: 12.7943 - mse: 12.7943 - mae: 1.5532 - val_loss: 15.6827 - val_mse: 15.6827 - val_mae: 1.5391 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 13/100\n",
            "1000/1000 - 4s - loss: 12.7273 - mse: 12.7273 - mae: 1.5560 - val_loss: 16.9772 - val_mse: 16.9772 - val_mae: 1.6706 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Score for fold 3: loss of 16.97724151611328\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 4s - loss: 13.7913 - mse: 13.7913 - mae: 1.5534 - val_loss: 12.0793 - val_mse: 12.0793 - val_mae: 1.6212 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 5s - loss: 13.6772 - mse: 13.6772 - mae: 1.5539 - val_loss: 13.0672 - val_mse: 13.0672 - val_mae: 1.5462 - lr: 0.0010 - 5s/epoch - 5ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 5s - loss: 13.7158 - mse: 13.7158 - mae: 1.5543 - val_loss: 11.9415 - val_mse: 11.9415 - val_mae: 1.6039 - lr: 0.0010 - 5s/epoch - 5ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 5s - loss: 13.7105 - mse: 13.7105 - mae: 1.5518 - val_loss: 11.9266 - val_mse: 11.9266 - val_mae: 1.6123 - lr: 0.0010 - 5s/epoch - 5ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 5s - loss: 13.6726 - mse: 13.6726 - mae: 1.5554 - val_loss: 11.8897 - val_mse: 11.8897 - val_mae: 1.5608 - lr: 0.0010 - 5s/epoch - 5ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 5s - loss: 13.6910 - mse: 13.6910 - mae: 1.5506 - val_loss: 12.0994 - val_mse: 12.0994 - val_mae: 1.5098 - lr: 0.0010 - 5s/epoch - 5ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 4s - loss: 13.7285 - mse: 13.7285 - mae: 1.5555 - val_loss: 11.9269 - val_mse: 11.9269 - val_mae: 1.5619 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 5s - loss: 13.6550 - mse: 13.6550 - mae: 1.5627 - val_loss: 12.0543 - val_mse: 12.0543 - val_mae: 1.5981 - lr: 0.0010 - 5s/epoch - 5ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 5s - loss: 13.6677 - mse: 13.6677 - mae: 1.5509 - val_loss: 11.9784 - val_mse: 11.9784 - val_mae: 1.5597 - lr: 0.0010 - 5s/epoch - 5ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 5s - loss: 13.8995 - mse: 13.8995 - mae: 1.5518 - val_loss: 11.9087 - val_mse: 11.9087 - val_mae: 1.5780 - lr: 0.0010 - 5s/epoch - 5ms/step\n",
            "Score for fold 4: loss of 11.90869140625\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 4s - loss: 12.0970 - mse: 12.0970 - mae: 1.5601 - val_loss: 18.0699 - val_mse: 18.0699 - val_mae: 1.5997 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 4s - loss: 12.1638 - mse: 12.1638 - mae: 1.5629 - val_loss: 18.6210 - val_mse: 18.6210 - val_mae: 1.6677 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 5s - loss: 12.1705 - mse: 12.1705 - mae: 1.5621 - val_loss: 18.1933 - val_mse: 18.1933 - val_mae: 1.5716 - lr: 0.0010 - 5s/epoch - 5ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 4s - loss: 12.0903 - mse: 12.0903 - mae: 1.5611 - val_loss: 19.5068 - val_mse: 19.5068 - val_mae: 1.6735 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 4s - loss: 12.2002 - mse: 12.2002 - mae: 1.5547 - val_loss: 18.1449 - val_mse: 18.1449 - val_mae: 1.5283 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 4s - loss: 12.0830 - mse: 12.0830 - mae: 1.5612 - val_loss: 18.0638 - val_mse: 18.0638 - val_mae: 1.5362 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 4s - loss: 12.2438 - mse: 12.2438 - mae: 1.5671 - val_loss: 18.1584 - val_mse: 18.1584 - val_mae: 1.5174 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 4s - loss: 12.1211 - mse: 12.1211 - mae: 1.5588 - val_loss: 18.2310 - val_mse: 18.2310 - val_mae: 1.5261 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 4s - loss: 12.1053 - mse: 12.1053 - mae: 1.5619 - val_loss: 18.1374 - val_mse: 18.1374 - val_mae: 1.5824 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 4s - loss: 12.2894 - mse: 12.2894 - mae: 1.5596 - val_loss: 18.1157 - val_mse: 18.1157 - val_mae: 1.5372 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 5s - loss: 12.1220 - mse: 12.1220 - mae: 1.5576 - val_loss: 18.3780 - val_mse: 18.3780 - val_mae: 1.5861 - lr: 0.0010 - 5s/epoch - 5ms/step\n",
            "Score for fold 5: loss of 18.37795639038086\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-10-12 07:51:26,105]\u001b[0m Finished trial#8 resulted in value: 13.551999999999998. Current best value is 10.788 with parameters: {'activation': 'relu', 'num_hidden_layer': 4, 'i': 8, 'learning_rate': 0.0001160126182319962}.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 14.7335 - mse: 14.7335 - mae: 1.6285 - val_loss: 11.3634 - val_mse: 11.3634 - val_mae: 1.5129 - lr: 0.0018 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 14.2140 - mse: 14.2140 - mae: 1.5766 - val_loss: 11.1572 - val_mse: 11.1572 - val_mae: 1.6277 - lr: 0.0018 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 14.1027 - mse: 14.1027 - mae: 1.5720 - val_loss: 11.1004 - val_mse: 11.1004 - val_mae: 1.5123 - lr: 0.0018 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 14.1288 - mse: 14.1288 - mae: 1.5696 - val_loss: 10.3237 - val_mse: 10.3237 - val_mae: 1.5443 - lr: 0.0018 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 14.0621 - mse: 14.0621 - mae: 1.5663 - val_loss: 10.9822 - val_mse: 10.9822 - val_mae: 1.4743 - lr: 0.0018 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 14.1247 - mse: 14.1247 - mae: 1.5697 - val_loss: 11.0259 - val_mse: 11.0259 - val_mae: 1.5189 - lr: 0.0018 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 14.1356 - mse: 14.1356 - mae: 1.5638 - val_loss: 11.1485 - val_mse: 11.1485 - val_mae: 1.4916 - lr: 0.0018 - 2s/epoch - 2ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 2s - loss: 14.0293 - mse: 14.0293 - mae: 1.5679 - val_loss: 10.7290 - val_mse: 10.7290 - val_mae: 1.5320 - lr: 0.0018 - 2s/epoch - 2ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 2s - loss: 14.0756 - mse: 14.0756 - mae: 1.5627 - val_loss: 10.8394 - val_mse: 10.8394 - val_mae: 1.5146 - lr: 0.0018 - 2s/epoch - 2ms/step\n",
            "Score for fold 1: loss of 10.839400291442871\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 14.1349 - mse: 14.1349 - mae: 1.5555 - val_loss: 10.0975 - val_mse: 10.0975 - val_mae: 1.5583 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 14.0864 - mse: 14.0864 - mae: 1.5581 - val_loss: 10.3613 - val_mse: 10.3613 - val_mae: 1.5973 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 14.0677 - mse: 14.0677 - mae: 1.5634 - val_loss: 10.2446 - val_mse: 10.2446 - val_mae: 1.5963 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 14.0636 - mse: 14.0636 - mae: 1.5622 - val_loss: 10.1764 - val_mse: 10.1764 - val_mae: 1.5229 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 14.0486 - mse: 14.0486 - mae: 1.5571 - val_loss: 10.0910 - val_mse: 10.0910 - val_mae: 1.5571 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 14.0760 - mse: 14.0760 - mae: 1.5608 - val_loss: 10.1098 - val_mse: 10.1098 - val_mae: 1.5658 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 14.0045 - mse: 14.0045 - mae: 1.5643 - val_loss: 10.1253 - val_mse: 10.1253 - val_mae: 1.4950 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 2s - loss: 14.0693 - mse: 14.0693 - mae: 1.5568 - val_loss: 10.1737 - val_mse: 10.1737 - val_mae: 1.4746 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 2s - loss: 14.0360 - mse: 14.0360 - mae: 1.5607 - val_loss: 10.1305 - val_mse: 10.1305 - val_mae: 1.5592 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 2s - loss: 14.0567 - mse: 14.0567 - mae: 1.5608 - val_loss: 10.0863 - val_mse: 10.0863 - val_mae: 1.4761 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 2s - loss: 14.0798 - mse: 14.0798 - mae: 1.5607 - val_loss: 10.1220 - val_mse: 10.1220 - val_mae: 1.5439 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 2s - loss: 14.1055 - mse: 14.1055 - mae: 1.5542 - val_loss: 10.2136 - val_mse: 10.2136 - val_mae: 1.4572 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 13/100\n",
            "1000/1000 - 2s - loss: 14.0432 - mse: 14.0432 - mae: 1.5657 - val_loss: 10.1347 - val_mse: 10.1347 - val_mae: 1.4798 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 14/100\n",
            "1000/1000 - 2s - loss: 14.0059 - mse: 14.0059 - mae: 1.5583 - val_loss: 10.1197 - val_mse: 10.1197 - val_mae: 1.5587 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 15/100\n",
            "1000/1000 - 2s - loss: 14.0357 - mse: 14.0357 - mae: 1.5564 - val_loss: 10.1043 - val_mse: 10.1043 - val_mae: 1.4770 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Score for fold 2: loss of 10.104272842407227\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 13.5825 - mse: 13.5825 - mae: 1.5538 - val_loss: 12.2125 - val_mse: 12.2125 - val_mae: 1.5816 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 13.6005 - mse: 13.6005 - mae: 1.5570 - val_loss: 12.1428 - val_mse: 12.1428 - val_mae: 1.5510 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 13.5481 - mse: 13.5481 - mae: 1.5544 - val_loss: 12.2313 - val_mse: 12.2313 - val_mae: 1.5275 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 13.5396 - mse: 13.5396 - mae: 1.5476 - val_loss: 12.1167 - val_mse: 12.1167 - val_mae: 1.5799 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 13.5544 - mse: 13.5544 - mae: 1.5517 - val_loss: 12.0603 - val_mse: 12.0603 - val_mae: 1.4976 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 13.5771 - mse: 13.5771 - mae: 1.5609 - val_loss: 12.3530 - val_mse: 12.3530 - val_mae: 1.5882 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 13.6232 - mse: 13.6232 - mae: 1.5517 - val_loss: 12.2226 - val_mse: 12.2226 - val_mae: 1.5681 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 2s - loss: 13.5296 - mse: 13.5296 - mae: 1.5501 - val_loss: 12.2227 - val_mse: 12.2227 - val_mae: 1.5831 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 2s - loss: 13.5437 - mse: 13.5437 - mae: 1.5552 - val_loss: 12.2279 - val_mse: 12.2279 - val_mae: 1.5727 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 2s - loss: 13.7510 - mse: 13.7510 - mae: 1.5562 - val_loss: 12.6958 - val_mse: 12.6958 - val_mae: 1.6460 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Score for fold 3: loss of 12.695815086364746\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 11.8211 - mse: 11.8211 - mae: 1.5382 - val_loss: 20.1196 - val_mse: 20.1196 - val_mae: 1.7641 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 11.7656 - mse: 11.7656 - mae: 1.5407 - val_loss: 19.5502 - val_mse: 19.5502 - val_mae: 1.6181 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 11.7731 - mse: 11.7731 - mae: 1.5393 - val_loss: 19.5438 - val_mse: 19.5438 - val_mae: 1.5984 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 11.7621 - mse: 11.7621 - mae: 1.5379 - val_loss: 19.4625 - val_mse: 19.4625 - val_mae: 1.5634 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 11.6769 - mse: 11.6769 - mae: 1.5406 - val_loss: 19.4261 - val_mse: 19.4261 - val_mae: 1.5931 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 11.7688 - mse: 11.7688 - mae: 1.5366 - val_loss: 19.5939 - val_mse: 19.5939 - val_mae: 1.5505 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 11.7436 - mse: 11.7436 - mae: 1.5421 - val_loss: 19.4765 - val_mse: 19.4765 - val_mae: 1.5486 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 2s - loss: 11.6542 - mse: 11.6542 - mae: 1.5419 - val_loss: 19.4267 - val_mse: 19.4267 - val_mae: 1.5711 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 2s - loss: 11.6052 - mse: 11.6052 - mae: 1.5340 - val_loss: 19.3651 - val_mse: 19.3651 - val_mae: 1.6255 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 2s - loss: 11.6960 - mse: 11.6960 - mae: 1.5435 - val_loss: 19.4263 - val_mse: 19.4263 - val_mae: 1.5754 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 2s - loss: 11.7104 - mse: 11.7104 - mae: 1.5411 - val_loss: 19.3868 - val_mse: 19.3868 - val_mae: 1.5675 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 2s - loss: 11.6437 - mse: 11.6437 - mae: 1.5356 - val_loss: 19.4291 - val_mse: 19.4291 - val_mae: 1.5773 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 13/100\n",
            "1000/1000 - 2s - loss: 11.7286 - mse: 11.7286 - mae: 1.5421 - val_loss: 19.5143 - val_mse: 19.5143 - val_mae: 1.5380 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 14/100\n",
            "1000/1000 - 2s - loss: 11.5755 - mse: 11.5755 - mae: 1.5358 - val_loss: 19.4116 - val_mse: 19.4116 - val_mae: 1.5923 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Score for fold 4: loss of 19.411624908447266\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 13.0643 - mse: 13.0643 - mae: 1.5536 - val_loss: 13.9885 - val_mse: 13.9885 - val_mae: 1.4889 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 13.1006 - mse: 13.1006 - mae: 1.5556 - val_loss: 13.7773 - val_mse: 13.7773 - val_mae: 1.5687 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 13.1096 - mse: 13.1096 - mae: 1.5542 - val_loss: 14.0118 - val_mse: 14.0118 - val_mae: 1.6262 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 13.1924 - mse: 13.1924 - mae: 1.5577 - val_loss: 13.7687 - val_mse: 13.7687 - val_mae: 1.5079 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 13.1608 - mse: 13.1608 - mae: 1.5553 - val_loss: 13.8498 - val_mse: 13.8498 - val_mae: 1.5961 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 13.2136 - mse: 13.2136 - mae: 1.5626 - val_loss: 13.7591 - val_mse: 13.7591 - val_mae: 1.5484 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 13.0825 - mse: 13.0825 - mae: 1.5625 - val_loss: 14.1783 - val_mse: 14.1783 - val_mae: 1.6160 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 2s - loss: 13.1657 - mse: 13.1657 - mae: 1.5557 - val_loss: 13.8524 - val_mse: 13.8524 - val_mae: 1.4997 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 2s - loss: 13.2077 - mse: 13.2077 - mae: 1.5574 - val_loss: 13.9121 - val_mse: 13.9121 - val_mae: 1.5951 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 2s - loss: 13.0601 - mse: 13.0601 - mae: 1.5526 - val_loss: 13.8208 - val_mse: 13.8208 - val_mae: 1.5600 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 2s - loss: 13.2538 - mse: 13.2538 - mae: 1.5538 - val_loss: 13.8040 - val_mse: 13.8040 - val_mae: 1.5788 - lr: 0.0010 - 2s/epoch - 2ms/step\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-10-12 07:53:19,581]\u001b[0m Finished trial#9 resulted in value: 13.37. Current best value is 10.788 with parameters: {'activation': 'relu', 'num_hidden_layer': 4, 'i': 8, 'learning_rate': 0.0001160126182319962}.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Score for fold 5: loss of 13.803996086120605\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 17.2225 - mse: 17.2225 - mae: 1.6937 - val_loss: 10.5387 - val_mse: 10.5387 - val_mae: 1.5433 - lr: 1.0006e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 1s - loss: 15.1116 - mse: 15.1116 - mae: 1.5716 - val_loss: 9.7535 - val_mse: 9.7535 - val_mae: 1.5143 - lr: 1.0006e-04 - 1s/epoch - 1ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 1s - loss: 14.5093 - mse: 14.5093 - mae: 1.5524 - val_loss: 9.3716 - val_mse: 9.3716 - val_mae: 1.5133 - lr: 1.0006e-04 - 1s/epoch - 1ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 1s - loss: 14.1409 - mse: 14.1409 - mae: 1.5394 - val_loss: 9.1398 - val_mse: 9.1398 - val_mae: 1.4944 - lr: 1.0006e-04 - 1s/epoch - 1ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 1s - loss: 13.9975 - mse: 13.9975 - mae: 1.5286 - val_loss: 8.9957 - val_mse: 8.9957 - val_mae: 1.4848 - lr: 1.0006e-04 - 1s/epoch - 1ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 1s - loss: 13.8886 - mse: 13.8886 - mae: 1.5213 - val_loss: 8.9024 - val_mse: 8.9024 - val_mae: 1.4866 - lr: 1.0006e-04 - 1s/epoch - 1ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 1s - loss: 13.7918 - mse: 13.7918 - mae: 1.5172 - val_loss: 8.8281 - val_mse: 8.8281 - val_mae: 1.4767 - lr: 1.0006e-04 - 1s/epoch - 1ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 1s - loss: 13.7198 - mse: 13.7198 - mae: 1.5146 - val_loss: 8.7914 - val_mse: 8.7914 - val_mae: 1.4667 - lr: 1.0006e-04 - 1s/epoch - 1ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 1s - loss: 13.6689 - mse: 13.6689 - mae: 1.5095 - val_loss: 8.7343 - val_mse: 8.7343 - val_mae: 1.4729 - lr: 1.0006e-04 - 1s/epoch - 1ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 1s - loss: 13.6221 - mse: 13.6221 - mae: 1.5042 - val_loss: 8.6922 - val_mse: 8.6922 - val_mae: 1.4725 - lr: 1.0006e-04 - 1s/epoch - 1ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 1s - loss: 13.5584 - mse: 13.5584 - mae: 1.5036 - val_loss: 8.6485 - val_mse: 8.6485 - val_mae: 1.4697 - lr: 1.0006e-04 - 1s/epoch - 1ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 1s - loss: 13.5215 - mse: 13.5215 - mae: 1.5003 - val_loss: 8.6527 - val_mse: 8.6527 - val_mae: 1.4534 - lr: 1.0006e-04 - 1s/epoch - 1ms/step\n",
            "Epoch 13/100\n",
            "1000/1000 - 1s - loss: 13.4721 - mse: 13.4721 - mae: 1.4983 - val_loss: 8.6100 - val_mse: 8.6100 - val_mae: 1.4451 - lr: 1.0006e-04 - 1s/epoch - 1ms/step\n",
            "Epoch 14/100\n",
            "1000/1000 - 1s - loss: 13.4452 - mse: 13.4452 - mae: 1.4940 - val_loss: 8.5802 - val_mse: 8.5802 - val_mae: 1.4533 - lr: 1.0006e-04 - 1s/epoch - 1ms/step\n",
            "Epoch 15/100\n",
            "1000/1000 - 1s - loss: 13.4000 - mse: 13.4000 - mae: 1.4931 - val_loss: 8.5550 - val_mse: 8.5550 - val_mae: 1.4520 - lr: 1.0006e-04 - 1s/epoch - 1ms/step\n",
            "Epoch 16/100\n",
            "1000/1000 - 1s - loss: 13.3751 - mse: 13.3751 - mae: 1.4909 - val_loss: 8.5230 - val_mse: 8.5230 - val_mae: 1.4639 - lr: 1.0006e-04 - 1s/epoch - 1ms/step\n",
            "Epoch 17/100\n",
            "1000/1000 - 1s - loss: 13.3327 - mse: 13.3327 - mae: 1.4889 - val_loss: 8.5243 - val_mse: 8.5243 - val_mae: 1.4406 - lr: 1.0006e-04 - 1s/epoch - 1ms/step\n",
            "Epoch 18/100\n",
            "1000/1000 - 1s - loss: 13.3278 - mse: 13.3278 - mae: 1.4866 - val_loss: 8.4923 - val_mse: 8.4923 - val_mae: 1.4503 - lr: 1.0006e-04 - 1s/epoch - 1ms/step\n",
            "Epoch 19/100\n",
            "1000/1000 - 1s - loss: 13.2855 - mse: 13.2855 - mae: 1.4886 - val_loss: 8.4724 - val_mse: 8.4724 - val_mae: 1.4559 - lr: 1.0006e-04 - 1s/epoch - 1ms/step\n",
            "Epoch 20/100\n",
            "1000/1000 - 1s - loss: 13.2612 - mse: 13.2612 - mae: 1.4843 - val_loss: 8.4624 - val_mse: 8.4624 - val_mae: 1.4503 - lr: 1.0006e-04 - 1s/epoch - 1ms/step\n",
            "Epoch 21/100\n",
            "1000/1000 - 1s - loss: 13.2374 - mse: 13.2374 - mae: 1.4839 - val_loss: 8.4636 - val_mse: 8.4636 - val_mae: 1.4451 - lr: 1.0006e-04 - 1s/epoch - 1ms/step\n",
            "Epoch 22/100\n",
            "1000/1000 - 1s - loss: 13.2284 - mse: 13.2284 - mae: 1.4802 - val_loss: 8.4456 - val_mse: 8.4456 - val_mae: 1.4585 - lr: 1.0006e-04 - 1s/epoch - 1ms/step\n",
            "Epoch 23/100\n",
            "1000/1000 - 1s - loss: 13.2152 - mse: 13.2152 - mae: 1.4835 - val_loss: 8.4490 - val_mse: 8.4490 - val_mae: 1.4321 - lr: 1.0006e-04 - 1s/epoch - 1ms/step\n",
            "Epoch 24/100\n",
            "1000/1000 - 1s - loss: 13.1807 - mse: 13.1807 - mae: 1.4780 - val_loss: 8.4438 - val_mse: 8.4438 - val_mae: 1.4310 - lr: 1.0006e-04 - 1s/epoch - 1ms/step\n",
            "Epoch 25/100\n",
            "1000/1000 - 1s - loss: 13.1563 - mse: 13.1563 - mae: 1.4766 - val_loss: 8.4260 - val_mse: 8.4260 - val_mae: 1.4436 - lr: 1.0006e-04 - 1s/epoch - 1ms/step\n",
            "Epoch 26/100\n",
            "1000/1000 - 1s - loss: 13.1275 - mse: 13.1275 - mae: 1.4759 - val_loss: 8.4129 - val_mse: 8.4129 - val_mae: 1.4448 - lr: 1.0006e-04 - 1s/epoch - 1ms/step\n",
            "Epoch 27/100\n",
            "1000/1000 - 1s - loss: 13.1154 - mse: 13.1154 - mae: 1.4750 - val_loss: 8.3777 - val_mse: 8.3777 - val_mae: 1.4453 - lr: 1.0006e-04 - 1s/epoch - 1ms/step\n",
            "Epoch 28/100\n",
            "1000/1000 - 1s - loss: 13.0951 - mse: 13.0951 - mae: 1.4735 - val_loss: 8.3828 - val_mse: 8.3828 - val_mae: 1.4367 - lr: 1.0006e-04 - 1s/epoch - 1ms/step\n",
            "Epoch 29/100\n",
            "1000/1000 - 1s - loss: 13.0937 - mse: 13.0937 - mae: 1.4723 - val_loss: 8.3823 - val_mse: 8.3823 - val_mae: 1.4373 - lr: 1.0006e-04 - 1s/epoch - 1ms/step\n",
            "Epoch 30/100\n",
            "1000/1000 - 1s - loss: 13.0709 - mse: 13.0709 - mae: 1.4745 - val_loss: 8.3958 - val_mse: 8.3958 - val_mae: 1.4217 - lr: 1.0006e-04 - 1s/epoch - 1ms/step\n",
            "Epoch 31/100\n",
            "1000/1000 - 1s - loss: 13.0670 - mse: 13.0670 - mae: 1.4689 - val_loss: 8.4025 - val_mse: 8.4025 - val_mae: 1.4268 - lr: 1.0006e-04 - 1s/epoch - 1ms/step\n",
            "Epoch 32/100\n",
            "1000/1000 - 1s - loss: 13.0415 - mse: 13.0415 - mae: 1.4689 - val_loss: 8.3273 - val_mse: 8.3273 - val_mae: 1.4283 - lr: 1.0006e-04 - 1s/epoch - 1ms/step\n",
            "Epoch 33/100\n",
            "1000/1000 - 1s - loss: 13.0087 - mse: 13.0087 - mae: 1.4675 - val_loss: 8.3594 - val_mse: 8.3594 - val_mae: 1.4199 - lr: 1.0006e-04 - 1s/epoch - 1ms/step\n",
            "Epoch 34/100\n",
            "1000/1000 - 1s - loss: 13.0080 - mse: 13.0080 - mae: 1.4644 - val_loss: 8.3697 - val_mse: 8.3697 - val_mae: 1.4305 - lr: 1.0006e-04 - 1s/epoch - 1ms/step\n",
            "Epoch 35/100\n",
            "1000/1000 - 1s - loss: 12.9860 - mse: 12.9860 - mae: 1.4663 - val_loss: 8.3598 - val_mse: 8.3598 - val_mae: 1.4257 - lr: 1.0006e-04 - 1s/epoch - 1ms/step\n",
            "Epoch 36/100\n",
            "1000/1000 - 1s - loss: 12.9837 - mse: 12.9837 - mae: 1.4623 - val_loss: 8.3169 - val_mse: 8.3169 - val_mae: 1.4355 - lr: 1.0006e-04 - 1s/epoch - 1ms/step\n",
            "Epoch 37/100\n",
            "1000/1000 - 1s - loss: 12.9737 - mse: 12.9737 - mae: 1.4643 - val_loss: 8.3412 - val_mse: 8.3412 - val_mae: 1.4453 - lr: 1.0006e-04 - 1s/epoch - 1ms/step\n",
            "Epoch 38/100\n",
            "1000/1000 - 1s - loss: 12.9409 - mse: 12.9409 - mae: 1.4637 - val_loss: 8.3420 - val_mse: 8.3420 - val_mae: 1.4252 - lr: 1.0006e-04 - 1s/epoch - 1ms/step\n",
            "Epoch 39/100\n",
            "1000/1000 - 1s - loss: 12.9566 - mse: 12.9566 - mae: 1.4596 - val_loss: 8.2988 - val_mse: 8.2988 - val_mae: 1.4257 - lr: 1.0006e-04 - 1s/epoch - 1ms/step\n",
            "Epoch 40/100\n",
            "1000/1000 - 1s - loss: 12.9360 - mse: 12.9360 - mae: 1.4602 - val_loss: 8.3066 - val_mse: 8.3066 - val_mae: 1.4268 - lr: 1.0006e-04 - 1s/epoch - 1ms/step\n",
            "Epoch 41/100\n",
            "1000/1000 - 1s - loss: 12.9183 - mse: 12.9183 - mae: 1.4607 - val_loss: 8.2952 - val_mse: 8.2952 - val_mae: 1.4316 - lr: 1.0006e-04 - 1s/epoch - 1ms/step\n",
            "Epoch 42/100\n",
            "1000/1000 - 1s - loss: 12.9057 - mse: 12.9057 - mae: 1.4575 - val_loss: 8.3238 - val_mse: 8.3238 - val_mae: 1.4258 - lr: 1.0006e-04 - 1s/epoch - 1ms/step\n",
            "Epoch 43/100\n",
            "1000/1000 - 1s - loss: 12.9104 - mse: 12.9104 - mae: 1.4572 - val_loss: 8.2898 - val_mse: 8.2898 - val_mae: 1.4242 - lr: 1.0006e-04 - 1s/epoch - 1ms/step\n",
            "Epoch 44/100\n",
            "1000/1000 - 1s - loss: 12.8812 - mse: 12.8812 - mae: 1.4572 - val_loss: 8.2606 - val_mse: 8.2606 - val_mae: 1.4321 - lr: 1.0006e-04 - 1s/epoch - 1ms/step\n",
            "Epoch 45/100\n",
            "1000/1000 - 1s - loss: 12.8906 - mse: 12.8906 - mae: 1.4567 - val_loss: 8.3038 - val_mse: 8.3038 - val_mae: 1.4332 - lr: 1.0006e-04 - 1s/epoch - 1ms/step\n",
            "Epoch 46/100\n",
            "1000/1000 - 1s - loss: 12.8786 - mse: 12.8786 - mae: 1.4576 - val_loss: 8.2935 - val_mse: 8.2935 - val_mae: 1.4190 - lr: 1.0006e-04 - 1s/epoch - 1ms/step\n",
            "Epoch 47/100\n",
            "1000/1000 - 1s - loss: 12.8754 - mse: 12.8754 - mae: 1.4555 - val_loss: 8.2873 - val_mse: 8.2873 - val_mae: 1.4245 - lr: 1.0006e-04 - 1s/epoch - 1ms/step\n",
            "Epoch 48/100\n",
            "1000/1000 - 1s - loss: 12.8516 - mse: 12.8516 - mae: 1.4533 - val_loss: 8.2906 - val_mse: 8.2906 - val_mae: 1.4330 - lr: 1.0006e-04 - 1s/epoch - 1ms/step\n",
            "Epoch 49/100\n",
            "1000/1000 - 1s - loss: 12.8654 - mse: 12.8654 - mae: 1.4537 - val_loss: 8.2392 - val_mse: 8.2392 - val_mae: 1.4327 - lr: 1.0006e-04 - 1s/epoch - 1ms/step\n",
            "Epoch 50/100\n",
            "1000/1000 - 1s - loss: 12.8392 - mse: 12.8392 - mae: 1.4530 - val_loss: 8.2509 - val_mse: 8.2509 - val_mae: 1.4277 - lr: 1.0006e-04 - 1s/epoch - 1ms/step\n",
            "Epoch 51/100\n",
            "1000/1000 - 1s - loss: 12.8182 - mse: 12.8182 - mae: 1.4550 - val_loss: 8.2744 - val_mse: 8.2744 - val_mae: 1.4186 - lr: 1.0006e-04 - 1s/epoch - 1ms/step\n",
            "Epoch 52/100\n",
            "1000/1000 - 1s - loss: 12.8309 - mse: 12.8309 - mae: 1.4520 - val_loss: 8.2574 - val_mse: 8.2574 - val_mae: 1.4233 - lr: 1.0006e-04 - 1s/epoch - 1ms/step\n",
            "Epoch 53/100\n",
            "1000/1000 - 1s - loss: 12.8196 - mse: 12.8196 - mae: 1.4526 - val_loss: 8.2565 - val_mse: 8.2565 - val_mae: 1.4275 - lr: 1.0006e-04 - 1s/epoch - 1ms/step\n",
            "Epoch 54/100\n",
            "1000/1000 - 1s - loss: 12.7958 - mse: 12.7958 - mae: 1.4526 - val_loss: 8.2865 - val_mse: 8.2865 - val_mae: 1.4104 - lr: 1.0006e-04 - 1s/epoch - 1ms/step\n",
            "Score for fold 1: loss of 8.28647518157959\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 1s - loss: 12.7549 - mse: 12.7549 - mae: 1.4482 - val_loss: 8.4837 - val_mse: 8.4837 - val_mae: 1.4313 - lr: 1.0006e-04 - 1s/epoch - 1ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 1s - loss: 12.7167 - mse: 12.7167 - mae: 1.4475 - val_loss: 8.4369 - val_mse: 8.4369 - val_mae: 1.4285 - lr: 1.0006e-04 - 1s/epoch - 1ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 1s - loss: 12.7230 - mse: 12.7230 - mae: 1.4466 - val_loss: 8.4341 - val_mse: 8.4341 - val_mae: 1.4336 - lr: 1.0006e-04 - 1s/epoch - 1ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 1s - loss: 12.7064 - mse: 12.7064 - mae: 1.4450 - val_loss: 8.4811 - val_mse: 8.4811 - val_mae: 1.4293 - lr: 1.0006e-04 - 1s/epoch - 1ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 1s - loss: 12.6703 - mse: 12.6703 - mae: 1.4449 - val_loss: 8.4424 - val_mse: 8.4424 - val_mae: 1.4475 - lr: 1.0006e-04 - 1s/epoch - 1ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 1s - loss: 12.6854 - mse: 12.6854 - mae: 1.4452 - val_loss: 8.4817 - val_mse: 8.4817 - val_mae: 1.4360 - lr: 1.0006e-04 - 1s/epoch - 1ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 1s - loss: 12.6684 - mse: 12.6684 - mae: 1.4432 - val_loss: 8.4613 - val_mse: 8.4613 - val_mae: 1.4277 - lr: 1.0006e-04 - 1s/epoch - 1ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 1s - loss: 12.6573 - mse: 12.6573 - mae: 1.4454 - val_loss: 8.4487 - val_mse: 8.4487 - val_mae: 1.4298 - lr: 1.0006e-04 - 1s/epoch - 1ms/step\n",
            "Score for fold 2: loss of 8.44874095916748\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 1s - loss: 11.8981 - mse: 11.8981 - mae: 1.4381 - val_loss: 11.4589 - val_mse: 11.4589 - val_mae: 1.4393 - lr: 1.0006e-04 - 1s/epoch - 1ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 1s - loss: 11.9183 - mse: 11.9183 - mae: 1.4364 - val_loss: 11.4216 - val_mse: 11.4216 - val_mae: 1.4434 - lr: 1.0006e-04 - 1s/epoch - 1ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 1s - loss: 11.8851 - mse: 11.8851 - mae: 1.4385 - val_loss: 11.4358 - val_mse: 11.4358 - val_mae: 1.4490 - lr: 1.0006e-04 - 1s/epoch - 1ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 1s - loss: 11.8980 - mse: 11.8980 - mae: 1.4370 - val_loss: 11.4508 - val_mse: 11.4508 - val_mae: 1.4444 - lr: 1.0006e-04 - 1s/epoch - 1ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 1s - loss: 11.8978 - mse: 11.8978 - mae: 1.4353 - val_loss: 11.4624 - val_mse: 11.4624 - val_mae: 1.4321 - lr: 1.0006e-04 - 1s/epoch - 1ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 1s - loss: 11.8888 - mse: 11.8888 - mae: 1.4368 - val_loss: 11.4387 - val_mse: 11.4387 - val_mae: 1.4492 - lr: 1.0006e-04 - 1s/epoch - 1ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 1s - loss: 11.8733 - mse: 11.8733 - mae: 1.4357 - val_loss: 11.4073 - val_mse: 11.4073 - val_mae: 1.4677 - lr: 1.0006e-04 - 1s/epoch - 1ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 1s - loss: 11.8672 - mse: 11.8672 - mae: 1.4360 - val_loss: 11.4485 - val_mse: 11.4485 - val_mae: 1.4500 - lr: 1.0006e-04 - 1s/epoch - 1ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 1s - loss: 11.8693 - mse: 11.8693 - mae: 1.4358 - val_loss: 11.4122 - val_mse: 11.4122 - val_mae: 1.4554 - lr: 1.0006e-04 - 1s/epoch - 1ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 1s - loss: 11.8582 - mse: 11.8582 - mae: 1.4324 - val_loss: 11.3957 - val_mse: 11.3957 - val_mae: 1.4639 - lr: 1.0006e-04 - 1s/epoch - 1ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 1s - loss: 11.8391 - mse: 11.8391 - mae: 1.4359 - val_loss: 11.4469 - val_mse: 11.4469 - val_mae: 1.4431 - lr: 1.0006e-04 - 1s/epoch - 1ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 1s - loss: 11.8324 - mse: 11.8324 - mae: 1.4343 - val_loss: 11.4526 - val_mse: 11.4526 - val_mae: 1.4495 - lr: 1.0006e-04 - 1s/epoch - 1ms/step\n",
            "Epoch 13/100\n",
            "1000/1000 - 1s - loss: 11.8679 - mse: 11.8679 - mae: 1.4304 - val_loss: 11.4294 - val_mse: 11.4294 - val_mae: 1.4579 - lr: 1.0006e-04 - 1s/epoch - 1ms/step\n",
            "Epoch 14/100\n",
            "1000/1000 - 1s - loss: 11.8535 - mse: 11.8535 - mae: 1.4335 - val_loss: 11.4385 - val_mse: 11.4385 - val_mae: 1.4632 - lr: 1.0006e-04 - 1s/epoch - 1ms/step\n",
            "Epoch 15/100\n",
            "1000/1000 - 1s - loss: 11.8294 - mse: 11.8294 - mae: 1.4319 - val_loss: 11.4899 - val_mse: 11.4899 - val_mae: 1.4426 - lr: 1.0006e-04 - 1s/epoch - 1ms/step\n",
            "Score for fold 3: loss of 11.489919662475586\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 1s - loss: 9.2092 - mse: 9.2092 - mae: 1.4301 - val_loss: 21.8660 - val_mse: 21.8660 - val_mae: 1.4694 - lr: 1.0006e-04 - 1s/epoch - 1ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 1s - loss: 9.1633 - mse: 9.1633 - mae: 1.4290 - val_loss: 21.9030 - val_mse: 21.9030 - val_mae: 1.4527 - lr: 1.0006e-04 - 1s/epoch - 1ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 1s - loss: 9.1662 - mse: 9.1662 - mae: 1.4265 - val_loss: 21.8914 - val_mse: 21.8914 - val_mae: 1.4691 - lr: 1.0006e-04 - 1s/epoch - 1ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 1s - loss: 9.1449 - mse: 9.1449 - mae: 1.4277 - val_loss: 21.9131 - val_mse: 21.9131 - val_mae: 1.4738 - lr: 1.0006e-04 - 1s/epoch - 1ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 1s - loss: 9.1175 - mse: 9.1175 - mae: 1.4282 - val_loss: 21.9256 - val_mse: 21.9256 - val_mae: 1.4745 - lr: 1.0006e-04 - 1s/epoch - 1ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 1s - loss: 9.1232 - mse: 9.1232 - mae: 1.4286 - val_loss: 21.9141 - val_mse: 21.9141 - val_mae: 1.4690 - lr: 1.0006e-04 - 1s/epoch - 1ms/step\n",
            "Score for fold 4: loss of 21.914043426513672\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 1s - loss: 12.3795 - mse: 12.3795 - mae: 1.4353 - val_loss: 8.9744 - val_mse: 8.9744 - val_mae: 1.4186 - lr: 1.0006e-04 - 1s/epoch - 1ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 1s - loss: 12.3668 - mse: 12.3668 - mae: 1.4342 - val_loss: 8.9354 - val_mse: 8.9354 - val_mae: 1.4293 - lr: 1.0006e-04 - 1s/epoch - 1ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 1s - loss: 12.3558 - mse: 12.3558 - mae: 1.4333 - val_loss: 9.0126 - val_mse: 9.0126 - val_mae: 1.4081 - lr: 1.0006e-04 - 1s/epoch - 1ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 1s - loss: 12.3482 - mse: 12.3482 - mae: 1.4314 - val_loss: 8.9430 - val_mse: 8.9430 - val_mae: 1.4395 - lr: 1.0006e-04 - 1s/epoch - 1ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 1s - loss: 12.3517 - mse: 12.3517 - mae: 1.4353 - val_loss: 8.9622 - val_mse: 8.9622 - val_mae: 1.4316 - lr: 1.0006e-04 - 1s/epoch - 1ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 1s - loss: 12.3347 - mse: 12.3347 - mae: 1.4326 - val_loss: 9.0181 - val_mse: 9.0181 - val_mae: 1.4329 - lr: 1.0006e-04 - 1s/epoch - 1ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 1s - loss: 12.3175 - mse: 12.3175 - mae: 1.4319 - val_loss: 8.9783 - val_mse: 8.9783 - val_mae: 1.4300 - lr: 1.0006e-04 - 1s/epoch - 1ms/step\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-10-12 07:55:26,247]\u001b[0m Finished trial#10 resulted in value: 11.824000000000002. Current best value is 10.788 with parameters: {'activation': 'relu', 'num_hidden_layer': 4, 'i': 8, 'learning_rate': 0.0001160126182319962}.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Score for fold 5: loss of 8.97825813293457\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 6s - loss: 13.8146 - mse: 13.8146 - mae: 1.5488 - val_loss: 11.0258 - val_mse: 11.0258 - val_mae: 1.4349 - lr: 3.2562e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 6s - loss: 12.6155 - mse: 12.6155 - mae: 1.4967 - val_loss: 10.9690 - val_mse: 10.9690 - val_mae: 1.3921 - lr: 3.2562e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 6s - loss: 12.6681 - mse: 12.6681 - mae: 1.4826 - val_loss: 11.1050 - val_mse: 11.1050 - val_mae: 1.3806 - lr: 3.2562e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 6s - loss: 12.5150 - mse: 12.5150 - mae: 1.4713 - val_loss: 10.8561 - val_mse: 10.8561 - val_mae: 1.4502 - lr: 3.2562e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 6s - loss: 12.3421 - mse: 12.3421 - mae: 1.4620 - val_loss: 10.5568 - val_mse: 10.5568 - val_mae: 1.3768 - lr: 3.2562e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 6s - loss: 12.1314 - mse: 12.1314 - mae: 1.4539 - val_loss: 10.5982 - val_mse: 10.5982 - val_mae: 1.4727 - lr: 3.2562e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 6s - loss: 12.0990 - mse: 12.0990 - mae: 1.4474 - val_loss: 10.8209 - val_mse: 10.8209 - val_mae: 1.4830 - lr: 3.2562e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 5s - loss: 11.9163 - mse: 11.9163 - mae: 1.4406 - val_loss: 11.0246 - val_mse: 11.0246 - val_mae: 1.4035 - lr: 3.2562e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 6s - loss: 11.9213 - mse: 11.9213 - mae: 1.4357 - val_loss: 10.5953 - val_mse: 10.5953 - val_mae: 1.4220 - lr: 3.2562e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 5s - loss: 11.6495 - mse: 11.6495 - mae: 1.4265 - val_loss: 10.3398 - val_mse: 10.3398 - val_mae: 1.3949 - lr: 3.2562e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 5s - loss: 11.4657 - mse: 11.4657 - mae: 1.4198 - val_loss: 10.7617 - val_mse: 10.7617 - val_mae: 1.3952 - lr: 3.2562e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 5s - loss: 11.3415 - mse: 11.3415 - mae: 1.4052 - val_loss: 10.7332 - val_mse: 10.7332 - val_mae: 1.4325 - lr: 3.2562e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 13/100\n",
            "1000/1000 - 5s - loss: 11.0457 - mse: 11.0457 - mae: 1.4000 - val_loss: 10.9140 - val_mse: 10.9140 - val_mae: 1.4282 - lr: 3.2562e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 14/100\n",
            "1000/1000 - 5s - loss: 10.9747 - mse: 10.9747 - mae: 1.3928 - val_loss: 10.5095 - val_mse: 10.5095 - val_mae: 1.4128 - lr: 3.2562e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 15/100\n",
            "1000/1000 - 5s - loss: 10.7655 - mse: 10.7655 - mae: 1.3834 - val_loss: 10.5964 - val_mse: 10.5964 - val_mae: 1.4328 - lr: 3.2562e-04 - 5s/epoch - 5ms/step\n",
            "Score for fold 1: loss of 10.596430778503418\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 5s - loss: 11.4028 - mse: 11.4028 - mae: 1.3932 - val_loss: 9.1006 - val_mse: 9.1006 - val_mae: 1.3632 - lr: 3.2562e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 5s - loss: 11.1796 - mse: 11.1796 - mae: 1.3785 - val_loss: 8.4591 - val_mse: 8.4591 - val_mae: 1.3867 - lr: 3.2562e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 6s - loss: 10.9929 - mse: 10.9929 - mae: 1.3721 - val_loss: 9.7530 - val_mse: 9.7530 - val_mae: 1.3583 - lr: 3.2562e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 6s - loss: 10.9103 - mse: 10.9103 - mae: 1.3611 - val_loss: 8.4678 - val_mse: 8.4678 - val_mae: 1.3830 - lr: 3.2562e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 6s - loss: 10.6334 - mse: 10.6334 - mae: 1.3491 - val_loss: 8.5531 - val_mse: 8.5531 - val_mae: 1.3543 - lr: 3.2562e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 6s - loss: 10.3996 - mse: 10.3996 - mae: 1.3402 - val_loss: 9.6091 - val_mse: 9.6091 - val_mae: 1.4062 - lr: 3.2562e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 6s - loss: 10.2194 - mse: 10.2194 - mae: 1.3261 - val_loss: 9.3845 - val_mse: 9.3845 - val_mae: 1.3993 - lr: 3.2562e-04 - 6s/epoch - 6ms/step\n",
            "Score for fold 2: loss of 9.384511947631836\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 5s - loss: 9.8730 - mse: 9.8730 - mae: 1.3389 - val_loss: 9.5125 - val_mse: 9.5125 - val_mae: 1.3849 - lr: 3.2562e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 5s - loss: 9.6636 - mse: 9.6636 - mae: 1.3176 - val_loss: 9.6943 - val_mse: 9.6943 - val_mae: 1.3605 - lr: 3.2562e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 5s - loss: 9.4999 - mse: 9.4999 - mae: 1.3012 - val_loss: 9.9744 - val_mse: 9.9744 - val_mae: 1.3275 - lr: 3.2562e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 5s - loss: 9.3683 - mse: 9.3683 - mae: 1.2891 - val_loss: 10.1640 - val_mse: 10.1640 - val_mae: 1.4005 - lr: 3.2562e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 5s - loss: 9.2148 - mse: 9.2148 - mae: 1.2758 - val_loss: 9.9237 - val_mse: 9.9237 - val_mae: 1.3192 - lr: 3.2562e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 5s - loss: 8.7954 - mse: 8.7954 - mae: 1.2593 - val_loss: 10.0129 - val_mse: 10.0129 - val_mae: 1.3730 - lr: 3.2562e-04 - 5s/epoch - 5ms/step\n",
            "Score for fold 3: loss of 10.012897491455078\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 5s - loss: 7.9297 - mse: 7.9297 - mae: 1.2820 - val_loss: 13.9220 - val_mse: 13.9220 - val_mae: 1.2782 - lr: 3.2562e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 5s - loss: 7.9233 - mse: 7.9233 - mae: 1.2669 - val_loss: 14.2645 - val_mse: 14.2645 - val_mae: 1.2517 - lr: 3.2562e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 5s - loss: 7.5702 - mse: 7.5702 - mae: 1.2476 - val_loss: 14.4036 - val_mse: 14.4036 - val_mae: 1.2778 - lr: 3.2562e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 6s - loss: 7.2284 - mse: 7.2284 - mae: 1.2317 - val_loss: 14.2125 - val_mse: 14.2125 - val_mae: 1.2786 - lr: 3.2562e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 6s - loss: 7.2644 - mse: 7.2644 - mae: 1.2167 - val_loss: 14.3839 - val_mse: 14.3839 - val_mae: 1.3147 - lr: 3.2562e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 5s - loss: 7.1261 - mse: 7.1261 - mae: 1.2030 - val_loss: 14.5167 - val_mse: 14.5167 - val_mae: 1.3186 - lr: 3.2562e-04 - 5s/epoch - 5ms/step\n",
            "Score for fold 4: loss of 14.516667366027832\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 5s - loss: 9.2143 - mse: 9.2143 - mae: 1.2314 - val_loss: 5.4061 - val_mse: 5.4061 - val_mae: 1.1592 - lr: 3.2562e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 5s - loss: 8.9296 - mse: 8.9296 - mae: 1.2087 - val_loss: 5.5960 - val_mse: 5.5960 - val_mae: 1.1748 - lr: 3.2562e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 5s - loss: 8.7289 - mse: 8.7289 - mae: 1.1907 - val_loss: 5.5637 - val_mse: 5.5637 - val_mae: 1.1924 - lr: 3.2562e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 5s - loss: 8.4652 - mse: 8.4652 - mae: 1.1749 - val_loss: 5.8597 - val_mse: 5.8597 - val_mae: 1.2126 - lr: 3.2562e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 5s - loss: 8.2718 - mse: 8.2718 - mae: 1.1547 - val_loss: 5.7440 - val_mse: 5.7440 - val_mae: 1.2339 - lr: 3.2562e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 5s - loss: 8.0393 - mse: 8.0393 - mae: 1.1392 - val_loss: 5.8224 - val_mse: 5.8224 - val_mae: 1.2644 - lr: 3.2562e-04 - 5s/epoch - 5ms/step\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-10-12 07:59:11,946]\u001b[0m Finished trial#11 resulted in value: 10.066. Current best value is 10.066 with parameters: {'activation': 'relu', 'num_hidden_layer': 3, 'i': 10, 'learning_rate': 0.00032562293934037016}.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Score for fold 5: loss of 5.822378635406494\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 13.3176 - mse: 13.3176 - mae: 1.5520 - val_loss: 14.7659 - val_mse: 14.7659 - val_mae: 1.5377 - lr: 2.7604e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 12.3919 - mse: 12.3919 - mae: 1.5007 - val_loss: 14.4372 - val_mse: 14.4372 - val_mae: 1.5051 - lr: 2.7604e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 12.1072 - mse: 12.1072 - mae: 1.4850 - val_loss: 14.1361 - val_mse: 14.1361 - val_mae: 1.5188 - lr: 2.7604e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 11.9004 - mse: 11.9004 - mae: 1.4753 - val_loss: 14.1909 - val_mse: 14.1909 - val_mae: 1.4807 - lr: 2.7604e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 11.8209 - mse: 11.8209 - mae: 1.4701 - val_loss: 14.0287 - val_mse: 14.0287 - val_mae: 1.4882 - lr: 2.7604e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 11.7452 - mse: 11.7452 - mae: 1.4622 - val_loss: 13.8530 - val_mse: 13.8530 - val_mae: 1.5214 - lr: 2.7604e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 11.6628 - mse: 11.6628 - mae: 1.4566 - val_loss: 13.7858 - val_mse: 13.7858 - val_mae: 1.4835 - lr: 2.7604e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 2s - loss: 11.6551 - mse: 11.6551 - mae: 1.4520 - val_loss: 13.7109 - val_mse: 13.7109 - val_mae: 1.4811 - lr: 2.7604e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 2s - loss: 11.5228 - mse: 11.5228 - mae: 1.4466 - val_loss: 13.6855 - val_mse: 13.6855 - val_mae: 1.5083 - lr: 2.7604e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 2s - loss: 11.4884 - mse: 11.4884 - mae: 1.4460 - val_loss: 13.6466 - val_mse: 13.6466 - val_mae: 1.4943 - lr: 2.7604e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 2s - loss: 11.4905 - mse: 11.4905 - mae: 1.4432 - val_loss: 13.6140 - val_mse: 13.6140 - val_mae: 1.5024 - lr: 2.7604e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 2s - loss: 11.4105 - mse: 11.4105 - mae: 1.4402 - val_loss: 13.5475 - val_mse: 13.5475 - val_mae: 1.4839 - lr: 2.7604e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 13/100\n",
            "1000/1000 - 2s - loss: 11.4458 - mse: 11.4458 - mae: 1.4393 - val_loss: 13.5441 - val_mse: 13.5441 - val_mae: 1.4799 - lr: 2.7604e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 14/100\n",
            "1000/1000 - 2s - loss: 11.4012 - mse: 11.4012 - mae: 1.4391 - val_loss: 13.6035 - val_mse: 13.6035 - val_mae: 1.4776 - lr: 2.7604e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 15/100\n",
            "1000/1000 - 2s - loss: 11.3545 - mse: 11.3545 - mae: 1.4353 - val_loss: 13.5550 - val_mse: 13.5550 - val_mae: 1.5043 - lr: 2.7604e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 16/100\n",
            "1000/1000 - 2s - loss: 11.2933 - mse: 11.2933 - mae: 1.4334 - val_loss: 13.6976 - val_mse: 13.6976 - val_mae: 1.5010 - lr: 2.7604e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 17/100\n",
            "1000/1000 - 2s - loss: 11.2524 - mse: 11.2524 - mae: 1.4324 - val_loss: 13.5049 - val_mse: 13.5049 - val_mae: 1.5183 - lr: 2.7604e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 18/100\n",
            "1000/1000 - 2s - loss: 11.3058 - mse: 11.3058 - mae: 1.4308 - val_loss: 13.4172 - val_mse: 13.4172 - val_mae: 1.4854 - lr: 2.7604e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 19/100\n",
            "1000/1000 - 2s - loss: 11.2275 - mse: 11.2275 - mae: 1.4285 - val_loss: 13.4905 - val_mse: 13.4905 - val_mae: 1.4809 - lr: 2.7604e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 20/100\n",
            "1000/1000 - 2s - loss: 11.2456 - mse: 11.2456 - mae: 1.4265 - val_loss: 13.5076 - val_mse: 13.5076 - val_mae: 1.4953 - lr: 2.7604e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 21/100\n",
            "1000/1000 - 2s - loss: 11.2008 - mse: 11.2008 - mae: 1.4280 - val_loss: 13.4671 - val_mse: 13.4671 - val_mae: 1.4845 - lr: 2.7604e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 22/100\n",
            "1000/1000 - 2s - loss: 11.2087 - mse: 11.2087 - mae: 1.4278 - val_loss: 13.5052 - val_mse: 13.5052 - val_mae: 1.4898 - lr: 2.7604e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 23/100\n",
            "1000/1000 - 2s - loss: 11.2127 - mse: 11.2127 - mae: 1.4241 - val_loss: 13.4030 - val_mse: 13.4030 - val_mae: 1.4836 - lr: 2.7604e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 24/100\n",
            "1000/1000 - 2s - loss: 11.1933 - mse: 11.1933 - mae: 1.4259 - val_loss: 13.4258 - val_mse: 13.4258 - val_mae: 1.4642 - lr: 2.7604e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 25/100\n",
            "1000/1000 - 2s - loss: 11.1390 - mse: 11.1390 - mae: 1.4232 - val_loss: 13.4670 - val_mse: 13.4670 - val_mae: 1.5115 - lr: 2.7604e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 26/100\n",
            "1000/1000 - 2s - loss: 11.1206 - mse: 11.1206 - mae: 1.4229 - val_loss: 13.4988 - val_mse: 13.4988 - val_mae: 1.4491 - lr: 2.7604e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 27/100\n",
            "1000/1000 - 2s - loss: 11.1429 - mse: 11.1429 - mae: 1.4234 - val_loss: 13.4148 - val_mse: 13.4148 - val_mae: 1.5129 - lr: 2.7604e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 28/100\n",
            "1000/1000 - 2s - loss: 11.1045 - mse: 11.1045 - mae: 1.4200 - val_loss: 13.4185 - val_mse: 13.4185 - val_mae: 1.4769 - lr: 2.7604e-04 - 2s/epoch - 2ms/step\n",
            "Score for fold 1: loss of 13.418463706970215\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 12.5748 - mse: 12.5748 - mae: 1.4409 - val_loss: 7.7057 - val_mse: 7.7057 - val_mae: 1.3701 - lr: 2.7604e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 12.5101 - mse: 12.5101 - mae: 1.4400 - val_loss: 7.7311 - val_mse: 7.7311 - val_mae: 1.3941 - lr: 2.7604e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 12.4453 - mse: 12.4453 - mae: 1.4344 - val_loss: 7.6812 - val_mse: 7.6812 - val_mae: 1.4230 - lr: 2.7604e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 12.4331 - mse: 12.4331 - mae: 1.4363 - val_loss: 7.8441 - val_mse: 7.8441 - val_mae: 1.3877 - lr: 2.7604e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 12.4606 - mse: 12.4606 - mae: 1.4361 - val_loss: 7.7719 - val_mse: 7.7719 - val_mae: 1.4116 - lr: 2.7604e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 12.4046 - mse: 12.4046 - mae: 1.4333 - val_loss: 7.7921 - val_mse: 7.7921 - val_mae: 1.4074 - lr: 2.7604e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 12.4225 - mse: 12.4225 - mae: 1.4349 - val_loss: 7.7877 - val_mse: 7.7877 - val_mae: 1.3662 - lr: 2.7604e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 2s - loss: 12.4659 - mse: 12.4659 - mae: 1.4352 - val_loss: 7.7932 - val_mse: 7.7932 - val_mae: 1.3857 - lr: 2.7604e-04 - 2s/epoch - 2ms/step\n",
            "Score for fold 2: loss of 7.79323148727417\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 11.4220 - mse: 11.4220 - mae: 1.4269 - val_loss: 11.8508 - val_mse: 11.8508 - val_mae: 1.4051 - lr: 2.7604e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 11.3999 - mse: 11.3999 - mae: 1.4249 - val_loss: 11.9827 - val_mse: 11.9827 - val_mae: 1.4023 - lr: 2.7604e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 11.3694 - mse: 11.3694 - mae: 1.4206 - val_loss: 11.8855 - val_mse: 11.8855 - val_mae: 1.3881 - lr: 2.7604e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 11.3738 - mse: 11.3738 - mae: 1.4216 - val_loss: 11.8681 - val_mse: 11.8681 - val_mae: 1.4267 - lr: 2.7604e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 11.3510 - mse: 11.3510 - mae: 1.4190 - val_loss: 11.8780 - val_mse: 11.8780 - val_mae: 1.4449 - lr: 2.7604e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 11.3367 - mse: 11.3367 - mae: 1.4165 - val_loss: 11.9335 - val_mse: 11.9335 - val_mae: 1.4133 - lr: 2.7604e-04 - 2s/epoch - 2ms/step\n",
            "Score for fold 3: loss of 11.933478355407715\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 11.8358 - mse: 11.8358 - mae: 1.4204 - val_loss: 9.8436 - val_mse: 9.8436 - val_mae: 1.4221 - lr: 2.7604e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 11.8009 - mse: 11.8009 - mae: 1.4181 - val_loss: 9.7708 - val_mse: 9.7708 - val_mae: 1.4475 - lr: 2.7604e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 11.7721 - mse: 11.7721 - mae: 1.4209 - val_loss: 9.9378 - val_mse: 9.9378 - val_mae: 1.4306 - lr: 2.7604e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 11.7798 - mse: 11.7798 - mae: 1.4122 - val_loss: 9.7376 - val_mse: 9.7376 - val_mae: 1.4619 - lr: 2.7604e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 11.7388 - mse: 11.7388 - mae: 1.4172 - val_loss: 9.9650 - val_mse: 9.9650 - val_mae: 1.4194 - lr: 2.7604e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 11.7495 - mse: 11.7495 - mae: 1.4107 - val_loss: 9.9159 - val_mse: 9.9159 - val_mae: 1.4201 - lr: 2.7604e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 11.7205 - mse: 11.7205 - mae: 1.4133 - val_loss: 9.8784 - val_mse: 9.8784 - val_mae: 1.4429 - lr: 2.7604e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 2s - loss: 11.6944 - mse: 11.6944 - mae: 1.4100 - val_loss: 9.8237 - val_mse: 9.8237 - val_mae: 1.4551 - lr: 2.7604e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 2s - loss: 11.6892 - mse: 11.6892 - mae: 1.4108 - val_loss: 9.9669 - val_mse: 9.9669 - val_mae: 1.4757 - lr: 2.7604e-04 - 2s/epoch - 2ms/step\n",
            "Score for fold 4: loss of 9.966919898986816\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 10.5470 - mse: 10.5470 - mae: 1.4131 - val_loss: 14.6050 - val_mse: 14.6050 - val_mae: 1.4077 - lr: 2.7604e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 10.5081 - mse: 10.5081 - mae: 1.4109 - val_loss: 14.6279 - val_mse: 14.6279 - val_mae: 1.4724 - lr: 2.7604e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 10.4383 - mse: 10.4383 - mae: 1.4096 - val_loss: 14.6892 - val_mse: 14.6892 - val_mae: 1.4052 - lr: 2.7604e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 10.4610 - mse: 10.4610 - mae: 1.4105 - val_loss: 14.6243 - val_mse: 14.6243 - val_mae: 1.4295 - lr: 2.7604e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 10.4056 - mse: 10.4056 - mae: 1.4095 - val_loss: 14.6611 - val_mse: 14.6611 - val_mae: 1.4408 - lr: 2.7604e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 10.4038 - mse: 10.4038 - mae: 1.4074 - val_loss: 14.7168 - val_mse: 14.7168 - val_mae: 1.4489 - lr: 2.7604e-04 - 2s/epoch - 2ms/step\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-10-12 08:00:53,399]\u001b[0m Finished trial#12 resulted in value: 11.565999999999999. Current best value is 10.066 with parameters: {'activation': 'relu', 'num_hidden_layer': 3, 'i': 10, 'learning_rate': 0.00032562293934037016}.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Score for fold 5: loss of 14.716832160949707\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 10s - loss: 15.4237 - mse: 15.4237 - mae: 1.6353 - val_loss: 10.1946 - val_mse: 10.1946 - val_mae: 1.5310 - lr: 2.3400e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 9s - loss: 14.7967 - mse: 14.7967 - mae: 1.5521 - val_loss: 9.5483 - val_mse: 9.5483 - val_mae: 1.4518 - lr: 2.3400e-04 - 9s/epoch - 9ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 9s - loss: 14.5370 - mse: 14.5370 - mae: 1.5376 - val_loss: 9.3151 - val_mse: 9.3151 - val_mae: 1.5505 - lr: 2.3400e-04 - 9s/epoch - 9ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 9s - loss: 14.3767 - mse: 14.3767 - mae: 1.5289 - val_loss: 9.6449 - val_mse: 9.6449 - val_mae: 1.5086 - lr: 2.3400e-04 - 9s/epoch - 9ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 9s - loss: 14.3280 - mse: 14.3280 - mae: 1.5190 - val_loss: 9.2564 - val_mse: 9.2564 - val_mae: 1.4783 - lr: 2.3400e-04 - 9s/epoch - 9ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 9s - loss: 14.2515 - mse: 14.2515 - mae: 1.5171 - val_loss: 9.2666 - val_mse: 9.2666 - val_mae: 1.4633 - lr: 2.3400e-04 - 9s/epoch - 9ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 9s - loss: 14.0499 - mse: 14.0499 - mae: 1.5050 - val_loss: 9.1568 - val_mse: 9.1568 - val_mae: 1.5169 - lr: 2.3400e-04 - 9s/epoch - 9ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 9s - loss: 14.0328 - mse: 14.0328 - mae: 1.5041 - val_loss: 9.1514 - val_mse: 9.1514 - val_mae: 1.5170 - lr: 2.3400e-04 - 9s/epoch - 9ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 9s - loss: 13.9204 - mse: 13.9204 - mae: 1.4947 - val_loss: 9.1185 - val_mse: 9.1185 - val_mae: 1.4499 - lr: 2.3400e-04 - 9s/epoch - 9ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 9s - loss: 13.7673 - mse: 13.7673 - mae: 1.4904 - val_loss: 9.1174 - val_mse: 9.1174 - val_mae: 1.3996 - lr: 2.3400e-04 - 9s/epoch - 9ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 9s - loss: 13.7130 - mse: 13.7130 - mae: 1.4834 - val_loss: 9.0073 - val_mse: 9.0073 - val_mae: 1.4524 - lr: 2.3400e-04 - 9s/epoch - 9ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 9s - loss: 13.6626 - mse: 13.6626 - mae: 1.4774 - val_loss: 9.1706 - val_mse: 9.1706 - val_mae: 1.6098 - lr: 2.3400e-04 - 9s/epoch - 9ms/step\n",
            "Epoch 13/100\n",
            "1000/1000 - 9s - loss: 13.5882 - mse: 13.5882 - mae: 1.4751 - val_loss: 8.9306 - val_mse: 8.9306 - val_mae: 1.4828 - lr: 2.3400e-04 - 9s/epoch - 9ms/step\n",
            "Epoch 14/100\n",
            "1000/1000 - 9s - loss: 13.4460 - mse: 13.4460 - mae: 1.4743 - val_loss: 8.8675 - val_mse: 8.8675 - val_mae: 1.4287 - lr: 2.3400e-04 - 9s/epoch - 9ms/step\n",
            "Epoch 15/100\n",
            "1000/1000 - 9s - loss: 13.3333 - mse: 13.3333 - mae: 1.4626 - val_loss: 8.9183 - val_mse: 8.9183 - val_mae: 1.4508 - lr: 2.3400e-04 - 9s/epoch - 9ms/step\n",
            "Epoch 16/100\n",
            "1000/1000 - 9s - loss: 13.2942 - mse: 13.2942 - mae: 1.4574 - val_loss: 8.8025 - val_mse: 8.8025 - val_mae: 1.4382 - lr: 2.3400e-04 - 9s/epoch - 9ms/step\n",
            "Epoch 17/100\n",
            "1000/1000 - 9s - loss: 13.1385 - mse: 13.1385 - mae: 1.4505 - val_loss: 8.9520 - val_mse: 8.9520 - val_mae: 1.4077 - lr: 2.3400e-04 - 9s/epoch - 9ms/step\n",
            "Epoch 18/100\n",
            "1000/1000 - 9s - loss: 12.9926 - mse: 12.9926 - mae: 1.4456 - val_loss: 8.8847 - val_mse: 8.8847 - val_mae: 1.5190 - lr: 2.3400e-04 - 9s/epoch - 9ms/step\n",
            "Epoch 19/100\n",
            "1000/1000 - 9s - loss: 12.8685 - mse: 12.8685 - mae: 1.4331 - val_loss: 8.7929 - val_mse: 8.7929 - val_mae: 1.4630 - lr: 2.3400e-04 - 9s/epoch - 9ms/step\n",
            "Epoch 20/100\n",
            "1000/1000 - 9s - loss: 12.7912 - mse: 12.7912 - mae: 1.4290 - val_loss: 8.8307 - val_mse: 8.8307 - val_mae: 1.4595 - lr: 2.3400e-04 - 9s/epoch - 9ms/step\n",
            "Epoch 21/100\n",
            "1000/1000 - 10s - loss: 12.6248 - mse: 12.6248 - mae: 1.4187 - val_loss: 8.8413 - val_mse: 8.8413 - val_mae: 1.4690 - lr: 2.3400e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 22/100\n",
            "1000/1000 - 10s - loss: 12.5244 - mse: 12.5244 - mae: 1.4143 - val_loss: 8.7484 - val_mse: 8.7484 - val_mae: 1.4432 - lr: 2.3400e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 23/100\n",
            "1000/1000 - 11s - loss: 12.3142 - mse: 12.3142 - mae: 1.4000 - val_loss: 8.7951 - val_mse: 8.7951 - val_mae: 1.4220 - lr: 2.3400e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 24/100\n",
            "1000/1000 - 10s - loss: 12.1262 - mse: 12.1262 - mae: 1.3917 - val_loss: 8.6501 - val_mse: 8.6501 - val_mae: 1.4837 - lr: 2.3400e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 25/100\n",
            "1000/1000 - 10s - loss: 11.8845 - mse: 11.8845 - mae: 1.3788 - val_loss: 8.5666 - val_mse: 8.5666 - val_mae: 1.5317 - lr: 2.3400e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 26/100\n",
            "1000/1000 - 10s - loss: 11.7178 - mse: 11.7178 - mae: 1.3611 - val_loss: 8.8758 - val_mse: 8.8758 - val_mae: 1.4814 - lr: 2.3400e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 27/100\n",
            "1000/1000 - 10s - loss: 11.4694 - mse: 11.4694 - mae: 1.3508 - val_loss: 8.9970 - val_mse: 8.9970 - val_mae: 1.4739 - lr: 2.3400e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 28/100\n",
            "1000/1000 - 10s - loss: 11.2521 - mse: 11.2521 - mae: 1.3329 - val_loss: 8.7261 - val_mse: 8.7261 - val_mae: 1.5693 - lr: 2.3400e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 29/100\n",
            "1000/1000 - 10s - loss: 10.9168 - mse: 10.9168 - mae: 1.3188 - val_loss: 8.9365 - val_mse: 8.9365 - val_mae: 1.4932 - lr: 2.3400e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 30/100\n",
            "1000/1000 - 10s - loss: 10.7545 - mse: 10.7545 - mae: 1.3045 - val_loss: 8.8624 - val_mse: 8.8624 - val_mae: 1.6829 - lr: 2.3400e-04 - 10s/epoch - 10ms/step\n",
            "Score for fold 1: loss of 8.862404823303223\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 10s - loss: 8.2593 - mse: 8.2593 - mae: 1.3544 - val_loss: 18.5289 - val_mse: 18.5289 - val_mae: 1.3103 - lr: 2.3400e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 10s - loss: 7.9338 - mse: 7.9338 - mae: 1.3299 - val_loss: 19.0106 - val_mse: 19.0106 - val_mae: 1.4282 - lr: 2.3400e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 10s - loss: 7.6861 - mse: 7.6861 - mae: 1.3117 - val_loss: 18.9851 - val_mse: 18.9851 - val_mae: 1.4504 - lr: 2.3400e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 10s - loss: 7.4210 - mse: 7.4210 - mae: 1.2736 - val_loss: 18.9733 - val_mse: 18.9733 - val_mae: 1.4482 - lr: 2.3400e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 10s - loss: 6.9684 - mse: 6.9684 - mae: 1.2547 - val_loss: 18.8601 - val_mse: 18.8601 - val_mae: 1.4176 - lr: 2.3400e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 10s - loss: 6.7785 - mse: 6.7785 - mae: 1.2303 - val_loss: 19.1031 - val_mse: 19.1031 - val_mae: 1.4208 - lr: 2.3400e-04 - 10s/epoch - 10ms/step\n",
            "Score for fold 2: loss of 19.10308837890625\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 9s - loss: 10.0259 - mse: 10.0259 - mae: 1.2809 - val_loss: 5.2047 - val_mse: 5.2047 - val_mae: 1.2347 - lr: 2.3400e-04 - 9s/epoch - 9ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 9s - loss: 9.6127 - mse: 9.6127 - mae: 1.2405 - val_loss: 5.1367 - val_mse: 5.1367 - val_mae: 1.1858 - lr: 2.3400e-04 - 9s/epoch - 9ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 10s - loss: 9.1713 - mse: 9.1713 - mae: 1.2009 - val_loss: 5.2284 - val_mse: 5.2284 - val_mae: 1.2759 - lr: 2.3400e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 9s - loss: 8.8700 - mse: 8.8700 - mae: 1.1708 - val_loss: 5.3330 - val_mse: 5.3330 - val_mae: 1.2521 - lr: 2.3400e-04 - 9s/epoch - 9ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 9s - loss: 8.4831 - mse: 8.4831 - mae: 1.1366 - val_loss: 5.4503 - val_mse: 5.4503 - val_mae: 1.2867 - lr: 2.3400e-04 - 9s/epoch - 9ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 10s - loss: 8.2331 - mse: 8.2331 - mae: 1.0998 - val_loss: 5.4159 - val_mse: 5.4159 - val_mae: 1.2327 - lr: 2.3400e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 10s - loss: 7.8206 - mse: 7.8206 - mae: 1.0648 - val_loss: 5.5712 - val_mse: 5.5712 - val_mae: 1.2973 - lr: 2.3400e-04 - 10s/epoch - 10ms/step\n",
            "Score for fold 3: loss of 5.571246147155762\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 10s - loss: 7.6737 - mse: 7.6737 - mae: 1.1244 - val_loss: 5.6791 - val_mse: 5.6791 - val_mae: 1.0420 - lr: 2.3400e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 10s - loss: 7.3356 - mse: 7.3356 - mae: 1.0817 - val_loss: 5.8774 - val_mse: 5.8774 - val_mae: 1.0609 - lr: 2.3400e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 10s - loss: 6.9048 - mse: 6.9048 - mae: 1.0399 - val_loss: 5.9938 - val_mse: 5.9938 - val_mae: 1.0609 - lr: 2.3400e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 9s - loss: 6.7096 - mse: 6.7096 - mae: 1.0010 - val_loss: 5.9668 - val_mse: 5.9668 - val_mae: 1.0728 - lr: 2.3400e-04 - 9s/epoch - 9ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 9s - loss: 6.4064 - mse: 6.4064 - mae: 0.9632 - val_loss: 6.2167 - val_mse: 6.2167 - val_mae: 1.1152 - lr: 2.3400e-04 - 9s/epoch - 9ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 9s - loss: 6.0952 - mse: 6.0952 - mae: 0.9347 - val_loss: 6.2311 - val_mse: 6.2311 - val_mae: 1.1188 - lr: 2.3400e-04 - 9s/epoch - 9ms/step\n",
            "Score for fold 4: loss of 6.231084823608398\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 9s - loss: 6.6302 - mse: 6.6302 - mae: 0.9921 - val_loss: 3.7417 - val_mse: 3.7417 - val_mae: 0.8867 - lr: 2.3400e-04 - 9s/epoch - 9ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 9s - loss: 6.3089 - mse: 6.3089 - mae: 0.9413 - val_loss: 4.1311 - val_mse: 4.1311 - val_mae: 0.9694 - lr: 2.3400e-04 - 9s/epoch - 9ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 9s - loss: 6.0619 - mse: 6.0619 - mae: 0.9011 - val_loss: 3.9695 - val_mse: 3.9695 - val_mae: 0.9312 - lr: 2.3400e-04 - 9s/epoch - 9ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 9s - loss: 6.0580 - mse: 6.0580 - mae: 0.8734 - val_loss: 3.8637 - val_mse: 3.8637 - val_mae: 0.9592 - lr: 2.3400e-04 - 9s/epoch - 9ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 10s - loss: 5.6612 - mse: 5.6612 - mae: 0.8430 - val_loss: 4.1306 - val_mse: 4.1306 - val_mae: 0.9951 - lr: 2.3400e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 9s - loss: 5.4679 - mse: 5.4679 - mae: 0.8198 - val_loss: 4.3716 - val_mse: 4.3716 - val_mae: 0.9996 - lr: 2.3400e-04 - 9s/epoch - 9ms/step\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-10-12 08:09:42,874]\u001b[0m Finished trial#13 resulted in value: 8.826. Current best value is 8.826 with parameters: {'activation': 'tanh', 'num_hidden_layer': 4, 'i': 10, 'learning_rate': 0.00023399735711621326}.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Score for fold 5: loss of 4.3716044425964355\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 7s - loss: 14.6432 - mse: 14.6432 - mae: 1.6171 - val_loss: 12.3812 - val_mse: 12.3812 - val_mae: 1.5597 - lr: 3.6475e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 6s - loss: 13.9637 - mse: 13.9637 - mae: 1.5259 - val_loss: 12.1847 - val_mse: 12.1847 - val_mae: 1.5445 - lr: 3.6475e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 6s - loss: 13.7759 - mse: 13.7759 - mae: 1.5082 - val_loss: 12.3544 - val_mse: 12.3544 - val_mae: 1.6130 - lr: 3.6475e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 6s - loss: 13.6319 - mse: 13.6319 - mae: 1.5027 - val_loss: 11.9667 - val_mse: 11.9667 - val_mae: 1.5003 - lr: 3.6475e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 6s - loss: 13.5497 - mse: 13.5497 - mae: 1.4901 - val_loss: 11.8658 - val_mse: 11.8658 - val_mae: 1.4899 - lr: 3.6475e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 6s - loss: 13.4375 - mse: 13.4375 - mae: 1.4852 - val_loss: 11.8379 - val_mse: 11.8379 - val_mae: 1.4780 - lr: 3.6475e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 6s - loss: 13.4138 - mse: 13.4138 - mae: 1.4822 - val_loss: 11.9344 - val_mse: 11.9344 - val_mae: 1.4588 - lr: 3.6475e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 6s - loss: 13.2983 - mse: 13.2983 - mae: 1.4720 - val_loss: 11.7192 - val_mse: 11.7192 - val_mae: 1.5628 - lr: 3.6475e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 6s - loss: 13.2364 - mse: 13.2364 - mae: 1.4704 - val_loss: 11.8644 - val_mse: 11.8644 - val_mae: 1.4567 - lr: 3.6475e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 6s - loss: 13.1504 - mse: 13.1504 - mae: 1.4644 - val_loss: 11.6386 - val_mse: 11.6386 - val_mae: 1.5445 - lr: 3.6475e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 6s - loss: 13.0296 - mse: 13.0296 - mae: 1.4644 - val_loss: 11.5382 - val_mse: 11.5382 - val_mae: 1.4882 - lr: 3.6475e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 6s - loss: 12.9735 - mse: 12.9735 - mae: 1.4594 - val_loss: 11.5167 - val_mse: 11.5167 - val_mae: 1.4740 - lr: 3.6475e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 13/100\n",
            "1000/1000 - 6s - loss: 12.8945 - mse: 12.8945 - mae: 1.4544 - val_loss: 11.5604 - val_mse: 11.5604 - val_mae: 1.5531 - lr: 3.6475e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 14/100\n",
            "1000/1000 - 6s - loss: 12.7666 - mse: 12.7666 - mae: 1.4480 - val_loss: 11.4552 - val_mse: 11.4552 - val_mae: 1.4907 - lr: 3.6475e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 15/100\n",
            "1000/1000 - 6s - loss: 12.6524 - mse: 12.6524 - mae: 1.4432 - val_loss: 11.3608 - val_mse: 11.3608 - val_mae: 1.5463 - lr: 3.6475e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 16/100\n",
            "1000/1000 - 6s - loss: 12.5932 - mse: 12.5932 - mae: 1.4443 - val_loss: 11.3606 - val_mse: 11.3606 - val_mae: 1.5175 - lr: 3.6475e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 17/100\n",
            "1000/1000 - 6s - loss: 12.4648 - mse: 12.4648 - mae: 1.4357 - val_loss: 11.3691 - val_mse: 11.3691 - val_mae: 1.4932 - lr: 3.6475e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 18/100\n",
            "1000/1000 - 6s - loss: 12.3448 - mse: 12.3448 - mae: 1.4314 - val_loss: 11.5359 - val_mse: 11.5359 - val_mae: 1.5401 - lr: 3.6475e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 19/100\n",
            "1000/1000 - 6s - loss: 12.2333 - mse: 12.2333 - mae: 1.4254 - val_loss: 11.4395 - val_mse: 11.4395 - val_mae: 1.5644 - lr: 3.6475e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 20/100\n",
            "1000/1000 - 6s - loss: 12.1275 - mse: 12.1275 - mae: 1.4164 - val_loss: 11.3128 - val_mse: 11.3128 - val_mae: 1.5149 - lr: 3.6475e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 21/100\n",
            "1000/1000 - 6s - loss: 12.0658 - mse: 12.0658 - mae: 1.4156 - val_loss: 11.1738 - val_mse: 11.1738 - val_mae: 1.5210 - lr: 3.6475e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 22/100\n",
            "1000/1000 - 6s - loss: 11.8992 - mse: 11.8992 - mae: 1.4091 - val_loss: 11.5149 - val_mse: 11.5149 - val_mae: 1.6317 - lr: 3.6475e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 23/100\n",
            "1000/1000 - 6s - loss: 11.8380 - mse: 11.8380 - mae: 1.4043 - val_loss: 11.2225 - val_mse: 11.2225 - val_mae: 1.5376 - lr: 3.6475e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 24/100\n",
            "1000/1000 - 6s - loss: 11.7092 - mse: 11.7092 - mae: 1.4005 - val_loss: 11.1549 - val_mse: 11.1549 - val_mae: 1.5306 - lr: 3.6475e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 25/100\n",
            "1000/1000 - 6s - loss: 11.6080 - mse: 11.6080 - mae: 1.3948 - val_loss: 11.3956 - val_mse: 11.3956 - val_mae: 1.6559 - lr: 3.6475e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 26/100\n",
            "1000/1000 - 6s - loss: 11.4533 - mse: 11.4533 - mae: 1.3902 - val_loss: 11.2333 - val_mse: 11.2333 - val_mae: 1.5570 - lr: 3.6475e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 27/100\n",
            "1000/1000 - 6s - loss: 11.3750 - mse: 11.3750 - mae: 1.3926 - val_loss: 11.2545 - val_mse: 11.2545 - val_mae: 1.5581 - lr: 3.6475e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 28/100\n",
            "1000/1000 - 6s - loss: 11.3009 - mse: 11.3009 - mae: 1.3841 - val_loss: 11.1522 - val_mse: 11.1522 - val_mae: 1.5367 - lr: 3.6475e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 29/100\n",
            "1000/1000 - 6s - loss: 11.2093 - mse: 11.2093 - mae: 1.3811 - val_loss: 11.1790 - val_mse: 11.1790 - val_mae: 1.5845 - lr: 3.6475e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 30/100\n",
            "1000/1000 - 6s - loss: 10.9930 - mse: 10.9930 - mae: 1.3740 - val_loss: 11.4548 - val_mse: 11.4548 - val_mae: 1.6247 - lr: 3.6475e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 31/100\n",
            "1000/1000 - 6s - loss: 10.9496 - mse: 10.9496 - mae: 1.3680 - val_loss: 11.3130 - val_mse: 11.3130 - val_mae: 1.6320 - lr: 3.6475e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 32/100\n",
            "1000/1000 - 6s - loss: 10.8009 - mse: 10.8009 - mae: 1.3625 - val_loss: 11.2065 - val_mse: 11.2065 - val_mae: 1.6069 - lr: 3.6475e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 33/100\n",
            "1000/1000 - 6s - loss: 10.7612 - mse: 10.7612 - mae: 1.3623 - val_loss: 11.1653 - val_mse: 11.1653 - val_mae: 1.6114 - lr: 3.6475e-04 - 6s/epoch - 6ms/step\n",
            "Score for fold 1: loss of 11.165260314941406\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 6s - loss: 10.5277 - mse: 10.5277 - mae: 1.4234 - val_loss: 12.4302 - val_mse: 12.4302 - val_mae: 1.3942 - lr: 3.6475e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 6s - loss: 10.2979 - mse: 10.2979 - mae: 1.4017 - val_loss: 12.5203 - val_mse: 12.5203 - val_mae: 1.4010 - lr: 3.6475e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 6s - loss: 10.1935 - mse: 10.1935 - mae: 1.3980 - val_loss: 12.7448 - val_mse: 12.7448 - val_mae: 1.4544 - lr: 3.6475e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 6s - loss: 10.0357 - mse: 10.0357 - mae: 1.3848 - val_loss: 12.6878 - val_mse: 12.6878 - val_mae: 1.4708 - lr: 3.6475e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 6s - loss: 9.8925 - mse: 9.8925 - mae: 1.3739 - val_loss: 12.8038 - val_mse: 12.8038 - val_mae: 1.4356 - lr: 3.6475e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 6s - loss: 9.7468 - mse: 9.7468 - mae: 1.3673 - val_loss: 13.0311 - val_mse: 13.0311 - val_mae: 1.5600 - lr: 3.6475e-04 - 6s/epoch - 6ms/step\n",
            "Score for fold 2: loss of 13.031113624572754\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 5s - loss: 10.9483 - mse: 10.9483 - mae: 1.4061 - val_loss: 7.7233 - val_mse: 7.7233 - val_mae: 1.3059 - lr: 3.6475e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 5s - loss: 10.8199 - mse: 10.8199 - mae: 1.3893 - val_loss: 7.9373 - val_mse: 7.9373 - val_mae: 1.3311 - lr: 3.6475e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 6s - loss: 10.6037 - mse: 10.6037 - mae: 1.3766 - val_loss: 8.3682 - val_mse: 8.3682 - val_mae: 1.3708 - lr: 3.6475e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 5s - loss: 10.5090 - mse: 10.5090 - mae: 1.3686 - val_loss: 8.0461 - val_mse: 8.0461 - val_mae: 1.4432 - lr: 3.6475e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 6s - loss: 10.3850 - mse: 10.3850 - mae: 1.3611 - val_loss: 8.1286 - val_mse: 8.1286 - val_mae: 1.4299 - lr: 3.6475e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 6s - loss: 10.2645 - mse: 10.2645 - mae: 1.3617 - val_loss: 8.9796 - val_mse: 8.9796 - val_mae: 1.4880 - lr: 3.6475e-04 - 6s/epoch - 6ms/step\n",
            "Score for fold 3: loss of 8.979604721069336\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 6s - loss: 8.6822 - mse: 8.6822 - mae: 1.3799 - val_loss: 14.9414 - val_mse: 14.9414 - val_mae: 1.3404 - lr: 3.6475e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 6s - loss: 8.4530 - mse: 8.4530 - mae: 1.3648 - val_loss: 15.0950 - val_mse: 15.0950 - val_mae: 1.3651 - lr: 3.6475e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 6s - loss: 8.2998 - mse: 8.2998 - mae: 1.3544 - val_loss: 15.1086 - val_mse: 15.1086 - val_mae: 1.3465 - lr: 3.6475e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 6s - loss: 8.1807 - mse: 8.1807 - mae: 1.3484 - val_loss: 15.4365 - val_mse: 15.4365 - val_mae: 1.4125 - lr: 3.6475e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 6s - loss: 8.1088 - mse: 8.1088 - mae: 1.3418 - val_loss: 15.3996 - val_mse: 15.3996 - val_mae: 1.4024 - lr: 3.6475e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 7s - loss: 7.9293 - mse: 7.9293 - mae: 1.3315 - val_loss: 15.3585 - val_mse: 15.3585 - val_mae: 1.4211 - lr: 3.6475e-04 - 7s/epoch - 7ms/step\n",
            "Score for fold 4: loss of 15.358511924743652\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 7s - loss: 10.3262 - mse: 10.3262 - mae: 1.3742 - val_loss: 5.6135 - val_mse: 5.6135 - val_mae: 1.2668 - lr: 3.6475e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 7s - loss: 10.1244 - mse: 10.1244 - mae: 1.3537 - val_loss: 5.9875 - val_mse: 5.9875 - val_mae: 1.3453 - lr: 3.6475e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 7s - loss: 9.9690 - mse: 9.9690 - mae: 1.3423 - val_loss: 6.0828 - val_mse: 6.0828 - val_mae: 1.3617 - lr: 3.6475e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 7s - loss: 9.8914 - mse: 9.8914 - mae: 1.3334 - val_loss: 6.0567 - val_mse: 6.0567 - val_mae: 1.3624 - lr: 3.6475e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 7s - loss: 9.7493 - mse: 9.7493 - mae: 1.3293 - val_loss: 6.2203 - val_mse: 6.2203 - val_mae: 1.3671 - lr: 3.6475e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 7s - loss: 9.5916 - mse: 9.5916 - mae: 1.3192 - val_loss: 6.2030 - val_mse: 6.2030 - val_mae: 1.4187 - lr: 3.6475e-04 - 7s/epoch - 7ms/step\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-10-12 08:15:21,707]\u001b[0m Finished trial#14 resulted in value: 10.948. Current best value is 8.826 with parameters: {'activation': 'tanh', 'num_hidden_layer': 4, 'i': 10, 'learning_rate': 0.00023399735711621326}.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Score for fold 5: loss of 6.202962875366211\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 11s - loss: 14.8569 - mse: 14.8569 - mae: 1.6305 - val_loss: 12.6309 - val_mse: 12.6309 - val_mae: 1.5471 - lr: 3.0508e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 10s - loss: 14.1294 - mse: 14.1294 - mae: 1.5430 - val_loss: 12.6918 - val_mse: 12.6918 - val_mae: 1.5503 - lr: 3.0508e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 10s - loss: 14.0144 - mse: 14.0144 - mae: 1.5318 - val_loss: 12.4113 - val_mse: 12.4113 - val_mae: 1.5467 - lr: 3.0508e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 10s - loss: 13.8810 - mse: 13.8810 - mae: 1.5209 - val_loss: 12.5191 - val_mse: 12.5191 - val_mae: 1.4599 - lr: 3.0508e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 10s - loss: 13.7353 - mse: 13.7353 - mae: 1.5096 - val_loss: 12.4563 - val_mse: 12.4563 - val_mae: 1.6677 - lr: 3.0508e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 10s - loss: 13.6267 - mse: 13.6267 - mae: 1.4976 - val_loss: 12.1366 - val_mse: 12.1366 - val_mae: 1.4575 - lr: 3.0508e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 10s - loss: 13.5132 - mse: 13.5132 - mae: 1.4880 - val_loss: 11.9970 - val_mse: 11.9970 - val_mae: 1.5698 - lr: 3.0508e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 10s - loss: 13.3968 - mse: 13.3968 - mae: 1.4862 - val_loss: 11.9727 - val_mse: 11.9727 - val_mae: 1.4636 - lr: 3.0508e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 10s - loss: 13.3317 - mse: 13.3317 - mae: 1.4797 - val_loss: 11.8451 - val_mse: 11.8451 - val_mae: 1.5572 - lr: 3.0508e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 10s - loss: 13.3570 - mse: 13.3570 - mae: 1.4780 - val_loss: 11.8283 - val_mse: 11.8283 - val_mae: 1.5561 - lr: 3.0508e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 10s - loss: 13.2143 - mse: 13.2143 - mae: 1.4736 - val_loss: 11.7449 - val_mse: 11.7449 - val_mae: 1.4729 - lr: 3.0508e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 10s - loss: 13.1440 - mse: 13.1440 - mae: 1.4735 - val_loss: 11.6372 - val_mse: 11.6372 - val_mae: 1.5125 - lr: 3.0508e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 13/100\n",
            "1000/1000 - 10s - loss: 13.0490 - mse: 13.0490 - mae: 1.4694 - val_loss: 11.7715 - val_mse: 11.7715 - val_mae: 1.5110 - lr: 3.0508e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 14/100\n",
            "1000/1000 - 10s - loss: 12.9294 - mse: 12.9294 - mae: 1.4636 - val_loss: 11.9211 - val_mse: 11.9211 - val_mae: 1.4541 - lr: 3.0508e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 15/100\n",
            "1000/1000 - 10s - loss: 12.9408 - mse: 12.9408 - mae: 1.4626 - val_loss: 11.6478 - val_mse: 11.6478 - val_mae: 1.4647 - lr: 3.0508e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 16/100\n",
            "1000/1000 - 10s - loss: 12.8680 - mse: 12.8680 - mae: 1.4619 - val_loss: 11.3765 - val_mse: 11.3765 - val_mae: 1.4630 - lr: 3.0508e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 17/100\n",
            "1000/1000 - 10s - loss: 12.8100 - mse: 12.8100 - mae: 1.4607 - val_loss: 11.4135 - val_mse: 11.4135 - val_mae: 1.4456 - lr: 3.0508e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 18/100\n",
            "1000/1000 - 10s - loss: 12.6725 - mse: 12.6725 - mae: 1.4542 - val_loss: 11.5302 - val_mse: 11.5302 - val_mae: 1.5071 - lr: 3.0508e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 19/100\n",
            "1000/1000 - 10s - loss: 12.6441 - mse: 12.6441 - mae: 1.4521 - val_loss: 11.6258 - val_mse: 11.6258 - val_mae: 1.4092 - lr: 3.0508e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 20/100\n",
            "1000/1000 - 10s - loss: 12.5647 - mse: 12.5647 - mae: 1.4509 - val_loss: 11.4733 - val_mse: 11.4733 - val_mae: 1.6359 - lr: 3.0508e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 21/100\n",
            "1000/1000 - 11s - loss: 12.4514 - mse: 12.4514 - mae: 1.4488 - val_loss: 11.7270 - val_mse: 11.7270 - val_mae: 1.3971 - lr: 3.0508e-04 - 11s/epoch - 11ms/step\n",
            "Score for fold 1: loss of 11.726972579956055\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 11s - loss: 12.8948 - mse: 12.8948 - mae: 1.4612 - val_loss: 9.4481 - val_mse: 9.4481 - val_mae: 1.5208 - lr: 3.0508e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 11s - loss: 12.7375 - mse: 12.7375 - mae: 1.4551 - val_loss: 9.7931 - val_mse: 9.7931 - val_mae: 1.4434 - lr: 3.0508e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 11s - loss: 12.6151 - mse: 12.6151 - mae: 1.4478 - val_loss: 10.0908 - val_mse: 10.0908 - val_mae: 1.6228 - lr: 3.0508e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 10s - loss: 12.5812 - mse: 12.5812 - mae: 1.4548 - val_loss: 9.7467 - val_mse: 9.7467 - val_mae: 1.4877 - lr: 3.0508e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 10s - loss: 12.3685 - mse: 12.3685 - mae: 1.4396 - val_loss: 9.6061 - val_mse: 9.6061 - val_mae: 1.3826 - lr: 3.0508e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 10s - loss: 12.2135 - mse: 12.2135 - mae: 1.4326 - val_loss: 9.7357 - val_mse: 9.7357 - val_mae: 1.5849 - lr: 3.0508e-04 - 10s/epoch - 10ms/step\n",
            "Score for fold 2: loss of 9.735715866088867\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 10s - loss: 9.8014 - mse: 9.8014 - mae: 1.4274 - val_loss: 19.5769 - val_mse: 19.5769 - val_mae: 1.5510 - lr: 3.0508e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 10s - loss: 9.6574 - mse: 9.6574 - mae: 1.4231 - val_loss: 19.7272 - val_mse: 19.7272 - val_mae: 1.5696 - lr: 3.0508e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 10s - loss: 9.3828 - mse: 9.3828 - mae: 1.4104 - val_loss: 19.7009 - val_mse: 19.7009 - val_mae: 1.4659 - lr: 3.0508e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 10s - loss: 9.4049 - mse: 9.4049 - mae: 1.4043 - val_loss: 19.5607 - val_mse: 19.5607 - val_mae: 1.4943 - lr: 3.0508e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 9s - loss: 9.0648 - mse: 9.0648 - mae: 1.3910 - val_loss: 19.6194 - val_mse: 19.6194 - val_mae: 1.4949 - lr: 3.0508e-04 - 9s/epoch - 9ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 9s - loss: 8.9844 - mse: 8.9844 - mae: 1.3729 - val_loss: 19.6017 - val_mse: 19.6017 - val_mae: 1.6186 - lr: 3.0508e-04 - 9s/epoch - 9ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 9s - loss: 8.6162 - mse: 8.6162 - mae: 1.3634 - val_loss: 19.7478 - val_mse: 19.7478 - val_mae: 1.4304 - lr: 3.0508e-04 - 9s/epoch - 9ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 9s - loss: 8.5426 - mse: 8.5426 - mae: 1.3473 - val_loss: 19.7926 - val_mse: 19.7926 - val_mae: 1.4647 - lr: 3.0508e-04 - 9s/epoch - 9ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 9s - loss: 8.3837 - mse: 8.3837 - mae: 1.3378 - val_loss: 19.6080 - val_mse: 19.6080 - val_mae: 1.5818 - lr: 3.0508e-04 - 9s/epoch - 9ms/step\n",
            "Score for fold 3: loss of 19.6080265045166\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 9s - loss: 10.8137 - mse: 10.8137 - mae: 1.3694 - val_loss: 9.2672 - val_mse: 9.2672 - val_mae: 1.3761 - lr: 3.0508e-04 - 9s/epoch - 9ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 9s - loss: 10.4570 - mse: 10.4570 - mae: 1.3529 - val_loss: 9.3760 - val_mse: 9.3760 - val_mae: 1.3547 - lr: 3.0508e-04 - 9s/epoch - 9ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 9s - loss: 10.2964 - mse: 10.2964 - mae: 1.3341 - val_loss: 9.9109 - val_mse: 9.9109 - val_mae: 1.3449 - lr: 3.0508e-04 - 9s/epoch - 9ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 9s - loss: 9.9380 - mse: 9.9380 - mae: 1.3078 - val_loss: 9.5952 - val_mse: 9.5952 - val_mae: 1.4392 - lr: 3.0508e-04 - 9s/epoch - 9ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 9s - loss: 9.7210 - mse: 9.7210 - mae: 1.2931 - val_loss: 9.7103 - val_mse: 9.7103 - val_mae: 1.3750 - lr: 3.0508e-04 - 9s/epoch - 9ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 9s - loss: 9.3809 - mse: 9.3809 - mae: 1.2706 - val_loss: 9.4094 - val_mse: 9.4094 - val_mae: 1.4329 - lr: 3.0508e-04 - 9s/epoch - 9ms/step\n",
            "Score for fold 4: loss of 9.409399032592773\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 9s - loss: 10.2469 - mse: 10.2469 - mae: 1.3072 - val_loss: 5.9651 - val_mse: 5.9651 - val_mae: 1.2052 - lr: 3.0508e-04 - 9s/epoch - 9ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 9s - loss: 9.8798 - mse: 9.8798 - mae: 1.2872 - val_loss: 5.8342 - val_mse: 5.8342 - val_mae: 1.2295 - lr: 3.0508e-04 - 9s/epoch - 9ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 9s - loss: 9.5870 - mse: 9.5870 - mae: 1.2607 - val_loss: 6.1736 - val_mse: 6.1736 - val_mae: 1.3034 - lr: 3.0508e-04 - 9s/epoch - 9ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 9s - loss: 9.1940 - mse: 9.1940 - mae: 1.2317 - val_loss: 6.0875 - val_mse: 6.0875 - val_mae: 1.2687 - lr: 3.0508e-04 - 9s/epoch - 9ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 9s - loss: 8.9479 - mse: 8.9479 - mae: 1.2093 - val_loss: 6.1347 - val_mse: 6.1347 - val_mae: 1.2922 - lr: 3.0508e-04 - 9s/epoch - 9ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 9s - loss: 8.7707 - mse: 8.7707 - mae: 1.1845 - val_loss: 6.1801 - val_mse: 6.1801 - val_mae: 1.2943 - lr: 3.0508e-04 - 9s/epoch - 9ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 9s - loss: 8.4215 - mse: 8.4215 - mae: 1.1502 - val_loss: 6.5654 - val_mse: 6.5654 - val_mae: 1.3902 - lr: 3.0508e-04 - 9s/epoch - 9ms/step\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-10-12 08:23:30,074]\u001b[0m Finished trial#15 resulted in value: 11.411999999999999. Current best value is 8.826 with parameters: {'activation': 'tanh', 'num_hidden_layer': 4, 'i': 10, 'learning_rate': 0.00023399735711621326}.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Score for fold 5: loss of 6.56544303894043\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 15.3004 - mse: 15.3004 - mae: 1.5952 - val_loss: 10.8119 - val_mse: 10.8119 - val_mae: 1.5306 - lr: 5.3977e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 14.3014 - mse: 14.3014 - mae: 1.5228 - val_loss: 10.6406 - val_mse: 10.6406 - val_mae: 1.5631 - lr: 5.3977e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 14.0724 - mse: 14.0724 - mae: 1.5068 - val_loss: 10.4741 - val_mse: 10.4741 - val_mae: 1.5359 - lr: 5.3977e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 13.9956 - mse: 13.9956 - mae: 1.4966 - val_loss: 10.4017 - val_mse: 10.4017 - val_mae: 1.4874 - lr: 5.3977e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 13.8575 - mse: 13.8575 - mae: 1.4904 - val_loss: 10.3404 - val_mse: 10.3404 - val_mae: 1.4936 - lr: 5.3977e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 13.7955 - mse: 13.7955 - mae: 1.4861 - val_loss: 10.3941 - val_mse: 10.3941 - val_mae: 1.4584 - lr: 5.3977e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 13.7422 - mse: 13.7422 - mae: 1.4805 - val_loss: 10.2662 - val_mse: 10.2662 - val_mae: 1.4660 - lr: 5.3977e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 2s - loss: 13.7331 - mse: 13.7331 - mae: 1.4746 - val_loss: 10.4672 - val_mse: 10.4672 - val_mae: 1.4981 - lr: 5.3977e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 2s - loss: 13.6466 - mse: 13.6466 - mae: 1.4693 - val_loss: 10.2738 - val_mse: 10.2738 - val_mae: 1.4768 - lr: 5.3977e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 2s - loss: 13.6229 - mse: 13.6229 - mae: 1.4682 - val_loss: 10.1921 - val_mse: 10.1921 - val_mae: 1.4468 - lr: 5.3977e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 2s - loss: 13.5572 - mse: 13.5572 - mae: 1.4669 - val_loss: 10.1554 - val_mse: 10.1554 - val_mae: 1.4880 - lr: 5.3977e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 2s - loss: 13.5110 - mse: 13.5110 - mae: 1.4621 - val_loss: 10.1791 - val_mse: 10.1791 - val_mae: 1.4682 - lr: 5.3977e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 13/100\n",
            "1000/1000 - 2s - loss: 13.4872 - mse: 13.4872 - mae: 1.4590 - val_loss: 10.1746 - val_mse: 10.1746 - val_mae: 1.4765 - lr: 5.3977e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 14/100\n",
            "1000/1000 - 2s - loss: 13.4341 - mse: 13.4341 - mae: 1.4581 - val_loss: 10.1511 - val_mse: 10.1511 - val_mae: 1.4682 - lr: 5.3977e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 15/100\n",
            "1000/1000 - 2s - loss: 13.4376 - mse: 13.4376 - mae: 1.4558 - val_loss: 10.0941 - val_mse: 10.0941 - val_mae: 1.4388 - lr: 5.3977e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 16/100\n",
            "1000/1000 - 2s - loss: 13.3918 - mse: 13.3918 - mae: 1.4552 - val_loss: 10.1040 - val_mse: 10.1040 - val_mae: 1.4555 - lr: 5.3977e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 17/100\n",
            "1000/1000 - 2s - loss: 13.3565 - mse: 13.3565 - mae: 1.4514 - val_loss: 10.1253 - val_mse: 10.1253 - val_mae: 1.4787 - lr: 5.3977e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 18/100\n",
            "1000/1000 - 2s - loss: 13.3787 - mse: 13.3787 - mae: 1.4515 - val_loss: 10.0199 - val_mse: 10.0199 - val_mae: 1.4976 - lr: 5.3977e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 19/100\n",
            "1000/1000 - 2s - loss: 13.3014 - mse: 13.3014 - mae: 1.4520 - val_loss: 10.1812 - val_mse: 10.1812 - val_mae: 1.4990 - lr: 5.3977e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 20/100\n",
            "1000/1000 - 2s - loss: 13.2923 - mse: 13.2923 - mae: 1.4515 - val_loss: 10.0541 - val_mse: 10.0541 - val_mae: 1.4942 - lr: 5.3977e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 21/100\n",
            "1000/1000 - 2s - loss: 13.2502 - mse: 13.2502 - mae: 1.4462 - val_loss: 10.0008 - val_mse: 10.0008 - val_mae: 1.4801 - lr: 5.3977e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 22/100\n",
            "1000/1000 - 2s - loss: 13.2425 - mse: 13.2425 - mae: 1.4480 - val_loss: 10.0465 - val_mse: 10.0465 - val_mae: 1.4508 - lr: 5.3977e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 23/100\n",
            "1000/1000 - 2s - loss: 13.2461 - mse: 13.2461 - mae: 1.4439 - val_loss: 10.1810 - val_mse: 10.1810 - val_mae: 1.4683 - lr: 5.3977e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 24/100\n",
            "1000/1000 - 2s - loss: 13.1999 - mse: 13.1999 - mae: 1.4445 - val_loss: 10.1251 - val_mse: 10.1251 - val_mae: 1.4840 - lr: 5.3977e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 25/100\n",
            "1000/1000 - 2s - loss: 13.1704 - mse: 13.1704 - mae: 1.4433 - val_loss: 9.9776 - val_mse: 9.9776 - val_mae: 1.4783 - lr: 5.3977e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 26/100\n",
            "1000/1000 - 2s - loss: 13.1583 - mse: 13.1583 - mae: 1.4437 - val_loss: 10.1087 - val_mse: 10.1087 - val_mae: 1.4716 - lr: 5.3977e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 27/100\n",
            "1000/1000 - 2s - loss: 13.0865 - mse: 13.0865 - mae: 1.4416 - val_loss: 9.9973 - val_mse: 9.9973 - val_mae: 1.4480 - lr: 5.3977e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 28/100\n",
            "1000/1000 - 2s - loss: 13.1148 - mse: 13.1148 - mae: 1.4439 - val_loss: 10.0803 - val_mse: 10.0803 - val_mae: 1.4378 - lr: 5.3977e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 29/100\n",
            "1000/1000 - 2s - loss: 13.0885 - mse: 13.0885 - mae: 1.4376 - val_loss: 9.9533 - val_mse: 9.9533 - val_mae: 1.4502 - lr: 5.3977e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 30/100\n",
            "1000/1000 - 2s - loss: 13.0309 - mse: 13.0309 - mae: 1.4395 - val_loss: 10.0881 - val_mse: 10.0881 - val_mae: 1.4492 - lr: 5.3977e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 31/100\n",
            "1000/1000 - 2s - loss: 13.0124 - mse: 13.0124 - mae: 1.4372 - val_loss: 9.9779 - val_mse: 9.9779 - val_mae: 1.4604 - lr: 5.3977e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 32/100\n",
            "1000/1000 - 2s - loss: 12.9984 - mse: 12.9984 - mae: 1.4357 - val_loss: 10.0110 - val_mse: 10.0110 - val_mae: 1.4710 - lr: 5.3977e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 33/100\n",
            "1000/1000 - 2s - loss: 13.0059 - mse: 13.0059 - mae: 1.4334 - val_loss: 9.9611 - val_mse: 9.9611 - val_mae: 1.4810 - lr: 5.3977e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 34/100\n",
            "1000/1000 - 2s - loss: 12.9692 - mse: 12.9692 - mae: 1.4355 - val_loss: 9.9013 - val_mse: 9.9013 - val_mae: 1.4750 - lr: 5.3977e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 35/100\n",
            "1000/1000 - 2s - loss: 12.9612 - mse: 12.9612 - mae: 1.4364 - val_loss: 9.9743 - val_mse: 9.9743 - val_mae: 1.4619 - lr: 5.3977e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 36/100\n",
            "1000/1000 - 2s - loss: 12.9214 - mse: 12.9214 - mae: 1.4373 - val_loss: 10.0152 - val_mse: 10.0152 - val_mae: 1.4747 - lr: 5.3977e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 37/100\n",
            "1000/1000 - 2s - loss: 12.9071 - mse: 12.9071 - mae: 1.4355 - val_loss: 9.9405 - val_mse: 9.9405 - val_mae: 1.4529 - lr: 5.3977e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 38/100\n",
            "1000/1000 - 2s - loss: 12.8798 - mse: 12.8798 - mae: 1.4317 - val_loss: 9.8973 - val_mse: 9.8973 - val_mae: 1.4694 - lr: 5.3977e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 39/100\n",
            "1000/1000 - 2s - loss: 12.8543 - mse: 12.8543 - mae: 1.4302 - val_loss: 9.9393 - val_mse: 9.9393 - val_mae: 1.4384 - lr: 5.3977e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 40/100\n",
            "1000/1000 - 2s - loss: 12.8086 - mse: 12.8086 - mae: 1.4299 - val_loss: 10.0052 - val_mse: 10.0052 - val_mae: 1.4372 - lr: 5.3977e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 41/100\n",
            "1000/1000 - 2s - loss: 12.8374 - mse: 12.8374 - mae: 1.4287 - val_loss: 9.8654 - val_mse: 9.8654 - val_mae: 1.4600 - lr: 5.3977e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 42/100\n",
            "1000/1000 - 2s - loss: 12.7852 - mse: 12.7852 - mae: 1.4268 - val_loss: 9.9755 - val_mse: 9.9755 - val_mae: 1.4701 - lr: 5.3977e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 43/100\n",
            "1000/1000 - 2s - loss: 12.7743 - mse: 12.7743 - mae: 1.4276 - val_loss: 9.9517 - val_mse: 9.9517 - val_mae: 1.4484 - lr: 5.3977e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 44/100\n",
            "1000/1000 - 2s - loss: 12.7270 - mse: 12.7270 - mae: 1.4251 - val_loss: 9.9061 - val_mse: 9.9061 - val_mae: 1.4816 - lr: 5.3977e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 45/100\n",
            "1000/1000 - 2s - loss: 12.7207 - mse: 12.7207 - mae: 1.4275 - val_loss: 9.8811 - val_mse: 9.8811 - val_mae: 1.4715 - lr: 5.3977e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 46/100\n",
            "1000/1000 - 2s - loss: 12.7076 - mse: 12.7076 - mae: 1.4262 - val_loss: 9.8876 - val_mse: 9.8876 - val_mae: 1.4707 - lr: 5.3977e-04 - 2s/epoch - 2ms/step\n",
            "Score for fold 1: loss of 9.88762378692627\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 12.2766 - mse: 12.2766 - mae: 1.4357 - val_loss: 11.6586 - val_mse: 11.6586 - val_mae: 1.4215 - lr: 5.3977e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 12.2957 - mse: 12.2957 - mae: 1.4357 - val_loss: 11.6610 - val_mse: 11.6610 - val_mae: 1.4007 - lr: 5.3977e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 12.2575 - mse: 12.2575 - mae: 1.4306 - val_loss: 11.5486 - val_mse: 11.5486 - val_mae: 1.4489 - lr: 5.3977e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 12.2529 - mse: 12.2529 - mae: 1.4304 - val_loss: 11.6785 - val_mse: 11.6785 - val_mae: 1.4349 - lr: 5.3977e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 12.2524 - mse: 12.2524 - mae: 1.4297 - val_loss: 11.7411 - val_mse: 11.7411 - val_mae: 1.4701 - lr: 5.3977e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 12.2225 - mse: 12.2225 - mae: 1.4323 - val_loss: 11.6670 - val_mse: 11.6670 - val_mae: 1.4486 - lr: 5.3977e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 12.2411 - mse: 12.2411 - mae: 1.4277 - val_loss: 11.6801 - val_mse: 11.6801 - val_mae: 1.4356 - lr: 5.3977e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 2s - loss: 12.1899 - mse: 12.1899 - mae: 1.4287 - val_loss: 11.7400 - val_mse: 11.7400 - val_mae: 1.4129 - lr: 5.3977e-04 - 2s/epoch - 2ms/step\n",
            "Score for fold 2: loss of 11.739999771118164\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 13.0830 - mse: 13.0830 - mae: 1.4421 - val_loss: 8.0975 - val_mse: 8.0975 - val_mae: 1.3888 - lr: 5.3977e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 13.0399 - mse: 13.0399 - mae: 1.4409 - val_loss: 8.2083 - val_mse: 8.2083 - val_mae: 1.3722 - lr: 5.3977e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 13.0074 - mse: 13.0074 - mae: 1.4404 - val_loss: 8.1506 - val_mse: 8.1506 - val_mae: 1.3807 - lr: 5.3977e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 12.9829 - mse: 12.9829 - mae: 1.4390 - val_loss: 8.2560 - val_mse: 8.2560 - val_mae: 1.3786 - lr: 5.3977e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 12.9897 - mse: 12.9897 - mae: 1.4402 - val_loss: 8.3662 - val_mse: 8.3662 - val_mae: 1.3980 - lr: 5.3977e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 12.9498 - mse: 12.9498 - mae: 1.4388 - val_loss: 8.3062 - val_mse: 8.3062 - val_mae: 1.4176 - lr: 5.3977e-04 - 2s/epoch - 2ms/step\n",
            "Score for fold 3: loss of 8.306230545043945\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 10.2453 - mse: 10.2453 - mae: 1.4221 - val_loss: 18.8851 - val_mse: 18.8851 - val_mae: 1.4498 - lr: 5.3977e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 10.2148 - mse: 10.2148 - mae: 1.4183 - val_loss: 18.9547 - val_mse: 18.9547 - val_mae: 1.4512 - lr: 5.3977e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 10.2192 - mse: 10.2192 - mae: 1.4226 - val_loss: 18.9394 - val_mse: 18.9394 - val_mae: 1.4597 - lr: 5.3977e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 10.2192 - mse: 10.2192 - mae: 1.4203 - val_loss: 19.1445 - val_mse: 19.1445 - val_mae: 1.4404 - lr: 5.3977e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 10.1881 - mse: 10.1881 - mae: 1.4185 - val_loss: 19.1321 - val_mse: 19.1321 - val_mae: 1.4758 - lr: 5.3977e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 10.1797 - mse: 10.1797 - mae: 1.4163 - val_loss: 19.1101 - val_mse: 19.1101 - val_mae: 1.4677 - lr: 5.3977e-04 - 2s/epoch - 2ms/step\n",
            "Score for fold 4: loss of 19.110050201416016\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 11.9536 - mse: 11.9536 - mae: 1.4321 - val_loss: 11.6530 - val_mse: 11.6530 - val_mae: 1.3939 - lr: 5.3977e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 11.9522 - mse: 11.9522 - mae: 1.4271 - val_loss: 11.7322 - val_mse: 11.7322 - val_mae: 1.4615 - lr: 5.3977e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 11.8883 - mse: 11.8883 - mae: 1.4277 - val_loss: 11.6818 - val_mse: 11.6818 - val_mae: 1.4422 - lr: 5.3977e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 11.8592 - mse: 11.8592 - mae: 1.4242 - val_loss: 11.7800 - val_mse: 11.7800 - val_mae: 1.4319 - lr: 5.3977e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 11.8353 - mse: 11.8353 - mae: 1.4265 - val_loss: 11.7580 - val_mse: 11.7580 - val_mae: 1.4456 - lr: 5.3977e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 11.7975 - mse: 11.7975 - mae: 1.4212 - val_loss: 11.7481 - val_mse: 11.7481 - val_mae: 1.4455 - lr: 5.3977e-04 - 2s/epoch - 2ms/step\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-10-12 08:25:40,350]\u001b[0m Finished trial#16 resulted in value: 12.16. Current best value is 8.826 with parameters: {'activation': 'tanh', 'num_hidden_layer': 4, 'i': 10, 'learning_rate': 0.00023399735711621326}.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Score for fold 5: loss of 11.748119354248047\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 8s - loss: 13.2586 - mse: 13.2586 - mae: 1.6037 - val_loss: 18.0241 - val_mse: 18.0241 - val_mae: 1.6559 - lr: 1.9147e-04 - 8s/epoch - 8ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 7s - loss: 12.3947 - mse: 12.3947 - mae: 1.5173 - val_loss: 17.8061 - val_mse: 17.8061 - val_mae: 1.6127 - lr: 1.9147e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 7s - loss: 12.2442 - mse: 12.2442 - mae: 1.4983 - val_loss: 17.5439 - val_mse: 17.5439 - val_mae: 1.5206 - lr: 1.9147e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 8s - loss: 12.1333 - mse: 12.1333 - mae: 1.4834 - val_loss: 17.4507 - val_mse: 17.4507 - val_mae: 1.5333 - lr: 1.9147e-04 - 8s/epoch - 8ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 7s - loss: 12.0941 - mse: 12.0941 - mae: 1.4775 - val_loss: 17.4338 - val_mse: 17.4338 - val_mae: 1.5349 - lr: 1.9147e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 7s - loss: 12.0167 - mse: 12.0167 - mae: 1.4750 - val_loss: 17.4039 - val_mse: 17.4039 - val_mae: 1.5188 - lr: 1.9147e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 7s - loss: 11.9873 - mse: 11.9873 - mae: 1.4682 - val_loss: 17.6272 - val_mse: 17.6272 - val_mae: 1.5962 - lr: 1.9147e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 7s - loss: 11.9320 - mse: 11.9320 - mae: 1.4651 - val_loss: 17.3832 - val_mse: 17.3832 - val_mae: 1.4841 - lr: 1.9147e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 7s - loss: 11.8398 - mse: 11.8398 - mae: 1.4643 - val_loss: 17.2542 - val_mse: 17.2542 - val_mae: 1.5433 - lr: 1.9147e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 8s - loss: 11.7516 - mse: 11.7516 - mae: 1.4587 - val_loss: 17.3302 - val_mse: 17.3302 - val_mae: 1.5093 - lr: 1.9147e-04 - 8s/epoch - 8ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 8s - loss: 11.6930 - mse: 11.6930 - mae: 1.4549 - val_loss: 17.2527 - val_mse: 17.2527 - val_mae: 1.5545 - lr: 1.9147e-04 - 8s/epoch - 8ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 7s - loss: 11.6808 - mse: 11.6808 - mae: 1.4549 - val_loss: 17.4326 - val_mse: 17.4326 - val_mae: 1.5024 - lr: 1.9147e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 13/100\n",
            "1000/1000 - 7s - loss: 11.6279 - mse: 11.6279 - mae: 1.4465 - val_loss: 17.3682 - val_mse: 17.3682 - val_mae: 1.5519 - lr: 1.9147e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 14/100\n",
            "1000/1000 - 7s - loss: 11.5547 - mse: 11.5547 - mae: 1.4445 - val_loss: 17.3869 - val_mse: 17.3869 - val_mae: 1.5030 - lr: 1.9147e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 15/100\n",
            "1000/1000 - 7s - loss: 11.5103 - mse: 11.5103 - mae: 1.4479 - val_loss: 17.0929 - val_mse: 17.0929 - val_mae: 1.5140 - lr: 1.9147e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 16/100\n",
            "1000/1000 - 6s - loss: 11.3574 - mse: 11.3574 - mae: 1.4407 - val_loss: 17.6261 - val_mse: 17.6261 - val_mae: 1.5121 - lr: 1.9147e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 17/100\n",
            "1000/1000 - 7s - loss: 11.3633 - mse: 11.3633 - mae: 1.4390 - val_loss: 17.1164 - val_mse: 17.1164 - val_mae: 1.5249 - lr: 1.9147e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 18/100\n",
            "1000/1000 - 7s - loss: 11.2824 - mse: 11.2824 - mae: 1.4329 - val_loss: 17.0595 - val_mse: 17.0595 - val_mae: 1.4983 - lr: 1.9147e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 19/100\n",
            "1000/1000 - 7s - loss: 11.1951 - mse: 11.1951 - mae: 1.4328 - val_loss: 17.2979 - val_mse: 17.2979 - val_mae: 1.4900 - lr: 1.9147e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 20/100\n",
            "1000/1000 - 6s - loss: 11.1522 - mse: 11.1522 - mae: 1.4257 - val_loss: 17.2593 - val_mse: 17.2593 - val_mae: 1.5717 - lr: 1.9147e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 21/100\n",
            "1000/1000 - 7s - loss: 11.0891 - mse: 11.0891 - mae: 1.4272 - val_loss: 17.0913 - val_mse: 17.0913 - val_mae: 1.5035 - lr: 1.9147e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 22/100\n",
            "1000/1000 - 7s - loss: 10.9954 - mse: 10.9954 - mae: 1.4252 - val_loss: 17.2414 - val_mse: 17.2414 - val_mae: 1.5328 - lr: 1.9147e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 23/100\n",
            "1000/1000 - 7s - loss: 10.9705 - mse: 10.9705 - mae: 1.4226 - val_loss: 17.2411 - val_mse: 17.2411 - val_mae: 1.4884 - lr: 1.9147e-04 - 7s/epoch - 7ms/step\n",
            "Score for fold 1: loss of 17.241073608398438\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 7s - loss: 13.5564 - mse: 13.5564 - mae: 1.4542 - val_loss: 6.8418 - val_mse: 6.8418 - val_mae: 1.4243 - lr: 1.9147e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 7s - loss: 13.4257 - mse: 13.4257 - mae: 1.4460 - val_loss: 7.0237 - val_mse: 7.0237 - val_mae: 1.4344 - lr: 1.9147e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 7s - loss: 13.3159 - mse: 13.3159 - mae: 1.4353 - val_loss: 6.8062 - val_mse: 6.8062 - val_mae: 1.3887 - lr: 1.9147e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 8s - loss: 13.2754 - mse: 13.2754 - mae: 1.4390 - val_loss: 6.8897 - val_mse: 6.8897 - val_mae: 1.3791 - lr: 1.9147e-04 - 8s/epoch - 8ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 8s - loss: 13.1704 - mse: 13.1704 - mae: 1.4326 - val_loss: 6.8976 - val_mse: 6.8976 - val_mae: 1.4325 - lr: 1.9147e-04 - 8s/epoch - 8ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 7s - loss: 13.0888 - mse: 13.0888 - mae: 1.4320 - val_loss: 6.9219 - val_mse: 6.9219 - val_mae: 1.4059 - lr: 1.9147e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 7s - loss: 12.9830 - mse: 12.9830 - mae: 1.4291 - val_loss: 7.0581 - val_mse: 7.0581 - val_mae: 1.4187 - lr: 1.9147e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 7s - loss: 12.9299 - mse: 12.9299 - mae: 1.4259 - val_loss: 6.9216 - val_mse: 6.9216 - val_mae: 1.4152 - lr: 1.9147e-04 - 7s/epoch - 7ms/step\n",
            "Score for fold 2: loss of 6.921574115753174\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 6s - loss: 12.7012 - mse: 12.7012 - mae: 1.4258 - val_loss: 7.5121 - val_mse: 7.5121 - val_mae: 1.3933 - lr: 1.9147e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 6s - loss: 12.6051 - mse: 12.6051 - mae: 1.4230 - val_loss: 7.7062 - val_mse: 7.7062 - val_mae: 1.4270 - lr: 1.9147e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 6s - loss: 12.5184 - mse: 12.5184 - mae: 1.4162 - val_loss: 7.9291 - val_mse: 7.9291 - val_mae: 1.4258 - lr: 1.9147e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 6s - loss: 12.4537 - mse: 12.4537 - mae: 1.4108 - val_loss: 7.7714 - val_mse: 7.7714 - val_mae: 1.4253 - lr: 1.9147e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 6s - loss: 12.3361 - mse: 12.3361 - mae: 1.4050 - val_loss: 7.7458 - val_mse: 7.7458 - val_mae: 1.4265 - lr: 1.9147e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 6s - loss: 12.2478 - mse: 12.2478 - mae: 1.4027 - val_loss: 7.7668 - val_mse: 7.7668 - val_mae: 1.4122 - lr: 1.9147e-04 - 6s/epoch - 6ms/step\n",
            "Score for fold 3: loss of 7.766849994659424\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 6s - loss: 12.1712 - mse: 12.1712 - mae: 1.4196 - val_loss: 7.8352 - val_mse: 7.8352 - val_mae: 1.3796 - lr: 1.9147e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 6s - loss: 12.0835 - mse: 12.0835 - mae: 1.4120 - val_loss: 7.8910 - val_mse: 7.8910 - val_mae: 1.4072 - lr: 1.9147e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 6s - loss: 11.9208 - mse: 11.9208 - mae: 1.4048 - val_loss: 8.0081 - val_mse: 8.0081 - val_mae: 1.3896 - lr: 1.9147e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 6s - loss: 11.8164 - mse: 11.8164 - mae: 1.4001 - val_loss: 8.0129 - val_mse: 8.0129 - val_mae: 1.3871 - lr: 1.9147e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 6s - loss: 11.8029 - mse: 11.8029 - mae: 1.3993 - val_loss: 7.9606 - val_mse: 7.9606 - val_mae: 1.3851 - lr: 1.9147e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 6s - loss: 11.6472 - mse: 11.6472 - mae: 1.3877 - val_loss: 7.9899 - val_mse: 7.9899 - val_mae: 1.4374 - lr: 1.9147e-04 - 6s/epoch - 6ms/step\n",
            "Score for fold 4: loss of 7.989941120147705\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 6s - loss: 9.0164 - mse: 9.0164 - mae: 1.3887 - val_loss: 18.4589 - val_mse: 18.4589 - val_mae: 1.4164 - lr: 1.9147e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 7s - loss: 9.0094 - mse: 9.0094 - mae: 1.3805 - val_loss: 18.5025 - val_mse: 18.5025 - val_mae: 1.4300 - lr: 1.9147e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 7s - loss: 8.8828 - mse: 8.8828 - mae: 1.3751 - val_loss: 18.2302 - val_mse: 18.2302 - val_mae: 1.4550 - lr: 1.9147e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 6s - loss: 8.7519 - mse: 8.7519 - mae: 1.3717 - val_loss: 18.4411 - val_mse: 18.4411 - val_mae: 1.4261 - lr: 1.9147e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 6s - loss: 8.7340 - mse: 8.7340 - mae: 1.3617 - val_loss: 18.3763 - val_mse: 18.3763 - val_mae: 1.4467 - lr: 1.9147e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 6s - loss: 8.6009 - mse: 8.6009 - mae: 1.3574 - val_loss: 18.5039 - val_mse: 18.5039 - val_mae: 1.4275 - lr: 1.9147e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 7s - loss: 8.5136 - mse: 8.5136 - mae: 1.3474 - val_loss: 18.6565 - val_mse: 18.6565 - val_mae: 1.4537 - lr: 1.9147e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 7s - loss: 8.4607 - mse: 8.4607 - mae: 1.3499 - val_loss: 18.8185 - val_mse: 18.8185 - val_mae: 1.4592 - lr: 1.9147e-04 - 7s/epoch - 7ms/step\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-10-12 08:31:35,976]\u001b[0m Finished trial#17 resulted in value: 11.748. Current best value is 8.826 with parameters: {'activation': 'tanh', 'num_hidden_layer': 4, 'i': 10, 'learning_rate': 0.00023399735711621326}.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Score for fold 5: loss of 18.818483352661133\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 6s - loss: 15.3917 - mse: 15.3917 - mae: 1.6272 - val_loss: 10.5602 - val_mse: 10.5602 - val_mae: 1.5001 - lr: 5.8379e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 6s - loss: 14.7613 - mse: 14.7613 - mae: 1.5477 - val_loss: 10.2379 - val_mse: 10.2379 - val_mae: 1.4872 - lr: 5.8379e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 6s - loss: 14.5838 - mse: 14.5838 - mae: 1.5406 - val_loss: 10.0586 - val_mse: 10.0586 - val_mae: 1.6055 - lr: 5.8379e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 6s - loss: 14.4066 - mse: 14.4066 - mae: 1.5264 - val_loss: 9.7118 - val_mse: 9.7118 - val_mae: 1.4689 - lr: 5.8379e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 6s - loss: 14.2181 - mse: 14.2181 - mae: 1.5207 - val_loss: 10.0174 - val_mse: 10.0174 - val_mae: 1.4762 - lr: 5.8379e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 6s - loss: 14.1282 - mse: 14.1282 - mae: 1.5095 - val_loss: 9.9777 - val_mse: 9.9777 - val_mae: 1.5763 - lr: 5.8379e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 6s - loss: 13.9471 - mse: 13.9471 - mae: 1.5026 - val_loss: 9.7689 - val_mse: 9.7689 - val_mae: 1.5664 - lr: 5.8379e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 6s - loss: 13.9282 - mse: 13.9282 - mae: 1.5008 - val_loss: 9.5020 - val_mse: 9.5020 - val_mae: 1.5096 - lr: 5.8379e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 6s - loss: 13.8698 - mse: 13.8698 - mae: 1.4972 - val_loss: 9.3789 - val_mse: 9.3789 - val_mae: 1.4549 - lr: 5.8379e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 6s - loss: 13.7507 - mse: 13.7507 - mae: 1.4922 - val_loss: 9.7201 - val_mse: 9.7201 - val_mae: 1.4452 - lr: 5.8379e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 6s - loss: 13.7796 - mse: 13.7796 - mae: 1.4905 - val_loss: 9.4035 - val_mse: 9.4035 - val_mae: 1.4771 - lr: 5.8379e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 6s - loss: 13.6427 - mse: 13.6427 - mae: 1.4892 - val_loss: 9.6057 - val_mse: 9.6057 - val_mae: 1.4222 - lr: 5.8379e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 13/100\n",
            "1000/1000 - 6s - loss: 13.5723 - mse: 13.5723 - mae: 1.4826 - val_loss: 9.2997 - val_mse: 9.2997 - val_mae: 1.5030 - lr: 5.8379e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 14/100\n",
            "1000/1000 - 6s - loss: 13.4111 - mse: 13.4111 - mae: 1.4789 - val_loss: 9.3790 - val_mse: 9.3790 - val_mae: 1.4530 - lr: 5.8379e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 15/100\n",
            "1000/1000 - 6s - loss: 13.3866 - mse: 13.3866 - mae: 1.4756 - val_loss: 9.4670 - val_mse: 9.4670 - val_mae: 1.4462 - lr: 5.8379e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 16/100\n",
            "1000/1000 - 6s - loss: 13.4179 - mse: 13.4179 - mae: 1.4807 - val_loss: 9.4513 - val_mse: 9.4513 - val_mae: 1.5509 - lr: 5.8379e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 17/100\n",
            "1000/1000 - 6s - loss: 13.3007 - mse: 13.3007 - mae: 1.4715 - val_loss: 9.3539 - val_mse: 9.3539 - val_mae: 1.4848 - lr: 5.8379e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 18/100\n",
            "1000/1000 - 6s - loss: 13.2638 - mse: 13.2638 - mae: 1.4746 - val_loss: 9.2979 - val_mse: 9.2979 - val_mae: 1.5195 - lr: 5.8379e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 19/100\n",
            "1000/1000 - 6s - loss: 13.1640 - mse: 13.1640 - mae: 1.4666 - val_loss: 9.2989 - val_mse: 9.2989 - val_mae: 1.4291 - lr: 5.8379e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 20/100\n",
            "1000/1000 - 6s - loss: 13.0346 - mse: 13.0346 - mae: 1.4633 - val_loss: 9.2652 - val_mse: 9.2652 - val_mae: 1.5282 - lr: 5.8379e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 21/100\n",
            "1000/1000 - 6s - loss: 13.1120 - mse: 13.1120 - mae: 1.4625 - val_loss: 9.4702 - val_mse: 9.4702 - val_mae: 1.5078 - lr: 5.8379e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 22/100\n",
            "1000/1000 - 6s - loss: 13.0274 - mse: 13.0274 - mae: 1.4636 - val_loss: 9.4355 - val_mse: 9.4355 - val_mae: 1.4373 - lr: 5.8379e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 23/100\n",
            "1000/1000 - 7s - loss: 12.9478 - mse: 12.9478 - mae: 1.4574 - val_loss: 9.3256 - val_mse: 9.3256 - val_mae: 1.4328 - lr: 5.8379e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 24/100\n",
            "1000/1000 - 8s - loss: 12.7982 - mse: 12.7982 - mae: 1.4515 - val_loss: 9.3376 - val_mse: 9.3376 - val_mae: 1.4911 - lr: 5.8379e-04 - 8s/epoch - 8ms/step\n",
            "Epoch 25/100\n",
            "1000/1000 - 7s - loss: 12.8352 - mse: 12.8352 - mae: 1.4443 - val_loss: 9.2185 - val_mse: 9.2185 - val_mae: 1.4230 - lr: 5.8379e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 26/100\n",
            "1000/1000 - 7s - loss: 12.5876 - mse: 12.5876 - mae: 1.4443 - val_loss: 9.1374 - val_mse: 9.1374 - val_mae: 1.4974 - lr: 5.8379e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 27/100\n",
            "1000/1000 - 7s - loss: 12.5481 - mse: 12.5481 - mae: 1.4333 - val_loss: 9.5296 - val_mse: 9.5296 - val_mae: 1.5297 - lr: 5.8379e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 28/100\n",
            "1000/1000 - 7s - loss: 12.5371 - mse: 12.5371 - mae: 1.4406 - val_loss: 9.2081 - val_mse: 9.2081 - val_mae: 1.4482 - lr: 5.8379e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 29/100\n",
            "1000/1000 - 7s - loss: 12.3341 - mse: 12.3341 - mae: 1.4287 - val_loss: 9.2252 - val_mse: 9.2252 - val_mae: 1.4453 - lr: 5.8379e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 30/100\n",
            "1000/1000 - 7s - loss: 12.2885 - mse: 12.2885 - mae: 1.4247 - val_loss: 9.3978 - val_mse: 9.3978 - val_mae: 1.4165 - lr: 5.8379e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 31/100\n",
            "1000/1000 - 7s - loss: 12.0534 - mse: 12.0534 - mae: 1.4131 - val_loss: 9.2436 - val_mse: 9.2436 - val_mae: 1.5178 - lr: 5.8379e-04 - 7s/epoch - 7ms/step\n",
            "Score for fold 1: loss of 9.243637084960938\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 7s - loss: 11.4906 - mse: 11.4906 - mae: 1.4216 - val_loss: 11.9326 - val_mse: 11.9326 - val_mae: 1.5279 - lr: 5.8379e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 6s - loss: 11.2488 - mse: 11.2488 - mae: 1.4155 - val_loss: 12.0498 - val_mse: 12.0498 - val_mae: 1.4274 - lr: 5.8379e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 6s - loss: 11.1844 - mse: 11.1844 - mae: 1.4100 - val_loss: 12.3482 - val_mse: 12.3482 - val_mae: 1.3918 - lr: 5.8379e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 6s - loss: 10.9423 - mse: 10.9423 - mae: 1.4004 - val_loss: 12.0493 - val_mse: 12.0493 - val_mae: 1.4020 - lr: 5.8379e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 7s - loss: 10.7592 - mse: 10.7592 - mae: 1.3938 - val_loss: 12.3777 - val_mse: 12.3777 - val_mae: 1.4449 - lr: 5.8379e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 6s - loss: 10.8085 - mse: 10.8085 - mae: 1.3843 - val_loss: 12.0305 - val_mse: 12.0305 - val_mae: 1.4875 - lr: 5.8379e-04 - 6s/epoch - 6ms/step\n",
            "Score for fold 2: loss of 12.03049087524414\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 6s - loss: 11.7570 - mse: 11.7570 - mae: 1.4209 - val_loss: 8.1260 - val_mse: 8.1260 - val_mae: 1.3636 - lr: 5.8379e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 6s - loss: 11.4344 - mse: 11.4344 - mae: 1.4015 - val_loss: 8.1707 - val_mse: 8.1707 - val_mae: 1.3259 - lr: 5.8379e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 6s - loss: 11.2806 - mse: 11.2806 - mae: 1.3911 - val_loss: 9.1622 - val_mse: 9.1622 - val_mae: 1.3983 - lr: 5.8379e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 7s - loss: 11.0362 - mse: 11.0362 - mae: 1.3789 - val_loss: 8.9933 - val_mse: 8.9933 - val_mae: 1.2834 - lr: 5.8379e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 7s - loss: 10.9264 - mse: 10.9264 - mae: 1.3724 - val_loss: 8.6896 - val_mse: 8.6896 - val_mae: 1.3492 - lr: 5.8379e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 7s - loss: 10.6248 - mse: 10.6248 - mae: 1.3631 - val_loss: 9.3163 - val_mse: 9.3163 - val_mae: 1.3757 - lr: 5.8379e-04 - 7s/epoch - 7ms/step\n",
            "Score for fold 3: loss of 9.31628131866455\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 7s - loss: 10.8360 - mse: 10.8360 - mae: 1.3730 - val_loss: 8.3448 - val_mse: 8.3448 - val_mae: 1.4553 - lr: 5.8379e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 7s - loss: 10.4928 - mse: 10.4928 - mae: 1.3614 - val_loss: 8.6374 - val_mse: 8.6374 - val_mae: 1.3773 - lr: 5.8379e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 7s - loss: 10.3021 - mse: 10.3021 - mae: 1.3457 - val_loss: 8.4953 - val_mse: 8.4953 - val_mae: 1.2886 - lr: 5.8379e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 7s - loss: 10.1107 - mse: 10.1107 - mae: 1.3434 - val_loss: 8.4241 - val_mse: 8.4241 - val_mae: 1.3604 - lr: 5.8379e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 7s - loss: 9.8557 - mse: 9.8557 - mae: 1.3311 - val_loss: 8.5041 - val_mse: 8.5041 - val_mae: 1.3166 - lr: 5.8379e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 7s - loss: 9.6841 - mse: 9.6841 - mae: 1.3171 - val_loss: 8.5913 - val_mse: 8.5913 - val_mae: 1.3424 - lr: 5.8379e-04 - 7s/epoch - 7ms/step\n",
            "Score for fold 4: loss of 8.591306686401367\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 7s - loss: 7.9035 - mse: 7.9035 - mae: 1.3283 - val_loss: 16.0309 - val_mse: 16.0309 - val_mae: 1.3384 - lr: 5.8379e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 7s - loss: 7.6995 - mse: 7.6995 - mae: 1.3114 - val_loss: 15.9999 - val_mse: 15.9999 - val_mae: 1.4763 - lr: 5.8379e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 7s - loss: 7.4779 - mse: 7.4779 - mae: 1.2925 - val_loss: 16.4708 - val_mse: 16.4708 - val_mae: 1.3624 - lr: 5.8379e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 7s - loss: 7.3152 - mse: 7.3152 - mae: 1.2869 - val_loss: 15.9220 - val_mse: 15.9220 - val_mae: 1.3425 - lr: 5.8379e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 7s - loss: 7.0246 - mse: 7.0246 - mae: 1.2725 - val_loss: 15.9955 - val_mse: 15.9955 - val_mae: 1.4313 - lr: 5.8379e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 7s - loss: 6.9452 - mse: 6.9452 - mae: 1.2656 - val_loss: 16.9432 - val_mse: 16.9432 - val_mae: 1.3564 - lr: 5.8379e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 7s - loss: 6.6624 - mse: 6.6624 - mae: 1.2501 - val_loss: 16.8925 - val_mse: 16.8925 - val_mae: 1.4110 - lr: 5.8379e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 7s - loss: 6.5656 - mse: 6.5656 - mae: 1.2406 - val_loss: 16.6801 - val_mse: 16.6801 - val_mae: 1.3333 - lr: 5.8379e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 7s - loss: 6.4391 - mse: 6.4391 - mae: 1.2301 - val_loss: 16.7956 - val_mse: 16.7956 - val_mae: 1.5899 - lr: 5.8379e-04 - 7s/epoch - 7ms/step\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-10-12 08:38:03,696]\u001b[0m Finished trial#18 resulted in value: 11.196000000000002. Current best value is 8.826 with parameters: {'activation': 'tanh', 'num_hidden_layer': 4, 'i': 10, 'learning_rate': 0.00023399735711621326}.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Score for fold 5: loss of 16.795625686645508\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 8s - loss: 13.9422 - mse: 13.9422 - mae: 1.5453 - val_loss: 10.4986 - val_mse: 10.4986 - val_mae: 1.5765 - lr: 1.8525e-04 - 8s/epoch - 8ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 7s - loss: 13.4331 - mse: 13.4331 - mae: 1.4995 - val_loss: 8.7529 - val_mse: 8.7529 - val_mae: 1.4715 - lr: 1.8525e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 7s - loss: 13.0257 - mse: 13.0257 - mae: 1.4770 - val_loss: 8.6954 - val_mse: 8.6954 - val_mae: 1.4484 - lr: 1.8525e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 7s - loss: 12.9430 - mse: 12.9430 - mae: 1.4668 - val_loss: 8.7705 - val_mse: 8.7705 - val_mae: 1.4425 - lr: 1.8525e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 7s - loss: 12.7884 - mse: 12.7884 - mae: 1.4533 - val_loss: 9.0561 - val_mse: 9.0561 - val_mae: 1.5213 - lr: 1.8525e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 7s - loss: 12.7879 - mse: 12.7879 - mae: 1.4455 - val_loss: 8.8008 - val_mse: 8.8008 - val_mae: 1.4103 - lr: 1.8525e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 7s - loss: 12.5605 - mse: 12.5605 - mae: 1.4399 - val_loss: 8.6947 - val_mse: 8.6947 - val_mae: 1.5384 - lr: 1.8525e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 7s - loss: 12.4595 - mse: 12.4595 - mae: 1.4329 - val_loss: 8.7502 - val_mse: 8.7502 - val_mae: 1.4916 - lr: 1.8525e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 7s - loss: 12.4737 - mse: 12.4737 - mae: 1.4302 - val_loss: 8.8638 - val_mse: 8.8638 - val_mae: 1.4494 - lr: 1.8525e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 7s - loss: 12.3551 - mse: 12.3551 - mae: 1.4258 - val_loss: 8.8487 - val_mse: 8.8487 - val_mae: 1.4401 - lr: 1.8525e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 7s - loss: 12.1186 - mse: 12.1186 - mae: 1.4174 - val_loss: 8.7601 - val_mse: 8.7601 - val_mae: 1.4791 - lr: 1.8525e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 7s - loss: 12.0670 - mse: 12.0670 - mae: 1.4102 - val_loss: 8.8258 - val_mse: 8.8258 - val_mae: 1.4782 - lr: 1.8525e-04 - 7s/epoch - 7ms/step\n",
            "Score for fold 1: loss of 8.825784683227539\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 7s - loss: 12.4728 - mse: 12.4728 - mae: 1.4165 - val_loss: 7.5629 - val_mse: 7.5629 - val_mae: 1.3648 - lr: 1.8525e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 7s - loss: 12.1353 - mse: 12.1353 - mae: 1.4086 - val_loss: 7.5448 - val_mse: 7.5448 - val_mae: 1.3666 - lr: 1.8525e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 7s - loss: 12.1527 - mse: 12.1527 - mae: 1.3982 - val_loss: 7.5414 - val_mse: 7.5414 - val_mae: 1.4000 - lr: 1.8525e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 7s - loss: 11.9201 - mse: 11.9201 - mae: 1.3952 - val_loss: 7.6359 - val_mse: 7.6359 - val_mae: 1.4279 - lr: 1.8525e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 7s - loss: 11.6315 - mse: 11.6315 - mae: 1.3818 - val_loss: 7.6764 - val_mse: 7.6764 - val_mae: 1.3725 - lr: 1.8525e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 7s - loss: 11.4710 - mse: 11.4710 - mae: 1.3742 - val_loss: 7.7163 - val_mse: 7.7163 - val_mae: 1.3840 - lr: 1.8525e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 6s - loss: 11.3667 - mse: 11.3667 - mae: 1.3627 - val_loss: 7.6947 - val_mse: 7.6947 - val_mae: 1.4142 - lr: 1.8525e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 6s - loss: 11.1652 - mse: 11.1652 - mae: 1.3529 - val_loss: 7.7995 - val_mse: 7.7995 - val_mae: 1.4210 - lr: 1.8525e-04 - 6s/epoch - 6ms/step\n",
            "Score for fold 2: loss of 7.79945707321167\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 6s - loss: 11.2086 - mse: 11.2086 - mae: 1.3772 - val_loss: 7.4580 - val_mse: 7.4580 - val_mae: 1.3782 - lr: 1.8525e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 6s - loss: 10.9563 - mse: 10.9563 - mae: 1.3643 - val_loss: 7.4557 - val_mse: 7.4557 - val_mae: 1.3106 - lr: 1.8525e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 6s - loss: 10.8763 - mse: 10.8763 - mae: 1.3553 - val_loss: 7.5865 - val_mse: 7.5865 - val_mae: 1.3562 - lr: 1.8525e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 6s - loss: 10.7413 - mse: 10.7413 - mae: 1.3446 - val_loss: 7.7182 - val_mse: 7.7182 - val_mae: 1.2953 - lr: 1.8525e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 6s - loss: 10.5646 - mse: 10.5646 - mae: 1.3364 - val_loss: 8.0179 - val_mse: 8.0179 - val_mae: 1.3452 - lr: 1.8525e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 6s - loss: 10.3917 - mse: 10.3917 - mae: 1.3237 - val_loss: 7.8748 - val_mse: 7.8748 - val_mae: 1.3732 - lr: 1.8525e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 6s - loss: 10.2697 - mse: 10.2697 - mae: 1.3141 - val_loss: 8.3917 - val_mse: 8.3917 - val_mae: 1.3600 - lr: 1.8525e-04 - 6s/epoch - 6ms/step\n",
            "Score for fold 3: loss of 8.391688346862793\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 7s - loss: 10.0412 - mse: 10.0412 - mae: 1.3282 - val_loss: 8.9597 - val_mse: 8.9597 - val_mae: 1.2736 - lr: 1.8525e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 6s - loss: 9.8023 - mse: 9.8023 - mae: 1.3088 - val_loss: 9.0445 - val_mse: 9.0445 - val_mae: 1.2670 - lr: 1.8525e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 7s - loss: 9.6845 - mse: 9.6845 - mae: 1.2985 - val_loss: 9.2433 - val_mse: 9.2433 - val_mae: 1.2640 - lr: 1.8525e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 7s - loss: 9.4914 - mse: 9.4914 - mae: 1.2860 - val_loss: 9.2496 - val_mse: 9.2496 - val_mae: 1.2865 - lr: 1.8525e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 7s - loss: 9.3542 - mse: 9.3542 - mae: 1.2715 - val_loss: 9.1560 - val_mse: 9.1560 - val_mae: 1.3107 - lr: 1.8525e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 7s - loss: 9.3643 - mse: 9.3643 - mae: 1.2651 - val_loss: 9.0420 - val_mse: 9.0420 - val_mae: 1.3799 - lr: 1.8525e-04 - 7s/epoch - 7ms/step\n",
            "Score for fold 4: loss of 9.042024612426758\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 6s - loss: 6.8117 - mse: 6.8117 - mae: 1.2718 - val_loss: 18.3936 - val_mse: 18.3936 - val_mae: 1.2970 - lr: 1.8525e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 6s - loss: 6.9685 - mse: 6.9685 - mae: 1.2517 - val_loss: 18.6884 - val_mse: 18.6884 - val_mae: 1.2719 - lr: 1.8525e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 6s - loss: 6.8475 - mse: 6.8475 - mae: 1.2442 - val_loss: 18.4089 - val_mse: 18.4089 - val_mae: 1.2809 - lr: 1.8525e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 6s - loss: 6.3713 - mse: 6.3713 - mae: 1.2253 - val_loss: 18.5167 - val_mse: 18.5167 - val_mae: 1.2900 - lr: 1.8525e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 6s - loss: 6.2388 - mse: 6.2388 - mae: 1.2147 - val_loss: 18.9039 - val_mse: 18.9039 - val_mae: 1.2929 - lr: 1.8525e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 6s - loss: 5.9705 - mse: 5.9705 - mae: 1.2017 - val_loss: 18.9152 - val_mse: 18.9152 - val_mae: 1.3089 - lr: 1.8525e-04 - 6s/epoch - 6ms/step\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-10-12 08:42:30,594]\u001b[0m Finished trial#19 resulted in value: 10.596. Current best value is 8.826 with parameters: {'activation': 'tanh', 'num_hidden_layer': 4, 'i': 10, 'learning_rate': 0.00023399735711621326}.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Score for fold 5: loss of 18.915163040161133\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 14.7107 - mse: 14.7107 - mae: 1.5967 - val_loss: 12.4279 - val_mse: 12.4279 - val_mae: 1.4782 - lr: 5.6837e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 13.9266 - mse: 13.9266 - mae: 1.5250 - val_loss: 12.1632 - val_mse: 12.1632 - val_mae: 1.4638 - lr: 5.6837e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 13.7116 - mse: 13.7116 - mae: 1.5107 - val_loss: 12.1638 - val_mse: 12.1638 - val_mae: 1.5090 - lr: 5.6837e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 13.5878 - mse: 13.5878 - mae: 1.5023 - val_loss: 12.1635 - val_mse: 12.1635 - val_mae: 1.4898 - lr: 5.6837e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 13.5672 - mse: 13.5672 - mae: 1.4935 - val_loss: 12.0745 - val_mse: 12.0745 - val_mae: 1.5148 - lr: 5.6837e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 13.4118 - mse: 13.4118 - mae: 1.4914 - val_loss: 12.0077 - val_mse: 12.0077 - val_mae: 1.4513 - lr: 5.6837e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 13.3890 - mse: 13.3890 - mae: 1.4846 - val_loss: 11.9845 - val_mse: 11.9845 - val_mae: 1.4422 - lr: 5.6837e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 2s - loss: 13.3554 - mse: 13.3554 - mae: 1.4802 - val_loss: 11.8771 - val_mse: 11.8771 - val_mae: 1.4930 - lr: 5.6837e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 2s - loss: 13.2973 - mse: 13.2973 - mae: 1.4756 - val_loss: 11.8991 - val_mse: 11.8991 - val_mae: 1.4735 - lr: 5.6837e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 2s - loss: 13.2612 - mse: 13.2612 - mae: 1.4729 - val_loss: 11.8341 - val_mse: 11.8341 - val_mae: 1.4841 - lr: 5.6837e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 2s - loss: 13.2106 - mse: 13.2106 - mae: 1.4718 - val_loss: 11.8424 - val_mse: 11.8424 - val_mae: 1.4371 - lr: 5.6837e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 2s - loss: 13.1789 - mse: 13.1789 - mae: 1.4660 - val_loss: 11.8577 - val_mse: 11.8577 - val_mae: 1.4652 - lr: 5.6837e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 13/100\n",
            "1000/1000 - 2s - loss: 13.1690 - mse: 13.1690 - mae: 1.4669 - val_loss: 11.8938 - val_mse: 11.8938 - val_mae: 1.4520 - lr: 5.6837e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 14/100\n",
            "1000/1000 - 2s - loss: 13.1362 - mse: 13.1362 - mae: 1.4653 - val_loss: 11.8096 - val_mse: 11.8096 - val_mae: 1.4749 - lr: 5.6837e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 15/100\n",
            "1000/1000 - 2s - loss: 13.0870 - mse: 13.0870 - mae: 1.4602 - val_loss: 11.7820 - val_mse: 11.7820 - val_mae: 1.4869 - lr: 5.6837e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 16/100\n",
            "1000/1000 - 2s - loss: 13.0857 - mse: 13.0857 - mae: 1.4633 - val_loss: 11.7695 - val_mse: 11.7695 - val_mae: 1.4531 - lr: 5.6837e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 17/100\n",
            "1000/1000 - 2s - loss: 13.0364 - mse: 13.0364 - mae: 1.4607 - val_loss: 11.8568 - val_mse: 11.8568 - val_mae: 1.4266 - lr: 5.6837e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 18/100\n",
            "1000/1000 - 2s - loss: 13.0458 - mse: 13.0458 - mae: 1.4592 - val_loss: 11.7491 - val_mse: 11.7491 - val_mae: 1.4585 - lr: 5.6837e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 19/100\n",
            "1000/1000 - 2s - loss: 13.0233 - mse: 13.0233 - mae: 1.4554 - val_loss: 11.6811 - val_mse: 11.6811 - val_mae: 1.4408 - lr: 5.6837e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 20/100\n",
            "1000/1000 - 2s - loss: 13.0124 - mse: 13.0124 - mae: 1.4541 - val_loss: 11.6411 - val_mse: 11.6411 - val_mae: 1.4655 - lr: 5.6837e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 21/100\n",
            "1000/1000 - 2s - loss: 12.9714 - mse: 12.9714 - mae: 1.4548 - val_loss: 11.6691 - val_mse: 11.6691 - val_mae: 1.4771 - lr: 5.6837e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 22/100\n",
            "1000/1000 - 2s - loss: 12.9201 - mse: 12.9201 - mae: 1.4512 - val_loss: 11.7060 - val_mse: 11.7060 - val_mae: 1.4488 - lr: 5.6837e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 23/100\n",
            "1000/1000 - 2s - loss: 12.9640 - mse: 12.9640 - mae: 1.4545 - val_loss: 11.7161 - val_mse: 11.7161 - val_mae: 1.4435 - lr: 5.6837e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 24/100\n",
            "1000/1000 - 2s - loss: 12.9270 - mse: 12.9270 - mae: 1.4493 - val_loss: 11.6066 - val_mse: 11.6066 - val_mae: 1.4615 - lr: 5.6837e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 25/100\n",
            "1000/1000 - 2s - loss: 12.9019 - mse: 12.9019 - mae: 1.4509 - val_loss: 11.6881 - val_mse: 11.6881 - val_mae: 1.4082 - lr: 5.6837e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 26/100\n",
            "1000/1000 - 2s - loss: 12.8618 - mse: 12.8618 - mae: 1.4475 - val_loss: 11.6798 - val_mse: 11.6798 - val_mae: 1.4282 - lr: 5.6837e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 27/100\n",
            "1000/1000 - 2s - loss: 12.8726 - mse: 12.8726 - mae: 1.4488 - val_loss: 11.6317 - val_mse: 11.6317 - val_mae: 1.4444 - lr: 5.6837e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 28/100\n",
            "1000/1000 - 2s - loss: 12.7918 - mse: 12.7918 - mae: 1.4444 - val_loss: 11.6305 - val_mse: 11.6305 - val_mae: 1.4337 - lr: 5.6837e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 29/100\n",
            "1000/1000 - 2s - loss: 12.7957 - mse: 12.7957 - mae: 1.4443 - val_loss: 11.5822 - val_mse: 11.5822 - val_mae: 1.4425 - lr: 5.6837e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 30/100\n",
            "1000/1000 - 2s - loss: 12.7476 - mse: 12.7476 - mae: 1.4439 - val_loss: 11.7281 - val_mse: 11.7281 - val_mae: 1.4657 - lr: 5.6837e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 31/100\n",
            "1000/1000 - 2s - loss: 12.7648 - mse: 12.7648 - mae: 1.4428 - val_loss: 11.5696 - val_mse: 11.5696 - val_mae: 1.4315 - lr: 5.6837e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 32/100\n",
            "1000/1000 - 2s - loss: 12.7262 - mse: 12.7262 - mae: 1.4425 - val_loss: 11.4912 - val_mse: 11.4912 - val_mae: 1.4580 - lr: 5.6837e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 33/100\n",
            "1000/1000 - 2s - loss: 12.7054 - mse: 12.7054 - mae: 1.4414 - val_loss: 11.5771 - val_mse: 11.5771 - val_mae: 1.4093 - lr: 5.6837e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 34/100\n",
            "1000/1000 - 2s - loss: 12.6875 - mse: 12.6875 - mae: 1.4394 - val_loss: 11.4934 - val_mse: 11.4934 - val_mae: 1.4610 - lr: 5.6837e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 35/100\n",
            "1000/1000 - 2s - loss: 12.6908 - mse: 12.6908 - mae: 1.4390 - val_loss: 11.3859 - val_mse: 11.3859 - val_mae: 1.4556 - lr: 5.6837e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 36/100\n",
            "1000/1000 - 2s - loss: 12.6483 - mse: 12.6483 - mae: 1.4410 - val_loss: 11.3523 - val_mse: 11.3523 - val_mae: 1.4619 - lr: 5.6837e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 37/100\n",
            "1000/1000 - 2s - loss: 12.6376 - mse: 12.6376 - mae: 1.4352 - val_loss: 11.4618 - val_mse: 11.4618 - val_mae: 1.4913 - lr: 5.6837e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 38/100\n",
            "1000/1000 - 2s - loss: 12.5827 - mse: 12.5827 - mae: 1.4401 - val_loss: 11.4321 - val_mse: 11.4321 - val_mae: 1.4294 - lr: 5.6837e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 39/100\n",
            "1000/1000 - 2s - loss: 12.5795 - mse: 12.5795 - mae: 1.4369 - val_loss: 11.3486 - val_mse: 11.3486 - val_mae: 1.4803 - lr: 5.6837e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 40/100\n",
            "1000/1000 - 2s - loss: 12.5913 - mse: 12.5913 - mae: 1.4358 - val_loss: 11.2887 - val_mse: 11.2887 - val_mae: 1.4391 - lr: 5.6837e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 41/100\n",
            "1000/1000 - 2s - loss: 12.5327 - mse: 12.5327 - mae: 1.4367 - val_loss: 11.4119 - val_mse: 11.4119 - val_mae: 1.4769 - lr: 5.6837e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 42/100\n",
            "1000/1000 - 2s - loss: 12.5748 - mse: 12.5748 - mae: 1.4367 - val_loss: 11.3352 - val_mse: 11.3352 - val_mae: 1.4466 - lr: 5.6837e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 43/100\n",
            "1000/1000 - 2s - loss: 12.5396 - mse: 12.5396 - mae: 1.4346 - val_loss: 11.4439 - val_mse: 11.4439 - val_mae: 1.4386 - lr: 5.6837e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 44/100\n",
            "1000/1000 - 2s - loss: 12.5167 - mse: 12.5167 - mae: 1.4347 - val_loss: 11.2620 - val_mse: 11.2620 - val_mae: 1.4433 - lr: 5.6837e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 45/100\n",
            "1000/1000 - 2s - loss: 12.4501 - mse: 12.4501 - mae: 1.4348 - val_loss: 11.3925 - val_mse: 11.3925 - val_mae: 1.4515 - lr: 5.6837e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 46/100\n",
            "1000/1000 - 2s - loss: 12.4895 - mse: 12.4895 - mae: 1.4327 - val_loss: 11.2064 - val_mse: 11.2064 - val_mae: 1.4651 - lr: 5.6837e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 47/100\n",
            "1000/1000 - 2s - loss: 12.4540 - mse: 12.4540 - mae: 1.4332 - val_loss: 11.3813 - val_mse: 11.3813 - val_mae: 1.4658 - lr: 5.6837e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 48/100\n",
            "1000/1000 - 2s - loss: 12.4281 - mse: 12.4281 - mae: 1.4313 - val_loss: 11.2869 - val_mse: 11.2869 - val_mae: 1.4530 - lr: 5.6837e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 49/100\n",
            "1000/1000 - 2s - loss: 12.4263 - mse: 12.4263 - mae: 1.4323 - val_loss: 11.3665 - val_mse: 11.3665 - val_mae: 1.4051 - lr: 5.6837e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 50/100\n",
            "1000/1000 - 2s - loss: 12.4050 - mse: 12.4050 - mae: 1.4310 - val_loss: 11.3582 - val_mse: 11.3582 - val_mae: 1.4477 - lr: 5.6837e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 51/100\n",
            "1000/1000 - 2s - loss: 12.4146 - mse: 12.4146 - mae: 1.4344 - val_loss: 11.1475 - val_mse: 11.1475 - val_mae: 1.4639 - lr: 5.6837e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 52/100\n",
            "1000/1000 - 2s - loss: 12.3929 - mse: 12.3929 - mae: 1.4322 - val_loss: 11.2549 - val_mse: 11.2549 - val_mae: 1.4647 - lr: 5.6837e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 53/100\n",
            "1000/1000 - 2s - loss: 12.3657 - mse: 12.3657 - mae: 1.4267 - val_loss: 11.3034 - val_mse: 11.3034 - val_mae: 1.4632 - lr: 5.6837e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 54/100\n",
            "1000/1000 - 2s - loss: 12.3582 - mse: 12.3582 - mae: 1.4292 - val_loss: 11.3027 - val_mse: 11.3027 - val_mae: 1.4429 - lr: 5.6837e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 55/100\n",
            "1000/1000 - 2s - loss: 12.3410 - mse: 12.3410 - mae: 1.4299 - val_loss: 11.2899 - val_mse: 11.2899 - val_mae: 1.4345 - lr: 5.6837e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 56/100\n",
            "1000/1000 - 2s - loss: 12.3454 - mse: 12.3454 - mae: 1.4268 - val_loss: 11.1857 - val_mse: 11.1857 - val_mae: 1.4901 - lr: 5.6837e-04 - 2s/epoch - 2ms/step\n",
            "Score for fold 1: loss of 11.185691833496094\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 11.9275 - mse: 11.9275 - mae: 1.4293 - val_loss: 12.8177 - val_mse: 12.8177 - val_mae: 1.4374 - lr: 5.6837e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 11.8900 - mse: 11.8900 - mae: 1.4278 - val_loss: 12.9143 - val_mse: 12.9143 - val_mae: 1.4276 - lr: 5.6837e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 11.8578 - mse: 11.8578 - mae: 1.4279 - val_loss: 12.8630 - val_mse: 12.8630 - val_mae: 1.4389 - lr: 5.6837e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 11.8001 - mse: 11.8001 - mae: 1.4260 - val_loss: 12.8960 - val_mse: 12.8960 - val_mae: 1.4648 - lr: 5.6837e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 11.7725 - mse: 11.7725 - mae: 1.4221 - val_loss: 12.9322 - val_mse: 12.9322 - val_mae: 1.4284 - lr: 5.6837e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 11.7349 - mse: 11.7349 - mae: 1.4222 - val_loss: 12.9737 - val_mse: 12.9737 - val_mae: 1.4517 - lr: 5.6837e-04 - 2s/epoch - 2ms/step\n",
            "Score for fold 2: loss of 12.973700523376465\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 12.6225 - mse: 12.6225 - mae: 1.4281 - val_loss: 9.4176 - val_mse: 9.4176 - val_mae: 1.4792 - lr: 5.6837e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 12.6170 - mse: 12.6170 - mae: 1.4241 - val_loss: 9.3494 - val_mse: 9.3494 - val_mae: 1.4504 - lr: 5.6837e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 12.5393 - mse: 12.5393 - mae: 1.4211 - val_loss: 9.4108 - val_mse: 9.4108 - val_mae: 1.4214 - lr: 5.6837e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 12.5105 - mse: 12.5105 - mae: 1.4185 - val_loss: 9.4806 - val_mse: 9.4806 - val_mae: 1.4691 - lr: 5.6837e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 12.4700 - mse: 12.4700 - mae: 1.4185 - val_loss: 9.4837 - val_mse: 9.4837 - val_mae: 1.4428 - lr: 5.6837e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 12.4881 - mse: 12.4881 - mae: 1.4224 - val_loss: 9.5961 - val_mse: 9.5961 - val_mae: 1.4825 - lr: 5.6837e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 12.4079 - mse: 12.4079 - mae: 1.4189 - val_loss: 9.5484 - val_mse: 9.5484 - val_mae: 1.4330 - lr: 5.6837e-04 - 2s/epoch - 2ms/step\n",
            "Score for fold 3: loss of 9.548364639282227\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 10.8227 - mse: 10.8227 - mae: 1.4321 - val_loss: 15.8891 - val_mse: 15.8891 - val_mae: 1.4332 - lr: 5.6837e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 10.7553 - mse: 10.7553 - mae: 1.4309 - val_loss: 15.9639 - val_mse: 15.9639 - val_mae: 1.4146 - lr: 5.6837e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 10.7210 - mse: 10.7210 - mae: 1.4268 - val_loss: 16.0393 - val_mse: 16.0393 - val_mae: 1.4056 - lr: 5.6837e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 10.6975 - mse: 10.6975 - mae: 1.4259 - val_loss: 16.0609 - val_mse: 16.0609 - val_mae: 1.3829 - lr: 5.6837e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 10.6472 - mse: 10.6472 - mae: 1.4253 - val_loss: 16.2132 - val_mse: 16.2132 - val_mae: 1.4420 - lr: 5.6837e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 10.6687 - mse: 10.6687 - mae: 1.4241 - val_loss: 16.0653 - val_mse: 16.0653 - val_mae: 1.4264 - lr: 5.6837e-04 - 2s/epoch - 2ms/step\n",
            "Score for fold 4: loss of 16.065275192260742\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 12.0929 - mse: 12.0929 - mae: 1.4296 - val_loss: 10.3049 - val_mse: 10.3049 - val_mae: 1.3782 - lr: 5.6837e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 12.0678 - mse: 12.0678 - mae: 1.4265 - val_loss: 10.2735 - val_mse: 10.2735 - val_mae: 1.4100 - lr: 5.6837e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 12.0595 - mse: 12.0595 - mae: 1.4254 - val_loss: 10.4569 - val_mse: 10.4569 - val_mae: 1.4203 - lr: 5.6837e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 12.0448 - mse: 12.0448 - mae: 1.4238 - val_loss: 10.4603 - val_mse: 10.4603 - val_mae: 1.4523 - lr: 5.6837e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 12.0350 - mse: 12.0350 - mae: 1.4260 - val_loss: 10.5438 - val_mse: 10.5438 - val_mae: 1.4171 - lr: 5.6837e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 12.0121 - mse: 12.0121 - mae: 1.4204 - val_loss: 10.3529 - val_mse: 10.3529 - val_mae: 1.4182 - lr: 5.6837e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 11.9605 - mse: 11.9605 - mae: 1.4210 - val_loss: 10.4979 - val_mse: 10.4979 - val_mae: 1.4064 - lr: 5.6837e-04 - 2s/epoch - 2ms/step\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-10-12 08:45:01,344]\u001b[0m Finished trial#20 resulted in value: 12.056000000000001. Current best value is 8.826 with parameters: {'activation': 'tanh', 'num_hidden_layer': 4, 'i': 10, 'learning_rate': 0.00023399735711621326}.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Score for fold 5: loss of 10.497871398925781\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 7s - loss: 13.7052 - mse: 13.7052 - mae: 1.5500 - val_loss: 10.5359 - val_mse: 10.5359 - val_mae: 1.5240 - lr: 1.8326e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 7s - loss: 12.8239 - mse: 12.8239 - mae: 1.5036 - val_loss: 10.2492 - val_mse: 10.2492 - val_mae: 1.4662 - lr: 1.8326e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 6s - loss: 12.6336 - mse: 12.6336 - mae: 1.4826 - val_loss: 10.5222 - val_mse: 10.5222 - val_mae: 1.4088 - lr: 1.8326e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 6s - loss: 12.6093 - mse: 12.6093 - mae: 1.4665 - val_loss: 10.4967 - val_mse: 10.4967 - val_mae: 1.3883 - lr: 1.8326e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 6s - loss: 12.4567 - mse: 12.4567 - mae: 1.4618 - val_loss: 10.1892 - val_mse: 10.1892 - val_mae: 1.5067 - lr: 1.8326e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 6s - loss: 12.4096 - mse: 12.4096 - mae: 1.4555 - val_loss: 10.6654 - val_mse: 10.6654 - val_mae: 1.4256 - lr: 1.8326e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 6s - loss: 12.0898 - mse: 12.0898 - mae: 1.4447 - val_loss: 10.3808 - val_mse: 10.3808 - val_mae: 1.4202 - lr: 1.8326e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 6s - loss: 12.0163 - mse: 12.0163 - mae: 1.4385 - val_loss: 10.3645 - val_mse: 10.3645 - val_mae: 1.4339 - lr: 1.8326e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 6s - loss: 11.9660 - mse: 11.9660 - mae: 1.4352 - val_loss: 10.5419 - val_mse: 10.5419 - val_mae: 1.4070 - lr: 1.8326e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 6s - loss: 11.8806 - mse: 11.8806 - mae: 1.4269 - val_loss: 10.4765 - val_mse: 10.4765 - val_mae: 1.3943 - lr: 1.8326e-04 - 6s/epoch - 6ms/step\n",
            "Score for fold 1: loss of 10.47653579711914\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 6s - loss: 11.3767 - mse: 11.3767 - mae: 1.4183 - val_loss: 12.4069 - val_mse: 12.4069 - val_mae: 1.4552 - lr: 1.8326e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 6s - loss: 11.2457 - mse: 11.2457 - mae: 1.4121 - val_loss: 12.4405 - val_mse: 12.4405 - val_mae: 1.3822 - lr: 1.8326e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 6s - loss: 11.0614 - mse: 11.0614 - mae: 1.4039 - val_loss: 12.1837 - val_mse: 12.1837 - val_mae: 1.3823 - lr: 1.8326e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 6s - loss: 10.9585 - mse: 10.9585 - mae: 1.3950 - val_loss: 12.1820 - val_mse: 12.1820 - val_mae: 1.4319 - lr: 1.8326e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 6s - loss: 10.9381 - mse: 10.9381 - mae: 1.3931 - val_loss: 12.3478 - val_mse: 12.3478 - val_mae: 1.4557 - lr: 1.8326e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 6s - loss: 10.8151 - mse: 10.8151 - mae: 1.3874 - val_loss: 12.3079 - val_mse: 12.3079 - val_mae: 1.3984 - lr: 1.8326e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 6s - loss: 10.6533 - mse: 10.6533 - mae: 1.3755 - val_loss: 11.8396 - val_mse: 11.8396 - val_mae: 1.4228 - lr: 1.8326e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 6s - loss: 10.4737 - mse: 10.4737 - mae: 1.3643 - val_loss: 12.0261 - val_mse: 12.0261 - val_mae: 1.4367 - lr: 1.8326e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 6s - loss: 10.3371 - mse: 10.3371 - mae: 1.3577 - val_loss: 12.2410 - val_mse: 12.2410 - val_mae: 1.4034 - lr: 1.8326e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 6s - loss: 10.2933 - mse: 10.2933 - mae: 1.3503 - val_loss: 11.6945 - val_mse: 11.6945 - val_mae: 1.4746 - lr: 1.8326e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 6s - loss: 10.2241 - mse: 10.2241 - mae: 1.3434 - val_loss: 11.8744 - val_mse: 11.8744 - val_mae: 1.4123 - lr: 1.8326e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 6s - loss: 10.0444 - mse: 10.0444 - mae: 1.3346 - val_loss: 12.2796 - val_mse: 12.2796 - val_mae: 1.3885 - lr: 1.8326e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 13/100\n",
            "1000/1000 - 6s - loss: 9.8665 - mse: 9.8665 - mae: 1.3220 - val_loss: 12.0014 - val_mse: 12.0014 - val_mae: 1.4565 - lr: 1.8326e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 14/100\n",
            "1000/1000 - 6s - loss: 9.7293 - mse: 9.7293 - mae: 1.3143 - val_loss: 12.2899 - val_mse: 12.2899 - val_mae: 1.4211 - lr: 1.8326e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 15/100\n",
            "1000/1000 - 6s - loss: 9.6687 - mse: 9.6687 - mae: 1.3075 - val_loss: 11.9348 - val_mse: 11.9348 - val_mae: 1.4748 - lr: 1.8326e-04 - 6s/epoch - 6ms/step\n",
            "Score for fold 2: loss of 11.934830665588379\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 6s - loss: 9.3247 - mse: 9.3247 - mae: 1.3417 - val_loss: 14.8828 - val_mse: 14.8828 - val_mae: 1.3579 - lr: 1.8326e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 6s - loss: 8.8391 - mse: 8.8391 - mae: 1.3245 - val_loss: 15.4743 - val_mse: 15.4743 - val_mae: 1.3724 - lr: 1.8326e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 6s - loss: 8.5870 - mse: 8.5870 - mae: 1.3112 - val_loss: 15.0507 - val_mse: 15.0507 - val_mae: 1.3651 - lr: 1.8326e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 6s - loss: 8.5139 - mse: 8.5139 - mae: 1.3017 - val_loss: 15.3402 - val_mse: 15.3402 - val_mae: 1.3617 - lr: 1.8326e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 6s - loss: 8.4476 - mse: 8.4476 - mae: 1.2897 - val_loss: 15.0873 - val_mse: 15.0873 - val_mae: 1.3525 - lr: 1.8326e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 6s - loss: 8.1568 - mse: 8.1568 - mae: 1.2793 - val_loss: 15.4056 - val_mse: 15.4056 - val_mae: 1.3603 - lr: 1.8326e-04 - 6s/epoch - 6ms/step\n",
            "Score for fold 3: loss of 15.405577659606934\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 6s - loss: 10.1791 - mse: 10.1791 - mae: 1.3016 - val_loss: 6.6774 - val_mse: 6.6774 - val_mae: 1.2889 - lr: 1.8326e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 6s - loss: 9.8974 - mse: 9.8974 - mae: 1.2858 - val_loss: 6.9921 - val_mse: 6.9921 - val_mae: 1.2196 - lr: 1.8326e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 6s - loss: 9.7315 - mse: 9.7315 - mae: 1.2743 - val_loss: 6.9939 - val_mse: 6.9939 - val_mae: 1.2758 - lr: 1.8326e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 6s - loss: 9.7925 - mse: 9.7925 - mae: 1.2612 - val_loss: 7.1172 - val_mse: 7.1172 - val_mae: 1.2885 - lr: 1.8326e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 6s - loss: 9.5300 - mse: 9.5300 - mae: 1.2508 - val_loss: 7.1195 - val_mse: 7.1195 - val_mae: 1.2978 - lr: 1.8326e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 6s - loss: 9.2961 - mse: 9.2961 - mae: 1.2325 - val_loss: 7.1678 - val_mse: 7.1678 - val_mae: 1.3654 - lr: 1.8326e-04 - 6s/epoch - 6ms/step\n",
            "Score for fold 4: loss of 7.167783737182617\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 6s - loss: 9.1915 - mse: 9.1915 - mae: 1.2538 - val_loss: 6.9481 - val_mse: 6.9481 - val_mae: 1.2262 - lr: 1.8326e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 6s - loss: 9.1651 - mse: 9.1651 - mae: 1.2391 - val_loss: 6.9972 - val_mse: 6.9972 - val_mae: 1.2332 - lr: 1.8326e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 6s - loss: 8.8733 - mse: 8.8733 - mae: 1.2234 - val_loss: 7.2251 - val_mse: 7.2251 - val_mae: 1.2279 - lr: 1.8326e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 6s - loss: 8.7632 - mse: 8.7632 - mae: 1.2116 - val_loss: 7.3099 - val_mse: 7.3099 - val_mae: 1.2619 - lr: 1.8326e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 6s - loss: 8.5906 - mse: 8.5906 - mae: 1.1966 - val_loss: 7.2388 - val_mse: 7.2388 - val_mae: 1.2239 - lr: 1.8326e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 7s - loss: 8.4578 - mse: 8.4578 - mae: 1.1812 - val_loss: 7.3541 - val_mse: 7.3541 - val_mae: 1.2475 - lr: 1.8326e-04 - 7s/epoch - 7ms/step\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-10-12 08:49:28,807]\u001b[0m Finished trial#21 resulted in value: 10.468. Current best value is 8.826 with parameters: {'activation': 'tanh', 'num_hidden_layer': 4, 'i': 10, 'learning_rate': 0.00023399735711621326}.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Score for fold 5: loss of 7.354133129119873\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 8s - loss: 12.0257 - mse: 12.0257 - mae: 1.5407 - val_loss: 17.6072 - val_mse: 17.6072 - val_mae: 1.5662 - lr: 1.8897e-04 - 8s/epoch - 8ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 8s - loss: 11.1790 - mse: 11.1790 - mae: 1.4926 - val_loss: 17.2006 - val_mse: 17.2006 - val_mae: 1.4583 - lr: 1.8897e-04 - 8s/epoch - 8ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 7s - loss: 10.8746 - mse: 10.8746 - mae: 1.4637 - val_loss: 17.2840 - val_mse: 17.2840 - val_mae: 1.5635 - lr: 1.8897e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 7s - loss: 10.7155 - mse: 10.7155 - mae: 1.4562 - val_loss: 17.1412 - val_mse: 17.1412 - val_mae: 1.5043 - lr: 1.8897e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 7s - loss: 10.6530 - mse: 10.6530 - mae: 1.4475 - val_loss: 16.9578 - val_mse: 16.9578 - val_mae: 1.4971 - lr: 1.8897e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 7s - loss: 10.6894 - mse: 10.6894 - mae: 1.4439 - val_loss: 17.2503 - val_mse: 17.2503 - val_mae: 1.4503 - lr: 1.8897e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 7s - loss: 10.6051 - mse: 10.6051 - mae: 1.4395 - val_loss: 17.4508 - val_mse: 17.4508 - val_mae: 1.4741 - lr: 1.8897e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 7s - loss: 10.2764 - mse: 10.2764 - mae: 1.4295 - val_loss: 17.5503 - val_mse: 17.5503 - val_mae: 1.4532 - lr: 1.8897e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 7s - loss: 10.2887 - mse: 10.2887 - mae: 1.4232 - val_loss: 17.3059 - val_mse: 17.3059 - val_mae: 1.4703 - lr: 1.8897e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 7s - loss: 10.3543 - mse: 10.3543 - mae: 1.4199 - val_loss: 17.3028 - val_mse: 17.3028 - val_mae: 1.3936 - lr: 1.8897e-04 - 7s/epoch - 7ms/step\n",
            "Score for fold 1: loss of 17.302839279174805\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 7s - loss: 11.4928 - mse: 11.4928 - mae: 1.4267 - val_loss: 11.0921 - val_mse: 11.0921 - val_mae: 1.4036 - lr: 1.8897e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 6s - loss: 11.4315 - mse: 11.4315 - mae: 1.4178 - val_loss: 11.3566 - val_mse: 11.3566 - val_mae: 1.3697 - lr: 1.8897e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 6s - loss: 11.3769 - mse: 11.3769 - mae: 1.4124 - val_loss: 11.1573 - val_mse: 11.1573 - val_mae: 1.3996 - lr: 1.8897e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 6s - loss: 11.2412 - mse: 11.2412 - mae: 1.4031 - val_loss: 11.0235 - val_mse: 11.0235 - val_mae: 1.4277 - lr: 1.8897e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 6s - loss: 11.0170 - mse: 11.0170 - mae: 1.3968 - val_loss: 11.2295 - val_mse: 11.2295 - val_mae: 1.4088 - lr: 1.8897e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 7s - loss: 10.9972 - mse: 10.9972 - mae: 1.3900 - val_loss: 11.8063 - val_mse: 11.8063 - val_mae: 1.4076 - lr: 1.8897e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 7s - loss: 10.5851 - mse: 10.5851 - mae: 1.3815 - val_loss: 11.3726 - val_mse: 11.3726 - val_mae: 1.4036 - lr: 1.8897e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 7s - loss: 10.5817 - mse: 10.5817 - mae: 1.3737 - val_loss: 11.1876 - val_mse: 11.1876 - val_mae: 1.4476 - lr: 1.8897e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 7s - loss: 10.6493 - mse: 10.6493 - mae: 1.3663 - val_loss: 11.4099 - val_mse: 11.4099 - val_mae: 1.3831 - lr: 1.8897e-04 - 7s/epoch - 7ms/step\n",
            "Score for fold 2: loss of 11.409890174865723\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 6s - loss: 10.4482 - mse: 10.4482 - mae: 1.3749 - val_loss: 11.3097 - val_mse: 11.3097 - val_mae: 1.4694 - lr: 1.8897e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 6s - loss: 10.5608 - mse: 10.5608 - mae: 1.3633 - val_loss: 11.5975 - val_mse: 11.5975 - val_mae: 1.4061 - lr: 1.8897e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 6s - loss: 10.3038 - mse: 10.3038 - mae: 1.3556 - val_loss: 11.1873 - val_mse: 11.1873 - val_mae: 1.3689 - lr: 1.8897e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 6s - loss: 9.9143 - mse: 9.9143 - mae: 1.3404 - val_loss: 11.3834 - val_mse: 11.3834 - val_mae: 1.4199 - lr: 1.8897e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 7s - loss: 9.9006 - mse: 9.9006 - mae: 1.3318 - val_loss: 11.5363 - val_mse: 11.5363 - val_mae: 1.3602 - lr: 1.8897e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 6s - loss: 9.8560 - mse: 9.8560 - mae: 1.3208 - val_loss: 11.9700 - val_mse: 11.9700 - val_mae: 1.4139 - lr: 1.8897e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 6s - loss: 9.5017 - mse: 9.5017 - mae: 1.3103 - val_loss: 11.5448 - val_mse: 11.5448 - val_mae: 1.3916 - lr: 1.8897e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 6s - loss: 9.4139 - mse: 9.4139 - mae: 1.3024 - val_loss: 11.8400 - val_mse: 11.8400 - val_mae: 1.3792 - lr: 1.8897e-04 - 6s/epoch - 6ms/step\n",
            "Score for fold 3: loss of 11.83999252319336\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 6s - loss: 10.6860 - mse: 10.6860 - mae: 1.3282 - val_loss: 6.8708 - val_mse: 6.8708 - val_mae: 1.3187 - lr: 1.8897e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 7s - loss: 10.4247 - mse: 10.4247 - mae: 1.3129 - val_loss: 7.0167 - val_mse: 7.0167 - val_mae: 1.3061 - lr: 1.8897e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 6s - loss: 10.1536 - mse: 10.1536 - mae: 1.3015 - val_loss: 7.9129 - val_mse: 7.9129 - val_mae: 1.3335 - lr: 1.8897e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 6s - loss: 9.9127 - mse: 9.9127 - mae: 1.2893 - val_loss: 7.2760 - val_mse: 7.2760 - val_mae: 1.2878 - lr: 1.8897e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 6s - loss: 9.9785 - mse: 9.9785 - mae: 1.2808 - val_loss: 7.1613 - val_mse: 7.1613 - val_mae: 1.2892 - lr: 1.8897e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 6s - loss: 9.7340 - mse: 9.7340 - mae: 1.2673 - val_loss: 7.5017 - val_mse: 7.5017 - val_mae: 1.3231 - lr: 1.8897e-04 - 6s/epoch - 6ms/step\n",
            "Score for fold 4: loss of 7.50167989730835\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 6s - loss: 10.3505 - mse: 10.3505 - mae: 1.2886 - val_loss: 5.3742 - val_mse: 5.3742 - val_mae: 1.3112 - lr: 1.8897e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 7s - loss: 10.0903 - mse: 10.0903 - mae: 1.2747 - val_loss: 5.5228 - val_mse: 5.5228 - val_mae: 1.2419 - lr: 1.8897e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 6s - loss: 9.7950 - mse: 9.7950 - mae: 1.2544 - val_loss: 5.0493 - val_mse: 5.0493 - val_mae: 1.2527 - lr: 1.8897e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 7s - loss: 9.7546 - mse: 9.7546 - mae: 1.2474 - val_loss: 5.3184 - val_mse: 5.3184 - val_mae: 1.2414 - lr: 1.8897e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 6s - loss: 9.6538 - mse: 9.6538 - mae: 1.2305 - val_loss: 5.5037 - val_mse: 5.5037 - val_mae: 1.2521 - lr: 1.8897e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 6s - loss: 9.4656 - mse: 9.4656 - mae: 1.2201 - val_loss: 5.4645 - val_mse: 5.4645 - val_mae: 1.2550 - lr: 1.8897e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 6s - loss: 9.3216 - mse: 9.3216 - mae: 1.2050 - val_loss: 5.3388 - val_mse: 5.3388 - val_mae: 1.2766 - lr: 1.8897e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 6s - loss: 9.0589 - mse: 9.0589 - mae: 1.1940 - val_loss: 5.3838 - val_mse: 5.3838 - val_mae: 1.2954 - lr: 1.8897e-04 - 6s/epoch - 6ms/step\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-10-12 08:54:08,082]\u001b[0m Finished trial#22 resulted in value: 10.686. Current best value is 8.826 with parameters: {'activation': 'tanh', 'num_hidden_layer': 4, 'i': 10, 'learning_rate': 0.00023399735711621326}.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Score for fold 5: loss of 5.383771896362305\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 7s - loss: 13.9253 - mse: 13.9253 - mae: 1.5409 - val_loss: 8.4404 - val_mse: 8.4404 - val_mae: 1.4709 - lr: 3.5723e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 6s - loss: 13.3718 - mse: 13.3718 - mae: 1.4945 - val_loss: 8.2742 - val_mse: 8.2742 - val_mae: 1.4159 - lr: 3.5723e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 6s - loss: 13.1226 - mse: 13.1226 - mae: 1.4682 - val_loss: 8.1422 - val_mse: 8.1422 - val_mae: 1.4605 - lr: 3.5723e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 7s - loss: 13.1109 - mse: 13.1109 - mae: 1.4640 - val_loss: 8.1049 - val_mse: 8.1049 - val_mae: 1.4594 - lr: 3.5723e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 7s - loss: 12.9496 - mse: 12.9496 - mae: 1.4552 - val_loss: 8.1638 - val_mse: 8.1638 - val_mae: 1.4416 - lr: 3.5723e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 7s - loss: 12.7428 - mse: 12.7428 - mae: 1.4487 - val_loss: 7.9268 - val_mse: 7.9268 - val_mae: 1.4074 - lr: 3.5723e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 7s - loss: 12.8563 - mse: 12.8563 - mae: 1.4416 - val_loss: 8.2366 - val_mse: 8.2366 - val_mae: 1.5083 - lr: 3.5723e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 7s - loss: 12.5135 - mse: 12.5135 - mae: 1.4324 - val_loss: 8.0828 - val_mse: 8.0828 - val_mae: 1.4129 - lr: 3.5723e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 6s - loss: 12.2714 - mse: 12.2714 - mae: 1.4243 - val_loss: 7.9476 - val_mse: 7.9476 - val_mae: 1.4326 - lr: 3.5723e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 6s - loss: 12.4040 - mse: 12.4040 - mae: 1.4201 - val_loss: 8.0011 - val_mse: 8.0011 - val_mae: 1.3970 - lr: 3.5723e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 6s - loss: 12.0860 - mse: 12.0860 - mae: 1.4093 - val_loss: 8.1547 - val_mse: 8.1547 - val_mae: 1.4204 - lr: 3.5723e-04 - 6s/epoch - 6ms/step\n",
            "Score for fold 1: loss of 8.154722213745117\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 6s - loss: 12.0235 - mse: 12.0235 - mae: 1.4257 - val_loss: 8.2342 - val_mse: 8.2342 - val_mae: 1.3314 - lr: 3.5723e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 6s - loss: 11.7476 - mse: 11.7476 - mae: 1.4124 - val_loss: 8.4038 - val_mse: 8.4038 - val_mae: 1.3969 - lr: 3.5723e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 6s - loss: 11.5074 - mse: 11.5074 - mae: 1.4033 - val_loss: 8.5047 - val_mse: 8.5047 - val_mae: 1.4031 - lr: 3.5723e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 6s - loss: 11.4395 - mse: 11.4395 - mae: 1.3959 - val_loss: 9.3429 - val_mse: 9.3429 - val_mae: 1.4193 - lr: 3.5723e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 6s - loss: 11.2257 - mse: 11.2257 - mae: 1.3811 - val_loss: 8.9945 - val_mse: 8.9945 - val_mae: 1.4317 - lr: 3.5723e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 7s - loss: 11.3154 - mse: 11.3154 - mae: 1.3763 - val_loss: 9.6691 - val_mse: 9.6691 - val_mae: 1.3985 - lr: 3.5723e-04 - 7s/epoch - 7ms/step\n",
            "Score for fold 2: loss of 9.669060707092285\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 6s - loss: 9.8949 - mse: 9.8949 - mae: 1.3816 - val_loss: 14.1845 - val_mse: 14.1845 - val_mae: 1.3625 - lr: 3.5723e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 7s - loss: 9.6339 - mse: 9.6339 - mae: 1.3638 - val_loss: 14.3173 - val_mse: 14.3173 - val_mae: 1.3400 - lr: 3.5723e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 6s - loss: 9.2095 - mse: 9.2095 - mae: 1.3497 - val_loss: 14.6843 - val_mse: 14.6843 - val_mae: 1.4041 - lr: 3.5723e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 7s - loss: 8.9681 - mse: 8.9681 - mae: 1.3371 - val_loss: 14.1542 - val_mse: 14.1542 - val_mae: 1.3877 - lr: 3.5723e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 7s - loss: 8.8471 - mse: 8.8471 - mae: 1.3257 - val_loss: 15.4448 - val_mse: 15.4448 - val_mae: 1.4300 - lr: 3.5723e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 7s - loss: 8.7039 - mse: 8.7039 - mae: 1.3122 - val_loss: 14.6470 - val_mse: 14.6470 - val_mae: 1.4097 - lr: 3.5723e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 7s - loss: 8.5293 - mse: 8.5293 - mae: 1.2998 - val_loss: 14.7590 - val_mse: 14.7590 - val_mae: 1.4071 - lr: 3.5723e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 7s - loss: 8.2959 - mse: 8.2959 - mae: 1.2873 - val_loss: 14.8387 - val_mse: 14.8387 - val_mae: 1.3988 - lr: 3.5723e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 6s - loss: 8.1624 - mse: 8.1624 - mae: 1.2716 - val_loss: 15.0439 - val_mse: 15.0439 - val_mae: 1.3945 - lr: 3.5723e-04 - 6s/epoch - 6ms/step\n",
            "Score for fold 3: loss of 15.04384994506836\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 6s - loss: 8.2093 - mse: 8.2093 - mae: 1.2893 - val_loss: 15.2622 - val_mse: 15.2622 - val_mae: 1.3477 - lr: 3.5723e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 6s - loss: 8.0477 - mse: 8.0477 - mae: 1.2748 - val_loss: 14.3038 - val_mse: 14.3038 - val_mae: 1.3176 - lr: 3.5723e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 6s - loss: 7.7061 - mse: 7.7061 - mae: 1.2540 - val_loss: 14.5119 - val_mse: 14.5119 - val_mae: 1.3581 - lr: 3.5723e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 6s - loss: 7.5350 - mse: 7.5350 - mae: 1.2402 - val_loss: 15.4921 - val_mse: 15.4921 - val_mae: 1.3431 - lr: 3.5723e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 6s - loss: 7.3401 - mse: 7.3401 - mae: 1.2221 - val_loss: 14.7142 - val_mse: 14.7142 - val_mae: 1.3369 - lr: 3.5723e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 7s - loss: 7.1120 - mse: 7.1120 - mae: 1.2051 - val_loss: 14.3797 - val_mse: 14.3797 - val_mae: 1.3487 - lr: 3.5723e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 7s - loss: 7.0168 - mse: 7.0168 - mae: 1.1897 - val_loss: 14.9296 - val_mse: 14.9296 - val_mae: 1.3919 - lr: 3.5723e-04 - 7s/epoch - 7ms/step\n",
            "Score for fold 4: loss of 14.929640769958496\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 7s - loss: 9.3702 - mse: 9.3702 - mae: 1.2346 - val_loss: 4.8415 - val_mse: 4.8415 - val_mae: 1.1342 - lr: 3.5723e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 7s - loss: 9.0384 - mse: 9.0384 - mae: 1.2115 - val_loss: 5.2885 - val_mse: 5.2885 - val_mae: 1.1729 - lr: 3.5723e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 7s - loss: 8.6904 - mse: 8.6904 - mae: 1.1869 - val_loss: 5.1106 - val_mse: 5.1106 - val_mae: 1.1824 - lr: 3.5723e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 7s - loss: 8.4212 - mse: 8.4212 - mae: 1.1687 - val_loss: 5.7212 - val_mse: 5.7212 - val_mae: 1.2036 - lr: 3.5723e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 7s - loss: 8.4646 - mse: 8.4646 - mae: 1.1518 - val_loss: 5.1336 - val_mse: 5.1336 - val_mae: 1.1970 - lr: 3.5723e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 7s - loss: 8.2945 - mse: 8.2945 - mae: 1.1342 - val_loss: 5.2645 - val_mse: 5.2645 - val_mae: 1.2289 - lr: 3.5723e-04 - 7s/epoch - 7ms/step\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-10-12 08:58:31,311]\u001b[0m Finished trial#23 resulted in value: 10.61. Current best value is 8.826 with parameters: {'activation': 'tanh', 'num_hidden_layer': 4, 'i': 10, 'learning_rate': 0.00023399735711621326}.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Score for fold 5: loss of 5.264473915100098\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 5s - loss: 14.4834 - mse: 14.4834 - mae: 1.5575 - val_loss: 8.9319 - val_mse: 8.9319 - val_mae: 1.4971 - lr: 1.6570e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 4s - loss: 13.4462 - mse: 13.4462 - mae: 1.5056 - val_loss: 8.6658 - val_mse: 8.6658 - val_mae: 1.4411 - lr: 1.6570e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 4s - loss: 13.1651 - mse: 13.1651 - mae: 1.4886 - val_loss: 8.3258 - val_mse: 8.3258 - val_mae: 1.4552 - lr: 1.6570e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 4s - loss: 12.8944 - mse: 12.8944 - mae: 1.4702 - val_loss: 8.3384 - val_mse: 8.3384 - val_mae: 1.4132 - lr: 1.6570e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 4s - loss: 13.0053 - mse: 13.0053 - mae: 1.4647 - val_loss: 8.2913 - val_mse: 8.2913 - val_mae: 1.4804 - lr: 1.6570e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 4s - loss: 12.8435 - mse: 12.8435 - mae: 1.4594 - val_loss: 8.1848 - val_mse: 8.1848 - val_mae: 1.4354 - lr: 1.6570e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 4s - loss: 12.7368 - mse: 12.7368 - mae: 1.4514 - val_loss: 8.1222 - val_mse: 8.1222 - val_mae: 1.4460 - lr: 1.6570e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 3s - loss: 12.6378 - mse: 12.6378 - mae: 1.4462 - val_loss: 8.1856 - val_mse: 8.1856 - val_mae: 1.4528 - lr: 1.6570e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 3s - loss: 12.5947 - mse: 12.5947 - mae: 1.4437 - val_loss: 8.4208 - val_mse: 8.4208 - val_mae: 1.4509 - lr: 1.6570e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 3s - loss: 12.6556 - mse: 12.6556 - mae: 1.4416 - val_loss: 8.1940 - val_mse: 8.1940 - val_mae: 1.4508 - lr: 1.6570e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 3s - loss: 12.6397 - mse: 12.6397 - mae: 1.4357 - val_loss: 8.1491 - val_mse: 8.1491 - val_mae: 1.4614 - lr: 1.6570e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 3s - loss: 12.3615 - mse: 12.3615 - mae: 1.4300 - val_loss: 8.1247 - val_mse: 8.1247 - val_mae: 1.3697 - lr: 1.6570e-04 - 3s/epoch - 3ms/step\n",
            "Score for fold 1: loss of 8.124666213989258\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 4s - loss: 12.0025 - mse: 12.0025 - mae: 1.4330 - val_loss: 10.4467 - val_mse: 10.4467 - val_mae: 1.3838 - lr: 1.6570e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 4s - loss: 11.8520 - mse: 11.8520 - mae: 1.4268 - val_loss: 10.1916 - val_mse: 10.1916 - val_mae: 1.4263 - lr: 1.6570e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 4s - loss: 11.7559 - mse: 11.7559 - mae: 1.4191 - val_loss: 10.2013 - val_mse: 10.2013 - val_mae: 1.3565 - lr: 1.6570e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 4s - loss: 11.6080 - mse: 11.6080 - mae: 1.4108 - val_loss: 10.2693 - val_mse: 10.2693 - val_mae: 1.4179 - lr: 1.6570e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 4s - loss: 11.5914 - mse: 11.5914 - mae: 1.4102 - val_loss: 10.2586 - val_mse: 10.2586 - val_mae: 1.3751 - lr: 1.6570e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 4s - loss: 11.6148 - mse: 11.6148 - mae: 1.4065 - val_loss: 10.5342 - val_mse: 10.5342 - val_mae: 1.3742 - lr: 1.6570e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 4s - loss: 11.4307 - mse: 11.4307 - mae: 1.4021 - val_loss: 10.3661 - val_mse: 10.3661 - val_mae: 1.4251 - lr: 1.6570e-04 - 4s/epoch - 4ms/step\n",
            "Score for fold 2: loss of 10.366142272949219\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 3s - loss: 12.1428 - mse: 12.1428 - mae: 1.4110 - val_loss: 7.2760 - val_mse: 7.2760 - val_mae: 1.3487 - lr: 1.6570e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 4s - loss: 12.1230 - mse: 12.1230 - mae: 1.4017 - val_loss: 7.2943 - val_mse: 7.2943 - val_mae: 1.3898 - lr: 1.6570e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 4s - loss: 11.9596 - mse: 11.9596 - mae: 1.4009 - val_loss: 7.4029 - val_mse: 7.4029 - val_mae: 1.4313 - lr: 1.6570e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 3s - loss: 11.9128 - mse: 11.9128 - mae: 1.3969 - val_loss: 7.4809 - val_mse: 7.4809 - val_mae: 1.4353 - lr: 1.6570e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 3s - loss: 11.8550 - mse: 11.8550 - mae: 1.3934 - val_loss: 7.5242 - val_mse: 7.5242 - val_mae: 1.3528 - lr: 1.6570e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 3s - loss: 11.7456 - mse: 11.7456 - mae: 1.3882 - val_loss: 7.4335 - val_mse: 7.4335 - val_mae: 1.3869 - lr: 1.6570e-04 - 3s/epoch - 3ms/step\n",
            "Score for fold 3: loss of 7.433492183685303\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 3s - loss: 9.5483 - mse: 9.5483 - mae: 1.3813 - val_loss: 16.6599 - val_mse: 16.6599 - val_mae: 1.4369 - lr: 1.6570e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 3s - loss: 9.4048 - mse: 9.4048 - mae: 1.3752 - val_loss: 16.8374 - val_mse: 16.8374 - val_mae: 1.3746 - lr: 1.6570e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 3s - loss: 9.2414 - mse: 9.2414 - mae: 1.3694 - val_loss: 16.9592 - val_mse: 16.9592 - val_mae: 1.3824 - lr: 1.6570e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 3s - loss: 9.2322 - mse: 9.2322 - mae: 1.3643 - val_loss: 16.7197 - val_mse: 16.7197 - val_mae: 1.3801 - lr: 1.6570e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 3s - loss: 9.1492 - mse: 9.1492 - mae: 1.3621 - val_loss: 17.0268 - val_mse: 17.0268 - val_mae: 1.3717 - lr: 1.6570e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 3s - loss: 8.9860 - mse: 8.9860 - mae: 1.3523 - val_loss: 17.4842 - val_mse: 17.4842 - val_mae: 1.4346 - lr: 1.6570e-04 - 3s/epoch - 3ms/step\n",
            "Score for fold 4: loss of 17.484167098999023\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 3s - loss: 9.9502 - mse: 9.9502 - mae: 1.3637 - val_loss: 12.9437 - val_mse: 12.9437 - val_mae: 1.4344 - lr: 1.6570e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 4s - loss: 9.9050 - mse: 9.9050 - mae: 1.3534 - val_loss: 12.9288 - val_mse: 12.9288 - val_mae: 1.3427 - lr: 1.6570e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 4s - loss: 9.8950 - mse: 9.8950 - mae: 1.3496 - val_loss: 12.7153 - val_mse: 12.7153 - val_mae: 1.3859 - lr: 1.6570e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 4s - loss: 9.6589 - mse: 9.6589 - mae: 1.3433 - val_loss: 13.0576 - val_mse: 13.0576 - val_mae: 1.4059 - lr: 1.6570e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 4s - loss: 9.5660 - mse: 9.5660 - mae: 1.3354 - val_loss: 12.7899 - val_mse: 12.7899 - val_mae: 1.4272 - lr: 1.6570e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 4s - loss: 9.5820 - mse: 9.5820 - mae: 1.3361 - val_loss: 12.9151 - val_mse: 12.9151 - val_mae: 1.4466 - lr: 1.6570e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 4s - loss: 9.4733 - mse: 9.4733 - mae: 1.3273 - val_loss: 12.8074 - val_mse: 12.8074 - val_mae: 1.3789 - lr: 1.6570e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 4s - loss: 9.5035 - mse: 9.5035 - mae: 1.3203 - val_loss: 12.8566 - val_mse: 12.8566 - val_mae: 1.4000 - lr: 1.6570e-04 - 4s/epoch - 4ms/step\n",
            "Score for fold 5: loss of 12.856644630432129\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-10-12 09:01:00,956]\u001b[0m Finished trial#24 resulted in value: 11.251999999999999. Current best value is 8.826 with parameters: {'activation': 'tanh', 'num_hidden_layer': 4, 'i': 10, 'learning_rate': 0.00023399735711621326}.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 11s - loss: 13.8093 - mse: 13.8093 - mae: 1.5496 - val_loss: 10.0197 - val_mse: 10.0197 - val_mae: 1.4306 - lr: 1.0085e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 10s - loss: 12.8552 - mse: 12.8552 - mae: 1.4884 - val_loss: 9.8386 - val_mse: 9.8386 - val_mae: 1.4978 - lr: 1.0085e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 10s - loss: 12.5889 - mse: 12.5889 - mae: 1.4725 - val_loss: 9.9393 - val_mse: 9.9393 - val_mae: 1.3954 - lr: 1.0085e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 10s - loss: 12.6547 - mse: 12.6547 - mae: 1.4674 - val_loss: 10.6411 - val_mse: 10.6411 - val_mae: 1.3676 - lr: 1.0085e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 10s - loss: 12.5050 - mse: 12.5050 - mae: 1.4533 - val_loss: 9.7370 - val_mse: 9.7370 - val_mae: 1.4593 - lr: 1.0085e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 10s - loss: 12.3020 - mse: 12.3020 - mae: 1.4446 - val_loss: 10.2731 - val_mse: 10.2731 - val_mae: 1.4821 - lr: 1.0085e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 10s - loss: 12.1286 - mse: 12.1286 - mae: 1.4292 - val_loss: 9.8709 - val_mse: 9.8709 - val_mae: 1.4466 - lr: 1.0085e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 10s - loss: 12.0331 - mse: 12.0331 - mae: 1.4237 - val_loss: 9.9465 - val_mse: 9.9465 - val_mae: 1.4647 - lr: 1.0085e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 11s - loss: 11.7829 - mse: 11.7829 - mae: 1.4153 - val_loss: 9.8181 - val_mse: 9.8181 - val_mae: 1.4631 - lr: 1.0085e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 12s - loss: 11.6568 - mse: 11.6568 - mae: 1.4029 - val_loss: 9.7766 - val_mse: 9.7766 - val_mae: 1.3869 - lr: 1.0085e-04 - 12s/epoch - 12ms/step\n",
            "Score for fold 1: loss of 9.776604652404785\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 12s - loss: 11.7241 - mse: 11.7241 - mae: 1.4066 - val_loss: 9.0196 - val_mse: 9.0196 - val_mae: 1.4277 - lr: 1.0085e-04 - 12s/epoch - 12ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 12s - loss: 11.2786 - mse: 11.2786 - mae: 1.3957 - val_loss: 9.5936 - val_mse: 9.5936 - val_mae: 1.4161 - lr: 1.0085e-04 - 12s/epoch - 12ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 12s - loss: 11.0322 - mse: 11.0322 - mae: 1.3779 - val_loss: 9.6023 - val_mse: 9.6023 - val_mae: 1.4418 - lr: 1.0085e-04 - 12s/epoch - 12ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 11s - loss: 10.9120 - mse: 10.9120 - mae: 1.3650 - val_loss: 9.5561 - val_mse: 9.5561 - val_mae: 1.4099 - lr: 1.0085e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 11s - loss: 10.5600 - mse: 10.5600 - mae: 1.3500 - val_loss: 10.2937 - val_mse: 10.2937 - val_mae: 1.4876 - lr: 1.0085e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 11s - loss: 10.4391 - mse: 10.4391 - mae: 1.3382 - val_loss: 11.4984 - val_mse: 11.4984 - val_mae: 1.3979 - lr: 1.0085e-04 - 11s/epoch - 11ms/step\n",
            "Score for fold 2: loss of 11.498380661010742\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 10s - loss: 8.9967 - mse: 8.9967 - mae: 1.3474 - val_loss: 14.5431 - val_mse: 14.5431 - val_mae: 1.3906 - lr: 1.0085e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 9s - loss: 8.8919 - mse: 8.8919 - mae: 1.3314 - val_loss: 14.9963 - val_mse: 14.9963 - val_mae: 1.4109 - lr: 1.0085e-04 - 9s/epoch - 9ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 9s - loss: 8.3454 - mse: 8.3454 - mae: 1.3087 - val_loss: 14.9750 - val_mse: 14.9750 - val_mae: 1.3404 - lr: 1.0085e-04 - 9s/epoch - 9ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 9s - loss: 8.2884 - mse: 8.2884 - mae: 1.2937 - val_loss: 14.7369 - val_mse: 14.7369 - val_mae: 1.3979 - lr: 1.0085e-04 - 9s/epoch - 9ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 9s - loss: 8.0846 - mse: 8.0846 - mae: 1.2728 - val_loss: 15.5833 - val_mse: 15.5833 - val_mae: 1.4929 - lr: 1.0085e-04 - 9s/epoch - 9ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 9s - loss: 7.8577 - mse: 7.8577 - mae: 1.2482 - val_loss: 15.5776 - val_mse: 15.5776 - val_mae: 1.4029 - lr: 1.0085e-04 - 9s/epoch - 9ms/step\n",
            "Score for fold 3: loss of 15.577603340148926\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 9s - loss: 8.7562 - mse: 8.7562 - mae: 1.2925 - val_loss: 10.6119 - val_mse: 10.6119 - val_mae: 1.2544 - lr: 1.0085e-04 - 9s/epoch - 9ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 9s - loss: 8.3643 - mse: 8.3643 - mae: 1.2576 - val_loss: 11.2894 - val_mse: 11.2894 - val_mae: 1.2280 - lr: 1.0085e-04 - 9s/epoch - 9ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 9s - loss: 8.0263 - mse: 8.0263 - mae: 1.2415 - val_loss: 10.6920 - val_mse: 10.6920 - val_mae: 1.3070 - lr: 1.0085e-04 - 9s/epoch - 9ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 9s - loss: 7.7887 - mse: 7.7887 - mae: 1.2138 - val_loss: 11.3112 - val_mse: 11.3112 - val_mae: 1.2992 - lr: 1.0085e-04 - 9s/epoch - 9ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 9s - loss: 7.5195 - mse: 7.5195 - mae: 1.1926 - val_loss: 12.2065 - val_mse: 12.2065 - val_mae: 1.3524 - lr: 1.0085e-04 - 9s/epoch - 9ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 10s - loss: 7.1991 - mse: 7.1991 - mae: 1.1739 - val_loss: 11.6189 - val_mse: 11.6189 - val_mae: 1.3348 - lr: 1.0085e-04 - 10s/epoch - 10ms/step\n",
            "Score for fold 4: loss of 11.618850708007812\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 9s - loss: 8.6586 - mse: 8.6586 - mae: 1.2136 - val_loss: 4.3442 - val_mse: 4.3442 - val_mae: 1.1203 - lr: 1.0085e-04 - 9s/epoch - 9ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 9s - loss: 8.3656 - mse: 8.3656 - mae: 1.1797 - val_loss: 5.5701 - val_mse: 5.5701 - val_mae: 1.2057 - lr: 1.0085e-04 - 9s/epoch - 9ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 9s - loss: 8.0452 - mse: 8.0452 - mae: 1.1657 - val_loss: 5.4737 - val_mse: 5.4737 - val_mae: 1.1700 - lr: 1.0085e-04 - 9s/epoch - 9ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 9s - loss: 7.7934 - mse: 7.7934 - mae: 1.1416 - val_loss: 4.4298 - val_mse: 4.4298 - val_mae: 1.1711 - lr: 1.0085e-04 - 9s/epoch - 9ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 9s - loss: 7.5993 - mse: 7.5993 - mae: 1.1190 - val_loss: 4.8248 - val_mse: 4.8248 - val_mae: 1.2731 - lr: 1.0085e-04 - 9s/epoch - 9ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 9s - loss: 7.2261 - mse: 7.2261 - mae: 1.0932 - val_loss: 4.9893 - val_mse: 4.9893 - val_mae: 1.2191 - lr: 1.0085e-04 - 9s/epoch - 9ms/step\n",
            "Score for fold 5: loss of 4.9892497062683105\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-10-12 09:06:47,433]\u001b[0m Finished trial#25 resulted in value: 10.693999999999999. Current best value is 8.826 with parameters: {'activation': 'tanh', 'num_hidden_layer': 4, 'i': 10, 'learning_rate': 0.00023399735711621326}.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 6s - loss: 15.1678 - mse: 15.1678 - mae: 1.6197 - val_loss: 10.8858 - val_mse: 10.8858 - val_mae: 1.5653 - lr: 2.5005e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 5s - loss: 14.3679 - mse: 14.3679 - mae: 1.5323 - val_loss: 10.1578 - val_mse: 10.1578 - val_mae: 1.5072 - lr: 2.5005e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 6s - loss: 14.2064 - mse: 14.2064 - mae: 1.5110 - val_loss: 10.3287 - val_mse: 10.3287 - val_mae: 1.5224 - lr: 2.5005e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 6s - loss: 14.1267 - mse: 14.1267 - mae: 1.5030 - val_loss: 10.1946 - val_mse: 10.1946 - val_mae: 1.4779 - lr: 2.5005e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 6s - loss: 14.0029 - mse: 14.0029 - mae: 1.4949 - val_loss: 10.1421 - val_mse: 10.1421 - val_mae: 1.4767 - lr: 2.5005e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 6s - loss: 13.8798 - mse: 13.8798 - mae: 1.4897 - val_loss: 10.1589 - val_mse: 10.1589 - val_mae: 1.5193 - lr: 2.5005e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 6s - loss: 13.7958 - mse: 13.7958 - mae: 1.4804 - val_loss: 10.0073 - val_mse: 10.0073 - val_mae: 1.5018 - lr: 2.5005e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 6s - loss: 13.6768 - mse: 13.6768 - mae: 1.4760 - val_loss: 10.2913 - val_mse: 10.2913 - val_mae: 1.4795 - lr: 2.5005e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 6s - loss: 13.6476 - mse: 13.6476 - mae: 1.4723 - val_loss: 10.0649 - val_mse: 10.0649 - val_mae: 1.4333 - lr: 2.5005e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 6s - loss: 13.5509 - mse: 13.5509 - mae: 1.4742 - val_loss: 9.9340 - val_mse: 9.9340 - val_mae: 1.4333 - lr: 2.5005e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 6s - loss: 13.4679 - mse: 13.4679 - mae: 1.4703 - val_loss: 10.0088 - val_mse: 10.0088 - val_mae: 1.5258 - lr: 2.5005e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 6s - loss: 13.3743 - mse: 13.3743 - mae: 1.4654 - val_loss: 9.9117 - val_mse: 9.9117 - val_mae: 1.4707 - lr: 2.5005e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 13/100\n",
            "1000/1000 - 7s - loss: 13.3359 - mse: 13.3359 - mae: 1.4608 - val_loss: 9.9396 - val_mse: 9.9396 - val_mae: 1.4846 - lr: 2.5005e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 14/100\n",
            "1000/1000 - 7s - loss: 13.2155 - mse: 13.2155 - mae: 1.4557 - val_loss: 9.9421 - val_mse: 9.9421 - val_mae: 1.4570 - lr: 2.5005e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 15/100\n",
            "1000/1000 - 7s - loss: 13.1334 - mse: 13.1334 - mae: 1.4548 - val_loss: 10.0047 - val_mse: 10.0047 - val_mae: 1.4708 - lr: 2.5005e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 16/100\n",
            "1000/1000 - 7s - loss: 13.0141 - mse: 13.0141 - mae: 1.4504 - val_loss: 10.0866 - val_mse: 10.0866 - val_mae: 1.5448 - lr: 2.5005e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 17/100\n",
            "1000/1000 - 7s - loss: 12.9185 - mse: 12.9185 - mae: 1.4445 - val_loss: 9.9044 - val_mse: 9.9044 - val_mae: 1.4817 - lr: 2.5005e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 18/100\n",
            "1000/1000 - 7s - loss: 12.8332 - mse: 12.8332 - mae: 1.4431 - val_loss: 9.8778 - val_mse: 9.8778 - val_mae: 1.4555 - lr: 2.5005e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 19/100\n",
            "1000/1000 - 7s - loss: 12.7991 - mse: 12.7991 - mae: 1.4368 - val_loss: 9.7935 - val_mse: 9.7935 - val_mae: 1.4960 - lr: 2.5005e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 20/100\n",
            "1000/1000 - 7s - loss: 12.6924 - mse: 12.6924 - mae: 1.4332 - val_loss: 9.7822 - val_mse: 9.7822 - val_mae: 1.4747 - lr: 2.5005e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 21/100\n",
            "1000/1000 - 7s - loss: 12.6228 - mse: 12.6228 - mae: 1.4361 - val_loss: 9.8868 - val_mse: 9.8868 - val_mae: 1.4930 - lr: 2.5005e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 22/100\n",
            "1000/1000 - 7s - loss: 12.5301 - mse: 12.5301 - mae: 1.4244 - val_loss: 9.9015 - val_mse: 9.9015 - val_mae: 1.4945 - lr: 2.5005e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 23/100\n",
            "1000/1000 - 6s - loss: 12.4809 - mse: 12.4809 - mae: 1.4225 - val_loss: 9.7930 - val_mse: 9.7930 - val_mae: 1.5117 - lr: 2.5005e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 24/100\n",
            "1000/1000 - 6s - loss: 12.3720 - mse: 12.3720 - mae: 1.4191 - val_loss: 9.8226 - val_mse: 9.8226 - val_mae: 1.4773 - lr: 2.5005e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 25/100\n",
            "1000/1000 - 6s - loss: 12.2955 - mse: 12.2955 - mae: 1.4135 - val_loss: 9.7582 - val_mse: 9.7582 - val_mae: 1.4916 - lr: 2.5005e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 26/100\n",
            "1000/1000 - 6s - loss: 12.1596 - mse: 12.1596 - mae: 1.4093 - val_loss: 9.7765 - val_mse: 9.7765 - val_mae: 1.5168 - lr: 2.5005e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 27/100\n",
            "1000/1000 - 6s - loss: 12.0425 - mse: 12.0425 - mae: 1.4072 - val_loss: 9.7281 - val_mse: 9.7281 - val_mae: 1.4846 - lr: 2.5005e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 28/100\n",
            "1000/1000 - 6s - loss: 11.9443 - mse: 11.9443 - mae: 1.3989 - val_loss: 9.7556 - val_mse: 9.7556 - val_mae: 1.4922 - lr: 2.5005e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 29/100\n",
            "1000/1000 - 6s - loss: 11.8588 - mse: 11.8588 - mae: 1.3992 - val_loss: 9.9458 - val_mse: 9.9458 - val_mae: 1.5560 - lr: 2.5005e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 30/100\n",
            "1000/1000 - 7s - loss: 11.7235 - mse: 11.7235 - mae: 1.3917 - val_loss: 9.7360 - val_mse: 9.7360 - val_mae: 1.5369 - lr: 2.5005e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 31/100\n",
            "1000/1000 - 7s - loss: 11.7339 - mse: 11.7339 - mae: 1.3865 - val_loss: 9.8109 - val_mse: 9.8109 - val_mae: 1.5154 - lr: 2.5005e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 32/100\n",
            "1000/1000 - 7s - loss: 11.5746 - mse: 11.5746 - mae: 1.3827 - val_loss: 9.6891 - val_mse: 9.6891 - val_mae: 1.5340 - lr: 2.5005e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 33/100\n",
            "1000/1000 - 7s - loss: 11.4607 - mse: 11.4607 - mae: 1.3809 - val_loss: 9.8965 - val_mse: 9.8965 - val_mae: 1.5472 - lr: 2.5005e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 34/100\n",
            "1000/1000 - 7s - loss: 11.3721 - mse: 11.3721 - mae: 1.3739 - val_loss: 10.0093 - val_mse: 10.0093 - val_mae: 1.6123 - lr: 2.5005e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 35/100\n",
            "1000/1000 - 7s - loss: 11.2049 - mse: 11.2049 - mae: 1.3664 - val_loss: 9.7773 - val_mse: 9.7773 - val_mae: 1.5612 - lr: 2.5005e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 36/100\n",
            "1000/1000 - 6s - loss: 11.1597 - mse: 11.1597 - mae: 1.3645 - val_loss: 9.7553 - val_mse: 9.7553 - val_mae: 1.5417 - lr: 2.5005e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 37/100\n",
            "1000/1000 - 6s - loss: 11.0535 - mse: 11.0535 - mae: 1.3599 - val_loss: 9.7693 - val_mse: 9.7693 - val_mae: 1.5579 - lr: 2.5005e-04 - 6s/epoch - 6ms/step\n",
            "Score for fold 1: loss of 9.76929759979248\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 6s - loss: 11.9007 - mse: 11.9007 - mae: 1.4167 - val_loss: 6.3358 - val_mse: 6.3358 - val_mae: 1.3291 - lr: 2.5005e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 6s - loss: 11.7869 - mse: 11.7869 - mae: 1.4056 - val_loss: 6.7208 - val_mse: 6.7208 - val_mae: 1.4632 - lr: 2.5005e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 6s - loss: 11.6043 - mse: 11.6043 - mae: 1.3946 - val_loss: 6.9568 - val_mse: 6.9568 - val_mae: 1.4113 - lr: 2.5005e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 6s - loss: 11.4781 - mse: 11.4781 - mae: 1.3829 - val_loss: 6.8091 - val_mse: 6.8091 - val_mae: 1.4188 - lr: 2.5005e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 6s - loss: 11.3825 - mse: 11.3825 - mae: 1.3801 - val_loss: 6.9332 - val_mse: 6.9332 - val_mae: 1.3665 - lr: 2.5005e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 6s - loss: 11.2956 - mse: 11.2956 - mae: 1.3740 - val_loss: 6.8964 - val_mse: 6.8964 - val_mae: 1.4181 - lr: 2.5005e-04 - 6s/epoch - 6ms/step\n",
            "Score for fold 2: loss of 6.896398067474365\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 6s - loss: 11.4430 - mse: 11.4430 - mae: 1.3996 - val_loss: 5.8669 - val_mse: 5.8669 - val_mae: 1.2713 - lr: 2.5005e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 6s - loss: 11.3381 - mse: 11.3381 - mae: 1.3893 - val_loss: 6.0321 - val_mse: 6.0321 - val_mae: 1.3113 - lr: 2.5005e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 6s - loss: 11.2459 - mse: 11.2459 - mae: 1.3772 - val_loss: 6.1466 - val_mse: 6.1466 - val_mae: 1.3271 - lr: 2.5005e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 6s - loss: 11.1155 - mse: 11.1155 - mae: 1.3747 - val_loss: 6.2187 - val_mse: 6.2187 - val_mae: 1.3673 - lr: 2.5005e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 6s - loss: 10.9831 - mse: 10.9831 - mae: 1.3679 - val_loss: 6.2731 - val_mse: 6.2731 - val_mae: 1.3592 - lr: 2.5005e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 6s - loss: 10.8746 - mse: 10.8746 - mae: 1.3588 - val_loss: 6.4116 - val_mse: 6.4116 - val_mae: 1.3667 - lr: 2.5005e-04 - 6s/epoch - 6ms/step\n",
            "Score for fold 3: loss of 6.411623954772949\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 6s - loss: 9.4698 - mse: 9.4698 - mae: 1.3723 - val_loss: 11.6837 - val_mse: 11.6837 - val_mae: 1.3396 - lr: 2.5005e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 6s - loss: 9.3496 - mse: 9.3496 - mae: 1.3626 - val_loss: 11.8857 - val_mse: 11.8857 - val_mae: 1.3995 - lr: 2.5005e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 6s - loss: 9.2063 - mse: 9.2063 - mae: 1.3531 - val_loss: 12.0292 - val_mse: 12.0292 - val_mae: 1.3828 - lr: 2.5005e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 6s - loss: 9.0654 - mse: 9.0654 - mae: 1.3429 - val_loss: 12.1890 - val_mse: 12.1890 - val_mae: 1.3700 - lr: 2.5005e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 7s - loss: 9.0942 - mse: 9.0942 - mae: 1.3372 - val_loss: 12.3539 - val_mse: 12.3539 - val_mae: 1.4278 - lr: 2.5005e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 7s - loss: 8.9334 - mse: 8.9334 - mae: 1.3353 - val_loss: 12.4401 - val_mse: 12.4401 - val_mae: 1.4901 - lr: 2.5005e-04 - 7s/epoch - 7ms/step\n",
            "Score for fold 4: loss of 12.440115928649902\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 8s - loss: 7.6341 - mse: 7.6341 - mae: 1.3579 - val_loss: 17.2105 - val_mse: 17.2105 - val_mae: 1.3024 - lr: 2.5005e-04 - 8s/epoch - 8ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 8s - loss: 7.4639 - mse: 7.4639 - mae: 1.3427 - val_loss: 17.4709 - val_mse: 17.4709 - val_mae: 1.3779 - lr: 2.5005e-04 - 8s/epoch - 8ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 8s - loss: 7.3251 - mse: 7.3251 - mae: 1.3340 - val_loss: 18.5055 - val_mse: 18.5055 - val_mae: 1.4675 - lr: 2.5005e-04 - 8s/epoch - 8ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 8s - loss: 7.2647 - mse: 7.2647 - mae: 1.3265 - val_loss: 18.2763 - val_mse: 18.2763 - val_mae: 1.4084 - lr: 2.5005e-04 - 8s/epoch - 8ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 8s - loss: 7.2202 - mse: 7.2202 - mae: 1.3219 - val_loss: 17.9054 - val_mse: 17.9054 - val_mae: 1.4118 - lr: 2.5005e-04 - 8s/epoch - 8ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 7s - loss: 7.1332 - mse: 7.1332 - mae: 1.3146 - val_loss: 17.8179 - val_mse: 17.8179 - val_mae: 1.4354 - lr: 2.5005e-04 - 7s/epoch - 7ms/step\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-10-12 09:13:32,494]\u001b[0m Finished trial#26 resulted in value: 10.668000000000001. Current best value is 8.826 with parameters: {'activation': 'tanh', 'num_hidden_layer': 4, 'i': 10, 'learning_rate': 0.00023399735711621326}.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Score for fold 5: loss of 17.817893981933594\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 13.5523 - mse: 13.5523 - mae: 1.5556 - val_loss: 13.0454 - val_mse: 13.0454 - val_mae: 1.5187 - lr: 4.5167e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 12.6351 - mse: 12.6351 - mae: 1.5042 - val_loss: 12.7260 - val_mse: 12.7260 - val_mae: 1.4696 - lr: 4.5167e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 12.3489 - mse: 12.3489 - mae: 1.4897 - val_loss: 12.5430 - val_mse: 12.5430 - val_mae: 1.4947 - lr: 4.5167e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 12.2193 - mse: 12.2193 - mae: 1.4799 - val_loss: 12.4302 - val_mse: 12.4302 - val_mae: 1.4637 - lr: 4.5167e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 12.0235 - mse: 12.0235 - mae: 1.4698 - val_loss: 12.3723 - val_mse: 12.3723 - val_mae: 1.4382 - lr: 4.5167e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 11.9635 - mse: 11.9635 - mae: 1.4656 - val_loss: 12.2892 - val_mse: 12.2892 - val_mae: 1.4493 - lr: 4.5167e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 11.8808 - mse: 11.8808 - mae: 1.4594 - val_loss: 12.2859 - val_mse: 12.2859 - val_mae: 1.4376 - lr: 4.5167e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 2s - loss: 11.8795 - mse: 11.8795 - mae: 1.4576 - val_loss: 12.3244 - val_mse: 12.3244 - val_mae: 1.4669 - lr: 4.5167e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 2s - loss: 11.7589 - mse: 11.7589 - mae: 1.4577 - val_loss: 12.2735 - val_mse: 12.2735 - val_mae: 1.4437 - lr: 4.5167e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 2s - loss: 11.7170 - mse: 11.7170 - mae: 1.4535 - val_loss: 12.3311 - val_mse: 12.3311 - val_mae: 1.4441 - lr: 4.5167e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 2s - loss: 11.7409 - mse: 11.7409 - mae: 1.4501 - val_loss: 12.2580 - val_mse: 12.2580 - val_mae: 1.4705 - lr: 4.5167e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 2s - loss: 11.6759 - mse: 11.6759 - mae: 1.4504 - val_loss: 12.2558 - val_mse: 12.2558 - val_mae: 1.4524 - lr: 4.5167e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 13/100\n",
            "1000/1000 - 2s - loss: 11.6736 - mse: 11.6736 - mae: 1.4466 - val_loss: 12.2845 - val_mse: 12.2845 - val_mae: 1.4223 - lr: 4.5167e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 14/100\n",
            "1000/1000 - 2s - loss: 11.6887 - mse: 11.6887 - mae: 1.4425 - val_loss: 12.1795 - val_mse: 12.1795 - val_mae: 1.4394 - lr: 4.5167e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 15/100\n",
            "1000/1000 - 2s - loss: 11.5838 - mse: 11.5838 - mae: 1.4422 - val_loss: 12.3992 - val_mse: 12.3992 - val_mae: 1.4555 - lr: 4.5167e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 16/100\n",
            "1000/1000 - 2s - loss: 11.6578 - mse: 11.6578 - mae: 1.4437 - val_loss: 12.2360 - val_mse: 12.2360 - val_mae: 1.4375 - lr: 4.5167e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 17/100\n",
            "1000/1000 - 2s - loss: 11.5111 - mse: 11.5111 - mae: 1.4365 - val_loss: 12.2908 - val_mse: 12.2908 - val_mae: 1.4405 - lr: 4.5167e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 18/100\n",
            "1000/1000 - 2s - loss: 11.5794 - mse: 11.5794 - mae: 1.4376 - val_loss: 12.3366 - val_mse: 12.3366 - val_mae: 1.4597 - lr: 4.5167e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 19/100\n",
            "1000/1000 - 2s - loss: 11.4958 - mse: 11.4958 - mae: 1.4322 - val_loss: 12.2201 - val_mse: 12.2201 - val_mae: 1.4044 - lr: 4.5167e-04 - 2s/epoch - 2ms/step\n",
            "Score for fold 1: loss of 12.220104217529297\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 12.4679 - mse: 12.4679 - mae: 1.4401 - val_loss: 8.5784 - val_mse: 8.5784 - val_mae: 1.5110 - lr: 4.5167e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 12.3723 - mse: 12.3723 - mae: 1.4389 - val_loss: 8.6366 - val_mse: 8.6366 - val_mae: 1.4211 - lr: 4.5167e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 12.3557 - mse: 12.3557 - mae: 1.4324 - val_loss: 8.5858 - val_mse: 8.5858 - val_mae: 1.4031 - lr: 4.5167e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 12.3578 - mse: 12.3578 - mae: 1.4346 - val_loss: 8.8360 - val_mse: 8.8360 - val_mae: 1.5264 - lr: 4.5167e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 12.3832 - mse: 12.3832 - mae: 1.4323 - val_loss: 8.6232 - val_mse: 8.6232 - val_mae: 1.4499 - lr: 4.5167e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 12.3157 - mse: 12.3157 - mae: 1.4324 - val_loss: 8.5274 - val_mse: 8.5274 - val_mae: 1.4444 - lr: 4.5167e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 12.2533 - mse: 12.2533 - mae: 1.4296 - val_loss: 8.6056 - val_mse: 8.6056 - val_mae: 1.4330 - lr: 4.5167e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 2s - loss: 12.2610 - mse: 12.2610 - mae: 1.4284 - val_loss: 9.0245 - val_mse: 9.0245 - val_mae: 1.4275 - lr: 4.5167e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 2s - loss: 12.2907 - mse: 12.2907 - mae: 1.4310 - val_loss: 8.6320 - val_mse: 8.6320 - val_mae: 1.4517 - lr: 4.5167e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 2s - loss: 12.2750 - mse: 12.2750 - mae: 1.4294 - val_loss: 8.6090 - val_mse: 8.6090 - val_mae: 1.4270 - lr: 4.5167e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 2s - loss: 12.3022 - mse: 12.3022 - mae: 1.4224 - val_loss: 8.5944 - val_mse: 8.5944 - val_mae: 1.4407 - lr: 4.5167e-04 - 2s/epoch - 2ms/step\n",
            "Score for fold 2: loss of 8.594399452209473\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 11.7355 - mse: 11.7355 - mae: 1.4279 - val_loss: 10.5665 - val_mse: 10.5665 - val_mae: 1.4389 - lr: 4.5167e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 11.6609 - mse: 11.6609 - mae: 1.4258 - val_loss: 10.9799 - val_mse: 10.9799 - val_mae: 1.4529 - lr: 4.5167e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 11.6572 - mse: 11.6572 - mae: 1.4242 - val_loss: 10.4809 - val_mse: 10.4809 - val_mae: 1.4395 - lr: 4.5167e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 11.6245 - mse: 11.6245 - mae: 1.4202 - val_loss: 10.7591 - val_mse: 10.7591 - val_mae: 1.4564 - lr: 4.5167e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 11.6060 - mse: 11.6060 - mae: 1.4184 - val_loss: 10.8848 - val_mse: 10.8848 - val_mae: 1.4804 - lr: 4.5167e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 11.5423 - mse: 11.5423 - mae: 1.4165 - val_loss: 10.5580 - val_mse: 10.5580 - val_mae: 1.4690 - lr: 4.5167e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 11.5400 - mse: 11.5400 - mae: 1.4171 - val_loss: 10.5792 - val_mse: 10.5792 - val_mae: 1.4315 - lr: 4.5167e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 2s - loss: 11.5286 - mse: 11.5286 - mae: 1.4144 - val_loss: 10.9072 - val_mse: 10.9072 - val_mae: 1.4184 - lr: 4.5167e-04 - 2s/epoch - 2ms/step\n",
            "Score for fold 3: loss of 10.907217979431152\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 9.7539 - mse: 9.7539 - mae: 1.4203 - val_loss: 17.7567 - val_mse: 17.7567 - val_mae: 1.4722 - lr: 4.5167e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 9.7264 - mse: 9.7264 - mae: 1.4204 - val_loss: 17.8049 - val_mse: 17.8049 - val_mae: 1.4442 - lr: 4.5167e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 9.6800 - mse: 9.6800 - mae: 1.4202 - val_loss: 17.7472 - val_mse: 17.7472 - val_mae: 1.4188 - lr: 4.5167e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 9.6434 - mse: 9.6434 - mae: 1.4191 - val_loss: 17.9350 - val_mse: 17.9350 - val_mae: 1.4437 - lr: 4.5167e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 9.6949 - mse: 9.6949 - mae: 1.4203 - val_loss: 17.8779 - val_mse: 17.8779 - val_mae: 1.4573 - lr: 4.5167e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 9.6281 - mse: 9.6281 - mae: 1.4138 - val_loss: 17.8241 - val_mse: 17.8241 - val_mae: 1.4399 - lr: 4.5167e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 9.6153 - mse: 9.6153 - mae: 1.4122 - val_loss: 17.8396 - val_mse: 17.8396 - val_mae: 1.4509 - lr: 4.5167e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 2s - loss: 9.5648 - mse: 9.5648 - mae: 1.4121 - val_loss: 17.8338 - val_mse: 17.8338 - val_mae: 1.4549 - lr: 4.5167e-04 - 2s/epoch - 2ms/step\n",
            "Score for fold 4: loss of 17.83376693725586\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 12.0204 - mse: 12.0204 - mae: 1.4252 - val_loss: 7.9969 - val_mse: 7.9969 - val_mae: 1.4142 - lr: 4.5167e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 11.9901 - mse: 11.9901 - mae: 1.4198 - val_loss: 8.1547 - val_mse: 8.1547 - val_mae: 1.4136 - lr: 4.5167e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 11.9729 - mse: 11.9729 - mae: 1.4197 - val_loss: 8.0644 - val_mse: 8.0644 - val_mae: 1.4172 - lr: 4.5167e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 11.9440 - mse: 11.9440 - mae: 1.4167 - val_loss: 8.1483 - val_mse: 8.1483 - val_mae: 1.3968 - lr: 4.5167e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 11.8645 - mse: 11.8645 - mae: 1.4156 - val_loss: 8.1067 - val_mse: 8.1067 - val_mae: 1.3895 - lr: 4.5167e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 11.8840 - mse: 11.8840 - mae: 1.4164 - val_loss: 8.2283 - val_mse: 8.2283 - val_mae: 1.4437 - lr: 4.5167e-04 - 2s/epoch - 2ms/step\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-10-12 09:15:08,201]\u001b[0m Finished trial#27 resulted in value: 11.556000000000001. Current best value is 8.826 with parameters: {'activation': 'tanh', 'num_hidden_layer': 4, 'i': 10, 'learning_rate': 0.00023399735711621326}.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Score for fold 5: loss of 8.228265762329102\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 14.6528 - mse: 14.6528 - mae: 1.5883 - val_loss: 11.2503 - val_mse: 11.2503 - val_mae: 1.5160 - lr: 2.3311e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 13.3515 - mse: 13.3515 - mae: 1.5276 - val_loss: 10.8496 - val_mse: 10.8496 - val_mae: 1.4769 - lr: 2.3311e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 12.9959 - mse: 12.9959 - mae: 1.5055 - val_loss: 11.0154 - val_mse: 11.0154 - val_mae: 1.4515 - lr: 2.3311e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 12.7634 - mse: 12.7634 - mae: 1.4930 - val_loss: 11.0647 - val_mse: 11.0647 - val_mae: 1.4526 - lr: 2.3311e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 12.6021 - mse: 12.6021 - mae: 1.4841 - val_loss: 11.1986 - val_mse: 11.1986 - val_mae: 1.4360 - lr: 2.3311e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 12.4327 - mse: 12.4327 - mae: 1.4757 - val_loss: 10.8155 - val_mse: 10.8155 - val_mae: 1.4973 - lr: 2.3311e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 12.2900 - mse: 12.2900 - mae: 1.4694 - val_loss: 11.4483 - val_mse: 11.4483 - val_mae: 1.4114 - lr: 2.3311e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 2s - loss: 12.2178 - mse: 12.2178 - mae: 1.4569 - val_loss: 10.9804 - val_mse: 10.9804 - val_mae: 1.4528 - lr: 2.3311e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 2s - loss: 12.1947 - mse: 12.1947 - mae: 1.4588 - val_loss: 11.2824 - val_mse: 11.2824 - val_mae: 1.4338 - lr: 2.3311e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 2s - loss: 12.1459 - mse: 12.1459 - mae: 1.4484 - val_loss: 11.0037 - val_mse: 11.0037 - val_mae: 1.4181 - lr: 2.3311e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 2s - loss: 12.0914 - mse: 12.0914 - mae: 1.4487 - val_loss: 11.1623 - val_mse: 11.1623 - val_mae: 1.4101 - lr: 2.3311e-04 - 2s/epoch - 2ms/step\n",
            "Score for fold 1: loss of 11.162250518798828\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 10.3579 - mse: 10.3579 - mae: 1.4370 - val_loss: 17.9237 - val_mse: 17.9237 - val_mae: 1.5024 - lr: 2.3311e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 10.3901 - mse: 10.3901 - mae: 1.4333 - val_loss: 18.0654 - val_mse: 18.0654 - val_mae: 1.4347 - lr: 2.3311e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 10.2866 - mse: 10.2866 - mae: 1.4289 - val_loss: 18.1211 - val_mse: 18.1211 - val_mae: 1.5224 - lr: 2.3311e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 10.2311 - mse: 10.2311 - mae: 1.4240 - val_loss: 18.3027 - val_mse: 18.3027 - val_mae: 1.4924 - lr: 2.3311e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 10.2269 - mse: 10.2269 - mae: 1.4242 - val_loss: 18.1955 - val_mse: 18.1955 - val_mae: 1.4734 - lr: 2.3311e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 10.1738 - mse: 10.1738 - mae: 1.4180 - val_loss: 18.0931 - val_mse: 18.0931 - val_mae: 1.4979 - lr: 2.3311e-04 - 2s/epoch - 2ms/step\n",
            "Score for fold 2: loss of 18.093109130859375\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 12.7330 - mse: 12.7330 - mae: 1.4389 - val_loss: 7.6014 - val_mse: 7.6014 - val_mae: 1.3617 - lr: 2.3311e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 12.6626 - mse: 12.6626 - mae: 1.4317 - val_loss: 7.6080 - val_mse: 7.6080 - val_mae: 1.4051 - lr: 2.3311e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 12.6664 - mse: 12.6664 - mae: 1.4311 - val_loss: 7.6825 - val_mse: 7.6825 - val_mae: 1.3666 - lr: 2.3311e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 12.6391 - mse: 12.6391 - mae: 1.4288 - val_loss: 7.6979 - val_mse: 7.6979 - val_mae: 1.3722 - lr: 2.3311e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 12.5327 - mse: 12.5327 - mae: 1.4266 - val_loss: 7.6486 - val_mse: 7.6486 - val_mae: 1.4090 - lr: 2.3311e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 12.5302 - mse: 12.5302 - mae: 1.4254 - val_loss: 7.6523 - val_mse: 7.6523 - val_mae: 1.4420 - lr: 2.3311e-04 - 2s/epoch - 2ms/step\n",
            "Score for fold 3: loss of 7.652304649353027\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 11.4041 - mse: 11.4041 - mae: 1.4197 - val_loss: 12.5785 - val_mse: 12.5785 - val_mae: 1.4131 - lr: 2.3311e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 11.3201 - mse: 11.3201 - mae: 1.4176 - val_loss: 12.0032 - val_mse: 12.0032 - val_mae: 1.4639 - lr: 2.3311e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 11.2956 - mse: 11.2956 - mae: 1.4159 - val_loss: 12.5602 - val_mse: 12.5602 - val_mae: 1.3934 - lr: 2.3311e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 11.2447 - mse: 11.2447 - mae: 1.4142 - val_loss: 12.1242 - val_mse: 12.1242 - val_mae: 1.4605 - lr: 2.3311e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 11.2310 - mse: 11.2310 - mae: 1.4131 - val_loss: 12.0902 - val_mse: 12.0902 - val_mae: 1.4356 - lr: 2.3311e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 11.1274 - mse: 11.1274 - mae: 1.4090 - val_loss: 12.1673 - val_mse: 12.1673 - val_mae: 1.4170 - lr: 2.3311e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 11.1258 - mse: 11.1258 - mae: 1.4057 - val_loss: 12.4152 - val_mse: 12.4152 - val_mae: 1.4432 - lr: 2.3311e-04 - 2s/epoch - 2ms/step\n",
            "Score for fold 4: loss of 12.415239334106445\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 11.8588 - mse: 11.8588 - mae: 1.4172 - val_loss: 9.5416 - val_mse: 9.5416 - val_mae: 1.3648 - lr: 2.3311e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 11.7411 - mse: 11.7411 - mae: 1.4145 - val_loss: 9.5241 - val_mse: 9.5241 - val_mae: 1.3978 - lr: 2.3311e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 11.7048 - mse: 11.7048 - mae: 1.4101 - val_loss: 9.5689 - val_mse: 9.5689 - val_mae: 1.3752 - lr: 2.3311e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 11.5750 - mse: 11.5750 - mae: 1.4104 - val_loss: 9.7115 - val_mse: 9.7115 - val_mae: 1.3535 - lr: 2.3311e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 11.6328 - mse: 11.6328 - mae: 1.4065 - val_loss: 9.5856 - val_mse: 9.5856 - val_mae: 1.4161 - lr: 2.3311e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 11.5593 - mse: 11.5593 - mae: 1.4049 - val_loss: 9.6055 - val_mse: 9.6055 - val_mae: 1.3854 - lr: 2.3311e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 11.5777 - mse: 11.5777 - mae: 1.4018 - val_loss: 9.7625 - val_mse: 9.7625 - val_mae: 1.4117 - lr: 2.3311e-04 - 2s/epoch - 2ms/step\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-10-12 09:16:12,804]\u001b[0m Finished trial#28 resulted in value: 11.815999999999999. Current best value is 8.826 with parameters: {'activation': 'tanh', 'num_hidden_layer': 4, 'i': 10, 'learning_rate': 0.00023399735711621326}.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Score for fold 5: loss of 9.762463569641113\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 6s - loss: 11.9644 - mse: 11.9644 - mae: 1.5853 - val_loss: 19.2366 - val_mse: 19.2366 - val_mae: 1.4619 - lr: 0.0051 - 6s/epoch - 6ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 5s - loss: 10.9508 - mse: 10.9508 - mae: 1.4979 - val_loss: 18.4122 - val_mse: 18.4122 - val_mae: 1.4584 - lr: 0.0051 - 5s/epoch - 5ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 5s - loss: 10.8828 - mse: 10.8828 - mae: 1.5055 - val_loss: 19.4907 - val_mse: 19.4907 - val_mae: 1.4799 - lr: 0.0051 - 5s/epoch - 5ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 5s - loss: 10.8548 - mse: 10.8548 - mae: 1.4998 - val_loss: 18.1681 - val_mse: 18.1681 - val_mae: 1.4975 - lr: 0.0051 - 5s/epoch - 5ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 5s - loss: 10.6697 - mse: 10.6697 - mae: 1.4978 - val_loss: 18.6780 - val_mse: 18.6780 - val_mae: 1.5863 - lr: 0.0051 - 5s/epoch - 5ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 5s - loss: 10.7021 - mse: 10.7021 - mae: 1.4965 - val_loss: 18.0393 - val_mse: 18.0393 - val_mae: 1.4637 - lr: 0.0051 - 5s/epoch - 5ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 6s - loss: 10.5197 - mse: 10.5197 - mae: 1.4885 - val_loss: 18.4417 - val_mse: 18.4417 - val_mae: 1.4492 - lr: 0.0051 - 6s/epoch - 6ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 6s - loss: 10.3475 - mse: 10.3475 - mae: 1.4846 - val_loss: 18.1769 - val_mse: 18.1769 - val_mae: 1.4748 - lr: 0.0051 - 6s/epoch - 6ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 6s - loss: 10.3202 - mse: 10.3202 - mae: 1.4835 - val_loss: 18.2115 - val_mse: 18.2115 - val_mae: 1.5644 - lr: 0.0051 - 6s/epoch - 6ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 6s - loss: 10.2537 - mse: 10.2537 - mae: 1.4782 - val_loss: 18.2670 - val_mse: 18.2670 - val_mae: 1.4861 - lr: 0.0051 - 6s/epoch - 6ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 6s - loss: 10.1837 - mse: 10.1837 - mae: 1.4759 - val_loss: 18.4325 - val_mse: 18.4325 - val_mae: 1.4699 - lr: 0.0051 - 6s/epoch - 6ms/step\n",
            "Score for fold 1: loss of 18.4324893951416\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 6s - loss: 11.8318 - mse: 11.8318 - mae: 1.4518 - val_loss: 9.3849 - val_mse: 9.3849 - val_mae: 1.4157 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 6s - loss: 11.7355 - mse: 11.7355 - mae: 1.4417 - val_loss: 9.1929 - val_mse: 9.1929 - val_mae: 1.4323 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 6s - loss: 11.4763 - mse: 11.4763 - mae: 1.4350 - val_loss: 9.1918 - val_mse: 9.1918 - val_mae: 1.4408 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 6s - loss: 11.3618 - mse: 11.3618 - mae: 1.4266 - val_loss: 9.3796 - val_mse: 9.3796 - val_mae: 1.4441 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 6s - loss: 11.3072 - mse: 11.3072 - mae: 1.4231 - val_loss: 9.3162 - val_mse: 9.3162 - val_mae: 1.4407 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 6s - loss: 11.2289 - mse: 11.2289 - mae: 1.4193 - val_loss: 9.4093 - val_mse: 9.4093 - val_mae: 1.4704 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 6s - loss: 11.0929 - mse: 11.0929 - mae: 1.4134 - val_loss: 9.1844 - val_mse: 9.1844 - val_mae: 1.4461 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 5s - loss: 11.0220 - mse: 11.0220 - mae: 1.4065 - val_loss: 9.4043 - val_mse: 9.4043 - val_mae: 1.4204 - lr: 0.0010 - 5s/epoch - 5ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 5s - loss: 10.9520 - mse: 10.9520 - mae: 1.4048 - val_loss: 9.4566 - val_mse: 9.4566 - val_mae: 1.4365 - lr: 0.0010 - 5s/epoch - 5ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 5s - loss: 10.8270 - mse: 10.8270 - mae: 1.3967 - val_loss: 9.5176 - val_mse: 9.5176 - val_mae: 1.4552 - lr: 0.0010 - 5s/epoch - 5ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 5s - loss: 10.7942 - mse: 10.7942 - mae: 1.3948 - val_loss: 9.4190 - val_mse: 9.4190 - val_mae: 1.4347 - lr: 0.0010 - 5s/epoch - 5ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 6s - loss: 10.6206 - mse: 10.6206 - mae: 1.3892 - val_loss: 9.5216 - val_mse: 9.5216 - val_mae: 1.4549 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
            "Score for fold 2: loss of 9.521632194519043\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 6s - loss: 10.9578 - mse: 10.9578 - mae: 1.3964 - val_loss: 8.1014 - val_mse: 8.1014 - val_mae: 1.4073 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 6s - loss: 10.8914 - mse: 10.8914 - mae: 1.3937 - val_loss: 8.2048 - val_mse: 8.2048 - val_mae: 1.3971 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 6s - loss: 10.7874 - mse: 10.7874 - mae: 1.3870 - val_loss: 8.2227 - val_mse: 8.2227 - val_mae: 1.4110 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 6s - loss: 10.7123 - mse: 10.7123 - mae: 1.3819 - val_loss: 8.3174 - val_mse: 8.3174 - val_mae: 1.4208 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 6s - loss: 10.6104 - mse: 10.6104 - mae: 1.3777 - val_loss: 8.4984 - val_mse: 8.4984 - val_mae: 1.4199 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 6s - loss: 10.5245 - mse: 10.5245 - mae: 1.3752 - val_loss: 8.5106 - val_mse: 8.5106 - val_mae: 1.4160 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
            "Score for fold 3: loss of 8.510590553283691\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 6s - loss: 9.6626 - mse: 9.6626 - mae: 1.3835 - val_loss: 11.7044 - val_mse: 11.7044 - val_mae: 1.3628 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 6s - loss: 9.8729 - mse: 9.8729 - mae: 1.3837 - val_loss: 11.6405 - val_mse: 11.6405 - val_mae: 1.3706 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 6s - loss: 9.4937 - mse: 9.4937 - mae: 1.3728 - val_loss: 11.9065 - val_mse: 11.9065 - val_mae: 1.3541 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 6s - loss: 9.3305 - mse: 9.3305 - mae: 1.3691 - val_loss: 11.8338 - val_mse: 11.8338 - val_mae: 1.3822 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 5s - loss: 9.4116 - mse: 9.4116 - mae: 1.3669 - val_loss: 11.6527 - val_mse: 11.6527 - val_mae: 1.4014 - lr: 0.0010 - 5s/epoch - 5ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 5s - loss: 9.3537 - mse: 9.3537 - mae: 1.3589 - val_loss: 11.8168 - val_mse: 11.8168 - val_mae: 1.3872 - lr: 0.0010 - 5s/epoch - 5ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 5s - loss: 9.2113 - mse: 9.2113 - mae: 1.3568 - val_loss: 11.8772 - val_mse: 11.8772 - val_mae: 1.3988 - lr: 0.0010 - 5s/epoch - 5ms/step\n",
            "Score for fold 4: loss of 11.877152442932129\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 5s - loss: 10.6551 - mse: 10.6551 - mae: 1.3719 - val_loss: 5.8609 - val_mse: 5.8609 - val_mae: 1.3553 - lr: 0.0010 - 5s/epoch - 5ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 5s - loss: 10.7686 - mse: 10.7686 - mae: 1.3698 - val_loss: 5.7836 - val_mse: 5.7836 - val_mae: 1.3470 - lr: 0.0010 - 5s/epoch - 5ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 5s - loss: 10.5403 - mse: 10.5403 - mae: 1.3610 - val_loss: 5.7355 - val_mse: 5.7355 - val_mae: 1.3625 - lr: 0.0010 - 5s/epoch - 5ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 5s - loss: 10.5461 - mse: 10.5461 - mae: 1.3569 - val_loss: 5.9202 - val_mse: 5.9202 - val_mae: 1.3496 - lr: 0.0010 - 5s/epoch - 5ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 6s - loss: 10.4355 - mse: 10.4355 - mae: 1.3492 - val_loss: 6.0800 - val_mse: 6.0800 - val_mae: 1.3573 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 6s - loss: 10.3659 - mse: 10.3659 - mae: 1.3461 - val_loss: 6.0104 - val_mse: 6.0104 - val_mae: 1.3501 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 6s - loss: 10.1475 - mse: 10.1475 - mae: 1.3395 - val_loss: 6.0855 - val_mse: 6.0855 - val_mae: 1.3412 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 6s - loss: 10.1782 - mse: 10.1782 - mae: 1.3385 - val_loss: 6.3288 - val_mse: 6.3288 - val_mae: 1.3807 - lr: 0.0010 - 6s/epoch - 6ms/step\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-10-12 09:20:27,487]\u001b[0m Finished trial#29 resulted in value: 10.934000000000001. Current best value is 8.826 with parameters: {'activation': 'tanh', 'num_hidden_layer': 4, 'i': 10, 'learning_rate': 0.00023399735711621326}.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Score for fold 5: loss of 6.328821659088135\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 6s - loss: 14.9625 - mse: 14.9625 - mae: 1.6279 - val_loss: 12.8878 - val_mse: 12.8878 - val_mae: 1.5493 - lr: 7.6288e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 6s - loss: 14.3227 - mse: 14.3227 - mae: 1.5590 - val_loss: 12.3856 - val_mse: 12.3856 - val_mae: 1.5394 - lr: 7.6288e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 6s - loss: 14.0537 - mse: 14.0537 - mae: 1.5337 - val_loss: 12.0594 - val_mse: 12.0594 - val_mae: 1.5122 - lr: 7.6288e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 6s - loss: 13.8856 - mse: 13.8856 - mae: 1.5159 - val_loss: 12.3281 - val_mse: 12.3281 - val_mae: 1.5300 - lr: 7.6288e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 6s - loss: 13.6818 - mse: 13.6818 - mae: 1.5134 - val_loss: 11.7145 - val_mse: 11.7145 - val_mae: 1.4593 - lr: 7.6288e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 5s - loss: 13.5710 - mse: 13.5710 - mae: 1.4990 - val_loss: 11.6203 - val_mse: 11.6203 - val_mae: 1.4167 - lr: 7.6288e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 5s - loss: 13.5197 - mse: 13.5197 - mae: 1.4970 - val_loss: 11.4783 - val_mse: 11.4783 - val_mae: 1.4744 - lr: 7.6288e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 5s - loss: 13.4603 - mse: 13.4603 - mae: 1.4904 - val_loss: 11.4579 - val_mse: 11.4579 - val_mae: 1.4884 - lr: 7.6288e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 5s - loss: 13.3257 - mse: 13.3257 - mae: 1.4837 - val_loss: 11.2512 - val_mse: 11.2512 - val_mae: 1.5099 - lr: 7.6288e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 5s - loss: 13.2870 - mse: 13.2870 - mae: 1.4835 - val_loss: 11.5377 - val_mse: 11.5377 - val_mae: 1.3935 - lr: 7.6288e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 5s - loss: 13.1713 - mse: 13.1713 - mae: 1.4837 - val_loss: 11.0684 - val_mse: 11.0684 - val_mae: 1.5126 - lr: 7.6288e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 5s - loss: 13.1650 - mse: 13.1650 - mae: 1.4785 - val_loss: 11.0437 - val_mse: 11.0437 - val_mae: 1.4803 - lr: 7.6288e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 13/100\n",
            "1000/1000 - 5s - loss: 13.0722 - mse: 13.0722 - mae: 1.4727 - val_loss: 11.2292 - val_mse: 11.2292 - val_mae: 1.5741 - lr: 7.6288e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 14/100\n",
            "1000/1000 - 5s - loss: 13.0427 - mse: 13.0427 - mae: 1.4703 - val_loss: 11.1231 - val_mse: 11.1231 - val_mae: 1.4312 - lr: 7.6288e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 15/100\n",
            "1000/1000 - 5s - loss: 12.9741 - mse: 12.9741 - mae: 1.4759 - val_loss: 11.1191 - val_mse: 11.1191 - val_mae: 1.4022 - lr: 7.6288e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 16/100\n",
            "1000/1000 - 5s - loss: 12.8442 - mse: 12.8442 - mae: 1.4705 - val_loss: 10.7314 - val_mse: 10.7314 - val_mae: 1.5335 - lr: 7.6288e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 17/100\n",
            "1000/1000 - 5s - loss: 12.8645 - mse: 12.8645 - mae: 1.4671 - val_loss: 10.8043 - val_mse: 10.8043 - val_mae: 1.4700 - lr: 7.6288e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 18/100\n",
            "1000/1000 - 5s - loss: 12.7103 - mse: 12.7103 - mae: 1.4647 - val_loss: 10.6276 - val_mse: 10.6276 - val_mae: 1.6385 - lr: 7.6288e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 19/100\n",
            "1000/1000 - 5s - loss: 12.6259 - mse: 12.6259 - mae: 1.4555 - val_loss: 10.8270 - val_mse: 10.8270 - val_mae: 1.4740 - lr: 7.6288e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 20/100\n",
            "1000/1000 - 5s - loss: 12.6457 - mse: 12.6457 - mae: 1.4649 - val_loss: 10.1677 - val_mse: 10.1677 - val_mae: 1.4023 - lr: 7.6288e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 21/100\n",
            "1000/1000 - 5s - loss: 12.4289 - mse: 12.4289 - mae: 1.4572 - val_loss: 10.5886 - val_mse: 10.5886 - val_mae: 1.4780 - lr: 7.6288e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 22/100\n",
            "1000/1000 - 5s - loss: 12.4464 - mse: 12.4464 - mae: 1.4503 - val_loss: 10.5743 - val_mse: 10.5743 - val_mae: 1.4100 - lr: 7.6288e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 23/100\n",
            "1000/1000 - 5s - loss: 12.3054 - mse: 12.3054 - mae: 1.4454 - val_loss: 10.2568 - val_mse: 10.2568 - val_mae: 1.5369 - lr: 7.6288e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 24/100\n",
            "1000/1000 - 5s - loss: 12.2927 - mse: 12.2927 - mae: 1.4425 - val_loss: 10.6498 - val_mse: 10.6498 - val_mae: 1.6643 - lr: 7.6288e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 25/100\n",
            "1000/1000 - 5s - loss: 12.1573 - mse: 12.1573 - mae: 1.4438 - val_loss: 11.4951 - val_mse: 11.4951 - val_mae: 1.6777 - lr: 7.6288e-04 - 5s/epoch - 5ms/step\n",
            "Score for fold 1: loss of 11.49509334564209\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 5s - loss: 10.9039 - mse: 10.9039 - mae: 1.4538 - val_loss: 15.8897 - val_mse: 15.8897 - val_mae: 1.5284 - lr: 7.6288e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 5s - loss: 10.7096 - mse: 10.7096 - mae: 1.4430 - val_loss: 15.8216 - val_mse: 15.8216 - val_mae: 1.4732 - lr: 7.6288e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 5s - loss: 10.5134 - mse: 10.5134 - mae: 1.4372 - val_loss: 15.9824 - val_mse: 15.9824 - val_mae: 1.5873 - lr: 7.6288e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 5s - loss: 10.6142 - mse: 10.6142 - mae: 1.4360 - val_loss: 16.3081 - val_mse: 16.3081 - val_mae: 1.5443 - lr: 7.6288e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 6s - loss: 10.4388 - mse: 10.4388 - mae: 1.4333 - val_loss: 15.7037 - val_mse: 15.7037 - val_mae: 1.4713 - lr: 7.6288e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 6s - loss: 10.3033 - mse: 10.3033 - mae: 1.4302 - val_loss: 15.9812 - val_mse: 15.9812 - val_mae: 1.4789 - lr: 7.6288e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 5s - loss: 10.2415 - mse: 10.2415 - mae: 1.4286 - val_loss: 16.2673 - val_mse: 16.2673 - val_mae: 1.3997 - lr: 7.6288e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 5s - loss: 10.3192 - mse: 10.3192 - mae: 1.4254 - val_loss: 16.2523 - val_mse: 16.2523 - val_mae: 1.3949 - lr: 7.6288e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 5s - loss: 9.9722 - mse: 9.9722 - mae: 1.4020 - val_loss: 16.0474 - val_mse: 16.0474 - val_mae: 1.4518 - lr: 7.6288e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 5s - loss: 9.8505 - mse: 9.8505 - mae: 1.4014 - val_loss: 16.2286 - val_mse: 16.2286 - val_mae: 1.5839 - lr: 7.6288e-04 - 5s/epoch - 5ms/step\n",
            "Score for fold 2: loss of 16.22863006591797\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 5s - loss: 11.9698 - mse: 11.9698 - mae: 1.4333 - val_loss: 7.9015 - val_mse: 7.9015 - val_mae: 1.5266 - lr: 7.6288e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 5s - loss: 11.8601 - mse: 11.8601 - mae: 1.4276 - val_loss: 7.9114 - val_mse: 7.9114 - val_mae: 1.3803 - lr: 7.6288e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 5s - loss: 11.6526 - mse: 11.6526 - mae: 1.4303 - val_loss: 8.1023 - val_mse: 8.1023 - val_mae: 1.3712 - lr: 7.6288e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 5s - loss: 11.7708 - mse: 11.7708 - mae: 1.4203 - val_loss: 8.1413 - val_mse: 8.1413 - val_mae: 1.3943 - lr: 7.6288e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 5s - loss: 11.5374 - mse: 11.5374 - mae: 1.4218 - val_loss: 7.9305 - val_mse: 7.9305 - val_mae: 1.4320 - lr: 7.6288e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 5s - loss: 11.3467 - mse: 11.3467 - mae: 1.3987 - val_loss: 8.3566 - val_mse: 8.3566 - val_mae: 1.4805 - lr: 7.6288e-04 - 5s/epoch - 5ms/step\n",
            "Score for fold 3: loss of 8.356600761413574\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 5s - loss: 10.7323 - mse: 10.7323 - mae: 1.4210 - val_loss: 10.5893 - val_mse: 10.5893 - val_mae: 1.3364 - lr: 7.6288e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 5s - loss: 10.5783 - mse: 10.5783 - mae: 1.4084 - val_loss: 10.6419 - val_mse: 10.6419 - val_mae: 1.3322 - lr: 7.6288e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 5s - loss: 10.4776 - mse: 10.4776 - mae: 1.3974 - val_loss: 10.6858 - val_mse: 10.6858 - val_mae: 1.4213 - lr: 7.6288e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 5s - loss: 10.2015 - mse: 10.2015 - mae: 1.3946 - val_loss: 10.7635 - val_mse: 10.7635 - val_mae: 1.4380 - lr: 7.6288e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 5s - loss: 10.1404 - mse: 10.1404 - mae: 1.3888 - val_loss: 10.9365 - val_mse: 10.9365 - val_mae: 1.3824 - lr: 7.6288e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 5s - loss: 9.9598 - mse: 9.9598 - mae: 1.3799 - val_loss: 10.9924 - val_mse: 10.9924 - val_mae: 1.4215 - lr: 7.6288e-04 - 5s/epoch - 5ms/step\n",
            "Score for fold 4: loss of 10.992424011230469\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 5s - loss: 9.9732 - mse: 9.9732 - mae: 1.3859 - val_loss: 11.0544 - val_mse: 11.0544 - val_mae: 1.5438 - lr: 7.6288e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 5s - loss: 9.8442 - mse: 9.8442 - mae: 1.3799 - val_loss: 11.7590 - val_mse: 11.7590 - val_mae: 1.3152 - lr: 7.6288e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 5s - loss: 9.6124 - mse: 9.6124 - mae: 1.3624 - val_loss: 11.3345 - val_mse: 11.3345 - val_mae: 1.3471 - lr: 7.6288e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 5s - loss: 9.5062 - mse: 9.5062 - mae: 1.3639 - val_loss: 11.5765 - val_mse: 11.5765 - val_mae: 1.3731 - lr: 7.6288e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 5s - loss: 9.4470 - mse: 9.4470 - mae: 1.3560 - val_loss: 11.7237 - val_mse: 11.7237 - val_mae: 1.3727 - lr: 7.6288e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 5s - loss: 9.1529 - mse: 9.1529 - mae: 1.3485 - val_loss: 11.7742 - val_mse: 11.7742 - val_mae: 1.4393 - lr: 7.6288e-04 - 5s/epoch - 5ms/step\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-10-12 09:24:57,495]\u001b[0m Finished trial#30 resulted in value: 11.770000000000001. Current best value is 8.826 with parameters: {'activation': 'tanh', 'num_hidden_layer': 4, 'i': 10, 'learning_rate': 0.00023399735711621326}.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Score for fold 5: loss of 11.774177551269531\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 6s - loss: 13.5004 - mse: 13.5004 - mae: 1.5384 - val_loss: 11.4617 - val_mse: 11.4617 - val_mae: 1.5238 - lr: 1.9808e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 6s - loss: 12.6080 - mse: 12.6080 - mae: 1.4792 - val_loss: 11.1261 - val_mse: 11.1261 - val_mae: 1.5178 - lr: 1.9808e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 5s - loss: 12.3392 - mse: 12.3392 - mae: 1.4600 - val_loss: 11.1759 - val_mse: 11.1759 - val_mae: 1.4716 - lr: 1.9808e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 5s - loss: 12.2767 - mse: 12.2767 - mae: 1.4497 - val_loss: 12.2817 - val_mse: 12.2817 - val_mae: 1.4443 - lr: 1.9808e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 6s - loss: 12.1349 - mse: 12.1349 - mae: 1.4434 - val_loss: 11.2910 - val_mse: 11.2910 - val_mae: 1.4827 - lr: 1.9808e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 6s - loss: 12.0484 - mse: 12.0484 - mae: 1.4325 - val_loss: 11.2368 - val_mse: 11.2368 - val_mae: 1.4839 - lr: 1.9808e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 6s - loss: 12.0247 - mse: 12.0247 - mae: 1.4261 - val_loss: 11.2182 - val_mse: 11.2182 - val_mae: 1.5065 - lr: 1.9808e-04 - 6s/epoch - 6ms/step\n",
            "Score for fold 1: loss of 11.218175888061523\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 6s - loss: 11.6573 - mse: 11.6573 - mae: 1.4270 - val_loss: 11.9227 - val_mse: 11.9227 - val_mae: 1.4697 - lr: 1.9808e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 6s - loss: 11.6209 - mse: 11.6209 - mae: 1.4193 - val_loss: 11.6050 - val_mse: 11.6050 - val_mae: 1.4848 - lr: 1.9808e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 6s - loss: 11.5062 - mse: 11.5062 - mae: 1.4117 - val_loss: 11.8566 - val_mse: 11.8566 - val_mae: 1.5000 - lr: 1.9808e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 6s - loss: 11.4130 - mse: 11.4130 - mae: 1.4093 - val_loss: 11.7173 - val_mse: 11.7173 - val_mae: 1.4306 - lr: 1.9808e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 6s - loss: 11.2307 - mse: 11.2307 - mae: 1.4034 - val_loss: 11.6943 - val_mse: 11.6943 - val_mae: 1.4717 - lr: 1.9808e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 6s - loss: 11.2264 - mse: 11.2264 - mae: 1.3987 - val_loss: 11.7097 - val_mse: 11.7097 - val_mae: 1.4514 - lr: 1.9808e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 6s - loss: 10.9819 - mse: 10.9819 - mae: 1.3863 - val_loss: 11.8052 - val_mse: 11.8052 - val_mae: 1.4465 - lr: 1.9808e-04 - 6s/epoch - 6ms/step\n",
            "Score for fold 2: loss of 11.805215835571289\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 6s - loss: 10.9871 - mse: 10.9871 - mae: 1.4088 - val_loss: 11.1075 - val_mse: 11.1075 - val_mae: 1.3969 - lr: 1.9808e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 6s - loss: 11.1009 - mse: 11.1009 - mae: 1.3985 - val_loss: 11.7335 - val_mse: 11.7335 - val_mae: 1.4234 - lr: 1.9808e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 6s - loss: 10.7663 - mse: 10.7663 - mae: 1.3928 - val_loss: 11.0717 - val_mse: 11.0717 - val_mae: 1.3605 - lr: 1.9808e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 6s - loss: 10.6182 - mse: 10.6182 - mae: 1.3813 - val_loss: 11.0654 - val_mse: 11.0654 - val_mae: 1.4115 - lr: 1.9808e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 6s - loss: 10.5890 - mse: 10.5890 - mae: 1.3723 - val_loss: 12.0281 - val_mse: 12.0281 - val_mae: 1.4245 - lr: 1.9808e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 6s - loss: 10.3153 - mse: 10.3153 - mae: 1.3646 - val_loss: 11.2765 - val_mse: 11.2765 - val_mae: 1.4063 - lr: 1.9808e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 6s - loss: 10.1864 - mse: 10.1864 - mae: 1.3574 - val_loss: 10.9785 - val_mse: 10.9785 - val_mae: 1.4537 - lr: 1.9808e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 6s - loss: 10.1967 - mse: 10.1967 - mae: 1.3461 - val_loss: 11.0382 - val_mse: 11.0382 - val_mae: 1.4582 - lr: 1.9808e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 7s - loss: 10.1669 - mse: 10.1669 - mae: 1.3367 - val_loss: 11.2346 - val_mse: 11.2346 - val_mae: 1.4220 - lr: 1.9808e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 7s - loss: 10.0149 - mse: 10.0149 - mae: 1.3305 - val_loss: 11.2859 - val_mse: 11.2859 - val_mae: 1.4039 - lr: 1.9808e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 7s - loss: 9.6197 - mse: 9.6197 - mae: 1.3176 - val_loss: 11.2074 - val_mse: 11.2074 - val_mae: 1.4309 - lr: 1.9808e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 7s - loss: 9.6611 - mse: 9.6611 - mae: 1.3070 - val_loss: 11.0126 - val_mse: 11.0126 - val_mae: 1.4370 - lr: 1.9808e-04 - 7s/epoch - 7ms/step\n",
            "Score for fold 3: loss of 11.01258373260498\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 7s - loss: 10.6412 - mse: 10.6412 - mae: 1.3356 - val_loss: 7.2916 - val_mse: 7.2916 - val_mae: 1.3468 - lr: 1.9808e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 6s - loss: 10.3746 - mse: 10.3746 - mae: 1.3215 - val_loss: 7.6530 - val_mse: 7.6530 - val_mae: 1.3248 - lr: 1.9808e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 6s - loss: 10.2336 - mse: 10.2336 - mae: 1.3127 - val_loss: 7.1873 - val_mse: 7.1873 - val_mae: 1.2464 - lr: 1.9808e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 6s - loss: 10.0067 - mse: 10.0067 - mae: 1.2971 - val_loss: 7.1170 - val_mse: 7.1170 - val_mae: 1.3217 - lr: 1.9808e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 7s - loss: 9.8369 - mse: 9.8369 - mae: 1.2853 - val_loss: 8.6295 - val_mse: 8.6295 - val_mae: 1.3591 - lr: 1.9808e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 6s - loss: 9.7324 - mse: 9.7324 - mae: 1.2768 - val_loss: 7.4192 - val_mse: 7.4192 - val_mae: 1.3606 - lr: 1.9808e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 6s - loss: 9.5700 - mse: 9.5700 - mae: 1.2647 - val_loss: 7.5210 - val_mse: 7.5210 - val_mae: 1.3247 - lr: 1.9808e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 6s - loss: 9.5796 - mse: 9.5796 - mae: 1.2528 - val_loss: 7.8004 - val_mse: 7.8004 - val_mae: 1.3214 - lr: 1.9808e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 6s - loss: 9.3177 - mse: 9.3177 - mae: 1.2414 - val_loss: 7.6278 - val_mse: 7.6278 - val_mae: 1.3187 - lr: 1.9808e-04 - 6s/epoch - 6ms/step\n",
            "Score for fold 4: loss of 7.627769947052002\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 6s - loss: 8.2624 - mse: 8.2624 - mae: 1.2718 - val_loss: 12.0031 - val_mse: 12.0031 - val_mae: 1.1987 - lr: 1.9808e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 5s - loss: 7.8266 - mse: 7.8266 - mae: 1.2480 - val_loss: 12.3210 - val_mse: 12.3210 - val_mae: 1.1752 - lr: 1.9808e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 6s - loss: 7.7317 - mse: 7.7317 - mae: 1.2384 - val_loss: 12.3299 - val_mse: 12.3299 - val_mae: 1.1991 - lr: 1.9808e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 5s - loss: 7.5208 - mse: 7.5208 - mae: 1.2224 - val_loss: 12.3430 - val_mse: 12.3430 - val_mae: 1.2288 - lr: 1.9808e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 6s - loss: 7.3636 - mse: 7.3636 - mae: 1.2080 - val_loss: 12.4214 - val_mse: 12.4214 - val_mae: 1.2547 - lr: 1.9808e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 6s - loss: 7.2886 - mse: 7.2886 - mae: 1.1938 - val_loss: 12.4152 - val_mse: 12.4152 - val_mae: 1.2365 - lr: 1.9808e-04 - 6s/epoch - 6ms/step\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-10-12 09:29:05,998]\u001b[0m Finished trial#31 resulted in value: 10.818000000000001. Current best value is 8.826 with parameters: {'activation': 'tanh', 'num_hidden_layer': 4, 'i': 10, 'learning_rate': 0.00023399735711621326}.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Score for fold 5: loss of 12.415234565734863\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 6s - loss: 13.9897 - mse: 13.9897 - mae: 1.5468 - val_loss: 11.1034 - val_mse: 11.1034 - val_mae: 1.4900 - lr: 1.6233e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 5s - loss: 12.7569 - mse: 12.7569 - mae: 1.4856 - val_loss: 11.5912 - val_mse: 11.5912 - val_mae: 1.5306 - lr: 1.6233e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 6s - loss: 12.4404 - mse: 12.4404 - mae: 1.4695 - val_loss: 10.8548 - val_mse: 10.8548 - val_mae: 1.4471 - lr: 1.6233e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 6s - loss: 12.3682 - mse: 12.3682 - mae: 1.4568 - val_loss: 10.9366 - val_mse: 10.9366 - val_mae: 1.6175 - lr: 1.6233e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 6s - loss: 12.2313 - mse: 12.2313 - mae: 1.4494 - val_loss: 11.9879 - val_mse: 11.9879 - val_mae: 1.4439 - lr: 1.6233e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 6s - loss: 12.1352 - mse: 12.1352 - mae: 1.4382 - val_loss: 10.8229 - val_mse: 10.8229 - val_mae: 1.4737 - lr: 1.6233e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 6s - loss: 12.0280 - mse: 12.0280 - mae: 1.4351 - val_loss: 11.1557 - val_mse: 11.1557 - val_mae: 1.4142 - lr: 1.6233e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 6s - loss: 11.8959 - mse: 11.8959 - mae: 1.4260 - val_loss: 11.8964 - val_mse: 11.8964 - val_mae: 1.4580 - lr: 1.6233e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 6s - loss: 11.8459 - mse: 11.8459 - mae: 1.4230 - val_loss: 11.4647 - val_mse: 11.4647 - val_mae: 1.4793 - lr: 1.6233e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 6s - loss: 11.8323 - mse: 11.8323 - mae: 1.4218 - val_loss: 11.3164 - val_mse: 11.3164 - val_mae: 1.3951 - lr: 1.6233e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 6s - loss: 11.6651 - mse: 11.6651 - mae: 1.4156 - val_loss: 10.6381 - val_mse: 10.6381 - val_mae: 1.4344 - lr: 1.6233e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 6s - loss: 11.4726 - mse: 11.4726 - mae: 1.4081 - val_loss: 11.2498 - val_mse: 11.2498 - val_mae: 1.4209 - lr: 1.6233e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 13/100\n",
            "1000/1000 - 6s - loss: 11.3959 - mse: 11.3959 - mae: 1.3988 - val_loss: 10.9876 - val_mse: 10.9876 - val_mae: 1.4132 - lr: 1.6233e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 14/100\n",
            "1000/1000 - 6s - loss: 11.3260 - mse: 11.3260 - mae: 1.3965 - val_loss: 10.8575 - val_mse: 10.8575 - val_mae: 1.4535 - lr: 1.6233e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 15/100\n",
            "1000/1000 - 6s - loss: 11.2261 - mse: 11.2261 - mae: 1.3888 - val_loss: 10.5682 - val_mse: 10.5682 - val_mae: 1.4495 - lr: 1.6233e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 16/100\n",
            "1000/1000 - 6s - loss: 11.1780 - mse: 11.1780 - mae: 1.3813 - val_loss: 10.6894 - val_mse: 10.6894 - val_mae: 1.4664 - lr: 1.6233e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 17/100\n",
            "1000/1000 - 6s - loss: 10.9865 - mse: 10.9865 - mae: 1.3713 - val_loss: 10.6766 - val_mse: 10.6766 - val_mae: 1.4634 - lr: 1.6233e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 18/100\n",
            "1000/1000 - 6s - loss: 10.8965 - mse: 10.8965 - mae: 1.3665 - val_loss: 10.3413 - val_mse: 10.3413 - val_mae: 1.4354 - lr: 1.6233e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 19/100\n",
            "1000/1000 - 5s - loss: 10.8701 - mse: 10.8701 - mae: 1.3624 - val_loss: 12.0164 - val_mse: 12.0164 - val_mae: 1.4642 - lr: 1.6233e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 20/100\n",
            "1000/1000 - 6s - loss: 10.6609 - mse: 10.6609 - mae: 1.3548 - val_loss: 11.4385 - val_mse: 11.4385 - val_mae: 1.4638 - lr: 1.6233e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 21/100\n",
            "1000/1000 - 6s - loss: 10.5530 - mse: 10.5530 - mae: 1.3416 - val_loss: 11.8331 - val_mse: 11.8331 - val_mae: 1.4218 - lr: 1.6233e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 22/100\n",
            "1000/1000 - 6s - loss: 10.5073 - mse: 10.5073 - mae: 1.3384 - val_loss: 11.8546 - val_mse: 11.8546 - val_mae: 1.4621 - lr: 1.6233e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 23/100\n",
            "1000/1000 - 6s - loss: 10.3160 - mse: 10.3160 - mae: 1.3297 - val_loss: 10.8793 - val_mse: 10.8793 - val_mae: 1.4243 - lr: 1.6233e-04 - 6s/epoch - 6ms/step\n",
            "Score for fold 1: loss of 10.879313468933105\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 6s - loss: 8.4238 - mse: 8.4238 - mae: 1.3529 - val_loss: 18.0650 - val_mse: 18.0650 - val_mae: 1.3445 - lr: 1.6233e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 6s - loss: 8.2813 - mse: 8.2813 - mae: 1.3403 - val_loss: 18.0913 - val_mse: 18.0913 - val_mae: 1.3805 - lr: 1.6233e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 7s - loss: 8.0318 - mse: 8.0318 - mae: 1.3280 - val_loss: 18.3178 - val_mse: 18.3178 - val_mae: 1.3608 - lr: 1.6233e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 7s - loss: 7.9864 - mse: 7.9864 - mae: 1.3222 - val_loss: 18.4520 - val_mse: 18.4520 - val_mae: 1.3968 - lr: 1.6233e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 7s - loss: 7.8539 - mse: 7.8539 - mae: 1.3113 - val_loss: 18.3973 - val_mse: 18.3973 - val_mae: 1.3657 - lr: 1.6233e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 8s - loss: 7.9384 - mse: 7.9384 - mae: 1.3038 - val_loss: 18.7146 - val_mse: 18.7146 - val_mae: 1.3998 - lr: 1.6233e-04 - 8s/epoch - 8ms/step\n",
            "Score for fold 2: loss of 18.714567184448242\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 7s - loss: 9.8177 - mse: 9.8177 - mae: 1.3276 - val_loss: 10.3268 - val_mse: 10.3268 - val_mae: 1.3229 - lr: 1.6233e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 7s - loss: 9.4887 - mse: 9.4887 - mae: 1.3109 - val_loss: 10.9922 - val_mse: 10.9922 - val_mae: 1.2821 - lr: 1.6233e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 7s - loss: 9.4775 - mse: 9.4775 - mae: 1.2994 - val_loss: 10.7450 - val_mse: 10.7450 - val_mae: 1.3411 - lr: 1.6233e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 7s - loss: 9.2879 - mse: 9.2879 - mae: 1.2868 - val_loss: 11.3301 - val_mse: 11.3301 - val_mae: 1.3121 - lr: 1.6233e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 7s - loss: 9.0979 - mse: 9.0979 - mae: 1.2796 - val_loss: 10.8939 - val_mse: 10.8939 - val_mae: 1.3184 - lr: 1.6233e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 7s - loss: 8.9265 - mse: 8.9265 - mae: 1.2647 - val_loss: 10.8287 - val_mse: 10.8287 - val_mae: 1.3230 - lr: 1.6233e-04 - 7s/epoch - 7ms/step\n",
            "Score for fold 3: loss of 10.828695297241211\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 6s - loss: 10.5152 - mse: 10.5152 - mae: 1.2949 - val_loss: 4.8353 - val_mse: 4.8353 - val_mae: 1.2222 - lr: 1.6233e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 7s - loss: 10.2430 - mse: 10.2430 - mae: 1.2736 - val_loss: 5.1335 - val_mse: 5.1335 - val_mae: 1.1895 - lr: 1.6233e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 6s - loss: 10.1098 - mse: 10.1098 - mae: 1.2632 - val_loss: 5.4662 - val_mse: 5.4662 - val_mae: 1.2145 - lr: 1.6233e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 7s - loss: 9.9210 - mse: 9.9210 - mae: 1.2528 - val_loss: 5.1329 - val_mse: 5.1329 - val_mae: 1.2507 - lr: 1.6233e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 6s - loss: 9.7073 - mse: 9.7073 - mae: 1.2423 - val_loss: 5.2905 - val_mse: 5.2905 - val_mae: 1.2461 - lr: 1.6233e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 6s - loss: 9.5697 - mse: 9.5697 - mae: 1.2300 - val_loss: 5.3118 - val_mse: 5.3118 - val_mae: 1.2583 - lr: 1.6233e-04 - 6s/epoch - 6ms/step\n",
            "Score for fold 4: loss of 5.311770915985107\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 6s - loss: 9.6155 - mse: 9.6155 - mae: 1.2492 - val_loss: 4.6227 - val_mse: 4.6227 - val_mae: 1.1910 - lr: 1.6233e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 6s - loss: 9.6148 - mse: 9.6148 - mae: 1.2355 - val_loss: 4.5985 - val_mse: 4.5985 - val_mae: 1.2288 - lr: 1.6233e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 6s - loss: 9.3361 - mse: 9.3361 - mae: 1.2200 - val_loss: 5.0333 - val_mse: 5.0333 - val_mae: 1.2474 - lr: 1.6233e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 6s - loss: 9.2297 - mse: 9.2297 - mae: 1.2055 - val_loss: 5.1192 - val_mse: 5.1192 - val_mae: 1.1969 - lr: 1.6233e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 6s - loss: 9.0914 - mse: 9.0914 - mae: 1.1970 - val_loss: 4.9916 - val_mse: 4.9916 - val_mae: 1.2160 - lr: 1.6233e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 6s - loss: 9.0275 - mse: 9.0275 - mae: 1.1847 - val_loss: 5.0863 - val_mse: 5.0863 - val_mae: 1.1834 - lr: 1.6233e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 6s - loss: 8.7805 - mse: 8.7805 - mae: 1.1723 - val_loss: 4.9618 - val_mse: 4.9618 - val_mae: 1.2486 - lr: 1.6233e-04 - 6s/epoch - 6ms/step\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-10-12 09:34:08,279]\u001b[0m Finished trial#32 resulted in value: 10.138000000000002. Current best value is 8.826 with parameters: {'activation': 'tanh', 'num_hidden_layer': 4, 'i': 10, 'learning_rate': 0.00023399735711621326}.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Score for fold 5: loss of 4.961838245391846\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 7s - loss: 13.8634 - mse: 13.8634 - mae: 1.5431 - val_loss: 11.9583 - val_mse: 11.9583 - val_mae: 1.6758 - lr: 1.3277e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 6s - loss: 12.7921 - mse: 12.7921 - mae: 1.4922 - val_loss: 11.4992 - val_mse: 11.4992 - val_mae: 1.5335 - lr: 1.3277e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 6s - loss: 12.5152 - mse: 12.5152 - mae: 1.4706 - val_loss: 11.0874 - val_mse: 11.0874 - val_mae: 1.4108 - lr: 1.3277e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 6s - loss: 12.3437 - mse: 12.3437 - mae: 1.4574 - val_loss: 11.0569 - val_mse: 11.0569 - val_mae: 1.4572 - lr: 1.3277e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 6s - loss: 12.1165 - mse: 12.1165 - mae: 1.4488 - val_loss: 10.9386 - val_mse: 10.9386 - val_mae: 1.5148 - lr: 1.3277e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 6s - loss: 12.1759 - mse: 12.1759 - mae: 1.4421 - val_loss: 10.7521 - val_mse: 10.7521 - val_mae: 1.5315 - lr: 1.3277e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 6s - loss: 12.0783 - mse: 12.0783 - mae: 1.4346 - val_loss: 10.8153 - val_mse: 10.8153 - val_mae: 1.5170 - lr: 1.3277e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 6s - loss: 11.9317 - mse: 11.9317 - mae: 1.4308 - val_loss: 10.8516 - val_mse: 10.8516 - val_mae: 1.4662 - lr: 1.3277e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 6s - loss: 11.9332 - mse: 11.9332 - mae: 1.4249 - val_loss: 10.9040 - val_mse: 10.9040 - val_mae: 1.4917 - lr: 1.3277e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 6s - loss: 11.8365 - mse: 11.8365 - mae: 1.4178 - val_loss: 10.6677 - val_mse: 10.6677 - val_mae: 1.4730 - lr: 1.3277e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 6s - loss: 11.7223 - mse: 11.7223 - mae: 1.4154 - val_loss: 10.8111 - val_mse: 10.8111 - val_mae: 1.4750 - lr: 1.3277e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 6s - loss: 11.6844 - mse: 11.6844 - mae: 1.4064 - val_loss: 11.0755 - val_mse: 11.0755 - val_mae: 1.4417 - lr: 1.3277e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 13/100\n",
            "1000/1000 - 6s - loss: 11.7631 - mse: 11.7631 - mae: 1.4023 - val_loss: 10.7547 - val_mse: 10.7547 - val_mae: 1.5211 - lr: 1.3277e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 14/100\n",
            "1000/1000 - 7s - loss: 11.4803 - mse: 11.4803 - mae: 1.3946 - val_loss: 10.7793 - val_mse: 10.7793 - val_mae: 1.4899 - lr: 1.3277e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 15/100\n",
            "1000/1000 - 6s - loss: 11.5836 - mse: 11.5836 - mae: 1.3949 - val_loss: 10.7547 - val_mse: 10.7547 - val_mae: 1.4899 - lr: 1.3277e-04 - 6s/epoch - 6ms/step\n",
            "Score for fold 1: loss of 10.75471019744873\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 6s - loss: 12.2511 - mse: 12.2511 - mae: 1.4132 - val_loss: 7.5774 - val_mse: 7.5774 - val_mae: 1.3709 - lr: 1.3277e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 6s - loss: 12.0205 - mse: 12.0205 - mae: 1.4076 - val_loss: 7.7193 - val_mse: 7.7193 - val_mae: 1.4085 - lr: 1.3277e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 7s - loss: 11.9564 - mse: 11.9564 - mae: 1.4012 - val_loss: 7.4359 - val_mse: 7.4359 - val_mae: 1.3794 - lr: 1.3277e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 7s - loss: 11.7778 - mse: 11.7778 - mae: 1.3947 - val_loss: 8.1459 - val_mse: 8.1459 - val_mae: 1.4186 - lr: 1.3277e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 7s - loss: 11.6588 - mse: 11.6588 - mae: 1.3864 - val_loss: 7.8867 - val_mse: 7.8867 - val_mae: 1.3604 - lr: 1.3277e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 7s - loss: 11.7498 - mse: 11.7498 - mae: 1.3791 - val_loss: 8.0890 - val_mse: 8.0890 - val_mae: 1.3985 - lr: 1.3277e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 7s - loss: 11.5947 - mse: 11.5947 - mae: 1.3763 - val_loss: 7.7360 - val_mse: 7.7360 - val_mae: 1.4358 - lr: 1.3277e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 7s - loss: 11.4013 - mse: 11.4013 - mae: 1.3697 - val_loss: 7.7055 - val_mse: 7.7055 - val_mae: 1.3658 - lr: 1.3277e-04 - 7s/epoch - 7ms/step\n",
            "Score for fold 2: loss of 7.705541133880615\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 7s - loss: 9.7537 - mse: 9.7537 - mae: 1.3663 - val_loss: 14.1549 - val_mse: 14.1549 - val_mae: 1.4147 - lr: 1.3277e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 7s - loss: 9.6252 - mse: 9.6252 - mae: 1.3574 - val_loss: 14.4313 - val_mse: 14.4313 - val_mae: 1.4428 - lr: 1.3277e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 7s - loss: 9.5413 - mse: 9.5413 - mae: 1.3527 - val_loss: 14.1426 - val_mse: 14.1426 - val_mae: 1.4432 - lr: 1.3277e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 6s - loss: 9.4913 - mse: 9.4913 - mae: 1.3405 - val_loss: 14.2771 - val_mse: 14.2771 - val_mae: 1.4464 - lr: 1.3277e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 7s - loss: 9.3180 - mse: 9.3180 - mae: 1.3344 - val_loss: 14.2772 - val_mse: 14.2772 - val_mae: 1.3681 - lr: 1.3277e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 7s - loss: 9.1587 - mse: 9.1587 - mae: 1.3265 - val_loss: 14.2366 - val_mse: 14.2366 - val_mae: 1.4547 - lr: 1.3277e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 7s - loss: 9.0158 - mse: 9.0158 - mae: 1.3201 - val_loss: 14.7638 - val_mse: 14.7638 - val_mae: 1.3820 - lr: 1.3277e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 7s - loss: 8.9613 - mse: 8.9613 - mae: 1.3135 - val_loss: 14.4887 - val_mse: 14.4887 - val_mae: 1.3815 - lr: 1.3277e-04 - 7s/epoch - 7ms/step\n",
            "Score for fold 3: loss of 14.488715171813965\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 7s - loss: 11.0908 - mse: 11.0908 - mae: 1.3458 - val_loss: 7.0170 - val_mse: 7.0170 - val_mae: 1.2538 - lr: 1.3277e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 7s - loss: 10.6735 - mse: 10.6735 - mae: 1.3364 - val_loss: 7.3277 - val_mse: 7.3277 - val_mae: 1.3281 - lr: 1.3277e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 6s - loss: 10.5732 - mse: 10.5732 - mae: 1.3273 - val_loss: 7.1417 - val_mse: 7.1417 - val_mae: 1.3203 - lr: 1.3277e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 7s - loss: 10.2945 - mse: 10.2945 - mae: 1.3113 - val_loss: 7.5858 - val_mse: 7.5858 - val_mae: 1.2935 - lr: 1.3277e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 7s - loss: 10.2745 - mse: 10.2745 - mae: 1.3050 - val_loss: 7.2434 - val_mse: 7.2434 - val_mae: 1.3348 - lr: 1.3277e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 7s - loss: 10.0589 - mse: 10.0589 - mae: 1.2968 - val_loss: 7.2084 - val_mse: 7.2084 - val_mae: 1.3142 - lr: 1.3277e-04 - 7s/epoch - 7ms/step\n",
            "Score for fold 4: loss of 7.208392143249512\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 7s - loss: 8.4384 - mse: 8.4384 - mae: 1.2989 - val_loss: 13.3945 - val_mse: 13.3945 - val_mae: 1.2894 - lr: 1.3277e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 7s - loss: 8.4451 - mse: 8.4451 - mae: 1.2876 - val_loss: 13.1222 - val_mse: 13.1222 - val_mae: 1.2926 - lr: 1.3277e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 7s - loss: 8.3863 - mse: 8.3863 - mae: 1.2834 - val_loss: 13.4966 - val_mse: 13.4966 - val_mae: 1.2845 - lr: 1.3277e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 7s - loss: 8.1226 - mse: 8.1226 - mae: 1.2671 - val_loss: 13.6786 - val_mse: 13.6786 - val_mae: 1.3003 - lr: 1.3277e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 6s - loss: 8.2348 - mse: 8.2348 - mae: 1.2597 - val_loss: 13.5412 - val_mse: 13.5412 - val_mae: 1.2834 - lr: 1.3277e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 7s - loss: 7.8877 - mse: 7.8877 - mae: 1.2458 - val_loss: 13.6738 - val_mse: 13.6738 - val_mae: 1.3346 - lr: 1.3277e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 6s - loss: 7.8414 - mse: 7.8414 - mae: 1.2450 - val_loss: 13.5412 - val_mse: 13.5412 - val_mae: 1.2841 - lr: 1.3277e-04 - 6s/epoch - 6ms/step\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-10-12 09:39:04,160]\u001b[0m Finished trial#33 resulted in value: 10.74. Current best value is 8.826 with parameters: {'activation': 'tanh', 'num_hidden_layer': 4, 'i': 10, 'learning_rate': 0.00023399735711621326}.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Score for fold 5: loss of 13.5411958694458\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 7s - loss: 11.2429 - mse: 11.2429 - mae: 1.5444 - val_loss: 21.1291 - val_mse: 21.1291 - val_mae: 1.5256 - lr: 1.4097e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 6s - loss: 10.3255 - mse: 10.3255 - mae: 1.4950 - val_loss: 21.1627 - val_mse: 21.1627 - val_mae: 1.4303 - lr: 1.4097e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 6s - loss: 9.9728 - mse: 9.9728 - mae: 1.4733 - val_loss: 21.3800 - val_mse: 21.3800 - val_mae: 1.4415 - lr: 1.4097e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 6s - loss: 9.8001 - mse: 9.8001 - mae: 1.4576 - val_loss: 20.8570 - val_mse: 20.8570 - val_mae: 1.5479 - lr: 1.4097e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 6s - loss: 9.7090 - mse: 9.7090 - mae: 1.4492 - val_loss: 21.0201 - val_mse: 21.0201 - val_mae: 1.4746 - lr: 1.4097e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 6s - loss: 9.6971 - mse: 9.6971 - mae: 1.4455 - val_loss: 21.2189 - val_mse: 21.2189 - val_mae: 1.4558 - lr: 1.4097e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 7s - loss: 9.6004 - mse: 9.6004 - mae: 1.4356 - val_loss: 21.2880 - val_mse: 21.2880 - val_mae: 1.4846 - lr: 1.4097e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 6s - loss: 9.4851 - mse: 9.4851 - mae: 1.4316 - val_loss: 21.2397 - val_mse: 21.2397 - val_mae: 1.4539 - lr: 1.4097e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 7s - loss: 9.3243 - mse: 9.3243 - mae: 1.4229 - val_loss: 20.8663 - val_mse: 20.8663 - val_mae: 1.4739 - lr: 1.4097e-04 - 7s/epoch - 7ms/step\n",
            "Score for fold 1: loss of 20.86630630493164\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 6s - loss: 12.1108 - mse: 12.1108 - mae: 1.4357 - val_loss: 9.1434 - val_mse: 9.1434 - val_mae: 1.3966 - lr: 1.4097e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 6s - loss: 12.1851 - mse: 12.1851 - mae: 1.4254 - val_loss: 9.3438 - val_mse: 9.3438 - val_mae: 1.3609 - lr: 1.4097e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 7s - loss: 12.1851 - mse: 12.1851 - mae: 1.4249 - val_loss: 9.3092 - val_mse: 9.3092 - val_mae: 1.3904 - lr: 1.4097e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 7s - loss: 11.8643 - mse: 11.8643 - mae: 1.4181 - val_loss: 9.2683 - val_mse: 9.2683 - val_mae: 1.3842 - lr: 1.4097e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 7s - loss: 11.9310 - mse: 11.9310 - mae: 1.4132 - val_loss: 9.3704 - val_mse: 9.3704 - val_mae: 1.4087 - lr: 1.4097e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 7s - loss: 11.6838 - mse: 11.6838 - mae: 1.4036 - val_loss: 9.5669 - val_mse: 9.5669 - val_mae: 1.4338 - lr: 1.4097e-04 - 7s/epoch - 7ms/step\n",
            "Score for fold 2: loss of 9.56692886352539\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 7s - loss: 11.2404 - mse: 11.2404 - mae: 1.4005 - val_loss: 11.4247 - val_mse: 11.4247 - val_mae: 1.3346 - lr: 1.4097e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 7s - loss: 11.2117 - mse: 11.2117 - mae: 1.3934 - val_loss: 10.9904 - val_mse: 10.9904 - val_mae: 1.4496 - lr: 1.4097e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 8s - loss: 11.0744 - mse: 11.0744 - mae: 1.3884 - val_loss: 11.2845 - val_mse: 11.2845 - val_mae: 1.4259 - lr: 1.4097e-04 - 8s/epoch - 8ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 7s - loss: 10.8307 - mse: 10.8307 - mae: 1.3767 - val_loss: 11.1408 - val_mse: 11.1408 - val_mae: 1.4191 - lr: 1.4097e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 7s - loss: 10.6434 - mse: 10.6434 - mae: 1.3714 - val_loss: 11.3044 - val_mse: 11.3044 - val_mae: 1.4157 - lr: 1.4097e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 7s - loss: 10.7660 - mse: 10.7660 - mae: 1.3657 - val_loss: 11.3979 - val_mse: 11.3979 - val_mae: 1.4593 - lr: 1.4097e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 7s - loss: 10.5369 - mse: 10.5369 - mae: 1.3596 - val_loss: 11.2199 - val_mse: 11.2199 - val_mae: 1.4149 - lr: 1.4097e-04 - 7s/epoch - 7ms/step\n",
            "Score for fold 3: loss of 11.219874382019043\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 7s - loss: 11.4534 - mse: 11.4534 - mae: 1.3831 - val_loss: 7.5248 - val_mse: 7.5248 - val_mae: 1.3098 - lr: 1.4097e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 7s - loss: 11.2530 - mse: 11.2530 - mae: 1.3742 - val_loss: 7.8730 - val_mse: 7.8730 - val_mae: 1.3060 - lr: 1.4097e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 7s - loss: 11.1747 - mse: 11.1747 - mae: 1.3653 - val_loss: 8.0979 - val_mse: 8.0979 - val_mae: 1.3460 - lr: 1.4097e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 7s - loss: 10.9732 - mse: 10.9732 - mae: 1.3591 - val_loss: 7.9157 - val_mse: 7.9157 - val_mae: 1.3631 - lr: 1.4097e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 7s - loss: 10.9828 - mse: 10.9828 - mae: 1.3552 - val_loss: 7.9442 - val_mse: 7.9442 - val_mae: 1.3584 - lr: 1.4097e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 7s - loss: 10.8838 - mse: 10.8838 - mae: 1.3429 - val_loss: 7.9996 - val_mse: 7.9996 - val_mae: 1.2999 - lr: 1.4097e-04 - 7s/epoch - 7ms/step\n",
            "Score for fold 4: loss of 7.999599933624268\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 7s - loss: 11.1666 - mse: 11.1666 - mae: 1.3480 - val_loss: 5.7956 - val_mse: 5.7956 - val_mae: 1.3003 - lr: 1.4097e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 7s - loss: 11.1033 - mse: 11.1033 - mae: 1.3389 - val_loss: 5.9485 - val_mse: 5.9485 - val_mae: 1.3064 - lr: 1.4097e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 7s - loss: 11.2471 - mse: 11.2471 - mae: 1.3251 - val_loss: 6.3919 - val_mse: 6.3919 - val_mae: 1.4258 - lr: 1.4097e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 6s - loss: 10.6599 - mse: 10.6599 - mae: 1.3199 - val_loss: 6.3655 - val_mse: 6.3655 - val_mae: 1.3885 - lr: 1.4097e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 7s - loss: 10.9071 - mse: 10.9071 - mae: 1.3105 - val_loss: 6.3343 - val_mse: 6.3343 - val_mae: 1.3738 - lr: 1.4097e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 7s - loss: 10.6709 - mse: 10.6709 - mae: 1.3028 - val_loss: 6.3707 - val_mse: 6.3707 - val_mae: 1.3534 - lr: 1.4097e-04 - 7s/epoch - 7ms/step\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-10-12 09:43:00,359]\u001b[0m Finished trial#34 resulted in value: 11.206. Current best value is 8.826 with parameters: {'activation': 'tanh', 'num_hidden_layer': 4, 'i': 10, 'learning_rate': 0.00023399735711621326}.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Score for fold 5: loss of 6.370693683624268\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 13.5189 - mse: 13.5189 - mae: 1.5519 - val_loss: 12.9578 - val_mse: 12.9578 - val_mae: 1.5405 - lr: 3.8508e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 12.6901 - mse: 12.6901 - mae: 1.4936 - val_loss: 12.8363 - val_mse: 12.8363 - val_mae: 1.5109 - lr: 3.8508e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 12.4604 - mse: 12.4604 - mae: 1.4825 - val_loss: 12.7243 - val_mse: 12.7243 - val_mae: 1.5311 - lr: 3.8508e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 12.3753 - mse: 12.3753 - mae: 1.4745 - val_loss: 12.3122 - val_mse: 12.3122 - val_mae: 1.5435 - lr: 3.8508e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 12.1838 - mse: 12.1838 - mae: 1.4626 - val_loss: 12.1343 - val_mse: 12.1343 - val_mae: 1.4859 - lr: 3.8508e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 12.1003 - mse: 12.1003 - mae: 1.4618 - val_loss: 12.0997 - val_mse: 12.0997 - val_mae: 1.4638 - lr: 3.8508e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 12.0676 - mse: 12.0676 - mae: 1.4537 - val_loss: 12.0568 - val_mse: 12.0568 - val_mae: 1.5213 - lr: 3.8508e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 2s - loss: 12.0280 - mse: 12.0280 - mae: 1.4521 - val_loss: 11.8985 - val_mse: 11.8985 - val_mae: 1.4476 - lr: 3.8508e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 2s - loss: 11.9631 - mse: 11.9631 - mae: 1.4487 - val_loss: 11.9917 - val_mse: 11.9917 - val_mae: 1.5184 - lr: 3.8508e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 2s - loss: 11.9199 - mse: 11.9199 - mae: 1.4467 - val_loss: 11.8541 - val_mse: 11.8541 - val_mae: 1.5095 - lr: 3.8508e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 2s - loss: 11.8820 - mse: 11.8820 - mae: 1.4415 - val_loss: 11.8018 - val_mse: 11.8018 - val_mae: 1.4551 - lr: 3.8508e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 2s - loss: 11.8550 - mse: 11.8550 - mae: 1.4389 - val_loss: 11.8349 - val_mse: 11.8349 - val_mae: 1.4471 - lr: 3.8508e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 13/100\n",
            "1000/1000 - 2s - loss: 11.8169 - mse: 11.8169 - mae: 1.4395 - val_loss: 11.7859 - val_mse: 11.7859 - val_mae: 1.4937 - lr: 3.8508e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 14/100\n",
            "1000/1000 - 2s - loss: 11.8638 - mse: 11.8638 - mae: 1.4372 - val_loss: 11.6373 - val_mse: 11.6373 - val_mae: 1.4728 - lr: 3.8508e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 15/100\n",
            "1000/1000 - 2s - loss: 11.7695 - mse: 11.7695 - mae: 1.4364 - val_loss: 11.7603 - val_mse: 11.7603 - val_mae: 1.5001 - lr: 3.8508e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 16/100\n",
            "1000/1000 - 2s - loss: 11.6808 - mse: 11.6808 - mae: 1.4320 - val_loss: 11.7189 - val_mse: 11.7189 - val_mae: 1.4611 - lr: 3.8508e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 17/100\n",
            "1000/1000 - 2s - loss: 11.7652 - mse: 11.7652 - mae: 1.4334 - val_loss: 11.6702 - val_mse: 11.6702 - val_mae: 1.4419 - lr: 3.8508e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 18/100\n",
            "1000/1000 - 2s - loss: 11.7381 - mse: 11.7381 - mae: 1.4320 - val_loss: 11.7026 - val_mse: 11.7026 - val_mae: 1.4436 - lr: 3.8508e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 19/100\n",
            "1000/1000 - 2s - loss: 11.5977 - mse: 11.5977 - mae: 1.4285 - val_loss: 11.7157 - val_mse: 11.7157 - val_mae: 1.4633 - lr: 3.8508e-04 - 2s/epoch - 2ms/step\n",
            "Score for fold 1: loss of 11.715699195861816\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 12.1346 - mse: 12.1346 - mae: 1.4420 - val_loss: 10.1309 - val_mse: 10.1309 - val_mae: 1.3993 - lr: 3.8508e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 12.1548 - mse: 12.1548 - mae: 1.4422 - val_loss: 9.6972 - val_mse: 9.6972 - val_mae: 1.3935 - lr: 3.8508e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 12.0838 - mse: 12.0838 - mae: 1.4384 - val_loss: 9.8772 - val_mse: 9.8772 - val_mae: 1.4551 - lr: 3.8508e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 12.0841 - mse: 12.0841 - mae: 1.4388 - val_loss: 9.8550 - val_mse: 9.8550 - val_mae: 1.4354 - lr: 3.8508e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 12.0547 - mse: 12.0547 - mae: 1.4360 - val_loss: 9.9999 - val_mse: 9.9999 - val_mae: 1.4708 - lr: 3.8508e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 12.0488 - mse: 12.0488 - mae: 1.4376 - val_loss: 9.8723 - val_mse: 9.8723 - val_mae: 1.3867 - lr: 3.8508e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 11.9844 - mse: 11.9844 - mae: 1.4324 - val_loss: 10.1835 - val_mse: 10.1835 - val_mae: 1.3965 - lr: 3.8508e-04 - 2s/epoch - 2ms/step\n",
            "Score for fold 2: loss of 10.183473587036133\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 11.3315 - mse: 11.3315 - mae: 1.4301 - val_loss: 12.4680 - val_mse: 12.4680 - val_mae: 1.4239 - lr: 3.8508e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 11.2878 - mse: 11.2878 - mae: 1.4299 - val_loss: 12.3996 - val_mse: 12.3996 - val_mae: 1.4349 - lr: 3.8508e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 11.2647 - mse: 11.2647 - mae: 1.4202 - val_loss: 12.3997 - val_mse: 12.3997 - val_mae: 1.4489 - lr: 3.8508e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 11.2114 - mse: 11.2114 - mae: 1.4239 - val_loss: 12.4269 - val_mse: 12.4269 - val_mae: 1.4626 - lr: 3.8508e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 11.2250 - mse: 11.2250 - mae: 1.4234 - val_loss: 12.4699 - val_mse: 12.4699 - val_mae: 1.4997 - lr: 3.8508e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 11.1780 - mse: 11.1780 - mae: 1.4208 - val_loss: 12.4926 - val_mse: 12.4926 - val_mae: 1.5061 - lr: 3.8508e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 11.2004 - mse: 11.2004 - mae: 1.4192 - val_loss: 12.4630 - val_mse: 12.4630 - val_mae: 1.4601 - lr: 3.8508e-04 - 2s/epoch - 2ms/step\n",
            "Score for fold 3: loss of 12.462958335876465\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 12.3860 - mse: 12.3860 - mae: 1.4276 - val_loss: 7.7929 - val_mse: 7.7929 - val_mae: 1.3902 - lr: 3.8508e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 12.3034 - mse: 12.3034 - mae: 1.4259 - val_loss: 7.8270 - val_mse: 7.8270 - val_mae: 1.3960 - lr: 3.8508e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 12.2379 - mse: 12.2379 - mae: 1.4223 - val_loss: 7.9078 - val_mse: 7.9078 - val_mae: 1.3915 - lr: 3.8508e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 12.3254 - mse: 12.3254 - mae: 1.4242 - val_loss: 7.7788 - val_mse: 7.7788 - val_mae: 1.3974 - lr: 3.8508e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 12.2794 - mse: 12.2794 - mae: 1.4192 - val_loss: 7.8424 - val_mse: 7.8424 - val_mae: 1.4211 - lr: 3.8508e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 12.3233 - mse: 12.3233 - mae: 1.4221 - val_loss: 7.8778 - val_mse: 7.8778 - val_mae: 1.3817 - lr: 3.8508e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 12.3125 - mse: 12.3125 - mae: 1.4196 - val_loss: 7.9002 - val_mse: 7.9002 - val_mae: 1.4455 - lr: 3.8508e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 2s - loss: 12.2132 - mse: 12.2132 - mae: 1.4193 - val_loss: 8.0586 - val_mse: 8.0586 - val_mae: 1.4080 - lr: 3.8508e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 2s - loss: 12.2634 - mse: 12.2634 - mae: 1.4142 - val_loss: 7.8299 - val_mse: 7.8299 - val_mae: 1.4134 - lr: 3.8508e-04 - 2s/epoch - 2ms/step\n",
            "Score for fold 4: loss of 7.8298821449279785\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 10.1680 - mse: 10.1680 - mae: 1.4237 - val_loss: 16.0750 - val_mse: 16.0750 - val_mae: 1.4339 - lr: 3.8508e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 10.1811 - mse: 10.1811 - mae: 1.4223 - val_loss: 16.0535 - val_mse: 16.0535 - val_mae: 1.3815 - lr: 3.8508e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 10.1884 - mse: 10.1884 - mae: 1.4187 - val_loss: 16.0008 - val_mse: 16.0008 - val_mae: 1.4108 - lr: 3.8508e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 10.1134 - mse: 10.1134 - mae: 1.4157 - val_loss: 16.0553 - val_mse: 16.0553 - val_mae: 1.4208 - lr: 3.8508e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 10.1186 - mse: 10.1186 - mae: 1.4148 - val_loss: 16.0506 - val_mse: 16.0506 - val_mae: 1.4383 - lr: 3.8508e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 10.1170 - mse: 10.1170 - mae: 1.4188 - val_loss: 16.0891 - val_mse: 16.0891 - val_mae: 1.3930 - lr: 3.8508e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 10.0841 - mse: 10.0841 - mae: 1.4146 - val_loss: 16.1805 - val_mse: 16.1805 - val_mae: 1.4357 - lr: 3.8508e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 2s - loss: 10.0882 - mse: 10.0882 - mae: 1.4109 - val_loss: 16.1863 - val_mse: 16.1863 - val_mae: 1.3910 - lr: 3.8508e-04 - 2s/epoch - 2ms/step\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-10-12 09:44:39,335]\u001b[0m Finished trial#35 resulted in value: 11.675999999999998. Current best value is 8.826 with parameters: {'activation': 'tanh', 'num_hidden_layer': 4, 'i': 10, 'learning_rate': 0.00023399735711621326}.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Score for fold 5: loss of 16.18634605407715\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 3s - loss: 13.5886 - mse: 13.5886 - mae: 1.5562 - val_loss: 12.0512 - val_mse: 12.0512 - val_mae: 1.4775 - lr: 2.5226e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 12.7769 - mse: 12.7769 - mae: 1.5022 - val_loss: 11.6348 - val_mse: 11.6348 - val_mae: 1.4861 - lr: 2.5226e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 3s - loss: 12.5121 - mse: 12.5121 - mae: 1.4856 - val_loss: 11.6110 - val_mse: 11.6110 - val_mae: 1.4697 - lr: 2.5226e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 3s - loss: 12.3055 - mse: 12.3055 - mae: 1.4748 - val_loss: 11.4170 - val_mse: 11.4170 - val_mae: 1.5169 - lr: 2.5226e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 3s - loss: 12.1547 - mse: 12.1547 - mae: 1.4616 - val_loss: 11.3510 - val_mse: 11.3510 - val_mae: 1.4727 - lr: 2.5226e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 3s - loss: 12.0746 - mse: 12.0746 - mae: 1.4591 - val_loss: 11.2509 - val_mse: 11.2509 - val_mae: 1.4844 - lr: 2.5226e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 3s - loss: 12.0140 - mse: 12.0140 - mae: 1.4501 - val_loss: 11.2912 - val_mse: 11.2912 - val_mae: 1.5283 - lr: 2.5226e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 3s - loss: 11.9499 - mse: 11.9499 - mae: 1.4449 - val_loss: 11.1693 - val_mse: 11.1693 - val_mae: 1.4239 - lr: 2.5226e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 3s - loss: 11.9027 - mse: 11.9027 - mae: 1.4430 - val_loss: 11.1756 - val_mse: 11.1756 - val_mae: 1.5088 - lr: 2.5226e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 3s - loss: 11.8867 - mse: 11.8867 - mae: 1.4386 - val_loss: 11.2758 - val_mse: 11.2758 - val_mae: 1.4778 - lr: 2.5226e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 3s - loss: 11.8003 - mse: 11.8003 - mae: 1.4357 - val_loss: 11.2136 - val_mse: 11.2136 - val_mae: 1.4906 - lr: 2.5226e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 3s - loss: 11.8983 - mse: 11.8983 - mae: 1.4309 - val_loss: 11.1590 - val_mse: 11.1590 - val_mae: 1.4437 - lr: 2.5226e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 13/100\n",
            "1000/1000 - 3s - loss: 11.7378 - mse: 11.7378 - mae: 1.4270 - val_loss: 11.2266 - val_mse: 11.2266 - val_mae: 1.4605 - lr: 2.5226e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 14/100\n",
            "1000/1000 - 3s - loss: 11.6656 - mse: 11.6656 - mae: 1.4236 - val_loss: 11.1674 - val_mse: 11.1674 - val_mae: 1.4335 - lr: 2.5226e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 15/100\n",
            "1000/1000 - 3s - loss: 11.6388 - mse: 11.6388 - mae: 1.4176 - val_loss: 11.1267 - val_mse: 11.1267 - val_mae: 1.4998 - lr: 2.5226e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 16/100\n",
            "1000/1000 - 3s - loss: 11.5850 - mse: 11.5850 - mae: 1.4175 - val_loss: 11.1639 - val_mse: 11.1639 - val_mae: 1.4492 - lr: 2.5226e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 17/100\n",
            "1000/1000 - 3s - loss: 11.6276 - mse: 11.6276 - mae: 1.4159 - val_loss: 11.0822 - val_mse: 11.0822 - val_mae: 1.4312 - lr: 2.5226e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 18/100\n",
            "1000/1000 - 3s - loss: 11.6242 - mse: 11.6242 - mae: 1.4120 - val_loss: 11.0729 - val_mse: 11.0729 - val_mae: 1.4174 - lr: 2.5226e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 19/100\n",
            "1000/1000 - 3s - loss: 11.4110 - mse: 11.4110 - mae: 1.4077 - val_loss: 11.1310 - val_mse: 11.1310 - val_mae: 1.5420 - lr: 2.5226e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 20/100\n",
            "1000/1000 - 3s - loss: 11.4393 - mse: 11.4393 - mae: 1.4025 - val_loss: 11.1499 - val_mse: 11.1499 - val_mae: 1.4866 - lr: 2.5226e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 21/100\n",
            "1000/1000 - 3s - loss: 11.3860 - mse: 11.3860 - mae: 1.4016 - val_loss: 11.0594 - val_mse: 11.0594 - val_mae: 1.4293 - lr: 2.5226e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 22/100\n",
            "1000/1000 - 3s - loss: 11.2266 - mse: 11.2266 - mae: 1.3993 - val_loss: 11.1058 - val_mse: 11.1058 - val_mae: 1.4565 - lr: 2.5226e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 23/100\n",
            "1000/1000 - 3s - loss: 11.1678 - mse: 11.1678 - mae: 1.3958 - val_loss: 10.9944 - val_mse: 10.9944 - val_mae: 1.4166 - lr: 2.5226e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 24/100\n",
            "1000/1000 - 3s - loss: 11.1100 - mse: 11.1100 - mae: 1.3898 - val_loss: 11.0968 - val_mse: 11.0968 - val_mae: 1.4656 - lr: 2.5226e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 25/100\n",
            "1000/1000 - 3s - loss: 11.0732 - mse: 11.0732 - mae: 1.3883 - val_loss: 11.0444 - val_mse: 11.0444 - val_mae: 1.4165 - lr: 2.5226e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 26/100\n",
            "1000/1000 - 3s - loss: 10.9609 - mse: 10.9609 - mae: 1.3810 - val_loss: 11.0416 - val_mse: 11.0416 - val_mae: 1.4265 - lr: 2.5226e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 27/100\n",
            "1000/1000 - 3s - loss: 11.0232 - mse: 11.0232 - mae: 1.3847 - val_loss: 11.0542 - val_mse: 11.0542 - val_mae: 1.4607 - lr: 2.5226e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 28/100\n",
            "1000/1000 - 3s - loss: 10.9260 - mse: 10.9260 - mae: 1.3781 - val_loss: 10.9966 - val_mse: 10.9966 - val_mae: 1.5029 - lr: 2.5226e-04 - 3s/epoch - 3ms/step\n",
            "Score for fold 1: loss of 10.99658203125\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 3s - loss: 11.6389 - mse: 11.6389 - mae: 1.3969 - val_loss: 9.1462 - val_mse: 9.1462 - val_mae: 1.4253 - lr: 2.5226e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 3s - loss: 11.4326 - mse: 11.4326 - mae: 1.3904 - val_loss: 8.6917 - val_mse: 8.6917 - val_mae: 1.3213 - lr: 2.5226e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 3s - loss: 11.3753 - mse: 11.3753 - mae: 1.3858 - val_loss: 8.6074 - val_mse: 8.6074 - val_mae: 1.3641 - lr: 2.5226e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 3s - loss: 11.2437 - mse: 11.2437 - mae: 1.3812 - val_loss: 8.8437 - val_mse: 8.8437 - val_mae: 1.3400 - lr: 2.5226e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 3s - loss: 11.2052 - mse: 11.2052 - mae: 1.3746 - val_loss: 9.0245 - val_mse: 9.0245 - val_mae: 1.4051 - lr: 2.5226e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 3s - loss: 11.1118 - mse: 11.1118 - mae: 1.3721 - val_loss: 8.9628 - val_mse: 8.9628 - val_mae: 1.3475 - lr: 2.5226e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 3s - loss: 11.1146 - mse: 11.1146 - mae: 1.3685 - val_loss: 8.9001 - val_mse: 8.9001 - val_mae: 1.4221 - lr: 2.5226e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 3s - loss: 11.0094 - mse: 11.0094 - mae: 1.3635 - val_loss: 9.3354 - val_mse: 9.3354 - val_mae: 1.3954 - lr: 2.5226e-04 - 3s/epoch - 3ms/step\n",
            "Score for fold 2: loss of 9.335393905639648\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 3s - loss: 9.4138 - mse: 9.4138 - mae: 1.3746 - val_loss: 15.2912 - val_mse: 15.2912 - val_mae: 1.3024 - lr: 2.5226e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 3s - loss: 9.3745 - mse: 9.3745 - mae: 1.3681 - val_loss: 15.3639 - val_mse: 15.3639 - val_mae: 1.4157 - lr: 2.5226e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 3s - loss: 9.2458 - mse: 9.2458 - mae: 1.3586 - val_loss: 15.6965 - val_mse: 15.6965 - val_mae: 1.3663 - lr: 2.5226e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 3s - loss: 9.1243 - mse: 9.1243 - mae: 1.3553 - val_loss: 15.5470 - val_mse: 15.5470 - val_mae: 1.3852 - lr: 2.5226e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 3s - loss: 9.0857 - mse: 9.0857 - mae: 1.3516 - val_loss: 15.5299 - val_mse: 15.5299 - val_mae: 1.3818 - lr: 2.5226e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 3s - loss: 9.0831 - mse: 9.0831 - mae: 1.3504 - val_loss: 16.0927 - val_mse: 16.0927 - val_mae: 1.3899 - lr: 2.5226e-04 - 3s/epoch - 3ms/step\n",
            "Score for fold 3: loss of 16.092727661132812\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 3s - loss: 10.7730 - mse: 10.7730 - mae: 1.3586 - val_loss: 8.3848 - val_mse: 8.3848 - val_mae: 1.3152 - lr: 2.5226e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 3s - loss: 10.6273 - mse: 10.6273 - mae: 1.3533 - val_loss: 8.4990 - val_mse: 8.4990 - val_mae: 1.3406 - lr: 2.5226e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 3s - loss: 10.5642 - mse: 10.5642 - mae: 1.3445 - val_loss: 8.3996 - val_mse: 8.3996 - val_mae: 1.3647 - lr: 2.5226e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 3s - loss: 10.5748 - mse: 10.5748 - mae: 1.3419 - val_loss: 8.8124 - val_mse: 8.8124 - val_mae: 1.3138 - lr: 2.5226e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 3s - loss: 10.4170 - mse: 10.4170 - mae: 1.3369 - val_loss: 8.6600 - val_mse: 8.6600 - val_mae: 1.3482 - lr: 2.5226e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 3s - loss: 10.3141 - mse: 10.3141 - mae: 1.3278 - val_loss: 8.7376 - val_mse: 8.7376 - val_mae: 1.3974 - lr: 2.5226e-04 - 3s/epoch - 3ms/step\n",
            "Score for fold 4: loss of 8.737651824951172\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 3s - loss: 10.1778 - mse: 10.1778 - mae: 1.3445 - val_loss: 9.1618 - val_mse: 9.1618 - val_mae: 1.2942 - lr: 2.5226e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 3s - loss: 10.0827 - mse: 10.0827 - mae: 1.3303 - val_loss: 9.5416 - val_mse: 9.5416 - val_mae: 1.3064 - lr: 2.5226e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 3s - loss: 10.0285 - mse: 10.0285 - mae: 1.3317 - val_loss: 9.5334 - val_mse: 9.5334 - val_mae: 1.3393 - lr: 2.5226e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 3s - loss: 10.0045 - mse: 10.0045 - mae: 1.3248 - val_loss: 9.3950 - val_mse: 9.3950 - val_mae: 1.3408 - lr: 2.5226e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 3s - loss: 9.8312 - mse: 9.8312 - mae: 1.3183 - val_loss: 9.2957 - val_mse: 9.2957 - val_mae: 1.3633 - lr: 2.5226e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 3s - loss: 9.8119 - mse: 9.8119 - mae: 1.3147 - val_loss: 9.5061 - val_mse: 9.5061 - val_mae: 1.3230 - lr: 2.5226e-04 - 3s/epoch - 3ms/step\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-10-12 09:47:18,816]\u001b[0m Finished trial#36 resulted in value: 10.936. Current best value is 8.826 with parameters: {'activation': 'tanh', 'num_hidden_layer': 4, 'i': 10, 'learning_rate': 0.00023399735711621326}.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Score for fold 5: loss of 9.506067276000977\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 7s - loss: 14.4516 - mse: 14.4516 - mae: 1.6536 - val_loss: 13.1677 - val_mse: 13.1677 - val_mae: 1.6177 - lr: 1.4758e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 6s - loss: 13.7266 - mse: 13.7266 - mae: 1.5753 - val_loss: 13.1381 - val_mse: 13.1381 - val_mae: 1.6049 - lr: 1.4758e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 6s - loss: 13.5504 - mse: 13.5504 - mae: 1.5646 - val_loss: 12.9061 - val_mse: 12.9061 - val_mae: 1.5421 - lr: 1.4758e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 6s - loss: 13.5476 - mse: 13.5476 - mae: 1.5597 - val_loss: 12.9548 - val_mse: 12.9548 - val_mae: 1.5457 - lr: 1.4758e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 6s - loss: 13.5897 - mse: 13.5897 - mae: 1.5574 - val_loss: 13.1023 - val_mse: 13.1023 - val_mae: 1.5144 - lr: 1.4758e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 6s - loss: 13.4298 - mse: 13.4298 - mae: 1.5527 - val_loss: 12.7984 - val_mse: 12.7984 - val_mae: 1.5553 - lr: 1.4758e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 6s - loss: 13.4783 - mse: 13.4783 - mae: 1.5550 - val_loss: 12.9006 - val_mse: 12.9006 - val_mae: 1.5118 - lr: 1.4758e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 6s - loss: 13.5019 - mse: 13.5019 - mae: 1.5534 - val_loss: 12.8117 - val_mse: 12.8117 - val_mae: 1.5492 - lr: 1.4758e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 6s - loss: 13.3607 - mse: 13.3607 - mae: 1.5520 - val_loss: 12.8213 - val_mse: 12.8213 - val_mae: 1.5397 - lr: 1.4758e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 6s - loss: 13.4017 - mse: 13.4017 - mae: 1.5499 - val_loss: 12.7862 - val_mse: 12.7862 - val_mae: 1.5704 - lr: 1.4758e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 6s - loss: 13.4027 - mse: 13.4027 - mae: 1.5508 - val_loss: 12.7562 - val_mse: 12.7562 - val_mae: 1.5323 - lr: 1.4758e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 6s - loss: 13.3338 - mse: 13.3338 - mae: 1.5468 - val_loss: 12.7515 - val_mse: 12.7515 - val_mae: 1.5306 - lr: 1.4758e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 13/100\n",
            "1000/1000 - 6s - loss: 13.4026 - mse: 13.4026 - mae: 1.5503 - val_loss: 12.8425 - val_mse: 12.8425 - val_mae: 1.5562 - lr: 1.4758e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 14/100\n",
            "1000/1000 - 6s - loss: 13.3942 - mse: 13.3942 - mae: 1.5507 - val_loss: 12.8805 - val_mse: 12.8805 - val_mae: 1.5097 - lr: 1.4758e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 15/100\n",
            "1000/1000 - 6s - loss: 13.3847 - mse: 13.3847 - mae: 1.5495 - val_loss: 12.8196 - val_mse: 12.8196 - val_mae: 1.5462 - lr: 1.4758e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 16/100\n",
            "1000/1000 - 6s - loss: 13.4124 - mse: 13.4124 - mae: 1.5461 - val_loss: 12.7995 - val_mse: 12.7995 - val_mae: 1.5201 - lr: 1.4758e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 17/100\n",
            "1000/1000 - 6s - loss: 13.3738 - mse: 13.3738 - mae: 1.5466 - val_loss: 12.8323 - val_mse: 12.8323 - val_mae: 1.5321 - lr: 1.4758e-04 - 6s/epoch - 6ms/step\n",
            "Score for fold 1: loss of 12.832274436950684\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 6s - loss: 13.9894 - mse: 13.9894 - mae: 1.5479 - val_loss: 10.4023 - val_mse: 10.4023 - val_mae: 1.5576 - lr: 1.4758e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 6s - loss: 13.9624 - mse: 13.9624 - mae: 1.5445 - val_loss: 10.5454 - val_mse: 10.5454 - val_mae: 1.5936 - lr: 1.4758e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 6s - loss: 13.9388 - mse: 13.9388 - mae: 1.5445 - val_loss: 10.6040 - val_mse: 10.6040 - val_mae: 1.5653 - lr: 1.4758e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 6s - loss: 14.0249 - mse: 14.0249 - mae: 1.5480 - val_loss: 10.5293 - val_mse: 10.5293 - val_mae: 1.5371 - lr: 1.4758e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 5s - loss: 13.9698 - mse: 13.9698 - mae: 1.5442 - val_loss: 10.4817 - val_mse: 10.4817 - val_mae: 1.5380 - lr: 1.4758e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 6s - loss: 13.9305 - mse: 13.9305 - mae: 1.5430 - val_loss: 10.6000 - val_mse: 10.6000 - val_mae: 1.5493 - lr: 1.4758e-04 - 6s/epoch - 6ms/step\n",
            "Score for fold 2: loss of 10.600028991699219\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 6s - loss: 13.6660 - mse: 13.6660 - mae: 1.5532 - val_loss: 11.5453 - val_mse: 11.5453 - val_mae: 1.5339 - lr: 1.4758e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 6s - loss: 13.6046 - mse: 13.6046 - mae: 1.5494 - val_loss: 11.4362 - val_mse: 11.4362 - val_mae: 1.5033 - lr: 1.4758e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 6s - loss: 13.6612 - mse: 13.6612 - mae: 1.5486 - val_loss: 11.5216 - val_mse: 11.5216 - val_mae: 1.5385 - lr: 1.4758e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 6s - loss: 13.5883 - mse: 13.5883 - mae: 1.5486 - val_loss: 11.4572 - val_mse: 11.4572 - val_mae: 1.5077 - lr: 1.4758e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 6s - loss: 13.6179 - mse: 13.6179 - mae: 1.5512 - val_loss: 11.6265 - val_mse: 11.6265 - val_mae: 1.5585 - lr: 1.4758e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 6s - loss: 13.6960 - mse: 13.6960 - mae: 1.5498 - val_loss: 11.5219 - val_mse: 11.5219 - val_mae: 1.5506 - lr: 1.4758e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 6s - loss: 13.7124 - mse: 13.7124 - mae: 1.5497 - val_loss: 11.4929 - val_mse: 11.4929 - val_mae: 1.4916 - lr: 1.4758e-04 - 6s/epoch - 6ms/step\n",
            "Score for fold 3: loss of 11.492915153503418\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 6s - loss: 12.0605 - mse: 12.0605 - mae: 1.5459 - val_loss: 17.7391 - val_mse: 17.7391 - val_mae: 1.5503 - lr: 1.4758e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 6s - loss: 12.0155 - mse: 12.0155 - mae: 1.5494 - val_loss: 17.8186 - val_mse: 17.8186 - val_mae: 1.5436 - lr: 1.4758e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 6s - loss: 12.0996 - mse: 12.0996 - mae: 1.5483 - val_loss: 17.8199 - val_mse: 17.8199 - val_mae: 1.5042 - lr: 1.4758e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 5s - loss: 12.1053 - mse: 12.1053 - mae: 1.5489 - val_loss: 17.7416 - val_mse: 17.7416 - val_mae: 1.5335 - lr: 1.4758e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 5s - loss: 12.1284 - mse: 12.1284 - mae: 1.5522 - val_loss: 17.7152 - val_mse: 17.7152 - val_mae: 1.5684 - lr: 1.4758e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 6s - loss: 12.1880 - mse: 12.1880 - mae: 1.5480 - val_loss: 17.8090 - val_mse: 17.8090 - val_mae: 1.5033 - lr: 1.4758e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 6s - loss: 12.1659 - mse: 12.1659 - mae: 1.5504 - val_loss: 17.7277 - val_mse: 17.7277 - val_mae: 1.5574 - lr: 1.4758e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 6s - loss: 12.0890 - mse: 12.0890 - mae: 1.5509 - val_loss: 17.7371 - val_mse: 17.7371 - val_mae: 1.5371 - lr: 1.4758e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 6s - loss: 12.0934 - mse: 12.0934 - mae: 1.5567 - val_loss: 17.9828 - val_mse: 17.9828 - val_mae: 1.4626 - lr: 1.4758e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 6s - loss: 12.1554 - mse: 12.1554 - mae: 1.5496 - val_loss: 17.7647 - val_mse: 17.7647 - val_mae: 1.5523 - lr: 1.4758e-04 - 6s/epoch - 6ms/step\n",
            "Score for fold 4: loss of 17.764741897583008\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 6s - loss: 13.1276 - mse: 13.1276 - mae: 1.5405 - val_loss: 13.5755 - val_mse: 13.5755 - val_mae: 1.5696 - lr: 1.4758e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 6s - loss: 13.1866 - mse: 13.1866 - mae: 1.5444 - val_loss: 14.0305 - val_mse: 14.0305 - val_mae: 1.7264 - lr: 1.4758e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 6s - loss: 13.1908 - mse: 13.1908 - mae: 1.5487 - val_loss: 13.6124 - val_mse: 13.6124 - val_mae: 1.5144 - lr: 1.4758e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 6s - loss: 13.2432 - mse: 13.2432 - mae: 1.5448 - val_loss: 13.6031 - val_mse: 13.6031 - val_mae: 1.5681 - lr: 1.4758e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 5s - loss: 13.1431 - mse: 13.1431 - mae: 1.5469 - val_loss: 13.6147 - val_mse: 13.6147 - val_mae: 1.5680 - lr: 1.4758e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 6s - loss: 13.1125 - mse: 13.1125 - mae: 1.5442 - val_loss: 13.6755 - val_mse: 13.6755 - val_mae: 1.5108 - lr: 1.4758e-04 - 6s/epoch - 6ms/step\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-10-12 09:51:52,696]\u001b[0m Finished trial#37 resulted in value: 13.272000000000002. Current best value is 8.826 with parameters: {'activation': 'tanh', 'num_hidden_layer': 4, 'i': 10, 'learning_rate': 0.00023399735711621326}.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Score for fold 5: loss of 13.67546272277832\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 10s - loss: 13.1692 - mse: 13.1692 - mae: 1.5413 - val_loss: 13.2903 - val_mse: 13.2903 - val_mae: 1.4714 - lr: 1.0776e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 9s - loss: 12.4027 - mse: 12.4027 - mae: 1.4869 - val_loss: 12.6316 - val_mse: 12.6316 - val_mae: 1.4794 - lr: 1.0776e-04 - 9s/epoch - 9ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 9s - loss: 12.1930 - mse: 12.1930 - mae: 1.4705 - val_loss: 12.1757 - val_mse: 12.1757 - val_mae: 1.5443 - lr: 1.0776e-04 - 9s/epoch - 9ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 9s - loss: 12.0126 - mse: 12.0126 - mae: 1.4554 - val_loss: 12.0013 - val_mse: 12.0013 - val_mae: 1.4596 - lr: 1.0776e-04 - 9s/epoch - 9ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 9s - loss: 11.9026 - mse: 11.9026 - mae: 1.4424 - val_loss: 13.5698 - val_mse: 13.5698 - val_mae: 1.6208 - lr: 1.0776e-04 - 9s/epoch - 9ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 9s - loss: 11.7915 - mse: 11.7915 - mae: 1.4418 - val_loss: 11.9582 - val_mse: 11.9582 - val_mae: 1.4381 - lr: 1.0776e-04 - 9s/epoch - 9ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 9s - loss: 11.5491 - mse: 11.5491 - mae: 1.4270 - val_loss: 11.8340 - val_mse: 11.8340 - val_mae: 1.4886 - lr: 1.0776e-04 - 9s/epoch - 9ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 10s - loss: 11.4057 - mse: 11.4057 - mae: 1.4169 - val_loss: 11.9300 - val_mse: 11.9300 - val_mae: 1.5113 - lr: 1.0776e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 12s - loss: 11.1857 - mse: 11.1857 - mae: 1.4061 - val_loss: 12.7498 - val_mse: 12.7498 - val_mae: 1.4414 - lr: 1.0776e-04 - 12s/epoch - 12ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 13s - loss: 11.0781 - mse: 11.0781 - mae: 1.3991 - val_loss: 12.5033 - val_mse: 12.5033 - val_mae: 1.4817 - lr: 1.0776e-04 - 13s/epoch - 13ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 12s - loss: 10.8936 - mse: 10.8936 - mae: 1.3838 - val_loss: 11.7505 - val_mse: 11.7505 - val_mae: 1.4746 - lr: 1.0776e-04 - 12s/epoch - 12ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 10s - loss: 10.5865 - mse: 10.5865 - mae: 1.3674 - val_loss: 12.2519 - val_mse: 12.2519 - val_mae: 1.5001 - lr: 1.0776e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 13/100\n",
            "1000/1000 - 9s - loss: 10.3165 - mse: 10.3165 - mae: 1.3531 - val_loss: 12.4510 - val_mse: 12.4510 - val_mae: 1.5781 - lr: 1.0776e-04 - 9s/epoch - 9ms/step\n",
            "Epoch 14/100\n",
            "1000/1000 - 9s - loss: 10.1482 - mse: 10.1482 - mae: 1.3365 - val_loss: 12.0222 - val_mse: 12.0222 - val_mae: 1.5352 - lr: 1.0776e-04 - 9s/epoch - 9ms/step\n",
            "Epoch 15/100\n",
            "1000/1000 - 9s - loss: 9.9339 - mse: 9.9339 - mae: 1.3266 - val_loss: 11.8086 - val_mse: 11.8086 - val_mae: 1.4859 - lr: 1.0776e-04 - 9s/epoch - 9ms/step\n",
            "Epoch 16/100\n",
            "1000/1000 - 9s - loss: 9.4194 - mse: 9.4194 - mae: 1.3047 - val_loss: 12.2190 - val_mse: 12.2190 - val_mae: 1.4590 - lr: 1.0776e-04 - 9s/epoch - 9ms/step\n",
            "Score for fold 1: loss of 12.219001770019531\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 10s - loss: 10.8033 - mse: 10.8033 - mae: 1.3571 - val_loss: 7.4493 - val_mse: 7.4493 - val_mae: 1.3884 - lr: 1.0776e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 9s - loss: 10.4567 - mse: 10.4567 - mae: 1.3298 - val_loss: 7.7088 - val_mse: 7.7088 - val_mae: 1.3185 - lr: 1.0776e-04 - 9s/epoch - 9ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 10s - loss: 10.2801 - mse: 10.2801 - mae: 1.3120 - val_loss: 7.1539 - val_mse: 7.1539 - val_mae: 1.3314 - lr: 1.0776e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 10s - loss: 9.8976 - mse: 9.8976 - mae: 1.2942 - val_loss: 8.0769 - val_mse: 8.0769 - val_mae: 1.3443 - lr: 1.0776e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 9s - loss: 9.6927 - mse: 9.6927 - mae: 1.2721 - val_loss: 7.2811 - val_mse: 7.2811 - val_mae: 1.3106 - lr: 1.0776e-04 - 9s/epoch - 9ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 9s - loss: 9.1481 - mse: 9.1481 - mae: 1.2463 - val_loss: 7.8343 - val_mse: 7.8343 - val_mae: 1.3405 - lr: 1.0776e-04 - 9s/epoch - 9ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 10s - loss: 9.0206 - mse: 9.0206 - mae: 1.2286 - val_loss: 8.3350 - val_mse: 8.3350 - val_mae: 1.3193 - lr: 1.0776e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 9s - loss: 8.4970 - mse: 8.4970 - mae: 1.2035 - val_loss: 8.5172 - val_mse: 8.5172 - val_mae: 1.3987 - lr: 1.0776e-04 - 9s/epoch - 9ms/step\n",
            "Score for fold 2: loss of 8.517230033874512\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 9s - loss: 9.1871 - mse: 9.1871 - mae: 1.2447 - val_loss: 4.3898 - val_mse: 4.3898 - val_mae: 1.1605 - lr: 1.0776e-04 - 9s/epoch - 9ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 9s - loss: 8.8931 - mse: 8.8931 - mae: 1.2169 - val_loss: 4.5164 - val_mse: 4.5164 - val_mae: 1.1527 - lr: 1.0776e-04 - 9s/epoch - 9ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 9s - loss: 8.4432 - mse: 8.4432 - mae: 1.1906 - val_loss: 4.5106 - val_mse: 4.5106 - val_mae: 1.1913 - lr: 1.0776e-04 - 9s/epoch - 9ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 9s - loss: 8.2478 - mse: 8.2478 - mae: 1.1685 - val_loss: 4.9105 - val_mse: 4.9105 - val_mae: 1.1859 - lr: 1.0776e-04 - 9s/epoch - 9ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 9s - loss: 7.9821 - mse: 7.9821 - mae: 1.1475 - val_loss: 5.0070 - val_mse: 5.0070 - val_mae: 1.2472 - lr: 1.0776e-04 - 9s/epoch - 9ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 9s - loss: 7.7216 - mse: 7.7216 - mae: 1.1335 - val_loss: 5.1751 - val_mse: 5.1751 - val_mae: 1.1971 - lr: 1.0776e-04 - 9s/epoch - 9ms/step\n",
            "Score for fold 3: loss of 5.175097942352295\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 9s - loss: 7.4339 - mse: 7.4339 - mae: 1.1687 - val_loss: 5.6441 - val_mse: 5.6441 - val_mae: 1.1085 - lr: 1.0776e-04 - 9s/epoch - 9ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 9s - loss: 7.0212 - mse: 7.0212 - mae: 1.1347 - val_loss: 6.0559 - val_mse: 6.0559 - val_mae: 1.1865 - lr: 1.0776e-04 - 9s/epoch - 9ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 9s - loss: 6.4956 - mse: 6.4956 - mae: 1.1109 - val_loss: 7.9190 - val_mse: 7.9190 - val_mae: 1.1543 - lr: 1.0776e-04 - 9s/epoch - 9ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 9s - loss: 6.2807 - mse: 6.2807 - mae: 1.0863 - val_loss: 7.7133 - val_mse: 7.7133 - val_mae: 1.1162 - lr: 1.0776e-04 - 9s/epoch - 9ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 10s - loss: 6.1849 - mse: 6.1849 - mae: 1.0710 - val_loss: 7.1472 - val_mse: 7.1472 - val_mae: 1.1270 - lr: 1.0776e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 10s - loss: 5.6262 - mse: 5.6262 - mae: 1.0533 - val_loss: 6.3417 - val_mse: 6.3417 - val_mae: 1.1622 - lr: 1.0776e-04 - 10s/epoch - 10ms/step\n",
            "Score for fold 4: loss of 6.341660976409912\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 10s - loss: 4.8752 - mse: 4.8752 - mae: 1.0817 - val_loss: 10.3094 - val_mse: 10.3094 - val_mae: 1.0650 - lr: 1.0776e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 10s - loss: 4.5673 - mse: 4.5673 - mae: 1.0628 - val_loss: 9.8944 - val_mse: 9.8944 - val_mae: 1.0133 - lr: 1.0776e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 10s - loss: 4.5876 - mse: 4.5876 - mae: 1.0388 - val_loss: 10.5035 - val_mse: 10.5035 - val_mae: 1.0745 - lr: 1.0776e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 10s - loss: 4.3551 - mse: 4.3551 - mae: 1.0198 - val_loss: 10.0164 - val_mse: 10.0164 - val_mae: 1.0861 - lr: 1.0776e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 10s - loss: 4.0975 - mse: 4.0975 - mae: 1.0028 - val_loss: 11.4081 - val_mse: 11.4081 - val_mae: 1.0976 - lr: 1.0776e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 10s - loss: 3.8565 - mse: 3.8565 - mae: 0.9825 - val_loss: 11.3088 - val_mse: 11.3088 - val_mae: 1.0900 - lr: 1.0776e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 10s - loss: 3.8281 - mse: 3.8281 - mae: 0.9693 - val_loss: 11.0756 - val_mse: 11.0756 - val_mae: 1.0944 - lr: 1.0776e-04 - 10s/epoch - 10ms/step\n",
            "Score for fold 5: loss of 11.07559871673584\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-10-12 09:58:58,689]\u001b[0m Finished trial#38 resulted in value: 8.668000000000001. Current best value is 8.668000000000001 with parameters: {'activation': 'relu', 'num_hidden_layer': 4, 'i': 10, 'learning_rate': 0.0001077579645735244}.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 4s - loss: 12.6139 - mse: 12.6139 - mae: 1.5663 - val_loss: 19.2260 - val_mse: 19.2260 - val_mae: 1.6558 - lr: 1.0542e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 3s - loss: 11.2618 - mse: 11.2618 - mae: 1.4968 - val_loss: 18.7421 - val_mse: 18.7421 - val_mae: 1.5566 - lr: 1.0542e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 3s - loss: 10.8917 - mse: 10.8917 - mae: 1.4751 - val_loss: 18.4529 - val_mse: 18.4529 - val_mae: 1.5046 - lr: 1.0542e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 4s - loss: 10.7742 - mse: 10.7742 - mae: 1.4636 - val_loss: 18.3084 - val_mse: 18.3084 - val_mae: 1.4895 - lr: 1.0542e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 4s - loss: 10.5756 - mse: 10.5756 - mae: 1.4527 - val_loss: 18.1590 - val_mse: 18.1590 - val_mae: 1.4910 - lr: 1.0542e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 4s - loss: 10.4930 - mse: 10.4930 - mae: 1.4439 - val_loss: 18.2676 - val_mse: 18.2676 - val_mae: 1.4860 - lr: 1.0542e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 4s - loss: 10.3685 - mse: 10.3685 - mae: 1.4374 - val_loss: 18.1568 - val_mse: 18.1568 - val_mae: 1.5467 - lr: 1.0542e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 4s - loss: 10.3794 - mse: 10.3794 - mae: 1.4344 - val_loss: 18.0606 - val_mse: 18.0606 - val_mae: 1.5278 - lr: 1.0542e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 4s - loss: 10.2783 - mse: 10.2783 - mae: 1.4271 - val_loss: 18.1385 - val_mse: 18.1385 - val_mae: 1.5746 - lr: 1.0542e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 4s - loss: 10.2358 - mse: 10.2358 - mae: 1.4225 - val_loss: 18.1761 - val_mse: 18.1761 - val_mae: 1.4507 - lr: 1.0542e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 4s - loss: 10.1683 - mse: 10.1683 - mae: 1.4194 - val_loss: 18.1096 - val_mse: 18.1096 - val_mae: 1.4659 - lr: 1.0542e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 4s - loss: 9.9398 - mse: 9.9398 - mae: 1.4119 - val_loss: 18.2865 - val_mse: 18.2865 - val_mae: 1.4719 - lr: 1.0542e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 13/100\n",
            "1000/1000 - 4s - loss: 9.9576 - mse: 9.9576 - mae: 1.4075 - val_loss: 18.1065 - val_mse: 18.1065 - val_mae: 1.4881 - lr: 1.0542e-04 - 4s/epoch - 4ms/step\n",
            "Score for fold 1: loss of 18.106542587280273\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 4s - loss: 12.4135 - mse: 12.4135 - mae: 1.4385 - val_loss: 8.3361 - val_mse: 8.3361 - val_mae: 1.3964 - lr: 1.0542e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 4s - loss: 12.3007 - mse: 12.3007 - mae: 1.4305 - val_loss: 8.4216 - val_mse: 8.4216 - val_mae: 1.4448 - lr: 1.0542e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 4s - loss: 12.3224 - mse: 12.3224 - mae: 1.4267 - val_loss: 8.4577 - val_mse: 8.4577 - val_mae: 1.3872 - lr: 1.0542e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 3s - loss: 12.2213 - mse: 12.2213 - mae: 1.4253 - val_loss: 8.4596 - val_mse: 8.4596 - val_mae: 1.4240 - lr: 1.0542e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 3s - loss: 12.0589 - mse: 12.0589 - mae: 1.4140 - val_loss: 8.4925 - val_mse: 8.4925 - val_mae: 1.4274 - lr: 1.0542e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 3s - loss: 12.0511 - mse: 12.0511 - mae: 1.4123 - val_loss: 8.4484 - val_mse: 8.4484 - val_mae: 1.4072 - lr: 1.0542e-04 - 3s/epoch - 3ms/step\n",
            "Score for fold 2: loss of 8.44835376739502\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 3s - loss: 11.3273 - mse: 11.3273 - mae: 1.4081 - val_loss: 11.5158 - val_mse: 11.5158 - val_mae: 1.4245 - lr: 1.0542e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 3s - loss: 11.0794 - mse: 11.0794 - mae: 1.3995 - val_loss: 11.7273 - val_mse: 11.7273 - val_mae: 1.4109 - lr: 1.0542e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 3s - loss: 11.0486 - mse: 11.0486 - mae: 1.3938 - val_loss: 11.3781 - val_mse: 11.3781 - val_mae: 1.3819 - lr: 1.0542e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 3s - loss: 10.9146 - mse: 10.9146 - mae: 1.3871 - val_loss: 11.4681 - val_mse: 11.4681 - val_mae: 1.4092 - lr: 1.0542e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 3s - loss: 10.8908 - mse: 10.8908 - mae: 1.3820 - val_loss: 11.0857 - val_mse: 11.0857 - val_mae: 1.4482 - lr: 1.0542e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 3s - loss: 10.8031 - mse: 10.8031 - mae: 1.3793 - val_loss: 11.3329 - val_mse: 11.3329 - val_mae: 1.4135 - lr: 1.0542e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 3s - loss: 10.7395 - mse: 10.7395 - mae: 1.3773 - val_loss: 11.2200 - val_mse: 11.2200 - val_mae: 1.4149 - lr: 1.0542e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 3s - loss: 10.6404 - mse: 10.6404 - mae: 1.3680 - val_loss: 12.1335 - val_mse: 12.1335 - val_mae: 1.4530 - lr: 1.0542e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 3s - loss: 10.5203 - mse: 10.5203 - mae: 1.3647 - val_loss: 11.9156 - val_mse: 11.9156 - val_mae: 1.4437 - lr: 1.0542e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 3s - loss: 10.5124 - mse: 10.5124 - mae: 1.3617 - val_loss: 11.3637 - val_mse: 11.3637 - val_mae: 1.4850 - lr: 1.0542e-04 - 3s/epoch - 3ms/step\n",
            "Score for fold 3: loss of 11.363646507263184\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 3s - loss: 11.3768 - mse: 11.3768 - mae: 1.3873 - val_loss: 7.4903 - val_mse: 7.4903 - val_mae: 1.3388 - lr: 1.0542e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 3s - loss: 11.2749 - mse: 11.2749 - mae: 1.3810 - val_loss: 7.8026 - val_mse: 7.8026 - val_mae: 1.4034 - lr: 1.0542e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 3s - loss: 11.2824 - mse: 11.2824 - mae: 1.3749 - val_loss: 7.6174 - val_mse: 7.6174 - val_mae: 1.3622 - lr: 1.0542e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 3s - loss: 11.2561 - mse: 11.2561 - mae: 1.3686 - val_loss: 7.6390 - val_mse: 7.6390 - val_mae: 1.3637 - lr: 1.0542e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 3s - loss: 11.1387 - mse: 11.1387 - mae: 1.3639 - val_loss: 7.8350 - val_mse: 7.8350 - val_mae: 1.3537 - lr: 1.0542e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 3s - loss: 11.0803 - mse: 11.0803 - mae: 1.3565 - val_loss: 7.6401 - val_mse: 7.6401 - val_mae: 1.3706 - lr: 1.0542e-04 - 3s/epoch - 3ms/step\n",
            "Score for fold 4: loss of 7.640110492706299\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 3s - loss: 10.4775 - mse: 10.4775 - mae: 1.3717 - val_loss: 9.9365 - val_mse: 9.9365 - val_mae: 1.2889 - lr: 1.0542e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 3s - loss: 10.6103 - mse: 10.6103 - mae: 1.3662 - val_loss: 9.9770 - val_mse: 9.9770 - val_mae: 1.2624 - lr: 1.0542e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 3s - loss: 10.1904 - mse: 10.1904 - mae: 1.3584 - val_loss: 9.9538 - val_mse: 9.9538 - val_mae: 1.2982 - lr: 1.0542e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 3s - loss: 10.0055 - mse: 10.0055 - mae: 1.3504 - val_loss: 9.9679 - val_mse: 9.9679 - val_mae: 1.3253 - lr: 1.0542e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 3s - loss: 9.9279 - mse: 9.9279 - mae: 1.3431 - val_loss: 9.9834 - val_mse: 9.9834 - val_mae: 1.3505 - lr: 1.0542e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 3s - loss: 9.9027 - mse: 9.9027 - mae: 1.3389 - val_loss: 10.1355 - val_mse: 10.1355 - val_mae: 1.3818 - lr: 1.0542e-04 - 3s/epoch - 3ms/step\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-10-12 10:01:17,789]\u001b[0m Finished trial#39 resulted in value: 11.14. Current best value is 8.668000000000001 with parameters: {'activation': 'relu', 'num_hidden_layer': 4, 'i': 10, 'learning_rate': 0.0001077579645735244}.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Score for fold 5: loss of 10.135541915893555\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 5s - loss: 14.3252 - mse: 14.3252 - mae: 1.6594 - val_loss: 13.0621 - val_mse: 13.0621 - val_mae: 1.5585 - lr: 1.1905e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 4s - loss: 13.6491 - mse: 13.6491 - mae: 1.5718 - val_loss: 12.7721 - val_mse: 12.7721 - val_mae: 1.5933 - lr: 1.1905e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 4s - loss: 13.5827 - mse: 13.5827 - mae: 1.5680 - val_loss: 12.9769 - val_mse: 12.9769 - val_mae: 1.5492 - lr: 1.1905e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 4s - loss: 13.4787 - mse: 13.4787 - mae: 1.5615 - val_loss: 12.5812 - val_mse: 12.5812 - val_mae: 1.5334 - lr: 1.1905e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 4s - loss: 13.5843 - mse: 13.5843 - mae: 1.5616 - val_loss: 12.5595 - val_mse: 12.5595 - val_mae: 1.5270 - lr: 1.1905e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 4s - loss: 13.4445 - mse: 13.4445 - mae: 1.5580 - val_loss: 12.6344 - val_mse: 12.6344 - val_mae: 1.5151 - lr: 1.1905e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 4s - loss: 13.4866 - mse: 13.4866 - mae: 1.5589 - val_loss: 12.7133 - val_mse: 12.7133 - val_mae: 1.5647 - lr: 1.1905e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 4s - loss: 13.4098 - mse: 13.4098 - mae: 1.5596 - val_loss: 12.8665 - val_mse: 12.8665 - val_mae: 1.5490 - lr: 1.1905e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 4s - loss: 13.3672 - mse: 13.3672 - mae: 1.5521 - val_loss: 13.0072 - val_mse: 13.0072 - val_mae: 1.6055 - lr: 1.1905e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 4s - loss: 13.4528 - mse: 13.4528 - mae: 1.5548 - val_loss: 12.6966 - val_mse: 12.6966 - val_mae: 1.5175 - lr: 1.1905e-04 - 4s/epoch - 4ms/step\n",
            "Score for fold 1: loss of 12.696646690368652\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 4s - loss: 13.3872 - mse: 13.3872 - mae: 1.5500 - val_loss: 13.0933 - val_mse: 13.0933 - val_mae: 1.5183 - lr: 1.1905e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 4s - loss: 13.3968 - mse: 13.3968 - mae: 1.5462 - val_loss: 12.9601 - val_mse: 12.9601 - val_mae: 1.5593 - lr: 1.1905e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 4s - loss: 13.3473 - mse: 13.3473 - mae: 1.5473 - val_loss: 12.9061 - val_mse: 12.9061 - val_mae: 1.5547 - lr: 1.1905e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 4s - loss: 13.3197 - mse: 13.3197 - mae: 1.5446 - val_loss: 13.1178 - val_mse: 13.1178 - val_mae: 1.5340 - lr: 1.1905e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 4s - loss: 13.3358 - mse: 13.3358 - mae: 1.5458 - val_loss: 12.9706 - val_mse: 12.9706 - val_mae: 1.5401 - lr: 1.1905e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 4s - loss: 13.3775 - mse: 13.3775 - mae: 1.5461 - val_loss: 12.9980 - val_mse: 12.9980 - val_mae: 1.5201 - lr: 1.1905e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 4s - loss: 13.2348 - mse: 13.2348 - mae: 1.5436 - val_loss: 12.9125 - val_mse: 12.9125 - val_mae: 1.5597 - lr: 1.1905e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 4s - loss: 13.3495 - mse: 13.3495 - mae: 1.5449 - val_loss: 12.9080 - val_mse: 12.9080 - val_mae: 1.5850 - lr: 1.1905e-04 - 4s/epoch - 4ms/step\n",
            "Score for fold 2: loss of 12.90799331665039\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 4s - loss: 11.1999 - mse: 11.1999 - mae: 1.5419 - val_loss: 21.5304 - val_mse: 21.5304 - val_mae: 1.5806 - lr: 1.1905e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 5s - loss: 11.1874 - mse: 11.1874 - mae: 1.5454 - val_loss: 21.7456 - val_mse: 21.7456 - val_mae: 1.5552 - lr: 1.1905e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 4s - loss: 11.2159 - mse: 11.2159 - mae: 1.5380 - val_loss: 21.8444 - val_mse: 21.8444 - val_mae: 1.6201 - lr: 1.1905e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 4s - loss: 11.2268 - mse: 11.2268 - mae: 1.5406 - val_loss: 21.5011 - val_mse: 21.5011 - val_mae: 1.5853 - lr: 1.1905e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 4s - loss: 11.2800 - mse: 11.2800 - mae: 1.5448 - val_loss: 21.4485 - val_mse: 21.4485 - val_mae: 1.6297 - lr: 1.1905e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 4s - loss: 11.1901 - mse: 11.1901 - mae: 1.5425 - val_loss: 21.4728 - val_mse: 21.4728 - val_mae: 1.5924 - lr: 1.1905e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 4s - loss: 11.2314 - mse: 11.2314 - mae: 1.5412 - val_loss: 21.5843 - val_mse: 21.5843 - val_mae: 1.5686 - lr: 1.1905e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 4s - loss: 11.1491 - mse: 11.1491 - mae: 1.5381 - val_loss: 21.5745 - val_mse: 21.5745 - val_mae: 1.5491 - lr: 1.1905e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 5s - loss: 11.2010 - mse: 11.2010 - mae: 1.5382 - val_loss: 21.4319 - val_mse: 21.4319 - val_mae: 1.5915 - lr: 1.1905e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 4s - loss: 11.1875 - mse: 11.1875 - mae: 1.5429 - val_loss: 21.5018 - val_mse: 21.5018 - val_mae: 1.5653 - lr: 1.1905e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 5s - loss: 11.2204 - mse: 11.2204 - mae: 1.5404 - val_loss: 21.6306 - val_mse: 21.6306 - val_mae: 1.6007 - lr: 1.1905e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 5s - loss: 11.2068 - mse: 11.2068 - mae: 1.5405 - val_loss: 21.5136 - val_mse: 21.5136 - val_mae: 1.5727 - lr: 1.1905e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 13/100\n",
            "1000/1000 - 5s - loss: 11.1532 - mse: 11.1532 - mae: 1.5376 - val_loss: 21.5078 - val_mse: 21.5078 - val_mae: 1.5579 - lr: 1.1905e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 14/100\n",
            "1000/1000 - 5s - loss: 11.1888 - mse: 11.1888 - mae: 1.5421 - val_loss: 21.5156 - val_mse: 21.5156 - val_mae: 1.5713 - lr: 1.1905e-04 - 5s/epoch - 5ms/step\n",
            "Score for fold 3: loss of 21.51560401916504\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 5s - loss: 14.1166 - mse: 14.1166 - mae: 1.5466 - val_loss: 9.8928 - val_mse: 9.8928 - val_mae: 1.5500 - lr: 1.1905e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 5s - loss: 14.0771 - mse: 14.0771 - mae: 1.5429 - val_loss: 9.9272 - val_mse: 9.9272 - val_mae: 1.5575 - lr: 1.1905e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 5s - loss: 14.1525 - mse: 14.1525 - mae: 1.5468 - val_loss: 9.9051 - val_mse: 9.9051 - val_mae: 1.5370 - lr: 1.1905e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 5s - loss: 14.0838 - mse: 14.0838 - mae: 1.5420 - val_loss: 9.8854 - val_mse: 9.8854 - val_mae: 1.5385 - lr: 1.1905e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 5s - loss: 14.0328 - mse: 14.0328 - mae: 1.5474 - val_loss: 9.8922 - val_mse: 9.8922 - val_mae: 1.5598 - lr: 1.1905e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 5s - loss: 14.1082 - mse: 14.1082 - mae: 1.5455 - val_loss: 10.0599 - val_mse: 10.0599 - val_mae: 1.5679 - lr: 1.1905e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 5s - loss: 14.0692 - mse: 14.0692 - mae: 1.5396 - val_loss: 9.8942 - val_mse: 9.8942 - val_mae: 1.5777 - lr: 1.1905e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 5s - loss: 14.0583 - mse: 14.0583 - mae: 1.5470 - val_loss: 9.8919 - val_mse: 9.8919 - val_mae: 1.5678 - lr: 1.1905e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 5s - loss: 14.0393 - mse: 14.0393 - mae: 1.5469 - val_loss: 9.8858 - val_mse: 9.8858 - val_mae: 1.5280 - lr: 1.1905e-04 - 5s/epoch - 5ms/step\n",
            "Score for fold 4: loss of 9.885788917541504\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 5s - loss: 14.2288 - mse: 14.2288 - mae: 1.5466 - val_loss: 9.1738 - val_mse: 9.1738 - val_mae: 1.5362 - lr: 1.1905e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 5s - loss: 14.1560 - mse: 14.1560 - mae: 1.5506 - val_loss: 9.1547 - val_mse: 9.1547 - val_mae: 1.5219 - lr: 1.1905e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 5s - loss: 14.1898 - mse: 14.1898 - mae: 1.5543 - val_loss: 9.1351 - val_mse: 9.1351 - val_mae: 1.5263 - lr: 1.1905e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 5s - loss: 14.2005 - mse: 14.2005 - mae: 1.5507 - val_loss: 9.1497 - val_mse: 9.1497 - val_mae: 1.5295 - lr: 1.1905e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 5s - loss: 14.1922 - mse: 14.1922 - mae: 1.5489 - val_loss: 9.1437 - val_mse: 9.1437 - val_mae: 1.5457 - lr: 1.1905e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 5s - loss: 14.2371 - mse: 14.2371 - mae: 1.5519 - val_loss: 9.1378 - val_mse: 9.1378 - val_mae: 1.5267 - lr: 1.1905e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 5s - loss: 14.2483 - mse: 14.2483 - mae: 1.5577 - val_loss: 9.1640 - val_mse: 9.1640 - val_mae: 1.5103 - lr: 1.1905e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 5s - loss: 14.1725 - mse: 14.1725 - mae: 1.5475 - val_loss: 9.1743 - val_mse: 9.1743 - val_mae: 1.5139 - lr: 1.1905e-04 - 5s/epoch - 5ms/step\n",
            "Score for fold 5: loss of 9.174286842346191\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-10-12 10:05:17,139]\u001b[0m Finished trial#40 resulted in value: 13.238. Current best value is 8.668000000000001 with parameters: {'activation': 'relu', 'num_hidden_layer': 4, 'i': 10, 'learning_rate': 0.0001077579645735244}.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 10s - loss: 13.4879 - mse: 13.4879 - mae: 1.5432 - val_loss: 11.9737 - val_mse: 11.9737 - val_mae: 1.3742 - lr: 2.1824e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 9s - loss: 12.9001 - mse: 12.9001 - mae: 1.4956 - val_loss: 11.4425 - val_mse: 11.4425 - val_mae: 1.4651 - lr: 2.1824e-04 - 9s/epoch - 9ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 9s - loss: 12.5584 - mse: 12.5584 - mae: 1.4834 - val_loss: 11.4057 - val_mse: 11.4057 - val_mae: 1.4483 - lr: 2.1824e-04 - 9s/epoch - 9ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 9s - loss: 12.4005 - mse: 12.4005 - mae: 1.4677 - val_loss: 11.2824 - val_mse: 11.2824 - val_mae: 1.4264 - lr: 2.1824e-04 - 9s/epoch - 9ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 9s - loss: 12.1608 - mse: 12.1608 - mae: 1.4547 - val_loss: 11.5523 - val_mse: 11.5523 - val_mae: 1.4399 - lr: 2.1824e-04 - 9s/epoch - 9ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 9s - loss: 12.1765 - mse: 12.1765 - mae: 1.4559 - val_loss: 11.2581 - val_mse: 11.2581 - val_mae: 1.4251 - lr: 2.1824e-04 - 9s/epoch - 9ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 9s - loss: 11.9792 - mse: 11.9792 - mae: 1.4387 - val_loss: 11.0842 - val_mse: 11.0842 - val_mae: 1.4958 - lr: 2.1824e-04 - 9s/epoch - 9ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 9s - loss: 11.7361 - mse: 11.7361 - mae: 1.4302 - val_loss: 11.0970 - val_mse: 11.0970 - val_mae: 1.4772 - lr: 2.1824e-04 - 9s/epoch - 9ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 9s - loss: 11.3774 - mse: 11.3774 - mae: 1.4190 - val_loss: 11.2788 - val_mse: 11.2788 - val_mae: 1.4094 - lr: 2.1824e-04 - 9s/epoch - 9ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 9s - loss: 11.3290 - mse: 11.3290 - mae: 1.4083 - val_loss: 11.7391 - val_mse: 11.7391 - val_mae: 1.4614 - lr: 2.1824e-04 - 9s/epoch - 9ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 9s - loss: 11.0994 - mse: 11.0994 - mae: 1.3950 - val_loss: 11.1014 - val_mse: 11.1014 - val_mae: 1.4004 - lr: 2.1824e-04 - 9s/epoch - 9ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 9s - loss: 10.6136 - mse: 10.6136 - mae: 1.3734 - val_loss: 11.1658 - val_mse: 11.1658 - val_mae: 1.5171 - lr: 2.1824e-04 - 9s/epoch - 9ms/step\n",
            "Score for fold 1: loss of 11.165780067443848\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 9s - loss: 9.7681 - mse: 9.7681 - mae: 1.3882 - val_loss: 15.2208 - val_mse: 15.2208 - val_mae: 1.3969 - lr: 2.1824e-04 - 9s/epoch - 9ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 9s - loss: 9.1808 - mse: 9.1808 - mae: 1.3651 - val_loss: 15.6171 - val_mse: 15.6171 - val_mae: 1.3888 - lr: 2.1824e-04 - 9s/epoch - 9ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 9s - loss: 9.0864 - mse: 9.0864 - mae: 1.3600 - val_loss: 15.1862 - val_mse: 15.1862 - val_mae: 1.4021 - lr: 2.1824e-04 - 9s/epoch - 9ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 9s - loss: 8.7945 - mse: 8.7945 - mae: 1.3390 - val_loss: 16.1570 - val_mse: 16.1570 - val_mae: 1.4289 - lr: 2.1824e-04 - 9s/epoch - 9ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 9s - loss: 8.4670 - mse: 8.4670 - mae: 1.3198 - val_loss: 15.2997 - val_mse: 15.2997 - val_mae: 1.4195 - lr: 2.1824e-04 - 9s/epoch - 9ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 9s - loss: 8.1762 - mse: 8.1762 - mae: 1.2996 - val_loss: 15.8529 - val_mse: 15.8529 - val_mae: 1.4443 - lr: 2.1824e-04 - 9s/epoch - 9ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 9s - loss: 7.6695 - mse: 7.6695 - mae: 1.2836 - val_loss: 15.5396 - val_mse: 15.5396 - val_mae: 1.4402 - lr: 2.1824e-04 - 9s/epoch - 9ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 10s - loss: 7.9891 - mse: 7.9891 - mae: 1.2681 - val_loss: 15.6003 - val_mse: 15.6003 - val_mae: 1.4202 - lr: 2.1824e-04 - 10s/epoch - 10ms/step\n",
            "Score for fold 2: loss of 15.600301742553711\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 10s - loss: 9.8993 - mse: 9.8993 - mae: 1.3214 - val_loss: 6.5859 - val_mse: 6.5859 - val_mae: 1.2817 - lr: 2.1824e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 9s - loss: 9.3411 - mse: 9.3411 - mae: 1.2937 - val_loss: 7.1878 - val_mse: 7.1878 - val_mae: 1.2383 - lr: 2.1824e-04 - 9s/epoch - 9ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 9s - loss: 9.0355 - mse: 9.0355 - mae: 1.2726 - val_loss: 8.0168 - val_mse: 8.0168 - val_mae: 1.3130 - lr: 2.1824e-04 - 9s/epoch - 9ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 9s - loss: 8.8069 - mse: 8.8069 - mae: 1.2590 - val_loss: 7.5847 - val_mse: 7.5847 - val_mae: 1.2196 - lr: 2.1824e-04 - 9s/epoch - 9ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 9s - loss: 8.6584 - mse: 8.6584 - mae: 1.2446 - val_loss: 7.4158 - val_mse: 7.4158 - val_mae: 1.2657 - lr: 2.1824e-04 - 9s/epoch - 9ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 9s - loss: 8.1801 - mse: 8.1801 - mae: 1.2228 - val_loss: 7.4080 - val_mse: 7.4080 - val_mae: 1.2317 - lr: 2.1824e-04 - 9s/epoch - 9ms/step\n",
            "Score for fold 3: loss of 7.407991409301758\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 9s - loss: 8.3478 - mse: 8.3478 - mae: 1.2585 - val_loss: 6.2226 - val_mse: 6.2226 - val_mae: 1.1862 - lr: 2.1824e-04 - 9s/epoch - 9ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 9s - loss: 8.2727 - mse: 8.2727 - mae: 1.2273 - val_loss: 6.5654 - val_mse: 6.5654 - val_mae: 1.1366 - lr: 2.1824e-04 - 9s/epoch - 9ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 9s - loss: 7.6870 - mse: 7.6870 - mae: 1.2133 - val_loss: 6.9240 - val_mse: 6.9240 - val_mae: 1.2917 - lr: 2.1824e-04 - 9s/epoch - 9ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 9s - loss: 7.6834 - mse: 7.6834 - mae: 1.2003 - val_loss: 6.7983 - val_mse: 6.7983 - val_mae: 1.3244 - lr: 2.1824e-04 - 9s/epoch - 9ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 9s - loss: 7.2352 - mse: 7.2352 - mae: 1.1790 - val_loss: 7.0616 - val_mse: 7.0616 - val_mae: 1.2658 - lr: 2.1824e-04 - 9s/epoch - 9ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 9s - loss: 6.9522 - mse: 6.9522 - mae: 1.1639 - val_loss: 7.0562 - val_mse: 7.0562 - val_mae: 1.1647 - lr: 2.1824e-04 - 9s/epoch - 9ms/step\n",
            "Score for fold 4: loss of 7.056225776672363\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 9s - loss: 7.4763 - mse: 7.4763 - mae: 1.1877 - val_loss: 5.1028 - val_mse: 5.1028 - val_mae: 1.0779 - lr: 2.1824e-04 - 9s/epoch - 9ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 9s - loss: 7.1228 - mse: 7.1228 - mae: 1.1573 - val_loss: 5.1648 - val_mse: 5.1648 - val_mae: 1.3212 - lr: 2.1824e-04 - 9s/epoch - 9ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 9s - loss: 6.7523 - mse: 6.7523 - mae: 1.1384 - val_loss: 5.4739 - val_mse: 5.4739 - val_mae: 1.1164 - lr: 2.1824e-04 - 9s/epoch - 9ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 9s - loss: 6.5675 - mse: 6.5675 - mae: 1.1140 - val_loss: 5.7903 - val_mse: 5.7903 - val_mae: 1.1302 - lr: 2.1824e-04 - 9s/epoch - 9ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 9s - loss: 6.3899 - mse: 6.3899 - mae: 1.0966 - val_loss: 5.3241 - val_mse: 5.3241 - val_mae: 1.2325 - lr: 2.1824e-04 - 9s/epoch - 9ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 9s - loss: 6.3321 - mse: 6.3321 - mae: 1.0827 - val_loss: 6.0502 - val_mse: 6.0502 - val_mae: 1.1239 - lr: 2.1824e-04 - 9s/epoch - 9ms/step\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-10-12 10:11:19,836]\u001b[0m Finished trial#41 resulted in value: 9.458. Current best value is 8.668000000000001 with parameters: {'activation': 'relu', 'num_hidden_layer': 4, 'i': 10, 'learning_rate': 0.0001077579645735244}.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Score for fold 5: loss of 6.050167560577393\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 10s - loss: 13.5018 - mse: 13.5018 - mae: 1.5376 - val_loss: 10.6648 - val_mse: 10.6648 - val_mae: 1.5770 - lr: 3.2619e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 9s - loss: 12.6610 - mse: 12.6610 - mae: 1.4889 - val_loss: 11.1828 - val_mse: 11.1828 - val_mae: 1.4710 - lr: 3.2619e-04 - 9s/epoch - 9ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 9s - loss: 12.5027 - mse: 12.5027 - mae: 1.4766 - val_loss: 10.6706 - val_mse: 10.6706 - val_mae: 1.4725 - lr: 3.2619e-04 - 9s/epoch - 9ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 9s - loss: 12.3872 - mse: 12.3872 - mae: 1.4684 - val_loss: 12.5341 - val_mse: 12.5341 - val_mae: 1.4471 - lr: 3.2619e-04 - 9s/epoch - 9ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 10s - loss: 12.1778 - mse: 12.1778 - mae: 1.4587 - val_loss: 11.7634 - val_mse: 11.7634 - val_mae: 1.4782 - lr: 3.2619e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 9s - loss: 12.1036 - mse: 12.1036 - mae: 1.4479 - val_loss: 10.6147 - val_mse: 10.6147 - val_mae: 1.4466 - lr: 3.2619e-04 - 9s/epoch - 9ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 10s - loss: 11.9033 - mse: 11.9033 - mae: 1.4393 - val_loss: 10.8800 - val_mse: 10.8800 - val_mae: 1.4210 - lr: 3.2619e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 10s - loss: 11.7819 - mse: 11.7819 - mae: 1.4307 - val_loss: 10.3254 - val_mse: 10.3254 - val_mae: 1.4805 - lr: 3.2619e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 10s - loss: 11.4252 - mse: 11.4252 - mae: 1.4209 - val_loss: 10.4708 - val_mse: 10.4708 - val_mae: 1.4496 - lr: 3.2619e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 10s - loss: 11.3038 - mse: 11.3038 - mae: 1.4115 - val_loss: 12.3122 - val_mse: 12.3122 - val_mae: 1.5385 - lr: 3.2619e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 11s - loss: 11.0588 - mse: 11.0588 - mae: 1.3951 - val_loss: 10.3203 - val_mse: 10.3203 - val_mae: 1.4590 - lr: 3.2619e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 11s - loss: 10.9165 - mse: 10.9165 - mae: 1.3858 - val_loss: 9.9410 - val_mse: 9.9410 - val_mae: 1.4791 - lr: 3.2619e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 13/100\n",
            "1000/1000 - 11s - loss: 10.4662 - mse: 10.4662 - mae: 1.3661 - val_loss: 11.0107 - val_mse: 11.0107 - val_mae: 1.4924 - lr: 3.2619e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 14/100\n",
            "1000/1000 - 11s - loss: 10.1912 - mse: 10.1912 - mae: 1.3519 - val_loss: 12.2070 - val_mse: 12.2070 - val_mae: 1.4754 - lr: 3.2619e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 15/100\n",
            "1000/1000 - 10s - loss: 10.0434 - mse: 10.0434 - mae: 1.3378 - val_loss: 10.5104 - val_mse: 10.5104 - val_mae: 1.4751 - lr: 3.2619e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 16/100\n",
            "1000/1000 - 10s - loss: 9.8219 - mse: 9.8219 - mae: 1.3288 - val_loss: 10.5853 - val_mse: 10.5853 - val_mae: 1.6992 - lr: 3.2619e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 17/100\n",
            "1000/1000 - 10s - loss: 9.5572 - mse: 9.5572 - mae: 1.3163 - val_loss: 10.7845 - val_mse: 10.7845 - val_mae: 1.5202 - lr: 3.2619e-04 - 10s/epoch - 10ms/step\n",
            "Score for fold 1: loss of 10.78450870513916\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 10s - loss: 8.6441 - mse: 8.6441 - mae: 1.3610 - val_loss: 14.8728 - val_mse: 14.8728 - val_mae: 1.3598 - lr: 3.2619e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 10s - loss: 8.7886 - mse: 8.7886 - mae: 1.3465 - val_loss: 14.8415 - val_mse: 14.8415 - val_mae: 1.3098 - lr: 3.2619e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 10s - loss: 8.0209 - mse: 8.0209 - mae: 1.3218 - val_loss: 15.0169 - val_mse: 15.0169 - val_mae: 1.5024 - lr: 3.2619e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 10s - loss: 7.9993 - mse: 7.9993 - mae: 1.3128 - val_loss: 14.7275 - val_mse: 14.7275 - val_mae: 1.3528 - lr: 3.2619e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 10s - loss: 7.9025 - mse: 7.9025 - mae: 1.2997 - val_loss: 15.2221 - val_mse: 15.2221 - val_mae: 1.4043 - lr: 3.2619e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 10s - loss: 7.5750 - mse: 7.5750 - mae: 1.2861 - val_loss: 15.3653 - val_mse: 15.3653 - val_mae: 1.3652 - lr: 3.2619e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 10s - loss: 7.1321 - mse: 7.1321 - mae: 1.2669 - val_loss: 15.2523 - val_mse: 15.2523 - val_mae: 1.3866 - lr: 3.2619e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 10s - loss: 7.3136 - mse: 7.3136 - mae: 1.2524 - val_loss: 15.1559 - val_mse: 15.1559 - val_mae: 1.3228 - lr: 3.2619e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 9s - loss: 6.8175 - mse: 6.8175 - mae: 1.2407 - val_loss: 15.5695 - val_mse: 15.5695 - val_mae: 1.3943 - lr: 3.2619e-04 - 9s/epoch - 9ms/step\n",
            "Score for fold 2: loss of 15.569464683532715\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 9s - loss: 8.7945 - mse: 8.7945 - mae: 1.2845 - val_loss: 7.5803 - val_mse: 7.5803 - val_mae: 1.1889 - lr: 3.2619e-04 - 9s/epoch - 9ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 10s - loss: 8.7704 - mse: 8.7704 - mae: 1.2689 - val_loss: 8.0622 - val_mse: 8.0622 - val_mae: 1.1697 - lr: 3.2619e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 10s - loss: 8.5252 - mse: 8.5252 - mae: 1.2481 - val_loss: 7.8260 - val_mse: 7.8260 - val_mae: 1.2430 - lr: 3.2619e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 10s - loss: 8.0392 - mse: 8.0392 - mae: 1.2297 - val_loss: 8.2778 - val_mse: 8.2778 - val_mae: 1.2613 - lr: 3.2619e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 10s - loss: 8.1179 - mse: 8.1179 - mae: 1.2140 - val_loss: 8.7432 - val_mse: 8.7432 - val_mae: 1.2842 - lr: 3.2619e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 10s - loss: 7.3075 - mse: 7.3075 - mae: 1.1900 - val_loss: 8.5959 - val_mse: 8.5959 - val_mae: 1.2599 - lr: 3.2619e-04 - 10s/epoch - 10ms/step\n",
            "Score for fold 3: loss of 8.595891952514648\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 10s - loss: 7.6227 - mse: 7.6227 - mae: 1.2228 - val_loss: 7.0339 - val_mse: 7.0339 - val_mae: 1.3125 - lr: 3.2619e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 10s - loss: 7.6650 - mse: 7.6650 - mae: 1.2057 - val_loss: 6.8284 - val_mse: 6.8284 - val_mae: 1.1732 - lr: 3.2619e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 10s - loss: 6.9256 - mse: 6.9256 - mae: 1.1828 - val_loss: 7.2085 - val_mse: 7.2085 - val_mae: 1.2524 - lr: 3.2619e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 10s - loss: 7.0802 - mse: 7.0802 - mae: 1.1721 - val_loss: 7.5318 - val_mse: 7.5318 - val_mae: 1.1826 - lr: 3.2619e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 10s - loss: 6.7014 - mse: 6.7014 - mae: 1.1586 - val_loss: 7.4787 - val_mse: 7.4787 - val_mae: 1.1703 - lr: 3.2619e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 10s - loss: 6.7321 - mse: 6.7321 - mae: 1.1419 - val_loss: 7.5438 - val_mse: 7.5438 - val_mae: 1.1599 - lr: 3.2619e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 11s - loss: 6.2626 - mse: 6.2626 - mae: 1.1201 - val_loss: 7.4164 - val_mse: 7.4164 - val_mae: 1.2555 - lr: 3.2619e-04 - 11s/epoch - 11ms/step\n",
            "Score for fold 4: loss of 7.416418075561523\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 12s - loss: 7.5248 - mse: 7.5248 - mae: 1.1618 - val_loss: 3.4634 - val_mse: 3.4634 - val_mae: 1.2257 - lr: 3.2619e-04 - 12s/epoch - 12ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 12s - loss: 6.6680 - mse: 6.6680 - mae: 1.1416 - val_loss: 3.3193 - val_mse: 3.3193 - val_mae: 1.0710 - lr: 3.2619e-04 - 12s/epoch - 12ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 12s - loss: 6.4962 - mse: 6.4962 - mae: 1.1263 - val_loss: 3.7570 - val_mse: 3.7570 - val_mae: 1.1660 - lr: 3.2619e-04 - 12s/epoch - 12ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 12s - loss: 6.1318 - mse: 6.1318 - mae: 1.1106 - val_loss: 3.7494 - val_mse: 3.7494 - val_mae: 1.0547 - lr: 3.2619e-04 - 12s/epoch - 12ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 12s - loss: 6.0139 - mse: 6.0139 - mae: 1.0953 - val_loss: 3.6058 - val_mse: 3.6058 - val_mae: 1.1532 - lr: 3.2619e-04 - 12s/epoch - 12ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 12s - loss: 6.2188 - mse: 6.2188 - mae: 1.0920 - val_loss: 4.2009 - val_mse: 4.2009 - val_mae: 1.2010 - lr: 3.2619e-04 - 12s/epoch - 12ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 12s - loss: 5.7809 - mse: 5.7809 - mae: 1.0735 - val_loss: 3.8895 - val_mse: 3.8895 - val_mae: 1.1874 - lr: 3.2619e-04 - 12s/epoch - 12ms/step\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-10-12 10:19:17,268]\u001b[0m Finished trial#42 resulted in value: 9.252. Current best value is 8.668000000000001 with parameters: {'activation': 'relu', 'num_hidden_layer': 4, 'i': 10, 'learning_rate': 0.0001077579645735244}.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Score for fold 5: loss of 3.8894553184509277\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 12s - loss: 14.2050 - mse: 14.2050 - mae: 1.5793 - val_loss: 9.4217 - val_mse: 9.4217 - val_mae: 1.5364 - lr: 7.2743e-04 - 12s/epoch - 12ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 11s - loss: 13.1993 - mse: 13.1993 - mae: 1.5128 - val_loss: 9.2897 - val_mse: 9.2897 - val_mae: 1.4387 - lr: 7.2743e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 11s - loss: 13.0859 - mse: 13.0859 - mae: 1.4925 - val_loss: 9.4251 - val_mse: 9.4251 - val_mae: 1.4463 - lr: 7.2743e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 11s - loss: 12.9597 - mse: 12.9597 - mae: 1.4852 - val_loss: 9.2916 - val_mse: 9.2916 - val_mae: 1.4186 - lr: 7.2743e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 11s - loss: 12.9300 - mse: 12.9300 - mae: 1.4817 - val_loss: 9.3795 - val_mse: 9.3795 - val_mae: 1.4743 - lr: 7.2743e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 11s - loss: 12.6179 - mse: 12.6179 - mae: 1.4674 - val_loss: 9.0517 - val_mse: 9.0517 - val_mae: 1.4502 - lr: 7.2743e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 11s - loss: 12.5112 - mse: 12.5112 - mae: 1.4585 - val_loss: 9.2794 - val_mse: 9.2794 - val_mae: 1.4572 - lr: 7.2743e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 11s - loss: 12.1901 - mse: 12.1901 - mae: 1.4609 - val_loss: 9.0934 - val_mse: 9.0934 - val_mae: 1.3767 - lr: 7.2743e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 11s - loss: 12.3148 - mse: 12.3148 - mae: 1.4518 - val_loss: 8.8921 - val_mse: 8.8921 - val_mae: 1.4473 - lr: 7.2743e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 11s - loss: 12.0527 - mse: 12.0527 - mae: 1.4345 - val_loss: 8.9707 - val_mse: 8.9707 - val_mae: 1.5105 - lr: 7.2743e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 12s - loss: 12.2517 - mse: 12.2517 - mae: 1.4370 - val_loss: 8.8346 - val_mse: 8.8346 - val_mae: 1.4340 - lr: 7.2743e-04 - 12s/epoch - 12ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 12s - loss: 11.7121 - mse: 11.7121 - mae: 1.4228 - val_loss: 9.0226 - val_mse: 9.0226 - val_mae: 1.4007 - lr: 7.2743e-04 - 12s/epoch - 12ms/step\n",
            "Epoch 13/100\n",
            "1000/1000 - 12s - loss: 11.8444 - mse: 11.8444 - mae: 1.4164 - val_loss: 8.7764 - val_mse: 8.7764 - val_mae: 1.4097 - lr: 7.2743e-04 - 12s/epoch - 12ms/step\n",
            "Epoch 14/100\n",
            "1000/1000 - 11s - loss: 11.6895 - mse: 11.6895 - mae: 1.4136 - val_loss: 8.9734 - val_mse: 8.9734 - val_mae: 1.4361 - lr: 7.2743e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 15/100\n",
            "1000/1000 - 12s - loss: 11.6652 - mse: 11.6652 - mae: 1.4105 - val_loss: 9.0192 - val_mse: 9.0192 - val_mae: 1.4317 - lr: 7.2743e-04 - 12s/epoch - 12ms/step\n",
            "Epoch 16/100\n",
            "1000/1000 - 12s - loss: 11.4673 - mse: 11.4673 - mae: 1.4016 - val_loss: 9.4535 - val_mse: 9.4535 - val_mae: 1.3813 - lr: 7.2743e-04 - 12s/epoch - 12ms/step\n",
            "Epoch 17/100\n",
            "1000/1000 - 12s - loss: 10.8665 - mse: 10.8665 - mae: 1.3843 - val_loss: 8.9360 - val_mse: 8.9360 - val_mae: 1.4773 - lr: 7.2743e-04 - 12s/epoch - 12ms/step\n",
            "Epoch 18/100\n",
            "1000/1000 - 12s - loss: 10.8631 - mse: 10.8631 - mae: 1.3772 - val_loss: 9.3469 - val_mse: 9.3469 - val_mae: 1.5294 - lr: 7.2743e-04 - 12s/epoch - 12ms/step\n",
            "Score for fold 1: loss of 9.34691333770752\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 12s - loss: 10.9212 - mse: 10.9212 - mae: 1.3915 - val_loss: 7.9330 - val_mse: 7.9330 - val_mae: 1.3542 - lr: 7.2743e-04 - 12s/epoch - 12ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 12s - loss: 10.9169 - mse: 10.9169 - mae: 1.3812 - val_loss: 8.3587 - val_mse: 8.3587 - val_mae: 1.3704 - lr: 7.2743e-04 - 12s/epoch - 12ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 12s - loss: 10.6143 - mse: 10.6143 - mae: 1.3706 - val_loss: 8.1235 - val_mse: 8.1235 - val_mae: 1.3667 - lr: 7.2743e-04 - 12s/epoch - 12ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 12s - loss: 10.3496 - mse: 10.3496 - mae: 1.3586 - val_loss: 8.8714 - val_mse: 8.8714 - val_mae: 1.3972 - lr: 7.2743e-04 - 12s/epoch - 12ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 12s - loss: 10.3409 - mse: 10.3409 - mae: 1.3530 - val_loss: 8.5922 - val_mse: 8.5922 - val_mae: 1.3537 - lr: 7.2743e-04 - 12s/epoch - 12ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 12s - loss: 10.2705 - mse: 10.2705 - mae: 1.3476 - val_loss: 8.9825 - val_mse: 8.9825 - val_mae: 1.3730 - lr: 7.2743e-04 - 12s/epoch - 12ms/step\n",
            "Score for fold 2: loss of 8.982522010803223\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 12s - loss: 10.7253 - mse: 10.7253 - mae: 1.3601 - val_loss: 6.3697 - val_mse: 6.3697 - val_mae: 1.3329 - lr: 7.2743e-04 - 12s/epoch - 12ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 12s - loss: 10.2714 - mse: 10.2714 - mae: 1.3405 - val_loss: 6.4629 - val_mse: 6.4629 - val_mae: 1.3201 - lr: 7.2743e-04 - 12s/epoch - 12ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 12s - loss: 10.2552 - mse: 10.2552 - mae: 1.3320 - val_loss: 6.7223 - val_mse: 6.7223 - val_mae: 1.3673 - lr: 7.2743e-04 - 12s/epoch - 12ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 13s - loss: 9.8627 - mse: 9.8627 - mae: 1.3154 - val_loss: 6.9211 - val_mse: 6.9211 - val_mae: 1.3882 - lr: 7.2743e-04 - 13s/epoch - 13ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 12s - loss: 10.0453 - mse: 10.0453 - mae: 1.3110 - val_loss: 7.1164 - val_mse: 7.1164 - val_mae: 1.3577 - lr: 7.2743e-04 - 12s/epoch - 12ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 12s - loss: 9.7731 - mse: 9.7731 - mae: 1.3026 - val_loss: 7.1770 - val_mse: 7.1770 - val_mae: 1.3673 - lr: 7.2743e-04 - 12s/epoch - 12ms/step\n",
            "Score for fold 3: loss of 7.177044868469238\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 12s - loss: 7.8249 - mse: 7.8249 - mae: 1.3227 - val_loss: 13.7294 - val_mse: 13.7294 - val_mae: 1.2634 - lr: 7.2743e-04 - 12s/epoch - 12ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 12s - loss: 7.7876 - mse: 7.7876 - mae: 1.3005 - val_loss: 14.2185 - val_mse: 14.2185 - val_mae: 1.3188 - lr: 7.2743e-04 - 12s/epoch - 12ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 12s - loss: 7.7055 - mse: 7.7055 - mae: 1.2951 - val_loss: 14.5456 - val_mse: 14.5456 - val_mae: 1.2635 - lr: 7.2743e-04 - 12s/epoch - 12ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 12s - loss: 7.4005 - mse: 7.4005 - mae: 1.2843 - val_loss: 15.4591 - val_mse: 15.4591 - val_mae: 1.3367 - lr: 7.2743e-04 - 12s/epoch - 12ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 11s - loss: 7.2699 - mse: 7.2699 - mae: 1.2758 - val_loss: 14.5707 - val_mse: 14.5707 - val_mae: 1.3042 - lr: 7.2743e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 11s - loss: 7.0290 - mse: 7.0290 - mae: 1.2661 - val_loss: 15.2527 - val_mse: 15.2527 - val_mae: 1.2965 - lr: 7.2743e-04 - 11s/epoch - 11ms/step\n",
            "Score for fold 4: loss of 15.252738952636719\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 11s - loss: 8.4645 - mse: 8.4645 - mae: 1.2812 - val_loss: 9.6681 - val_mse: 9.6681 - val_mae: 1.2024 - lr: 7.2743e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 11s - loss: 8.1291 - mse: 8.1291 - mae: 1.2629 - val_loss: 9.7181 - val_mse: 9.7181 - val_mae: 1.2685 - lr: 7.2743e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 12s - loss: 8.0132 - mse: 8.0132 - mae: 1.2540 - val_loss: 9.6006 - val_mse: 9.6006 - val_mae: 1.2726 - lr: 7.2743e-04 - 12s/epoch - 12ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 12s - loss: 8.1378 - mse: 8.1378 - mae: 1.2422 - val_loss: 9.9328 - val_mse: 9.9328 - val_mae: 1.3301 - lr: 7.2743e-04 - 12s/epoch - 12ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 12s - loss: 7.8310 - mse: 7.8310 - mae: 1.2273 - val_loss: 9.7934 - val_mse: 9.7934 - val_mae: 1.3088 - lr: 7.2743e-04 - 12s/epoch - 12ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 12s - loss: 7.5904 - mse: 7.5904 - mae: 1.2193 - val_loss: 9.8338 - val_mse: 9.8338 - val_mae: 1.2849 - lr: 7.2743e-04 - 12s/epoch - 12ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 12s - loss: 7.3794 - mse: 7.3794 - mae: 1.2038 - val_loss: 10.3502 - val_mse: 10.3502 - val_mae: 1.2715 - lr: 7.2743e-04 - 12s/epoch - 12ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 12s - loss: 7.2155 - mse: 7.2155 - mae: 1.1971 - val_loss: 10.6358 - val_mse: 10.6358 - val_mae: 1.3044 - lr: 7.2743e-04 - 12s/epoch - 12ms/step\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-10-12 10:28:01,988]\u001b[0m Finished trial#43 resulted in value: 10.28. Current best value is 8.668000000000001 with parameters: {'activation': 'relu', 'num_hidden_layer': 4, 'i': 10, 'learning_rate': 0.0001077579645735244}.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Score for fold 5: loss of 10.635836601257324\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 13s - loss: 14.3862 - mse: 14.3862 - mae: 1.5392 - val_loss: 10.0243 - val_mse: 10.0243 - val_mae: 1.6005 - lr: 3.2435e-04 - 13s/epoch - 13ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 12s - loss: 13.1649 - mse: 13.1649 - mae: 1.4933 - val_loss: 9.2103 - val_mse: 9.2103 - val_mae: 1.5147 - lr: 3.2435e-04 - 12s/epoch - 12ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 12s - loss: 12.9837 - mse: 12.9837 - mae: 1.4777 - val_loss: 9.4153 - val_mse: 9.4153 - val_mae: 1.4975 - lr: 3.2435e-04 - 12s/epoch - 12ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 12s - loss: 12.9592 - mse: 12.9592 - mae: 1.4665 - val_loss: 8.8069 - val_mse: 8.8069 - val_mae: 1.4371 - lr: 3.2435e-04 - 12s/epoch - 12ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 13s - loss: 12.7020 - mse: 12.7020 - mae: 1.4552 - val_loss: 8.8589 - val_mse: 8.8589 - val_mae: 1.4232 - lr: 3.2435e-04 - 13s/epoch - 13ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 12s - loss: 12.5880 - mse: 12.5880 - mae: 1.4473 - val_loss: 8.7264 - val_mse: 8.7264 - val_mae: 1.4561 - lr: 3.2435e-04 - 12s/epoch - 12ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 14s - loss: 12.4898 - mse: 12.4898 - mae: 1.4387 - val_loss: 9.3755 - val_mse: 9.3755 - val_mae: 1.4420 - lr: 3.2435e-04 - 14s/epoch - 14ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 12s - loss: 12.2154 - mse: 12.2154 - mae: 1.4311 - val_loss: 8.8963 - val_mse: 8.8963 - val_mae: 1.5056 - lr: 3.2435e-04 - 12s/epoch - 12ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 12s - loss: 12.0470 - mse: 12.0470 - mae: 1.4211 - val_loss: 8.9059 - val_mse: 8.9059 - val_mae: 1.4279 - lr: 3.2435e-04 - 12s/epoch - 12ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 12s - loss: 12.7560 - mse: 12.7560 - mae: 1.4118 - val_loss: 8.6305 - val_mse: 8.6305 - val_mae: 1.4744 - lr: 3.2435e-04 - 12s/epoch - 12ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 12s - loss: 11.3080 - mse: 11.3080 - mae: 1.3887 - val_loss: 9.1282 - val_mse: 9.1282 - val_mae: 1.4329 - lr: 3.2435e-04 - 12s/epoch - 12ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 12s - loss: 11.7088 - mse: 11.7088 - mae: 1.3854 - val_loss: 9.1000 - val_mse: 9.1000 - val_mae: 1.4198 - lr: 3.2435e-04 - 12s/epoch - 12ms/step\n",
            "Epoch 13/100\n",
            "1000/1000 - 11s - loss: 11.1548 - mse: 11.1548 - mae: 1.3696 - val_loss: 8.9170 - val_mse: 8.9170 - val_mae: 1.5316 - lr: 3.2435e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 14/100\n",
            "1000/1000 - 11s - loss: 10.9935 - mse: 10.9935 - mae: 1.3572 - val_loss: 8.8841 - val_mse: 8.8841 - val_mae: 1.4979 - lr: 3.2435e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 15/100\n",
            "1000/1000 - 11s - loss: 10.7498 - mse: 10.7498 - mae: 1.3393 - val_loss: 9.3389 - val_mse: 9.3389 - val_mae: 1.5455 - lr: 3.2435e-04 - 11s/epoch - 11ms/step\n",
            "Score for fold 1: loss of 9.338905334472656\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 10s - loss: 10.7865 - mse: 10.7865 - mae: 1.3853 - val_loss: 9.3656 - val_mse: 9.3656 - val_mae: 1.3097 - lr: 3.2435e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 10s - loss: 10.6531 - mse: 10.6531 - mae: 1.3677 - val_loss: 8.9841 - val_mse: 8.9841 - val_mae: 1.2965 - lr: 3.2435e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 9s - loss: 10.1142 - mse: 10.1142 - mae: 1.3536 - val_loss: 9.6707 - val_mse: 9.6707 - val_mae: 1.4561 - lr: 3.2435e-04 - 9s/epoch - 9ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 10s - loss: 9.8131 - mse: 9.8131 - mae: 1.3342 - val_loss: 9.3871 - val_mse: 9.3871 - val_mae: 1.3661 - lr: 3.2435e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 10s - loss: 9.4839 - mse: 9.4839 - mae: 1.3180 - val_loss: 9.6947 - val_mse: 9.6947 - val_mae: 1.4518 - lr: 3.2435e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 10s - loss: 9.3252 - mse: 9.3252 - mae: 1.3102 - val_loss: 9.8908 - val_mse: 9.8908 - val_mae: 1.4290 - lr: 3.2435e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 10s - loss: 9.2413 - mse: 9.2413 - mae: 1.3020 - val_loss: 10.0994 - val_mse: 10.0994 - val_mae: 1.3893 - lr: 3.2435e-04 - 10s/epoch - 10ms/step\n",
            "Score for fold 2: loss of 10.09936237335205\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 10s - loss: 7.8957 - mse: 7.8957 - mae: 1.3311 - val_loss: 15.2982 - val_mse: 15.2982 - val_mae: 1.3102 - lr: 3.2435e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 10s - loss: 7.5169 - mse: 7.5169 - mae: 1.3142 - val_loss: 15.0921 - val_mse: 15.0921 - val_mae: 1.3406 - lr: 3.2435e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 10s - loss: 7.7219 - mse: 7.7219 - mae: 1.3083 - val_loss: 15.9675 - val_mse: 15.9675 - val_mae: 1.3883 - lr: 3.2435e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 10s - loss: 7.0342 - mse: 7.0342 - mae: 1.2859 - val_loss: 16.3291 - val_mse: 16.3291 - val_mae: 1.2718 - lr: 3.2435e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 10s - loss: 6.7462 - mse: 6.7462 - mae: 1.2631 - val_loss: 15.9302 - val_mse: 15.9302 - val_mae: 1.4143 - lr: 3.2435e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 10s - loss: 6.5029 - mse: 6.5029 - mae: 1.2406 - val_loss: 16.8806 - val_mse: 16.8806 - val_mae: 1.3398 - lr: 3.2435e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 10s - loss: 6.3850 - mse: 6.3850 - mae: 1.2411 - val_loss: 16.4966 - val_mse: 16.4966 - val_mae: 1.3296 - lr: 3.2435e-04 - 10s/epoch - 10ms/step\n",
            "Score for fold 3: loss of 16.49664306640625\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 10s - loss: 8.6291 - mse: 8.6291 - mae: 1.2748 - val_loss: 9.1074 - val_mse: 9.1074 - val_mae: 1.1614 - lr: 3.2435e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 11s - loss: 8.3532 - mse: 8.3532 - mae: 1.2557 - val_loss: 7.4820 - val_mse: 7.4820 - val_mae: 1.1722 - lr: 3.2435e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 10s - loss: 8.1259 - mse: 8.1259 - mae: 1.2370 - val_loss: 6.4341 - val_mse: 6.4341 - val_mae: 1.2501 - lr: 3.2435e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 10s - loss: 7.5858 - mse: 7.5858 - mae: 1.2131 - val_loss: 6.9162 - val_mse: 6.9162 - val_mae: 1.2812 - lr: 3.2435e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 10s - loss: 7.5164 - mse: 7.5164 - mae: 1.1951 - val_loss: 8.2691 - val_mse: 8.2691 - val_mae: 1.3839 - lr: 3.2435e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 10s - loss: 7.2613 - mse: 7.2613 - mae: 1.1844 - val_loss: 7.3568 - val_mse: 7.3568 - val_mae: 1.2966 - lr: 3.2435e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 10s - loss: 7.0371 - mse: 7.0371 - mae: 1.1760 - val_loss: 7.0486 - val_mse: 7.0486 - val_mae: 1.2576 - lr: 3.2435e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 10s - loss: 6.8516 - mse: 6.8516 - mae: 1.1679 - val_loss: 8.4698 - val_mse: 8.4698 - val_mae: 1.2795 - lr: 3.2435e-04 - 10s/epoch - 10ms/step\n",
            "Score for fold 4: loss of 8.469817161560059\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 10s - loss: 7.6124 - mse: 7.6124 - mae: 1.2103 - val_loss: 5.2577 - val_mse: 5.2577 - val_mae: 1.2272 - lr: 3.2435e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 10s - loss: 7.4267 - mse: 7.4267 - mae: 1.1906 - val_loss: 4.9338 - val_mse: 4.9338 - val_mae: 1.1478 - lr: 3.2435e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 10s - loss: 6.8839 - mse: 6.8839 - mae: 1.1627 - val_loss: 5.3862 - val_mse: 5.3862 - val_mae: 1.1586 - lr: 3.2435e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 10s - loss: 6.5792 - mse: 6.5792 - mae: 1.1533 - val_loss: 4.9047 - val_mse: 4.9047 - val_mae: 1.1139 - lr: 3.2435e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 10s - loss: 6.3515 - mse: 6.3515 - mae: 1.1370 - val_loss: 5.1988 - val_mse: 5.1988 - val_mae: 1.3060 - lr: 3.2435e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 10s - loss: 6.2772 - mse: 6.2772 - mae: 1.1200 - val_loss: 5.4222 - val_mse: 5.4222 - val_mae: 1.1292 - lr: 3.2435e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 10s - loss: 5.8861 - mse: 5.8861 - mae: 1.1066 - val_loss: 5.6794 - val_mse: 5.6794 - val_mae: 1.1715 - lr: 3.2435e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 10s - loss: 5.7190 - mse: 5.7190 - mae: 1.1017 - val_loss: 5.6614 - val_mse: 5.6614 - val_mae: 1.3071 - lr: 3.2435e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 11s - loss: 5.7047 - mse: 5.7047 - mae: 1.0855 - val_loss: 5.7034 - val_mse: 5.7034 - val_mae: 1.2045 - lr: 3.2435e-04 - 11s/epoch - 11ms/step\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-10-12 10:36:22,339]\u001b[0m Finished trial#44 resulted in value: 10.022. Current best value is 8.668000000000001 with parameters: {'activation': 'relu', 'num_hidden_layer': 4, 'i': 10, 'learning_rate': 0.0001077579645735244}.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Score for fold 5: loss of 5.703446388244629\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 13s - loss: 11.7755 - mse: 11.7755 - mae: 1.5458 - val_loss: 18.2436 - val_mse: 18.2436 - val_mae: 1.5157 - lr: 4.5304e-04 - 13s/epoch - 13ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 12s - loss: 11.0996 - mse: 11.0996 - mae: 1.4957 - val_loss: 17.4965 - val_mse: 17.4965 - val_mae: 1.5089 - lr: 4.5304e-04 - 12s/epoch - 12ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 11s - loss: 11.0545 - mse: 11.0545 - mae: 1.4833 - val_loss: 17.3520 - val_mse: 17.3520 - val_mae: 1.5410 - lr: 4.5304e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 10s - loss: 10.8823 - mse: 10.8823 - mae: 1.4729 - val_loss: 17.4905 - val_mse: 17.4905 - val_mae: 1.4372 - lr: 4.5304e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 10s - loss: 10.7597 - mse: 10.7597 - mae: 1.4617 - val_loss: 17.4208 - val_mse: 17.4208 - val_mae: 1.4591 - lr: 4.5304e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 10s - loss: 10.4604 - mse: 10.4604 - mae: 1.4512 - val_loss: 17.4004 - val_mse: 17.4004 - val_mae: 1.4460 - lr: 4.5304e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 10s - loss: 10.3635 - mse: 10.3635 - mae: 1.4459 - val_loss: 17.2585 - val_mse: 17.2585 - val_mae: 1.4321 - lr: 4.5304e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 10s - loss: 10.3281 - mse: 10.3281 - mae: 1.4396 - val_loss: 17.4094 - val_mse: 17.4094 - val_mae: 1.4230 - lr: 4.5304e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 9s - loss: 10.0694 - mse: 10.0694 - mae: 1.4287 - val_loss: 17.1371 - val_mse: 17.1371 - val_mae: 1.4659 - lr: 4.5304e-04 - 9s/epoch - 9ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 9s - loss: 9.7034 - mse: 9.7034 - mae: 1.4157 - val_loss: 17.7564 - val_mse: 17.7564 - val_mae: 1.4371 - lr: 4.5304e-04 - 9s/epoch - 9ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 9s - loss: 9.7018 - mse: 9.7018 - mae: 1.4098 - val_loss: 17.0426 - val_mse: 17.0426 - val_mae: 1.5539 - lr: 4.5304e-04 - 9s/epoch - 9ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 9s - loss: 9.4767 - mse: 9.4767 - mae: 1.4017 - val_loss: 18.3482 - val_mse: 18.3482 - val_mae: 1.4911 - lr: 4.5304e-04 - 9s/epoch - 9ms/step\n",
            "Epoch 13/100\n",
            "1000/1000 - 9s - loss: 9.4628 - mse: 9.4628 - mae: 1.3956 - val_loss: 18.3254 - val_mse: 18.3254 - val_mae: 1.4752 - lr: 4.5304e-04 - 9s/epoch - 9ms/step\n",
            "Epoch 14/100\n",
            "1000/1000 - 9s - loss: 9.2422 - mse: 9.2422 - mae: 1.3860 - val_loss: 17.3181 - val_mse: 17.3181 - val_mae: 1.4999 - lr: 4.5304e-04 - 9s/epoch - 9ms/step\n",
            "Epoch 15/100\n",
            "1000/1000 - 9s - loss: 9.1489 - mse: 9.1489 - mae: 1.3789 - val_loss: 17.1995 - val_mse: 17.1995 - val_mae: 1.4560 - lr: 4.5304e-04 - 9s/epoch - 9ms/step\n",
            "Epoch 16/100\n",
            "1000/1000 - 9s - loss: 9.0857 - mse: 9.0857 - mae: 1.3669 - val_loss: 17.2025 - val_mse: 17.2025 - val_mae: 1.5185 - lr: 4.5304e-04 - 9s/epoch - 9ms/step\n",
            "Score for fold 1: loss of 17.202468872070312\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 8s - loss: 10.9058 - mse: 10.9058 - mae: 1.3839 - val_loss: 10.4478 - val_mse: 10.4478 - val_mae: 1.4772 - lr: 4.5304e-04 - 8s/epoch - 8ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 8s - loss: 10.2172 - mse: 10.2172 - mae: 1.3693 - val_loss: 11.2614 - val_mse: 11.2614 - val_mae: 1.4179 - lr: 4.5304e-04 - 8s/epoch - 8ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 8s - loss: 10.1574 - mse: 10.1574 - mae: 1.3541 - val_loss: 10.2417 - val_mse: 10.2417 - val_mae: 1.4251 - lr: 4.5304e-04 - 8s/epoch - 8ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 8s - loss: 9.8842 - mse: 9.8842 - mae: 1.3416 - val_loss: 11.7318 - val_mse: 11.7318 - val_mae: 1.4838 - lr: 4.5304e-04 - 8s/epoch - 8ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 8s - loss: 9.7953 - mse: 9.7953 - mae: 1.3276 - val_loss: 10.5181 - val_mse: 10.5181 - val_mae: 1.4545 - lr: 4.5304e-04 - 8s/epoch - 8ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 8s - loss: 9.5134 - mse: 9.5134 - mae: 1.3115 - val_loss: 10.8994 - val_mse: 10.8994 - val_mae: 1.4676 - lr: 4.5304e-04 - 8s/epoch - 8ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 8s - loss: 9.3891 - mse: 9.3891 - mae: 1.3018 - val_loss: 11.7274 - val_mse: 11.7274 - val_mae: 1.4559 - lr: 4.5304e-04 - 8s/epoch - 8ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 8s - loss: 9.0594 - mse: 9.0594 - mae: 1.2932 - val_loss: 11.5202 - val_mse: 11.5202 - val_mae: 1.3940 - lr: 4.5304e-04 - 8s/epoch - 8ms/step\n",
            "Score for fold 2: loss of 11.52023983001709\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 8s - loss: 10.3238 - mse: 10.3238 - mae: 1.3364 - val_loss: 7.9851 - val_mse: 7.9851 - val_mae: 1.3528 - lr: 4.5304e-04 - 8s/epoch - 8ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 8s - loss: 10.0187 - mse: 10.0187 - mae: 1.3260 - val_loss: 7.7854 - val_mse: 7.7854 - val_mae: 1.3353 - lr: 4.5304e-04 - 8s/epoch - 8ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 8s - loss: 9.9257 - mse: 9.9257 - mae: 1.3119 - val_loss: 7.9349 - val_mse: 7.9349 - val_mae: 1.3478 - lr: 4.5304e-04 - 8s/epoch - 8ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 8s - loss: 9.7328 - mse: 9.7328 - mae: 1.2981 - val_loss: 7.7700 - val_mse: 7.7700 - val_mae: 1.3081 - lr: 4.5304e-04 - 8s/epoch - 8ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 8s - loss: 9.2577 - mse: 9.2577 - mae: 1.2840 - val_loss: 7.6607 - val_mse: 7.6607 - val_mae: 1.3193 - lr: 4.5304e-04 - 8s/epoch - 8ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 8s - loss: 9.1021 - mse: 9.1021 - mae: 1.2748 - val_loss: 7.7575 - val_mse: 7.7575 - val_mae: 1.3334 - lr: 4.5304e-04 - 8s/epoch - 8ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 9s - loss: 9.0227 - mse: 9.0227 - mae: 1.2643 - val_loss: 8.2777 - val_mse: 8.2777 - val_mae: 1.3026 - lr: 4.5304e-04 - 9s/epoch - 9ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 9s - loss: 9.2186 - mse: 9.2186 - mae: 1.2476 - val_loss: 8.2367 - val_mse: 8.2367 - val_mae: 1.3415 - lr: 4.5304e-04 - 9s/epoch - 9ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 10s - loss: 8.5262 - mse: 8.5262 - mae: 1.2358 - val_loss: 8.3425 - val_mse: 8.3425 - val_mae: 1.3349 - lr: 4.5304e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 10s - loss: 8.5860 - mse: 8.5860 - mae: 1.2291 - val_loss: 8.1472 - val_mse: 8.1472 - val_mae: 1.3606 - lr: 4.5304e-04 - 10s/epoch - 10ms/step\n",
            "Score for fold 3: loss of 8.147195816040039\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 9s - loss: 9.1969 - mse: 9.1969 - mae: 1.2704 - val_loss: 6.3728 - val_mse: 6.3728 - val_mae: 1.1676 - lr: 4.5304e-04 - 9s/epoch - 9ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 9s - loss: 8.5672 - mse: 8.5672 - mae: 1.2471 - val_loss: 6.5332 - val_mse: 6.5332 - val_mae: 1.2819 - lr: 4.5304e-04 - 9s/epoch - 9ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 9s - loss: 8.5831 - mse: 8.5831 - mae: 1.2378 - val_loss: 6.5152 - val_mse: 6.5152 - val_mae: 1.2470 - lr: 4.5304e-04 - 9s/epoch - 9ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 9s - loss: 8.2161 - mse: 8.2161 - mae: 1.2155 - val_loss: 6.6993 - val_mse: 6.6993 - val_mae: 1.2050 - lr: 4.5304e-04 - 9s/epoch - 9ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 9s - loss: 8.0366 - mse: 8.0366 - mae: 1.2093 - val_loss: 6.9758 - val_mse: 6.9758 - val_mae: 1.2624 - lr: 4.5304e-04 - 9s/epoch - 9ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 9s - loss: 8.1033 - mse: 8.1033 - mae: 1.1986 - val_loss: 6.8605 - val_mse: 6.8605 - val_mae: 1.2241 - lr: 4.5304e-04 - 9s/epoch - 9ms/step\n",
            "Score for fold 4: loss of 6.8604888916015625\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 10s - loss: 8.1968 - mse: 8.1968 - mae: 1.2230 - val_loss: 5.3334 - val_mse: 5.3334 - val_mae: 1.1883 - lr: 4.5304e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 10s - loss: 7.9756 - mse: 7.9756 - mae: 1.2051 - val_loss: 5.5029 - val_mse: 5.5029 - val_mae: 1.1202 - lr: 4.5304e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 10s - loss: 7.6104 - mse: 7.6104 - mae: 1.1857 - val_loss: 5.4511 - val_mse: 5.4511 - val_mae: 1.1200 - lr: 4.5304e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 10s - loss: 7.2540 - mse: 7.2540 - mae: 1.1747 - val_loss: 5.7156 - val_mse: 5.7156 - val_mae: 1.1881 - lr: 4.5304e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 10s - loss: 7.2392 - mse: 7.2392 - mae: 1.1692 - val_loss: 5.9554 - val_mse: 5.9554 - val_mae: 1.2056 - lr: 4.5304e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 10s - loss: 6.9170 - mse: 6.9170 - mae: 1.1553 - val_loss: 6.0116 - val_mse: 6.0116 - val_mae: 1.1490 - lr: 4.5304e-04 - 10s/epoch - 10ms/step\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-10-12 10:43:32,304]\u001b[0m Finished trial#45 resulted in value: 9.947999999999999. Current best value is 8.668000000000001 with parameters: {'activation': 'relu', 'num_hidden_layer': 4, 'i': 10, 'learning_rate': 0.0001077579645735244}.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Score for fold 5: loss of 6.011600017547607\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 14.2943 - mse: 14.2943 - mae: 1.5761 - val_loss: 10.2842 - val_mse: 10.2842 - val_mae: 1.4858 - lr: 4.4509e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 13.2168 - mse: 13.2168 - mae: 1.5149 - val_loss: 10.7204 - val_mse: 10.7204 - val_mae: 1.4259 - lr: 4.4509e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 12.8669 - mse: 12.8669 - mae: 1.4966 - val_loss: 10.5682 - val_mse: 10.5682 - val_mae: 1.4471 - lr: 4.4509e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 12.6673 - mse: 12.6673 - mae: 1.4834 - val_loss: 10.0177 - val_mse: 10.0177 - val_mae: 1.4227 - lr: 4.4509e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 12.5056 - mse: 12.5056 - mae: 1.4701 - val_loss: 10.0922 - val_mse: 10.0922 - val_mae: 1.4405 - lr: 4.4509e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 12.4496 - mse: 12.4496 - mae: 1.4670 - val_loss: 10.2184 - val_mse: 10.2184 - val_mae: 1.4593 - lr: 4.4509e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 12.3750 - mse: 12.3750 - mae: 1.4620 - val_loss: 10.3649 - val_mse: 10.3649 - val_mae: 1.4436 - lr: 4.4509e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 2s - loss: 12.3974 - mse: 12.3974 - mae: 1.4553 - val_loss: 10.3103 - val_mse: 10.3103 - val_mae: 1.4473 - lr: 4.4509e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 2s - loss: 12.3297 - mse: 12.3297 - mae: 1.4535 - val_loss: 10.2919 - val_mse: 10.2919 - val_mae: 1.4661 - lr: 4.4509e-04 - 2s/epoch - 2ms/step\n",
            "Score for fold 1: loss of 10.291862487792969\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 12.3652 - mse: 12.3652 - mae: 1.4507 - val_loss: 10.0032 - val_mse: 10.0032 - val_mae: 1.4500 - lr: 4.4509e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 12.3128 - mse: 12.3128 - mae: 1.4480 - val_loss: 9.7647 - val_mse: 9.7647 - val_mae: 1.4690 - lr: 4.4509e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 12.2520 - mse: 12.2520 - mae: 1.4425 - val_loss: 10.1657 - val_mse: 10.1657 - val_mae: 1.4318 - lr: 4.4509e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 12.2084 - mse: 12.2084 - mae: 1.4399 - val_loss: 9.8059 - val_mse: 9.8059 - val_mae: 1.4398 - lr: 4.4509e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 12.1718 - mse: 12.1718 - mae: 1.4377 - val_loss: 10.3774 - val_mse: 10.3774 - val_mae: 1.4286 - lr: 4.4509e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 12.1541 - mse: 12.1541 - mae: 1.4365 - val_loss: 10.1374 - val_mse: 10.1374 - val_mae: 1.4282 - lr: 4.4509e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 12.0187 - mse: 12.0187 - mae: 1.4285 - val_loss: 9.8970 - val_mse: 9.8970 - val_mae: 1.4581 - lr: 4.4509e-04 - 2s/epoch - 2ms/step\n",
            "Score for fold 2: loss of 9.897003173828125\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 12.6712 - mse: 12.6712 - mae: 1.4399 - val_loss: 7.4084 - val_mse: 7.4084 - val_mae: 1.4376 - lr: 4.4509e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 12.5245 - mse: 12.5245 - mae: 1.4312 - val_loss: 7.4646 - val_mse: 7.4646 - val_mae: 1.4251 - lr: 4.4509e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 12.4505 - mse: 12.4505 - mae: 1.4326 - val_loss: 7.7692 - val_mse: 7.7692 - val_mae: 1.4282 - lr: 4.4509e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 12.3230 - mse: 12.3230 - mae: 1.4259 - val_loss: 7.7992 - val_mse: 7.7992 - val_mae: 1.3852 - lr: 4.4509e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 12.2975 - mse: 12.2975 - mae: 1.4209 - val_loss: 7.5945 - val_mse: 7.5945 - val_mae: 1.4228 - lr: 4.4509e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 12.2075 - mse: 12.2075 - mae: 1.4232 - val_loss: 7.8216 - val_mse: 7.8216 - val_mae: 1.4778 - lr: 4.4509e-04 - 2s/epoch - 2ms/step\n",
            "Score for fold 3: loss of 7.821563243865967\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 8.8909 - mse: 8.8909 - mae: 1.4072 - val_loss: 21.2795 - val_mse: 21.2795 - val_mae: 1.4935 - lr: 4.4509e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 8.5529 - mse: 8.5529 - mae: 1.3981 - val_loss: 21.2948 - val_mse: 21.2948 - val_mae: 1.4623 - lr: 4.4509e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 8.6095 - mse: 8.6095 - mae: 1.3964 - val_loss: 21.5334 - val_mse: 21.5334 - val_mae: 1.5148 - lr: 4.4509e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 8.7974 - mse: 8.7974 - mae: 1.3960 - val_loss: 21.3015 - val_mse: 21.3015 - val_mae: 1.5052 - lr: 4.4509e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 8.5370 - mse: 8.5370 - mae: 1.3915 - val_loss: 21.4802 - val_mse: 21.4802 - val_mae: 1.5391 - lr: 4.4509e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 8.5933 - mse: 8.5933 - mae: 1.3861 - val_loss: 21.3920 - val_mse: 21.3920 - val_mae: 1.5306 - lr: 4.4509e-04 - 2s/epoch - 2ms/step\n",
            "Score for fold 4: loss of 21.392004013061523\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 11.5769 - mse: 11.5769 - mae: 1.4261 - val_loss: 8.8776 - val_mse: 8.8776 - val_mae: 1.3506 - lr: 4.4509e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 11.4585 - mse: 11.4585 - mae: 1.4218 - val_loss: 8.9893 - val_mse: 8.9893 - val_mae: 1.3537 - lr: 4.4509e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 11.2932 - mse: 11.2932 - mae: 1.4136 - val_loss: 8.9690 - val_mse: 8.9690 - val_mae: 1.3684 - lr: 4.4509e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 11.1815 - mse: 11.1815 - mae: 1.4138 - val_loss: 9.1504 - val_mse: 9.1504 - val_mae: 1.3643 - lr: 4.4509e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 11.4634 - mse: 11.4634 - mae: 1.4139 - val_loss: 9.2762 - val_mse: 9.2762 - val_mae: 1.4081 - lr: 4.4509e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 11.4091 - mse: 11.4091 - mae: 1.4143 - val_loss: 9.2076 - val_mse: 9.2076 - val_mae: 1.4144 - lr: 4.4509e-04 - 2s/epoch - 2ms/step\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-10-12 10:44:32,363]\u001b[0m Finished trial#46 resulted in value: 11.722. Current best value is 8.668000000000001 with parameters: {'activation': 'relu', 'num_hidden_layer': 4, 'i': 10, 'learning_rate': 0.0001077579645735244}.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Score for fold 5: loss of 9.207576751708984\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 10s - loss: 15.3903 - mse: 15.3903 - mae: 1.6721 - val_loss: 13.3367 - val_mse: 13.3367 - val_mae: 1.4851 - lr: 0.0014 - 10s/epoch - 10ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 10s - loss: 14.3069 - mse: 14.3069 - mae: 1.5950 - val_loss: 12.2377 - val_mse: 12.2377 - val_mae: 1.6378 - lr: 0.0014 - 10s/epoch - 10ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 10s - loss: 14.0347 - mse: 14.0347 - mae: 1.5873 - val_loss: 12.3275 - val_mse: 12.3275 - val_mae: 1.7904 - lr: 0.0014 - 10s/epoch - 10ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 10s - loss: 13.7029 - mse: 13.7029 - mae: 1.5755 - val_loss: 12.2035 - val_mse: 12.2035 - val_mae: 1.7344 - lr: 0.0014 - 10s/epoch - 10ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 10s - loss: 13.5010 - mse: 13.5010 - mae: 1.5715 - val_loss: 13.6906 - val_mse: 13.6906 - val_mae: 2.2888 - lr: 0.0014 - 10s/epoch - 10ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 10s - loss: 13.4023 - mse: 13.4023 - mae: 1.5597 - val_loss: 12.4863 - val_mse: 12.4863 - val_mae: 1.9630 - lr: 0.0014 - 10s/epoch - 10ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 10s - loss: 13.4784 - mse: 13.4784 - mae: 1.5614 - val_loss: 13.3082 - val_mse: 13.3082 - val_mae: 1.6452 - lr: 0.0014 - 10s/epoch - 10ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 10s - loss: 13.2329 - mse: 13.2329 - mae: 1.5479 - val_loss: 12.0138 - val_mse: 12.0138 - val_mae: 1.4205 - lr: 0.0014 - 10s/epoch - 10ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 10s - loss: 13.0983 - mse: 13.0983 - mae: 1.5420 - val_loss: 11.6344 - val_mse: 11.6344 - val_mae: 1.5513 - lr: 0.0014 - 10s/epoch - 10ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 11s - loss: 13.1587 - mse: 13.1587 - mae: 1.5624 - val_loss: 11.7041 - val_mse: 11.7041 - val_mae: 1.6137 - lr: 0.0014 - 11s/epoch - 11ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 10s - loss: 13.1261 - mse: 13.1261 - mae: 1.5416 - val_loss: 12.1421 - val_mse: 12.1421 - val_mae: 1.8491 - lr: 0.0014 - 10s/epoch - 10ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 10s - loss: 13.0218 - mse: 13.0218 - mae: 1.5485 - val_loss: 12.1662 - val_mse: 12.1662 - val_mae: 1.3927 - lr: 0.0014 - 10s/epoch - 10ms/step\n",
            "Epoch 13/100\n",
            "1000/1000 - 9s - loss: 12.8673 - mse: 12.8673 - mae: 1.5282 - val_loss: 11.6121 - val_mse: 11.6121 - val_mae: 1.5565 - lr: 0.0014 - 9s/epoch - 9ms/step\n",
            "Epoch 14/100\n",
            "1000/1000 - 9s - loss: 12.7652 - mse: 12.7652 - mae: 1.5303 - val_loss: 11.7456 - val_mse: 11.7456 - val_mae: 1.6367 - lr: 0.0014 - 9s/epoch - 9ms/step\n",
            "Epoch 15/100\n",
            "1000/1000 - 9s - loss: 12.8767 - mse: 12.8767 - mae: 1.5400 - val_loss: 11.5971 - val_mse: 11.5971 - val_mae: 1.5417 - lr: 0.0014 - 9s/epoch - 9ms/step\n",
            "Epoch 16/100\n",
            "1000/1000 - 9s - loss: 12.8480 - mse: 12.8480 - mae: 1.5557 - val_loss: 11.7389 - val_mse: 11.7389 - val_mae: 1.6927 - lr: 0.0014 - 9s/epoch - 9ms/step\n",
            "Epoch 17/100\n",
            "1000/1000 - 9s - loss: 12.9365 - mse: 12.9365 - mae: 1.5531 - val_loss: 11.4922 - val_mse: 11.4922 - val_mae: 1.6200 - lr: 0.0014 - 9s/epoch - 9ms/step\n",
            "Epoch 18/100\n",
            "1000/1000 - 9s - loss: 12.9065 - mse: 12.9065 - mae: 1.5442 - val_loss: 11.6221 - val_mse: 11.6221 - val_mae: 1.4839 - lr: 0.0014 - 9s/epoch - 9ms/step\n",
            "Epoch 19/100\n",
            "1000/1000 - 9s - loss: 12.6839 - mse: 12.6839 - mae: 1.5383 - val_loss: 12.6525 - val_mse: 12.6525 - val_mae: 2.0435 - lr: 0.0014 - 9s/epoch - 9ms/step\n",
            "Epoch 20/100\n",
            "1000/1000 - 10s - loss: 12.6679 - mse: 12.6679 - mae: 1.5383 - val_loss: 11.5599 - val_mse: 11.5599 - val_mae: 1.4007 - lr: 0.0014 - 10s/epoch - 10ms/step\n",
            "Epoch 21/100\n",
            "1000/1000 - 10s - loss: 12.4826 - mse: 12.4826 - mae: 1.5282 - val_loss: 12.4785 - val_mse: 12.4785 - val_mae: 2.0673 - lr: 0.0014 - 10s/epoch - 10ms/step\n",
            "Epoch 22/100\n",
            "1000/1000 - 10s - loss: 12.6380 - mse: 12.6380 - mae: 1.5437 - val_loss: 11.3816 - val_mse: 11.3816 - val_mae: 1.5634 - lr: 0.0014 - 10s/epoch - 10ms/step\n",
            "Epoch 23/100\n",
            "1000/1000 - 9s - loss: 12.3945 - mse: 12.3945 - mae: 1.5220 - val_loss: 11.4419 - val_mse: 11.4419 - val_mae: 1.6163 - lr: 0.0014 - 9s/epoch - 9ms/step\n",
            "Epoch 24/100\n",
            "1000/1000 - 9s - loss: 12.4512 - mse: 12.4512 - mae: 1.5210 - val_loss: 11.8853 - val_mse: 11.8853 - val_mae: 1.4171 - lr: 0.0014 - 9s/epoch - 9ms/step\n",
            "Epoch 25/100\n",
            "1000/1000 - 10s - loss: 12.5720 - mse: 12.5720 - mae: 1.5305 - val_loss: 11.5520 - val_mse: 11.5520 - val_mae: 1.4088 - lr: 0.0014 - 10s/epoch - 10ms/step\n",
            "Epoch 26/100\n",
            "1000/1000 - 10s - loss: 12.4844 - mse: 12.4844 - mae: 1.5294 - val_loss: 11.0993 - val_mse: 11.0993 - val_mae: 1.4211 - lr: 0.0014 - 10s/epoch - 10ms/step\n",
            "Epoch 27/100\n",
            "1000/1000 - 10s - loss: 12.3276 - mse: 12.3276 - mae: 1.5368 - val_loss: 11.3194 - val_mse: 11.3194 - val_mae: 1.7069 - lr: 0.0014 - 10s/epoch - 10ms/step\n",
            "Epoch 28/100\n",
            "1000/1000 - 10s - loss: 12.3778 - mse: 12.3778 - mae: 1.5303 - val_loss: 11.2102 - val_mse: 11.2102 - val_mae: 1.5093 - lr: 0.0014 - 10s/epoch - 10ms/step\n",
            "Epoch 29/100\n",
            "1000/1000 - 10s - loss: 12.3897 - mse: 12.3897 - mae: 1.5372 - val_loss: 11.0639 - val_mse: 11.0639 - val_mae: 1.5047 - lr: 0.0014 - 10s/epoch - 10ms/step\n",
            "Epoch 30/100\n",
            "1000/1000 - 10s - loss: 12.1789 - mse: 12.1789 - mae: 1.5189 - val_loss: 11.2147 - val_mse: 11.2147 - val_mae: 1.6622 - lr: 0.0014 - 10s/epoch - 10ms/step\n",
            "Epoch 31/100\n",
            "1000/1000 - 10s - loss: 12.2918 - mse: 12.2918 - mae: 1.5572 - val_loss: 11.1161 - val_mse: 11.1161 - val_mae: 1.6714 - lr: 0.0014 - 10s/epoch - 10ms/step\n",
            "Epoch 32/100\n",
            "1000/1000 - 10s - loss: 12.3134 - mse: 12.3134 - mae: 1.5325 - val_loss: 11.1080 - val_mse: 11.1080 - val_mae: 1.4012 - lr: 0.0014 - 10s/epoch - 10ms/step\n",
            "Epoch 33/100\n",
            "1000/1000 - 10s - loss: 12.2202 - mse: 12.2202 - mae: 1.5320 - val_loss: 11.4077 - val_mse: 11.4077 - val_mae: 1.7927 - lr: 0.0014 - 10s/epoch - 10ms/step\n",
            "Epoch 34/100\n",
            "1000/1000 - 11s - loss: 12.0416 - mse: 12.0416 - mae: 1.5115 - val_loss: 11.1784 - val_mse: 11.1784 - val_mae: 1.3863 - lr: 0.0014 - 11s/epoch - 11ms/step\n",
            "Score for fold 1: loss of 11.178400039672852\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 12s - loss: 11.4913 - mse: 11.4913 - mae: 1.5108 - val_loss: 12.4586 - val_mse: 12.4586 - val_mae: 1.6294 - lr: 0.0010 - 12s/epoch - 12ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 12s - loss: 11.2999 - mse: 11.2999 - mae: 1.4864 - val_loss: 12.5512 - val_mse: 12.5512 - val_mae: 1.4303 - lr: 0.0010 - 12s/epoch - 12ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 11s - loss: 11.1585 - mse: 11.1585 - mae: 1.4801 - val_loss: 13.5226 - val_mse: 13.5226 - val_mae: 2.0323 - lr: 0.0010 - 11s/epoch - 11ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 11s - loss: 11.1770 - mse: 11.1770 - mae: 1.4811 - val_loss: 12.4970 - val_mse: 12.4970 - val_mae: 1.6068 - lr: 0.0010 - 11s/epoch - 11ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 10s - loss: 11.0585 - mse: 11.0585 - mae: 1.4811 - val_loss: 12.5307 - val_mse: 12.5307 - val_mae: 1.4886 - lr: 0.0010 - 10s/epoch - 10ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 11s - loss: 11.1379 - mse: 11.1379 - mae: 1.4874 - val_loss: 12.7403 - val_mse: 12.7403 - val_mae: 1.3806 - lr: 0.0010 - 11s/epoch - 11ms/step\n",
            "Score for fold 2: loss of 12.740262031555176\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 12s - loss: 10.6978 - mse: 10.6978 - mae: 1.4899 - val_loss: 14.2533 - val_mse: 14.2533 - val_mae: 1.3261 - lr: 0.0010 - 12s/epoch - 12ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 11s - loss: 10.6045 - mse: 10.6045 - mae: 1.4867 - val_loss: 14.4402 - val_mse: 14.4402 - val_mae: 1.5361 - lr: 0.0010 - 11s/epoch - 11ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 11s - loss: 10.5808 - mse: 10.5808 - mae: 1.4966 - val_loss: 14.6840 - val_mse: 14.6840 - val_mae: 1.3400 - lr: 0.0010 - 11s/epoch - 11ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 11s - loss: 10.5870 - mse: 10.5870 - mae: 1.4844 - val_loss: 14.1105 - val_mse: 14.1105 - val_mae: 1.4711 - lr: 0.0010 - 11s/epoch - 11ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 11s - loss: 10.3588 - mse: 10.3588 - mae: 1.4841 - val_loss: 14.2797 - val_mse: 14.2797 - val_mae: 1.5704 - lr: 0.0010 - 11s/epoch - 11ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 10s - loss: 10.3643 - mse: 10.3643 - mae: 1.4788 - val_loss: 14.3865 - val_mse: 14.3865 - val_mae: 1.5992 - lr: 0.0010 - 10s/epoch - 10ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 11s - loss: 10.4057 - mse: 10.4057 - mae: 1.4841 - val_loss: 14.8470 - val_mse: 14.8470 - val_mae: 1.4942 - lr: 0.0010 - 11s/epoch - 11ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 11s - loss: 10.2523 - mse: 10.2523 - mae: 1.4826 - val_loss: 14.3609 - val_mse: 14.3609 - val_mae: 1.5796 - lr: 0.0010 - 11s/epoch - 11ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 11s - loss: 10.3389 - mse: 10.3389 - mae: 1.4950 - val_loss: 14.6618 - val_mse: 14.6618 - val_mae: 1.4035 - lr: 0.0010 - 11s/epoch - 11ms/step\n",
            "Score for fold 3: loss of 14.661774635314941\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 10s - loss: 11.7604 - mse: 11.7604 - mae: 1.4597 - val_loss: 8.2671 - val_mse: 8.2671 - val_mae: 1.3155 - lr: 0.0010 - 10s/epoch - 10ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 11s - loss: 11.7911 - mse: 11.7911 - mae: 1.4683 - val_loss: 9.2258 - val_mse: 9.2258 - val_mae: 1.3231 - lr: 0.0010 - 11s/epoch - 11ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 10s - loss: 11.7275 - mse: 11.7275 - mae: 1.4771 - val_loss: 8.5039 - val_mse: 8.5039 - val_mae: 1.3002 - lr: 0.0010 - 10s/epoch - 10ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 10s - loss: 11.6528 - mse: 11.6528 - mae: 1.4835 - val_loss: 8.7837 - val_mse: 8.7837 - val_mae: 1.5986 - lr: 0.0010 - 10s/epoch - 10ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 10s - loss: 11.6599 - mse: 11.6599 - mae: 1.4726 - val_loss: 8.8033 - val_mse: 8.8033 - val_mae: 1.4887 - lr: 0.0010 - 10s/epoch - 10ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 10s - loss: 11.4725 - mse: 11.4725 - mae: 1.4780 - val_loss: 8.8963 - val_mse: 8.8963 - val_mae: 1.3478 - lr: 0.0010 - 10s/epoch - 10ms/step\n",
            "Score for fold 4: loss of 8.896343231201172\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 10s - loss: 11.2126 - mse: 11.2126 - mae: 1.4761 - val_loss: 10.1334 - val_mse: 10.1334 - val_mae: 1.3564 - lr: 0.0010 - 10s/epoch - 10ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 10s - loss: 10.9434 - mse: 10.9434 - mae: 1.4728 - val_loss: 10.6506 - val_mse: 10.6506 - val_mae: 1.3972 - lr: 0.0010 - 10s/epoch - 10ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 10s - loss: 11.1105 - mse: 11.1105 - mae: 1.4865 - val_loss: 10.7180 - val_mse: 10.7180 - val_mae: 1.3496 - lr: 0.0010 - 10s/epoch - 10ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 10s - loss: 10.9700 - mse: 10.9700 - mae: 1.4600 - val_loss: 10.6982 - val_mse: 10.6982 - val_mae: 1.6371 - lr: 0.0010 - 10s/epoch - 10ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 11s - loss: 10.8954 - mse: 10.8954 - mae: 1.4647 - val_loss: 10.9292 - val_mse: 10.9292 - val_mae: 1.3438 - lr: 0.0010 - 11s/epoch - 11ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 10s - loss: 10.9351 - mse: 10.9351 - mae: 1.4476 - val_loss: 10.5480 - val_mse: 10.5480 - val_mae: 1.4036 - lr: 0.0010 - 10s/epoch - 10ms/step\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-10-12 10:55:06,151]\u001b[0m Finished trial#47 resulted in value: 11.606. Current best value is 8.668000000000001 with parameters: {'activation': 'relu', 'num_hidden_layer': 4, 'i': 10, 'learning_rate': 0.0001077579645735244}.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Score for fold 5: loss of 10.54798698425293\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 6s - loss: 14.1269 - mse: 14.1269 - mae: 1.5472 - val_loss: 9.0719 - val_mse: 9.0719 - val_mae: 1.5371 - lr: 4.6017e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 6s - loss: 13.3120 - mse: 13.3120 - mae: 1.4923 - val_loss: 8.6988 - val_mse: 8.6988 - val_mae: 1.4878 - lr: 4.6017e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 6s - loss: 13.0561 - mse: 13.0561 - mae: 1.4794 - val_loss: 8.7545 - val_mse: 8.7545 - val_mae: 1.4153 - lr: 4.6017e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 6s - loss: 12.8974 - mse: 12.8974 - mae: 1.4673 - val_loss: 8.5011 - val_mse: 8.5011 - val_mae: 1.4789 - lr: 4.6017e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 6s - loss: 12.8303 - mse: 12.8303 - mae: 1.4622 - val_loss: 9.0738 - val_mse: 9.0738 - val_mae: 1.5136 - lr: 4.6017e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 6s - loss: 12.6199 - mse: 12.6199 - mae: 1.4495 - val_loss: 8.6508 - val_mse: 8.6508 - val_mae: 1.4437 - lr: 4.6017e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 6s - loss: 12.4162 - mse: 12.4162 - mae: 1.4405 - val_loss: 9.2514 - val_mse: 9.2514 - val_mae: 1.4592 - lr: 4.6017e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 7s - loss: 12.2197 - mse: 12.2197 - mae: 1.4302 - val_loss: 8.6312 - val_mse: 8.6312 - val_mae: 1.4560 - lr: 4.6017e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 7s - loss: 12.1986 - mse: 12.1986 - mae: 1.4198 - val_loss: 8.8095 - val_mse: 8.8095 - val_mae: 1.4288 - lr: 4.6017e-04 - 7s/epoch - 7ms/step\n",
            "Score for fold 1: loss of 8.809490203857422\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 7s - loss: 12.0611 - mse: 12.0611 - mae: 1.4272 - val_loss: 8.6465 - val_mse: 8.6465 - val_mae: 1.4516 - lr: 4.6017e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 7s - loss: 11.7244 - mse: 11.7244 - mae: 1.4120 - val_loss: 8.2503 - val_mse: 8.2503 - val_mae: 1.4022 - lr: 4.6017e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 7s - loss: 11.6632 - mse: 11.6632 - mae: 1.4038 - val_loss: 9.0232 - val_mse: 9.0232 - val_mae: 1.4613 - lr: 4.6017e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 7s - loss: 11.5180 - mse: 11.5180 - mae: 1.3978 - val_loss: 9.3156 - val_mse: 9.3156 - val_mae: 1.4386 - lr: 4.6017e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 7s - loss: 11.1102 - mse: 11.1102 - mae: 1.3856 - val_loss: 8.6028 - val_mse: 8.6028 - val_mae: 1.5370 - lr: 4.6017e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 7s - loss: 11.1141 - mse: 11.1141 - mae: 1.3800 - val_loss: 9.0867 - val_mse: 9.0867 - val_mae: 1.4682 - lr: 4.6017e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 7s - loss: 10.7415 - mse: 10.7415 - mae: 1.3581 - val_loss: 8.8074 - val_mse: 8.8074 - val_mae: 1.4940 - lr: 4.6017e-04 - 7s/epoch - 7ms/step\n",
            "Score for fold 2: loss of 8.807405471801758\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 7s - loss: 8.8214 - mse: 8.8214 - mae: 1.3877 - val_loss: 17.1309 - val_mse: 17.1309 - val_mae: 1.3587 - lr: 4.6017e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 7s - loss: 8.7265 - mse: 8.7265 - mae: 1.3730 - val_loss: 17.0809 - val_mse: 17.0809 - val_mae: 1.3652 - lr: 4.6017e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 7s - loss: 8.4856 - mse: 8.4856 - mae: 1.3590 - val_loss: 17.1087 - val_mse: 17.1087 - val_mae: 1.4229 - lr: 4.6017e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 7s - loss: 8.0440 - mse: 8.0440 - mae: 1.3485 - val_loss: 17.0276 - val_mse: 17.0276 - val_mae: 1.3775 - lr: 4.6017e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 7s - loss: 8.0568 - mse: 8.0568 - mae: 1.3382 - val_loss: 17.7220 - val_mse: 17.7220 - val_mae: 1.4760 - lr: 4.6017e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 7s - loss: 7.8118 - mse: 7.8118 - mae: 1.3252 - val_loss: 18.0163 - val_mse: 18.0163 - val_mae: 1.4097 - lr: 4.6017e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 7s - loss: 7.3766 - mse: 7.3766 - mae: 1.3117 - val_loss: 17.8066 - val_mse: 17.8066 - val_mae: 1.4519 - lr: 4.6017e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 6s - loss: 7.3987 - mse: 7.3987 - mae: 1.3022 - val_loss: 17.6763 - val_mse: 17.6763 - val_mae: 1.4202 - lr: 4.6017e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 6s - loss: 7.2424 - mse: 7.2424 - mae: 1.2965 - val_loss: 18.0075 - val_mse: 18.0075 - val_mae: 1.5883 - lr: 4.6017e-04 - 6s/epoch - 6ms/step\n",
            "Score for fold 3: loss of 18.007503509521484\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 6s - loss: 9.2884 - mse: 9.2884 - mae: 1.3221 - val_loss: 9.9127 - val_mse: 9.9127 - val_mae: 1.2613 - lr: 4.6017e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 6s - loss: 9.3428 - mse: 9.3428 - mae: 1.3033 - val_loss: 9.7575 - val_mse: 9.7575 - val_mae: 1.3706 - lr: 4.6017e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 6s - loss: 8.5769 - mse: 8.5769 - mae: 1.2919 - val_loss: 10.0575 - val_mse: 10.0575 - val_mae: 1.2967 - lr: 4.6017e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 6s - loss: 8.3388 - mse: 8.3388 - mae: 1.2818 - val_loss: 10.0425 - val_mse: 10.0425 - val_mae: 1.3725 - lr: 4.6017e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 6s - loss: 8.2085 - mse: 8.2085 - mae: 1.2657 - val_loss: 10.3346 - val_mse: 10.3346 - val_mae: 1.3054 - lr: 4.6017e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 6s - loss: 8.1174 - mse: 8.1174 - mae: 1.2647 - val_loss: 10.3622 - val_mse: 10.3622 - val_mae: 1.3336 - lr: 4.6017e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 6s - loss: 7.9019 - mse: 7.9019 - mae: 1.2485 - val_loss: 10.7573 - val_mse: 10.7573 - val_mae: 1.2960 - lr: 4.6017e-04 - 6s/epoch - 6ms/step\n",
            "Score for fold 4: loss of 10.757275581359863\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 5s - loss: 9.4046 - mse: 9.4046 - mae: 1.2954 - val_loss: 5.1737 - val_mse: 5.1737 - val_mae: 1.2688 - lr: 4.6017e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 5s - loss: 8.9725 - mse: 8.9725 - mae: 1.2724 - val_loss: 5.5287 - val_mse: 5.5287 - val_mae: 1.1470 - lr: 4.6017e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 6s - loss: 8.8879 - mse: 8.8879 - mae: 1.2622 - val_loss: 5.7336 - val_mse: 5.7336 - val_mae: 1.1723 - lr: 4.6017e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 6s - loss: 8.5888 - mse: 8.5888 - mae: 1.2489 - val_loss: 5.8766 - val_mse: 5.8766 - val_mae: 1.2302 - lr: 4.6017e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 5s - loss: 8.4234 - mse: 8.4234 - mae: 1.2395 - val_loss: 5.7370 - val_mse: 5.7370 - val_mae: 1.2693 - lr: 4.6017e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 5s - loss: 8.3304 - mse: 8.3304 - mae: 1.2270 - val_loss: 6.3048 - val_mse: 6.3048 - val_mae: 1.2628 - lr: 4.6017e-04 - 5s/epoch - 5ms/step\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-10-12 10:59:11,697]\u001b[0m Finished trial#48 resulted in value: 10.538. Current best value is 8.668000000000001 with parameters: {'activation': 'relu', 'num_hidden_layer': 4, 'i': 10, 'learning_rate': 0.0001077579645735244}.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Score for fold 5: loss of 6.30478572845459\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 9s - loss: 13.7766 - mse: 13.7766 - mae: 1.6483 - val_loss: 14.6446 - val_mse: 14.6446 - val_mae: 1.5622 - lr: 2.3080e-04 - 9s/epoch - 9ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 9s - loss: 13.3071 - mse: 13.3071 - mae: 1.5725 - val_loss: 14.6800 - val_mse: 14.6800 - val_mae: 1.5752 - lr: 2.3080e-04 - 9s/epoch - 9ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 9s - loss: 13.1719 - mse: 13.1719 - mae: 1.5667 - val_loss: 14.6958 - val_mse: 14.6958 - val_mae: 1.5051 - lr: 2.3080e-04 - 9s/epoch - 9ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 9s - loss: 13.1952 - mse: 13.1952 - mae: 1.5586 - val_loss: 14.5537 - val_mse: 14.5537 - val_mae: 1.4927 - lr: 2.3080e-04 - 9s/epoch - 9ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 9s - loss: 12.9211 - mse: 12.9211 - mae: 1.5592 - val_loss: 14.6483 - val_mse: 14.6483 - val_mae: 1.5355 - lr: 2.3080e-04 - 9s/epoch - 9ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 9s - loss: 13.0369 - mse: 13.0369 - mae: 1.5607 - val_loss: 14.8640 - val_mse: 14.8640 - val_mae: 1.5447 - lr: 2.3080e-04 - 9s/epoch - 9ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 8s - loss: 12.9658 - mse: 12.9658 - mae: 1.5469 - val_loss: 14.7242 - val_mse: 14.7242 - val_mae: 1.5906 - lr: 2.3080e-04 - 8s/epoch - 8ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 9s - loss: 13.0456 - mse: 13.0456 - mae: 1.5586 - val_loss: 14.8051 - val_mse: 14.8051 - val_mae: 1.5733 - lr: 2.3080e-04 - 9s/epoch - 9ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 8s - loss: 12.9124 - mse: 12.9124 - mae: 1.5516 - val_loss: 14.6927 - val_mse: 14.6927 - val_mae: 1.5561 - lr: 2.3080e-04 - 8s/epoch - 8ms/step\n",
            "Score for fold 1: loss of 14.692697525024414\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 9s - loss: 13.8631 - mse: 13.8631 - mae: 1.5536 - val_loss: 11.1966 - val_mse: 11.1966 - val_mae: 1.5202 - lr: 2.3080e-04 - 9s/epoch - 9ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 9s - loss: 13.8715 - mse: 13.8715 - mae: 1.5582 - val_loss: 11.5095 - val_mse: 11.5095 - val_mae: 1.4630 - lr: 2.3080e-04 - 9s/epoch - 9ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 9s - loss: 13.7739 - mse: 13.7739 - mae: 1.5592 - val_loss: 11.1459 - val_mse: 11.1459 - val_mae: 1.5255 - lr: 2.3080e-04 - 9s/epoch - 9ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 9s - loss: 13.8255 - mse: 13.8255 - mae: 1.5513 - val_loss: 11.1559 - val_mse: 11.1559 - val_mae: 1.5568 - lr: 2.3080e-04 - 9s/epoch - 9ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 9s - loss: 13.7754 - mse: 13.7754 - mae: 1.5610 - val_loss: 11.2294 - val_mse: 11.2294 - val_mae: 1.5535 - lr: 2.3080e-04 - 9s/epoch - 9ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 10s - loss: 13.8189 - mse: 13.8189 - mae: 1.5601 - val_loss: 11.1740 - val_mse: 11.1740 - val_mae: 1.5412 - lr: 2.3080e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 10s - loss: 13.6139 - mse: 13.6139 - mae: 1.5526 - val_loss: 11.2258 - val_mse: 11.2258 - val_mae: 1.5269 - lr: 2.3080e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 11s - loss: 13.7944 - mse: 13.7944 - mae: 1.5603 - val_loss: 11.3894 - val_mse: 11.3894 - val_mae: 1.4777 - lr: 2.3080e-04 - 11s/epoch - 11ms/step\n",
            "Score for fold 2: loss of 11.389389038085938\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 11s - loss: 12.1453 - mse: 12.1453 - mae: 1.5530 - val_loss: 17.7802 - val_mse: 17.7802 - val_mae: 1.5126 - lr: 2.3080e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 11s - loss: 12.2962 - mse: 12.2962 - mae: 1.5606 - val_loss: 17.7895 - val_mse: 17.7895 - val_mae: 1.5107 - lr: 2.3080e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 11s - loss: 12.1855 - mse: 12.1855 - mae: 1.5589 - val_loss: 17.7734 - val_mse: 17.7734 - val_mae: 1.5023 - lr: 2.3080e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 10s - loss: 12.1721 - mse: 12.1721 - mae: 1.5517 - val_loss: 17.6647 - val_mse: 17.6647 - val_mae: 1.5996 - lr: 2.3080e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 10s - loss: 12.2104 - mse: 12.2104 - mae: 1.5585 - val_loss: 17.6989 - val_mse: 17.6989 - val_mae: 1.5909 - lr: 2.3080e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 10s - loss: 12.1658 - mse: 12.1658 - mae: 1.5550 - val_loss: 17.6883 - val_mse: 17.6883 - val_mae: 1.5772 - lr: 2.3080e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 10s - loss: 12.2044 - mse: 12.2044 - mae: 1.5574 - val_loss: 17.6542 - val_mse: 17.6542 - val_mae: 1.5531 - lr: 2.3080e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 10s - loss: 12.1717 - mse: 12.1717 - mae: 1.5511 - val_loss: 17.7907 - val_mse: 17.7907 - val_mae: 1.5104 - lr: 2.3080e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 10s - loss: 12.1128 - mse: 12.1128 - mae: 1.5579 - val_loss: 17.6500 - val_mse: 17.6500 - val_mae: 1.5614 - lr: 2.3080e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 11s - loss: 12.0570 - mse: 12.0570 - mae: 1.5568 - val_loss: 17.6859 - val_mse: 17.6859 - val_mae: 1.5855 - lr: 2.3080e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 11s - loss: 12.0496 - mse: 12.0496 - mae: 1.5516 - val_loss: 18.0728 - val_mse: 18.0728 - val_mae: 1.4797 - lr: 2.3080e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 10s - loss: 12.0658 - mse: 12.0658 - mae: 1.5575 - val_loss: 17.6558 - val_mse: 17.6558 - val_mae: 1.5694 - lr: 2.3080e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 13/100\n",
            "1000/1000 - 11s - loss: 12.0878 - mse: 12.0878 - mae: 1.5538 - val_loss: 17.8120 - val_mse: 17.8120 - val_mae: 1.5149 - lr: 2.3080e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 14/100\n",
            "1000/1000 - 11s - loss: 12.1855 - mse: 12.1855 - mae: 1.5564 - val_loss: 18.0526 - val_mse: 18.0526 - val_mae: 1.4949 - lr: 2.3080e-04 - 11s/epoch - 11ms/step\n",
            "Score for fold 3: loss of 18.05264663696289\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 10s - loss: 13.4324 - mse: 13.4324 - mae: 1.5428 - val_loss: 12.9398 - val_mse: 12.9398 - val_mae: 1.5291 - lr: 2.3080e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 11s - loss: 13.3375 - mse: 13.3375 - mae: 1.5362 - val_loss: 12.9224 - val_mse: 12.9224 - val_mae: 1.6022 - lr: 2.3080e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 11s - loss: 13.4032 - mse: 13.4032 - mae: 1.5388 - val_loss: 12.9323 - val_mse: 12.9323 - val_mae: 1.5458 - lr: 2.3080e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 11s - loss: 13.4071 - mse: 13.4071 - mae: 1.5427 - val_loss: 12.8923 - val_mse: 12.8923 - val_mae: 1.5832 - lr: 2.3080e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 11s - loss: 13.3687 - mse: 13.3687 - mae: 1.5430 - val_loss: 13.0679 - val_mse: 13.0679 - val_mae: 1.6543 - lr: 2.3080e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 11s - loss: 13.3335 - mse: 13.3335 - mae: 1.5393 - val_loss: 12.9081 - val_mse: 12.9081 - val_mae: 1.5882 - lr: 2.3080e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 10s - loss: 13.3371 - mse: 13.3371 - mae: 1.5406 - val_loss: 13.0161 - val_mse: 13.0161 - val_mae: 1.6240 - lr: 2.3080e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 10s - loss: 13.4050 - mse: 13.4050 - mae: 1.5428 - val_loss: 13.0144 - val_mse: 13.0144 - val_mae: 1.5994 - lr: 2.3080e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 10s - loss: 13.3576 - mse: 13.3576 - mae: 1.5414 - val_loss: 12.9131 - val_mse: 12.9131 - val_mae: 1.5451 - lr: 2.3080e-04 - 10s/epoch - 10ms/step\n",
            "Score for fold 4: loss of 12.913078308105469\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 10s - loss: 14.1023 - mse: 14.1023 - mae: 1.5495 - val_loss: 9.7224 - val_mse: 9.7224 - val_mae: 1.5520 - lr: 2.3080e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 11s - loss: 14.1828 - mse: 14.1828 - mae: 1.5534 - val_loss: 10.0824 - val_mse: 10.0824 - val_mae: 1.6618 - lr: 2.3080e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 11s - loss: 14.1219 - mse: 14.1219 - mae: 1.5546 - val_loss: 9.9209 - val_mse: 9.9209 - val_mae: 1.5395 - lr: 2.3080e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 11s - loss: 14.1663 - mse: 14.1663 - mae: 1.5497 - val_loss: 9.8208 - val_mse: 9.8208 - val_mae: 1.5290 - lr: 2.3080e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 11s - loss: 14.1578 - mse: 14.1578 - mae: 1.5544 - val_loss: 9.8611 - val_mse: 9.8611 - val_mae: 1.4911 - lr: 2.3080e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 11s - loss: 14.1055 - mse: 14.1055 - mae: 1.5492 - val_loss: 9.7399 - val_mse: 9.7399 - val_mae: 1.5372 - lr: 2.3080e-04 - 11s/epoch - 11ms/step\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-10-12 11:06:58,076]\u001b[0m Finished trial#49 resulted in value: 13.355999999999998. Current best value is 8.668000000000001 with parameters: {'activation': 'relu', 'num_hidden_layer': 4, 'i': 10, 'learning_rate': 0.0001077579645735244}.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Score for fold 5: loss of 9.739895820617676\n"
          ]
        }
      ],
      "source": [
        "model_list=[]\n",
        "history_list=[]\n",
        "study_name = 'NN_study_5'\n",
        "study = optuna.create_study(study_name=study_name, load_if_exists=True)\n",
        "func = lambda trial: objective(trial,training_shuffled4,labelsForTrain_shuffled4)\n",
        "study.optimize(func, n_trials=50,)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#{'activation': 'relu', 'num_hidden_layer': 4, 'i': 10, 'learning_rate': 0.0001077579645735244}.\n",
        "optimizer = Adam(learning_rate=0.0001077579645735244 ,clipnorm=1.0)\n",
        "model_5 = create_model(activation=\"relu\",num_hidden_layer=4,num_hidden_unit=1024)\n",
        "\n",
        "es = EarlyStopping(monitor='mse', patience=5)\n",
        "reduce_lr = ReduceLROnPlateau(monitor='val_mse', factor=0.2,\n",
        "                              patience=20, min_lr=0.001)\n",
        "model_5.compile(loss='mse',optimizer=optimizer,metrics=['mse','mae'])\n",
        "history = model_5.fit(training,labelsForTrain,\n",
        "                batch_size=64,\n",
        "                epochs=20,\n",
        "                verbose=2,\n",
        "                validation_data=(valing,labelsForVal),\n",
        "                validation_batch_size=64,\n",
        "                callbacks=[es,reduce_lr])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dI6V1X2t6ZoY",
        "outputId": "624cc6ba-daa5-4b98-898d-05c2fbcf88de"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "1250/1250 - 11s - loss: 13.1022 - mse: 13.1022 - mae: 1.5362 - val_loss: 10.0333 - val_mse: 10.0333 - val_mae: 1.4624 - lr: 1.0776e-04 - 11s/epoch - 8ms/step\n",
            "Epoch 2/20\n",
            "1250/1250 - 9s - loss: 12.2175 - mse: 12.2175 - mae: 1.4767 - val_loss: 9.5300 - val_mse: 9.5300 - val_mae: 1.4798 - lr: 1.0776e-04 - 9s/epoch - 7ms/step\n",
            "Epoch 3/20\n",
            "1250/1250 - 9s - loss: 12.0146 - mse: 12.0146 - mae: 1.4637 - val_loss: 10.2994 - val_mse: 10.2994 - val_mae: 1.4243 - lr: 1.0776e-04 - 9s/epoch - 8ms/step\n",
            "Epoch 4/20\n",
            "1250/1250 - 10s - loss: 12.0255 - mse: 12.0255 - mae: 1.4528 - val_loss: 9.5157 - val_mse: 9.5157 - val_mae: 1.5997 - lr: 1.0776e-04 - 10s/epoch - 8ms/step\n",
            "Epoch 5/20\n",
            "1250/1250 - 9s - loss: 11.7889 - mse: 11.7889 - mae: 1.4467 - val_loss: 9.8825 - val_mse: 9.8825 - val_mae: 1.4775 - lr: 1.0776e-04 - 9s/epoch - 8ms/step\n",
            "Epoch 6/20\n",
            "1250/1250 - 9s - loss: 11.5922 - mse: 11.5922 - mae: 1.4383 - val_loss: 9.2051 - val_mse: 9.2051 - val_mae: 1.4826 - lr: 1.0776e-04 - 9s/epoch - 8ms/step\n",
            "Epoch 7/20\n",
            "1250/1250 - 9s - loss: 11.5508 - mse: 11.5508 - mae: 1.4308 - val_loss: 8.9127 - val_mse: 8.9127 - val_mae: 1.4783 - lr: 1.0776e-04 - 9s/epoch - 8ms/step\n",
            "Epoch 8/20\n",
            "1250/1250 - 10s - loss: 11.4708 - mse: 11.4708 - mae: 1.4203 - val_loss: 9.7348 - val_mse: 9.7348 - val_mae: 1.4626 - lr: 1.0776e-04 - 10s/epoch - 8ms/step\n",
            "Epoch 9/20\n",
            "1250/1250 - 10s - loss: 11.2309 - mse: 11.2309 - mae: 1.4106 - val_loss: 9.6807 - val_mse: 9.6807 - val_mae: 1.5638 - lr: 1.0776e-04 - 10s/epoch - 8ms/step\n",
            "Epoch 10/20\n",
            "1250/1250 - 9s - loss: 11.0055 - mse: 11.0055 - mae: 1.4004 - val_loss: 9.5229 - val_mse: 9.5229 - val_mae: 1.4667 - lr: 1.0776e-04 - 9s/epoch - 8ms/step\n",
            "Epoch 11/20\n",
            "1250/1250 - 9s - loss: 11.0568 - mse: 11.0568 - mae: 1.3906 - val_loss: 9.6779 - val_mse: 9.6779 - val_mae: 1.5010 - lr: 1.0776e-04 - 9s/epoch - 8ms/step\n",
            "Epoch 12/20\n",
            "1250/1250 - 9s - loss: 10.6456 - mse: 10.6456 - mae: 1.3754 - val_loss: 8.7884 - val_mse: 8.7884 - val_mae: 1.5083 - lr: 1.0776e-04 - 9s/epoch - 8ms/step\n",
            "Epoch 13/20\n",
            "1250/1250 - 9s - loss: 10.3279 - mse: 10.3279 - mae: 1.3621 - val_loss: 9.4814 - val_mse: 9.4814 - val_mae: 1.4515 - lr: 1.0776e-04 - 9s/epoch - 8ms/step\n",
            "Epoch 14/20\n",
            "1250/1250 - 9s - loss: 10.0859 - mse: 10.0859 - mae: 1.3471 - val_loss: 9.3390 - val_mse: 9.3390 - val_mae: 1.4880 - lr: 1.0776e-04 - 9s/epoch - 8ms/step\n",
            "Epoch 15/20\n",
            "1250/1250 - 10s - loss: 10.0929 - mse: 10.0929 - mae: 1.3351 - val_loss: 9.2142 - val_mse: 9.2142 - val_mae: 1.5293 - lr: 1.0776e-04 - 10s/epoch - 8ms/step\n",
            "Epoch 16/20\n",
            "1250/1250 - 10s - loss: 9.7953 - mse: 9.7953 - mae: 1.3214 - val_loss: 9.0047 - val_mse: 9.0047 - val_mae: 1.5008 - lr: 1.0776e-04 - 10s/epoch - 8ms/step\n",
            "Epoch 17/20\n",
            "1250/1250 - 10s - loss: 9.2672 - mse: 9.2672 - mae: 1.2957 - val_loss: 9.6610 - val_mse: 9.6610 - val_mae: 1.5655 - lr: 1.0776e-04 - 10s/epoch - 8ms/step\n",
            "Epoch 18/20\n",
            "1250/1250 - 10s - loss: 9.0063 - mse: 9.0063 - mae: 1.2778 - val_loss: 10.5173 - val_mse: 10.5173 - val_mae: 1.4957 - lr: 1.0776e-04 - 10s/epoch - 8ms/step\n",
            "Epoch 19/20\n",
            "1250/1250 - 10s - loss: 8.7593 - mse: 8.7593 - mae: 1.2638 - val_loss: 9.3966 - val_mse: 9.3966 - val_mae: 1.5014 - lr: 1.0776e-04 - 10s/epoch - 8ms/step\n",
            "Epoch 20/20\n",
            "1250/1250 - 10s - loss: 8.4944 - mse: 8.4944 - mae: 1.2430 - val_loss: 9.8652 - val_mse: 9.8652 - val_mae: 1.5739 - lr: 1.0776e-04 - 10s/epoch - 8ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "results_model5 = model_5.evaluate(testing, labelsForTest, batch_size=32)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qMw3_s1a7iev",
        "outputId": "e7db7f0e-4daf-46e3-f8b3-cd2290090280"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "313/313 [==============================] - 1s 2ms/step - loss: 11.0216 - mse: 11.0216 - mae: 1.5538\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PIN_hslsf2Xw"
      },
      "source": [
        "##Shuffle Repetation 6"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t2JRYxcVf0K_"
      },
      "outputs": [],
      "source": [
        "# Shuffle the data after train\n",
        "shuffled5 = shuffle(train_df, random_state=5)\n",
        "training_shuffled5,labelsForTrain_shuffled5=process_shuffle_dataset(shuffled5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "WtAarMOHf6v_",
        "outputId": "d8adb001-cf3d-4da0-c009-dfa832bb023d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 3s - loss: 14.8427 - mse: 14.8427 - mae: 1.6359 - val_loss: 10.8482 - val_mse: 10.8482 - val_mae: 1.5431 - lr: 3.6229e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 14.0354 - mse: 14.0354 - mae: 1.5639 - val_loss: 10.7369 - val_mse: 10.7369 - val_mae: 1.5762 - lr: 3.6229e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 14.0106 - mse: 14.0106 - mae: 1.5559 - val_loss: 10.6725 - val_mse: 10.6725 - val_mae: 1.5345 - lr: 3.6229e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 14.0044 - mse: 14.0044 - mae: 1.5553 - val_loss: 10.5252 - val_mse: 10.5252 - val_mae: 1.5520 - lr: 3.6229e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 14.0021 - mse: 14.0021 - mae: 1.5557 - val_loss: 10.5133 - val_mse: 10.5133 - val_mae: 1.5539 - lr: 3.6229e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 13.8690 - mse: 13.8690 - mae: 1.5503 - val_loss: 10.6462 - val_mse: 10.6462 - val_mae: 1.5535 - lr: 3.6229e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 14.0394 - mse: 14.0394 - mae: 1.5536 - val_loss: 10.5921 - val_mse: 10.5921 - val_mae: 1.5345 - lr: 3.6229e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 2s - loss: 14.0050 - mse: 14.0050 - mae: 1.5528 - val_loss: 10.6011 - val_mse: 10.6011 - val_mae: 1.5149 - lr: 3.6229e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 2s - loss: 13.9944 - mse: 13.9944 - mae: 1.5500 - val_loss: 10.9254 - val_mse: 10.9254 - val_mae: 1.5191 - lr: 3.6229e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 2s - loss: 13.8887 - mse: 13.8887 - mae: 1.5513 - val_loss: 10.6299 - val_mse: 10.6299 - val_mae: 1.5135 - lr: 3.6229e-04 - 2s/epoch - 2ms/step\n",
            "Score for fold 1: loss of 10.629863739013672\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 12.5444 - mse: 12.5444 - mae: 1.5480 - val_loss: 15.8632 - val_mse: 15.8632 - val_mae: 1.5354 - lr: 3.6229e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 12.6452 - mse: 12.6452 - mae: 1.5491 - val_loss: 15.8945 - val_mse: 15.8945 - val_mae: 1.5313 - lr: 3.6229e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 12.6460 - mse: 12.6460 - mae: 1.5449 - val_loss: 15.8774 - val_mse: 15.8774 - val_mae: 1.5406 - lr: 3.6229e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 12.5403 - mse: 12.5403 - mae: 1.5445 - val_loss: 15.9162 - val_mse: 15.9162 - val_mae: 1.5702 - lr: 3.6229e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 12.6087 - mse: 12.6087 - mae: 1.5467 - val_loss: 15.8569 - val_mse: 15.8569 - val_mae: 1.5803 - lr: 3.6229e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 12.5462 - mse: 12.5462 - mae: 1.5475 - val_loss: 15.8923 - val_mse: 15.8923 - val_mae: 1.5426 - lr: 3.6229e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 12.5809 - mse: 12.5809 - mae: 1.5468 - val_loss: 15.8883 - val_mse: 15.8883 - val_mae: 1.5677 - lr: 3.6229e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 2s - loss: 12.5600 - mse: 12.5600 - mae: 1.5447 - val_loss: 15.9521 - val_mse: 15.9521 - val_mae: 1.5772 - lr: 3.6229e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 2s - loss: 12.5946 - mse: 12.5946 - mae: 1.5489 - val_loss: 15.8583 - val_mse: 15.8583 - val_mae: 1.5467 - lr: 3.6229e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 2s - loss: 12.5523 - mse: 12.5523 - mae: 1.5469 - val_loss: 15.9113 - val_mse: 15.9113 - val_mae: 1.5688 - lr: 3.6229e-04 - 2s/epoch - 2ms/step\n",
            "Score for fold 2: loss of 15.911271095275879\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 13.7674 - mse: 13.7674 - mae: 1.5466 - val_loss: 10.9933 - val_mse: 10.9933 - val_mae: 1.5443 - lr: 3.6229e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 13.8077 - mse: 13.8077 - mae: 1.5474 - val_loss: 11.0158 - val_mse: 11.0158 - val_mae: 1.5424 - lr: 3.6229e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 13.8166 - mse: 13.8166 - mae: 1.5470 - val_loss: 11.0252 - val_mse: 11.0252 - val_mae: 1.5516 - lr: 3.6229e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 13.8298 - mse: 13.8298 - mae: 1.5490 - val_loss: 10.9726 - val_mse: 10.9726 - val_mae: 1.5369 - lr: 3.6229e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 13.7418 - mse: 13.7418 - mae: 1.5434 - val_loss: 10.9767 - val_mse: 10.9767 - val_mae: 1.5600 - lr: 3.6229e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 13.7681 - mse: 13.7681 - mae: 1.5430 - val_loss: 11.0607 - val_mse: 11.0607 - val_mae: 1.5555 - lr: 3.6229e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 13.8212 - mse: 13.8212 - mae: 1.5476 - val_loss: 11.1337 - val_mse: 11.1337 - val_mae: 1.5183 - lr: 3.6229e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 2s - loss: 13.8251 - mse: 13.8251 - mae: 1.5470 - val_loss: 11.0484 - val_mse: 11.0484 - val_mae: 1.5387 - lr: 3.6229e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 2s - loss: 13.7948 - mse: 13.7948 - mae: 1.5449 - val_loss: 11.0008 - val_mse: 11.0008 - val_mae: 1.5434 - lr: 3.6229e-04 - 2s/epoch - 2ms/step\n",
            "Score for fold 3: loss of 11.000782012939453\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 12.2995 - mse: 12.2995 - mae: 1.5533 - val_loss: 16.9488 - val_mse: 16.9488 - val_mae: 1.5433 - lr: 3.6229e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 12.3178 - mse: 12.3178 - mae: 1.5537 - val_loss: 17.0280 - val_mse: 17.0280 - val_mae: 1.5679 - lr: 3.6229e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 12.3110 - mse: 12.3110 - mae: 1.5513 - val_loss: 16.8558 - val_mse: 16.8558 - val_mae: 1.5284 - lr: 3.6229e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 12.2595 - mse: 12.2595 - mae: 1.5553 - val_loss: 16.9320 - val_mse: 16.9320 - val_mae: 1.5315 - lr: 3.6229e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 12.2917 - mse: 12.2917 - mae: 1.5514 - val_loss: 16.8733 - val_mse: 16.8733 - val_mae: 1.5331 - lr: 3.6229e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 12.2566 - mse: 12.2566 - mae: 1.5544 - val_loss: 17.0109 - val_mse: 17.0109 - val_mae: 1.5436 - lr: 3.6229e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 12.2857 - mse: 12.2857 - mae: 1.5520 - val_loss: 17.0039 - val_mse: 17.0039 - val_mae: 1.5279 - lr: 3.6229e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 2s - loss: 12.2801 - mse: 12.2801 - mae: 1.5472 - val_loss: 16.8504 - val_mse: 16.8504 - val_mae: 1.5162 - lr: 3.6229e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 2s - loss: 12.2577 - mse: 12.2577 - mae: 1.5508 - val_loss: 16.8295 - val_mse: 16.8295 - val_mae: 1.4953 - lr: 3.6229e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 2s - loss: 12.2416 - mse: 12.2416 - mae: 1.5520 - val_loss: 16.8999 - val_mse: 16.8999 - val_mae: 1.5301 - lr: 3.6229e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 2s - loss: 12.2308 - mse: 12.2308 - mae: 1.5546 - val_loss: 17.0094 - val_mse: 17.0094 - val_mae: 1.5417 - lr: 3.6229e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 2s - loss: 12.2549 - mse: 12.2549 - mae: 1.5472 - val_loss: 16.8230 - val_mse: 16.8230 - val_mae: 1.5088 - lr: 3.6229e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 13/100\n",
            "1000/1000 - 2s - loss: 12.3066 - mse: 12.3066 - mae: 1.5572 - val_loss: 16.8377 - val_mse: 16.8377 - val_mae: 1.4946 - lr: 3.6229e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 14/100\n",
            "1000/1000 - 2s - loss: 12.2734 - mse: 12.2734 - mae: 1.5522 - val_loss: 16.8555 - val_mse: 16.8555 - val_mae: 1.5203 - lr: 3.6229e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 15/100\n",
            "1000/1000 - 2s - loss: 12.2826 - mse: 12.2826 - mae: 1.5543 - val_loss: 16.8744 - val_mse: 16.8744 - val_mae: 1.5243 - lr: 3.6229e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 16/100\n",
            "1000/1000 - 2s - loss: 12.2524 - mse: 12.2524 - mae: 1.5514 - val_loss: 16.9274 - val_mse: 16.9274 - val_mae: 1.5354 - lr: 3.6229e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 17/100\n",
            "1000/1000 - 2s - loss: 12.2697 - mse: 12.2697 - mae: 1.5540 - val_loss: 16.8727 - val_mse: 16.8727 - val_mae: 1.5367 - lr: 3.6229e-04 - 2s/epoch - 2ms/step\n",
            "Score for fold 4: loss of 16.872724533081055\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 13.6303 - mse: 13.6303 - mae: 1.5438 - val_loss: 11.7029 - val_mse: 11.7029 - val_mae: 1.5432 - lr: 3.6229e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 13.6403 - mse: 13.6403 - mae: 1.5400 - val_loss: 11.7285 - val_mse: 11.7285 - val_mae: 1.5461 - lr: 3.6229e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 13.5530 - mse: 13.5530 - mae: 1.5383 - val_loss: 11.8312 - val_mse: 11.8312 - val_mae: 1.5092 - lr: 3.6229e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 13.5492 - mse: 13.5492 - mae: 1.5427 - val_loss: 11.8387 - val_mse: 11.8387 - val_mae: 1.5946 - lr: 3.6229e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 13.5545 - mse: 13.5545 - mae: 1.5405 - val_loss: 12.1114 - val_mse: 12.1114 - val_mae: 1.6414 - lr: 3.6229e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 13.6100 - mse: 13.6100 - mae: 1.5384 - val_loss: 11.7522 - val_mse: 11.7522 - val_mae: 1.5574 - lr: 3.6229e-04 - 2s/epoch - 2ms/step\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-10-12 11:14:14,657]\u001b[0m Finished trial#0 resulted in value: 13.232. Current best value is 13.232 with parameters: {'activation': 'linear', 'num_hidden_layer': 4, 'i': 6, 'learning_rate': 0.00036229473045514004}.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Score for fold 5: loss of 11.752156257629395\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 3s - loss: 14.0677 - mse: 14.0677 - mae: 1.5606 - val_loss: 9.5006 - val_mse: 9.5006 - val_mae: 1.5671 - lr: 0.0033 - 3s/epoch - 3ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 13.3055 - mse: 13.3055 - mae: 1.5107 - val_loss: 9.3695 - val_mse: 9.3695 - val_mae: 1.4912 - lr: 0.0033 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 13.0639 - mse: 13.0639 - mae: 1.4929 - val_loss: 9.7589 - val_mse: 9.7589 - val_mae: 1.4590 - lr: 0.0033 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 13.2087 - mse: 13.2087 - mae: 1.4888 - val_loss: 9.2995 - val_mse: 9.2995 - val_mae: 1.4715 - lr: 0.0033 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 12.9335 - mse: 12.9335 - mae: 1.4805 - val_loss: 9.1997 - val_mse: 9.1997 - val_mae: 1.4564 - lr: 0.0033 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 12.8959 - mse: 12.8959 - mae: 1.4748 - val_loss: 9.1954 - val_mse: 9.1954 - val_mae: 1.4249 - lr: 0.0033 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 12.9046 - mse: 12.9046 - mae: 1.4690 - val_loss: 9.5555 - val_mse: 9.5555 - val_mae: 1.5054 - lr: 0.0033 - 2s/epoch - 2ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 2s - loss: 12.7624 - mse: 12.7624 - mae: 1.4661 - val_loss: 9.0676 - val_mse: 9.0676 - val_mae: 1.4844 - lr: 0.0033 - 2s/epoch - 2ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 2s - loss: 12.4190 - mse: 12.4190 - mae: 1.4608 - val_loss: 9.2774 - val_mse: 9.2774 - val_mae: 1.4360 - lr: 0.0033 - 2s/epoch - 2ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 2s - loss: 12.7857 - mse: 12.7857 - mae: 1.4575 - val_loss: 9.2249 - val_mse: 9.2249 - val_mae: 1.4209 - lr: 0.0033 - 2s/epoch - 2ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 2s - loss: 12.3062 - mse: 12.3062 - mae: 1.4497 - val_loss: 9.0915 - val_mse: 9.0915 - val_mae: 1.4418 - lr: 0.0033 - 2s/epoch - 2ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 2s - loss: 12.3699 - mse: 12.3699 - mae: 1.4471 - val_loss: 9.1242 - val_mse: 9.1242 - val_mae: 1.4619 - lr: 0.0033 - 2s/epoch - 2ms/step\n",
            "Epoch 13/100\n",
            "1000/1000 - 2s - loss: 12.4897 - mse: 12.4897 - mae: 1.4476 - val_loss: 9.0903 - val_mse: 9.0903 - val_mae: 1.4242 - lr: 0.0033 - 2s/epoch - 2ms/step\n",
            "Score for fold 1: loss of 9.090291023254395\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 9.7065 - mse: 9.7065 - mae: 1.4236 - val_loss: 17.7004 - val_mse: 17.7004 - val_mae: 1.4452 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 9.5775 - mse: 9.5775 - mae: 1.4178 - val_loss: 17.6207 - val_mse: 17.6207 - val_mae: 1.4564 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 9.4527 - mse: 9.4527 - mae: 1.4132 - val_loss: 17.6246 - val_mse: 17.6246 - val_mae: 1.4526 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 9.4042 - mse: 9.4042 - mae: 1.4092 - val_loss: 17.6039 - val_mse: 17.6039 - val_mae: 1.4634 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 9.3022 - mse: 9.3022 - mae: 1.4065 - val_loss: 17.6170 - val_mse: 17.6170 - val_mae: 1.4547 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 9.3021 - mse: 9.3021 - mae: 1.4038 - val_loss: 17.7210 - val_mse: 17.7210 - val_mae: 1.4832 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 9.2156 - mse: 9.2156 - mae: 1.4017 - val_loss: 17.6517 - val_mse: 17.6517 - val_mae: 1.5068 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 2s - loss: 9.2060 - mse: 9.2060 - mae: 1.3988 - val_loss: 17.9044 - val_mse: 17.9044 - val_mae: 1.4190 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 2s - loss: 9.1131 - mse: 9.1131 - mae: 1.3975 - val_loss: 17.7978 - val_mse: 17.7978 - val_mae: 1.4334 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Score for fold 2: loss of 17.797819137573242\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 11.1944 - mse: 11.1944 - mae: 1.4065 - val_loss: 10.1048 - val_mse: 10.1048 - val_mae: 1.4203 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 11.2029 - mse: 11.2029 - mae: 1.4040 - val_loss: 9.0712 - val_mse: 9.0712 - val_mae: 1.4596 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 11.0990 - mse: 11.0990 - mae: 1.4001 - val_loss: 9.4283 - val_mse: 9.4283 - val_mae: 1.3773 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 11.0848 - mse: 11.0848 - mae: 1.3983 - val_loss: 9.1942 - val_mse: 9.1942 - val_mae: 1.4001 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 11.0139 - mse: 11.0139 - mae: 1.3952 - val_loss: 9.2604 - val_mse: 9.2604 - val_mae: 1.3938 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 10.9328 - mse: 10.9328 - mae: 1.3887 - val_loss: 9.1224 - val_mse: 9.1224 - val_mae: 1.4141 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 10.9051 - mse: 10.9051 - mae: 1.3894 - val_loss: 9.2758 - val_mse: 9.2758 - val_mae: 1.3772 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Score for fold 3: loss of 9.275822639465332\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 11.5956 - mse: 11.5956 - mae: 1.4041 - val_loss: 7.0415 - val_mse: 7.0415 - val_mae: 1.3729 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 11.5262 - mse: 11.5262 - mae: 1.4005 - val_loss: 6.3718 - val_mse: 6.3718 - val_mae: 1.3787 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 11.4384 - mse: 11.4384 - mae: 1.3971 - val_loss: 6.6760 - val_mse: 6.6760 - val_mae: 1.3492 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 11.3386 - mse: 11.3386 - mae: 1.3908 - val_loss: 6.5703 - val_mse: 6.5703 - val_mae: 1.3477 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 11.2622 - mse: 11.2622 - mae: 1.3862 - val_loss: 6.9119 - val_mse: 6.9119 - val_mae: 1.3792 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 11.2737 - mse: 11.2737 - mae: 1.3874 - val_loss: 6.9637 - val_mse: 6.9637 - val_mae: 1.3770 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 11.1760 - mse: 11.1760 - mae: 1.3819 - val_loss: 7.1010 - val_mse: 7.1010 - val_mae: 1.3730 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Score for fold 4: loss of 7.100975513458252\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 9.9782 - mse: 9.9782 - mae: 1.3816 - val_loss: 11.2202 - val_mse: 11.2202 - val_mae: 1.3845 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 9.9964 - mse: 9.9964 - mae: 1.3792 - val_loss: 11.4709 - val_mse: 11.4709 - val_mae: 1.3788 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 9.8577 - mse: 9.8577 - mae: 1.3725 - val_loss: 11.8451 - val_mse: 11.8451 - val_mae: 1.4207 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 9.9451 - mse: 9.9451 - mae: 1.3723 - val_loss: 11.8014 - val_mse: 11.8014 - val_mae: 1.3909 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 9.8104 - mse: 9.8104 - mae: 1.3692 - val_loss: 11.8655 - val_mse: 11.8655 - val_mae: 1.4109 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 9.8008 - mse: 9.8008 - mae: 1.3615 - val_loss: 11.8811 - val_mse: 11.8811 - val_mae: 1.3704 - lr: 0.0010 - 2s/epoch - 2ms/step\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-10-12 11:15:38,385]\u001b[0m Finished trial#1 resulted in value: 11.030000000000001. Current best value is 11.030000000000001 with parameters: {'activation': 'relu', 'num_hidden_layer': 4, 'i': 6, 'learning_rate': 0.0033229492757648315}.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Score for fold 5: loss of 11.88109016418457\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 5s - loss: 15.8831 - mse: 15.8831 - mae: 1.6083 - val_loss: 9.0867 - val_mse: 9.0867 - val_mae: 1.5194 - lr: 2.3838e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 4s - loss: 14.8319 - mse: 14.8319 - mae: 1.5167 - val_loss: 8.6180 - val_mse: 8.6180 - val_mae: 1.4849 - lr: 2.3838e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 4s - loss: 14.6342 - mse: 14.6342 - mae: 1.5039 - val_loss: 8.5508 - val_mse: 8.5508 - val_mae: 1.4598 - lr: 2.3838e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 4s - loss: 14.5109 - mse: 14.5109 - mae: 1.4956 - val_loss: 8.6024 - val_mse: 8.6024 - val_mae: 1.4498 - lr: 2.3838e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 4s - loss: 14.4225 - mse: 14.4225 - mae: 1.4912 - val_loss: 8.4174 - val_mse: 8.4174 - val_mae: 1.4457 - lr: 2.3838e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 4s - loss: 14.3267 - mse: 14.3267 - mae: 1.4883 - val_loss: 8.4437 - val_mse: 8.4437 - val_mae: 1.4548 - lr: 2.3838e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 4s - loss: 14.2220 - mse: 14.2220 - mae: 1.4794 - val_loss: 8.6143 - val_mse: 8.6143 - val_mae: 1.4346 - lr: 2.3838e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 4s - loss: 14.1664 - mse: 14.1664 - mae: 1.4747 - val_loss: 8.2798 - val_mse: 8.2798 - val_mae: 1.4820 - lr: 2.3838e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 4s - loss: 14.0361 - mse: 14.0361 - mae: 1.4728 - val_loss: 8.3422 - val_mse: 8.3422 - val_mae: 1.4865 - lr: 2.3838e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 4s - loss: 13.9555 - mse: 13.9555 - mae: 1.4679 - val_loss: 8.3812 - val_mse: 8.3812 - val_mae: 1.4557 - lr: 2.3838e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 4s - loss: 13.8743 - mse: 13.8743 - mae: 1.4653 - val_loss: 8.2610 - val_mse: 8.2610 - val_mae: 1.4822 - lr: 2.3838e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 4s - loss: 13.8185 - mse: 13.8185 - mae: 1.4641 - val_loss: 8.1528 - val_mse: 8.1528 - val_mae: 1.5182 - lr: 2.3838e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 13/100\n",
            "1000/1000 - 4s - loss: 13.7156 - mse: 13.7156 - mae: 1.4588 - val_loss: 8.2751 - val_mse: 8.2751 - val_mae: 1.4894 - lr: 2.3838e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 14/100\n",
            "1000/1000 - 4s - loss: 13.6638 - mse: 13.6638 - mae: 1.4544 - val_loss: 8.0437 - val_mse: 8.0437 - val_mae: 1.4491 - lr: 2.3838e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 15/100\n",
            "1000/1000 - 4s - loss: 13.5312 - mse: 13.5312 - mae: 1.4521 - val_loss: 8.2059 - val_mse: 8.2059 - val_mae: 1.4827 - lr: 2.3838e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 16/100\n",
            "1000/1000 - 4s - loss: 13.5241 - mse: 13.5241 - mae: 1.4515 - val_loss: 8.2950 - val_mse: 8.2950 - val_mae: 1.4384 - lr: 2.3838e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 17/100\n",
            "1000/1000 - 4s - loss: 13.4328 - mse: 13.4328 - mae: 1.4451 - val_loss: 8.1875 - val_mse: 8.1875 - val_mae: 1.4425 - lr: 2.3838e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 18/100\n",
            "1000/1000 - 4s - loss: 13.3639 - mse: 13.3639 - mae: 1.4454 - val_loss: 8.0850 - val_mse: 8.0850 - val_mae: 1.4623 - lr: 2.3838e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 19/100\n",
            "1000/1000 - 4s - loss: 13.2762 - mse: 13.2762 - mae: 1.4361 - val_loss: 8.2127 - val_mse: 8.2127 - val_mae: 1.4713 - lr: 2.3838e-04 - 4s/epoch - 4ms/step\n",
            "Score for fold 1: loss of 8.212689399719238\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 4s - loss: 12.8786 - mse: 12.8786 - mae: 1.4528 - val_loss: 9.6888 - val_mse: 9.6888 - val_mae: 1.4156 - lr: 2.3838e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 4s - loss: 12.7777 - mse: 12.7777 - mae: 1.4455 - val_loss: 9.6486 - val_mse: 9.6486 - val_mae: 1.4355 - lr: 2.3838e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 4s - loss: 12.7026 - mse: 12.7026 - mae: 1.4404 - val_loss: 9.7324 - val_mse: 9.7324 - val_mae: 1.4076 - lr: 2.3838e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 4s - loss: 12.6392 - mse: 12.6392 - mae: 1.4344 - val_loss: 9.7139 - val_mse: 9.7139 - val_mae: 1.4303 - lr: 2.3838e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 4s - loss: 12.5379 - mse: 12.5379 - mae: 1.4251 - val_loss: 9.8335 - val_mse: 9.8335 - val_mae: 1.4682 - lr: 2.3838e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 4s - loss: 12.4166 - mse: 12.4166 - mae: 1.4258 - val_loss: 9.8042 - val_mse: 9.8042 - val_mae: 1.4493 - lr: 2.3838e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 4s - loss: 12.3470 - mse: 12.3470 - mae: 1.4188 - val_loss: 9.7404 - val_mse: 9.7404 - val_mae: 1.4383 - lr: 2.3838e-04 - 4s/epoch - 4ms/step\n",
            "Score for fold 2: loss of 9.740378379821777\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 4s - loss: 9.5161 - mse: 9.5161 - mae: 1.4165 - val_loss: 20.8927 - val_mse: 20.8927 - val_mae: 1.4489 - lr: 2.3838e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 4s - loss: 9.4402 - mse: 9.4402 - mae: 1.4069 - val_loss: 20.7670 - val_mse: 20.7670 - val_mae: 1.4409 - lr: 2.3838e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 4s - loss: 9.3481 - mse: 9.3481 - mae: 1.3989 - val_loss: 20.8269 - val_mse: 20.8269 - val_mae: 1.4455 - lr: 2.3838e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 4s - loss: 9.2621 - mse: 9.2621 - mae: 1.3961 - val_loss: 20.9773 - val_mse: 20.9773 - val_mae: 1.4471 - lr: 2.3838e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 4s - loss: 9.2101 - mse: 9.2101 - mae: 1.3939 - val_loss: 20.8795 - val_mse: 20.8795 - val_mae: 1.4522 - lr: 2.3838e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 4s - loss: 9.1230 - mse: 9.1230 - mae: 1.3879 - val_loss: 20.9649 - val_mse: 20.9649 - val_mae: 1.4637 - lr: 2.3838e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 4s - loss: 9.0352 - mse: 9.0352 - mae: 1.3830 - val_loss: 20.9227 - val_mse: 20.9227 - val_mae: 1.4533 - lr: 2.3838e-04 - 4s/epoch - 4ms/step\n",
            "Score for fold 3: loss of 20.92267417907715\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 4s - loss: 12.0479 - mse: 12.0479 - mae: 1.4152 - val_loss: 8.5920 - val_mse: 8.5920 - val_mae: 1.3392 - lr: 2.3838e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 4s - loss: 11.9005 - mse: 11.9005 - mae: 1.4092 - val_loss: 8.7816 - val_mse: 8.7816 - val_mae: 1.3479 - lr: 2.3838e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 4s - loss: 11.8271 - mse: 11.8271 - mae: 1.3979 - val_loss: 8.8998 - val_mse: 8.8998 - val_mae: 1.3681 - lr: 2.3838e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 4s - loss: 11.7205 - mse: 11.7205 - mae: 1.4000 - val_loss: 8.7289 - val_mse: 8.7289 - val_mae: 1.3334 - lr: 2.3838e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 4s - loss: 11.6453 - mse: 11.6453 - mae: 1.3916 - val_loss: 8.8787 - val_mse: 8.8787 - val_mae: 1.3552 - lr: 2.3838e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 4s - loss: 11.5379 - mse: 11.5379 - mae: 1.3855 - val_loss: 8.8935 - val_mse: 8.8935 - val_mae: 1.4172 - lr: 2.3838e-04 - 4s/epoch - 4ms/step\n",
            "Score for fold 4: loss of 8.893468856811523\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 4s - loss: 10.9568 - mse: 10.9568 - mae: 1.3752 - val_loss: 10.9675 - val_mse: 10.9675 - val_mae: 1.3904 - lr: 2.3838e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 4s - loss: 10.8606 - mse: 10.8606 - mae: 1.3680 - val_loss: 11.2026 - val_mse: 11.2026 - val_mae: 1.4041 - lr: 2.3838e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 4s - loss: 10.8116 - mse: 10.8116 - mae: 1.3613 - val_loss: 11.1062 - val_mse: 11.1062 - val_mae: 1.4302 - lr: 2.3838e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 4s - loss: 10.6727 - mse: 10.6727 - mae: 1.3505 - val_loss: 11.1446 - val_mse: 11.1446 - val_mae: 1.4153 - lr: 2.3838e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 4s - loss: 10.5880 - mse: 10.5880 - mae: 1.3450 - val_loss: 11.2719 - val_mse: 11.2719 - val_mae: 1.4445 - lr: 2.3838e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 4s - loss: 10.4834 - mse: 10.4834 - mae: 1.3409 - val_loss: 11.2807 - val_mse: 11.2807 - val_mae: 1.4251 - lr: 2.3838e-04 - 4s/epoch - 4ms/step\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-10-12 11:18:34,997]\u001b[0m Finished trial#2 resulted in value: 11.808000000000002. Current best value is 11.030000000000001 with parameters: {'activation': 'relu', 'num_hidden_layer': 4, 'i': 6, 'learning_rate': 0.0033229492757648315}.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Score for fold 5: loss of 11.2806978225708\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 3s - loss: 11.8251 - mse: 11.8251 - mae: 1.5381 - val_loss: 17.9977 - val_mse: 17.9977 - val_mae: 1.6001 - lr: 0.0027 - 3s/epoch - 3ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 11.1379 - mse: 11.1379 - mae: 1.4968 - val_loss: 17.6067 - val_mse: 17.6067 - val_mae: 1.5839 - lr: 0.0027 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 10.9847 - mse: 10.9847 - mae: 1.4818 - val_loss: 17.8404 - val_mse: 17.8404 - val_mae: 1.4874 - lr: 0.0027 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 10.9492 - mse: 10.9492 - mae: 1.4701 - val_loss: 17.8976 - val_mse: 17.8976 - val_mae: 1.4617 - lr: 0.0027 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 10.7361 - mse: 10.7361 - mae: 1.4611 - val_loss: 17.8183 - val_mse: 17.8183 - val_mae: 1.5331 - lr: 0.0027 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 10.6704 - mse: 10.6704 - mae: 1.4559 - val_loss: 17.3682 - val_mse: 17.3682 - val_mae: 1.4817 - lr: 0.0027 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 10.6898 - mse: 10.6898 - mae: 1.4626 - val_loss: 17.6044 - val_mse: 17.6044 - val_mae: 1.5122 - lr: 0.0027 - 2s/epoch - 2ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 2s - loss: 10.5874 - mse: 10.5874 - mae: 1.4660 - val_loss: 17.2519 - val_mse: 17.2519 - val_mae: 1.5145 - lr: 0.0027 - 2s/epoch - 2ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 2s - loss: 10.5137 - mse: 10.5137 - mae: 1.4643 - val_loss: 17.6697 - val_mse: 17.6697 - val_mae: 1.4719 - lr: 0.0027 - 2s/epoch - 2ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 2s - loss: 10.4374 - mse: 10.4374 - mae: 1.4580 - val_loss: 17.4062 - val_mse: 17.4062 - val_mae: 1.4670 - lr: 0.0027 - 2s/epoch - 2ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 2s - loss: 10.4064 - mse: 10.4064 - mae: 1.4446 - val_loss: 17.2661 - val_mse: 17.2661 - val_mae: 1.4635 - lr: 0.0027 - 2s/epoch - 2ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 2s - loss: 10.2515 - mse: 10.2515 - mae: 1.4380 - val_loss: 17.4809 - val_mse: 17.4809 - val_mae: 1.4991 - lr: 0.0027 - 2s/epoch - 2ms/step\n",
            "Epoch 13/100\n",
            "1000/1000 - 2s - loss: 10.0986 - mse: 10.0986 - mae: 1.4298 - val_loss: 17.3703 - val_mse: 17.3703 - val_mae: 1.4465 - lr: 0.0027 - 2s/epoch - 2ms/step\n",
            "Score for fold 1: loss of 17.370269775390625\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 12.1019 - mse: 12.1019 - mae: 1.4325 - val_loss: 8.1875 - val_mse: 8.1875 - val_mae: 1.3771 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 11.9489 - mse: 11.9489 - mae: 1.4228 - val_loss: 8.1740 - val_mse: 8.1740 - val_mae: 1.4083 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 11.7993 - mse: 11.7993 - mae: 1.4172 - val_loss: 8.0737 - val_mse: 8.0737 - val_mae: 1.3983 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 11.7083 - mse: 11.7083 - mae: 1.4143 - val_loss: 8.2003 - val_mse: 8.2003 - val_mae: 1.4090 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 11.5581 - mse: 11.5581 - mae: 1.4081 - val_loss: 8.3378 - val_mse: 8.3378 - val_mae: 1.4041 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 11.4607 - mse: 11.4607 - mae: 1.4052 - val_loss: 8.2597 - val_mse: 8.2597 - val_mae: 1.4144 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 11.4734 - mse: 11.4734 - mae: 1.4028 - val_loss: 8.2399 - val_mse: 8.2399 - val_mae: 1.3941 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 2s - loss: 11.4285 - mse: 11.4285 - mae: 1.3996 - val_loss: 8.3789 - val_mse: 8.3789 - val_mae: 1.3961 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Score for fold 2: loss of 8.378877639770508\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 10.8848 - mse: 10.8848 - mae: 1.4077 - val_loss: 10.4246 - val_mse: 10.4246 - val_mae: 1.3976 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 10.7575 - mse: 10.7575 - mae: 1.4017 - val_loss: 10.5226 - val_mse: 10.5226 - val_mae: 1.3972 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 10.7123 - mse: 10.7123 - mae: 1.3977 - val_loss: 10.3779 - val_mse: 10.3779 - val_mae: 1.3894 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 10.7237 - mse: 10.7237 - mae: 1.3955 - val_loss: 10.5518 - val_mse: 10.5518 - val_mae: 1.4407 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 10.6333 - mse: 10.6333 - mae: 1.3933 - val_loss: 10.7650 - val_mse: 10.7650 - val_mae: 1.4492 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 10.6342 - mse: 10.6342 - mae: 1.3906 - val_loss: 10.9629 - val_mse: 10.9629 - val_mae: 1.3778 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 10.4830 - mse: 10.4830 - mae: 1.3871 - val_loss: 10.8282 - val_mse: 10.8282 - val_mae: 1.4107 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 2s - loss: 10.4146 - mse: 10.4146 - mae: 1.3854 - val_loss: 10.7475 - val_mse: 10.7475 - val_mae: 1.4783 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Score for fold 3: loss of 10.74750804901123\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 11.1804 - mse: 11.1804 - mae: 1.3982 - val_loss: 7.5902 - val_mse: 7.5902 - val_mae: 1.3765 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 11.0336 - mse: 11.0336 - mae: 1.3884 - val_loss: 7.3856 - val_mse: 7.3856 - val_mae: 1.3787 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 11.1498 - mse: 11.1498 - mae: 1.3917 - val_loss: 7.5144 - val_mse: 7.5144 - val_mae: 1.3659 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 10.9474 - mse: 10.9474 - mae: 1.3869 - val_loss: 7.5497 - val_mse: 7.5497 - val_mae: 1.3723 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 10.9108 - mse: 10.9108 - mae: 1.3838 - val_loss: 7.8913 - val_mse: 7.8913 - val_mae: 1.4060 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 10.7878 - mse: 10.7878 - mae: 1.3823 - val_loss: 7.8016 - val_mse: 7.8016 - val_mae: 1.3755 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 10.8479 - mse: 10.8479 - mae: 1.3778 - val_loss: 8.2317 - val_mse: 8.2317 - val_mae: 1.4258 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Score for fold 4: loss of 8.231689453125\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 10.2477 - mse: 10.2477 - mae: 1.3837 - val_loss: 10.7862 - val_mse: 10.7862 - val_mae: 1.3379 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 10.0347 - mse: 10.0347 - mae: 1.3751 - val_loss: 11.0984 - val_mse: 11.0984 - val_mae: 1.4223 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 9.9485 - mse: 9.9485 - mae: 1.3719 - val_loss: 10.9016 - val_mse: 10.9016 - val_mae: 1.3763 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 9.9319 - mse: 9.9319 - mae: 1.3671 - val_loss: 11.1099 - val_mse: 11.1099 - val_mae: 1.3768 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 9.8309 - mse: 9.8309 - mae: 1.3671 - val_loss: 11.3765 - val_mse: 11.3765 - val_mae: 1.4137 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 9.8631 - mse: 9.8631 - mae: 1.3658 - val_loss: 11.5299 - val_mse: 11.5299 - val_mae: 1.4598 - lr: 0.0010 - 2s/epoch - 2ms/step\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-10-12 11:20:00,275]\u001b[0m Finished trial#3 resulted in value: 11.252. Current best value is 11.030000000000001 with parameters: {'activation': 'relu', 'num_hidden_layer': 4, 'i': 6, 'learning_rate': 0.0033229492757648315}.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Score for fold 5: loss of 11.529940605163574\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 11.8984 - mse: 11.8984 - mae: 1.5534 - val_loss: 19.1990 - val_mse: 19.1990 - val_mae: 1.6559 - lr: 0.0058 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 11.2183 - mse: 11.2183 - mae: 1.5153 - val_loss: 18.5045 - val_mse: 18.5045 - val_mae: 1.5072 - lr: 0.0058 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 10.8728 - mse: 10.8728 - mae: 1.4998 - val_loss: 18.2187 - val_mse: 18.2187 - val_mae: 1.4941 - lr: 0.0058 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 10.7715 - mse: 10.7715 - mae: 1.4940 - val_loss: 18.2844 - val_mse: 18.2844 - val_mae: 1.4749 - lr: 0.0058 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 10.7513 - mse: 10.7513 - mae: 1.4942 - val_loss: 18.3708 - val_mse: 18.3708 - val_mae: 1.4972 - lr: 0.0058 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 10.8186 - mse: 10.8186 - mae: 1.4927 - val_loss: 18.3714 - val_mse: 18.3714 - val_mae: 1.4940 - lr: 0.0058 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 10.7309 - mse: 10.7309 - mae: 1.4907 - val_loss: 18.0622 - val_mse: 18.0622 - val_mae: 1.4825 - lr: 0.0058 - 2s/epoch - 2ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 2s - loss: 10.8992 - mse: 10.8992 - mae: 1.4922 - val_loss: 18.3614 - val_mse: 18.3614 - val_mae: 1.4451 - lr: 0.0058 - 2s/epoch - 2ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 2s - loss: 10.6900 - mse: 10.6900 - mae: 1.4886 - val_loss: 18.2770 - val_mse: 18.2770 - val_mae: 1.4623 - lr: 0.0058 - 2s/epoch - 2ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 2s - loss: 10.5904 - mse: 10.5904 - mae: 1.4936 - val_loss: 18.1498 - val_mse: 18.1498 - val_mae: 1.4846 - lr: 0.0058 - 2s/epoch - 2ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 2s - loss: 10.7323 - mse: 10.7323 - mae: 1.4879 - val_loss: 18.0404 - val_mse: 18.0404 - val_mae: 1.5047 - lr: 0.0058 - 2s/epoch - 2ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 2s - loss: 10.3704 - mse: 10.3704 - mae: 1.4818 - val_loss: 18.1490 - val_mse: 18.1490 - val_mae: 1.5089 - lr: 0.0058 - 2s/epoch - 2ms/step\n",
            "Epoch 13/100\n",
            "1000/1000 - 2s - loss: 10.5906 - mse: 10.5906 - mae: 1.4916 - val_loss: 18.0181 - val_mse: 18.0181 - val_mae: 1.4812 - lr: 0.0058 - 2s/epoch - 2ms/step\n",
            "Epoch 14/100\n",
            "1000/1000 - 2s - loss: 10.3639 - mse: 10.3639 - mae: 1.4858 - val_loss: 18.1396 - val_mse: 18.1396 - val_mae: 1.5719 - lr: 0.0058 - 2s/epoch - 2ms/step\n",
            "Epoch 15/100\n",
            "1000/1000 - 2s - loss: 10.4513 - mse: 10.4513 - mae: 1.4838 - val_loss: 18.2601 - val_mse: 18.2601 - val_mae: 1.4592 - lr: 0.0058 - 2s/epoch - 2ms/step\n",
            "Epoch 16/100\n",
            "1000/1000 - 2s - loss: 10.3601 - mse: 10.3601 - mae: 1.4808 - val_loss: 18.1246 - val_mse: 18.1246 - val_mae: 1.5990 - lr: 0.0058 - 2s/epoch - 2ms/step\n",
            "Epoch 17/100\n",
            "1000/1000 - 2s - loss: 10.4323 - mse: 10.4323 - mae: 1.4815 - val_loss: 18.0057 - val_mse: 18.0057 - val_mae: 1.4770 - lr: 0.0058 - 2s/epoch - 2ms/step\n",
            "Epoch 18/100\n",
            "1000/1000 - 2s - loss: 10.5939 - mse: 10.5939 - mae: 1.4762 - val_loss: 18.1532 - val_mse: 18.1532 - val_mae: 1.6211 - lr: 0.0058 - 2s/epoch - 2ms/step\n",
            "Epoch 19/100\n",
            "1000/1000 - 2s - loss: 10.3128 - mse: 10.3128 - mae: 1.4808 - val_loss: 18.2951 - val_mse: 18.2951 - val_mae: 1.5526 - lr: 0.0058 - 2s/epoch - 2ms/step\n",
            "Epoch 20/100\n",
            "1000/1000 - 2s - loss: 10.5248 - mse: 10.5248 - mae: 1.4852 - val_loss: 18.1954 - val_mse: 18.1954 - val_mae: 1.5142 - lr: 0.0058 - 2s/epoch - 2ms/step\n",
            "Epoch 21/100\n",
            "1000/1000 - 2s - loss: 10.4714 - mse: 10.4714 - mae: 1.4765 - val_loss: 18.0759 - val_mse: 18.0759 - val_mae: 1.4894 - lr: 0.0058 - 2s/epoch - 2ms/step\n",
            "Epoch 22/100\n",
            "1000/1000 - 2s - loss: 10.3517 - mse: 10.3517 - mae: 1.4878 - val_loss: 17.9915 - val_mse: 17.9915 - val_mae: 1.4856 - lr: 0.0058 - 2s/epoch - 2ms/step\n",
            "Epoch 23/100\n",
            "1000/1000 - 2s - loss: 10.5024 - mse: 10.5024 - mae: 1.4843 - val_loss: 18.2414 - val_mse: 18.2414 - val_mae: 1.6128 - lr: 0.0058 - 2s/epoch - 2ms/step\n",
            "Epoch 24/100\n",
            "1000/1000 - 2s - loss: 10.4841 - mse: 10.4841 - mae: 1.4804 - val_loss: 18.1462 - val_mse: 18.1462 - val_mae: 1.5245 - lr: 0.0058 - 2s/epoch - 2ms/step\n",
            "Epoch 25/100\n",
            "1000/1000 - 2s - loss: 10.4623 - mse: 10.4623 - mae: 1.4756 - val_loss: 17.8774 - val_mse: 17.8774 - val_mae: 1.5215 - lr: 0.0058 - 2s/epoch - 2ms/step\n",
            "Epoch 26/100\n",
            "1000/1000 - 2s - loss: 10.5928 - mse: 10.5928 - mae: 1.4820 - val_loss: 18.0667 - val_mse: 18.0667 - val_mae: 1.4983 - lr: 0.0058 - 2s/epoch - 2ms/step\n",
            "Epoch 27/100\n",
            "1000/1000 - 2s - loss: 10.5381 - mse: 10.5381 - mae: 1.4808 - val_loss: 18.1781 - val_mse: 18.1781 - val_mae: 1.5007 - lr: 0.0058 - 2s/epoch - 2ms/step\n",
            "Epoch 28/100\n",
            "1000/1000 - 2s - loss: 10.4886 - mse: 10.4886 - mae: 1.4796 - val_loss: 18.5793 - val_mse: 18.5793 - val_mae: 1.4712 - lr: 0.0058 - 2s/epoch - 2ms/step\n",
            "Epoch 29/100\n",
            "1000/1000 - 2s - loss: 10.2730 - mse: 10.2730 - mae: 1.4774 - val_loss: 18.0309 - val_mse: 18.0309 - val_mae: 1.4639 - lr: 0.0058 - 2s/epoch - 2ms/step\n",
            "Epoch 30/100\n",
            "1000/1000 - 2s - loss: 10.4661 - mse: 10.4661 - mae: 1.4765 - val_loss: 18.0018 - val_mse: 18.0018 - val_mae: 1.4927 - lr: 0.0058 - 2s/epoch - 2ms/step\n",
            "Score for fold 1: loss of 18.001781463623047\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 12.4693 - mse: 12.4693 - mae: 1.4468 - val_loss: 8.3383 - val_mse: 8.3383 - val_mae: 1.4197 - lr: 0.0012 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 12.4234 - mse: 12.4234 - mae: 1.4424 - val_loss: 8.0935 - val_mse: 8.0935 - val_mae: 1.4662 - lr: 0.0012 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 12.3525 - mse: 12.3525 - mae: 1.4408 - val_loss: 8.1471 - val_mse: 8.1471 - val_mae: 1.4423 - lr: 0.0012 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 12.2830 - mse: 12.2830 - mae: 1.4354 - val_loss: 8.0063 - val_mse: 8.0063 - val_mae: 1.4816 - lr: 0.0012 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 12.2384 - mse: 12.2384 - mae: 1.4351 - val_loss: 8.6487 - val_mse: 8.6487 - val_mae: 1.3994 - lr: 0.0012 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 12.2207 - mse: 12.2207 - mae: 1.4362 - val_loss: 8.4155 - val_mse: 8.4155 - val_mae: 1.4099 - lr: 0.0012 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 12.2410 - mse: 12.2410 - mae: 1.4348 - val_loss: 8.3029 - val_mse: 8.3029 - val_mae: 1.4326 - lr: 0.0012 - 2s/epoch - 2ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 2s - loss: 12.2559 - mse: 12.2559 - mae: 1.4350 - val_loss: 8.3932 - val_mse: 8.3932 - val_mae: 1.4124 - lr: 0.0012 - 2s/epoch - 2ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 2s - loss: 12.1883 - mse: 12.1883 - mae: 1.4329 - val_loss: 8.4839 - val_mse: 8.4839 - val_mae: 1.4044 - lr: 0.0012 - 2s/epoch - 2ms/step\n",
            "Score for fold 2: loss of 8.483928680419922\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 11.4847 - mse: 11.4847 - mae: 1.4393 - val_loss: 11.2001 - val_mse: 11.2001 - val_mae: 1.4505 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 11.3938 - mse: 11.3938 - mae: 1.4376 - val_loss: 11.1635 - val_mse: 11.1635 - val_mae: 1.3987 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 11.4101 - mse: 11.4101 - mae: 1.4387 - val_loss: 11.0941 - val_mse: 11.0941 - val_mae: 1.4195 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 11.2996 - mse: 11.2996 - mae: 1.4345 - val_loss: 11.2613 - val_mse: 11.2613 - val_mae: 1.3878 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 11.3238 - mse: 11.3238 - mae: 1.4340 - val_loss: 11.2524 - val_mse: 11.2524 - val_mae: 1.3821 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 11.2797 - mse: 11.2797 - mae: 1.4335 - val_loss: 11.2591 - val_mse: 11.2591 - val_mae: 1.4444 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 11.3916 - mse: 11.3916 - mae: 1.4326 - val_loss: 11.2890 - val_mse: 11.2890 - val_mae: 1.4656 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 2s - loss: 11.2858 - mse: 11.2858 - mae: 1.4332 - val_loss: 11.1439 - val_mse: 11.1439 - val_mae: 1.4094 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Score for fold 3: loss of 11.143881797790527\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 11.7107 - mse: 11.7107 - mae: 1.4342 - val_loss: 9.5631 - val_mse: 9.5631 - val_mae: 1.4340 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 11.6665 - mse: 11.6665 - mae: 1.4331 - val_loss: 9.5745 - val_mse: 9.5745 - val_mae: 1.4387 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 11.5700 - mse: 11.5700 - mae: 1.4296 - val_loss: 9.5567 - val_mse: 9.5567 - val_mae: 1.4512 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 11.6557 - mse: 11.6557 - mae: 1.4308 - val_loss: 9.5168 - val_mse: 9.5168 - val_mae: 1.4535 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 11.6871 - mse: 11.6871 - mae: 1.4292 - val_loss: 9.6444 - val_mse: 9.6444 - val_mae: 1.4140 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 11.6167 - mse: 11.6167 - mae: 1.4302 - val_loss: 9.6940 - val_mse: 9.6940 - val_mae: 1.4381 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 11.7029 - mse: 11.7029 - mae: 1.4334 - val_loss: 9.6652 - val_mse: 9.6652 - val_mae: 1.4448 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 2s - loss: 11.7193 - mse: 11.7193 - mae: 1.4275 - val_loss: 9.7520 - val_mse: 9.7520 - val_mae: 1.4023 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 2s - loss: 11.6006 - mse: 11.6006 - mae: 1.4296 - val_loss: 9.7996 - val_mse: 9.7996 - val_mae: 1.4573 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Score for fold 4: loss of 9.799560546875\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 11.6268 - mse: 11.6268 - mae: 1.4329 - val_loss: 10.1179 - val_mse: 10.1179 - val_mae: 1.4410 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 11.4510 - mse: 11.4510 - mae: 1.4308 - val_loss: 10.2629 - val_mse: 10.2629 - val_mae: 1.4138 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 11.4424 - mse: 11.4424 - mae: 1.4293 - val_loss: 10.1842 - val_mse: 10.1842 - val_mae: 1.4307 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 11.5678 - mse: 11.5678 - mae: 1.4278 - val_loss: 10.1479 - val_mse: 10.1479 - val_mae: 1.4687 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 11.5373 - mse: 11.5373 - mae: 1.4266 - val_loss: 10.2334 - val_mse: 10.2334 - val_mae: 1.4488 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 11.4636 - mse: 11.4636 - mae: 1.4276 - val_loss: 10.5384 - val_mse: 10.5384 - val_mae: 1.4314 - lr: 0.0010 - 2s/epoch - 2ms/step\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-10-12 11:21:46,194]\u001b[0m Finished trial#4 resulted in value: 11.592. Current best value is 11.030000000000001 with parameters: {'activation': 'relu', 'num_hidden_layer': 4, 'i': 6, 'learning_rate': 0.0033229492757648315}.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Score for fold 5: loss of 10.538405418395996\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 3s - loss: 15.4068 - mse: 15.4068 - mae: 1.6286 - val_loss: 15.9151 - val_mse: 15.9151 - val_mae: 1.5906 - lr: 1.0252e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 13.3188 - mse: 13.3188 - mae: 1.5395 - val_loss: 15.0034 - val_mse: 15.0034 - val_mae: 1.5486 - lr: 1.0252e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 12.9033 - mse: 12.9033 - mae: 1.5239 - val_loss: 14.4659 - val_mse: 14.4659 - val_mae: 1.5423 - lr: 1.0252e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 12.6755 - mse: 12.6755 - mae: 1.5081 - val_loss: 14.2733 - val_mse: 14.2733 - val_mae: 1.5191 - lr: 1.0252e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 12.5361 - mse: 12.5361 - mae: 1.5013 - val_loss: 14.0881 - val_mse: 14.0881 - val_mae: 1.5195 - lr: 1.0252e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 12.4257 - mse: 12.4257 - mae: 1.4921 - val_loss: 13.9985 - val_mse: 13.9985 - val_mae: 1.5246 - lr: 1.0252e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 12.3298 - mse: 12.3298 - mae: 1.4871 - val_loss: 13.9747 - val_mse: 13.9747 - val_mae: 1.5060 - lr: 1.0252e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 2s - loss: 12.2667 - mse: 12.2667 - mae: 1.4816 - val_loss: 13.9142 - val_mse: 13.9142 - val_mae: 1.5136 - lr: 1.0252e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 2s - loss: 12.1981 - mse: 12.1981 - mae: 1.4794 - val_loss: 13.9683 - val_mse: 13.9683 - val_mae: 1.4863 - lr: 1.0252e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 2s - loss: 12.1473 - mse: 12.1473 - mae: 1.4760 - val_loss: 13.9177 - val_mse: 13.9177 - val_mae: 1.5085 - lr: 1.0252e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 2s - loss: 12.1176 - mse: 12.1176 - mae: 1.4737 - val_loss: 13.8783 - val_mse: 13.8783 - val_mae: 1.5139 - lr: 1.0252e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 2s - loss: 12.0440 - mse: 12.0440 - mae: 1.4727 - val_loss: 13.8879 - val_mse: 13.8879 - val_mae: 1.4965 - lr: 1.0252e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 13/100\n",
            "1000/1000 - 2s - loss: 12.0060 - mse: 12.0060 - mae: 1.4680 - val_loss: 13.8518 - val_mse: 13.8518 - val_mae: 1.5287 - lr: 1.0252e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 14/100\n",
            "1000/1000 - 2s - loss: 11.9566 - mse: 11.9566 - mae: 1.4669 - val_loss: 13.8963 - val_mse: 13.8963 - val_mae: 1.5080 - lr: 1.0252e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 15/100\n",
            "1000/1000 - 2s - loss: 11.9354 - mse: 11.9354 - mae: 1.4695 - val_loss: 13.8713 - val_mse: 13.8713 - val_mae: 1.5017 - lr: 1.0252e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 16/100\n",
            "1000/1000 - 2s - loss: 11.9008 - mse: 11.9008 - mae: 1.4615 - val_loss: 13.8694 - val_mse: 13.8694 - val_mae: 1.5008 - lr: 1.0252e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 17/100\n",
            "1000/1000 - 2s - loss: 11.8594 - mse: 11.8594 - mae: 1.4604 - val_loss: 13.8355 - val_mse: 13.8355 - val_mae: 1.4895 - lr: 1.0252e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 18/100\n",
            "1000/1000 - 2s - loss: 11.8213 - mse: 11.8213 - mae: 1.4571 - val_loss: 13.7849 - val_mse: 13.7849 - val_mae: 1.5188 - lr: 1.0252e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 19/100\n",
            "1000/1000 - 2s - loss: 11.7945 - mse: 11.7945 - mae: 1.4593 - val_loss: 13.8672 - val_mse: 13.8672 - val_mae: 1.4986 - lr: 1.0252e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 20/100\n",
            "1000/1000 - 2s - loss: 11.7760 - mse: 11.7760 - mae: 1.4546 - val_loss: 13.8745 - val_mse: 13.8745 - val_mae: 1.4952 - lr: 1.0252e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 21/100\n",
            "1000/1000 - 2s - loss: 11.7366 - mse: 11.7366 - mae: 1.4509 - val_loss: 13.8962 - val_mse: 13.8962 - val_mae: 1.4956 - lr: 1.0252e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 22/100\n",
            "1000/1000 - 2s - loss: 11.6786 - mse: 11.6786 - mae: 1.4493 - val_loss: 13.9862 - val_mse: 13.9862 - val_mae: 1.4951 - lr: 1.0252e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 23/100\n",
            "1000/1000 - 2s - loss: 11.7145 - mse: 11.7145 - mae: 1.4498 - val_loss: 13.9019 - val_mse: 13.9019 - val_mae: 1.5048 - lr: 1.0252e-04 - 2s/epoch - 2ms/step\n",
            "Score for fold 1: loss of 13.901935577392578\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 12.6234 - mse: 12.6234 - mae: 1.4640 - val_loss: 10.0418 - val_mse: 10.0418 - val_mae: 1.4326 - lr: 1.0252e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 12.5475 - mse: 12.5475 - mae: 1.4647 - val_loss: 10.0213 - val_mse: 10.0213 - val_mae: 1.4192 - lr: 1.0252e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 12.5413 - mse: 12.5413 - mae: 1.4584 - val_loss: 10.0000 - val_mse: 10.0000 - val_mae: 1.4346 - lr: 1.0252e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 12.5171 - mse: 12.5171 - mae: 1.4573 - val_loss: 9.9821 - val_mse: 9.9821 - val_mae: 1.4253 - lr: 1.0252e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 12.4660 - mse: 12.4660 - mae: 1.4586 - val_loss: 10.0195 - val_mse: 10.0195 - val_mae: 1.4158 - lr: 1.0252e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 12.4758 - mse: 12.4758 - mae: 1.4544 - val_loss: 9.9697 - val_mse: 9.9697 - val_mae: 1.4221 - lr: 1.0252e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 12.4696 - mse: 12.4696 - mae: 1.4554 - val_loss: 9.9701 - val_mse: 9.9701 - val_mae: 1.4279 - lr: 1.0252e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 2s - loss: 12.4531 - mse: 12.4531 - mae: 1.4545 - val_loss: 9.9752 - val_mse: 9.9752 - val_mae: 1.4136 - lr: 1.0252e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 2s - loss: 12.4084 - mse: 12.4084 - mae: 1.4510 - val_loss: 9.9647 - val_mse: 9.9647 - val_mae: 1.4388 - lr: 1.0252e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 2s - loss: 12.3775 - mse: 12.3775 - mae: 1.4515 - val_loss: 9.9377 - val_mse: 9.9377 - val_mae: 1.4312 - lr: 1.0252e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 2s - loss: 12.3848 - mse: 12.3848 - mae: 1.4531 - val_loss: 9.9462 - val_mse: 9.9462 - val_mae: 1.4080 - lr: 1.0252e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 2s - loss: 12.3507 - mse: 12.3507 - mae: 1.4494 - val_loss: 9.9464 - val_mse: 9.9464 - val_mae: 1.4387 - lr: 1.0252e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 13/100\n",
            "1000/1000 - 2s - loss: 12.3250 - mse: 12.3250 - mae: 1.4484 - val_loss: 9.9348 - val_mse: 9.9348 - val_mae: 1.4362 - lr: 1.0252e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 14/100\n",
            "1000/1000 - 2s - loss: 12.3317 - mse: 12.3317 - mae: 1.4510 - val_loss: 9.9718 - val_mse: 9.9718 - val_mae: 1.4287 - lr: 1.0252e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 15/100\n",
            "1000/1000 - 2s - loss: 12.3125 - mse: 12.3125 - mae: 1.4501 - val_loss: 9.9339 - val_mse: 9.9339 - val_mae: 1.4148 - lr: 1.0252e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 16/100\n",
            "1000/1000 - 2s - loss: 12.3003 - mse: 12.3003 - mae: 1.4442 - val_loss: 9.9439 - val_mse: 9.9439 - val_mae: 1.4289 - lr: 1.0252e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 17/100\n",
            "1000/1000 - 2s - loss: 12.2830 - mse: 12.2830 - mae: 1.4455 - val_loss: 9.9083 - val_mse: 9.9083 - val_mae: 1.4380 - lr: 1.0252e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 18/100\n",
            "1000/1000 - 2s - loss: 12.2981 - mse: 12.2981 - mae: 1.4470 - val_loss: 9.9424 - val_mse: 9.9424 - val_mae: 1.4209 - lr: 1.0252e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 19/100\n",
            "1000/1000 - 2s - loss: 12.2792 - mse: 12.2792 - mae: 1.4446 - val_loss: 9.9029 - val_mse: 9.9029 - val_mae: 1.4492 - lr: 1.0252e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 20/100\n",
            "1000/1000 - 2s - loss: 12.2353 - mse: 12.2353 - mae: 1.4457 - val_loss: 9.9104 - val_mse: 9.9104 - val_mae: 1.4329 - lr: 1.0252e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 21/100\n",
            "1000/1000 - 2s - loss: 12.2728 - mse: 12.2728 - mae: 1.4421 - val_loss: 9.9073 - val_mse: 9.9073 - val_mae: 1.4134 - lr: 1.0252e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 22/100\n",
            "1000/1000 - 2s - loss: 12.2223 - mse: 12.2223 - mae: 1.4430 - val_loss: 9.9279 - val_mse: 9.9279 - val_mae: 1.4193 - lr: 1.0252e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 23/100\n",
            "1000/1000 - 2s - loss: 12.2358 - mse: 12.2358 - mae: 1.4416 - val_loss: 9.9107 - val_mse: 9.9107 - val_mae: 1.4140 - lr: 1.0252e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 24/100\n",
            "1000/1000 - 2s - loss: 12.2458 - mse: 12.2458 - mae: 1.4408 - val_loss: 9.9205 - val_mse: 9.9205 - val_mae: 1.4267 - lr: 1.0252e-04 - 2s/epoch - 2ms/step\n",
            "Score for fold 2: loss of 9.92050838470459\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 10.6126 - mse: 10.6126 - mae: 1.4442 - val_loss: 16.3347 - val_mse: 16.3347 - val_mae: 1.4015 - lr: 1.0252e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 10.5653 - mse: 10.5653 - mae: 1.4411 - val_loss: 16.3380 - val_mse: 16.3380 - val_mae: 1.4024 - lr: 1.0252e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 10.5818 - mse: 10.5818 - mae: 1.4404 - val_loss: 16.3329 - val_mse: 16.3329 - val_mae: 1.4139 - lr: 1.0252e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 10.5494 - mse: 10.5494 - mae: 1.4389 - val_loss: 16.3551 - val_mse: 16.3551 - val_mae: 1.4065 - lr: 1.0252e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 10.5554 - mse: 10.5554 - mae: 1.4424 - val_loss: 16.3213 - val_mse: 16.3213 - val_mae: 1.4099 - lr: 1.0252e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 10.5565 - mse: 10.5565 - mae: 1.4380 - val_loss: 16.3450 - val_mse: 16.3450 - val_mae: 1.3997 - lr: 1.0252e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 10.5642 - mse: 10.5642 - mae: 1.4382 - val_loss: 16.3417 - val_mse: 16.3417 - val_mae: 1.4115 - lr: 1.0252e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 2s - loss: 10.5229 - mse: 10.5229 - mae: 1.4400 - val_loss: 16.3487 - val_mse: 16.3487 - val_mae: 1.4027 - lr: 1.0252e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 2s - loss: 10.5357 - mse: 10.5357 - mae: 1.4361 - val_loss: 16.3340 - val_mse: 16.3340 - val_mae: 1.4096 - lr: 1.0252e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 2s - loss: 10.5454 - mse: 10.5454 - mae: 1.4356 - val_loss: 16.3363 - val_mse: 16.3363 - val_mae: 1.4101 - lr: 1.0252e-04 - 2s/epoch - 2ms/step\n",
            "Score for fold 3: loss of 16.33632469177246\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 12.0882 - mse: 12.0882 - mae: 1.4289 - val_loss: 10.0473 - val_mse: 10.0473 - val_mae: 1.4555 - lr: 1.0252e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 12.0300 - mse: 12.0300 - mae: 1.4311 - val_loss: 10.0764 - val_mse: 10.0764 - val_mae: 1.4312 - lr: 1.0252e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 12.0489 - mse: 12.0489 - mae: 1.4287 - val_loss: 10.1323 - val_mse: 10.1323 - val_mae: 1.4451 - lr: 1.0252e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 12.0390 - mse: 12.0390 - mae: 1.4272 - val_loss: 10.1224 - val_mse: 10.1224 - val_mae: 1.4346 - lr: 1.0252e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 12.0347 - mse: 12.0347 - mae: 1.4268 - val_loss: 10.1158 - val_mse: 10.1158 - val_mae: 1.4275 - lr: 1.0252e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 12.0038 - mse: 12.0038 - mae: 1.4256 - val_loss: 10.1693 - val_mse: 10.1693 - val_mae: 1.4576 - lr: 1.0252e-04 - 2s/epoch - 2ms/step\n",
            "Score for fold 4: loss of 10.169333457946777\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 12.3511 - mse: 12.3511 - mae: 1.4278 - val_loss: 8.7131 - val_mse: 8.7131 - val_mae: 1.4433 - lr: 1.0252e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 12.3365 - mse: 12.3365 - mae: 1.4257 - val_loss: 8.7230 - val_mse: 8.7230 - val_mae: 1.4455 - lr: 1.0252e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 12.3085 - mse: 12.3085 - mae: 1.4259 - val_loss: 8.7584 - val_mse: 8.7584 - val_mae: 1.4394 - lr: 1.0252e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 12.3375 - mse: 12.3375 - mae: 1.4232 - val_loss: 8.7376 - val_mse: 8.7376 - val_mae: 1.4263 - lr: 1.0252e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 12.3062 - mse: 12.3062 - mae: 1.4251 - val_loss: 8.7625 - val_mse: 8.7625 - val_mae: 1.4370 - lr: 1.0252e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 12.3111 - mse: 12.3111 - mae: 1.4226 - val_loss: 8.7860 - val_mse: 8.7860 - val_mae: 1.4479 - lr: 1.0252e-04 - 2s/epoch - 2ms/step\n",
            "Score for fold 5: loss of 8.786043167114258\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-10-12 11:23:57,022]\u001b[0m Finished trial#5 resulted in value: 11.824. Current best value is 11.030000000000001 with parameters: {'activation': 'relu', 'num_hidden_layer': 4, 'i': 6, 'learning_rate': 0.0033229492757648315}.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 3s - loss: 13.7796 - mse: 13.7796 - mae: 1.5478 - val_loss: 9.2014 - val_mse: 9.2014 - val_mae: 1.3954 - lr: 0.0035 - 3s/epoch - 3ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 13.3459 - mse: 13.3459 - mae: 1.5038 - val_loss: 9.1207 - val_mse: 9.1207 - val_mae: 1.4417 - lr: 0.0035 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 12.9681 - mse: 12.9681 - mae: 1.4829 - val_loss: 8.7808 - val_mse: 8.7808 - val_mae: 1.4673 - lr: 0.0035 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 12.8107 - mse: 12.8107 - mae: 1.4735 - val_loss: 9.0193 - val_mse: 9.0193 - val_mae: 1.4413 - lr: 0.0035 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 12.8555 - mse: 12.8555 - mae: 1.4817 - val_loss: 8.8877 - val_mse: 8.8877 - val_mae: 1.5089 - lr: 0.0035 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 12.8766 - mse: 12.8766 - mae: 1.4762 - val_loss: 9.3415 - val_mse: 9.3415 - val_mae: 1.4856 - lr: 0.0035 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 12.5991 - mse: 12.5991 - mae: 1.4571 - val_loss: 8.6905 - val_mse: 8.6905 - val_mae: 1.5045 - lr: 0.0035 - 2s/epoch - 2ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 2s - loss: 12.5812 - mse: 12.5812 - mae: 1.4564 - val_loss: 8.7647 - val_mse: 8.7647 - val_mae: 1.4351 - lr: 0.0035 - 2s/epoch - 2ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 2s - loss: 12.4422 - mse: 12.4422 - mae: 1.4522 - val_loss: 8.8326 - val_mse: 8.8326 - val_mae: 1.4221 - lr: 0.0035 - 2s/epoch - 2ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 2s - loss: 12.3239 - mse: 12.3239 - mae: 1.4489 - val_loss: 8.6650 - val_mse: 8.6650 - val_mae: 1.4130 - lr: 0.0035 - 2s/epoch - 2ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 2s - loss: 12.3788 - mse: 12.3788 - mae: 1.4471 - val_loss: 9.6352 - val_mse: 9.6352 - val_mae: 1.5337 - lr: 0.0035 - 2s/epoch - 2ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 2s - loss: 12.0764 - mse: 12.0764 - mae: 1.4430 - val_loss: 8.5012 - val_mse: 8.5012 - val_mae: 1.4837 - lr: 0.0035 - 2s/epoch - 2ms/step\n",
            "Epoch 13/100\n",
            "1000/1000 - 2s - loss: 12.0701 - mse: 12.0701 - mae: 1.4382 - val_loss: 8.7082 - val_mse: 8.7082 - val_mae: 1.4536 - lr: 0.0035 - 2s/epoch - 2ms/step\n",
            "Epoch 14/100\n",
            "1000/1000 - 2s - loss: 12.0630 - mse: 12.0630 - mae: 1.4415 - val_loss: 8.9910 - val_mse: 8.9910 - val_mae: 1.4451 - lr: 0.0035 - 2s/epoch - 2ms/step\n",
            "Epoch 15/100\n",
            "1000/1000 - 2s - loss: 11.9268 - mse: 11.9268 - mae: 1.4335 - val_loss: 8.6403 - val_mse: 8.6403 - val_mae: 1.4936 - lr: 0.0035 - 2s/epoch - 2ms/step\n",
            "Epoch 16/100\n",
            "1000/1000 - 2s - loss: 12.0004 - mse: 12.0004 - mae: 1.4364 - val_loss: 9.0384 - val_mse: 9.0384 - val_mae: 1.4557 - lr: 0.0035 - 2s/epoch - 2ms/step\n",
            "Epoch 17/100\n",
            "1000/1000 - 2s - loss: 11.8720 - mse: 11.8720 - mae: 1.4279 - val_loss: 8.7337 - val_mse: 8.7337 - val_mae: 1.4368 - lr: 0.0035 - 2s/epoch - 2ms/step\n",
            "Score for fold 1: loss of 8.733718872070312\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 9.0015 - mse: 9.0015 - mae: 1.4060 - val_loss: 17.9102 - val_mse: 17.9102 - val_mae: 1.3957 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 8.7567 - mse: 8.7567 - mae: 1.3920 - val_loss: 18.0162 - val_mse: 18.0162 - val_mae: 1.4246 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 8.6471 - mse: 8.6471 - mae: 1.3882 - val_loss: 17.9018 - val_mse: 17.9018 - val_mae: 1.4324 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 8.5943 - mse: 8.5943 - mae: 1.3841 - val_loss: 18.2632 - val_mse: 18.2632 - val_mae: 1.4364 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 8.5096 - mse: 8.5096 - mae: 1.3807 - val_loss: 18.6594 - val_mse: 18.6594 - val_mae: 1.4075 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 8.3793 - mse: 8.3793 - mae: 1.3773 - val_loss: 18.1590 - val_mse: 18.1590 - val_mae: 1.4139 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 8.3359 - mse: 8.3359 - mae: 1.3697 - val_loss: 18.3262 - val_mse: 18.3262 - val_mae: 1.4060 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 2s - loss: 8.2573 - mse: 8.2573 - mae: 1.3678 - val_loss: 18.3631 - val_mse: 18.3631 - val_mae: 1.4058 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Score for fold 2: loss of 18.36309242248535\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 10.4052 - mse: 10.4052 - mae: 1.3821 - val_loss: 9.5517 - val_mse: 9.5517 - val_mae: 1.3341 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 10.3869 - mse: 10.3869 - mae: 1.3776 - val_loss: 9.7575 - val_mse: 9.7575 - val_mae: 1.3644 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 10.2339 - mse: 10.2339 - mae: 1.3711 - val_loss: 9.8482 - val_mse: 9.8482 - val_mae: 1.3539 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 10.2149 - mse: 10.2149 - mae: 1.3720 - val_loss: 9.8873 - val_mse: 9.8873 - val_mae: 1.3426 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 10.1172 - mse: 10.1172 - mae: 1.3646 - val_loss: 9.8579 - val_mse: 9.8579 - val_mae: 1.3757 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 10.0429 - mse: 10.0429 - mae: 1.3602 - val_loss: 9.8208 - val_mse: 9.8208 - val_mae: 1.4039 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Score for fold 3: loss of 9.82082462310791\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 10.2856 - mse: 10.2856 - mae: 1.3707 - val_loss: 9.0042 - val_mse: 9.0042 - val_mae: 1.3336 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 10.2506 - mse: 10.2506 - mae: 1.3630 - val_loss: 8.9644 - val_mse: 8.9644 - val_mae: 1.3708 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 10.0985 - mse: 10.0985 - mae: 1.3605 - val_loss: 8.9377 - val_mse: 8.9377 - val_mae: 1.3908 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 10.0409 - mse: 10.0409 - mae: 1.3534 - val_loss: 9.1789 - val_mse: 9.1789 - val_mae: 1.3886 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 10.0103 - mse: 10.0103 - mae: 1.3496 - val_loss: 9.1196 - val_mse: 9.1196 - val_mae: 1.3687 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 9.9856 - mse: 9.9856 - mae: 1.3442 - val_loss: 9.1398 - val_mse: 9.1398 - val_mae: 1.3945 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 9.8949 - mse: 9.8949 - mae: 1.3401 - val_loss: 9.1992 - val_mse: 9.1992 - val_mae: 1.3789 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 2s - loss: 9.9374 - mse: 9.9374 - mae: 1.3352 - val_loss: 9.3529 - val_mse: 9.3529 - val_mae: 1.3769 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Score for fold 4: loss of 9.352869987487793\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 10.5506 - mse: 10.5506 - mae: 1.3557 - val_loss: 6.7153 - val_mse: 6.7153 - val_mae: 1.3082 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 10.3388 - mse: 10.3388 - mae: 1.3511 - val_loss: 6.8829 - val_mse: 6.8829 - val_mae: 1.3378 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 10.2373 - mse: 10.2373 - mae: 1.3437 - val_loss: 6.9049 - val_mse: 6.9049 - val_mae: 1.3406 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 10.3011 - mse: 10.3011 - mae: 1.3462 - val_loss: 7.0016 - val_mse: 7.0016 - val_mae: 1.3658 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 10.2328 - mse: 10.2328 - mae: 1.3407 - val_loss: 6.9693 - val_mse: 6.9693 - val_mae: 1.3295 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 10.1674 - mse: 10.1674 - mae: 1.3347 - val_loss: 7.2141 - val_mse: 7.2141 - val_mae: 1.3139 - lr: 0.0010 - 2s/epoch - 2ms/step\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-10-12 11:25:36,634]\u001b[0m Finished trial#6 resulted in value: 10.693999999999999. Current best value is 10.693999999999999 with parameters: {'activation': 'relu', 'num_hidden_layer': 3, 'i': 7, 'learning_rate': 0.0034867788821243673}.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Score for fold 5: loss of 7.214088439941406\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 3s - loss: 15.0783 - mse: 15.0783 - mae: 1.6159 - val_loss: 11.6004 - val_mse: 11.6004 - val_mae: 1.5050 - lr: 1.2919e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 3s - loss: 13.4461 - mse: 13.4461 - mae: 1.5208 - val_loss: 10.8451 - val_mse: 10.8451 - val_mae: 1.5556 - lr: 1.2919e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 3s - loss: 13.1682 - mse: 13.1682 - mae: 1.5033 - val_loss: 10.5917 - val_mse: 10.5917 - val_mae: 1.5557 - lr: 1.2919e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 12.8824 - mse: 12.8824 - mae: 1.4908 - val_loss: 10.4194 - val_mse: 10.4194 - val_mae: 1.4937 - lr: 1.2919e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 12.7607 - mse: 12.7607 - mae: 1.4786 - val_loss: 10.2299 - val_mse: 10.2299 - val_mae: 1.5540 - lr: 1.2919e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 3s - loss: 12.6390 - mse: 12.6390 - mae: 1.4705 - val_loss: 10.1912 - val_mse: 10.1912 - val_mae: 1.5749 - lr: 1.2919e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 3s - loss: 12.6289 - mse: 12.6289 - mae: 1.4650 - val_loss: 10.1077 - val_mse: 10.1077 - val_mae: 1.4734 - lr: 1.2919e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 3s - loss: 12.4933 - mse: 12.4933 - mae: 1.4567 - val_loss: 10.1641 - val_mse: 10.1641 - val_mae: 1.4649 - lr: 1.2919e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 2s - loss: 12.4678 - mse: 12.4678 - mae: 1.4509 - val_loss: 10.0654 - val_mse: 10.0654 - val_mae: 1.5018 - lr: 1.2919e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 3s - loss: 12.4638 - mse: 12.4638 - mae: 1.4486 - val_loss: 10.1513 - val_mse: 10.1513 - val_mae: 1.4641 - lr: 1.2919e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 3s - loss: 12.3512 - mse: 12.3512 - mae: 1.4426 - val_loss: 10.0226 - val_mse: 10.0226 - val_mae: 1.4853 - lr: 1.2919e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 3s - loss: 12.2734 - mse: 12.2734 - mae: 1.4405 - val_loss: 10.1430 - val_mse: 10.1430 - val_mae: 1.4393 - lr: 1.2919e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 13/100\n",
            "1000/1000 - 2s - loss: 12.2532 - mse: 12.2532 - mae: 1.4405 - val_loss: 10.0393 - val_mse: 10.0393 - val_mae: 1.4723 - lr: 1.2919e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 14/100\n",
            "1000/1000 - 3s - loss: 12.2606 - mse: 12.2606 - mae: 1.4323 - val_loss: 10.1301 - val_mse: 10.1301 - val_mae: 1.4982 - lr: 1.2919e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 15/100\n",
            "1000/1000 - 3s - loss: 12.1499 - mse: 12.1499 - mae: 1.4321 - val_loss: 10.0785 - val_mse: 10.0785 - val_mae: 1.4959 - lr: 1.2919e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 16/100\n",
            "1000/1000 - 3s - loss: 12.1418 - mse: 12.1418 - mae: 1.4291 - val_loss: 9.9952 - val_mse: 9.9952 - val_mae: 1.4931 - lr: 1.2919e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 17/100\n",
            "1000/1000 - 3s - loss: 12.1340 - mse: 12.1340 - mae: 1.4276 - val_loss: 10.1050 - val_mse: 10.1050 - val_mae: 1.5478 - lr: 1.2919e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 18/100\n",
            "1000/1000 - 3s - loss: 12.0026 - mse: 12.0026 - mae: 1.4244 - val_loss: 10.0373 - val_mse: 10.0373 - val_mae: 1.5334 - lr: 1.2919e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 19/100\n",
            "1000/1000 - 3s - loss: 12.0483 - mse: 12.0483 - mae: 1.4236 - val_loss: 10.1531 - val_mse: 10.1531 - val_mae: 1.4983 - lr: 1.2919e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 20/100\n",
            "1000/1000 - 3s - loss: 11.9358 - mse: 11.9358 - mae: 1.4171 - val_loss: 10.0311 - val_mse: 10.0311 - val_mae: 1.4882 - lr: 1.2919e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 21/100\n",
            "1000/1000 - 3s - loss: 12.0159 - mse: 12.0159 - mae: 1.4179 - val_loss: 10.2688 - val_mse: 10.2688 - val_mae: 1.4571 - lr: 1.2919e-04 - 3s/epoch - 3ms/step\n",
            "Score for fold 1: loss of 10.268837928771973\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 3s - loss: 11.7422 - mse: 11.7422 - mae: 1.4335 - val_loss: 10.9532 - val_mse: 10.9532 - val_mae: 1.4153 - lr: 1.2919e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 3s - loss: 11.6181 - mse: 11.6181 - mae: 1.4253 - val_loss: 10.8902 - val_mse: 10.8902 - val_mae: 1.4330 - lr: 1.2919e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 3s - loss: 11.4867 - mse: 11.4867 - mae: 1.4251 - val_loss: 10.9291 - val_mse: 10.9291 - val_mae: 1.3884 - lr: 1.2919e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 3s - loss: 11.6201 - mse: 11.6201 - mae: 1.4234 - val_loss: 11.0628 - val_mse: 11.0628 - val_mae: 1.4456 - lr: 1.2919e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 3s - loss: 11.4235 - mse: 11.4235 - mae: 1.4168 - val_loss: 11.0783 - val_mse: 11.0783 - val_mae: 1.4503 - lr: 1.2919e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 3s - loss: 11.4314 - mse: 11.4314 - mae: 1.4155 - val_loss: 10.9008 - val_mse: 10.9008 - val_mae: 1.4082 - lr: 1.2919e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 3s - loss: 11.3400 - mse: 11.3400 - mae: 1.4125 - val_loss: 11.0097 - val_mse: 11.0097 - val_mae: 1.4142 - lr: 1.2919e-04 - 3s/epoch - 3ms/step\n",
            "Score for fold 2: loss of 11.009650230407715\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 3s - loss: 11.5347 - mse: 11.5347 - mae: 1.4082 - val_loss: 10.4291 - val_mse: 10.4291 - val_mae: 1.4290 - lr: 1.2919e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 3s - loss: 11.4386 - mse: 11.4386 - mae: 1.4061 - val_loss: 10.4776 - val_mse: 10.4776 - val_mae: 1.4521 - lr: 1.2919e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 3s - loss: 11.4219 - mse: 11.4219 - mae: 1.4062 - val_loss: 10.4511 - val_mse: 10.4511 - val_mae: 1.4385 - lr: 1.2919e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 3s - loss: 11.4072 - mse: 11.4072 - mae: 1.4008 - val_loss: 10.4267 - val_mse: 10.4267 - val_mae: 1.4411 - lr: 1.2919e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 3s - loss: 11.4060 - mse: 11.4060 - mae: 1.3977 - val_loss: 10.3082 - val_mse: 10.3082 - val_mae: 1.4767 - lr: 1.2919e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 3s - loss: 11.2690 - mse: 11.2690 - mae: 1.3977 - val_loss: 10.6079 - val_mse: 10.6079 - val_mae: 1.4207 - lr: 1.2919e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 3s - loss: 11.2135 - mse: 11.2135 - mae: 1.3922 - val_loss: 10.3796 - val_mse: 10.3796 - val_mae: 1.4539 - lr: 1.2919e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 3s - loss: 11.2234 - mse: 11.2234 - mae: 1.3952 - val_loss: 10.4690 - val_mse: 10.4690 - val_mae: 1.4478 - lr: 1.2919e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 3s - loss: 11.1729 - mse: 11.1729 - mae: 1.3896 - val_loss: 10.2979 - val_mse: 10.2979 - val_mae: 1.4762 - lr: 1.2919e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 3s - loss: 11.1792 - mse: 11.1792 - mae: 1.3861 - val_loss: 10.5092 - val_mse: 10.5092 - val_mae: 1.4821 - lr: 1.2919e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 3s - loss: 11.1232 - mse: 11.1232 - mae: 1.3862 - val_loss: 10.4059 - val_mse: 10.4059 - val_mae: 1.4405 - lr: 1.2919e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 3s - loss: 11.0778 - mse: 11.0778 - mae: 1.3825 - val_loss: 10.6335 - val_mse: 10.6335 - val_mae: 1.4444 - lr: 1.2919e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 13/100\n",
            "1000/1000 - 3s - loss: 11.0390 - mse: 11.0390 - mae: 1.3830 - val_loss: 10.7302 - val_mse: 10.7302 - val_mae: 1.4121 - lr: 1.2919e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 14/100\n",
            "1000/1000 - 3s - loss: 11.0272 - mse: 11.0272 - mae: 1.3786 - val_loss: 10.9354 - val_mse: 10.9354 - val_mae: 1.4189 - lr: 1.2919e-04 - 3s/epoch - 3ms/step\n",
            "Score for fold 3: loss of 10.935423851013184\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 3s - loss: 9.2670 - mse: 9.2670 - mae: 1.3928 - val_loss: 17.4373 - val_mse: 17.4373 - val_mae: 1.4517 - lr: 1.2919e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 3s - loss: 9.2039 - mse: 9.2039 - mae: 1.3886 - val_loss: 17.5240 - val_mse: 17.5240 - val_mae: 1.3800 - lr: 1.2919e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 3s - loss: 9.1401 - mse: 9.1401 - mae: 1.3860 - val_loss: 17.5367 - val_mse: 17.5367 - val_mae: 1.3579 - lr: 1.2919e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 3s - loss: 9.0974 - mse: 9.0974 - mae: 1.3794 - val_loss: 17.5604 - val_mse: 17.5604 - val_mae: 1.3974 - lr: 1.2919e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 3s - loss: 9.0059 - mse: 9.0059 - mae: 1.3775 - val_loss: 17.4779 - val_mse: 17.4779 - val_mae: 1.4119 - lr: 1.2919e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 3s - loss: 9.0442 - mse: 9.0442 - mae: 1.3738 - val_loss: 17.9714 - val_mse: 17.9714 - val_mae: 1.3854 - lr: 1.2919e-04 - 3s/epoch - 3ms/step\n",
            "Score for fold 4: loss of 17.971410751342773\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 3s - loss: 11.6601 - mse: 11.6601 - mae: 1.3975 - val_loss: 7.0757 - val_mse: 7.0757 - val_mae: 1.2910 - lr: 1.2919e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 3s - loss: 11.6588 - mse: 11.6588 - mae: 1.3980 - val_loss: 7.3933 - val_mse: 7.3933 - val_mae: 1.3957 - lr: 1.2919e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 3s - loss: 11.5104 - mse: 11.5104 - mae: 1.3916 - val_loss: 7.1336 - val_mse: 7.1336 - val_mae: 1.2982 - lr: 1.2919e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 3s - loss: 11.3483 - mse: 11.3483 - mae: 1.3873 - val_loss: 7.2409 - val_mse: 7.2409 - val_mae: 1.3113 - lr: 1.2919e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 3s - loss: 11.3719 - mse: 11.3719 - mae: 1.3856 - val_loss: 7.1996 - val_mse: 7.1996 - val_mae: 1.3283 - lr: 1.2919e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 3s - loss: 11.2809 - mse: 11.2809 - mae: 1.3850 - val_loss: 7.9440 - val_mse: 7.9440 - val_mae: 1.3173 - lr: 1.2919e-04 - 3s/epoch - 3ms/step\n",
            "Score for fold 5: loss of 7.943982124328613\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-10-12 11:28:07,938]\u001b[0m Finished trial#7 resulted in value: 11.626. Current best value is 10.693999999999999 with parameters: {'activation': 'relu', 'num_hidden_layer': 3, 'i': 7, 'learning_rate': 0.0034867788821243673}.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 3s - loss: 14.2612 - mse: 14.2612 - mae: 1.6179 - val_loss: 16.2142 - val_mse: 16.2142 - val_mae: 1.4809 - lr: 0.0045 - 3s/epoch - 3ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 13.5738 - mse: 13.5738 - mae: 1.5497 - val_loss: 16.0740 - val_mse: 16.0740 - val_mae: 1.6094 - lr: 0.0045 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 13.2720 - mse: 13.2720 - mae: 1.5302 - val_loss: 15.4957 - val_mse: 15.4957 - val_mae: 1.4583 - lr: 0.0045 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 13.0504 - mse: 13.0504 - mae: 1.5199 - val_loss: 15.3470 - val_mse: 15.3470 - val_mae: 1.6491 - lr: 0.0045 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 12.9534 - mse: 12.9534 - mae: 1.5106 - val_loss: 15.1383 - val_mse: 15.1383 - val_mae: 1.5197 - lr: 0.0045 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 12.7594 - mse: 12.7594 - mae: 1.5064 - val_loss: 14.8388 - val_mse: 14.8388 - val_mae: 1.5016 - lr: 0.0045 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 12.7371 - mse: 12.7371 - mae: 1.4989 - val_loss: 14.9214 - val_mse: 14.9214 - val_mae: 1.5691 - lr: 0.0045 - 2s/epoch - 2ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 2s - loss: 12.6381 - mse: 12.6381 - mae: 1.4998 - val_loss: 14.5634 - val_mse: 14.5634 - val_mae: 1.6206 - lr: 0.0045 - 2s/epoch - 2ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 2s - loss: 12.5837 - mse: 12.5837 - mae: 1.4889 - val_loss: 14.8386 - val_mse: 14.8386 - val_mae: 1.4167 - lr: 0.0045 - 2s/epoch - 2ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 2s - loss: 12.4293 - mse: 12.4293 - mae: 1.4817 - val_loss: 14.4695 - val_mse: 14.4695 - val_mae: 1.5251 - lr: 0.0045 - 2s/epoch - 2ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 2s - loss: 12.3302 - mse: 12.3302 - mae: 1.4830 - val_loss: 14.4244 - val_mse: 14.4244 - val_mae: 1.6589 - lr: 0.0045 - 2s/epoch - 2ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 2s - loss: 12.3436 - mse: 12.3436 - mae: 1.4783 - val_loss: 14.5762 - val_mse: 14.5762 - val_mae: 1.6977 - lr: 0.0045 - 2s/epoch - 2ms/step\n",
            "Epoch 13/100\n",
            "1000/1000 - 2s - loss: 12.2713 - mse: 12.2713 - mae: 1.4793 - val_loss: 14.2489 - val_mse: 14.2489 - val_mae: 1.4821 - lr: 0.0045 - 2s/epoch - 2ms/step\n",
            "Epoch 14/100\n",
            "1000/1000 - 2s - loss: 12.1422 - mse: 12.1422 - mae: 1.4669 - val_loss: 14.6169 - val_mse: 14.6169 - val_mae: 1.5894 - lr: 0.0045 - 2s/epoch - 2ms/step\n",
            "Epoch 15/100\n",
            "1000/1000 - 2s - loss: 12.1856 - mse: 12.1856 - mae: 1.4783 - val_loss: 14.1749 - val_mse: 14.1749 - val_mae: 1.4501 - lr: 0.0045 - 2s/epoch - 2ms/step\n",
            "Epoch 16/100\n",
            "1000/1000 - 2s - loss: 12.1588 - mse: 12.1588 - mae: 1.4773 - val_loss: 14.1700 - val_mse: 14.1700 - val_mae: 1.5032 - lr: 0.0045 - 2s/epoch - 2ms/step\n",
            "Epoch 17/100\n",
            "1000/1000 - 2s - loss: 12.1431 - mse: 12.1431 - mae: 1.4782 - val_loss: 14.1173 - val_mse: 14.1173 - val_mae: 1.4886 - lr: 0.0045 - 2s/epoch - 2ms/step\n",
            "Epoch 18/100\n",
            "1000/1000 - 2s - loss: 12.0542 - mse: 12.0542 - mae: 1.4763 - val_loss: 14.2377 - val_mse: 14.2377 - val_mae: 1.5169 - lr: 0.0045 - 2s/epoch - 2ms/step\n",
            "Epoch 19/100\n",
            "1000/1000 - 2s - loss: 11.9862 - mse: 11.9862 - mae: 1.4790 - val_loss: 14.6910 - val_mse: 14.6910 - val_mae: 1.4587 - lr: 0.0045 - 2s/epoch - 2ms/step\n",
            "Epoch 20/100\n",
            "1000/1000 - 2s - loss: 12.0048 - mse: 12.0048 - mae: 1.4760 - val_loss: 14.8602 - val_mse: 14.8602 - val_mae: 1.8069 - lr: 0.0045 - 2s/epoch - 2ms/step\n",
            "Epoch 21/100\n",
            "1000/1000 - 2s - loss: 11.9771 - mse: 11.9771 - mae: 1.4768 - val_loss: 14.6768 - val_mse: 14.6768 - val_mae: 1.4330 - lr: 0.0045 - 2s/epoch - 2ms/step\n",
            "Epoch 22/100\n",
            "1000/1000 - 2s - loss: 11.9253 - mse: 11.9253 - mae: 1.4653 - val_loss: 14.6454 - val_mse: 14.6454 - val_mae: 1.4397 - lr: 0.0045 - 2s/epoch - 2ms/step\n",
            "Score for fold 1: loss of 14.645447731018066\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 10.7911 - mse: 10.7911 - mae: 1.4383 - val_loss: 17.2421 - val_mse: 17.2421 - val_mae: 1.4344 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 10.4129 - mse: 10.4129 - mae: 1.4219 - val_loss: 17.1188 - val_mse: 17.1188 - val_mae: 1.4464 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 10.2115 - mse: 10.2115 - mae: 1.4109 - val_loss: 17.0815 - val_mse: 17.0815 - val_mae: 1.4447 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 10.0444 - mse: 10.0444 - mae: 1.4064 - val_loss: 17.1961 - val_mse: 17.1961 - val_mae: 1.4640 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 9.9706 - mse: 9.9706 - mae: 1.4029 - val_loss: 17.1381 - val_mse: 17.1381 - val_mae: 1.4376 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 9.8271 - mse: 9.8271 - mae: 1.3990 - val_loss: 17.1465 - val_mse: 17.1465 - val_mae: 1.4510 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 9.6827 - mse: 9.6827 - mae: 1.3912 - val_loss: 17.1231 - val_mse: 17.1231 - val_mae: 1.4714 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 2s - loss: 9.5594 - mse: 9.5594 - mae: 1.3902 - val_loss: 17.3196 - val_mse: 17.3196 - val_mae: 1.5689 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Score for fold 2: loss of 17.319625854492188\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 11.0852 - mse: 11.0852 - mae: 1.3883 - val_loss: 10.8098 - val_mse: 10.8098 - val_mae: 1.4180 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 11.0366 - mse: 11.0366 - mae: 1.3821 - val_loss: 10.9168 - val_mse: 10.9168 - val_mae: 1.3939 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 10.8184 - mse: 10.8184 - mae: 1.3740 - val_loss: 11.1631 - val_mse: 11.1631 - val_mae: 1.4164 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 10.8434 - mse: 10.8434 - mae: 1.3689 - val_loss: 11.1312 - val_mse: 11.1312 - val_mae: 1.4356 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 10.7171 - mse: 10.7171 - mae: 1.3636 - val_loss: 11.2333 - val_mse: 11.2333 - val_mae: 1.4127 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 10.6211 - mse: 10.6211 - mae: 1.3577 - val_loss: 11.3731 - val_mse: 11.3731 - val_mae: 1.3813 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Score for fold 3: loss of 11.373087882995605\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 11.5839 - mse: 11.5839 - mae: 1.3941 - val_loss: 7.0875 - val_mse: 7.0875 - val_mae: 1.3039 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 11.4971 - mse: 11.4971 - mae: 1.3843 - val_loss: 7.1864 - val_mse: 7.1864 - val_mae: 1.3674 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 11.4904 - mse: 11.4904 - mae: 1.3851 - val_loss: 7.2620 - val_mse: 7.2620 - val_mae: 1.3514 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 11.2808 - mse: 11.2808 - mae: 1.3765 - val_loss: 7.4057 - val_mse: 7.4057 - val_mae: 1.3593 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 11.2098 - mse: 11.2098 - mae: 1.3709 - val_loss: 7.3805 - val_mse: 7.3805 - val_mae: 1.3249 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 11.1707 - mse: 11.1707 - mae: 1.3678 - val_loss: 7.4474 - val_mse: 7.4474 - val_mae: 1.3082 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Score for fold 4: loss of 7.447362899780273\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 11.2210 - mse: 11.2210 - mae: 1.3773 - val_loss: 6.9701 - val_mse: 6.9701 - val_mae: 1.3988 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 11.1594 - mse: 11.1594 - mae: 1.3685 - val_loss: 7.0331 - val_mse: 7.0331 - val_mae: 1.3056 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 11.0632 - mse: 11.0632 - mae: 1.3614 - val_loss: 7.1886 - val_mse: 7.1886 - val_mae: 1.3624 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 11.0163 - mse: 11.0163 - mae: 1.3573 - val_loss: 7.2189 - val_mse: 7.2189 - val_mae: 1.3256 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 10.9426 - mse: 10.9426 - mae: 1.3554 - val_loss: 7.3245 - val_mse: 7.3245 - val_mae: 1.3630 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 10.8668 - mse: 10.8668 - mae: 1.3495 - val_loss: 7.3296 - val_mse: 7.3296 - val_mae: 1.3709 - lr: 0.0010 - 2s/epoch - 2ms/step\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-10-12 11:29:55,356]\u001b[0m Finished trial#8 resulted in value: 11.623999999999999. Current best value is 10.693999999999999 with parameters: {'activation': 'relu', 'num_hidden_layer': 3, 'i': 7, 'learning_rate': 0.0034867788821243673}.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Score for fold 5: loss of 7.329595565795898\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 11s - loss: 12.8735 - mse: 12.8735 - mae: 1.6542 - val_loss: 20.3397 - val_mse: 20.3397 - val_mae: 1.4981 - lr: 1.7489e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 10s - loss: 11.9939 - mse: 11.9939 - mae: 1.5716 - val_loss: 20.3674 - val_mse: 20.3674 - val_mae: 1.5848 - lr: 1.7489e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 11s - loss: 11.6977 - mse: 11.6977 - mae: 1.5629 - val_loss: 20.1827 - val_mse: 20.1827 - val_mae: 1.5249 - lr: 1.7489e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 11s - loss: 11.6413 - mse: 11.6413 - mae: 1.5569 - val_loss: 20.0970 - val_mse: 20.0970 - val_mae: 1.5289 - lr: 1.7489e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 11s - loss: 11.5392 - mse: 11.5392 - mae: 1.5538 - val_loss: 20.4171 - val_mse: 20.4171 - val_mae: 1.6057 - lr: 1.7489e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 11s - loss: 11.5883 - mse: 11.5883 - mae: 1.5575 - val_loss: 20.3274 - val_mse: 20.3274 - val_mae: 1.5477 - lr: 1.7489e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 11s - loss: 11.6245 - mse: 11.6245 - mae: 1.5482 - val_loss: 20.2215 - val_mse: 20.2215 - val_mae: 1.5271 - lr: 1.7489e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 10s - loss: 11.6297 - mse: 11.6297 - mae: 1.5490 - val_loss: 20.2061 - val_mse: 20.2061 - val_mae: 1.5921 - lr: 1.7489e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 10s - loss: 11.5860 - mse: 11.5860 - mae: 1.5517 - val_loss: 20.1929 - val_mse: 20.1929 - val_mae: 1.5727 - lr: 1.7489e-04 - 10s/epoch - 10ms/step\n",
            "Score for fold 1: loss of 20.19292449951172\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 11s - loss: 13.8528 - mse: 13.8528 - mae: 1.5593 - val_loss: 10.8856 - val_mse: 10.8856 - val_mae: 1.5333 - lr: 1.7489e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 11s - loss: 13.9661 - mse: 13.9661 - mae: 1.5560 - val_loss: 10.9985 - val_mse: 10.9985 - val_mae: 1.5775 - lr: 1.7489e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 11s - loss: 13.9260 - mse: 13.9260 - mae: 1.5600 - val_loss: 10.9105 - val_mse: 10.9105 - val_mae: 1.5246 - lr: 1.7489e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 11s - loss: 13.8691 - mse: 13.8691 - mae: 1.5564 - val_loss: 10.9282 - val_mse: 10.9282 - val_mae: 1.5480 - lr: 1.7489e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 11s - loss: 13.8136 - mse: 13.8136 - mae: 1.5520 - val_loss: 10.9855 - val_mse: 10.9855 - val_mae: 1.5884 - lr: 1.7489e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 11s - loss: 13.8607 - mse: 13.8607 - mae: 1.5562 - val_loss: 11.2757 - val_mse: 11.2757 - val_mae: 1.5719 - lr: 1.7489e-04 - 11s/epoch - 11ms/step\n",
            "Score for fold 2: loss of 11.275712013244629\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 11s - loss: 13.2257 - mse: 13.2257 - mae: 1.5486 - val_loss: 13.8726 - val_mse: 13.8726 - val_mae: 1.4925 - lr: 1.7489e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 11s - loss: 13.2087 - mse: 13.2087 - mae: 1.5475 - val_loss: 13.5174 - val_mse: 13.5174 - val_mae: 1.5935 - lr: 1.7489e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 11s - loss: 13.1886 - mse: 13.1886 - mae: 1.5505 - val_loss: 13.5314 - val_mse: 13.5314 - val_mae: 1.6048 - lr: 1.7489e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 11s - loss: 13.2109 - mse: 13.2109 - mae: 1.5490 - val_loss: 13.5867 - val_mse: 13.5867 - val_mae: 1.5320 - lr: 1.7489e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 11s - loss: 13.1841 - mse: 13.1841 - mae: 1.5465 - val_loss: 13.5521 - val_mse: 13.5521 - val_mae: 1.5539 - lr: 1.7489e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 11s - loss: 13.2307 - mse: 13.2307 - mae: 1.5454 - val_loss: 13.6281 - val_mse: 13.6281 - val_mae: 1.4972 - lr: 1.7489e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 11s - loss: 13.1857 - mse: 13.1857 - mae: 1.5437 - val_loss: 13.4892 - val_mse: 13.4892 - val_mae: 1.5748 - lr: 1.7489e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 11s - loss: 13.1801 - mse: 13.1801 - mae: 1.5473 - val_loss: 13.5152 - val_mse: 13.5152 - val_mae: 1.5887 - lr: 1.7489e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 11s - loss: 13.1367 - mse: 13.1367 - mae: 1.5443 - val_loss: 13.5441 - val_mse: 13.5441 - val_mae: 1.5368 - lr: 1.7489e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 11s - loss: 13.1474 - mse: 13.1474 - mae: 1.5507 - val_loss: 13.5028 - val_mse: 13.5028 - val_mae: 1.5408 - lr: 1.7489e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 11s - loss: 13.1353 - mse: 13.1353 - mae: 1.5466 - val_loss: 13.6947 - val_mse: 13.6947 - val_mae: 1.5159 - lr: 1.7489e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 11s - loss: 13.1924 - mse: 13.1924 - mae: 1.5477 - val_loss: 13.5628 - val_mse: 13.5628 - val_mae: 1.5345 - lr: 1.7489e-04 - 11s/epoch - 11ms/step\n",
            "Score for fold 3: loss of 13.56284236907959\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 11s - loss: 13.6728 - mse: 13.6728 - mae: 1.5465 - val_loss: 11.6458 - val_mse: 11.6458 - val_mae: 1.5246 - lr: 1.7489e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 11s - loss: 13.6228 - mse: 13.6228 - mae: 1.5548 - val_loss: 11.7628 - val_mse: 11.7628 - val_mae: 1.5030 - lr: 1.7489e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 11s - loss: 13.7005 - mse: 13.7005 - mae: 1.5548 - val_loss: 11.6331 - val_mse: 11.6331 - val_mae: 1.6250 - lr: 1.7489e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 11s - loss: 13.6296 - mse: 13.6296 - mae: 1.5468 - val_loss: 11.6839 - val_mse: 11.6839 - val_mae: 1.5287 - lr: 1.7489e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 11s - loss: 13.5901 - mse: 13.5901 - mae: 1.5523 - val_loss: 11.5710 - val_mse: 11.5710 - val_mae: 1.5439 - lr: 1.7489e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 11s - loss: 13.6650 - mse: 13.6650 - mae: 1.5518 - val_loss: 11.7409 - val_mse: 11.7409 - val_mae: 1.5120 - lr: 1.7489e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 11s - loss: 13.6254 - mse: 13.6254 - mae: 1.5480 - val_loss: 11.8061 - val_mse: 11.8061 - val_mae: 1.5038 - lr: 1.7489e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 11s - loss: 13.7281 - mse: 13.7281 - mae: 1.5524 - val_loss: 11.7590 - val_mse: 11.7590 - val_mae: 1.5229 - lr: 1.7489e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 11s - loss: 13.5768 - mse: 13.5768 - mae: 1.5438 - val_loss: 11.7511 - val_mse: 11.7511 - val_mae: 1.5555 - lr: 1.7489e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 11s - loss: 13.6795 - mse: 13.6795 - mae: 1.5431 - val_loss: 11.7776 - val_mse: 11.7776 - val_mae: 1.4971 - lr: 1.7489e-04 - 11s/epoch - 11ms/step\n",
            "Score for fold 4: loss of 11.777582168579102\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 12s - loss: 14.1181 - mse: 14.1181 - mae: 1.5438 - val_loss: 9.9377 - val_mse: 9.9377 - val_mae: 1.5321 - lr: 1.7489e-04 - 12s/epoch - 12ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 11s - loss: 14.0649 - mse: 14.0649 - mae: 1.5495 - val_loss: 9.9870 - val_mse: 9.9870 - val_mae: 1.5652 - lr: 1.7489e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 11s - loss: 14.0662 - mse: 14.0662 - mae: 1.5453 - val_loss: 10.0980 - val_mse: 10.0980 - val_mae: 1.6358 - lr: 1.7489e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 11s - loss: 14.0268 - mse: 14.0268 - mae: 1.5515 - val_loss: 9.9244 - val_mse: 9.9244 - val_mae: 1.5473 - lr: 1.7489e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 11s - loss: 14.0330 - mse: 14.0330 - mae: 1.5513 - val_loss: 10.0123 - val_mse: 10.0123 - val_mae: 1.5372 - lr: 1.7489e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 10s - loss: 14.0492 - mse: 14.0492 - mae: 1.5446 - val_loss: 9.9150 - val_mse: 9.9150 - val_mae: 1.5541 - lr: 1.7489e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 11s - loss: 14.0664 - mse: 14.0664 - mae: 1.5421 - val_loss: 9.8682 - val_mse: 9.8682 - val_mae: 1.5474 - lr: 1.7489e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 10s - loss: 14.0765 - mse: 14.0765 - mae: 1.5468 - val_loss: 9.9695 - val_mse: 9.9695 - val_mae: 1.5747 - lr: 1.7489e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 10s - loss: 14.0572 - mse: 14.0572 - mae: 1.5450 - val_loss: 9.8730 - val_mse: 9.8730 - val_mae: 1.5414 - lr: 1.7489e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 11s - loss: 14.0359 - mse: 14.0359 - mae: 1.5476 - val_loss: 9.9985 - val_mse: 9.9985 - val_mae: 1.6169 - lr: 1.7489e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 11s - loss: 14.0401 - mse: 14.0401 - mae: 1.5458 - val_loss: 9.9485 - val_mse: 9.9485 - val_mae: 1.5595 - lr: 1.7489e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 12s - loss: 14.0944 - mse: 14.0944 - mae: 1.5506 - val_loss: 9.9301 - val_mse: 9.9301 - val_mae: 1.5554 - lr: 1.7489e-04 - 12s/epoch - 12ms/step\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-10-12 11:38:57,450]\u001b[0m Finished trial#9 resulted in value: 13.348000000000003. Current best value is 10.693999999999999 with parameters: {'activation': 'relu', 'num_hidden_layer': 3, 'i': 7, 'learning_rate': 0.0034867788821243673}.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Score for fold 5: loss of 9.930076599121094\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 9s - loss: 12.5769 - mse: 12.5769 - mae: 1.5380 - val_loss: 14.3790 - val_mse: 14.3790 - val_mae: 1.4474 - lr: 0.0012 - 9s/epoch - 9ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 8s - loss: 11.9620 - mse: 11.9620 - mae: 1.4943 - val_loss: 14.2069 - val_mse: 14.2069 - val_mae: 1.4776 - lr: 0.0012 - 8s/epoch - 8ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 8s - loss: 11.9217 - mse: 11.9217 - mae: 1.4756 - val_loss: 13.8796 - val_mse: 13.8796 - val_mae: 1.4900 - lr: 0.0012 - 8s/epoch - 8ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 8s - loss: 11.6032 - mse: 11.6032 - mae: 1.4621 - val_loss: 13.9140 - val_mse: 13.9140 - val_mae: 1.5505 - lr: 0.0012 - 8s/epoch - 8ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 8s - loss: 11.5751 - mse: 11.5751 - mae: 1.4584 - val_loss: 13.8430 - val_mse: 13.8430 - val_mae: 1.4192 - lr: 0.0012 - 8s/epoch - 8ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 8s - loss: 11.4869 - mse: 11.4869 - mae: 1.4481 - val_loss: 13.8450 - val_mse: 13.8450 - val_mae: 1.5266 - lr: 0.0012 - 8s/epoch - 8ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 8s - loss: 11.3827 - mse: 11.3827 - mae: 1.4413 - val_loss: 13.8256 - val_mse: 13.8256 - val_mae: 1.4497 - lr: 0.0012 - 8s/epoch - 8ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 8s - loss: 11.1659 - mse: 11.1659 - mae: 1.4349 - val_loss: 13.8117 - val_mse: 13.8117 - val_mae: 1.4383 - lr: 0.0012 - 8s/epoch - 8ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 8s - loss: 11.1326 - mse: 11.1326 - mae: 1.4276 - val_loss: 13.7851 - val_mse: 13.7851 - val_mae: 1.4263 - lr: 0.0012 - 8s/epoch - 8ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 8s - loss: 11.0921 - mse: 11.0921 - mae: 1.4239 - val_loss: 13.8593 - val_mse: 13.8593 - val_mae: 1.4927 - lr: 0.0012 - 8s/epoch - 8ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 8s - loss: 10.9586 - mse: 10.9586 - mae: 1.4271 - val_loss: 13.7776 - val_mse: 13.7776 - val_mae: 1.5195 - lr: 0.0012 - 8s/epoch - 8ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 8s - loss: 10.8519 - mse: 10.8519 - mae: 1.4321 - val_loss: 13.7415 - val_mse: 13.7415 - val_mae: 1.4833 - lr: 0.0012 - 8s/epoch - 8ms/step\n",
            "Epoch 13/100\n",
            "1000/1000 - 8s - loss: 10.7062 - mse: 10.7062 - mae: 1.4274 - val_loss: 14.0145 - val_mse: 14.0145 - val_mae: 1.4955 - lr: 0.0012 - 8s/epoch - 8ms/step\n",
            "Epoch 14/100\n",
            "1000/1000 - 8s - loss: 10.7320 - mse: 10.7320 - mae: 1.4148 - val_loss: 14.0461 - val_mse: 14.0461 - val_mae: 1.4393 - lr: 0.0012 - 8s/epoch - 8ms/step\n",
            "Epoch 15/100\n",
            "1000/1000 - 7s - loss: 10.7123 - mse: 10.7123 - mae: 1.4080 - val_loss: 13.9960 - val_mse: 13.9960 - val_mae: 1.4638 - lr: 0.0012 - 7s/epoch - 7ms/step\n",
            "Epoch 16/100\n",
            "1000/1000 - 7s - loss: 10.4795 - mse: 10.4795 - mae: 1.3963 - val_loss: 13.9455 - val_mse: 13.9455 - val_mae: 1.4651 - lr: 0.0012 - 7s/epoch - 7ms/step\n",
            "Epoch 17/100\n",
            "1000/1000 - 7s - loss: 10.4620 - mse: 10.4620 - mae: 1.3935 - val_loss: 13.9872 - val_mse: 13.9872 - val_mae: 1.4657 - lr: 0.0012 - 7s/epoch - 7ms/step\n",
            "Score for fold 1: loss of 13.987200736999512\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 7s - loss: 11.7312 - mse: 11.7312 - mae: 1.4077 - val_loss: 7.6319 - val_mse: 7.6319 - val_mae: 1.4212 - lr: 0.0010 - 7s/epoch - 7ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 7s - loss: 11.4218 - mse: 11.4218 - mae: 1.3977 - val_loss: 7.7535 - val_mse: 7.7535 - val_mae: 1.3917 - lr: 0.0010 - 7s/epoch - 7ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 7s - loss: 11.3671 - mse: 11.3671 - mae: 1.3848 - val_loss: 7.7418 - val_mse: 7.7418 - val_mae: 1.3697 - lr: 0.0010 - 7s/epoch - 7ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 6s - loss: 11.1961 - mse: 11.1961 - mae: 1.3820 - val_loss: 7.8741 - val_mse: 7.8741 - val_mae: 1.3527 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 7s - loss: 11.0262 - mse: 11.0262 - mae: 1.3704 - val_loss: 8.1318 - val_mse: 8.1318 - val_mae: 1.4509 - lr: 0.0010 - 7s/epoch - 7ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 7s - loss: 10.7557 - mse: 10.7557 - mae: 1.3577 - val_loss: 8.0289 - val_mse: 8.0289 - val_mae: 1.4010 - lr: 0.0010 - 7s/epoch - 7ms/step\n",
            "Score for fold 2: loss of 8.028922080993652\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 7s - loss: 8.9878 - mse: 8.9878 - mae: 1.3621 - val_loss: 15.6430 - val_mse: 15.6430 - val_mae: 1.3558 - lr: 0.0010 - 7s/epoch - 7ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 7s - loss: 8.7108 - mse: 8.7108 - mae: 1.3538 - val_loss: 15.2065 - val_mse: 15.2065 - val_mae: 1.3841 - lr: 0.0010 - 7s/epoch - 7ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 7s - loss: 8.6512 - mse: 8.6512 - mae: 1.3437 - val_loss: 16.1255 - val_mse: 16.1255 - val_mae: 1.3754 - lr: 0.0010 - 7s/epoch - 7ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 7s - loss: 8.2544 - mse: 8.2544 - mae: 1.3281 - val_loss: 17.2095 - val_mse: 17.2095 - val_mae: 1.4078 - lr: 0.0010 - 7s/epoch - 7ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 7s - loss: 8.2588 - mse: 8.2588 - mae: 1.3178 - val_loss: 16.3724 - val_mse: 16.3724 - val_mae: 1.3891 - lr: 0.0010 - 7s/epoch - 7ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 7s - loss: 8.0568 - mse: 8.0568 - mae: 1.3057 - val_loss: 16.9897 - val_mse: 16.9897 - val_mae: 1.4355 - lr: 0.0010 - 7s/epoch - 7ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 7s - loss: 7.9834 - mse: 7.9834 - mae: 1.2954 - val_loss: 15.7245 - val_mse: 15.7245 - val_mae: 1.3857 - lr: 0.0010 - 7s/epoch - 7ms/step\n",
            "Score for fold 3: loss of 15.724512100219727\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 7s - loss: 10.5082 - mse: 10.5082 - mae: 1.3273 - val_loss: 6.8346 - val_mse: 6.8346 - val_mae: 1.3087 - lr: 0.0010 - 7s/epoch - 7ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 7s - loss: 10.1638 - mse: 10.1638 - mae: 1.3092 - val_loss: 7.0644 - val_mse: 7.0644 - val_mae: 1.3323 - lr: 0.0010 - 7s/epoch - 7ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 7s - loss: 9.8479 - mse: 9.8479 - mae: 1.2951 - val_loss: 6.7378 - val_mse: 6.7378 - val_mae: 1.2894 - lr: 0.0010 - 7s/epoch - 7ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 7s - loss: 9.6233 - mse: 9.6233 - mae: 1.2824 - val_loss: 6.5295 - val_mse: 6.5295 - val_mae: 1.3502 - lr: 0.0010 - 7s/epoch - 7ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 7s - loss: 9.6190 - mse: 9.6190 - mae: 1.2687 - val_loss: 6.8397 - val_mse: 6.8397 - val_mae: 1.3190 - lr: 0.0010 - 7s/epoch - 7ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 7s - loss: 9.2282 - mse: 9.2282 - mae: 1.2531 - val_loss: 7.0484 - val_mse: 7.0484 - val_mae: 1.3738 - lr: 0.0010 - 7s/epoch - 7ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 7s - loss: 9.2104 - mse: 9.2104 - mae: 1.2395 - val_loss: 7.3723 - val_mse: 7.3723 - val_mae: 1.3430 - lr: 0.0010 - 7s/epoch - 7ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 6s - loss: 8.9288 - mse: 8.9288 - mae: 1.2295 - val_loss: 7.4323 - val_mse: 7.4323 - val_mae: 1.3153 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 6s - loss: 8.8174 - mse: 8.8174 - mae: 1.2129 - val_loss: 7.2657 - val_mse: 7.2657 - val_mae: 1.3165 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
            "Score for fold 4: loss of 7.265725135803223\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 6s - loss: 9.0689 - mse: 9.0689 - mae: 1.2614 - val_loss: 6.1877 - val_mse: 6.1877 - val_mae: 1.2075 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 6s - loss: 8.6477 - mse: 8.6477 - mae: 1.2392 - val_loss: 6.6275 - val_mse: 6.6275 - val_mae: 1.1809 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 6s - loss: 8.6553 - mse: 8.6553 - mae: 1.2250 - val_loss: 6.4697 - val_mse: 6.4697 - val_mae: 1.2213 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 6s - loss: 8.2828 - mse: 8.2828 - mae: 1.2111 - val_loss: 6.4579 - val_mse: 6.4579 - val_mae: 1.2437 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 6s - loss: 8.1876 - mse: 8.1876 - mae: 1.1939 - val_loss: 6.7639 - val_mse: 6.7639 - val_mae: 1.2566 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 6s - loss: 8.0286 - mse: 8.0286 - mae: 1.1861 - val_loss: 6.8404 - val_mse: 6.8404 - val_mae: 1.2661 - lr: 0.0010 - 6s/epoch - 6ms/step\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-10-12 11:44:27,617]\u001b[0m Finished trial#10 resulted in value: 10.370000000000001. Current best value is 10.370000000000001 with parameters: {'activation': 'relu', 'num_hidden_layer': 3, 'i': 10, 'learning_rate': 0.0011741866408039843}.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Score for fold 5: loss of 6.840414047241211\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 7s - loss: 13.8143 - mse: 13.8143 - mae: 1.5458 - val_loss: 10.0403 - val_mse: 10.0403 - val_mae: 1.5414 - lr: 0.0010 - 7s/epoch - 7ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 6s - loss: 12.9788 - mse: 12.9788 - mae: 1.5063 - val_loss: 10.0958 - val_mse: 10.0958 - val_mae: 1.4840 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 6s - loss: 12.8569 - mse: 12.8569 - mae: 1.4781 - val_loss: 9.8374 - val_mse: 9.8374 - val_mae: 1.4258 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 6s - loss: 12.6263 - mse: 12.6263 - mae: 1.4728 - val_loss: 9.6620 - val_mse: 9.6620 - val_mae: 1.4978 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 6s - loss: 12.6776 - mse: 12.6776 - mae: 1.4607 - val_loss: 10.0175 - val_mse: 10.0175 - val_mae: 1.4238 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 6s - loss: 12.4527 - mse: 12.4527 - mae: 1.4562 - val_loss: 9.7871 - val_mse: 9.7871 - val_mae: 1.4525 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 6s - loss: 12.7202 - mse: 12.7202 - mae: 1.4551 - val_loss: 10.1273 - val_mse: 10.1273 - val_mae: 1.4162 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 6s - loss: 12.3950 - mse: 12.3950 - mae: 1.4408 - val_loss: 10.5456 - val_mse: 10.5456 - val_mae: 1.4339 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 6s - loss: 12.1108 - mse: 12.1108 - mae: 1.4406 - val_loss: 9.8383 - val_mse: 9.8383 - val_mae: 1.4174 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
            "Score for fold 1: loss of 9.838254928588867\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 6s - loss: 11.5310 - mse: 11.5310 - mae: 1.4281 - val_loss: 12.7480 - val_mse: 12.7480 - val_mae: 1.4214 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 6s - loss: 11.0005 - mse: 11.0005 - mae: 1.4186 - val_loss: 13.4763 - val_mse: 13.4763 - val_mae: 1.4658 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 6s - loss: 11.6318 - mse: 11.6318 - mae: 1.4232 - val_loss: 12.6277 - val_mse: 12.6277 - val_mae: 1.4615 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 6s - loss: 10.9046 - mse: 10.9046 - mae: 1.4210 - val_loss: 12.8755 - val_mse: 12.8755 - val_mae: 1.4636 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 6s - loss: 10.7792 - mse: 10.7792 - mae: 1.4133 - val_loss: 13.1902 - val_mse: 13.1902 - val_mae: 1.4209 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 6s - loss: 10.5339 - mse: 10.5339 - mae: 1.4041 - val_loss: 14.2181 - val_mse: 14.2181 - val_mae: 1.4423 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 6s - loss: 10.5620 - mse: 10.5620 - mae: 1.3957 - val_loss: 12.9381 - val_mse: 12.9381 - val_mae: 1.4773 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 6s - loss: 10.5336 - mse: 10.5336 - mae: 1.3854 - val_loss: 12.7822 - val_mse: 12.7822 - val_mae: 1.4453 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
            "Score for fold 2: loss of 12.782204627990723\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 6s - loss: 8.9132 - mse: 8.9132 - mae: 1.4014 - val_loss: 18.4693 - val_mse: 18.4693 - val_mae: 1.4097 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 6s - loss: 8.9787 - mse: 8.9787 - mae: 1.3931 - val_loss: 18.3853 - val_mse: 18.3853 - val_mae: 1.4066 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 6s - loss: 8.9513 - mse: 8.9513 - mae: 1.3791 - val_loss: 18.5619 - val_mse: 18.5619 - val_mae: 1.4120 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 6s - loss: 8.4647 - mse: 8.4647 - mae: 1.3643 - val_loss: 19.1377 - val_mse: 19.1377 - val_mae: 1.3950 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 6s - loss: 8.4587 - mse: 8.4587 - mae: 1.3563 - val_loss: 18.7167 - val_mse: 18.7167 - val_mae: 1.4301 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 6s - loss: 8.3054 - mse: 8.3054 - mae: 1.3487 - val_loss: 19.1315 - val_mse: 19.1315 - val_mae: 1.3978 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 6s - loss: 8.2321 - mse: 8.2321 - mae: 1.3323 - val_loss: 18.8443 - val_mse: 18.8443 - val_mae: 1.4381 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
            "Score for fold 3: loss of 18.844350814819336\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 6s - loss: 11.3844 - mse: 11.3844 - mae: 1.3621 - val_loss: 5.7714 - val_mse: 5.7714 - val_mae: 1.3107 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 6s - loss: 10.9022 - mse: 10.9022 - mae: 1.3440 - val_loss: 5.6416 - val_mse: 5.6416 - val_mae: 1.2971 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 6s - loss: 10.8814 - mse: 10.8814 - mae: 1.3342 - val_loss: 5.5633 - val_mse: 5.5633 - val_mae: 1.3607 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 6s - loss: 11.1461 - mse: 11.1461 - mae: 1.3276 - val_loss: 5.9248 - val_mse: 5.9248 - val_mae: 1.3322 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 6s - loss: 10.5767 - mse: 10.5767 - mae: 1.3123 - val_loss: 5.9873 - val_mse: 5.9873 - val_mae: 1.3536 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 6s - loss: 10.4091 - mse: 10.4091 - mae: 1.3015 - val_loss: 5.7074 - val_mse: 5.7074 - val_mae: 1.3142 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 6s - loss: 10.1695 - mse: 10.1695 - mae: 1.2854 - val_loss: 5.9106 - val_mse: 5.9106 - val_mae: 1.3559 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 6s - loss: 9.8233 - mse: 9.8233 - mae: 1.2687 - val_loss: 6.4910 - val_mse: 6.4910 - val_mae: 1.3314 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
            "Score for fold 4: loss of 6.491039276123047\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 6s - loss: 9.8269 - mse: 9.8269 - mae: 1.2947 - val_loss: 6.4371 - val_mse: 6.4371 - val_mae: 1.2345 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 6s - loss: 9.4701 - mse: 9.4701 - mae: 1.2727 - val_loss: 6.4489 - val_mse: 6.4489 - val_mae: 1.2853 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 6s - loss: 9.2754 - mse: 9.2754 - mae: 1.2577 - val_loss: 6.4112 - val_mse: 6.4112 - val_mae: 1.2723 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 6s - loss: 9.2074 - mse: 9.2074 - mae: 1.2394 - val_loss: 6.9394 - val_mse: 6.9394 - val_mae: 1.3088 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 6s - loss: 8.9956 - mse: 8.9956 - mae: 1.2290 - val_loss: 7.1539 - val_mse: 7.1539 - val_mae: 1.3248 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 6s - loss: 8.7575 - mse: 8.7575 - mae: 1.2123 - val_loss: 7.1649 - val_mse: 7.1649 - val_mae: 1.3372 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 6s - loss: 8.6113 - mse: 8.6113 - mae: 1.2023 - val_loss: 7.2917 - val_mse: 7.2917 - val_mae: 1.3074 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 6s - loss: 8.2915 - mse: 8.2915 - mae: 1.1829 - val_loss: 7.5214 - val_mse: 7.5214 - val_mae: 1.3396 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
            "Score for fold 5: loss of 7.521367073059082\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-10-12 11:48:45,613]\u001b[0m Finished trial#11 resulted in value: 11.094. Current best value is 10.370000000000001 with parameters: {'activation': 'relu', 'num_hidden_layer': 3, 'i': 10, 'learning_rate': 0.0011741866408039843}.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 4s - loss: 13.7861 - mse: 13.7861 - mae: 1.5462 - val_loss: 9.6361 - val_mse: 9.6361 - val_mae: 1.4716 - lr: 0.0012 - 4s/epoch - 4ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 4s - loss: 13.1074 - mse: 13.1074 - mae: 1.4966 - val_loss: 9.5476 - val_mse: 9.5476 - val_mae: 1.4706 - lr: 0.0012 - 4s/epoch - 4ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 4s - loss: 13.0009 - mse: 13.0009 - mae: 1.4800 - val_loss: 9.0212 - val_mse: 9.0212 - val_mae: 1.4324 - lr: 0.0012 - 4s/epoch - 4ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 4s - loss: 12.7718 - mse: 12.7718 - mae: 1.4682 - val_loss: 9.2225 - val_mse: 9.2225 - val_mae: 1.3877 - lr: 0.0012 - 4s/epoch - 4ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 4s - loss: 12.6921 - mse: 12.6921 - mae: 1.4633 - val_loss: 9.3408 - val_mse: 9.3408 - val_mae: 1.4268 - lr: 0.0012 - 4s/epoch - 4ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 4s - loss: 12.5623 - mse: 12.5623 - mae: 1.4535 - val_loss: 9.2358 - val_mse: 9.2358 - val_mae: 1.4593 - lr: 0.0012 - 4s/epoch - 4ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 4s - loss: 12.5041 - mse: 12.5041 - mae: 1.4489 - val_loss: 9.2133 - val_mse: 9.2133 - val_mae: 1.4599 - lr: 0.0012 - 4s/epoch - 4ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 4s - loss: 12.4176 - mse: 12.4176 - mae: 1.4446 - val_loss: 9.2643 - val_mse: 9.2643 - val_mae: 1.4060 - lr: 0.0012 - 4s/epoch - 4ms/step\n",
            "Score for fold 1: loss of 9.26425838470459\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 4s - loss: 12.5537 - mse: 12.5537 - mae: 1.4387 - val_loss: 7.9146 - val_mse: 7.9146 - val_mae: 1.4556 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 4s - loss: 12.4827 - mse: 12.4827 - mae: 1.4376 - val_loss: 8.1198 - val_mse: 8.1198 - val_mae: 1.4294 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 4s - loss: 12.0211 - mse: 12.0211 - mae: 1.4319 - val_loss: 7.8403 - val_mse: 7.8403 - val_mae: 1.4022 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 4s - loss: 12.0708 - mse: 12.0708 - mae: 1.4261 - val_loss: 8.5570 - val_mse: 8.5570 - val_mae: 1.4348 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 4s - loss: 12.1778 - mse: 12.1778 - mae: 1.4244 - val_loss: 8.1245 - val_mse: 8.1245 - val_mae: 1.4257 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 4s - loss: 11.5773 - mse: 11.5773 - mae: 1.4128 - val_loss: 11.3165 - val_mse: 11.3165 - val_mae: 1.4332 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 4s - loss: 11.7117 - mse: 11.7117 - mae: 1.4078 - val_loss: 8.7950 - val_mse: 8.7950 - val_mae: 1.4489 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 4s - loss: 11.4458 - mse: 11.4458 - mae: 1.3977 - val_loss: 8.6988 - val_mse: 8.6988 - val_mae: 1.4551 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Score for fold 2: loss of 8.698761940002441\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 4s - loss: 8.4986 - mse: 8.4986 - mae: 1.3885 - val_loss: 20.4816 - val_mse: 20.4816 - val_mae: 1.4514 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 4s - loss: 8.2630 - mse: 8.2630 - mae: 1.3803 - val_loss: 19.8489 - val_mse: 19.8489 - val_mae: 1.4734 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 4s - loss: 8.1300 - mse: 8.1300 - mae: 1.3660 - val_loss: 20.3818 - val_mse: 20.3818 - val_mae: 1.4590 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 4s - loss: 7.9898 - mse: 7.9898 - mae: 1.3597 - val_loss: 21.0422 - val_mse: 21.0422 - val_mae: 1.4709 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 4s - loss: 7.9315 - mse: 7.9315 - mae: 1.3518 - val_loss: 21.3679 - val_mse: 21.3679 - val_mae: 1.4454 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 4s - loss: 7.7663 - mse: 7.7663 - mae: 1.3435 - val_loss: 21.3263 - val_mse: 21.3263 - val_mae: 1.4404 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 4s - loss: 7.7261 - mse: 7.7261 - mae: 1.3344 - val_loss: 21.7097 - val_mse: 21.7097 - val_mae: 1.4946 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Score for fold 3: loss of 21.709688186645508\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 4s - loss: 11.1264 - mse: 11.1264 - mae: 1.3768 - val_loss: 6.6118 - val_mse: 6.6118 - val_mae: 1.2657 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 4s - loss: 11.2536 - mse: 11.2536 - mae: 1.3650 - val_loss: 6.6096 - val_mse: 6.6096 - val_mae: 1.2999 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 4s - loss: 10.8129 - mse: 10.8129 - mae: 1.3497 - val_loss: 6.8877 - val_mse: 6.8877 - val_mae: 1.3656 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 4s - loss: 10.6425 - mse: 10.6425 - mae: 1.3430 - val_loss: 6.7454 - val_mse: 6.7454 - val_mae: 1.3636 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 4s - loss: 10.5868 - mse: 10.5868 - mae: 1.3310 - val_loss: 6.8826 - val_mse: 6.8826 - val_mae: 1.3424 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 4s - loss: 10.5591 - mse: 10.5591 - mae: 1.3267 - val_loss: 7.0527 - val_mse: 7.0527 - val_mae: 1.4041 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 4s - loss: 10.5354 - mse: 10.5354 - mae: 1.3144 - val_loss: 6.9079 - val_mse: 6.9079 - val_mae: 1.3839 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Score for fold 4: loss of 6.907886028289795\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 4s - loss: 9.8210 - mse: 9.8210 - mae: 1.3290 - val_loss: 8.7763 - val_mse: 8.7763 - val_mae: 1.2614 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 4s - loss: 9.4626 - mse: 9.4626 - mae: 1.3087 - val_loss: 9.0380 - val_mse: 9.0380 - val_mae: 1.3251 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 4s - loss: 9.2921 - mse: 9.2921 - mae: 1.2979 - val_loss: 9.5155 - val_mse: 9.5155 - val_mae: 1.3136 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 4s - loss: 9.1252 - mse: 9.1252 - mae: 1.2858 - val_loss: 10.3998 - val_mse: 10.3998 - val_mae: 1.3639 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 4s - loss: 9.2283 - mse: 9.2283 - mae: 1.2781 - val_loss: 9.6683 - val_mse: 9.6683 - val_mae: 1.3457 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 4s - loss: 8.8757 - mse: 8.8757 - mae: 1.2622 - val_loss: 9.4817 - val_mse: 9.4817 - val_mae: 1.3408 - lr: 0.0010 - 4s/epoch - 4ms/step\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-10-12 11:51:04,545]\u001b[0m Finished trial#12 resulted in value: 11.212. Current best value is 10.370000000000001 with parameters: {'activation': 'relu', 'num_hidden_layer': 3, 'i': 10, 'learning_rate': 0.0011741866408039843}.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Score for fold 5: loss of 9.481728553771973\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 4s - loss: 14.2236 - mse: 14.2236 - mae: 1.5605 - val_loss: 8.9322 - val_mse: 8.9322 - val_mae: 1.4140 - lr: 0.0019 - 4s/epoch - 4ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 4s - loss: 13.3469 - mse: 13.3469 - mae: 1.5049 - val_loss: 9.0289 - val_mse: 9.0289 - val_mae: 1.4208 - lr: 0.0019 - 4s/epoch - 4ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 4s - loss: 13.2666 - mse: 13.2666 - mae: 1.4880 - val_loss: 8.8408 - val_mse: 8.8408 - val_mae: 1.4386 - lr: 0.0019 - 4s/epoch - 4ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 4s - loss: 13.1826 - mse: 13.1826 - mae: 1.4790 - val_loss: 8.6496 - val_mse: 8.6496 - val_mae: 1.4554 - lr: 0.0019 - 4s/epoch - 4ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 4s - loss: 13.0494 - mse: 13.0494 - mae: 1.4693 - val_loss: 8.4841 - val_mse: 8.4841 - val_mae: 1.4605 - lr: 0.0019 - 4s/epoch - 4ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 4s - loss: 12.7891 - mse: 12.7891 - mae: 1.4625 - val_loss: 8.4769 - val_mse: 8.4769 - val_mae: 1.4543 - lr: 0.0019 - 4s/epoch - 4ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 4s - loss: 12.9733 - mse: 12.9733 - mae: 1.4659 - val_loss: 8.3693 - val_mse: 8.3693 - val_mae: 1.5194 - lr: 0.0019 - 4s/epoch - 4ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 4s - loss: 12.7008 - mse: 12.7008 - mae: 1.4681 - val_loss: 8.7731 - val_mse: 8.7731 - val_mae: 1.4482 - lr: 0.0019 - 4s/epoch - 4ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 4s - loss: 12.6950 - mse: 12.6950 - mae: 1.4586 - val_loss: 8.5601 - val_mse: 8.5601 - val_mae: 1.4198 - lr: 0.0019 - 4s/epoch - 4ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 4s - loss: 12.6331 - mse: 12.6331 - mae: 1.4473 - val_loss: 8.5509 - val_mse: 8.5509 - val_mae: 1.4262 - lr: 0.0019 - 4s/epoch - 4ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 4s - loss: 12.5252 - mse: 12.5252 - mae: 1.4421 - val_loss: 8.3945 - val_mse: 8.3945 - val_mae: 1.4009 - lr: 0.0019 - 4s/epoch - 4ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 4s - loss: 12.4254 - mse: 12.4254 - mae: 1.4326 - val_loss: 8.3009 - val_mse: 8.3009 - val_mae: 1.4536 - lr: 0.0019 - 4s/epoch - 4ms/step\n",
            "Epoch 13/100\n",
            "1000/1000 - 4s - loss: 12.2264 - mse: 12.2264 - mae: 1.4270 - val_loss: 8.3579 - val_mse: 8.3579 - val_mae: 1.4527 - lr: 0.0019 - 4s/epoch - 4ms/step\n",
            "Epoch 14/100\n",
            "1000/1000 - 4s - loss: 12.3345 - mse: 12.3345 - mae: 1.4287 - val_loss: 8.4965 - val_mse: 8.4965 - val_mae: 1.4388 - lr: 0.0019 - 4s/epoch - 4ms/step\n",
            "Epoch 15/100\n",
            "1000/1000 - 4s - loss: 11.8075 - mse: 11.8075 - mae: 1.4203 - val_loss: 8.1746 - val_mse: 8.1746 - val_mae: 1.4233 - lr: 0.0019 - 4s/epoch - 4ms/step\n",
            "Epoch 16/100\n",
            "1000/1000 - 4s - loss: 11.9223 - mse: 11.9223 - mae: 1.4154 - val_loss: 8.7795 - val_mse: 8.7795 - val_mae: 1.5684 - lr: 0.0019 - 4s/epoch - 4ms/step\n",
            "Epoch 17/100\n",
            "1000/1000 - 4s - loss: 11.6852 - mse: 11.6852 - mae: 1.4094 - val_loss: 8.5697 - val_mse: 8.5697 - val_mae: 1.4139 - lr: 0.0019 - 4s/epoch - 4ms/step\n",
            "Epoch 18/100\n",
            "1000/1000 - 4s - loss: 11.6966 - mse: 11.6966 - mae: 1.4076 - val_loss: 8.3716 - val_mse: 8.3716 - val_mae: 1.4345 - lr: 0.0019 - 4s/epoch - 4ms/step\n",
            "Epoch 19/100\n",
            "1000/1000 - 4s - loss: 11.6892 - mse: 11.6892 - mae: 1.4051 - val_loss: 8.5091 - val_mse: 8.5091 - val_mae: 1.4626 - lr: 0.0019 - 4s/epoch - 4ms/step\n",
            "Epoch 20/100\n",
            "1000/1000 - 4s - loss: 11.5585 - mse: 11.5585 - mae: 1.3974 - val_loss: 8.5892 - val_mse: 8.5892 - val_mae: 1.5109 - lr: 0.0019 - 4s/epoch - 4ms/step\n",
            "Score for fold 1: loss of 8.589223861694336\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 4s - loss: 9.1505 - mse: 9.1505 - mae: 1.3789 - val_loss: 16.1242 - val_mse: 16.1242 - val_mae: 1.3803 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 4s - loss: 8.7126 - mse: 8.7126 - mae: 1.3630 - val_loss: 16.6105 - val_mse: 16.6105 - val_mae: 1.3884 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 4s - loss: 8.6829 - mse: 8.6829 - mae: 1.3552 - val_loss: 16.3233 - val_mse: 16.3233 - val_mae: 1.4099 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 4s - loss: 8.4258 - mse: 8.4258 - mae: 1.3431 - val_loss: 18.0565 - val_mse: 18.0565 - val_mae: 1.4192 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 4s - loss: 8.2584 - mse: 8.2584 - mae: 1.3335 - val_loss: 16.3964 - val_mse: 16.3964 - val_mae: 1.3702 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 4s - loss: 8.1389 - mse: 8.1389 - mae: 1.3281 - val_loss: 16.3050 - val_mse: 16.3050 - val_mae: 1.4017 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Score for fold 2: loss of 16.30497932434082\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 4s - loss: 9.8774 - mse: 9.8774 - mae: 1.3484 - val_loss: 9.2000 - val_mse: 9.2000 - val_mae: 1.2977 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 4s - loss: 9.8674 - mse: 9.8674 - mae: 1.3386 - val_loss: 9.3671 - val_mse: 9.3671 - val_mae: 1.3097 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 4s - loss: 9.5836 - mse: 9.5836 - mae: 1.3237 - val_loss: 9.3410 - val_mse: 9.3410 - val_mae: 1.2968 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 4s - loss: 9.4731 - mse: 9.4731 - mae: 1.3159 - val_loss: 9.5910 - val_mse: 9.5910 - val_mae: 1.3439 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 4s - loss: 9.3181 - mse: 9.3181 - mae: 1.3096 - val_loss: 9.3563 - val_mse: 9.3563 - val_mae: 1.3336 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 4s - loss: 9.2207 - mse: 9.2207 - mae: 1.2996 - val_loss: 9.6798 - val_mse: 9.6798 - val_mae: 1.3520 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Score for fold 3: loss of 9.679786682128906\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 4s - loss: 9.4673 - mse: 9.4673 - mae: 1.3188 - val_loss: 9.2655 - val_mse: 9.2655 - val_mae: 1.2813 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 4s - loss: 9.1297 - mse: 9.1297 - mae: 1.3001 - val_loss: 9.5117 - val_mse: 9.5117 - val_mae: 1.3402 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 4s - loss: 9.0590 - mse: 9.0590 - mae: 1.2922 - val_loss: 9.5530 - val_mse: 9.5530 - val_mae: 1.2832 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 4s - loss: 8.9244 - mse: 8.9244 - mae: 1.2776 - val_loss: 9.4989 - val_mse: 9.4989 - val_mae: 1.3341 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 4s - loss: 8.6961 - mse: 8.6961 - mae: 1.2714 - val_loss: 9.7272 - val_mse: 9.7272 - val_mae: 1.3256 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 4s - loss: 8.7792 - mse: 8.7792 - mae: 1.2645 - val_loss: 9.9322 - val_mse: 9.9322 - val_mae: 1.3263 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Score for fold 4: loss of 9.932246208190918\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 4s - loss: 9.6113 - mse: 9.6113 - mae: 1.2781 - val_loss: 5.8659 - val_mse: 5.8659 - val_mae: 1.2798 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 4s - loss: 9.5546 - mse: 9.5546 - mae: 1.2694 - val_loss: 5.7922 - val_mse: 5.7922 - val_mae: 1.2763 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 4s - loss: 9.4538 - mse: 9.4538 - mae: 1.2577 - val_loss: 6.4017 - val_mse: 6.4017 - val_mae: 1.2573 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 4s - loss: 9.1115 - mse: 9.1115 - mae: 1.2481 - val_loss: 6.9803 - val_mse: 6.9803 - val_mae: 1.2548 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 4s - loss: 9.1537 - mse: 9.1537 - mae: 1.2398 - val_loss: 6.3173 - val_mse: 6.3173 - val_mae: 1.2854 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 4s - loss: 8.9894 - mse: 8.9894 - mae: 1.2302 - val_loss: 6.3533 - val_mse: 6.3533 - val_mae: 1.2998 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 4s - loss: 8.8484 - mse: 8.8484 - mae: 1.2213 - val_loss: 6.3936 - val_mse: 6.3936 - val_mae: 1.3028 - lr: 0.0010 - 4s/epoch - 4ms/step\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-10-12 11:54:00,250]\u001b[0m Finished trial#13 resulted in value: 10.178. Current best value is 10.178 with parameters: {'activation': 'relu', 'num_hidden_layer': 3, 'i': 9, 'learning_rate': 0.0019093737064063846}.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Score for fold 5: loss of 6.3936028480529785\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 14.1509 - mse: 14.1509 - mae: 1.5563 - val_loss: 10.3389 - val_mse: 10.3389 - val_mae: 1.5105 - lr: 5.3150e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 13.2923 - mse: 13.2923 - mae: 1.5081 - val_loss: 9.9850 - val_mse: 9.9850 - val_mae: 1.4616 - lr: 5.3150e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 13.0547 - mse: 13.0547 - mae: 1.4913 - val_loss: 9.7775 - val_mse: 9.7775 - val_mae: 1.4405 - lr: 5.3150e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 12.9035 - mse: 12.9035 - mae: 1.4801 - val_loss: 9.6152 - val_mse: 9.6152 - val_mae: 1.4972 - lr: 5.3150e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 12.6958 - mse: 12.6958 - mae: 1.4739 - val_loss: 9.6131 - val_mse: 9.6131 - val_mae: 1.4831 - lr: 5.3150e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 12.6497 - mse: 12.6497 - mae: 1.4680 - val_loss: 9.4398 - val_mse: 9.4398 - val_mae: 1.4500 - lr: 5.3150e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 12.5443 - mse: 12.5443 - mae: 1.4636 - val_loss: 9.6887 - val_mse: 9.6887 - val_mae: 1.5545 - lr: 5.3150e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 2s - loss: 12.5760 - mse: 12.5760 - mae: 1.4619 - val_loss: 9.4040 - val_mse: 9.4040 - val_mae: 1.4782 - lr: 5.3150e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 2s - loss: 12.5697 - mse: 12.5697 - mae: 1.4568 - val_loss: 9.4305 - val_mse: 9.4305 - val_mae: 1.4352 - lr: 5.3150e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 2s - loss: 12.4996 - mse: 12.4996 - mae: 1.4544 - val_loss: 9.4195 - val_mse: 9.4195 - val_mae: 1.4916 - lr: 5.3150e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 2s - loss: 12.4543 - mse: 12.4543 - mae: 1.4519 - val_loss: 9.3649 - val_mse: 9.3649 - val_mae: 1.4104 - lr: 5.3150e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 2s - loss: 12.3771 - mse: 12.3771 - mae: 1.4527 - val_loss: 9.4403 - val_mse: 9.4403 - val_mae: 1.4181 - lr: 5.3150e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 13/100\n",
            "1000/1000 - 2s - loss: 12.3886 - mse: 12.3886 - mae: 1.4489 - val_loss: 9.4051 - val_mse: 9.4051 - val_mae: 1.4526 - lr: 5.3150e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 14/100\n",
            "1000/1000 - 2s - loss: 12.2395 - mse: 12.2395 - mae: 1.4463 - val_loss: 9.4985 - val_mse: 9.4985 - val_mae: 1.4141 - lr: 5.3150e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 15/100\n",
            "1000/1000 - 2s - loss: 12.3113 - mse: 12.3113 - mae: 1.4466 - val_loss: 9.3497 - val_mse: 9.3497 - val_mae: 1.4423 - lr: 5.3150e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 16/100\n",
            "1000/1000 - 2s - loss: 12.3404 - mse: 12.3404 - mae: 1.4475 - val_loss: 9.3375 - val_mse: 9.3375 - val_mae: 1.4068 - lr: 5.3150e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 17/100\n",
            "1000/1000 - 2s - loss: 12.2599 - mse: 12.2599 - mae: 1.4453 - val_loss: 9.3042 - val_mse: 9.3042 - val_mae: 1.4755 - lr: 5.3150e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 18/100\n",
            "1000/1000 - 2s - loss: 12.1738 - mse: 12.1738 - mae: 1.4438 - val_loss: 9.4332 - val_mse: 9.4332 - val_mae: 1.4525 - lr: 5.3150e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 19/100\n",
            "1000/1000 - 2s - loss: 12.1631 - mse: 12.1631 - mae: 1.4400 - val_loss: 9.4231 - val_mse: 9.4231 - val_mae: 1.4235 - lr: 5.3150e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 20/100\n",
            "1000/1000 - 2s - loss: 12.0915 - mse: 12.0915 - mae: 1.4389 - val_loss: 9.3850 - val_mse: 9.3850 - val_mae: 1.4026 - lr: 5.3150e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 21/100\n",
            "1000/1000 - 2s - loss: 12.1823 - mse: 12.1823 - mae: 1.4380 - val_loss: 9.4477 - val_mse: 9.4477 - val_mae: 1.4246 - lr: 5.3150e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 22/100\n",
            "1000/1000 - 2s - loss: 12.1159 - mse: 12.1159 - mae: 1.4358 - val_loss: 9.3421 - val_mse: 9.3421 - val_mae: 1.4401 - lr: 5.3150e-04 - 2s/epoch - 2ms/step\n",
            "Score for fold 1: loss of 9.342066764831543\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 12.3312 - mse: 12.3312 - mae: 1.4402 - val_loss: 8.6608 - val_mse: 8.6608 - val_mae: 1.4206 - lr: 5.3150e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 12.2927 - mse: 12.2927 - mae: 1.4372 - val_loss: 8.6643 - val_mse: 8.6643 - val_mae: 1.4245 - lr: 5.3150e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 12.2695 - mse: 12.2695 - mae: 1.4332 - val_loss: 8.6642 - val_mse: 8.6642 - val_mae: 1.4493 - lr: 5.3150e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 12.2347 - mse: 12.2347 - mae: 1.4302 - val_loss: 8.7105 - val_mse: 8.7105 - val_mae: 1.4536 - lr: 5.3150e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 12.1340 - mse: 12.1340 - mae: 1.4351 - val_loss: 8.7963 - val_mse: 8.7963 - val_mae: 1.4648 - lr: 5.3150e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 12.2315 - mse: 12.2315 - mae: 1.4340 - val_loss: 8.8188 - val_mse: 8.8188 - val_mae: 1.4361 - lr: 5.3150e-04 - 2s/epoch - 2ms/step\n",
            "Score for fold 2: loss of 8.818760871887207\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 11.8059 - mse: 11.8059 - mae: 1.4347 - val_loss: 10.1579 - val_mse: 10.1579 - val_mae: 1.4457 - lr: 5.3150e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 11.7919 - mse: 11.7919 - mae: 1.4357 - val_loss: 10.1539 - val_mse: 10.1539 - val_mae: 1.4323 - lr: 5.3150e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 11.7384 - mse: 11.7384 - mae: 1.4308 - val_loss: 10.3463 - val_mse: 10.3463 - val_mae: 1.4786 - lr: 5.3150e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 11.7237 - mse: 11.7237 - mae: 1.4309 - val_loss: 10.8328 - val_mse: 10.8328 - val_mae: 1.4506 - lr: 5.3150e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 11.7123 - mse: 11.7123 - mae: 1.4311 - val_loss: 10.3297 - val_mse: 10.3297 - val_mae: 1.4286 - lr: 5.3150e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 11.6347 - mse: 11.6347 - mae: 1.4287 - val_loss: 10.2884 - val_mse: 10.2884 - val_mae: 1.4400 - lr: 5.3150e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 11.7088 - mse: 11.7088 - mae: 1.4307 - val_loss: 10.2439 - val_mse: 10.2439 - val_mae: 1.4285 - lr: 5.3150e-04 - 2s/epoch - 2ms/step\n",
            "Score for fold 3: loss of 10.243880271911621\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 10.0387 - mse: 10.0387 - mae: 1.4236 - val_loss: 16.8460 - val_mse: 16.8460 - val_mae: 1.4210 - lr: 5.3150e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 10.0516 - mse: 10.0516 - mae: 1.4226 - val_loss: 16.8800 - val_mse: 16.8800 - val_mae: 1.4120 - lr: 5.3150e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 9.9200 - mse: 9.9200 - mae: 1.4208 - val_loss: 16.7693 - val_mse: 16.7693 - val_mae: 1.4467 - lr: 5.3150e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 9.9646 - mse: 9.9646 - mae: 1.4191 - val_loss: 16.8136 - val_mse: 16.8136 - val_mae: 1.4198 - lr: 5.3150e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 9.9193 - mse: 9.9193 - mae: 1.4206 - val_loss: 17.0023 - val_mse: 17.0023 - val_mae: 1.4457 - lr: 5.3150e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 9.9695 - mse: 9.9695 - mae: 1.4193 - val_loss: 16.9601 - val_mse: 16.9601 - val_mae: 1.3885 - lr: 5.3150e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 9.8608 - mse: 9.8608 - mae: 1.4163 - val_loss: 16.9944 - val_mse: 16.9944 - val_mae: 1.4834 - lr: 5.3150e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 2s - loss: 9.9307 - mse: 9.9307 - mae: 1.4208 - val_loss: 16.9536 - val_mse: 16.9536 - val_mae: 1.4538 - lr: 5.3150e-04 - 2s/epoch - 2ms/step\n",
            "Score for fold 4: loss of 16.953594207763672\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 10.9866 - mse: 10.9866 - mae: 1.4188 - val_loss: 12.0662 - val_mse: 12.0662 - val_mae: 1.4173 - lr: 5.3150e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 11.0148 - mse: 11.0148 - mae: 1.4204 - val_loss: 12.3636 - val_mse: 12.3636 - val_mae: 1.4096 - lr: 5.3150e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 10.9345 - mse: 10.9345 - mae: 1.4163 - val_loss: 12.3414 - val_mse: 12.3414 - val_mae: 1.4474 - lr: 5.3150e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 10.8757 - mse: 10.8757 - mae: 1.4136 - val_loss: 12.4671 - val_mse: 12.4671 - val_mae: 1.4635 - lr: 5.3150e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 10.9472 - mse: 10.9472 - mae: 1.4155 - val_loss: 12.8702 - val_mse: 12.8702 - val_mae: 1.4319 - lr: 5.3150e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 10.9159 - mse: 10.9159 - mae: 1.4157 - val_loss: 12.3606 - val_mse: 12.3606 - val_mae: 1.3991 - lr: 5.3150e-04 - 2s/epoch - 2ms/step\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-10-12 11:55:33,477]\u001b[0m Finished trial#14 resulted in value: 11.541999999999998. Current best value is 10.178 with parameters: {'activation': 'relu', 'num_hidden_layer': 3, 'i': 9, 'learning_rate': 0.0019093737064063846}.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Score for fold 5: loss of 12.360587120056152\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 4s - loss: 15.8921 - mse: 15.8921 - mae: 1.6440 - val_loss: 9.8150 - val_mse: 9.8150 - val_mae: 1.5684 - lr: 0.0017 - 4s/epoch - 4ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 4s - loss: 15.2393 - mse: 15.2393 - mae: 1.5813 - val_loss: 9.3765 - val_mse: 9.3765 - val_mae: 1.5487 - lr: 0.0017 - 4s/epoch - 4ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 4s - loss: 14.9553 - mse: 14.9553 - mae: 1.5534 - val_loss: 8.7976 - val_mse: 8.7976 - val_mae: 1.5028 - lr: 0.0017 - 4s/epoch - 4ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 4s - loss: 14.7270 - mse: 14.7270 - mae: 1.5398 - val_loss: 8.7396 - val_mse: 8.7396 - val_mae: 1.4512 - lr: 0.0017 - 4s/epoch - 4ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 4s - loss: 14.5653 - mse: 14.5653 - mae: 1.5363 - val_loss: 8.6743 - val_mse: 8.6743 - val_mae: 1.4663 - lr: 0.0017 - 4s/epoch - 4ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 4s - loss: 14.4927 - mse: 14.4927 - mae: 1.5366 - val_loss: 8.3527 - val_mse: 8.3527 - val_mae: 1.5292 - lr: 0.0017 - 4s/epoch - 4ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 4s - loss: 14.2939 - mse: 14.2939 - mae: 1.5236 - val_loss: 8.5627 - val_mse: 8.5627 - val_mae: 1.4514 - lr: 0.0017 - 4s/epoch - 4ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 4s - loss: 14.2507 - mse: 14.2507 - mae: 1.5188 - val_loss: 8.3554 - val_mse: 8.3554 - val_mae: 1.5179 - lr: 0.0017 - 4s/epoch - 4ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 4s - loss: 14.1619 - mse: 14.1619 - mae: 1.5200 - val_loss: 8.5368 - val_mse: 8.5368 - val_mae: 1.4320 - lr: 0.0017 - 4s/epoch - 4ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 4s - loss: 14.1408 - mse: 14.1408 - mae: 1.5191 - val_loss: 8.2164 - val_mse: 8.2164 - val_mae: 1.4063 - lr: 0.0017 - 4s/epoch - 4ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 4s - loss: 14.0806 - mse: 14.0806 - mae: 1.5261 - val_loss: 8.2472 - val_mse: 8.2472 - val_mae: 1.5092 - lr: 0.0017 - 4s/epoch - 4ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 4s - loss: 13.9663 - mse: 13.9663 - mae: 1.5270 - val_loss: 8.1028 - val_mse: 8.1028 - val_mae: 1.4095 - lr: 0.0017 - 4s/epoch - 4ms/step\n",
            "Epoch 13/100\n",
            "1000/1000 - 4s - loss: 13.9240 - mse: 13.9240 - mae: 1.5158 - val_loss: 8.2708 - val_mse: 8.2708 - val_mae: 1.4588 - lr: 0.0017 - 4s/epoch - 4ms/step\n",
            "Epoch 14/100\n",
            "1000/1000 - 4s - loss: 13.9722 - mse: 13.9722 - mae: 1.5050 - val_loss: 8.5842 - val_mse: 8.5842 - val_mae: 1.7739 - lr: 0.0017 - 4s/epoch - 4ms/step\n",
            "Epoch 15/100\n",
            "1000/1000 - 4s - loss: 13.7642 - mse: 13.7642 - mae: 1.5025 - val_loss: 8.0380 - val_mse: 8.0380 - val_mae: 1.4236 - lr: 0.0017 - 4s/epoch - 4ms/step\n",
            "Epoch 16/100\n",
            "1000/1000 - 4s - loss: 13.8333 - mse: 13.8333 - mae: 1.5086 - val_loss: 8.3872 - val_mse: 8.3872 - val_mae: 1.6463 - lr: 0.0017 - 4s/epoch - 4ms/step\n",
            "Epoch 17/100\n",
            "1000/1000 - 4s - loss: 13.6558 - mse: 13.6558 - mae: 1.5025 - val_loss: 8.5616 - val_mse: 8.5616 - val_mae: 1.4191 - lr: 0.0017 - 4s/epoch - 4ms/step\n",
            "Epoch 18/100\n",
            "1000/1000 - 4s - loss: 13.5323 - mse: 13.5323 - mae: 1.4954 - val_loss: 8.3653 - val_mse: 8.3653 - val_mae: 1.4251 - lr: 0.0017 - 4s/epoch - 4ms/step\n",
            "Epoch 19/100\n",
            "1000/1000 - 4s - loss: 13.5528 - mse: 13.5528 - mae: 1.5019 - val_loss: 8.5017 - val_mse: 8.5017 - val_mae: 1.3988 - lr: 0.0017 - 4s/epoch - 4ms/step\n",
            "Epoch 20/100\n",
            "1000/1000 - 4s - loss: 13.3999 - mse: 13.3999 - mae: 1.5070 - val_loss: 8.2250 - val_mse: 8.2250 - val_mae: 1.4534 - lr: 0.0017 - 4s/epoch - 4ms/step\n",
            "Score for fold 1: loss of 8.225018501281738\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 4s - loss: 12.9303 - mse: 12.9303 - mae: 1.4684 - val_loss: 8.7154 - val_mse: 8.7154 - val_mae: 1.4572 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 4s - loss: 12.8048 - mse: 12.8048 - mae: 1.4583 - val_loss: 8.6019 - val_mse: 8.6019 - val_mae: 1.4688 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 4s - loss: 12.6323 - mse: 12.6323 - mae: 1.4586 - val_loss: 8.8395 - val_mse: 8.8395 - val_mae: 1.4587 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 4s - loss: 12.5795 - mse: 12.5795 - mae: 1.4563 - val_loss: 9.1488 - val_mse: 9.1488 - val_mae: 1.4112 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 4s - loss: 12.5780 - mse: 12.5780 - mae: 1.4523 - val_loss: 8.9087 - val_mse: 8.9087 - val_mae: 1.4734 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 4s - loss: 12.4895 - mse: 12.4895 - mae: 1.4483 - val_loss: 8.7494 - val_mse: 8.7494 - val_mae: 1.4252 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 4s - loss: 12.3109 - mse: 12.3109 - mae: 1.4390 - val_loss: 9.1703 - val_mse: 9.1703 - val_mae: 1.3763 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Score for fold 2: loss of 9.170260429382324\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 4s - loss: 9.0126 - mse: 9.0126 - mae: 1.4393 - val_loss: 22.6539 - val_mse: 22.6539 - val_mae: 1.4990 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 4s - loss: 8.8520 - mse: 8.8520 - mae: 1.4363 - val_loss: 22.4089 - val_mse: 22.4089 - val_mae: 1.5078 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 4s - loss: 8.7657 - mse: 8.7657 - mae: 1.4320 - val_loss: 22.6129 - val_mse: 22.6129 - val_mae: 1.5820 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 4s - loss: 8.7279 - mse: 8.7279 - mae: 1.4329 - val_loss: 22.8961 - val_mse: 22.8961 - val_mae: 1.5438 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 4s - loss: 8.5670 - mse: 8.5670 - mae: 1.4203 - val_loss: 22.8702 - val_mse: 22.8702 - val_mae: 1.5149 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 4s - loss: 8.5870 - mse: 8.5870 - mae: 1.4193 - val_loss: 22.8414 - val_mse: 22.8414 - val_mae: 1.4973 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 4s - loss: 8.4597 - mse: 8.4597 - mae: 1.4194 - val_loss: 23.2104 - val_mse: 23.2104 - val_mae: 1.5733 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Score for fold 3: loss of 23.210420608520508\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 4s - loss: 12.0352 - mse: 12.0352 - mae: 1.4524 - val_loss: 8.4931 - val_mse: 8.4931 - val_mae: 1.4851 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 4s - loss: 11.7753 - mse: 11.7753 - mae: 1.4406 - val_loss: 8.3973 - val_mse: 8.3973 - val_mae: 1.3971 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 4s - loss: 11.6832 - mse: 11.6832 - mae: 1.4423 - val_loss: 8.4378 - val_mse: 8.4378 - val_mae: 1.4229 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 4s - loss: 11.6018 - mse: 11.6018 - mae: 1.4342 - val_loss: 8.6192 - val_mse: 8.6192 - val_mae: 1.3479 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 4s - loss: 11.5147 - mse: 11.5147 - mae: 1.4322 - val_loss: 8.6555 - val_mse: 8.6555 - val_mae: 1.5541 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 4s - loss: 11.4369 - mse: 11.4369 - mae: 1.4216 - val_loss: 8.6055 - val_mse: 8.6055 - val_mae: 1.3571 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 4s - loss: 11.3575 - mse: 11.3575 - mae: 1.4208 - val_loss: 8.5738 - val_mse: 8.5738 - val_mae: 1.4333 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Score for fold 4: loss of 8.573780059814453\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 4s - loss: 11.1277 - mse: 11.1277 - mae: 1.4225 - val_loss: 9.5325 - val_mse: 9.5325 - val_mae: 1.3366 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 4s - loss: 11.0300 - mse: 11.0300 - mae: 1.4217 - val_loss: 10.5924 - val_mse: 10.5924 - val_mae: 1.3671 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 4s - loss: 11.0235 - mse: 11.0235 - mae: 1.4203 - val_loss: 9.9035 - val_mse: 9.9035 - val_mae: 1.5354 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 4s - loss: 10.9345 - mse: 10.9345 - mae: 1.4109 - val_loss: 10.3104 - val_mse: 10.3104 - val_mae: 1.3950 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 4s - loss: 10.7845 - mse: 10.7845 - mae: 1.3959 - val_loss: 10.1099 - val_mse: 10.1099 - val_mae: 1.4430 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 4s - loss: 10.6958 - mse: 10.6958 - mae: 1.3953 - val_loss: 10.5038 - val_mse: 10.5038 - val_mae: 1.5172 - lr: 0.0010 - 4s/epoch - 4ms/step\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-10-12 11:58:32,169]\u001b[0m Finished trial#15 resulted in value: 11.936. Current best value is 10.178 with parameters: {'activation': 'relu', 'num_hidden_layer': 3, 'i': 9, 'learning_rate': 0.0019093737064063846}.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Score for fold 5: loss of 10.503779411315918\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 6s - loss: 26.0875 - mse: 26.0875 - mae: 2.3398 - val_loss: 17.8230 - val_mse: 17.8230 - val_mae: 2.2319 - lr: 0.0095 - 6s/epoch - 6ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 6s - loss: 21.9316 - mse: 21.9316 - mae: 2.1749 - val_loss: 13.9487 - val_mse: 13.9487 - val_mae: 1.6878 - lr: 0.0095 - 6s/epoch - 6ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 6s - loss: 22.4760 - mse: 22.4760 - mae: 2.1616 - val_loss: 20.2184 - val_mse: 20.2184 - val_mae: 2.5784 - lr: 0.0095 - 6s/epoch - 6ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 6s - loss: 27.3569 - mse: 27.3569 - mae: 2.2106 - val_loss: 25.6255 - val_mse: 25.6255 - val_mae: 2.7221 - lr: 0.0095 - 6s/epoch - 6ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 6s - loss: 20.2864 - mse: 20.2864 - mae: 2.1732 - val_loss: 14.8735 - val_mse: 14.8735 - val_mae: 1.7318 - lr: 0.0095 - 6s/epoch - 6ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 6s - loss: 27.2353 - mse: 27.2353 - mae: 2.1433 - val_loss: 20.7458 - val_mse: 20.7458 - val_mae: 2.2311 - lr: 0.0095 - 6s/epoch - 6ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 6s - loss: 20.7827 - mse: 20.7827 - mae: 2.1494 - val_loss: 14.1862 - val_mse: 14.1862 - val_mae: 1.8039 - lr: 0.0095 - 6s/epoch - 6ms/step\n",
            "Score for fold 1: loss of 14.186235427856445\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 6s - loss: 12.9063 - mse: 12.9063 - mae: 1.5972 - val_loss: 17.8648 - val_mse: 17.8648 - val_mae: 1.5470 - lr: 0.0019 - 6s/epoch - 6ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 6s - loss: 12.8109 - mse: 12.8109 - mae: 1.5861 - val_loss: 18.3283 - val_mse: 18.3283 - val_mae: 1.6005 - lr: 0.0019 - 6s/epoch - 6ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 6s - loss: 12.6018 - mse: 12.6018 - mae: 1.5883 - val_loss: 17.8731 - val_mse: 17.8731 - val_mae: 1.5880 - lr: 0.0019 - 6s/epoch - 6ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 6s - loss: 12.7822 - mse: 12.7822 - mae: 1.5973 - val_loss: 18.2126 - val_mse: 18.2126 - val_mae: 1.5549 - lr: 0.0019 - 6s/epoch - 6ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 6s - loss: 12.6493 - mse: 12.6493 - mae: 1.5836 - val_loss: 18.0809 - val_mse: 18.0809 - val_mae: 1.5754 - lr: 0.0019 - 6s/epoch - 6ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 6s - loss: 12.4093 - mse: 12.4093 - mae: 1.5841 - val_loss: 17.8579 - val_mse: 17.8579 - val_mae: 1.5239 - lr: 0.0019 - 6s/epoch - 6ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 6s - loss: 12.4629 - mse: 12.4629 - mae: 1.5883 - val_loss: 17.6307 - val_mse: 17.6307 - val_mae: 1.5792 - lr: 0.0019 - 6s/epoch - 6ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 6s - loss: 12.4500 - mse: 12.4500 - mae: 1.5826 - val_loss: 17.4523 - val_mse: 17.4523 - val_mae: 1.5578 - lr: 0.0019 - 6s/epoch - 6ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 6s - loss: 12.2665 - mse: 12.2665 - mae: 1.5828 - val_loss: 17.6784 - val_mse: 17.6784 - val_mae: 1.5659 - lr: 0.0019 - 6s/epoch - 6ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 6s - loss: 12.5159 - mse: 12.5159 - mae: 1.5865 - val_loss: 18.7670 - val_mse: 18.7670 - val_mae: 1.6198 - lr: 0.0019 - 6s/epoch - 6ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 6s - loss: 12.2930 - mse: 12.2930 - mae: 1.5831 - val_loss: 17.6179 - val_mse: 17.6179 - val_mae: 1.5700 - lr: 0.0019 - 6s/epoch - 6ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 6s - loss: 12.4896 - mse: 12.4896 - mae: 1.5888 - val_loss: 17.5025 - val_mse: 17.5025 - val_mae: 1.5339 - lr: 0.0019 - 6s/epoch - 6ms/step\n",
            "Epoch 13/100\n",
            "1000/1000 - 6s - loss: 12.4084 - mse: 12.4084 - mae: 1.5811 - val_loss: 19.2644 - val_mse: 19.2644 - val_mae: 1.6266 - lr: 0.0019 - 6s/epoch - 6ms/step\n",
            "Score for fold 2: loss of 19.264415740966797\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 6s - loss: 13.8191 - mse: 13.8191 - mae: 1.5711 - val_loss: 11.3612 - val_mse: 11.3612 - val_mae: 1.5217 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 6s - loss: 13.7013 - mse: 13.7013 - mae: 1.5631 - val_loss: 11.4380 - val_mse: 11.4380 - val_mae: 1.5483 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 6s - loss: 13.8019 - mse: 13.8019 - mae: 1.5679 - val_loss: 11.4331 - val_mse: 11.4331 - val_mae: 1.5281 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 6s - loss: 13.8036 - mse: 13.8036 - mae: 1.5706 - val_loss: 11.4248 - val_mse: 11.4248 - val_mae: 1.5060 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 6s - loss: 13.7942 - mse: 13.7942 - mae: 1.5659 - val_loss: 11.3847 - val_mse: 11.3847 - val_mae: 1.5246 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 6s - loss: 13.7740 - mse: 13.7740 - mae: 1.5756 - val_loss: 11.4143 - val_mse: 11.4143 - val_mae: 1.5273 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
            "Score for fold 3: loss of 11.41434097290039\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 6s - loss: 13.0250 - mse: 13.0250 - mae: 1.5496 - val_loss: 14.8797 - val_mse: 14.8797 - val_mae: 1.5820 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 6s - loss: 12.9254 - mse: 12.9254 - mae: 1.5424 - val_loss: 14.6402 - val_mse: 14.6402 - val_mae: 1.5990 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 6s - loss: 12.9107 - mse: 12.9107 - mae: 1.5456 - val_loss: 14.9069 - val_mse: 14.9069 - val_mae: 1.5804 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 6s - loss: 12.8545 - mse: 12.8545 - mae: 1.5502 - val_loss: 14.5383 - val_mse: 14.5383 - val_mae: 1.6390 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 6s - loss: 12.9457 - mse: 12.9457 - mae: 1.5490 - val_loss: 14.6808 - val_mse: 14.6808 - val_mae: 1.5794 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 6s - loss: 12.8618 - mse: 12.8618 - mae: 1.5455 - val_loss: 14.6603 - val_mse: 14.6603 - val_mae: 1.5841 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 6s - loss: 12.8806 - mse: 12.8806 - mae: 1.5482 - val_loss: 14.7214 - val_mse: 14.7214 - val_mae: 1.6241 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 6s - loss: 12.9020 - mse: 12.9020 - mae: 1.5470 - val_loss: 14.5289 - val_mse: 14.5289 - val_mae: 1.5997 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 6s - loss: 12.9500 - mse: 12.9500 - mae: 1.5457 - val_loss: 14.6226 - val_mse: 14.6226 - val_mae: 1.5802 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 6s - loss: 12.9835 - mse: 12.9835 - mae: 1.5480 - val_loss: 14.3802 - val_mse: 14.3802 - val_mae: 1.5963 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 6s - loss: 12.9524 - mse: 12.9524 - mae: 1.5499 - val_loss: 14.5444 - val_mse: 14.5444 - val_mae: 1.5858 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 6s - loss: 12.9606 - mse: 12.9606 - mae: 1.5468 - val_loss: 14.6351 - val_mse: 14.6351 - val_mae: 1.5956 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
            "Epoch 13/100\n",
            "1000/1000 - 6s - loss: 12.9415 - mse: 12.9415 - mae: 1.5451 - val_loss: 14.4760 - val_mse: 14.4760 - val_mae: 1.6257 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
            "Epoch 14/100\n",
            "1000/1000 - 6s - loss: 12.8995 - mse: 12.8995 - mae: 1.5461 - val_loss: 14.7442 - val_mse: 14.7442 - val_mae: 1.5952 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
            "Epoch 15/100\n",
            "1000/1000 - 6s - loss: 12.8496 - mse: 12.8496 - mae: 1.5480 - val_loss: 14.7952 - val_mse: 14.7952 - val_mae: 1.5758 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
            "Score for fold 4: loss of 14.795166015625\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 6s - loss: 14.2196 - mse: 14.2196 - mae: 1.5625 - val_loss: 9.3728 - val_mse: 9.3728 - val_mae: 1.5462 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 6s - loss: 14.3042 - mse: 14.3042 - mae: 1.5615 - val_loss: 9.3650 - val_mse: 9.3650 - val_mae: 1.5407 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 6s - loss: 14.2195 - mse: 14.2195 - mae: 1.5636 - val_loss: 9.4911 - val_mse: 9.4911 - val_mae: 1.5131 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 6s - loss: 14.2524 - mse: 14.2524 - mae: 1.5613 - val_loss: 9.6530 - val_mse: 9.6530 - val_mae: 1.5135 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 6s - loss: 14.2585 - mse: 14.2585 - mae: 1.5591 - val_loss: 9.3714 - val_mse: 9.3714 - val_mae: 1.5253 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 6s - loss: 14.4392 - mse: 14.4392 - mae: 1.5632 - val_loss: 9.5489 - val_mse: 9.5489 - val_mae: 1.5774 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 6s - loss: 14.2773 - mse: 14.2773 - mae: 1.5599 - val_loss: 9.3827 - val_mse: 9.3827 - val_mae: 1.5131 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
            "Score for fold 5: loss of 9.382702827453613\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-10-12 12:03:19,848]\u001b[0m Finished trial#16 resulted in value: 13.807999999999998. Current best value is 10.178 with parameters: {'activation': 'relu', 'num_hidden_layer': 3, 'i': 9, 'learning_rate': 0.0019093737064063846}.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 12.2924 - mse: 12.2924 - mae: 1.5432 - val_loss: 16.8794 - val_mse: 16.8794 - val_mae: 1.5781 - lr: 6.1266e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 11.5433 - mse: 11.5433 - mae: 1.4942 - val_loss: 16.6050 - val_mse: 16.6050 - val_mae: 1.5565 - lr: 6.1266e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 11.3238 - mse: 11.3238 - mae: 1.4796 - val_loss: 16.2473 - val_mse: 16.2473 - val_mae: 1.5088 - lr: 6.1266e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 11.2010 - mse: 11.2010 - mae: 1.4698 - val_loss: 16.7546 - val_mse: 16.7546 - val_mae: 1.4540 - lr: 6.1266e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 11.0695 - mse: 11.0695 - mae: 1.4621 - val_loss: 16.2270 - val_mse: 16.2270 - val_mae: 1.4774 - lr: 6.1266e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 11.0053 - mse: 11.0053 - mae: 1.4570 - val_loss: 16.1240 - val_mse: 16.1240 - val_mae: 1.5071 - lr: 6.1266e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 11.0107 - mse: 11.0107 - mae: 1.4561 - val_loss: 16.1032 - val_mse: 16.1032 - val_mae: 1.4373 - lr: 6.1266e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 2s - loss: 10.9539 - mse: 10.9539 - mae: 1.4548 - val_loss: 16.0727 - val_mse: 16.0727 - val_mae: 1.4575 - lr: 6.1266e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 2s - loss: 10.8433 - mse: 10.8433 - mae: 1.4505 - val_loss: 15.8870 - val_mse: 15.8870 - val_mae: 1.5188 - lr: 6.1266e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 2s - loss: 10.8602 - mse: 10.8602 - mae: 1.4461 - val_loss: 16.0289 - val_mse: 16.0289 - val_mae: 1.5139 - lr: 6.1266e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 2s - loss: 10.8953 - mse: 10.8953 - mae: 1.4474 - val_loss: 15.6395 - val_mse: 15.6395 - val_mae: 1.5207 - lr: 6.1266e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 2s - loss: 10.8320 - mse: 10.8320 - mae: 1.4451 - val_loss: 15.6883 - val_mse: 15.6883 - val_mae: 1.4921 - lr: 6.1266e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 13/100\n",
            "1000/1000 - 2s - loss: 10.7730 - mse: 10.7730 - mae: 1.4433 - val_loss: 15.5230 - val_mse: 15.5230 - val_mae: 1.4572 - lr: 6.1266e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 14/100\n",
            "1000/1000 - 2s - loss: 10.7668 - mse: 10.7668 - mae: 1.4389 - val_loss: 15.7710 - val_mse: 15.7710 - val_mae: 1.4961 - lr: 6.1266e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 15/100\n",
            "1000/1000 - 2s - loss: 10.7422 - mse: 10.7422 - mae: 1.4385 - val_loss: 16.0473 - val_mse: 16.0473 - val_mae: 1.4252 - lr: 6.1266e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 16/100\n",
            "1000/1000 - 2s - loss: 10.6873 - mse: 10.6873 - mae: 1.4358 - val_loss: 15.8127 - val_mse: 15.8127 - val_mae: 1.4796 - lr: 6.1266e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 17/100\n",
            "1000/1000 - 2s - loss: 10.7950 - mse: 10.7950 - mae: 1.4362 - val_loss: 15.5604 - val_mse: 15.5604 - val_mae: 1.4876 - lr: 6.1266e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 18/100\n",
            "1000/1000 - 2s - loss: 10.6996 - mse: 10.6996 - mae: 1.4338 - val_loss: 15.6777 - val_mse: 15.6777 - val_mae: 1.4622 - lr: 6.1266e-04 - 2s/epoch - 2ms/step\n",
            "Score for fold 1: loss of 15.677708625793457\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 11.3626 - mse: 11.3626 - mae: 1.4472 - val_loss: 12.7529 - val_mse: 12.7529 - val_mae: 1.4021 - lr: 6.1266e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 11.3179 - mse: 11.3179 - mae: 1.4421 - val_loss: 12.8428 - val_mse: 12.8428 - val_mae: 1.4593 - lr: 6.1266e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 11.2924 - mse: 11.2924 - mae: 1.4407 - val_loss: 13.1892 - val_mse: 13.1892 - val_mae: 1.4454 - lr: 6.1266e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 11.3173 - mse: 11.3173 - mae: 1.4419 - val_loss: 12.6359 - val_mse: 12.6359 - val_mae: 1.4306 - lr: 6.1266e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 11.2301 - mse: 11.2301 - mae: 1.4372 - val_loss: 13.0403 - val_mse: 13.0403 - val_mae: 1.4129 - lr: 6.1266e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 11.2496 - mse: 11.2496 - mae: 1.4340 - val_loss: 12.8407 - val_mse: 12.8407 - val_mae: 1.4880 - lr: 6.1266e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 11.2415 - mse: 11.2415 - mae: 1.4354 - val_loss: 12.9720 - val_mse: 12.9720 - val_mae: 1.4681 - lr: 6.1266e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 2s - loss: 11.2011 - mse: 11.2011 - mae: 1.4347 - val_loss: 12.7701 - val_mse: 12.7701 - val_mae: 1.4490 - lr: 6.1266e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 2s - loss: 11.2212 - mse: 11.2212 - mae: 1.4348 - val_loss: 13.0000 - val_mse: 13.0000 - val_mae: 1.4549 - lr: 6.1266e-04 - 2s/epoch - 2ms/step\n",
            "Score for fold 2: loss of 12.999967575073242\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 12.0052 - mse: 12.0052 - mae: 1.4255 - val_loss: 9.7595 - val_mse: 9.7595 - val_mae: 1.4469 - lr: 6.1266e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 11.8570 - mse: 11.8570 - mae: 1.4242 - val_loss: 9.7301 - val_mse: 9.7301 - val_mae: 1.4419 - lr: 6.1266e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 11.7940 - mse: 11.7940 - mae: 1.4257 - val_loss: 9.7018 - val_mse: 9.7018 - val_mae: 1.5190 - lr: 6.1266e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 11.8554 - mse: 11.8554 - mae: 1.4213 - val_loss: 9.6954 - val_mse: 9.6954 - val_mae: 1.4138 - lr: 6.1266e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 11.7128 - mse: 11.7128 - mae: 1.4223 - val_loss: 10.0470 - val_mse: 10.0470 - val_mae: 1.4701 - lr: 6.1266e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 11.7849 - mse: 11.7849 - mae: 1.4224 - val_loss: 9.7823 - val_mse: 9.7823 - val_mae: 1.4999 - lr: 6.1266e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 11.8055 - mse: 11.8055 - mae: 1.4228 - val_loss: 9.7486 - val_mse: 9.7486 - val_mae: 1.5083 - lr: 6.1266e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 2s - loss: 11.7513 - mse: 11.7513 - mae: 1.4191 - val_loss: 9.7899 - val_mse: 9.7899 - val_mae: 1.4560 - lr: 6.1266e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 2s - loss: 11.7011 - mse: 11.7011 - mae: 1.4197 - val_loss: 10.2559 - val_mse: 10.2559 - val_mae: 1.5026 - lr: 6.1266e-04 - 2s/epoch - 2ms/step\n",
            "Score for fold 3: loss of 10.255908012390137\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 11.1117 - mse: 11.1117 - mae: 1.4231 - val_loss: 11.9616 - val_mse: 11.9616 - val_mae: 1.5136 - lr: 6.1266e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 11.1082 - mse: 11.1082 - mae: 1.4246 - val_loss: 12.0121 - val_mse: 12.0121 - val_mae: 1.3969 - lr: 6.1266e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 11.1278 - mse: 11.1278 - mae: 1.4253 - val_loss: 12.0068 - val_mse: 12.0068 - val_mae: 1.4251 - lr: 6.1266e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 11.1296 - mse: 11.1296 - mae: 1.4223 - val_loss: 12.1855 - val_mse: 12.1855 - val_mae: 1.4130 - lr: 6.1266e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 11.0556 - mse: 11.0556 - mae: 1.4191 - val_loss: 12.3023 - val_mse: 12.3023 - val_mae: 1.4264 - lr: 6.1266e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 11.0296 - mse: 11.0296 - mae: 1.4124 - val_loss: 12.0063 - val_mse: 12.0063 - val_mae: 1.4109 - lr: 6.1266e-04 - 2s/epoch - 2ms/step\n",
            "Score for fold 4: loss of 12.006258010864258\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 12.2533 - mse: 12.2533 - mae: 1.4331 - val_loss: 7.2964 - val_mse: 7.2964 - val_mae: 1.4275 - lr: 6.1266e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 12.1830 - mse: 12.1830 - mae: 1.4353 - val_loss: 7.4843 - val_mse: 7.4843 - val_mae: 1.3564 - lr: 6.1266e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 12.2254 - mse: 12.2254 - mae: 1.4305 - val_loss: 7.3742 - val_mse: 7.3742 - val_mae: 1.3811 - lr: 6.1266e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 12.0799 - mse: 12.0799 - mae: 1.4306 - val_loss: 7.4367 - val_mse: 7.4367 - val_mae: 1.3920 - lr: 6.1266e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 12.1373 - mse: 12.1373 - mae: 1.4316 - val_loss: 7.3774 - val_mse: 7.3774 - val_mae: 1.4153 - lr: 6.1266e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 12.0263 - mse: 12.0263 - mae: 1.4261 - val_loss: 7.5962 - val_mse: 7.5962 - val_mae: 1.3683 - lr: 6.1266e-04 - 2s/epoch - 2ms/step\n",
            "Score for fold 5: loss of 7.596163749694824\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-10-12 12:04:50,536]\u001b[0m Finished trial#17 resulted in value: 11.709999999999999. Current best value is 10.178 with parameters: {'activation': 'relu', 'num_hidden_layer': 3, 'i': 9, 'learning_rate': 0.0019093737064063846}.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 7s - loss: 14.7226 - mse: 14.7226 - mae: 1.5587 - val_loss: 8.0506 - val_mse: 8.0506 - val_mae: 1.5191 - lr: 0.0016 - 7s/epoch - 7ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 6s - loss: 13.5702 - mse: 13.5702 - mae: 1.5127 - val_loss: 7.9751 - val_mse: 7.9751 - val_mae: 1.4873 - lr: 0.0016 - 6s/epoch - 6ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 6s - loss: 13.2816 - mse: 13.2816 - mae: 1.4894 - val_loss: 8.3698 - val_mse: 8.3698 - val_mae: 1.4579 - lr: 0.0016 - 6s/epoch - 6ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 6s - loss: 13.2166 - mse: 13.2166 - mae: 1.4770 - val_loss: 8.3486 - val_mse: 8.3486 - val_mae: 1.4164 - lr: 0.0016 - 6s/epoch - 6ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 6s - loss: 13.0157 - mse: 13.0157 - mae: 1.4687 - val_loss: 8.3115 - val_mse: 8.3115 - val_mae: 1.4693 - lr: 0.0016 - 6s/epoch - 6ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 6s - loss: 12.9483 - mse: 12.9483 - mae: 1.4568 - val_loss: 8.4684 - val_mse: 8.4684 - val_mae: 1.4197 - lr: 0.0016 - 6s/epoch - 6ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 6s - loss: 12.9457 - mse: 12.9457 - mae: 1.4507 - val_loss: 8.4239 - val_mse: 8.4239 - val_mae: 1.4461 - lr: 0.0016 - 6s/epoch - 6ms/step\n",
            "Score for fold 1: loss of 8.42385196685791\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 6s - loss: 11.5167 - mse: 11.5167 - mae: 1.4462 - val_loss: 11.8447 - val_mse: 11.8447 - val_mae: 1.4200 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 6s - loss: 11.4225 - mse: 11.4225 - mae: 1.4432 - val_loss: 12.1042 - val_mse: 12.1042 - val_mae: 1.4201 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 6s - loss: 11.2521 - mse: 11.2521 - mae: 1.4352 - val_loss: 12.1876 - val_mse: 12.1876 - val_mae: 1.4043 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 6s - loss: 11.2993 - mse: 11.2993 - mae: 1.4314 - val_loss: 12.1286 - val_mse: 12.1286 - val_mae: 1.4174 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 6s - loss: 10.9518 - mse: 10.9518 - mae: 1.4226 - val_loss: 12.0988 - val_mse: 12.0988 - val_mae: 1.4380 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 6s - loss: 10.9602 - mse: 10.9602 - mae: 1.4151 - val_loss: 11.7661 - val_mse: 11.7661 - val_mae: 1.4761 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 6s - loss: 10.7670 - mse: 10.7670 - mae: 1.4123 - val_loss: 12.0554 - val_mse: 12.0554 - val_mae: 1.3979 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 6s - loss: 10.8514 - mse: 10.8514 - mae: 1.4050 - val_loss: 12.1480 - val_mse: 12.1480 - val_mae: 1.4426 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 6s - loss: 10.7482 - mse: 10.7482 - mae: 1.3970 - val_loss: 11.9392 - val_mse: 11.9392 - val_mae: 1.4432 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 6s - loss: 10.3606 - mse: 10.3606 - mae: 1.3899 - val_loss: 12.0028 - val_mse: 12.0028 - val_mae: 1.4022 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 6s - loss: 10.3823 - mse: 10.3823 - mae: 1.3804 - val_loss: 12.0562 - val_mse: 12.0562 - val_mae: 1.4322 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
            "Score for fold 2: loss of 12.056190490722656\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 6s - loss: 11.8887 - mse: 11.8887 - mae: 1.3995 - val_loss: 5.9763 - val_mse: 5.9763 - val_mae: 1.3440 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 6s - loss: 11.7046 - mse: 11.7046 - mae: 1.3898 - val_loss: 6.0692 - val_mse: 6.0692 - val_mae: 1.3384 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 6s - loss: 11.3198 - mse: 11.3198 - mae: 1.3762 - val_loss: 5.9178 - val_mse: 5.9178 - val_mae: 1.3252 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 6s - loss: 11.2780 - mse: 11.2780 - mae: 1.3675 - val_loss: 6.3382 - val_mse: 6.3382 - val_mae: 1.3884 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 6s - loss: 11.0898 - mse: 11.0898 - mae: 1.3540 - val_loss: 6.5703 - val_mse: 6.5703 - val_mae: 1.3676 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 6s - loss: 11.1526 - mse: 11.1526 - mae: 1.3490 - val_loss: 6.4872 - val_mse: 6.4872 - val_mae: 1.4229 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 6s - loss: 10.7775 - mse: 10.7775 - mae: 1.3340 - val_loss: 6.8837 - val_mse: 6.8837 - val_mae: 1.3937 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 6s - loss: 10.6386 - mse: 10.6386 - mae: 1.3265 - val_loss: 9.1189 - val_mse: 9.1189 - val_mae: 1.3495 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
            "Score for fold 3: loss of 9.118875503540039\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 6s - loss: 8.4463 - mse: 8.4463 - mae: 1.3330 - val_loss: 15.3149 - val_mse: 15.3149 - val_mae: 1.3694 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 6s - loss: 8.5097 - mse: 8.5097 - mae: 1.3236 - val_loss: 15.5554 - val_mse: 15.5554 - val_mae: 1.3553 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 6s - loss: 8.2451 - mse: 8.2451 - mae: 1.3093 - val_loss: 15.4188 - val_mse: 15.4188 - val_mae: 1.3906 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 6s - loss: 7.8879 - mse: 7.8879 - mae: 1.2951 - val_loss: 15.4319 - val_mse: 15.4319 - val_mae: 1.3780 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 6s - loss: 7.8261 - mse: 7.8261 - mae: 1.2809 - val_loss: 15.4800 - val_mse: 15.4800 - val_mae: 1.4177 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 6s - loss: 7.6663 - mse: 7.6663 - mae: 1.2741 - val_loss: 15.9767 - val_mse: 15.9767 - val_mae: 1.4128 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
            "Score for fold 4: loss of 15.976680755615234\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 6s - loss: 8.7638 - mse: 8.7638 - mae: 1.2945 - val_loss: 10.8053 - val_mse: 10.8053 - val_mae: 1.2905 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 6s - loss: 8.4865 - mse: 8.4865 - mae: 1.2810 - val_loss: 10.9271 - val_mse: 10.9271 - val_mae: 1.3162 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 6s - loss: 8.3508 - mse: 8.3508 - mae: 1.2603 - val_loss: 11.2388 - val_mse: 11.2388 - val_mae: 1.3270 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 6s - loss: 8.2775 - mse: 8.2775 - mae: 1.2551 - val_loss: 11.1321 - val_mse: 11.1321 - val_mae: 1.3218 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 6s - loss: 8.0519 - mse: 8.0519 - mae: 1.2421 - val_loss: 11.7086 - val_mse: 11.7086 - val_mae: 1.3775 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 6s - loss: 7.9709 - mse: 7.9709 - mae: 1.2305 - val_loss: 11.5996 - val_mse: 11.5996 - val_mae: 1.3703 - lr: 0.0010 - 6s/epoch - 6ms/step\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-10-12 12:08:54,357]\u001b[0m Finished trial#18 resulted in value: 11.436. Current best value is 10.178 with parameters: {'activation': 'relu', 'num_hidden_layer': 3, 'i': 9, 'learning_rate': 0.0019093737064063846}.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Score for fold 5: loss of 11.599563598632812\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 4s - loss: 13.6670 - mse: 13.6670 - mae: 1.5361 - val_loss: 9.8800 - val_mse: 9.8800 - val_mae: 1.4441 - lr: 7.9946e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 4s - loss: 12.9832 - mse: 12.9832 - mae: 1.4890 - val_loss: 10.2521 - val_mse: 10.2521 - val_mae: 1.3828 - lr: 7.9946e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 4s - loss: 12.8997 - mse: 12.8997 - mae: 1.4721 - val_loss: 9.6197 - val_mse: 9.6197 - val_mae: 1.5111 - lr: 7.9946e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 4s - loss: 12.8264 - mse: 12.8264 - mae: 1.4628 - val_loss: 9.7154 - val_mse: 9.7154 - val_mae: 1.4343 - lr: 7.9946e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 4s - loss: 12.6259 - mse: 12.6259 - mae: 1.4530 - val_loss: 9.7885 - val_mse: 9.7885 - val_mae: 1.5409 - lr: 7.9946e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 4s - loss: 12.4449 - mse: 12.4449 - mae: 1.4458 - val_loss: 9.5139 - val_mse: 9.5139 - val_mae: 1.5196 - lr: 7.9946e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 4s - loss: 12.4492 - mse: 12.4492 - mae: 1.4452 - val_loss: 9.7035 - val_mse: 9.7035 - val_mae: 1.4378 - lr: 7.9946e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 4s - loss: 12.2285 - mse: 12.2285 - mae: 1.4344 - val_loss: 9.7442 - val_mse: 9.7442 - val_mae: 1.5791 - lr: 7.9946e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 4s - loss: 11.9193 - mse: 11.9193 - mae: 1.4290 - val_loss: 10.1262 - val_mse: 10.1262 - val_mae: 1.4885 - lr: 7.9946e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 4s - loss: 12.0642 - mse: 12.0642 - mae: 1.4259 - val_loss: 9.2426 - val_mse: 9.2426 - val_mae: 1.4794 - lr: 7.9946e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 4s - loss: 11.9807 - mse: 11.9807 - mae: 1.4158 - val_loss: 9.6640 - val_mse: 9.6640 - val_mae: 1.4336 - lr: 7.9946e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 4s - loss: 11.5844 - mse: 11.5844 - mae: 1.4062 - val_loss: 9.8497 - val_mse: 9.8497 - val_mae: 1.4924 - lr: 7.9946e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 13/100\n",
            "1000/1000 - 4s - loss: 12.0220 - mse: 12.0220 - mae: 1.4039 - val_loss: 9.9896 - val_mse: 9.9896 - val_mae: 1.4687 - lr: 7.9946e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 14/100\n",
            "1000/1000 - 4s - loss: 11.5901 - mse: 11.5901 - mae: 1.3961 - val_loss: 9.6434 - val_mse: 9.6434 - val_mae: 1.4654 - lr: 7.9946e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 15/100\n",
            "1000/1000 - 4s - loss: 11.3356 - mse: 11.3356 - mae: 1.3905 - val_loss: 10.0485 - val_mse: 10.0485 - val_mae: 1.4566 - lr: 7.9946e-04 - 4s/epoch - 4ms/step\n",
            "Score for fold 1: loss of 10.048537254333496\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 4s - loss: 11.5230 - mse: 11.5230 - mae: 1.4175 - val_loss: 8.7353 - val_mse: 8.7353 - val_mae: 1.3326 - lr: 7.9946e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 4s - loss: 11.4666 - mse: 11.4666 - mae: 1.4100 - val_loss: 9.6916 - val_mse: 9.6916 - val_mae: 1.3913 - lr: 7.9946e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 4s - loss: 11.1388 - mse: 11.1388 - mae: 1.3947 - val_loss: 9.2271 - val_mse: 9.2271 - val_mae: 1.3907 - lr: 7.9946e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 4s - loss: 11.0494 - mse: 11.0494 - mae: 1.3890 - val_loss: 9.2574 - val_mse: 9.2574 - val_mae: 1.3551 - lr: 7.9946e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 4s - loss: 10.8191 - mse: 10.8191 - mae: 1.3766 - val_loss: 9.7618 - val_mse: 9.7618 - val_mae: 1.4113 - lr: 7.9946e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 4s - loss: 10.6047 - mse: 10.6047 - mae: 1.3639 - val_loss: 8.9821 - val_mse: 8.9821 - val_mae: 1.3692 - lr: 7.9946e-04 - 4s/epoch - 4ms/step\n",
            "Score for fold 2: loss of 8.982133865356445\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 4s - loss: 8.3918 - mse: 8.3918 - mae: 1.3568 - val_loss: 18.5067 - val_mse: 18.5067 - val_mae: 1.4072 - lr: 7.9946e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 4s - loss: 8.0213 - mse: 8.0213 - mae: 1.3417 - val_loss: 17.8981 - val_mse: 17.8981 - val_mae: 1.3626 - lr: 7.9946e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 4s - loss: 7.8543 - mse: 7.8543 - mae: 1.3277 - val_loss: 18.8606 - val_mse: 18.8606 - val_mae: 1.4353 - lr: 7.9946e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 4s - loss: 7.8200 - mse: 7.8200 - mae: 1.3170 - val_loss: 18.6579 - val_mse: 18.6579 - val_mae: 1.3969 - lr: 7.9946e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 4s - loss: 7.5540 - mse: 7.5540 - mae: 1.3056 - val_loss: 19.1489 - val_mse: 19.1489 - val_mae: 1.4174 - lr: 7.9946e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 4s - loss: 7.3118 - mse: 7.3118 - mae: 1.2851 - val_loss: 18.2589 - val_mse: 18.2589 - val_mae: 1.4179 - lr: 7.9946e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 4s - loss: 7.1270 - mse: 7.1270 - mae: 1.2754 - val_loss: 18.7145 - val_mse: 18.7145 - val_mae: 1.4335 - lr: 7.9946e-04 - 4s/epoch - 4ms/step\n",
            "Score for fold 3: loss of 18.71448516845703\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 4s - loss: 10.1221 - mse: 10.1221 - mae: 1.3082 - val_loss: 7.4814 - val_mse: 7.4814 - val_mae: 1.2632 - lr: 7.9946e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 4s - loss: 9.9695 - mse: 9.9695 - mae: 1.2953 - val_loss: 7.4007 - val_mse: 7.4007 - val_mae: 1.3182 - lr: 7.9946e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 4s - loss: 9.4206 - mse: 9.4206 - mae: 1.2759 - val_loss: 7.4469 - val_mse: 7.4469 - val_mae: 1.3027 - lr: 7.9946e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 4s - loss: 9.5477 - mse: 9.5477 - mae: 1.2682 - val_loss: 7.5901 - val_mse: 7.5901 - val_mae: 1.2988 - lr: 7.9946e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 4s - loss: 8.9733 - mse: 8.9733 - mae: 1.2508 - val_loss: 7.6307 - val_mse: 7.6307 - val_mae: 1.3824 - lr: 7.9946e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 4s - loss: 9.0296 - mse: 9.0296 - mae: 1.2382 - val_loss: 7.7946 - val_mse: 7.7946 - val_mae: 1.2918 - lr: 7.9946e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 4s - loss: 8.9199 - mse: 8.9199 - mae: 1.2243 - val_loss: 7.9123 - val_mse: 7.9123 - val_mae: 1.3194 - lr: 7.9946e-04 - 4s/epoch - 4ms/step\n",
            "Score for fold 4: loss of 7.9123358726501465\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 4s - loss: 9.0153 - mse: 9.0153 - mae: 1.2544 - val_loss: 6.4298 - val_mse: 6.4298 - val_mae: 1.2475 - lr: 7.9946e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 4s - loss: 8.8138 - mse: 8.8138 - mae: 1.2371 - val_loss: 6.6702 - val_mse: 6.6702 - val_mae: 1.2403 - lr: 7.9946e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 4s - loss: 8.4059 - mse: 8.4059 - mae: 1.2167 - val_loss: 6.7606 - val_mse: 6.7606 - val_mae: 1.2547 - lr: 7.9946e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 4s - loss: 8.3214 - mse: 8.3214 - mae: 1.2083 - val_loss: 7.2711 - val_mse: 7.2711 - val_mae: 1.2152 - lr: 7.9946e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 4s - loss: 8.1723 - mse: 8.1723 - mae: 1.1864 - val_loss: 7.0217 - val_mse: 7.0217 - val_mae: 1.2631 - lr: 7.9946e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 4s - loss: 8.0246 - mse: 8.0246 - mae: 1.1760 - val_loss: 6.8443 - val_mse: 6.8443 - val_mae: 1.2863 - lr: 7.9946e-04 - 4s/epoch - 4ms/step\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-10-12 12:11:31,975]\u001b[0m Finished trial#19 resulted in value: 10.498000000000001. Current best value is 10.178 with parameters: {'activation': 'relu', 'num_hidden_layer': 3, 'i': 9, 'learning_rate': 0.0019093737064063846}.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Score for fold 5: loss of 6.8442535400390625\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 3s - loss: 14.8582 - mse: 14.8582 - mae: 1.5888 - val_loss: 11.3862 - val_mse: 11.3862 - val_mae: 1.6044 - lr: 0.0020 - 3s/epoch - 3ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 14.4839 - mse: 14.4839 - mae: 1.5251 - val_loss: 11.1413 - val_mse: 11.1413 - val_mae: 1.5959 - lr: 0.0020 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 14.2828 - mse: 14.2828 - mae: 1.5191 - val_loss: 10.9838 - val_mse: 10.9838 - val_mae: 1.5823 - lr: 0.0020 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 14.1932 - mse: 14.1932 - mae: 1.5119 - val_loss: 11.0245 - val_mse: 11.0245 - val_mae: 1.5483 - lr: 0.0020 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 14.2171 - mse: 14.2171 - mae: 1.5038 - val_loss: 11.1197 - val_mse: 11.1197 - val_mae: 1.5375 - lr: 0.0020 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 14.1645 - mse: 14.1645 - mae: 1.5017 - val_loss: 10.9437 - val_mse: 10.9437 - val_mae: 1.5792 - lr: 0.0020 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 14.0282 - mse: 14.0282 - mae: 1.4946 - val_loss: 10.8794 - val_mse: 10.8794 - val_mae: 1.5372 - lr: 0.0020 - 2s/epoch - 2ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 2s - loss: 14.0144 - mse: 14.0144 - mae: 1.4911 - val_loss: 11.0145 - val_mse: 11.0145 - val_mae: 1.4730 - lr: 0.0020 - 2s/epoch - 2ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 2s - loss: 13.9234 - mse: 13.9234 - mae: 1.4933 - val_loss: 10.8035 - val_mse: 10.8035 - val_mae: 1.5338 - lr: 0.0020 - 2s/epoch - 2ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 2s - loss: 13.9034 - mse: 13.9034 - mae: 1.4858 - val_loss: 11.0094 - val_mse: 11.0094 - val_mae: 1.5183 - lr: 0.0020 - 2s/epoch - 2ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 2s - loss: 13.8793 - mse: 13.8793 - mae: 1.4882 - val_loss: 10.9544 - val_mse: 10.9544 - val_mae: 1.4852 - lr: 0.0020 - 2s/epoch - 2ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 2s - loss: 13.7806 - mse: 13.7806 - mae: 1.4834 - val_loss: 10.9248 - val_mse: 10.9248 - val_mae: 1.5013 - lr: 0.0020 - 2s/epoch - 2ms/step\n",
            "Epoch 13/100\n",
            "1000/1000 - 2s - loss: 13.7736 - mse: 13.7736 - mae: 1.4798 - val_loss: 10.9325 - val_mse: 10.9325 - val_mae: 1.5230 - lr: 0.0020 - 2s/epoch - 2ms/step\n",
            "Epoch 14/100\n",
            "1000/1000 - 2s - loss: 13.6906 - mse: 13.6906 - mae: 1.4787 - val_loss: 10.7691 - val_mse: 10.7691 - val_mae: 1.4893 - lr: 0.0020 - 2s/epoch - 2ms/step\n",
            "Epoch 15/100\n",
            "1000/1000 - 2s - loss: 13.5981 - mse: 13.5981 - mae: 1.4762 - val_loss: 10.7148 - val_mse: 10.7148 - val_mae: 1.5426 - lr: 0.0020 - 2s/epoch - 2ms/step\n",
            "Epoch 16/100\n",
            "1000/1000 - 2s - loss: 13.6073 - mse: 13.6073 - mae: 1.4755 - val_loss: 10.8110 - val_mse: 10.8110 - val_mae: 1.4920 - lr: 0.0020 - 2s/epoch - 2ms/step\n",
            "Epoch 17/100\n",
            "1000/1000 - 2s - loss: 13.5584 - mse: 13.5584 - mae: 1.4724 - val_loss: 10.8407 - val_mse: 10.8407 - val_mae: 1.5505 - lr: 0.0020 - 2s/epoch - 2ms/step\n",
            "Epoch 18/100\n",
            "1000/1000 - 2s - loss: 13.4760 - mse: 13.4760 - mae: 1.4671 - val_loss: 10.6237 - val_mse: 10.6237 - val_mae: 1.4807 - lr: 0.0020 - 2s/epoch - 2ms/step\n",
            "Epoch 19/100\n",
            "1000/1000 - 2s - loss: 13.4094 - mse: 13.4094 - mae: 1.4674 - val_loss: 10.7121 - val_mse: 10.7121 - val_mae: 1.4973 - lr: 0.0020 - 2s/epoch - 2ms/step\n",
            "Epoch 20/100\n",
            "1000/1000 - 2s - loss: 13.4496 - mse: 13.4496 - mae: 1.4667 - val_loss: 10.6655 - val_mse: 10.6655 - val_mae: 1.4724 - lr: 0.0020 - 2s/epoch - 2ms/step\n",
            "Epoch 21/100\n",
            "1000/1000 - 2s - loss: 13.3733 - mse: 13.3733 - mae: 1.4691 - val_loss: 10.7109 - val_mse: 10.7109 - val_mae: 1.5135 - lr: 0.0020 - 2s/epoch - 2ms/step\n",
            "Epoch 22/100\n",
            "1000/1000 - 2s - loss: 13.3043 - mse: 13.3043 - mae: 1.4656 - val_loss: 10.6457 - val_mse: 10.6457 - val_mae: 1.4981 - lr: 0.0020 - 2s/epoch - 2ms/step\n",
            "Epoch 23/100\n",
            "1000/1000 - 2s - loss: 13.2811 - mse: 13.2811 - mae: 1.4631 - val_loss: 10.5768 - val_mse: 10.5768 - val_mae: 1.5366 - lr: 0.0020 - 2s/epoch - 2ms/step\n",
            "Epoch 24/100\n",
            "1000/1000 - 2s - loss: 13.2347 - mse: 13.2347 - mae: 1.4620 - val_loss: 10.4860 - val_mse: 10.4860 - val_mae: 1.5188 - lr: 0.0020 - 2s/epoch - 2ms/step\n",
            "Epoch 25/100\n",
            "1000/1000 - 2s - loss: 13.2510 - mse: 13.2510 - mae: 1.4641 - val_loss: 10.5257 - val_mse: 10.5257 - val_mae: 1.4666 - lr: 0.0020 - 2s/epoch - 2ms/step\n",
            "Epoch 26/100\n",
            "1000/1000 - 2s - loss: 13.1964 - mse: 13.1964 - mae: 1.4673 - val_loss: 10.6022 - val_mse: 10.6022 - val_mae: 1.4625 - lr: 0.0020 - 2s/epoch - 2ms/step\n",
            "Epoch 27/100\n",
            "1000/1000 - 2s - loss: 13.1698 - mse: 13.1698 - mae: 1.4664 - val_loss: 10.5687 - val_mse: 10.5687 - val_mae: 1.5005 - lr: 0.0020 - 2s/epoch - 2ms/step\n",
            "Epoch 28/100\n",
            "1000/1000 - 2s - loss: 13.0843 - mse: 13.0843 - mae: 1.4646 - val_loss: 10.6701 - val_mse: 10.6701 - val_mae: 1.5107 - lr: 0.0020 - 2s/epoch - 2ms/step\n",
            "Epoch 29/100\n",
            "1000/1000 - 2s - loss: 13.0848 - mse: 13.0848 - mae: 1.4597 - val_loss: 10.7353 - val_mse: 10.7353 - val_mae: 1.5618 - lr: 0.0020 - 2s/epoch - 2ms/step\n",
            "Score for fold 1: loss of 10.735319137573242\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 10.5619 - mse: 10.5619 - mae: 1.4432 - val_loss: 19.8748 - val_mse: 19.8748 - val_mae: 1.4364 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 10.5035 - mse: 10.5035 - mae: 1.4392 - val_loss: 20.0810 - val_mse: 20.0810 - val_mae: 1.4653 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 10.5425 - mse: 10.5425 - mae: 1.4372 - val_loss: 20.1866 - val_mse: 20.1866 - val_mae: 1.4675 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 10.5227 - mse: 10.5227 - mae: 1.4330 - val_loss: 19.9259 - val_mse: 19.9259 - val_mae: 1.4622 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 10.4591 - mse: 10.4591 - mae: 1.4353 - val_loss: 20.1256 - val_mse: 20.1256 - val_mae: 1.4606 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 10.4494 - mse: 10.4494 - mae: 1.4326 - val_loss: 20.1867 - val_mse: 20.1867 - val_mae: 1.4887 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Score for fold 2: loss of 20.186683654785156\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 13.2237 - mse: 13.2237 - mae: 1.4489 - val_loss: 8.9052 - val_mse: 8.9052 - val_mae: 1.3965 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 13.1879 - mse: 13.1879 - mae: 1.4432 - val_loss: 8.8039 - val_mse: 8.8039 - val_mae: 1.4312 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 13.1584 - mse: 13.1584 - mae: 1.4415 - val_loss: 8.9047 - val_mse: 8.9047 - val_mae: 1.4672 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 13.1322 - mse: 13.1322 - mae: 1.4440 - val_loss: 8.8393 - val_mse: 8.8393 - val_mae: 1.4685 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 13.1370 - mse: 13.1370 - mae: 1.4452 - val_loss: 8.9322 - val_mse: 8.9322 - val_mae: 1.4216 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 13.1089 - mse: 13.1089 - mae: 1.4439 - val_loss: 8.9630 - val_mse: 8.9630 - val_mae: 1.4414 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 13.0360 - mse: 13.0360 - mae: 1.4434 - val_loss: 9.0735 - val_mse: 9.0735 - val_mae: 1.4056 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Score for fold 3: loss of 9.073521614074707\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 12.1057 - mse: 12.1057 - mae: 1.4404 - val_loss: 12.9572 - val_mse: 12.9572 - val_mae: 1.4447 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 12.0784 - mse: 12.0784 - mae: 1.4379 - val_loss: 12.9015 - val_mse: 12.9015 - val_mae: 1.4113 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 12.0227 - mse: 12.0227 - mae: 1.4369 - val_loss: 12.9076 - val_mse: 12.9076 - val_mae: 1.4985 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 11.9942 - mse: 11.9942 - mae: 1.4372 - val_loss: 12.9547 - val_mse: 12.9547 - val_mae: 1.4271 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 11.9781 - mse: 11.9781 - mae: 1.4347 - val_loss: 12.8981 - val_mse: 12.8981 - val_mae: 1.4682 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 11.9313 - mse: 11.9313 - mae: 1.4332 - val_loss: 12.9751 - val_mse: 12.9751 - val_mae: 1.4401 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 11.9373 - mse: 11.9373 - mae: 1.4379 - val_loss: 12.9506 - val_mse: 12.9506 - val_mae: 1.4523 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 2s - loss: 11.9482 - mse: 11.9482 - mae: 1.4349 - val_loss: 12.9806 - val_mse: 12.9806 - val_mae: 1.4556 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 2s - loss: 11.9082 - mse: 11.9082 - mae: 1.4318 - val_loss: 12.9479 - val_mse: 12.9479 - val_mae: 1.4288 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 2s - loss: 11.8941 - mse: 11.8941 - mae: 1.4308 - val_loss: 12.8839 - val_mse: 12.8839 - val_mae: 1.4425 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 2s - loss: 11.8714 - mse: 11.8714 - mae: 1.4309 - val_loss: 13.0982 - val_mse: 13.0982 - val_mae: 1.5017 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 2s - loss: 11.8573 - mse: 11.8573 - mae: 1.4338 - val_loss: 13.0356 - val_mse: 13.0356 - val_mae: 1.4959 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 13/100\n",
            "1000/1000 - 2s - loss: 11.8768 - mse: 11.8768 - mae: 1.4269 - val_loss: 13.0271 - val_mse: 13.0271 - val_mae: 1.5051 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 14/100\n",
            "1000/1000 - 2s - loss: 11.8036 - mse: 11.8036 - mae: 1.4330 - val_loss: 13.0135 - val_mse: 13.0135 - val_mae: 1.4494 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 15/100\n",
            "1000/1000 - 2s - loss: 11.8382 - mse: 11.8382 - mae: 1.4301 - val_loss: 12.9910 - val_mse: 12.9910 - val_mae: 1.5154 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Score for fold 4: loss of 12.99095344543457\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 12.7068 - mse: 12.7068 - mae: 1.4490 - val_loss: 9.5268 - val_mse: 9.5268 - val_mae: 1.3965 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 12.6900 - mse: 12.6900 - mae: 1.4496 - val_loss: 9.5765 - val_mse: 9.5765 - val_mae: 1.3956 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 12.6487 - mse: 12.6487 - mae: 1.4477 - val_loss: 9.5004 - val_mse: 9.5004 - val_mae: 1.4352 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 12.6123 - mse: 12.6123 - mae: 1.4477 - val_loss: 9.6476 - val_mse: 9.6476 - val_mae: 1.4169 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 12.6240 - mse: 12.6240 - mae: 1.4439 - val_loss: 9.5993 - val_mse: 9.5993 - val_mae: 1.4030 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 12.6252 - mse: 12.6252 - mae: 1.4443 - val_loss: 9.6665 - val_mse: 9.6665 - val_mae: 1.4282 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 12.5529 - mse: 12.5529 - mae: 1.4426 - val_loss: 9.7183 - val_mse: 9.7183 - val_mae: 1.4191 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 2s - loss: 12.5966 - mse: 12.5966 - mae: 1.4442 - val_loss: 9.7190 - val_mse: 9.7190 - val_mae: 1.4390 - lr: 0.0010 - 2s/epoch - 2ms/step\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-10-12 12:14:10,420]\u001b[0m Finished trial#20 resulted in value: 12.542. Current best value is 10.178 with parameters: {'activation': 'relu', 'num_hidden_layer': 3, 'i': 9, 'learning_rate': 0.0019093737064063846}.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Score for fold 5: loss of 9.718949317932129\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 4s - loss: 13.8669 - mse: 13.8669 - mae: 1.5390 - val_loss: 9.5901 - val_mse: 9.5901 - val_mae: 1.5909 - lr: 7.8510e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 4s - loss: 13.1180 - mse: 13.1180 - mae: 1.4867 - val_loss: 9.1188 - val_mse: 9.1188 - val_mae: 1.4901 - lr: 7.8510e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 4s - loss: 12.8913 - mse: 12.8913 - mae: 1.4656 - val_loss: 9.5217 - val_mse: 9.5217 - val_mae: 1.4659 - lr: 7.8510e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 4s - loss: 12.7191 - mse: 12.7191 - mae: 1.4601 - val_loss: 9.2853 - val_mse: 9.2853 - val_mae: 1.4455 - lr: 7.8510e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 4s - loss: 12.5648 - mse: 12.5648 - mae: 1.4520 - val_loss: 9.0579 - val_mse: 9.0579 - val_mae: 1.4835 - lr: 7.8510e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 4s - loss: 12.5240 - mse: 12.5240 - mae: 1.4427 - val_loss: 9.1021 - val_mse: 9.1021 - val_mae: 1.4124 - lr: 7.8510e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 4s - loss: 12.6026 - mse: 12.6026 - mae: 1.4363 - val_loss: 8.8590 - val_mse: 8.8590 - val_mae: 1.4493 - lr: 7.8510e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 4s - loss: 12.1621 - mse: 12.1621 - mae: 1.4290 - val_loss: 10.2210 - val_mse: 10.2210 - val_mae: 1.4526 - lr: 7.8510e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 4s - loss: 12.1936 - mse: 12.1936 - mae: 1.4227 - val_loss: 9.7466 - val_mse: 9.7466 - val_mae: 1.4857 - lr: 7.8510e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 4s - loss: 12.1307 - mse: 12.1307 - mae: 1.4227 - val_loss: 9.0270 - val_mse: 9.0270 - val_mae: 1.5027 - lr: 7.8510e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 4s - loss: 11.9313 - mse: 11.9313 - mae: 1.4126 - val_loss: 8.8396 - val_mse: 8.8396 - val_mae: 1.4549 - lr: 7.8510e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 4s - loss: 11.8274 - mse: 11.8274 - mae: 1.4058 - val_loss: 9.0629 - val_mse: 9.0629 - val_mae: 1.4877 - lr: 7.8510e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 13/100\n",
            "1000/1000 - 4s - loss: 11.6733 - mse: 11.6733 - mae: 1.3957 - val_loss: 8.8284 - val_mse: 8.8284 - val_mae: 1.4392 - lr: 7.8510e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 14/100\n",
            "1000/1000 - 4s - loss: 11.6613 - mse: 11.6613 - mae: 1.3948 - val_loss: 9.0201 - val_mse: 9.0201 - val_mae: 1.4899 - lr: 7.8510e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 15/100\n",
            "1000/1000 - 4s - loss: 11.4969 - mse: 11.4969 - mae: 1.3899 - val_loss: 9.1306 - val_mse: 9.1306 - val_mae: 1.4483 - lr: 7.8510e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 16/100\n",
            "1000/1000 - 4s - loss: 11.3385 - mse: 11.3385 - mae: 1.3819 - val_loss: 9.1270 - val_mse: 9.1270 - val_mae: 1.4530 - lr: 7.8510e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 17/100\n",
            "1000/1000 - 4s - loss: 11.2471 - mse: 11.2471 - mae: 1.3770 - val_loss: 8.7705 - val_mse: 8.7705 - val_mae: 1.4510 - lr: 7.8510e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 18/100\n",
            "1000/1000 - 4s - loss: 11.1154 - mse: 11.1154 - mae: 1.3669 - val_loss: 9.5711 - val_mse: 9.5711 - val_mae: 1.4856 - lr: 7.8510e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 19/100\n",
            "1000/1000 - 4s - loss: 10.8071 - mse: 10.8071 - mae: 1.3566 - val_loss: 9.8559 - val_mse: 9.8559 - val_mae: 1.4896 - lr: 7.8510e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 20/100\n",
            "1000/1000 - 4s - loss: 10.6977 - mse: 10.6977 - mae: 1.3464 - val_loss: 9.4194 - val_mse: 9.4194 - val_mae: 1.4724 - lr: 7.8510e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 21/100\n",
            "1000/1000 - 4s - loss: 10.5768 - mse: 10.5768 - mae: 1.3397 - val_loss: 9.0002 - val_mse: 9.0002 - val_mae: 1.4703 - lr: 7.8510e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 22/100\n",
            "1000/1000 - 4s - loss: 10.5228 - mse: 10.5228 - mae: 1.3285 - val_loss: 9.6429 - val_mse: 9.6429 - val_mae: 1.4834 - lr: 7.8510e-04 - 4s/epoch - 4ms/step\n",
            "Score for fold 1: loss of 9.642876625061035\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 4s - loss: 10.5463 - mse: 10.5463 - mae: 1.3708 - val_loss: 9.8047 - val_mse: 9.8047 - val_mae: 1.3521 - lr: 7.8510e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 4s - loss: 10.1432 - mse: 10.1432 - mae: 1.3552 - val_loss: 9.6534 - val_mse: 9.6534 - val_mae: 1.3717 - lr: 7.8510e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 4s - loss: 9.7951 - mse: 9.7951 - mae: 1.3369 - val_loss: 9.7450 - val_mse: 9.7450 - val_mae: 1.4100 - lr: 7.8510e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 4s - loss: 9.7465 - mse: 9.7465 - mae: 1.3276 - val_loss: 10.0748 - val_mse: 10.0748 - val_mae: 1.3617 - lr: 7.8510e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 4s - loss: 9.7214 - mse: 9.7214 - mae: 1.3184 - val_loss: 10.0518 - val_mse: 10.0518 - val_mae: 1.4203 - lr: 7.8510e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 4s - loss: 9.4148 - mse: 9.4148 - mae: 1.3058 - val_loss: 11.0345 - val_mse: 11.0345 - val_mae: 1.4414 - lr: 7.8510e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 4s - loss: 9.3203 - mse: 9.3203 - mae: 1.2911 - val_loss: 10.0891 - val_mse: 10.0891 - val_mae: 1.3827 - lr: 7.8510e-04 - 4s/epoch - 4ms/step\n",
            "Score for fold 2: loss of 10.089073181152344\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 4s - loss: 8.0753 - mse: 8.0753 - mae: 1.3088 - val_loss: 15.6467 - val_mse: 15.6467 - val_mae: 1.2852 - lr: 7.8510e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 4s - loss: 7.9660 - mse: 7.9660 - mae: 1.2915 - val_loss: 15.1243 - val_mse: 15.1243 - val_mae: 1.3024 - lr: 7.8510e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 4s - loss: 7.7069 - mse: 7.7069 - mae: 1.2760 - val_loss: 15.3325 - val_mse: 15.3325 - val_mae: 1.3224 - lr: 7.8510e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 4s - loss: 7.4215 - mse: 7.4215 - mae: 1.2612 - val_loss: 16.2635 - val_mse: 16.2635 - val_mae: 1.3063 - lr: 7.8510e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 4s - loss: 7.4845 - mse: 7.4845 - mae: 1.2478 - val_loss: 15.4632 - val_mse: 15.4632 - val_mae: 1.3436 - lr: 7.8510e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 4s - loss: 7.0065 - mse: 7.0065 - mae: 1.2269 - val_loss: 15.9095 - val_mse: 15.9095 - val_mae: 1.3259 - lr: 7.8510e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 4s - loss: 6.8882 - mse: 6.8882 - mae: 1.2146 - val_loss: 15.1255 - val_mse: 15.1255 - val_mae: 1.3795 - lr: 7.8510e-04 - 4s/epoch - 4ms/step\n",
            "Score for fold 3: loss of 15.12549114227295\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 4s - loss: 9.1912 - mse: 9.1912 - mae: 1.2565 - val_loss: 6.4421 - val_mse: 6.4421 - val_mae: 1.2212 - lr: 7.8510e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 4s - loss: 8.9994 - mse: 8.9994 - mae: 1.2311 - val_loss: 7.2098 - val_mse: 7.2098 - val_mae: 1.2377 - lr: 7.8510e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 4s - loss: 8.7793 - mse: 8.7793 - mae: 1.2143 - val_loss: 6.1312 - val_mse: 6.1312 - val_mae: 1.2323 - lr: 7.8510e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 4s - loss: 8.4301 - mse: 8.4301 - mae: 1.2004 - val_loss: 6.6493 - val_mse: 6.6493 - val_mae: 1.2533 - lr: 7.8510e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 4s - loss: 8.3964 - mse: 8.3964 - mae: 1.1861 - val_loss: 6.3154 - val_mse: 6.3154 - val_mae: 1.3115 - lr: 7.8510e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 4s - loss: 8.1053 - mse: 8.1053 - mae: 1.1722 - val_loss: 6.8354 - val_mse: 6.8354 - val_mae: 1.2886 - lr: 7.8510e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 4s - loss: 8.0647 - mse: 8.0647 - mae: 1.1592 - val_loss: 6.6648 - val_mse: 6.6648 - val_mae: 1.2633 - lr: 7.8510e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 4s - loss: 8.0005 - mse: 8.0005 - mae: 1.1427 - val_loss: 6.5108 - val_mse: 6.5108 - val_mae: 1.2662 - lr: 7.8510e-04 - 4s/epoch - 4ms/step\n",
            "Score for fold 4: loss of 6.510818958282471\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 4s - loss: 7.7149 - mse: 7.7149 - mae: 1.1851 - val_loss: 6.9699 - val_mse: 6.9699 - val_mae: 1.1109 - lr: 7.8510e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 4s - loss: 7.4070 - mse: 7.4070 - mae: 1.1640 - val_loss: 7.1583 - val_mse: 7.1583 - val_mae: 1.1435 - lr: 7.8510e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 4s - loss: 7.3156 - mse: 7.3156 - mae: 1.1483 - val_loss: 7.2784 - val_mse: 7.2784 - val_mae: 1.1649 - lr: 7.8510e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 4s - loss: 7.4022 - mse: 7.4022 - mae: 1.1359 - val_loss: 7.2424 - val_mse: 7.2424 - val_mae: 1.1731 - lr: 7.8510e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 4s - loss: 6.8785 - mse: 6.8785 - mae: 1.1199 - val_loss: 7.2042 - val_mse: 7.2042 - val_mae: 1.1712 - lr: 7.8510e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 4s - loss: 6.8953 - mse: 6.8953 - mae: 1.1069 - val_loss: 7.7161 - val_mse: 7.7161 - val_mae: 1.1913 - lr: 7.8510e-04 - 4s/epoch - 4ms/step\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-10-12 12:17:23,892]\u001b[0m Finished trial#21 resulted in value: 9.818. Current best value is 9.818 with parameters: {'activation': 'relu', 'num_hidden_layer': 3, 'i': 9, 'learning_rate': 0.0007851045775996924}.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Score for fold 5: loss of 7.716129779815674\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 4s - loss: 13.6675 - mse: 13.6675 - mae: 1.5393 - val_loss: 10.6946 - val_mse: 10.6946 - val_mae: 1.6087 - lr: 4.8951e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 4s - loss: 12.8145 - mse: 12.8145 - mae: 1.4808 - val_loss: 11.3154 - val_mse: 11.3154 - val_mae: 1.4002 - lr: 4.8951e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 4s - loss: 12.6211 - mse: 12.6211 - mae: 1.4609 - val_loss: 10.3120 - val_mse: 10.3120 - val_mae: 1.4398 - lr: 4.8951e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 4s - loss: 12.6463 - mse: 12.6463 - mae: 1.4561 - val_loss: 10.5215 - val_mse: 10.5215 - val_mae: 1.5977 - lr: 4.8951e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 4s - loss: 12.4145 - mse: 12.4145 - mae: 1.4484 - val_loss: 10.1585 - val_mse: 10.1585 - val_mae: 1.4568 - lr: 4.8951e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 4s - loss: 12.2393 - mse: 12.2393 - mae: 1.4398 - val_loss: 10.3013 - val_mse: 10.3013 - val_mae: 1.5067 - lr: 4.8951e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 4s - loss: 12.1508 - mse: 12.1508 - mae: 1.4350 - val_loss: 10.2562 - val_mse: 10.2562 - val_mae: 1.5040 - lr: 4.8951e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 4s - loss: 12.1257 - mse: 12.1257 - mae: 1.4314 - val_loss: 10.1783 - val_mse: 10.1783 - val_mae: 1.4132 - lr: 4.8951e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 4s - loss: 11.8010 - mse: 11.8010 - mae: 1.4198 - val_loss: 10.0181 - val_mse: 10.0181 - val_mae: 1.4690 - lr: 4.8951e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 4s - loss: 11.7615 - mse: 11.7615 - mae: 1.4143 - val_loss: 10.0393 - val_mse: 10.0393 - val_mae: 1.4625 - lr: 4.8951e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 4s - loss: 11.6387 - mse: 11.6387 - mae: 1.4052 - val_loss: 10.3740 - val_mse: 10.3740 - val_mae: 1.4688 - lr: 4.8951e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 4s - loss: 11.4519 - mse: 11.4519 - mae: 1.3986 - val_loss: 10.2273 - val_mse: 10.2273 - val_mae: 1.4673 - lr: 4.8951e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 13/100\n",
            "1000/1000 - 4s - loss: 11.5047 - mse: 11.5047 - mae: 1.3958 - val_loss: 10.3350 - val_mse: 10.3350 - val_mae: 1.4819 - lr: 4.8951e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 14/100\n",
            "1000/1000 - 4s - loss: 11.3355 - mse: 11.3355 - mae: 1.3837 - val_loss: 9.9922 - val_mse: 9.9922 - val_mae: 1.4945 - lr: 4.8951e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 15/100\n",
            "1000/1000 - 4s - loss: 11.1830 - mse: 11.1830 - mae: 1.3777 - val_loss: 9.9392 - val_mse: 9.9392 - val_mae: 1.4791 - lr: 4.8951e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 16/100\n",
            "1000/1000 - 4s - loss: 11.0043 - mse: 11.0043 - mae: 1.3684 - val_loss: 10.1694 - val_mse: 10.1694 - val_mae: 1.4981 - lr: 4.8951e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 17/100\n",
            "1000/1000 - 4s - loss: 10.7594 - mse: 10.7594 - mae: 1.3595 - val_loss: 9.9487 - val_mse: 9.9487 - val_mae: 1.4342 - lr: 4.8951e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 18/100\n",
            "1000/1000 - 4s - loss: 10.7871 - mse: 10.7871 - mae: 1.3476 - val_loss: 10.4919 - val_mse: 10.4919 - val_mae: 1.4660 - lr: 4.8951e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 19/100\n",
            "1000/1000 - 4s - loss: 10.4502 - mse: 10.4502 - mae: 1.3355 - val_loss: 10.5061 - val_mse: 10.5061 - val_mae: 1.5173 - lr: 4.8951e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 20/100\n",
            "1000/1000 - 4s - loss: 10.2843 - mse: 10.2843 - mae: 1.3284 - val_loss: 10.1873 - val_mse: 10.1873 - val_mae: 1.4503 - lr: 4.8951e-04 - 4s/epoch - 4ms/step\n",
            "Score for fold 1: loss of 10.187320709228516\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 4s - loss: 11.3016 - mse: 11.3016 - mae: 1.3674 - val_loss: 7.2990 - val_mse: 7.2990 - val_mae: 1.2980 - lr: 4.8951e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 4s - loss: 10.8024 - mse: 10.8024 - mae: 1.3523 - val_loss: 7.3303 - val_mse: 7.3303 - val_mae: 1.3460 - lr: 4.8951e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 4s - loss: 10.9043 - mse: 10.9043 - mae: 1.3412 - val_loss: 7.5739 - val_mse: 7.5739 - val_mae: 1.3342 - lr: 4.8951e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 4s - loss: 10.4950 - mse: 10.4950 - mae: 1.3325 - val_loss: 7.5141 - val_mse: 7.5141 - val_mae: 1.3739 - lr: 4.8951e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 4s - loss: 10.2944 - mse: 10.2944 - mae: 1.3191 - val_loss: 7.6901 - val_mse: 7.6901 - val_mae: 1.3694 - lr: 4.8951e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 4s - loss: 10.1677 - mse: 10.1677 - mae: 1.3041 - val_loss: 7.7520 - val_mse: 7.7520 - val_mae: 1.3368 - lr: 4.8951e-04 - 4s/epoch - 4ms/step\n",
            "Score for fold 2: loss of 7.752044200897217\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 4s - loss: 9.8858 - mse: 9.8858 - mae: 1.3255 - val_loss: 7.8200 - val_mse: 7.8200 - val_mae: 1.2872 - lr: 4.8951e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 4s - loss: 9.8617 - mse: 9.8617 - mae: 1.3071 - val_loss: 7.7578 - val_mse: 7.7578 - val_mae: 1.3142 - lr: 4.8951e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 4s - loss: 9.6144 - mse: 9.6144 - mae: 1.2900 - val_loss: 8.1778 - val_mse: 8.1778 - val_mae: 1.3424 - lr: 4.8951e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 4s - loss: 9.4184 - mse: 9.4184 - mae: 1.2772 - val_loss: 8.5615 - val_mse: 8.5615 - val_mae: 1.3674 - lr: 4.8951e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 4s - loss: 9.2224 - mse: 9.2224 - mae: 1.2683 - val_loss: 8.6794 - val_mse: 8.6794 - val_mae: 1.3380 - lr: 4.8951e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 4s - loss: 9.1749 - mse: 9.1749 - mae: 1.2530 - val_loss: 8.4259 - val_mse: 8.4259 - val_mae: 1.3248 - lr: 4.8951e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 4s - loss: 8.7920 - mse: 8.7920 - mae: 1.2355 - val_loss: 8.3040 - val_mse: 8.3040 - val_mae: 1.3118 - lr: 4.8951e-04 - 4s/epoch - 4ms/step\n",
            "Score for fold 3: loss of 8.304046630859375\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 4s - loss: 6.9781 - mse: 6.9781 - mae: 1.2547 - val_loss: 15.6472 - val_mse: 15.6472 - val_mae: 1.2515 - lr: 4.8951e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 4s - loss: 7.0720 - mse: 7.0720 - mae: 1.2377 - val_loss: 15.6798 - val_mse: 15.6798 - val_mae: 1.2700 - lr: 4.8951e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 4s - loss: 6.4683 - mse: 6.4683 - mae: 1.2187 - val_loss: 16.0875 - val_mse: 16.0875 - val_mae: 1.2613 - lr: 4.8951e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 4s - loss: 6.4109 - mse: 6.4109 - mae: 1.2018 - val_loss: 16.1278 - val_mse: 16.1278 - val_mae: 1.2579 - lr: 4.8951e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 4s - loss: 6.6395 - mse: 6.6395 - mae: 1.1905 - val_loss: 16.0224 - val_mse: 16.0224 - val_mae: 1.3120 - lr: 4.8951e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 4s - loss: 6.0686 - mse: 6.0686 - mae: 1.1752 - val_loss: 16.6066 - val_mse: 16.6066 - val_mae: 1.3233 - lr: 4.8951e-04 - 4s/epoch - 4ms/step\n",
            "Score for fold 4: loss of 16.606557846069336\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 4s - loss: 8.6776 - mse: 8.6776 - mae: 1.2062 - val_loss: 5.8953 - val_mse: 5.8953 - val_mae: 1.1235 - lr: 4.8951e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 4s - loss: 8.6781 - mse: 8.6781 - mae: 1.1916 - val_loss: 5.9871 - val_mse: 5.9871 - val_mae: 1.1584 - lr: 4.8951e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 4s - loss: 8.1047 - mse: 8.1047 - mae: 1.1736 - val_loss: 6.2165 - val_mse: 6.2165 - val_mae: 1.1771 - lr: 4.8951e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 4s - loss: 7.9164 - mse: 7.9164 - mae: 1.1546 - val_loss: 6.1809 - val_mse: 6.1809 - val_mae: 1.1498 - lr: 4.8951e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 4s - loss: 8.1193 - mse: 8.1193 - mae: 1.1452 - val_loss: 6.2951 - val_mse: 6.2951 - val_mae: 1.1756 - lr: 4.8951e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 4s - loss: 7.5951 - mse: 7.5951 - mae: 1.1275 - val_loss: 6.5630 - val_mse: 6.5630 - val_mae: 1.2156 - lr: 4.8951e-04 - 4s/epoch - 4ms/step\n",
            "Score for fold 5: loss of 6.5630316734313965\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-10-12 12:20:19,105]\u001b[0m Finished trial#22 resulted in value: 9.882. Current best value is 9.818 with parameters: {'activation': 'relu', 'num_hidden_layer': 3, 'i': 9, 'learning_rate': 0.0007851045775996924}.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 3s - loss: 12.0386 - mse: 12.0386 - mae: 1.5521 - val_loss: 18.3571 - val_mse: 18.3571 - val_mae: 1.4772 - lr: 3.7138e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 11.0909 - mse: 11.0909 - mae: 1.4952 - val_loss: 17.9702 - val_mse: 17.9702 - val_mae: 1.4732 - lr: 3.7138e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 10.7459 - mse: 10.7459 - mae: 1.4778 - val_loss: 17.9483 - val_mse: 17.9483 - val_mae: 1.5784 - lr: 3.7138e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 10.6845 - mse: 10.6845 - mae: 1.4667 - val_loss: 17.7546 - val_mse: 17.7546 - val_mae: 1.4531 - lr: 3.7138e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 10.5350 - mse: 10.5350 - mae: 1.4550 - val_loss: 17.9799 - val_mse: 17.9799 - val_mae: 1.5131 - lr: 3.7138e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 10.2768 - mse: 10.2768 - mae: 1.4510 - val_loss: 18.0376 - val_mse: 18.0376 - val_mae: 1.4324 - lr: 3.7138e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 10.2405 - mse: 10.2405 - mae: 1.4415 - val_loss: 17.8119 - val_mse: 17.8119 - val_mae: 1.4632 - lr: 3.7138e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 2s - loss: 10.2344 - mse: 10.2344 - mae: 1.4382 - val_loss: 17.7457 - val_mse: 17.7457 - val_mae: 1.4058 - lr: 3.7138e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 2s - loss: 10.2043 - mse: 10.2043 - mae: 1.4369 - val_loss: 17.8208 - val_mse: 17.8208 - val_mae: 1.4512 - lr: 3.7138e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 2s - loss: 10.1808 - mse: 10.1808 - mae: 1.4317 - val_loss: 18.0140 - val_mse: 18.0140 - val_mae: 1.3831 - lr: 3.7138e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 2s - loss: 10.0479 - mse: 10.0479 - mae: 1.4268 - val_loss: 17.5974 - val_mse: 17.5974 - val_mae: 1.4457 - lr: 3.7138e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 2s - loss: 9.9354 - mse: 9.9354 - mae: 1.4179 - val_loss: 17.9269 - val_mse: 17.9269 - val_mae: 1.4150 - lr: 3.7138e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 13/100\n",
            "1000/1000 - 2s - loss: 9.9349 - mse: 9.9349 - mae: 1.4145 - val_loss: 17.7894 - val_mse: 17.7894 - val_mae: 1.4249 - lr: 3.7138e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 14/100\n",
            "1000/1000 - 2s - loss: 9.7768 - mse: 9.7768 - mae: 1.4128 - val_loss: 17.7334 - val_mse: 17.7334 - val_mae: 1.4853 - lr: 3.7138e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 15/100\n",
            "1000/1000 - 2s - loss: 9.9201 - mse: 9.9201 - mae: 1.4105 - val_loss: 17.6824 - val_mse: 17.6824 - val_mae: 1.4793 - lr: 3.7138e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 16/100\n",
            "1000/1000 - 2s - loss: 9.7413 - mse: 9.7413 - mae: 1.4030 - val_loss: 17.7360 - val_mse: 17.7360 - val_mae: 1.4387 - lr: 3.7138e-04 - 2s/epoch - 2ms/step\n",
            "Score for fold 1: loss of 17.73597526550293\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 11.0198 - mse: 11.0198 - mae: 1.4112 - val_loss: 12.3111 - val_mse: 12.3111 - val_mae: 1.3963 - lr: 3.7138e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 10.8827 - mse: 10.8827 - mae: 1.4061 - val_loss: 12.1565 - val_mse: 12.1565 - val_mae: 1.3634 - lr: 3.7138e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 10.9004 - mse: 10.9004 - mae: 1.3999 - val_loss: 12.3398 - val_mse: 12.3398 - val_mae: 1.3956 - lr: 3.7138e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 10.8165 - mse: 10.8165 - mae: 1.3920 - val_loss: 12.4356 - val_mse: 12.4356 - val_mae: 1.4140 - lr: 3.7138e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 10.5426 - mse: 10.5426 - mae: 1.3885 - val_loss: 12.4365 - val_mse: 12.4365 - val_mae: 1.4227 - lr: 3.7138e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 3s - loss: 10.5061 - mse: 10.5061 - mae: 1.3839 - val_loss: 12.2301 - val_mse: 12.2301 - val_mae: 1.4462 - lr: 3.7138e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 3s - loss: 10.4173 - mse: 10.4173 - mae: 1.3795 - val_loss: 12.5585 - val_mse: 12.5585 - val_mae: 1.4165 - lr: 3.7138e-04 - 3s/epoch - 3ms/step\n",
            "Score for fold 2: loss of 12.558478355407715\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 11.3461 - mse: 11.3461 - mae: 1.3831 - val_loss: 9.2670 - val_mse: 9.2670 - val_mae: 1.4071 - lr: 3.7138e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 11.0877 - mse: 11.0877 - mae: 1.3710 - val_loss: 9.0803 - val_mse: 9.0803 - val_mae: 1.3893 - lr: 3.7138e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 11.0605 - mse: 11.0605 - mae: 1.3659 - val_loss: 8.9230 - val_mse: 8.9230 - val_mae: 1.4043 - lr: 3.7138e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 10.9623 - mse: 10.9623 - mae: 1.3599 - val_loss: 9.3488 - val_mse: 9.3488 - val_mae: 1.3847 - lr: 3.7138e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 10.9611 - mse: 10.9611 - mae: 1.3530 - val_loss: 9.3957 - val_mse: 9.3957 - val_mae: 1.4418 - lr: 3.7138e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 10.8281 - mse: 10.8281 - mae: 1.3524 - val_loss: 9.1377 - val_mse: 9.1377 - val_mae: 1.4641 - lr: 3.7138e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 10.6769 - mse: 10.6769 - mae: 1.3438 - val_loss: 9.1052 - val_mse: 9.1052 - val_mae: 1.4359 - lr: 3.7138e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 2s - loss: 10.6497 - mse: 10.6497 - mae: 1.3372 - val_loss: 9.1288 - val_mse: 9.1288 - val_mae: 1.4243 - lr: 3.7138e-04 - 2s/epoch - 2ms/step\n",
            "Score for fold 3: loss of 9.128801345825195\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 11.4456 - mse: 11.4456 - mae: 1.3677 - val_loss: 6.5369 - val_mse: 6.5369 - val_mae: 1.3067 - lr: 3.7138e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 11.3097 - mse: 11.3097 - mae: 1.3601 - val_loss: 6.4291 - val_mse: 6.4291 - val_mae: 1.3733 - lr: 3.7138e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 11.0752 - mse: 11.0752 - mae: 1.3539 - val_loss: 6.4873 - val_mse: 6.4873 - val_mae: 1.3300 - lr: 3.7138e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 11.0578 - mse: 11.0578 - mae: 1.3424 - val_loss: 6.9550 - val_mse: 6.9550 - val_mae: 1.3350 - lr: 3.7138e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 10.9004 - mse: 10.9004 - mae: 1.3340 - val_loss: 6.4752 - val_mse: 6.4752 - val_mae: 1.3259 - lr: 3.7138e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 10.8735 - mse: 10.8735 - mae: 1.3347 - val_loss: 6.5715 - val_mse: 6.5715 - val_mae: 1.3499 - lr: 3.7138e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 10.7152 - mse: 10.7152 - mae: 1.3250 - val_loss: 6.7678 - val_mse: 6.7678 - val_mae: 1.3315 - lr: 3.7138e-04 - 2s/epoch - 2ms/step\n",
            "Score for fold 4: loss of 6.7678093910217285\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 10.2783 - mse: 10.2783 - mae: 1.3408 - val_loss: 7.7566 - val_mse: 7.7566 - val_mae: 1.2717 - lr: 3.7138e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 10.4917 - mse: 10.4917 - mae: 1.3322 - val_loss: 7.9870 - val_mse: 7.9870 - val_mae: 1.2897 - lr: 3.7138e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 10.2803 - mse: 10.2803 - mae: 1.3256 - val_loss: 8.4771 - val_mse: 8.4771 - val_mae: 1.2646 - lr: 3.7138e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 10.1302 - mse: 10.1302 - mae: 1.3220 - val_loss: 8.4249 - val_mse: 8.4249 - val_mae: 1.2743 - lr: 3.7138e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 10.0612 - mse: 10.0612 - mae: 1.3117 - val_loss: 7.9608 - val_mse: 7.9608 - val_mae: 1.3280 - lr: 3.7138e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 9.8504 - mse: 9.8504 - mae: 1.3011 - val_loss: 8.2466 - val_mse: 8.2466 - val_mae: 1.3030 - lr: 3.7138e-04 - 2s/epoch - 2ms/step\n",
            "Score for fold 5: loss of 8.246620178222656\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-10-12 12:22:10,780]\u001b[0m Finished trial#23 resulted in value: 10.89. Current best value is 9.818 with parameters: {'activation': 'relu', 'num_hidden_layer': 3, 'i': 9, 'learning_rate': 0.0007851045775996924}.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 4s - loss: 13.9841 - mse: 13.9841 - mae: 1.5517 - val_loss: 8.9811 - val_mse: 8.9811 - val_mae: 1.4084 - lr: 5.9846e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 4s - loss: 13.2683 - mse: 13.2683 - mae: 1.4944 - val_loss: 8.6438 - val_mse: 8.6438 - val_mae: 1.4058 - lr: 5.9846e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 4s - loss: 13.0135 - mse: 13.0135 - mae: 1.4807 - val_loss: 8.5097 - val_mse: 8.5097 - val_mae: 1.4582 - lr: 5.9846e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 4s - loss: 13.1043 - mse: 13.1043 - mae: 1.4698 - val_loss: 8.5276 - val_mse: 8.5276 - val_mae: 1.3990 - lr: 5.9846e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 4s - loss: 12.8378 - mse: 12.8378 - mae: 1.4615 - val_loss: 8.3283 - val_mse: 8.3283 - val_mae: 1.4957 - lr: 5.9846e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 4s - loss: 12.6681 - mse: 12.6681 - mae: 1.4541 - val_loss: 8.3757 - val_mse: 8.3757 - val_mae: 1.4445 - lr: 5.9846e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 4s - loss: 12.5970 - mse: 12.5970 - mae: 1.4462 - val_loss: 8.5726 - val_mse: 8.5726 - val_mae: 1.4097 - lr: 5.9846e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 4s - loss: 12.3162 - mse: 12.3162 - mae: 1.4384 - val_loss: 8.8783 - val_mse: 8.8783 - val_mae: 1.4218 - lr: 5.9846e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 4s - loss: 12.4024 - mse: 12.4024 - mae: 1.4361 - val_loss: 8.3065 - val_mse: 8.3065 - val_mae: 1.4586 - lr: 5.9846e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 4s - loss: 12.1352 - mse: 12.1352 - mae: 1.4290 - val_loss: 8.3102 - val_mse: 8.3102 - val_mae: 1.4319 - lr: 5.9846e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 4s - loss: 12.2870 - mse: 12.2870 - mae: 1.4239 - val_loss: 8.5106 - val_mse: 8.5106 - val_mae: 1.4427 - lr: 5.9846e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 4s - loss: 11.9473 - mse: 11.9473 - mae: 1.4137 - val_loss: 8.4129 - val_mse: 8.4129 - val_mae: 1.4388 - lr: 5.9846e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 13/100\n",
            "1000/1000 - 4s - loss: 11.6793 - mse: 11.6793 - mae: 1.4040 - val_loss: 8.5367 - val_mse: 8.5367 - val_mae: 1.3841 - lr: 5.9846e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 14/100\n",
            "1000/1000 - 4s - loss: 11.6675 - mse: 11.6675 - mae: 1.4029 - val_loss: 8.3044 - val_mse: 8.3044 - val_mae: 1.4019 - lr: 5.9846e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 15/100\n",
            "1000/1000 - 4s - loss: 11.3632 - mse: 11.3632 - mae: 1.3908 - val_loss: 8.5809 - val_mse: 8.5809 - val_mae: 1.4367 - lr: 5.9846e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 16/100\n",
            "1000/1000 - 4s - loss: 11.3750 - mse: 11.3750 - mae: 1.3862 - val_loss: 8.4395 - val_mse: 8.4395 - val_mae: 1.4162 - lr: 5.9846e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 17/100\n",
            "1000/1000 - 4s - loss: 11.3761 - mse: 11.3761 - mae: 1.3760 - val_loss: 8.4621 - val_mse: 8.4621 - val_mae: 1.4088 - lr: 5.9846e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 18/100\n",
            "1000/1000 - 4s - loss: 11.1190 - mse: 11.1190 - mae: 1.3650 - val_loss: 8.6607 - val_mse: 8.6607 - val_mae: 1.4176 - lr: 5.9846e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 19/100\n",
            "1000/1000 - 4s - loss: 11.0143 - mse: 11.0143 - mae: 1.3599 - val_loss: 8.5394 - val_mse: 8.5394 - val_mae: 1.4462 - lr: 5.9846e-04 - 4s/epoch - 4ms/step\n",
            "Score for fold 1: loss of 8.539409637451172\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 4s - loss: 9.6214 - mse: 9.6214 - mae: 1.3695 - val_loss: 13.0459 - val_mse: 13.0459 - val_mae: 1.4220 - lr: 5.9846e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 4s - loss: 9.4992 - mse: 9.4992 - mae: 1.3559 - val_loss: 13.2137 - val_mse: 13.2137 - val_mae: 1.4064 - lr: 5.9846e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 4s - loss: 9.5096 - mse: 9.5096 - mae: 1.3503 - val_loss: 13.5394 - val_mse: 13.5394 - val_mae: 1.3949 - lr: 5.9846e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 4s - loss: 9.2852 - mse: 9.2852 - mae: 1.3371 - val_loss: 13.7478 - val_mse: 13.7478 - val_mae: 1.3825 - lr: 5.9846e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 4s - loss: 9.1258 - mse: 9.1258 - mae: 1.3238 - val_loss: 13.5264 - val_mse: 13.5264 - val_mae: 1.4567 - lr: 5.9846e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 4s - loss: 9.0522 - mse: 9.0522 - mae: 1.3152 - val_loss: 13.7160 - val_mse: 13.7160 - val_mae: 1.3986 - lr: 5.9846e-04 - 4s/epoch - 4ms/step\n",
            "Score for fold 2: loss of 13.715970993041992\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 4s - loss: 8.4317 - mse: 8.4317 - mae: 1.3294 - val_loss: 15.8106 - val_mse: 15.8106 - val_mae: 1.3370 - lr: 5.9846e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 4s - loss: 8.3187 - mse: 8.3187 - mae: 1.3158 - val_loss: 15.9475 - val_mse: 15.9475 - val_mae: 1.3425 - lr: 5.9846e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 4s - loss: 7.9143 - mse: 7.9143 - mae: 1.2992 - val_loss: 15.5896 - val_mse: 15.5896 - val_mae: 1.3490 - lr: 5.9846e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 4s - loss: 7.7151 - mse: 7.7151 - mae: 1.2896 - val_loss: 15.3839 - val_mse: 15.3839 - val_mae: 1.3425 - lr: 5.9846e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 4s - loss: 7.7186 - mse: 7.7186 - mae: 1.2726 - val_loss: 15.7984 - val_mse: 15.7984 - val_mae: 1.3589 - lr: 5.9846e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 4s - loss: 7.4656 - mse: 7.4656 - mae: 1.2606 - val_loss: 16.8159 - val_mse: 16.8159 - val_mae: 1.3478 - lr: 5.9846e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 4s - loss: 7.3488 - mse: 7.3488 - mae: 1.2466 - val_loss: 16.1542 - val_mse: 16.1542 - val_mae: 1.3960 - lr: 5.9846e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 4s - loss: 7.2479 - mse: 7.2479 - mae: 1.2420 - val_loss: 16.5993 - val_mse: 16.5993 - val_mae: 1.3738 - lr: 5.9846e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 4s - loss: 6.8990 - mse: 6.8990 - mae: 1.2206 - val_loss: 15.9579 - val_mse: 15.9579 - val_mae: 1.3886 - lr: 5.9846e-04 - 4s/epoch - 4ms/step\n",
            "Score for fold 3: loss of 15.95788288116455\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 4s - loss: 9.7430 - mse: 9.7430 - mae: 1.2687 - val_loss: 5.1516 - val_mse: 5.1516 - val_mae: 1.1780 - lr: 5.9846e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 4s - loss: 9.2673 - mse: 9.2673 - mae: 1.2497 - val_loss: 5.0968 - val_mse: 5.0968 - val_mae: 1.1846 - lr: 5.9846e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 4s - loss: 9.1587 - mse: 9.1587 - mae: 1.2325 - val_loss: 6.2879 - val_mse: 6.2879 - val_mae: 1.2212 - lr: 5.9846e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 4s - loss: 9.0615 - mse: 9.0615 - mae: 1.2173 - val_loss: 5.4226 - val_mse: 5.4226 - val_mae: 1.2545 - lr: 5.9846e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 4s - loss: 8.9892 - mse: 8.9892 - mae: 1.2022 - val_loss: 5.4228 - val_mse: 5.4228 - val_mae: 1.2427 - lr: 5.9846e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 4s - loss: 8.5826 - mse: 8.5826 - mae: 1.1881 - val_loss: 5.4379 - val_mse: 5.4379 - val_mae: 1.2371 - lr: 5.9846e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 4s - loss: 8.3096 - mse: 8.3096 - mae: 1.1719 - val_loss: 6.2231 - val_mse: 6.2231 - val_mae: 1.2743 - lr: 5.9846e-04 - 4s/epoch - 4ms/step\n",
            "Score for fold 4: loss of 6.2231035232543945\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 4s - loss: 8.9847 - mse: 8.9847 - mae: 1.2071 - val_loss: 4.6502 - val_mse: 4.6502 - val_mae: 1.1198 - lr: 5.9846e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 4s - loss: 8.4834 - mse: 8.4834 - mae: 1.1826 - val_loss: 5.0788 - val_mse: 5.0788 - val_mae: 1.1709 - lr: 5.9846e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 4s - loss: 8.2035 - mse: 8.2035 - mae: 1.1716 - val_loss: 5.0841 - val_mse: 5.0841 - val_mae: 1.1376 - lr: 5.9846e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 4s - loss: 7.9026 - mse: 7.9026 - mae: 1.1519 - val_loss: 5.3904 - val_mse: 5.3904 - val_mae: 1.1973 - lr: 5.9846e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 4s - loss: 7.8630 - mse: 7.8630 - mae: 1.1367 - val_loss: 5.3343 - val_mse: 5.3343 - val_mae: 1.2043 - lr: 5.9846e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 5s - loss: 7.7141 - mse: 7.7141 - mae: 1.1237 - val_loss: 5.7505 - val_mse: 5.7505 - val_mae: 1.2261 - lr: 5.9846e-04 - 5s/epoch - 5ms/step\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-10-12 12:25:15,511]\u001b[0m Finished trial#24 resulted in value: 10.038. Current best value is 9.818 with parameters: {'activation': 'relu', 'num_hidden_layer': 3, 'i': 9, 'learning_rate': 0.0007851045775996924}.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Score for fold 5: loss of 5.750537395477295\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 4s - loss: 15.4042 - mse: 15.4042 - mae: 1.6447 - val_loss: 9.4909 - val_mse: 9.4909 - val_mae: 1.5454 - lr: 3.4887e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 3s - loss: 14.4150 - mse: 14.4150 - mae: 1.5718 - val_loss: 9.2777 - val_mse: 9.2777 - val_mae: 1.5703 - lr: 3.4887e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 3s - loss: 14.4133 - mse: 14.4133 - mae: 1.5686 - val_loss: 9.1432 - val_mse: 9.1432 - val_mae: 1.5235 - lr: 3.4887e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 3s - loss: 14.3321 - mse: 14.3321 - mae: 1.5642 - val_loss: 9.1764 - val_mse: 9.1764 - val_mae: 1.4985 - lr: 3.4887e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 3s - loss: 14.3570 - mse: 14.3570 - mae: 1.5623 - val_loss: 9.2186 - val_mse: 9.2186 - val_mae: 1.5371 - lr: 3.4887e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 3s - loss: 14.2558 - mse: 14.2558 - mae: 1.5584 - val_loss: 9.1580 - val_mse: 9.1580 - val_mae: 1.5511 - lr: 3.4887e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 3s - loss: 14.1973 - mse: 14.1973 - mae: 1.5551 - val_loss: 9.2919 - val_mse: 9.2919 - val_mae: 1.4710 - lr: 3.4887e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 3s - loss: 14.3309 - mse: 14.3309 - mae: 1.5559 - val_loss: 9.1667 - val_mse: 9.1667 - val_mae: 1.5249 - lr: 3.4887e-04 - 3s/epoch - 3ms/step\n",
            "Score for fold 1: loss of 9.166677474975586\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 3s - loss: 13.6768 - mse: 13.6768 - mae: 1.5475 - val_loss: 11.6955 - val_mse: 11.6955 - val_mae: 1.5370 - lr: 3.4887e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 3s - loss: 13.7021 - mse: 13.7021 - mae: 1.5474 - val_loss: 12.0475 - val_mse: 12.0475 - val_mae: 1.4788 - lr: 3.4887e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 3s - loss: 13.6125 - mse: 13.6125 - mae: 1.5501 - val_loss: 11.6976 - val_mse: 11.6976 - val_mae: 1.5325 - lr: 3.4887e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 3s - loss: 13.6669 - mse: 13.6669 - mae: 1.5517 - val_loss: 11.8439 - val_mse: 11.8439 - val_mae: 1.5028 - lr: 3.4887e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 3s - loss: 13.6999 - mse: 13.6999 - mae: 1.5482 - val_loss: 11.6431 - val_mse: 11.6431 - val_mae: 1.5246 - lr: 3.4887e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 3s - loss: 13.6684 - mse: 13.6684 - mae: 1.5537 - val_loss: 11.7302 - val_mse: 11.7302 - val_mae: 1.5523 - lr: 3.4887e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 3s - loss: 13.6551 - mse: 13.6551 - mae: 1.5471 - val_loss: 11.7348 - val_mse: 11.7348 - val_mae: 1.5775 - lr: 3.4887e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 3s - loss: 13.7233 - mse: 13.7233 - mae: 1.5562 - val_loss: 11.6979 - val_mse: 11.6979 - val_mae: 1.5399 - lr: 3.4887e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 3s - loss: 13.6188 - mse: 13.6188 - mae: 1.5461 - val_loss: 11.6477 - val_mse: 11.6477 - val_mae: 1.5520 - lr: 3.4887e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 3s - loss: 13.6310 - mse: 13.6310 - mae: 1.5507 - val_loss: 11.7411 - val_mse: 11.7411 - val_mae: 1.5084 - lr: 3.4887e-04 - 3s/epoch - 3ms/step\n",
            "Score for fold 2: loss of 11.741090774536133\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 3s - loss: 13.7499 - mse: 13.7499 - mae: 1.5549 - val_loss: 11.0893 - val_mse: 11.0893 - val_mae: 1.4820 - lr: 3.4887e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 3s - loss: 13.7641 - mse: 13.7641 - mae: 1.5581 - val_loss: 11.2060 - val_mse: 11.2060 - val_mae: 1.4851 - lr: 3.4887e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 3s - loss: 13.8025 - mse: 13.8025 - mae: 1.5633 - val_loss: 11.0315 - val_mse: 11.0315 - val_mae: 1.5545 - lr: 3.4887e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 3s - loss: 13.7744 - mse: 13.7744 - mae: 1.5575 - val_loss: 11.0255 - val_mse: 11.0255 - val_mae: 1.5472 - lr: 3.4887e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 3s - loss: 13.8005 - mse: 13.8005 - mae: 1.5591 - val_loss: 11.0039 - val_mse: 11.0039 - val_mae: 1.5199 - lr: 3.4887e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 3s - loss: 13.7717 - mse: 13.7717 - mae: 1.5576 - val_loss: 11.0008 - val_mse: 11.0008 - val_mae: 1.5321 - lr: 3.4887e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 3s - loss: 13.7752 - mse: 13.7752 - mae: 1.5557 - val_loss: 11.0238 - val_mse: 11.0238 - val_mae: 1.5209 - lr: 3.4887e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 3s - loss: 13.7751 - mse: 13.7751 - mae: 1.5564 - val_loss: 10.9944 - val_mse: 10.9944 - val_mae: 1.5633 - lr: 3.4887e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 3s - loss: 13.8102 - mse: 13.8102 - mae: 1.5574 - val_loss: 10.9875 - val_mse: 10.9875 - val_mae: 1.5640 - lr: 3.4887e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 3s - loss: 13.8146 - mse: 13.8146 - mae: 1.5599 - val_loss: 11.0973 - val_mse: 11.0973 - val_mae: 1.4848 - lr: 3.4887e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 3s - loss: 13.8200 - mse: 13.8200 - mae: 1.5554 - val_loss: 11.0148 - val_mse: 11.0148 - val_mae: 1.5226 - lr: 3.4887e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 3s - loss: 13.7800 - mse: 13.7800 - mae: 1.5586 - val_loss: 11.0984 - val_mse: 11.0984 - val_mae: 1.4976 - lr: 3.4887e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 13/100\n",
            "1000/1000 - 3s - loss: 13.7685 - mse: 13.7685 - mae: 1.5590 - val_loss: 10.9926 - val_mse: 10.9926 - val_mae: 1.5471 - lr: 3.4887e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 14/100\n",
            "1000/1000 - 3s - loss: 13.7784 - mse: 13.7784 - mae: 1.5608 - val_loss: 10.9746 - val_mse: 10.9746 - val_mae: 1.5167 - lr: 3.4887e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 15/100\n",
            "1000/1000 - 3s - loss: 13.7611 - mse: 13.7611 - mae: 1.5599 - val_loss: 11.0475 - val_mse: 11.0475 - val_mae: 1.4744 - lr: 3.4887e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 16/100\n",
            "1000/1000 - 3s - loss: 13.7597 - mse: 13.7597 - mae: 1.5596 - val_loss: 11.0477 - val_mse: 11.0477 - val_mae: 1.4922 - lr: 3.4887e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 17/100\n",
            "1000/1000 - 3s - loss: 13.8393 - mse: 13.8393 - mae: 1.5656 - val_loss: 11.0631 - val_mse: 11.0631 - val_mae: 1.4692 - lr: 3.4887e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 18/100\n",
            "1000/1000 - 3s - loss: 13.7796 - mse: 13.7796 - mae: 1.5581 - val_loss: 11.0185 - val_mse: 11.0185 - val_mae: 1.5189 - lr: 3.4887e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 19/100\n",
            "1000/1000 - 3s - loss: 13.7484 - mse: 13.7484 - mae: 1.5666 - val_loss: 11.0163 - val_mse: 11.0163 - val_mae: 1.4952 - lr: 3.4887e-04 - 3s/epoch - 3ms/step\n",
            "Score for fold 3: loss of 11.016274452209473\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 3s - loss: 13.1330 - mse: 13.1330 - mae: 1.5400 - val_loss: 13.6074 - val_mse: 13.6074 - val_mae: 1.5255 - lr: 3.4887e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 3s - loss: 13.1434 - mse: 13.1434 - mae: 1.5429 - val_loss: 13.5341 - val_mse: 13.5341 - val_mae: 1.5352 - lr: 3.4887e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 3s - loss: 13.1173 - mse: 13.1173 - mae: 1.5445 - val_loss: 13.5909 - val_mse: 13.5909 - val_mae: 1.5786 - lr: 3.4887e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 3s - loss: 13.1815 - mse: 13.1815 - mae: 1.5457 - val_loss: 13.6885 - val_mse: 13.6885 - val_mae: 1.5126 - lr: 3.4887e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 3s - loss: 13.1486 - mse: 13.1486 - mae: 1.5425 - val_loss: 13.5569 - val_mse: 13.5569 - val_mae: 1.5782 - lr: 3.4887e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 3s - loss: 13.1115 - mse: 13.1115 - mae: 1.5446 - val_loss: 13.5346 - val_mse: 13.5346 - val_mae: 1.5557 - lr: 3.4887e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 3s - loss: 13.0774 - mse: 13.0774 - mae: 1.5431 - val_loss: 13.5602 - val_mse: 13.5602 - val_mae: 1.5701 - lr: 3.4887e-04 - 3s/epoch - 3ms/step\n",
            "Score for fold 4: loss of 13.560208320617676\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 3s - loss: 11.3482 - mse: 11.3482 - mae: 1.5342 - val_loss: 20.5781 - val_mse: 20.5781 - val_mae: 1.5917 - lr: 3.4887e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 3s - loss: 11.3168 - mse: 11.3168 - mae: 1.5325 - val_loss: 20.5357 - val_mse: 20.5357 - val_mae: 1.5733 - lr: 3.4887e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 3s - loss: 11.3485 - mse: 11.3485 - mae: 1.5344 - val_loss: 20.5983 - val_mse: 20.5983 - val_mae: 1.6053 - lr: 3.4887e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 3s - loss: 11.3511 - mse: 11.3511 - mae: 1.5384 - val_loss: 20.7157 - val_mse: 20.7157 - val_mae: 1.6039 - lr: 3.4887e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 3s - loss: 11.3650 - mse: 11.3650 - mae: 1.5390 - val_loss: 20.9482 - val_mse: 20.9482 - val_mae: 1.6422 - lr: 3.4887e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 3s - loss: 11.3801 - mse: 11.3801 - mae: 1.5358 - val_loss: 20.7652 - val_mse: 20.7652 - val_mae: 1.5879 - lr: 3.4887e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 3s - loss: 11.4105 - mse: 11.4105 - mae: 1.5355 - val_loss: 20.6320 - val_mse: 20.6320 - val_mae: 1.5665 - lr: 3.4887e-04 - 3s/epoch - 3ms/step\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-10-12 12:27:45,538]\u001b[0m Finished trial#25 resulted in value: 13.224. Current best value is 9.818 with parameters: {'activation': 'relu', 'num_hidden_layer': 3, 'i': 9, 'learning_rate': 0.0007851045775996924}.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Score for fold 5: loss of 20.631996154785156\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 5s - loss: 10.9229 - mse: 10.9229 - mae: 1.5281 - val_loss: 21.3059 - val_mse: 21.3059 - val_mae: 1.5990 - lr: 6.7615e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 5s - loss: 10.1184 - mse: 10.1184 - mae: 1.4678 - val_loss: 21.3156 - val_mse: 21.3156 - val_mae: 1.4680 - lr: 6.7615e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 5s - loss: 10.1372 - mse: 10.1372 - mae: 1.4614 - val_loss: 21.0948 - val_mse: 21.0948 - val_mae: 1.5749 - lr: 6.7615e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 5s - loss: 9.8523 - mse: 9.8523 - mae: 1.4549 - val_loss: 21.0787 - val_mse: 21.0787 - val_mae: 1.4641 - lr: 6.7615e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 5s - loss: 9.6828 - mse: 9.6828 - mae: 1.4411 - val_loss: 20.8557 - val_mse: 20.8557 - val_mae: 1.4921 - lr: 6.7615e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 5s - loss: 9.6179 - mse: 9.6179 - mae: 1.4281 - val_loss: 20.8012 - val_mse: 20.8012 - val_mae: 1.5569 - lr: 6.7615e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 5s - loss: 9.4090 - mse: 9.4090 - mae: 1.4272 - val_loss: 20.7071 - val_mse: 20.7071 - val_mae: 1.4968 - lr: 6.7615e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 5s - loss: 9.3289 - mse: 9.3289 - mae: 1.4213 - val_loss: 20.8919 - val_mse: 20.8919 - val_mae: 1.4501 - lr: 6.7615e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 5s - loss: 9.1694 - mse: 9.1694 - mae: 1.4103 - val_loss: 20.6888 - val_mse: 20.6888 - val_mae: 1.5610 - lr: 6.7615e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 5s - loss: 9.3077 - mse: 9.3077 - mae: 1.4070 - val_loss: 20.6195 - val_mse: 20.6195 - val_mae: 1.5502 - lr: 6.7615e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 5s - loss: 9.0096 - mse: 9.0096 - mae: 1.3989 - val_loss: 20.8882 - val_mse: 20.8882 - val_mae: 1.4874 - lr: 6.7615e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 5s - loss: 8.8522 - mse: 8.8522 - mae: 1.3912 - val_loss: 20.8574 - val_mse: 20.8574 - val_mae: 1.4938 - lr: 6.7615e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 13/100\n",
            "1000/1000 - 5s - loss: 8.7406 - mse: 8.7406 - mae: 1.3838 - val_loss: 20.7959 - val_mse: 20.7959 - val_mae: 1.5335 - lr: 6.7615e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 14/100\n",
            "1000/1000 - 5s - loss: 8.7062 - mse: 8.7062 - mae: 1.3796 - val_loss: 20.9135 - val_mse: 20.9135 - val_mae: 1.5804 - lr: 6.7615e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 15/100\n",
            "1000/1000 - 5s - loss: 8.5721 - mse: 8.5721 - mae: 1.3691 - val_loss: 20.8652 - val_mse: 20.8652 - val_mae: 1.5039 - lr: 6.7615e-04 - 5s/epoch - 5ms/step\n",
            "Score for fold 1: loss of 20.865161895751953\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 5s - loss: 11.7510 - mse: 11.7510 - mae: 1.4068 - val_loss: 7.1146 - val_mse: 7.1146 - val_mae: 1.3312 - lr: 6.7615e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 5s - loss: 11.7895 - mse: 11.7895 - mae: 1.3985 - val_loss: 7.3204 - val_mse: 7.3204 - val_mae: 1.3925 - lr: 6.7615e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 5s - loss: 11.4338 - mse: 11.4338 - mae: 1.3916 - val_loss: 7.6411 - val_mse: 7.6411 - val_mae: 1.3978 - lr: 6.7615e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 5s - loss: 11.4480 - mse: 11.4480 - mae: 1.3856 - val_loss: 7.4898 - val_mse: 7.4898 - val_mae: 1.3997 - lr: 6.7615e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 5s - loss: 11.4719 - mse: 11.4719 - mae: 1.3815 - val_loss: 7.1383 - val_mse: 7.1383 - val_mae: 1.4142 - lr: 6.7615e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 5s - loss: 11.0372 - mse: 11.0372 - mae: 1.3692 - val_loss: 7.4367 - val_mse: 7.4367 - val_mae: 1.3808 - lr: 6.7615e-04 - 5s/epoch - 5ms/step\n",
            "Score for fold 2: loss of 7.436729431152344\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 5s - loss: 10.4087 - mse: 10.4087 - mae: 1.3640 - val_loss: 9.6620 - val_mse: 9.6620 - val_mae: 1.3227 - lr: 6.7615e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 5s - loss: 10.3869 - mse: 10.3869 - mae: 1.3577 - val_loss: 9.5650 - val_mse: 9.5650 - val_mae: 1.3145 - lr: 6.7615e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 5s - loss: 10.0980 - mse: 10.0980 - mae: 1.3438 - val_loss: 9.6059 - val_mse: 9.6059 - val_mae: 1.3656 - lr: 6.7615e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 5s - loss: 9.9123 - mse: 9.9123 - mae: 1.3315 - val_loss: 9.7908 - val_mse: 9.7908 - val_mae: 1.3631 - lr: 6.7615e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 5s - loss: 9.6340 - mse: 9.6340 - mae: 1.3174 - val_loss: 10.6751 - val_mse: 10.6751 - val_mae: 1.3785 - lr: 6.7615e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 5s - loss: 9.8187 - mse: 9.8187 - mae: 1.3102 - val_loss: 10.0986 - val_mse: 10.0986 - val_mae: 1.3574 - lr: 6.7615e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 5s - loss: 9.6909 - mse: 9.6909 - mae: 1.2961 - val_loss: 10.0640 - val_mse: 10.0640 - val_mae: 1.3749 - lr: 6.7615e-04 - 5s/epoch - 5ms/step\n",
            "Score for fold 3: loss of 10.064056396484375\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 5s - loss: 10.2735 - mse: 10.2735 - mae: 1.3133 - val_loss: 6.8084 - val_mse: 6.8084 - val_mae: 1.2751 - lr: 6.7615e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 5s - loss: 10.0477 - mse: 10.0477 - mae: 1.2916 - val_loss: 6.4824 - val_mse: 6.4824 - val_mae: 1.2765 - lr: 6.7615e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 4s - loss: 9.6683 - mse: 9.6683 - mae: 1.2765 - val_loss: 6.4011 - val_mse: 6.4011 - val_mae: 1.3018 - lr: 6.7615e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 5s - loss: 9.5494 - mse: 9.5494 - mae: 1.2645 - val_loss: 9.5591 - val_mse: 9.5591 - val_mae: 1.3161 - lr: 6.7615e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 5s - loss: 9.3784 - mse: 9.3784 - mae: 1.2473 - val_loss: 7.2856 - val_mse: 7.2856 - val_mae: 1.3274 - lr: 6.7615e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 5s - loss: 9.0972 - mse: 9.0972 - mae: 1.2325 - val_loss: 7.1250 - val_mse: 7.1250 - val_mae: 1.3135 - lr: 6.7615e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 5s - loss: 9.1316 - mse: 9.1316 - mae: 1.2218 - val_loss: 7.3341 - val_mse: 7.3341 - val_mae: 1.3453 - lr: 6.7615e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 5s - loss: 8.7663 - mse: 8.7663 - mae: 1.2042 - val_loss: 7.6431 - val_mse: 7.6431 - val_mae: 1.3404 - lr: 6.7615e-04 - 5s/epoch - 5ms/step\n",
            "Score for fold 4: loss of 7.643052577972412\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 5s - loss: 9.1894 - mse: 9.1894 - mae: 1.2502 - val_loss: 6.1700 - val_mse: 6.1700 - val_mae: 1.1717 - lr: 6.7615e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 5s - loss: 8.8220 - mse: 8.8220 - mae: 1.2249 - val_loss: 6.3058 - val_mse: 6.3058 - val_mae: 1.2142 - lr: 6.7615e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 5s - loss: 8.5999 - mse: 8.5999 - mae: 1.2097 - val_loss: 6.8463 - val_mse: 6.8463 - val_mae: 1.2533 - lr: 6.7615e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 5s - loss: 8.4026 - mse: 8.4026 - mae: 1.1941 - val_loss: 8.2890 - val_mse: 8.2890 - val_mae: 1.2060 - lr: 6.7615e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 5s - loss: 8.2166 - mse: 8.2166 - mae: 1.1782 - val_loss: 6.6641 - val_mse: 6.6641 - val_mae: 1.2519 - lr: 6.7615e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 5s - loss: 8.2409 - mse: 8.2409 - mae: 1.1663 - val_loss: 6.7510 - val_mse: 6.7510 - val_mae: 1.2612 - lr: 6.7615e-04 - 5s/epoch - 5ms/step\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-10-12 12:31:10,857]\u001b[0m Finished trial#26 resulted in value: 10.552000000000001. Current best value is 9.818 with parameters: {'activation': 'relu', 'num_hidden_layer': 3, 'i': 9, 'learning_rate': 0.0007851045775996924}.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Score for fold 5: loss of 6.7510151863098145\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 3s - loss: 14.7602 - mse: 14.7602 - mae: 1.5601 - val_loss: 8.0945 - val_mse: 8.0945 - val_mae: 1.4703 - lr: 5.2518e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 13.7121 - mse: 13.7121 - mae: 1.5114 - val_loss: 7.9202 - val_mse: 7.9202 - val_mae: 1.4654 - lr: 5.2518e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 13.5236 - mse: 13.5236 - mae: 1.5005 - val_loss: 7.6700 - val_mse: 7.6700 - val_mae: 1.4311 - lr: 5.2518e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 13.3276 - mse: 13.3276 - mae: 1.4867 - val_loss: 7.5604 - val_mse: 7.5604 - val_mae: 1.4264 - lr: 5.2518e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 13.2671 - mse: 13.2671 - mae: 1.4797 - val_loss: 7.5399 - val_mse: 7.5399 - val_mae: 1.4447 - lr: 5.2518e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 13.1046 - mse: 13.1046 - mae: 1.4750 - val_loss: 7.5768 - val_mse: 7.5768 - val_mae: 1.4008 - lr: 5.2518e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 13.1206 - mse: 13.1206 - mae: 1.4678 - val_loss: 7.5261 - val_mse: 7.5261 - val_mae: 1.4303 - lr: 5.2518e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 2s - loss: 13.0442 - mse: 13.0442 - mae: 1.4671 - val_loss: 7.4556 - val_mse: 7.4556 - val_mae: 1.4500 - lr: 5.2518e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 2s - loss: 13.0400 - mse: 13.0400 - mae: 1.4637 - val_loss: 7.5922 - val_mse: 7.5922 - val_mae: 1.4061 - lr: 5.2518e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 2s - loss: 12.8776 - mse: 12.8776 - mae: 1.4601 - val_loss: 7.5731 - val_mse: 7.5731 - val_mae: 1.4416 - lr: 5.2518e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 2s - loss: 12.8563 - mse: 12.8563 - mae: 1.4590 - val_loss: 7.5184 - val_mse: 7.5184 - val_mae: 1.4295 - lr: 5.2518e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 2s - loss: 12.7978 - mse: 12.7978 - mae: 1.4518 - val_loss: 7.5763 - val_mse: 7.5763 - val_mae: 1.3878 - lr: 5.2518e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 13/100\n",
            "1000/1000 - 2s - loss: 12.8233 - mse: 12.8233 - mae: 1.4560 - val_loss: 7.5319 - val_mse: 7.5319 - val_mae: 1.3884 - lr: 5.2518e-04 - 2s/epoch - 2ms/step\n",
            "Score for fold 1: loss of 7.531910419464111\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 12.4179 - mse: 12.4179 - mae: 1.4463 - val_loss: 9.0969 - val_mse: 9.0969 - val_mae: 1.4003 - lr: 5.2518e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 12.4659 - mse: 12.4659 - mae: 1.4418 - val_loss: 8.9086 - val_mse: 8.9086 - val_mae: 1.4573 - lr: 5.2518e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 12.3252 - mse: 12.3252 - mae: 1.4430 - val_loss: 9.1456 - val_mse: 9.1456 - val_mae: 1.4583 - lr: 5.2518e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 12.3374 - mse: 12.3374 - mae: 1.4363 - val_loss: 8.9336 - val_mse: 8.9336 - val_mae: 1.4425 - lr: 5.2518e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 12.3407 - mse: 12.3407 - mae: 1.4400 - val_loss: 9.0122 - val_mse: 9.0122 - val_mae: 1.4218 - lr: 5.2518e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 12.3005 - mse: 12.3005 - mae: 1.4412 - val_loss: 9.0804 - val_mse: 9.0804 - val_mae: 1.4826 - lr: 5.2518e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 12.2832 - mse: 12.2832 - mae: 1.4374 - val_loss: 9.1163 - val_mse: 9.1163 - val_mae: 1.4053 - lr: 5.2518e-04 - 2s/epoch - 2ms/step\n",
            "Score for fold 2: loss of 9.116341590881348\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 10.6574 - mse: 10.6574 - mae: 1.4251 - val_loss: 15.5234 - val_mse: 15.5234 - val_mae: 1.4867 - lr: 5.2518e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 10.5369 - mse: 10.5369 - mae: 1.4195 - val_loss: 16.0079 - val_mse: 16.0079 - val_mae: 1.4419 - lr: 5.2518e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 10.5911 - mse: 10.5911 - mae: 1.4161 - val_loss: 15.9930 - val_mse: 15.9930 - val_mae: 1.4642 - lr: 5.2518e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 10.5880 - mse: 10.5880 - mae: 1.4167 - val_loss: 15.7592 - val_mse: 15.7592 - val_mae: 1.4557 - lr: 5.2518e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 10.5565 - mse: 10.5565 - mae: 1.4149 - val_loss: 15.6486 - val_mse: 15.6486 - val_mae: 1.4931 - lr: 5.2518e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 10.4807 - mse: 10.4807 - mae: 1.4170 - val_loss: 15.6406 - val_mse: 15.6406 - val_mae: 1.4740 - lr: 5.2518e-04 - 2s/epoch - 2ms/step\n",
            "Score for fold 3: loss of 15.640549659729004\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 10.8010 - mse: 10.8010 - mae: 1.4438 - val_loss: 14.3033 - val_mse: 14.3033 - val_mae: 1.4053 - lr: 5.2518e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 10.7644 - mse: 10.7644 - mae: 1.4436 - val_loss: 14.2736 - val_mse: 14.2736 - val_mae: 1.4022 - lr: 5.2518e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 10.7719 - mse: 10.7719 - mae: 1.4415 - val_loss: 14.2809 - val_mse: 14.2809 - val_mae: 1.4095 - lr: 5.2518e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 10.6594 - mse: 10.6594 - mae: 1.4391 - val_loss: 14.5188 - val_mse: 14.5188 - val_mae: 1.4314 - lr: 5.2518e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 10.7127 - mse: 10.7127 - mae: 1.4372 - val_loss: 14.3899 - val_mse: 14.3899 - val_mae: 1.3541 - lr: 5.2518e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 10.6277 - mse: 10.6277 - mae: 1.4357 - val_loss: 14.2794 - val_mse: 14.2794 - val_mae: 1.3941 - lr: 5.2518e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 10.5779 - mse: 10.5779 - mae: 1.4370 - val_loss: 14.2348 - val_mse: 14.2348 - val_mae: 1.3814 - lr: 5.2518e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 2s - loss: 10.6656 - mse: 10.6656 - mae: 1.4326 - val_loss: 14.4492 - val_mse: 14.4492 - val_mae: 1.3686 - lr: 5.2518e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 2s - loss: 10.6649 - mse: 10.6649 - mae: 1.4317 - val_loss: 14.3555 - val_mse: 14.3555 - val_mae: 1.4070 - lr: 5.2518e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 2s - loss: 10.6230 - mse: 10.6230 - mae: 1.4350 - val_loss: 14.4515 - val_mse: 14.4515 - val_mae: 1.3930 - lr: 5.2518e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 2s - loss: 10.4610 - mse: 10.4610 - mae: 1.4290 - val_loss: 14.5051 - val_mse: 14.5051 - val_mae: 1.4632 - lr: 5.2518e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 2s - loss: 10.5651 - mse: 10.5651 - mae: 1.4314 - val_loss: 14.3013 - val_mse: 14.3013 - val_mae: 1.4383 - lr: 5.2518e-04 - 2s/epoch - 2ms/step\n",
            "Score for fold 4: loss of 14.301348686218262\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 11.3477 - mse: 11.3477 - mae: 1.4268 - val_loss: 11.3241 - val_mse: 11.3241 - val_mae: 1.4182 - lr: 5.2518e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 11.2711 - mse: 11.2711 - mae: 1.4217 - val_loss: 11.4290 - val_mse: 11.4290 - val_mae: 1.4505 - lr: 5.2518e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 11.2579 - mse: 11.2579 - mae: 1.4146 - val_loss: 11.4060 - val_mse: 11.4060 - val_mae: 1.4784 - lr: 5.2518e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 11.2800 - mse: 11.2800 - mae: 1.4191 - val_loss: 11.4533 - val_mse: 11.4533 - val_mae: 1.4562 - lr: 5.2518e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 11.2195 - mse: 11.2195 - mae: 1.4172 - val_loss: 11.3295 - val_mse: 11.3295 - val_mae: 1.4139 - lr: 5.2518e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 11.1826 - mse: 11.1826 - mae: 1.4119 - val_loss: 11.6133 - val_mse: 11.6133 - val_mae: 1.4148 - lr: 5.2518e-04 - 2s/epoch - 2ms/step\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-10-12 12:32:41,886]\u001b[0m Finished trial#27 resulted in value: 11.64. Current best value is 9.818 with parameters: {'activation': 'relu', 'num_hidden_layer': 3, 'i': 9, 'learning_rate': 0.0007851045775996924}.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Score for fold 5: loss of 11.613285064697266\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 3s - loss: 14.6174 - mse: 14.6174 - mae: 1.5597 - val_loss: 9.3003 - val_mse: 9.3003 - val_mae: 1.4856 - lr: 2.5328e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 13.5328 - mse: 13.5328 - mae: 1.5034 - val_loss: 8.8977 - val_mse: 8.8977 - val_mae: 1.4478 - lr: 2.5328e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 13.1720 - mse: 13.1720 - mae: 1.4874 - val_loss: 8.6615 - val_mse: 8.6615 - val_mae: 1.4589 - lr: 2.5328e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 12.9995 - mse: 12.9995 - mae: 1.4732 - val_loss: 8.5710 - val_mse: 8.5710 - val_mae: 1.4738 - lr: 2.5328e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 12.9358 - mse: 12.9358 - mae: 1.4636 - val_loss: 8.4921 - val_mse: 8.4921 - val_mae: 1.4483 - lr: 2.5328e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 12.8406 - mse: 12.8406 - mae: 1.4592 - val_loss: 8.5296 - val_mse: 8.5296 - val_mae: 1.4757 - lr: 2.5328e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 12.8389 - mse: 12.8389 - mae: 1.4584 - val_loss: 8.4365 - val_mse: 8.4365 - val_mae: 1.4913 - lr: 2.5328e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 2s - loss: 12.6926 - mse: 12.6926 - mae: 1.4476 - val_loss: 8.3247 - val_mse: 8.3247 - val_mae: 1.4613 - lr: 2.5328e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 2s - loss: 12.6093 - mse: 12.6093 - mae: 1.4435 - val_loss: 8.2168 - val_mse: 8.2168 - val_mae: 1.4776 - lr: 2.5328e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 2s - loss: 12.6537 - mse: 12.6537 - mae: 1.4352 - val_loss: 8.4961 - val_mse: 8.4961 - val_mae: 1.5473 - lr: 2.5328e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 2s - loss: 12.5979 - mse: 12.5979 - mae: 1.4395 - val_loss: 8.2242 - val_mse: 8.2242 - val_mae: 1.4047 - lr: 2.5328e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 2s - loss: 12.4645 - mse: 12.4645 - mae: 1.4329 - val_loss: 8.4095 - val_mse: 8.4095 - val_mae: 1.4263 - lr: 2.5328e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 13/100\n",
            "1000/1000 - 2s - loss: 12.3524 - mse: 12.3524 - mae: 1.4281 - val_loss: 8.3543 - val_mse: 8.3543 - val_mae: 1.4183 - lr: 2.5328e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 14/100\n",
            "1000/1000 - 2s - loss: 12.3414 - mse: 12.3414 - mae: 1.4231 - val_loss: 8.1585 - val_mse: 8.1585 - val_mae: 1.4128 - lr: 2.5328e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 15/100\n",
            "1000/1000 - 2s - loss: 12.2320 - mse: 12.2320 - mae: 1.4204 - val_loss: 8.3042 - val_mse: 8.3042 - val_mae: 1.4242 - lr: 2.5328e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 16/100\n",
            "1000/1000 - 2s - loss: 12.1845 - mse: 12.1845 - mae: 1.4183 - val_loss: 8.1473 - val_mse: 8.1473 - val_mae: 1.4442 - lr: 2.5328e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 17/100\n",
            "1000/1000 - 2s - loss: 12.3367 - mse: 12.3367 - mae: 1.4136 - val_loss: 8.1646 - val_mse: 8.1646 - val_mae: 1.4078 - lr: 2.5328e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 18/100\n",
            "1000/1000 - 2s - loss: 12.1618 - mse: 12.1618 - mae: 1.4121 - val_loss: 8.2658 - val_mse: 8.2658 - val_mae: 1.4146 - lr: 2.5328e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 19/100\n",
            "1000/1000 - 2s - loss: 11.9847 - mse: 11.9847 - mae: 1.4066 - val_loss: 8.2266 - val_mse: 8.2266 - val_mae: 1.4395 - lr: 2.5328e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 20/100\n",
            "1000/1000 - 2s - loss: 12.0031 - mse: 12.0031 - mae: 1.4064 - val_loss: 8.2622 - val_mse: 8.2622 - val_mae: 1.4319 - lr: 2.5328e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 21/100\n",
            "1000/1000 - 2s - loss: 11.9997 - mse: 11.9997 - mae: 1.4017 - val_loss: 8.2005 - val_mse: 8.2005 - val_mae: 1.4413 - lr: 2.5328e-04 - 2s/epoch - 2ms/step\n",
            "Score for fold 1: loss of 8.200514793395996\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 12.2083 - mse: 12.2083 - mae: 1.4097 - val_loss: 7.1532 - val_mse: 7.1532 - val_mae: 1.4202 - lr: 2.5328e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 12.0715 - mse: 12.0715 - mae: 1.4063 - val_loss: 7.0977 - val_mse: 7.0977 - val_mae: 1.3974 - lr: 2.5328e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 12.0833 - mse: 12.0833 - mae: 1.3996 - val_loss: 7.1840 - val_mse: 7.1840 - val_mae: 1.4512 - lr: 2.5328e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 12.0023 - mse: 12.0023 - mae: 1.3977 - val_loss: 7.6076 - val_mse: 7.6076 - val_mae: 1.3826 - lr: 2.5328e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 11.9115 - mse: 11.9115 - mae: 1.3924 - val_loss: 7.3767 - val_mse: 7.3767 - val_mae: 1.4092 - lr: 2.5328e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 11.9282 - mse: 11.9282 - mae: 1.3866 - val_loss: 7.1464 - val_mse: 7.1464 - val_mae: 1.4595 - lr: 2.5328e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 11.7710 - mse: 11.7710 - mae: 1.3873 - val_loss: 7.5302 - val_mse: 7.5302 - val_mae: 1.3998 - lr: 2.5328e-04 - 2s/epoch - 2ms/step\n",
            "Score for fold 2: loss of 7.530195713043213\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 10.9372 - mse: 10.9372 - mae: 1.3920 - val_loss: 10.3994 - val_mse: 10.3994 - val_mae: 1.4009 - lr: 2.5328e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 10.8406 - mse: 10.8406 - mae: 1.3844 - val_loss: 10.3409 - val_mse: 10.3409 - val_mae: 1.3933 - lr: 2.5328e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 10.7674 - mse: 10.7674 - mae: 1.3785 - val_loss: 10.3302 - val_mse: 10.3302 - val_mae: 1.4077 - lr: 2.5328e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 10.7542 - mse: 10.7542 - mae: 1.3759 - val_loss: 10.6741 - val_mse: 10.6741 - val_mae: 1.4165 - lr: 2.5328e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 10.7156 - mse: 10.7156 - mae: 1.3709 - val_loss: 10.7309 - val_mse: 10.7309 - val_mae: 1.4122 - lr: 2.5328e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 10.6419 - mse: 10.6419 - mae: 1.3681 - val_loss: 10.5461 - val_mse: 10.5461 - val_mae: 1.3870 - lr: 2.5328e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 10.5421 - mse: 10.5421 - mae: 1.3623 - val_loss: 10.5526 - val_mse: 10.5526 - val_mae: 1.4281 - lr: 2.5328e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 2s - loss: 10.4669 - mse: 10.4669 - mae: 1.3579 - val_loss: 10.4953 - val_mse: 10.4953 - val_mae: 1.4420 - lr: 2.5328e-04 - 2s/epoch - 2ms/step\n",
            "Score for fold 3: loss of 10.495333671569824\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 9.2205 - mse: 9.2205 - mae: 1.3670 - val_loss: 15.3606 - val_mse: 15.3606 - val_mae: 1.3514 - lr: 2.5328e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 9.1986 - mse: 9.1986 - mae: 1.3643 - val_loss: 15.5160 - val_mse: 15.5160 - val_mae: 1.3424 - lr: 2.5328e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 9.0889 - mse: 9.0889 - mae: 1.3593 - val_loss: 15.5690 - val_mse: 15.5690 - val_mae: 1.3815 - lr: 2.5328e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 8.9646 - mse: 8.9646 - mae: 1.3568 - val_loss: 15.7998 - val_mse: 15.7998 - val_mae: 1.3241 - lr: 2.5328e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 8.9609 - mse: 8.9609 - mae: 1.3514 - val_loss: 15.7820 - val_mse: 15.7820 - val_mae: 1.3840 - lr: 2.5328e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 8.8828 - mse: 8.8828 - mae: 1.3472 - val_loss: 15.6340 - val_mse: 15.6340 - val_mae: 1.3595 - lr: 2.5328e-04 - 2s/epoch - 2ms/step\n",
            "Score for fold 4: loss of 15.633990287780762\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 9.7176 - mse: 9.7176 - mae: 1.3509 - val_loss: 12.2742 - val_mse: 12.2742 - val_mae: 1.3556 - lr: 2.5328e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 9.5117 - mse: 9.5117 - mae: 1.3423 - val_loss: 12.3182 - val_mse: 12.3182 - val_mae: 1.3558 - lr: 2.5328e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 9.5345 - mse: 9.5345 - mae: 1.3354 - val_loss: 12.5606 - val_mse: 12.5606 - val_mae: 1.3546 - lr: 2.5328e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 9.3805 - mse: 9.3805 - mae: 1.3298 - val_loss: 12.5340 - val_mse: 12.5340 - val_mae: 1.3747 - lr: 2.5328e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 9.3205 - mse: 9.3205 - mae: 1.3280 - val_loss: 12.7985 - val_mse: 12.7985 - val_mae: 1.3875 - lr: 2.5328e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 9.4445 - mse: 9.4445 - mae: 1.3262 - val_loss: 12.7577 - val_mse: 12.7577 - val_mae: 1.3661 - lr: 2.5328e-04 - 2s/epoch - 2ms/step\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-10-12 12:34:42,821]\u001b[0m Finished trial#28 resulted in value: 10.924. Current best value is 9.818 with parameters: {'activation': 'relu', 'num_hidden_layer': 3, 'i': 9, 'learning_rate': 0.0007851045775996924}.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Score for fold 5: loss of 12.757674217224121\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 6s - loss: 12.4007 - mse: 12.4007 - mae: 1.6237 - val_loss: 20.7404 - val_mse: 20.7404 - val_mae: 1.5432 - lr: 3.7945e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 5s - loss: 11.6997 - mse: 11.6997 - mae: 1.5623 - val_loss: 20.4269 - val_mse: 20.4269 - val_mae: 1.5735 - lr: 3.7945e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 5s - loss: 11.6660 - mse: 11.6660 - mae: 1.5558 - val_loss: 20.5149 - val_mse: 20.5149 - val_mae: 1.5405 - lr: 3.7945e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 5s - loss: 11.4909 - mse: 11.4909 - mae: 1.5538 - val_loss: 20.5130 - val_mse: 20.5130 - val_mae: 1.5143 - lr: 3.7945e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 5s - loss: 11.6099 - mse: 11.6099 - mae: 1.5474 - val_loss: 20.4573 - val_mse: 20.4573 - val_mae: 1.5459 - lr: 3.7945e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 5s - loss: 11.5364 - mse: 11.5364 - mae: 1.5432 - val_loss: 20.4344 - val_mse: 20.4344 - val_mae: 1.5562 - lr: 3.7945e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 5s - loss: 11.5091 - mse: 11.5091 - mae: 1.5533 - val_loss: 20.5945 - val_mse: 20.5945 - val_mae: 1.4990 - lr: 3.7945e-04 - 5s/epoch - 5ms/step\n",
            "Score for fold 1: loss of 20.594512939453125\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 5s - loss: 13.5170 - mse: 13.5170 - mae: 1.5437 - val_loss: 12.5105 - val_mse: 12.5105 - val_mae: 1.5695 - lr: 3.7945e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 5s - loss: 13.5357 - mse: 13.5357 - mae: 1.5470 - val_loss: 12.6173 - val_mse: 12.6173 - val_mae: 1.5362 - lr: 3.7945e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 5s - loss: 13.5986 - mse: 13.5986 - mae: 1.5462 - val_loss: 12.3618 - val_mse: 12.3618 - val_mae: 1.5892 - lr: 3.7945e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 5s - loss: 13.4714 - mse: 13.4714 - mae: 1.5406 - val_loss: 12.4641 - val_mse: 12.4641 - val_mae: 1.5291 - lr: 3.7945e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 5s - loss: 13.5569 - mse: 13.5569 - mae: 1.5456 - val_loss: 12.5147 - val_mse: 12.5147 - val_mae: 1.5493 - lr: 3.7945e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 5s - loss: 13.5022 - mse: 13.5022 - mae: 1.5461 - val_loss: 12.5251 - val_mse: 12.5251 - val_mae: 1.5206 - lr: 3.7945e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 5s - loss: 13.4973 - mse: 13.4973 - mae: 1.5449 - val_loss: 12.4165 - val_mse: 12.4165 - val_mae: 1.5552 - lr: 3.7945e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 5s - loss: 13.3961 - mse: 13.3961 - mae: 1.5438 - val_loss: 12.4091 - val_mse: 12.4091 - val_mae: 1.5691 - lr: 3.7945e-04 - 5s/epoch - 5ms/step\n",
            "Score for fold 2: loss of 12.409135818481445\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 5s - loss: 13.1320 - mse: 13.1320 - mae: 1.5549 - val_loss: 13.6004 - val_mse: 13.6004 - val_mae: 1.5500 - lr: 3.7945e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 5s - loss: 13.2515 - mse: 13.2515 - mae: 1.5567 - val_loss: 13.5779 - val_mse: 13.5779 - val_mae: 1.5065 - lr: 3.7945e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 5s - loss: 13.1713 - mse: 13.1713 - mae: 1.5581 - val_loss: 13.5507 - val_mse: 13.5507 - val_mae: 1.5308 - lr: 3.7945e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 5s - loss: 13.1391 - mse: 13.1391 - mae: 1.5561 - val_loss: 13.6005 - val_mse: 13.6005 - val_mae: 1.5011 - lr: 3.7945e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 5s - loss: 13.1855 - mse: 13.1855 - mae: 1.5499 - val_loss: 13.5791 - val_mse: 13.5791 - val_mae: 1.5049 - lr: 3.7945e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 5s - loss: 13.1460 - mse: 13.1460 - mae: 1.5556 - val_loss: 13.5459 - val_mse: 13.5459 - val_mae: 1.5280 - lr: 3.7945e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 5s - loss: 13.1485 - mse: 13.1485 - mae: 1.5498 - val_loss: 13.6497 - val_mse: 13.6497 - val_mae: 1.4814 - lr: 3.7945e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 5s - loss: 13.1274 - mse: 13.1274 - mae: 1.5588 - val_loss: 13.6217 - val_mse: 13.6217 - val_mae: 1.5493 - lr: 3.7945e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 5s - loss: 13.2150 - mse: 13.2150 - mae: 1.5570 - val_loss: 13.5314 - val_mse: 13.5314 - val_mae: 1.5571 - lr: 3.7945e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 5s - loss: 13.1825 - mse: 13.1825 - mae: 1.5505 - val_loss: 13.5567 - val_mse: 13.5567 - val_mae: 1.5438 - lr: 3.7945e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 5s - loss: 13.1044 - mse: 13.1044 - mae: 1.5525 - val_loss: 13.5928 - val_mse: 13.5928 - val_mae: 1.6110 - lr: 3.7945e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 5s - loss: 13.1119 - mse: 13.1119 - mae: 1.5582 - val_loss: 13.5627 - val_mse: 13.5627 - val_mae: 1.5383 - lr: 3.7945e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 13/100\n",
            "1000/1000 - 5s - loss: 13.1655 - mse: 13.1655 - mae: 1.5535 - val_loss: 13.5537 - val_mse: 13.5537 - val_mae: 1.5666 - lr: 3.7945e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 14/100\n",
            "1000/1000 - 5s - loss: 13.1773 - mse: 13.1773 - mae: 1.5549 - val_loss: 13.6021 - val_mse: 13.6021 - val_mae: 1.5284 - lr: 3.7945e-04 - 5s/epoch - 5ms/step\n",
            "Score for fold 3: loss of 13.602075576782227\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 5s - loss: 13.7301 - mse: 13.7301 - mae: 1.5420 - val_loss: 11.5389 - val_mse: 11.5389 - val_mae: 1.5557 - lr: 3.7945e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 5s - loss: 13.7507 - mse: 13.7507 - mae: 1.5432 - val_loss: 11.7029 - val_mse: 11.7029 - val_mae: 1.5204 - lr: 3.7945e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 5s - loss: 13.6590 - mse: 13.6590 - mae: 1.5403 - val_loss: 11.6287 - val_mse: 11.6287 - val_mae: 1.5433 - lr: 3.7945e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 5s - loss: 13.6939 - mse: 13.6939 - mae: 1.5463 - val_loss: 11.5505 - val_mse: 11.5505 - val_mae: 1.5762 - lr: 3.7945e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 5s - loss: 13.7209 - mse: 13.7209 - mae: 1.5466 - val_loss: 11.5012 - val_mse: 11.5012 - val_mae: 1.6215 - lr: 3.7945e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 5s - loss: 13.6562 - mse: 13.6562 - mae: 1.5518 - val_loss: 11.5333 - val_mse: 11.5333 - val_mae: 1.6229 - lr: 3.7945e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 5s - loss: 13.7400 - mse: 13.7400 - mae: 1.5397 - val_loss: 11.5045 - val_mse: 11.5045 - val_mae: 1.6289 - lr: 3.7945e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 5s - loss: 13.7682 - mse: 13.7682 - mae: 1.5469 - val_loss: 11.7789 - val_mse: 11.7789 - val_mae: 1.5105 - lr: 3.7945e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 5s - loss: 13.6424 - mse: 13.6424 - mae: 1.5416 - val_loss: 11.7105 - val_mse: 11.7105 - val_mae: 1.5182 - lr: 3.7945e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 5s - loss: 13.7243 - mse: 13.7243 - mae: 1.5470 - val_loss: 11.7830 - val_mse: 11.7830 - val_mae: 1.5087 - lr: 3.7945e-04 - 5s/epoch - 5ms/step\n",
            "Score for fold 4: loss of 11.783024787902832\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 5s - loss: 14.5155 - mse: 14.5155 - mae: 1.5771 - val_loss: 8.0545 - val_mse: 8.0545 - val_mae: 1.4633 - lr: 3.7945e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 5s - loss: 14.4786 - mse: 14.4786 - mae: 1.5695 - val_loss: 8.0318 - val_mse: 8.0318 - val_mae: 1.4894 - lr: 3.7945e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 5s - loss: 14.5538 - mse: 14.5538 - mae: 1.5767 - val_loss: 8.1372 - val_mse: 8.1372 - val_mae: 1.4429 - lr: 3.7945e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 5s - loss: 14.5845 - mse: 14.5845 - mae: 1.5700 - val_loss: 8.0913 - val_mse: 8.0913 - val_mae: 1.4561 - lr: 3.7945e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 5s - loss: 14.6130 - mse: 14.6130 - mae: 1.5719 - val_loss: 8.0597 - val_mse: 8.0597 - val_mae: 1.4736 - lr: 3.7945e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 5s - loss: 14.5667 - mse: 14.5667 - mae: 1.5704 - val_loss: 8.0549 - val_mse: 8.0549 - val_mae: 1.4433 - lr: 3.7945e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 5s - loss: 14.5222 - mse: 14.5222 - mae: 1.5730 - val_loss: 8.0259 - val_mse: 8.0259 - val_mae: 1.4656 - lr: 3.7945e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 5s - loss: 14.6420 - mse: 14.6420 - mae: 1.5753 - val_loss: 8.1215 - val_mse: 8.1215 - val_mae: 1.5070 - lr: 3.7945e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 5s - loss: 14.5232 - mse: 14.5232 - mae: 1.5749 - val_loss: 8.0843 - val_mse: 8.0843 - val_mae: 1.4754 - lr: 3.7945e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 5s - loss: 14.5139 - mse: 14.5139 - mae: 1.5666 - val_loss: 8.0564 - val_mse: 8.0564 - val_mae: 1.4419 - lr: 3.7945e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 5s - loss: 14.6262 - mse: 14.6262 - mae: 1.5757 - val_loss: 8.0798 - val_mse: 8.0798 - val_mae: 1.4104 - lr: 3.7945e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 5s - loss: 14.6710 - mse: 14.6710 - mae: 1.5694 - val_loss: 8.1200 - val_mse: 8.1200 - val_mae: 1.5233 - lr: 3.7945e-04 - 5s/epoch - 5ms/step\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-10-12 12:39:06,731]\u001b[0m Finished trial#29 resulted in value: 13.3. Current best value is 9.818 with parameters: {'activation': 'relu', 'num_hidden_layer': 3, 'i': 9, 'learning_rate': 0.0007851045775996924}.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Score for fold 5: loss of 8.12000560760498\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 4s - loss: 13.5179 - mse: 13.5179 - mae: 1.5434 - val_loss: 10.7419 - val_mse: 10.7419 - val_mae: 1.3981 - lr: 7.4742e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 4s - loss: 12.6758 - mse: 12.6758 - mae: 1.4870 - val_loss: 10.4502 - val_mse: 10.4502 - val_mae: 1.4457 - lr: 7.4742e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 4s - loss: 12.5551 - mse: 12.5551 - mae: 1.4754 - val_loss: 10.7618 - val_mse: 10.7618 - val_mae: 1.4818 - lr: 7.4742e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 4s - loss: 12.4628 - mse: 12.4628 - mae: 1.4681 - val_loss: 10.7725 - val_mse: 10.7725 - val_mae: 1.4461 - lr: 7.4742e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 4s - loss: 12.4164 - mse: 12.4164 - mae: 1.4605 - val_loss: 10.5010 - val_mse: 10.5010 - val_mae: 1.4677 - lr: 7.4742e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 4s - loss: 12.1234 - mse: 12.1234 - mae: 1.4512 - val_loss: 10.4833 - val_mse: 10.4833 - val_mae: 1.4422 - lr: 7.4742e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 4s - loss: 12.0233 - mse: 12.0233 - mae: 1.4444 - val_loss: 11.4767 - val_mse: 11.4767 - val_mae: 1.5170 - lr: 7.4742e-04 - 4s/epoch - 4ms/step\n",
            "Score for fold 1: loss of 11.476731300354004\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 4s - loss: 12.0222 - mse: 12.0222 - mae: 1.4336 - val_loss: 10.8280 - val_mse: 10.8280 - val_mae: 1.4381 - lr: 7.4742e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 4s - loss: 11.7573 - mse: 11.7573 - mae: 1.4242 - val_loss: 11.0020 - val_mse: 11.0020 - val_mae: 1.4989 - lr: 7.4742e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 4s - loss: 11.6949 - mse: 11.6949 - mae: 1.4182 - val_loss: 11.0064 - val_mse: 11.0064 - val_mae: 1.4958 - lr: 7.4742e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 4s - loss: 11.6167 - mse: 11.6167 - mae: 1.4153 - val_loss: 11.1894 - val_mse: 11.1894 - val_mae: 1.4158 - lr: 7.4742e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 4s - loss: 11.3976 - mse: 11.3976 - mae: 1.4070 - val_loss: 10.8104 - val_mse: 10.8104 - val_mae: 1.4713 - lr: 7.4742e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 4s - loss: 11.2724 - mse: 11.2724 - mae: 1.3996 - val_loss: 10.9351 - val_mse: 10.9351 - val_mae: 1.4634 - lr: 7.4742e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 4s - loss: 11.1154 - mse: 11.1154 - mae: 1.3919 - val_loss: 10.8132 - val_mse: 10.8132 - val_mae: 1.4425 - lr: 7.4742e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 4s - loss: 10.9388 - mse: 10.9388 - mae: 1.3850 - val_loss: 10.8779 - val_mse: 10.8779 - val_mae: 1.4452 - lr: 7.4742e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 4s - loss: 11.1035 - mse: 11.1035 - mae: 1.3816 - val_loss: 11.0140 - val_mse: 11.0140 - val_mae: 1.5315 - lr: 7.4742e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 4s - loss: 11.1554 - mse: 11.1554 - mae: 1.3773 - val_loss: 10.8715 - val_mse: 10.8715 - val_mae: 1.4384 - lr: 7.4742e-04 - 4s/epoch - 4ms/step\n",
            "Score for fold 2: loss of 10.871453285217285\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 4s - loss: 11.0135 - mse: 11.0135 - mae: 1.3912 - val_loss: 9.2604 - val_mse: 9.2604 - val_mae: 1.3707 - lr: 7.4742e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 4s - loss: 10.8881 - mse: 10.8881 - mae: 1.3791 - val_loss: 9.2264 - val_mse: 9.2264 - val_mae: 1.4073 - lr: 7.4742e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 4s - loss: 10.6215 - mse: 10.6215 - mae: 1.3701 - val_loss: 9.3789 - val_mse: 9.3789 - val_mae: 1.3840 - lr: 7.4742e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 4s - loss: 11.0179 - mse: 11.0179 - mae: 1.3642 - val_loss: 9.0285 - val_mse: 9.0285 - val_mae: 1.4361 - lr: 7.4742e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 4s - loss: 10.2096 - mse: 10.2096 - mae: 1.3429 - val_loss: 9.7635 - val_mse: 9.7635 - val_mae: 1.4016 - lr: 7.4742e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 4s - loss: 10.1705 - mse: 10.1705 - mae: 1.3374 - val_loss: 9.3897 - val_mse: 9.3897 - val_mae: 1.4011 - lr: 7.4742e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 4s - loss: 10.2422 - mse: 10.2422 - mae: 1.3267 - val_loss: 9.3099 - val_mse: 9.3099 - val_mae: 1.4566 - lr: 7.4742e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 4s - loss: 9.9672 - mse: 9.9672 - mae: 1.3148 - val_loss: 10.0729 - val_mse: 10.0729 - val_mae: 1.4346 - lr: 7.4742e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 4s - loss: 9.7618 - mse: 9.7618 - mae: 1.3040 - val_loss: 10.2960 - val_mse: 10.2960 - val_mae: 1.4038 - lr: 7.4742e-04 - 4s/epoch - 4ms/step\n",
            "Score for fold 3: loss of 10.296051979064941\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 4s - loss: 8.2686 - mse: 8.2686 - mae: 1.3286 - val_loss: 15.9197 - val_mse: 15.9197 - val_mae: 1.2995 - lr: 7.4742e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 4s - loss: 7.9927 - mse: 7.9927 - mae: 1.3106 - val_loss: 15.6453 - val_mse: 15.6453 - val_mae: 1.3614 - lr: 7.4742e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 4s - loss: 7.6767 - mse: 7.6767 - mae: 1.2939 - val_loss: 15.9604 - val_mse: 15.9604 - val_mae: 1.3730 - lr: 7.4742e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 4s - loss: 7.7400 - mse: 7.7400 - mae: 1.2863 - val_loss: 16.0713 - val_mse: 16.0713 - val_mae: 1.3553 - lr: 7.4742e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 4s - loss: 7.4075 - mse: 7.4075 - mae: 1.2693 - val_loss: 16.1662 - val_mse: 16.1662 - val_mae: 1.3582 - lr: 7.4742e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 4s - loss: 7.3887 - mse: 7.3887 - mae: 1.2523 - val_loss: 16.3574 - val_mse: 16.3574 - val_mae: 1.3609 - lr: 7.4742e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 4s - loss: 7.0731 - mse: 7.0731 - mae: 1.2373 - val_loss: 16.6093 - val_mse: 16.6093 - val_mae: 1.3732 - lr: 7.4742e-04 - 4s/epoch - 4ms/step\n",
            "Score for fold 4: loss of 16.609302520751953\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 4s - loss: 10.0455 - mse: 10.0455 - mae: 1.2773 - val_loss: 4.9833 - val_mse: 4.9833 - val_mae: 1.2296 - lr: 7.4742e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 4s - loss: 9.5732 - mse: 9.5732 - mae: 1.2599 - val_loss: 4.8062 - val_mse: 4.8062 - val_mae: 1.2220 - lr: 7.4742e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 4s - loss: 9.4269 - mse: 9.4269 - mae: 1.2405 - val_loss: 5.0466 - val_mse: 5.0466 - val_mae: 1.2156 - lr: 7.4742e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 4s - loss: 9.3093 - mse: 9.3093 - mae: 1.2259 - val_loss: 5.0419 - val_mse: 5.0419 - val_mae: 1.2462 - lr: 7.4742e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 4s - loss: 8.9565 - mse: 8.9565 - mae: 1.2100 - val_loss: 5.1577 - val_mse: 5.1577 - val_mae: 1.2190 - lr: 7.4742e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 4s - loss: 8.8958 - mse: 8.8958 - mae: 1.1973 - val_loss: 5.5834 - val_mse: 5.5834 - val_mae: 1.2717 - lr: 7.4742e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 4s - loss: 8.4591 - mse: 8.4591 - mae: 1.1788 - val_loss: 5.6262 - val_mse: 5.6262 - val_mae: 1.2558 - lr: 7.4742e-04 - 4s/epoch - 4ms/step\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-10-12 12:41:42,735]\u001b[0m Finished trial#30 resulted in value: 10.978000000000002. Current best value is 9.818 with parameters: {'activation': 'relu', 'num_hidden_layer': 3, 'i': 9, 'learning_rate': 0.0007851045775996924}.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Score for fold 5: loss of 5.626181602478027\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 4s - loss: 13.8001 - mse: 13.8001 - mae: 1.5353 - val_loss: 10.0918 - val_mse: 10.0918 - val_mae: 1.4186 - lr: 8.7646e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 4s - loss: 13.0007 - mse: 13.0007 - mae: 1.4886 - val_loss: 10.2808 - val_mse: 10.2808 - val_mae: 1.4485 - lr: 8.7646e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 4s - loss: 12.9673 - mse: 12.9673 - mae: 1.4761 - val_loss: 9.9943 - val_mse: 9.9943 - val_mae: 1.4670 - lr: 8.7646e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 4s - loss: 12.8625 - mse: 12.8625 - mae: 1.4621 - val_loss: 9.9631 - val_mse: 9.9631 - val_mae: 1.4438 - lr: 8.7646e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 4s - loss: 12.4569 - mse: 12.4569 - mae: 1.4584 - val_loss: 9.8469 - val_mse: 9.8469 - val_mae: 1.4625 - lr: 8.7646e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 4s - loss: 12.4472 - mse: 12.4472 - mae: 1.4490 - val_loss: 10.1896 - val_mse: 10.1896 - val_mae: 1.4524 - lr: 8.7646e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 4s - loss: 12.3883 - mse: 12.3883 - mae: 1.4444 - val_loss: 9.5349 - val_mse: 9.5349 - val_mae: 1.5038 - lr: 8.7646e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 4s - loss: 12.1708 - mse: 12.1708 - mae: 1.4391 - val_loss: 9.9015 - val_mse: 9.9015 - val_mae: 1.4737 - lr: 8.7646e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 4s - loss: 11.9586 - mse: 11.9586 - mae: 1.4307 - val_loss: 9.9103 - val_mse: 9.9103 - val_mae: 1.5136 - lr: 8.7646e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 4s - loss: 11.9577 - mse: 11.9577 - mae: 1.4264 - val_loss: 9.8718 - val_mse: 9.8718 - val_mae: 1.4421 - lr: 8.7646e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 4s - loss: 11.9316 - mse: 11.9316 - mae: 1.4227 - val_loss: 10.0757 - val_mse: 10.0757 - val_mae: 1.4907 - lr: 8.7646e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 4s - loss: 11.6054 - mse: 11.6054 - mae: 1.4145 - val_loss: 10.0544 - val_mse: 10.0544 - val_mae: 1.4527 - lr: 8.7646e-04 - 4s/epoch - 4ms/step\n",
            "Score for fold 1: loss of 10.054384231567383\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 4s - loss: 9.0814 - mse: 9.0814 - mae: 1.4113 - val_loss: 21.1120 - val_mse: 21.1120 - val_mae: 1.4937 - lr: 8.7646e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 4s - loss: 8.9281 - mse: 8.9281 - mae: 1.4038 - val_loss: 19.8124 - val_mse: 19.8124 - val_mae: 1.4641 - lr: 8.7646e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 4s - loss: 8.7907 - mse: 8.7907 - mae: 1.4002 - val_loss: 20.4646 - val_mse: 20.4646 - val_mae: 1.4826 - lr: 8.7646e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 4s - loss: 8.5654 - mse: 8.5654 - mae: 1.3873 - val_loss: 20.5762 - val_mse: 20.5762 - val_mae: 1.5301 - lr: 8.7646e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 4s - loss: 8.4741 - mse: 8.4741 - mae: 1.3786 - val_loss: 19.8866 - val_mse: 19.8866 - val_mae: 1.4360 - lr: 8.7646e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 4s - loss: 8.1877 - mse: 8.1877 - mae: 1.3678 - val_loss: 20.3181 - val_mse: 20.3181 - val_mae: 1.4845 - lr: 8.7646e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 4s - loss: 8.1778 - mse: 8.1778 - mae: 1.3632 - val_loss: 20.9572 - val_mse: 20.9572 - val_mae: 1.4728 - lr: 8.7646e-04 - 4s/epoch - 4ms/step\n",
            "Score for fold 2: loss of 20.957151412963867\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 4s - loss: 10.4872 - mse: 10.4872 - mae: 1.3775 - val_loss: 10.8381 - val_mse: 10.8381 - val_mae: 1.3786 - lr: 8.7646e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 4s - loss: 10.4761 - mse: 10.4761 - mae: 1.3675 - val_loss: 10.5356 - val_mse: 10.5356 - val_mae: 1.3584 - lr: 8.7646e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 4s - loss: 10.3847 - mse: 10.3847 - mae: 1.3612 - val_loss: 10.9075 - val_mse: 10.9075 - val_mae: 1.4070 - lr: 8.7646e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 4s - loss: 10.1393 - mse: 10.1393 - mae: 1.3470 - val_loss: 10.6125 - val_mse: 10.6125 - val_mae: 1.4054 - lr: 8.7646e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 4s - loss: 9.8989 - mse: 9.8989 - mae: 1.3319 - val_loss: 10.6909 - val_mse: 10.6909 - val_mae: 1.4145 - lr: 8.7646e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 4s - loss: 9.9386 - mse: 9.9386 - mae: 1.3265 - val_loss: 11.0365 - val_mse: 11.0365 - val_mae: 1.4269 - lr: 8.7646e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 4s - loss: 9.5978 - mse: 9.5978 - mae: 1.3143 - val_loss: 10.8099 - val_mse: 10.8099 - val_mae: 1.3912 - lr: 8.7646e-04 - 4s/epoch - 4ms/step\n",
            "Score for fold 3: loss of 10.809886932373047\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 4s - loss: 10.4697 - mse: 10.4697 - mae: 1.3463 - val_loss: 6.5545 - val_mse: 6.5545 - val_mae: 1.2405 - lr: 8.7646e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 4s - loss: 10.5576 - mse: 10.5576 - mae: 1.3323 - val_loss: 6.2682 - val_mse: 6.2682 - val_mae: 1.2999 - lr: 8.7646e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 4s - loss: 10.2185 - mse: 10.2185 - mae: 1.3167 - val_loss: 6.4891 - val_mse: 6.4891 - val_mae: 1.2776 - lr: 8.7646e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 4s - loss: 9.9166 - mse: 9.9166 - mae: 1.3022 - val_loss: 6.5446 - val_mse: 6.5446 - val_mae: 1.2921 - lr: 8.7646e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 4s - loss: 10.0097 - mse: 10.0097 - mae: 1.2889 - val_loss: 7.2164 - val_mse: 7.2164 - val_mae: 1.3123 - lr: 8.7646e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 4s - loss: 9.6606 - mse: 9.6606 - mae: 1.2751 - val_loss: 6.7820 - val_mse: 6.7820 - val_mae: 1.3086 - lr: 8.7646e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 4s - loss: 9.6029 - mse: 9.6029 - mae: 1.2604 - val_loss: 6.9651 - val_mse: 6.9651 - val_mae: 1.3201 - lr: 8.7646e-04 - 4s/epoch - 4ms/step\n",
            "Score for fold 4: loss of 6.965066432952881\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 4s - loss: 9.7624 - mse: 9.7624 - mae: 1.2811 - val_loss: 5.4482 - val_mse: 5.4482 - val_mae: 1.2266 - lr: 8.7646e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 4s - loss: 9.5037 - mse: 9.5037 - mae: 1.2652 - val_loss: 5.3817 - val_mse: 5.3817 - val_mae: 1.2248 - lr: 8.7646e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 4s - loss: 9.2606 - mse: 9.2606 - mae: 1.2486 - val_loss: 5.5294 - val_mse: 5.5294 - val_mae: 1.2807 - lr: 8.7646e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 4s - loss: 9.0547 - mse: 9.0547 - mae: 1.2345 - val_loss: 5.6705 - val_mse: 5.6705 - val_mae: 1.2359 - lr: 8.7646e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 4s - loss: 8.9707 - mse: 8.9707 - mae: 1.2200 - val_loss: 5.7984 - val_mse: 5.7984 - val_mae: 1.2802 - lr: 8.7646e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 4s - loss: 8.7733 - mse: 8.7733 - mae: 1.2044 - val_loss: 5.9178 - val_mse: 5.9178 - val_mae: 1.2753 - lr: 8.7646e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 4s - loss: 8.7053 - mse: 8.7053 - mae: 1.1917 - val_loss: 5.7830 - val_mse: 5.7830 - val_mae: 1.3275 - lr: 8.7646e-04 - 4s/epoch - 4ms/step\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-10-12 12:44:19,166]\u001b[0m Finished trial#31 resulted in value: 10.914. Current best value is 9.818 with parameters: {'activation': 'relu', 'num_hidden_layer': 3, 'i': 9, 'learning_rate': 0.0007851045775996924}.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Score for fold 5: loss of 5.782991886138916\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 4s - loss: 11.1557 - mse: 11.1557 - mae: 1.5254 - val_loss: 20.5750 - val_mse: 20.5750 - val_mae: 1.5280 - lr: 4.6630e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 4s - loss: 10.3717 - mse: 10.3717 - mae: 1.4729 - val_loss: 19.8967 - val_mse: 19.8967 - val_mae: 1.6005 - lr: 4.6630e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 4s - loss: 10.3021 - mse: 10.3021 - mae: 1.4611 - val_loss: 19.3820 - val_mse: 19.3820 - val_mae: 1.5199 - lr: 4.6630e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 4s - loss: 10.3106 - mse: 10.3106 - mae: 1.4523 - val_loss: 19.4174 - val_mse: 19.4174 - val_mae: 1.4974 - lr: 4.6630e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 4s - loss: 10.0042 - mse: 10.0042 - mae: 1.4468 - val_loss: 19.3066 - val_mse: 19.3066 - val_mae: 1.4615 - lr: 4.6630e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 4s - loss: 9.9339 - mse: 9.9339 - mae: 1.4380 - val_loss: 19.5732 - val_mse: 19.5732 - val_mae: 1.4424 - lr: 4.6630e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 4s - loss: 9.7688 - mse: 9.7688 - mae: 1.4303 - val_loss: 19.4900 - val_mse: 19.4900 - val_mae: 1.4386 - lr: 4.6630e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 4s - loss: 9.8700 - mse: 9.8700 - mae: 1.4251 - val_loss: 19.7907 - val_mse: 19.7907 - val_mae: 1.4778 - lr: 4.6630e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 4s - loss: 9.6876 - mse: 9.6876 - mae: 1.4155 - val_loss: 19.3276 - val_mse: 19.3276 - val_mae: 1.4845 - lr: 4.6630e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 4s - loss: 9.6212 - mse: 9.6212 - mae: 1.4140 - val_loss: 19.5028 - val_mse: 19.5028 - val_mae: 1.5151 - lr: 4.6630e-04 - 4s/epoch - 4ms/step\n",
            "Score for fold 1: loss of 19.502836227416992\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 4s - loss: 12.4419 - mse: 12.4419 - mae: 1.4316 - val_loss: 7.7182 - val_mse: 7.7182 - val_mae: 1.4208 - lr: 4.6630e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 4s - loss: 12.1809 - mse: 12.1809 - mae: 1.4269 - val_loss: 7.8646 - val_mse: 7.8646 - val_mae: 1.3558 - lr: 4.6630e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 4s - loss: 12.1684 - mse: 12.1684 - mae: 1.4214 - val_loss: 8.0242 - val_mse: 8.0242 - val_mae: 1.3821 - lr: 4.6630e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 4s - loss: 12.0317 - mse: 12.0317 - mae: 1.4123 - val_loss: 7.9120 - val_mse: 7.9120 - val_mae: 1.4066 - lr: 4.6630e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 4s - loss: 11.6634 - mse: 11.6634 - mae: 1.4018 - val_loss: 7.8964 - val_mse: 7.8964 - val_mae: 1.4236 - lr: 4.6630e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 4s - loss: 11.6794 - mse: 11.6794 - mae: 1.3975 - val_loss: 7.9491 - val_mse: 7.9491 - val_mae: 1.3978 - lr: 4.6630e-04 - 4s/epoch - 4ms/step\n",
            "Score for fold 2: loss of 7.9490532875061035\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 4s - loss: 10.7097 - mse: 10.7097 - mae: 1.3853 - val_loss: 11.4933 - val_mse: 11.4933 - val_mae: 1.3898 - lr: 4.6630e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 4s - loss: 10.6140 - mse: 10.6140 - mae: 1.3766 - val_loss: 11.8632 - val_mse: 11.8632 - val_mae: 1.4133 - lr: 4.6630e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 4s - loss: 10.2963 - mse: 10.2963 - mae: 1.3633 - val_loss: 11.6929 - val_mse: 11.6929 - val_mae: 1.3938 - lr: 4.6630e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 4s - loss: 10.1089 - mse: 10.1089 - mae: 1.3543 - val_loss: 12.6485 - val_mse: 12.6485 - val_mae: 1.4361 - lr: 4.6630e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 4s - loss: 9.9799 - mse: 9.9799 - mae: 1.3404 - val_loss: 11.9800 - val_mse: 11.9800 - val_mae: 1.4605 - lr: 4.6630e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 4s - loss: 10.0473 - mse: 10.0473 - mae: 1.3386 - val_loss: 12.4188 - val_mse: 12.4188 - val_mae: 1.4907 - lr: 4.6630e-04 - 4s/epoch - 4ms/step\n",
            "Score for fold 3: loss of 12.418770790100098\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 4s - loss: 10.5490 - mse: 10.5490 - mae: 1.3590 - val_loss: 8.9817 - val_mse: 8.9817 - val_mae: 1.3603 - lr: 4.6630e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 4s - loss: 10.4362 - mse: 10.4362 - mae: 1.3496 - val_loss: 9.2259 - val_mse: 9.2259 - val_mae: 1.2999 - lr: 4.6630e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 4s - loss: 10.5552 - mse: 10.5552 - mae: 1.3430 - val_loss: 9.1615 - val_mse: 9.1615 - val_mae: 1.3194 - lr: 4.6630e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 4s - loss: 10.1368 - mse: 10.1368 - mae: 1.3305 - val_loss: 9.5464 - val_mse: 9.5464 - val_mae: 1.3519 - lr: 4.6630e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 4s - loss: 9.8110 - mse: 9.8110 - mae: 1.3155 - val_loss: 9.3003 - val_mse: 9.3003 - val_mae: 1.3394 - lr: 4.6630e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 4s - loss: 9.8746 - mse: 9.8746 - mae: 1.3086 - val_loss: 9.2806 - val_mse: 9.2806 - val_mae: 1.3520 - lr: 4.6630e-04 - 4s/epoch - 4ms/step\n",
            "Score for fold 4: loss of 9.280603408813477\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 4s - loss: 10.6310 - mse: 10.6310 - mae: 1.3315 - val_loss: 4.9234 - val_mse: 4.9234 - val_mae: 1.2754 - lr: 4.6630e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 4s - loss: 10.4918 - mse: 10.4918 - mae: 1.3120 - val_loss: 5.3181 - val_mse: 5.3181 - val_mae: 1.3114 - lr: 4.6630e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 4s - loss: 10.2224 - mse: 10.2224 - mae: 1.2997 - val_loss: 5.2114 - val_mse: 5.2114 - val_mae: 1.2599 - lr: 4.6630e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 4s - loss: 10.0926 - mse: 10.0926 - mae: 1.2884 - val_loss: 5.7018 - val_mse: 5.7018 - val_mae: 1.2885 - lr: 4.6630e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 4s - loss: 10.0948 - mse: 10.0948 - mae: 1.2802 - val_loss: 5.6591 - val_mse: 5.6591 - val_mae: 1.2868 - lr: 4.6630e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 4s - loss: 9.9343 - mse: 9.9343 - mae: 1.2656 - val_loss: 5.8428 - val_mse: 5.8428 - val_mae: 1.3437 - lr: 4.6630e-04 - 4s/epoch - 4ms/step\n",
            "Score for fold 5: loss of 5.8428215980529785\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-10-12 12:46:33,902]\u001b[0m Finished trial#32 resulted in value: 10.998. Current best value is 9.818 with parameters: {'activation': 'relu', 'num_hidden_layer': 3, 'i': 9, 'learning_rate': 0.0007851045775996924}.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 3s - loss: 14.0127 - mse: 14.0127 - mae: 1.5578 - val_loss: 8.9745 - val_mse: 8.9745 - val_mae: 1.3499 - lr: 0.0013 - 3s/epoch - 3ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 13.3720 - mse: 13.3720 - mae: 1.4991 - val_loss: 8.7821 - val_mse: 8.7821 - val_mae: 1.4469 - lr: 0.0013 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 13.0427 - mse: 13.0427 - mae: 1.4851 - val_loss: 10.3254 - val_mse: 10.3254 - val_mae: 1.4365 - lr: 0.0013 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 13.1204 - mse: 13.1204 - mae: 1.4778 - val_loss: 8.6182 - val_mse: 8.6182 - val_mae: 1.4520 - lr: 0.0013 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 12.9488 - mse: 12.9488 - mae: 1.4711 - val_loss: 8.9320 - val_mse: 8.9320 - val_mae: 1.4932 - lr: 0.0013 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 12.8615 - mse: 12.8615 - mae: 1.4628 - val_loss: 9.2003 - val_mse: 9.2003 - val_mae: 1.3691 - lr: 0.0013 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 12.6314 - mse: 12.6314 - mae: 1.4555 - val_loss: 8.8775 - val_mse: 8.8775 - val_mae: 1.5052 - lr: 0.0013 - 2s/epoch - 2ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 2s - loss: 12.5990 - mse: 12.5990 - mae: 1.4502 - val_loss: 8.7431 - val_mse: 8.7431 - val_mae: 1.4485 - lr: 0.0013 - 2s/epoch - 2ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 2s - loss: 12.4601 - mse: 12.4601 - mae: 1.4494 - val_loss: 9.0605 - val_mse: 9.0605 - val_mae: 1.4555 - lr: 0.0013 - 2s/epoch - 2ms/step\n",
            "Score for fold 1: loss of 9.060542106628418\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 10.3918 - mse: 10.3918 - mae: 1.4382 - val_loss: 15.9967 - val_mse: 15.9967 - val_mae: 1.4770 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 10.3638 - mse: 10.3638 - mae: 1.4328 - val_loss: 15.9768 - val_mse: 15.9768 - val_mae: 1.4633 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 10.1582 - mse: 10.1582 - mae: 1.4238 - val_loss: 16.4332 - val_mse: 16.4332 - val_mae: 1.4171 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 10.0834 - mse: 10.0834 - mae: 1.4189 - val_loss: 16.1464 - val_mse: 16.1464 - val_mae: 1.4045 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 10.0146 - mse: 10.0146 - mae: 1.4153 - val_loss: 16.5627 - val_mse: 16.5627 - val_mae: 1.4272 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 9.7638 - mse: 9.7638 - mae: 1.4057 - val_loss: 16.5999 - val_mse: 16.5999 - val_mae: 1.4639 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 10.0208 - mse: 10.0208 - mae: 1.4037 - val_loss: 16.3731 - val_mse: 16.3731 - val_mae: 1.5089 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Score for fold 2: loss of 16.373090744018555\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 10.8814 - mse: 10.8814 - mae: 1.3998 - val_loss: 11.4085 - val_mse: 11.4085 - val_mae: 1.4470 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 10.9302 - mse: 10.9302 - mae: 1.3898 - val_loss: 11.8725 - val_mse: 11.8725 - val_mae: 1.4379 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 10.7453 - mse: 10.7453 - mae: 1.3832 - val_loss: 11.1567 - val_mse: 11.1567 - val_mae: 1.4273 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 10.4391 - mse: 10.4391 - mae: 1.3738 - val_loss: 11.8516 - val_mse: 11.8516 - val_mae: 1.4585 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 10.6556 - mse: 10.6556 - mae: 1.3687 - val_loss: 11.5897 - val_mse: 11.5897 - val_mae: 1.4310 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 10.3490 - mse: 10.3490 - mae: 1.3599 - val_loss: 11.8648 - val_mse: 11.8648 - val_mae: 1.4744 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 10.3348 - mse: 10.3348 - mae: 1.3532 - val_loss: 11.5589 - val_mse: 11.5589 - val_mae: 1.4371 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 2s - loss: 10.0421 - mse: 10.0421 - mae: 1.3478 - val_loss: 12.4488 - val_mse: 12.4488 - val_mae: 1.4626 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Score for fold 3: loss of 12.448792457580566\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 9.7793 - mse: 9.7793 - mae: 1.3654 - val_loss: 12.1880 - val_mse: 12.1880 - val_mae: 1.3658 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 9.8927 - mse: 9.8927 - mae: 1.3540 - val_loss: 12.6314 - val_mse: 12.6314 - val_mae: 1.3415 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 9.5044 - mse: 9.5044 - mae: 1.3427 - val_loss: 12.7032 - val_mse: 12.7032 - val_mae: 1.3658 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 9.3771 - mse: 9.3771 - mae: 1.3325 - val_loss: 12.4515 - val_mse: 12.4515 - val_mae: 1.3885 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 9.2024 - mse: 9.2024 - mae: 1.3280 - val_loss: 12.6201 - val_mse: 12.6201 - val_mae: 1.4090 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 9.4550 - mse: 9.4550 - mae: 1.3209 - val_loss: 12.9623 - val_mse: 12.9623 - val_mae: 1.3935 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Score for fold 4: loss of 12.962273597717285\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 10.8199 - mse: 10.8199 - mae: 1.3465 - val_loss: 5.4335 - val_mse: 5.4335 - val_mae: 1.2695 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 11.0876 - mse: 11.0876 - mae: 1.3383 - val_loss: 5.7588 - val_mse: 5.7588 - val_mae: 1.2984 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 10.6202 - mse: 10.6202 - mae: 1.3297 - val_loss: 6.1808 - val_mse: 6.1808 - val_mae: 1.2871 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 10.4002 - mse: 10.4002 - mae: 1.3191 - val_loss: 5.6846 - val_mse: 5.6846 - val_mae: 1.2774 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 10.3589 - mse: 10.3589 - mae: 1.3143 - val_loss: 6.2583 - val_mse: 6.2583 - val_mae: 1.3523 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 10.1422 - mse: 10.1422 - mae: 1.2993 - val_loss: 6.2412 - val_mse: 6.2412 - val_mae: 1.3000 - lr: 0.0010 - 2s/epoch - 2ms/step\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-10-12 12:48:06,123]\u001b[0m Finished trial#33 resulted in value: 11.416. Current best value is 9.818 with parameters: {'activation': 'relu', 'num_hidden_layer': 3, 'i': 9, 'learning_rate': 0.0007851045775996924}.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Score for fold 5: loss of 6.241211414337158\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 6s - loss: 13.7036 - mse: 13.7036 - mae: 1.5705 - val_loss: 13.4411 - val_mse: 13.4411 - val_mae: 1.4943 - lr: 0.0023 - 6s/epoch - 6ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 5s - loss: 13.1234 - mse: 13.1234 - mae: 1.5279 - val_loss: 11.7292 - val_mse: 11.7292 - val_mae: 1.4832 - lr: 0.0023 - 5s/epoch - 5ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 5s - loss: 12.7219 - mse: 12.7219 - mae: 1.5165 - val_loss: 11.4344 - val_mse: 11.4344 - val_mae: 1.4645 - lr: 0.0023 - 5s/epoch - 5ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 5s - loss: 12.8888 - mse: 12.8888 - mae: 1.5061 - val_loss: 11.8243 - val_mse: 11.8243 - val_mae: 1.5027 - lr: 0.0023 - 5s/epoch - 5ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 5s - loss: 12.6809 - mse: 12.6809 - mae: 1.5027 - val_loss: 11.5495 - val_mse: 11.5495 - val_mae: 1.5750 - lr: 0.0023 - 5s/epoch - 5ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 5s - loss: 12.5198 - mse: 12.5198 - mae: 1.4952 - val_loss: 11.6236 - val_mse: 11.6236 - val_mae: 1.6138 - lr: 0.0023 - 5s/epoch - 5ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 5s - loss: 12.5083 - mse: 12.5083 - mae: 1.5009 - val_loss: 11.2332 - val_mse: 11.2332 - val_mae: 1.5600 - lr: 0.0023 - 5s/epoch - 5ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 5s - loss: 12.5552 - mse: 12.5552 - mae: 1.5045 - val_loss: 11.2181 - val_mse: 11.2181 - val_mae: 1.5301 - lr: 0.0023 - 5s/epoch - 5ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 5s - loss: 12.2867 - mse: 12.2867 - mae: 1.4898 - val_loss: 11.6254 - val_mse: 11.6254 - val_mae: 1.5399 - lr: 0.0023 - 5s/epoch - 5ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 5s - loss: 12.0618 - mse: 12.0618 - mae: 1.4756 - val_loss: 10.9816 - val_mse: 10.9816 - val_mae: 1.6293 - lr: 0.0023 - 5s/epoch - 5ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 5s - loss: 12.2142 - mse: 12.2142 - mae: 1.4897 - val_loss: 12.2702 - val_mse: 12.2702 - val_mae: 1.4964 - lr: 0.0023 - 5s/epoch - 5ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 5s - loss: 12.4675 - mse: 12.4675 - mae: 1.5035 - val_loss: 12.9133 - val_mse: 12.9133 - val_mae: 1.6201 - lr: 0.0023 - 5s/epoch - 5ms/step\n",
            "Epoch 13/100\n",
            "1000/1000 - 5s - loss: 12.5836 - mse: 12.5836 - mae: 1.5561 - val_loss: 12.3406 - val_mse: 12.3406 - val_mae: 1.7651 - lr: 0.0023 - 5s/epoch - 5ms/step\n",
            "Epoch 14/100\n",
            "1000/1000 - 5s - loss: 12.4777 - mse: 12.4777 - mae: 1.5219 - val_loss: 11.2374 - val_mse: 11.2374 - val_mae: 1.4706 - lr: 0.0023 - 5s/epoch - 5ms/step\n",
            "Epoch 15/100\n",
            "1000/1000 - 5s - loss: 12.1750 - mse: 12.1750 - mae: 1.4973 - val_loss: 11.0059 - val_mse: 11.0059 - val_mae: 1.4577 - lr: 0.0023 - 5s/epoch - 5ms/step\n",
            "Score for fold 1: loss of 11.00593376159668\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 5s - loss: 12.2936 - mse: 12.2936 - mae: 1.4570 - val_loss: 8.8437 - val_mse: 8.8437 - val_mae: 1.3956 - lr: 0.0010 - 5s/epoch - 5ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 6s - loss: 11.6482 - mse: 11.6482 - mae: 1.4442 - val_loss: 8.7994 - val_mse: 8.7994 - val_mae: 1.3800 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 5s - loss: 11.5839 - mse: 11.5839 - mae: 1.4323 - val_loss: 9.2258 - val_mse: 9.2258 - val_mae: 1.4255 - lr: 0.0010 - 5s/epoch - 5ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 5s - loss: 11.3241 - mse: 11.3241 - mae: 1.4195 - val_loss: 9.1508 - val_mse: 9.1508 - val_mae: 1.4724 - lr: 0.0010 - 5s/epoch - 5ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 5s - loss: 11.2590 - mse: 11.2590 - mae: 1.4142 - val_loss: 9.1221 - val_mse: 9.1221 - val_mae: 1.4310 - lr: 0.0010 - 5s/epoch - 5ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 5s - loss: 11.1422 - mse: 11.1422 - mae: 1.4052 - val_loss: 9.0366 - val_mse: 9.0366 - val_mae: 1.4853 - lr: 0.0010 - 5s/epoch - 5ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 5s - loss: 10.9494 - mse: 10.9494 - mae: 1.4029 - val_loss: 8.9715 - val_mse: 8.9715 - val_mae: 1.4226 - lr: 0.0010 - 5s/epoch - 5ms/step\n",
            "Score for fold 2: loss of 8.97154712677002\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 5s - loss: 9.6931 - mse: 9.6931 - mae: 1.4086 - val_loss: 14.3157 - val_mse: 14.3157 - val_mae: 1.4304 - lr: 0.0010 - 5s/epoch - 5ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 5s - loss: 9.3469 - mse: 9.3469 - mae: 1.3922 - val_loss: 14.8809 - val_mse: 14.8809 - val_mae: 1.4404 - lr: 0.0010 - 5s/epoch - 5ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 5s - loss: 9.3972 - mse: 9.3972 - mae: 1.3888 - val_loss: 14.6570 - val_mse: 14.6570 - val_mae: 1.3500 - lr: 0.0010 - 5s/epoch - 5ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 5s - loss: 9.1540 - mse: 9.1540 - mae: 1.3801 - val_loss: 15.2508 - val_mse: 15.2508 - val_mae: 1.4198 - lr: 0.0010 - 5s/epoch - 5ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 5s - loss: 8.9029 - mse: 8.9029 - mae: 1.3725 - val_loss: 14.8931 - val_mse: 14.8931 - val_mae: 1.3916 - lr: 0.0010 - 5s/epoch - 5ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 5s - loss: 8.9458 - mse: 8.9458 - mae: 1.3661 - val_loss: 16.0202 - val_mse: 16.0202 - val_mae: 1.4032 - lr: 0.0010 - 5s/epoch - 5ms/step\n",
            "Score for fold 3: loss of 16.020244598388672\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 5s - loss: 10.7757 - mse: 10.7757 - mae: 1.3796 - val_loss: 8.6266 - val_mse: 8.6266 - val_mae: 1.3605 - lr: 0.0010 - 5s/epoch - 5ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 5s - loss: 10.4973 - mse: 10.4973 - mae: 1.3744 - val_loss: 8.5959 - val_mse: 8.5959 - val_mae: 1.3301 - lr: 0.0010 - 5s/epoch - 5ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 5s - loss: 10.3432 - mse: 10.3432 - mae: 1.3650 - val_loss: 8.5625 - val_mse: 8.5625 - val_mae: 1.3564 - lr: 0.0010 - 5s/epoch - 5ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 5s - loss: 10.1148 - mse: 10.1148 - mae: 1.3517 - val_loss: 8.7824 - val_mse: 8.7824 - val_mae: 1.3773 - lr: 0.0010 - 5s/epoch - 5ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 5s - loss: 10.1055 - mse: 10.1055 - mae: 1.3489 - val_loss: 8.9407 - val_mse: 8.9407 - val_mae: 1.4130 - lr: 0.0010 - 5s/epoch - 5ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 5s - loss: 10.0212 - mse: 10.0212 - mae: 1.3446 - val_loss: 8.7929 - val_mse: 8.7929 - val_mae: 1.3755 - lr: 0.0010 - 5s/epoch - 5ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 5s - loss: 10.0862 - mse: 10.0862 - mae: 1.3375 - val_loss: 9.3772 - val_mse: 9.3772 - val_mae: 1.3270 - lr: 0.0010 - 5s/epoch - 5ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 5s - loss: 9.9111 - mse: 9.9111 - mae: 1.3318 - val_loss: 9.1851 - val_mse: 9.1851 - val_mae: 1.4442 - lr: 0.0010 - 5s/epoch - 5ms/step\n",
            "Score for fold 4: loss of 9.18509292602539\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 5s - loss: 9.6906 - mse: 9.6906 - mae: 1.3485 - val_loss: 9.7995 - val_mse: 9.7995 - val_mae: 1.3047 - lr: 0.0010 - 5s/epoch - 5ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 5s - loss: 9.6463 - mse: 9.6463 - mae: 1.3384 - val_loss: 10.0960 - val_mse: 10.0960 - val_mae: 1.4254 - lr: 0.0010 - 5s/epoch - 5ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 5s - loss: 9.3510 - mse: 9.3510 - mae: 1.3274 - val_loss: 10.1680 - val_mse: 10.1680 - val_mae: 1.3112 - lr: 0.0010 - 5s/epoch - 5ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 5s - loss: 9.4733 - mse: 9.4733 - mae: 1.3200 - val_loss: 10.1301 - val_mse: 10.1301 - val_mae: 1.3980 - lr: 0.0010 - 5s/epoch - 5ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 5s - loss: 9.5100 - mse: 9.5100 - mae: 1.3258 - val_loss: 10.0714 - val_mse: 10.0714 - val_mae: 1.3692 - lr: 0.0010 - 5s/epoch - 5ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 5s - loss: 9.1471 - mse: 9.1471 - mae: 1.3091 - val_loss: 10.0641 - val_mse: 10.0641 - val_mae: 1.3556 - lr: 0.0010 - 5s/epoch - 5ms/step\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-10-12 12:51:58,922]\u001b[0m Finished trial#34 resulted in value: 11.05. Current best value is 9.818 with parameters: {'activation': 'relu', 'num_hidden_layer': 3, 'i': 9, 'learning_rate': 0.0007851045775996924}.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Score for fold 5: loss of 10.064139366149902\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 3s - loss: 14.6503 - mse: 14.6503 - mae: 1.5565 - val_loss: 10.0457 - val_mse: 10.0457 - val_mae: 1.4269 - lr: 3.2347e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 13.2550 - mse: 13.2550 - mae: 1.4972 - val_loss: 9.4274 - val_mse: 9.4274 - val_mae: 1.5174 - lr: 3.2347e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 12.9240 - mse: 12.9240 - mae: 1.4788 - val_loss: 9.5858 - val_mse: 9.5858 - val_mae: 1.4781 - lr: 3.2347e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 12.7853 - mse: 12.7853 - mae: 1.4706 - val_loss: 9.9077 - val_mse: 9.9077 - val_mae: 1.4673 - lr: 3.2347e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 12.5793 - mse: 12.5793 - mae: 1.4544 - val_loss: 9.7011 - val_mse: 9.7011 - val_mae: 1.4866 - lr: 3.2347e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 12.5155 - mse: 12.5155 - mae: 1.4504 - val_loss: 9.5941 - val_mse: 9.5941 - val_mae: 1.4747 - lr: 3.2347e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 12.4575 - mse: 12.4575 - mae: 1.4466 - val_loss: 9.9507 - val_mse: 9.9507 - val_mae: 1.3962 - lr: 3.2347e-04 - 2s/epoch - 2ms/step\n",
            "Score for fold 1: loss of 9.950733184814453\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 10.2294 - mse: 10.2294 - mae: 1.4401 - val_loss: 18.3587 - val_mse: 18.3587 - val_mae: 1.4243 - lr: 3.2347e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 9.9971 - mse: 9.9971 - mae: 1.4351 - val_loss: 18.5205 - val_mse: 18.5205 - val_mae: 1.4416 - lr: 3.2347e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 9.9844 - mse: 9.9844 - mae: 1.4291 - val_loss: 18.3467 - val_mse: 18.3467 - val_mae: 1.4332 - lr: 3.2347e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 9.9896 - mse: 9.9896 - mae: 1.4268 - val_loss: 18.4419 - val_mse: 18.4419 - val_mae: 1.4377 - lr: 3.2347e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 9.8995 - mse: 9.8995 - mae: 1.4203 - val_loss: 18.4286 - val_mse: 18.4286 - val_mae: 1.4593 - lr: 3.2347e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 9.8844 - mse: 9.8844 - mae: 1.4148 - val_loss: 18.4854 - val_mse: 18.4854 - val_mae: 1.4113 - lr: 3.2347e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 9.9201 - mse: 9.9201 - mae: 1.4112 - val_loss: 18.5058 - val_mse: 18.5058 - val_mae: 1.5120 - lr: 3.2347e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 2s - loss: 9.7998 - mse: 9.7998 - mae: 1.4105 - val_loss: 18.7672 - val_mse: 18.7672 - val_mae: 1.5045 - lr: 3.2347e-04 - 2s/epoch - 2ms/step\n",
            "Score for fold 2: loss of 18.767234802246094\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 12.2911 - mse: 12.2911 - mae: 1.4292 - val_loss: 8.0550 - val_mse: 8.0550 - val_mae: 1.3351 - lr: 3.2347e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 12.2927 - mse: 12.2927 - mae: 1.4244 - val_loss: 7.7543 - val_mse: 7.7543 - val_mae: 1.3740 - lr: 3.2347e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 12.1702 - mse: 12.1702 - mae: 1.4191 - val_loss: 8.1899 - val_mse: 8.1899 - val_mae: 1.4008 - lr: 3.2347e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 12.0786 - mse: 12.0786 - mae: 1.4171 - val_loss: 8.0390 - val_mse: 8.0390 - val_mae: 1.3721 - lr: 3.2347e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 11.9806 - mse: 11.9806 - mae: 1.4105 - val_loss: 8.0775 - val_mse: 8.0775 - val_mae: 1.3532 - lr: 3.2347e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 11.8654 - mse: 11.8654 - mae: 1.4036 - val_loss: 8.0842 - val_mse: 8.0842 - val_mae: 1.3867 - lr: 3.2347e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 11.7497 - mse: 11.7497 - mae: 1.3983 - val_loss: 7.9831 - val_mse: 7.9831 - val_mae: 1.3728 - lr: 3.2347e-04 - 2s/epoch - 2ms/step\n",
            "Score for fold 3: loss of 7.983075141906738\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 11.6356 - mse: 11.6356 - mae: 1.3982 - val_loss: 8.4842 - val_mse: 8.4842 - val_mae: 1.3629 - lr: 3.2347e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 11.5078 - mse: 11.5078 - mae: 1.3914 - val_loss: 8.5002 - val_mse: 8.5002 - val_mae: 1.3866 - lr: 3.2347e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 11.5054 - mse: 11.5054 - mae: 1.3888 - val_loss: 8.4994 - val_mse: 8.4994 - val_mae: 1.4022 - lr: 3.2347e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 11.4325 - mse: 11.4325 - mae: 1.3816 - val_loss: 8.8181 - val_mse: 8.8181 - val_mae: 1.3821 - lr: 3.2347e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 11.1661 - mse: 11.1661 - mae: 1.3752 - val_loss: 8.8117 - val_mse: 8.8117 - val_mae: 1.4081 - lr: 3.2347e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 11.1506 - mse: 11.1506 - mae: 1.3736 - val_loss: 8.8123 - val_mse: 8.8123 - val_mae: 1.3949 - lr: 3.2347e-04 - 2s/epoch - 2ms/step\n",
            "Score for fold 4: loss of 8.812271118164062\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 10.3358 - mse: 10.3358 - mae: 1.3726 - val_loss: 12.3974 - val_mse: 12.3974 - val_mae: 1.3620 - lr: 3.2347e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 10.1009 - mse: 10.1009 - mae: 1.3639 - val_loss: 12.2916 - val_mse: 12.2916 - val_mae: 1.4274 - lr: 3.2347e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 10.0979 - mse: 10.0979 - mae: 1.3574 - val_loss: 12.2682 - val_mse: 12.2682 - val_mae: 1.3897 - lr: 3.2347e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 9.8910 - mse: 9.8910 - mae: 1.3525 - val_loss: 12.6471 - val_mse: 12.6471 - val_mae: 1.3565 - lr: 3.2347e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 9.9083 - mse: 9.9083 - mae: 1.3468 - val_loss: 12.3209 - val_mse: 12.3209 - val_mae: 1.4054 - lr: 3.2347e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 9.7771 - mse: 9.7771 - mae: 1.3422 - val_loss: 12.3719 - val_mse: 12.3719 - val_mae: 1.3902 - lr: 3.2347e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 9.7655 - mse: 9.7655 - mae: 1.3376 - val_loss: 12.5229 - val_mse: 12.5229 - val_mae: 1.4065 - lr: 3.2347e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 2s - loss: 9.5993 - mse: 9.5993 - mae: 1.3311 - val_loss: 12.4918 - val_mse: 12.4918 - val_mae: 1.3834 - lr: 3.2347e-04 - 2s/epoch - 2ms/step\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-10-12 12:53:30,675]\u001b[0m Finished trial#35 resulted in value: 11.600000000000001. Current best value is 9.818 with parameters: {'activation': 'relu', 'num_hidden_layer': 3, 'i': 9, 'learning_rate': 0.0007851045775996924}.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Score for fold 5: loss of 12.491819381713867\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 7s - loss: 14.4153 - mse: 14.4153 - mae: 1.6180 - val_loss: 13.2722 - val_mse: 13.2722 - val_mae: 1.5106 - lr: 2.3963e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 6s - loss: 13.6662 - mse: 13.6662 - mae: 1.5270 - val_loss: 13.2396 - val_mse: 13.2396 - val_mae: 1.5615 - lr: 2.3963e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 6s - loss: 13.4584 - mse: 13.4584 - mae: 1.5016 - val_loss: 13.1281 - val_mse: 13.1281 - val_mae: 1.5904 - lr: 2.3963e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 6s - loss: 13.2771 - mse: 13.2771 - mae: 1.4938 - val_loss: 13.1210 - val_mse: 13.1210 - val_mae: 1.5402 - lr: 2.3963e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 6s - loss: 13.2193 - mse: 13.2193 - mae: 1.4864 - val_loss: 13.3135 - val_mse: 13.3135 - val_mae: 1.4729 - lr: 2.3963e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 6s - loss: 13.1504 - mse: 13.1504 - mae: 1.4785 - val_loss: 13.0309 - val_mse: 13.0309 - val_mae: 1.5276 - lr: 2.3963e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 6s - loss: 13.0424 - mse: 13.0424 - mae: 1.4754 - val_loss: 13.0253 - val_mse: 13.0253 - val_mae: 1.5526 - lr: 2.3963e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 6s - loss: 12.9570 - mse: 12.9570 - mae: 1.4750 - val_loss: 13.2789 - val_mse: 13.2789 - val_mae: 1.4573 - lr: 2.3963e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 6s - loss: 12.9471 - mse: 12.9471 - mae: 1.4675 - val_loss: 12.9787 - val_mse: 12.9787 - val_mae: 1.4790 - lr: 2.3963e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 6s - loss: 12.8845 - mse: 12.8845 - mae: 1.4638 - val_loss: 12.9287 - val_mse: 12.9287 - val_mae: 1.4565 - lr: 2.3963e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 7s - loss: 12.7922 - mse: 12.7922 - mae: 1.4591 - val_loss: 12.8334 - val_mse: 12.8334 - val_mae: 1.5155 - lr: 2.3963e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 7s - loss: 12.6780 - mse: 12.6780 - mae: 1.4597 - val_loss: 12.9764 - val_mse: 12.9764 - val_mae: 1.4944 - lr: 2.3963e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 13/100\n",
            "1000/1000 - 7s - loss: 12.6467 - mse: 12.6467 - mae: 1.4513 - val_loss: 12.7601 - val_mse: 12.7601 - val_mae: 1.5399 - lr: 2.3963e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 14/100\n",
            "1000/1000 - 6s - loss: 12.5292 - mse: 12.5292 - mae: 1.4472 - val_loss: 12.6842 - val_mse: 12.6842 - val_mae: 1.5048 - lr: 2.3963e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 15/100\n",
            "1000/1000 - 6s - loss: 12.4209 - mse: 12.4209 - mae: 1.4431 - val_loss: 12.7168 - val_mse: 12.7168 - val_mae: 1.4641 - lr: 2.3963e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 16/100\n",
            "1000/1000 - 6s - loss: 12.3889 - mse: 12.3889 - mae: 1.4464 - val_loss: 12.8867 - val_mse: 12.8867 - val_mae: 1.5399 - lr: 2.3963e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 17/100\n",
            "1000/1000 - 6s - loss: 12.3127 - mse: 12.3127 - mae: 1.4409 - val_loss: 12.6991 - val_mse: 12.6991 - val_mae: 1.5060 - lr: 2.3963e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 18/100\n",
            "1000/1000 - 6s - loss: 12.2413 - mse: 12.2413 - mae: 1.4315 - val_loss: 12.7630 - val_mse: 12.7630 - val_mae: 1.5085 - lr: 2.3963e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 19/100\n",
            "1000/1000 - 6s - loss: 12.1520 - mse: 12.1520 - mae: 1.4293 - val_loss: 12.6142 - val_mse: 12.6142 - val_mae: 1.5232 - lr: 2.3963e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 20/100\n",
            "1000/1000 - 6s - loss: 12.0624 - mse: 12.0624 - mae: 1.4263 - val_loss: 12.8568 - val_mse: 12.8568 - val_mae: 1.5834 - lr: 2.3963e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 21/100\n",
            "1000/1000 - 6s - loss: 11.9332 - mse: 11.9332 - mae: 1.4177 - val_loss: 12.6100 - val_mse: 12.6100 - val_mae: 1.5356 - lr: 2.3963e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 22/100\n",
            "1000/1000 - 6s - loss: 11.8862 - mse: 11.8862 - mae: 1.4242 - val_loss: 12.4601 - val_mse: 12.4601 - val_mae: 1.5078 - lr: 2.3963e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 23/100\n",
            "1000/1000 - 6s - loss: 11.7841 - mse: 11.7841 - mae: 1.4152 - val_loss: 12.6462 - val_mse: 12.6462 - val_mae: 1.5675 - lr: 2.3963e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 24/100\n",
            "1000/1000 - 6s - loss: 11.6659 - mse: 11.6659 - mae: 1.4134 - val_loss: 12.4061 - val_mse: 12.4061 - val_mae: 1.5403 - lr: 2.3963e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 25/100\n",
            "1000/1000 - 6s - loss: 11.6446 - mse: 11.6446 - mae: 1.4036 - val_loss: 12.4423 - val_mse: 12.4423 - val_mae: 1.5454 - lr: 2.3963e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 26/100\n",
            "1000/1000 - 6s - loss: 11.4982 - mse: 11.4982 - mae: 1.4006 - val_loss: 12.4391 - val_mse: 12.4391 - val_mae: 1.5667 - lr: 2.3963e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 27/100\n",
            "1000/1000 - 6s - loss: 11.3923 - mse: 11.3923 - mae: 1.4004 - val_loss: 12.3500 - val_mse: 12.3500 - val_mae: 1.5458 - lr: 2.3963e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 28/100\n",
            "1000/1000 - 6s - loss: 11.3025 - mse: 11.3025 - mae: 1.3922 - val_loss: 12.2996 - val_mse: 12.2996 - val_mae: 1.5406 - lr: 2.3963e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 29/100\n",
            "1000/1000 - 6s - loss: 11.1749 - mse: 11.1749 - mae: 1.3870 - val_loss: 12.4554 - val_mse: 12.4554 - val_mae: 1.5490 - lr: 2.3963e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 30/100\n",
            "1000/1000 - 6s - loss: 11.1842 - mse: 11.1842 - mae: 1.3834 - val_loss: 12.4418 - val_mse: 12.4418 - val_mae: 1.6110 - lr: 2.3963e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 31/100\n",
            "1000/1000 - 6s - loss: 11.0004 - mse: 11.0004 - mae: 1.3833 - val_loss: 12.3484 - val_mse: 12.3484 - val_mae: 1.6133 - lr: 2.3963e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 32/100\n",
            "1000/1000 - 6s - loss: 10.8839 - mse: 10.8839 - mae: 1.3819 - val_loss: 12.3358 - val_mse: 12.3358 - val_mae: 1.5660 - lr: 2.3963e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 33/100\n",
            "1000/1000 - 6s - loss: 10.7984 - mse: 10.7984 - mae: 1.3731 - val_loss: 12.4220 - val_mse: 12.4220 - val_mae: 1.5742 - lr: 2.3963e-04 - 6s/epoch - 6ms/step\n",
            "Score for fold 1: loss of 12.422038078308105\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 6s - loss: 11.6374 - mse: 11.6374 - mae: 1.4352 - val_loss: 9.0606 - val_mse: 9.0606 - val_mae: 1.3304 - lr: 2.3963e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 6s - loss: 11.4611 - mse: 11.4611 - mae: 1.4240 - val_loss: 9.1635 - val_mse: 9.1635 - val_mae: 1.3306 - lr: 2.3963e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 6s - loss: 11.2849 - mse: 11.2849 - mae: 1.4152 - val_loss: 9.1781 - val_mse: 9.1781 - val_mae: 1.3685 - lr: 2.3963e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 6s - loss: 11.2445 - mse: 11.2445 - mae: 1.4073 - val_loss: 9.3864 - val_mse: 9.3864 - val_mae: 1.3454 - lr: 2.3963e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 6s - loss: 11.0994 - mse: 11.0994 - mae: 1.3997 - val_loss: 9.5375 - val_mse: 9.5375 - val_mae: 1.3755 - lr: 2.3963e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 6s - loss: 11.0748 - mse: 11.0748 - mae: 1.3942 - val_loss: 9.3625 - val_mse: 9.3625 - val_mae: 1.3985 - lr: 2.3963e-04 - 6s/epoch - 6ms/step\n",
            "Score for fold 2: loss of 9.362457275390625\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 6s - loss: 11.3920 - mse: 11.3920 - mae: 1.3938 - val_loss: 7.5486 - val_mse: 7.5486 - val_mae: 1.3876 - lr: 2.3963e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 6s - loss: 11.2459 - mse: 11.2459 - mae: 1.3815 - val_loss: 7.6073 - val_mse: 7.6073 - val_mae: 1.3607 - lr: 2.3963e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 6s - loss: 11.1400 - mse: 11.1400 - mae: 1.3720 - val_loss: 7.8140 - val_mse: 7.8140 - val_mae: 1.4393 - lr: 2.3963e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 6s - loss: 10.9860 - mse: 10.9860 - mae: 1.3653 - val_loss: 7.8357 - val_mse: 7.8357 - val_mae: 1.3927 - lr: 2.3963e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 6s - loss: 10.8516 - mse: 10.8516 - mae: 1.3609 - val_loss: 7.9164 - val_mse: 7.9164 - val_mae: 1.4244 - lr: 2.3963e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 7s - loss: 10.7153 - mse: 10.7153 - mae: 1.3584 - val_loss: 8.0824 - val_mse: 8.0824 - val_mae: 1.4770 - lr: 2.3963e-04 - 7s/epoch - 7ms/step\n",
            "Score for fold 3: loss of 8.082411766052246\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 6s - loss: 8.4154 - mse: 8.4154 - mae: 1.3768 - val_loss: 17.1720 - val_mse: 17.1720 - val_mae: 1.3625 - lr: 2.3963e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 6s - loss: 8.2290 - mse: 8.2290 - mae: 1.3624 - val_loss: 17.4158 - val_mse: 17.4158 - val_mae: 1.3991 - lr: 2.3963e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 7s - loss: 8.1191 - mse: 8.1191 - mae: 1.3534 - val_loss: 17.3921 - val_mse: 17.3921 - val_mae: 1.4086 - lr: 2.3963e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 6s - loss: 7.9535 - mse: 7.9535 - mae: 1.3489 - val_loss: 17.6200 - val_mse: 17.6200 - val_mae: 1.4076 - lr: 2.3963e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 6s - loss: 7.8671 - mse: 7.8671 - mae: 1.3496 - val_loss: 17.4665 - val_mse: 17.4665 - val_mae: 1.4143 - lr: 2.3963e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 6s - loss: 7.7646 - mse: 7.7646 - mae: 1.3440 - val_loss: 17.7201 - val_mse: 17.7201 - val_mae: 1.3935 - lr: 2.3963e-04 - 6s/epoch - 6ms/step\n",
            "Score for fold 4: loss of 17.7200870513916\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 7s - loss: 10.4347 - mse: 10.4347 - mae: 1.3690 - val_loss: 6.8562 - val_mse: 6.8562 - val_mae: 1.3095 - lr: 2.3963e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 6s - loss: 10.2826 - mse: 10.2826 - mae: 1.3497 - val_loss: 7.0069 - val_mse: 7.0069 - val_mae: 1.3117 - lr: 2.3963e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 6s - loss: 10.1480 - mse: 10.1480 - mae: 1.3484 - val_loss: 7.1191 - val_mse: 7.1191 - val_mae: 1.3508 - lr: 2.3963e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 6s - loss: 10.0428 - mse: 10.0428 - mae: 1.3358 - val_loss: 7.4971 - val_mse: 7.4971 - val_mae: 1.3868 - lr: 2.3963e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 6s - loss: 9.9246 - mse: 9.9246 - mae: 1.3252 - val_loss: 7.3965 - val_mse: 7.3965 - val_mae: 1.3925 - lr: 2.3963e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 6s - loss: 9.8603 - mse: 9.8603 - mae: 1.3234 - val_loss: 7.5480 - val_mse: 7.5480 - val_mae: 1.3941 - lr: 2.3963e-04 - 6s/epoch - 6ms/step\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-10-12 12:59:42,420]\u001b[0m Finished trial#36 resulted in value: 11.026. Current best value is 9.818 with parameters: {'activation': 'relu', 'num_hidden_layer': 3, 'i': 9, 'learning_rate': 0.0007851045775996924}.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Score for fold 5: loss of 7.548006057739258\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 4s - loss: 14.1375 - mse: 14.1375 - mae: 1.5615 - val_loss: 10.7879 - val_mse: 10.7879 - val_mae: 1.5666 - lr: 9.5859e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 3s - loss: 13.1160 - mse: 13.1160 - mae: 1.5079 - val_loss: 9.9510 - val_mse: 9.9510 - val_mae: 1.3652 - lr: 9.5859e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 3s - loss: 12.9555 - mse: 12.9555 - mae: 1.4945 - val_loss: 9.8277 - val_mse: 9.8277 - val_mae: 1.4171 - lr: 9.5859e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 3s - loss: 12.7561 - mse: 12.7561 - mae: 1.4764 - val_loss: 9.8070 - val_mse: 9.8070 - val_mae: 1.3885 - lr: 9.5859e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 3s - loss: 12.9043 - mse: 12.9043 - mae: 1.4697 - val_loss: 9.8894 - val_mse: 9.8894 - val_mae: 1.4657 - lr: 9.5859e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 3s - loss: 12.5352 - mse: 12.5352 - mae: 1.4629 - val_loss: 9.7291 - val_mse: 9.7291 - val_mae: 1.4987 - lr: 9.5859e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 3s - loss: 12.6139 - mse: 12.6139 - mae: 1.4581 - val_loss: 9.6933 - val_mse: 9.6933 - val_mae: 1.4489 - lr: 9.5859e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 3s - loss: 12.2970 - mse: 12.2970 - mae: 1.4523 - val_loss: 10.3142 - val_mse: 10.3142 - val_mae: 1.3946 - lr: 9.5859e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 3s - loss: 12.1393 - mse: 12.1393 - mae: 1.4456 - val_loss: 9.7500 - val_mse: 9.7500 - val_mae: 1.4155 - lr: 9.5859e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 3s - loss: 12.1284 - mse: 12.1284 - mae: 1.4398 - val_loss: 10.1238 - val_mse: 10.1238 - val_mae: 1.4509 - lr: 9.5859e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 3s - loss: 11.8599 - mse: 11.8599 - mae: 1.4382 - val_loss: 9.6995 - val_mse: 9.6995 - val_mae: 1.3683 - lr: 9.5859e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 3s - loss: 11.8317 - mse: 11.8317 - mae: 1.4268 - val_loss: 9.5779 - val_mse: 9.5779 - val_mae: 1.4066 - lr: 9.5859e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 13/100\n",
            "1000/1000 - 3s - loss: 11.7024 - mse: 11.7024 - mae: 1.4168 - val_loss: 9.5690 - val_mse: 9.5690 - val_mae: 1.4067 - lr: 9.5859e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 14/100\n",
            "1000/1000 - 3s - loss: 11.2855 - mse: 11.2855 - mae: 1.4108 - val_loss: 9.6311 - val_mse: 9.6311 - val_mae: 1.4571 - lr: 9.5859e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 15/100\n",
            "1000/1000 - 3s - loss: 11.4517 - mse: 11.4517 - mae: 1.4055 - val_loss: 10.2188 - val_mse: 10.2188 - val_mae: 1.5267 - lr: 9.5859e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 16/100\n",
            "1000/1000 - 3s - loss: 10.9221 - mse: 10.9221 - mae: 1.3939 - val_loss: 9.7160 - val_mse: 9.7160 - val_mae: 1.4088 - lr: 9.5859e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 17/100\n",
            "1000/1000 - 3s - loss: 11.0119 - mse: 11.0119 - mae: 1.3910 - val_loss: 9.8311 - val_mse: 9.8311 - val_mae: 1.4054 - lr: 9.5859e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 18/100\n",
            "1000/1000 - 3s - loss: 10.7576 - mse: 10.7576 - mae: 1.3826 - val_loss: 10.2011 - val_mse: 10.2011 - val_mae: 1.4167 - lr: 9.5859e-04 - 3s/epoch - 3ms/step\n",
            "Score for fold 1: loss of 10.201092720031738\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 3s - loss: 9.2963 - mse: 9.2963 - mae: 1.3969 - val_loss: 16.2542 - val_mse: 16.2542 - val_mae: 1.3678 - lr: 9.5859e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 3s - loss: 9.1641 - mse: 9.1641 - mae: 1.3888 - val_loss: 17.2642 - val_mse: 17.2642 - val_mae: 1.4230 - lr: 9.5859e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 3s - loss: 8.7423 - mse: 8.7423 - mae: 1.3779 - val_loss: 16.5713 - val_mse: 16.5713 - val_mae: 1.3726 - lr: 9.5859e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 3s - loss: 8.8632 - mse: 8.8632 - mae: 1.3662 - val_loss: 16.6536 - val_mse: 16.6536 - val_mae: 1.4157 - lr: 9.5859e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 3s - loss: 8.6596 - mse: 8.6596 - mae: 1.3607 - val_loss: 16.4722 - val_mse: 16.4722 - val_mae: 1.4246 - lr: 9.5859e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 3s - loss: 8.6183 - mse: 8.6183 - mae: 1.3533 - val_loss: 16.5805 - val_mse: 16.5805 - val_mae: 1.3959 - lr: 9.5859e-04 - 3s/epoch - 3ms/step\n",
            "Score for fold 2: loss of 16.58050537109375\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 3s - loss: 9.5835 - mse: 9.5835 - mae: 1.3675 - val_loss: 11.6757 - val_mse: 11.6757 - val_mae: 1.3324 - lr: 9.5859e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 3s - loss: 9.3939 - mse: 9.3939 - mae: 1.3542 - val_loss: 12.0853 - val_mse: 12.0853 - val_mae: 1.3282 - lr: 9.5859e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 3s - loss: 9.1767 - mse: 9.1767 - mae: 1.3410 - val_loss: 12.1429 - val_mse: 12.1429 - val_mae: 1.4215 - lr: 9.5859e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 3s - loss: 9.0343 - mse: 9.0343 - mae: 1.3347 - val_loss: 12.3010 - val_mse: 12.3010 - val_mae: 1.3857 - lr: 9.5859e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 3s - loss: 9.0108 - mse: 9.0108 - mae: 1.3288 - val_loss: 12.1636 - val_mse: 12.1636 - val_mae: 1.4416 - lr: 9.5859e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 3s - loss: 9.0127 - mse: 9.0127 - mae: 1.3177 - val_loss: 12.6137 - val_mse: 12.6137 - val_mae: 1.3853 - lr: 9.5859e-04 - 3s/epoch - 3ms/step\n",
            "Score for fold 3: loss of 12.613670349121094\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 3s - loss: 10.3032 - mse: 10.3032 - mae: 1.3494 - val_loss: 6.8331 - val_mse: 6.8331 - val_mae: 1.3264 - lr: 9.5859e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 3s - loss: 10.0567 - mse: 10.0567 - mae: 1.3318 - val_loss: 6.6630 - val_mse: 6.6630 - val_mae: 1.3251 - lr: 9.5859e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 3s - loss: 9.7652 - mse: 9.7652 - mae: 1.3137 - val_loss: 7.0903 - val_mse: 7.0903 - val_mae: 1.3636 - lr: 9.5859e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 3s - loss: 9.7061 - mse: 9.7061 - mae: 1.3114 - val_loss: 7.1656 - val_mse: 7.1656 - val_mae: 1.2929 - lr: 9.5859e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 3s - loss: 9.5009 - mse: 9.5009 - mae: 1.2985 - val_loss: 7.4240 - val_mse: 7.4240 - val_mae: 1.3930 - lr: 9.5859e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 3s - loss: 9.4916 - mse: 9.4916 - mae: 1.2928 - val_loss: 7.5063 - val_mse: 7.5063 - val_mae: 1.3487 - lr: 9.5859e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 3s - loss: 9.4752 - mse: 9.4752 - mae: 1.2820 - val_loss: 7.5308 - val_mse: 7.5308 - val_mae: 1.3718 - lr: 9.5859e-04 - 3s/epoch - 3ms/step\n",
            "Score for fold 4: loss of 7.530799865722656\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 3s - loss: 9.8604 - mse: 9.8604 - mae: 1.3111 - val_loss: 4.8766 - val_mse: 4.8766 - val_mae: 1.2901 - lr: 9.5859e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 3s - loss: 9.7821 - mse: 9.7821 - mae: 1.2933 - val_loss: 5.1078 - val_mse: 5.1078 - val_mae: 1.2877 - lr: 9.5859e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 3s - loss: 9.5869 - mse: 9.5869 - mae: 1.2849 - val_loss: 5.5909 - val_mse: 5.5909 - val_mae: 1.3004 - lr: 9.5859e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 3s - loss: 9.4813 - mse: 9.4813 - mae: 1.2757 - val_loss: 5.2593 - val_mse: 5.2593 - val_mae: 1.3019 - lr: 9.5859e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 3s - loss: 9.5144 - mse: 9.5144 - mae: 1.2670 - val_loss: 5.0962 - val_mse: 5.0962 - val_mae: 1.3130 - lr: 9.5859e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 3s - loss: 9.2390 - mse: 9.2390 - mae: 1.2571 - val_loss: 5.4385 - val_mse: 5.4385 - val_mae: 1.2948 - lr: 9.5859e-04 - 3s/epoch - 3ms/step\n",
            "Score for fold 5: loss of 5.438472270965576\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-10-12 13:02:01,684]\u001b[0m Finished trial#37 resulted in value: 10.472. Current best value is 9.818 with parameters: {'activation': 'relu', 'num_hidden_layer': 3, 'i': 9, 'learning_rate': 0.0007851045775996924}.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 4s - loss: 14.2455 - mse: 14.2455 - mae: 1.5572 - val_loss: 8.6255 - val_mse: 8.6255 - val_mae: 1.6298 - lr: 0.0017 - 4s/epoch - 4ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 4s - loss: 13.4467 - mse: 13.4467 - mae: 1.5126 - val_loss: 8.2481 - val_mse: 8.2481 - val_mae: 1.3915 - lr: 0.0017 - 4s/epoch - 4ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 4s - loss: 13.1016 - mse: 13.1016 - mae: 1.4879 - val_loss: 8.6365 - val_mse: 8.6365 - val_mae: 1.4845 - lr: 0.0017 - 4s/epoch - 4ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 4s - loss: 13.3283 - mse: 13.3283 - mae: 1.4802 - val_loss: 8.0382 - val_mse: 8.0382 - val_mae: 1.3996 - lr: 0.0017 - 4s/epoch - 4ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 4s - loss: 12.9599 - mse: 12.9599 - mae: 1.4704 - val_loss: 8.2663 - val_mse: 8.2663 - val_mae: 1.3926 - lr: 0.0017 - 4s/epoch - 4ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 4s - loss: 13.1840 - mse: 13.1840 - mae: 1.4627 - val_loss: 7.8915 - val_mse: 7.8915 - val_mae: 1.4576 - lr: 0.0017 - 4s/epoch - 4ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 4s - loss: 12.7509 - mse: 12.7509 - mae: 1.4507 - val_loss: 7.9010 - val_mse: 7.9010 - val_mae: 1.4264 - lr: 0.0017 - 4s/epoch - 4ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 4s - loss: 12.6345 - mse: 12.6345 - mae: 1.4504 - val_loss: 8.1384 - val_mse: 8.1384 - val_mae: 1.4253 - lr: 0.0017 - 4s/epoch - 4ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 4s - loss: 12.7943 - mse: 12.7943 - mae: 1.4629 - val_loss: 8.2560 - val_mse: 8.2560 - val_mae: 1.4226 - lr: 0.0017 - 4s/epoch - 4ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 4s - loss: 12.4799 - mse: 12.4799 - mae: 1.4531 - val_loss: 8.0260 - val_mse: 8.0260 - val_mae: 1.4441 - lr: 0.0017 - 4s/epoch - 4ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 4s - loss: 12.3939 - mse: 12.3939 - mae: 1.4415 - val_loss: 8.0475 - val_mse: 8.0475 - val_mae: 1.4263 - lr: 0.0017 - 4s/epoch - 4ms/step\n",
            "Score for fold 1: loss of 8.047538757324219\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 4s - loss: 10.9325 - mse: 10.9325 - mae: 1.4143 - val_loss: 12.7108 - val_mse: 12.7108 - val_mae: 1.4479 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 4s - loss: 10.7628 - mse: 10.7628 - mae: 1.4046 - val_loss: 12.6795 - val_mse: 12.6795 - val_mae: 1.4587 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 4s - loss: 10.6270 - mse: 10.6270 - mae: 1.3953 - val_loss: 12.7056 - val_mse: 12.7056 - val_mae: 1.4512 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 4s - loss: 10.7154 - mse: 10.7154 - mae: 1.3918 - val_loss: 12.6679 - val_mse: 12.6679 - val_mae: 1.4380 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 4s - loss: 10.5631 - mse: 10.5631 - mae: 1.3873 - val_loss: 12.0909 - val_mse: 12.0909 - val_mae: 1.4596 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 4s - loss: 10.2926 - mse: 10.2926 - mae: 1.3783 - val_loss: 12.6492 - val_mse: 12.6492 - val_mae: 1.4606 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 4s - loss: 10.0720 - mse: 10.0720 - mae: 1.3660 - val_loss: 12.3305 - val_mse: 12.3305 - val_mae: 1.4613 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 4s - loss: 10.1800 - mse: 10.1800 - mae: 1.3626 - val_loss: 12.7093 - val_mse: 12.7093 - val_mae: 1.4972 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 4s - loss: 9.9975 - mse: 9.9975 - mae: 1.3537 - val_loss: 12.4402 - val_mse: 12.4402 - val_mae: 1.4321 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 4s - loss: 10.0854 - mse: 10.0854 - mae: 1.3512 - val_loss: 12.9838 - val_mse: 12.9838 - val_mae: 1.4487 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Score for fold 2: loss of 12.983769416809082\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 4s - loss: 10.7490 - mse: 10.7490 - mae: 1.3777 - val_loss: 9.1277 - val_mse: 9.1277 - val_mae: 1.3954 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 4s - loss: 10.8205 - mse: 10.8205 - mae: 1.3660 - val_loss: 9.0573 - val_mse: 9.0573 - val_mae: 1.3612 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 4s - loss: 10.4572 - mse: 10.4572 - mae: 1.3564 - val_loss: 9.2400 - val_mse: 9.2400 - val_mae: 1.3909 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 4s - loss: 10.4054 - mse: 10.4054 - mae: 1.3461 - val_loss: 9.7821 - val_mse: 9.7821 - val_mae: 1.3717 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 4s - loss: 10.0796 - mse: 10.0796 - mae: 1.3375 - val_loss: 9.4700 - val_mse: 9.4700 - val_mae: 1.3675 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 4s - loss: 10.1018 - mse: 10.1018 - mae: 1.3311 - val_loss: 9.3006 - val_mse: 9.3006 - val_mae: 1.4378 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 4s - loss: 9.7851 - mse: 9.7851 - mae: 1.3197 - val_loss: 9.5429 - val_mse: 9.5429 - val_mae: 1.3916 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Score for fold 3: loss of 9.542935371398926\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 4s - loss: 8.4996 - mse: 8.4996 - mae: 1.3422 - val_loss: 15.1613 - val_mse: 15.1613 - val_mae: 1.3173 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 4s - loss: 8.0555 - mse: 8.0555 - mae: 1.3253 - val_loss: 15.4602 - val_mse: 15.4602 - val_mae: 1.3484 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 4s - loss: 8.1118 - mse: 8.1118 - mae: 1.3178 - val_loss: 15.3371 - val_mse: 15.3371 - val_mae: 1.3601 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 4s - loss: 8.0212 - mse: 8.0212 - mae: 1.3056 - val_loss: 15.6845 - val_mse: 15.6845 - val_mae: 1.3601 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 4s - loss: 7.6733 - mse: 7.6733 - mae: 1.2942 - val_loss: 15.9368 - val_mse: 15.9368 - val_mae: 1.3252 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 4s - loss: 7.5092 - mse: 7.5092 - mae: 1.2852 - val_loss: 15.5155 - val_mse: 15.5155 - val_mae: 1.3609 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Score for fold 4: loss of 15.515531539916992\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 4s - loss: 9.9003 - mse: 9.9003 - mae: 1.3037 - val_loss: 8.0517 - val_mse: 8.0517 - val_mae: 1.2404 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 4s - loss: 9.4887 - mse: 9.4887 - mae: 1.2895 - val_loss: 7.1970 - val_mse: 7.1970 - val_mae: 1.2554 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 4s - loss: 9.2349 - mse: 9.2349 - mae: 1.2761 - val_loss: 7.5928 - val_mse: 7.5928 - val_mae: 1.2858 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 4s - loss: 9.0311 - mse: 9.0311 - mae: 1.2631 - val_loss: 8.1347 - val_mse: 8.1347 - val_mae: 1.3218 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 4s - loss: 8.9656 - mse: 8.9656 - mae: 1.2552 - val_loss: 7.3391 - val_mse: 7.3391 - val_mae: 1.2891 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 4s - loss: 8.9905 - mse: 8.9905 - mae: 1.2454 - val_loss: 7.9410 - val_mse: 7.9410 - val_mae: 1.3294 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 4s - loss: 8.6214 - mse: 8.6214 - mae: 1.2329 - val_loss: 7.6693 - val_mse: 7.6693 - val_mae: 1.3144 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Score for fold 5: loss of 7.669297218322754\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-10-12 13:04:38,747]\u001b[0m Finished trial#38 resulted in value: 10.752. Current best value is 9.818 with parameters: {'activation': 'relu', 'num_hidden_layer': 3, 'i': 9, 'learning_rate': 0.0007851045775996924}.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 3s - loss: 13.8856 - mse: 13.8856 - mae: 1.5883 - val_loss: 11.3675 - val_mse: 11.3675 - val_mae: 1.6051 - lr: 0.0035 - 3s/epoch - 3ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 13.3161 - mse: 13.3161 - mae: 1.5401 - val_loss: 11.1284 - val_mse: 11.1284 - val_mae: 1.4789 - lr: 0.0035 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 13.0445 - mse: 13.0445 - mae: 1.5258 - val_loss: 10.6062 - val_mse: 10.6062 - val_mae: 1.5067 - lr: 0.0035 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 12.9018 - mse: 12.9018 - mae: 1.5194 - val_loss: 10.5143 - val_mse: 10.5143 - val_mae: 1.4054 - lr: 0.0035 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 12.8523 - mse: 12.8523 - mae: 1.5061 - val_loss: 10.4376 - val_mse: 10.4376 - val_mae: 1.4963 - lr: 0.0035 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 12.8347 - mse: 12.8347 - mae: 1.5046 - val_loss: 10.6031 - val_mse: 10.6031 - val_mae: 1.3856 - lr: 0.0035 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 12.7804 - mse: 12.7804 - mae: 1.5006 - val_loss: 10.6701 - val_mse: 10.6701 - val_mae: 1.4233 - lr: 0.0035 - 2s/epoch - 2ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 2s - loss: 12.5688 - mse: 12.5688 - mae: 1.4943 - val_loss: 10.3685 - val_mse: 10.3685 - val_mae: 1.4384 - lr: 0.0035 - 2s/epoch - 2ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 2s - loss: 12.5549 - mse: 12.5549 - mae: 1.4930 - val_loss: 10.8886 - val_mse: 10.8886 - val_mae: 1.4626 - lr: 0.0035 - 2s/epoch - 2ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 2s - loss: 12.5729 - mse: 12.5729 - mae: 1.4949 - val_loss: 10.5262 - val_mse: 10.5262 - val_mae: 1.5275 - lr: 0.0035 - 2s/epoch - 2ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 2s - loss: 12.7774 - mse: 12.7774 - mae: 1.4876 - val_loss: 10.3250 - val_mse: 10.3250 - val_mae: 1.4569 - lr: 0.0035 - 2s/epoch - 2ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 2s - loss: 12.6075 - mse: 12.6075 - mae: 1.4881 - val_loss: 10.4045 - val_mse: 10.4045 - val_mae: 1.4062 - lr: 0.0035 - 2s/epoch - 2ms/step\n",
            "Epoch 13/100\n",
            "1000/1000 - 2s - loss: 12.6041 - mse: 12.6041 - mae: 1.4841 - val_loss: 10.4188 - val_mse: 10.4188 - val_mae: 1.4212 - lr: 0.0035 - 2s/epoch - 2ms/step\n",
            "Epoch 14/100\n",
            "1000/1000 - 2s - loss: 12.5180 - mse: 12.5180 - mae: 1.4902 - val_loss: 10.9853 - val_mse: 10.9853 - val_mae: 1.4059 - lr: 0.0035 - 2s/epoch - 2ms/step\n",
            "Epoch 15/100\n",
            "1000/1000 - 2s - loss: 12.5950 - mse: 12.5950 - mae: 1.4842 - val_loss: 10.6091 - val_mse: 10.6091 - val_mae: 1.4589 - lr: 0.0035 - 2s/epoch - 2ms/step\n",
            "Epoch 16/100\n",
            "1000/1000 - 2s - loss: 12.5796 - mse: 12.5796 - mae: 1.4857 - val_loss: 10.4186 - val_mse: 10.4186 - val_mae: 1.4491 - lr: 0.0035 - 2s/epoch - 2ms/step\n",
            "Score for fold 1: loss of 10.418621063232422\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 11.6088 - mse: 11.6088 - mae: 1.4304 - val_loss: 12.0298 - val_mse: 12.0298 - val_mae: 1.4919 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 11.2765 - mse: 11.2765 - mae: 1.4226 - val_loss: 12.0944 - val_mse: 12.0944 - val_mae: 1.5240 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 11.1517 - mse: 11.1517 - mae: 1.4201 - val_loss: 12.0923 - val_mse: 12.0923 - val_mae: 1.5007 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 11.0290 - mse: 11.0290 - mae: 1.4134 - val_loss: 12.4764 - val_mse: 12.4764 - val_mae: 1.5152 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 10.9914 - mse: 10.9914 - mae: 1.4152 - val_loss: 12.0875 - val_mse: 12.0875 - val_mae: 1.5213 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 11.0675 - mse: 11.0675 - mae: 1.4159 - val_loss: 12.1523 - val_mse: 12.1523 - val_mae: 1.4723 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Score for fold 2: loss of 12.152252197265625\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 9.5493 - mse: 9.5493 - mae: 1.4174 - val_loss: 17.9586 - val_mse: 17.9586 - val_mae: 1.4549 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 9.6087 - mse: 9.6087 - mae: 1.4186 - val_loss: 17.5282 - val_mse: 17.5282 - val_mae: 1.4602 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 9.4084 - mse: 9.4084 - mae: 1.4093 - val_loss: 17.9930 - val_mse: 17.9930 - val_mae: 1.4841 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 9.4303 - mse: 9.4303 - mae: 1.4104 - val_loss: 17.9070 - val_mse: 17.9070 - val_mae: 1.4598 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 9.5028 - mse: 9.5028 - mae: 1.4114 - val_loss: 17.8080 - val_mse: 17.8080 - val_mae: 1.4548 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 9.4012 - mse: 9.4012 - mae: 1.4027 - val_loss: 17.9156 - val_mse: 17.9156 - val_mae: 1.4668 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 9.3459 - mse: 9.3459 - mae: 1.4017 - val_loss: 17.6716 - val_mse: 17.6716 - val_mae: 1.4892 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Score for fold 3: loss of 17.671606063842773\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 11.4530 - mse: 11.4530 - mae: 1.4212 - val_loss: 8.8812 - val_mse: 8.8812 - val_mae: 1.4184 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 11.4442 - mse: 11.4442 - mae: 1.4142 - val_loss: 9.0312 - val_mse: 9.0312 - val_mae: 1.4280 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 11.4304 - mse: 11.4304 - mae: 1.4145 - val_loss: 9.1367 - val_mse: 9.1367 - val_mae: 1.4786 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 11.4693 - mse: 11.4693 - mae: 1.4133 - val_loss: 9.3267 - val_mse: 9.3267 - val_mae: 1.3794 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 11.4308 - mse: 11.4308 - mae: 1.4118 - val_loss: 9.1498 - val_mse: 9.1498 - val_mae: 1.4578 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 11.3702 - mse: 11.3702 - mae: 1.4064 - val_loss: 9.1197 - val_mse: 9.1197 - val_mae: 1.4315 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Score for fold 4: loss of 9.119706153869629\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 11.9060 - mse: 11.9060 - mae: 1.4277 - val_loss: 6.7660 - val_mse: 6.7660 - val_mae: 1.3500 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 11.9376 - mse: 11.9376 - mae: 1.4223 - val_loss: 6.6980 - val_mse: 6.6980 - val_mae: 1.3512 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 11.8322 - mse: 11.8322 - mae: 1.4187 - val_loss: 6.6944 - val_mse: 6.6944 - val_mae: 1.3681 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 11.8713 - mse: 11.8713 - mae: 1.4167 - val_loss: 6.9372 - val_mse: 6.9372 - val_mae: 1.3715 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 11.7002 - mse: 11.7002 - mae: 1.4144 - val_loss: 6.9788 - val_mse: 6.9788 - val_mae: 1.3569 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 11.8062 - mse: 11.8062 - mae: 1.4099 - val_loss: 6.9156 - val_mse: 6.9156 - val_mae: 1.3818 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 11.6896 - mse: 11.6896 - mae: 1.4135 - val_loss: 7.1424 - val_mse: 7.1424 - val_mae: 1.3734 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 2s - loss: 11.6735 - mse: 11.6735 - mae: 1.4102 - val_loss: 7.1612 - val_mse: 7.1612 - val_mae: 1.3726 - lr: 0.0010 - 2s/epoch - 2ms/step\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-10-12 13:06:20,675]\u001b[0m Finished trial#39 resulted in value: 11.303999999999998. Current best value is 9.818 with parameters: {'activation': 'relu', 'num_hidden_layer': 3, 'i': 9, 'learning_rate': 0.0007851045775996924}.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Score for fold 5: loss of 7.16123104095459\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 6s - loss: 14.2348 - mse: 14.2348 - mae: 1.5472 - val_loss: 9.2004 - val_mse: 9.2004 - val_mae: 1.5208 - lr: 1.7926e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 5s - loss: 13.3391 - mse: 13.3391 - mae: 1.4886 - val_loss: 8.8544 - val_mse: 8.8544 - val_mae: 1.4245 - lr: 1.7926e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 5s - loss: 12.8970 - mse: 12.8970 - mae: 1.4684 - val_loss: 8.6673 - val_mse: 8.6673 - val_mae: 1.5295 - lr: 1.7926e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 5s - loss: 12.9393 - mse: 12.9393 - mae: 1.4592 - val_loss: 8.5935 - val_mse: 8.5935 - val_mae: 1.5241 - lr: 1.7926e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 5s - loss: 12.9179 - mse: 12.9179 - mae: 1.4552 - val_loss: 8.5270 - val_mse: 8.5270 - val_mae: 1.4264 - lr: 1.7926e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 5s - loss: 12.6520 - mse: 12.6520 - mae: 1.4445 - val_loss: 9.0771 - val_mse: 9.0771 - val_mae: 1.4519 - lr: 1.7926e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 5s - loss: 12.6131 - mse: 12.6131 - mae: 1.4355 - val_loss: 8.5268 - val_mse: 8.5268 - val_mae: 1.4950 - lr: 1.7926e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 5s - loss: 12.4109 - mse: 12.4109 - mae: 1.4292 - val_loss: 8.5655 - val_mse: 8.5655 - val_mae: 1.5212 - lr: 1.7926e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 5s - loss: 12.2466 - mse: 12.2466 - mae: 1.4227 - val_loss: 8.5968 - val_mse: 8.5968 - val_mae: 1.4518 - lr: 1.7926e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 5s - loss: 12.1246 - mse: 12.1246 - mae: 1.4111 - val_loss: 8.4748 - val_mse: 8.4748 - val_mae: 1.4753 - lr: 1.7926e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 5s - loss: 11.9782 - mse: 11.9782 - mae: 1.4037 - val_loss: 8.9849 - val_mse: 8.9849 - val_mae: 1.3992 - lr: 1.7926e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 5s - loss: 11.6569 - mse: 11.6569 - mae: 1.3858 - val_loss: 8.5740 - val_mse: 8.5740 - val_mae: 1.5561 - lr: 1.7926e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 13/100\n",
            "1000/1000 - 5s - loss: 11.5628 - mse: 11.5628 - mae: 1.3829 - val_loss: 8.4794 - val_mse: 8.4794 - val_mae: 1.4174 - lr: 1.7926e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 14/100\n",
            "1000/1000 - 5s - loss: 11.4967 - mse: 11.4967 - mae: 1.3674 - val_loss: 8.9032 - val_mse: 8.9032 - val_mae: 1.5686 - lr: 1.7926e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 15/100\n",
            "1000/1000 - 5s - loss: 11.1849 - mse: 11.1849 - mae: 1.3507 - val_loss: 8.6424 - val_mse: 8.6424 - val_mae: 1.4313 - lr: 1.7926e-04 - 5s/epoch - 5ms/step\n",
            "Score for fold 1: loss of 8.642430305480957\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 5s - loss: 10.7730 - mse: 10.7730 - mae: 1.3790 - val_loss: 10.0647 - val_mse: 10.0647 - val_mae: 1.3392 - lr: 1.7926e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 5s - loss: 10.4967 - mse: 10.4967 - mae: 1.3671 - val_loss: 10.1164 - val_mse: 10.1164 - val_mae: 1.3279 - lr: 1.7926e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 5s - loss: 10.3807 - mse: 10.3807 - mae: 1.3527 - val_loss: 10.6169 - val_mse: 10.6169 - val_mae: 1.4246 - lr: 1.7926e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 5s - loss: 10.3866 - mse: 10.3866 - mae: 1.3388 - val_loss: 10.2853 - val_mse: 10.2853 - val_mae: 1.3328 - lr: 1.7926e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 5s - loss: 9.6048 - mse: 9.6048 - mae: 1.3185 - val_loss: 10.0960 - val_mse: 10.0960 - val_mae: 1.3907 - lr: 1.7926e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 5s - loss: 9.3537 - mse: 9.3537 - mae: 1.3023 - val_loss: 10.2943 - val_mse: 10.2943 - val_mae: 1.4194 - lr: 1.7926e-04 - 5s/epoch - 5ms/step\n",
            "Score for fold 2: loss of 10.294305801391602\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 5s - loss: 10.3943 - mse: 10.3943 - mae: 1.3275 - val_loss: 7.0025 - val_mse: 7.0025 - val_mae: 1.2833 - lr: 1.7926e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 6s - loss: 10.2971 - mse: 10.2971 - mae: 1.3094 - val_loss: 7.6259 - val_mse: 7.6259 - val_mae: 1.2875 - lr: 1.7926e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 6s - loss: 9.7730 - mse: 9.7730 - mae: 1.2859 - val_loss: 7.4403 - val_mse: 7.4403 - val_mae: 1.2864 - lr: 1.7926e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 6s - loss: 9.5521 - mse: 9.5521 - mae: 1.2713 - val_loss: 6.9806 - val_mse: 6.9806 - val_mae: 1.3771 - lr: 1.7926e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 6s - loss: 8.8814 - mse: 8.8814 - mae: 1.2487 - val_loss: 6.9927 - val_mse: 6.9927 - val_mae: 1.3149 - lr: 1.7926e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 6s - loss: 8.9506 - mse: 8.9506 - mae: 1.2381 - val_loss: 7.3873 - val_mse: 7.3873 - val_mae: 1.3444 - lr: 1.7926e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 6s - loss: 8.8102 - mse: 8.8102 - mae: 1.2220 - val_loss: 10.3344 - val_mse: 10.3344 - val_mae: 1.3779 - lr: 1.7926e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 5s - loss: 8.2670 - mse: 8.2670 - mae: 1.2057 - val_loss: 10.6861 - val_mse: 10.6861 - val_mae: 1.3371 - lr: 1.7926e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 5s - loss: 8.3062 - mse: 8.3062 - mae: 1.1955 - val_loss: 7.1048 - val_mse: 7.1048 - val_mae: 1.4143 - lr: 1.7926e-04 - 5s/epoch - 5ms/step\n",
            "Score for fold 3: loss of 7.104840278625488\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 5s - loss: 6.4639 - mse: 6.4639 - mae: 1.2364 - val_loss: 13.6679 - val_mse: 13.6679 - val_mae: 1.1510 - lr: 1.7926e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 5s - loss: 6.2232 - mse: 6.2232 - mae: 1.2085 - val_loss: 14.0034 - val_mse: 14.0034 - val_mae: 1.2201 - lr: 1.7926e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 5s - loss: 6.0190 - mse: 6.0190 - mae: 1.1893 - val_loss: 13.5736 - val_mse: 13.5736 - val_mae: 1.2214 - lr: 1.7926e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 5s - loss: 5.6194 - mse: 5.6194 - mae: 1.1739 - val_loss: 14.5172 - val_mse: 14.5172 - val_mae: 1.2335 - lr: 1.7926e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 5s - loss: 5.6288 - mse: 5.6288 - mae: 1.1651 - val_loss: 14.3659 - val_mse: 14.3659 - val_mae: 1.2094 - lr: 1.7926e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 5s - loss: 5.2060 - mse: 5.2060 - mae: 1.1446 - val_loss: 14.1616 - val_mse: 14.1616 - val_mae: 1.2215 - lr: 1.7926e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 5s - loss: 5.0766 - mse: 5.0766 - mae: 1.1312 - val_loss: 14.3863 - val_mse: 14.3863 - val_mae: 1.2515 - lr: 1.7926e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 5s - loss: 4.8768 - mse: 4.8768 - mae: 1.1215 - val_loss: 14.6109 - val_mse: 14.6109 - val_mae: 1.2557 - lr: 1.7926e-04 - 5s/epoch - 5ms/step\n",
            "Score for fold 4: loss of 14.61094856262207\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 5s - loss: 7.2125 - mse: 7.2125 - mae: 1.1650 - val_loss: 5.9710 - val_mse: 5.9710 - val_mae: 1.1785 - lr: 1.7926e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 5s - loss: 6.8239 - mse: 6.8239 - mae: 1.1443 - val_loss: 5.2611 - val_mse: 5.2611 - val_mae: 1.0718 - lr: 1.7926e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 5s - loss: 6.9694 - mse: 6.9694 - mae: 1.1296 - val_loss: 6.2902 - val_mse: 6.2902 - val_mae: 1.1889 - lr: 1.7926e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 5s - loss: 6.3155 - mse: 6.3155 - mae: 1.1133 - val_loss: 5.6944 - val_mse: 5.6944 - val_mae: 1.1111 - lr: 1.7926e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 5s - loss: 6.2644 - mse: 6.2644 - mae: 1.1031 - val_loss: 6.7689 - val_mse: 6.7689 - val_mae: 1.1423 - lr: 1.7926e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 5s - loss: 5.9505 - mse: 5.9505 - mae: 1.0858 - val_loss: 6.2752 - val_mse: 6.2752 - val_mae: 1.1592 - lr: 1.7926e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 5s - loss: 6.0106 - mse: 6.0106 - mae: 1.0743 - val_loss: 6.5587 - val_mse: 6.5587 - val_mae: 1.1465 - lr: 1.7926e-04 - 5s/epoch - 5ms/step\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-10-12 13:10:30,167]\u001b[0m Finished trial#40 resulted in value: 9.440000000000001. Current best value is 9.440000000000001 with parameters: {'activation': 'relu', 'num_hidden_layer': 4, 'i': 9, 'learning_rate': 0.0001792631082155351}.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Score for fold 5: loss of 6.558738708496094\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 6s - loss: 14.1092 - mse: 14.1092 - mae: 1.5605 - val_loss: 9.8146 - val_mse: 9.8146 - val_mae: 1.4785 - lr: 1.7924e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 5s - loss: 13.0028 - mse: 13.0028 - mae: 1.5025 - val_loss: 9.6234 - val_mse: 9.6234 - val_mae: 1.4515 - lr: 1.7924e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 5s - loss: 12.8642 - mse: 12.8642 - mae: 1.4803 - val_loss: 9.6696 - val_mse: 9.6696 - val_mae: 1.4390 - lr: 1.7924e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 5s - loss: 12.6600 - mse: 12.6600 - mae: 1.4686 - val_loss: 9.5895 - val_mse: 9.5895 - val_mae: 1.4542 - lr: 1.7924e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 5s - loss: 12.4922 - mse: 12.4922 - mae: 1.4608 - val_loss: 9.9343 - val_mse: 9.9343 - val_mae: 1.4695 - lr: 1.7924e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 5s - loss: 12.4055 - mse: 12.4055 - mae: 1.4558 - val_loss: 9.5443 - val_mse: 9.5443 - val_mae: 1.4977 - lr: 1.7924e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 5s - loss: 12.2156 - mse: 12.2156 - mae: 1.4454 - val_loss: 9.8402 - val_mse: 9.8402 - val_mae: 1.4531 - lr: 1.7924e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 6s - loss: 12.1739 - mse: 12.1739 - mae: 1.4379 - val_loss: 9.7911 - val_mse: 9.7911 - val_mae: 1.4941 - lr: 1.7924e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 6s - loss: 11.8862 - mse: 11.8862 - mae: 1.4299 - val_loss: 9.6338 - val_mse: 9.6338 - val_mae: 1.4510 - lr: 1.7924e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 6s - loss: 11.8224 - mse: 11.8224 - mae: 1.4178 - val_loss: 9.7837 - val_mse: 9.7837 - val_mae: 1.4371 - lr: 1.7924e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 6s - loss: 11.6772 - mse: 11.6772 - mae: 1.4040 - val_loss: 9.8611 - val_mse: 9.8611 - val_mae: 1.4594 - lr: 1.7924e-04 - 6s/epoch - 6ms/step\n",
            "Score for fold 1: loss of 9.861146926879883\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 5s - loss: 10.8308 - mse: 10.8308 - mae: 1.4039 - val_loss: 12.2799 - val_mse: 12.2799 - val_mae: 1.4576 - lr: 1.7924e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 5s - loss: 10.4848 - mse: 10.4848 - mae: 1.3941 - val_loss: 12.7005 - val_mse: 12.7005 - val_mae: 1.4561 - lr: 1.7924e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 5s - loss: 10.2638 - mse: 10.2638 - mae: 1.3800 - val_loss: 12.5897 - val_mse: 12.5897 - val_mae: 1.4153 - lr: 1.7924e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 5s - loss: 10.3380 - mse: 10.3380 - mae: 1.3685 - val_loss: 13.5871 - val_mse: 13.5871 - val_mae: 1.4198 - lr: 1.7924e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 5s - loss: 9.7995 - mse: 9.7995 - mae: 1.3510 - val_loss: 12.6623 - val_mse: 12.6623 - val_mae: 1.4924 - lr: 1.7924e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 5s - loss: 9.6309 - mse: 9.6309 - mae: 1.3370 - val_loss: 13.3346 - val_mse: 13.3346 - val_mae: 1.4900 - lr: 1.7924e-04 - 5s/epoch - 5ms/step\n",
            "Score for fold 2: loss of 13.334648132324219\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 5s - loss: 11.0812 - mse: 11.0812 - mae: 1.3651 - val_loss: 6.4965 - val_mse: 6.4965 - val_mae: 1.3548 - lr: 1.7924e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 5s - loss: 11.1237 - mse: 11.1237 - mae: 1.3515 - val_loss: 6.0374 - val_mse: 6.0374 - val_mae: 1.3367 - lr: 1.7924e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 5s - loss: 10.9280 - mse: 10.9280 - mae: 1.3342 - val_loss: 7.1019 - val_mse: 7.1019 - val_mae: 1.3578 - lr: 1.7924e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 5s - loss: 10.2812 - mse: 10.2812 - mae: 1.3185 - val_loss: 6.2385 - val_mse: 6.2385 - val_mae: 1.2807 - lr: 1.7924e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 5s - loss: 10.0123 - mse: 10.0123 - mae: 1.3038 - val_loss: 6.4975 - val_mse: 6.4975 - val_mae: 1.3883 - lr: 1.7924e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 5s - loss: 9.8283 - mse: 9.8283 - mae: 1.2889 - val_loss: 6.3854 - val_mse: 6.3854 - val_mae: 1.3618 - lr: 1.7924e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 5s - loss: 9.8731 - mse: 9.8731 - mae: 1.2755 - val_loss: 6.4745 - val_mse: 6.4745 - val_mae: 1.4022 - lr: 1.7924e-04 - 5s/epoch - 5ms/step\n",
            "Score for fold 3: loss of 6.474523544311523\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 5s - loss: 9.6776 - mse: 9.6776 - mae: 1.2970 - val_loss: 7.0225 - val_mse: 7.0225 - val_mae: 1.2492 - lr: 1.7924e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 5s - loss: 9.5650 - mse: 9.5650 - mae: 1.2806 - val_loss: 6.7925 - val_mse: 6.7925 - val_mae: 1.2238 - lr: 1.7924e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 5s - loss: 9.1020 - mse: 9.1020 - mae: 1.2621 - val_loss: 7.2486 - val_mse: 7.2486 - val_mae: 1.3083 - lr: 1.7924e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 5s - loss: 8.9946 - mse: 8.9946 - mae: 1.2432 - val_loss: 6.9308 - val_mse: 6.9308 - val_mae: 1.2658 - lr: 1.7924e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 5s - loss: 8.6608 - mse: 8.6608 - mae: 1.2259 - val_loss: 8.3709 - val_mse: 8.3709 - val_mae: 1.2933 - lr: 1.7924e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 5s - loss: 8.3197 - mse: 8.3197 - mae: 1.2056 - val_loss: 7.1494 - val_mse: 7.1494 - val_mae: 1.2993 - lr: 1.7924e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 5s - loss: 8.2119 - mse: 8.2119 - mae: 1.1973 - val_loss: 7.0129 - val_mse: 7.0129 - val_mae: 1.3604 - lr: 1.7924e-04 - 5s/epoch - 5ms/step\n",
            "Score for fold 4: loss of 7.012936592102051\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 5s - loss: 6.7754 - mse: 6.7754 - mae: 1.2288 - val_loss: 11.6903 - val_mse: 11.6903 - val_mae: 1.1704 - lr: 1.7924e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 5s - loss: 6.6460 - mse: 6.6460 - mae: 1.2037 - val_loss: 11.7397 - val_mse: 11.7397 - val_mae: 1.2267 - lr: 1.7924e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 5s - loss: 6.3407 - mse: 6.3407 - mae: 1.1887 - val_loss: 12.5398 - val_mse: 12.5398 - val_mae: 1.2775 - lr: 1.7924e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 5s - loss: 6.1765 - mse: 6.1765 - mae: 1.1725 - val_loss: 12.5746 - val_mse: 12.5746 - val_mae: 1.2166 - lr: 1.7924e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 5s - loss: 6.1909 - mse: 6.1909 - mae: 1.1568 - val_loss: 12.6877 - val_mse: 12.6877 - val_mae: 1.2292 - lr: 1.7924e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 5s - loss: 5.9073 - mse: 5.9073 - mae: 1.1386 - val_loss: 12.9553 - val_mse: 12.9553 - val_mae: 1.2325 - lr: 1.7924e-04 - 5s/epoch - 5ms/step\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-10-12 13:13:56,808]\u001b[0m Finished trial#41 resulted in value: 9.925999999999998. Current best value is 9.440000000000001 with parameters: {'activation': 'relu', 'num_hidden_layer': 4, 'i': 9, 'learning_rate': 0.0001792631082155351}.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Score for fold 5: loss of 12.955337524414062\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 6s - loss: 13.7291 - mse: 13.7291 - mae: 1.5492 - val_loss: 10.7628 - val_mse: 10.7628 - val_mae: 1.4762 - lr: 1.7636e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 5s - loss: 12.8509 - mse: 12.8509 - mae: 1.4946 - val_loss: 10.4014 - val_mse: 10.4014 - val_mae: 1.4762 - lr: 1.7636e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 5s - loss: 12.5415 - mse: 12.5415 - mae: 1.4776 - val_loss: 10.3560 - val_mse: 10.3560 - val_mae: 1.4755 - lr: 1.7636e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 5s - loss: 12.3637 - mse: 12.3637 - mae: 1.4634 - val_loss: 10.3618 - val_mse: 10.3618 - val_mae: 1.5146 - lr: 1.7636e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 5s - loss: 12.3521 - mse: 12.3521 - mae: 1.4579 - val_loss: 10.4454 - val_mse: 10.4454 - val_mae: 1.4310 - lr: 1.7636e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 5s - loss: 12.3547 - mse: 12.3547 - mae: 1.4487 - val_loss: 10.1620 - val_mse: 10.1620 - val_mae: 1.4808 - lr: 1.7636e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 5s - loss: 12.1329 - mse: 12.1329 - mae: 1.4419 - val_loss: 10.0530 - val_mse: 10.0530 - val_mae: 1.4409 - lr: 1.7636e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 5s - loss: 12.0457 - mse: 12.0457 - mae: 1.4307 - val_loss: 10.0761 - val_mse: 10.0761 - val_mae: 1.5056 - lr: 1.7636e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 5s - loss: 11.8503 - mse: 11.8503 - mae: 1.4210 - val_loss: 9.9096 - val_mse: 9.9096 - val_mae: 1.4374 - lr: 1.7636e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 5s - loss: 11.7276 - mse: 11.7276 - mae: 1.4145 - val_loss: 9.8919 - val_mse: 9.8919 - val_mae: 1.4957 - lr: 1.7636e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 5s - loss: 11.2855 - mse: 11.2855 - mae: 1.4030 - val_loss: 10.0166 - val_mse: 10.0166 - val_mae: 1.4651 - lr: 1.7636e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 5s - loss: 11.2803 - mse: 11.2803 - mae: 1.3923 - val_loss: 9.9351 - val_mse: 9.9351 - val_mae: 1.4505 - lr: 1.7636e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 13/100\n",
            "1000/1000 - 5s - loss: 11.0061 - mse: 11.0061 - mae: 1.3741 - val_loss: 9.8918 - val_mse: 9.8918 - val_mae: 1.4209 - lr: 1.7636e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 14/100\n",
            "1000/1000 - 5s - loss: 10.7355 - mse: 10.7355 - mae: 1.3628 - val_loss: 9.9985 - val_mse: 9.9985 - val_mae: 1.4753 - lr: 1.7636e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 15/100\n",
            "1000/1000 - 5s - loss: 10.7506 - mse: 10.7506 - mae: 1.3499 - val_loss: 10.1936 - val_mse: 10.1936 - val_mae: 1.4348 - lr: 1.7636e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 16/100\n",
            "1000/1000 - 6s - loss: 10.4551 - mse: 10.4551 - mae: 1.3386 - val_loss: 10.2406 - val_mse: 10.2406 - val_mae: 1.4472 - lr: 1.7636e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 17/100\n",
            "1000/1000 - 6s - loss: 10.2664 - mse: 10.2664 - mae: 1.3268 - val_loss: 9.9997 - val_mse: 9.9997 - val_mae: 1.4626 - lr: 1.7636e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 18/100\n",
            "1000/1000 - 6s - loss: 9.9197 - mse: 9.9197 - mae: 1.3098 - val_loss: 10.2287 - val_mse: 10.2287 - val_mae: 1.4691 - lr: 1.7636e-04 - 6s/epoch - 6ms/step\n",
            "Score for fold 1: loss of 10.228653907775879\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 5s - loss: 10.7849 - mse: 10.7849 - mae: 1.3544 - val_loss: 7.0284 - val_mse: 7.0284 - val_mae: 1.3364 - lr: 1.7636e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 5s - loss: 10.4404 - mse: 10.4404 - mae: 1.3378 - val_loss: 7.2826 - val_mse: 7.2826 - val_mae: 1.3121 - lr: 1.7636e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 5s - loss: 10.5782 - mse: 10.5782 - mae: 1.3263 - val_loss: 7.8935 - val_mse: 7.8935 - val_mae: 1.3591 - lr: 1.7636e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 5s - loss: 9.8845 - mse: 9.8845 - mae: 1.3074 - val_loss: 7.5250 - val_mse: 7.5250 - val_mae: 1.4331 - lr: 1.7636e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 5s - loss: 9.9627 - mse: 9.9627 - mae: 1.2942 - val_loss: 7.5676 - val_mse: 7.5676 - val_mae: 1.3483 - lr: 1.7636e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 5s - loss: 9.5037 - mse: 9.5037 - mae: 1.2757 - val_loss: 8.2361 - val_mse: 8.2361 - val_mae: 1.3678 - lr: 1.7636e-04 - 5s/epoch - 5ms/step\n",
            "Score for fold 2: loss of 8.236109733581543\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 5s - loss: 8.3145 - mse: 8.3145 - mae: 1.3030 - val_loss: 12.3532 - val_mse: 12.3532 - val_mae: 1.2628 - lr: 1.7636e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 5s - loss: 7.9319 - mse: 7.9319 - mae: 1.2743 - val_loss: 12.3960 - val_mse: 12.3960 - val_mae: 1.2680 - lr: 1.7636e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 5s - loss: 7.6731 - mse: 7.6731 - mae: 1.2626 - val_loss: 12.7971 - val_mse: 12.7971 - val_mae: 1.3379 - lr: 1.7636e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 5s - loss: 7.3743 - mse: 7.3743 - mae: 1.2427 - val_loss: 13.1124 - val_mse: 13.1124 - val_mae: 1.2361 - lr: 1.7636e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 5s - loss: 7.0380 - mse: 7.0380 - mae: 1.2330 - val_loss: 12.5573 - val_mse: 12.5573 - val_mae: 1.2643 - lr: 1.7636e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 5s - loss: 6.8501 - mse: 6.8501 - mae: 1.2134 - val_loss: 12.9848 - val_mse: 12.9848 - val_mae: 1.3836 - lr: 1.7636e-04 - 5s/epoch - 5ms/step\n",
            "Score for fold 3: loss of 12.984783172607422\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 5s - loss: 8.3245 - mse: 8.3245 - mae: 1.2369 - val_loss: 7.6025 - val_mse: 7.6025 - val_mae: 1.2012 - lr: 1.7636e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 5s - loss: 7.6802 - mse: 7.6802 - mae: 1.2036 - val_loss: 7.3002 - val_mse: 7.3002 - val_mae: 1.2421 - lr: 1.7636e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 5s - loss: 7.5311 - mse: 7.5311 - mae: 1.1851 - val_loss: 8.0411 - val_mse: 8.0411 - val_mae: 1.2423 - lr: 1.7636e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 5s - loss: 7.2921 - mse: 7.2921 - mae: 1.1730 - val_loss: 7.6393 - val_mse: 7.6393 - val_mae: 1.2583 - lr: 1.7636e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 5s - loss: 7.0390 - mse: 7.0390 - mae: 1.1553 - val_loss: 8.1046 - val_mse: 8.1046 - val_mae: 1.3189 - lr: 1.7636e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 5s - loss: 6.8756 - mse: 6.8756 - mae: 1.1443 - val_loss: 7.9633 - val_mse: 7.9633 - val_mae: 1.2434 - lr: 1.7636e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 5s - loss: 6.6130 - mse: 6.6130 - mae: 1.1248 - val_loss: 8.4833 - val_mse: 8.4833 - val_mae: 1.2758 - lr: 1.7636e-04 - 5s/epoch - 5ms/step\n",
            "Score for fold 4: loss of 8.483287811279297\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 5s - loss: 7.1490 - mse: 7.1490 - mae: 1.1708 - val_loss: 5.9646 - val_mse: 5.9646 - val_mae: 1.2001 - lr: 1.7636e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 5s - loss: 6.6451 - mse: 6.6451 - mae: 1.1424 - val_loss: 6.5320 - val_mse: 6.5320 - val_mae: 1.1561 - lr: 1.7636e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 5s - loss: 6.4929 - mse: 6.4929 - mae: 1.1292 - val_loss: 6.6926 - val_mse: 6.6926 - val_mae: 1.1463 - lr: 1.7636e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 5s - loss: 6.0336 - mse: 6.0336 - mae: 1.1080 - val_loss: 6.4979 - val_mse: 6.4979 - val_mae: 1.1315 - lr: 1.7636e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 5s - loss: 6.0664 - mse: 6.0664 - mae: 1.0956 - val_loss: 7.4372 - val_mse: 7.4372 - val_mae: 1.2005 - lr: 1.7636e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 5s - loss: 5.8050 - mse: 5.8050 - mae: 1.0845 - val_loss: 7.8256 - val_mse: 7.8256 - val_mae: 1.2082 - lr: 1.7636e-04 - 5s/epoch - 5ms/step\n",
            "Score for fold 5: loss of 7.825629711151123\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-10-12 13:17:53,611]\u001b[0m Finished trial#42 resulted in value: 9.552. Current best value is 9.440000000000001 with parameters: {'activation': 'relu', 'num_hidden_layer': 4, 'i': 9, 'learning_rate': 0.0001792631082155351}.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 4s - loss: 14.1831 - mse: 14.1831 - mae: 1.5608 - val_loss: 9.6583 - val_mse: 9.6583 - val_mae: 1.5093 - lr: 2.1192e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 3s - loss: 13.3110 - mse: 13.3110 - mae: 1.5000 - val_loss: 8.9544 - val_mse: 8.9544 - val_mae: 1.4215 - lr: 2.1192e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 3s - loss: 13.0333 - mse: 13.0333 - mae: 1.4819 - val_loss: 8.6086 - val_mse: 8.6086 - val_mae: 1.4853 - lr: 2.1192e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 3s - loss: 12.8783 - mse: 12.8783 - mae: 1.4700 - val_loss: 8.5257 - val_mse: 8.5257 - val_mae: 1.5225 - lr: 2.1192e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 3s - loss: 12.8124 - mse: 12.8124 - mae: 1.4601 - val_loss: 8.6652 - val_mse: 8.6652 - val_mae: 1.5091 - lr: 2.1192e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 3s - loss: 12.6987 - mse: 12.6987 - mae: 1.4547 - val_loss: 8.3815 - val_mse: 8.3815 - val_mae: 1.4540 - lr: 2.1192e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 3s - loss: 12.4698 - mse: 12.4698 - mae: 1.4448 - val_loss: 8.4824 - val_mse: 8.4824 - val_mae: 1.4121 - lr: 2.1192e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 3s - loss: 12.4782 - mse: 12.4782 - mae: 1.4382 - val_loss: 8.4234 - val_mse: 8.4234 - val_mae: 1.4857 - lr: 2.1192e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 3s - loss: 12.3218 - mse: 12.3218 - mae: 1.4341 - val_loss: 8.6325 - val_mse: 8.6325 - val_mae: 1.4068 - lr: 2.1192e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 3s - loss: 12.3347 - mse: 12.3347 - mae: 1.4286 - val_loss: 8.3048 - val_mse: 8.3048 - val_mae: 1.4220 - lr: 2.1192e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 3s - loss: 12.1758 - mse: 12.1758 - mae: 1.4206 - val_loss: 8.4427 - val_mse: 8.4427 - val_mae: 1.4389 - lr: 2.1192e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 3s - loss: 12.1189 - mse: 12.1189 - mae: 1.4166 - val_loss: 8.8305 - val_mse: 8.8305 - val_mae: 1.4556 - lr: 2.1192e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 13/100\n",
            "1000/1000 - 3s - loss: 12.0876 - mse: 12.0876 - mae: 1.4099 - val_loss: 8.2872 - val_mse: 8.2872 - val_mae: 1.4663 - lr: 2.1192e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 14/100\n",
            "1000/1000 - 3s - loss: 11.8117 - mse: 11.8117 - mae: 1.3986 - val_loss: 8.5328 - val_mse: 8.5328 - val_mae: 1.4803 - lr: 2.1192e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 15/100\n",
            "1000/1000 - 3s - loss: 11.8471 - mse: 11.8471 - mae: 1.3943 - val_loss: 8.5265 - val_mse: 8.5265 - val_mae: 1.5001 - lr: 2.1192e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 16/100\n",
            "1000/1000 - 3s - loss: 11.7155 - mse: 11.7155 - mae: 1.3897 - val_loss: 8.4253 - val_mse: 8.4253 - val_mae: 1.4485 - lr: 2.1192e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 17/100\n",
            "1000/1000 - 3s - loss: 11.5703 - mse: 11.5703 - mae: 1.3811 - val_loss: 8.6453 - val_mse: 8.6453 - val_mae: 1.4101 - lr: 2.1192e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 18/100\n",
            "1000/1000 - 3s - loss: 11.0827 - mse: 11.0827 - mae: 1.3708 - val_loss: 8.7658 - val_mse: 8.7658 - val_mae: 1.5048 - lr: 2.1192e-04 - 3s/epoch - 3ms/step\n",
            "Score for fold 1: loss of 8.765786170959473\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 3s - loss: 11.2243 - mse: 11.2243 - mae: 1.3876 - val_loss: 9.6416 - val_mse: 9.6416 - val_mae: 1.3448 - lr: 2.1192e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 3s - loss: 10.7058 - mse: 10.7058 - mae: 1.3788 - val_loss: 10.4780 - val_mse: 10.4780 - val_mae: 1.3672 - lr: 2.1192e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 3s - loss: 10.5452 - mse: 10.5452 - mae: 1.3701 - val_loss: 9.8207 - val_mse: 9.8207 - val_mae: 1.4009 - lr: 2.1192e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 3s - loss: 10.6037 - mse: 10.6037 - mae: 1.3585 - val_loss: 9.7858 - val_mse: 9.7858 - val_mae: 1.3668 - lr: 2.1192e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 3s - loss: 10.3709 - mse: 10.3709 - mae: 1.3504 - val_loss: 9.6796 - val_mse: 9.6796 - val_mae: 1.3815 - lr: 2.1192e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 3s - loss: 10.0941 - mse: 10.0941 - mae: 1.3417 - val_loss: 10.6419 - val_mse: 10.6419 - val_mae: 1.3860 - lr: 2.1192e-04 - 3s/epoch - 3ms/step\n",
            "Score for fold 2: loss of 10.641940116882324\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 3s - loss: 10.1932 - mse: 10.1932 - mae: 1.3595 - val_loss: 10.2455 - val_mse: 10.2455 - val_mae: 1.3073 - lr: 2.1192e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 3s - loss: 9.6937 - mse: 9.6937 - mae: 1.3507 - val_loss: 10.5775 - val_mse: 10.5775 - val_mae: 1.3436 - lr: 2.1192e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 3s - loss: 9.6083 - mse: 9.6083 - mae: 1.3395 - val_loss: 10.5013 - val_mse: 10.5013 - val_mae: 1.3469 - lr: 2.1192e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 3s - loss: 9.5093 - mse: 9.5093 - mae: 1.3269 - val_loss: 11.0792 - val_mse: 11.0792 - val_mae: 1.3561 - lr: 2.1192e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 3s - loss: 9.3324 - mse: 9.3324 - mae: 1.3214 - val_loss: 10.8475 - val_mse: 10.8475 - val_mae: 1.2867 - lr: 2.1192e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 3s - loss: 9.2846 - mse: 9.2846 - mae: 1.3127 - val_loss: 11.0190 - val_mse: 11.0190 - val_mae: 1.3101 - lr: 2.1192e-04 - 3s/epoch - 3ms/step\n",
            "Score for fold 3: loss of 11.019041061401367\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 3s - loss: 8.9229 - mse: 8.9229 - mae: 1.3204 - val_loss: 13.6635 - val_mse: 13.6635 - val_mae: 1.3013 - lr: 2.1192e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 3s - loss: 8.7499 - mse: 8.7499 - mae: 1.3069 - val_loss: 14.3466 - val_mse: 14.3466 - val_mae: 1.3158 - lr: 2.1192e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 3s - loss: 8.1891 - mse: 8.1891 - mae: 1.2911 - val_loss: 14.1764 - val_mse: 14.1764 - val_mae: 1.4028 - lr: 2.1192e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 3s - loss: 7.7849 - mse: 7.7849 - mae: 1.2763 - val_loss: 13.9569 - val_mse: 13.9569 - val_mae: 1.3365 - lr: 2.1192e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 3s - loss: 7.6900 - mse: 7.6900 - mae: 1.2669 - val_loss: 14.3008 - val_mse: 14.3008 - val_mae: 1.3395 - lr: 2.1192e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 3s - loss: 7.6434 - mse: 7.6434 - mae: 1.2587 - val_loss: 14.6226 - val_mse: 14.6226 - val_mae: 1.3504 - lr: 2.1192e-04 - 3s/epoch - 3ms/step\n",
            "Score for fold 4: loss of 14.62256908416748\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 3s - loss: 9.5582 - mse: 9.5582 - mae: 1.3026 - val_loss: 6.7571 - val_mse: 6.7571 - val_mae: 1.2174 - lr: 2.1192e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 3s - loss: 9.2365 - mse: 9.2365 - mae: 1.2856 - val_loss: 7.0332 - val_mse: 7.0332 - val_mae: 1.2693 - lr: 2.1192e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 3s - loss: 9.1700 - mse: 9.1700 - mae: 1.2718 - val_loss: 7.0783 - val_mse: 7.0783 - val_mae: 1.2756 - lr: 2.1192e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 3s - loss: 8.8316 - mse: 8.8316 - mae: 1.2613 - val_loss: 8.8073 - val_mse: 8.8073 - val_mae: 1.2371 - lr: 2.1192e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 3s - loss: 9.2078 - mse: 9.2078 - mae: 1.2571 - val_loss: 8.3076 - val_mse: 8.3076 - val_mae: 1.2698 - lr: 2.1192e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 3s - loss: 8.4619 - mse: 8.4619 - mae: 1.2459 - val_loss: 7.7593 - val_mse: 7.7593 - val_mae: 1.3536 - lr: 2.1192e-04 - 3s/epoch - 3ms/step\n",
            "Score for fold 5: loss of 7.759346008300781\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-10-12 13:20:09,003]\u001b[0m Finished trial#43 resulted in value: 10.562. Current best value is 9.440000000000001 with parameters: {'activation': 'relu', 'num_hidden_layer': 4, 'i': 9, 'learning_rate': 0.0001792631082155351}.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 6s - loss: 14.0776 - mse: 14.0776 - mae: 1.5568 - val_loss: 11.3689 - val_mse: 11.3689 - val_mae: 1.4929 - lr: 1.0146e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 5s - loss: 12.8308 - mse: 12.8308 - mae: 1.4969 - val_loss: 10.9175 - val_mse: 10.9175 - val_mae: 1.4247 - lr: 1.0146e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 5s - loss: 12.5830 - mse: 12.5830 - mae: 1.4777 - val_loss: 10.7146 - val_mse: 10.7146 - val_mae: 1.4641 - lr: 1.0146e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 5s - loss: 12.4717 - mse: 12.4717 - mae: 1.4689 - val_loss: 10.6003 - val_mse: 10.6003 - val_mae: 1.4818 - lr: 1.0146e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 5s - loss: 12.2989 - mse: 12.2989 - mae: 1.4583 - val_loss: 10.6638 - val_mse: 10.6638 - val_mae: 1.5290 - lr: 1.0146e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 6s - loss: 12.1389 - mse: 12.1389 - mae: 1.4459 - val_loss: 10.7990 - val_mse: 10.7990 - val_mae: 1.5268 - lr: 1.0146e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 5s - loss: 12.0996 - mse: 12.0996 - mae: 1.4382 - val_loss: 10.6250 - val_mse: 10.6250 - val_mae: 1.4962 - lr: 1.0146e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 5s - loss: 11.9903 - mse: 11.9903 - mae: 1.4351 - val_loss: 10.6016 - val_mse: 10.6016 - val_mae: 1.4143 - lr: 1.0146e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 5s - loss: 11.9431 - mse: 11.9431 - mae: 1.4288 - val_loss: 11.1212 - val_mse: 11.1212 - val_mae: 1.4023 - lr: 1.0146e-04 - 5s/epoch - 5ms/step\n",
            "Score for fold 1: loss of 11.121146202087402\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 5s - loss: 9.3005 - mse: 9.3005 - mae: 1.4227 - val_loss: 21.6698 - val_mse: 21.6698 - val_mae: 1.5456 - lr: 1.0146e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 5s - loss: 9.0334 - mse: 9.0334 - mae: 1.4151 - val_loss: 21.5651 - val_mse: 21.5651 - val_mae: 1.4524 - lr: 1.0146e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 5s - loss: 9.1230 - mse: 9.1230 - mae: 1.4085 - val_loss: 21.4806 - val_mse: 21.4806 - val_mae: 1.4627 - lr: 1.0146e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 5s - loss: 8.9904 - mse: 8.9904 - mae: 1.4002 - val_loss: 21.5282 - val_mse: 21.5282 - val_mae: 1.4960 - lr: 1.0146e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 5s - loss: 8.7719 - mse: 8.7719 - mae: 1.3912 - val_loss: 21.5253 - val_mse: 21.5253 - val_mae: 1.4641 - lr: 1.0146e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 5s - loss: 8.6162 - mse: 8.6162 - mae: 1.3857 - val_loss: 21.4467 - val_mse: 21.4467 - val_mae: 1.4657 - lr: 1.0146e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 5s - loss: 8.3506 - mse: 8.3506 - mae: 1.3756 - val_loss: 22.0334 - val_mse: 22.0334 - val_mae: 1.4131 - lr: 1.0146e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 5s - loss: 8.3015 - mse: 8.3015 - mae: 1.3700 - val_loss: 22.2244 - val_mse: 22.2244 - val_mae: 1.4372 - lr: 1.0146e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 5s - loss: 8.1126 - mse: 8.1126 - mae: 1.3601 - val_loss: 21.7148 - val_mse: 21.7148 - val_mae: 1.5427 - lr: 1.0146e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 5s - loss: 8.0841 - mse: 8.0841 - mae: 1.3519 - val_loss: 21.6063 - val_mse: 21.6063 - val_mae: 1.6032 - lr: 1.0146e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 5s - loss: 7.8501 - mse: 7.8501 - mae: 1.3371 - val_loss: 21.6212 - val_mse: 21.6212 - val_mae: 1.4951 - lr: 1.0146e-04 - 5s/epoch - 5ms/step\n",
            "Score for fold 2: loss of 21.62120819091797\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 5s - loss: 11.3498 - mse: 11.3498 - mae: 1.3765 - val_loss: 7.4601 - val_mse: 7.4601 - val_mae: 1.2890 - lr: 1.0146e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 5s - loss: 10.9704 - mse: 10.9704 - mae: 1.3614 - val_loss: 8.2306 - val_mse: 8.2306 - val_mae: 1.3456 - lr: 1.0146e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 5s - loss: 10.9636 - mse: 10.9636 - mae: 1.3494 - val_loss: 7.4133 - val_mse: 7.4133 - val_mae: 1.3608 - lr: 1.0146e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 5s - loss: 10.7909 - mse: 10.7909 - mae: 1.3403 - val_loss: 7.8545 - val_mse: 7.8545 - val_mae: 1.3847 - lr: 1.0146e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 5s - loss: 10.5916 - mse: 10.5916 - mae: 1.3352 - val_loss: 7.7498 - val_mse: 7.7498 - val_mae: 1.3060 - lr: 1.0146e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 5s - loss: 10.4902 - mse: 10.4902 - mae: 1.3234 - val_loss: 8.0676 - val_mse: 8.0676 - val_mae: 1.3334 - lr: 1.0146e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 5s - loss: 10.1464 - mse: 10.1464 - mae: 1.3068 - val_loss: 8.7205 - val_mse: 8.7205 - val_mae: 1.4424 - lr: 1.0146e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 5s - loss: 10.2106 - mse: 10.2106 - mae: 1.2967 - val_loss: 8.0901 - val_mse: 8.0901 - val_mae: 1.4008 - lr: 1.0146e-04 - 5s/epoch - 5ms/step\n",
            "Score for fold 3: loss of 8.090113639831543\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 5s - loss: 10.6334 - mse: 10.6334 - mae: 1.3219 - val_loss: 6.4891 - val_mse: 6.4891 - val_mae: 1.3122 - lr: 1.0146e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 5s - loss: 10.3236 - mse: 10.3236 - mae: 1.3027 - val_loss: 6.3286 - val_mse: 6.3286 - val_mae: 1.2294 - lr: 1.0146e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 5s - loss: 10.2193 - mse: 10.2193 - mae: 1.2940 - val_loss: 5.9987 - val_mse: 5.9987 - val_mae: 1.2594 - lr: 1.0146e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 5s - loss: 9.9462 - mse: 9.9462 - mae: 1.2792 - val_loss: 6.7337 - val_mse: 6.7337 - val_mae: 1.2793 - lr: 1.0146e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 5s - loss: 9.7386 - mse: 9.7386 - mae: 1.2679 - val_loss: 7.0461 - val_mse: 7.0461 - val_mae: 1.2552 - lr: 1.0146e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 5s - loss: 9.7170 - mse: 9.7170 - mae: 1.2523 - val_loss: 5.9932 - val_mse: 5.9932 - val_mae: 1.3357 - lr: 1.0146e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 5s - loss: 9.3245 - mse: 9.3245 - mae: 1.2440 - val_loss: 6.0062 - val_mse: 6.0062 - val_mae: 1.2914 - lr: 1.0146e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 5s - loss: 9.1906 - mse: 9.1906 - mae: 1.2284 - val_loss: 6.2125 - val_mse: 6.2125 - val_mae: 1.4217 - lr: 1.0146e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 5s - loss: 8.9093 - mse: 8.9093 - mae: 1.2154 - val_loss: 6.1364 - val_mse: 6.1364 - val_mae: 1.3338 - lr: 1.0146e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 5s - loss: 8.7867 - mse: 8.7867 - mae: 1.2074 - val_loss: 8.4963 - val_mse: 8.4963 - val_mae: 1.3916 - lr: 1.0146e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 5s - loss: 8.4839 - mse: 8.4839 - mae: 1.1932 - val_loss: 6.8139 - val_mse: 6.8139 - val_mae: 1.3079 - lr: 1.0146e-04 - 5s/epoch - 5ms/step\n",
            "Score for fold 4: loss of 6.813896656036377\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 5s - loss: 8.8559 - mse: 8.8559 - mae: 1.2364 - val_loss: 5.8412 - val_mse: 5.8412 - val_mae: 1.1375 - lr: 1.0146e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 5s - loss: 9.2580 - mse: 9.2580 - mae: 1.2215 - val_loss: 6.3052 - val_mse: 6.3052 - val_mae: 1.2591 - lr: 1.0146e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 5s - loss: 8.4409 - mse: 8.4409 - mae: 1.2053 - val_loss: 6.1463 - val_mse: 6.1463 - val_mae: 1.3174 - lr: 1.0146e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 5s - loss: 8.2571 - mse: 8.2571 - mae: 1.1929 - val_loss: 6.5594 - val_mse: 6.5594 - val_mae: 1.3533 - lr: 1.0146e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 5s - loss: 8.0678 - mse: 8.0678 - mae: 1.1736 - val_loss: 6.7072 - val_mse: 6.7072 - val_mae: 1.3377 - lr: 1.0146e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 5s - loss: 9.0541 - mse: 9.0541 - mae: 1.1648 - val_loss: 6.9277 - val_mse: 6.9277 - val_mae: 1.2778 - lr: 1.0146e-04 - 5s/epoch - 5ms/step\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-10-12 13:24:17,971]\u001b[0m Finished trial#44 resulted in value: 10.914. Current best value is 9.440000000000001 with parameters: {'activation': 'relu', 'num_hidden_layer': 4, 'i': 9, 'learning_rate': 0.0001792631082155351}.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Score for fold 5: loss of 6.927670001983643\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 11s - loss: 12.4304 - mse: 12.4304 - mae: 1.5482 - val_loss: 16.8217 - val_mse: 16.8217 - val_mae: 1.4215 - lr: 1.5213e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 10s - loss: 11.4809 - mse: 11.4809 - mae: 1.4887 - val_loss: 16.5432 - val_mse: 16.5432 - val_mae: 1.3818 - lr: 1.5213e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 10s - loss: 10.9961 - mse: 10.9961 - mae: 1.4693 - val_loss: 16.4429 - val_mse: 16.4429 - val_mae: 1.4451 - lr: 1.5213e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 10s - loss: 11.1216 - mse: 11.1216 - mae: 1.4581 - val_loss: 16.6578 - val_mse: 16.6578 - val_mae: 1.4649 - lr: 1.5213e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 10s - loss: 10.7687 - mse: 10.7687 - mae: 1.4499 - val_loss: 16.5314 - val_mse: 16.5314 - val_mae: 1.5069 - lr: 1.5213e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 10s - loss: 10.8627 - mse: 10.8627 - mae: 1.4452 - val_loss: 16.4262 - val_mse: 16.4262 - val_mae: 1.4837 - lr: 1.5213e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 10s - loss: 10.4444 - mse: 10.4444 - mae: 1.4283 - val_loss: 16.1636 - val_mse: 16.1636 - val_mae: 1.4625 - lr: 1.5213e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 10s - loss: 10.3065 - mse: 10.3065 - mae: 1.4156 - val_loss: 16.0610 - val_mse: 16.0610 - val_mae: 1.4566 - lr: 1.5213e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 10s - loss: 10.4320 - mse: 10.4320 - mae: 1.4070 - val_loss: 16.4390 - val_mse: 16.4390 - val_mae: 1.5106 - lr: 1.5213e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 10s - loss: 9.9991 - mse: 9.9991 - mae: 1.3912 - val_loss: 16.1188 - val_mse: 16.1188 - val_mae: 1.4449 - lr: 1.5213e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 10s - loss: 9.6908 - mse: 9.6908 - mae: 1.3802 - val_loss: 16.3278 - val_mse: 16.3278 - val_mae: 1.4578 - lr: 1.5213e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 10s - loss: 9.4782 - mse: 9.4782 - mae: 1.3679 - val_loss: 16.6634 - val_mse: 16.6634 - val_mae: 1.4772 - lr: 1.5213e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 13/100\n",
            "1000/1000 - 10s - loss: 9.1061 - mse: 9.1061 - mae: 1.3462 - val_loss: 16.1598 - val_mse: 16.1598 - val_mae: 1.4701 - lr: 1.5213e-04 - 10s/epoch - 10ms/step\n",
            "Score for fold 1: loss of 16.159793853759766\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 10s - loss: 10.7888 - mse: 10.7888 - mae: 1.3774 - val_loss: 9.1096 - val_mse: 9.1096 - val_mae: 1.3515 - lr: 1.5213e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 10s - loss: 10.5318 - mse: 10.5318 - mae: 1.3519 - val_loss: 9.9085 - val_mse: 9.9085 - val_mae: 1.3686 - lr: 1.5213e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 10s - loss: 10.2325 - mse: 10.2325 - mae: 1.3362 - val_loss: 9.3595 - val_mse: 9.3595 - val_mae: 1.3549 - lr: 1.5213e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 10s - loss: 9.9070 - mse: 9.9070 - mae: 1.3136 - val_loss: 9.9752 - val_mse: 9.9752 - val_mae: 1.4532 - lr: 1.5213e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 10s - loss: 9.4251 - mse: 9.4251 - mae: 1.2903 - val_loss: 10.1520 - val_mse: 10.1520 - val_mae: 1.4530 - lr: 1.5213e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 10s - loss: 9.3978 - mse: 9.3978 - mae: 1.2690 - val_loss: 10.2845 - val_mse: 10.2845 - val_mae: 1.4756 - lr: 1.5213e-04 - 10s/epoch - 10ms/step\n",
            "Score for fold 2: loss of 10.284476280212402\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 10s - loss: 9.2202 - mse: 9.2202 - mae: 1.3108 - val_loss: 9.7862 - val_mse: 9.7862 - val_mae: 1.2404 - lr: 1.5213e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 10s - loss: 8.8626 - mse: 8.8626 - mae: 1.2831 - val_loss: 10.4856 - val_mse: 10.4856 - val_mae: 1.2675 - lr: 1.5213e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 10s - loss: 8.6537 - mse: 8.6537 - mae: 1.2648 - val_loss: 10.0465 - val_mse: 10.0465 - val_mae: 1.2595 - lr: 1.5213e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 10s - loss: 8.1799 - mse: 8.1799 - mae: 1.2364 - val_loss: 10.2646 - val_mse: 10.2646 - val_mae: 1.2798 - lr: 1.5213e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 10s - loss: 7.9197 - mse: 7.9197 - mae: 1.2144 - val_loss: 10.3723 - val_mse: 10.3723 - val_mae: 1.3204 - lr: 1.5213e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 10s - loss: 7.4716 - mse: 7.4716 - mae: 1.1974 - val_loss: 10.4010 - val_mse: 10.4010 - val_mae: 1.2736 - lr: 1.5213e-04 - 10s/epoch - 10ms/step\n",
            "Score for fold 3: loss of 10.400951385498047\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 10s - loss: 8.2608 - mse: 8.2608 - mae: 1.2244 - val_loss: 7.7579 - val_mse: 7.7579 - val_mae: 1.1538 - lr: 1.5213e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 10s - loss: 7.8561 - mse: 7.8561 - mae: 1.2049 - val_loss: 7.1187 - val_mse: 7.1187 - val_mae: 1.2138 - lr: 1.5213e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 10s - loss: 7.5303 - mse: 7.5303 - mae: 1.1677 - val_loss: 7.8215 - val_mse: 7.8215 - val_mae: 1.2472 - lr: 1.5213e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 10s - loss: 7.3315 - mse: 7.3315 - mae: 1.1579 - val_loss: 7.4792 - val_mse: 7.4792 - val_mae: 1.2273 - lr: 1.5213e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 10s - loss: 6.8132 - mse: 6.8132 - mae: 1.1385 - val_loss: 8.1877 - val_mse: 8.1877 - val_mae: 1.2747 - lr: 1.5213e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 10s - loss: 6.8624 - mse: 6.8624 - mae: 1.1201 - val_loss: 8.3358 - val_mse: 8.3358 - val_mae: 1.1860 - lr: 1.5213e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 10s - loss: 6.5514 - mse: 6.5514 - mae: 1.1100 - val_loss: 7.6560 - val_mse: 7.6560 - val_mae: 1.1992 - lr: 1.5213e-04 - 10s/epoch - 10ms/step\n",
            "Score for fold 4: loss of 7.656035900115967\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 10s - loss: 7.6653 - mse: 7.6653 - mae: 1.1540 - val_loss: 3.0954 - val_mse: 3.0954 - val_mae: 1.0324 - lr: 1.5213e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 10s - loss: 7.3127 - mse: 7.3127 - mae: 1.1199 - val_loss: 3.2024 - val_mse: 3.2024 - val_mae: 1.0644 - lr: 1.5213e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 10s - loss: 7.0999 - mse: 7.0999 - mae: 1.1023 - val_loss: 3.2867 - val_mse: 3.2867 - val_mae: 1.0508 - lr: 1.5213e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 10s - loss: 6.7858 - mse: 6.7858 - mae: 1.0822 - val_loss: 3.3243 - val_mse: 3.3243 - val_mae: 1.1033 - lr: 1.5213e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 10s - loss: 6.6338 - mse: 6.6338 - mae: 1.0687 - val_loss: 3.6478 - val_mse: 3.6478 - val_mae: 1.1978 - lr: 1.5213e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 10s - loss: 5.9885 - mse: 5.9885 - mae: 1.0500 - val_loss: 3.6186 - val_mse: 3.6186 - val_mae: 1.0614 - lr: 1.5213e-04 - 10s/epoch - 10ms/step\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-10-12 13:30:55,421]\u001b[0m Finished trial#45 resulted in value: 9.623999999999999. Current best value is 9.440000000000001 with parameters: {'activation': 'relu', 'num_hidden_layer': 4, 'i': 9, 'learning_rate': 0.0001792631082155351}.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Score for fold 5: loss of 3.6186015605926514\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 11s - loss: 11.3057 - mse: 11.3057 - mae: 1.5251 - val_loss: 19.8022 - val_mse: 19.8022 - val_mae: 1.5954 - lr: 1.3078e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 10s - loss: 10.7174 - mse: 10.7174 - mae: 1.4730 - val_loss: 19.9433 - val_mse: 19.9433 - val_mae: 1.4588 - lr: 1.3078e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 10s - loss: 10.3310 - mse: 10.3310 - mae: 1.4572 - val_loss: 19.5568 - val_mse: 19.5568 - val_mae: 1.5439 - lr: 1.3078e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 10s - loss: 10.2112 - mse: 10.2112 - mae: 1.4443 - val_loss: 19.9315 - val_mse: 19.9315 - val_mae: 1.4810 - lr: 1.3078e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 10s - loss: 10.1024 - mse: 10.1024 - mae: 1.4376 - val_loss: 19.7564 - val_mse: 19.7564 - val_mae: 1.4820 - lr: 1.3078e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 10s - loss: 9.8457 - mse: 9.8457 - mae: 1.4240 - val_loss: 19.4761 - val_mse: 19.4761 - val_mae: 1.4889 - lr: 1.3078e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 10s - loss: 9.6982 - mse: 9.6982 - mae: 1.4191 - val_loss: 19.6819 - val_mse: 19.6819 - val_mae: 1.4685 - lr: 1.3078e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 10s - loss: 9.4766 - mse: 9.4766 - mae: 1.4036 - val_loss: 19.5395 - val_mse: 19.5395 - val_mae: 1.4935 - lr: 1.3078e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 10s - loss: 9.4249 - mse: 9.4249 - mae: 1.3891 - val_loss: 19.7231 - val_mse: 19.7231 - val_mae: 1.5034 - lr: 1.3078e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 10s - loss: 9.0316 - mse: 9.0316 - mae: 1.3734 - val_loss: 20.0665 - val_mse: 20.0665 - val_mae: 1.4849 - lr: 1.3078e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 10s - loss: 9.2275 - mse: 9.2275 - mae: 1.3670 - val_loss: 19.8736 - val_mse: 19.8736 - val_mae: 1.5874 - lr: 1.3078e-04 - 10s/epoch - 10ms/step\n",
            "Score for fold 1: loss of 19.87358856201172\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 10s - loss: 10.9650 - mse: 10.9650 - mae: 1.3988 - val_loss: 10.4529 - val_mse: 10.4529 - val_mae: 1.3803 - lr: 1.3078e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 10s - loss: 10.7179 - mse: 10.7179 - mae: 1.3811 - val_loss: 10.6072 - val_mse: 10.6072 - val_mae: 1.3473 - lr: 1.3078e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 10s - loss: 10.5709 - mse: 10.5709 - mae: 1.3636 - val_loss: 10.6925 - val_mse: 10.6925 - val_mae: 1.3602 - lr: 1.3078e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 10s - loss: 10.0645 - mse: 10.0645 - mae: 1.3443 - val_loss: 11.3333 - val_mse: 11.3333 - val_mae: 1.4411 - lr: 1.3078e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 10s - loss: 9.8333 - mse: 9.8333 - mae: 1.3267 - val_loss: 10.7182 - val_mse: 10.7182 - val_mae: 1.3910 - lr: 1.3078e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 10s - loss: 10.0220 - mse: 10.0220 - mae: 1.3104 - val_loss: 11.1860 - val_mse: 11.1860 - val_mae: 1.3823 - lr: 1.3078e-04 - 10s/epoch - 10ms/step\n",
            "Score for fold 2: loss of 11.18594741821289\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 10s - loss: 10.5139 - mse: 10.5139 - mae: 1.3372 - val_loss: 6.6698 - val_mse: 6.6698 - val_mae: 1.2929 - lr: 1.3078e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 10s - loss: 10.1774 - mse: 10.1774 - mae: 1.3082 - val_loss: 6.6074 - val_mse: 6.6074 - val_mae: 1.3175 - lr: 1.3078e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 10s - loss: 9.9280 - mse: 9.9280 - mae: 1.2904 - val_loss: 6.6894 - val_mse: 6.6894 - val_mae: 1.3703 - lr: 1.3078e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 10s - loss: 9.6433 - mse: 9.6433 - mae: 1.2648 - val_loss: 7.8029 - val_mse: 7.8029 - val_mae: 1.3148 - lr: 1.3078e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 10s - loss: 9.0760 - mse: 9.0760 - mae: 1.2459 - val_loss: 7.1102 - val_mse: 7.1102 - val_mae: 1.2807 - lr: 1.3078e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 10s - loss: 8.9278 - mse: 8.9278 - mae: 1.2249 - val_loss: 7.3163 - val_mse: 7.3163 - val_mae: 1.3360 - lr: 1.3078e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 10s - loss: 8.3942 - mse: 8.3942 - mae: 1.2045 - val_loss: 7.6204 - val_mse: 7.6204 - val_mae: 1.4074 - lr: 1.3078e-04 - 10s/epoch - 10ms/step\n",
            "Score for fold 3: loss of 7.620419502258301\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 10s - loss: 8.6156 - mse: 8.6156 - mae: 1.2492 - val_loss: 6.4352 - val_mse: 6.4352 - val_mae: 1.1337 - lr: 1.3078e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 10s - loss: 8.6864 - mse: 8.6864 - mae: 1.2219 - val_loss: 6.7682 - val_mse: 6.7682 - val_mae: 1.1514 - lr: 1.3078e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 10s - loss: 7.8798 - mse: 7.8798 - mae: 1.1961 - val_loss: 6.8637 - val_mse: 6.8637 - val_mae: 1.2124 - lr: 1.3078e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 10s - loss: 7.4395 - mse: 7.4395 - mae: 1.1796 - val_loss: 7.1849 - val_mse: 7.1849 - val_mae: 1.1731 - lr: 1.3078e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 10s - loss: 7.5401 - mse: 7.5401 - mae: 1.1588 - val_loss: 7.1548 - val_mse: 7.1548 - val_mae: 1.2528 - lr: 1.3078e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 10s - loss: 7.3579 - mse: 7.3579 - mae: 1.1415 - val_loss: 7.1230 - val_mse: 7.1230 - val_mae: 1.2563 - lr: 1.3078e-04 - 10s/epoch - 10ms/step\n",
            "Score for fold 4: loss of 7.122962951660156\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 10s - loss: 7.9100 - mse: 7.9100 - mae: 1.1878 - val_loss: 3.7599 - val_mse: 3.7599 - val_mae: 1.0403 - lr: 1.3078e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 10s - loss: 7.6497 - mse: 7.6497 - mae: 1.1594 - val_loss: 4.4712 - val_mse: 4.4712 - val_mae: 1.1721 - lr: 1.3078e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 10s - loss: 7.2979 - mse: 7.2979 - mae: 1.1336 - val_loss: 3.9421 - val_mse: 3.9421 - val_mae: 1.1583 - lr: 1.3078e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 10s - loss: 6.9337 - mse: 6.9337 - mae: 1.1146 - val_loss: 4.1732 - val_mse: 4.1732 - val_mae: 1.1059 - lr: 1.3078e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 10s - loss: 6.5993 - mse: 6.5993 - mae: 1.0918 - val_loss: 6.6610 - val_mse: 6.6610 - val_mae: 1.1449 - lr: 1.3078e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 10s - loss: 6.6159 - mse: 6.6159 - mae: 1.0753 - val_loss: 4.7717 - val_mse: 4.7717 - val_mae: 1.2047 - lr: 1.3078e-04 - 10s/epoch - 10ms/step\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-10-12 13:37:10,746]\u001b[0m Finished trial#46 resulted in value: 10.113999999999999. Current best value is 9.440000000000001 with parameters: {'activation': 'relu', 'num_hidden_layer': 4, 'i': 9, 'learning_rate': 0.0001792631082155351}.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Score for fold 5: loss of 4.77165412902832\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 3s - loss: 14.4161 - mse: 14.4161 - mae: 1.5991 - val_loss: 12.7404 - val_mse: 12.7404 - val_mae: 1.5907 - lr: 1.3115e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 13.0712 - mse: 13.0712 - mae: 1.5112 - val_loss: 11.9398 - val_mse: 11.9398 - val_mae: 1.5158 - lr: 1.3115e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 12.7665 - mse: 12.7665 - mae: 1.4934 - val_loss: 11.6801 - val_mse: 11.6801 - val_mae: 1.5088 - lr: 1.3115e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 12.5370 - mse: 12.5370 - mae: 1.4805 - val_loss: 11.3549 - val_mse: 11.3549 - val_mae: 1.5183 - lr: 1.3115e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 12.3734 - mse: 12.3734 - mae: 1.4683 - val_loss: 11.5206 - val_mse: 11.5206 - val_mae: 1.4785 - lr: 1.3115e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 12.3529 - mse: 12.3529 - mae: 1.4643 - val_loss: 11.4985 - val_mse: 11.4985 - val_mae: 1.4757 - lr: 1.3115e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 12.2007 - mse: 12.2007 - mae: 1.4549 - val_loss: 11.7092 - val_mse: 11.7092 - val_mae: 1.4625 - lr: 1.3115e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 2s - loss: 12.2443 - mse: 12.2443 - mae: 1.4557 - val_loss: 11.4021 - val_mse: 11.4021 - val_mae: 1.4943 - lr: 1.3115e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 2s - loss: 12.1598 - mse: 12.1598 - mae: 1.4478 - val_loss: 11.2595 - val_mse: 11.2595 - val_mae: 1.5131 - lr: 1.3115e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 2s - loss: 12.1749 - mse: 12.1749 - mae: 1.4456 - val_loss: 11.2481 - val_mse: 11.2481 - val_mae: 1.4842 - lr: 1.3115e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 2s - loss: 12.0982 - mse: 12.0982 - mae: 1.4418 - val_loss: 11.1655 - val_mse: 11.1655 - val_mae: 1.5066 - lr: 1.3115e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 2s - loss: 12.0519 - mse: 12.0519 - mae: 1.4371 - val_loss: 10.8022 - val_mse: 10.8022 - val_mae: 1.4937 - lr: 1.3115e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 13/100\n",
            "1000/1000 - 2s - loss: 12.0118 - mse: 12.0118 - mae: 1.4345 - val_loss: 10.9422 - val_mse: 10.9422 - val_mae: 1.4943 - lr: 1.3115e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 14/100\n",
            "1000/1000 - 2s - loss: 11.9368 - mse: 11.9368 - mae: 1.4325 - val_loss: 11.0416 - val_mse: 11.0416 - val_mae: 1.4794 - lr: 1.3115e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 15/100\n",
            "1000/1000 - 2s - loss: 11.9742 - mse: 11.9742 - mae: 1.4277 - val_loss: 10.8421 - val_mse: 10.8421 - val_mae: 1.4847 - lr: 1.3115e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 16/100\n",
            "1000/1000 - 2s - loss: 11.9531 - mse: 11.9531 - mae: 1.4278 - val_loss: 10.7788 - val_mse: 10.7788 - val_mae: 1.5065 - lr: 1.3115e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 17/100\n",
            "1000/1000 - 2s - loss: 11.9069 - mse: 11.9069 - mae: 1.4261 - val_loss: 10.9201 - val_mse: 10.9201 - val_mae: 1.4633 - lr: 1.3115e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 18/100\n",
            "1000/1000 - 2s - loss: 11.8237 - mse: 11.8237 - mae: 1.4263 - val_loss: 10.7726 - val_mse: 10.7726 - val_mae: 1.5269 - lr: 1.3115e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 19/100\n",
            "1000/1000 - 2s - loss: 11.8401 - mse: 11.8401 - mae: 1.4205 - val_loss: 11.0188 - val_mse: 11.0188 - val_mae: 1.5160 - lr: 1.3115e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 20/100\n",
            "1000/1000 - 2s - loss: 11.8277 - mse: 11.8277 - mae: 1.4201 - val_loss: 10.7725 - val_mse: 10.7725 - val_mae: 1.4692 - lr: 1.3115e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 21/100\n",
            "1000/1000 - 2s - loss: 11.8273 - mse: 11.8273 - mae: 1.4134 - val_loss: 10.7956 - val_mse: 10.7956 - val_mae: 1.5316 - lr: 1.3115e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 22/100\n",
            "1000/1000 - 2s - loss: 11.7330 - mse: 11.7330 - mae: 1.4142 - val_loss: 10.9450 - val_mse: 10.9450 - val_mae: 1.4838 - lr: 1.3115e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 23/100\n",
            "1000/1000 - 2s - loss: 11.6595 - mse: 11.6595 - mae: 1.4117 - val_loss: 10.8207 - val_mse: 10.8207 - val_mae: 1.4980 - lr: 1.3115e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 24/100\n",
            "1000/1000 - 2s - loss: 11.6145 - mse: 11.6145 - mae: 1.4117 - val_loss: 10.8277 - val_mse: 10.8277 - val_mae: 1.4879 - lr: 1.3115e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 25/100\n",
            "1000/1000 - 2s - loss: 11.5984 - mse: 11.5984 - mae: 1.4091 - val_loss: 10.7657 - val_mse: 10.7657 - val_mae: 1.4761 - lr: 1.3115e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 26/100\n",
            "1000/1000 - 2s - loss: 11.5584 - mse: 11.5584 - mae: 1.4073 - val_loss: 10.7890 - val_mse: 10.7890 - val_mae: 1.4690 - lr: 1.3115e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 27/100\n",
            "1000/1000 - 2s - loss: 11.5089 - mse: 11.5089 - mae: 1.4019 - val_loss: 10.9232 - val_mse: 10.9232 - val_mae: 1.4713 - lr: 1.3115e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 28/100\n",
            "1000/1000 - 2s - loss: 11.4557 - mse: 11.4557 - mae: 1.4011 - val_loss: 10.8811 - val_mse: 10.8811 - val_mae: 1.4931 - lr: 1.3115e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 29/100\n",
            "1000/1000 - 2s - loss: 11.4697 - mse: 11.4697 - mae: 1.3987 - val_loss: 10.7919 - val_mse: 10.7919 - val_mae: 1.4881 - lr: 1.3115e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 30/100\n",
            "1000/1000 - 2s - loss: 11.3741 - mse: 11.3741 - mae: 1.3961 - val_loss: 10.7945 - val_mse: 10.7945 - val_mae: 1.5374 - lr: 1.3115e-04 - 2s/epoch - 2ms/step\n",
            "Score for fold 1: loss of 10.79446792602539\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 12.3814 - mse: 12.3814 - mae: 1.4273 - val_loss: 6.8703 - val_mse: 6.8703 - val_mae: 1.3511 - lr: 1.3115e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 12.2032 - mse: 12.2032 - mae: 1.4216 - val_loss: 6.9144 - val_mse: 6.9144 - val_mae: 1.3582 - lr: 1.3115e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 12.2446 - mse: 12.2446 - mae: 1.4154 - val_loss: 7.0632 - val_mse: 7.0632 - val_mae: 1.3621 - lr: 1.3115e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 12.1620 - mse: 12.1620 - mae: 1.4191 - val_loss: 7.0287 - val_mse: 7.0287 - val_mae: 1.3586 - lr: 1.3115e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 12.0996 - mse: 12.0996 - mae: 1.4122 - val_loss: 6.9721 - val_mse: 6.9721 - val_mae: 1.3883 - lr: 1.3115e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 12.1761 - mse: 12.1761 - mae: 1.4152 - val_loss: 7.0455 - val_mse: 7.0455 - val_mae: 1.3802 - lr: 1.3115e-04 - 2s/epoch - 2ms/step\n",
            "Score for fold 2: loss of 7.04547119140625\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 10.6160 - mse: 10.6160 - mae: 1.4048 - val_loss: 12.7140 - val_mse: 12.7140 - val_mae: 1.4149 - lr: 1.3115e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 10.6443 - mse: 10.6443 - mae: 1.4011 - val_loss: 12.7371 - val_mse: 12.7371 - val_mae: 1.3942 - lr: 1.3115e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 10.6038 - mse: 10.6038 - mae: 1.3957 - val_loss: 12.7777 - val_mse: 12.7777 - val_mae: 1.4351 - lr: 1.3115e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 10.4849 - mse: 10.4849 - mae: 1.3961 - val_loss: 13.0740 - val_mse: 13.0740 - val_mae: 1.4386 - lr: 1.3115e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 10.5853 - mse: 10.5853 - mae: 1.3938 - val_loss: 13.4046 - val_mse: 13.4046 - val_mae: 1.3858 - lr: 1.3115e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 10.4532 - mse: 10.4532 - mae: 1.3912 - val_loss: 12.9111 - val_mse: 12.9111 - val_mae: 1.4539 - lr: 1.3115e-04 - 2s/epoch - 2ms/step\n",
            "Score for fold 3: loss of 12.9111328125\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 11.5824 - mse: 11.5824 - mae: 1.3959 - val_loss: 7.9681 - val_mse: 7.9681 - val_mae: 1.3868 - lr: 1.3115e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 11.5561 - mse: 11.5561 - mae: 1.3943 - val_loss: 7.8991 - val_mse: 7.8991 - val_mae: 1.3592 - lr: 1.3115e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 11.6444 - mse: 11.6444 - mae: 1.3944 - val_loss: 8.2664 - val_mse: 8.2664 - val_mae: 1.3968 - lr: 1.3115e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 11.5305 - mse: 11.5305 - mae: 1.3881 - val_loss: 8.1604 - val_mse: 8.1604 - val_mae: 1.4338 - lr: 1.3115e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 11.5025 - mse: 11.5025 - mae: 1.3883 - val_loss: 8.1678 - val_mse: 8.1678 - val_mae: 1.4036 - lr: 1.3115e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 11.3025 - mse: 11.3025 - mae: 1.3887 - val_loss: 8.1585 - val_mse: 8.1585 - val_mae: 1.3894 - lr: 1.3115e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 11.3753 - mse: 11.3753 - mae: 1.3818 - val_loss: 8.4623 - val_mse: 8.4623 - val_mae: 1.3793 - lr: 1.3115e-04 - 2s/epoch - 2ms/step\n",
            "Score for fold 4: loss of 8.462288856506348\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 9.1594 - mse: 9.1594 - mae: 1.3887 - val_loss: 16.2835 - val_mse: 16.2835 - val_mae: 1.4093 - lr: 1.3115e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 9.1971 - mse: 9.1971 - mae: 1.3849 - val_loss: 16.7251 - val_mse: 16.7251 - val_mae: 1.3564 - lr: 1.3115e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 9.1223 - mse: 9.1223 - mae: 1.3832 - val_loss: 16.7847 - val_mse: 16.7847 - val_mae: 1.4073 - lr: 1.3115e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 8.9887 - mse: 8.9887 - mae: 1.3757 - val_loss: 17.2059 - val_mse: 17.2059 - val_mae: 1.3837 - lr: 1.3115e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 9.1661 - mse: 9.1661 - mae: 1.3778 - val_loss: 16.8606 - val_mse: 16.8606 - val_mae: 1.3725 - lr: 1.3115e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 8.9921 - mse: 8.9921 - mae: 1.3706 - val_loss: 17.1098 - val_mse: 17.1098 - val_mae: 1.4237 - lr: 1.3115e-04 - 2s/epoch - 2ms/step\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-10-12 13:39:07,228]\u001b[0m Finished trial#47 resulted in value: 11.264. Current best value is 9.440000000000001 with parameters: {'activation': 'relu', 'num_hidden_layer': 4, 'i': 9, 'learning_rate': 0.0001792631082155351}.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Score for fold 5: loss of 17.109771728515625\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 11s - loss: 14.7131 - mse: 14.7131 - mae: 1.6336 - val_loss: 13.1567 - val_mse: 13.1567 - val_mae: 1.4679 - lr: 1.6779e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 10s - loss: 13.8354 - mse: 13.8354 - mae: 1.5393 - val_loss: 12.4955 - val_mse: 12.4955 - val_mae: 1.5089 - lr: 1.6779e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 10s - loss: 13.7490 - mse: 13.7490 - mae: 1.5268 - val_loss: 12.2584 - val_mse: 12.2584 - val_mae: 1.4509 - lr: 1.6779e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 10s - loss: 13.5824 - mse: 13.5824 - mae: 1.5108 - val_loss: 12.2663 - val_mse: 12.2663 - val_mae: 1.5202 - lr: 1.6779e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 10s - loss: 13.5697 - mse: 13.5697 - mae: 1.5048 - val_loss: 12.3053 - val_mse: 12.3053 - val_mae: 1.4526 - lr: 1.6779e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 9s - loss: 13.4510 - mse: 13.4510 - mae: 1.4965 - val_loss: 12.1166 - val_mse: 12.1166 - val_mae: 1.5604 - lr: 1.6779e-04 - 9s/epoch - 9ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 8s - loss: 13.3625 - mse: 13.3625 - mae: 1.4901 - val_loss: 11.8427 - val_mse: 11.8427 - val_mae: 1.4852 - lr: 1.6779e-04 - 8s/epoch - 8ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 9s - loss: 13.2994 - mse: 13.2994 - mae: 1.4848 - val_loss: 11.9712 - val_mse: 11.9712 - val_mae: 1.4926 - lr: 1.6779e-04 - 9s/epoch - 9ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 9s - loss: 13.1921 - mse: 13.1921 - mae: 1.4830 - val_loss: 11.7339 - val_mse: 11.7339 - val_mae: 1.5315 - lr: 1.6779e-04 - 9s/epoch - 9ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 9s - loss: 13.1580 - mse: 13.1580 - mae: 1.4830 - val_loss: 11.9061 - val_mse: 11.9061 - val_mae: 1.4342 - lr: 1.6779e-04 - 9s/epoch - 9ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 9s - loss: 13.0577 - mse: 13.0577 - mae: 1.4735 - val_loss: 12.0622 - val_mse: 12.0622 - val_mae: 1.4576 - lr: 1.6779e-04 - 9s/epoch - 9ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 9s - loss: 12.9966 - mse: 12.9966 - mae: 1.4714 - val_loss: 11.8446 - val_mse: 11.8446 - val_mae: 1.4754 - lr: 1.6779e-04 - 9s/epoch - 9ms/step\n",
            "Epoch 13/100\n",
            "1000/1000 - 9s - loss: 12.8712 - mse: 12.8712 - mae: 1.4625 - val_loss: 12.2054 - val_mse: 12.2054 - val_mae: 1.4425 - lr: 1.6779e-04 - 9s/epoch - 9ms/step\n",
            "Epoch 14/100\n",
            "1000/1000 - 9s - loss: 12.8324 - mse: 12.8324 - mae: 1.4560 - val_loss: 11.6956 - val_mse: 11.6956 - val_mae: 1.4620 - lr: 1.6779e-04 - 9s/epoch - 9ms/step\n",
            "Epoch 15/100\n",
            "1000/1000 - 9s - loss: 12.7443 - mse: 12.7443 - mae: 1.4539 - val_loss: 11.6547 - val_mse: 11.6547 - val_mae: 1.4785 - lr: 1.6779e-04 - 9s/epoch - 9ms/step\n",
            "Epoch 16/100\n",
            "1000/1000 - 9s - loss: 12.6137 - mse: 12.6137 - mae: 1.4438 - val_loss: 11.6833 - val_mse: 11.6833 - val_mae: 1.4742 - lr: 1.6779e-04 - 9s/epoch - 9ms/step\n",
            "Epoch 17/100\n",
            "1000/1000 - 9s - loss: 12.5290 - mse: 12.5290 - mae: 1.4421 - val_loss: 11.6161 - val_mse: 11.6161 - val_mae: 1.4961 - lr: 1.6779e-04 - 9s/epoch - 9ms/step\n",
            "Epoch 18/100\n",
            "1000/1000 - 9s - loss: 12.4539 - mse: 12.4539 - mae: 1.4341 - val_loss: 11.6664 - val_mse: 11.6664 - val_mae: 1.5012 - lr: 1.6779e-04 - 9s/epoch - 9ms/step\n",
            "Epoch 19/100\n",
            "1000/1000 - 9s - loss: 12.2773 - mse: 12.2773 - mae: 1.4254 - val_loss: 12.0484 - val_mse: 12.0484 - val_mae: 1.4831 - lr: 1.6779e-04 - 9s/epoch - 9ms/step\n",
            "Epoch 20/100\n",
            "1000/1000 - 9s - loss: 12.1673 - mse: 12.1673 - mae: 1.4108 - val_loss: 11.6237 - val_mse: 11.6237 - val_mae: 1.5035 - lr: 1.6779e-04 - 9s/epoch - 9ms/step\n",
            "Epoch 21/100\n",
            "1000/1000 - 9s - loss: 12.0258 - mse: 12.0258 - mae: 1.4042 - val_loss: 11.5726 - val_mse: 11.5726 - val_mae: 1.5370 - lr: 1.6779e-04 - 9s/epoch - 9ms/step\n",
            "Epoch 22/100\n",
            "1000/1000 - 9s - loss: 11.9552 - mse: 11.9552 - mae: 1.4027 - val_loss: 11.5291 - val_mse: 11.5291 - val_mae: 1.5456 - lr: 1.6779e-04 - 9s/epoch - 9ms/step\n",
            "Epoch 23/100\n",
            "1000/1000 - 9s - loss: 11.7525 - mse: 11.7525 - mae: 1.3893 - val_loss: 11.3529 - val_mse: 11.3529 - val_mae: 1.6106 - lr: 1.6779e-04 - 9s/epoch - 9ms/step\n",
            "Epoch 24/100\n",
            "1000/1000 - 9s - loss: 11.6257 - mse: 11.6257 - mae: 1.3780 - val_loss: 11.3067 - val_mse: 11.3067 - val_mae: 1.5042 - lr: 1.6779e-04 - 9s/epoch - 9ms/step\n",
            "Epoch 25/100\n",
            "1000/1000 - 9s - loss: 11.5156 - mse: 11.5156 - mae: 1.3651 - val_loss: 11.4033 - val_mse: 11.4033 - val_mae: 1.5333 - lr: 1.6779e-04 - 9s/epoch - 9ms/step\n",
            "Epoch 26/100\n",
            "1000/1000 - 9s - loss: 11.3439 - mse: 11.3439 - mae: 1.3583 - val_loss: 11.4850 - val_mse: 11.4850 - val_mae: 1.5314 - lr: 1.6779e-04 - 9s/epoch - 9ms/step\n",
            "Epoch 27/100\n",
            "1000/1000 - 9s - loss: 11.1285 - mse: 11.1285 - mae: 1.3446 - val_loss: 11.4445 - val_mse: 11.4445 - val_mae: 1.5764 - lr: 1.6779e-04 - 9s/epoch - 9ms/step\n",
            "Epoch 28/100\n",
            "1000/1000 - 9s - loss: 10.9654 - mse: 10.9654 - mae: 1.3373 - val_loss: 11.4464 - val_mse: 11.4464 - val_mae: 1.5734 - lr: 1.6779e-04 - 9s/epoch - 9ms/step\n",
            "Epoch 29/100\n",
            "1000/1000 - 9s - loss: 10.7867 - mse: 10.7867 - mae: 1.3212 - val_loss: 11.7020 - val_mse: 11.7020 - val_mae: 1.5383 - lr: 1.6779e-04 - 9s/epoch - 9ms/step\n",
            "Score for fold 1: loss of 11.70202922821045\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 9s - loss: 11.8584 - mse: 11.8584 - mae: 1.3755 - val_loss: 6.9007 - val_mse: 6.9007 - val_mae: 1.3187 - lr: 1.6779e-04 - 9s/epoch - 9ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 9s - loss: 11.5954 - mse: 11.5954 - mae: 1.3460 - val_loss: 6.8459 - val_mse: 6.8459 - val_mae: 1.3229 - lr: 1.6779e-04 - 9s/epoch - 9ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 9s - loss: 11.3355 - mse: 11.3355 - mae: 1.3311 - val_loss: 7.0587 - val_mse: 7.0587 - val_mae: 1.3607 - lr: 1.6779e-04 - 9s/epoch - 9ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 9s - loss: 11.1282 - mse: 11.1282 - mae: 1.3153 - val_loss: 7.1537 - val_mse: 7.1537 - val_mae: 1.3465 - lr: 1.6779e-04 - 9s/epoch - 9ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 9s - loss: 10.8037 - mse: 10.8037 - mae: 1.2974 - val_loss: 7.0141 - val_mse: 7.0141 - val_mae: 1.3563 - lr: 1.6779e-04 - 9s/epoch - 9ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 9s - loss: 10.5659 - mse: 10.5659 - mae: 1.2764 - val_loss: 7.1334 - val_mse: 7.1334 - val_mae: 1.3637 - lr: 1.6779e-04 - 9s/epoch - 9ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 9s - loss: 10.2697 - mse: 10.2697 - mae: 1.2582 - val_loss: 7.0789 - val_mse: 7.0789 - val_mae: 1.3886 - lr: 1.6779e-04 - 9s/epoch - 9ms/step\n",
            "Score for fold 2: loss of 7.078897953033447\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 9s - loss: 9.7678 - mse: 9.7678 - mae: 1.2924 - val_loss: 8.5485 - val_mse: 8.5485 - val_mae: 1.2718 - lr: 1.6779e-04 - 9s/epoch - 9ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 9s - loss: 9.3611 - mse: 9.3611 - mae: 1.2613 - val_loss: 8.9595 - val_mse: 8.9595 - val_mae: 1.2459 - lr: 1.6779e-04 - 9s/epoch - 9ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 9s - loss: 9.0925 - mse: 9.0925 - mae: 1.2351 - val_loss: 8.6050 - val_mse: 8.6050 - val_mae: 1.2739 - lr: 1.6779e-04 - 9s/epoch - 9ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 9s - loss: 8.9145 - mse: 8.9145 - mae: 1.2121 - val_loss: 8.7772 - val_mse: 8.7772 - val_mae: 1.2689 - lr: 1.6779e-04 - 9s/epoch - 9ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 9s - loss: 8.5058 - mse: 8.5058 - mae: 1.1879 - val_loss: 8.9663 - val_mse: 8.9663 - val_mae: 1.2882 - lr: 1.6779e-04 - 9s/epoch - 9ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 9s - loss: 8.1299 - mse: 8.1299 - mae: 1.1603 - val_loss: 9.1231 - val_mse: 9.1231 - val_mae: 1.3025 - lr: 1.6779e-04 - 9s/epoch - 9ms/step\n",
            "Score for fold 3: loss of 9.123144149780273\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 9s - loss: 7.8614 - mse: 7.8614 - mae: 1.1909 - val_loss: 9.4861 - val_mse: 9.4861 - val_mae: 1.1201 - lr: 1.6779e-04 - 9s/epoch - 9ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 9s - loss: 7.4898 - mse: 7.4898 - mae: 1.1532 - val_loss: 9.7441 - val_mse: 9.7441 - val_mae: 1.1699 - lr: 1.6779e-04 - 9s/epoch - 9ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 9s - loss: 7.1599 - mse: 7.1599 - mae: 1.1202 - val_loss: 9.6105 - val_mse: 9.6105 - val_mae: 1.1665 - lr: 1.6779e-04 - 9s/epoch - 9ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 9s - loss: 6.8381 - mse: 6.8381 - mae: 1.0874 - val_loss: 10.0377 - val_mse: 10.0377 - val_mae: 1.2178 - lr: 1.6779e-04 - 9s/epoch - 9ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 9s - loss: 6.6005 - mse: 6.6005 - mae: 1.0560 - val_loss: 9.9886 - val_mse: 9.9886 - val_mae: 1.1905 - lr: 1.6779e-04 - 9s/epoch - 9ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 9s - loss: 6.3515 - mse: 6.3515 - mae: 1.0277 - val_loss: 10.1166 - val_mse: 10.1166 - val_mae: 1.2542 - lr: 1.6779e-04 - 9s/epoch - 9ms/step\n",
            "Score for fold 4: loss of 10.116580963134766\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 9s - loss: 5.9952 - mse: 5.9952 - mae: 1.0759 - val_loss: 10.6128 - val_mse: 10.6128 - val_mae: 0.9867 - lr: 1.6779e-04 - 9s/epoch - 9ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 9s - loss: 5.5547 - mse: 5.5547 - mae: 1.0297 - val_loss: 10.7168 - val_mse: 10.7168 - val_mae: 1.0037 - lr: 1.6779e-04 - 9s/epoch - 9ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 9s - loss: 5.2733 - mse: 5.2733 - mae: 0.9922 - val_loss: 10.8392 - val_mse: 10.8392 - val_mae: 1.0184 - lr: 1.6779e-04 - 9s/epoch - 9ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 9s - loss: 5.0102 - mse: 5.0102 - mae: 0.9568 - val_loss: 11.0094 - val_mse: 11.0094 - val_mae: 1.0660 - lr: 1.6779e-04 - 9s/epoch - 9ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 9s - loss: 4.7199 - mse: 4.7199 - mae: 0.9225 - val_loss: 11.1003 - val_mse: 11.1003 - val_mae: 1.0765 - lr: 1.6779e-04 - 9s/epoch - 9ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 9s - loss: 4.4859 - mse: 4.4859 - mae: 0.8881 - val_loss: 11.1402 - val_mse: 11.1402 - val_mae: 1.1057 - lr: 1.6779e-04 - 9s/epoch - 9ms/step\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-10-12 13:47:27,837]\u001b[0m Finished trial#48 resulted in value: 9.831999999999999. Current best value is 9.440000000000001 with parameters: {'activation': 'relu', 'num_hidden_layer': 4, 'i': 9, 'learning_rate': 0.0001792631082155351}.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Score for fold 5: loss of 11.140243530273438\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 10s - loss: 15.1057 - mse: 15.1057 - mae: 1.6235 - val_loss: 11.3315 - val_mse: 11.3315 - val_mae: 1.4390 - lr: 1.6935e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 9s - loss: 14.1790 - mse: 14.1790 - mae: 1.5348 - val_loss: 11.3171 - val_mse: 11.3171 - val_mae: 1.5155 - lr: 1.6935e-04 - 9s/epoch - 9ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 9s - loss: 14.1101 - mse: 14.1101 - mae: 1.5245 - val_loss: 10.9418 - val_mse: 10.9418 - val_mae: 1.4901 - lr: 1.6935e-04 - 9s/epoch - 9ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 9s - loss: 13.9279 - mse: 13.9279 - mae: 1.5094 - val_loss: 11.2353 - val_mse: 11.2353 - val_mae: 1.4692 - lr: 1.6935e-04 - 9s/epoch - 9ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 9s - loss: 13.7782 - mse: 13.7782 - mae: 1.5032 - val_loss: 11.1967 - val_mse: 11.1967 - val_mae: 1.5033 - lr: 1.6935e-04 - 9s/epoch - 9ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 9s - loss: 13.6234 - mse: 13.6234 - mae: 1.4955 - val_loss: 11.0328 - val_mse: 11.0328 - val_mae: 1.4697 - lr: 1.6935e-04 - 9s/epoch - 9ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 9s - loss: 13.6669 - mse: 13.6669 - mae: 1.4953 - val_loss: 10.7003 - val_mse: 10.7003 - val_mae: 1.5236 - lr: 1.6935e-04 - 9s/epoch - 9ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 9s - loss: 13.5613 - mse: 13.5613 - mae: 1.4884 - val_loss: 11.0077 - val_mse: 11.0077 - val_mae: 1.5041 - lr: 1.6935e-04 - 9s/epoch - 9ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 9s - loss: 13.4450 - mse: 13.4450 - mae: 1.4816 - val_loss: 10.7845 - val_mse: 10.7845 - val_mae: 1.4851 - lr: 1.6935e-04 - 9s/epoch - 9ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 9s - loss: 13.3501 - mse: 13.3501 - mae: 1.4762 - val_loss: 10.7550 - val_mse: 10.7550 - val_mae: 1.4506 - lr: 1.6935e-04 - 9s/epoch - 9ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 9s - loss: 13.2789 - mse: 13.2789 - mae: 1.4745 - val_loss: 10.6191 - val_mse: 10.6191 - val_mae: 1.4721 - lr: 1.6935e-04 - 9s/epoch - 9ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 9s - loss: 13.2044 - mse: 13.2044 - mae: 1.4687 - val_loss: 10.5993 - val_mse: 10.5993 - val_mae: 1.4687 - lr: 1.6935e-04 - 9s/epoch - 9ms/step\n",
            "Epoch 13/100\n",
            "1000/1000 - 10s - loss: 13.1958 - mse: 13.1958 - mae: 1.4662 - val_loss: 10.6018 - val_mse: 10.6018 - val_mae: 1.5121 - lr: 1.6935e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 14/100\n",
            "1000/1000 - 10s - loss: 13.0205 - mse: 13.0205 - mae: 1.4563 - val_loss: 10.5422 - val_mse: 10.5422 - val_mae: 1.5225 - lr: 1.6935e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 15/100\n",
            "1000/1000 - 10s - loss: 12.9676 - mse: 12.9676 - mae: 1.4540 - val_loss: 10.5410 - val_mse: 10.5410 - val_mae: 1.4487 - lr: 1.6935e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 16/100\n",
            "1000/1000 - 10s - loss: 12.8637 - mse: 12.8637 - mae: 1.4490 - val_loss: 10.5514 - val_mse: 10.5514 - val_mae: 1.5184 - lr: 1.6935e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 17/100\n",
            "1000/1000 - 10s - loss: 12.7514 - mse: 12.7514 - mae: 1.4443 - val_loss: 10.7378 - val_mse: 10.7378 - val_mae: 1.5316 - lr: 1.6935e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 18/100\n",
            "1000/1000 - 10s - loss: 12.5903 - mse: 12.5903 - mae: 1.4336 - val_loss: 10.5211 - val_mse: 10.5211 - val_mae: 1.4807 - lr: 1.6935e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 19/100\n",
            "1000/1000 - 10s - loss: 12.5261 - mse: 12.5261 - mae: 1.4273 - val_loss: 10.5799 - val_mse: 10.5799 - val_mae: 1.4982 - lr: 1.6935e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 20/100\n",
            "1000/1000 - 10s - loss: 12.3264 - mse: 12.3264 - mae: 1.4200 - val_loss: 10.9041 - val_mse: 10.9041 - val_mae: 1.5063 - lr: 1.6935e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 21/100\n",
            "1000/1000 - 10s - loss: 12.2520 - mse: 12.2520 - mae: 1.4141 - val_loss: 10.4226 - val_mse: 10.4226 - val_mae: 1.4726 - lr: 1.6935e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 22/100\n",
            "1000/1000 - 10s - loss: 12.0870 - mse: 12.0870 - mae: 1.4032 - val_loss: 10.4957 - val_mse: 10.4957 - val_mae: 1.5355 - lr: 1.6935e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 23/100\n",
            "1000/1000 - 10s - loss: 11.9859 - mse: 11.9859 - mae: 1.3904 - val_loss: 10.3280 - val_mse: 10.3280 - val_mae: 1.4812 - lr: 1.6935e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 24/100\n",
            "1000/1000 - 10s - loss: 11.6906 - mse: 11.6906 - mae: 1.3800 - val_loss: 10.2875 - val_mse: 10.2875 - val_mae: 1.4858 - lr: 1.6935e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 25/100\n",
            "1000/1000 - 9s - loss: 11.5695 - mse: 11.5695 - mae: 1.3701 - val_loss: 10.3432 - val_mse: 10.3432 - val_mae: 1.5096 - lr: 1.6935e-04 - 9s/epoch - 9ms/step\n",
            "Epoch 26/100\n",
            "1000/1000 - 9s - loss: 11.4132 - mse: 11.4132 - mae: 1.3617 - val_loss: 10.3061 - val_mse: 10.3061 - val_mae: 1.5452 - lr: 1.6935e-04 - 9s/epoch - 9ms/step\n",
            "Epoch 27/100\n",
            "1000/1000 - 9s - loss: 11.1866 - mse: 11.1866 - mae: 1.3482 - val_loss: 10.1218 - val_mse: 10.1218 - val_mae: 1.5388 - lr: 1.6935e-04 - 9s/epoch - 9ms/step\n",
            "Epoch 28/100\n",
            "1000/1000 - 10s - loss: 10.9217 - mse: 10.9217 - mae: 1.3303 - val_loss: 10.2161 - val_mse: 10.2161 - val_mae: 1.5037 - lr: 1.6935e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 29/100\n",
            "1000/1000 - 9s - loss: 10.8160 - mse: 10.8160 - mae: 1.3168 - val_loss: 10.1822 - val_mse: 10.1822 - val_mae: 1.5831 - lr: 1.6935e-04 - 9s/epoch - 9ms/step\n",
            "Epoch 30/100\n",
            "1000/1000 - 9s - loss: 10.5359 - mse: 10.5359 - mae: 1.2992 - val_loss: 10.1879 - val_mse: 10.1879 - val_mae: 1.5270 - lr: 1.6935e-04 - 9s/epoch - 9ms/step\n",
            "Epoch 31/100\n",
            "1000/1000 - 9s - loss: 10.2982 - mse: 10.2982 - mae: 1.2902 - val_loss: 10.1363 - val_mse: 10.1363 - val_mae: 1.6252 - lr: 1.6935e-04 - 9s/epoch - 9ms/step\n",
            "Epoch 32/100\n",
            "1000/1000 - 9s - loss: 10.2207 - mse: 10.2207 - mae: 1.2779 - val_loss: 10.3091 - val_mse: 10.3091 - val_mae: 1.6073 - lr: 1.6935e-04 - 9s/epoch - 9ms/step\n",
            "Score for fold 1: loss of 10.309077262878418\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 9s - loss: 10.0036 - mse: 10.0036 - mae: 1.3491 - val_loss: 10.4716 - val_mse: 10.4716 - val_mae: 1.2425 - lr: 1.6935e-04 - 9s/epoch - 9ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 10s - loss: 9.5744 - mse: 9.5744 - mae: 1.3203 - val_loss: 10.7000 - val_mse: 10.7000 - val_mae: 1.2787 - lr: 1.6935e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 9s - loss: 9.3118 - mse: 9.3118 - mae: 1.2956 - val_loss: 10.6513 - val_mse: 10.6513 - val_mae: 1.3405 - lr: 1.6935e-04 - 9s/epoch - 9ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 9s - loss: 9.0320 - mse: 9.0320 - mae: 1.2731 - val_loss: 10.7684 - val_mse: 10.7684 - val_mae: 1.3358 - lr: 1.6935e-04 - 9s/epoch - 9ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 9s - loss: 8.7457 - mse: 8.7457 - mae: 1.2465 - val_loss: 10.8347 - val_mse: 10.8347 - val_mae: 1.3080 - lr: 1.6935e-04 - 9s/epoch - 9ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 9s - loss: 8.4281 - mse: 8.4281 - mae: 1.2229 - val_loss: 10.8382 - val_mse: 10.8382 - val_mae: 1.3313 - lr: 1.6935e-04 - 9s/epoch - 9ms/step\n",
            "Score for fold 2: loss of 10.838239669799805\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 9s - loss: 9.7217 - mse: 9.7217 - mae: 1.2548 - val_loss: 5.1701 - val_mse: 5.1701 - val_mae: 1.1551 - lr: 1.6935e-04 - 9s/epoch - 9ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 9s - loss: 9.3119 - mse: 9.3119 - mae: 1.2192 - val_loss: 5.3735 - val_mse: 5.3735 - val_mae: 1.1726 - lr: 1.6935e-04 - 9s/epoch - 9ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 9s - loss: 9.1308 - mse: 9.1308 - mae: 1.1893 - val_loss: 5.3495 - val_mse: 5.3495 - val_mae: 1.1805 - lr: 1.6935e-04 - 9s/epoch - 9ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 11s - loss: 8.7658 - mse: 8.7658 - mae: 1.1641 - val_loss: 5.5689 - val_mse: 5.5689 - val_mae: 1.2303 - lr: 1.6935e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 11s - loss: 8.3806 - mse: 8.3806 - mae: 1.1311 - val_loss: 5.4320 - val_mse: 5.4320 - val_mae: 1.2360 - lr: 1.6935e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 11s - loss: 8.1345 - mse: 8.1345 - mae: 1.1010 - val_loss: 5.5001 - val_mse: 5.5001 - val_mae: 1.2784 - lr: 1.6935e-04 - 11s/epoch - 11ms/step\n",
            "Score for fold 3: loss of 5.500119686126709\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 11s - loss: 6.0328 - mse: 6.0328 - mae: 1.1357 - val_loss: 13.2791 - val_mse: 13.2791 - val_mae: 1.0931 - lr: 1.6935e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 11s - loss: 5.6924 - mse: 5.6924 - mae: 1.0929 - val_loss: 13.6515 - val_mse: 13.6515 - val_mae: 1.0977 - lr: 1.6935e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 11s - loss: 5.3188 - mse: 5.3188 - mae: 1.0498 - val_loss: 13.5351 - val_mse: 13.5351 - val_mae: 1.1549 - lr: 1.6935e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 11s - loss: 5.0242 - mse: 5.0242 - mae: 1.0169 - val_loss: 13.7586 - val_mse: 13.7586 - val_mae: 1.1241 - lr: 1.6935e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 11s - loss: 4.7298 - mse: 4.7298 - mae: 0.9839 - val_loss: 13.6222 - val_mse: 13.6222 - val_mae: 1.1379 - lr: 1.6935e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 11s - loss: 4.5919 - mse: 4.5919 - mae: 0.9481 - val_loss: 13.7250 - val_mse: 13.7250 - val_mae: 1.1940 - lr: 1.6935e-04 - 11s/epoch - 11ms/step\n",
            "Score for fold 4: loss of 13.72500991821289\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 11s - loss: 7.0205 - mse: 7.0205 - mae: 1.0062 - val_loss: 3.3943 - val_mse: 3.3943 - val_mae: 0.8933 - lr: 1.6935e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 12s - loss: 6.7791 - mse: 6.7791 - mae: 0.9609 - val_loss: 3.4801 - val_mse: 3.4801 - val_mae: 0.9341 - lr: 1.6935e-04 - 12s/epoch - 12ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 12s - loss: 6.4139 - mse: 6.4139 - mae: 0.9235 - val_loss: 3.6425 - val_mse: 3.6425 - val_mae: 0.9471 - lr: 1.6935e-04 - 12s/epoch - 12ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 12s - loss: 6.1620 - mse: 6.1620 - mae: 0.8834 - val_loss: 3.7050 - val_mse: 3.7050 - val_mae: 0.9692 - lr: 1.6935e-04 - 12s/epoch - 12ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 12s - loss: 5.8821 - mse: 5.8821 - mae: 0.8493 - val_loss: 3.7858 - val_mse: 3.7858 - val_mae: 0.9919 - lr: 1.6935e-04 - 12s/epoch - 12ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 12s - loss: 5.7082 - mse: 5.7082 - mae: 0.8214 - val_loss: 3.7439 - val_mse: 3.7439 - val_mae: 1.0031 - lr: 1.6935e-04 - 12s/epoch - 12ms/step\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-10-12 13:56:50,693]\u001b[0m Finished trial#49 resulted in value: 8.824. Current best value is 8.824 with parameters: {'activation': 'tanh', 'num_hidden_layer': 4, 'i': 10, 'learning_rate': 0.0001693505815299836}.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Score for fold 5: loss of 3.743870258331299\n"
          ]
        }
      ],
      "source": [
        "model_list=[]\n",
        "history_list=[]\n",
        "study_name = 'NN_study_6'\n",
        "study = optuna.create_study(study_name=study_name, load_if_exists=True)\n",
        "func = lambda trial: objective(trial,training_shuffled5,labelsForTrain_shuffled5)\n",
        "study.optimize(func, n_trials=50,)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#{'activation': 'tanh', 'num_hidden_layer': 4, 'i': 10, 'learning_rate': 0.0001693505815299836}.\n",
        "optimizer = Adam(learning_rate=0.0001693505815299836 ,clipnorm=1.0)\n",
        "model_6 = create_model(activation=\"relu\",num_hidden_layer=4,num_hidden_unit=1024)\n",
        "\n",
        "es = EarlyStopping(monitor='mse', patience=5)\n",
        "reduce_lr = ReduceLROnPlateau(monitor='val_mse', factor=0.2,\n",
        "                              patience=20, min_lr=0.001)\n",
        "model_6.compile(loss='mse',optimizer=optimizer,metrics=['mse','mae'])\n",
        "history = model_6.fit(training,labelsForTrain,\n",
        "                batch_size=64,\n",
        "                epochs=20,\n",
        "                verbose=2,\n",
        "                validation_data=(valing,labelsForVal),\n",
        "                validation_batch_size=64,\n",
        "                callbacks=[es,reduce_lr])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wNz7DEVq6XI_",
        "outputId": "09d58377-f2a0-4af2-edac-959ba4c9630c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "1250/1250 - 10s - loss: 13.1178 - mse: 13.1178 - mae: 1.5348 - val_loss: 9.7526 - val_mse: 9.7526 - val_mae: 1.5831 - lr: 1.6935e-04 - 10s/epoch - 8ms/step\n",
            "Epoch 2/20\n",
            "1250/1250 - 9s - loss: 12.3713 - mse: 12.3713 - mae: 1.4843 - val_loss: 9.8513 - val_mse: 9.8513 - val_mae: 1.4229 - lr: 1.6935e-04 - 9s/epoch - 8ms/step\n",
            "Epoch 3/20\n",
            "1250/1250 - 10s - loss: 12.1781 - mse: 12.1781 - mae: 1.4706 - val_loss: 10.0224 - val_mse: 10.0224 - val_mae: 1.4299 - lr: 1.6935e-04 - 10s/epoch - 8ms/step\n",
            "Epoch 4/20\n",
            "1250/1250 - 10s - loss: 11.9600 - mse: 11.9600 - mae: 1.4556 - val_loss: 9.4870 - val_mse: 9.4870 - val_mae: 1.4765 - lr: 1.6935e-04 - 10s/epoch - 8ms/step\n",
            "Epoch 5/20\n",
            "1250/1250 - 10s - loss: 11.8970 - mse: 11.8970 - mae: 1.4517 - val_loss: 9.9998 - val_mse: 9.9998 - val_mae: 1.5009 - lr: 1.6935e-04 - 10s/epoch - 8ms/step\n",
            "Epoch 6/20\n",
            "1250/1250 - 10s - loss: 11.5142 - mse: 11.5142 - mae: 1.4394 - val_loss: 9.9970 - val_mse: 9.9970 - val_mae: 1.5335 - lr: 1.6935e-04 - 10s/epoch - 8ms/step\n",
            "Epoch 7/20\n",
            "1250/1250 - 10s - loss: 11.5375 - mse: 11.5375 - mae: 1.4313 - val_loss: 9.0981 - val_mse: 9.0981 - val_mae: 1.5423 - lr: 1.6935e-04 - 10s/epoch - 8ms/step\n",
            "Epoch 8/20\n",
            "1250/1250 - 10s - loss: 11.3020 - mse: 11.3020 - mae: 1.4167 - val_loss: 9.8123 - val_mse: 9.8123 - val_mae: 1.5536 - lr: 1.6935e-04 - 10s/epoch - 8ms/step\n",
            "Epoch 9/20\n",
            "1250/1250 - 10s - loss: 11.0872 - mse: 11.0872 - mae: 1.4114 - val_loss: 9.8149 - val_mse: 9.8149 - val_mae: 1.4427 - lr: 1.6935e-04 - 10s/epoch - 8ms/step\n",
            "Epoch 10/20\n",
            "1250/1250 - 9s - loss: 10.8065 - mse: 10.8065 - mae: 1.3979 - val_loss: 9.8013 - val_mse: 9.8013 - val_mae: 1.5185 - lr: 1.6935e-04 - 9s/epoch - 8ms/step\n",
            "Epoch 11/20\n",
            "1250/1250 - 10s - loss: 10.6785 - mse: 10.6785 - mae: 1.3863 - val_loss: 9.0435 - val_mse: 9.0435 - val_mae: 1.5368 - lr: 1.6935e-04 - 10s/epoch - 8ms/step\n",
            "Epoch 12/20\n",
            "1250/1250 - 10s - loss: 10.4757 - mse: 10.4757 - mae: 1.3717 - val_loss: 9.4553 - val_mse: 9.4553 - val_mae: 1.5087 - lr: 1.6935e-04 - 10s/epoch - 8ms/step\n",
            "Epoch 13/20\n",
            "1250/1250 - 10s - loss: 10.2073 - mse: 10.2073 - mae: 1.3603 - val_loss: 8.8601 - val_mse: 8.8601 - val_mae: 1.4820 - lr: 1.6935e-04 - 10s/epoch - 8ms/step\n",
            "Epoch 14/20\n",
            "1250/1250 - 10s - loss: 9.9886 - mse: 9.9886 - mae: 1.3458 - val_loss: 9.5686 - val_mse: 9.5686 - val_mae: 1.5258 - lr: 1.6935e-04 - 10s/epoch - 8ms/step\n",
            "Epoch 15/20\n",
            "1250/1250 - 10s - loss: 9.5963 - mse: 9.5963 - mae: 1.3298 - val_loss: 9.7077 - val_mse: 9.7077 - val_mae: 1.4939 - lr: 1.6935e-04 - 10s/epoch - 8ms/step\n",
            "Epoch 16/20\n",
            "1250/1250 - 10s - loss: 9.4125 - mse: 9.4125 - mae: 1.3151 - val_loss: 9.5768 - val_mse: 9.5768 - val_mae: 1.6030 - lr: 1.6935e-04 - 10s/epoch - 8ms/step\n",
            "Epoch 17/20\n",
            "1250/1250 - 10s - loss: 9.0774 - mse: 9.0774 - mae: 1.2943 - val_loss: 9.4146 - val_mse: 9.4146 - val_mae: 1.5424 - lr: 1.6935e-04 - 10s/epoch - 8ms/step\n",
            "Epoch 18/20\n",
            "1250/1250 - 10s - loss: 8.8377 - mse: 8.8377 - mae: 1.2767 - val_loss: 10.4869 - val_mse: 10.4869 - val_mae: 1.6323 - lr: 1.6935e-04 - 10s/epoch - 8ms/step\n",
            "Epoch 19/20\n",
            "1250/1250 - 10s - loss: 8.7273 - mse: 8.7273 - mae: 1.2671 - val_loss: 10.4626 - val_mse: 10.4626 - val_mae: 1.4840 - lr: 1.6935e-04 - 10s/epoch - 8ms/step\n",
            "Epoch 20/20\n",
            "1250/1250 - 10s - loss: 8.3438 - mse: 8.3438 - mae: 1.2471 - val_loss: 10.3846 - val_mse: 10.3846 - val_mae: 1.5224 - lr: 1.6935e-04 - 10s/epoch - 8ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "results_model6 = model_6.evaluate(testing, labelsForTest, batch_size=32)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cZfW5Qer8vPF",
        "outputId": "58899731-3e06-42c9-ebc3-55b201801546"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "313/313 [==============================] - 1s 2ms/step - loss: 11.1147 - mse: 11.1147 - mae: 1.5101\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The models performance on validation datasets:"
      ],
      "metadata": {
        "id": "xynvcNcc80kF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "MSE_val=np.array([9.667,9.731,10.4371,9.574,9.8652,10.3846])\n",
        "RMSE_val=np.sqrt(MSE_val)\n",
        "MAE_val=np.array([1.6457,1.4818,1.6127,1.5909,1.5739,1.5224])"
      ],
      "metadata": {
        "id": "auBWNvOY87OQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mean_RMSE_val=np.mean(RMSE_val)\n",
        "mean_MAE_val=np.mean(MAE_val)"
      ],
      "metadata": {
        "id": "R4BRo87p9sml"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(mean_RMSE_val)\n",
        "print(mean_MAE_val)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2xuTrDyj93hc",
        "outputId": "42e54691-48eb-42df-c5c6-1abc18177aeb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3.1528132276291814\n",
            "1.571233333333333\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(np.std(RMSE_val))\n",
        "print(np.std(MAE_val))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-cJ7KG1WGg6z",
        "outputId": "bf58bdf0-1fe5-4a91-9241-c7f2778ddcf4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.05402547257048609\n",
            "0.05483562305249713\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Combine the trainning dataset and validation dataset for training \"the best of best\" model"
      ],
      "metadata": {
        "id": "xbL-szW2CVS0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_val=pd.concat([train_df, val_df])"
      ],
      "metadata": {
        "id": "WlM_LkDuCdMh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_val_input,labelsFortrain_val=process_shuffle_dataset(train_val)"
      ],
      "metadata": {
        "id": "cb0W1elMG4Aa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_val_input"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "BGvn-JwKHIQv",
        "outputId": "e21aad14-8bb7-4955-fc23-d0f4d005c19a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "             0         1         2         3         4         5         6   \\\n",
              "0      0.902011  0.995920 -0.804756 -0.330617 -0.379517 -0.206158 -0.188756   \n",
              "1      0.902011  0.995920 -0.603161 -0.026909  0.119132 -1.204293 -0.111687   \n",
              "2      0.902011  0.995920  0.930712  0.589285  0.861888  2.393625  0.795618   \n",
              "3      0.253331  0.061782 -0.033437 -0.104152 -0.014398  2.512132  0.505354   \n",
              "4     -1.692709 -1.495115  3.542680  0.059113 -0.394122 -1.233808  0.157354   \n",
              "...         ...       ...       ...       ...       ...       ...       ...   \n",
              "89995 -0.395349 -0.664770 -0.883641 -0.330617 -0.358653 -0.920665 -0.188756   \n",
              "89996  0.902011  0.995920  0.781707  0.194289 -0.068644 -0.004835 -0.056484   \n",
              "89997  0.253331  0.061782  0.483698  0.066135  1.788247  0.171983 -0.123963   \n",
              "89998  0.253331  0.061782 -0.243797  0.417242  1.992714  2.826385 -0.065426   \n",
              "89999  0.253331  0.061782  1.517967 -0.132241  0.271438  0.244796  0.035134   \n",
              "\n",
              "             7         8         9   ...        25        26        27  \\\n",
              "0     -0.610631 -0.172660  1.410127  ...  1.010500  0.728954  1.191707   \n",
              "1      1.015724  0.012508  0.583623  ...  0.362933 -1.006542  0.347735   \n",
              "2      3.292620  0.218251  0.535006  ... -1.024711 -0.059908  1.191707   \n",
              "3      1.015724  0.005650 -0.084872  ... -1.487259  0.728954 -0.496237   \n",
              "4     -0.610631 -0.172660 -3.305804  ... -2.689883  0.886726  1.191707   \n",
              "...         ...       ...       ...  ...       ...       ...       ...   \n",
              "89995 -0.610631 -0.165802 -0.024100  ...  0.640461 -0.533225 -0.496237   \n",
              "89996  0.039911 -0.124653  0.413461  ... -0.562163  0.571181  1.191707   \n",
              "89997  0.690453  0.122238  0.401307  ... -0.284634 -0.217680  0.347735   \n",
              "89998  3.292620  0.499433  0.510697  ...  0.270423 -0.533225 -0.496237   \n",
              "89999  1.991536  0.060515 -0.558896  ... -0.747182  0.097864  2.879650   \n",
              "\n",
              "             28        29        30        31        32        33        34  \n",
              "0     -0.797708 -0.915846 -0.878536 -0.600577 -0.037368 -0.012793 -0.565006  \n",
              "1     -0.567887 -0.699348 -0.878536 -0.070421 -0.037368 -0.012793  0.222404  \n",
              "2      1.270684  0.491392  1.120411  0.636455 -0.037368 -0.012793 -0.171301  \n",
              "3      0.305434  0.058396 -0.434325 -0.423858 -0.037368 -0.012793 -0.565006  \n",
              "4      3.109255  1.249135  3.563567  3.817395 -0.037368 -0.012793 -0.565006  \n",
              "...         ...       ...       ...       ...       ...       ...       ...  \n",
              "89995 -0.935601 -0.591099 -0.767483 -0.600577 -0.037368 -0.012793 -0.565006  \n",
              "89996  0.489291  0.707890  1.342516  0.283017 -0.037368 -0.012793  0.222404  \n",
              "89997  1.086827  0.924388  0.343043 -0.423858 -0.037368 -0.012793 -0.171301  \n",
              "89998 -0.154209  0.383143  0.343043 -0.070421 -0.037368 -0.012793 -0.565006  \n",
              "89999  2.052077  2.439875  0.565148  0.636455 -0.037368 -0.012793 -0.565006  \n",
              "\n",
              "[90000 rows x 35 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-1f3a415a-ccd1-4def-84af-736ffc950214\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>...</th>\n",
              "      <th>25</th>\n",
              "      <th>26</th>\n",
              "      <th>27</th>\n",
              "      <th>28</th>\n",
              "      <th>29</th>\n",
              "      <th>30</th>\n",
              "      <th>31</th>\n",
              "      <th>32</th>\n",
              "      <th>33</th>\n",
              "      <th>34</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.902011</td>\n",
              "      <td>0.995920</td>\n",
              "      <td>-0.804756</td>\n",
              "      <td>-0.330617</td>\n",
              "      <td>-0.379517</td>\n",
              "      <td>-0.206158</td>\n",
              "      <td>-0.188756</td>\n",
              "      <td>-0.610631</td>\n",
              "      <td>-0.172660</td>\n",
              "      <td>1.410127</td>\n",
              "      <td>...</td>\n",
              "      <td>1.010500</td>\n",
              "      <td>0.728954</td>\n",
              "      <td>1.191707</td>\n",
              "      <td>-0.797708</td>\n",
              "      <td>-0.915846</td>\n",
              "      <td>-0.878536</td>\n",
              "      <td>-0.600577</td>\n",
              "      <td>-0.037368</td>\n",
              "      <td>-0.012793</td>\n",
              "      <td>-0.565006</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.902011</td>\n",
              "      <td>0.995920</td>\n",
              "      <td>-0.603161</td>\n",
              "      <td>-0.026909</td>\n",
              "      <td>0.119132</td>\n",
              "      <td>-1.204293</td>\n",
              "      <td>-0.111687</td>\n",
              "      <td>1.015724</td>\n",
              "      <td>0.012508</td>\n",
              "      <td>0.583623</td>\n",
              "      <td>...</td>\n",
              "      <td>0.362933</td>\n",
              "      <td>-1.006542</td>\n",
              "      <td>0.347735</td>\n",
              "      <td>-0.567887</td>\n",
              "      <td>-0.699348</td>\n",
              "      <td>-0.878536</td>\n",
              "      <td>-0.070421</td>\n",
              "      <td>-0.037368</td>\n",
              "      <td>-0.012793</td>\n",
              "      <td>0.222404</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.902011</td>\n",
              "      <td>0.995920</td>\n",
              "      <td>0.930712</td>\n",
              "      <td>0.589285</td>\n",
              "      <td>0.861888</td>\n",
              "      <td>2.393625</td>\n",
              "      <td>0.795618</td>\n",
              "      <td>3.292620</td>\n",
              "      <td>0.218251</td>\n",
              "      <td>0.535006</td>\n",
              "      <td>...</td>\n",
              "      <td>-1.024711</td>\n",
              "      <td>-0.059908</td>\n",
              "      <td>1.191707</td>\n",
              "      <td>1.270684</td>\n",
              "      <td>0.491392</td>\n",
              "      <td>1.120411</td>\n",
              "      <td>0.636455</td>\n",
              "      <td>-0.037368</td>\n",
              "      <td>-0.012793</td>\n",
              "      <td>-0.171301</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.253331</td>\n",
              "      <td>0.061782</td>\n",
              "      <td>-0.033437</td>\n",
              "      <td>-0.104152</td>\n",
              "      <td>-0.014398</td>\n",
              "      <td>2.512132</td>\n",
              "      <td>0.505354</td>\n",
              "      <td>1.015724</td>\n",
              "      <td>0.005650</td>\n",
              "      <td>-0.084872</td>\n",
              "      <td>...</td>\n",
              "      <td>-1.487259</td>\n",
              "      <td>0.728954</td>\n",
              "      <td>-0.496237</td>\n",
              "      <td>0.305434</td>\n",
              "      <td>0.058396</td>\n",
              "      <td>-0.434325</td>\n",
              "      <td>-0.423858</td>\n",
              "      <td>-0.037368</td>\n",
              "      <td>-0.012793</td>\n",
              "      <td>-0.565006</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>-1.692709</td>\n",
              "      <td>-1.495115</td>\n",
              "      <td>3.542680</td>\n",
              "      <td>0.059113</td>\n",
              "      <td>-0.394122</td>\n",
              "      <td>-1.233808</td>\n",
              "      <td>0.157354</td>\n",
              "      <td>-0.610631</td>\n",
              "      <td>-0.172660</td>\n",
              "      <td>-3.305804</td>\n",
              "      <td>...</td>\n",
              "      <td>-2.689883</td>\n",
              "      <td>0.886726</td>\n",
              "      <td>1.191707</td>\n",
              "      <td>3.109255</td>\n",
              "      <td>1.249135</td>\n",
              "      <td>3.563567</td>\n",
              "      <td>3.817395</td>\n",
              "      <td>-0.037368</td>\n",
              "      <td>-0.012793</td>\n",
              "      <td>-0.565006</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>89995</th>\n",
              "      <td>-0.395349</td>\n",
              "      <td>-0.664770</td>\n",
              "      <td>-0.883641</td>\n",
              "      <td>-0.330617</td>\n",
              "      <td>-0.358653</td>\n",
              "      <td>-0.920665</td>\n",
              "      <td>-0.188756</td>\n",
              "      <td>-0.610631</td>\n",
              "      <td>-0.165802</td>\n",
              "      <td>-0.024100</td>\n",
              "      <td>...</td>\n",
              "      <td>0.640461</td>\n",
              "      <td>-0.533225</td>\n",
              "      <td>-0.496237</td>\n",
              "      <td>-0.935601</td>\n",
              "      <td>-0.591099</td>\n",
              "      <td>-0.767483</td>\n",
              "      <td>-0.600577</td>\n",
              "      <td>-0.037368</td>\n",
              "      <td>-0.012793</td>\n",
              "      <td>-0.565006</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>89996</th>\n",
              "      <td>0.902011</td>\n",
              "      <td>0.995920</td>\n",
              "      <td>0.781707</td>\n",
              "      <td>0.194289</td>\n",
              "      <td>-0.068644</td>\n",
              "      <td>-0.004835</td>\n",
              "      <td>-0.056484</td>\n",
              "      <td>0.039911</td>\n",
              "      <td>-0.124653</td>\n",
              "      <td>0.413461</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.562163</td>\n",
              "      <td>0.571181</td>\n",
              "      <td>1.191707</td>\n",
              "      <td>0.489291</td>\n",
              "      <td>0.707890</td>\n",
              "      <td>1.342516</td>\n",
              "      <td>0.283017</td>\n",
              "      <td>-0.037368</td>\n",
              "      <td>-0.012793</td>\n",
              "      <td>0.222404</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>89997</th>\n",
              "      <td>0.253331</td>\n",
              "      <td>0.061782</td>\n",
              "      <td>0.483698</td>\n",
              "      <td>0.066135</td>\n",
              "      <td>1.788247</td>\n",
              "      <td>0.171983</td>\n",
              "      <td>-0.123963</td>\n",
              "      <td>0.690453</td>\n",
              "      <td>0.122238</td>\n",
              "      <td>0.401307</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.284634</td>\n",
              "      <td>-0.217680</td>\n",
              "      <td>0.347735</td>\n",
              "      <td>1.086827</td>\n",
              "      <td>0.924388</td>\n",
              "      <td>0.343043</td>\n",
              "      <td>-0.423858</td>\n",
              "      <td>-0.037368</td>\n",
              "      <td>-0.012793</td>\n",
              "      <td>-0.171301</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>89998</th>\n",
              "      <td>0.253331</td>\n",
              "      <td>0.061782</td>\n",
              "      <td>-0.243797</td>\n",
              "      <td>0.417242</td>\n",
              "      <td>1.992714</td>\n",
              "      <td>2.826385</td>\n",
              "      <td>-0.065426</td>\n",
              "      <td>3.292620</td>\n",
              "      <td>0.499433</td>\n",
              "      <td>0.510697</td>\n",
              "      <td>...</td>\n",
              "      <td>0.270423</td>\n",
              "      <td>-0.533225</td>\n",
              "      <td>-0.496237</td>\n",
              "      <td>-0.154209</td>\n",
              "      <td>0.383143</td>\n",
              "      <td>0.343043</td>\n",
              "      <td>-0.070421</td>\n",
              "      <td>-0.037368</td>\n",
              "      <td>-0.012793</td>\n",
              "      <td>-0.565006</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>89999</th>\n",
              "      <td>0.253331</td>\n",
              "      <td>0.061782</td>\n",
              "      <td>1.517967</td>\n",
              "      <td>-0.132241</td>\n",
              "      <td>0.271438</td>\n",
              "      <td>0.244796</td>\n",
              "      <td>0.035134</td>\n",
              "      <td>1.991536</td>\n",
              "      <td>0.060515</td>\n",
              "      <td>-0.558896</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.747182</td>\n",
              "      <td>0.097864</td>\n",
              "      <td>2.879650</td>\n",
              "      <td>2.052077</td>\n",
              "      <td>2.439875</td>\n",
              "      <td>0.565148</td>\n",
              "      <td>0.636455</td>\n",
              "      <td>-0.037368</td>\n",
              "      <td>-0.012793</td>\n",
              "      <td>-0.565006</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>90000 rows × 35 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-1f3a415a-ccd1-4def-84af-736ffc950214')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-1f3a415a-ccd1-4def-84af-736ffc950214 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-1f3a415a-ccd1-4def-84af-736ffc950214');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#{'activation': 'relu', 'num_hidden_layer': 4, 'i': 10, 'learning_rate': 0.00015658492778156685}.\n",
        "from keras.utils.vis_utils import plot_model\n",
        "optimizer = Adam(learning_rate=0.00015658492778156685 ,clipnorm=1.0)\n",
        "best_model = create_model(activation=\"relu\",num_hidden_layer=4,num_hidden_unit=1024)\n",
        "\n",
        "es = EarlyStopping(monitor='val_mse', patience=2)\n",
        "reduce_lr = ReduceLROnPlateau(monitor='mse', factor=0.2,\n",
        "                              patience=100, min_lr=0.001)\n",
        "best_model.compile(loss='mse',optimizer=optimizer,metrics=['mse','mae'])\n",
        "plot_model(best_model,show_shapes=True, show_layer_names=True)\n",
        "history = best_model.fit(train_val_input,labelsFortrain_val,\n",
        "                batch_size=64,\n",
        "                epochs=100,\n",
        "                verbose=2,\n",
        "                validation_data=(testing,labelsForTest),\n",
        "                validation_batch_size=64,\n",
        "                callbacks=[es,reduce_lr])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XyM66ov2IUCO",
        "outputId": "f347737e-557b-4599-d6de-188402d7532e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "1407/1407 - 13s - loss: 12.5738 - mse: 12.5738 - mae: 1.5308 - val_loss: 13.2951 - val_mse: 13.2951 - val_mae: 1.5871 - lr: 1.5658e-04 - 13s/epoch - 9ms/step\n",
            "Epoch 2/100\n",
            "1407/1407 - 15s - loss: 12.0881 - mse: 12.0881 - mae: 1.4782 - val_loss: 10.8713 - val_mse: 10.8713 - val_mae: 1.5099 - lr: 1.5658e-04 - 15s/epoch - 11ms/step\n",
            "Epoch 3/100\n",
            "1407/1407 - 19s - loss: 12.1061 - mse: 12.1061 - mae: 1.4758 - val_loss: 11.0676 - val_mse: 11.0676 - val_mae: 1.4263 - lr: 1.5658e-04 - 19s/epoch - 13ms/step\n",
            "Epoch 4/100\n",
            "1407/1407 - 19s - loss: 11.7164 - mse: 11.7164 - mae: 1.4634 - val_loss: 11.0235 - val_mse: 11.0235 - val_mae: 1.4513 - lr: 1.5658e-04 - 19s/epoch - 14ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plot_model(best_model,show_shapes=True, show_layer_names=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 533
        },
        "id": "MFSBoSm22rsT",
        "outputId": "89423789-6ecd-4b08-f5cf-3f76e3df2ecb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "results_best_model = best_model.evaluate(testing, labelsForTest, batch_size=32)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2x9tPx7KIs_s",
        "outputId": "fd2d2dd0-a4cc-47c8-e7bc-bcd848c50e7f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "313/313 [==============================] - 1s 4ms/step - loss: 11.0235 - mse: 11.0235 - mae: 1.4513\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "collapsed_sections": [],
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}