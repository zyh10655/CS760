{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "ZBJ7gOb3wO1h",
        "outputId": "4372e138-28ce-46f9-835e-1c6b72840119"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting optuna==0.14.0\n",
            "  Downloading optuna-0.14.0.tar.gz (91 kB)\n",
            "\u001b[K     |████████████████████████████████| 91 kB 3.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: sqlalchemy>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from optuna==0.14.0) (1.4.41)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from optuna==0.14.0) (1.21.6)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from optuna==0.14.0) (1.7.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from optuna==0.14.0) (1.15.0)\n",
            "Collecting cliff\n",
            "  Downloading cliff-3.10.1-py3-none-any.whl (81 kB)\n",
            "\u001b[K     |████████████████████████████████| 81 kB 8.0 MB/s \n",
            "\u001b[?25hCollecting colorlog\n",
            "  Downloading colorlog-6.7.0-py2.py3-none-any.whl (11 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from optuna==0.14.0) (1.3.5)\n",
            "Collecting alembic\n",
            "  Downloading alembic-1.8.1-py3-none-any.whl (209 kB)\n",
            "\u001b[K     |████████████████████████████████| 209 kB 55.7 MB/s \n",
            "\u001b[?25hCollecting typing\n",
            "  Downloading typing-3.7.4.3.tar.gz (78 kB)\n",
            "\u001b[K     |████████████████████████████████| 78 kB 7.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from sqlalchemy>=1.1.0->optuna==0.14.0) (5.0.0)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.7/dist-packages (from sqlalchemy>=1.1.0->optuna==0.14.0) (1.1.3)\n",
            "Collecting Mako\n",
            "  Downloading Mako-1.2.3-py3-none-any.whl (78 kB)\n",
            "\u001b[K     |████████████████████████████████| 78 kB 7.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: importlib-resources in /usr/local/lib/python3.7/dist-packages (from alembic->optuna==0.14.0) (5.9.0)\n",
            "Requirement already satisfied: pyparsing>=2.1.0 in /usr/local/lib/python3.7/dist-packages (from cliff->optuna==0.14.0) (3.0.9)\n",
            "Collecting autopage>=0.4.0\n",
            "  Downloading autopage-0.5.1-py3-none-any.whl (29 kB)\n",
            "Requirement already satisfied: PrettyTable>=0.7.2 in /usr/local/lib/python3.7/dist-packages (from cliff->optuna==0.14.0) (3.4.1)\n",
            "Collecting stevedore>=2.0.1\n",
            "  Downloading stevedore-3.5.0-py3-none-any.whl (49 kB)\n",
            "\u001b[K     |████████████████████████████████| 49 kB 5.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: PyYAML>=3.12 in /usr/local/lib/python3.7/dist-packages (from cliff->optuna==0.14.0) (6.0)\n",
            "Collecting cmd2>=1.0.0\n",
            "  Downloading cmd2-2.4.2-py3-none-any.whl (147 kB)\n",
            "\u001b[K     |████████████████████████████████| 147 kB 65.9 MB/s \n",
            "\u001b[?25hCollecting pbr!=2.1.0,>=2.0.0\n",
            "  Downloading pbr-5.10.0-py2.py3-none-any.whl (112 kB)\n",
            "\u001b[K     |████████████████████████████████| 112 kB 67.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: wcwidth>=0.1.7 in /usr/local/lib/python3.7/dist-packages (from cmd2>=1.0.0->cliff->optuna==0.14.0) (0.2.5)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from cmd2>=1.0.0->cliff->optuna==0.14.0) (4.1.1)\n",
            "Collecting pyperclip>=1.6\n",
            "  Downloading pyperclip-1.8.2.tar.gz (20 kB)\n",
            "Requirement already satisfied: attrs>=16.3.0 in /usr/local/lib/python3.7/dist-packages (from cmd2>=1.0.0->cliff->optuna==0.14.0) (22.1.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->sqlalchemy>=1.1.0->optuna==0.14.0) (3.8.1)\n",
            "Requirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.7/dist-packages (from Mako->alembic->optuna==0.14.0) (2.0.1)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas->optuna==0.14.0) (2022.4)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->optuna==0.14.0) (2.8.2)\n",
            "Building wheels for collected packages: optuna, pyperclip, typing\n",
            "  Building wheel for optuna (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for optuna: filename=optuna-0.14.0-py3-none-any.whl size=125709 sha256=5b2309556e9edd9a3255913f8c2898fad8e7e79746d2c581437bb244b3733df4\n",
            "  Stored in directory: /root/.cache/pip/wheels/c1/41/64/03b183676c5d5e978de160cab6268d5b4fb095dff63f720e01\n",
            "  Building wheel for pyperclip (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyperclip: filename=pyperclip-1.8.2-py3-none-any.whl size=11137 sha256=a609ece9b60e063080931dc671f8139217858f40fcef9d7df71c49e1af3e9648\n",
            "  Stored in directory: /root/.cache/pip/wheels/9f/18/84/8f69f8b08169c7bae2dde6bd7daf0c19fca8c8e500ee620a28\n",
            "  Building wheel for typing (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for typing: filename=typing-3.7.4.3-py3-none-any.whl size=26325 sha256=d6476ca90ec20a4ca6d7e39ad6d1004457af1868910f124d5b0d7f0a0aecc4a0\n",
            "  Stored in directory: /root/.cache/pip/wheels/35/f3/15/01aa6571f0a72ee6ae7b827c1491c37a1f72d686fd22b43b0e\n",
            "Successfully built optuna pyperclip typing\n",
            "Installing collected packages: pyperclip, pbr, stevedore, Mako, cmd2, autopage, typing, colorlog, cliff, alembic, optuna\n",
            "Successfully installed Mako-1.2.3 alembic-1.8.1 autopage-0.5.1 cliff-3.10.1 cmd2-2.4.2 colorlog-6.7.0 optuna-0.14.0 pbr-5.10.0 pyperclip-1.8.2 stevedore-3.5.0 typing-3.7.4.3\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "typing"
                ]
              }
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting optkeras==0.0.7\n",
            "  Downloading optkeras-0.0.7-py3-none-any.whl (6.9 kB)\n",
            "Requirement already satisfied: optuna>=0.9.0 in /usr/local/lib/python3.7/dist-packages (from optkeras==0.0.7) (0.14.0)\n",
            "Requirement already satisfied: keras in /usr/local/lib/python3.7/dist-packages (from optkeras==0.0.7) (2.8.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from optkeras==0.0.7) (1.21.6)\n",
            "Requirement already satisfied: colorlog in /usr/local/lib/python3.7/dist-packages (from optuna>=0.9.0->optkeras==0.0.7) (6.7.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from optuna>=0.9.0->optkeras==0.0.7) (1.7.3)\n",
            "Requirement already satisfied: typing in /usr/local/lib/python3.7/dist-packages (from optuna>=0.9.0->optkeras==0.0.7) (3.7.4.3)\n",
            "Requirement already satisfied: cliff in /usr/local/lib/python3.7/dist-packages (from optuna>=0.9.0->optkeras==0.0.7) (3.10.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from optuna>=0.9.0->optkeras==0.0.7) (1.3.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from optuna>=0.9.0->optkeras==0.0.7) (1.15.0)\n",
            "Requirement already satisfied: sqlalchemy>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from optuna>=0.9.0->optkeras==0.0.7) (1.4.41)\n",
            "Requirement already satisfied: alembic in /usr/local/lib/python3.7/dist-packages (from optuna>=0.9.0->optkeras==0.0.7) (1.8.1)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.7/dist-packages (from sqlalchemy>=1.1.0->optuna>=0.9.0->optkeras==0.0.7) (1.1.3)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from sqlalchemy>=1.1.0->optuna>=0.9.0->optkeras==0.0.7) (5.0.0)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.7/dist-packages (from alembic->optuna>=0.9.0->optkeras==0.0.7) (5.9.0)\n",
            "Requirement already satisfied: Mako in /usr/local/lib/python3.7/dist-packages (from alembic->optuna>=0.9.0->optkeras==0.0.7) (1.2.3)\n",
            "Requirement already satisfied: cmd2>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from cliff->optuna>=0.9.0->optkeras==0.0.7) (2.4.2)\n",
            "Requirement already satisfied: PyYAML>=3.12 in /usr/local/lib/python3.7/dist-packages (from cliff->optuna>=0.9.0->optkeras==0.0.7) (6.0)\n",
            "Requirement already satisfied: PrettyTable>=0.7.2 in /usr/local/lib/python3.7/dist-packages (from cliff->optuna>=0.9.0->optkeras==0.0.7) (3.4.1)\n",
            "Requirement already satisfied: pyparsing>=2.1.0 in /usr/local/lib/python3.7/dist-packages (from cliff->optuna>=0.9.0->optkeras==0.0.7) (3.0.9)\n",
            "Requirement already satisfied: stevedore>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from cliff->optuna>=0.9.0->optkeras==0.0.7) (3.5.0)\n",
            "Requirement already satisfied: pbr!=2.1.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from cliff->optuna>=0.9.0->optkeras==0.0.7) (5.10.0)\n",
            "Requirement already satisfied: autopage>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from cliff->optuna>=0.9.0->optkeras==0.0.7) (0.5.1)\n",
            "Requirement already satisfied: attrs>=16.3.0 in /usr/local/lib/python3.7/dist-packages (from cmd2>=1.0.0->cliff->optuna>=0.9.0->optkeras==0.0.7) (22.1.0)\n",
            "Requirement already satisfied: wcwidth>=0.1.7 in /usr/local/lib/python3.7/dist-packages (from cmd2>=1.0.0->cliff->optuna>=0.9.0->optkeras==0.0.7) (0.2.5)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from cmd2>=1.0.0->cliff->optuna>=0.9.0->optkeras==0.0.7) (4.1.1)\n",
            "Requirement already satisfied: pyperclip>=1.6 in /usr/local/lib/python3.7/dist-packages (from cmd2>=1.0.0->cliff->optuna>=0.9.0->optkeras==0.0.7) (1.8.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->sqlalchemy>=1.1.0->optuna>=0.9.0->optkeras==0.0.7) (3.8.1)\n",
            "Requirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.7/dist-packages (from Mako->alembic->optuna>=0.9.0->optkeras==0.0.7) (2.0.1)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas->optuna>=0.9.0->optkeras==0.0.7) (2022.4)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->optuna>=0.9.0->optkeras==0.0.7) (2.8.2)\n",
            "Installing collected packages: optkeras\n",
            "Successfully installed optkeras-0.0.7\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting fastparquet\n",
            "  Downloading fastparquet-0.8.1-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.5 MB 5.0 MB/s \n",
            "\u001b[?25hCollecting cramjam>=2.3.0\n",
            "  Downloading cramjam-2.5.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.whl (1.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.6 MB 60.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.18 in /usr/local/lib/python3.7/dist-packages (from fastparquet) (1.21.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.7/dist-packages (from fastparquet) (2022.8.2)\n",
            "Requirement already satisfied: pandas>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from fastparquet) (1.3.5)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=1.1.0->fastparquet) (2022.4)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=1.1.0->fastparquet) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas>=1.1.0->fastparquet) (1.15.0)\n",
            "Installing collected packages: cramjam, fastparquet\n",
            "Successfully installed cramjam-2.5.0 fastparquet-0.8.1\n"
          ]
        }
      ],
      "source": [
        "!pip3 install optuna==0.14.0\n",
        "!pip install fastparquet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MKNp87O8w6mp"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import keras\n",
        "import tensorflow as tf\n",
        "\n",
        "from keras.datasets import mnist\n",
        "from keras.models import Sequential,Model\n",
        "from keras.layers import Activation, Flatten, Dense, Conv2D, Conv1D,Input\n",
        "from keras.layers import MaxPooling1D, Dropout, BatchNormalization\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.optimizers import SGD, Adagrad, RMSprop, Adam, Adadelta, Adamax, Nadam\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.impute import SimpleImputer\n",
        "from keras.layers.core import *\n",
        "from keras.callbacks import EarlyStopping\n",
        "from keras.callbacks import ReduceLROnPlateau\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "import keras.backend as K\n",
        "import optuna\n",
        "import math\n",
        "import random\n",
        "from copy import deepcopy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KwrJMH1Iw97N",
        "outputId": "0b8545b8-90d4-46ec-cd8a-d6a743d1f173"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Keras 2.8.0\n",
            "TensorFlow 2.8.2\n",
            "Optuna 0.14.0\n"
          ]
        }
      ],
      "source": [
        "print('Keras', keras.__version__)\n",
        "print('TensorFlow', tf.__version__)\n",
        "# import Optuna and OptKeras after Keras\n",
        "print('Optuna', optuna.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WoDlc6e0xKMt"
      },
      "outputs": [],
      "source": [
        "train_df = pd.read_parquet(\"/content/drive/MyDrive/New Data/18features/18_train_main.parquet.snappy\",engine='fastparquet')\n",
        "#subsample for nest CV\n",
        "np.random.seed(760)\n",
        "train_df=train_df.loc[np.random.choice(train_df.index, 80000, replace=False)].reset_index()\n",
        "test_df = pd.read_parquet(\"/content/drive/MyDrive/New Data/18features/18_test_main.parquet.snappy\",engine='fastparquet')\n",
        "test_df=test_df.loc[np.random.choice(test_df.index, 20000, replace=False)].reset_index()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0mt78fdMbRAV",
        "outputId": "51ac3a8a-8cf0-473c-daf6-c4f5b254af98"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "             0    1     2      3      4      5          6         7    8   \\\n",
            "0      268123.0  5.0  25.0  133.0  418.0  630.0  72.978978  0.400000  8.0   \n",
            "1      340360.0  1.0   1.0  425.0   93.0   17.0  57.201016  0.000000  0.0   \n",
            "2      245244.0  4.0  16.0   78.0  375.0  506.0  63.609630  1.705534  8.0   \n",
            "3      135424.0  4.0  16.0  114.0   66.0  196.0  76.291694  0.535714  3.0   \n",
            "4      267072.0  1.0   1.0  225.0   16.0    1.0  29.199427  0.000000  0.0   \n",
            "...         ...  ...   ...    ...    ...    ...        ...       ...  ...   \n",
            "79995  214952.0  5.0  25.0   46.0    4.0   95.0  64.888960  0.884211  1.0   \n",
            "79996  181917.0  1.0   1.0   30.0  243.0    6.0   0.000004  0.000000  0.0   \n",
            "79997  185008.0  5.0  25.0  170.0  188.0  436.0  58.579512  0.924312  7.0   \n",
            "79998  247910.0  2.0   4.0  280.0  103.0  137.0  16.853052  0.613139  2.0   \n",
            "79999   84136.0  2.0   4.0  116.0    1.0    9.0  24.637658  0.000000  0.0   \n",
            "\n",
            "         9     10      11   12      13   14     15        16        17     18  \\\n",
            "0      48.0  4.05  1526.0  3.5    67.0  7.0   53.0  0.135302  0.657992  83.36   \n",
            "1       0.0  4.39     8.0  4.0  7400.0  7.0  119.0 -0.029082  0.376052  83.29   \n",
            "2      68.0  3.63  2036.0  4.5   298.0  5.0   30.0  0.382812  0.565972  75.40   \n",
            "3       7.0  3.41   532.0  4.5    17.0  7.0   61.0  0.165000  0.413333  69.41   \n",
            "4       0.0  1.00     2.0  3.0   393.0  7.0   77.0 -0.179487  0.310490  84.91   \n",
            "...     ...   ...     ...  ...     ...  ...    ...       ...       ...    ...   \n",
            "79995   4.0  4.67   129.0  4.5   730.0  6.0   42.5  0.500000  0.686250  81.70   \n",
            "79996   0.0  3.67     2.0  4.0  1147.0  7.0   92.0 -0.112857  0.397143  73.68   \n",
            "79997  23.0  3.68  1123.0  4.5   188.0  7.0   55.5  0.231713  0.524877  81.63   \n",
            "79998  21.0  3.33   388.0  3.5  3160.0  7.0   77.0  0.010766  0.476183  72.36   \n",
            "79999   0.0  3.50     5.0  3.5    98.0  7.0   89.0 -0.031818  0.289484  82.24   \n",
            "\n",
            "         19  \n",
            "0       4.0  \n",
            "1       3.0  \n",
            "2      10.0  \n",
            "3       3.0  \n",
            "4       2.0  \n",
            "...     ...  \n",
            "79995   1.0  \n",
            "79996   1.0  \n",
            "79997   1.0  \n",
            "79998   4.0  \n",
            "79999   1.0  \n",
            "\n",
            "[80000 rows x 20 columns]\n",
            "[ 0.59408079  0.90410064  0.99804464 -0.00459476  0.39452865  0.92642253\n",
            "  0.88025975 -0.05397725  1.97663461  0.17379868  0.40221625  0.15230488\n",
            " -0.29909827 -0.40736856  0.61731118 -0.64373117 -0.2396887   1.4520941 ]\n"
          ]
        }
      ],
      "source": [
        "kfold = KFold(n_splits=5, shuffle=True)\n",
        "\n",
        "training_withaim=train_df.drop(labels=\"r_id\", axis=1)\n",
        "testing_withaim=test_df.drop(labels=\"r_id\", axis=1)\n",
        "\n",
        "#Check the NaN in data and drop them\n",
        "imp_train=SimpleImputer(missing_values=np.NaN)\n",
        "training=pd.DataFrame(imp_train.fit_transform(training_withaim))\n",
        "\n",
        "imp_test=SimpleImputer(missing_values=np.NaN)\n",
        "testing=pd.DataFrame(imp_test.fit_transform(testing_withaim))\n",
        "print(training)\n",
        "# There aren't any nan data in the dataframe \n",
        "training.isnull().values.sum()\n",
        "testing.isnull().values.sum()\n",
        "training = training.iloc[: , 0:18]\n",
        "testing = testing.iloc[: , 0:18]\n",
        "scaler = StandardScaler()\n",
        "\n",
        "# scale skewed and target features\n",
        "std_train_df = train_df.copy(deep=True)\n",
        "std_train_df = scaler.fit_transform(training)\n",
        "std_test_df = test_df.copy(deep=True)\n",
        "std_test_df = scaler.transform(testing)\n",
        "#std_test_df[['r_stars','r_stars_square','r_length', 'u_friends_count', 'u_review_count', 'u_month_age', 'b_stars','b_review_count','r_rea']] = scaler.transform(test_df[['r_stars','r_stars_square','r_length', 'u_friends_count', 'u_review_count', 'u_month_age', 'b_stars','b_review_count','r_rea']])\n",
        "\n",
        "print(std_train_df[0])\n",
        "std_train_df = pd.DataFrame(std_train_df)\n",
        "std_test_df = pd.DataFrame(std_test_df)\n",
        "\n",
        "training = std_train_df.iloc[: , 0:18]\n",
        "testing = std_test_df.iloc[: , 0:18]\n",
        "labelsForTrain=training_withaim.iloc[: , -1]\n",
        "labelsForTest=testing_withaim.iloc[: , -1]\n",
        "input_shape = training.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r2g3EZG7xb9z",
        "outputId": "7a99d697-a0d6-4424-f792-2e63eb5f8387"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(80000, 18)"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "training.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oZ7qCoso8FJt"
      },
      "outputs": [],
      "source": [
        "def create_model(activation, num_hidden_layer, num_hidden_unit):\n",
        "  inputs = Input(shape=(training.shape[1],))\n",
        "  model = inputs\n",
        "  for i in range(1,num_hidden_layer):\n",
        "    model = Dense(num_hidden_unit, activation=activation,)(model)\n",
        "        \n",
        "        \n",
        "  model = Dense(1,)(model)\n",
        "  model = Model(inputs, model)\n",
        "\n",
        "  return model"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Code for nested CV"
      ],
      "metadata": {
        "id": "gv1FhIFf5CPo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.utils import validation\n",
        "def objective(trial:optuna.Trial,data_train,result_train,data_val,result_val):\n",
        "  K.clear_session()\n",
        "    \n",
        "  activation = trial.suggest_categorical('activation',['relu','tanh','linear'])\n",
        "  #Leave fewer optimizer\n",
        "    \n",
        "  num_hidden_layer = trial.suggest_int('num_hidden_layer',2,4)\n",
        "  #get more features per layer, add num of hidden unit if have time\n",
        "\n",
        "  #define the number of unit with 2^n\n",
        "  i = trial.suggest_int('i',3,9)\n",
        "  num_hidden_unit = 2**i\n",
        "  \n",
        "  #Try to adjust learning_rate\n",
        "\n",
        "  learning_rate = trial.suggest_loguniform('learning_rate', 0.0001,0.01)\n",
        "  \n",
        "  # Gradient Clipping\n",
        "\n",
        "  optimizer = Adam(learning_rate=learning_rate,clipnorm=1.0)\n",
        "    \n",
        "  num_folds = 3\n",
        "\n",
        "  loss_per_fold = []\n",
        "  es = EarlyStopping(monitor='val_mse', patience=5)\n",
        "\n",
        "  model = create_model(activation,num_hidden_layer,num_hidden_unit)\n",
        "  model_list.append(model)\n",
        "  model.compile(loss='mse',optimizer=optimizer,metrics=['mse','mae'])\n",
        "  #learning scheduler\n",
        "  reduce_lr = ReduceLROnPlateau(monitor='val_mse', factor=0.2,\n",
        "                              patience=5, min_lr=0.001)\n",
        "      # Fit data to model\n",
        "  score_int=[]\n",
        "  history = model.fit(data_train, result_train,\n",
        "                  batch_size=64,\n",
        "                  epochs=500,\n",
        "                  verbose=2,\n",
        "                  validation_data=(data_val,result_val),\n",
        "                  validation_batch_size=64,\n",
        "                  callbacks=[es,reduce_lr])\n",
        "  \n",
        "  scores=model.evaluate(data_val,result_val,verbose=0)\n",
        "\n",
        "  print(f'Score for inner fold : {model.metrics_names[0]} of {scores[0]}')\n",
        "  score_int.append(round(scores[1],2))\n",
        "  #loss_per_fold.append(scores[0])\n",
        "\n",
        "  history_list.append(history)\n",
        "  return np.mean(score_int)"
      ],
      "metadata": {
        "id": "cKoob9OgYkpf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def objective_cv(trial:optuna.Trial,train_outer,result_outer)->np.array:\n",
        "  kfold = KFold(n_splits=3,shuffle=True)\n",
        "  scores=list()\n",
        "  print(f'Trial Number:{trial.number}')\n",
        "  for fold, (train,test) in enumerate(kfold.split(train_outer,result_outer)):\n",
        "    print(f\"-----Fold:{fold}--------\")\n",
        "\n",
        "    new_train_inner=train_outer.iloc[train]\n",
        "    new_label=result_outer.iloc[train]\n",
        "\n",
        "    val_inner=train_outer.iloc[test]\n",
        "    result_inner=train_outer.iloc[test]\n",
        "    \n",
        "    val_loss = objective(trial, new_train_inner,new_label,val_inner,result_inner)\n",
        "    scores.append(val_loss)\n",
        "  return np.mean(scores)"
      ],
      "metadata": {
        "id": "nWqku3Xag9iz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def nested_cross_val(training,labelsForTrain):\n",
        "  kfold = KFold(n_splits=3,shuffle=True)\n",
        "  scores=list()\n",
        "  for fold, (train_idx, val_idx) in enumerate(kfold.split(training,labelsForTrain)):\n",
        "      print(f\"------ Outer Fold - {fold} -------- \")\n",
        "      train_outer=training.iloc[train_idx]\n",
        "      result_outer=labelsForTrain.iloc[train_idx]\n",
        "      study = optuna.create_study(\n",
        "            study_name = f\"NN-study-outer-fold{fold}\",\n",
        "            direction = \"minimize\",\n",
        "            load_if_exists=True\n",
        "        )\n",
        "      func = lambda trial: objective_cv(trial, train_outer,result_outer)\n",
        "      study.optimize(\n",
        "            func,\n",
        "            n_trials=50,\n",
        "        )\n",
        "      print(f\"Number of finished trial : {len(study.trials)}\")\n",
        "      print(f\"Best Trial Value : {study.best_trial.value}\")"
      ],
      "metadata": {
        "id": "m1c3wruTj1mo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_list=[]\n",
        "history_list=[]\n",
        "nested_cross_val(training,labelsForTrain)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FiX_SEPnqBKw",
        "outputId": "eb6e9b1d-3cf8-4b9e-c9fe-1377eb0b88ba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "------ Outer Fold - 0 -------- \n",
            "Trial Number:0\n",
            "-----Fold:0--------\n",
            "Epoch 1/500\n",
            "556/556 - 2s - loss: 34.0137 - mse: 34.0137 - mae: 1.6310 - val_loss: 13.7315 - val_mse: 13.7315 - val_mae: 2.7998 - lr: 0.0026 - 2s/epoch - 3ms/step\n",
            "Epoch 2/500\n",
            "556/556 - 1s - loss: 33.1798 - mse: 33.1798 - mae: 1.5914 - val_loss: 13.7150 - val_mse: 13.7150 - val_mae: 2.7937 - lr: 0.0026 - 1s/epoch - 2ms/step\n",
            "Epoch 3/500\n",
            "556/556 - 1s - loss: 32.3004 - mse: 32.3004 - mae: 1.5690 - val_loss: 13.2640 - val_mse: 13.2640 - val_mae: 2.7402 - lr: 0.0026 - 1s/epoch - 2ms/step\n",
            "Epoch 4/500\n",
            "556/556 - 1s - loss: 32.1878 - mse: 32.1878 - mae: 1.5625 - val_loss: 14.6481 - val_mse: 14.6481 - val_mae: 2.8382 - lr: 0.0026 - 1s/epoch - 2ms/step\n",
            "Epoch 5/500\n",
            "556/556 - 1s - loss: 32.7200 - mse: 32.7200 - mae: 1.5599 - val_loss: 14.1608 - val_mse: 14.1608 - val_mae: 2.8368 - lr: 0.0026 - 1s/epoch - 2ms/step\n",
            "Epoch 6/500\n",
            "556/556 - 1s - loss: 32.6926 - mse: 32.6926 - mae: 1.5587 - val_loss: 12.2293 - val_mse: 12.2293 - val_mae: 2.6400 - lr: 0.0026 - 1s/epoch - 2ms/step\n",
            "Epoch 7/500\n",
            "556/556 - 1s - loss: 32.5708 - mse: 32.5708 - mae: 1.5614 - val_loss: 12.9205 - val_mse: 12.9205 - val_mae: 2.6200 - lr: 0.0026 - 1s/epoch - 2ms/step\n",
            "Epoch 8/500\n",
            "556/556 - 1s - loss: 32.3598 - mse: 32.3598 - mae: 1.5520 - val_loss: 12.6005 - val_mse: 12.6005 - val_mae: 2.6344 - lr: 0.0026 - 1s/epoch - 2ms/step\n",
            "Epoch 9/500\n",
            "556/556 - 1s - loss: 32.3661 - mse: 32.3661 - mae: 1.5499 - val_loss: 15.1860 - val_mse: 15.1860 - val_mae: 2.8410 - lr: 0.0026 - 1s/epoch - 2ms/step\n",
            "Epoch 10/500\n",
            "556/556 - 1s - loss: 32.3964 - mse: 32.3964 - mae: 1.5470 - val_loss: 13.2820 - val_mse: 13.2820 - val_mae: 2.7779 - lr: 0.0026 - 1s/epoch - 2ms/step\n",
            "Epoch 11/500\n",
            "556/556 - 1s - loss: 32.4078 - mse: 32.4078 - mae: 1.5474 - val_loss: 12.2036 - val_mse: 12.2036 - val_mae: 2.5098 - lr: 0.0026 - 1s/epoch - 2ms/step\n",
            "Epoch 12/500\n",
            "556/556 - 1s - loss: 32.4758 - mse: 32.4758 - mae: 1.5429 - val_loss: 12.2420 - val_mse: 12.2420 - val_mae: 2.6693 - lr: 0.0026 - 1s/epoch - 2ms/step\n",
            "Epoch 13/500\n",
            "556/556 - 1s - loss: 32.2989 - mse: 32.2989 - mae: 1.5435 - val_loss: 9.9613 - val_mse: 9.9613 - val_mae: 2.5001 - lr: 0.0026 - 1s/epoch - 2ms/step\n",
            "Epoch 14/500\n",
            "556/556 - 1s - loss: 32.3818 - mse: 32.3818 - mae: 1.5497 - val_loss: 14.7420 - val_mse: 14.7420 - val_mae: 2.8110 - lr: 0.0026 - 1s/epoch - 2ms/step\n",
            "Epoch 15/500\n",
            "556/556 - 1s - loss: 32.3384 - mse: 32.3384 - mae: 1.5351 - val_loss: 12.2716 - val_mse: 12.2716 - val_mae: 2.6047 - lr: 0.0026 - 1s/epoch - 2ms/step\n",
            "Epoch 16/500\n",
            "556/556 - 1s - loss: 32.0073 - mse: 32.0073 - mae: 1.5388 - val_loss: 11.2836 - val_mse: 11.2836 - val_mae: 2.5410 - lr: 0.0026 - 1s/epoch - 2ms/step\n",
            "Epoch 17/500\n",
            "556/556 - 1s - loss: 32.2496 - mse: 32.2496 - mae: 1.5346 - val_loss: 11.9746 - val_mse: 11.9746 - val_mae: 2.5000 - lr: 0.0026 - 1s/epoch - 2ms/step\n",
            "Epoch 18/500\n",
            "556/556 - 1s - loss: 31.7888 - mse: 31.7888 - mae: 1.5385 - val_loss: 10.8233 - val_mse: 10.8233 - val_mae: 2.5214 - lr: 0.0026 - 1s/epoch - 2ms/step\n",
            "Score for inner fold : loss of 10.823332786560059\n",
            "-----Fold:1--------\n",
            "Epoch 1/500\n",
            "556/556 - 2s - loss: 11.1814 - mse: 11.1814 - mae: 1.5660 - val_loss: 12.9425 - val_mse: 12.9425 - val_mae: 2.5917 - lr: 0.0026 - 2s/epoch - 3ms/step\n",
            "Epoch 2/500\n",
            "556/556 - 1s - loss: 10.6140 - mse: 10.6140 - mae: 1.5282 - val_loss: 16.8420 - val_mse: 16.8420 - val_mae: 3.0097 - lr: 0.0026 - 1s/epoch - 2ms/step\n",
            "Epoch 3/500\n",
            "556/556 - 1s - loss: 10.3736 - mse: 10.3736 - mae: 1.5131 - val_loss: 12.7951 - val_mse: 12.7951 - val_mae: 2.6277 - lr: 0.0026 - 1s/epoch - 2ms/step\n",
            "Epoch 4/500\n",
            "556/556 - 1s - loss: 10.2288 - mse: 10.2288 - mae: 1.5009 - val_loss: 11.9172 - val_mse: 11.9172 - val_mae: 2.6012 - lr: 0.0026 - 1s/epoch - 2ms/step\n",
            "Epoch 5/500\n",
            "556/556 - 1s - loss: 9.9171 - mse: 9.9171 - mae: 1.4899 - val_loss: 14.1776 - val_mse: 14.1776 - val_mae: 2.6609 - lr: 0.0026 - 1s/epoch - 2ms/step\n",
            "Epoch 6/500\n",
            "556/556 - 1s - loss: 9.8233 - mse: 9.8233 - mae: 1.4870 - val_loss: 14.3305 - val_mse: 14.3305 - val_mae: 2.6959 - lr: 0.0026 - 1s/epoch - 2ms/step\n",
            "Epoch 7/500\n",
            "556/556 - 1s - loss: 9.8458 - mse: 9.8458 - mae: 1.4858 - val_loss: 17.8105 - val_mse: 17.8105 - val_mae: 2.8789 - lr: 0.0026 - 1s/epoch - 2ms/step\n",
            "Epoch 8/500\n",
            "556/556 - 1s - loss: 9.7868 - mse: 9.7868 - mae: 1.4888 - val_loss: 13.4702 - val_mse: 13.4702 - val_mae: 2.6417 - lr: 0.0026 - 1s/epoch - 2ms/step\n",
            "Epoch 9/500\n",
            "556/556 - 1s - loss: 9.8581 - mse: 9.8581 - mae: 1.4827 - val_loss: 17.2418 - val_mse: 17.2418 - val_mae: 2.8248 - lr: 0.0026 - 1s/epoch - 2ms/step\n",
            "Score for inner fold : loss of 17.241806030273438\n",
            "-----Fold:2--------\n",
            "Epoch 1/500\n",
            "556/556 - 2s - loss: 31.5284 - mse: 31.5284 - mae: 1.5804 - val_loss: 14.9427 - val_mse: 14.9427 - val_mae: 2.7853 - lr: 0.0026 - 2s/epoch - 3ms/step\n",
            "Epoch 2/500\n",
            "556/556 - 1s - loss: 30.6845 - mse: 30.6845 - mae: 1.5394 - val_loss: 12.2627 - val_mse: 12.2627 - val_mae: 2.6072 - lr: 0.0026 - 1s/epoch - 2ms/step\n",
            "Epoch 3/500\n",
            "556/556 - 1s - loss: 30.5549 - mse: 30.5549 - mae: 1.5285 - val_loss: 13.6039 - val_mse: 13.6039 - val_mae: 2.5593 - lr: 0.0026 - 1s/epoch - 2ms/step\n",
            "Epoch 4/500\n",
            "556/556 - 1s - loss: 30.3069 - mse: 30.3069 - mae: 1.5245 - val_loss: 17.4304 - val_mse: 17.4304 - val_mae: 2.8970 - lr: 0.0026 - 1s/epoch - 2ms/step\n",
            "Epoch 5/500\n",
            "556/556 - 1s - loss: 30.3485 - mse: 30.3485 - mae: 1.5087 - val_loss: 12.3414 - val_mse: 12.3414 - val_mae: 2.6334 - lr: 0.0026 - 1s/epoch - 2ms/step\n",
            "Epoch 6/500\n",
            "556/556 - 1s - loss: 30.1396 - mse: 30.1396 - mae: 1.5076 - val_loss: 16.1599 - val_mse: 16.1599 - val_mae: 2.9111 - lr: 0.0026 - 1s/epoch - 2ms/step\n",
            "Epoch 7/500\n",
            "556/556 - 1s - loss: 30.1153 - mse: 30.1153 - mae: 1.5055 - val_loss: 15.3780 - val_mse: 15.3780 - val_mae: 2.5729 - lr: 0.0026 - 1s/epoch - 2ms/step\n",
            "Score for inner fold : loss of 15.377976417541504\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-10-10 09:12:19,164]\u001b[0m Finished trial#0 resulted in value: 14.479999999999999. Current best value is 14.479999999999999 with parameters: {'activation': 'relu', 'num_hidden_layer': 2, 'i': 7, 'learning_rate': 0.0026033795314011196}.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trial Number:1\n",
            "-----Fold:0--------\n",
            "Epoch 1/500\n",
            "556/556 - 2s - loss: 36.3448 - mse: 36.3448 - mae: 1.8240 - val_loss: 8.7354 - val_mse: 8.7354 - val_mae: 2.5830 - lr: 0.0010 - 2s/epoch - 3ms/step\n",
            "Epoch 2/500\n",
            "556/556 - 1s - loss: 33.2038 - mse: 33.2038 - mae: 1.6028 - val_loss: 9.1771 - val_mse: 9.1771 - val_mae: 2.5921 - lr: 0.0010 - 1s/epoch - 2ms/step\n",
            "Epoch 3/500\n",
            "556/556 - 1s - loss: 32.6691 - mse: 32.6691 - mae: 1.5754 - val_loss: 9.7745 - val_mse: 9.7745 - val_mae: 2.5916 - lr: 0.0010 - 1s/epoch - 2ms/step\n",
            "Epoch 4/500\n",
            "556/556 - 1s - loss: 32.3196 - mse: 32.3196 - mae: 1.5593 - val_loss: 9.8126 - val_mse: 9.8126 - val_mae: 2.6036 - lr: 0.0010 - 1s/epoch - 2ms/step\n",
            "Epoch 5/500\n",
            "556/556 - 1s - loss: 32.0748 - mse: 32.0748 - mae: 1.5471 - val_loss: 10.6782 - val_mse: 10.6782 - val_mae: 2.6823 - lr: 0.0010 - 1s/epoch - 2ms/step\n",
            "Epoch 6/500\n",
            "556/556 - 1s - loss: 31.8685 - mse: 31.8685 - mae: 1.5456 - val_loss: 10.3877 - val_mse: 10.3877 - val_mae: 2.6785 - lr: 0.0010 - 1s/epoch - 2ms/step\n",
            "Score for inner fold : loss of 10.387682914733887\n",
            "-----Fold:1--------\n",
            "Epoch 1/500\n",
            "556/556 - 2s - loss: 37.0257 - mse: 37.0257 - mae: 1.7699 - val_loss: 8.6287 - val_mse: 8.6287 - val_mae: 2.5389 - lr: 0.0010 - 2s/epoch - 3ms/step\n",
            "Epoch 2/500\n",
            "556/556 - 1s - loss: 33.6020 - mse: 33.6020 - mae: 1.5790 - val_loss: 9.6904 - val_mse: 9.6904 - val_mae: 2.6137 - lr: 0.0010 - 1s/epoch - 2ms/step\n",
            "Epoch 3/500\n",
            "556/556 - 1s - loss: 33.1138 - mse: 33.1138 - mae: 1.5580 - val_loss: 9.6428 - val_mse: 9.6428 - val_mae: 2.5793 - lr: 0.0010 - 1s/epoch - 2ms/step\n",
            "Epoch 4/500\n",
            "556/556 - 1s - loss: 32.8770 - mse: 32.8770 - mae: 1.5457 - val_loss: 9.6404 - val_mse: 9.6404 - val_mae: 2.5546 - lr: 0.0010 - 1s/epoch - 2ms/step\n",
            "Epoch 5/500\n",
            "556/556 - 1s - loss: 32.6635 - mse: 32.6635 - mae: 1.5390 - val_loss: 10.1659 - val_mse: 10.1659 - val_mae: 2.6428 - lr: 0.0010 - 1s/epoch - 2ms/step\n",
            "Epoch 6/500\n",
            "556/556 - 1s - loss: 32.4733 - mse: 32.4733 - mae: 1.5325 - val_loss: 10.5193 - val_mse: 10.5193 - val_mae: 2.6475 - lr: 0.0010 - 1s/epoch - 2ms/step\n",
            "Score for inner fold : loss of 10.519248008728027\n",
            "-----Fold:2--------\n",
            "Epoch 1/500\n",
            "556/556 - 2s - loss: 14.6421 - mse: 14.6421 - mae: 1.7730 - val_loss: 8.8473 - val_mse: 8.8473 - val_mae: 2.5753 - lr: 0.0010 - 2s/epoch - 3ms/step\n",
            "Epoch 2/500\n",
            "556/556 - 1s - loss: 11.6113 - mse: 11.6113 - mae: 1.5587 - val_loss: 9.7968 - val_mse: 9.7968 - val_mae: 2.6556 - lr: 0.0010 - 1s/epoch - 2ms/step\n",
            "Epoch 3/500\n",
            "556/556 - 1s - loss: 11.2010 - mse: 11.2010 - mae: 1.5432 - val_loss: 10.2861 - val_mse: 10.2861 - val_mae: 2.6674 - lr: 0.0010 - 1s/epoch - 2ms/step\n",
            "Epoch 4/500\n",
            "556/556 - 1s - loss: 10.9217 - mse: 10.9217 - mae: 1.5261 - val_loss: 9.8195 - val_mse: 9.8195 - val_mae: 2.6228 - lr: 0.0010 - 1s/epoch - 2ms/step\n",
            "Epoch 5/500\n",
            "556/556 - 1s - loss: 10.7186 - mse: 10.7186 - mae: 1.5183 - val_loss: 9.9253 - val_mse: 9.9253 - val_mae: 2.6000 - lr: 0.0010 - 1s/epoch - 2ms/step\n",
            "Epoch 6/500\n",
            "556/556 - 1s - loss: 10.5380 - mse: 10.5380 - mae: 1.5116 - val_loss: 10.2928 - val_mse: 10.2928 - val_mae: 2.6913 - lr: 0.0010 - 1s/epoch - 2ms/step\n",
            "Score for inner fold : loss of 10.292769432067871\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-10-10 09:12:43,013]\u001b[0m Finished trial#1 resulted in value: 10.4. Current best value is 10.4 with parameters: {'activation': 'tanh', 'num_hidden_layer': 2, 'i': 5, 'learning_rate': 0.001033313287653061}.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trial Number:2\n",
            "-----Fold:0--------\n",
            "Epoch 1/500\n",
            "556/556 - 2s - loss: 11.9834 - mse: 11.9834 - mae: 1.6250 - val_loss: 15.5860 - val_mse: 15.5860 - val_mae: 2.7325 - lr: 0.0015 - 2s/epoch - 3ms/step\n",
            "Epoch 2/500\n",
            "556/556 - 1s - loss: 11.3987 - mse: 11.3987 - mae: 1.5740 - val_loss: 16.0762 - val_mse: 16.0762 - val_mae: 2.7411 - lr: 0.0015 - 1s/epoch - 2ms/step\n",
            "Epoch 3/500\n",
            "556/556 - 1s - loss: 11.3440 - mse: 11.3440 - mae: 1.5721 - val_loss: 14.7112 - val_mse: 14.7112 - val_mae: 2.6418 - lr: 0.0015 - 1s/epoch - 2ms/step\n",
            "Epoch 4/500\n",
            "556/556 - 1s - loss: 11.2318 - mse: 11.2318 - mae: 1.5692 - val_loss: 15.3170 - val_mse: 15.3170 - val_mae: 2.7651 - lr: 0.0015 - 1s/epoch - 2ms/step\n",
            "Epoch 5/500\n",
            "556/556 - 1s - loss: 11.3911 - mse: 11.3911 - mae: 1.5671 - val_loss: 13.4293 - val_mse: 13.4293 - val_mae: 2.6655 - lr: 0.0015 - 1s/epoch - 2ms/step\n",
            "Epoch 6/500\n",
            "556/556 - 1s - loss: 11.4289 - mse: 11.4289 - mae: 1.5750 - val_loss: 13.3429 - val_mse: 13.3429 - val_mae: 2.6051 - lr: 0.0015 - 1s/epoch - 2ms/step\n",
            "Epoch 7/500\n",
            "556/556 - 1s - loss: 11.4901 - mse: 11.4901 - mae: 1.5696 - val_loss: 15.8998 - val_mse: 15.8998 - val_mae: 2.7995 - lr: 0.0015 - 1s/epoch - 2ms/step\n",
            "Epoch 8/500\n",
            "556/556 - 1s - loss: 11.4706 - mse: 11.4706 - mae: 1.5666 - val_loss: 14.4477 - val_mse: 14.4477 - val_mae: 2.7495 - lr: 0.0015 - 1s/epoch - 2ms/step\n",
            "Epoch 9/500\n",
            "556/556 - 1s - loss: 11.3812 - mse: 11.3812 - mae: 1.5722 - val_loss: 15.0581 - val_mse: 15.0581 - val_mae: 2.6523 - lr: 0.0015 - 1s/epoch - 2ms/step\n",
            "Epoch 10/500\n",
            "556/556 - 1s - loss: 11.4156 - mse: 11.4156 - mae: 1.5683 - val_loss: 12.7377 - val_mse: 12.7377 - val_mae: 2.6299 - lr: 0.0015 - 1s/epoch - 2ms/step\n",
            "Epoch 11/500\n",
            "556/556 - 1s - loss: 11.4285 - mse: 11.4285 - mae: 1.5631 - val_loss: 15.0144 - val_mse: 15.0144 - val_mae: 2.8458 - lr: 0.0015 - 1s/epoch - 2ms/step\n",
            "Epoch 12/500\n",
            "556/556 - 1s - loss: 11.3206 - mse: 11.3206 - mae: 1.5693 - val_loss: 15.6387 - val_mse: 15.6387 - val_mae: 2.8442 - lr: 0.0015 - 1s/epoch - 2ms/step\n",
            "Epoch 13/500\n",
            "556/556 - 1s - loss: 11.3736 - mse: 11.3736 - mae: 1.5676 - val_loss: 13.2285 - val_mse: 13.2285 - val_mae: 2.6439 - lr: 0.0015 - 1s/epoch - 2ms/step\n",
            "Epoch 14/500\n",
            "556/556 - 1s - loss: 11.3327 - mse: 11.3327 - mae: 1.5699 - val_loss: 14.0571 - val_mse: 14.0571 - val_mae: 2.6682 - lr: 0.0015 - 1s/epoch - 2ms/step\n",
            "Epoch 15/500\n",
            "556/556 - 1s - loss: 11.1782 - mse: 11.1782 - mae: 1.5679 - val_loss: 13.6027 - val_mse: 13.6027 - val_mae: 2.5175 - lr: 0.0015 - 1s/epoch - 2ms/step\n",
            "Score for inner fold : loss of 13.602700233459473\n",
            "-----Fold:1--------\n",
            "Epoch 1/500\n",
            "556/556 - 2s - loss: 32.3895 - mse: 32.3895 - mae: 1.6715 - val_loss: 13.0660 - val_mse: 13.0660 - val_mae: 2.6516 - lr: 0.0015 - 2s/epoch - 3ms/step\n",
            "Epoch 2/500\n",
            "556/556 - 1s - loss: 32.2370 - mse: 32.2370 - mae: 1.6187 - val_loss: 15.7265 - val_mse: 15.7265 - val_mae: 2.6517 - lr: 0.0015 - 1s/epoch - 2ms/step\n",
            "Epoch 3/500\n",
            "556/556 - 1s - loss: 32.3194 - mse: 32.3194 - mae: 1.6129 - val_loss: 13.2605 - val_mse: 13.2605 - val_mae: 2.4899 - lr: 0.0015 - 1s/epoch - 2ms/step\n",
            "Epoch 4/500\n",
            "556/556 - 1s - loss: 32.1698 - mse: 32.1698 - mae: 1.6159 - val_loss: 14.7206 - val_mse: 14.7206 - val_mae: 2.6401 - lr: 0.0015 - 1s/epoch - 2ms/step\n",
            "Epoch 5/500\n",
            "556/556 - 1s - loss: 32.0556 - mse: 32.0556 - mae: 1.6089 - val_loss: 13.7861 - val_mse: 13.7861 - val_mae: 2.6704 - lr: 0.0015 - 1s/epoch - 2ms/step\n",
            "Epoch 6/500\n",
            "556/556 - 1s - loss: 32.1485 - mse: 32.1485 - mae: 1.6121 - val_loss: 14.7899 - val_mse: 14.7899 - val_mae: 2.7602 - lr: 0.0015 - 1s/epoch - 2ms/step\n",
            "Score for inner fold : loss of 14.789933204650879\n",
            "-----Fold:2--------\n",
            "Epoch 1/500\n",
            "556/556 - 2s - loss: 33.3012 - mse: 33.3012 - mae: 1.6743 - val_loss: 11.9162 - val_mse: 11.9162 - val_mae: 2.6847 - lr: 0.0015 - 2s/epoch - 3ms/step\n",
            "Epoch 2/500\n",
            "556/556 - 1s - loss: 33.2908 - mse: 33.2908 - mae: 1.6214 - val_loss: 12.2244 - val_mse: 12.2244 - val_mae: 2.6927 - lr: 0.0015 - 1s/epoch - 2ms/step\n",
            "Epoch 3/500\n",
            "556/556 - 1s - loss: 32.9127 - mse: 32.9127 - mae: 1.6161 - val_loss: 13.1087 - val_mse: 13.1087 - val_mae: 2.7785 - lr: 0.0015 - 1s/epoch - 2ms/step\n",
            "Epoch 4/500\n",
            "556/556 - 1s - loss: 32.8643 - mse: 32.8643 - mae: 1.6133 - val_loss: 11.7260 - val_mse: 11.7260 - val_mae: 2.6459 - lr: 0.0015 - 1s/epoch - 2ms/step\n",
            "Epoch 5/500\n",
            "556/556 - 1s - loss: 32.6892 - mse: 32.6892 - mae: 1.6146 - val_loss: 12.2774 - val_mse: 12.2774 - val_mae: 2.6801 - lr: 0.0015 - 1s/epoch - 2ms/step\n",
            "Epoch 6/500\n",
            "556/556 - 1s - loss: 32.6976 - mse: 32.6976 - mae: 1.6110 - val_loss: 12.5509 - val_mse: 12.5509 - val_mae: 2.7468 - lr: 0.0015 - 1s/epoch - 2ms/step\n",
            "Epoch 7/500\n",
            "556/556 - 1s - loss: 33.0615 - mse: 33.0615 - mae: 1.6103 - val_loss: 10.5711 - val_mse: 10.5711 - val_mae: 2.5336 - lr: 0.0015 - 1s/epoch - 2ms/step\n",
            "Epoch 8/500\n",
            "556/556 - 1s - loss: 32.8876 - mse: 32.8876 - mae: 1.6127 - val_loss: 11.8543 - val_mse: 11.8543 - val_mae: 2.6110 - lr: 0.0015 - 1s/epoch - 2ms/step\n",
            "Epoch 9/500\n",
            "556/556 - 1s - loss: 32.7378 - mse: 32.7378 - mae: 1.6084 - val_loss: 13.1103 - val_mse: 13.1103 - val_mae: 2.6739 - lr: 0.0015 - 1s/epoch - 2ms/step\n",
            "Epoch 10/500\n",
            "556/556 - 1s - loss: 32.7214 - mse: 32.7214 - mae: 1.6131 - val_loss: 13.1718 - val_mse: 13.1718 - val_mae: 2.7567 - lr: 0.0015 - 1s/epoch - 2ms/step\n",
            "Epoch 11/500\n",
            "556/556 - 1s - loss: 33.0207 - mse: 33.0207 - mae: 1.6163 - val_loss: 12.0233 - val_mse: 12.0233 - val_mae: 2.6354 - lr: 0.0015 - 1s/epoch - 2ms/step\n",
            "Epoch 12/500\n",
            "556/556 - 1s - loss: 32.7380 - mse: 32.7380 - mae: 1.6082 - val_loss: 12.0754 - val_mse: 12.0754 - val_mae: 2.6777 - lr: 0.0015 - 1s/epoch - 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-10-10 09:13:28,467]\u001b[0m Finished trial#2 resulted in value: 13.49. Current best value is 10.4 with parameters: {'activation': 'tanh', 'num_hidden_layer': 2, 'i': 5, 'learning_rate': 0.001033313287653061}.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Score for inner fold : loss of 12.075369834899902\n",
            "Trial Number:3\n",
            "-----Fold:0--------\n",
            "Epoch 1/500\n",
            "556/556 - 2s - loss: 33.2883 - mse: 33.2883 - mae: 1.6344 - val_loss: 12.1288 - val_mse: 12.1288 - val_mae: 2.6540 - lr: 0.0082 - 2s/epoch - 3ms/step\n",
            "Epoch 2/500\n",
            "556/556 - 1s - loss: 32.8568 - mse: 32.8568 - mae: 1.5918 - val_loss: 9.3674 - val_mse: 9.3674 - val_mae: 2.4065 - lr: 0.0082 - 973ms/epoch - 2ms/step\n",
            "Epoch 3/500\n",
            "556/556 - 1s - loss: 32.5959 - mse: 32.5959 - mae: 1.5776 - val_loss: 13.5047 - val_mse: 13.5047 - val_mae: 2.8560 - lr: 0.0082 - 958ms/epoch - 2ms/step\n",
            "Epoch 4/500\n",
            "556/556 - 1s - loss: 32.1973 - mse: 32.1973 - mae: 1.5709 - val_loss: 11.7995 - val_mse: 11.7995 - val_mae: 2.6877 - lr: 0.0082 - 974ms/epoch - 2ms/step\n",
            "Epoch 5/500\n",
            "556/556 - 1s - loss: 31.9751 - mse: 31.9751 - mae: 1.5621 - val_loss: 12.5908 - val_mse: 12.5908 - val_mae: 2.6783 - lr: 0.0082 - 996ms/epoch - 2ms/step\n",
            "Epoch 6/500\n",
            "556/556 - 1s - loss: 31.8211 - mse: 31.8211 - mae: 1.5601 - val_loss: 12.7464 - val_mse: 12.7464 - val_mae: 2.7093 - lr: 0.0082 - 968ms/epoch - 2ms/step\n",
            "Epoch 7/500\n",
            "556/556 - 1s - loss: 32.0546 - mse: 32.0546 - mae: 1.5526 - val_loss: 13.4433 - val_mse: 13.4433 - val_mae: 2.7222 - lr: 0.0082 - 985ms/epoch - 2ms/step\n",
            "Score for inner fold : loss of 13.443303108215332\n",
            "-----Fold:1--------\n",
            "Epoch 1/500\n",
            "556/556 - 2s - loss: 32.5491 - mse: 32.5491 - mae: 1.6404 - val_loss: 12.8512 - val_mse: 12.8512 - val_mae: 2.6611 - lr: 0.0082 - 2s/epoch - 3ms/step\n",
            "Epoch 2/500\n",
            "556/556 - 1s - loss: 31.6096 - mse: 31.6096 - mae: 1.5817 - val_loss: 10.5626 - val_mse: 10.5626 - val_mae: 2.3230 - lr: 0.0082 - 960ms/epoch - 2ms/step\n",
            "Epoch 3/500\n",
            "556/556 - 1s - loss: 31.3950 - mse: 31.3950 - mae: 1.5702 - val_loss: 11.8034 - val_mse: 11.8034 - val_mae: 2.6245 - lr: 0.0082 - 974ms/epoch - 2ms/step\n",
            "Epoch 4/500\n",
            "556/556 - 1s - loss: 31.1658 - mse: 31.1658 - mae: 1.5568 - val_loss: 12.0740 - val_mse: 12.0740 - val_mae: 2.6542 - lr: 0.0082 - 972ms/epoch - 2ms/step\n",
            "Epoch 5/500\n",
            "556/556 - 1s - loss: 30.9105 - mse: 30.9105 - mae: 1.5494 - val_loss: 15.1200 - val_mse: 15.1200 - val_mae: 2.7280 - lr: 0.0082 - 978ms/epoch - 2ms/step\n",
            "Epoch 6/500\n",
            "556/556 - 1s - loss: 30.8781 - mse: 30.8781 - mae: 1.5428 - val_loss: 14.3888 - val_mse: 14.3888 - val_mae: 2.8108 - lr: 0.0082 - 991ms/epoch - 2ms/step\n",
            "Epoch 7/500\n",
            "556/556 - 1s - loss: 30.8849 - mse: 30.8849 - mae: 1.5385 - val_loss: 17.9928 - val_mse: 17.9928 - val_mae: 2.9270 - lr: 0.0082 - 976ms/epoch - 2ms/step\n",
            "Score for inner fold : loss of 17.992807388305664\n",
            "-----Fold:2--------\n",
            "Epoch 1/500\n",
            "556/556 - 2s - loss: 11.6244 - mse: 11.6244 - mae: 1.5991 - val_loss: 13.4123 - val_mse: 13.4123 - val_mae: 2.6605 - lr: 0.0082 - 2s/epoch - 3ms/step\n",
            "Epoch 2/500\n",
            "556/556 - 1s - loss: 10.3312 - mse: 10.3312 - mae: 1.5372 - val_loss: 12.8486 - val_mse: 12.8486 - val_mae: 2.8721 - lr: 0.0082 - 986ms/epoch - 2ms/step\n",
            "Epoch 3/500\n",
            "556/556 - 1s - loss: 10.1111 - mse: 10.1111 - mae: 1.5197 - val_loss: 12.0044 - val_mse: 12.0044 - val_mae: 2.6881 - lr: 0.0082 - 986ms/epoch - 2ms/step\n",
            "Epoch 4/500\n",
            "556/556 - 1s - loss: 10.0670 - mse: 10.0670 - mae: 1.5207 - val_loss: 13.6236 - val_mse: 13.6236 - val_mae: 2.6306 - lr: 0.0082 - 962ms/epoch - 2ms/step\n",
            "Epoch 5/500\n",
            "556/556 - 1s - loss: 10.1334 - mse: 10.1334 - mae: 1.5104 - val_loss: 10.6311 - val_mse: 10.6311 - val_mae: 2.4497 - lr: 0.0082 - 946ms/epoch - 2ms/step\n",
            "Epoch 6/500\n",
            "556/556 - 1s - loss: 9.8713 - mse: 9.8713 - mae: 1.5081 - val_loss: 11.9665 - val_mse: 11.9665 - val_mae: 2.7164 - lr: 0.0082 - 993ms/epoch - 2ms/step\n",
            "Epoch 7/500\n",
            "556/556 - 1s - loss: 9.8967 - mse: 9.8967 - mae: 1.5046 - val_loss: 10.1545 - val_mse: 10.1545 - val_mae: 2.5383 - lr: 0.0082 - 973ms/epoch - 2ms/step\n",
            "Epoch 8/500\n",
            "556/556 - 1s - loss: 9.8856 - mse: 9.8856 - mae: 1.5053 - val_loss: 12.5093 - val_mse: 12.5093 - val_mae: 2.6926 - lr: 0.0082 - 986ms/epoch - 2ms/step\n",
            "Epoch 9/500\n",
            "556/556 - 1s - loss: 9.7778 - mse: 9.7778 - mae: 1.5069 - val_loss: 10.4855 - val_mse: 10.4855 - val_mae: 2.4099 - lr: 0.0082 - 981ms/epoch - 2ms/step\n",
            "Epoch 10/500\n",
            "556/556 - 1s - loss: 9.9256 - mse: 9.9256 - mae: 1.5011 - val_loss: 15.0913 - val_mse: 15.0913 - val_mae: 2.8711 - lr: 0.0082 - 981ms/epoch - 2ms/step\n",
            "Epoch 11/500\n",
            "556/556 - 1s - loss: 9.7084 - mse: 9.7084 - mae: 1.4967 - val_loss: 9.5284 - val_mse: 9.5284 - val_mae: 2.3673 - lr: 0.0082 - 977ms/epoch - 2ms/step\n",
            "Epoch 12/500\n",
            "556/556 - 1s - loss: 10.0303 - mse: 10.0303 - mae: 1.5063 - val_loss: 14.7625 - val_mse: 14.7625 - val_mae: 2.9736 - lr: 0.0082 - 1s/epoch - 2ms/step\n",
            "Epoch 13/500\n",
            "556/556 - 1s - loss: 9.7249 - mse: 9.7249 - mae: 1.5025 - val_loss: 13.2477 - val_mse: 13.2477 - val_mae: 2.7018 - lr: 0.0082 - 979ms/epoch - 2ms/step\n",
            "Epoch 14/500\n",
            "556/556 - 1s - loss: 9.7163 - mse: 9.7163 - mae: 1.5015 - val_loss: 11.0204 - val_mse: 11.0204 - val_mae: 2.6179 - lr: 0.0082 - 997ms/epoch - 2ms/step\n",
            "Epoch 15/500\n",
            "556/556 - 1s - loss: 9.7523 - mse: 9.7523 - mae: 1.4979 - val_loss: 14.9077 - val_mse: 14.9077 - val_mae: 2.9198 - lr: 0.0082 - 992ms/epoch - 2ms/step\n",
            "Epoch 16/500\n",
            "556/556 - 1s - loss: 9.7562 - mse: 9.7562 - mae: 1.5030 - val_loss: 12.7321 - val_mse: 12.7321 - val_mae: 2.7208 - lr: 0.0082 - 1s/epoch - 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-10-10 09:14:02,523]\u001b[0m Finished trial#3 resulted in value: 14.719999999999999. Current best value is 10.4 with parameters: {'activation': 'tanh', 'num_hidden_layer': 2, 'i': 5, 'learning_rate': 0.001033313287653061}.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Score for inner fold : loss of 12.732087135314941\n",
            "Trial Number:4\n",
            "-----Fold:0--------\n",
            "Epoch 1/500\n",
            "556/556 - 3s - loss: 13.2428 - mse: 13.2428 - mae: 1.6510 - val_loss: 10.6669 - val_mse: 10.6669 - val_mae: 2.6639 - lr: 8.9413e-04 - 3s/epoch - 5ms/step\n",
            "Epoch 2/500\n",
            "556/556 - 2s - loss: 12.1892 - mse: 12.1892 - mae: 1.5801 - val_loss: 9.2886 - val_mse: 9.2886 - val_mae: 2.5037 - lr: 8.9413e-04 - 2s/epoch - 4ms/step\n",
            "Epoch 3/500\n",
            "556/556 - 2s - loss: 11.9856 - mse: 11.9856 - mae: 1.5673 - val_loss: 9.9687 - val_mse: 9.9687 - val_mae: 2.6098 - lr: 8.9413e-04 - 2s/epoch - 4ms/step\n",
            "Epoch 4/500\n",
            "556/556 - 2s - loss: 11.7229 - mse: 11.7229 - mae: 1.5507 - val_loss: 11.3465 - val_mse: 11.3465 - val_mae: 2.5673 - lr: 8.9413e-04 - 2s/epoch - 4ms/step\n",
            "Epoch 5/500\n",
            "556/556 - 2s - loss: 11.6315 - mse: 11.6315 - mae: 1.5432 - val_loss: 12.0699 - val_mse: 12.0699 - val_mae: 2.7723 - lr: 8.9413e-04 - 2s/epoch - 4ms/step\n",
            "Epoch 6/500\n",
            "556/556 - 2s - loss: 11.4895 - mse: 11.4895 - mae: 1.5471 - val_loss: 11.5788 - val_mse: 11.5788 - val_mae: 2.6782 - lr: 8.9413e-04 - 2s/epoch - 3ms/step\n",
            "Epoch 7/500\n",
            "556/556 - 2s - loss: 11.2159 - mse: 11.2159 - mae: 1.5429 - val_loss: 11.3383 - val_mse: 11.3383 - val_mae: 2.6810 - lr: 8.9413e-04 - 2s/epoch - 3ms/step\n",
            "Score for inner fold : loss of 11.338332176208496\n",
            "-----Fold:1--------\n",
            "Epoch 1/500\n",
            "556/556 - 3s - loss: 31.5684 - mse: 31.5684 - mae: 1.6303 - val_loss: 10.6903 - val_mse: 10.6903 - val_mae: 2.6281 - lr: 8.9413e-04 - 3s/epoch - 5ms/step\n",
            "Epoch 2/500\n",
            "556/556 - 2s - loss: 30.6892 - mse: 30.6892 - mae: 1.5517 - val_loss: 10.8720 - val_mse: 10.8720 - val_mae: 2.6253 - lr: 8.9413e-04 - 2s/epoch - 3ms/step\n",
            "Epoch 3/500\n",
            "556/556 - 2s - loss: 30.3690 - mse: 30.3690 - mae: 1.5411 - val_loss: 11.1387 - val_mse: 11.1387 - val_mae: 2.7117 - lr: 8.9413e-04 - 2s/epoch - 3ms/step\n",
            "Epoch 4/500\n",
            "556/556 - 2s - loss: 30.3299 - mse: 30.3299 - mae: 1.5296 - val_loss: 12.1812 - val_mse: 12.1812 - val_mae: 2.7530 - lr: 8.9413e-04 - 2s/epoch - 3ms/step\n",
            "Epoch 5/500\n",
            "556/556 - 2s - loss: 30.1678 - mse: 30.1678 - mae: 1.5208 - val_loss: 11.0991 - val_mse: 11.0991 - val_mae: 2.6558 - lr: 8.9413e-04 - 2s/epoch - 3ms/step\n",
            "Epoch 6/500\n",
            "556/556 - 2s - loss: 29.9515 - mse: 29.9515 - mae: 1.5194 - val_loss: 10.8225 - val_mse: 10.8225 - val_mae: 2.5921 - lr: 8.9413e-04 - 2s/epoch - 3ms/step\n",
            "Score for inner fold : loss of 10.822548866271973\n",
            "-----Fold:2--------\n",
            "Epoch 1/500\n",
            "556/556 - 3s - loss: 32.8710 - mse: 32.8710 - mae: 1.6660 - val_loss: 8.6578 - val_mse: 8.6578 - val_mae: 2.4011 - lr: 8.9413e-04 - 3s/epoch - 5ms/step\n",
            "Epoch 2/500\n",
            "556/556 - 2s - loss: 31.8820 - mse: 31.8820 - mae: 1.5753 - val_loss: 10.2364 - val_mse: 10.2364 - val_mae: 2.5586 - lr: 8.9413e-04 - 2s/epoch - 3ms/step\n",
            "Epoch 3/500\n",
            "556/556 - 2s - loss: 31.6586 - mse: 31.6586 - mae: 1.5672 - val_loss: 10.6873 - val_mse: 10.6873 - val_mae: 2.5821 - lr: 8.9413e-04 - 2s/epoch - 3ms/step\n",
            "Epoch 4/500\n",
            "556/556 - 2s - loss: 31.4285 - mse: 31.4285 - mae: 1.5524 - val_loss: 10.8152 - val_mse: 10.8152 - val_mae: 2.5360 - lr: 8.9413e-04 - 2s/epoch - 3ms/step\n",
            "Epoch 5/500\n",
            "556/556 - 2s - loss: 31.2703 - mse: 31.2703 - mae: 1.5493 - val_loss: 12.6528 - val_mse: 12.6528 - val_mae: 2.7660 - lr: 8.9413e-04 - 2s/epoch - 3ms/step\n",
            "Epoch 6/500\n",
            "556/556 - 2s - loss: 31.1085 - mse: 31.1085 - mae: 1.5387 - val_loss: 11.5974 - val_mse: 11.5974 - val_mae: 2.7366 - lr: 8.9413e-04 - 2s/epoch - 4ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-10-10 09:14:44,054]\u001b[0m Finished trial#4 resulted in value: 11.253333333333332. Current best value is 10.4 with parameters: {'activation': 'tanh', 'num_hidden_layer': 2, 'i': 5, 'learning_rate': 0.001033313287653061}.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Score for inner fold : loss of 11.597445487976074\n",
            "Trial Number:5\n",
            "-----Fold:0--------\n",
            "Epoch 1/500\n",
            "556/556 - 2s - loss: 31.8361 - mse: 31.8361 - mae: 1.6287 - val_loss: 14.3134 - val_mse: 14.3134 - val_mae: 3.0403 - lr: 0.0087 - 2s/epoch - 4ms/step\n",
            "Epoch 2/500\n",
            "556/556 - 1s - loss: 31.0654 - mse: 31.0654 - mae: 1.5758 - val_loss: 11.2218 - val_mse: 11.2218 - val_mae: 2.6831 - lr: 0.0087 - 1s/epoch - 2ms/step\n",
            "Epoch 3/500\n",
            "556/556 - 1s - loss: 31.1479 - mse: 31.1479 - mae: 1.5700 - val_loss: 10.5947 - val_mse: 10.5947 - val_mae: 2.6189 - lr: 0.0087 - 1s/epoch - 2ms/step\n",
            "Epoch 4/500\n",
            "556/556 - 1s - loss: 30.7911 - mse: 30.7911 - mae: 1.5635 - val_loss: 10.6448 - val_mse: 10.6448 - val_mae: 2.3723 - lr: 0.0087 - 1s/epoch - 2ms/step\n",
            "Epoch 5/500\n",
            "556/556 - 1s - loss: 30.8306 - mse: 30.8306 - mae: 1.5546 - val_loss: 9.2819 - val_mse: 9.2819 - val_mae: 2.3180 - lr: 0.0087 - 1s/epoch - 2ms/step\n",
            "Epoch 6/500\n",
            "556/556 - 1s - loss: 30.8400 - mse: 30.8400 - mae: 1.5499 - val_loss: 12.8906 - val_mse: 12.8906 - val_mae: 2.8201 - lr: 0.0087 - 1s/epoch - 2ms/step\n",
            "Epoch 7/500\n",
            "556/556 - 1s - loss: 30.6299 - mse: 30.6299 - mae: 1.5574 - val_loss: 15.8746 - val_mse: 15.8746 - val_mae: 2.9154 - lr: 0.0087 - 1s/epoch - 2ms/step\n",
            "Epoch 8/500\n",
            "556/556 - 1s - loss: 31.5527 - mse: 31.5527 - mae: 1.5615 - val_loss: 10.4155 - val_mse: 10.4155 - val_mae: 2.5866 - lr: 0.0087 - 1s/epoch - 2ms/step\n",
            "Epoch 9/500\n",
            "556/556 - 1s - loss: 30.8018 - mse: 30.8018 - mae: 1.5421 - val_loss: 11.7444 - val_mse: 11.7444 - val_mae: 2.7657 - lr: 0.0087 - 1s/epoch - 2ms/step\n",
            "Epoch 10/500\n",
            "556/556 - 1s - loss: 30.5074 - mse: 30.5074 - mae: 1.5423 - val_loss: 14.1259 - val_mse: 14.1259 - val_mae: 3.0012 - lr: 0.0087 - 1s/epoch - 2ms/step\n",
            "Score for inner fold : loss of 14.125901222229004\n",
            "-----Fold:1--------\n",
            "Epoch 1/500\n",
            "556/556 - 2s - loss: 33.1281 - mse: 33.1281 - mae: 1.6481 - val_loss: 18.1176 - val_mse: 18.1176 - val_mae: 2.7738 - lr: 0.0087 - 2s/epoch - 4ms/step\n",
            "Epoch 2/500\n",
            "556/556 - 1s - loss: 32.7155 - mse: 32.7155 - mae: 1.5941 - val_loss: 7.6674 - val_mse: 7.6674 - val_mae: 2.1943 - lr: 0.0087 - 1s/epoch - 2ms/step\n",
            "Epoch 3/500\n",
            "556/556 - 1s - loss: 32.8503 - mse: 32.8503 - mae: 1.5845 - val_loss: 10.8790 - val_mse: 10.8790 - val_mae: 2.3277 - lr: 0.0087 - 1s/epoch - 2ms/step\n",
            "Epoch 4/500\n",
            "556/556 - 1s - loss: 32.4562 - mse: 32.4562 - mae: 1.5823 - val_loss: 12.8454 - val_mse: 12.8454 - val_mae: 2.3509 - lr: 0.0087 - 1s/epoch - 2ms/step\n",
            "Epoch 5/500\n",
            "556/556 - 1s - loss: 32.6354 - mse: 32.6354 - mae: 1.5709 - val_loss: 13.9183 - val_mse: 13.9183 - val_mae: 2.9771 - lr: 0.0087 - 1s/epoch - 2ms/step\n",
            "Epoch 6/500\n",
            "556/556 - 1s - loss: 31.6818 - mse: 31.6818 - mae: 1.5702 - val_loss: 20.1861 - val_mse: 20.1861 - val_mae: 2.7866 - lr: 0.0087 - 1s/epoch - 2ms/step\n",
            "Epoch 7/500\n",
            "556/556 - 1s - loss: 32.5288 - mse: 32.5288 - mae: 1.5633 - val_loss: 15.7829 - val_mse: 15.7829 - val_mae: 2.5935 - lr: 0.0087 - 1s/epoch - 2ms/step\n",
            "Score for inner fold : loss of 15.782888412475586\n",
            "-----Fold:2--------\n",
            "Epoch 1/500\n",
            "556/556 - 2s - loss: 12.3292 - mse: 12.3292 - mae: 1.6131 - val_loss: 10.7266 - val_mse: 10.7266 - val_mae: 2.5352 - lr: 0.0087 - 2s/epoch - 4ms/step\n",
            "Epoch 2/500\n",
            "556/556 - 1s - loss: 10.8669 - mse: 10.8669 - mae: 1.5421 - val_loss: 24.0377 - val_mse: 24.0377 - val_mae: 2.8181 - lr: 0.0087 - 1s/epoch - 2ms/step\n",
            "Epoch 3/500\n",
            "556/556 - 1s - loss: 11.1978 - mse: 11.1978 - mae: 1.5336 - val_loss: 16.2130 - val_mse: 16.2130 - val_mae: 2.9761 - lr: 0.0087 - 1s/epoch - 2ms/step\n",
            "Epoch 4/500\n",
            "556/556 - 1s - loss: 10.8170 - mse: 10.8170 - mae: 1.5218 - val_loss: 17.7619 - val_mse: 17.7619 - val_mae: 2.8313 - lr: 0.0087 - 1s/epoch - 2ms/step\n",
            "Epoch 5/500\n",
            "556/556 - 1s - loss: 11.0298 - mse: 11.0298 - mae: 1.5225 - val_loss: 10.2534 - val_mse: 10.2534 - val_mae: 2.3922 - lr: 0.0087 - 1s/epoch - 2ms/step\n",
            "Epoch 6/500\n",
            "556/556 - 1s - loss: 10.4985 - mse: 10.4985 - mae: 1.5266 - val_loss: 10.9936 - val_mse: 10.9936 - val_mae: 2.5676 - lr: 0.0087 - 1s/epoch - 2ms/step\n",
            "Epoch 7/500\n",
            "556/556 - 1s - loss: 10.0255 - mse: 10.0255 - mae: 1.5185 - val_loss: 15.5906 - val_mse: 15.5906 - val_mae: 2.4574 - lr: 0.0087 - 1s/epoch - 2ms/step\n",
            "Epoch 8/500\n",
            "556/556 - 1s - loss: 11.1507 - mse: 11.1507 - mae: 1.5118 - val_loss: 15.6046 - val_mse: 15.6046 - val_mae: 2.9234 - lr: 0.0087 - 1s/epoch - 2ms/step\n",
            "Epoch 9/500\n",
            "556/556 - 1s - loss: 11.2451 - mse: 11.2451 - mae: 1.5121 - val_loss: 23.0393 - val_mse: 23.0393 - val_mae: 2.5552 - lr: 0.0087 - 1s/epoch - 2ms/step\n",
            "Epoch 10/500\n",
            "556/556 - 1s - loss: 9.7334 - mse: 9.7334 - mae: 1.4966 - val_loss: 10.5918 - val_mse: 10.5918 - val_mae: 2.4780 - lr: 0.0087 - 1s/epoch - 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-10-10 09:15:24,597]\u001b[0m Finished trial#5 resulted in value: 13.5. Current best value is 10.4 with parameters: {'activation': 'tanh', 'num_hidden_layer': 2, 'i': 5, 'learning_rate': 0.001033313287653061}.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Score for inner fold : loss of 10.591785430908203\n",
            "Trial Number:6\n",
            "-----Fold:0--------\n",
            "Epoch 1/500\n",
            "556/556 - 2s - loss: 16.7570 - mse: 16.7570 - mae: 1.8345 - val_loss: 8.0809 - val_mse: 8.0809 - val_mae: 2.5497 - lr: 0.0011 - 2s/epoch - 3ms/step\n",
            "Epoch 2/500\n",
            "556/556 - 1s - loss: 13.9468 - mse: 13.9468 - mae: 1.6209 - val_loss: 8.2301 - val_mse: 8.2301 - val_mae: 2.5146 - lr: 0.0011 - 1s/epoch - 2ms/step\n",
            "Epoch 3/500\n",
            "556/556 - 1s - loss: 13.3503 - mse: 13.3503 - mae: 1.5871 - val_loss: 8.9171 - val_mse: 8.9171 - val_mae: 2.5933 - lr: 0.0011 - 1s/epoch - 2ms/step\n",
            "Epoch 4/500\n",
            "556/556 - 1s - loss: 13.0004 - mse: 13.0004 - mae: 1.5662 - val_loss: 8.8413 - val_mse: 8.8413 - val_mae: 2.5522 - lr: 0.0011 - 1s/epoch - 2ms/step\n",
            "Epoch 5/500\n",
            "556/556 - 1s - loss: 12.7075 - mse: 12.7075 - mae: 1.5541 - val_loss: 8.6079 - val_mse: 8.6079 - val_mae: 2.5080 - lr: 0.0011 - 1s/epoch - 2ms/step\n",
            "Epoch 6/500\n",
            "556/556 - 1s - loss: 12.4412 - mse: 12.4412 - mae: 1.5369 - val_loss: 10.0074 - val_mse: 10.0074 - val_mae: 2.6788 - lr: 0.0011 - 1s/epoch - 2ms/step\n",
            "Score for inner fold : loss of 10.007436752319336\n",
            "-----Fold:1--------\n",
            "Epoch 1/500\n",
            "556/556 - 2s - loss: 36.6649 - mse: 36.6649 - mae: 1.8407 - val_loss: 8.3425 - val_mse: 8.3425 - val_mae: 2.5960 - lr: 0.0011 - 2s/epoch - 3ms/step\n",
            "Epoch 2/500\n",
            "556/556 - 1s - loss: 33.8592 - mse: 33.8592 - mae: 1.6391 - val_loss: 8.9798 - val_mse: 8.9798 - val_mae: 2.6454 - lr: 0.0011 - 1s/epoch - 2ms/step\n",
            "Epoch 3/500\n",
            "556/556 - 1s - loss: 33.2706 - mse: 33.2706 - mae: 1.6103 - val_loss: 9.4784 - val_mse: 9.4784 - val_mae: 2.6775 - lr: 0.0011 - 1s/epoch - 2ms/step\n",
            "Epoch 4/500\n",
            "556/556 - 1s - loss: 32.8773 - mse: 32.8773 - mae: 1.6004 - val_loss: 8.7377 - val_mse: 8.7377 - val_mae: 2.5137 - lr: 0.0011 - 1s/epoch - 2ms/step\n",
            "Epoch 5/500\n",
            "556/556 - 1s - loss: 32.6145 - mse: 32.6145 - mae: 1.5843 - val_loss: 9.8428 - val_mse: 9.8428 - val_mae: 2.6359 - lr: 0.0011 - 981ms/epoch - 2ms/step\n",
            "Epoch 6/500\n",
            "556/556 - 1s - loss: 32.3432 - mse: 32.3432 - mae: 1.5802 - val_loss: 9.9670 - val_mse: 9.9670 - val_mae: 2.6888 - lr: 0.0011 - 984ms/epoch - 2ms/step\n",
            "Score for inner fold : loss of 9.966998100280762\n",
            "-----Fold:2--------\n",
            "Epoch 1/500\n",
            "556/556 - 2s - loss: 36.7562 - mse: 36.7562 - mae: 1.8196 - val_loss: 8.3786 - val_mse: 8.3786 - val_mae: 2.5302 - lr: 0.0011 - 2s/epoch - 3ms/step\n",
            "Epoch 2/500\n",
            "556/556 - 1s - loss: 34.4188 - mse: 34.4188 - mae: 1.6317 - val_loss: 9.3507 - val_mse: 9.3507 - val_mae: 2.6479 - lr: 0.0011 - 1s/epoch - 2ms/step\n",
            "Epoch 3/500\n",
            "556/556 - 1s - loss: 33.9537 - mse: 33.9537 - mae: 1.6210 - val_loss: 8.9766 - val_mse: 8.9766 - val_mae: 2.5832 - lr: 0.0011 - 1s/epoch - 2ms/step\n",
            "Epoch 4/500\n",
            "556/556 - 1s - loss: 33.5948 - mse: 33.5948 - mae: 1.5958 - val_loss: 9.4747 - val_mse: 9.4747 - val_mae: 2.6000 - lr: 0.0011 - 1s/epoch - 2ms/step\n",
            "Epoch 5/500\n",
            "556/556 - 1s - loss: 33.2730 - mse: 33.2730 - mae: 1.5912 - val_loss: 9.7171 - val_mse: 9.7171 - val_mae: 2.6386 - lr: 0.0011 - 1s/epoch - 2ms/step\n",
            "Epoch 6/500\n",
            "556/556 - 1s - loss: 33.0439 - mse: 33.0439 - mae: 1.5858 - val_loss: 10.1164 - val_mse: 10.1164 - val_mae: 2.6612 - lr: 0.0011 - 1s/epoch - 2ms/step\n",
            "Score for inner fold : loss of 10.116438865661621\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-10-10 09:15:48,281]\u001b[0m Finished trial#6 resulted in value: 10.033333333333333. Current best value is 10.033333333333333 with parameters: {'activation': 'tanh', 'num_hidden_layer': 3, 'i': 3, 'learning_rate': 0.0011400297275064822}.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trial Number:7\n",
            "-----Fold:0--------\n",
            "Epoch 1/500\n",
            "556/556 - 2s - loss: 13.9493 - mse: 13.9493 - mae: 1.6969 - val_loss: 10.0736 - val_mse: 10.0736 - val_mae: 2.5703 - lr: 2.0077e-04 - 2s/epoch - 3ms/step\n",
            "Epoch 2/500\n",
            "556/556 - 1s - loss: 11.2347 - mse: 11.2347 - mae: 1.5960 - val_loss: 11.4071 - val_mse: 11.4071 - val_mae: 2.6234 - lr: 2.0077e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 3/500\n",
            "556/556 - 1s - loss: 11.0475 - mse: 11.0475 - mae: 1.5718 - val_loss: 12.2367 - val_mse: 12.2367 - val_mae: 2.6511 - lr: 2.0077e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 4/500\n",
            "556/556 - 1s - loss: 10.8022 - mse: 10.8022 - mae: 1.5573 - val_loss: 12.1911 - val_mse: 12.1911 - val_mae: 2.6023 - lr: 2.0077e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 5/500\n",
            "556/556 - 1s - loss: 10.6834 - mse: 10.6834 - mae: 1.5446 - val_loss: 13.4407 - val_mse: 13.4407 - val_mae: 2.6898 - lr: 2.0077e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 6/500\n",
            "556/556 - 1s - loss: 10.6552 - mse: 10.6552 - mae: 1.5468 - val_loss: 13.5911 - val_mse: 13.5911 - val_mae: 2.6492 - lr: 2.0077e-04 - 1s/epoch - 2ms/step\n",
            "Score for inner fold : loss of 13.59107780456543\n",
            "-----Fold:1--------\n",
            "Epoch 1/500\n",
            "556/556 - 2s - loss: 35.2734 - mse: 35.2734 - mae: 1.6875 - val_loss: 9.5636 - val_mse: 9.5636 - val_mae: 2.4890 - lr: 2.0077e-04 - 2s/epoch - 3ms/step\n",
            "Epoch 2/500\n",
            "556/556 - 1s - loss: 33.2393 - mse: 33.2393 - mae: 1.6250 - val_loss: 10.8709 - val_mse: 10.8709 - val_mae: 2.6028 - lr: 2.0077e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 3/500\n",
            "556/556 - 1s - loss: 32.7411 - mse: 32.7411 - mae: 1.6042 - val_loss: 11.6070 - val_mse: 11.6070 - val_mae: 2.6260 - lr: 2.0077e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 4/500\n",
            "556/556 - 1s - loss: 32.6442 - mse: 32.6442 - mae: 1.5904 - val_loss: 11.6070 - val_mse: 11.6070 - val_mae: 2.6073 - lr: 2.0077e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 5/500\n",
            "556/556 - 1s - loss: 32.5841 - mse: 32.5841 - mae: 1.5820 - val_loss: 11.3260 - val_mse: 11.3260 - val_mae: 2.5637 - lr: 2.0077e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 6/500\n",
            "556/556 - 1s - loss: 32.5052 - mse: 32.5052 - mae: 1.5695 - val_loss: 11.1370 - val_mse: 11.1370 - val_mae: 2.5592 - lr: 2.0077e-04 - 1s/epoch - 2ms/step\n",
            "Score for inner fold : loss of 11.13703727722168\n",
            "-----Fold:2--------\n",
            "Epoch 1/500\n",
            "556/556 - 2s - loss: 34.4224 - mse: 34.4224 - mae: 1.6630 - val_loss: 10.2393 - val_mse: 10.2393 - val_mae: 2.5489 - lr: 2.0077e-04 - 2s/epoch - 3ms/step\n",
            "Epoch 2/500\n",
            "556/556 - 1s - loss: 32.6404 - mse: 32.6404 - mae: 1.6124 - val_loss: 10.8675 - val_mse: 10.8675 - val_mae: 2.5432 - lr: 2.0077e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 3/500\n",
            "556/556 - 1s - loss: 32.2495 - mse: 32.2495 - mae: 1.5895 - val_loss: 11.5993 - val_mse: 11.5993 - val_mae: 2.5887 - lr: 2.0077e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 4/500\n",
            "556/556 - 1s - loss: 32.0888 - mse: 32.0888 - mae: 1.5769 - val_loss: 12.3610 - val_mse: 12.3610 - val_mae: 2.6169 - lr: 2.0077e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 5/500\n",
            "556/556 - 1s - loss: 31.9587 - mse: 31.9587 - mae: 1.5647 - val_loss: 12.7512 - val_mse: 12.7512 - val_mae: 2.6230 - lr: 2.0077e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 6/500\n",
            "556/556 - 1s - loss: 31.8892 - mse: 31.8892 - mae: 1.5516 - val_loss: 13.3923 - val_mse: 13.3923 - val_mae: 2.6854 - lr: 2.0077e-04 - 1s/epoch - 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-10-10 09:16:14,012]\u001b[0m Finished trial#7 resulted in value: 12.706666666666669. Current best value is 10.033333333333333 with parameters: {'activation': 'tanh', 'num_hidden_layer': 3, 'i': 3, 'learning_rate': 0.0011400297275064822}.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Score for inner fold : loss of 13.392334938049316\n",
            "Trial Number:8\n",
            "-----Fold:0--------\n",
            "Epoch 1/500\n",
            "556/556 - 2s - loss: 12.5348 - mse: 12.5348 - mae: 1.6203 - val_loss: 11.4731 - val_mse: 11.4731 - val_mae: 2.8435 - lr: 0.0045 - 2s/epoch - 4ms/step\n",
            "Epoch 2/500\n",
            "556/556 - 1s - loss: 11.9626 - mse: 11.9626 - mae: 1.5672 - val_loss: 7.9043 - val_mse: 7.9043 - val_mae: 2.3615 - lr: 0.0045 - 1s/epoch - 3ms/step\n",
            "Epoch 3/500\n",
            "556/556 - 1s - loss: 11.6672 - mse: 11.6672 - mae: 1.5487 - val_loss: 11.7287 - val_mse: 11.7287 - val_mae: 2.8319 - lr: 0.0045 - 1s/epoch - 3ms/step\n",
            "Epoch 4/500\n",
            "556/556 - 1s - loss: 11.5593 - mse: 11.5593 - mae: 1.5457 - val_loss: 12.4886 - val_mse: 12.4886 - val_mae: 2.8287 - lr: 0.0045 - 1s/epoch - 2ms/step\n",
            "Epoch 5/500\n",
            "556/556 - 1s - loss: 11.4357 - mse: 11.4357 - mae: 1.5487 - val_loss: 10.0332 - val_mse: 10.0332 - val_mae: 2.5891 - lr: 0.0045 - 1s/epoch - 3ms/step\n",
            "Epoch 6/500\n",
            "556/556 - 1s - loss: 11.4354 - mse: 11.4354 - mae: 1.5389 - val_loss: 11.0511 - val_mse: 11.0511 - val_mae: 2.6019 - lr: 0.0045 - 1s/epoch - 3ms/step\n",
            "Epoch 7/500\n",
            "556/556 - 1s - loss: 11.3592 - mse: 11.3592 - mae: 1.5364 - val_loss: 13.0487 - val_mse: 13.0487 - val_mae: 2.8382 - lr: 0.0045 - 1s/epoch - 3ms/step\n",
            "Score for inner fold : loss of 13.048662185668945\n",
            "-----Fold:1--------\n",
            "Epoch 1/500\n",
            "556/556 - 2s - loss: 33.5048 - mse: 33.5048 - mae: 1.6563 - val_loss: 7.1875 - val_mse: 7.1875 - val_mae: 2.2453 - lr: 0.0045 - 2s/epoch - 4ms/step\n",
            "Epoch 2/500\n",
            "556/556 - 1s - loss: 32.8071 - mse: 32.8071 - mae: 1.5957 - val_loss: 11.5866 - val_mse: 11.5866 - val_mae: 2.6832 - lr: 0.0045 - 1s/epoch - 2ms/step\n",
            "Epoch 3/500\n",
            "556/556 - 1s - loss: 32.5822 - mse: 32.5822 - mae: 1.5834 - val_loss: 12.1496 - val_mse: 12.1496 - val_mae: 2.6376 - lr: 0.0045 - 1s/epoch - 2ms/step\n",
            "Epoch 4/500\n",
            "556/556 - 1s - loss: 32.3055 - mse: 32.3055 - mae: 1.5762 - val_loss: 12.7776 - val_mse: 12.7776 - val_mae: 2.8363 - lr: 0.0045 - 1s/epoch - 2ms/step\n",
            "Epoch 5/500\n",
            "556/556 - 1s - loss: 32.2545 - mse: 32.2545 - mae: 1.5772 - val_loss: 13.0007 - val_mse: 13.0007 - val_mae: 2.6448 - lr: 0.0045 - 1s/epoch - 2ms/step\n",
            "Epoch 6/500\n",
            "556/556 - 1s - loss: 32.0139 - mse: 32.0139 - mae: 1.5723 - val_loss: 12.0204 - val_mse: 12.0204 - val_mae: 2.6965 - lr: 0.0045 - 1s/epoch - 2ms/step\n",
            "Score for inner fold : loss of 12.020383834838867\n",
            "-----Fold:2--------\n",
            "Epoch 1/500\n",
            "556/556 - 2s - loss: 31.1214 - mse: 31.1214 - mae: 1.6341 - val_loss: 12.1213 - val_mse: 12.1213 - val_mae: 2.8592 - lr: 0.0045 - 2s/epoch - 4ms/step\n",
            "Epoch 2/500\n",
            "556/556 - 1s - loss: 30.5185 - mse: 30.5185 - mae: 1.5789 - val_loss: 11.5178 - val_mse: 11.5178 - val_mae: 2.8089 - lr: 0.0045 - 1s/epoch - 2ms/step\n",
            "Epoch 3/500\n",
            "556/556 - 1s - loss: 30.3323 - mse: 30.3323 - mae: 1.5678 - val_loss: 11.3167 - val_mse: 11.3167 - val_mae: 2.8167 - lr: 0.0045 - 1s/epoch - 2ms/step\n",
            "Epoch 4/500\n",
            "556/556 - 1s - loss: 30.1263 - mse: 30.1263 - mae: 1.5573 - val_loss: 16.6392 - val_mse: 16.6392 - val_mae: 3.0685 - lr: 0.0045 - 1s/epoch - 2ms/step\n",
            "Epoch 5/500\n",
            "556/556 - 1s - loss: 29.9010 - mse: 29.9010 - mae: 1.5567 - val_loss: 12.4880 - val_mse: 12.4880 - val_mae: 2.7268 - lr: 0.0045 - 1s/epoch - 3ms/step\n",
            "Epoch 6/500\n",
            "556/556 - 1s - loss: 29.7869 - mse: 29.7869 - mae: 1.5613 - val_loss: 12.4780 - val_mse: 12.4780 - val_mae: 2.6086 - lr: 0.0045 - 1s/epoch - 2ms/step\n",
            "Epoch 7/500\n",
            "556/556 - 1s - loss: 29.6417 - mse: 29.6417 - mae: 1.5458 - val_loss: 10.9137 - val_mse: 10.9137 - val_mae: 2.6187 - lr: 0.0045 - 1s/epoch - 2ms/step\n",
            "Epoch 8/500\n",
            "556/556 - 1s - loss: 29.5895 - mse: 29.5895 - mae: 1.5425 - val_loss: 12.5927 - val_mse: 12.5927 - val_mae: 2.7511 - lr: 0.0045 - 1s/epoch - 2ms/step\n",
            "Epoch 9/500\n",
            "556/556 - 1s - loss: 29.5856 - mse: 29.5856 - mae: 1.5477 - val_loss: 14.1545 - val_mse: 14.1545 - val_mae: 2.9444 - lr: 0.0045 - 1s/epoch - 2ms/step\n",
            "Epoch 10/500\n",
            "556/556 - 1s - loss: 29.4028 - mse: 29.4028 - mae: 1.5428 - val_loss: 10.7096 - val_mse: 10.7096 - val_mae: 2.6248 - lr: 0.0045 - 1s/epoch - 2ms/step\n",
            "Epoch 11/500\n",
            "556/556 - 1s - loss: 29.2406 - mse: 29.2406 - mae: 1.5341 - val_loss: 11.3643 - val_mse: 11.3643 - val_mae: 2.6689 - lr: 0.0045 - 1s/epoch - 2ms/step\n",
            "Epoch 12/500\n",
            "556/556 - 1s - loss: 29.2688 - mse: 29.2688 - mae: 1.5427 - val_loss: 10.3982 - val_mse: 10.3982 - val_mae: 2.4308 - lr: 0.0045 - 1s/epoch - 3ms/step\n",
            "Epoch 13/500\n",
            "556/556 - 1s - loss: 29.0917 - mse: 29.0917 - mae: 1.5274 - val_loss: 11.3140 - val_mse: 11.3140 - val_mae: 2.4751 - lr: 0.0045 - 1s/epoch - 3ms/step\n",
            "Epoch 14/500\n",
            "556/556 - 1s - loss: 29.1260 - mse: 29.1260 - mae: 1.5261 - val_loss: 13.1573 - val_mse: 13.1573 - val_mae: 2.6777 - lr: 0.0045 - 1s/epoch - 3ms/step\n",
            "Epoch 15/500\n",
            "556/556 - 1s - loss: 29.0339 - mse: 29.0339 - mae: 1.5222 - val_loss: 11.7636 - val_mse: 11.7636 - val_mae: 2.5595 - lr: 0.0045 - 1s/epoch - 3ms/step\n",
            "Epoch 16/500\n",
            "556/556 - 1s - loss: 28.9376 - mse: 28.9376 - mae: 1.5196 - val_loss: 13.5278 - val_mse: 13.5278 - val_mae: 2.8509 - lr: 0.0045 - 1s/epoch - 3ms/step\n",
            "Epoch 17/500\n",
            "556/556 - 1s - loss: 28.8979 - mse: 28.8979 - mae: 1.5286 - val_loss: 13.3412 - val_mse: 13.3412 - val_mae: 2.5962 - lr: 0.0045 - 1s/epoch - 3ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-10-10 09:17:00,468]\u001b[0m Finished trial#8 resulted in value: 12.803333333333333. Current best value is 10.033333333333333 with parameters: {'activation': 'tanh', 'num_hidden_layer': 3, 'i': 3, 'learning_rate': 0.0011400297275064822}.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Score for inner fold : loss of 13.341240882873535\n",
            "Trial Number:9\n",
            "-----Fold:0--------\n",
            "Epoch 1/500\n",
            "556/556 - 2s - loss: 32.8355 - mse: 32.8355 - mae: 1.7126 - val_loss: 14.8958 - val_mse: 14.8958 - val_mae: 2.6907 - lr: 0.0024 - 2s/epoch - 3ms/step\n",
            "Epoch 2/500\n",
            "556/556 - 1s - loss: 31.6947 - mse: 31.6947 - mae: 1.6025 - val_loss: 14.8601 - val_mse: 14.8601 - val_mae: 2.7237 - lr: 0.0024 - 1s/epoch - 2ms/step\n",
            "Epoch 3/500\n",
            "556/556 - 1s - loss: 31.7949 - mse: 31.7949 - mae: 1.6006 - val_loss: 12.3246 - val_mse: 12.3246 - val_mae: 2.5200 - lr: 0.0024 - 1s/epoch - 2ms/step\n",
            "Epoch 4/500\n",
            "556/556 - 1s - loss: 31.6337 - mse: 31.6337 - mae: 1.5955 - val_loss: 13.7449 - val_mse: 13.7449 - val_mae: 2.7330 - lr: 0.0024 - 1s/epoch - 2ms/step\n",
            "Epoch 5/500\n",
            "556/556 - 1s - loss: 31.6241 - mse: 31.6241 - mae: 1.5962 - val_loss: 13.9586 - val_mse: 13.9586 - val_mae: 2.7501 - lr: 0.0024 - 1s/epoch - 2ms/step\n",
            "Epoch 6/500\n",
            "556/556 - 1s - loss: 31.6189 - mse: 31.6189 - mae: 1.5965 - val_loss: 13.2361 - val_mse: 13.2361 - val_mae: 2.6551 - lr: 0.0024 - 1s/epoch - 2ms/step\n",
            "Epoch 7/500\n",
            "556/556 - 1s - loss: 31.5842 - mse: 31.5842 - mae: 1.5986 - val_loss: 12.5650 - val_mse: 12.5650 - val_mae: 2.6057 - lr: 0.0024 - 1s/epoch - 2ms/step\n",
            "Epoch 8/500\n",
            "556/556 - 1s - loss: 31.6829 - mse: 31.6829 - mae: 1.5968 - val_loss: 13.8671 - val_mse: 13.8671 - val_mae: 2.6825 - lr: 0.0024 - 1s/epoch - 2ms/step\n",
            "Score for inner fold : loss of 13.867147445678711\n",
            "-----Fold:1--------\n",
            "Epoch 1/500\n",
            "556/556 - 2s - loss: 34.2529 - mse: 34.2529 - mae: 1.7037 - val_loss: 11.8982 - val_mse: 11.8982 - val_mae: 2.6030 - lr: 0.0024 - 2s/epoch - 3ms/step\n",
            "Epoch 2/500\n",
            "556/556 - 1s - loss: 33.0525 - mse: 33.0525 - mae: 1.6160 - val_loss: 13.5875 - val_mse: 13.5875 - val_mae: 2.6605 - lr: 0.0024 - 1s/epoch - 2ms/step\n",
            "Epoch 3/500\n",
            "556/556 - 1s - loss: 32.9046 - mse: 32.9046 - mae: 1.6078 - val_loss: 13.2337 - val_mse: 13.2337 - val_mae: 2.6853 - lr: 0.0024 - 982ms/epoch - 2ms/step\n",
            "Epoch 4/500\n",
            "556/556 - 1s - loss: 32.9028 - mse: 32.9028 - mae: 1.6100 - val_loss: 13.9843 - val_mse: 13.9843 - val_mae: 2.6669 - lr: 0.0024 - 1s/epoch - 2ms/step\n",
            "Epoch 5/500\n",
            "556/556 - 1s - loss: 33.0167 - mse: 33.0167 - mae: 1.6060 - val_loss: 13.9166 - val_mse: 13.9166 - val_mae: 2.6123 - lr: 0.0024 - 1s/epoch - 2ms/step\n",
            "Epoch 6/500\n",
            "556/556 - 1s - loss: 32.8687 - mse: 32.8687 - mae: 1.6083 - val_loss: 13.5797 - val_mse: 13.5797 - val_mae: 2.5635 - lr: 0.0024 - 1s/epoch - 2ms/step\n",
            "Score for inner fold : loss of 13.579732894897461\n",
            "-----Fold:2--------\n",
            "Epoch 1/500\n",
            "556/556 - 2s - loss: 12.8151 - mse: 12.8151 - mae: 1.6912 - val_loss: 10.7422 - val_mse: 10.7422 - val_mae: 2.6285 - lr: 0.0024 - 2s/epoch - 3ms/step\n",
            "Epoch 2/500\n",
            "556/556 - 1s - loss: 11.5706 - mse: 11.5706 - mae: 1.5888 - val_loss: 12.5679 - val_mse: 12.5679 - val_mae: 2.7143 - lr: 0.0024 - 1s/epoch - 2ms/step\n",
            "Epoch 3/500\n",
            "556/556 - 1s - loss: 11.8806 - mse: 11.8806 - mae: 1.5828 - val_loss: 10.9101 - val_mse: 10.9101 - val_mae: 2.6036 - lr: 0.0024 - 1s/epoch - 2ms/step\n",
            "Epoch 4/500\n",
            "556/556 - 1s - loss: 11.9424 - mse: 11.9424 - mae: 1.5826 - val_loss: 13.7430 - val_mse: 13.7430 - val_mae: 2.8802 - lr: 0.0024 - 1s/epoch - 2ms/step\n",
            "Epoch 5/500\n",
            "556/556 - 1s - loss: 11.6772 - mse: 11.6772 - mae: 1.5849 - val_loss: 13.4776 - val_mse: 13.4776 - val_mae: 2.7344 - lr: 0.0024 - 995ms/epoch - 2ms/step\n",
            "Epoch 6/500\n",
            "556/556 - 1s - loss: 11.7554 - mse: 11.7554 - mae: 1.5820 - val_loss: 12.6478 - val_mse: 12.6478 - val_mae: 2.7072 - lr: 0.0024 - 1s/epoch - 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-10-10 09:17:25,901]\u001b[0m Finished trial#9 resulted in value: 13.366666666666667. Current best value is 10.033333333333333 with parameters: {'activation': 'tanh', 'num_hidden_layer': 3, 'i': 3, 'learning_rate': 0.0011400297275064822}.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Score for inner fold : loss of 12.647814750671387\n",
            "Trial Number:10\n",
            "-----Fold:0--------\n",
            "Epoch 1/500\n",
            "556/556 - 2s - loss: 18.1465 - mse: 18.1465 - mae: 1.9459 - val_loss: 6.2781 - val_mse: 6.2781 - val_mae: 2.2583 - lr: 3.2527e-04 - 2s/epoch - 4ms/step\n",
            "Epoch 2/500\n",
            "556/556 - 1s - loss: 14.0413 - mse: 14.0413 - mae: 1.6907 - val_loss: 7.8045 - val_mse: 7.8045 - val_mae: 2.5184 - lr: 3.2527e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 3/500\n",
            "556/556 - 1s - loss: 13.4075 - mse: 13.4075 - mae: 1.6512 - val_loss: 8.0892 - val_mse: 8.0892 - val_mae: 2.5486 - lr: 3.2527e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 4/500\n",
            "556/556 - 1s - loss: 13.0222 - mse: 13.0222 - mae: 1.6183 - val_loss: 8.4123 - val_mse: 8.4123 - val_mae: 2.5690 - lr: 3.2527e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 5/500\n",
            "556/556 - 1s - loss: 12.7445 - mse: 12.7445 - mae: 1.5989 - val_loss: 8.3464 - val_mse: 8.3464 - val_mae: 2.5424 - lr: 3.2527e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 6/500\n",
            "556/556 - 1s - loss: 12.5157 - mse: 12.5157 - mae: 1.5848 - val_loss: 8.4546 - val_mse: 8.4546 - val_mae: 2.5398 - lr: 3.2527e-04 - 1s/epoch - 2ms/step\n",
            "Score for inner fold : loss of 8.45460319519043\n",
            "-----Fold:1--------\n",
            "Epoch 1/500\n",
            "556/556 - 2s - loss: 40.1592 - mse: 40.1592 - mae: 2.0212 - val_loss: 7.4428 - val_mse: 7.4428 - val_mae: 2.4681 - lr: 3.2527e-04 - 2s/epoch - 3ms/step\n",
            "Epoch 2/500\n",
            "556/556 - 1s - loss: 35.9427 - mse: 35.9427 - mae: 1.7229 - val_loss: 8.1462 - val_mse: 8.1462 - val_mae: 2.5652 - lr: 3.2527e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 3/500\n",
            "556/556 - 1s - loss: 35.3936 - mse: 35.3936 - mae: 1.6821 - val_loss: 8.2066 - val_mse: 8.2066 - val_mae: 2.5295 - lr: 3.2527e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 4/500\n",
            "556/556 - 1s - loss: 35.0436 - mse: 35.0436 - mae: 1.6657 - val_loss: 8.3385 - val_mse: 8.3385 - val_mae: 2.5227 - lr: 3.2527e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 5/500\n",
            "556/556 - 1s - loss: 34.8112 - mse: 34.8112 - mae: 1.6500 - val_loss: 8.9135 - val_mse: 8.9135 - val_mae: 2.6107 - lr: 3.2527e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 6/500\n",
            "556/556 - 1s - loss: 34.6137 - mse: 34.6137 - mae: 1.6453 - val_loss: 8.7229 - val_mse: 8.7229 - val_mae: 2.5606 - lr: 3.2527e-04 - 1s/epoch - 2ms/step\n",
            "Score for inner fold : loss of 8.722879409790039\n",
            "-----Fold:2--------\n",
            "Epoch 1/500\n",
            "556/556 - 2s - loss: 39.7702 - mse: 39.7702 - mae: 2.0525 - val_loss: 6.2093 - val_mse: 6.2093 - val_mae: 2.2324 - lr: 3.2527e-04 - 2s/epoch - 4ms/step\n",
            "Epoch 2/500\n",
            "556/556 - 1s - loss: 35.7503 - mse: 35.7503 - mae: 1.7161 - val_loss: 7.9724 - val_mse: 7.9724 - val_mae: 2.5377 - lr: 3.2527e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 3/500\n",
            "556/556 - 1s - loss: 35.3642 - mse: 35.3642 - mae: 1.6945 - val_loss: 8.1894 - val_mse: 8.1894 - val_mae: 2.5633 - lr: 3.2527e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 4/500\n",
            "556/556 - 1s - loss: 35.0620 - mse: 35.0620 - mae: 1.6790 - val_loss: 8.4875 - val_mse: 8.4875 - val_mae: 2.6003 - lr: 3.2527e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 5/500\n",
            "556/556 - 1s - loss: 34.8217 - mse: 34.8217 - mae: 1.6640 - val_loss: 8.5450 - val_mse: 8.5450 - val_mae: 2.5822 - lr: 3.2527e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 6/500\n",
            "556/556 - 1s - loss: 34.6217 - mse: 34.6217 - mae: 1.6520 - val_loss: 8.4403 - val_mse: 8.4403 - val_mae: 2.5412 - lr: 3.2527e-04 - 1s/epoch - 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-10-10 09:17:50,759]\u001b[0m Finished trial#10 resulted in value: 8.536666666666667. Current best value is 8.536666666666667 with parameters: {'activation': 'tanh', 'num_hidden_layer': 4, 'i': 3, 'learning_rate': 0.00032526560986643067}.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Score for inner fold : loss of 8.440271377563477\n",
            "Trial Number:11\n",
            "-----Fold:0--------\n",
            "Epoch 1/500\n",
            "556/556 - 2s - loss: 20.5841 - mse: 20.5841 - mae: 2.1999 - val_loss: 3.5186 - val_mse: 3.5186 - val_mae: 1.5706 - lr: 2.2059e-04 - 2s/epoch - 4ms/step\n",
            "Epoch 2/500\n",
            "556/556 - 1s - loss: 15.6850 - mse: 15.6850 - mae: 1.7012 - val_loss: 7.8887 - val_mse: 7.8887 - val_mae: 2.5424 - lr: 2.2059e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 3/500\n",
            "556/556 - 1s - loss: 14.8132 - mse: 14.8132 - mae: 1.7106 - val_loss: 8.0755 - val_mse: 8.0755 - val_mae: 2.5436 - lr: 2.2059e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 4/500\n",
            "556/556 - 1s - loss: 14.4990 - mse: 14.4990 - mae: 1.6821 - val_loss: 8.1036 - val_mse: 8.1036 - val_mae: 2.5350 - lr: 2.2059e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 5/500\n",
            "556/556 - 1s - loss: 14.2513 - mse: 14.2513 - mae: 1.6666 - val_loss: 8.0458 - val_mse: 8.0458 - val_mae: 2.5147 - lr: 2.2059e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 6/500\n",
            "556/556 - 1s - loss: 14.0286 - mse: 14.0286 - mae: 1.6445 - val_loss: 8.2140 - val_mse: 8.2140 - val_mae: 2.5198 - lr: 2.2059e-04 - 1s/epoch - 2ms/step\n",
            "Score for inner fold : loss of 8.213987350463867\n",
            "-----Fold:1--------\n",
            "Epoch 1/500\n",
            "556/556 - 2s - loss: 41.6892 - mse: 41.6892 - mae: 2.1558 - val_loss: 3.7068 - val_mse: 3.7068 - val_mae: 1.6712 - lr: 2.2059e-04 - 2s/epoch - 4ms/step\n",
            "Epoch 2/500\n",
            "556/556 - 1s - loss: 36.8228 - mse: 36.8228 - mae: 1.7152 - val_loss: 7.6607 - val_mse: 7.6607 - val_mae: 2.5130 - lr: 2.2059e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 3/500\n",
            "556/556 - 1s - loss: 35.9674 - mse: 35.9674 - mae: 1.7122 - val_loss: 7.8996 - val_mse: 7.8996 - val_mae: 2.5245 - lr: 2.2059e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 4/500\n",
            "556/556 - 1s - loss: 35.6841 - mse: 35.6841 - mae: 1.6908 - val_loss: 8.0275 - val_mse: 8.0275 - val_mae: 2.5280 - lr: 2.2059e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 5/500\n",
            "556/556 - 1s - loss: 35.4658 - mse: 35.4658 - mae: 1.6752 - val_loss: 8.1769 - val_mse: 8.1769 - val_mae: 2.5488 - lr: 2.2059e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 6/500\n",
            "556/556 - 1s - loss: 35.2759 - mse: 35.2759 - mae: 1.6641 - val_loss: 8.3451 - val_mse: 8.3451 - val_mae: 2.5621 - lr: 2.2059e-04 - 1s/epoch - 2ms/step\n",
            "Score for inner fold : loss of 8.345142364501953\n",
            "-----Fold:2--------\n",
            "Epoch 1/500\n",
            "556/556 - 2s - loss: 40.0111 - mse: 40.0111 - mae: 2.1786 - val_loss: 2.9277 - val_mse: 2.9277 - val_mae: 1.4185 - lr: 2.2059e-04 - 2s/epoch - 4ms/step\n",
            "Epoch 2/500\n",
            "556/556 - 1s - loss: 35.4495 - mse: 35.4495 - mae: 1.6595 - val_loss: 7.7024 - val_mse: 7.7024 - val_mae: 2.5170 - lr: 2.2059e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 3/500\n",
            "556/556 - 1s - loss: 34.4495 - mse: 34.4495 - mae: 1.6922 - val_loss: 7.8867 - val_mse: 7.8867 - val_mae: 2.5305 - lr: 2.2059e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 4/500\n",
            "556/556 - 1s - loss: 34.1833 - mse: 34.1833 - mae: 1.6725 - val_loss: 7.9194 - val_mse: 7.9194 - val_mae: 2.5218 - lr: 2.2059e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 5/500\n",
            "556/556 - 1s - loss: 33.9715 - mse: 33.9715 - mae: 1.6535 - val_loss: 8.0903 - val_mse: 8.0903 - val_mae: 2.5292 - lr: 2.2059e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 6/500\n",
            "556/556 - 1s - loss: 33.7747 - mse: 33.7747 - mae: 1.6422 - val_loss: 8.4673 - val_mse: 8.4673 - val_mae: 2.5885 - lr: 2.2059e-04 - 1s/epoch - 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-10-10 09:18:15,723]\u001b[0m Finished trial#11 resulted in value: 8.343333333333334. Current best value is 8.343333333333334 with parameters: {'activation': 'tanh', 'num_hidden_layer': 4, 'i': 3, 'learning_rate': 0.0002205854006446135}.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Score for inner fold : loss of 8.467260360717773\n",
            "Trial Number:12\n",
            "-----Fold:0--------\n",
            "Epoch 1/500\n",
            "556/556 - 2s - loss: 21.3568 - mse: 21.3568 - mae: 2.2845 - val_loss: 2.3881 - val_mse: 2.3881 - val_mae: 1.2699 - lr: 1.8239e-04 - 2s/epoch - 4ms/step\n",
            "Epoch 2/500\n",
            "556/556 - 1s - loss: 16.8402 - mse: 16.8402 - mae: 1.6978 - val_loss: 6.8308 - val_mse: 6.8308 - val_mae: 2.3957 - lr: 1.8239e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 3/500\n",
            "556/556 - 1s - loss: 15.4958 - mse: 15.4958 - mae: 1.7054 - val_loss: 7.7065 - val_mse: 7.7065 - val_mae: 2.5170 - lr: 1.8239e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 4/500\n",
            "556/556 - 1s - loss: 15.1293 - mse: 15.1293 - mae: 1.6928 - val_loss: 7.8667 - val_mse: 7.8667 - val_mae: 2.5322 - lr: 1.8239e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 5/500\n",
            "556/556 - 1s - loss: 14.8513 - mse: 14.8513 - mae: 1.6714 - val_loss: 7.8202 - val_mse: 7.8202 - val_mae: 2.5066 - lr: 1.8239e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 6/500\n",
            "556/556 - 1s - loss: 14.6083 - mse: 14.6083 - mae: 1.6528 - val_loss: 7.9399 - val_mse: 7.9399 - val_mae: 2.5133 - lr: 1.8239e-04 - 1s/epoch - 2ms/step\n",
            "Score for inner fold : loss of 7.939857006072998\n",
            "-----Fold:1--------\n",
            "Epoch 1/500\n",
            "556/556 - 2s - loss: 41.4272 - mse: 41.4272 - mae: 2.3292 - val_loss: 1.9951 - val_mse: 1.9951 - val_mae: 1.1105 - lr: 1.8239e-04 - 2s/epoch - 4ms/step\n",
            "Epoch 2/500\n",
            "556/556 - 1s - loss: 36.7859 - mse: 36.7859 - mae: 1.6903 - val_loss: 7.0480 - val_mse: 7.0480 - val_mae: 2.4043 - lr: 1.8239e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 3/500\n",
            "556/556 - 1s - loss: 35.1943 - mse: 35.1943 - mae: 1.7130 - val_loss: 8.0567 - val_mse: 8.0567 - val_mae: 2.5592 - lr: 1.8239e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 4/500\n",
            "556/556 - 1s - loss: 34.9378 - mse: 34.9378 - mae: 1.7072 - val_loss: 8.2644 - val_mse: 8.2644 - val_mae: 2.5788 - lr: 1.8239e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 5/500\n",
            "556/556 - 1s - loss: 34.7563 - mse: 34.7563 - mae: 1.6979 - val_loss: 8.2842 - val_mse: 8.2842 - val_mae: 2.5750 - lr: 1.8239e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 6/500\n",
            "556/556 - 1s - loss: 34.5933 - mse: 34.5933 - mae: 1.6871 - val_loss: 8.3100 - val_mse: 8.3100 - val_mae: 2.5647 - lr: 1.8239e-04 - 1s/epoch - 2ms/step\n",
            "Score for inner fold : loss of 8.310016632080078\n",
            "-----Fold:2--------\n",
            "Epoch 1/500\n",
            "556/556 - 2s - loss: 41.3762 - mse: 41.3762 - mae: 2.3047 - val_loss: 2.1687 - val_mse: 2.1687 - val_mae: 1.1934 - lr: 1.8239e-04 - 2s/epoch - 4ms/step\n",
            "Epoch 2/500\n",
            "556/556 - 1s - loss: 36.2943 - mse: 36.2943 - mae: 1.6687 - val_loss: 7.2455 - val_mse: 7.2455 - val_mae: 2.4385 - lr: 1.8239e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 3/500\n",
            "556/556 - 1s - loss: 35.0144 - mse: 35.0144 - mae: 1.6738 - val_loss: 7.9621 - val_mse: 7.9621 - val_mae: 2.5427 - lr: 1.8239e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 4/500\n",
            "556/556 - 1s - loss: 34.7340 - mse: 34.7340 - mae: 1.6546 - val_loss: 7.8001 - val_mse: 7.8001 - val_mae: 2.5032 - lr: 1.8239e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 5/500\n",
            "556/556 - 1s - loss: 34.5417 - mse: 34.5417 - mae: 1.6336 - val_loss: 7.9448 - val_mse: 7.9448 - val_mae: 2.5069 - lr: 1.8239e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 6/500\n",
            "556/556 - 1s - loss: 34.3657 - mse: 34.3657 - mae: 1.6215 - val_loss: 7.9909 - val_mse: 7.9909 - val_mae: 2.4992 - lr: 1.8239e-04 - 1s/epoch - 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-10-10 09:18:40,566]\u001b[0m Finished trial#12 resulted in value: 8.08. Current best value is 8.08 with parameters: {'activation': 'tanh', 'num_hidden_layer': 4, 'i': 3, 'learning_rate': 0.00018239009621767336}.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Score for inner fold : loss of 7.990939140319824\n",
            "Trial Number:13\n",
            "-----Fold:0--------\n",
            "Epoch 1/500\n",
            "556/556 - 2s - loss: 21.7999 - mse: 21.7999 - mae: 2.4159 - val_loss: 1.9978 - val_mse: 1.9978 - val_mae: 1.0670 - lr: 1.0590e-04 - 2s/epoch - 4ms/step\n",
            "Epoch 2/500\n",
            "556/556 - 1s - loss: 17.2684 - mse: 17.2684 - mae: 1.7541 - val_loss: 6.1723 - val_mse: 6.1723 - val_mae: 2.1824 - lr: 1.0590e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 3/500\n",
            "556/556 - 1s - loss: 14.9976 - mse: 14.9976 - mae: 1.6594 - val_loss: 8.0395 - val_mse: 8.0395 - val_mae: 2.5417 - lr: 1.0590e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 4/500\n",
            "556/556 - 1s - loss: 14.6055 - mse: 14.6055 - mae: 1.6599 - val_loss: 8.2137 - val_mse: 8.2137 - val_mae: 2.5601 - lr: 1.0590e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 5/500\n",
            "556/556 - 1s - loss: 14.3454 - mse: 14.3454 - mae: 1.6454 - val_loss: 8.2077 - val_mse: 8.2077 - val_mae: 2.5396 - lr: 1.0590e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 6/500\n",
            "556/556 - 1s - loss: 14.1456 - mse: 14.1456 - mae: 1.6265 - val_loss: 8.4311 - val_mse: 8.4311 - val_mae: 2.5597 - lr: 1.0590e-04 - 1s/epoch - 2ms/step\n",
            "Score for inner fold : loss of 8.431106567382812\n",
            "-----Fold:1--------\n",
            "Epoch 1/500\n",
            "556/556 - 2s - loss: 42.2094 - mse: 42.2094 - mae: 2.3091 - val_loss: 2.0858 - val_mse: 2.0858 - val_mae: 1.1507 - lr: 1.0590e-04 - 2s/epoch - 4ms/step\n",
            "Epoch 2/500\n",
            "556/556 - 1s - loss: 37.5827 - mse: 37.5827 - mae: 1.7136 - val_loss: 6.6614 - val_mse: 6.6614 - val_mae: 2.3198 - lr: 1.0590e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 3/500\n",
            "556/556 - 1s - loss: 35.5266 - mse: 35.5266 - mae: 1.7053 - val_loss: 8.0705 - val_mse: 8.0705 - val_mae: 2.5528 - lr: 1.0590e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 4/500\n",
            "556/556 - 1s - loss: 35.0200 - mse: 35.0200 - mae: 1.6985 - val_loss: 8.1354 - val_mse: 8.1354 - val_mae: 2.5502 - lr: 1.0590e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 5/500\n",
            "556/556 - 1s - loss: 34.6991 - mse: 34.6991 - mae: 1.6710 - val_loss: 8.2378 - val_mse: 8.2378 - val_mae: 2.5511 - lr: 1.0590e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 6/500\n",
            "556/556 - 1s - loss: 34.4620 - mse: 34.4620 - mae: 1.6512 - val_loss: 8.3202 - val_mse: 8.3202 - val_mae: 2.5420 - lr: 1.0590e-04 - 1s/epoch - 2ms/step\n",
            "Score for inner fold : loss of 8.320170402526855\n",
            "-----Fold:2--------\n",
            "Epoch 1/500\n",
            "556/556 - 2s - loss: 41.1467 - mse: 41.1467 - mae: 2.3364 - val_loss: 2.1917 - val_mse: 2.1917 - val_mae: 1.1766 - lr: 1.0590e-04 - 2s/epoch - 3ms/step\n",
            "Epoch 2/500\n",
            "556/556 - 1s - loss: 36.2761 - mse: 36.2761 - mae: 1.6959 - val_loss: 6.9855 - val_mse: 6.9855 - val_mae: 2.3738 - lr: 1.0590e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 3/500\n",
            "556/556 - 1s - loss: 34.4455 - mse: 34.4455 - mae: 1.6630 - val_loss: 8.2137 - val_mse: 8.2137 - val_mae: 2.5602 - lr: 1.0590e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 4/500\n",
            "556/556 - 1s - loss: 34.0860 - mse: 34.0860 - mae: 1.6517 - val_loss: 8.3272 - val_mse: 8.3272 - val_mae: 2.5706 - lr: 1.0590e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 5/500\n",
            "556/556 - 1s - loss: 33.8626 - mse: 33.8626 - mae: 1.6407 - val_loss: 8.3247 - val_mse: 8.3247 - val_mae: 2.5430 - lr: 1.0590e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 6/500\n",
            "556/556 - 1s - loss: 33.6739 - mse: 33.6739 - mae: 1.6295 - val_loss: 8.3967 - val_mse: 8.3967 - val_mae: 2.5507 - lr: 1.0590e-04 - 1s/epoch - 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-10-10 09:19:05,384]\u001b[0m Finished trial#13 resulted in value: 8.383333333333333. Current best value is 8.08 with parameters: {'activation': 'tanh', 'num_hidden_layer': 4, 'i': 3, 'learning_rate': 0.00018239009621767336}.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Score for inner fold : loss of 8.396697998046875\n",
            "Trial Number:14\n",
            "-----Fold:0--------\n",
            "Epoch 1/500\n",
            "556/556 - 2s - loss: 37.6429 - mse: 37.6429 - mae: 1.9299 - val_loss: 7.8650 - val_mse: 7.8650 - val_mae: 2.5187 - lr: 3.8189e-04 - 2s/epoch - 4ms/step\n",
            "Epoch 2/500\n",
            "556/556 - 1s - loss: 34.0498 - mse: 34.0498 - mae: 1.6569 - val_loss: 8.3788 - val_mse: 8.3788 - val_mae: 2.5449 - lr: 3.8189e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 3/500\n",
            "556/556 - 1s - loss: 33.4373 - mse: 33.4373 - mae: 1.6284 - val_loss: 8.5040 - val_mse: 8.5040 - val_mae: 2.5279 - lr: 3.8189e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 4/500\n",
            "556/556 - 1s - loss: 33.1020 - mse: 33.1020 - mae: 1.6135 - val_loss: 9.4532 - val_mse: 9.4532 - val_mae: 2.6477 - lr: 3.8189e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 5/500\n",
            "556/556 - 1s - loss: 32.8789 - mse: 32.8789 - mae: 1.6023 - val_loss: 9.0875 - val_mse: 9.0875 - val_mae: 2.5817 - lr: 3.8189e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 6/500\n",
            "556/556 - 1s - loss: 32.6798 - mse: 32.6798 - mae: 1.5960 - val_loss: 8.8986 - val_mse: 8.8986 - val_mae: 2.5133 - lr: 3.8189e-04 - 1s/epoch - 2ms/step\n",
            "Score for inner fold : loss of 8.898590087890625\n",
            "-----Fold:1--------\n",
            "Epoch 1/500\n",
            "556/556 - 2s - loss: 38.0187 - mse: 38.0187 - mae: 1.8817 - val_loss: 7.9288 - val_mse: 7.9288 - val_mae: 2.4897 - lr: 3.8189e-04 - 2s/epoch - 4ms/step\n",
            "Epoch 2/500\n",
            "556/556 - 1s - loss: 34.9964 - mse: 34.9964 - mae: 1.6453 - val_loss: 8.3692 - val_mse: 8.3692 - val_mae: 2.5431 - lr: 3.8189e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 3/500\n",
            "556/556 - 1s - loss: 34.5757 - mse: 34.5757 - mae: 1.6212 - val_loss: 9.2607 - val_mse: 9.2607 - val_mae: 2.6437 - lr: 3.8189e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 4/500\n",
            "556/556 - 1s - loss: 34.3019 - mse: 34.3019 - mae: 1.6083 - val_loss: 8.7108 - val_mse: 8.7108 - val_mae: 2.5312 - lr: 3.8189e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 5/500\n",
            "556/556 - 1s - loss: 34.1136 - mse: 34.1136 - mae: 1.5994 - val_loss: 9.2376 - val_mse: 9.2376 - val_mae: 2.5928 - lr: 3.8189e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 6/500\n",
            "556/556 - 1s - loss: 33.9125 - mse: 33.9125 - mae: 1.5910 - val_loss: 8.9745 - val_mse: 8.9745 - val_mae: 2.5442 - lr: 3.8189e-04 - 1s/epoch - 2ms/step\n",
            "Score for inner fold : loss of 8.974456787109375\n",
            "-----Fold:2--------\n",
            "Epoch 1/500\n",
            "556/556 - 2s - loss: 17.0683 - mse: 17.0683 - mae: 1.8682 - val_loss: 8.0063 - val_mse: 8.0063 - val_mae: 2.5462 - lr: 3.8189e-04 - 2s/epoch - 4ms/step\n",
            "Epoch 2/500\n",
            "556/556 - 1s - loss: 13.8768 - mse: 13.8768 - mae: 1.6279 - val_loss: 8.8285 - val_mse: 8.8285 - val_mae: 2.6059 - lr: 3.8189e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 3/500\n",
            "556/556 - 1s - loss: 13.4055 - mse: 13.4055 - mae: 1.5988 - val_loss: 8.8569 - val_mse: 8.8569 - val_mae: 2.6019 - lr: 3.8189e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 4/500\n",
            "556/556 - 1s - loss: 13.1124 - mse: 13.1124 - mae: 1.5828 - val_loss: 8.6754 - val_mse: 8.6754 - val_mae: 2.5361 - lr: 3.8189e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 5/500\n",
            "556/556 - 1s - loss: 12.8728 - mse: 12.8728 - mae: 1.5707 - val_loss: 8.7890 - val_mse: 8.7890 - val_mae: 2.5188 - lr: 3.8189e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 6/500\n",
            "556/556 - 1s - loss: 12.6713 - mse: 12.6713 - mae: 1.5585 - val_loss: 9.5058 - val_mse: 9.5058 - val_mae: 2.6454 - lr: 3.8189e-04 - 1s/epoch - 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-10-10 09:19:30,493]\u001b[0m Finished trial#14 resulted in value: 9.126666666666667. Current best value is 8.08 with parameters: {'activation': 'tanh', 'num_hidden_layer': 4, 'i': 3, 'learning_rate': 0.00018239009621767336}.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Score for inner fold : loss of 9.505847930908203\n",
            "Trial Number:15\n",
            "-----Fold:0--------\n",
            "Epoch 1/500\n",
            "556/556 - 2s - loss: 40.7883 - mse: 40.7883 - mae: 2.1580 - val_loss: 4.6650 - val_mse: 4.6650 - val_mae: 1.8644 - lr: 1.1947e-04 - 2s/epoch - 4ms/step\n",
            "Epoch 2/500\n",
            "556/556 - 1s - loss: 35.6194 - mse: 35.6194 - mae: 1.6432 - val_loss: 8.1022 - val_mse: 8.1022 - val_mae: 2.5371 - lr: 1.1947e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 3/500\n",
            "556/556 - 1s - loss: 34.9236 - mse: 34.9236 - mae: 1.6370 - val_loss: 8.8415 - val_mse: 8.8415 - val_mae: 2.6170 - lr: 1.1947e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 4/500\n",
            "556/556 - 1s - loss: 34.5893 - mse: 34.5893 - mae: 1.6237 - val_loss: 8.5172 - val_mse: 8.5172 - val_mae: 2.5658 - lr: 1.1947e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 5/500\n",
            "556/556 - 1s - loss: 34.3685 - mse: 34.3685 - mae: 1.6045 - val_loss: 8.7506 - val_mse: 8.7506 - val_mae: 2.5710 - lr: 1.1947e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 6/500\n",
            "556/556 - 1s - loss: 34.1865 - mse: 34.1865 - mae: 1.5968 - val_loss: 9.0014 - val_mse: 9.0014 - val_mae: 2.5949 - lr: 1.1947e-04 - 1s/epoch - 2ms/step\n",
            "Score for inner fold : loss of 9.001425743103027\n",
            "-----Fold:1--------\n",
            "Epoch 1/500\n",
            "556/556 - 2s - loss: 18.9323 - mse: 18.9323 - mae: 2.1500 - val_loss: 5.6487 - val_mse: 5.6487 - val_mae: 2.0487 - lr: 1.1947e-04 - 2s/epoch - 4ms/step\n",
            "Epoch 2/500\n",
            "556/556 - 1s - loss: 14.1017 - mse: 14.1017 - mae: 1.6600 - val_loss: 8.4279 - val_mse: 8.4279 - val_mae: 2.5733 - lr: 1.1947e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 3/500\n",
            "556/556 - 1s - loss: 13.5007 - mse: 13.5007 - mae: 1.6358 - val_loss: 8.5649 - val_mse: 8.5649 - val_mae: 2.5504 - lr: 1.1947e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 4/500\n",
            "556/556 - 1s - loss: 13.2115 - mse: 13.2115 - mae: 1.6131 - val_loss: 8.7972 - val_mse: 8.7972 - val_mae: 2.5710 - lr: 1.1947e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 5/500\n",
            "556/556 - 1s - loss: 12.9980 - mse: 12.9980 - mae: 1.5993 - val_loss: 8.9557 - val_mse: 8.9557 - val_mae: 2.5835 - lr: 1.1947e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 6/500\n",
            "556/556 - 1s - loss: 12.8386 - mse: 12.8386 - mae: 1.5885 - val_loss: 8.5495 - val_mse: 8.5495 - val_mae: 2.5090 - lr: 1.1947e-04 - 1s/epoch - 2ms/step\n",
            "Score for inner fold : loss of 8.549524307250977\n",
            "-----Fold:2--------\n",
            "Epoch 1/500\n",
            "556/556 - 2s - loss: 38.9332 - mse: 38.9332 - mae: 2.1361 - val_loss: 5.5687 - val_mse: 5.5687 - val_mae: 2.0097 - lr: 1.1947e-04 - 2s/epoch - 4ms/step\n",
            "Epoch 2/500\n",
            "556/556 - 1s - loss: 33.7017 - mse: 33.7017 - mae: 1.6444 - val_loss: 8.7298 - val_mse: 8.7298 - val_mae: 2.5863 - lr: 1.1947e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 3/500\n",
            "556/556 - 1s - loss: 33.1311 - mse: 33.1311 - mae: 1.6343 - val_loss: 8.9341 - val_mse: 8.9341 - val_mae: 2.5976 - lr: 1.1947e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 4/500\n",
            "556/556 - 1s - loss: 32.8703 - mse: 32.8703 - mae: 1.6145 - val_loss: 9.0265 - val_mse: 9.0265 - val_mae: 2.5748 - lr: 1.1947e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 5/500\n",
            "556/556 - 1s - loss: 32.6931 - mse: 32.6931 - mae: 1.6070 - val_loss: 9.1198 - val_mse: 9.1198 - val_mae: 2.5568 - lr: 1.1947e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 6/500\n",
            "556/556 - 1s - loss: 32.5765 - mse: 32.5765 - mae: 1.5922 - val_loss: 9.4064 - val_mse: 9.4064 - val_mae: 2.5762 - lr: 1.1947e-04 - 1s/epoch - 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-10-10 09:19:57,699]\u001b[0m Finished trial#15 resulted in value: 8.986666666666666. Current best value is 8.08 with parameters: {'activation': 'tanh', 'num_hidden_layer': 4, 'i': 3, 'learning_rate': 0.00018239009621767336}.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Score for inner fold : loss of 9.406423568725586\n",
            "Trial Number:16\n",
            "-----Fold:0--------\n",
            "Epoch 1/500\n",
            "556/556 - 2s - loss: 36.0418 - mse: 36.0418 - mae: 1.9305 - val_loss: 10.0845 - val_mse: 10.0845 - val_mae: 2.6215 - lr: 4.3458e-04 - 2s/epoch - 3ms/step\n",
            "Epoch 2/500\n",
            "556/556 - 1s - loss: 32.5831 - mse: 32.5831 - mae: 1.6318 - val_loss: 10.2230 - val_mse: 10.2230 - val_mae: 2.6256 - lr: 4.3458e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 3/500\n",
            "556/556 - 1s - loss: 32.4983 - mse: 32.4983 - mae: 1.6179 - val_loss: 10.9071 - val_mse: 10.9071 - val_mae: 2.6312 - lr: 4.3458e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 4/500\n",
            "556/556 - 1s - loss: 32.5621 - mse: 32.5621 - mae: 1.6115 - val_loss: 11.3076 - val_mse: 11.3076 - val_mae: 2.6240 - lr: 4.3458e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 5/500\n",
            "556/556 - 1s - loss: 32.6120 - mse: 32.6120 - mae: 1.6150 - val_loss: 12.3835 - val_mse: 12.3835 - val_mae: 2.7167 - lr: 4.3458e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 6/500\n",
            "556/556 - 1s - loss: 32.6714 - mse: 32.6714 - mae: 1.6112 - val_loss: 12.2506 - val_mse: 12.2506 - val_mae: 2.7146 - lr: 4.3458e-04 - 1s/epoch - 2ms/step\n",
            "Score for inner fold : loss of 12.250545501708984\n",
            "-----Fold:1--------\n",
            "Epoch 1/500\n",
            "556/556 - 2s - loss: 35.0435 - mse: 35.0435 - mae: 1.8477 - val_loss: 12.9313 - val_mse: 12.9313 - val_mae: 2.5971 - lr: 4.3458e-04 - 2s/epoch - 3ms/step\n",
            "Epoch 2/500\n",
            "556/556 - 1s - loss: 31.5679 - mse: 31.5679 - mae: 1.5862 - val_loss: 14.8678 - val_mse: 14.8678 - val_mae: 2.6489 - lr: 4.3458e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 3/500\n",
            "556/556 - 1s - loss: 31.2311 - mse: 31.2311 - mae: 1.5779 - val_loss: 15.7583 - val_mse: 15.7583 - val_mae: 2.6881 - lr: 4.3458e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 4/500\n",
            "556/556 - 1s - loss: 31.0816 - mse: 31.0816 - mae: 1.5799 - val_loss: 15.1218 - val_mse: 15.1218 - val_mae: 2.6530 - lr: 4.3458e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 5/500\n",
            "556/556 - 1s - loss: 30.9926 - mse: 30.9926 - mae: 1.5708 - val_loss: 15.8987 - val_mse: 15.8987 - val_mae: 2.7295 - lr: 4.3458e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 6/500\n",
            "556/556 - 1s - loss: 30.9639 - mse: 30.9639 - mae: 1.5722 - val_loss: 15.4546 - val_mse: 15.4546 - val_mae: 2.6751 - lr: 4.3458e-04 - 1s/epoch - 2ms/step\n",
            "Score for inner fold : loss of 15.454601287841797\n",
            "-----Fold:2--------\n",
            "Epoch 1/500\n",
            "556/556 - 2s - loss: 16.5969 - mse: 16.5969 - mae: 1.9169 - val_loss: 10.1935 - val_mse: 10.1935 - val_mae: 2.5745 - lr: 4.3458e-04 - 2s/epoch - 4ms/step\n",
            "Epoch 2/500\n",
            "556/556 - 1s - loss: 12.3355 - mse: 12.3355 - mae: 1.6210 - val_loss: 11.5570 - val_mse: 11.5570 - val_mae: 2.6888 - lr: 4.3458e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 3/500\n",
            "556/556 - 1s - loss: 12.0727 - mse: 12.0727 - mae: 1.6083 - val_loss: 11.5072 - val_mse: 11.5072 - val_mae: 2.6371 - lr: 4.3458e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 4/500\n",
            "556/556 - 1s - loss: 12.2015 - mse: 12.2015 - mae: 1.5959 - val_loss: 12.1127 - val_mse: 12.1127 - val_mae: 2.6603 - lr: 4.3458e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 5/500\n",
            "556/556 - 1s - loss: 12.2940 - mse: 12.2940 - mae: 1.5988 - val_loss: 12.0717 - val_mse: 12.0717 - val_mae: 2.6595 - lr: 4.3458e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 6/500\n",
            "556/556 - 1s - loss: 12.3552 - mse: 12.3552 - mae: 1.5977 - val_loss: 12.7285 - val_mse: 12.7285 - val_mae: 2.6682 - lr: 4.3458e-04 - 1s/epoch - 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-10-10 09:20:22,114]\u001b[0m Finished trial#16 resulted in value: 13.476666666666667. Current best value is 8.08 with parameters: {'activation': 'tanh', 'num_hidden_layer': 4, 'i': 3, 'learning_rate': 0.00018239009621767336}.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Score for inner fold : loss of 12.72845458984375\n",
            "Trial Number:17\n",
            "-----Fold:0--------\n",
            "Epoch 1/500\n",
            "556/556 - 2s - loss: 40.2984 - mse: 40.2984 - mae: 2.1518 - val_loss: 4.5917 - val_mse: 4.5917 - val_mae: 1.8031 - lr: 1.7360e-04 - 2s/epoch - 4ms/step\n",
            "Epoch 2/500\n",
            "556/556 - 1s - loss: 34.7763 - mse: 34.7763 - mae: 1.6099 - val_loss: 8.3249 - val_mse: 8.3249 - val_mae: 2.5227 - lr: 1.7360e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 3/500\n",
            "556/556 - 1s - loss: 34.0399 - mse: 34.0399 - mae: 1.6100 - val_loss: 8.8987 - val_mse: 8.8987 - val_mae: 2.5756 - lr: 1.7360e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 4/500\n",
            "556/556 - 1s - loss: 33.7971 - mse: 33.7971 - mae: 1.6012 - val_loss: 9.2762 - val_mse: 9.2762 - val_mae: 2.6088 - lr: 1.7360e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 5/500\n",
            "556/556 - 1s - loss: 33.6487 - mse: 33.6487 - mae: 1.5915 - val_loss: 9.4805 - val_mse: 9.4805 - val_mae: 2.6245 - lr: 1.7360e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 6/500\n",
            "556/556 - 1s - loss: 33.5404 - mse: 33.5404 - mae: 1.5856 - val_loss: 9.2951 - val_mse: 9.2951 - val_mae: 2.5780 - lr: 1.7360e-04 - 1s/epoch - 2ms/step\n",
            "Score for inner fold : loss of 9.295108795166016\n",
            "-----Fold:1--------\n",
            "Epoch 1/500\n",
            "556/556 - 2s - loss: 39.1026 - mse: 39.1026 - mae: 2.1415 - val_loss: 4.9932 - val_mse: 4.9932 - val_mae: 1.8964 - lr: 1.7360e-04 - 2s/epoch - 3ms/step\n",
            "Epoch 2/500\n",
            "556/556 - 1s - loss: 33.9701 - mse: 33.9701 - mae: 1.6312 - val_loss: 8.6934 - val_mse: 8.6934 - val_mae: 2.5808 - lr: 1.7360e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 3/500\n",
            "556/556 - 1s - loss: 33.1703 - mse: 33.1703 - mae: 1.6178 - val_loss: 8.6175 - val_mse: 8.6175 - val_mae: 2.5503 - lr: 1.7360e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 4/500\n",
            "556/556 - 1s - loss: 32.8532 - mse: 32.8532 - mae: 1.5925 - val_loss: 9.1576 - val_mse: 9.1576 - val_mae: 2.5939 - lr: 1.7360e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 5/500\n",
            "556/556 - 1s - loss: 32.6310 - mse: 32.6310 - mae: 1.5830 - val_loss: 9.1502 - val_mse: 9.1502 - val_mae: 2.5868 - lr: 1.7360e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 6/500\n",
            "556/556 - 1s - loss: 32.4818 - mse: 32.4818 - mae: 1.5744 - val_loss: 9.3353 - val_mse: 9.3353 - val_mae: 2.6049 - lr: 1.7360e-04 - 1s/epoch - 2ms/step\n",
            "Score for inner fold : loss of 9.33531665802002\n",
            "-----Fold:2--------\n",
            "Epoch 1/500\n",
            "556/556 - 2s - loss: 18.8826 - mse: 18.8826 - mae: 2.1374 - val_loss: 4.8527 - val_mse: 4.8527 - val_mae: 1.8587 - lr: 1.7360e-04 - 2s/epoch - 3ms/step\n",
            "Epoch 2/500\n",
            "556/556 - 1s - loss: 13.7687 - mse: 13.7687 - mae: 1.5984 - val_loss: 8.6288 - val_mse: 8.6288 - val_mae: 2.5745 - lr: 1.7360e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 3/500\n",
            "556/556 - 1s - loss: 13.1288 - mse: 13.1288 - mae: 1.6005 - val_loss: 8.6734 - val_mse: 8.6734 - val_mae: 2.5585 - lr: 1.7360e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 4/500\n",
            "556/556 - 1s - loss: 12.8436 - mse: 12.8436 - mae: 1.5798 - val_loss: 9.0324 - val_mse: 9.0324 - val_mae: 2.5808 - lr: 1.7360e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 5/500\n",
            "556/556 - 1s - loss: 12.6278 - mse: 12.6278 - mae: 1.5691 - val_loss: 9.4147 - val_mse: 9.4147 - val_mae: 2.6274 - lr: 1.7360e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 6/500\n",
            "556/556 - 1s - loss: 12.4742 - mse: 12.4742 - mae: 1.5599 - val_loss: 9.6532 - val_mse: 9.6532 - val_mae: 2.6477 - lr: 1.7360e-04 - 1s/epoch - 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-10-10 09:20:48,520]\u001b[0m Finished trial#17 resulted in value: 9.43. Current best value is 8.08 with parameters: {'activation': 'tanh', 'num_hidden_layer': 4, 'i': 3, 'learning_rate': 0.00018239009621767336}.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Score for inner fold : loss of 9.653212547302246\n",
            "Trial Number:18\n",
            "-----Fold:0--------\n",
            "Epoch 1/500\n",
            "556/556 - 2s - loss: 35.5702 - mse: 35.5702 - mae: 1.7971 - val_loss: 8.6705 - val_mse: 8.6705 - val_mae: 2.5873 - lr: 5.4374e-04 - 2s/epoch - 3ms/step\n",
            "Epoch 2/500\n",
            "556/556 - 1s - loss: 33.2207 - mse: 33.2207 - mae: 1.6188 - val_loss: 8.7945 - val_mse: 8.7945 - val_mae: 2.5357 - lr: 5.4374e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 3/500\n",
            "556/556 - 1s - loss: 32.8648 - mse: 32.8648 - mae: 1.5937 - val_loss: 9.4066 - val_mse: 9.4066 - val_mae: 2.6266 - lr: 5.4374e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 4/500\n",
            "556/556 - 1s - loss: 32.5912 - mse: 32.5912 - mae: 1.5808 - val_loss: 9.4180 - val_mse: 9.4180 - val_mae: 2.5949 - lr: 5.4374e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 5/500\n",
            "556/556 - 1s - loss: 32.3912 - mse: 32.3912 - mae: 1.5678 - val_loss: 9.9697 - val_mse: 9.9697 - val_mae: 2.6656 - lr: 5.4374e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 6/500\n",
            "556/556 - 1s - loss: 32.2249 - mse: 32.2249 - mae: 1.5614 - val_loss: 9.8143 - val_mse: 9.8143 - val_mae: 2.6140 - lr: 5.4374e-04 - 1s/epoch - 2ms/step\n",
            "Score for inner fold : loss of 9.814288139343262\n",
            "-----Fold:1--------\n",
            "Epoch 1/500\n",
            "556/556 - 2s - loss: 17.0902 - mse: 17.0902 - mae: 1.8026 - val_loss: 8.2276 - val_mse: 8.2276 - val_mae: 2.5465 - lr: 5.4374e-04 - 2s/epoch - 3ms/step\n",
            "Epoch 2/500\n",
            "556/556 - 1s - loss: 14.5255 - mse: 14.5255 - mae: 1.6214 - val_loss: 8.1529 - val_mse: 8.1529 - val_mae: 2.4869 - lr: 5.4374e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 3/500\n",
            "556/556 - 1s - loss: 14.0757 - mse: 14.0757 - mae: 1.5940 - val_loss: 9.3912 - val_mse: 9.3912 - val_mae: 2.6133 - lr: 5.4374e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 4/500\n",
            "556/556 - 1s - loss: 13.7618 - mse: 13.7618 - mae: 1.5880 - val_loss: 8.3725 - val_mse: 8.3725 - val_mae: 2.4336 - lr: 5.4374e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 5/500\n",
            "556/556 - 1s - loss: 13.5225 - mse: 13.5225 - mae: 1.5732 - val_loss: 9.7698 - val_mse: 9.7698 - val_mae: 2.6788 - lr: 5.4374e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 6/500\n",
            "556/556 - 1s - loss: 13.3302 - mse: 13.3302 - mae: 1.5667 - val_loss: 9.8038 - val_mse: 9.8038 - val_mae: 2.6165 - lr: 5.4374e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 7/500\n",
            "556/556 - 1s - loss: 13.1496 - mse: 13.1496 - mae: 1.5604 - val_loss: 9.6077 - val_mse: 9.6077 - val_mae: 2.5954 - lr: 5.4374e-04 - 1s/epoch - 2ms/step\n",
            "Score for inner fold : loss of 9.607749938964844\n",
            "-----Fold:2--------\n",
            "Epoch 1/500\n",
            "556/556 - 2s - loss: 36.8977 - mse: 36.8977 - mae: 1.8739 - val_loss: 8.3640 - val_mse: 8.3640 - val_mae: 2.5917 - lr: 5.4374e-04 - 2s/epoch - 4ms/step\n",
            "Epoch 2/500\n",
            "556/556 - 1s - loss: 33.8196 - mse: 33.8196 - mae: 1.6395 - val_loss: 8.4055 - val_mse: 8.4055 - val_mae: 2.5310 - lr: 5.4374e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 3/500\n",
            "556/556 - 1s - loss: 33.2839 - mse: 33.2839 - mae: 1.6089 - val_loss: 9.3244 - val_mse: 9.3244 - val_mae: 2.6483 - lr: 5.4374e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 4/500\n",
            "556/556 - 1s - loss: 32.9375 - mse: 32.9375 - mae: 1.5986 - val_loss: 9.3215 - val_mse: 9.3215 - val_mae: 2.5998 - lr: 5.4374e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 5/500\n",
            "556/556 - 1s - loss: 32.6840 - mse: 32.6840 - mae: 1.5864 - val_loss: 9.1076 - val_mse: 9.1076 - val_mae: 2.5591 - lr: 5.4374e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 6/500\n",
            "556/556 - 1s - loss: 32.4374 - mse: 32.4374 - mae: 1.5770 - val_loss: 10.3395 - val_mse: 10.3395 - val_mae: 2.7415 - lr: 5.4374e-04 - 1s/epoch - 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-10-10 09:21:14,184]\u001b[0m Finished trial#18 resulted in value: 9.92. Current best value is 8.08 with parameters: {'activation': 'tanh', 'num_hidden_layer': 4, 'i': 3, 'learning_rate': 0.00018239009621767336}.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Score for inner fold : loss of 10.339521408081055\n",
            "Trial Number:19\n",
            "-----Fold:0--------\n",
            "Epoch 1/500\n",
            "556/556 - 2s - loss: 42.9231 - mse: 42.9231 - mae: 2.3796 - val_loss: 2.1402 - val_mse: 2.1402 - val_mae: 1.1417 - lr: 2.1386e-04 - 2s/epoch - 3ms/step\n",
            "Epoch 2/500\n",
            "556/556 - 1s - loss: 38.2444 - mse: 38.2444 - mae: 1.7652 - val_loss: 5.7367 - val_mse: 5.7367 - val_mae: 2.1096 - lr: 2.1386e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 3/500\n",
            "556/556 - 1s - loss: 36.1192 - mse: 36.1192 - mae: 1.6655 - val_loss: 7.8538 - val_mse: 7.8538 - val_mae: 2.5162 - lr: 2.1386e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 4/500\n",
            "556/556 - 1s - loss: 35.6824 - mse: 35.6824 - mae: 1.6714 - val_loss: 8.0458 - val_mse: 8.0458 - val_mae: 2.5358 - lr: 2.1386e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 5/500\n",
            "556/556 - 1s - loss: 35.4220 - mse: 35.4220 - mae: 1.6601 - val_loss: 8.1647 - val_mse: 8.1647 - val_mae: 2.5432 - lr: 2.1386e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 6/500\n",
            "556/556 - 1s - loss: 35.2104 - mse: 35.2104 - mae: 1.6469 - val_loss: 8.2235 - val_mse: 8.2235 - val_mae: 2.5463 - lr: 2.1386e-04 - 1s/epoch - 2ms/step\n",
            "Score for inner fold : loss of 8.223496437072754\n",
            "-----Fold:1--------\n",
            "Epoch 1/500\n",
            "556/556 - 2s - loss: 42.4858 - mse: 42.4858 - mae: 2.4035 - val_loss: 1.8486 - val_mse: 1.8486 - val_mae: 1.0516 - lr: 2.1386e-04 - 2s/epoch - 3ms/step\n",
            "Epoch 2/500\n",
            "556/556 - 1s - loss: 38.0589 - mse: 38.0589 - mae: 1.7952 - val_loss: 4.7411 - val_mse: 4.7411 - val_mae: 1.9222 - lr: 2.1386e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 3/500\n",
            "556/556 - 1s - loss: 35.4730 - mse: 35.4730 - mae: 1.6620 - val_loss: 7.6016 - val_mse: 7.6016 - val_mae: 2.4911 - lr: 2.1386e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 4/500\n",
            "556/556 - 1s - loss: 34.8562 - mse: 34.8562 - mae: 1.6903 - val_loss: 8.0597 - val_mse: 8.0597 - val_mae: 2.5580 - lr: 2.1386e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 5/500\n",
            "556/556 - 1s - loss: 34.5720 - mse: 34.5720 - mae: 1.6786 - val_loss: 8.1365 - val_mse: 8.1365 - val_mae: 2.5581 - lr: 2.1386e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 6/500\n",
            "556/556 - 1s - loss: 34.3682 - mse: 34.3682 - mae: 1.6680 - val_loss: 8.1041 - val_mse: 8.1041 - val_mae: 2.5369 - lr: 2.1386e-04 - 998ms/epoch - 2ms/step\n",
            "Score for inner fold : loss of 8.104069709777832\n",
            "-----Fold:2--------\n",
            "Epoch 1/500\n",
            "556/556 - 2s - loss: 20.5337 - mse: 20.5337 - mae: 2.3343 - val_loss: 2.1113 - val_mse: 2.1113 - val_mae: 1.1297 - lr: 2.1386e-04 - 2s/epoch - 3ms/step\n",
            "Epoch 2/500\n",
            "556/556 - 1s - loss: 16.0355 - mse: 16.0355 - mae: 1.6740 - val_loss: 6.2055 - val_mse: 6.2055 - val_mae: 2.2142 - lr: 2.1386e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 3/500\n",
            "556/556 - 1s - loss: 14.0354 - mse: 14.0354 - mae: 1.6299 - val_loss: 7.9495 - val_mse: 7.9495 - val_mae: 2.5235 - lr: 2.1386e-04 - 993ms/epoch - 2ms/step\n",
            "Epoch 4/500\n",
            "556/556 - 1s - loss: 13.6900 - mse: 13.6900 - mae: 1.6331 - val_loss: 8.2181 - val_mse: 8.2181 - val_mae: 2.5456 - lr: 2.1386e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 5/500\n",
            "556/556 - 1s - loss: 13.4694 - mse: 13.4694 - mae: 1.6220 - val_loss: 8.4625 - val_mse: 8.4625 - val_mae: 2.5755 - lr: 2.1386e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 6/500\n",
            "556/556 - 1s - loss: 13.2982 - mse: 13.2982 - mae: 1.6120 - val_loss: 8.4582 - val_mse: 8.4582 - val_mae: 2.5584 - lr: 2.1386e-04 - 1s/epoch - 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-10-10 09:21:37,764]\u001b[0m Finished trial#19 resulted in value: 8.26. Current best value is 8.08 with parameters: {'activation': 'tanh', 'num_hidden_layer': 4, 'i': 3, 'learning_rate': 0.00018239009621767336}.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Score for inner fold : loss of 8.458176612854004\n",
            "Trial Number:20\n",
            "-----Fold:0--------\n",
            "Epoch 1/500\n",
            "556/556 - 2s - loss: 38.1910 - mse: 38.1910 - mae: 2.1116 - val_loss: 8.5765 - val_mse: 8.5765 - val_mae: 1.9466 - lr: 1.1160e-04 - 2s/epoch - 4ms/step\n",
            "Epoch 2/500\n",
            "556/556 - 1s - loss: 32.6671 - mse: 32.6671 - mae: 1.5771 - val_loss: 15.6740 - val_mse: 15.6740 - val_mae: 2.6994 - lr: 1.1160e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 3/500\n",
            "556/556 - 1s - loss: 32.3838 - mse: 32.3838 - mae: 1.5992 - val_loss: 16.7466 - val_mse: 16.7466 - val_mae: 2.7281 - lr: 1.1160e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 4/500\n",
            "556/556 - 1s - loss: 32.3826 - mse: 32.3826 - mae: 1.5988 - val_loss: 16.1297 - val_mse: 16.1297 - val_mae: 2.6768 - lr: 1.1160e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 5/500\n",
            "556/556 - 1s - loss: 32.4010 - mse: 32.4010 - mae: 1.5998 - val_loss: 17.9181 - val_mse: 17.9181 - val_mae: 2.7448 - lr: 1.1160e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 6/500\n",
            "556/556 - 1s - loss: 32.3800 - mse: 32.3800 - mae: 1.5965 - val_loss: 16.8971 - val_mse: 16.8971 - val_mae: 2.6755 - lr: 1.1160e-04 - 1s/epoch - 2ms/step\n",
            "Score for inner fold : loss of 16.897127151489258\n",
            "-----Fold:1--------\n",
            "Epoch 1/500\n",
            "556/556 - 2s - loss: 39.1558 - mse: 39.1558 - mae: 2.1439 - val_loss: 5.8109 - val_mse: 5.8109 - val_mae: 1.8717 - lr: 1.1160e-04 - 2s/epoch - 4ms/step\n",
            "Epoch 2/500\n",
            "556/556 - 1s - loss: 32.4129 - mse: 32.4129 - mae: 1.5871 - val_loss: 10.5886 - val_mse: 10.5886 - val_mae: 2.6074 - lr: 1.1160e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 3/500\n",
            "556/556 - 1s - loss: 32.5956 - mse: 32.5956 - mae: 1.6165 - val_loss: 11.0058 - val_mse: 11.0058 - val_mae: 2.6569 - lr: 1.1160e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 4/500\n",
            "556/556 - 1s - loss: 32.6270 - mse: 32.6270 - mae: 1.6120 - val_loss: 10.9101 - val_mse: 10.9101 - val_mae: 2.6138 - lr: 1.1160e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 5/500\n",
            "556/556 - 1s - loss: 32.7974 - mse: 32.7974 - mae: 1.6078 - val_loss: 11.0851 - val_mse: 11.0851 - val_mae: 2.6271 - lr: 1.1160e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 6/500\n",
            "556/556 - 1s - loss: 32.7065 - mse: 32.7065 - mae: 1.6112 - val_loss: 10.7865 - val_mse: 10.7865 - val_mae: 2.6152 - lr: 1.1160e-04 - 1s/epoch - 2ms/step\n",
            "Score for inner fold : loss of 10.786518096923828\n",
            "-----Fold:2--------\n",
            "Epoch 1/500\n",
            "556/556 - 2s - loss: 15.5872 - mse: 15.5872 - mae: 2.0445 - val_loss: 8.1016 - val_mse: 8.1016 - val_mae: 2.1499 - lr: 1.1160e-04 - 2s/epoch - 4ms/step\n",
            "Epoch 2/500\n",
            "556/556 - 1s - loss: 10.8820 - mse: 10.8820 - mae: 1.5555 - val_loss: 11.0673 - val_mse: 11.0673 - val_mae: 2.6416 - lr: 1.1160e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 3/500\n",
            "556/556 - 1s - loss: 10.9018 - mse: 10.9018 - mae: 1.5713 - val_loss: 12.4170 - val_mse: 12.4170 - val_mae: 2.7146 - lr: 1.1160e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 4/500\n",
            "556/556 - 1s - loss: 10.9950 - mse: 10.9950 - mae: 1.5749 - val_loss: 11.5739 - val_mse: 11.5739 - val_mae: 2.6489 - lr: 1.1160e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 5/500\n",
            "556/556 - 1s - loss: 11.0043 - mse: 11.0043 - mae: 1.5723 - val_loss: 11.8612 - val_mse: 11.8612 - val_mae: 2.6532 - lr: 1.1160e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 6/500\n",
            "556/556 - 1s - loss: 11.0937 - mse: 11.0937 - mae: 1.5663 - val_loss: 12.3485 - val_mse: 12.3485 - val_mae: 2.6848 - lr: 1.1160e-04 - 1s/epoch - 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-10-10 09:22:04,804]\u001b[0m Finished trial#20 resulted in value: 13.346666666666666. Current best value is 8.08 with parameters: {'activation': 'tanh', 'num_hidden_layer': 4, 'i': 3, 'learning_rate': 0.00018239009621767336}.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Score for inner fold : loss of 12.348503112792969\n",
            "Trial Number:21\n",
            "-----Fold:0--------\n",
            "Epoch 1/500\n",
            "556/556 - 2s - loss: 20.2239 - mse: 20.2239 - mae: 2.1896 - val_loss: 2.9664 - val_mse: 2.9664 - val_mae: 1.4450 - lr: 2.2104e-04 - 2s/epoch - 3ms/step\n",
            "Epoch 2/500\n",
            "556/556 - 1s - loss: 15.6840 - mse: 15.6840 - mae: 1.6655 - val_loss: 7.5396 - val_mse: 7.5396 - val_mae: 2.5031 - lr: 2.2104e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 3/500\n",
            "556/556 - 1s - loss: 14.5774 - mse: 14.5774 - mae: 1.7040 - val_loss: 8.0128 - val_mse: 8.0128 - val_mae: 2.5574 - lr: 2.2104e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 4/500\n",
            "556/556 - 1s - loss: 14.3088 - mse: 14.3088 - mae: 1.6930 - val_loss: 8.0645 - val_mse: 8.0645 - val_mae: 2.5520 - lr: 2.2104e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 5/500\n",
            "556/556 - 1s - loss: 14.1070 - mse: 14.1070 - mae: 1.6757 - val_loss: 8.1059 - val_mse: 8.1059 - val_mae: 2.5439 - lr: 2.2104e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 6/500\n",
            "556/556 - 1s - loss: 13.9086 - mse: 13.9086 - mae: 1.6557 - val_loss: 8.3272 - val_mse: 8.3272 - val_mae: 2.5712 - lr: 2.2104e-04 - 1s/epoch - 2ms/step\n",
            "Score for inner fold : loss of 8.32724666595459\n",
            "-----Fold:1--------\n",
            "Epoch 1/500\n",
            "556/556 - 2s - loss: 40.5145 - mse: 40.5145 - mae: 2.2189 - val_loss: 3.0621 - val_mse: 3.0621 - val_mae: 1.4673 - lr: 2.2104e-04 - 2s/epoch - 3ms/step\n",
            "Epoch 2/500\n",
            "556/556 - 1s - loss: 35.6955 - mse: 35.6955 - mae: 1.6420 - val_loss: 7.6898 - val_mse: 7.6898 - val_mae: 2.5053 - lr: 2.2104e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 3/500\n",
            "556/556 - 1s - loss: 34.9123 - mse: 34.9123 - mae: 1.6596 - val_loss: 7.9190 - val_mse: 7.9190 - val_mae: 2.5200 - lr: 2.2104e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 4/500\n",
            "556/556 - 1s - loss: 34.6264 - mse: 34.6264 - mae: 1.6477 - val_loss: 7.9632 - val_mse: 7.9632 - val_mae: 2.5224 - lr: 2.2104e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 5/500\n",
            "556/556 - 1s - loss: 34.4200 - mse: 34.4200 - mae: 1.6310 - val_loss: 8.0043 - val_mse: 8.0043 - val_mae: 2.5101 - lr: 2.2104e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 6/500\n",
            "556/556 - 1s - loss: 34.2329 - mse: 34.2329 - mae: 1.6273 - val_loss: 7.8064 - val_mse: 7.8064 - val_mae: 2.4656 - lr: 2.2104e-04 - 1s/epoch - 2ms/step\n",
            "Score for inner fold : loss of 7.806397914886475\n",
            "-----Fold:2--------\n",
            "Epoch 1/500\n",
            "556/556 - 2s - loss: 41.7297 - mse: 41.7297 - mae: 2.2457 - val_loss: 2.7263 - val_mse: 2.7263 - val_mae: 1.3686 - lr: 2.2104e-04 - 2s/epoch - 3ms/step\n",
            "Epoch 2/500\n",
            "556/556 - 1s - loss: 36.9236 - mse: 36.9236 - mae: 1.6868 - val_loss: 7.5274 - val_mse: 7.5274 - val_mae: 2.4872 - lr: 2.2104e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 3/500\n",
            "556/556 - 1s - loss: 35.8021 - mse: 35.8021 - mae: 1.7060 - val_loss: 8.0061 - val_mse: 8.0061 - val_mae: 2.5470 - lr: 2.2104e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 4/500\n",
            "556/556 - 1s - loss: 35.4420 - mse: 35.4420 - mae: 1.6746 - val_loss: 8.0362 - val_mse: 8.0362 - val_mae: 2.5373 - lr: 2.2104e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 5/500\n",
            "556/556 - 1s - loss: 35.1331 - mse: 35.1331 - mae: 1.6523 - val_loss: 8.2448 - val_mse: 8.2448 - val_mae: 2.5488 - lr: 2.2104e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 6/500\n",
            "556/556 - 1s - loss: 34.8791 - mse: 34.8791 - mae: 1.6442 - val_loss: 8.3368 - val_mse: 8.3368 - val_mae: 2.5477 - lr: 2.2104e-04 - 1s/epoch - 2ms/step\n",
            "Score for inner fold : loss of 8.336830139160156\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-10-10 09:22:29,275]\u001b[0m Finished trial#21 resulted in value: 8.16. Current best value is 8.08 with parameters: {'activation': 'tanh', 'num_hidden_layer': 4, 'i': 3, 'learning_rate': 0.00018239009621767336}.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trial Number:22\n",
            "-----Fold:0--------\n",
            "Epoch 1/500\n",
            "556/556 - 2s - loss: 41.1384 - mse: 41.1384 - mae: 2.1935 - val_loss: 3.4890 - val_mse: 3.4890 - val_mae: 1.5673 - lr: 2.5054e-04 - 2s/epoch - 3ms/step\n",
            "Epoch 2/500\n",
            "556/556 - 1s - loss: 35.8771 - mse: 35.8771 - mae: 1.6794 - val_loss: 8.2425 - val_mse: 8.2425 - val_mae: 2.5605 - lr: 2.5054e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 3/500\n",
            "556/556 - 1s - loss: 34.8509 - mse: 34.8509 - mae: 1.6797 - val_loss: 8.3580 - val_mse: 8.3580 - val_mae: 2.5574 - lr: 2.5054e-04 - 998ms/epoch - 2ms/step\n",
            "Epoch 4/500\n",
            "556/556 - 1s - loss: 34.5002 - mse: 34.5002 - mae: 1.6575 - val_loss: 8.5390 - val_mse: 8.5390 - val_mae: 2.5733 - lr: 2.5054e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 5/500\n",
            "556/556 - 1s - loss: 34.2589 - mse: 34.2589 - mae: 1.6405 - val_loss: 8.6940 - val_mse: 8.6940 - val_mae: 2.5795 - lr: 2.5054e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 6/500\n",
            "556/556 - 1s - loss: 34.0580 - mse: 34.0580 - mae: 1.6278 - val_loss: 8.8809 - val_mse: 8.8809 - val_mae: 2.5994 - lr: 2.5054e-04 - 993ms/epoch - 2ms/step\n",
            "Score for inner fold : loss of 8.880928039550781\n",
            "-----Fold:1--------\n",
            "Epoch 1/500\n",
            "556/556 - 2s - loss: 40.5892 - mse: 40.5892 - mae: 2.1500 - val_loss: 3.8435 - val_mse: 3.8435 - val_mae: 1.6757 - lr: 2.5054e-04 - 2s/epoch - 3ms/step\n",
            "Epoch 2/500\n",
            "556/556 - 1s - loss: 35.6046 - mse: 35.6046 - mae: 1.6352 - val_loss: 7.9719 - val_mse: 7.9719 - val_mae: 2.5211 - lr: 2.5054e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 3/500\n",
            "556/556 - 1s - loss: 34.7150 - mse: 34.7150 - mae: 1.6353 - val_loss: 8.2752 - val_mse: 8.2752 - val_mae: 2.5505 - lr: 2.5054e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 4/500\n",
            "556/556 - 1s - loss: 34.3175 - mse: 34.3175 - mae: 1.6172 - val_loss: 8.4578 - val_mse: 8.4578 - val_mae: 2.5467 - lr: 2.5054e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 5/500\n",
            "556/556 - 1s - loss: 34.0516 - mse: 34.0516 - mae: 1.6038 - val_loss: 8.8248 - val_mse: 8.8248 - val_mae: 2.5573 - lr: 2.5054e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 6/500\n",
            "556/556 - 1s - loss: 33.8655 - mse: 33.8655 - mae: 1.5959 - val_loss: 9.1426 - val_mse: 9.1426 - val_mae: 2.5880 - lr: 2.5054e-04 - 997ms/epoch - 2ms/step\n",
            "Score for inner fold : loss of 9.14257526397705\n",
            "-----Fold:2--------\n",
            "Epoch 1/500\n",
            "556/556 - 2s - loss: 19.1519 - mse: 19.1519 - mae: 2.1471 - val_loss: 4.1808 - val_mse: 4.1808 - val_mae: 1.7397 - lr: 2.5054e-04 - 2s/epoch - 3ms/step\n",
            "Epoch 2/500\n",
            "556/556 - 1s - loss: 14.0663 - mse: 14.0663 - mae: 1.6231 - val_loss: 8.1077 - val_mse: 8.1077 - val_mae: 2.5396 - lr: 2.5054e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 3/500\n",
            "556/556 - 1s - loss: 13.2266 - mse: 13.2266 - mae: 1.6309 - val_loss: 8.5188 - val_mse: 8.5188 - val_mae: 2.5793 - lr: 2.5054e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 4/500\n",
            "556/556 - 1s - loss: 12.8964 - mse: 12.8964 - mae: 1.6099 - val_loss: 8.4975 - val_mse: 8.4975 - val_mae: 2.5577 - lr: 2.5054e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 5/500\n",
            "556/556 - 1s - loss: 12.6781 - mse: 12.6781 - mae: 1.5946 - val_loss: 8.5750 - val_mse: 8.5750 - val_mae: 2.5625 - lr: 2.5054e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 6/500\n",
            "556/556 - 1s - loss: 12.4920 - mse: 12.4920 - mae: 1.5836 - val_loss: 9.1121 - val_mse: 9.1121 - val_mae: 2.6164 - lr: 2.5054e-04 - 1s/epoch - 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-10-10 09:22:52,843]\u001b[0m Finished trial#22 resulted in value: 9.043333333333335. Current best value is 8.08 with parameters: {'activation': 'tanh', 'num_hidden_layer': 4, 'i': 3, 'learning_rate': 0.00018239009621767336}.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Score for inner fold : loss of 9.112144470214844\n",
            "Trial Number:23\n",
            "-----Fold:0--------\n",
            "Epoch 1/500\n",
            "556/556 - 2s - loss: 17.8349 - mse: 17.8349 - mae: 1.9246 - val_loss: 7.8350 - val_mse: 7.8350 - val_mae: 2.5584 - lr: 5.9890e-04 - 2s/epoch - 3ms/step\n",
            "Epoch 2/500\n",
            "556/556 - 1s - loss: 14.6142 - mse: 14.6142 - mae: 1.6943 - val_loss: 7.7279 - val_mse: 7.7279 - val_mae: 2.4886 - lr: 5.9890e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 3/500\n",
            "556/556 - 1s - loss: 14.0583 - mse: 14.0583 - mae: 1.6489 - val_loss: 8.3091 - val_mse: 8.3091 - val_mae: 2.5726 - lr: 5.9890e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 4/500\n",
            "556/556 - 1s - loss: 13.6117 - mse: 13.6117 - mae: 1.6205 - val_loss: 8.5662 - val_mse: 8.5662 - val_mae: 2.5560 - lr: 5.9890e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 5/500\n",
            "556/556 - 1s - loss: 13.3047 - mse: 13.3047 - mae: 1.6034 - val_loss: 8.4711 - val_mse: 8.4711 - val_mae: 2.5264 - lr: 5.9890e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 6/500\n",
            "556/556 - 1s - loss: 13.0633 - mse: 13.0633 - mae: 1.5936 - val_loss: 8.6693 - val_mse: 8.6693 - val_mae: 2.5330 - lr: 5.9890e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 7/500\n",
            "556/556 - 1s - loss: 12.8864 - mse: 12.8864 - mae: 1.5862 - val_loss: 9.2832 - val_mse: 9.2832 - val_mae: 2.6316 - lr: 5.9890e-04 - 1s/epoch - 2ms/step\n",
            "Score for inner fold : loss of 9.283235549926758\n",
            "-----Fold:1--------\n",
            "Epoch 1/500\n",
            "556/556 - 2s - loss: 38.1129 - mse: 38.1129 - mae: 1.8886 - val_loss: 7.8654 - val_mse: 7.8654 - val_mae: 2.5271 - lr: 5.9890e-04 - 2s/epoch - 3ms/step\n",
            "Epoch 2/500\n",
            "556/556 - 1s - loss: 35.1610 - mse: 35.1610 - mae: 1.6861 - val_loss: 7.9099 - val_mse: 7.9099 - val_mae: 2.5035 - lr: 5.9890e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 3/500\n",
            "556/556 - 1s - loss: 34.4804 - mse: 34.4804 - mae: 1.6317 - val_loss: 8.3359 - val_mse: 8.3359 - val_mae: 2.5130 - lr: 5.9890e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 4/500\n",
            "556/556 - 1s - loss: 34.0430 - mse: 34.0430 - mae: 1.6053 - val_loss: 8.9832 - val_mse: 8.9832 - val_mae: 2.5886 - lr: 5.9890e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 5/500\n",
            "556/556 - 1s - loss: 33.7390 - mse: 33.7390 - mae: 1.5905 - val_loss: 8.8136 - val_mse: 8.8136 - val_mae: 2.5513 - lr: 5.9890e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 6/500\n",
            "556/556 - 1s - loss: 33.5263 - mse: 33.5263 - mae: 1.5792 - val_loss: 9.4832 - val_mse: 9.4832 - val_mae: 2.6490 - lr: 5.9890e-04 - 1s/epoch - 2ms/step\n",
            "Score for inner fold : loss of 9.483221054077148\n",
            "-----Fold:2--------\n",
            "Epoch 1/500\n",
            "556/556 - 2s - loss: 38.4682 - mse: 38.4682 - mae: 1.9618 - val_loss: 7.7595 - val_mse: 7.7595 - val_mae: 2.5699 - lr: 5.9890e-04 - 2s/epoch - 3ms/step\n",
            "Epoch 2/500\n",
            "556/556 - 1s - loss: 35.3166 - mse: 35.3166 - mae: 1.7134 - val_loss: 7.7538 - val_mse: 7.7538 - val_mae: 2.4939 - lr: 5.9890e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 3/500\n",
            "556/556 - 1s - loss: 34.7091 - mse: 34.7091 - mae: 1.6705 - val_loss: 9.0331 - val_mse: 9.0331 - val_mae: 2.6651 - lr: 5.9890e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 4/500\n",
            "556/556 - 1s - loss: 34.3106 - mse: 34.3106 - mae: 1.6538 - val_loss: 8.2271 - val_mse: 8.2271 - val_mae: 2.4935 - lr: 5.9890e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 5/500\n",
            "556/556 - 1s - loss: 34.0320 - mse: 34.0320 - mae: 1.6393 - val_loss: 9.0483 - val_mse: 9.0483 - val_mae: 2.6341 - lr: 5.9890e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 6/500\n",
            "556/556 - 1s - loss: 33.7957 - mse: 33.7957 - mae: 1.6279 - val_loss: 8.2764 - val_mse: 8.2764 - val_mae: 2.4764 - lr: 5.9890e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 7/500\n",
            "556/556 - 1s - loss: 33.5879 - mse: 33.5879 - mae: 1.6160 - val_loss: 9.2571 - val_mse: 9.2571 - val_mae: 2.6116 - lr: 5.9890e-04 - 1s/epoch - 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-10-10 09:23:19,572]\u001b[0m Finished trial#23 resulted in value: 9.339999999999998. Current best value is 8.08 with parameters: {'activation': 'tanh', 'num_hidden_layer': 4, 'i': 3, 'learning_rate': 0.00018239009621767336}.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Score for inner fold : loss of 9.25705623626709\n",
            "Trial Number:24\n",
            "-----Fold:0--------\n",
            "Epoch 1/500\n",
            "556/556 - 2s - loss: 22.6238 - mse: 22.6238 - mae: 2.3801 - val_loss: 1.5168 - val_mse: 1.5168 - val_mae: 0.9596 - lr: 1.5178e-04 - 2s/epoch - 3ms/step\n",
            "Epoch 2/500\n",
            "556/556 - 1s - loss: 17.9973 - mse: 17.9973 - mae: 1.7469 - val_loss: 4.8434 - val_mse: 4.8434 - val_mae: 1.9601 - lr: 1.5178e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 3/500\n",
            "556/556 - 1s - loss: 15.1442 - mse: 15.1442 - mae: 1.6133 - val_loss: 7.7124 - val_mse: 7.7124 - val_mae: 2.5021 - lr: 1.5178e-04 - 1000ms/epoch - 2ms/step\n",
            "Epoch 4/500\n",
            "556/556 - 1s - loss: 14.5274 - mse: 14.5274 - mae: 1.6423 - val_loss: 8.0480 - val_mse: 8.0480 - val_mae: 2.5331 - lr: 1.5178e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 5/500\n",
            "556/556 - 1s - loss: 14.2444 - mse: 14.2444 - mae: 1.6283 - val_loss: 8.1568 - val_mse: 8.1568 - val_mae: 2.5295 - lr: 1.5178e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 6/500\n",
            "556/556 - 1s - loss: 14.0221 - mse: 14.0221 - mae: 1.6136 - val_loss: 8.4084 - val_mse: 8.4084 - val_mae: 2.5524 - lr: 1.5178e-04 - 1s/epoch - 2ms/step\n",
            "Score for inner fold : loss of 8.408352851867676\n",
            "-----Fold:1--------\n",
            "Epoch 1/500\n",
            "556/556 - 2s - loss: 42.3807 - mse: 42.3807 - mae: 2.3851 - val_loss: 2.0615 - val_mse: 2.0615 - val_mae: 1.0999 - lr: 1.5178e-04 - 2s/epoch - 3ms/step\n",
            "Epoch 2/500\n",
            "556/556 - 1s - loss: 37.7893 - mse: 37.7893 - mae: 1.7384 - val_loss: 5.4952 - val_mse: 5.4952 - val_mae: 2.0489 - lr: 1.5178e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 3/500\n",
            "556/556 - 1s - loss: 35.1095 - mse: 35.1095 - mae: 1.6404 - val_loss: 8.1119 - val_mse: 8.1119 - val_mae: 2.5427 - lr: 1.5178e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 4/500\n",
            "556/556 - 1s - loss: 34.3848 - mse: 34.3848 - mae: 1.6553 - val_loss: 8.3854 - val_mse: 8.3854 - val_mae: 2.5585 - lr: 1.5178e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 5/500\n",
            "556/556 - 1s - loss: 34.0515 - mse: 34.0515 - mae: 1.6345 - val_loss: 8.6046 - val_mse: 8.6046 - val_mae: 2.5734 - lr: 1.5178e-04 - 977ms/epoch - 2ms/step\n",
            "Epoch 6/500\n",
            "556/556 - 1s - loss: 33.8590 - mse: 33.8590 - mae: 1.6226 - val_loss: 8.5955 - val_mse: 8.5955 - val_mae: 2.5536 - lr: 1.5178e-04 - 1s/epoch - 2ms/step\n",
            "Score for inner fold : loss of 8.59546184539795\n",
            "-----Fold:2--------\n",
            "Epoch 1/500\n",
            "556/556 - 2s - loss: 41.8735 - mse: 41.8735 - mae: 2.3959 - val_loss: 1.5896 - val_mse: 1.5896 - val_mae: 0.9550 - lr: 1.5178e-04 - 2s/epoch - 3ms/step\n",
            "Epoch 2/500\n",
            "556/556 - 1s - loss: 37.7910 - mse: 37.7910 - mae: 1.8408 - val_loss: 4.5931 - val_mse: 4.5931 - val_mae: 1.8459 - lr: 1.5178e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 3/500\n",
            "556/556 - 1s - loss: 35.0094 - mse: 35.0094 - mae: 1.6375 - val_loss: 7.7629 - val_mse: 7.7629 - val_mae: 2.5020 - lr: 1.5178e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 4/500\n",
            "556/556 - 1s - loss: 34.2447 - mse: 34.2447 - mae: 1.6441 - val_loss: 8.2062 - val_mse: 8.2062 - val_mae: 2.5585 - lr: 1.5178e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 5/500\n",
            "556/556 - 1s - loss: 33.8842 - mse: 33.8842 - mae: 1.6262 - val_loss: 8.3890 - val_mse: 8.3890 - val_mae: 2.5775 - lr: 1.5178e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 6/500\n",
            "556/556 - 1s - loss: 33.6478 - mse: 33.6478 - mae: 1.6154 - val_loss: 8.5659 - val_mse: 8.5659 - val_mae: 2.5884 - lr: 1.5178e-04 - 1s/epoch - 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-10-10 09:23:43,124]\u001b[0m Finished trial#24 resulted in value: 8.526666666666666. Current best value is 8.08 with parameters: {'activation': 'tanh', 'num_hidden_layer': 4, 'i': 3, 'learning_rate': 0.00018239009621767336}.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Score for inner fold : loss of 8.565937042236328\n",
            "Trial Number:25\n",
            "-----Fold:0--------\n",
            "Epoch 1/500\n",
            "556/556 - 2s - loss: 15.9595 - mse: 15.9595 - mae: 1.8552 - val_loss: 8.7620 - val_mse: 8.7620 - val_mae: 2.5993 - lr: 2.8583e-04 - 2s/epoch - 4ms/step\n",
            "Epoch 2/500\n",
            "556/556 - 1s - loss: 12.8681 - mse: 12.8681 - mae: 1.5978 - val_loss: 9.2402 - val_mse: 9.2402 - val_mae: 2.5980 - lr: 2.8583e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 3/500\n",
            "556/556 - 1s - loss: 12.4869 - mse: 12.4869 - mae: 1.5751 - val_loss: 9.1975 - val_mse: 9.1975 - val_mae: 2.5765 - lr: 2.8583e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 4/500\n",
            "556/556 - 1s - loss: 12.2543 - mse: 12.2543 - mae: 1.5584 - val_loss: 10.2875 - val_mse: 10.2875 - val_mae: 2.6992 - lr: 2.8583e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 5/500\n",
            "556/556 - 1s - loss: 12.0600 - mse: 12.0600 - mae: 1.5548 - val_loss: 8.8580 - val_mse: 8.8580 - val_mae: 2.5115 - lr: 2.8583e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 6/500\n",
            "556/556 - 1s - loss: 11.9067 - mse: 11.9067 - mae: 1.5406 - val_loss: 9.5310 - val_mse: 9.5310 - val_mae: 2.5994 - lr: 2.8583e-04 - 1s/epoch - 2ms/step\n",
            "Score for inner fold : loss of 9.53101634979248\n",
            "-----Fold:1--------\n",
            "Epoch 1/500\n",
            "556/556 - 2s - loss: 37.6893 - mse: 37.6893 - mae: 1.8886 - val_loss: 8.6899 - val_mse: 8.6899 - val_mae: 2.5811 - lr: 2.8583e-04 - 2s/epoch - 4ms/step\n",
            "Epoch 2/500\n",
            "556/556 - 1s - loss: 34.5117 - mse: 34.5117 - mae: 1.6294 - val_loss: 9.1304 - val_mse: 9.1304 - val_mae: 2.5961 - lr: 2.8583e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 3/500\n",
            "556/556 - 1s - loss: 34.0580 - mse: 34.0580 - mae: 1.6043 - val_loss: 9.1647 - val_mse: 9.1647 - val_mae: 2.5960 - lr: 2.8583e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 4/500\n",
            "556/556 - 1s - loss: 33.7818 - mse: 33.7818 - mae: 1.5962 - val_loss: 9.5239 - val_mse: 9.5239 - val_mae: 2.6088 - lr: 2.8583e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 5/500\n",
            "556/556 - 1s - loss: 33.5400 - mse: 33.5400 - mae: 1.5819 - val_loss: 9.7286 - val_mse: 9.7286 - val_mae: 2.6298 - lr: 2.8583e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 6/500\n",
            "556/556 - 1s - loss: 33.3381 - mse: 33.3381 - mae: 1.5701 - val_loss: 9.1992 - val_mse: 9.1992 - val_mae: 2.5444 - lr: 2.8583e-04 - 1s/epoch - 2ms/step\n",
            "Score for inner fold : loss of 9.199234008789062\n",
            "-----Fold:2--------\n",
            "Epoch 1/500\n",
            "556/556 - 2s - loss: 36.2791 - mse: 36.2791 - mae: 1.8645 - val_loss: 8.7517 - val_mse: 8.7517 - val_mae: 2.5900 - lr: 2.8583e-04 - 2s/epoch - 4ms/step\n",
            "Epoch 2/500\n",
            "556/556 - 1s - loss: 33.2640 - mse: 33.2640 - mae: 1.6129 - val_loss: 8.7886 - val_mse: 8.7886 - val_mae: 2.5572 - lr: 2.8583e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 3/500\n",
            "556/556 - 1s - loss: 32.9084 - mse: 32.9084 - mae: 1.5893 - val_loss: 9.3324 - val_mse: 9.3324 - val_mae: 2.5786 - lr: 2.8583e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 4/500\n",
            "556/556 - 1s - loss: 32.6957 - mse: 32.6957 - mae: 1.5804 - val_loss: 9.6979 - val_mse: 9.6979 - val_mae: 2.6464 - lr: 2.8583e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 5/500\n",
            "556/556 - 1s - loss: 32.5092 - mse: 32.5092 - mae: 1.5710 - val_loss: 9.3396 - val_mse: 9.3396 - val_mae: 2.5667 - lr: 2.8583e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 6/500\n",
            "556/556 - 1s - loss: 32.3186 - mse: 32.3186 - mae: 1.5647 - val_loss: 9.8720 - val_mse: 9.8720 - val_mae: 2.6224 - lr: 2.8583e-04 - 1s/epoch - 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-10-10 09:24:10,514]\u001b[0m Finished trial#25 resulted in value: 9.533333333333331. Current best value is 8.08 with parameters: {'activation': 'tanh', 'num_hidden_layer': 4, 'i': 3, 'learning_rate': 0.00018239009621767336}.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Score for inner fold : loss of 9.871970176696777\n",
            "Trial Number:26\n",
            "-----Fold:0--------\n",
            "Epoch 1/500\n",
            "556/556 - 2s - loss: 42.2451 - mse: 42.2451 - mae: 2.4203 - val_loss: 1.6558 - val_mse: 1.6558 - val_mae: 0.9887 - lr: 1.5121e-04 - 2s/epoch - 3ms/step\n",
            "Epoch 2/500\n",
            "556/556 - 1s - loss: 38.2574 - mse: 38.2574 - mae: 1.7999 - val_loss: 4.9402 - val_mse: 4.9402 - val_mae: 1.9710 - lr: 1.5121e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 3/500\n",
            "556/556 - 1s - loss: 35.8060 - mse: 35.8060 - mae: 1.6807 - val_loss: 7.4331 - val_mse: 7.4331 - val_mae: 2.4808 - lr: 1.5121e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 4/500\n",
            "556/556 - 1s - loss: 35.2745 - mse: 35.2745 - mae: 1.6989 - val_loss: 7.7205 - val_mse: 7.7205 - val_mae: 2.5049 - lr: 1.5121e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 5/500\n",
            "556/556 - 1s - loss: 35.0182 - mse: 35.0182 - mae: 1.6802 - val_loss: 7.9741 - val_mse: 7.9741 - val_mae: 2.5383 - lr: 1.5121e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 6/500\n",
            "556/556 - 1s - loss: 34.8031 - mse: 34.8031 - mae: 1.6647 - val_loss: 8.0120 - val_mse: 8.0120 - val_mae: 2.5198 - lr: 1.5121e-04 - 1s/epoch - 2ms/step\n",
            "Score for inner fold : loss of 8.012036323547363\n",
            "-----Fold:1--------\n",
            "Epoch 1/500\n",
            "556/556 - 2s - loss: 22.4344 - mse: 22.4344 - mae: 2.4085 - val_loss: 1.7992 - val_mse: 1.7992 - val_mae: 1.0315 - lr: 1.5121e-04 - 2s/epoch - 3ms/step\n",
            "Epoch 2/500\n",
            "556/556 - 1s - loss: 18.8133 - mse: 18.8133 - mae: 1.7949 - val_loss: 4.5901 - val_mse: 4.5901 - val_mae: 1.9029 - lr: 1.5121e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 3/500\n",
            "556/556 - 1s - loss: 16.5870 - mse: 16.5870 - mae: 1.6827 - val_loss: 7.1758 - val_mse: 7.1758 - val_mae: 2.4521 - lr: 1.5121e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 4/500\n",
            "556/556 - 1s - loss: 16.0762 - mse: 16.0762 - mae: 1.7278 - val_loss: 7.6874 - val_mse: 7.6874 - val_mae: 2.5328 - lr: 1.5121e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 5/500\n",
            "556/556 - 1s - loss: 15.8545 - mse: 15.8545 - mae: 1.7195 - val_loss: 7.9212 - val_mse: 7.9212 - val_mae: 2.5605 - lr: 1.5121e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 6/500\n",
            "556/556 - 1s - loss: 15.6729 - mse: 15.6729 - mae: 1.7060 - val_loss: 7.8773 - val_mse: 7.8773 - val_mae: 2.5442 - lr: 1.5121e-04 - 1s/epoch - 2ms/step\n",
            "Score for inner fold : loss of 7.877254486083984\n",
            "-----Fold:2--------\n",
            "Epoch 1/500\n",
            "556/556 - 2s - loss: 41.5078 - mse: 41.5078 - mae: 2.3610 - val_loss: 2.5987 - val_mse: 2.5987 - val_mae: 1.2784 - lr: 1.5121e-04 - 2s/epoch - 4ms/step\n",
            "Epoch 2/500\n",
            "556/556 - 1s - loss: 37.2669 - mse: 37.2669 - mae: 1.8175 - val_loss: 6.0764 - val_mse: 6.0764 - val_mae: 2.1859 - lr: 1.5121e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 3/500\n",
            "556/556 - 1s - loss: 35.3240 - mse: 35.3240 - mae: 1.7515 - val_loss: 7.8513 - val_mse: 7.8513 - val_mae: 2.5366 - lr: 1.5121e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 4/500\n",
            "556/556 - 1s - loss: 34.8111 - mse: 34.8111 - mae: 1.7341 - val_loss: 8.1427 - val_mse: 8.1427 - val_mae: 2.5807 - lr: 1.5121e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 5/500\n",
            "556/556 - 1s - loss: 34.5250 - mse: 34.5250 - mae: 1.7122 - val_loss: 8.2118 - val_mse: 8.2118 - val_mae: 2.5744 - lr: 1.5121e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 6/500\n",
            "556/556 - 1s - loss: 34.2990 - mse: 34.2990 - mae: 1.6937 - val_loss: 8.2815 - val_mse: 8.2815 - val_mae: 2.5768 - lr: 1.5121e-04 - 1s/epoch - 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-10-10 09:24:35,310]\u001b[0m Finished trial#26 resulted in value: 8.056666666666667. Current best value is 8.056666666666667 with parameters: {'activation': 'tanh', 'num_hidden_layer': 4, 'i': 3, 'learning_rate': 0.00015120585772264658}.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Score for inner fold : loss of 8.281547546386719\n",
            "Trial Number:27\n",
            "-----Fold:0--------\n",
            "Epoch 1/500\n",
            "556/556 - 2s - loss: 42.2890 - mse: 42.2890 - mae: 2.3400 - val_loss: 2.2294 - val_mse: 2.2294 - val_mae: 1.1753 - lr: 1.0198e-04 - 2s/epoch - 3ms/step\n",
            "Epoch 2/500\n",
            "556/556 - 1s - loss: 37.6340 - mse: 37.6340 - mae: 1.7684 - val_loss: 6.3304 - val_mse: 6.3304 - val_mae: 2.2146 - lr: 1.0198e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 3/500\n",
            "556/556 - 1s - loss: 35.5540 - mse: 35.5540 - mae: 1.6927 - val_loss: 8.0124 - val_mse: 8.0124 - val_mae: 2.5243 - lr: 1.0198e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 4/500\n",
            "556/556 - 1s - loss: 35.1133 - mse: 35.1133 - mae: 1.6804 - val_loss: 8.3145 - val_mse: 8.3145 - val_mae: 2.5609 - lr: 1.0198e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 5/500\n",
            "556/556 - 1s - loss: 34.8704 - mse: 34.8704 - mae: 1.6668 - val_loss: 8.3641 - val_mse: 8.3641 - val_mae: 2.5590 - lr: 1.0198e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 6/500\n",
            "556/556 - 1s - loss: 34.6683 - mse: 34.6683 - mae: 1.6525 - val_loss: 8.4841 - val_mse: 8.4841 - val_mae: 2.5661 - lr: 1.0198e-04 - 1s/epoch - 2ms/step\n",
            "Score for inner fold : loss of 8.484119415283203\n",
            "-----Fold:1--------\n",
            "Epoch 1/500\n",
            "556/556 - 2s - loss: 21.3306 - mse: 21.3306 - mae: 2.3496 - val_loss: 1.6957 - val_mse: 1.6957 - val_mae: 1.0062 - lr: 1.0198e-04 - 2s/epoch - 3ms/step\n",
            "Epoch 2/500\n",
            "556/556 - 1s - loss: 16.6452 - mse: 16.6452 - mae: 1.6726 - val_loss: 6.4536 - val_mse: 6.4536 - val_mae: 2.2675 - lr: 1.0198e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 3/500\n",
            "556/556 - 1s - loss: 14.4057 - mse: 14.4057 - mae: 1.6694 - val_loss: 8.1611 - val_mse: 8.1611 - val_mae: 2.5500 - lr: 1.0198e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 4/500\n",
            "556/556 - 1s - loss: 13.9931 - mse: 13.9931 - mae: 1.6695 - val_loss: 8.1769 - val_mse: 8.1769 - val_mae: 2.5423 - lr: 1.0198e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 5/500\n",
            "556/556 - 1s - loss: 13.7702 - mse: 13.7702 - mae: 1.6499 - val_loss: 8.4234 - val_mse: 8.4234 - val_mae: 2.5630 - lr: 1.0198e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 6/500\n",
            "556/556 - 1s - loss: 13.5879 - mse: 13.5879 - mae: 1.6424 - val_loss: 8.3094 - val_mse: 8.3094 - val_mae: 2.5364 - lr: 1.0198e-04 - 1s/epoch - 2ms/step\n",
            "Score for inner fold : loss of 8.309385299682617\n",
            "-----Fold:2--------\n",
            "Epoch 1/500\n",
            "556/556 - 2s - loss: 41.8009 - mse: 41.8009 - mae: 2.3694 - val_loss: 2.0637 - val_mse: 2.0637 - val_mae: 1.1282 - lr: 1.0198e-04 - 2s/epoch - 3ms/step\n",
            "Epoch 2/500\n",
            "556/556 - 1s - loss: 37.0350 - mse: 37.0350 - mae: 1.7232 - val_loss: 6.7340 - val_mse: 6.7340 - val_mae: 2.3122 - lr: 1.0198e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 3/500\n",
            "556/556 - 1s - loss: 35.1900 - mse: 35.1900 - mae: 1.6903 - val_loss: 7.9461 - val_mse: 7.9461 - val_mae: 2.5250 - lr: 1.0198e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 4/500\n",
            "556/556 - 1s - loss: 34.8233 - mse: 34.8233 - mae: 1.6883 - val_loss: 8.1083 - val_mse: 8.1083 - val_mae: 2.5363 - lr: 1.0198e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 5/500\n",
            "556/556 - 1s - loss: 34.5786 - mse: 34.5786 - mae: 1.6694 - val_loss: 8.1433 - val_mse: 8.1433 - val_mae: 2.5346 - lr: 1.0198e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 6/500\n",
            "556/556 - 1s - loss: 34.3677 - mse: 34.3677 - mae: 1.6554 - val_loss: 8.3443 - val_mse: 8.3443 - val_mae: 2.5629 - lr: 1.0198e-04 - 1s/epoch - 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-10-10 09:25:00,006]\u001b[0m Finished trial#27 resulted in value: 8.376666666666667. Current best value is 8.056666666666667 with parameters: {'activation': 'tanh', 'num_hidden_layer': 4, 'i': 3, 'learning_rate': 0.00015120585772264658}.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Score for inner fold : loss of 8.344342231750488\n",
            "Trial Number:28\n",
            "-----Fold:0--------\n",
            "Epoch 1/500\n",
            "556/556 - 2s - loss: 42.4639 - mse: 42.4639 - mae: 2.3591 - val_loss: 1.7400 - val_mse: 1.7400 - val_mae: 1.0478 - lr: 1.5227e-04 - 2s/epoch - 3ms/step\n",
            "Epoch 2/500\n",
            "556/556 - 1s - loss: 38.1400 - mse: 38.1400 - mae: 1.7056 - val_loss: 5.5544 - val_mse: 5.5544 - val_mae: 2.1495 - lr: 1.5227e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 3/500\n",
            "556/556 - 1s - loss: 35.9416 - mse: 35.9416 - mae: 1.7088 - val_loss: 7.5226 - val_mse: 7.5226 - val_mae: 2.5079 - lr: 1.5227e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 4/500\n",
            "556/556 - 1s - loss: 35.4237 - mse: 35.4237 - mae: 1.7270 - val_loss: 7.8790 - val_mse: 7.8790 - val_mae: 2.5477 - lr: 1.5227e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 5/500\n",
            "556/556 - 1s - loss: 35.1855 - mse: 35.1855 - mae: 1.7159 - val_loss: 7.9656 - val_mse: 7.9656 - val_mae: 2.5442 - lr: 1.5227e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 6/500\n",
            "556/556 - 1s - loss: 34.9849 - mse: 34.9849 - mae: 1.6990 - val_loss: 8.1240 - val_mse: 8.1240 - val_mae: 2.5601 - lr: 1.5227e-04 - 1s/epoch - 2ms/step\n",
            "Score for inner fold : loss of 8.124039649963379\n",
            "-----Fold:1--------\n",
            "Epoch 1/500\n",
            "556/556 - 2s - loss: 42.2106 - mse: 42.2106 - mae: 2.4806 - val_loss: 1.7589 - val_mse: 1.7589 - val_mae: 1.0164 - lr: 1.5227e-04 - 2s/epoch - 3ms/step\n",
            "Epoch 2/500\n",
            "556/556 - 1s - loss: 38.0299 - mse: 38.0299 - mae: 1.7900 - val_loss: 5.3998 - val_mse: 5.3998 - val_mae: 2.0620 - lr: 1.5227e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 3/500\n",
            "556/556 - 1s - loss: 35.8410 - mse: 35.8410 - mae: 1.6889 - val_loss: 7.6752 - val_mse: 7.6752 - val_mae: 2.5139 - lr: 1.5227e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 4/500\n",
            "556/556 - 1s - loss: 35.4762 - mse: 35.4762 - mae: 1.7132 - val_loss: 7.8578 - val_mse: 7.8578 - val_mae: 2.5256 - lr: 1.5227e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 5/500\n",
            "556/556 - 1s - loss: 35.2695 - mse: 35.2695 - mae: 1.7032 - val_loss: 7.9931 - val_mse: 7.9931 - val_mae: 2.5309 - lr: 1.5227e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 6/500\n",
            "556/556 - 1s - loss: 35.0968 - mse: 35.0968 - mae: 1.6965 - val_loss: 7.9840 - val_mse: 7.9840 - val_mae: 2.5293 - lr: 1.5227e-04 - 1s/epoch - 2ms/step\n",
            "Score for inner fold : loss of 7.9839911460876465\n",
            "-----Fold:2--------\n",
            "Epoch 1/500\n",
            "556/556 - 2s - loss: 21.6891 - mse: 21.6891 - mae: 2.3886 - val_loss: 1.8048 - val_mse: 1.8048 - val_mae: 1.0312 - lr: 1.5227e-04 - 2s/epoch - 3ms/step\n",
            "Epoch 2/500\n",
            "556/556 - 1s - loss: 17.7957 - mse: 17.7957 - mae: 1.7698 - val_loss: 5.0747 - val_mse: 5.0747 - val_mae: 1.9922 - lr: 1.5227e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 3/500\n",
            "556/556 - 1s - loss: 15.4986 - mse: 15.4986 - mae: 1.7055 - val_loss: 7.7406 - val_mse: 7.7406 - val_mae: 2.5205 - lr: 1.5227e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 4/500\n",
            "556/556 - 1s - loss: 14.9417 - mse: 14.9417 - mae: 1.7128 - val_loss: 8.1323 - val_mse: 8.1323 - val_mae: 2.5673 - lr: 1.5227e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 5/500\n",
            "556/556 - 1s - loss: 14.6643 - mse: 14.6643 - mae: 1.6866 - val_loss: 8.0771 - val_mse: 8.0771 - val_mae: 2.5438 - lr: 1.5227e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 6/500\n",
            "556/556 - 1s - loss: 14.4557 - mse: 14.4557 - mae: 1.6667 - val_loss: 7.9815 - val_mse: 7.9815 - val_mae: 2.5155 - lr: 1.5227e-04 - 1s/epoch - 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-10-10 09:25:24,506]\u001b[0m Finished trial#28 resulted in value: 8.026666666666667. Current best value is 8.026666666666667 with parameters: {'activation': 'tanh', 'num_hidden_layer': 4, 'i': 3, 'learning_rate': 0.00015227349581784595}.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Score for inner fold : loss of 7.98147439956665\n",
            "Trial Number:29\n",
            "-----Fold:0--------\n",
            "Epoch 1/500\n",
            "556/556 - 2s - loss: 37.3613 - mse: 37.3613 - mae: 1.9269 - val_loss: 8.7057 - val_mse: 8.7057 - val_mae: 2.4673 - lr: 1.5084e-04 - 2s/epoch - 4ms/step\n",
            "Epoch 2/500\n",
            "556/556 - 1s - loss: 33.4427 - mse: 33.4427 - mae: 1.6649 - val_loss: 9.2056 - val_mse: 9.2056 - val_mae: 2.5171 - lr: 1.5084e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 3/500\n",
            "556/556 - 1s - loss: 32.5528 - mse: 32.5528 - mae: 1.6235 - val_loss: 10.0685 - val_mse: 10.0685 - val_mae: 2.5460 - lr: 1.5084e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 4/500\n",
            "556/556 - 1s - loss: 32.0681 - mse: 32.0681 - mae: 1.6006 - val_loss: 10.1063 - val_mse: 10.1063 - val_mae: 2.4776 - lr: 1.5084e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 5/500\n",
            "556/556 - 1s - loss: 31.8159 - mse: 31.8159 - mae: 1.5863 - val_loss: 11.5556 - val_mse: 11.5556 - val_mae: 2.5824 - lr: 1.5084e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 6/500\n",
            "556/556 - 1s - loss: 31.6603 - mse: 31.6603 - mae: 1.5770 - val_loss: 12.2962 - val_mse: 12.2962 - val_mae: 2.6709 - lr: 1.5084e-04 - 1s/epoch - 2ms/step\n",
            "Score for inner fold : loss of 12.296220779418945\n",
            "-----Fold:1--------\n",
            "Epoch 1/500\n",
            "556/556 - 2s - loss: 34.6007 - mse: 34.6007 - mae: 1.7232 - val_loss: 9.7712 - val_mse: 9.7712 - val_mae: 2.5431 - lr: 1.5084e-04 - 2s/epoch - 4ms/step\n",
            "Epoch 2/500\n",
            "556/556 - 1s - loss: 32.9041 - mse: 32.9041 - mae: 1.6554 - val_loss: 10.5086 - val_mse: 10.5086 - val_mae: 2.5809 - lr: 1.5084e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 3/500\n",
            "556/556 - 1s - loss: 32.4218 - mse: 32.4218 - mae: 1.6187 - val_loss: 10.9891 - val_mse: 10.9891 - val_mae: 2.5929 - lr: 1.5084e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 4/500\n",
            "556/556 - 1s - loss: 32.2396 - mse: 32.2396 - mae: 1.6024 - val_loss: 11.6437 - val_mse: 11.6437 - val_mae: 2.6309 - lr: 1.5084e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 5/500\n",
            "556/556 - 1s - loss: 32.1717 - mse: 32.1717 - mae: 1.5933 - val_loss: 12.4323 - val_mse: 12.4323 - val_mae: 2.6671 - lr: 1.5084e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 6/500\n",
            "556/556 - 1s - loss: 32.1538 - mse: 32.1538 - mae: 1.5847 - val_loss: 12.2006 - val_mse: 12.2006 - val_mae: 2.6230 - lr: 1.5084e-04 - 1s/epoch - 2ms/step\n",
            "Score for inner fold : loss of 12.200563430786133\n",
            "-----Fold:2--------\n",
            "Epoch 1/500\n",
            "556/556 - 2s - loss: 16.4580 - mse: 16.4580 - mae: 1.9108 - val_loss: 9.1890 - val_mse: 9.1890 - val_mae: 2.5642 - lr: 1.5084e-04 - 2s/epoch - 4ms/step\n",
            "Epoch 2/500\n",
            "556/556 - 1s - loss: 12.4641 - mse: 12.4641 - mae: 1.6424 - val_loss: 10.6359 - val_mse: 10.6359 - val_mae: 2.6376 - lr: 1.5084e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 3/500\n",
            "556/556 - 1s - loss: 11.6051 - mse: 11.6051 - mae: 1.6069 - val_loss: 11.0243 - val_mse: 11.0243 - val_mae: 2.5903 - lr: 1.5084e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 4/500\n",
            "556/556 - 1s - loss: 11.2769 - mse: 11.2769 - mae: 1.5704 - val_loss: 11.6107 - val_mse: 11.6107 - val_mae: 2.6283 - lr: 1.5084e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 5/500\n",
            "556/556 - 1s - loss: 11.0752 - mse: 11.0752 - mae: 1.5659 - val_loss: 11.6290 - val_mse: 11.6290 - val_mae: 2.5931 - lr: 1.5084e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 6/500\n",
            "556/556 - 1s - loss: 11.0061 - mse: 11.0061 - mae: 1.5547 - val_loss: 12.5530 - val_mse: 12.5530 - val_mae: 2.6706 - lr: 1.5084e-04 - 1s/epoch - 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-10-10 09:25:51,697]\u001b[0m Finished trial#29 resulted in value: 12.35. Current best value is 8.026666666666667 with parameters: {'activation': 'tanh', 'num_hidden_layer': 4, 'i': 3, 'learning_rate': 0.00015227349581784595}.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Score for inner fold : loss of 12.553027153015137\n",
            "Trial Number:30\n",
            "-----Fold:0--------\n",
            "Epoch 1/500\n",
            "556/556 - 2s - loss: 43.9918 - mse: 43.9918 - mae: 2.4272 - val_loss: 1.6141 - val_mse: 1.6141 - val_mae: 0.9699 - lr: 1.5185e-04 - 2s/epoch - 3ms/step\n",
            "Epoch 2/500\n",
            "556/556 - 1s - loss: 39.8901 - mse: 39.8901 - mae: 1.8040 - val_loss: 4.3735 - val_mse: 4.3735 - val_mae: 1.8614 - lr: 1.5185e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 3/500\n",
            "556/556 - 1s - loss: 37.1818 - mse: 37.1818 - mae: 1.6807 - val_loss: 7.3570 - val_mse: 7.3570 - val_mae: 2.4862 - lr: 1.5185e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 4/500\n",
            "556/556 - 1s - loss: 36.5928 - mse: 36.5928 - mae: 1.7203 - val_loss: 7.9119 - val_mse: 7.9119 - val_mae: 2.5646 - lr: 1.5185e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 5/500\n",
            "556/556 - 1s - loss: 36.3397 - mse: 36.3397 - mae: 1.7146 - val_loss: 7.8796 - val_mse: 7.8796 - val_mae: 2.5380 - lr: 1.5185e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 6/500\n",
            "556/556 - 1s - loss: 36.1355 - mse: 36.1355 - mae: 1.6953 - val_loss: 8.0633 - val_mse: 8.0633 - val_mae: 2.5560 - lr: 1.5185e-04 - 1s/epoch - 2ms/step\n",
            "Score for inner fold : loss of 8.063346862792969\n",
            "-----Fold:1--------\n",
            "Epoch 1/500\n",
            "556/556 - 2s - loss: 20.6924 - mse: 20.6924 - mae: 2.2698 - val_loss: 2.5484 - val_mse: 2.5484 - val_mae: 1.2798 - lr: 1.5185e-04 - 2s/epoch - 3ms/step\n",
            "Epoch 2/500\n",
            "556/556 - 1s - loss: 16.6615 - mse: 16.6615 - mae: 1.7140 - val_loss: 6.3968 - val_mse: 6.3968 - val_mae: 2.2746 - lr: 1.5185e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 3/500\n",
            "556/556 - 1s - loss: 14.9050 - mse: 14.9050 - mae: 1.6993 - val_loss: 7.7900 - val_mse: 7.7900 - val_mae: 2.5328 - lr: 1.5185e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 4/500\n",
            "556/556 - 1s - loss: 14.4688 - mse: 14.4688 - mae: 1.6902 - val_loss: 7.9834 - val_mse: 7.9834 - val_mae: 2.5447 - lr: 1.5185e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 5/500\n",
            "556/556 - 1s - loss: 14.1910 - mse: 14.1910 - mae: 1.6711 - val_loss: 8.0506 - val_mse: 8.0506 - val_mae: 2.5422 - lr: 1.5185e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 6/500\n",
            "556/556 - 1s - loss: 13.9585 - mse: 13.9585 - mae: 1.6511 - val_loss: 8.0958 - val_mse: 8.0958 - val_mae: 2.5343 - lr: 1.5185e-04 - 1s/epoch - 2ms/step\n",
            "Score for inner fold : loss of 8.095836639404297\n",
            "-----Fold:2--------\n",
            "Epoch 1/500\n",
            "556/556 - 2s - loss: 40.9518 - mse: 40.9518 - mae: 2.4080 - val_loss: 2.1186 - val_mse: 2.1186 - val_mae: 1.1564 - lr: 1.5185e-04 - 2s/epoch - 4ms/step\n",
            "Epoch 2/500\n",
            "556/556 - 1s - loss: 36.6387 - mse: 36.6387 - mae: 1.7449 - val_loss: 6.3363 - val_mse: 6.3363 - val_mae: 2.2686 - lr: 1.5185e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 3/500\n",
            "556/556 - 1s - loss: 34.8712 - mse: 34.8712 - mae: 1.7041 - val_loss: 7.8714 - val_mse: 7.8714 - val_mae: 2.5460 - lr: 1.5185e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 4/500\n",
            "556/556 - 1s - loss: 34.5043 - mse: 34.5043 - mae: 1.7061 - val_loss: 8.0614 - val_mse: 8.0614 - val_mae: 2.5597 - lr: 1.5185e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 5/500\n",
            "556/556 - 1s - loss: 34.2687 - mse: 34.2687 - mae: 1.6875 - val_loss: 8.0163 - val_mse: 8.0163 - val_mae: 2.5407 - lr: 1.5185e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 6/500\n",
            "556/556 - 1s - loss: 34.0780 - mse: 34.0780 - mae: 1.6713 - val_loss: 8.1168 - val_mse: 8.1168 - val_mae: 2.5459 - lr: 1.5185e-04 - 1s/epoch - 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-10-10 09:26:16,468]\u001b[0m Finished trial#30 resulted in value: 8.093333333333334. Current best value is 8.026666666666667 with parameters: {'activation': 'tanh', 'num_hidden_layer': 4, 'i': 3, 'learning_rate': 0.00015227349581784595}.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Score for inner fold : loss of 8.116846084594727\n",
            "Trial Number:31\n",
            "-----Fold:0--------\n",
            "Epoch 1/500\n",
            "556/556 - 2s - loss: 42.1593 - mse: 42.1593 - mae: 2.3868 - val_loss: 1.8105 - val_mse: 1.8105 - val_mae: 1.0347 - lr: 1.4177e-04 - 2s/epoch - 3ms/step\n",
            "Epoch 2/500\n",
            "556/556 - 1s - loss: 38.2936 - mse: 38.2936 - mae: 1.8230 - val_loss: 4.9911 - val_mse: 4.9911 - val_mae: 1.9563 - lr: 1.4177e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 3/500\n",
            "556/556 - 1s - loss: 35.8658 - mse: 35.8658 - mae: 1.7198 - val_loss: 7.8632 - val_mse: 7.8632 - val_mae: 2.5428 - lr: 1.4177e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 4/500\n",
            "556/556 - 1s - loss: 35.3817 - mse: 35.3817 - mae: 1.7487 - val_loss: 8.0221 - val_mse: 8.0221 - val_mae: 2.5653 - lr: 1.4177e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 5/500\n",
            "556/556 - 1s - loss: 35.1785 - mse: 35.1785 - mae: 1.7244 - val_loss: 8.0518 - val_mse: 8.0518 - val_mae: 2.5616 - lr: 1.4177e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 6/500\n",
            "556/556 - 1s - loss: 35.0003 - mse: 35.0003 - mae: 1.7095 - val_loss: 8.0148 - val_mse: 8.0148 - val_mae: 2.5434 - lr: 1.4177e-04 - 1s/epoch - 2ms/step\n",
            "Score for inner fold : loss of 8.014775276184082\n",
            "-----Fold:1--------\n",
            "Epoch 1/500\n",
            "556/556 - 2s - loss: 20.5255 - mse: 20.5255 - mae: 2.2362 - val_loss: 2.7845 - val_mse: 2.7845 - val_mae: 1.3498 - lr: 1.4177e-04 - 2s/epoch - 3ms/step\n",
            "Epoch 2/500\n",
            "556/556 - 1s - loss: 16.5785 - mse: 16.5785 - mae: 1.7189 - val_loss: 6.7491 - val_mse: 6.7491 - val_mae: 2.3266 - lr: 1.4177e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 3/500\n",
            "556/556 - 1s - loss: 15.1609 - mse: 15.1609 - mae: 1.7031 - val_loss: 7.9813 - val_mse: 7.9813 - val_mae: 2.5493 - lr: 1.4177e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 4/500\n",
            "556/556 - 1s - loss: 14.7918 - mse: 14.7918 - mae: 1.7018 - val_loss: 8.1598 - val_mse: 8.1598 - val_mae: 2.5636 - lr: 1.4177e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 5/500\n",
            "556/556 - 1s - loss: 14.5548 - mse: 14.5548 - mae: 1.6836 - val_loss: 8.2215 - val_mse: 8.2215 - val_mae: 2.5667 - lr: 1.4177e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 6/500\n",
            "556/556 - 1s - loss: 14.3719 - mse: 14.3719 - mae: 1.6683 - val_loss: 8.2957 - val_mse: 8.2957 - val_mae: 2.5675 - lr: 1.4177e-04 - 1s/epoch - 2ms/step\n",
            "Score for inner fold : loss of 8.295731544494629\n",
            "-----Fold:2--------\n",
            "Epoch 1/500\n",
            "556/556 - 2s - loss: 42.4802 - mse: 42.4802 - mae: 2.4714 - val_loss: 1.4525 - val_mse: 1.4525 - val_mae: 0.8937 - lr: 1.4177e-04 - 2s/epoch - 3ms/step\n",
            "Epoch 2/500\n",
            "556/556 - 1s - loss: 39.1092 - mse: 39.1092 - mae: 1.8870 - val_loss: 3.6258 - val_mse: 3.6258 - val_mae: 1.6357 - lr: 1.4177e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 3/500\n",
            "556/556 - 1s - loss: 36.4380 - mse: 36.4380 - mae: 1.6439 - val_loss: 6.8495 - val_mse: 6.8495 - val_mae: 2.3851 - lr: 1.4177e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 4/500\n",
            "556/556 - 1s - loss: 35.6959 - mse: 35.6959 - mae: 1.7036 - val_loss: 7.6963 - val_mse: 7.6963 - val_mae: 2.5252 - lr: 1.4177e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 5/500\n",
            "556/556 - 1s - loss: 35.4596 - mse: 35.4596 - mae: 1.7074 - val_loss: 7.8236 - val_mse: 7.8236 - val_mae: 2.5316 - lr: 1.4177e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 6/500\n",
            "556/556 - 1s - loss: 35.2662 - mse: 35.2662 - mae: 1.6901 - val_loss: 7.8025 - val_mse: 7.8025 - val_mae: 2.5133 - lr: 1.4177e-04 - 1s/epoch - 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-10-10 09:26:40,966]\u001b[0m Finished trial#31 resulted in value: 8.036666666666667. Current best value is 8.026666666666667 with parameters: {'activation': 'tanh', 'num_hidden_layer': 4, 'i': 3, 'learning_rate': 0.00015227349581784595}.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Score for inner fold : loss of 7.802490711212158\n",
            "Trial Number:32\n",
            "-----Fold:0--------\n",
            "Epoch 1/500\n",
            "556/556 - 2s - loss: 21.5251 - mse: 21.5251 - mae: 2.2523 - val_loss: 2.6770 - val_mse: 2.6770 - val_mae: 1.3362 - lr: 1.3053e-04 - 2s/epoch - 4ms/step\n",
            "Epoch 2/500\n",
            "556/556 - 1s - loss: 16.3135 - mse: 16.3135 - mae: 1.6785 - val_loss: 7.7970 - val_mse: 7.7970 - val_mae: 2.5172 - lr: 1.3053e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 3/500\n",
            "556/556 - 1s - loss: 14.9725 - mse: 14.9725 - mae: 1.6771 - val_loss: 8.2569 - val_mse: 8.2569 - val_mae: 2.5661 - lr: 1.3053e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 4/500\n",
            "556/556 - 1s - loss: 14.6001 - mse: 14.6001 - mae: 1.6542 - val_loss: 8.3246 - val_mse: 8.3246 - val_mae: 2.5535 - lr: 1.3053e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 5/500\n",
            "556/556 - 1s - loss: 14.3592 - mse: 14.3592 - mae: 1.6379 - val_loss: 8.4221 - val_mse: 8.4221 - val_mae: 2.5513 - lr: 1.3053e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 6/500\n",
            "556/556 - 1s - loss: 14.1797 - mse: 14.1797 - mae: 1.6269 - val_loss: 8.8247 - val_mse: 8.8247 - val_mae: 2.6112 - lr: 1.3053e-04 - 1s/epoch - 2ms/step\n",
            "Score for inner fold : loss of 8.824698448181152\n",
            "-----Fold:1--------\n",
            "Epoch 1/500\n",
            "556/556 - 2s - loss: 40.1567 - mse: 40.1567 - mae: 2.3296 - val_loss: 2.3632 - val_mse: 2.3632 - val_mae: 1.2314 - lr: 1.3053e-04 - 2s/epoch - 3ms/step\n",
            "Epoch 2/500\n",
            "556/556 - 1s - loss: 35.2814 - mse: 35.2814 - mae: 1.6632 - val_loss: 7.7006 - val_mse: 7.7006 - val_mae: 2.4890 - lr: 1.3053e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 3/500\n",
            "556/556 - 1s - loss: 33.8046 - mse: 33.8046 - mae: 1.6673 - val_loss: 8.3861 - val_mse: 8.3861 - val_mae: 2.6035 - lr: 1.3053e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 4/500\n",
            "556/556 - 1s - loss: 33.4418 - mse: 33.4418 - mae: 1.6521 - val_loss: 8.2345 - val_mse: 8.2345 - val_mae: 2.5494 - lr: 1.3053e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 5/500\n",
            "556/556 - 1s - loss: 33.1883 - mse: 33.1883 - mae: 1.6330 - val_loss: 8.4819 - val_mse: 8.4819 - val_mae: 2.5603 - lr: 1.3053e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 6/500\n",
            "556/556 - 1s - loss: 32.9907 - mse: 32.9907 - mae: 1.6186 - val_loss: 8.6090 - val_mse: 8.6090 - val_mae: 2.5750 - lr: 1.3053e-04 - 1s/epoch - 2ms/step\n",
            "Score for inner fold : loss of 8.608970642089844\n",
            "-----Fold:2--------\n",
            "Epoch 1/500\n",
            "556/556 - 2s - loss: 41.8758 - mse: 41.8758 - mae: 2.2815 - val_loss: 2.4668 - val_mse: 2.4668 - val_mae: 1.2637 - lr: 1.3053e-04 - 2s/epoch - 3ms/step\n",
            "Epoch 2/500\n",
            "556/556 - 1s - loss: 36.4652 - mse: 36.4652 - mae: 1.6588 - val_loss: 7.9337 - val_mse: 7.9337 - val_mae: 2.5185 - lr: 1.3053e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 3/500\n",
            "556/556 - 1s - loss: 35.2261 - mse: 35.2261 - mae: 1.6835 - val_loss: 8.2832 - val_mse: 8.2832 - val_mae: 2.5486 - lr: 1.3053e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 4/500\n",
            "556/556 - 1s - loss: 34.9136 - mse: 34.9136 - mae: 1.6613 - val_loss: 8.4162 - val_mse: 8.4162 - val_mae: 2.5667 - lr: 1.3053e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 5/500\n",
            "556/556 - 1s - loss: 34.6751 - mse: 34.6751 - mae: 1.6430 - val_loss: 8.3896 - val_mse: 8.3896 - val_mae: 2.5419 - lr: 1.3053e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 6/500\n",
            "556/556 - 1s - loss: 34.4752 - mse: 34.4752 - mae: 1.6294 - val_loss: 8.6432 - val_mse: 8.6432 - val_mae: 2.5607 - lr: 1.3053e-04 - 1s/epoch - 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-10-10 09:27:05,547]\u001b[0m Finished trial#32 resulted in value: 8.69. Current best value is 8.026666666666667 with parameters: {'activation': 'tanh', 'num_hidden_layer': 4, 'i': 3, 'learning_rate': 0.00015227349581784595}.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Score for inner fold : loss of 8.643237113952637\n",
            "Trial Number:33\n",
            "-----Fold:0--------\n",
            "Epoch 1/500\n",
            "556/556 - 2s - loss: 22.4302 - mse: 22.4302 - mae: 2.4431 - val_loss: 1.2675 - val_mse: 1.2675 - val_mae: 0.8258 - lr: 1.0365e-04 - 2s/epoch - 3ms/step\n",
            "Epoch 2/500\n",
            "556/556 - 1s - loss: 20.1390 - mse: 20.1390 - mae: 2.0325 - val_loss: 2.0406 - val_mse: 2.0406 - val_mae: 1.1722 - lr: 1.0365e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 3/500\n",
            "556/556 - 1s - loss: 17.5165 - mse: 17.5165 - mae: 1.7000 - val_loss: 4.3168 - val_mse: 4.3168 - val_mae: 1.8521 - lr: 1.0365e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 4/500\n",
            "556/556 - 1s - loss: 15.6666 - mse: 15.6666 - mae: 1.6577 - val_loss: 6.8350 - val_mse: 6.8350 - val_mae: 2.3990 - lr: 1.0365e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 5/500\n",
            "556/556 - 1s - loss: 15.0149 - mse: 15.0149 - mae: 1.7048 - val_loss: 7.6241 - val_mse: 7.6241 - val_mae: 2.5311 - lr: 1.0365e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 6/500\n",
            "556/556 - 1s - loss: 14.7323 - mse: 14.7323 - mae: 1.7011 - val_loss: 7.7871 - val_mse: 7.7871 - val_mae: 2.5420 - lr: 1.0365e-04 - 1s/epoch - 2ms/step\n",
            "Score for inner fold : loss of 7.7871174812316895\n",
            "-----Fold:1--------\n",
            "Epoch 1/500\n",
            "556/556 - 2s - loss: 42.4963 - mse: 42.4963 - mae: 2.4257 - val_loss: 1.4322 - val_mse: 1.4322 - val_mae: 0.8946 - lr: 1.0365e-04 - 2s/epoch - 3ms/step\n",
            "Epoch 2/500\n",
            "556/556 - 1s - loss: 40.1028 - mse: 40.1028 - mae: 2.0073 - val_loss: 2.6895 - val_mse: 2.6895 - val_mae: 1.3654 - lr: 1.0365e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 3/500\n",
            "556/556 - 1s - loss: 37.4887 - mse: 37.4887 - mae: 1.7157 - val_loss: 5.2299 - val_mse: 5.2299 - val_mae: 2.0489 - lr: 1.0365e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 4/500\n",
            "556/556 - 1s - loss: 35.8463 - mse: 35.8463 - mae: 1.6813 - val_loss: 7.0278 - val_mse: 7.0278 - val_mae: 2.4093 - lr: 1.0365e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 5/500\n",
            "556/556 - 1s - loss: 35.2165 - mse: 35.2165 - mae: 1.6830 - val_loss: 7.5846 - val_mse: 7.5846 - val_mae: 2.4904 - lr: 1.0365e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 6/500\n",
            "556/556 - 1s - loss: 34.8680 - mse: 34.8680 - mae: 1.6735 - val_loss: 7.7980 - val_mse: 7.7980 - val_mae: 2.5129 - lr: 1.0365e-04 - 1s/epoch - 2ms/step\n",
            "Score for inner fold : loss of 7.798041343688965\n",
            "-----Fold:2--------\n",
            "Epoch 1/500\n",
            "556/556 - 2s - loss: 43.7879 - mse: 43.7879 - mae: 2.4786 - val_loss: 1.4148 - val_mse: 1.4148 - val_mae: 0.8566 - lr: 1.0365e-04 - 2s/epoch - 3ms/step\n",
            "Epoch 2/500\n",
            "556/556 - 1s - loss: 40.9466 - mse: 40.9466 - mae: 1.9921 - val_loss: 2.8185 - val_mse: 2.8185 - val_mae: 1.3854 - lr: 1.0365e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 3/500\n",
            "556/556 - 1s - loss: 38.0892 - mse: 38.0892 - mae: 1.6779 - val_loss: 6.0173 - val_mse: 6.0173 - val_mae: 2.2003 - lr: 1.0365e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 4/500\n",
            "556/556 - 1s - loss: 36.7589 - mse: 36.7589 - mae: 1.7239 - val_loss: 7.6753 - val_mse: 7.6753 - val_mae: 2.5247 - lr: 1.0365e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 5/500\n",
            "556/556 - 1s - loss: 36.3593 - mse: 36.3593 - mae: 1.7438 - val_loss: 7.8662 - val_mse: 7.8662 - val_mae: 2.5408 - lr: 1.0365e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 6/500\n",
            "556/556 - 1s - loss: 36.1248 - mse: 36.1248 - mae: 1.7321 - val_loss: 8.0424 - val_mse: 8.0424 - val_mae: 2.5577 - lr: 1.0365e-04 - 1s/epoch - 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-10-10 09:27:30,025]\u001b[0m Finished trial#33 resulted in value: 7.876666666666666. Current best value is 7.876666666666666 with parameters: {'activation': 'tanh', 'num_hidden_layer': 4, 'i': 3, 'learning_rate': 0.00010364528181587206}.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Score for inner fold : loss of 8.042363166809082\n",
            "Trial Number:34\n",
            "-----Fold:0--------\n",
            "Epoch 1/500\n",
            "556/556 - 2s - loss: 43.7257 - mse: 43.7257 - mae: 2.5139 - val_loss: 1.2519 - val_mse: 1.2519 - val_mae: 0.8041 - lr: 1.0292e-04 - 2s/epoch - 3ms/step\n",
            "Epoch 2/500\n",
            "556/556 - 1s - loss: 41.7355 - mse: 41.7355 - mae: 2.1430 - val_loss: 1.8608 - val_mse: 1.8608 - val_mae: 1.0812 - lr: 1.0292e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 3/500\n",
            "556/556 - 1s - loss: 39.3163 - mse: 39.3163 - mae: 1.7496 - val_loss: 3.6167 - val_mse: 3.6167 - val_mae: 1.6573 - lr: 1.0292e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 4/500\n",
            "556/556 - 1s - loss: 37.4313 - mse: 37.4313 - mae: 1.6265 - val_loss: 6.1203 - val_mse: 6.1203 - val_mae: 2.2624 - lr: 1.0292e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 5/500\n",
            "556/556 - 1s - loss: 36.6877 - mse: 36.6877 - mae: 1.6979 - val_loss: 7.1097 - val_mse: 7.1097 - val_mae: 2.4436 - lr: 1.0292e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 6/500\n",
            "556/556 - 1s - loss: 36.3812 - mse: 36.3812 - mae: 1.7001 - val_loss: 7.3378 - val_mse: 7.3378 - val_mae: 2.4675 - lr: 1.0292e-04 - 1s/epoch - 2ms/step\n",
            "Score for inner fold : loss of 7.337752819061279\n",
            "-----Fold:1--------\n",
            "Epoch 1/500\n",
            "556/556 - 2s - loss: 42.3879 - mse: 42.3879 - mae: 2.5208 - val_loss: 1.1295 - val_mse: 1.1295 - val_mae: 0.7548 - lr: 1.0292e-04 - 2s/epoch - 3ms/step\n",
            "Epoch 2/500\n",
            "556/556 - 1s - loss: 39.6568 - mse: 39.6568 - mae: 2.0782 - val_loss: 2.1000 - val_mse: 2.1000 - val_mae: 1.1869 - lr: 1.0292e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 3/500\n",
            "556/556 - 1s - loss: 36.7915 - mse: 36.7915 - mae: 1.7053 - val_loss: 4.7076 - val_mse: 4.7076 - val_mae: 1.9388 - lr: 1.0292e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 4/500\n",
            "556/556 - 1s - loss: 35.0905 - mse: 35.0905 - mae: 1.6815 - val_loss: 7.0808 - val_mse: 7.0808 - val_mae: 2.4345 - lr: 1.0292e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 5/500\n",
            "556/556 - 1s - loss: 34.5954 - mse: 34.5954 - mae: 1.7218 - val_loss: 7.6738 - val_mse: 7.6738 - val_mae: 2.5160 - lr: 1.0292e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 6/500\n",
            "556/556 - 1s - loss: 34.3728 - mse: 34.3728 - mae: 1.7187 - val_loss: 7.8697 - val_mse: 7.8697 - val_mae: 2.5366 - lr: 1.0292e-04 - 1s/epoch - 2ms/step\n",
            "Score for inner fold : loss of 7.869751453399658\n",
            "-----Fold:2--------\n",
            "Epoch 1/500\n",
            "556/556 - 2s - loss: 22.6770 - mse: 22.6770 - mae: 2.5272 - val_loss: 1.1535 - val_mse: 1.1535 - val_mae: 0.7580 - lr: 1.0292e-04 - 2s/epoch - 3ms/step\n",
            "Epoch 2/500\n",
            "556/556 - 1s - loss: 20.5063 - mse: 20.5063 - mae: 2.1108 - val_loss: 1.8509 - val_mse: 1.8509 - val_mae: 1.0890 - lr: 1.0292e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 3/500\n",
            "556/556 - 1s - loss: 17.9856 - mse: 17.9856 - mae: 1.7024 - val_loss: 3.8925 - val_mse: 3.8925 - val_mae: 1.7483 - lr: 1.0292e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 4/500\n",
            "556/556 - 1s - loss: 16.1927 - mse: 16.1927 - mae: 1.6248 - val_loss: 6.4542 - val_mse: 6.4542 - val_mae: 2.3281 - lr: 1.0292e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 5/500\n",
            "556/556 - 1s - loss: 15.5604 - mse: 15.5604 - mae: 1.6921 - val_loss: 7.5647 - val_mse: 7.5647 - val_mae: 2.5148 - lr: 1.0292e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 6/500\n",
            "556/556 - 1s - loss: 15.3232 - mse: 15.3232 - mae: 1.7144 - val_loss: 7.9555 - val_mse: 7.9555 - val_mae: 2.5673 - lr: 1.0292e-04 - 1s/epoch - 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-10-10 09:27:54,590]\u001b[0m Finished trial#34 resulted in value: 7.723333333333334. Current best value is 7.723333333333334 with parameters: {'activation': 'tanh', 'num_hidden_layer': 4, 'i': 3, 'learning_rate': 0.00010292007837467995}.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Score for inner fold : loss of 7.955455303192139\n",
            "Trial Number:35\n",
            "-----Fold:0--------\n",
            "Epoch 1/500\n",
            "556/556 - 2s - loss: 42.1425 - mse: 42.1425 - mae: 2.3559 - val_loss: 2.0804 - val_mse: 2.0804 - val_mae: 1.1427 - lr: 1.0338e-04 - 2s/epoch - 3ms/step\n",
            "Epoch 2/500\n",
            "556/556 - 1s - loss: 37.7671 - mse: 37.7671 - mae: 1.7665 - val_loss: 6.2002 - val_mse: 6.2002 - val_mae: 2.2165 - lr: 1.0338e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 3/500\n",
            "556/556 - 1s - loss: 35.4177 - mse: 35.4177 - mae: 1.6691 - val_loss: 8.1903 - val_mse: 8.1903 - val_mae: 2.5682 - lr: 1.0338e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 4/500\n",
            "556/556 - 1s - loss: 34.8918 - mse: 34.8918 - mae: 1.6594 - val_loss: 8.2955 - val_mse: 8.2955 - val_mae: 2.5646 - lr: 1.0338e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 5/500\n",
            "556/556 - 1s - loss: 34.6336 - mse: 34.6336 - mae: 1.6495 - val_loss: 8.1857 - val_mse: 8.1857 - val_mae: 2.5271 - lr: 1.0338e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 6/500\n",
            "556/556 - 1s - loss: 34.4577 - mse: 34.4577 - mae: 1.6342 - val_loss: 8.6075 - val_mse: 8.6075 - val_mae: 2.5881 - lr: 1.0338e-04 - 1s/epoch - 2ms/step\n",
            "Score for inner fold : loss of 8.607528686523438\n",
            "-----Fold:1--------\n",
            "Epoch 1/500\n",
            "556/556 - 2s - loss: 42.5491 - mse: 42.5491 - mae: 2.4004 - val_loss: 1.8377 - val_mse: 1.8377 - val_mae: 1.0572 - lr: 1.0338e-04 - 2s/epoch - 3ms/step\n",
            "Epoch 2/500\n",
            "556/556 - 1s - loss: 37.9361 - mse: 37.9361 - mae: 1.7571 - val_loss: 6.2650 - val_mse: 6.2650 - val_mae: 2.2340 - lr: 1.0338e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 3/500\n",
            "556/556 - 1s - loss: 35.5667 - mse: 35.5667 - mae: 1.7136 - val_loss: 7.8637 - val_mse: 7.8637 - val_mae: 2.5313 - lr: 1.0338e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 4/500\n",
            "556/556 - 1s - loss: 35.0003 - mse: 35.0003 - mae: 1.6915 - val_loss: 8.0326 - val_mse: 8.0326 - val_mae: 2.5264 - lr: 1.0338e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 5/500\n",
            "556/556 - 1s - loss: 34.6673 - mse: 34.6673 - mae: 1.6690 - val_loss: 8.0318 - val_mse: 8.0318 - val_mae: 2.5049 - lr: 1.0338e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 6/500\n",
            "556/556 - 1s - loss: 34.4375 - mse: 34.4375 - mae: 1.6494 - val_loss: 8.3849 - val_mse: 8.3849 - val_mae: 2.5459 - lr: 1.0338e-04 - 1s/epoch - 2ms/step\n",
            "Score for inner fold : loss of 8.384867668151855\n",
            "-----Fold:2--------\n",
            "Epoch 1/500\n",
            "556/556 - 2s - loss: 20.9816 - mse: 20.9816 - mae: 2.3722 - val_loss: 1.7333 - val_mse: 1.7333 - val_mae: 0.9979 - lr: 1.0338e-04 - 2s/epoch - 4ms/step\n",
            "Epoch 2/500\n",
            "556/556 - 1s - loss: 16.7589 - mse: 16.7589 - mae: 1.7118 - val_loss: 5.7397 - val_mse: 5.7397 - val_mae: 2.1241 - lr: 1.0338e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 3/500\n",
            "556/556 - 1s - loss: 14.2412 - mse: 14.2412 - mae: 1.6396 - val_loss: 7.9060 - val_mse: 7.9060 - val_mae: 2.5195 - lr: 1.0338e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 4/500\n",
            "556/556 - 1s - loss: 13.7905 - mse: 13.7905 - mae: 1.6505 - val_loss: 8.0373 - val_mse: 8.0373 - val_mae: 2.5247 - lr: 1.0338e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 5/500\n",
            "556/556 - 1s - loss: 13.5774 - mse: 13.5774 - mae: 1.6396 - val_loss: 8.2650 - val_mse: 8.2650 - val_mae: 2.5481 - lr: 1.0338e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 6/500\n",
            "556/556 - 1s - loss: 13.4235 - mse: 13.4235 - mae: 1.6286 - val_loss: 8.3531 - val_mse: 8.3531 - val_mae: 2.5456 - lr: 1.0338e-04 - 1s/epoch - 2ms/step\n",
            "Score for inner fold : loss of 8.353120803833008\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-10-10 09:28:19,522]\u001b[0m Finished trial#35 resulted in value: 8.446666666666667. Current best value is 7.723333333333334 with parameters: {'activation': 'tanh', 'num_hidden_layer': 4, 'i': 3, 'learning_rate': 0.00010292007837467995}.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trial Number:36\n",
            "-----Fold:0--------\n",
            "Epoch 1/500\n",
            "556/556 - 2s - loss: 40.3428 - mse: 40.3428 - mae: 2.3691 - val_loss: 3.0954 - val_mse: 3.0954 - val_mae: 1.2927 - lr: 1.0155e-04 - 2s/epoch - 3ms/step\n",
            "Epoch 2/500\n",
            "556/556 - 1s - loss: 33.2313 - mse: 33.2313 - mae: 1.6812 - val_loss: 8.8244 - val_mse: 8.8244 - val_mae: 2.4566 - lr: 1.0155e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 3/500\n",
            "556/556 - 1s - loss: 31.7936 - mse: 31.7936 - mae: 1.6289 - val_loss: 10.6034 - val_mse: 10.6034 - val_mae: 2.6766 - lr: 1.0155e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 4/500\n",
            "556/556 - 1s - loss: 31.5864 - mse: 31.5864 - mae: 1.6355 - val_loss: 11.2672 - val_mse: 11.2672 - val_mae: 2.7137 - lr: 1.0155e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 5/500\n",
            "556/556 - 1s - loss: 31.5405 - mse: 31.5405 - mae: 1.6293 - val_loss: 11.1473 - val_mse: 11.1473 - val_mae: 2.6738 - lr: 1.0155e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 6/500\n",
            "556/556 - 1s - loss: 31.4900 - mse: 31.4900 - mae: 1.6260 - val_loss: 11.7139 - val_mse: 11.7139 - val_mae: 2.7014 - lr: 1.0155e-04 - 1s/epoch - 2ms/step\n",
            "Score for inner fold : loss of 11.713871955871582\n",
            "-----Fold:1--------\n",
            "Epoch 1/500\n",
            "556/556 - 2s - loss: 19.9608 - mse: 19.9608 - mae: 2.3048 - val_loss: 4.1025 - val_mse: 4.1025 - val_mae: 1.4437 - lr: 1.0155e-04 - 2s/epoch - 3ms/step\n",
            "Epoch 2/500\n",
            "556/556 - 1s - loss: 13.3016 - mse: 13.3016 - mae: 1.6106 - val_loss: 9.9658 - val_mse: 9.9658 - val_mae: 2.5314 - lr: 1.0155e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 3/500\n",
            "556/556 - 1s - loss: 12.1209 - mse: 12.1209 - mae: 1.5810 - val_loss: 11.3092 - val_mse: 11.3092 - val_mae: 2.6600 - lr: 1.0155e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 4/500\n",
            "556/556 - 1s - loss: 11.9754 - mse: 11.9754 - mae: 1.5781 - val_loss: 12.0687 - val_mse: 12.0687 - val_mae: 2.6697 - lr: 1.0155e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 5/500\n",
            "556/556 - 1s - loss: 11.9534 - mse: 11.9534 - mae: 1.5726 - val_loss: 12.3630 - val_mse: 12.3630 - val_mae: 2.6999 - lr: 1.0155e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 6/500\n",
            "556/556 - 1s - loss: 11.8934 - mse: 11.8934 - mae: 1.5764 - val_loss: 13.0264 - val_mse: 13.0264 - val_mae: 2.6851 - lr: 1.0155e-04 - 1s/epoch - 2ms/step\n",
            "Score for inner fold : loss of 13.026440620422363\n",
            "-----Fold:2--------\n",
            "Epoch 1/500\n",
            "556/556 - 2s - loss: 41.2184 - mse: 41.2184 - mae: 2.2579 - val_loss: 4.2620 - val_mse: 4.2620 - val_mae: 1.5008 - lr: 1.0155e-04 - 2s/epoch - 3ms/step\n",
            "Epoch 2/500\n",
            "556/556 - 1s - loss: 34.1984 - mse: 34.1984 - mae: 1.6228 - val_loss: 10.1988 - val_mse: 10.1988 - val_mae: 2.5999 - lr: 1.0155e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 3/500\n",
            "556/556 - 1s - loss: 32.7199 - mse: 32.7199 - mae: 1.6333 - val_loss: 11.2540 - val_mse: 11.2540 - val_mae: 2.6565 - lr: 1.0155e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 4/500\n",
            "556/556 - 1s - loss: 32.6109 - mse: 32.6109 - mae: 1.6304 - val_loss: 11.6225 - val_mse: 11.6225 - val_mae: 2.6314 - lr: 1.0155e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 5/500\n",
            "556/556 - 1s - loss: 32.5576 - mse: 32.5576 - mae: 1.6226 - val_loss: 12.0618 - val_mse: 12.0618 - val_mae: 2.6201 - lr: 1.0155e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 6/500\n",
            "556/556 - 1s - loss: 32.5627 - mse: 32.5627 - mae: 1.6167 - val_loss: 12.6587 - val_mse: 12.6587 - val_mae: 2.6759 - lr: 1.0155e-04 - 1s/epoch - 2ms/step\n",
            "Score for inner fold : loss of 12.658686637878418\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-10-10 09:28:43,772]\u001b[0m Finished trial#36 resulted in value: 12.466666666666669. Current best value is 7.723333333333334 with parameters: {'activation': 'tanh', 'num_hidden_layer': 4, 'i': 3, 'learning_rate': 0.00010292007837467995}.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trial Number:37\n",
            "-----Fold:0--------\n",
            "Epoch 1/500\n",
            "556/556 - 2s - loss: 36.1487 - mse: 36.1487 - mae: 1.8043 - val_loss: 9.6895 - val_mse: 9.6895 - val_mae: 2.5764 - lr: 3.1302e-04 - 2s/epoch - 3ms/step\n",
            "Epoch 2/500\n",
            "556/556 - 1s - loss: 34.1449 - mse: 34.1449 - mae: 1.6798 - val_loss: 9.7258 - val_mse: 9.7258 - val_mae: 2.5822 - lr: 3.1302e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 3/500\n",
            "556/556 - 1s - loss: 33.6458 - mse: 33.6458 - mae: 1.6501 - val_loss: 9.8377 - val_mse: 9.8377 - val_mae: 2.5649 - lr: 3.1302e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 4/500\n",
            "556/556 - 1s - loss: 33.3957 - mse: 33.3957 - mae: 1.6311 - val_loss: 10.0260 - val_mse: 10.0260 - val_mae: 2.5876 - lr: 3.1302e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 5/500\n",
            "556/556 - 1s - loss: 33.1759 - mse: 33.1759 - mae: 1.6195 - val_loss: 11.3410 - val_mse: 11.3410 - val_mae: 2.7106 - lr: 3.1302e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 6/500\n",
            "556/556 - 1s - loss: 33.0782 - mse: 33.0782 - mae: 1.6116 - val_loss: 10.8709 - val_mse: 10.8709 - val_mae: 2.6512 - lr: 3.1302e-04 - 1s/epoch - 2ms/step\n",
            "Score for inner fold : loss of 10.870902061462402\n",
            "-----Fold:1--------\n",
            "Epoch 1/500\n",
            "556/556 - 2s - loss: 18.0618 - mse: 18.0618 - mae: 1.9716 - val_loss: 7.2389 - val_mse: 7.2389 - val_mae: 2.2696 - lr: 3.1302e-04 - 2s/epoch - 3ms/step\n",
            "Epoch 2/500\n",
            "556/556 - 1s - loss: 13.4505 - mse: 13.4505 - mae: 1.6785 - val_loss: 8.8969 - val_mse: 8.8969 - val_mae: 2.5530 - lr: 3.1302e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 3/500\n",
            "556/556 - 1s - loss: 12.4589 - mse: 12.4589 - mae: 1.6475 - val_loss: 9.9114 - val_mse: 9.9114 - val_mae: 2.6293 - lr: 3.1302e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 4/500\n",
            "556/556 - 1s - loss: 12.0770 - mse: 12.0770 - mae: 1.6281 - val_loss: 10.8969 - val_mse: 10.8969 - val_mae: 2.6855 - lr: 3.1302e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 5/500\n",
            "556/556 - 1s - loss: 11.7538 - mse: 11.7538 - mae: 1.6219 - val_loss: 10.7556 - val_mse: 10.7556 - val_mae: 2.6207 - lr: 3.1302e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 6/500\n",
            "556/556 - 1s - loss: 11.6296 - mse: 11.6296 - mae: 1.6018 - val_loss: 11.6686 - val_mse: 11.6686 - val_mae: 2.6940 - lr: 3.1302e-04 - 1s/epoch - 2ms/step\n",
            "Score for inner fold : loss of 11.668569564819336\n",
            "-----Fold:2--------\n",
            "Epoch 1/500\n",
            "556/556 - 2s - loss: 37.9777 - mse: 37.9777 - mae: 1.9468 - val_loss: 7.4370 - val_mse: 7.4370 - val_mae: 2.3375 - lr: 3.1302e-04 - 2s/epoch - 4ms/step\n",
            "Epoch 2/500\n",
            "556/556 - 1s - loss: 33.6423 - mse: 33.6423 - mae: 1.7114 - val_loss: 9.6773 - val_mse: 9.6773 - val_mae: 2.5819 - lr: 3.1302e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 3/500\n",
            "556/556 - 1s - loss: 32.7750 - mse: 32.7750 - mae: 1.6727 - val_loss: 9.9038 - val_mse: 9.9038 - val_mae: 2.5768 - lr: 3.1302e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 4/500\n",
            "556/556 - 1s - loss: 32.2446 - mse: 32.2446 - mae: 1.6479 - val_loss: 10.3480 - val_mse: 10.3480 - val_mae: 2.6057 - lr: 3.1302e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 5/500\n",
            "556/556 - 1s - loss: 31.9545 - mse: 31.9545 - mae: 1.6341 - val_loss: 10.2404 - val_mse: 10.2404 - val_mae: 2.5905 - lr: 3.1302e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 6/500\n",
            "556/556 - 1s - loss: 31.7766 - mse: 31.7766 - mae: 1.6291 - val_loss: 11.1297 - val_mse: 11.1297 - val_mae: 2.6484 - lr: 3.1302e-04 - 1s/epoch - 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-10-10 09:29:08,340]\u001b[0m Finished trial#37 resulted in value: 11.223333333333334. Current best value is 7.723333333333334 with parameters: {'activation': 'tanh', 'num_hidden_layer': 4, 'i': 3, 'learning_rate': 0.00010292007837467995}.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Score for inner fold : loss of 11.129743576049805\n",
            "Trial Number:38\n",
            "-----Fold:0--------\n",
            "Epoch 1/500\n",
            "556/556 - 2s - loss: 14.5182 - mse: 14.5182 - mae: 1.7983 - val_loss: 10.1944 - val_mse: 10.1944 - val_mae: 2.6250 - lr: 1.2314e-04 - 2s/epoch - 4ms/step\n",
            "Epoch 2/500\n",
            "556/556 - 2s - loss: 11.5629 - mse: 11.5629 - mae: 1.5588 - val_loss: 10.1767 - val_mse: 10.1767 - val_mae: 2.6797 - lr: 1.2314e-04 - 2s/epoch - 3ms/step\n",
            "Epoch 3/500\n",
            "556/556 - 2s - loss: 11.3063 - mse: 11.3063 - mae: 1.5473 - val_loss: 10.2189 - val_mse: 10.2189 - val_mae: 2.6450 - lr: 1.2314e-04 - 2s/epoch - 3ms/step\n",
            "Epoch 4/500\n",
            "556/556 - 2s - loss: 11.1308 - mse: 11.1308 - mae: 1.5295 - val_loss: 10.6655 - val_mse: 10.6655 - val_mae: 2.6652 - lr: 1.2314e-04 - 2s/epoch - 3ms/step\n",
            "Epoch 5/500\n",
            "556/556 - 2s - loss: 10.9733 - mse: 10.9733 - mae: 1.5220 - val_loss: 9.7556 - val_mse: 9.7556 - val_mae: 2.5357 - lr: 1.2314e-04 - 2s/epoch - 3ms/step\n",
            "Epoch 6/500\n",
            "556/556 - 2s - loss: 10.8373 - mse: 10.8373 - mae: 1.5099 - val_loss: 10.9050 - val_mse: 10.9050 - val_mae: 2.6323 - lr: 1.2314e-04 - 2s/epoch - 3ms/step\n",
            "Epoch 7/500\n",
            "556/556 - 2s - loss: 10.7262 - mse: 10.7262 - mae: 1.5076 - val_loss: 11.0440 - val_mse: 11.0440 - val_mae: 2.6702 - lr: 1.2314e-04 - 2s/epoch - 3ms/step\n",
            "Epoch 8/500\n",
            "556/556 - 2s - loss: 10.6395 - mse: 10.6395 - mae: 1.5068 - val_loss: 10.8083 - val_mse: 10.8083 - val_mae: 2.6190 - lr: 1.2314e-04 - 2s/epoch - 3ms/step\n",
            "Epoch 9/500\n",
            "556/556 - 2s - loss: 10.5758 - mse: 10.5758 - mae: 1.5032 - val_loss: 10.5749 - val_mse: 10.5749 - val_mae: 2.6223 - lr: 1.2314e-04 - 2s/epoch - 3ms/step\n",
            "Epoch 10/500\n",
            "556/556 - 2s - loss: 10.5080 - mse: 10.5080 - mae: 1.4948 - val_loss: 10.7970 - val_mse: 10.7970 - val_mae: 2.5987 - lr: 1.2314e-04 - 2s/epoch - 3ms/step\n",
            "Score for inner fold : loss of 10.79702091217041\n",
            "-----Fold:1--------\n",
            "Epoch 1/500\n",
            "556/556 - 2s - loss: 36.1790 - mse: 36.1790 - mae: 1.8222 - val_loss: 9.4007 - val_mse: 9.4007 - val_mae: 2.5842 - lr: 1.2314e-04 - 2s/epoch - 4ms/step\n",
            "Epoch 2/500\n",
            "556/556 - 2s - loss: 33.4981 - mse: 33.4981 - mae: 1.5894 - val_loss: 9.5340 - val_mse: 9.5340 - val_mae: 2.5860 - lr: 1.2314e-04 - 2s/epoch - 3ms/step\n",
            "Epoch 3/500\n",
            "556/556 - 2s - loss: 33.1326 - mse: 33.1326 - mae: 1.5756 - val_loss: 10.7438 - val_mse: 10.7438 - val_mae: 2.6963 - lr: 1.2314e-04 - 2s/epoch - 3ms/step\n",
            "Epoch 4/500\n",
            "556/556 - 2s - loss: 32.9702 - mse: 32.9702 - mae: 1.5662 - val_loss: 10.2708 - val_mse: 10.2708 - val_mae: 2.6699 - lr: 1.2314e-04 - 2s/epoch - 3ms/step\n",
            "Epoch 5/500\n",
            "556/556 - 2s - loss: 32.8110 - mse: 32.8110 - mae: 1.5548 - val_loss: 10.3381 - val_mse: 10.3381 - val_mae: 2.6161 - lr: 1.2314e-04 - 2s/epoch - 3ms/step\n",
            "Epoch 6/500\n",
            "556/556 - 2s - loss: 32.6543 - mse: 32.6543 - mae: 1.5514 - val_loss: 9.9913 - val_mse: 9.9913 - val_mae: 2.6151 - lr: 1.2314e-04 - 2s/epoch - 3ms/step\n",
            "Score for inner fold : loss of 9.991337776184082\n",
            "-----Fold:2--------\n",
            "Epoch 1/500\n",
            "556/556 - 3s - loss: 35.0332 - mse: 35.0332 - mae: 1.7988 - val_loss: 9.1701 - val_mse: 9.1701 - val_mae: 2.5567 - lr: 1.2314e-04 - 3s/epoch - 5ms/step\n",
            "Epoch 2/500\n",
            "556/556 - 2s - loss: 32.1864 - mse: 32.1864 - mae: 1.5698 - val_loss: 9.7037 - val_mse: 9.7037 - val_mae: 2.5802 - lr: 1.2314e-04 - 2s/epoch - 3ms/step\n",
            "Epoch 3/500\n",
            "556/556 - 2s - loss: 31.8846 - mse: 31.8846 - mae: 1.5539 - val_loss: 10.1664 - val_mse: 10.1664 - val_mae: 2.6176 - lr: 1.2314e-04 - 2s/epoch - 3ms/step\n",
            "Epoch 4/500\n",
            "556/556 - 2s - loss: 31.6886 - mse: 31.6886 - mae: 1.5447 - val_loss: 9.8992 - val_mse: 9.8992 - val_mae: 2.5797 - lr: 1.2314e-04 - 2s/epoch - 3ms/step\n",
            "Epoch 5/500\n",
            "556/556 - 2s - loss: 31.5187 - mse: 31.5187 - mae: 1.5365 - val_loss: 9.9818 - val_mse: 9.9818 - val_mae: 2.5841 - lr: 1.2314e-04 - 2s/epoch - 3ms/step\n",
            "Epoch 6/500\n",
            "556/556 - 2s - loss: 31.3784 - mse: 31.3784 - mae: 1.5250 - val_loss: 10.2402 - val_mse: 10.2402 - val_mae: 2.5895 - lr: 1.2314e-04 - 2s/epoch - 3ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-10-10 09:29:50,134]\u001b[0m Finished trial#38 resulted in value: 10.343333333333334. Current best value is 7.723333333333334 with parameters: {'activation': 'tanh', 'num_hidden_layer': 4, 'i': 3, 'learning_rate': 0.00010292007837467995}.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Score for inner fold : loss of 10.240235328674316\n",
            "Trial Number:39\n",
            "-----Fold:0--------\n",
            "Epoch 1/500\n",
            "556/556 - 5s - loss: 12.2454 - mse: 12.2454 - mae: 1.6776 - val_loss: 9.9691 - val_mse: 9.9691 - val_mae: 2.6738 - lr: 6.3806e-04 - 5s/epoch - 9ms/step\n",
            "Epoch 2/500\n",
            "556/556 - 4s - loss: 11.3649 - mse: 11.3649 - mae: 1.5964 - val_loss: 8.0890 - val_mse: 8.0890 - val_mae: 2.3751 - lr: 6.3806e-04 - 4s/epoch - 7ms/step\n",
            "Epoch 3/500\n",
            "556/556 - 4s - loss: 11.0756 - mse: 11.0756 - mae: 1.5676 - val_loss: 12.4393 - val_mse: 12.4393 - val_mae: 2.6788 - lr: 6.3806e-04 - 4s/epoch - 7ms/step\n",
            "Epoch 4/500\n",
            "556/556 - 4s - loss: 10.8477 - mse: 10.8477 - mae: 1.5558 - val_loss: 12.7373 - val_mse: 12.7373 - val_mae: 2.7817 - lr: 6.3806e-04 - 4s/epoch - 7ms/step\n",
            "Epoch 5/500\n",
            "556/556 - 4s - loss: 10.7202 - mse: 10.7202 - mae: 1.5541 - val_loss: 9.1847 - val_mse: 9.1847 - val_mae: 2.4150 - lr: 6.3806e-04 - 4s/epoch - 7ms/step\n",
            "Epoch 6/500\n",
            "556/556 - 4s - loss: 10.5828 - mse: 10.5828 - mae: 1.5456 - val_loss: 11.4909 - val_mse: 11.4909 - val_mae: 2.4882 - lr: 6.3806e-04 - 4s/epoch - 7ms/step\n",
            "Epoch 7/500\n",
            "556/556 - 4s - loss: 10.5960 - mse: 10.5960 - mae: 1.5393 - val_loss: 13.8763 - val_mse: 13.8763 - val_mae: 2.9213 - lr: 6.3806e-04 - 4s/epoch - 8ms/step\n",
            "Score for inner fold : loss of 13.876287460327148\n",
            "-----Fold:1--------\n",
            "Epoch 1/500\n",
            "556/556 - 5s - loss: 33.7624 - mse: 33.7624 - mae: 1.7001 - val_loss: 11.1023 - val_mse: 11.1023 - val_mae: 2.6763 - lr: 6.3806e-04 - 5s/epoch - 9ms/step\n",
            "Epoch 2/500\n",
            "556/556 - 4s - loss: 32.6387 - mse: 32.6387 - mae: 1.6132 - val_loss: 11.4064 - val_mse: 11.4064 - val_mae: 2.6899 - lr: 6.3806e-04 - 4s/epoch - 7ms/step\n",
            "Epoch 3/500\n",
            "556/556 - 4s - loss: 32.4436 - mse: 32.4436 - mae: 1.5933 - val_loss: 11.1693 - val_mse: 11.1693 - val_mae: 2.6234 - lr: 6.3806e-04 - 4s/epoch - 7ms/step\n",
            "Epoch 4/500\n",
            "556/556 - 4s - loss: 32.0803 - mse: 32.0803 - mae: 1.5972 - val_loss: 12.5569 - val_mse: 12.5569 - val_mae: 2.7806 - lr: 6.3806e-04 - 4s/epoch - 8ms/step\n",
            "Epoch 5/500\n",
            "556/556 - 4s - loss: 31.9817 - mse: 31.9817 - mae: 1.5920 - val_loss: 13.1392 - val_mse: 13.1392 - val_mae: 2.7813 - lr: 6.3806e-04 - 4s/epoch - 7ms/step\n",
            "Epoch 6/500\n",
            "556/556 - 4s - loss: 31.7907 - mse: 31.7907 - mae: 1.5793 - val_loss: 12.0229 - val_mse: 12.0229 - val_mae: 2.8556 - lr: 6.3806e-04 - 4s/epoch - 7ms/step\n",
            "Score for inner fold : loss of 12.022871017456055\n",
            "-----Fold:2--------\n",
            "Epoch 1/500\n",
            "556/556 - 5s - loss: 32.4201 - mse: 32.4201 - mae: 1.6949 - val_loss: 8.9547 - val_mse: 8.9547 - val_mae: 2.4189 - lr: 6.3806e-04 - 5s/epoch - 8ms/step\n",
            "Epoch 2/500\n",
            "556/556 - 4s - loss: 31.6644 - mse: 31.6644 - mae: 1.6191 - val_loss: 12.6742 - val_mse: 12.6742 - val_mae: 2.7882 - lr: 6.3806e-04 - 4s/epoch - 7ms/step\n",
            "Epoch 3/500\n",
            "556/556 - 4s - loss: 31.5147 - mse: 31.5147 - mae: 1.5968 - val_loss: 11.6158 - val_mse: 11.6158 - val_mae: 2.7382 - lr: 6.3806e-04 - 4s/epoch - 7ms/step\n",
            "Epoch 4/500\n",
            "556/556 - 4s - loss: 31.2417 - mse: 31.2417 - mae: 1.5894 - val_loss: 13.5550 - val_mse: 13.5550 - val_mae: 2.8738 - lr: 6.3806e-04 - 4s/epoch - 7ms/step\n",
            "Epoch 5/500\n",
            "556/556 - 4s - loss: 31.0840 - mse: 31.0840 - mae: 1.5776 - val_loss: 8.7032 - val_mse: 8.7032 - val_mae: 2.2833 - lr: 6.3806e-04 - 4s/epoch - 7ms/step\n",
            "Epoch 6/500\n",
            "556/556 - 4s - loss: 31.1493 - mse: 31.1493 - mae: 1.5746 - val_loss: 11.7397 - val_mse: 11.7397 - val_mae: 2.7187 - lr: 6.3806e-04 - 4s/epoch - 7ms/step\n",
            "Epoch 7/500\n",
            "556/556 - 4s - loss: 30.8956 - mse: 30.8956 - mae: 1.5641 - val_loss: 10.7563 - val_mse: 10.7563 - val_mae: 2.7828 - lr: 6.3806e-04 - 4s/epoch - 7ms/step\n",
            "Epoch 8/500\n",
            "556/556 - 4s - loss: 30.8156 - mse: 30.8156 - mae: 1.5525 - val_loss: 9.7636 - val_mse: 9.7636 - val_mae: 2.5248 - lr: 6.3806e-04 - 4s/epoch - 8ms/step\n",
            "Epoch 9/500\n",
            "556/556 - 4s - loss: 30.5694 - mse: 30.5694 - mae: 1.5487 - val_loss: 11.5701 - val_mse: 11.5701 - val_mae: 2.7642 - lr: 6.3806e-04 - 4s/epoch - 7ms/step\n",
            "Epoch 10/500\n",
            "556/556 - 4s - loss: 30.5142 - mse: 30.5142 - mae: 1.5488 - val_loss: 13.6856 - val_mse: 13.6856 - val_mae: 2.9187 - lr: 6.3806e-04 - 4s/epoch - 7ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-10-10 09:31:29,070]\u001b[0m Finished trial#39 resulted in value: 13.196666666666665. Current best value is 7.723333333333334 with parameters: {'activation': 'tanh', 'num_hidden_layer': 4, 'i': 3, 'learning_rate': 0.00010292007837467995}.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Score for inner fold : loss of 13.685567855834961\n",
            "Trial Number:40\n",
            "-----Fold:0--------\n",
            "Epoch 1/500\n",
            "556/556 - 2s - loss: 32.4504 - mse: 32.4504 - mae: 1.6305 - val_loss: 15.0436 - val_mse: 15.0436 - val_mae: 2.5586 - lr: 0.0038 - 2s/epoch - 4ms/step\n",
            "Epoch 2/500\n",
            "556/556 - 1s - loss: 31.3989 - mse: 31.3989 - mae: 1.5514 - val_loss: 12.0766 - val_mse: 12.0766 - val_mae: 2.7658 - lr: 0.0038 - 1s/epoch - 2ms/step\n",
            "Epoch 3/500\n",
            "556/556 - 1s - loss: 31.0716 - mse: 31.0716 - mae: 1.5492 - val_loss: 16.8829 - val_mse: 16.8829 - val_mae: 2.7980 - lr: 0.0038 - 1s/epoch - 2ms/step\n",
            "Epoch 4/500\n",
            "556/556 - 1s - loss: 31.0512 - mse: 31.0512 - mae: 1.5342 - val_loss: 16.8599 - val_mse: 16.8599 - val_mae: 2.6845 - lr: 0.0038 - 1s/epoch - 2ms/step\n",
            "Epoch 5/500\n",
            "556/556 - 1s - loss: 30.6980 - mse: 30.6980 - mae: 1.5266 - val_loss: 17.4099 - val_mse: 17.4099 - val_mae: 2.5844 - lr: 0.0038 - 1s/epoch - 2ms/step\n",
            "Epoch 6/500\n",
            "556/556 - 1s - loss: 30.8695 - mse: 30.8695 - mae: 1.5182 - val_loss: 16.7576 - val_mse: 16.7576 - val_mae: 2.6953 - lr: 0.0038 - 1s/epoch - 2ms/step\n",
            "Epoch 7/500\n",
            "556/556 - 1s - loss: 30.6503 - mse: 30.6503 - mae: 1.5112 - val_loss: 17.3413 - val_mse: 17.3413 - val_mae: 2.6600 - lr: 0.0038 - 1s/epoch - 2ms/step\n",
            "Score for inner fold : loss of 17.34128761291504\n",
            "-----Fold:1--------\n",
            "Epoch 1/500\n",
            "556/556 - 2s - loss: 11.3118 - mse: 11.3118 - mae: 1.6098 - val_loss: 12.4279 - val_mse: 12.4279 - val_mae: 2.5328 - lr: 0.0038 - 2s/epoch - 4ms/step\n",
            "Epoch 2/500\n",
            "556/556 - 1s - loss: 10.6258 - mse: 10.6258 - mae: 1.5538 - val_loss: 12.0500 - val_mse: 12.0500 - val_mae: 2.4476 - lr: 0.0038 - 1s/epoch - 2ms/step\n",
            "Epoch 3/500\n",
            "556/556 - 1s - loss: 10.0916 - mse: 10.0916 - mae: 1.5348 - val_loss: 14.3589 - val_mse: 14.3589 - val_mae: 2.8101 - lr: 0.0038 - 1s/epoch - 2ms/step\n",
            "Epoch 4/500\n",
            "556/556 - 1s - loss: 10.2922 - mse: 10.2922 - mae: 1.5369 - val_loss: 11.0363 - val_mse: 11.0363 - val_mae: 2.5900 - lr: 0.0038 - 1s/epoch - 2ms/step\n",
            "Epoch 5/500\n",
            "556/556 - 1s - loss: 10.2828 - mse: 10.2828 - mae: 1.5232 - val_loss: 14.2926 - val_mse: 14.2926 - val_mae: 2.7874 - lr: 0.0038 - 1s/epoch - 2ms/step\n",
            "Epoch 6/500\n",
            "556/556 - 1s - loss: 10.1036 - mse: 10.1036 - mae: 1.5149 - val_loss: 11.9043 - val_mse: 11.9043 - val_mae: 2.6621 - lr: 0.0038 - 1s/epoch - 2ms/step\n",
            "Epoch 7/500\n",
            "556/556 - 1s - loss: 11.2837 - mse: 11.2837 - mae: 1.5139 - val_loss: 10.9794 - val_mse: 10.9794 - val_mae: 2.6266 - lr: 0.0038 - 1s/epoch - 2ms/step\n",
            "Epoch 8/500\n",
            "556/556 - 1s - loss: 11.9662 - mse: 11.9662 - mae: 1.5172 - val_loss: 11.0111 - val_mse: 11.0111 - val_mae: 2.5460 - lr: 0.0038 - 1s/epoch - 2ms/step\n",
            "Epoch 9/500\n",
            "556/556 - 1s - loss: 10.3594 - mse: 10.3594 - mae: 1.5042 - val_loss: 11.9957 - val_mse: 11.9957 - val_mae: 2.5803 - lr: 0.0038 - 1s/epoch - 2ms/step\n",
            "Epoch 10/500\n",
            "556/556 - 1s - loss: 10.0190 - mse: 10.0190 - mae: 1.5034 - val_loss: 9.5759 - val_mse: 9.5759 - val_mae: 2.4404 - lr: 0.0038 - 1s/epoch - 2ms/step\n",
            "Epoch 11/500\n",
            "556/556 - 1s - loss: 11.0269 - mse: 11.0269 - mae: 1.5120 - val_loss: 14.4598 - val_mse: 14.4598 - val_mae: 2.7776 - lr: 0.0038 - 1s/epoch - 2ms/step\n",
            "Epoch 12/500\n",
            "556/556 - 1s - loss: 10.7873 - mse: 10.7873 - mae: 1.5032 - val_loss: 13.2851 - val_mse: 13.2851 - val_mae: 2.6938 - lr: 0.0038 - 1s/epoch - 2ms/step\n",
            "Epoch 13/500\n",
            "556/556 - 1s - loss: 10.3493 - mse: 10.3493 - mae: 1.4970 - val_loss: 13.5781 - val_mse: 13.5781 - val_mae: 2.6860 - lr: 0.0038 - 1s/epoch - 2ms/step\n",
            "Epoch 14/500\n",
            "556/556 - 1s - loss: 10.6780 - mse: 10.6780 - mae: 1.4944 - val_loss: 14.6508 - val_mse: 14.6508 - val_mae: 2.6629 - lr: 0.0038 - 1s/epoch - 2ms/step\n",
            "Epoch 15/500\n",
            "556/556 - 1s - loss: 10.5014 - mse: 10.5014 - mae: 1.4901 - val_loss: 12.5387 - val_mse: 12.5387 - val_mae: 2.6777 - lr: 0.0038 - 1s/epoch - 2ms/step\n",
            "Score for inner fold : loss of 12.538740158081055\n",
            "-----Fold:2--------\n",
            "Epoch 1/500\n",
            "556/556 - 2s - loss: 32.4601 - mse: 32.4601 - mae: 1.6064 - val_loss: 12.5574 - val_mse: 12.5574 - val_mae: 2.8451 - lr: 0.0038 - 2s/epoch - 4ms/step\n",
            "Epoch 2/500\n",
            "556/556 - 1s - loss: 31.4464 - mse: 31.4464 - mae: 1.5489 - val_loss: 10.7735 - val_mse: 10.7735 - val_mae: 2.6021 - lr: 0.0038 - 1s/epoch - 2ms/step\n",
            "Epoch 3/500\n",
            "556/556 - 1s - loss: 31.7260 - mse: 31.7260 - mae: 1.5353 - val_loss: 11.3956 - val_mse: 11.3956 - val_mae: 2.5875 - lr: 0.0038 - 1s/epoch - 2ms/step\n",
            "Epoch 4/500\n",
            "556/556 - 1s - loss: 31.6027 - mse: 31.6027 - mae: 1.5287 - val_loss: 9.3992 - val_mse: 9.3992 - val_mae: 2.4638 - lr: 0.0038 - 1s/epoch - 2ms/step\n",
            "Epoch 5/500\n",
            "556/556 - 1s - loss: 31.4261 - mse: 31.4261 - mae: 1.5207 - val_loss: 12.7202 - val_mse: 12.7202 - val_mae: 2.8232 - lr: 0.0038 - 1s/epoch - 2ms/step\n",
            "Epoch 6/500\n",
            "556/556 - 1s - loss: 31.4845 - mse: 31.4845 - mae: 1.5192 - val_loss: 10.4693 - val_mse: 10.4693 - val_mae: 2.5510 - lr: 0.0038 - 1s/epoch - 2ms/step\n",
            "Epoch 7/500\n",
            "556/556 - 1s - loss: 31.8826 - mse: 31.8826 - mae: 1.5138 - val_loss: 10.6017 - val_mse: 10.6017 - val_mae: 2.6165 - lr: 0.0038 - 1s/epoch - 2ms/step\n",
            "Epoch 8/500\n",
            "556/556 - 1s - loss: 31.3783 - mse: 31.3783 - mae: 1.5122 - val_loss: 11.7589 - val_mse: 11.7589 - val_mae: 2.8535 - lr: 0.0038 - 1s/epoch - 2ms/step\n",
            "Epoch 9/500\n",
            "556/556 - 1s - loss: 31.1392 - mse: 31.1392 - mae: 1.5022 - val_loss: 9.4464 - val_mse: 9.4464 - val_mae: 2.4627 - lr: 0.0038 - 1s/epoch - 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-10-10 09:32:11,619]\u001b[0m Finished trial#40 resulted in value: 13.11. Current best value is 7.723333333333334 with parameters: {'activation': 'tanh', 'num_hidden_layer': 4, 'i': 3, 'learning_rate': 0.00010292007837467995}.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Score for inner fold : loss of 9.446390151977539\n",
            "Trial Number:41\n",
            "-----Fold:0--------\n",
            "Epoch 1/500\n",
            "556/556 - 2s - loss: 43.0354 - mse: 43.0354 - mae: 2.4692 - val_loss: 1.3741 - val_mse: 1.3741 - val_mae: 0.8632 - lr: 1.3630e-04 - 2s/epoch - 3ms/step\n",
            "Epoch 2/500\n",
            "556/556 - 1s - loss: 39.4750 - mse: 39.4750 - mae: 1.8425 - val_loss: 3.7052 - val_mse: 3.7052 - val_mae: 1.6720 - lr: 1.3630e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 3/500\n",
            "556/556 - 1s - loss: 36.7473 - mse: 36.7473 - mae: 1.6625 - val_loss: 7.0022 - val_mse: 7.0022 - val_mae: 2.4204 - lr: 1.3630e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 4/500\n",
            "556/556 - 1s - loss: 36.0638 - mse: 36.0638 - mae: 1.7283 - val_loss: 7.6580 - val_mse: 7.6580 - val_mae: 2.5221 - lr: 1.3630e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 5/500\n",
            "556/556 - 1s - loss: 35.8414 - mse: 35.8414 - mae: 1.7211 - val_loss: 7.8700 - val_mse: 7.8700 - val_mae: 2.5444 - lr: 1.3630e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 6/500\n",
            "556/556 - 1s - loss: 35.6545 - mse: 35.6545 - mae: 1.7093 - val_loss: 7.9806 - val_mse: 7.9806 - val_mae: 2.5506 - lr: 1.3630e-04 - 1s/epoch - 2ms/step\n",
            "Score for inner fold : loss of 7.980552673339844\n",
            "-----Fold:1--------\n",
            "Epoch 1/500\n",
            "556/556 - 2s - loss: 43.4885 - mse: 43.4885 - mae: 2.3978 - val_loss: 1.7921 - val_mse: 1.7921 - val_mae: 1.0304 - lr: 1.3630e-04 - 2s/epoch - 3ms/step\n",
            "Epoch 2/500\n",
            "556/556 - 1s - loss: 39.0246 - mse: 39.0246 - mae: 1.7568 - val_loss: 5.5006 - val_mse: 5.5006 - val_mae: 2.0966 - lr: 1.3630e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 3/500\n",
            "556/556 - 1s - loss: 36.6124 - mse: 36.6124 - mae: 1.7009 - val_loss: 7.7784 - val_mse: 7.7784 - val_mae: 2.5319 - lr: 1.3630e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 4/500\n",
            "556/556 - 1s - loss: 36.1034 - mse: 36.1034 - mae: 1.7087 - val_loss: 8.1503 - val_mse: 8.1503 - val_mae: 2.5738 - lr: 1.3630e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 5/500\n",
            "556/556 - 1s - loss: 35.8105 - mse: 35.8105 - mae: 1.6878 - val_loss: 8.1264 - val_mse: 8.1264 - val_mae: 2.5455 - lr: 1.3630e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 6/500\n",
            "556/556 - 1s - loss: 35.5510 - mse: 35.5510 - mae: 1.6671 - val_loss: 8.4402 - val_mse: 8.4402 - val_mae: 2.5917 - lr: 1.3630e-04 - 1s/epoch - 2ms/step\n",
            "Score for inner fold : loss of 8.440181732177734\n",
            "-----Fold:2--------\n",
            "Epoch 1/500\n",
            "556/556 - 2s - loss: 20.2493 - mse: 20.2493 - mae: 2.3883 - val_loss: 1.7585 - val_mse: 1.7585 - val_mae: 1.0330 - lr: 1.3630e-04 - 2s/epoch - 3ms/step\n",
            "Epoch 2/500\n",
            "556/556 - 1s - loss: 16.2953 - mse: 16.2953 - mae: 1.7525 - val_loss: 4.8984 - val_mse: 4.8984 - val_mae: 1.9720 - lr: 1.3630e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 3/500\n",
            "556/556 - 1s - loss: 14.0565 - mse: 14.0565 - mae: 1.6664 - val_loss: 7.6331 - val_mse: 7.6331 - val_mae: 2.5264 - lr: 1.3630e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 4/500\n",
            "556/556 - 1s - loss: 13.5970 - mse: 13.5970 - mae: 1.7058 - val_loss: 7.8203 - val_mse: 7.8203 - val_mae: 2.5429 - lr: 1.3630e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 5/500\n",
            "556/556 - 1s - loss: 13.3730 - mse: 13.3730 - mae: 1.6893 - val_loss: 7.9720 - val_mse: 7.9720 - val_mae: 2.5609 - lr: 1.3630e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 6/500\n",
            "556/556 - 1s - loss: 13.1912 - mse: 13.1912 - mae: 1.6707 - val_loss: 7.8586 - val_mse: 7.8586 - val_mae: 2.5432 - lr: 1.3630e-04 - 1s/epoch - 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-10-10 09:32:36,095]\u001b[0m Finished trial#41 resulted in value: 8.093333333333334. Current best value is 7.723333333333334 with parameters: {'activation': 'tanh', 'num_hidden_layer': 4, 'i': 3, 'learning_rate': 0.00010292007837467995}.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Score for inner fold : loss of 7.858613014221191\n",
            "Trial Number:42\n",
            "-----Fold:0--------\n",
            "Epoch 1/500\n",
            "556/556 - 2s - loss: 42.3866 - mse: 42.3866 - mae: 2.3646 - val_loss: 2.0484 - val_mse: 2.0484 - val_mae: 1.1248 - lr: 1.6738e-04 - 2s/epoch - 3ms/step\n",
            "Epoch 2/500\n",
            "556/556 - 1s - loss: 37.7475 - mse: 37.7475 - mae: 1.7281 - val_loss: 6.3624 - val_mse: 6.3624 - val_mae: 2.2938 - lr: 1.6738e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 3/500\n",
            "556/556 - 1s - loss: 35.9378 - mse: 35.9378 - mae: 1.7290 - val_loss: 7.5865 - val_mse: 7.5865 - val_mae: 2.5149 - lr: 1.6738e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 4/500\n",
            "556/556 - 1s - loss: 35.5321 - mse: 35.5321 - mae: 1.7181 - val_loss: 7.9179 - val_mse: 7.9179 - val_mae: 2.5456 - lr: 1.6738e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 5/500\n",
            "556/556 - 1s - loss: 35.2707 - mse: 35.2707 - mae: 1.6992 - val_loss: 8.0558 - val_mse: 8.0558 - val_mae: 2.5519 - lr: 1.6738e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 6/500\n",
            "556/556 - 1s - loss: 35.0571 - mse: 35.0571 - mae: 1.6848 - val_loss: 7.8944 - val_mse: 7.8944 - val_mae: 2.5109 - lr: 1.6738e-04 - 1s/epoch - 2ms/step\n",
            "Score for inner fold : loss of 7.894371032714844\n",
            "-----Fold:1--------\n",
            "Epoch 1/500\n",
            "556/556 - 2s - loss: 42.3274 - mse: 42.3274 - mae: 2.3445 - val_loss: 2.1753 - val_mse: 2.1753 - val_mae: 1.1578 - lr: 1.6738e-04 - 2s/epoch - 3ms/step\n",
            "Epoch 2/500\n",
            "556/556 - 1s - loss: 37.4187 - mse: 37.4187 - mae: 1.6945 - val_loss: 6.7599 - val_mse: 6.7599 - val_mae: 2.3434 - lr: 1.6738e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 3/500\n",
            "556/556 - 1s - loss: 35.8215 - mse: 35.8215 - mae: 1.6982 - val_loss: 7.6358 - val_mse: 7.6358 - val_mae: 2.4814 - lr: 1.6738e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 4/500\n",
            "556/556 - 1s - loss: 35.4805 - mse: 35.4805 - mae: 1.6778 - val_loss: 7.9044 - val_mse: 7.9044 - val_mae: 2.5166 - lr: 1.6738e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 5/500\n",
            "556/556 - 1s - loss: 35.2553 - mse: 35.2553 - mae: 1.6673 - val_loss: 7.9926 - val_mse: 7.9926 - val_mae: 2.5132 - lr: 1.6738e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 6/500\n",
            "556/556 - 1s - loss: 35.0517 - mse: 35.0517 - mae: 1.6545 - val_loss: 8.1733 - val_mse: 8.1733 - val_mae: 2.5248 - lr: 1.6738e-04 - 1s/epoch - 2ms/step\n",
            "Score for inner fold : loss of 8.173274040222168\n",
            "-----Fold:2--------\n",
            "Epoch 1/500\n",
            "556/556 - 2s - loss: 20.2415 - mse: 20.2415 - mae: 2.2162 - val_loss: 2.7093 - val_mse: 2.7093 - val_mae: 1.3689 - lr: 1.6738e-04 - 2s/epoch - 3ms/step\n",
            "Epoch 2/500\n",
            "556/556 - 1s - loss: 15.3971 - mse: 15.3971 - mae: 1.6729 - val_loss: 7.1752 - val_mse: 7.1752 - val_mae: 2.4413 - lr: 1.6738e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 3/500\n",
            "556/556 - 1s - loss: 14.2736 - mse: 14.2736 - mae: 1.6912 - val_loss: 7.9041 - val_mse: 7.9041 - val_mae: 2.5451 - lr: 1.6738e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 4/500\n",
            "556/556 - 1s - loss: 13.9756 - mse: 13.9756 - mae: 1.6704 - val_loss: 7.9329 - val_mse: 7.9329 - val_mae: 2.5283 - lr: 1.6738e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 5/500\n",
            "556/556 - 1s - loss: 13.7312 - mse: 13.7312 - mae: 1.6502 - val_loss: 8.2357 - val_mse: 8.2357 - val_mae: 2.5661 - lr: 1.6738e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 6/500\n",
            "556/556 - 1s - loss: 13.5166 - mse: 13.5166 - mae: 1.6365 - val_loss: 8.2308 - val_mse: 8.2308 - val_mae: 2.5515 - lr: 1.6738e-04 - 1s/epoch - 2ms/step\n",
            "Score for inner fold : loss of 8.230757713317871\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-10-10 09:33:00,534]\u001b[0m Finished trial#42 resulted in value: 8.096666666666666. Current best value is 7.723333333333334 with parameters: {'activation': 'tanh', 'num_hidden_layer': 4, 'i': 3, 'learning_rate': 0.00010292007837467995}.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trial Number:43\n",
            "-----Fold:0--------\n",
            "Epoch 1/500\n",
            "556/556 - 2s - loss: 40.5939 - mse: 40.5939 - mae: 2.2503 - val_loss: 2.2066 - val_mse: 2.2066 - val_mae: 1.1936 - lr: 1.7912e-04 - 2s/epoch - 3ms/step\n",
            "Epoch 2/500\n",
            "556/556 - 1s - loss: 35.9575 - mse: 35.9575 - mae: 1.6735 - val_loss: 6.7127 - val_mse: 6.7127 - val_mae: 2.3505 - lr: 1.7912e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 3/500\n",
            "556/556 - 1s - loss: 34.3920 - mse: 34.3920 - mae: 1.7071 - val_loss: 7.8617 - val_mse: 7.8617 - val_mae: 2.5374 - lr: 1.7912e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 4/500\n",
            "556/556 - 1s - loss: 33.9798 - mse: 33.9798 - mae: 1.6832 - val_loss: 7.9179 - val_mse: 7.9179 - val_mae: 2.5220 - lr: 1.7912e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 5/500\n",
            "556/556 - 1s - loss: 33.6784 - mse: 33.6784 - mae: 1.6580 - val_loss: 8.0117 - val_mse: 8.0117 - val_mae: 2.5203 - lr: 1.7912e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 6/500\n",
            "556/556 - 1s - loss: 33.4498 - mse: 33.4498 - mae: 1.6422 - val_loss: 8.2986 - val_mse: 8.2986 - val_mae: 2.5481 - lr: 1.7912e-04 - 1s/epoch - 2ms/step\n",
            "Score for inner fold : loss of 8.298571586608887\n",
            "-----Fold:1--------\n",
            "Epoch 1/500\n",
            "556/556 - 2s - loss: 21.5611 - mse: 21.5611 - mae: 2.2978 - val_loss: 2.4547 - val_mse: 2.4547 - val_mae: 1.2649 - lr: 1.7912e-04 - 2s/epoch - 3ms/step\n",
            "Epoch 2/500\n",
            "556/556 - 1s - loss: 16.9529 - mse: 16.9529 - mae: 1.6923 - val_loss: 7.2328 - val_mse: 7.2328 - val_mae: 2.4499 - lr: 1.7912e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 3/500\n",
            "556/556 - 1s - loss: 15.4847 - mse: 15.4847 - mae: 1.7114 - val_loss: 7.8958 - val_mse: 7.8958 - val_mae: 2.5465 - lr: 1.7912e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 4/500\n",
            "556/556 - 1s - loss: 15.1413 - mse: 15.1413 - mae: 1.6922 - val_loss: 8.0022 - val_mse: 8.0022 - val_mae: 2.5496 - lr: 1.7912e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 5/500\n",
            "556/556 - 1s - loss: 14.9097 - mse: 14.9097 - mae: 1.6797 - val_loss: 8.0675 - val_mse: 8.0675 - val_mae: 2.5353 - lr: 1.7912e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 6/500\n",
            "556/556 - 1s - loss: 14.7005 - mse: 14.7005 - mae: 1.6618 - val_loss: 7.9102 - val_mse: 7.9102 - val_mae: 2.5004 - lr: 1.7912e-04 - 1s/epoch - 2ms/step\n",
            "Score for inner fold : loss of 7.910152435302734\n",
            "-----Fold:2--------\n",
            "Epoch 1/500\n",
            "556/556 - 2s - loss: 41.8061 - mse: 41.8061 - mae: 2.1889 - val_loss: 2.7301 - val_mse: 2.7301 - val_mae: 1.3761 - lr: 1.7912e-04 - 2s/epoch - 3ms/step\n",
            "Epoch 2/500\n",
            "556/556 - 1s - loss: 37.2898 - mse: 37.2898 - mae: 1.6749 - val_loss: 7.0964 - val_mse: 7.0964 - val_mae: 2.4316 - lr: 1.7912e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 3/500\n",
            "556/556 - 1s - loss: 35.9578 - mse: 35.9578 - mae: 1.7051 - val_loss: 7.8597 - val_mse: 7.8597 - val_mae: 2.5419 - lr: 1.7912e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 4/500\n",
            "556/556 - 1s - loss: 35.6374 - mse: 35.6374 - mae: 1.6923 - val_loss: 7.9808 - val_mse: 7.9808 - val_mae: 2.5401 - lr: 1.7912e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 5/500\n",
            "556/556 - 1s - loss: 35.4065 - mse: 35.4065 - mae: 1.6731 - val_loss: 7.9773 - val_mse: 7.9773 - val_mae: 2.5308 - lr: 1.7912e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 6/500\n",
            "556/556 - 1s - loss: 35.2049 - mse: 35.2049 - mae: 1.6567 - val_loss: 7.9919 - val_mse: 7.9919 - val_mae: 2.5104 - lr: 1.7912e-04 - 1s/epoch - 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-10-10 09:33:25,062]\u001b[0m Finished trial#43 resulted in value: 8.066666666666668. Current best value is 7.723333333333334 with parameters: {'activation': 'tanh', 'num_hidden_layer': 4, 'i': 3, 'learning_rate': 0.00010292007837467995}.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Score for inner fold : loss of 7.991932392120361\n",
            "Trial Number:44\n",
            "-----Fold:0--------\n",
            "Epoch 1/500\n",
            "556/556 - 2s - loss: 42.1188 - mse: 42.1188 - mae: 2.3431 - val_loss: 2.0334 - val_mse: 2.0334 - val_mae: 1.1197 - lr: 1.3024e-04 - 2s/epoch - 3ms/step\n",
            "Epoch 2/500\n",
            "556/556 - 1s - loss: 37.1753 - mse: 37.1753 - mae: 1.6958 - val_loss: 7.0655 - val_mse: 7.0655 - val_mae: 2.3911 - lr: 1.3024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 3/500\n",
            "556/556 - 1s - loss: 35.3084 - mse: 35.3084 - mae: 1.6878 - val_loss: 8.1701 - val_mse: 8.1701 - val_mae: 2.5674 - lr: 1.3024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 4/500\n",
            "556/556 - 1s - loss: 34.9070 - mse: 34.9070 - mae: 1.6795 - val_loss: 8.4113 - val_mse: 8.4113 - val_mae: 2.5849 - lr: 1.3024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 5/500\n",
            "556/556 - 1s - loss: 34.6366 - mse: 34.6366 - mae: 1.6665 - val_loss: 8.3165 - val_mse: 8.3165 - val_mae: 2.5442 - lr: 1.3024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 6/500\n",
            "556/556 - 1s - loss: 34.4217 - mse: 34.4217 - mae: 1.6490 - val_loss: 8.4985 - val_mse: 8.4985 - val_mae: 2.5708 - lr: 1.3024e-04 - 1s/epoch - 2ms/step\n",
            "Score for inner fold : loss of 8.498449325561523\n",
            "-----Fold:1--------\n",
            "Epoch 1/500\n",
            "556/556 - 2s - loss: 40.8396 - mse: 40.8396 - mae: 2.3379 - val_loss: 2.1012 - val_mse: 2.1012 - val_mae: 1.1419 - lr: 1.3024e-04 - 2s/epoch - 3ms/step\n",
            "Epoch 2/500\n",
            "556/556 - 1s - loss: 35.7408 - mse: 35.7408 - mae: 1.6595 - val_loss: 7.5332 - val_mse: 7.5332 - val_mae: 2.4728 - lr: 1.3024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 3/500\n",
            "556/556 - 1s - loss: 34.0779 - mse: 34.0779 - mae: 1.6738 - val_loss: 7.9021 - val_mse: 7.9021 - val_mae: 2.5092 - lr: 1.3024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 4/500\n",
            "556/556 - 1s - loss: 33.7039 - mse: 33.7039 - mae: 1.6534 - val_loss: 8.1624 - val_mse: 8.1624 - val_mae: 2.5444 - lr: 1.3024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 5/500\n",
            "556/556 - 1s - loss: 33.4495 - mse: 33.4495 - mae: 1.6390 - val_loss: 8.1338 - val_mse: 8.1338 - val_mae: 2.5139 - lr: 1.3024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 6/500\n",
            "556/556 - 1s - loss: 33.2579 - mse: 33.2579 - mae: 1.6264 - val_loss: 8.4324 - val_mse: 8.4324 - val_mae: 2.5536 - lr: 1.3024e-04 - 1s/epoch - 2ms/step\n",
            "Score for inner fold : loss of 8.432380676269531\n",
            "-----Fold:2--------\n",
            "Epoch 1/500\n",
            "556/556 - 2s - loss: 22.2466 - mse: 22.2466 - mae: 2.3156 - val_loss: 1.9967 - val_mse: 1.9967 - val_mae: 1.1355 - lr: 1.3024e-04 - 2s/epoch - 3ms/step\n",
            "Epoch 2/500\n",
            "556/556 - 1s - loss: 17.4095 - mse: 17.4095 - mae: 1.6971 - val_loss: 6.8412 - val_mse: 6.8412 - val_mae: 2.3473 - lr: 1.3024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 3/500\n",
            "556/556 - 1s - loss: 15.5421 - mse: 15.5421 - mae: 1.6817 - val_loss: 8.0946 - val_mse: 8.0946 - val_mae: 2.5528 - lr: 1.3024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 4/500\n",
            "556/556 - 1s - loss: 15.1648 - mse: 15.1648 - mae: 1.6719 - val_loss: 8.3182 - val_mse: 8.3182 - val_mae: 2.5852 - lr: 1.3024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 5/500\n",
            "556/556 - 1s - loss: 14.8926 - mse: 14.8926 - mae: 1.6534 - val_loss: 8.1534 - val_mse: 8.1534 - val_mae: 2.5422 - lr: 1.3024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 6/500\n",
            "556/556 - 1s - loss: 14.6660 - mse: 14.6660 - mae: 1.6351 - val_loss: 8.1761 - val_mse: 8.1761 - val_mae: 2.5200 - lr: 1.3024e-04 - 1s/epoch - 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-10-10 09:33:49,522]\u001b[0m Finished trial#44 resulted in value: 8.37. Current best value is 7.723333333333334 with parameters: {'activation': 'tanh', 'num_hidden_layer': 4, 'i': 3, 'learning_rate': 0.00010292007837467995}.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Score for inner fold : loss of 8.176109313964844\n",
            "Trial Number:45\n",
            "-----Fold:0--------\n",
            "Epoch 1/500\n",
            "556/556 - 2s - loss: 41.3408 - mse: 41.3408 - mae: 2.2391 - val_loss: 2.5480 - val_mse: 2.5480 - val_mae: 1.3185 - lr: 2.4608e-04 - 2s/epoch - 3ms/step\n",
            "Epoch 2/500\n",
            "556/556 - 1s - loss: 36.3341 - mse: 36.3341 - mae: 1.6566 - val_loss: 7.0140 - val_mse: 7.0140 - val_mae: 2.4150 - lr: 2.4608e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 3/500\n",
            "556/556 - 1s - loss: 35.3248 - mse: 35.3248 - mae: 1.6934 - val_loss: 7.8340 - val_mse: 7.8340 - val_mae: 2.5263 - lr: 2.4608e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 4/500\n",
            "556/556 - 1s - loss: 35.0410 - mse: 35.0410 - mae: 1.6900 - val_loss: 7.8231 - val_mse: 7.8231 - val_mae: 2.5201 - lr: 2.4608e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 5/500\n",
            "556/556 - 1s - loss: 34.7933 - mse: 34.7933 - mae: 1.6621 - val_loss: 8.1277 - val_mse: 8.1277 - val_mae: 2.5546 - lr: 2.4608e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 6/500\n",
            "556/556 - 1s - loss: 34.5582 - mse: 34.5582 - mae: 1.6492 - val_loss: 8.1534 - val_mse: 8.1534 - val_mae: 2.5425 - lr: 2.4608e-04 - 1s/epoch - 2ms/step\n",
            "Score for inner fold : loss of 8.153364181518555\n",
            "-----Fold:1--------\n",
            "Epoch 1/500\n",
            "556/556 - 2s - loss: 21.0670 - mse: 21.0670 - mae: 2.2788 - val_loss: 2.0345 - val_mse: 2.0345 - val_mae: 1.1509 - lr: 2.4608e-04 - 2s/epoch - 3ms/step\n",
            "Epoch 2/500\n",
            "556/556 - 1s - loss: 16.3017 - mse: 16.3017 - mae: 1.6537 - val_loss: 6.4391 - val_mse: 6.4391 - val_mae: 2.3319 - lr: 2.4608e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 3/500\n",
            "556/556 - 1s - loss: 14.7859 - mse: 14.7859 - mae: 1.6871 - val_loss: 7.5170 - val_mse: 7.5170 - val_mae: 2.4945 - lr: 2.4608e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 4/500\n",
            "556/556 - 1s - loss: 14.4075 - mse: 14.4075 - mae: 1.6847 - val_loss: 8.1057 - val_mse: 8.1057 - val_mae: 2.5743 - lr: 2.4608e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 5/500\n",
            "556/556 - 1s - loss: 14.0688 - mse: 14.0688 - mae: 1.6570 - val_loss: 7.6758 - val_mse: 7.6758 - val_mae: 2.4749 - lr: 2.4608e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 6/500\n",
            "556/556 - 1s - loss: 13.7766 - mse: 13.7766 - mae: 1.6317 - val_loss: 7.7953 - val_mse: 7.7953 - val_mae: 2.4779 - lr: 2.4608e-04 - 1s/epoch - 2ms/step\n",
            "Score for inner fold : loss of 7.795292854309082\n",
            "-----Fold:2--------\n",
            "Epoch 1/500\n",
            "556/556 - 2s - loss: 42.8018 - mse: 42.8018 - mae: 2.3760 - val_loss: 2.0931 - val_mse: 2.0931 - val_mae: 1.1648 - lr: 2.4608e-04 - 2s/epoch - 3ms/step\n",
            "Epoch 2/500\n",
            "556/556 - 1s - loss: 37.5358 - mse: 37.5358 - mae: 1.7034 - val_loss: 7.2550 - val_mse: 7.2550 - val_mae: 2.4764 - lr: 2.4608e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 3/500\n",
            "556/556 - 1s - loss: 36.2122 - mse: 36.2122 - mae: 1.7123 - val_loss: 7.6362 - val_mse: 7.6362 - val_mae: 2.5097 - lr: 2.4608e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 4/500\n",
            "556/556 - 1s - loss: 35.8475 - mse: 35.8475 - mae: 1.6815 - val_loss: 7.9592 - val_mse: 7.9592 - val_mae: 2.5403 - lr: 2.4608e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 5/500\n",
            "556/556 - 1s - loss: 35.5019 - mse: 35.5019 - mae: 1.6559 - val_loss: 7.7491 - val_mse: 7.7491 - val_mae: 2.4929 - lr: 2.4608e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 6/500\n",
            "556/556 - 1s - loss: 35.2206 - mse: 35.2206 - mae: 1.6392 - val_loss: 7.9338 - val_mse: 7.9338 - val_mae: 2.4909 - lr: 2.4608e-04 - 1s/epoch - 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-10-10 09:34:14,047]\u001b[0m Finished trial#45 resulted in value: 7.96. Current best value is 7.723333333333334 with parameters: {'activation': 'tanh', 'num_hidden_layer': 4, 'i': 3, 'learning_rate': 0.00010292007837467995}.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Score for inner fold : loss of 7.933830261230469\n",
            "Trial Number:46\n",
            "-----Fold:0--------\n",
            "Epoch 1/500\n",
            "556/556 - 2s - loss: 36.8225 - mse: 36.8225 - mae: 1.7723 - val_loss: 8.1282 - val_mse: 8.1282 - val_mae: 2.5062 - lr: 0.0018 - 2s/epoch - 3ms/step\n",
            "Epoch 2/500\n",
            "556/556 - 1s - loss: 34.6860 - mse: 34.6860 - mae: 1.6319 - val_loss: 8.7797 - val_mse: 8.7797 - val_mae: 2.5364 - lr: 0.0018 - 1s/epoch - 2ms/step\n",
            "Epoch 3/500\n",
            "556/556 - 1s - loss: 34.0521 - mse: 34.0521 - mae: 1.6091 - val_loss: 9.4364 - val_mse: 9.4364 - val_mae: 2.5904 - lr: 0.0018 - 1s/epoch - 2ms/step\n",
            "Epoch 4/500\n",
            "556/556 - 1s - loss: 33.6408 - mse: 33.6408 - mae: 1.5971 - val_loss: 9.4018 - val_mse: 9.4018 - val_mae: 2.6040 - lr: 0.0018 - 1s/epoch - 2ms/step\n",
            "Epoch 5/500\n",
            "556/556 - 1s - loss: 33.1817 - mse: 33.1817 - mae: 1.5778 - val_loss: 9.4137 - val_mse: 9.4137 - val_mae: 2.5714 - lr: 0.0018 - 1s/epoch - 2ms/step\n",
            "Epoch 6/500\n",
            "556/556 - 1s - loss: 32.9501 - mse: 32.9501 - mae: 1.5686 - val_loss: 8.9980 - val_mse: 8.9980 - val_mae: 2.4373 - lr: 0.0018 - 1s/epoch - 2ms/step\n",
            "Score for inner fold : loss of 8.997977256774902\n",
            "-----Fold:1--------\n",
            "Epoch 1/500\n",
            "556/556 - 2s - loss: 13.5492 - mse: 13.5492 - mae: 1.7192 - val_loss: 8.1380 - val_mse: 8.1380 - val_mae: 2.5095 - lr: 0.0018 - 2s/epoch - 3ms/step\n",
            "Epoch 2/500\n",
            "556/556 - 1s - loss: 11.7300 - mse: 11.7300 - mae: 1.5891 - val_loss: 9.1739 - val_mse: 9.1739 - val_mae: 2.6587 - lr: 0.0018 - 1s/epoch - 2ms/step\n",
            "Epoch 3/500\n",
            "556/556 - 1s - loss: 11.2271 - mse: 11.2271 - mae: 1.5739 - val_loss: 8.9883 - val_mse: 8.9883 - val_mae: 2.4978 - lr: 0.0018 - 1s/epoch - 2ms/step\n",
            "Epoch 4/500\n",
            "556/556 - 1s - loss: 10.9138 - mse: 10.9138 - mae: 1.5581 - val_loss: 9.5961 - val_mse: 9.5961 - val_mae: 2.6057 - lr: 0.0018 - 1s/epoch - 2ms/step\n",
            "Epoch 5/500\n",
            "556/556 - 1s - loss: 10.6886 - mse: 10.6886 - mae: 1.5442 - val_loss: 7.9270 - val_mse: 7.9270 - val_mae: 2.3484 - lr: 0.0018 - 1s/epoch - 2ms/step\n",
            "Epoch 6/500\n",
            "556/556 - 1s - loss: 10.4732 - mse: 10.4732 - mae: 1.5362 - val_loss: 8.7365 - val_mse: 8.7365 - val_mae: 2.4110 - lr: 0.0018 - 1s/epoch - 2ms/step\n",
            "Epoch 7/500\n",
            "556/556 - 1s - loss: 10.2896 - mse: 10.2896 - mae: 1.5291 - val_loss: 10.1014 - val_mse: 10.1014 - val_mae: 2.6365 - lr: 0.0018 - 1s/epoch - 2ms/step\n",
            "Epoch 8/500\n",
            "556/556 - 1s - loss: 10.1434 - mse: 10.1434 - mae: 1.5223 - val_loss: 10.7062 - val_mse: 10.7062 - val_mae: 2.6329 - lr: 0.0018 - 1s/epoch - 2ms/step\n",
            "Epoch 9/500\n",
            "556/556 - 1s - loss: 10.0072 - mse: 10.0072 - mae: 1.5208 - val_loss: 10.9155 - val_mse: 10.9155 - val_mae: 2.7427 - lr: 0.0018 - 1s/epoch - 2ms/step\n",
            "Epoch 10/500\n",
            "556/556 - 1s - loss: 9.8886 - mse: 9.8886 - mae: 1.5144 - val_loss: 9.2683 - val_mse: 9.2683 - val_mae: 2.4580 - lr: 0.0018 - 1s/epoch - 2ms/step\n",
            "Score for inner fold : loss of 9.268269538879395\n",
            "-----Fold:2--------\n",
            "Epoch 1/500\n",
            "556/556 - 2s - loss: 36.7808 - mse: 36.7808 - mae: 1.7603 - val_loss: 8.2950 - val_mse: 8.2950 - val_mae: 2.5403 - lr: 0.0018 - 2s/epoch - 3ms/step\n",
            "Epoch 2/500\n",
            "556/556 - 1s - loss: 34.6933 - mse: 34.6933 - mae: 1.6128 - val_loss: 9.2343 - val_mse: 9.2343 - val_mae: 2.6180 - lr: 0.0018 - 1s/epoch - 2ms/step\n",
            "Epoch 3/500\n",
            "556/556 - 1s - loss: 34.1469 - mse: 34.1469 - mae: 1.5933 - val_loss: 10.0181 - val_mse: 10.0181 - val_mae: 2.6902 - lr: 0.0018 - 1s/epoch - 2ms/step\n",
            "Epoch 4/500\n",
            "556/556 - 1s - loss: 33.7928 - mse: 33.7928 - mae: 1.5802 - val_loss: 9.0172 - val_mse: 9.0172 - val_mae: 2.5250 - lr: 0.0018 - 1s/epoch - 2ms/step\n",
            "Epoch 5/500\n",
            "556/556 - 1s - loss: 33.4840 - mse: 33.4840 - mae: 1.5672 - val_loss: 11.2314 - val_mse: 11.2314 - val_mae: 2.8658 - lr: 0.0018 - 1s/epoch - 2ms/step\n",
            "Epoch 6/500\n",
            "556/556 - 1s - loss: 33.3054 - mse: 33.3054 - mae: 1.5592 - val_loss: 9.1094 - val_mse: 9.1094 - val_mae: 2.5460 - lr: 0.0018 - 1s/epoch - 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-10-10 09:34:42,727]\u001b[0m Finished trial#46 resulted in value: 9.126666666666667. Current best value is 7.723333333333334 with parameters: {'activation': 'tanh', 'num_hidden_layer': 4, 'i': 3, 'learning_rate': 0.00010292007837467995}.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Score for inner fold : loss of 9.109392166137695\n",
            "Trial Number:47\n",
            "-----Fold:0--------\n",
            "Epoch 1/500\n",
            "556/556 - 2s - loss: 43.2009 - mse: 43.2009 - mae: 2.4602 - val_loss: 1.8128 - val_mse: 1.8128 - val_mae: 0.9991 - lr: 2.5240e-04 - 2s/epoch - 3ms/step\n",
            "Epoch 2/500\n",
            "556/556 - 1s - loss: 40.1819 - mse: 40.1819 - mae: 2.0453 - val_loss: 3.0035 - val_mse: 3.0035 - val_mae: 1.3988 - lr: 2.5240e-04 - 980ms/epoch - 2ms/step\n",
            "Epoch 3/500\n",
            "556/556 - 1s - loss: 37.5367 - mse: 37.5367 - mae: 1.7095 - val_loss: 5.4637 - val_mse: 5.4637 - val_mae: 2.0134 - lr: 2.5240e-04 - 994ms/epoch - 2ms/step\n",
            "Epoch 4/500\n",
            "556/556 - 1s - loss: 35.8303 - mse: 35.8303 - mae: 1.6337 - val_loss: 7.6790 - val_mse: 7.6790 - val_mae: 2.4557 - lr: 2.5240e-04 - 978ms/epoch - 2ms/step\n",
            "Epoch 5/500\n",
            "556/556 - 1s - loss: 35.2449 - mse: 35.2449 - mae: 1.6432 - val_loss: 8.3768 - val_mse: 8.3768 - val_mae: 2.5704 - lr: 2.5240e-04 - 963ms/epoch - 2ms/step\n",
            "Epoch 6/500\n",
            "556/556 - 1s - loss: 35.0000 - mse: 35.0000 - mae: 1.6360 - val_loss: 8.4251 - val_mse: 8.4251 - val_mae: 2.5674 - lr: 2.5240e-04 - 1s/epoch - 2ms/step\n",
            "Score for inner fold : loss of 8.425067901611328\n",
            "-----Fold:1--------\n",
            "Epoch 1/500\n",
            "556/556 - 2s - loss: 21.8220 - mse: 21.8220 - mae: 2.4862 - val_loss: 1.2941 - val_mse: 1.2941 - val_mae: 0.8542 - lr: 2.5240e-04 - 2s/epoch - 3ms/step\n",
            "Epoch 2/500\n",
            "556/556 - 1s - loss: 18.6178 - mse: 18.6178 - mae: 2.0397 - val_loss: 2.2457 - val_mse: 2.2457 - val_mae: 1.2188 - lr: 2.5240e-04 - 971ms/epoch - 2ms/step\n",
            "Epoch 3/500\n",
            "556/556 - 1s - loss: 15.8526 - mse: 15.8526 - mae: 1.6329 - val_loss: 4.7117 - val_mse: 4.7117 - val_mae: 1.8954 - lr: 2.5240e-04 - 945ms/epoch - 2ms/step\n",
            "Epoch 4/500\n",
            "556/556 - 1s - loss: 14.1579 - mse: 14.1579 - mae: 1.5692 - val_loss: 7.2693 - val_mse: 7.2693 - val_mae: 2.4190 - lr: 2.5240e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 5/500\n",
            "556/556 - 1s - loss: 13.6106 - mse: 13.6106 - mae: 1.6134 - val_loss: 8.0128 - val_mse: 8.0128 - val_mae: 2.5289 - lr: 2.5240e-04 - 972ms/epoch - 2ms/step\n",
            "Epoch 6/500\n",
            "556/556 - 1s - loss: 13.3638 - mse: 13.3638 - mae: 1.6114 - val_loss: 8.2404 - val_mse: 8.2404 - val_mae: 2.5562 - lr: 2.5240e-04 - 934ms/epoch - 2ms/step\n",
            "Score for inner fold : loss of 8.24038314819336\n",
            "-----Fold:2--------\n",
            "Epoch 1/500\n",
            "556/556 - 2s - loss: 42.5578 - mse: 42.5578 - mae: 2.3891 - val_loss: 1.7641 - val_mse: 1.7641 - val_mae: 0.9954 - lr: 2.5240e-04 - 2s/epoch - 3ms/step\n",
            "Epoch 2/500\n",
            "556/556 - 1s - loss: 38.9760 - mse: 38.9760 - mae: 1.8987 - val_loss: 3.0677 - val_mse: 3.0677 - val_mae: 1.4468 - lr: 2.5240e-04 - 949ms/epoch - 2ms/step\n",
            "Epoch 3/500\n",
            "556/556 - 1s - loss: 36.3823 - mse: 36.3823 - mae: 1.6286 - val_loss: 5.6622 - val_mse: 5.6622 - val_mae: 2.0922 - lr: 2.5240e-04 - 978ms/epoch - 2ms/step\n",
            "Epoch 4/500\n",
            "556/556 - 1s - loss: 34.8266 - mse: 34.8266 - mae: 1.6259 - val_loss: 7.4930 - val_mse: 7.4930 - val_mae: 2.4566 - lr: 2.5240e-04 - 950ms/epoch - 2ms/step\n",
            "Epoch 5/500\n",
            "556/556 - 1s - loss: 34.2809 - mse: 34.2809 - mae: 1.6457 - val_loss: 7.9809 - val_mse: 7.9809 - val_mae: 2.5261 - lr: 2.5240e-04 - 936ms/epoch - 2ms/step\n",
            "Epoch 6/500\n",
            "556/556 - 1s - loss: 34.0036 - mse: 34.0036 - mae: 1.6392 - val_loss: 8.1117 - val_mse: 8.1117 - val_mae: 2.5445 - lr: 2.5240e-04 - 956ms/epoch - 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-10-10 09:35:05,352]\u001b[0m Finished trial#47 resulted in value: 8.26. Current best value is 7.723333333333334 with parameters: {'activation': 'tanh', 'num_hidden_layer': 4, 'i': 3, 'learning_rate': 0.00010292007837467995}.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Score for inner fold : loss of 8.111695289611816\n",
            "Trial Number:48\n",
            "-----Fold:0--------\n",
            "Epoch 1/500\n",
            "556/556 - 2s - loss: 18.0441 - mse: 18.0441 - mae: 1.9472 - val_loss: 7.8220 - val_mse: 7.8220 - val_mae: 2.4999 - lr: 4.2911e-04 - 2s/epoch - 3ms/step\n",
            "Epoch 2/500\n",
            "556/556 - 1s - loss: 13.7338 - mse: 13.7338 - mae: 1.6338 - val_loss: 8.3392 - val_mse: 8.3392 - val_mae: 2.5453 - lr: 4.2911e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 3/500\n",
            "556/556 - 1s - loss: 13.0394 - mse: 13.0394 - mae: 1.5940 - val_loss: 8.9762 - val_mse: 8.9762 - val_mae: 2.6076 - lr: 4.2911e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 4/500\n",
            "556/556 - 1s - loss: 12.7229 - mse: 12.7229 - mae: 1.5847 - val_loss: 9.1706 - val_mse: 9.1706 - val_mae: 2.5988 - lr: 4.2911e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 5/500\n",
            "556/556 - 1s - loss: 12.4972 - mse: 12.4972 - mae: 1.5761 - val_loss: 8.9037 - val_mse: 8.9037 - val_mae: 2.5313 - lr: 4.2911e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 6/500\n",
            "556/556 - 1s - loss: 12.3357 - mse: 12.3357 - mae: 1.5673 - val_loss: 9.4753 - val_mse: 9.4753 - val_mae: 2.5892 - lr: 4.2911e-04 - 971ms/epoch - 2ms/step\n",
            "Score for inner fold : loss of 9.475296020507812\n",
            "-----Fold:1--------\n",
            "Epoch 1/500\n",
            "556/556 - 2s - loss: 38.0599 - mse: 38.0599 - mae: 1.9137 - val_loss: 8.1502 - val_mse: 8.1502 - val_mae: 2.5460 - lr: 4.2911e-04 - 2s/epoch - 3ms/step\n",
            "Epoch 2/500\n",
            "556/556 - 1s - loss: 34.3842 - mse: 34.3842 - mae: 1.6616 - val_loss: 9.0252 - val_mse: 9.0252 - val_mae: 2.6427 - lr: 4.2911e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 3/500\n",
            "556/556 - 1s - loss: 33.8466 - mse: 33.8466 - mae: 1.6304 - val_loss: 9.2343 - val_mse: 9.2343 - val_mae: 2.6275 - lr: 4.2911e-04 - 973ms/epoch - 2ms/step\n",
            "Epoch 4/500\n",
            "556/556 - 1s - loss: 33.5665 - mse: 33.5665 - mae: 1.6196 - val_loss: 8.9322 - val_mse: 8.9322 - val_mae: 2.5598 - lr: 4.2911e-04 - 993ms/epoch - 2ms/step\n",
            "Epoch 5/500\n",
            "556/556 - 1s - loss: 33.3771 - mse: 33.3771 - mae: 1.6088 - val_loss: 9.3078 - val_mse: 9.3078 - val_mae: 2.6191 - lr: 4.2911e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 6/500\n",
            "556/556 - 1s - loss: 33.2014 - mse: 33.2014 - mae: 1.6026 - val_loss: 8.9893 - val_mse: 8.9893 - val_mae: 2.5563 - lr: 4.2911e-04 - 959ms/epoch - 2ms/step\n",
            "Score for inner fold : loss of 8.989347457885742\n",
            "-----Fold:2--------\n",
            "Epoch 1/500\n",
            "556/556 - 2s - loss: 38.5820 - mse: 38.5820 - mae: 1.9553 - val_loss: 7.4823 - val_mse: 7.4823 - val_mae: 2.4583 - lr: 4.2911e-04 - 2s/epoch - 3ms/step\n",
            "Epoch 2/500\n",
            "556/556 - 1s - loss: 34.6147 - mse: 34.6147 - mae: 1.6334 - val_loss: 8.1136 - val_mse: 8.1136 - val_mae: 2.5245 - lr: 4.2911e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 3/500\n",
            "556/556 - 1s - loss: 33.9667 - mse: 33.9667 - mae: 1.5968 - val_loss: 8.5399 - val_mse: 8.5399 - val_mae: 2.5472 - lr: 4.2911e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 4/500\n",
            "556/556 - 1s - loss: 33.5808 - mse: 33.5808 - mae: 1.5807 - val_loss: 8.6131 - val_mse: 8.6131 - val_mae: 2.5315 - lr: 4.2911e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 5/500\n",
            "556/556 - 1s - loss: 33.3326 - mse: 33.3326 - mae: 1.5674 - val_loss: 9.0664 - val_mse: 9.0664 - val_mae: 2.5687 - lr: 4.2911e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 6/500\n",
            "556/556 - 1s - loss: 33.1473 - mse: 33.1473 - mae: 1.5617 - val_loss: 9.0925 - val_mse: 9.0925 - val_mae: 2.5741 - lr: 4.2911e-04 - 1s/epoch - 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-10-10 09:35:28,839]\u001b[0m Finished trial#48 resulted in value: 9.186666666666666. Current best value is 7.723333333333334 with parameters: {'activation': 'tanh', 'num_hidden_layer': 4, 'i': 3, 'learning_rate': 0.00010292007837467995}.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Score for inner fold : loss of 9.092474937438965\n",
            "Trial Number:49\n",
            "-----Fold:0--------\n",
            "Epoch 1/500\n",
            "556/556 - 3s - loss: 33.8254 - mse: 33.8254 - mae: 1.7118 - val_loss: 10.5160 - val_mse: 10.5160 - val_mae: 2.4371 - lr: 0.0013 - 3s/epoch - 5ms/step\n",
            "Epoch 2/500\n",
            "556/556 - 2s - loss: 33.2525 - mse: 33.2525 - mae: 1.6417 - val_loss: 9.6208 - val_mse: 9.6208 - val_mae: 2.4533 - lr: 0.0013 - 2s/epoch - 4ms/step\n",
            "Epoch 3/500\n",
            "556/556 - 2s - loss: 33.4864 - mse: 33.4864 - mae: 1.6447 - val_loss: 16.0372 - val_mse: 16.0372 - val_mae: 2.8097 - lr: 0.0013 - 2s/epoch - 4ms/step\n",
            "Epoch 4/500\n",
            "556/556 - 2s - loss: 33.1480 - mse: 33.1480 - mae: 1.6346 - val_loss: 13.7562 - val_mse: 13.7562 - val_mae: 2.5408 - lr: 0.0013 - 2s/epoch - 4ms/step\n",
            "Epoch 5/500\n",
            "556/556 - 2s - loss: 34.0208 - mse: 34.0208 - mae: 1.6425 - val_loss: 13.5641 - val_mse: 13.5641 - val_mae: 2.5839 - lr: 0.0013 - 2s/epoch - 4ms/step\n",
            "Epoch 6/500\n",
            "556/556 - 2s - loss: 33.2529 - mse: 33.2529 - mae: 1.6259 - val_loss: 14.6671 - val_mse: 14.6671 - val_mae: 2.7236 - lr: 0.0013 - 2s/epoch - 4ms/step\n",
            "Epoch 7/500\n",
            "556/556 - 2s - loss: 33.5156 - mse: 33.5156 - mae: 1.6268 - val_loss: 16.0442 - val_mse: 16.0442 - val_mae: 2.7322 - lr: 0.0013 - 2s/epoch - 4ms/step\n",
            "Score for inner fold : loss of 16.044252395629883\n",
            "-----Fold:1--------\n",
            "Epoch 1/500\n",
            "556/556 - 3s - loss: 32.3889 - mse: 32.3889 - mae: 1.6940 - val_loss: 11.7842 - val_mse: 11.7842 - val_mae: 2.5461 - lr: 0.0013 - 3s/epoch - 5ms/step\n",
            "Epoch 2/500\n",
            "556/556 - 2s - loss: 32.0593 - mse: 32.0593 - mae: 1.6182 - val_loss: 10.4337 - val_mse: 10.4337 - val_mae: 2.4192 - lr: 0.0013 - 2s/epoch - 4ms/step\n",
            "Epoch 3/500\n",
            "556/556 - 2s - loss: 31.6690 - mse: 31.6690 - mae: 1.5993 - val_loss: 19.1519 - val_mse: 19.1519 - val_mae: 3.1126 - lr: 0.0013 - 2s/epoch - 4ms/step\n",
            "Epoch 4/500\n",
            "556/556 - 2s - loss: 31.7912 - mse: 31.7912 - mae: 1.6040 - val_loss: 16.8320 - val_mse: 16.8320 - val_mae: 2.8118 - lr: 0.0013 - 2s/epoch - 4ms/step\n",
            "Epoch 5/500\n",
            "556/556 - 2s - loss: 31.7858 - mse: 31.7858 - mae: 1.6000 - val_loss: 13.8082 - val_mse: 13.8082 - val_mae: 2.6384 - lr: 0.0013 - 2s/epoch - 4ms/step\n",
            "Epoch 6/500\n",
            "556/556 - 2s - loss: 31.4972 - mse: 31.4972 - mae: 1.5939 - val_loss: 13.0816 - val_mse: 13.0816 - val_mae: 2.4935 - lr: 0.0013 - 2s/epoch - 4ms/step\n",
            "Epoch 7/500\n",
            "556/556 - 2s - loss: 31.8375 - mse: 31.8375 - mae: 1.5906 - val_loss: 16.2203 - val_mse: 16.2203 - val_mae: 2.6766 - lr: 0.0013 - 2s/epoch - 4ms/step\n",
            "Score for inner fold : loss of 16.220308303833008\n",
            "-----Fold:2--------\n",
            "Epoch 1/500\n",
            "556/556 - 3s - loss: 12.5718 - mse: 12.5718 - mae: 1.6827 - val_loss: 11.9863 - val_mse: 11.9863 - val_mae: 2.8270 - lr: 0.0013 - 3s/epoch - 6ms/step\n",
            "Epoch 2/500\n",
            "556/556 - 2s - loss: 12.0515 - mse: 12.0515 - mae: 1.6195 - val_loss: 13.2996 - val_mse: 13.2996 - val_mae: 2.7581 - lr: 0.0013 - 2s/epoch - 4ms/step\n",
            "Epoch 3/500\n",
            "556/556 - 2s - loss: 11.6059 - mse: 11.6059 - mae: 1.6082 - val_loss: 9.4691 - val_mse: 9.4691 - val_mae: 2.4812 - lr: 0.0013 - 2s/epoch - 4ms/step\n",
            "Epoch 4/500\n",
            "556/556 - 2s - loss: 11.8351 - mse: 11.8351 - mae: 1.5933 - val_loss: 8.1588 - val_mse: 8.1588 - val_mae: 2.2270 - lr: 0.0013 - 2s/epoch - 4ms/step\n",
            "Epoch 5/500\n",
            "556/556 - 2s - loss: 11.8745 - mse: 11.8745 - mae: 1.5914 - val_loss: 8.5792 - val_mse: 8.5792 - val_mae: 2.3834 - lr: 0.0013 - 2s/epoch - 4ms/step\n",
            "Epoch 6/500\n",
            "556/556 - 2s - loss: 11.7993 - mse: 11.7993 - mae: 1.5989 - val_loss: 11.6068 - val_mse: 11.6068 - val_mae: 2.5852 - lr: 0.0013 - 2s/epoch - 4ms/step\n",
            "Epoch 7/500\n",
            "556/556 - 2s - loss: 12.2326 - mse: 12.2326 - mae: 1.5993 - val_loss: 7.8803 - val_mse: 7.8803 - val_mae: 2.3839 - lr: 0.0013 - 2s/epoch - 4ms/step\n",
            "Epoch 8/500\n",
            "556/556 - 2s - loss: 11.7144 - mse: 11.7144 - mae: 1.5909 - val_loss: 12.2311 - val_mse: 12.2311 - val_mae: 2.7119 - lr: 0.0013 - 2s/epoch - 4ms/step\n",
            "Epoch 9/500\n",
            "556/556 - 2s - loss: 11.7681 - mse: 11.7681 - mae: 1.5933 - val_loss: 13.1254 - val_mse: 13.1254 - val_mae: 2.8028 - lr: 0.0013 - 2s/epoch - 4ms/step\n",
            "Epoch 10/500\n",
            "556/556 - 2s - loss: 11.7144 - mse: 11.7144 - mae: 1.5900 - val_loss: 8.3723 - val_mse: 8.3723 - val_mae: 2.4458 - lr: 0.0013 - 2s/epoch - 4ms/step\n",
            "Epoch 11/500\n",
            "556/556 - 2s - loss: 11.7736 - mse: 11.7736 - mae: 1.5925 - val_loss: 11.3947 - val_mse: 11.3947 - val_mae: 2.5988 - lr: 0.0013 - 2s/epoch - 4ms/step\n",
            "Epoch 12/500\n",
            "556/556 - 2s - loss: 11.6493 - mse: 11.6493 - mae: 1.5920 - val_loss: 17.9179 - val_mse: 17.9179 - val_mae: 3.1120 - lr: 0.0013 - 2s/epoch - 4ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-10-10 09:36:32,563]\u001b[0m Finished trial#49 resulted in value: 16.726666666666667. Current best value is 7.723333333333334 with parameters: {'activation': 'tanh', 'num_hidden_layer': 4, 'i': 3, 'learning_rate': 0.00010292007837467995}.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Score for inner fold : loss of 17.917930603027344\n",
            "Number of finished trial : 50\n",
            "Best Trial Value : 7.723333333333334\n",
            "------ Outer Fold - 1 -------- \n",
            "Trial Number:0\n",
            "-----Fold:0--------\n",
            "Epoch 1/500\n",
            "556/556 - 2s - loss: 37.0498 - mse: 37.0498 - mae: 2.0496 - val_loss: 7.6385 - val_mse: 7.6385 - val_mae: 2.1432 - lr: 3.7465e-04 - 2s/epoch - 3ms/step\n",
            "Epoch 2/500\n",
            "556/556 - 1s - loss: 32.3869 - mse: 32.3869 - mae: 1.6019 - val_loss: 11.1978 - val_mse: 11.1978 - val_mae: 2.6562 - lr: 3.7465e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 3/500\n",
            "556/556 - 1s - loss: 32.2964 - mse: 32.2964 - mae: 1.6194 - val_loss: 12.0810 - val_mse: 12.0810 - val_mae: 2.6912 - lr: 3.7465e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 4/500\n",
            "556/556 - 1s - loss: 32.3239 - mse: 32.3239 - mae: 1.6135 - val_loss: 12.3656 - val_mse: 12.3656 - val_mae: 2.7161 - lr: 3.7465e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 5/500\n",
            "556/556 - 1s - loss: 32.3223 - mse: 32.3223 - mae: 1.6194 - val_loss: 11.9697 - val_mse: 11.9697 - val_mae: 2.6514 - lr: 3.7465e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 6/500\n",
            "556/556 - 1s - loss: 32.3295 - mse: 32.3295 - mae: 1.6104 - val_loss: 12.1258 - val_mse: 12.1258 - val_mae: 2.6763 - lr: 3.7465e-04 - 1s/epoch - 2ms/step\n",
            "Score for inner fold : loss of 12.125794410705566\n",
            "-----Fold:1--------\n",
            "Epoch 1/500\n",
            "556/556 - 2s - loss: 38.4958 - mse: 38.4958 - mae: 2.0336 - val_loss: 8.1523 - val_mse: 8.1523 - val_mae: 2.1657 - lr: 3.7465e-04 - 2s/epoch - 3ms/step\n",
            "Epoch 2/500\n",
            "556/556 - 1s - loss: 33.1960 - mse: 33.1960 - mae: 1.6120 - val_loss: 12.7495 - val_mse: 12.7495 - val_mae: 2.6854 - lr: 3.7465e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 3/500\n",
            "556/556 - 1s - loss: 32.9262 - mse: 32.9262 - mae: 1.6243 - val_loss: 13.0847 - val_mse: 13.0847 - val_mae: 2.6506 - lr: 3.7465e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 4/500\n",
            "556/556 - 1s - loss: 32.8159 - mse: 32.8159 - mae: 1.6204 - val_loss: 13.6435 - val_mse: 13.6435 - val_mae: 2.6763 - lr: 3.7465e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 5/500\n",
            "556/556 - 1s - loss: 32.7765 - mse: 32.7765 - mae: 1.6168 - val_loss: 14.4819 - val_mse: 14.4819 - val_mae: 2.6901 - lr: 3.7465e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 6/500\n",
            "556/556 - 1s - loss: 32.7953 - mse: 32.7953 - mae: 1.6157 - val_loss: 13.9913 - val_mse: 13.9913 - val_mae: 2.6457 - lr: 3.7465e-04 - 1s/epoch - 2ms/step\n",
            "Score for inner fold : loss of 13.991266250610352\n",
            "-----Fold:2--------\n",
            "Epoch 1/500\n",
            "556/556 - 2s - loss: 17.1937 - mse: 17.1937 - mae: 1.9781 - val_loss: 7.4778 - val_mse: 7.4778 - val_mae: 2.1883 - lr: 3.7465e-04 - 2s/epoch - 3ms/step\n",
            "Epoch 2/500\n",
            "556/556 - 1s - loss: 11.0952 - mse: 11.0952 - mae: 1.5607 - val_loss: 11.7352 - val_mse: 11.7352 - val_mae: 2.6968 - lr: 3.7465e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 3/500\n",
            "556/556 - 1s - loss: 10.6804 - mse: 10.6804 - mae: 1.5674 - val_loss: 11.4871 - val_mse: 11.4871 - val_mae: 2.6529 - lr: 3.7465e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 4/500\n",
            "556/556 - 1s - loss: 10.7623 - mse: 10.7623 - mae: 1.5671 - val_loss: 12.0713 - val_mse: 12.0713 - val_mae: 2.6424 - lr: 3.7465e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 5/500\n",
            "556/556 - 1s - loss: 10.7668 - mse: 10.7668 - mae: 1.5565 - val_loss: 12.2691 - val_mse: 12.2691 - val_mae: 2.6511 - lr: 3.7465e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 6/500\n",
            "556/556 - 1s - loss: 10.7916 - mse: 10.7916 - mae: 1.5609 - val_loss: 13.0767 - val_mse: 13.0767 - val_mae: 2.6933 - lr: 3.7465e-04 - 1s/epoch - 2ms/step\n",
            "Score for inner fold : loss of 13.07669448852539\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-10-10 09:36:56,291]\u001b[0m Finished trial#0 resulted in value: 13.066666666666668. Current best value is 13.066666666666668 with parameters: {'activation': 'linear', 'num_hidden_layer': 2, 'i': 5, 'learning_rate': 0.0003746459090875119}.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trial Number:1\n",
            "-----Fold:0--------\n",
            "Epoch 1/500\n",
            "556/556 - 2s - loss: 41.3592 - mse: 41.3592 - mae: 2.5721 - val_loss: 2.0908 - val_mse: 2.0908 - val_mae: 0.9715 - lr: 1.2645e-04 - 2s/epoch - 3ms/step\n",
            "Epoch 2/500\n",
            "556/556 - 1s - loss: 37.8705 - mse: 37.8705 - mae: 2.2061 - val_loss: 3.3134 - val_mse: 3.3134 - val_mae: 1.1804 - lr: 1.2645e-04 - 925ms/epoch - 2ms/step\n",
            "Epoch 3/500\n",
            "556/556 - 1s - loss: 35.5340 - mse: 35.5340 - mae: 1.8554 - val_loss: 5.0201 - val_mse: 5.0201 - val_mae: 1.5579 - lr: 1.2645e-04 - 930ms/epoch - 2ms/step\n",
            "Epoch 4/500\n",
            "556/556 - 1s - loss: 33.7188 - mse: 33.7188 - mae: 1.5764 - val_loss: 7.4459 - val_mse: 7.4459 - val_mae: 2.0606 - lr: 1.2645e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 5/500\n",
            "556/556 - 1s - loss: 32.6945 - mse: 32.6945 - mae: 1.5300 - val_loss: 9.8726 - val_mse: 9.8726 - val_mae: 2.4758 - lr: 1.2645e-04 - 944ms/epoch - 2ms/step\n",
            "Epoch 6/500\n",
            "556/556 - 1s - loss: 32.3890 - mse: 32.3890 - mae: 1.5903 - val_loss: 10.6586 - val_mse: 10.6586 - val_mae: 2.6148 - lr: 1.2645e-04 - 947ms/epoch - 2ms/step\n",
            "Score for inner fold : loss of 10.658645629882812\n",
            "-----Fold:1--------\n",
            "Epoch 1/500\n",
            "556/556 - 2s - loss: 22.5599 - mse: 22.5599 - mae: 2.5165 - val_loss: 2.2482 - val_mse: 2.2482 - val_mae: 0.9755 - lr: 1.2645e-04 - 2s/epoch - 3ms/step\n",
            "Epoch 2/500\n",
            "556/556 - 1s - loss: 18.3346 - mse: 18.3346 - mae: 2.0960 - val_loss: 3.5729 - val_mse: 3.5729 - val_mae: 1.2843 - lr: 1.2645e-04 - 936ms/epoch - 2ms/step\n",
            "Epoch 3/500\n",
            "556/556 - 1s - loss: 15.7623 - mse: 15.7623 - mae: 1.7152 - val_loss: 5.4131 - val_mse: 5.4131 - val_mae: 1.7492 - lr: 1.2645e-04 - 972ms/epoch - 2ms/step\n",
            "Epoch 4/500\n",
            "556/556 - 1s - loss: 13.9423 - mse: 13.9423 - mae: 1.5268 - val_loss: 8.0138 - val_mse: 8.0138 - val_mae: 2.2794 - lr: 1.2645e-04 - 933ms/epoch - 2ms/step\n",
            "Epoch 5/500\n",
            "556/556 - 1s - loss: 13.0793 - mse: 13.0793 - mae: 1.5632 - val_loss: 9.4956 - val_mse: 9.4956 - val_mae: 2.5368 - lr: 1.2645e-04 - 946ms/epoch - 2ms/step\n",
            "Epoch 6/500\n",
            "556/556 - 1s - loss: 12.7187 - mse: 12.7187 - mae: 1.6018 - val_loss: 10.2108 - val_mse: 10.2108 - val_mae: 2.6202 - lr: 1.2645e-04 - 962ms/epoch - 2ms/step\n",
            "Score for inner fold : loss of 10.210755348205566\n",
            "-----Fold:2--------\n",
            "Epoch 1/500\n",
            "556/556 - 2s - loss: 41.4824 - mse: 41.4824 - mae: 2.5670 - val_loss: 2.0260 - val_mse: 2.0260 - val_mae: 0.9329 - lr: 1.2645e-04 - 2s/epoch - 3ms/step\n",
            "Epoch 2/500\n",
            "556/556 - 1s - loss: 38.1088 - mse: 38.1088 - mae: 2.1431 - val_loss: 3.0293 - val_mse: 3.0293 - val_mae: 1.1893 - lr: 1.2645e-04 - 962ms/epoch - 2ms/step\n",
            "Epoch 3/500\n",
            "556/556 - 1s - loss: 35.5395 - mse: 35.5395 - mae: 1.7614 - val_loss: 5.0633 - val_mse: 5.0633 - val_mae: 1.6233 - lr: 1.2645e-04 - 950ms/epoch - 2ms/step\n",
            "Epoch 4/500\n",
            "556/556 - 1s - loss: 33.5683 - mse: 33.5683 - mae: 1.5284 - val_loss: 7.9279 - val_mse: 7.9279 - val_mae: 2.1612 - lr: 1.2645e-04 - 967ms/epoch - 2ms/step\n",
            "Epoch 5/500\n",
            "556/556 - 1s - loss: 32.5705 - mse: 32.5705 - mae: 1.5348 - val_loss: 10.2432 - val_mse: 10.2432 - val_mae: 2.5206 - lr: 1.2645e-04 - 989ms/epoch - 2ms/step\n",
            "Epoch 6/500\n",
            "556/556 - 1s - loss: 32.2134 - mse: 32.2134 - mae: 1.5881 - val_loss: 11.2610 - val_mse: 11.2610 - val_mae: 2.6290 - lr: 1.2645e-04 - 942ms/epoch - 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-10-10 09:37:18,446]\u001b[0m Finished trial#1 resulted in value: 10.71. Current best value is 10.71 with parameters: {'activation': 'linear', 'num_hidden_layer': 2, 'i': 4, 'learning_rate': 0.00012644697036315954}.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Score for inner fold : loss of 11.26101016998291\n",
            "Trial Number:2\n",
            "-----Fold:0--------\n",
            "Epoch 1/500\n",
            "556/556 - 2s - loss: 37.1830 - mse: 37.1830 - mae: 1.9792 - val_loss: 10.0856 - val_mse: 10.0856 - val_mae: 2.5668 - lr: 3.4258e-04 - 2s/epoch - 3ms/step\n",
            "Epoch 2/500\n",
            "556/556 - 1s - loss: 33.4188 - mse: 33.4188 - mae: 1.6310 - val_loss: 10.5271 - val_mse: 10.5271 - val_mae: 2.6411 - lr: 3.4258e-04 - 951ms/epoch - 2ms/step\n",
            "Epoch 3/500\n",
            "556/556 - 1s - loss: 33.1119 - mse: 33.1119 - mae: 1.6184 - val_loss: 12.1199 - val_mse: 12.1199 - val_mae: 2.7183 - lr: 3.4258e-04 - 995ms/epoch - 2ms/step\n",
            "Epoch 4/500\n",
            "556/556 - 1s - loss: 33.0895 - mse: 33.0895 - mae: 1.6168 - val_loss: 12.1059 - val_mse: 12.1059 - val_mae: 2.6751 - lr: 3.4258e-04 - 998ms/epoch - 2ms/step\n",
            "Epoch 5/500\n",
            "556/556 - 1s - loss: 33.0651 - mse: 33.0651 - mae: 1.6157 - val_loss: 12.1125 - val_mse: 12.1125 - val_mae: 2.6817 - lr: 3.4258e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 6/500\n",
            "556/556 - 1s - loss: 33.0954 - mse: 33.0954 - mae: 1.6086 - val_loss: 12.2111 - val_mse: 12.2111 - val_mae: 2.6945 - lr: 3.4258e-04 - 1s/epoch - 2ms/step\n",
            "Score for inner fold : loss of 12.211097717285156\n",
            "-----Fold:1--------\n",
            "Epoch 1/500\n",
            "556/556 - 2s - loss: 35.5383 - mse: 35.5383 - mae: 1.9691 - val_loss: 10.4606 - val_mse: 10.4606 - val_mae: 2.5273 - lr: 3.4258e-04 - 2s/epoch - 3ms/step\n",
            "Epoch 2/500\n",
            "556/556 - 1s - loss: 31.4858 - mse: 31.4858 - mae: 1.5961 - val_loss: 11.3724 - val_mse: 11.3724 - val_mae: 2.5931 - lr: 3.4258e-04 - 984ms/epoch - 2ms/step\n",
            "Epoch 3/500\n",
            "556/556 - 1s - loss: 31.3520 - mse: 31.3520 - mae: 1.5885 - val_loss: 12.4788 - val_mse: 12.4788 - val_mae: 2.6564 - lr: 3.4258e-04 - 984ms/epoch - 2ms/step\n",
            "Epoch 4/500\n",
            "556/556 - 1s - loss: 31.3970 - mse: 31.3970 - mae: 1.5871 - val_loss: 13.6175 - val_mse: 13.6175 - val_mae: 2.7085 - lr: 3.4258e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 5/500\n",
            "556/556 - 1s - loss: 31.3558 - mse: 31.3558 - mae: 1.5894 - val_loss: 12.7631 - val_mse: 12.7631 - val_mae: 2.5981 - lr: 3.4258e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 6/500\n",
            "556/556 - 1s - loss: 31.4177 - mse: 31.4177 - mae: 1.5845 - val_loss: 13.3478 - val_mse: 13.3478 - val_mae: 2.6380 - lr: 3.4258e-04 - 1s/epoch - 2ms/step\n",
            "Score for inner fold : loss of 13.34778881072998\n",
            "-----Fold:2--------\n",
            "Epoch 1/500\n",
            "556/556 - 2s - loss: 17.4521 - mse: 17.4521 - mae: 1.9874 - val_loss: 9.0486 - val_mse: 9.0486 - val_mae: 2.4613 - lr: 3.4258e-04 - 2s/epoch - 3ms/step\n",
            "Epoch 2/500\n",
            "556/556 - 1s - loss: 11.6634 - mse: 11.6634 - mae: 1.6146 - val_loss: 11.3688 - val_mse: 11.3688 - val_mae: 2.6316 - lr: 3.4258e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 3/500\n",
            "556/556 - 1s - loss: 11.5698 - mse: 11.5698 - mae: 1.6025 - val_loss: 13.0209 - val_mse: 13.0209 - val_mae: 2.7439 - lr: 3.4258e-04 - 994ms/epoch - 2ms/step\n",
            "Epoch 4/500\n",
            "556/556 - 1s - loss: 11.5890 - mse: 11.5890 - mae: 1.6022 - val_loss: 13.1855 - val_mse: 13.1855 - val_mae: 2.7165 - lr: 3.4258e-04 - 973ms/epoch - 2ms/step\n",
            "Epoch 5/500\n",
            "556/556 - 1s - loss: 11.6030 - mse: 11.6030 - mae: 1.6021 - val_loss: 13.5442 - val_mse: 13.5442 - val_mae: 2.7237 - lr: 3.4258e-04 - 975ms/epoch - 2ms/step\n",
            "Epoch 6/500\n",
            "556/556 - 1s - loss: 11.5996 - mse: 11.5996 - mae: 1.5909 - val_loss: 13.6766 - val_mse: 13.6766 - val_mae: 2.6613 - lr: 3.4258e-04 - 1s/epoch - 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-10-10 09:37:41,692]\u001b[0m Finished trial#2 resulted in value: 13.08. Current best value is 10.71 with parameters: {'activation': 'linear', 'num_hidden_layer': 2, 'i': 4, 'learning_rate': 0.00012644697036315954}.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Score for inner fold : loss of 13.676580429077148\n",
            "Trial Number:3\n",
            "-----Fold:0--------\n",
            "Epoch 1/500\n",
            "556/556 - 2s - loss: 20.1963 - mse: 20.1963 - mae: 2.1913 - val_loss: 3.9771 - val_mse: 3.9771 - val_mae: 1.6205 - lr: 1.0908e-04 - 2s/epoch - 4ms/step\n",
            "Epoch 2/500\n",
            "556/556 - 1s - loss: 14.1800 - mse: 14.1800 - mae: 1.5777 - val_loss: 8.8042 - val_mse: 8.8042 - val_mae: 2.5767 - lr: 1.0908e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 3/500\n",
            "556/556 - 1s - loss: 13.2437 - mse: 13.2437 - mae: 1.5842 - val_loss: 9.4928 - val_mse: 9.4928 - val_mae: 2.6292 - lr: 1.0908e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 4/500\n",
            "556/556 - 1s - loss: 12.9433 - mse: 12.9433 - mae: 1.5770 - val_loss: 9.5167 - val_mse: 9.5167 - val_mae: 2.5977 - lr: 1.0908e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 5/500\n",
            "556/556 - 1s - loss: 12.7700 - mse: 12.7700 - mae: 1.5635 - val_loss: 9.8747 - val_mse: 9.8747 - val_mae: 2.6420 - lr: 1.0908e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 6/500\n",
            "556/556 - 1s - loss: 12.6358 - mse: 12.6358 - mae: 1.5592 - val_loss: 9.7180 - val_mse: 9.7180 - val_mae: 2.5997 - lr: 1.0908e-04 - 1s/epoch - 2ms/step\n",
            "Score for inner fold : loss of 9.71800422668457\n",
            "-----Fold:1--------\n",
            "Epoch 1/500\n",
            "556/556 - 2s - loss: 39.7988 - mse: 39.7988 - mae: 2.1966 - val_loss: 4.5344 - val_mse: 4.5344 - val_mae: 1.7191 - lr: 1.0908e-04 - 2s/epoch - 4ms/step\n",
            "Epoch 2/500\n",
            "556/556 - 1s - loss: 34.0003 - mse: 34.0003 - mae: 1.5727 - val_loss: 9.0361 - val_mse: 9.0361 - val_mae: 2.5755 - lr: 1.0908e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 3/500\n",
            "556/556 - 1s - loss: 33.3980 - mse: 33.3980 - mae: 1.5807 - val_loss: 9.4165 - val_mse: 9.4165 - val_mae: 2.6029 - lr: 1.0908e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 4/500\n",
            "556/556 - 1s - loss: 33.2071 - mse: 33.2071 - mae: 1.5679 - val_loss: 9.7136 - val_mse: 9.7136 - val_mae: 2.6179 - lr: 1.0908e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 5/500\n",
            "556/556 - 1s - loss: 33.0617 - mse: 33.0617 - mae: 1.5652 - val_loss: 9.5527 - val_mse: 9.5527 - val_mae: 2.6043 - lr: 1.0908e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 6/500\n",
            "556/556 - 1s - loss: 32.9515 - mse: 32.9515 - mae: 1.5573 - val_loss: 9.7583 - val_mse: 9.7583 - val_mae: 2.5874 - lr: 1.0908e-04 - 1s/epoch - 2ms/step\n",
            "Score for inner fold : loss of 9.758252143859863\n",
            "-----Fold:2--------\n",
            "Epoch 1/500\n",
            "556/556 - 2s - loss: 40.4238 - mse: 40.4238 - mae: 2.2439 - val_loss: 3.9543 - val_mse: 3.9543 - val_mae: 1.6085 - lr: 1.0908e-04 - 2s/epoch - 4ms/step\n",
            "Epoch 2/500\n",
            "556/556 - 1s - loss: 34.6738 - mse: 34.6738 - mae: 1.6066 - val_loss: 8.6342 - val_mse: 8.6342 - val_mae: 2.5436 - lr: 1.0908e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 3/500\n",
            "556/556 - 1s - loss: 33.6935 - mse: 33.6935 - mae: 1.6144 - val_loss: 9.2990 - val_mse: 9.2990 - val_mae: 2.6164 - lr: 1.0908e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 4/500\n",
            "556/556 - 1s - loss: 33.3870 - mse: 33.3870 - mae: 1.6010 - val_loss: 9.2838 - val_mse: 9.2838 - val_mae: 2.5875 - lr: 1.0908e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 5/500\n",
            "556/556 - 1s - loss: 33.1760 - mse: 33.1760 - mae: 1.5927 - val_loss: 9.2585 - val_mse: 9.2585 - val_mae: 2.5654 - lr: 1.0908e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 6/500\n",
            "556/556 - 1s - loss: 32.9931 - mse: 32.9931 - mae: 1.5837 - val_loss: 9.3860 - val_mse: 9.3860 - val_mae: 2.5650 - lr: 1.0908e-04 - 1s/epoch - 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-10-10 09:38:08,847]\u001b[0m Finished trial#3 resulted in value: 9.623333333333333. Current best value is 9.623333333333333 with parameters: {'activation': 'tanh', 'num_hidden_layer': 3, 'i': 6, 'learning_rate': 0.00010907972060886967}.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Score for inner fold : loss of 9.386003494262695\n",
            "Trial Number:4\n",
            "-----Fold:0--------\n",
            "Epoch 1/500\n",
            "556/556 - 2s - loss: 14.5274 - mse: 14.5274 - mae: 1.6777 - val_loss: 9.4647 - val_mse: 9.4647 - val_mae: 2.5816 - lr: 0.0025 - 2s/epoch - 3ms/step\n",
            "Epoch 2/500\n",
            "556/556 - 1s - loss: 12.3840 - mse: 12.3840 - mae: 1.5435 - val_loss: 9.5328 - val_mse: 9.5328 - val_mae: 2.5351 - lr: 0.0025 - 1s/epoch - 2ms/step\n",
            "Epoch 3/500\n",
            "556/556 - 1s - loss: 11.9312 - mse: 11.9312 - mae: 1.5278 - val_loss: 10.9482 - val_mse: 10.9482 - val_mae: 2.7032 - lr: 0.0025 - 1s/epoch - 2ms/step\n",
            "Epoch 4/500\n",
            "556/556 - 1s - loss: 11.5866 - mse: 11.5866 - mae: 1.5221 - val_loss: 9.6649 - val_mse: 9.6649 - val_mae: 2.5293 - lr: 0.0025 - 1s/epoch - 2ms/step\n",
            "Epoch 5/500\n",
            "556/556 - 1s - loss: 11.4026 - mse: 11.4026 - mae: 1.5206 - val_loss: 10.7084 - val_mse: 10.7084 - val_mae: 2.5715 - lr: 0.0025 - 1s/epoch - 2ms/step\n",
            "Epoch 6/500\n",
            "556/556 - 1s - loss: 11.2013 - mse: 11.2013 - mae: 1.5139 - val_loss: 10.1778 - val_mse: 10.1778 - val_mae: 2.5254 - lr: 0.0025 - 1s/epoch - 2ms/step\n",
            "Score for inner fold : loss of 10.177809715270996\n",
            "-----Fold:1--------\n",
            "Epoch 1/500\n",
            "556/556 - 2s - loss: 35.2611 - mse: 35.2611 - mae: 1.7049 - val_loss: 9.2596 - val_mse: 9.2596 - val_mae: 2.5617 - lr: 0.0025 - 2s/epoch - 3ms/step\n",
            "Epoch 2/500\n",
            "556/556 - 1s - loss: 33.1851 - mse: 33.1851 - mae: 1.5645 - val_loss: 10.1814 - val_mse: 10.1814 - val_mae: 2.5931 - lr: 0.0025 - 1s/epoch - 2ms/step\n",
            "Epoch 3/500\n",
            "556/556 - 1s - loss: 32.6684 - mse: 32.6684 - mae: 1.5455 - val_loss: 11.1861 - val_mse: 11.1861 - val_mae: 2.6816 - lr: 0.0025 - 1s/epoch - 2ms/step\n",
            "Epoch 4/500\n",
            "556/556 - 1s - loss: 32.3915 - mse: 32.3915 - mae: 1.5379 - val_loss: 10.3750 - val_mse: 10.3750 - val_mae: 2.5391 - lr: 0.0025 - 1s/epoch - 2ms/step\n",
            "Epoch 5/500\n",
            "556/556 - 1s - loss: 32.1898 - mse: 32.1898 - mae: 1.5335 - val_loss: 11.2525 - val_mse: 11.2525 - val_mae: 2.6689 - lr: 0.0025 - 1s/epoch - 2ms/step\n",
            "Epoch 6/500\n",
            "556/556 - 1s - loss: 32.0085 - mse: 32.0085 - mae: 1.5295 - val_loss: 12.2738 - val_mse: 12.2738 - val_mae: 2.7732 - lr: 0.0025 - 1s/epoch - 2ms/step\n",
            "Score for inner fold : loss of 12.273791313171387\n",
            "-----Fold:2--------\n",
            "Epoch 1/500\n",
            "556/556 - 2s - loss: 33.6375 - mse: 33.6375 - mae: 1.6861 - val_loss: 9.5219 - val_mse: 9.5219 - val_mae: 2.6123 - lr: 0.0025 - 2s/epoch - 3ms/step\n",
            "Epoch 2/500\n",
            "556/556 - 1s - loss: 31.7459 - mse: 31.7459 - mae: 1.5616 - val_loss: 10.2306 - val_mse: 10.2306 - val_mae: 2.6377 - lr: 0.0025 - 1s/epoch - 2ms/step\n",
            "Epoch 3/500\n",
            "556/556 - 1s - loss: 31.3286 - mse: 31.3286 - mae: 1.5406 - val_loss: 9.9596 - val_mse: 9.9596 - val_mae: 2.5164 - lr: 0.0025 - 1s/epoch - 2ms/step\n",
            "Epoch 4/500\n",
            "556/556 - 1s - loss: 31.0295 - mse: 31.0295 - mae: 1.5324 - val_loss: 11.8412 - val_mse: 11.8412 - val_mae: 2.7268 - lr: 0.0025 - 1s/epoch - 2ms/step\n",
            "Epoch 5/500\n",
            "556/556 - 1s - loss: 30.8928 - mse: 30.8928 - mae: 1.5294 - val_loss: 10.3234 - val_mse: 10.3234 - val_mae: 2.5415 - lr: 0.0025 - 1s/epoch - 2ms/step\n",
            "Epoch 6/500\n",
            "556/556 - 1s - loss: 30.8456 - mse: 30.8456 - mae: 1.5269 - val_loss: 11.3171 - val_mse: 11.3171 - val_mae: 2.6534 - lr: 0.0025 - 1s/epoch - 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-10-10 09:38:32,601]\u001b[0m Finished trial#4 resulted in value: 11.256666666666666. Current best value is 9.623333333333333 with parameters: {'activation': 'tanh', 'num_hidden_layer': 3, 'i': 6, 'learning_rate': 0.00010907972060886967}.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Score for inner fold : loss of 11.317086219787598\n",
            "Trial Number:5\n",
            "-----Fold:0--------\n",
            "Epoch 1/500\n",
            "556/556 - 5s - loss: 11.0365 - mse: 11.0365 - mae: 1.5766 - val_loss: 17.4527 - val_mse: 17.4527 - val_mae: 3.0051 - lr: 1.9070e-04 - 5s/epoch - 8ms/step\n",
            "Epoch 2/500\n",
            "556/556 - 4s - loss: 10.3253 - mse: 10.3253 - mae: 1.5298 - val_loss: 10.8785 - val_mse: 10.8785 - val_mae: 2.3280 - lr: 1.9070e-04 - 4s/epoch - 7ms/step\n",
            "Epoch 3/500\n",
            "556/556 - 4s - loss: 9.5385 - mse: 9.5385 - mae: 1.5012 - val_loss: 15.2349 - val_mse: 15.2349 - val_mae: 2.5695 - lr: 1.9070e-04 - 4s/epoch - 7ms/step\n",
            "Epoch 4/500\n",
            "556/556 - 4s - loss: 10.0552 - mse: 10.0552 - mae: 1.4872 - val_loss: 16.9429 - val_mse: 16.9429 - val_mae: 2.8186 - lr: 1.9070e-04 - 4s/epoch - 7ms/step\n",
            "Epoch 5/500\n",
            "556/556 - 4s - loss: 10.2153 - mse: 10.2153 - mae: 1.4849 - val_loss: 15.9132 - val_mse: 15.9132 - val_mae: 2.4682 - lr: 1.9070e-04 - 4s/epoch - 7ms/step\n",
            "Epoch 6/500\n",
            "556/556 - 4s - loss: 9.7558 - mse: 9.7558 - mae: 1.4742 - val_loss: 14.9197 - val_mse: 14.9197 - val_mae: 2.6926 - lr: 1.9070e-04 - 4s/epoch - 7ms/step\n",
            "Epoch 7/500\n",
            "556/556 - 4s - loss: 9.6749 - mse: 9.6749 - mae: 1.4662 - val_loss: 17.8827 - val_mse: 17.8827 - val_mae: 2.8013 - lr: 1.9070e-04 - 4s/epoch - 7ms/step\n",
            "Score for inner fold : loss of 17.882719039916992\n",
            "-----Fold:1--------\n",
            "Epoch 1/500\n",
            "556/556 - 5s - loss: 33.0930 - mse: 33.0930 - mae: 1.6164 - val_loss: 16.0809 - val_mse: 16.0809 - val_mae: 2.6936 - lr: 1.9070e-04 - 5s/epoch - 9ms/step\n",
            "Epoch 2/500\n",
            "556/556 - 4s - loss: 31.9348 - mse: 31.9348 - mae: 1.5550 - val_loss: 17.7934 - val_mse: 17.7934 - val_mae: 2.8872 - lr: 1.9070e-04 - 4s/epoch - 7ms/step\n",
            "Epoch 3/500\n",
            "556/556 - 4s - loss: 31.4721 - mse: 31.4721 - mae: 1.5284 - val_loss: 14.4595 - val_mse: 14.4595 - val_mae: 2.4996 - lr: 1.9070e-04 - 4s/epoch - 7ms/step\n",
            "Epoch 4/500\n",
            "556/556 - 4s - loss: 31.2824 - mse: 31.2824 - mae: 1.5187 - val_loss: 15.0609 - val_mse: 15.0609 - val_mae: 2.7220 - lr: 1.9070e-04 - 4s/epoch - 7ms/step\n",
            "Epoch 5/500\n",
            "556/556 - 4s - loss: 31.2369 - mse: 31.2369 - mae: 1.5054 - val_loss: 13.4955 - val_mse: 13.4955 - val_mae: 2.5535 - lr: 1.9070e-04 - 4s/epoch - 7ms/step\n",
            "Epoch 6/500\n",
            "556/556 - 4s - loss: 31.2133 - mse: 31.2133 - mae: 1.5055 - val_loss: 14.3507 - val_mse: 14.3507 - val_mae: 2.9164 - lr: 1.9070e-04 - 4s/epoch - 7ms/step\n",
            "Epoch 7/500\n",
            "556/556 - 4s - loss: 30.9019 - mse: 30.9019 - mae: 1.4956 - val_loss: 15.4697 - val_mse: 15.4697 - val_mae: 2.5757 - lr: 1.9070e-04 - 4s/epoch - 7ms/step\n",
            "Epoch 8/500\n",
            "556/556 - 4s - loss: 31.0574 - mse: 31.0574 - mae: 1.4887 - val_loss: 17.0578 - val_mse: 17.0578 - val_mae: 2.5884 - lr: 1.9070e-04 - 4s/epoch - 7ms/step\n",
            "Epoch 9/500\n",
            "556/556 - 4s - loss: 30.9585 - mse: 30.9585 - mae: 1.4817 - val_loss: 14.3742 - val_mse: 14.3742 - val_mae: 2.4990 - lr: 1.9070e-04 - 4s/epoch - 7ms/step\n",
            "Epoch 10/500\n",
            "556/556 - 4s - loss: 30.5394 - mse: 30.5394 - mae: 1.4715 - val_loss: 17.7299 - val_mse: 17.7299 - val_mae: 2.5076 - lr: 1.9070e-04 - 4s/epoch - 7ms/step\n",
            "Score for inner fold : loss of 17.729848861694336\n",
            "-----Fold:2--------\n",
            "Epoch 1/500\n",
            "556/556 - 5s - loss: 33.0959 - mse: 33.0959 - mae: 1.6343 - val_loss: 9.1572 - val_mse: 9.1572 - val_mae: 2.1201 - lr: 1.9070e-04 - 5s/epoch - 9ms/step\n",
            "Epoch 2/500\n",
            "556/556 - 4s - loss: 31.9866 - mse: 31.9866 - mae: 1.5619 - val_loss: 16.3889 - val_mse: 16.3889 - val_mae: 2.9390 - lr: 1.9070e-04 - 4s/epoch - 7ms/step\n",
            "Epoch 3/500\n",
            "556/556 - 4s - loss: 31.8920 - mse: 31.8920 - mae: 1.5489 - val_loss: 9.4549 - val_mse: 9.4549 - val_mae: 2.4659 - lr: 1.9070e-04 - 4s/epoch - 7ms/step\n",
            "Epoch 4/500\n",
            "556/556 - 4s - loss: 31.5093 - mse: 31.5093 - mae: 1.5364 - val_loss: 10.8594 - val_mse: 10.8594 - val_mae: 2.4977 - lr: 1.9070e-04 - 4s/epoch - 7ms/step\n",
            "Epoch 5/500\n",
            "556/556 - 4s - loss: 31.4036 - mse: 31.4036 - mae: 1.5274 - val_loss: 12.1788 - val_mse: 12.1788 - val_mae: 2.5376 - lr: 1.9070e-04 - 4s/epoch - 7ms/step\n",
            "Epoch 6/500\n",
            "556/556 - 4s - loss: 31.6362 - mse: 31.6362 - mae: 1.5221 - val_loss: 12.2059 - val_mse: 12.2059 - val_mae: 2.6705 - lr: 1.9070e-04 - 4s/epoch - 7ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-10-10 09:40:10,059]\u001b[0m Finished trial#5 resulted in value: 15.94. Current best value is 9.623333333333333 with parameters: {'activation': 'tanh', 'num_hidden_layer': 3, 'i': 6, 'learning_rate': 0.00010907972060886967}.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Score for inner fold : loss of 12.205855369567871\n",
            "Trial Number:6\n",
            "-----Fold:0--------\n",
            "Epoch 1/500\n",
            "556/556 - 3s - loss: 12.3443 - mse: 12.3443 - mae: 1.6138 - val_loss: 10.5014 - val_mse: 10.5014 - val_mae: 2.5878 - lr: 2.2537e-04 - 3s/epoch - 5ms/step\n",
            "Epoch 2/500\n",
            "556/556 - 2s - loss: 11.1465 - mse: 11.1465 - mae: 1.5433 - val_loss: 12.4498 - val_mse: 12.4498 - val_mae: 2.7955 - lr: 2.2537e-04 - 2s/epoch - 3ms/step\n",
            "Epoch 3/500\n",
            "556/556 - 2s - loss: 10.9290 - mse: 10.9290 - mae: 1.5168 - val_loss: 11.3243 - val_mse: 11.3243 - val_mae: 2.6265 - lr: 2.2537e-04 - 2s/epoch - 3ms/step\n",
            "Epoch 4/500\n",
            "556/556 - 2s - loss: 10.6378 - mse: 10.6378 - mae: 1.5124 - val_loss: 10.4847 - val_mse: 10.4847 - val_mae: 2.5478 - lr: 2.2537e-04 - 2s/epoch - 3ms/step\n",
            "Epoch 5/500\n",
            "556/556 - 2s - loss: 10.7103 - mse: 10.7103 - mae: 1.5070 - val_loss: 9.6616 - val_mse: 9.6616 - val_mae: 2.4469 - lr: 2.2537e-04 - 2s/epoch - 3ms/step\n",
            "Epoch 6/500\n",
            "556/556 - 2s - loss: 10.4894 - mse: 10.4894 - mae: 1.4976 - val_loss: 11.9263 - val_mse: 11.9263 - val_mae: 2.6213 - lr: 2.2537e-04 - 2s/epoch - 3ms/step\n",
            "Epoch 7/500\n",
            "556/556 - 2s - loss: 10.5205 - mse: 10.5205 - mae: 1.4911 - val_loss: 9.7888 - val_mse: 9.7888 - val_mae: 2.3546 - lr: 2.2537e-04 - 2s/epoch - 3ms/step\n",
            "Epoch 8/500\n",
            "556/556 - 2s - loss: 10.4912 - mse: 10.4912 - mae: 1.4855 - val_loss: 13.5446 - val_mse: 13.5446 - val_mae: 2.7361 - lr: 2.2537e-04 - 2s/epoch - 3ms/step\n",
            "Epoch 9/500\n",
            "556/556 - 2s - loss: 10.3885 - mse: 10.3885 - mae: 1.4822 - val_loss: 13.1868 - val_mse: 13.1868 - val_mae: 2.8029 - lr: 2.2537e-04 - 2s/epoch - 3ms/step\n",
            "Epoch 10/500\n",
            "556/556 - 2s - loss: 10.4113 - mse: 10.4113 - mae: 1.4803 - val_loss: 12.0347 - val_mse: 12.0347 - val_mae: 2.6231 - lr: 2.2537e-04 - 2s/epoch - 3ms/step\n",
            "Score for inner fold : loss of 12.034700393676758\n",
            "-----Fold:1--------\n",
            "Epoch 1/500\n",
            "556/556 - 3s - loss: 32.5225 - mse: 32.5225 - mae: 1.6088 - val_loss: 14.4676 - val_mse: 14.4676 - val_mae: 2.6400 - lr: 2.2537e-04 - 3s/epoch - 5ms/step\n",
            "Epoch 2/500\n",
            "556/556 - 2s - loss: 31.7104 - mse: 31.7104 - mae: 1.5517 - val_loss: 15.9334 - val_mse: 15.9334 - val_mae: 2.7002 - lr: 2.2537e-04 - 2s/epoch - 3ms/step\n",
            "Epoch 3/500\n",
            "556/556 - 2s - loss: 31.4308 - mse: 31.4308 - mae: 1.5331 - val_loss: 13.6365 - val_mse: 13.6365 - val_mae: 2.4750 - lr: 2.2537e-04 - 2s/epoch - 3ms/step\n",
            "Epoch 4/500\n",
            "556/556 - 2s - loss: 31.2837 - mse: 31.2837 - mae: 1.5171 - val_loss: 15.7912 - val_mse: 15.7912 - val_mae: 2.6625 - lr: 2.2537e-04 - 2s/epoch - 3ms/step\n",
            "Epoch 5/500\n",
            "556/556 - 2s - loss: 31.0081 - mse: 31.0081 - mae: 1.5047 - val_loss: 15.1287 - val_mse: 15.1287 - val_mae: 2.7550 - lr: 2.2537e-04 - 2s/epoch - 3ms/step\n",
            "Epoch 6/500\n",
            "556/556 - 2s - loss: 30.9125 - mse: 30.9125 - mae: 1.4991 - val_loss: 16.6446 - val_mse: 16.6446 - val_mae: 2.8380 - lr: 2.2537e-04 - 2s/epoch - 3ms/step\n",
            "Epoch 7/500\n",
            "556/556 - 2s - loss: 30.6848 - mse: 30.6848 - mae: 1.5028 - val_loss: 16.2692 - val_mse: 16.2692 - val_mae: 2.8167 - lr: 2.2537e-04 - 2s/epoch - 3ms/step\n",
            "Epoch 8/500\n",
            "556/556 - 2s - loss: 30.7750 - mse: 30.7750 - mae: 1.4912 - val_loss: 16.9851 - val_mse: 16.9851 - val_mae: 2.7452 - lr: 2.2537e-04 - 2s/epoch - 3ms/step\n",
            "Score for inner fold : loss of 16.985103607177734\n",
            "-----Fold:2--------\n",
            "Epoch 1/500\n",
            "556/556 - 3s - loss: 33.4418 - mse: 33.4418 - mae: 1.6616 - val_loss: 11.2708 - val_mse: 11.2708 - val_mae: 2.4007 - lr: 2.2537e-04 - 3s/epoch - 5ms/step\n",
            "Epoch 2/500\n",
            "556/556 - 2s - loss: 32.1017 - mse: 32.1017 - mae: 1.5779 - val_loss: 14.9828 - val_mse: 14.9828 - val_mae: 2.8024 - lr: 2.2537e-04 - 2s/epoch - 3ms/step\n",
            "Epoch 3/500\n",
            "556/556 - 2s - loss: 31.7949 - mse: 31.7949 - mae: 1.5651 - val_loss: 13.8045 - val_mse: 13.8045 - val_mae: 2.5878 - lr: 2.2537e-04 - 2s/epoch - 3ms/step\n",
            "Epoch 4/500\n",
            "556/556 - 2s - loss: 31.5703 - mse: 31.5703 - mae: 1.5573 - val_loss: 12.7128 - val_mse: 12.7128 - val_mae: 2.6959 - lr: 2.2537e-04 - 2s/epoch - 3ms/step\n",
            "Epoch 5/500\n",
            "556/556 - 2s - loss: 31.3876 - mse: 31.3876 - mae: 1.5450 - val_loss: 13.8425 - val_mse: 13.8425 - val_mae: 2.6631 - lr: 2.2537e-04 - 2s/epoch - 3ms/step\n",
            "Epoch 6/500\n",
            "556/556 - 2s - loss: 31.3595 - mse: 31.3595 - mae: 1.5380 - val_loss: 14.6134 - val_mse: 14.6134 - val_mae: 2.6530 - lr: 2.2537e-04 - 2s/epoch - 3ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-10-10 09:40:59,535]\u001b[0m Finished trial#6 resulted in value: 14.543333333333331. Current best value is 9.623333333333333 with parameters: {'activation': 'tanh', 'num_hidden_layer': 3, 'i': 6, 'learning_rate': 0.00010907972060886967}.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Score for inner fold : loss of 14.613447189331055\n",
            "Trial Number:7\n",
            "-----Fold:0--------\n",
            "Epoch 1/500\n",
            "556/556 - 2s - loss: 31.5006 - mse: 31.5006 - mae: 1.6216 - val_loss: 9.8764 - val_mse: 9.8764 - val_mae: 2.3496 - lr: 7.0944e-04 - 2s/epoch - 4ms/step\n",
            "Epoch 2/500\n",
            "556/556 - 2s - loss: 30.3054 - mse: 30.3054 - mae: 1.5452 - val_loss: 11.5986 - val_mse: 11.5986 - val_mae: 2.5840 - lr: 7.0944e-04 - 2s/epoch - 3ms/step\n",
            "Epoch 3/500\n",
            "556/556 - 2s - loss: 30.0106 - mse: 30.0106 - mae: 1.5324 - val_loss: 16.4866 - val_mse: 16.4866 - val_mae: 2.7218 - lr: 7.0944e-04 - 2s/epoch - 3ms/step\n",
            "Epoch 4/500\n",
            "556/556 - 2s - loss: 29.5280 - mse: 29.5280 - mae: 1.5140 - val_loss: 16.9475 - val_mse: 16.9475 - val_mae: 2.7142 - lr: 7.0944e-04 - 2s/epoch - 3ms/step\n",
            "Epoch 5/500\n",
            "556/556 - 2s - loss: 29.3844 - mse: 29.3844 - mae: 1.5035 - val_loss: 14.8120 - val_mse: 14.8120 - val_mae: 2.7805 - lr: 7.0944e-04 - 2s/epoch - 3ms/step\n",
            "Epoch 6/500\n",
            "556/556 - 2s - loss: 29.8341 - mse: 29.8341 - mae: 1.5029 - val_loss: 12.2618 - val_mse: 12.2618 - val_mae: 2.3237 - lr: 7.0944e-04 - 2s/epoch - 3ms/step\n",
            "Score for inner fold : loss of 12.261809349060059\n",
            "-----Fold:1--------\n",
            "Epoch 1/500\n",
            "556/556 - 2s - loss: 33.3339 - mse: 33.3339 - mae: 1.6341 - val_loss: 15.1017 - val_mse: 15.1017 - val_mae: 2.6666 - lr: 7.0944e-04 - 2s/epoch - 4ms/step\n",
            "Epoch 2/500\n",
            "556/556 - 2s - loss: 32.2539 - mse: 32.2539 - mae: 1.5545 - val_loss: 18.2944 - val_mse: 18.2944 - val_mae: 2.8548 - lr: 7.0944e-04 - 2s/epoch - 3ms/step\n",
            "Epoch 3/500\n",
            "556/556 - 2s - loss: 31.9691 - mse: 31.9691 - mae: 1.5409 - val_loss: 12.4111 - val_mse: 12.4111 - val_mae: 2.7014 - lr: 7.0944e-04 - 2s/epoch - 3ms/step\n",
            "Epoch 4/500\n",
            "556/556 - 2s - loss: 31.7822 - mse: 31.7822 - mae: 1.5405 - val_loss: 12.6427 - val_mse: 12.6427 - val_mae: 2.5516 - lr: 7.0944e-04 - 2s/epoch - 3ms/step\n",
            "Epoch 5/500\n",
            "556/556 - 2s - loss: 32.2257 - mse: 32.2257 - mae: 1.5308 - val_loss: 18.1458 - val_mse: 18.1458 - val_mae: 2.8967 - lr: 7.0944e-04 - 2s/epoch - 3ms/step\n",
            "Epoch 6/500\n",
            "556/556 - 2s - loss: 31.6547 - mse: 31.6547 - mae: 1.5201 - val_loss: 15.5805 - val_mse: 15.5805 - val_mae: 2.7823 - lr: 7.0944e-04 - 2s/epoch - 3ms/step\n",
            "Epoch 7/500\n",
            "556/556 - 2s - loss: 31.8295 - mse: 31.8295 - mae: 1.5091 - val_loss: 14.3997 - val_mse: 14.3997 - val_mae: 2.5368 - lr: 7.0944e-04 - 2s/epoch - 3ms/step\n",
            "Epoch 8/500\n",
            "556/556 - 2s - loss: 32.1981 - mse: 32.1981 - mae: 1.5164 - val_loss: 14.4824 - val_mse: 14.4824 - val_mae: 2.4572 - lr: 7.0944e-04 - 2s/epoch - 3ms/step\n",
            "Score for inner fold : loss of 14.482420921325684\n",
            "-----Fold:2--------\n",
            "Epoch 1/500\n",
            "556/556 - 2s - loss: 12.5629 - mse: 12.5629 - mae: 1.6134 - val_loss: 14.9872 - val_mse: 14.9872 - val_mae: 2.6217 - lr: 7.0944e-04 - 2s/epoch - 4ms/step\n",
            "Epoch 2/500\n",
            "556/556 - 2s - loss: 11.6241 - mse: 11.6241 - mae: 1.5478 - val_loss: 13.6394 - val_mse: 13.6394 - val_mae: 2.6001 - lr: 7.0944e-04 - 2s/epoch - 3ms/step\n",
            "Epoch 3/500\n",
            "556/556 - 2s - loss: 11.0847 - mse: 11.0847 - mae: 1.5214 - val_loss: 15.1562 - val_mse: 15.1562 - val_mae: 2.6864 - lr: 7.0944e-04 - 2s/epoch - 3ms/step\n",
            "Epoch 4/500\n",
            "556/556 - 2s - loss: 11.3401 - mse: 11.3401 - mae: 1.5153 - val_loss: 16.9681 - val_mse: 16.9681 - val_mae: 2.8297 - lr: 7.0944e-04 - 2s/epoch - 3ms/step\n",
            "Epoch 5/500\n",
            "556/556 - 2s - loss: 11.0852 - mse: 11.0852 - mae: 1.5118 - val_loss: 16.4385 - val_mse: 16.4385 - val_mae: 2.7960 - lr: 7.0944e-04 - 2s/epoch - 3ms/step\n",
            "Epoch 6/500\n",
            "556/556 - 2s - loss: 10.9794 - mse: 10.9794 - mae: 1.5084 - val_loss: 13.6800 - val_mse: 13.6800 - val_mae: 2.6801 - lr: 7.0944e-04 - 2s/epoch - 3ms/step\n",
            "Epoch 7/500\n",
            "556/556 - 2s - loss: 10.6437 - mse: 10.6437 - mae: 1.4914 - val_loss: 15.5366 - val_mse: 15.5366 - val_mae: 2.4906 - lr: 7.0944e-04 - 2s/epoch - 3ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-10-10 09:41:38,631]\u001b[0m Finished trial#7 resulted in value: 14.093333333333334. Current best value is 9.623333333333333 with parameters: {'activation': 'tanh', 'num_hidden_layer': 3, 'i': 6, 'learning_rate': 0.00010907972060886967}.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Score for inner fold : loss of 15.536599159240723\n",
            "Trial Number:8\n",
            "-----Fold:0--------\n",
            "Epoch 1/500\n",
            "556/556 - 2s - loss: 34.5281 - mse: 34.5281 - mae: 1.6600 - val_loss: 10.4159 - val_mse: 10.4159 - val_mae: 2.7084 - lr: 0.0016 - 2s/epoch - 4ms/step\n",
            "Epoch 2/500\n",
            "556/556 - 1s - loss: 33.0955 - mse: 33.0955 - mae: 1.5638 - val_loss: 9.7782 - val_mse: 9.7782 - val_mae: 2.5798 - lr: 0.0016 - 1s/epoch - 2ms/step\n",
            "Epoch 3/500\n",
            "556/556 - 1s - loss: 32.7114 - mse: 32.7114 - mae: 1.5571 - val_loss: 9.5513 - val_mse: 9.5513 - val_mae: 2.5676 - lr: 0.0016 - 1s/epoch - 2ms/step\n",
            "Epoch 4/500\n",
            "556/556 - 1s - loss: 32.5202 - mse: 32.5202 - mae: 1.5413 - val_loss: 11.3315 - val_mse: 11.3315 - val_mae: 2.7305 - lr: 0.0016 - 1s/epoch - 2ms/step\n",
            "Epoch 5/500\n",
            "556/556 - 1s - loss: 32.3766 - mse: 32.3766 - mae: 1.5423 - val_loss: 11.5133 - val_mse: 11.5133 - val_mae: 2.7683 - lr: 0.0016 - 1s/epoch - 2ms/step\n",
            "Epoch 6/500\n",
            "556/556 - 1s - loss: 32.2491 - mse: 32.2491 - mae: 1.5331 - val_loss: 12.2584 - val_mse: 12.2584 - val_mae: 2.9214 - lr: 0.0016 - 1s/epoch - 2ms/step\n",
            "Epoch 7/500\n",
            "556/556 - 1s - loss: 32.0159 - mse: 32.0159 - mae: 1.5287 - val_loss: 11.8109 - val_mse: 11.8109 - val_mae: 2.6662 - lr: 0.0016 - 1s/epoch - 2ms/step\n",
            "Epoch 8/500\n",
            "556/556 - 1s - loss: 31.9480 - mse: 31.9480 - mae: 1.5221 - val_loss: 9.8527 - val_mse: 9.8527 - val_mae: 2.4123 - lr: 0.0016 - 1s/epoch - 2ms/step\n",
            "Score for inner fold : loss of 9.852669715881348\n",
            "-----Fold:1--------\n",
            "Epoch 1/500\n",
            "556/556 - 2s - loss: 35.8970 - mse: 35.8970 - mae: 1.6860 - val_loss: 9.0646 - val_mse: 9.0646 - val_mae: 2.4601 - lr: 0.0016 - 2s/epoch - 4ms/step\n",
            "Epoch 2/500\n",
            "556/556 - 1s - loss: 34.2870 - mse: 34.2870 - mae: 1.5991 - val_loss: 12.5224 - val_mse: 12.5224 - val_mae: 2.9191 - lr: 0.0016 - 1s/epoch - 2ms/step\n",
            "Epoch 3/500\n",
            "556/556 - 1s - loss: 33.7168 - mse: 33.7168 - mae: 1.5889 - val_loss: 11.4138 - val_mse: 11.4138 - val_mae: 2.7725 - lr: 0.0016 - 1s/epoch - 2ms/step\n",
            "Epoch 4/500\n",
            "556/556 - 1s - loss: 33.3651 - mse: 33.3651 - mae: 1.5725 - val_loss: 10.1049 - val_mse: 10.1049 - val_mae: 2.4935 - lr: 0.0016 - 1s/epoch - 2ms/step\n",
            "Epoch 5/500\n",
            "556/556 - 1s - loss: 33.1406 - mse: 33.1406 - mae: 1.5723 - val_loss: 12.3497 - val_mse: 12.3497 - val_mae: 2.8097 - lr: 0.0016 - 1s/epoch - 2ms/step\n",
            "Epoch 6/500\n",
            "556/556 - 1s - loss: 32.8841 - mse: 32.8841 - mae: 1.5636 - val_loss: 10.3427 - val_mse: 10.3427 - val_mae: 2.5805 - lr: 0.0016 - 1s/epoch - 2ms/step\n",
            "Score for inner fold : loss of 10.342721939086914\n",
            "-----Fold:2--------\n",
            "Epoch 1/500\n",
            "556/556 - 2s - loss: 12.5225 - mse: 12.5225 - mae: 1.6623 - val_loss: 9.6910 - val_mse: 9.6910 - val_mae: 2.6411 - lr: 0.0016 - 2s/epoch - 4ms/step\n",
            "Epoch 2/500\n",
            "556/556 - 1s - loss: 11.0628 - mse: 11.0628 - mae: 1.5677 - val_loss: 10.5507 - val_mse: 10.5507 - val_mae: 2.6613 - lr: 0.0016 - 1s/epoch - 2ms/step\n",
            "Epoch 3/500\n",
            "556/556 - 1s - loss: 10.5754 - mse: 10.5754 - mae: 1.5603 - val_loss: 10.3145 - val_mse: 10.3145 - val_mae: 2.5918 - lr: 0.0016 - 1s/epoch - 2ms/step\n",
            "Epoch 4/500\n",
            "556/556 - 1s - loss: 10.2474 - mse: 10.2474 - mae: 1.5469 - val_loss: 11.0530 - val_mse: 11.0530 - val_mae: 2.6777 - lr: 0.0016 - 1s/epoch - 2ms/step\n",
            "Epoch 5/500\n",
            "556/556 - 1s - loss: 10.0192 - mse: 10.0192 - mae: 1.5455 - val_loss: 10.7026 - val_mse: 10.7026 - val_mae: 2.5592 - lr: 0.0016 - 1s/epoch - 2ms/step\n",
            "Epoch 6/500\n",
            "556/556 - 1s - loss: 9.8748 - mse: 9.8748 - mae: 1.5349 - val_loss: 9.8644 - val_mse: 9.8644 - val_mae: 2.5659 - lr: 0.0016 - 1s/epoch - 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-10-10 09:42:08,116]\u001b[0m Finished trial#8 resulted in value: 10.016666666666666. Current best value is 9.623333333333333 with parameters: {'activation': 'tanh', 'num_hidden_layer': 3, 'i': 6, 'learning_rate': 0.00010907972060886967}.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Score for inner fold : loss of 9.864359855651855\n",
            "Trial Number:9\n",
            "-----Fold:0--------\n",
            "Epoch 1/500\n",
            "556/556 - 2s - loss: 42.0811 - mse: 42.0811 - mae: 2.6188 - val_loss: 2.2958 - val_mse: 2.2958 - val_mae: 0.9797 - lr: 1.1175e-04 - 2s/epoch - 3ms/step\n",
            "Epoch 2/500\n",
            "556/556 - 1s - loss: 38.6033 - mse: 38.6033 - mae: 2.2590 - val_loss: 3.2577 - val_mse: 3.2577 - val_mae: 1.1465 - lr: 1.1175e-04 - 916ms/epoch - 2ms/step\n",
            "Epoch 3/500\n",
            "556/556 - 1s - loss: 36.2128 - mse: 36.2128 - mae: 1.9174 - val_loss: 4.7406 - val_mse: 4.7406 - val_mae: 1.4754 - lr: 1.1175e-04 - 947ms/epoch - 2ms/step\n",
            "Epoch 4/500\n",
            "556/556 - 1s - loss: 34.3685 - mse: 34.3685 - mae: 1.6382 - val_loss: 6.8675 - val_mse: 6.8675 - val_mae: 1.9159 - lr: 1.1175e-04 - 968ms/epoch - 2ms/step\n",
            "Epoch 5/500\n",
            "556/556 - 1s - loss: 33.2317 - mse: 33.2317 - mae: 1.5315 - val_loss: 9.3722 - val_mse: 9.3722 - val_mae: 2.3682 - lr: 1.1175e-04 - 963ms/epoch - 2ms/step\n",
            "Epoch 6/500\n",
            "556/556 - 1s - loss: 32.8054 - mse: 32.8054 - mae: 1.5801 - val_loss: 10.8004 - val_mse: 10.8004 - val_mae: 2.5848 - lr: 1.1175e-04 - 950ms/epoch - 2ms/step\n",
            "Score for inner fold : loss of 10.800378799438477\n",
            "-----Fold:1--------\n",
            "Epoch 1/500\n",
            "556/556 - 2s - loss: 43.1800 - mse: 43.1800 - mae: 2.5604 - val_loss: 1.9315 - val_mse: 1.9315 - val_mae: 0.9583 - lr: 1.1175e-04 - 2s/epoch - 3ms/step\n",
            "Epoch 2/500\n",
            "556/556 - 1s - loss: 39.9828 - mse: 39.9828 - mae: 2.2253 - val_loss: 2.7736 - val_mse: 2.7736 - val_mae: 1.1566 - lr: 1.1175e-04 - 938ms/epoch - 2ms/step\n",
            "Epoch 3/500\n",
            "556/556 - 1s - loss: 37.4651 - mse: 37.4651 - mae: 1.8851 - val_loss: 4.1261 - val_mse: 4.1261 - val_mae: 1.4925 - lr: 1.1175e-04 - 956ms/epoch - 2ms/step\n",
            "Epoch 4/500\n",
            "556/556 - 1s - loss: 35.4431 - mse: 35.4431 - mae: 1.6081 - val_loss: 6.2543 - val_mse: 6.2543 - val_mae: 1.9394 - lr: 1.1175e-04 - 967ms/epoch - 2ms/step\n",
            "Epoch 5/500\n",
            "556/556 - 1s - loss: 34.1019 - mse: 34.1019 - mae: 1.5219 - val_loss: 8.7150 - val_mse: 8.7150 - val_mae: 2.3783 - lr: 1.1175e-04 - 954ms/epoch - 2ms/step\n",
            "Epoch 6/500\n",
            "556/556 - 1s - loss: 33.3508 - mse: 33.3508 - mae: 1.5751 - val_loss: 10.2159 - val_mse: 10.2159 - val_mae: 2.5860 - lr: 1.1175e-04 - 969ms/epoch - 2ms/step\n",
            "Score for inner fold : loss of 10.215874671936035\n",
            "-----Fold:2--------\n",
            "Epoch 1/500\n",
            "556/556 - 2s - loss: 22.9887 - mse: 22.9887 - mae: 2.6489 - val_loss: 1.8427 - val_mse: 1.8427 - val_mae: 1.0075 - lr: 1.1175e-04 - 2s/epoch - 3ms/step\n",
            "Epoch 2/500\n",
            "556/556 - 1s - loss: 18.5642 - mse: 18.5642 - mae: 2.2876 - val_loss: 2.6532 - val_mse: 2.6532 - val_mae: 1.1580 - lr: 1.1175e-04 - 974ms/epoch - 2ms/step\n",
            "Epoch 3/500\n",
            "556/556 - 1s - loss: 15.7813 - mse: 15.7813 - mae: 1.9785 - val_loss: 3.8666 - val_mse: 3.8666 - val_mae: 1.4013 - lr: 1.1175e-04 - 940ms/epoch - 2ms/step\n",
            "Epoch 4/500\n",
            "556/556 - 1s - loss: 13.7502 - mse: 13.7502 - mae: 1.6888 - val_loss: 5.6837 - val_mse: 5.6837 - val_mae: 1.7853 - lr: 1.1175e-04 - 945ms/epoch - 2ms/step\n",
            "Epoch 5/500\n",
            "556/556 - 1s - loss: 12.3300 - mse: 12.3300 - mae: 1.5246 - val_loss: 7.8847 - val_mse: 7.8847 - val_mae: 2.2224 - lr: 1.1175e-04 - 943ms/epoch - 2ms/step\n",
            "Epoch 6/500\n",
            "556/556 - 1s - loss: 11.6087 - mse: 11.6087 - mae: 1.5357 - val_loss: 9.3293 - val_mse: 9.3293 - val_mae: 2.4929 - lr: 1.1175e-04 - 935ms/epoch - 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-10-10 09:42:30,269]\u001b[0m Finished trial#9 resulted in value: 10.116666666666667. Current best value is 9.623333333333333 with parameters: {'activation': 'tanh', 'num_hidden_layer': 3, 'i': 6, 'learning_rate': 0.00010907972060886967}.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Score for inner fold : loss of 9.32929515838623\n",
            "Trial Number:10\n",
            "-----Fold:0--------\n",
            "Epoch 1/500\n",
            "556/556 - 2s - loss: 34.5717 - mse: 34.5717 - mae: 1.7179 - val_loss: 8.4095 - val_mse: 8.4095 - val_mae: 2.2823 - lr: 0.0067 - 2s/epoch - 4ms/step\n",
            "Epoch 2/500\n",
            "556/556 - 1s - loss: 33.5747 - mse: 33.5747 - mae: 1.6640 - val_loss: 10.8182 - val_mse: 10.8182 - val_mae: 2.5499 - lr: 0.0067 - 1s/epoch - 3ms/step\n",
            "Epoch 3/500\n",
            "556/556 - 1s - loss: 33.2763 - mse: 33.2763 - mae: 1.6675 - val_loss: 13.9571 - val_mse: 13.9571 - val_mae: 2.9496 - lr: 0.0067 - 1s/epoch - 3ms/step\n",
            "Epoch 4/500\n",
            "556/556 - 1s - loss: 32.9248 - mse: 32.9248 - mae: 1.6482 - val_loss: 12.5348 - val_mse: 12.5348 - val_mae: 2.6122 - lr: 0.0067 - 1s/epoch - 2ms/step\n",
            "Epoch 5/500\n",
            "556/556 - 1s - loss: 32.5756 - mse: 32.5756 - mae: 1.6333 - val_loss: 12.3958 - val_mse: 12.3958 - val_mae: 2.4473 - lr: 0.0067 - 1s/epoch - 2ms/step\n",
            "Epoch 6/500\n",
            "556/556 - 1s - loss: 32.7498 - mse: 32.7498 - mae: 1.6384 - val_loss: 11.4116 - val_mse: 11.4116 - val_mae: 2.6625 - lr: 0.0067 - 1s/epoch - 2ms/step\n",
            "Score for inner fold : loss of 11.411626815795898\n",
            "-----Fold:1--------\n",
            "Epoch 1/500\n",
            "556/556 - 2s - loss: 12.5251 - mse: 12.5251 - mae: 1.6925 - val_loss: 10.2849 - val_mse: 10.2849 - val_mae: 2.5969 - lr: 0.0067 - 2s/epoch - 4ms/step\n",
            "Epoch 2/500\n",
            "556/556 - 1s - loss: 11.5885 - mse: 11.5885 - mae: 1.6353 - val_loss: 12.5240 - val_mse: 12.5240 - val_mae: 2.8099 - lr: 0.0067 - 1s/epoch - 2ms/step\n",
            "Epoch 3/500\n",
            "556/556 - 1s - loss: 11.3308 - mse: 11.3308 - mae: 1.6089 - val_loss: 12.0371 - val_mse: 12.0371 - val_mae: 2.7770 - lr: 0.0067 - 1s/epoch - 2ms/step\n",
            "Epoch 4/500\n",
            "556/556 - 1s - loss: 10.8180 - mse: 10.8180 - mae: 1.5967 - val_loss: 12.5378 - val_mse: 12.5378 - val_mae: 2.8836 - lr: 0.0067 - 1s/epoch - 3ms/step\n",
            "Epoch 5/500\n",
            "556/556 - 1s - loss: 10.7721 - mse: 10.7721 - mae: 1.5988 - val_loss: 14.0584 - val_mse: 14.0584 - val_mae: 2.7985 - lr: 0.0067 - 1s/epoch - 2ms/step\n",
            "Epoch 6/500\n",
            "556/556 - 1s - loss: 10.6636 - mse: 10.6636 - mae: 1.5869 - val_loss: 12.5665 - val_mse: 12.5665 - val_mae: 2.6944 - lr: 0.0067 - 1s/epoch - 2ms/step\n",
            "Score for inner fold : loss of 12.566479682922363\n",
            "-----Fold:2--------\n",
            "Epoch 1/500\n",
            "556/556 - 2s - loss: 33.0222 - mse: 33.0222 - mae: 1.6812 - val_loss: 9.5934 - val_mse: 9.5934 - val_mae: 2.4441 - lr: 0.0067 - 2s/epoch - 4ms/step\n",
            "Epoch 2/500\n",
            "556/556 - 1s - loss: 31.8935 - mse: 31.8935 - mae: 1.6115 - val_loss: 9.9662 - val_mse: 9.9662 - val_mae: 2.3399 - lr: 0.0067 - 1s/epoch - 2ms/step\n",
            "Epoch 3/500\n",
            "556/556 - 1s - loss: 31.7535 - mse: 31.7535 - mae: 1.5999 - val_loss: 13.9920 - val_mse: 13.9920 - val_mae: 2.8706 - lr: 0.0067 - 1s/epoch - 2ms/step\n",
            "Epoch 4/500\n",
            "556/556 - 1s - loss: 31.4979 - mse: 31.4979 - mae: 1.5837 - val_loss: 12.1445 - val_mse: 12.1445 - val_mae: 2.6861 - lr: 0.0067 - 1s/epoch - 3ms/step\n",
            "Epoch 5/500\n",
            "556/556 - 1s - loss: 31.0146 - mse: 31.0146 - mae: 1.5689 - val_loss: 15.0767 - val_mse: 15.0767 - val_mae: 2.8708 - lr: 0.0067 - 1s/epoch - 3ms/step\n",
            "Epoch 6/500\n",
            "556/556 - 1s - loss: 30.9287 - mse: 30.9287 - mae: 1.5772 - val_loss: 10.1108 - val_mse: 10.1108 - val_mae: 2.6122 - lr: 0.0067 - 1s/epoch - 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-10-10 09:43:00,684]\u001b[0m Finished trial#10 resulted in value: 11.363333333333335. Current best value is 9.623333333333333 with parameters: {'activation': 'tanh', 'num_hidden_layer': 3, 'i': 6, 'learning_rate': 0.00010907972060886967}.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Score for inner fold : loss of 10.110816955566406\n",
            "Trial Number:11\n",
            "-----Fold:0--------\n",
            "Epoch 1/500\n",
            "556/556 - 2s - loss: 32.8883 - mse: 32.8883 - mae: 1.6454 - val_loss: 11.7040 - val_mse: 11.7040 - val_mae: 2.7522 - lr: 0.0020 - 2s/epoch - 4ms/step\n",
            "Epoch 2/500\n",
            "556/556 - 1s - loss: 31.7600 - mse: 31.7600 - mae: 1.5829 - val_loss: 11.9358 - val_mse: 11.9358 - val_mae: 2.7707 - lr: 0.0020 - 1s/epoch - 2ms/step\n",
            "Epoch 3/500\n",
            "556/556 - 1s - loss: 31.3678 - mse: 31.3678 - mae: 1.5636 - val_loss: 12.0757 - val_mse: 12.0757 - val_mae: 2.8154 - lr: 0.0020 - 1s/epoch - 2ms/step\n",
            "Epoch 4/500\n",
            "556/556 - 1s - loss: 31.1142 - mse: 31.1142 - mae: 1.5520 - val_loss: 11.2518 - val_mse: 11.2518 - val_mae: 2.7209 - lr: 0.0020 - 1s/epoch - 2ms/step\n",
            "Epoch 5/500\n",
            "556/556 - 1s - loss: 30.9363 - mse: 30.9363 - mae: 1.5435 - val_loss: 13.1627 - val_mse: 13.1627 - val_mae: 2.8093 - lr: 0.0020 - 1s/epoch - 2ms/step\n",
            "Epoch 6/500\n",
            "556/556 - 1s - loss: 30.6519 - mse: 30.6519 - mae: 1.5334 - val_loss: 10.9989 - val_mse: 10.9989 - val_mae: 2.5466 - lr: 0.0020 - 1s/epoch - 2ms/step\n",
            "Epoch 7/500\n",
            "556/556 - 1s - loss: 30.5315 - mse: 30.5315 - mae: 1.5210 - val_loss: 12.8253 - val_mse: 12.8253 - val_mae: 2.7932 - lr: 0.0020 - 1s/epoch - 2ms/step\n",
            "Epoch 8/500\n",
            "556/556 - 1s - loss: 30.6087 - mse: 30.6087 - mae: 1.5173 - val_loss: 13.3402 - val_mse: 13.3402 - val_mae: 2.6654 - lr: 0.0020 - 1s/epoch - 2ms/step\n",
            "Epoch 9/500\n",
            "556/556 - 1s - loss: 30.3717 - mse: 30.3717 - mae: 1.5158 - val_loss: 11.3570 - val_mse: 11.3570 - val_mae: 2.5112 - lr: 0.0020 - 1s/epoch - 2ms/step\n",
            "Epoch 10/500\n",
            "556/556 - 1s - loss: 30.2276 - mse: 30.2276 - mae: 1.5133 - val_loss: 13.3361 - val_mse: 13.3361 - val_mae: 2.6603 - lr: 0.0020 - 1s/epoch - 2ms/step\n",
            "Epoch 11/500\n",
            "556/556 - 1s - loss: 30.3659 - mse: 30.3659 - mae: 1.5054 - val_loss: 12.3726 - val_mse: 12.3726 - val_mae: 2.7452 - lr: 0.0020 - 1s/epoch - 2ms/step\n",
            "Score for inner fold : loss of 12.372608184814453\n",
            "-----Fold:1--------\n",
            "Epoch 1/500\n",
            "556/556 - 2s - loss: 34.9752 - mse: 34.9752 - mae: 1.6851 - val_loss: 9.7616 - val_mse: 9.7616 - val_mae: 2.5546 - lr: 0.0020 - 2s/epoch - 4ms/step\n",
            "Epoch 2/500\n",
            "556/556 - 1s - loss: 33.6472 - mse: 33.6472 - mae: 1.6123 - val_loss: 11.3555 - val_mse: 11.3555 - val_mae: 2.6120 - lr: 0.0020 - 1s/epoch - 2ms/step\n",
            "Epoch 3/500\n",
            "556/556 - 1s - loss: 33.1759 - mse: 33.1759 - mae: 1.6004 - val_loss: 10.2458 - val_mse: 10.2458 - val_mae: 2.5208 - lr: 0.0020 - 1s/epoch - 2ms/step\n",
            "Epoch 4/500\n",
            "556/556 - 1s - loss: 32.8422 - mse: 32.8422 - mae: 1.5834 - val_loss: 11.4040 - val_mse: 11.4040 - val_mae: 2.7028 - lr: 0.0020 - 1s/epoch - 2ms/step\n",
            "Epoch 5/500\n",
            "556/556 - 1s - loss: 32.6609 - mse: 32.6609 - mae: 1.5818 - val_loss: 13.1992 - val_mse: 13.1992 - val_mae: 2.8603 - lr: 0.0020 - 1s/epoch - 2ms/step\n",
            "Epoch 6/500\n",
            "556/556 - 1s - loss: 32.3245 - mse: 32.3245 - mae: 1.5624 - val_loss: 11.8864 - val_mse: 11.8864 - val_mae: 2.6185 - lr: 0.0020 - 1s/epoch - 2ms/step\n",
            "Score for inner fold : loss of 11.88635540008545\n",
            "-----Fold:2--------\n",
            "Epoch 1/500\n",
            "556/556 - 2s - loss: 12.8814 - mse: 12.8814 - mae: 1.6440 - val_loss: 9.4313 - val_mse: 9.4313 - val_mae: 2.5717 - lr: 0.0020 - 2s/epoch - 4ms/step\n",
            "Epoch 2/500\n",
            "556/556 - 1s - loss: 11.7478 - mse: 11.7478 - mae: 1.5680 - val_loss: 10.8896 - val_mse: 10.8896 - val_mae: 2.7578 - lr: 0.0020 - 1s/epoch - 2ms/step\n",
            "Epoch 3/500\n",
            "556/556 - 1s - loss: 11.4442 - mse: 11.4442 - mae: 1.5614 - val_loss: 9.8104 - val_mse: 9.8104 - val_mae: 2.5749 - lr: 0.0020 - 1s/epoch - 2ms/step\n",
            "Epoch 4/500\n",
            "556/556 - 1s - loss: 11.1958 - mse: 11.1958 - mae: 1.5499 - val_loss: 10.7335 - val_mse: 10.7335 - val_mae: 2.5285 - lr: 0.0020 - 1s/epoch - 2ms/step\n",
            "Epoch 5/500\n",
            "556/556 - 1s - loss: 10.8732 - mse: 10.8732 - mae: 1.5399 - val_loss: 10.8502 - val_mse: 10.8502 - val_mae: 2.5811 - lr: 0.0020 - 1s/epoch - 2ms/step\n",
            "Epoch 6/500\n",
            "556/556 - 1s - loss: 10.7291 - mse: 10.7291 - mae: 1.5346 - val_loss: 10.3364 - val_mse: 10.3364 - val_mae: 2.4746 - lr: 0.0020 - 1s/epoch - 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-10-10 09:43:35,294]\u001b[0m Finished trial#11 resulted in value: 11.533333333333331. Current best value is 9.623333333333333 with parameters: {'activation': 'tanh', 'num_hidden_layer': 3, 'i': 6, 'learning_rate': 0.00010907972060886967}.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Score for inner fold : loss of 10.336363792419434\n",
            "Trial Number:12\n",
            "-----Fold:0--------\n",
            "Epoch 1/500\n",
            "556/556 - 2s - loss: 34.4990 - mse: 34.4990 - mae: 1.6527 - val_loss: 9.6356 - val_mse: 9.6356 - val_mae: 2.6145 - lr: 0.0015 - 2s/epoch - 4ms/step\n",
            "Epoch 2/500\n",
            "556/556 - 1s - loss: 33.4279 - mse: 33.4279 - mae: 1.5894 - val_loss: 11.8564 - val_mse: 11.8564 - val_mae: 2.7980 - lr: 0.0015 - 1s/epoch - 2ms/step\n",
            "Epoch 3/500\n",
            "556/556 - 1s - loss: 32.7999 - mse: 32.7999 - mae: 1.5699 - val_loss: 10.6605 - val_mse: 10.6605 - val_mae: 2.5872 - lr: 0.0015 - 1s/epoch - 2ms/step\n",
            "Epoch 4/500\n",
            "556/556 - 1s - loss: 32.5981 - mse: 32.5981 - mae: 1.5692 - val_loss: 11.1596 - val_mse: 11.1596 - val_mae: 2.4694 - lr: 0.0015 - 1s/epoch - 2ms/step\n",
            "Epoch 5/500\n",
            "556/556 - 1s - loss: 32.4145 - mse: 32.4145 - mae: 1.5531 - val_loss: 11.7639 - val_mse: 11.7639 - val_mae: 2.6689 - lr: 0.0015 - 1s/epoch - 2ms/step\n",
            "Epoch 6/500\n",
            "556/556 - 1s - loss: 32.1230 - mse: 32.1230 - mae: 1.5405 - val_loss: 13.2211 - val_mse: 13.2211 - val_mae: 2.8739 - lr: 0.0015 - 1s/epoch - 2ms/step\n",
            "Score for inner fold : loss of 13.221064567565918\n",
            "-----Fold:1--------\n",
            "Epoch 1/500\n",
            "556/556 - 2s - loss: 34.6614 - mse: 34.6614 - mae: 1.6584 - val_loss: 11.1875 - val_mse: 11.1875 - val_mae: 2.7258 - lr: 0.0015 - 2s/epoch - 4ms/step\n",
            "Epoch 2/500\n",
            "556/556 - 1s - loss: 33.2113 - mse: 33.2113 - mae: 1.5890 - val_loss: 12.7383 - val_mse: 12.7383 - val_mae: 2.7962 - lr: 0.0015 - 1s/epoch - 2ms/step\n",
            "Epoch 3/500\n",
            "556/556 - 1s - loss: 32.6959 - mse: 32.6959 - mae: 1.5747 - val_loss: 11.1896 - val_mse: 11.1896 - val_mae: 2.5763 - lr: 0.0015 - 1s/epoch - 2ms/step\n",
            "Epoch 4/500\n",
            "556/556 - 1s - loss: 32.3004 - mse: 32.3004 - mae: 1.5614 - val_loss: 11.2627 - val_mse: 11.2627 - val_mae: 2.6399 - lr: 0.0015 - 1s/epoch - 2ms/step\n",
            "Epoch 5/500\n",
            "556/556 - 1s - loss: 32.1635 - mse: 32.1635 - mae: 1.5546 - val_loss: 10.3616 - val_mse: 10.3616 - val_mae: 2.5830 - lr: 0.0015 - 1s/epoch - 2ms/step\n",
            "Epoch 6/500\n",
            "556/556 - 1s - loss: 31.9118 - mse: 31.9118 - mae: 1.5459 - val_loss: 11.4747 - val_mse: 11.4747 - val_mae: 2.7257 - lr: 0.0015 - 1s/epoch - 2ms/step\n",
            "Epoch 7/500\n",
            "556/556 - 1s - loss: 31.7200 - mse: 31.7200 - mae: 1.5436 - val_loss: 11.0684 - val_mse: 11.0684 - val_mae: 2.5511 - lr: 0.0015 - 1s/epoch - 2ms/step\n",
            "Epoch 8/500\n",
            "556/556 - 1s - loss: 31.5781 - mse: 31.5781 - mae: 1.5354 - val_loss: 10.7211 - val_mse: 10.7211 - val_mae: 2.5114 - lr: 0.0015 - 1s/epoch - 2ms/step\n",
            "Epoch 9/500\n",
            "556/556 - 1s - loss: 31.4072 - mse: 31.4072 - mae: 1.5382 - val_loss: 10.8690 - val_mse: 10.8690 - val_mae: 2.5557 - lr: 0.0015 - 1s/epoch - 2ms/step\n",
            "Epoch 10/500\n",
            "556/556 - 1s - loss: 31.2724 - mse: 31.2724 - mae: 1.5253 - val_loss: 12.6545 - val_mse: 12.6545 - val_mae: 2.8686 - lr: 0.0015 - 1s/epoch - 2ms/step\n",
            "Score for inner fold : loss of 12.65450668334961\n",
            "-----Fold:2--------\n",
            "Epoch 1/500\n",
            "556/556 - 2s - loss: 12.0644 - mse: 12.0644 - mae: 1.6480 - val_loss: 8.7015 - val_mse: 8.7015 - val_mae: 2.4245 - lr: 0.0015 - 2s/epoch - 4ms/step\n",
            "Epoch 2/500\n",
            "556/556 - 1s - loss: 10.9257 - mse: 10.9257 - mae: 1.5610 - val_loss: 9.4441 - val_mse: 9.4441 - val_mae: 2.4076 - lr: 0.0015 - 1s/epoch - 2ms/step\n",
            "Epoch 3/500\n",
            "556/556 - 1s - loss: 10.5816 - mse: 10.5816 - mae: 1.5504 - val_loss: 10.8703 - val_mse: 10.8703 - val_mae: 2.6294 - lr: 0.0015 - 1s/epoch - 2ms/step\n",
            "Epoch 4/500\n",
            "556/556 - 1s - loss: 10.3255 - mse: 10.3255 - mae: 1.5406 - val_loss: 11.6616 - val_mse: 11.6616 - val_mae: 2.5997 - lr: 0.0015 - 1s/epoch - 2ms/step\n",
            "Epoch 5/500\n",
            "556/556 - 1s - loss: 10.1601 - mse: 10.1601 - mae: 1.5358 - val_loss: 10.1570 - val_mse: 10.1570 - val_mae: 2.5162 - lr: 0.0015 - 1s/epoch - 2ms/step\n",
            "Epoch 6/500\n",
            "556/556 - 1s - loss: 9.9891 - mse: 9.9891 - mae: 1.5223 - val_loss: 10.9466 - val_mse: 10.9466 - val_mae: 2.4319 - lr: 0.0015 - 1s/epoch - 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-10-10 09:44:08,662]\u001b[0m Finished trial#12 resulted in value: 12.273333333333333. Current best value is 9.623333333333333 with parameters: {'activation': 'tanh', 'num_hidden_layer': 3, 'i': 6, 'learning_rate': 0.00010907972060886967}.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Score for inner fold : loss of 10.946603775024414\n",
            "Trial Number:13\n",
            "-----Fold:0--------\n",
            "Epoch 1/500\n",
            "556/556 - 2s - loss: 35.9091 - mse: 35.9091 - mae: 1.7204 - val_loss: 9.0068 - val_mse: 9.0068 - val_mae: 2.5799 - lr: 0.0037 - 2s/epoch - 3ms/step\n",
            "Epoch 2/500\n",
            "556/556 - 1s - loss: 33.7556 - mse: 33.7556 - mae: 1.6032 - val_loss: 10.1544 - val_mse: 10.1544 - val_mae: 2.6694 - lr: 0.0037 - 1s/epoch - 2ms/step\n",
            "Epoch 3/500\n",
            "556/556 - 1s - loss: 32.9789 - mse: 32.9789 - mae: 1.5834 - val_loss: 9.8531 - val_mse: 9.8531 - val_mae: 2.5879 - lr: 0.0037 - 997ms/epoch - 2ms/step\n",
            "Epoch 4/500\n",
            "556/556 - 1s - loss: 32.5172 - mse: 32.5172 - mae: 1.5749 - val_loss: 10.5279 - val_mse: 10.5279 - val_mae: 2.5607 - lr: 0.0037 - 1s/epoch - 2ms/step\n",
            "Epoch 5/500\n",
            "556/556 - 1s - loss: 32.3098 - mse: 32.3098 - mae: 1.5663 - val_loss: 10.6089 - val_mse: 10.6089 - val_mae: 2.6510 - lr: 0.0037 - 983ms/epoch - 2ms/step\n",
            "Epoch 6/500\n",
            "556/556 - 1s - loss: 32.0255 - mse: 32.0255 - mae: 1.5601 - val_loss: 11.3408 - val_mse: 11.3408 - val_mae: 2.7139 - lr: 0.0037 - 1s/epoch - 2ms/step\n",
            "Score for inner fold : loss of 11.340818405151367\n",
            "-----Fold:1--------\n",
            "Epoch 1/500\n",
            "556/556 - 2s - loss: 35.5610 - mse: 35.5610 - mae: 1.7251 - val_loss: 8.6289 - val_mse: 8.6289 - val_mae: 2.5462 - lr: 0.0037 - 2s/epoch - 3ms/step\n",
            "Epoch 2/500\n",
            "556/556 - 1s - loss: 33.5516 - mse: 33.5516 - mae: 1.6077 - val_loss: 9.1634 - val_mse: 9.1634 - val_mae: 2.5256 - lr: 0.0037 - 1s/epoch - 2ms/step\n",
            "Epoch 3/500\n",
            "556/556 - 1s - loss: 32.9040 - mse: 32.9040 - mae: 1.5855 - val_loss: 8.4304 - val_mse: 8.4304 - val_mae: 2.3923 - lr: 0.0037 - 999ms/epoch - 2ms/step\n",
            "Epoch 4/500\n",
            "556/556 - 1s - loss: 32.5605 - mse: 32.5605 - mae: 1.5695 - val_loss: 10.2882 - val_mse: 10.2882 - val_mae: 2.6322 - lr: 0.0037 - 991ms/epoch - 2ms/step\n",
            "Epoch 5/500\n",
            "556/556 - 1s - loss: 32.2978 - mse: 32.2978 - mae: 1.5617 - val_loss: 9.8823 - val_mse: 9.8823 - val_mae: 2.5433 - lr: 0.0037 - 1s/epoch - 2ms/step\n",
            "Epoch 6/500\n",
            "556/556 - 1s - loss: 32.0143 - mse: 32.0143 - mae: 1.5481 - val_loss: 11.1822 - val_mse: 11.1822 - val_mae: 2.7050 - lr: 0.0037 - 967ms/epoch - 2ms/step\n",
            "Epoch 7/500\n",
            "556/556 - 1s - loss: 31.8168 - mse: 31.8168 - mae: 1.5431 - val_loss: 10.0425 - val_mse: 10.0425 - val_mae: 2.4857 - lr: 0.0037 - 984ms/epoch - 2ms/step\n",
            "Epoch 8/500\n",
            "556/556 - 1s - loss: 31.6610 - mse: 31.6610 - mae: 1.5323 - val_loss: 9.6920 - val_mse: 9.6920 - val_mae: 2.5139 - lr: 0.0037 - 1s/epoch - 2ms/step\n",
            "Score for inner fold : loss of 9.692052841186523\n",
            "-----Fold:2--------\n",
            "Epoch 1/500\n",
            "556/556 - 2s - loss: 14.9318 - mse: 14.9318 - mae: 1.6942 - val_loss: 7.8367 - val_mse: 7.8367 - val_mae: 2.3917 - lr: 0.0037 - 2s/epoch - 3ms/step\n",
            "Epoch 2/500\n",
            "556/556 - 1s - loss: 12.8730 - mse: 12.8730 - mae: 1.5771 - val_loss: 8.8604 - val_mse: 8.8604 - val_mae: 2.5671 - lr: 0.0037 - 1s/epoch - 2ms/step\n",
            "Epoch 3/500\n",
            "556/556 - 1s - loss: 12.3425 - mse: 12.3425 - mae: 1.5577 - val_loss: 10.8604 - val_mse: 10.8604 - val_mae: 2.7484 - lr: 0.0037 - 1s/epoch - 2ms/step\n",
            "Epoch 4/500\n",
            "556/556 - 1s - loss: 11.9787 - mse: 11.9787 - mae: 1.5432 - val_loss: 10.6367 - val_mse: 10.6367 - val_mae: 2.7059 - lr: 0.0037 - 1s/epoch - 2ms/step\n",
            "Epoch 5/500\n",
            "556/556 - 1s - loss: 11.7946 - mse: 11.7946 - mae: 1.5396 - val_loss: 9.9045 - val_mse: 9.9045 - val_mae: 2.5337 - lr: 0.0037 - 987ms/epoch - 2ms/step\n",
            "Epoch 6/500\n",
            "556/556 - 1s - loss: 11.6046 - mse: 11.6046 - mae: 1.5316 - val_loss: 10.4422 - val_mse: 10.4422 - val_mae: 2.6723 - lr: 0.0037 - 992ms/epoch - 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-10-10 09:44:34,300]\u001b[0m Finished trial#13 resulted in value: 10.49. Current best value is 9.623333333333333 with parameters: {'activation': 'tanh', 'num_hidden_layer': 3, 'i': 6, 'learning_rate': 0.00010907972060886967}.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Score for inner fold : loss of 10.44217300415039\n",
            "Trial Number:14\n",
            "-----Fold:0--------\n",
            "Epoch 1/500\n",
            "556/556 - 2s - loss: 35.6403 - mse: 35.6403 - mae: 1.7279 - val_loss: 9.2807 - val_mse: 9.2807 - val_mae: 2.6187 - lr: 8.1753e-04 - 2s/epoch - 4ms/step\n",
            "Epoch 2/500\n",
            "556/556 - 1s - loss: 33.8219 - mse: 33.8219 - mae: 1.5993 - val_loss: 9.1377 - val_mse: 9.1377 - val_mae: 2.5323 - lr: 8.1753e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 3/500\n",
            "556/556 - 1s - loss: 33.3623 - mse: 33.3623 - mae: 1.5774 - val_loss: 9.6658 - val_mse: 9.6658 - val_mae: 2.6010 - lr: 8.1753e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 4/500\n",
            "556/556 - 1s - loss: 33.0025 - mse: 33.0025 - mae: 1.5635 - val_loss: 10.4048 - val_mse: 10.4048 - val_mae: 2.7221 - lr: 8.1753e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 5/500\n",
            "556/556 - 1s - loss: 32.7554 - mse: 32.7554 - mae: 1.5590 - val_loss: 10.3459 - val_mse: 10.3459 - val_mae: 2.5954 - lr: 8.1753e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 6/500\n",
            "556/556 - 1s - loss: 32.6226 - mse: 32.6226 - mae: 1.5498 - val_loss: 9.8571 - val_mse: 9.8571 - val_mae: 2.5087 - lr: 8.1753e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 7/500\n",
            "556/556 - 1s - loss: 32.4534 - mse: 32.4534 - mae: 1.5453 - val_loss: 11.5183 - val_mse: 11.5183 - val_mae: 2.7625 - lr: 8.1753e-04 - 1s/epoch - 2ms/step\n",
            "Score for inner fold : loss of 11.518326759338379\n",
            "-----Fold:1--------\n",
            "Epoch 1/500\n",
            "556/556 - 2s - loss: 35.2751 - mse: 35.2751 - mae: 1.7268 - val_loss: 8.7983 - val_mse: 8.7983 - val_mae: 2.4818 - lr: 8.1753e-04 - 2s/epoch - 4ms/step\n",
            "Epoch 2/500\n",
            "556/556 - 1s - loss: 33.3901 - mse: 33.3901 - mae: 1.6017 - val_loss: 9.5270 - val_mse: 9.5270 - val_mae: 2.5831 - lr: 8.1753e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 3/500\n",
            "556/556 - 1s - loss: 32.8335 - mse: 32.8335 - mae: 1.5809 - val_loss: 8.6622 - val_mse: 8.6622 - val_mae: 2.3269 - lr: 8.1753e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 4/500\n",
            "556/556 - 1s - loss: 32.4647 - mse: 32.4647 - mae: 1.5678 - val_loss: 10.8596 - val_mse: 10.8596 - val_mae: 2.7428 - lr: 8.1753e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 5/500\n",
            "556/556 - 1s - loss: 32.2530 - mse: 32.2530 - mae: 1.5611 - val_loss: 10.0266 - val_mse: 10.0266 - val_mae: 2.4967 - lr: 8.1753e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 6/500\n",
            "556/556 - 1s - loss: 32.0092 - mse: 32.0092 - mae: 1.5511 - val_loss: 10.6546 - val_mse: 10.6546 - val_mae: 2.6943 - lr: 8.1753e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 7/500\n",
            "556/556 - 1s - loss: 31.8534 - mse: 31.8534 - mae: 1.5525 - val_loss: 11.3614 - val_mse: 11.3614 - val_mae: 2.7507 - lr: 8.1753e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 8/500\n",
            "556/556 - 1s - loss: 31.7429 - mse: 31.7429 - mae: 1.5485 - val_loss: 10.5846 - val_mse: 10.5846 - val_mae: 2.5419 - lr: 8.1753e-04 - 1s/epoch - 2ms/step\n",
            "Score for inner fold : loss of 10.58456039428711\n",
            "-----Fold:2--------\n",
            "Epoch 1/500\n",
            "556/556 - 2s - loss: 14.2251 - mse: 14.2251 - mae: 1.6861 - val_loss: 8.9141 - val_mse: 8.9141 - val_mae: 2.5566 - lr: 8.1753e-04 - 2s/epoch - 4ms/step\n",
            "Epoch 2/500\n",
            "556/556 - 1s - loss: 12.6898 - mse: 12.6898 - mae: 1.5625 - val_loss: 8.9102 - val_mse: 8.9102 - val_mae: 2.5188 - lr: 8.1753e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 3/500\n",
            "556/556 - 1s - loss: 12.2103 - mse: 12.2103 - mae: 1.5332 - val_loss: 9.8117 - val_mse: 9.8117 - val_mae: 2.6190 - lr: 8.1753e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 4/500\n",
            "556/556 - 1s - loss: 11.8995 - mse: 11.8995 - mae: 1.5281 - val_loss: 9.8976 - val_mse: 9.8976 - val_mae: 2.6183 - lr: 8.1753e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 5/500\n",
            "556/556 - 1s - loss: 11.6451 - mse: 11.6451 - mae: 1.5226 - val_loss: 10.0101 - val_mse: 10.0101 - val_mae: 2.5513 - lr: 8.1753e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 6/500\n",
            "556/556 - 1s - loss: 11.4812 - mse: 11.4812 - mae: 1.5142 - val_loss: 10.1829 - val_mse: 10.1829 - val_mae: 2.5539 - lr: 8.1753e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 7/500\n",
            "556/556 - 1s - loss: 11.3573 - mse: 11.3573 - mae: 1.5194 - val_loss: 11.2063 - val_mse: 11.2063 - val_mae: 2.7504 - lr: 8.1753e-04 - 1s/epoch - 2ms/step\n",
            "Score for inner fold : loss of 11.206326484680176\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-10-10 09:45:06,765]\u001b[0m Finished trial#14 resulted in value: 11.103333333333333. Current best value is 9.623333333333333 with parameters: {'activation': 'tanh', 'num_hidden_layer': 3, 'i': 6, 'learning_rate': 0.00010907972060886967}.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trial Number:15\n",
            "-----Fold:0--------\n",
            "Epoch 1/500\n",
            "556/556 - 2s - loss: 33.7287 - mse: 33.7287 - mae: 1.7067 - val_loss: 10.6693 - val_mse: 10.6693 - val_mae: 2.7612 - lr: 0.0072 - 2s/epoch - 4ms/step\n",
            "Epoch 2/500\n",
            "556/556 - 1s - loss: 32.6115 - mse: 32.6115 - mae: 1.6309 - val_loss: 14.5398 - val_mse: 14.5398 - val_mae: 3.0601 - lr: 0.0072 - 1s/epoch - 3ms/step\n",
            "Epoch 3/500\n",
            "556/556 - 1s - loss: 32.1821 - mse: 32.1821 - mae: 1.6341 - val_loss: 16.8527 - val_mse: 16.8527 - val_mae: 3.2703 - lr: 0.0072 - 1s/epoch - 3ms/step\n",
            "Epoch 4/500\n",
            "556/556 - 1s - loss: 31.7736 - mse: 31.7736 - mae: 1.6094 - val_loss: 10.5160 - val_mse: 10.5160 - val_mae: 2.4026 - lr: 0.0072 - 1s/epoch - 3ms/step\n",
            "Epoch 5/500\n",
            "556/556 - 1s - loss: 31.6728 - mse: 31.6728 - mae: 1.5912 - val_loss: 11.5308 - val_mse: 11.5308 - val_mae: 2.5752 - lr: 0.0072 - 1s/epoch - 3ms/step\n",
            "Epoch 6/500\n",
            "556/556 - 1s - loss: 31.3549 - mse: 31.3549 - mae: 1.5643 - val_loss: 16.6856 - val_mse: 16.6856 - val_mae: 3.2410 - lr: 0.0072 - 1s/epoch - 3ms/step\n",
            "Epoch 7/500\n",
            "556/556 - 1s - loss: 31.3701 - mse: 31.3701 - mae: 1.5764 - val_loss: 12.0983 - val_mse: 12.0983 - val_mae: 2.6686 - lr: 0.0072 - 1s/epoch - 3ms/step\n",
            "Epoch 8/500\n",
            "556/556 - 1s - loss: 31.2118 - mse: 31.2118 - mae: 1.5626 - val_loss: 14.2167 - val_mse: 14.2167 - val_mae: 3.0862 - lr: 0.0072 - 1s/epoch - 2ms/step\n",
            "Epoch 9/500\n",
            "556/556 - 1s - loss: 31.1038 - mse: 31.1038 - mae: 1.5536 - val_loss: 11.4798 - val_mse: 11.4798 - val_mae: 2.5165 - lr: 0.0072 - 1s/epoch - 2ms/step\n",
            "Score for inner fold : loss of 11.479850769042969\n",
            "-----Fold:1--------\n",
            "Epoch 1/500\n",
            "556/556 - 2s - loss: 33.0961 - mse: 33.0961 - mae: 1.7097 - val_loss: 12.2177 - val_mse: 12.2177 - val_mae: 2.7408 - lr: 0.0072 - 2s/epoch - 4ms/step\n",
            "Epoch 2/500\n",
            "556/556 - 1s - loss: 31.8037 - mse: 31.8037 - mae: 1.6328 - val_loss: 11.2682 - val_mse: 11.2682 - val_mae: 2.6403 - lr: 0.0072 - 1s/epoch - 3ms/step\n",
            "Epoch 3/500\n",
            "556/556 - 1s - loss: 31.4726 - mse: 31.4726 - mae: 1.6140 - val_loss: 12.2898 - val_mse: 12.2898 - val_mae: 2.7109 - lr: 0.0072 - 1s/epoch - 3ms/step\n",
            "Epoch 4/500\n",
            "556/556 - 1s - loss: 31.2457 - mse: 31.2457 - mae: 1.6115 - val_loss: 10.6519 - val_mse: 10.6519 - val_mae: 2.7311 - lr: 0.0072 - 1s/epoch - 3ms/step\n",
            "Epoch 5/500\n",
            "556/556 - 1s - loss: 31.1846 - mse: 31.1846 - mae: 1.5866 - val_loss: 13.1006 - val_mse: 13.1006 - val_mae: 2.7241 - lr: 0.0072 - 1s/epoch - 3ms/step\n",
            "Epoch 6/500\n",
            "556/556 - 1s - loss: 30.9506 - mse: 30.9506 - mae: 1.5792 - val_loss: 12.3748 - val_mse: 12.3748 - val_mae: 2.7575 - lr: 0.0072 - 1s/epoch - 2ms/step\n",
            "Epoch 7/500\n",
            "556/556 - 1s - loss: 30.6349 - mse: 30.6349 - mae: 1.5704 - val_loss: 10.5677 - val_mse: 10.5677 - val_mae: 2.4693 - lr: 0.0072 - 1s/epoch - 3ms/step\n",
            "Epoch 8/500\n",
            "556/556 - 1s - loss: 30.5413 - mse: 30.5413 - mae: 1.5642 - val_loss: 15.7427 - val_mse: 15.7427 - val_mae: 3.0469 - lr: 0.0072 - 1s/epoch - 3ms/step\n",
            "Epoch 9/500\n",
            "556/556 - 1s - loss: 30.7386 - mse: 30.7386 - mae: 1.5751 - val_loss: 12.8423 - val_mse: 12.8423 - val_mae: 2.6569 - lr: 0.0072 - 1s/epoch - 3ms/step\n",
            "Epoch 10/500\n",
            "556/556 - 1s - loss: 30.6051 - mse: 30.6051 - mae: 1.5721 - val_loss: 11.1343 - val_mse: 11.1343 - val_mae: 2.4530 - lr: 0.0072 - 1s/epoch - 3ms/step\n",
            "Epoch 11/500\n",
            "556/556 - 1s - loss: 30.4460 - mse: 30.4460 - mae: 1.5724 - val_loss: 13.4563 - val_mse: 13.4563 - val_mae: 2.8220 - lr: 0.0072 - 1s/epoch - 3ms/step\n",
            "Epoch 12/500\n",
            "556/556 - 1s - loss: 30.4387 - mse: 30.4387 - mae: 1.5587 - val_loss: 11.8584 - val_mse: 11.8584 - val_mae: 2.5835 - lr: 0.0072 - 1s/epoch - 3ms/step\n",
            "Score for inner fold : loss of 11.858410835266113\n",
            "-----Fold:2--------\n",
            "Epoch 1/500\n",
            "556/556 - 2s - loss: 13.2062 - mse: 13.2062 - mae: 1.6890 - val_loss: 11.8473 - val_mse: 11.8473 - val_mae: 2.6808 - lr: 0.0072 - 2s/epoch - 4ms/step\n",
            "Epoch 2/500\n",
            "556/556 - 1s - loss: 12.3638 - mse: 12.3638 - mae: 1.6212 - val_loss: 8.6031 - val_mse: 8.6031 - val_mae: 2.2367 - lr: 0.0072 - 1s/epoch - 3ms/step\n",
            "Epoch 3/500\n",
            "556/556 - 1s - loss: 11.9936 - mse: 11.9936 - mae: 1.5930 - val_loss: 8.5054 - val_mse: 8.5054 - val_mae: 2.2421 - lr: 0.0072 - 1s/epoch - 3ms/step\n",
            "Epoch 4/500\n",
            "556/556 - 1s - loss: 11.7113 - mse: 11.7113 - mae: 1.5725 - val_loss: 10.9445 - val_mse: 10.9445 - val_mae: 2.7028 - lr: 0.0072 - 1s/epoch - 3ms/step\n",
            "Epoch 5/500\n",
            "556/556 - 1s - loss: 11.3706 - mse: 11.3706 - mae: 1.5585 - val_loss: 12.4716 - val_mse: 12.4716 - val_mae: 2.8090 - lr: 0.0072 - 1s/epoch - 3ms/step\n",
            "Epoch 6/500\n",
            "556/556 - 1s - loss: 11.1550 - mse: 11.1550 - mae: 1.5588 - val_loss: 10.8041 - val_mse: 10.8041 - val_mae: 2.6101 - lr: 0.0072 - 1s/epoch - 2ms/step\n",
            "Epoch 7/500\n",
            "556/556 - 1s - loss: 10.9088 - mse: 10.9088 - mae: 1.5428 - val_loss: 12.7619 - val_mse: 12.7619 - val_mae: 3.0356 - lr: 0.0072 - 1s/epoch - 2ms/step\n",
            "Epoch 8/500\n",
            "556/556 - 1s - loss: 11.0745 - mse: 11.0745 - mae: 1.5404 - val_loss: 12.1159 - val_mse: 12.1159 - val_mae: 2.3887 - lr: 0.0072 - 1s/epoch - 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-10-10 09:45:53,146]\u001b[0m Finished trial#15 resulted in value: 11.82. Current best value is 9.623333333333333 with parameters: {'activation': 'tanh', 'num_hidden_layer': 3, 'i': 6, 'learning_rate': 0.00010907972060886967}.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Score for inner fold : loss of 12.11593246459961\n",
            "Trial Number:16\n",
            "-----Fold:0--------\n",
            "Epoch 1/500\n",
            "556/556 - 2s - loss: 37.5428 - mse: 37.5428 - mae: 1.8461 - val_loss: 7.7725 - val_mse: 7.7725 - val_mae: 2.4872 - lr: 0.0011 - 2s/epoch - 3ms/step\n",
            "Epoch 2/500\n",
            "556/556 - 1s - loss: 35.1399 - mse: 35.1399 - mae: 1.6679 - val_loss: 8.6529 - val_mse: 8.6529 - val_mae: 2.5798 - lr: 0.0011 - 1s/epoch - 2ms/step\n",
            "Epoch 3/500\n",
            "556/556 - 1s - loss: 34.4173 - mse: 34.4173 - mae: 1.6260 - val_loss: 8.8545 - val_mse: 8.8545 - val_mae: 2.5764 - lr: 0.0011 - 1s/epoch - 2ms/step\n",
            "Epoch 4/500\n",
            "556/556 - 1s - loss: 33.9342 - mse: 33.9342 - mae: 1.6092 - val_loss: 8.9202 - val_mse: 8.9202 - val_mae: 2.5170 - lr: 0.0011 - 1s/epoch - 2ms/step\n",
            "Epoch 5/500\n",
            "556/556 - 1s - loss: 33.5589 - mse: 33.5589 - mae: 1.5915 - val_loss: 10.0806 - val_mse: 10.0806 - val_mae: 2.7123 - lr: 0.0011 - 1s/epoch - 2ms/step\n",
            "Epoch 6/500\n",
            "556/556 - 1s - loss: 33.3215 - mse: 33.3215 - mae: 1.5807 - val_loss: 8.9865 - val_mse: 8.9865 - val_mae: 2.5252 - lr: 0.0011 - 1s/epoch - 2ms/step\n",
            "Score for inner fold : loss of 8.986503601074219\n",
            "-----Fold:1--------\n",
            "Epoch 1/500\n",
            "556/556 - 2s - loss: 37.7050 - mse: 37.7050 - mae: 1.8537 - val_loss: 8.3842 - val_mse: 8.3842 - val_mae: 2.6133 - lr: 0.0011 - 2s/epoch - 3ms/step\n",
            "Epoch 2/500\n",
            "556/556 - 1s - loss: 35.0712 - mse: 35.0712 - mae: 1.6744 - val_loss: 8.2765 - val_mse: 8.2765 - val_mae: 2.5378 - lr: 0.0011 - 1s/epoch - 2ms/step\n",
            "Epoch 3/500\n",
            "556/556 - 1s - loss: 34.4059 - mse: 34.4059 - mae: 1.6357 - val_loss: 9.6559 - val_mse: 9.6559 - val_mae: 2.7006 - lr: 0.0011 - 1s/epoch - 2ms/step\n",
            "Epoch 4/500\n",
            "556/556 - 1s - loss: 33.9094 - mse: 33.9094 - mae: 1.6165 - val_loss: 8.4006 - val_mse: 8.4006 - val_mae: 2.4720 - lr: 0.0011 - 981ms/epoch - 2ms/step\n",
            "Epoch 5/500\n",
            "556/556 - 1s - loss: 33.5715 - mse: 33.5715 - mae: 1.5963 - val_loss: 9.0783 - val_mse: 9.0783 - val_mae: 2.5550 - lr: 0.0011 - 1s/epoch - 2ms/step\n",
            "Epoch 6/500\n",
            "556/556 - 1s - loss: 33.2704 - mse: 33.2704 - mae: 1.5861 - val_loss: 9.2038 - val_mse: 9.2038 - val_mae: 2.5446 - lr: 0.0011 - 1s/epoch - 2ms/step\n",
            "Epoch 7/500\n",
            "556/556 - 1s - loss: 33.0138 - mse: 33.0138 - mae: 1.5775 - val_loss: 9.7249 - val_mse: 9.7249 - val_mae: 2.5892 - lr: 0.0011 - 1s/epoch - 2ms/step\n",
            "Score for inner fold : loss of 9.724899291992188\n",
            "-----Fold:2--------\n",
            "Epoch 1/500\n",
            "556/556 - 2s - loss: 16.9823 - mse: 16.9823 - mae: 1.7750 - val_loss: 7.5691 - val_mse: 7.5691 - val_mae: 2.4512 - lr: 0.0011 - 2s/epoch - 3ms/step\n",
            "Epoch 2/500\n",
            "556/556 - 1s - loss: 14.6705 - mse: 14.6705 - mae: 1.6407 - val_loss: 8.6597 - val_mse: 8.6597 - val_mae: 2.5786 - lr: 0.0011 - 1s/epoch - 2ms/step\n",
            "Epoch 3/500\n",
            "556/556 - 1s - loss: 13.9758 - mse: 13.9758 - mae: 1.6073 - val_loss: 8.4471 - val_mse: 8.4471 - val_mae: 2.5148 - lr: 0.0011 - 1s/epoch - 2ms/step\n",
            "Epoch 4/500\n",
            "556/556 - 1s - loss: 13.5050 - mse: 13.5050 - mae: 1.5836 - val_loss: 9.2787 - val_mse: 9.2787 - val_mae: 2.5973 - lr: 0.0011 - 1s/epoch - 2ms/step\n",
            "Epoch 5/500\n",
            "556/556 - 1s - loss: 13.0816 - mse: 13.0816 - mae: 1.5625 - val_loss: 8.7192 - val_mse: 8.7192 - val_mae: 2.4502 - lr: 0.0011 - 1s/epoch - 2ms/step\n",
            "Epoch 6/500\n",
            "556/556 - 1s - loss: 12.7716 - mse: 12.7716 - mae: 1.5485 - val_loss: 9.4220 - val_mse: 9.4220 - val_mae: 2.5413 - lr: 0.0011 - 1s/epoch - 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-10-10 09:46:18,801]\u001b[0m Finished trial#16 resulted in value: 9.376666666666667. Current best value is 9.376666666666667 with parameters: {'activation': 'tanh', 'num_hidden_layer': 4, 'i': 3, 'learning_rate': 0.001073394009891707}.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Score for inner fold : loss of 9.421950340270996\n",
            "Trial Number:17\n",
            "-----Fold:0--------\n",
            "Epoch 1/500\n",
            "556/556 - 2s - loss: 18.7881 - mse: 18.7881 - mae: 2.0372 - val_loss: 5.9064 - val_mse: 5.9064 - val_mae: 2.1880 - lr: 5.4909e-04 - 2s/epoch - 3ms/step\n",
            "Epoch 2/500\n",
            "556/556 - 1s - loss: 14.1256 - mse: 14.1256 - mae: 1.6623 - val_loss: 8.1476 - val_mse: 8.1476 - val_mae: 2.5520 - lr: 5.4909e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 3/500\n",
            "556/556 - 1s - loss: 13.4909 - mse: 13.4909 - mae: 1.6310 - val_loss: 8.4342 - val_mse: 8.4342 - val_mae: 2.5755 - lr: 5.4909e-04 - 998ms/epoch - 2ms/step\n",
            "Epoch 4/500\n",
            "556/556 - 1s - loss: 13.0303 - mse: 13.0303 - mae: 1.6062 - val_loss: 8.3280 - val_mse: 8.3280 - val_mae: 2.5355 - lr: 5.4909e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 5/500\n",
            "556/556 - 1s - loss: 12.6662 - mse: 12.6662 - mae: 1.5719 - val_loss: 8.4465 - val_mse: 8.4465 - val_mae: 2.5285 - lr: 5.4909e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 6/500\n",
            "556/556 - 1s - loss: 12.3893 - mse: 12.3893 - mae: 1.5540 - val_loss: 9.0055 - val_mse: 9.0055 - val_mae: 2.5752 - lr: 5.4909e-04 - 1s/epoch - 2ms/step\n",
            "Score for inner fold : loss of 9.00547981262207\n",
            "-----Fold:1--------\n",
            "Epoch 1/500\n",
            "556/556 - 2s - loss: 40.3567 - mse: 40.3567 - mae: 1.9810 - val_loss: 7.1922 - val_mse: 7.1922 - val_mae: 2.4012 - lr: 5.4909e-04 - 2s/epoch - 3ms/step\n",
            "Epoch 2/500\n",
            "556/556 - 1s - loss: 36.2226 - mse: 36.2226 - mae: 1.6803 - val_loss: 7.9705 - val_mse: 7.9705 - val_mae: 2.5240 - lr: 5.4909e-04 - 995ms/epoch - 2ms/step\n",
            "Epoch 3/500\n",
            "556/556 - 1s - loss: 35.6785 - mse: 35.6785 - mae: 1.6445 - val_loss: 8.2355 - val_mse: 8.2355 - val_mae: 2.5252 - lr: 5.4909e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 4/500\n",
            "556/556 - 1s - loss: 35.3197 - mse: 35.3197 - mae: 1.6276 - val_loss: 8.4019 - val_mse: 8.4019 - val_mae: 2.5287 - lr: 5.4909e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 5/500\n",
            "556/556 - 1s - loss: 35.0398 - mse: 35.0398 - mae: 1.6144 - val_loss: 8.8108 - val_mse: 8.8108 - val_mae: 2.5509 - lr: 5.4909e-04 - 961ms/epoch - 2ms/step\n",
            "Epoch 6/500\n",
            "556/556 - 1s - loss: 34.8350 - mse: 34.8350 - mae: 1.6069 - val_loss: 8.9949 - val_mse: 8.9949 - val_mae: 2.5845 - lr: 5.4909e-04 - 994ms/epoch - 2ms/step\n",
            "Score for inner fold : loss of 8.994916915893555\n",
            "-----Fold:2--------\n",
            "Epoch 1/500\n",
            "556/556 - 2s - loss: 40.4059 - mse: 40.4059 - mae: 2.0332 - val_loss: 6.7343 - val_mse: 6.7343 - val_mae: 2.3321 - lr: 5.4909e-04 - 2s/epoch - 3ms/step\n",
            "Epoch 2/500\n",
            "556/556 - 1s - loss: 36.0073 - mse: 36.0073 - mae: 1.7068 - val_loss: 7.8717 - val_mse: 7.8717 - val_mae: 2.5031 - lr: 5.4909e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 3/500\n",
            "556/556 - 1s - loss: 35.3850 - mse: 35.3850 - mae: 1.6685 - val_loss: 8.3195 - val_mse: 8.3195 - val_mae: 2.5351 - lr: 5.4909e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 4/500\n",
            "556/556 - 1s - loss: 35.0152 - mse: 35.0152 - mae: 1.6571 - val_loss: 8.7462 - val_mse: 8.7462 - val_mae: 2.5900 - lr: 5.4909e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 5/500\n",
            "556/556 - 1s - loss: 34.7558 - mse: 34.7558 - mae: 1.6460 - val_loss: 8.8546 - val_mse: 8.8546 - val_mae: 2.5728 - lr: 5.4909e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 6/500\n",
            "556/556 - 1s - loss: 34.5323 - mse: 34.5323 - mae: 1.6365 - val_loss: 9.0323 - val_mse: 9.0323 - val_mae: 2.5911 - lr: 5.4909e-04 - 987ms/epoch - 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-10-10 09:46:42,590]\u001b[0m Finished trial#17 resulted in value: 9.01. Current best value is 9.01 with parameters: {'activation': 'tanh', 'num_hidden_layer': 3, 'i': 3, 'learning_rate': 0.0005490914910440853}.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Score for inner fold : loss of 9.032258987426758\n",
            "Trial Number:18\n",
            "-----Fold:0--------\n",
            "Epoch 1/500\n",
            "556/556 - 2s - loss: 18.1453 - mse: 18.1453 - mae: 1.9385 - val_loss: 8.0215 - val_mse: 8.0215 - val_mae: 2.5648 - lr: 4.7311e-04 - 2s/epoch - 3ms/step\n",
            "Epoch 2/500\n",
            "556/556 - 1s - loss: 14.9948 - mse: 14.9948 - mae: 1.7000 - val_loss: 7.9745 - val_mse: 7.9745 - val_mae: 2.5269 - lr: 4.7311e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 3/500\n",
            "556/556 - 1s - loss: 14.4468 - mse: 14.4468 - mae: 1.6549 - val_loss: 8.3934 - val_mse: 8.3934 - val_mae: 2.5836 - lr: 4.7311e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 4/500\n",
            "556/556 - 1s - loss: 14.0494 - mse: 14.0494 - mae: 1.6314 - val_loss: 8.5764 - val_mse: 8.5764 - val_mae: 2.5681 - lr: 4.7311e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 5/500\n",
            "556/556 - 1s - loss: 13.7326 - mse: 13.7326 - mae: 1.6113 - val_loss: 8.6893 - val_mse: 8.6893 - val_mae: 2.5794 - lr: 4.7311e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 6/500\n",
            "556/556 - 1s - loss: 13.5066 - mse: 13.5066 - mae: 1.5971 - val_loss: 8.9617 - val_mse: 8.9617 - val_mae: 2.5686 - lr: 4.7311e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 7/500\n",
            "556/556 - 1s - loss: 13.3064 - mse: 13.3064 - mae: 1.5876 - val_loss: 9.2447 - val_mse: 9.2447 - val_mae: 2.6120 - lr: 4.7311e-04 - 1s/epoch - 2ms/step\n",
            "Score for inner fold : loss of 9.24472713470459\n",
            "-----Fold:1--------\n",
            "Epoch 1/500\n",
            "556/556 - 2s - loss: 41.2122 - mse: 41.2122 - mae: 2.0351 - val_loss: 7.0685 - val_mse: 7.0685 - val_mae: 2.4409 - lr: 4.7311e-04 - 2s/epoch - 3ms/step\n",
            "Epoch 2/500\n",
            "556/556 - 1s - loss: 37.0070 - mse: 37.0070 - mae: 1.7603 - val_loss: 7.7136 - val_mse: 7.7136 - val_mae: 2.5045 - lr: 4.7311e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 3/500\n",
            "556/556 - 1s - loss: 36.3293 - mse: 36.3293 - mae: 1.7016 - val_loss: 8.0825 - val_mse: 8.0825 - val_mae: 2.5441 - lr: 4.7311e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 4/500\n",
            "556/556 - 1s - loss: 35.8396 - mse: 35.8396 - mae: 1.6673 - val_loss: 8.1880 - val_mse: 8.1880 - val_mae: 2.5213 - lr: 4.7311e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 5/500\n",
            "556/556 - 1s - loss: 35.4353 - mse: 35.4353 - mae: 1.6477 - val_loss: 8.3542 - val_mse: 8.3542 - val_mae: 2.5136 - lr: 4.7311e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 6/500\n",
            "556/556 - 1s - loss: 35.0741 - mse: 35.0741 - mae: 1.6308 - val_loss: 8.7015 - val_mse: 8.7015 - val_mae: 2.5570 - lr: 4.7311e-04 - 1s/epoch - 2ms/step\n",
            "Score for inner fold : loss of 8.701461791992188\n",
            "-----Fold:2--------\n",
            "Epoch 1/500\n",
            "556/556 - 2s - loss: 38.4042 - mse: 38.4042 - mae: 1.9542 - val_loss: 7.8353 - val_mse: 7.8353 - val_mae: 2.5365 - lr: 4.7311e-04 - 2s/epoch - 3ms/step\n",
            "Epoch 2/500\n",
            "556/556 - 1s - loss: 34.7421 - mse: 34.7421 - mae: 1.6739 - val_loss: 8.1070 - val_mse: 8.1070 - val_mae: 2.5186 - lr: 4.7311e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 3/500\n",
            "556/556 - 1s - loss: 34.2563 - mse: 34.2563 - mae: 1.6380 - val_loss: 8.5446 - val_mse: 8.5446 - val_mae: 2.5432 - lr: 4.7311e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 4/500\n",
            "556/556 - 1s - loss: 33.8981 - mse: 33.8981 - mae: 1.6229 - val_loss: 8.9008 - val_mse: 8.9008 - val_mae: 2.5894 - lr: 4.7311e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 5/500\n",
            "556/556 - 1s - loss: 33.6542 - mse: 33.6542 - mae: 1.6082 - val_loss: 8.8904 - val_mse: 8.8904 - val_mae: 2.5689 - lr: 4.7311e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 6/500\n",
            "556/556 - 1s - loss: 33.4448 - mse: 33.4448 - mae: 1.6009 - val_loss: 8.7973 - val_mse: 8.7973 - val_mae: 2.5518 - lr: 4.7311e-04 - 1s/epoch - 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-10-10 09:47:08,324]\u001b[0m Finished trial#18 resulted in value: 8.913333333333332. Current best value is 8.913333333333332 with parameters: {'activation': 'tanh', 'num_hidden_layer': 4, 'i': 3, 'learning_rate': 0.0004731076342361319}.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Score for inner fold : loss of 8.797311782836914\n",
            "Trial Number:19\n",
            "-----Fold:0--------\n",
            "Epoch 1/500\n",
            "556/556 - 2s - loss: 40.4885 - mse: 40.4885 - mae: 2.0299 - val_loss: 6.0625 - val_mse: 6.0625 - val_mae: 2.2040 - lr: 4.8716e-04 - 2s/epoch - 3ms/step\n",
            "Epoch 2/500\n",
            "556/556 - 1s - loss: 35.5396 - mse: 35.5396 - mae: 1.6728 - val_loss: 8.2998 - val_mse: 8.2998 - val_mae: 2.5791 - lr: 4.8716e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 3/500\n",
            "556/556 - 1s - loss: 34.8850 - mse: 34.8850 - mae: 1.6484 - val_loss: 8.3019 - val_mse: 8.3019 - val_mae: 2.5291 - lr: 4.8716e-04 - 995ms/epoch - 2ms/step\n",
            "Epoch 4/500\n",
            "556/556 - 1s - loss: 34.5069 - mse: 34.5069 - mae: 1.6292 - val_loss: 8.5576 - val_mse: 8.5576 - val_mae: 2.5562 - lr: 4.8716e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 5/500\n",
            "556/556 - 1s - loss: 34.2922 - mse: 34.2922 - mae: 1.6147 - val_loss: 9.1933 - val_mse: 9.1933 - val_mae: 2.6365 - lr: 4.8716e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 6/500\n",
            "556/556 - 1s - loss: 34.0818 - mse: 34.0818 - mae: 1.6108 - val_loss: 9.2100 - val_mse: 9.2100 - val_mae: 2.6322 - lr: 4.8716e-04 - 1s/epoch - 2ms/step\n",
            "Score for inner fold : loss of 9.21001148223877\n",
            "-----Fold:1--------\n",
            "Epoch 1/500\n",
            "556/556 - 2s - loss: 39.4073 - mse: 39.4073 - mae: 2.0482 - val_loss: 6.1398 - val_mse: 6.1398 - val_mae: 2.2074 - lr: 4.8716e-04 - 2s/epoch - 3ms/step\n",
            "Epoch 2/500\n",
            "556/556 - 1s - loss: 35.2808 - mse: 35.2808 - mae: 1.6756 - val_loss: 8.1021 - val_mse: 8.1021 - val_mae: 2.5700 - lr: 4.8716e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 3/500\n",
            "556/556 - 1s - loss: 34.6744 - mse: 34.6744 - mae: 1.6541 - val_loss: 8.1840 - val_mse: 8.1840 - val_mae: 2.5438 - lr: 4.8716e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 4/500\n",
            "556/556 - 1s - loss: 34.3062 - mse: 34.3062 - mae: 1.6274 - val_loss: 8.5127 - val_mse: 8.5127 - val_mae: 2.5639 - lr: 4.8716e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 5/500\n",
            "556/556 - 1s - loss: 34.0511 - mse: 34.0511 - mae: 1.6179 - val_loss: 8.5963 - val_mse: 8.5963 - val_mae: 2.5576 - lr: 4.8716e-04 - 997ms/epoch - 2ms/step\n",
            "Epoch 6/500\n",
            "556/556 - 1s - loss: 33.8405 - mse: 33.8405 - mae: 1.6057 - val_loss: 8.8567 - val_mse: 8.8567 - val_mae: 2.5743 - lr: 4.8716e-04 - 981ms/epoch - 2ms/step\n",
            "Score for inner fold : loss of 8.856744766235352\n",
            "-----Fold:2--------\n",
            "Epoch 1/500\n",
            "556/556 - 2s - loss: 20.2270 - mse: 20.2270 - mae: 2.0533 - val_loss: 6.0544 - val_mse: 6.0544 - val_mae: 2.1976 - lr: 4.8716e-04 - 2s/epoch - 3ms/step\n",
            "Epoch 2/500\n",
            "556/556 - 1s - loss: 15.7414 - mse: 15.7414 - mae: 1.7136 - val_loss: 8.1819 - val_mse: 8.1819 - val_mae: 2.5651 - lr: 4.8716e-04 - 972ms/epoch - 2ms/step\n",
            "Epoch 3/500\n",
            "556/556 - 1s - loss: 14.9978 - mse: 14.9978 - mae: 1.6689 - val_loss: 8.3381 - val_mse: 8.3381 - val_mae: 2.5600 - lr: 4.8716e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 4/500\n",
            "556/556 - 1s - loss: 14.5464 - mse: 14.5464 - mae: 1.6428 - val_loss: 8.4345 - val_mse: 8.4345 - val_mae: 2.5378 - lr: 4.8716e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 5/500\n",
            "556/556 - 1s - loss: 14.2080 - mse: 14.2080 - mae: 1.6209 - val_loss: 8.9990 - val_mse: 8.9990 - val_mae: 2.6139 - lr: 4.8716e-04 - 969ms/epoch - 2ms/step\n",
            "Epoch 6/500\n",
            "556/556 - 1s - loss: 13.9613 - mse: 13.9613 - mae: 1.6123 - val_loss: 8.7506 - val_mse: 8.7506 - val_mae: 2.5461 - lr: 4.8716e-04 - 982ms/epoch - 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-10-10 09:47:32,067]\u001b[0m Finished trial#19 resulted in value: 8.94. Current best value is 8.913333333333332 with parameters: {'activation': 'tanh', 'num_hidden_layer': 4, 'i': 3, 'learning_rate': 0.0004731076342361319}.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Score for inner fold : loss of 8.750608444213867\n",
            "Trial Number:20\n",
            "-----Fold:0--------\n",
            "Epoch 1/500\n",
            "556/556 - 2s - loss: 35.7471 - mse: 35.7471 - mae: 1.9228 - val_loss: 9.0746 - val_mse: 9.0746 - val_mae: 2.1800 - lr: 4.4988e-04 - 2s/epoch - 3ms/step\n",
            "Epoch 2/500\n",
            "556/556 - 1s - loss: 32.6997 - mse: 32.6997 - mae: 1.6515 - val_loss: 10.9855 - val_mse: 10.9855 - val_mae: 2.5250 - lr: 4.4988e-04 - 957ms/epoch - 2ms/step\n",
            "Epoch 3/500\n",
            "556/556 - 1s - loss: 32.1199 - mse: 32.1199 - mae: 1.6336 - val_loss: 11.2837 - val_mse: 11.2837 - val_mae: 2.5707 - lr: 4.4988e-04 - 963ms/epoch - 2ms/step\n",
            "Epoch 4/500\n",
            "556/556 - 1s - loss: 31.8666 - mse: 31.8666 - mae: 1.6144 - val_loss: 11.9985 - val_mse: 11.9985 - val_mae: 2.6119 - lr: 4.4988e-04 - 948ms/epoch - 2ms/step\n",
            "Epoch 5/500\n",
            "556/556 - 1s - loss: 31.6928 - mse: 31.6928 - mae: 1.6054 - val_loss: 12.6249 - val_mse: 12.6249 - val_mae: 2.6561 - lr: 4.4988e-04 - 952ms/epoch - 2ms/step\n",
            "Epoch 6/500\n",
            "556/556 - 1s - loss: 31.5584 - mse: 31.5584 - mae: 1.6035 - val_loss: 12.9677 - val_mse: 12.9677 - val_mae: 2.6664 - lr: 4.4988e-04 - 940ms/epoch - 2ms/step\n",
            "Score for inner fold : loss of 12.96768856048584\n",
            "-----Fold:1--------\n",
            "Epoch 1/500\n",
            "556/556 - 2s - loss: 42.2418 - mse: 42.2418 - mae: 2.3386 - val_loss: 4.6375 - val_mse: 4.6375 - val_mae: 1.6451 - lr: 4.4988e-04 - 2s/epoch - 3ms/step\n",
            "Epoch 2/500\n",
            "556/556 - 1s - loss: 34.5455 - mse: 34.5455 - mae: 1.6341 - val_loss: 9.5350 - val_mse: 9.5350 - val_mae: 2.4308 - lr: 4.4988e-04 - 936ms/epoch - 2ms/step\n",
            "Epoch 3/500\n",
            "556/556 - 1s - loss: 33.2957 - mse: 33.2957 - mae: 1.6148 - val_loss: 10.7790 - val_mse: 10.7790 - val_mae: 2.5892 - lr: 4.4988e-04 - 959ms/epoch - 2ms/step\n",
            "Epoch 4/500\n",
            "556/556 - 1s - loss: 32.9211 - mse: 32.9211 - mae: 1.6036 - val_loss: 10.8440 - val_mse: 10.8440 - val_mae: 2.6064 - lr: 4.4988e-04 - 939ms/epoch - 2ms/step\n",
            "Epoch 5/500\n",
            "556/556 - 1s - loss: 32.6647 - mse: 32.6647 - mae: 1.5948 - val_loss: 11.0954 - val_mse: 11.0954 - val_mae: 2.6270 - lr: 4.4988e-04 - 942ms/epoch - 2ms/step\n",
            "Epoch 6/500\n",
            "556/556 - 1s - loss: 32.4479 - mse: 32.4479 - mae: 1.5901 - val_loss: 11.3260 - val_mse: 11.3260 - val_mae: 2.6416 - lr: 4.4988e-04 - 960ms/epoch - 2ms/step\n",
            "Score for inner fold : loss of 11.326016426086426\n",
            "-----Fold:2--------\n",
            "Epoch 1/500\n",
            "556/556 - 2s - loss: 18.2925 - mse: 18.2925 - mae: 1.8908 - val_loss: 6.8168 - val_mse: 6.8168 - val_mae: 2.1989 - lr: 4.4988e-04 - 2s/epoch - 3ms/step\n",
            "Epoch 2/500\n",
            "556/556 - 1s - loss: 14.3344 - mse: 14.3344 - mae: 1.6801 - val_loss: 9.0926 - val_mse: 9.0926 - val_mae: 2.5111 - lr: 4.4988e-04 - 971ms/epoch - 2ms/step\n",
            "Epoch 3/500\n",
            "556/556 - 1s - loss: 13.1991 - mse: 13.1991 - mae: 1.6453 - val_loss: 9.9277 - val_mse: 9.9277 - val_mae: 2.5922 - lr: 4.4988e-04 - 936ms/epoch - 2ms/step\n",
            "Epoch 4/500\n",
            "556/556 - 1s - loss: 12.7497 - mse: 12.7497 - mae: 1.6248 - val_loss: 10.0943 - val_mse: 10.0943 - val_mae: 2.6046 - lr: 4.4988e-04 - 937ms/epoch - 2ms/step\n",
            "Epoch 5/500\n",
            "556/556 - 1s - loss: 12.4935 - mse: 12.4935 - mae: 1.6163 - val_loss: 10.7081 - val_mse: 10.7081 - val_mae: 2.6471 - lr: 4.4988e-04 - 958ms/epoch - 2ms/step\n",
            "Epoch 6/500\n",
            "556/556 - 1s - loss: 12.3217 - mse: 12.3217 - mae: 1.6120 - val_loss: 10.1027 - val_mse: 10.1027 - val_mae: 2.5682 - lr: 4.4988e-04 - 934ms/epoch - 2ms/step\n",
            "Score for inner fold : loss of 10.1027250289917\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-10-10 09:47:54,347]\u001b[0m Finished trial#20 resulted in value: 11.466666666666667. Current best value is 8.913333333333332 with parameters: {'activation': 'tanh', 'num_hidden_layer': 4, 'i': 3, 'learning_rate': 0.0004731076342361319}.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trial Number:21\n",
            "-----Fold:0--------\n",
            "Epoch 1/500\n",
            "556/556 - 2s - loss: 40.8114 - mse: 40.8114 - mae: 2.0167 - val_loss: 6.9792 - val_mse: 6.9792 - val_mae: 2.4048 - lr: 5.6058e-04 - 2s/epoch - 3ms/step\n",
            "Epoch 2/500\n",
            "556/556 - 1s - loss: 36.0028 - mse: 36.0028 - mae: 1.7044 - val_loss: 8.1175 - val_mse: 8.1175 - val_mae: 2.5420 - lr: 5.6058e-04 - 995ms/epoch - 2ms/step\n",
            "Epoch 3/500\n",
            "556/556 - 1s - loss: 35.3860 - mse: 35.3860 - mae: 1.6605 - val_loss: 8.3991 - val_mse: 8.3991 - val_mae: 2.5614 - lr: 5.6058e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 4/500\n",
            "556/556 - 1s - loss: 34.9751 - mse: 34.9751 - mae: 1.6346 - val_loss: 8.8598 - val_mse: 8.8598 - val_mae: 2.6006 - lr: 5.6058e-04 - 989ms/epoch - 2ms/step\n",
            "Epoch 5/500\n",
            "556/556 - 1s - loss: 34.7021 - mse: 34.7021 - mae: 1.6177 - val_loss: 9.4532 - val_mse: 9.4532 - val_mae: 2.6536 - lr: 5.6058e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 6/500\n",
            "556/556 - 1s - loss: 34.4718 - mse: 34.4718 - mae: 1.6108 - val_loss: 8.8556 - val_mse: 8.8556 - val_mae: 2.5290 - lr: 5.6058e-04 - 1s/epoch - 2ms/step\n",
            "Score for inner fold : loss of 8.855622291564941\n",
            "-----Fold:1--------\n",
            "Epoch 1/500\n",
            "556/556 - 2s - loss: 38.7417 - mse: 38.7417 - mae: 1.9543 - val_loss: 7.7111 - val_mse: 7.7111 - val_mae: 2.5024 - lr: 5.6058e-04 - 2s/epoch - 3ms/step\n",
            "Epoch 2/500\n",
            "556/556 - 1s - loss: 34.6666 - mse: 34.6666 - mae: 1.6486 - val_loss: 8.1669 - val_mse: 8.1669 - val_mae: 2.5188 - lr: 5.6058e-04 - 964ms/epoch - 2ms/step\n",
            "Epoch 3/500\n",
            "556/556 - 1s - loss: 34.1658 - mse: 34.1658 - mae: 1.6161 - val_loss: 8.5090 - val_mse: 8.5090 - val_mae: 2.5550 - lr: 5.6058e-04 - 993ms/epoch - 2ms/step\n",
            "Epoch 4/500\n",
            "556/556 - 1s - loss: 33.8300 - mse: 33.8300 - mae: 1.5993 - val_loss: 9.0518 - val_mse: 9.0518 - val_mae: 2.6046 - lr: 5.6058e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 5/500\n",
            "556/556 - 1s - loss: 33.5917 - mse: 33.5917 - mae: 1.5927 - val_loss: 9.1304 - val_mse: 9.1304 - val_mae: 2.6019 - lr: 5.6058e-04 - 996ms/epoch - 2ms/step\n",
            "Epoch 6/500\n",
            "556/556 - 1s - loss: 33.4055 - mse: 33.4055 - mae: 1.5817 - val_loss: 8.7918 - val_mse: 8.7918 - val_mae: 2.5249 - lr: 5.6058e-04 - 1s/epoch - 2ms/step\n",
            "Score for inner fold : loss of 8.791754722595215\n",
            "-----Fold:2--------\n",
            "Epoch 1/500\n",
            "556/556 - 2s - loss: 18.9676 - mse: 18.9676 - mae: 1.9984 - val_loss: 7.4785 - val_mse: 7.4785 - val_mae: 2.4620 - lr: 5.6058e-04 - 2s/epoch - 3ms/step\n",
            "Epoch 2/500\n",
            "556/556 - 1s - loss: 14.8844 - mse: 14.8844 - mae: 1.6715 - val_loss: 8.0832 - val_mse: 8.0832 - val_mae: 2.5343 - lr: 5.6058e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 3/500\n",
            "556/556 - 1s - loss: 14.3431 - mse: 14.3431 - mae: 1.6385 - val_loss: 8.6105 - val_mse: 8.6105 - val_mae: 2.5915 - lr: 5.6058e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 4/500\n",
            "556/556 - 1s - loss: 13.9412 - mse: 13.9412 - mae: 1.6124 - val_loss: 8.3378 - val_mse: 8.3378 - val_mae: 2.5138 - lr: 5.6058e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 5/500\n",
            "556/556 - 1s - loss: 13.6239 - mse: 13.6239 - mae: 1.5946 - val_loss: 8.7455 - val_mse: 8.7455 - val_mae: 2.5688 - lr: 5.6058e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 6/500\n",
            "556/556 - 1s - loss: 13.3503 - mse: 13.3503 - mae: 1.5869 - val_loss: 9.0082 - val_mse: 9.0082 - val_mae: 2.5896 - lr: 5.6058e-04 - 1s/epoch - 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-10-10 09:48:18,110]\u001b[0m Finished trial#21 resulted in value: 8.886666666666665. Current best value is 8.886666666666665 with parameters: {'activation': 'tanh', 'num_hidden_layer': 3, 'i': 3, 'learning_rate': 0.0005605756648219629}.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Score for inner fold : loss of 9.008225440979004\n",
            "Trial Number:22\n",
            "-----Fold:0--------\n",
            "Epoch 1/500\n",
            "556/556 - 2s - loss: 40.6130 - mse: 40.6130 - mae: 2.1146 - val_loss: 4.3219 - val_mse: 4.3219 - val_mae: 1.8009 - lr: 2.6968e-04 - 2s/epoch - 3ms/step\n",
            "Epoch 2/500\n",
            "556/556 - 1s - loss: 35.4082 - mse: 35.4082 - mae: 1.6462 - val_loss: 8.1006 - val_mse: 8.1006 - val_mae: 2.5310 - lr: 2.6968e-04 - 994ms/epoch - 2ms/step\n",
            "Epoch 3/500\n",
            "556/556 - 1s - loss: 34.6359 - mse: 34.6359 - mae: 1.6496 - val_loss: 8.3829 - val_mse: 8.3829 - val_mae: 2.5456 - lr: 2.6968e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 4/500\n",
            "556/556 - 1s - loss: 34.2697 - mse: 34.2697 - mae: 1.6315 - val_loss: 8.8511 - val_mse: 8.8511 - val_mae: 2.5867 - lr: 2.6968e-04 - 980ms/epoch - 2ms/step\n",
            "Epoch 5/500\n",
            "556/556 - 1s - loss: 33.9890 - mse: 33.9890 - mae: 1.6167 - val_loss: 9.2311 - val_mse: 9.2311 - val_mae: 2.6270 - lr: 2.6968e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 6/500\n",
            "556/556 - 1s - loss: 33.7762 - mse: 33.7762 - mae: 1.5997 - val_loss: 9.3621 - val_mse: 9.3621 - val_mae: 2.6194 - lr: 2.6968e-04 - 1s/epoch - 2ms/step\n",
            "Score for inner fold : loss of 9.362066268920898\n",
            "-----Fold:1--------\n",
            "Epoch 1/500\n",
            "556/556 - 2s - loss: 41.2162 - mse: 41.2162 - mae: 2.1975 - val_loss: 3.2602 - val_mse: 3.2602 - val_mae: 1.5382 - lr: 2.6968e-04 - 2s/epoch - 3ms/step\n",
            "Epoch 2/500\n",
            "556/556 - 1s - loss: 35.7507 - mse: 35.7507 - mae: 1.6456 - val_loss: 7.9814 - val_mse: 7.9814 - val_mae: 2.5308 - lr: 2.6968e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 3/500\n",
            "556/556 - 1s - loss: 34.6128 - mse: 34.6128 - mae: 1.6673 - val_loss: 8.3106 - val_mse: 8.3106 - val_mae: 2.5454 - lr: 2.6968e-04 - 993ms/epoch - 2ms/step\n",
            "Epoch 4/500\n",
            "556/556 - 1s - loss: 34.2189 - mse: 34.2189 - mae: 1.6428 - val_loss: 8.7434 - val_mse: 8.7434 - val_mae: 2.5801 - lr: 2.6968e-04 - 989ms/epoch - 2ms/step\n",
            "Epoch 5/500\n",
            "556/556 - 1s - loss: 33.9561 - mse: 33.9561 - mae: 1.6295 - val_loss: 8.8510 - val_mse: 8.8510 - val_mae: 2.5825 - lr: 2.6968e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 6/500\n",
            "556/556 - 1s - loss: 33.7578 - mse: 33.7578 - mae: 1.6177 - val_loss: 8.8451 - val_mse: 8.8451 - val_mae: 2.5539 - lr: 2.6968e-04 - 999ms/epoch - 2ms/step\n",
            "Score for inner fold : loss of 8.845126152038574\n",
            "-----Fold:2--------\n",
            "Epoch 1/500\n",
            "556/556 - 2s - loss: 21.0525 - mse: 21.0525 - mae: 2.1669 - val_loss: 4.2365 - val_mse: 4.2365 - val_mae: 1.7554 - lr: 2.6968e-04 - 2s/epoch - 3ms/step\n",
            "Epoch 2/500\n",
            "556/556 - 1s - loss: 15.6596 - mse: 15.6596 - mae: 1.6396 - val_loss: 8.2233 - val_mse: 8.2233 - val_mae: 2.5478 - lr: 2.6968e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 3/500\n",
            "556/556 - 1s - loss: 14.9326 - mse: 14.9326 - mae: 1.6406 - val_loss: 8.1620 - val_mse: 8.1620 - val_mae: 2.5076 - lr: 2.6968e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 4/500\n",
            "556/556 - 1s - loss: 14.6047 - mse: 14.6047 - mae: 1.6161 - val_loss: 8.6825 - val_mse: 8.6825 - val_mae: 2.5657 - lr: 2.6968e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 5/500\n",
            "556/556 - 1s - loss: 14.3412 - mse: 14.3412 - mae: 1.6001 - val_loss: 8.5985 - val_mse: 8.5985 - val_mae: 2.5352 - lr: 2.6968e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 6/500\n",
            "556/556 - 1s - loss: 14.1354 - mse: 14.1354 - mae: 1.5876 - val_loss: 8.6076 - val_mse: 8.6076 - val_mae: 2.5216 - lr: 2.6968e-04 - 1s/epoch - 2ms/step\n",
            "Score for inner fold : loss of 8.607616424560547\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-10-10 09:48:41,940]\u001b[0m Finished trial#22 resulted in value: 8.94. Current best value is 8.886666666666665 with parameters: {'activation': 'tanh', 'num_hidden_layer': 3, 'i': 3, 'learning_rate': 0.0005605756648219629}.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trial Number:23\n",
            "-----Fold:0--------\n",
            "Epoch 1/500\n",
            "556/556 - 2s - loss: 20.3444 - mse: 20.3444 - mae: 2.2408 - val_loss: 2.5662 - val_mse: 2.5662 - val_mae: 1.3007 - lr: 2.1227e-04 - 2s/epoch - 3ms/step\n",
            "Epoch 2/500\n",
            "556/556 - 1s - loss: 15.2146 - mse: 15.2146 - mae: 1.6250 - val_loss: 7.8115 - val_mse: 7.8115 - val_mae: 2.4986 - lr: 2.1227e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 3/500\n",
            "556/556 - 1s - loss: 13.9755 - mse: 13.9755 - mae: 1.6463 - val_loss: 8.4105 - val_mse: 8.4105 - val_mae: 2.5755 - lr: 2.1227e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 4/500\n",
            "556/556 - 1s - loss: 13.6283 - mse: 13.6283 - mae: 1.6352 - val_loss: 8.4014 - val_mse: 8.4014 - val_mae: 2.5570 - lr: 2.1227e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 5/500\n",
            "556/556 - 1s - loss: 13.3750 - mse: 13.3750 - mae: 1.6181 - val_loss: 8.5243 - val_mse: 8.5243 - val_mae: 2.5548 - lr: 2.1227e-04 - 997ms/epoch - 2ms/step\n",
            "Epoch 6/500\n",
            "556/556 - 1s - loss: 13.1722 - mse: 13.1722 - mae: 1.6046 - val_loss: 8.7996 - val_mse: 8.7996 - val_mae: 2.5871 - lr: 2.1227e-04 - 1s/epoch - 2ms/step\n",
            "Score for inner fold : loss of 8.79955005645752\n",
            "-----Fold:1--------\n",
            "Epoch 1/500\n",
            "556/556 - 2s - loss: 42.0842 - mse: 42.0842 - mae: 2.3191 - val_loss: 2.9384 - val_mse: 2.9384 - val_mae: 1.3968 - lr: 2.1227e-04 - 2s/epoch - 3ms/step\n",
            "Epoch 2/500\n",
            "556/556 - 1s - loss: 36.9962 - mse: 36.9962 - mae: 1.6913 - val_loss: 7.9594 - val_mse: 7.9594 - val_mae: 2.5244 - lr: 2.1227e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 3/500\n",
            "556/556 - 1s - loss: 35.6520 - mse: 35.6520 - mae: 1.6700 - val_loss: 8.2555 - val_mse: 8.2555 - val_mae: 2.5556 - lr: 2.1227e-04 - 999ms/epoch - 2ms/step\n",
            "Epoch 4/500\n",
            "556/556 - 1s - loss: 35.1562 - mse: 35.1562 - mae: 1.6413 - val_loss: 8.4487 - val_mse: 8.4487 - val_mae: 2.5502 - lr: 2.1227e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 5/500\n",
            "556/556 - 1s - loss: 34.8269 - mse: 34.8269 - mae: 1.6262 - val_loss: 8.9974 - val_mse: 8.9974 - val_mae: 2.5982 - lr: 2.1227e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 6/500\n",
            "556/556 - 1s - loss: 34.5897 - mse: 34.5897 - mae: 1.6160 - val_loss: 8.7669 - val_mse: 8.7669 - val_mae: 2.5554 - lr: 2.1227e-04 - 997ms/epoch - 2ms/step\n",
            "Score for inner fold : loss of 8.766894340515137\n",
            "-----Fold:2--------\n",
            "Epoch 1/500\n",
            "556/556 - 2s - loss: 41.9926 - mse: 41.9926 - mae: 2.3006 - val_loss: 2.8016 - val_mse: 2.8016 - val_mae: 1.3737 - lr: 2.1227e-04 - 2s/epoch - 3ms/step\n",
            "Epoch 2/500\n",
            "556/556 - 1s - loss: 36.2470 - mse: 36.2470 - mae: 1.6382 - val_loss: 7.9118 - val_mse: 7.9118 - val_mae: 2.5110 - lr: 2.1227e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 3/500\n",
            "556/556 - 1s - loss: 34.9140 - mse: 34.9140 - mae: 1.6455 - val_loss: 8.5036 - val_mse: 8.5036 - val_mae: 2.5639 - lr: 2.1227e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 4/500\n",
            "556/556 - 1s - loss: 34.5416 - mse: 34.5416 - mae: 1.6308 - val_loss: 8.6739 - val_mse: 8.6739 - val_mae: 2.5524 - lr: 2.1227e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 5/500\n",
            "556/556 - 1s - loss: 34.2978 - mse: 34.2978 - mae: 1.6226 - val_loss: 8.6463 - val_mse: 8.6463 - val_mae: 2.5298 - lr: 2.1227e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 6/500\n",
            "556/556 - 1s - loss: 34.1026 - mse: 34.1026 - mae: 1.6118 - val_loss: 8.9834 - val_mse: 8.9834 - val_mae: 2.5687 - lr: 2.1227e-04 - 1s/epoch - 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-10-10 09:49:05,846]\u001b[0m Finished trial#23 resulted in value: 8.85. Current best value is 8.85 with parameters: {'activation': 'tanh', 'num_hidden_layer': 3, 'i': 4, 'learning_rate': 0.00021226671518288895}.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Score for inner fold : loss of 8.98338794708252\n",
            "Trial Number:24\n",
            "-----Fold:0--------\n",
            "Epoch 1/500\n",
            "556/556 - 2s - loss: 42.4914 - mse: 42.4914 - mae: 2.2978 - val_loss: 2.2623 - val_mse: 2.2623 - val_mae: 1.2160 - lr: 1.8337e-04 - 2s/epoch - 3ms/step\n",
            "Epoch 2/500\n",
            "556/556 - 1s - loss: 37.1484 - mse: 37.1484 - mae: 1.6969 - val_loss: 7.2009 - val_mse: 7.2009 - val_mae: 2.3930 - lr: 1.8337e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 3/500\n",
            "556/556 - 1s - loss: 35.4267 - mse: 35.4267 - mae: 1.6810 - val_loss: 8.3443 - val_mse: 8.3443 - val_mae: 2.5728 - lr: 1.8337e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 4/500\n",
            "556/556 - 1s - loss: 34.9773 - mse: 34.9773 - mae: 1.6709 - val_loss: 8.1918 - val_mse: 8.1918 - val_mae: 2.5201 - lr: 1.8337e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 5/500\n",
            "556/556 - 1s - loss: 34.6906 - mse: 34.6906 - mae: 1.6463 - val_loss: 8.6070 - val_mse: 8.6070 - val_mae: 2.5736 - lr: 1.8337e-04 - 971ms/epoch - 2ms/step\n",
            "Epoch 6/500\n",
            "556/556 - 1s - loss: 34.4546 - mse: 34.4546 - mae: 1.6325 - val_loss: 8.7364 - val_mse: 8.7364 - val_mae: 2.5809 - lr: 1.8337e-04 - 1s/epoch - 2ms/step\n",
            "Score for inner fold : loss of 8.736390113830566\n",
            "-----Fold:1--------\n",
            "Epoch 1/500\n",
            "556/556 - 2s - loss: 42.2796 - mse: 42.2796 - mae: 2.3111 - val_loss: 1.9187 - val_mse: 1.9187 - val_mae: 1.0987 - lr: 1.8337e-04 - 2s/epoch - 3ms/step\n",
            "Epoch 2/500\n",
            "556/556 - 1s - loss: 36.7575 - mse: 36.7575 - mae: 1.7006 - val_loss: 6.8786 - val_mse: 6.8786 - val_mae: 2.3378 - lr: 1.8337e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 3/500\n",
            "556/556 - 1s - loss: 34.6157 - mse: 34.6157 - mae: 1.6787 - val_loss: 8.3141 - val_mse: 8.3141 - val_mae: 2.5679 - lr: 1.8337e-04 - 993ms/epoch - 2ms/step\n",
            "Epoch 4/500\n",
            "556/556 - 1s - loss: 34.1501 - mse: 34.1501 - mae: 1.6697 - val_loss: 8.1308 - val_mse: 8.1308 - val_mae: 2.5257 - lr: 1.8337e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 5/500\n",
            "556/556 - 1s - loss: 33.8463 - mse: 33.8463 - mae: 1.6454 - val_loss: 8.3033 - val_mse: 8.3033 - val_mae: 2.5386 - lr: 1.8337e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 6/500\n",
            "556/556 - 1s - loss: 33.6194 - mse: 33.6194 - mae: 1.6240 - val_loss: 8.5466 - val_mse: 8.5466 - val_mae: 2.5611 - lr: 1.8337e-04 - 1s/epoch - 2ms/step\n",
            "Score for inner fold : loss of 8.546562194824219\n",
            "-----Fold:2--------\n",
            "Epoch 1/500\n",
            "556/556 - 2s - loss: 22.4195 - mse: 22.4195 - mae: 2.3504 - val_loss: 2.0137 - val_mse: 2.0137 - val_mae: 1.1224 - lr: 1.8337e-04 - 2s/epoch - 3ms/step\n",
            "Epoch 2/500\n",
            "556/556 - 1s - loss: 17.4604 - mse: 17.4604 - mae: 1.6646 - val_loss: 7.0347 - val_mse: 7.0347 - val_mae: 2.3741 - lr: 1.8337e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 3/500\n",
            "556/556 - 1s - loss: 15.4973 - mse: 15.4973 - mae: 1.6433 - val_loss: 8.0065 - val_mse: 8.0065 - val_mae: 2.5242 - lr: 1.8337e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 4/500\n",
            "556/556 - 1s - loss: 15.0758 - mse: 15.0758 - mae: 1.6292 - val_loss: 8.3105 - val_mse: 8.3105 - val_mae: 2.5471 - lr: 1.8337e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 5/500\n",
            "556/556 - 1s - loss: 14.8069 - mse: 14.8069 - mae: 1.6169 - val_loss: 8.4688 - val_mse: 8.4688 - val_mae: 2.5538 - lr: 1.8337e-04 - 979ms/epoch - 2ms/step\n",
            "Epoch 6/500\n",
            "556/556 - 1s - loss: 14.6034 - mse: 14.6034 - mae: 1.6050 - val_loss: 8.5936 - val_mse: 8.5936 - val_mae: 2.5460 - lr: 1.8337e-04 - 1s/epoch - 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-10-10 09:49:29,854]\u001b[0m Finished trial#24 resulted in value: 8.626666666666667. Current best value is 8.626666666666667 with parameters: {'activation': 'tanh', 'num_hidden_layer': 3, 'i': 4, 'learning_rate': 0.00018336981375442575}.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Score for inner fold : loss of 8.593564987182617\n",
            "Trial Number:25\n",
            "-----Fold:0--------\n",
            "Epoch 1/500\n",
            "556/556 - 2s - loss: 42.5853 - mse: 42.5853 - mae: 2.2634 - val_loss: 2.5861 - val_mse: 2.5861 - val_mae: 1.2978 - lr: 1.6447e-04 - 2s/epoch - 3ms/step\n",
            "Epoch 2/500\n",
            "556/556 - 1s - loss: 37.5448 - mse: 37.5448 - mae: 1.6969 - val_loss: 7.2662 - val_mse: 7.2662 - val_mae: 2.3976 - lr: 1.6447e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 3/500\n",
            "556/556 - 1s - loss: 35.6636 - mse: 35.6636 - mae: 1.6830 - val_loss: 8.2790 - val_mse: 8.2790 - val_mae: 2.5525 - lr: 1.6447e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 4/500\n",
            "556/556 - 1s - loss: 35.1450 - mse: 35.1450 - mae: 1.6694 - val_loss: 8.2955 - val_mse: 8.2955 - val_mae: 2.5331 - lr: 1.6447e-04 - 995ms/epoch - 2ms/step\n",
            "Epoch 5/500\n",
            "556/556 - 1s - loss: 34.8647 - mse: 34.8647 - mae: 1.6460 - val_loss: 8.4808 - val_mse: 8.4808 - val_mae: 2.5512 - lr: 1.6447e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 6/500\n",
            "556/556 - 1s - loss: 34.6414 - mse: 34.6414 - mae: 1.6326 - val_loss: 8.8360 - val_mse: 8.8360 - val_mae: 2.5814 - lr: 1.6447e-04 - 1s/epoch - 2ms/step\n",
            "Score for inner fold : loss of 8.83598804473877\n",
            "-----Fold:1--------\n",
            "Epoch 1/500\n",
            "556/556 - 2s - loss: 41.1036 - mse: 41.1036 - mae: 2.3099 - val_loss: 2.3893 - val_mse: 2.3893 - val_mae: 1.2333 - lr: 1.6447e-04 - 2s/epoch - 3ms/step\n",
            "Epoch 2/500\n",
            "556/556 - 1s - loss: 36.8224 - mse: 36.8224 - mae: 1.7178 - val_loss: 6.7224 - val_mse: 6.7224 - val_mae: 2.2975 - lr: 1.6447e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 3/500\n",
            "556/556 - 1s - loss: 35.0049 - mse: 35.0049 - mae: 1.6638 - val_loss: 7.9844 - val_mse: 7.9844 - val_mae: 2.5240 - lr: 1.6447e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 4/500\n",
            "556/556 - 1s - loss: 34.4436 - mse: 34.4436 - mae: 1.6497 - val_loss: 8.3836 - val_mse: 8.3836 - val_mae: 2.5665 - lr: 1.6447e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 5/500\n",
            "556/556 - 1s - loss: 34.0765 - mse: 34.0765 - mae: 1.6400 - val_loss: 8.2930 - val_mse: 8.2930 - val_mae: 2.5306 - lr: 1.6447e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 6/500\n",
            "556/556 - 1s - loss: 33.8377 - mse: 33.8377 - mae: 1.6233 - val_loss: 8.4860 - val_mse: 8.4860 - val_mae: 2.5416 - lr: 1.6447e-04 - 1s/epoch - 2ms/step\n",
            "Score for inner fold : loss of 8.48603343963623\n",
            "-----Fold:2--------\n",
            "Epoch 1/500\n",
            "556/556 - 2s - loss: 21.4886 - mse: 21.4886 - mae: 2.3130 - val_loss: 2.5009 - val_mse: 2.5009 - val_mae: 1.2597 - lr: 1.6447e-04 - 2s/epoch - 3ms/step\n",
            "Epoch 2/500\n",
            "556/556 - 1s - loss: 16.4878 - mse: 16.4878 - mae: 1.6766 - val_loss: 7.4186 - val_mse: 7.4186 - val_mae: 2.4026 - lr: 1.6447e-04 - 965ms/epoch - 2ms/step\n",
            "Epoch 3/500\n",
            "556/556 - 1s - loss: 14.9403 - mse: 14.9403 - mae: 1.6607 - val_loss: 8.2590 - val_mse: 8.2590 - val_mae: 2.5410 - lr: 1.6447e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 4/500\n",
            "556/556 - 1s - loss: 14.5637 - mse: 14.5637 - mae: 1.6445 - val_loss: 8.5209 - val_mse: 8.5209 - val_mae: 2.5738 - lr: 1.6447e-04 - 992ms/epoch - 2ms/step\n",
            "Epoch 5/500\n",
            "556/556 - 1s - loss: 14.3044 - mse: 14.3044 - mae: 1.6275 - val_loss: 8.8574 - val_mse: 8.8574 - val_mae: 2.5901 - lr: 1.6447e-04 - 991ms/epoch - 2ms/step\n",
            "Epoch 6/500\n",
            "556/556 - 1s - loss: 14.0921 - mse: 14.0921 - mae: 1.6164 - val_loss: 8.9420 - val_mse: 8.9420 - val_mae: 2.5946 - lr: 1.6447e-04 - 1s/epoch - 2ms/step\n",
            "Score for inner fold : loss of 8.94205379486084\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-10-10 09:49:53,801]\u001b[0m Finished trial#25 resulted in value: 8.756666666666666. Current best value is 8.626666666666667 with parameters: {'activation': 'tanh', 'num_hidden_layer': 3, 'i': 4, 'learning_rate': 0.00018336981375442575}.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trial Number:26\n",
            "-----Fold:0--------\n",
            "Epoch 1/500\n",
            "556/556 - 2s - loss: 41.6223 - mse: 41.6223 - mae: 2.2894 - val_loss: 2.5776 - val_mse: 2.5776 - val_mae: 1.2868 - lr: 1.9068e-04 - 2s/epoch - 3ms/step\n",
            "Epoch 2/500\n",
            "556/556 - 1s - loss: 36.8744 - mse: 36.8744 - mae: 1.6855 - val_loss: 7.4316 - val_mse: 7.4316 - val_mae: 2.4300 - lr: 1.9068e-04 - 986ms/epoch - 2ms/step\n",
            "Epoch 3/500\n",
            "556/556 - 1s - loss: 35.1627 - mse: 35.1627 - mae: 1.6691 - val_loss: 8.1921 - val_mse: 8.1921 - val_mae: 2.5586 - lr: 1.9068e-04 - 974ms/epoch - 2ms/step\n",
            "Epoch 4/500\n",
            "556/556 - 1s - loss: 34.6464 - mse: 34.6464 - mae: 1.6474 - val_loss: 8.4002 - val_mse: 8.4002 - val_mae: 2.5484 - lr: 1.9068e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 5/500\n",
            "556/556 - 1s - loss: 34.3315 - mse: 34.3315 - mae: 1.6298 - val_loss: 8.6798 - val_mse: 8.6798 - val_mae: 2.5717 - lr: 1.9068e-04 - 983ms/epoch - 2ms/step\n",
            "Epoch 6/500\n",
            "556/556 - 1s - loss: 34.0791 - mse: 34.0791 - mae: 1.6177 - val_loss: 8.7631 - val_mse: 8.7631 - val_mae: 2.5763 - lr: 1.9068e-04 - 1s/epoch - 2ms/step\n",
            "Score for inner fold : loss of 8.763121604919434\n",
            "-----Fold:1--------\n",
            "Epoch 1/500\n",
            "556/556 - 2s - loss: 21.6722 - mse: 21.6722 - mae: 2.3484 - val_loss: 2.0600 - val_mse: 2.0600 - val_mae: 1.1371 - lr: 1.9068e-04 - 2s/epoch - 3ms/step\n",
            "Epoch 2/500\n",
            "556/556 - 1s - loss: 17.0194 - mse: 17.0194 - mae: 1.7341 - val_loss: 6.5277 - val_mse: 6.5277 - val_mae: 2.2784 - lr: 1.9068e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 3/500\n",
            "556/556 - 1s - loss: 14.7337 - mse: 14.7337 - mae: 1.6545 - val_loss: 8.1659 - val_mse: 8.1659 - val_mae: 2.5476 - lr: 1.9068e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 4/500\n",
            "556/556 - 1s - loss: 14.2204 - mse: 14.2204 - mae: 1.6497 - val_loss: 8.1897 - val_mse: 8.1897 - val_mae: 2.5300 - lr: 1.9068e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 5/500\n",
            "556/556 - 1s - loss: 13.9360 - mse: 13.9360 - mae: 1.6335 - val_loss: 8.2593 - val_mse: 8.2593 - val_mae: 2.5288 - lr: 1.9068e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 6/500\n",
            "556/556 - 1s - loss: 13.7249 - mse: 13.7249 - mae: 1.6182 - val_loss: 8.5290 - val_mse: 8.5290 - val_mae: 2.5467 - lr: 1.9068e-04 - 1s/epoch - 2ms/step\n",
            "Score for inner fold : loss of 8.529014587402344\n",
            "-----Fold:2--------\n",
            "Epoch 1/500\n",
            "556/556 - 2s - loss: 42.5047 - mse: 42.5047 - mae: 2.4083 - val_loss: 2.4179 - val_mse: 2.4179 - val_mae: 1.2326 - lr: 1.9068e-04 - 2s/epoch - 3ms/step\n",
            "Epoch 2/500\n",
            "556/556 - 1s - loss: 37.1276 - mse: 37.1276 - mae: 1.6908 - val_loss: 7.5274 - val_mse: 7.5274 - val_mae: 2.4478 - lr: 1.9068e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 3/500\n",
            "556/556 - 1s - loss: 35.5285 - mse: 35.5285 - mae: 1.6707 - val_loss: 8.5499 - val_mse: 8.5499 - val_mae: 2.5959 - lr: 1.9068e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 4/500\n",
            "556/556 - 1s - loss: 35.1069 - mse: 35.1069 - mae: 1.6589 - val_loss: 8.6633 - val_mse: 8.6633 - val_mae: 2.5858 - lr: 1.9068e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 5/500\n",
            "556/556 - 1s - loss: 34.8127 - mse: 34.8127 - mae: 1.6385 - val_loss: 8.5636 - val_mse: 8.5636 - val_mae: 2.5548 - lr: 1.9068e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 6/500\n",
            "556/556 - 1s - loss: 34.5882 - mse: 34.5882 - mae: 1.6218 - val_loss: 8.6293 - val_mse: 8.6293 - val_mae: 2.5515 - lr: 1.9068e-04 - 1s/epoch - 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-10-10 09:50:17,783]\u001b[0m Finished trial#26 resulted in value: 8.64. Current best value is 8.626666666666667 with parameters: {'activation': 'tanh', 'num_hidden_layer': 3, 'i': 4, 'learning_rate': 0.00018336981375442575}.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Score for inner fold : loss of 8.629325866699219\n",
            "Trial Number:27\n",
            "-----Fold:0--------\n",
            "Epoch 1/500\n",
            "556/556 - 2s - loss: 39.0434 - mse: 39.0434 - mae: 2.1615 - val_loss: 4.3473 - val_mse: 4.3473 - val_mae: 1.7482 - lr: 1.6091e-04 - 2s/epoch - 4ms/step\n",
            "Epoch 2/500\n",
            "556/556 - 1s - loss: 33.7022 - mse: 33.7022 - mae: 1.5989 - val_loss: 8.5090 - val_mse: 8.5090 - val_mae: 2.5653 - lr: 1.6091e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 3/500\n",
            "556/556 - 1s - loss: 32.9766 - mse: 32.9766 - mae: 1.6056 - val_loss: 9.0366 - val_mse: 9.0366 - val_mae: 2.6001 - lr: 1.6091e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 4/500\n",
            "556/556 - 1s - loss: 32.6980 - mse: 32.6980 - mae: 1.5945 - val_loss: 9.1106 - val_mse: 9.1106 - val_mae: 2.5752 - lr: 1.6091e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 5/500\n",
            "556/556 - 1s - loss: 32.5184 - mse: 32.5184 - mae: 1.5831 - val_loss: 9.1340 - val_mse: 9.1340 - val_mae: 2.5680 - lr: 1.6091e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 6/500\n",
            "556/556 - 1s - loss: 32.3834 - mse: 32.3834 - mae: 1.5760 - val_loss: 9.3946 - val_mse: 9.3946 - val_mae: 2.6073 - lr: 1.6091e-04 - 1s/epoch - 2ms/step\n",
            "Score for inner fold : loss of 9.394633293151855\n",
            "-----Fold:1--------\n",
            "Epoch 1/500\n",
            "556/556 - 2s - loss: 20.7015 - mse: 20.7015 - mae: 2.1585 - val_loss: 3.9306 - val_mse: 3.9306 - val_mae: 1.6585 - lr: 1.6091e-04 - 2s/epoch - 4ms/step\n",
            "Epoch 2/500\n",
            "556/556 - 1s - loss: 15.2194 - mse: 15.2194 - mae: 1.6169 - val_loss: 8.3408 - val_mse: 8.3408 - val_mae: 2.5403 - lr: 1.6091e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 3/500\n",
            "556/556 - 1s - loss: 14.1460 - mse: 14.1460 - mae: 1.6135 - val_loss: 8.8065 - val_mse: 8.8065 - val_mae: 2.5829 - lr: 1.6091e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 4/500\n",
            "556/556 - 1s - loss: 13.7774 - mse: 13.7774 - mae: 1.5899 - val_loss: 9.0298 - val_mse: 9.0298 - val_mae: 2.5896 - lr: 1.6091e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 5/500\n",
            "556/556 - 1s - loss: 13.5260 - mse: 13.5260 - mae: 1.5787 - val_loss: 9.3617 - val_mse: 9.3617 - val_mae: 2.6124 - lr: 1.6091e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 6/500\n",
            "556/556 - 1s - loss: 13.3435 - mse: 13.3435 - mae: 1.5716 - val_loss: 8.9154 - val_mse: 8.9154 - val_mae: 2.5428 - lr: 1.6091e-04 - 1s/epoch - 2ms/step\n",
            "Score for inner fold : loss of 8.915350914001465\n",
            "-----Fold:2--------\n",
            "Epoch 1/500\n",
            "556/556 - 2s - loss: 42.2095 - mse: 42.2095 - mae: 2.2421 - val_loss: 3.2261 - val_mse: 3.2261 - val_mae: 1.4899 - lr: 1.6091e-04 - 2s/epoch - 3ms/step\n",
            "Epoch 2/500\n",
            "556/556 - 1s - loss: 36.1196 - mse: 36.1196 - mae: 1.6365 - val_loss: 8.2556 - val_mse: 8.2556 - val_mae: 2.5244 - lr: 1.6091e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 3/500\n",
            "556/556 - 1s - loss: 35.1500 - mse: 35.1500 - mae: 1.6497 - val_loss: 8.7926 - val_mse: 8.7926 - val_mae: 2.5868 - lr: 1.6091e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 4/500\n",
            "556/556 - 1s - loss: 34.8617 - mse: 34.8617 - mae: 1.6384 - val_loss: 9.0362 - val_mse: 9.0362 - val_mae: 2.5900 - lr: 1.6091e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 5/500\n",
            "556/556 - 1s - loss: 34.6611 - mse: 34.6611 - mae: 1.6274 - val_loss: 8.8355 - val_mse: 8.8355 - val_mae: 2.5313 - lr: 1.6091e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 6/500\n",
            "556/556 - 1s - loss: 34.5159 - mse: 34.5159 - mae: 1.6171 - val_loss: 9.2564 - val_mse: 9.2564 - val_mae: 2.5945 - lr: 1.6091e-04 - 1s/epoch - 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-10-10 09:50:44,904]\u001b[0m Finished trial#27 resulted in value: 9.19. Current best value is 8.626666666666667 with parameters: {'activation': 'tanh', 'num_hidden_layer': 3, 'i': 4, 'learning_rate': 0.00018336981375442575}.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Score for inner fold : loss of 9.256400108337402\n",
            "Trial Number:28\n",
            "-----Fold:0--------\n",
            "Epoch 1/500\n",
            "556/556 - 2s - loss: 39.8109 - mse: 39.8109 - mae: 2.0687 - val_loss: 6.4501 - val_mse: 6.4501 - val_mae: 2.2234 - lr: 2.8857e-04 - 2s/epoch - 3ms/step\n",
            "Epoch 2/500\n",
            "556/556 - 1s - loss: 35.4768 - mse: 35.4768 - mae: 1.6524 - val_loss: 8.0575 - val_mse: 8.0575 - val_mae: 2.5201 - lr: 2.8857e-04 - 996ms/epoch - 2ms/step\n",
            "Epoch 3/500\n",
            "556/556 - 1s - loss: 34.7603 - mse: 34.7603 - mae: 1.6140 - val_loss: 8.6549 - val_mse: 8.6549 - val_mae: 2.5836 - lr: 2.8857e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 4/500\n",
            "556/556 - 1s - loss: 34.3482 - mse: 34.3482 - mae: 1.5957 - val_loss: 8.5785 - val_mse: 8.5785 - val_mae: 2.5396 - lr: 2.8857e-04 - 993ms/epoch - 2ms/step\n",
            "Epoch 5/500\n",
            "556/556 - 1s - loss: 34.1028 - mse: 34.1028 - mae: 1.5798 - val_loss: 8.4881 - val_mse: 8.4881 - val_mae: 2.5059 - lr: 2.8857e-04 - 968ms/epoch - 2ms/step\n",
            "Epoch 6/500\n",
            "556/556 - 1s - loss: 33.9370 - mse: 33.9370 - mae: 1.5681 - val_loss: 9.4770 - val_mse: 9.4770 - val_mae: 2.6444 - lr: 2.8857e-04 - 1s/epoch - 2ms/step\n",
            "Score for inner fold : loss of 9.477010726928711\n",
            "-----Fold:1--------\n",
            "Epoch 1/500\n",
            "556/556 - 2s - loss: 20.9427 - mse: 20.9427 - mae: 2.1970 - val_loss: 3.8713 - val_mse: 3.8713 - val_mae: 1.6650 - lr: 2.8857e-04 - 2s/epoch - 3ms/step\n",
            "Epoch 2/500\n",
            "556/556 - 1s - loss: 15.5797 - mse: 15.5797 - mae: 1.6773 - val_loss: 8.0129 - val_mse: 8.0129 - val_mae: 2.5357 - lr: 2.8857e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 3/500\n",
            "556/556 - 1s - loss: 14.4909 - mse: 14.4909 - mae: 1.6528 - val_loss: 8.4479 - val_mse: 8.4479 - val_mae: 2.5793 - lr: 2.8857e-04 - 979ms/epoch - 2ms/step\n",
            "Epoch 4/500\n",
            "556/556 - 1s - loss: 13.9447 - mse: 13.9447 - mae: 1.6244 - val_loss: 8.6130 - val_mse: 8.6130 - val_mae: 2.5696 - lr: 2.8857e-04 - 989ms/epoch - 2ms/step\n",
            "Epoch 5/500\n",
            "556/556 - 1s - loss: 13.5814 - mse: 13.5814 - mae: 1.6003 - val_loss: 8.7760 - val_mse: 8.7760 - val_mae: 2.5664 - lr: 2.8857e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 6/500\n",
            "556/556 - 1s - loss: 13.3203 - mse: 13.3203 - mae: 1.5869 - val_loss: 9.1184 - val_mse: 9.1184 - val_mae: 2.5999 - lr: 2.8857e-04 - 1s/epoch - 2ms/step\n",
            "Score for inner fold : loss of 9.118427276611328\n",
            "-----Fold:2--------\n",
            "Epoch 1/500\n",
            "556/556 - 2s - loss: 40.3832 - mse: 40.3832 - mae: 2.1657 - val_loss: 5.1043 - val_mse: 5.1043 - val_mae: 1.9397 - lr: 2.8857e-04 - 2s/epoch - 3ms/step\n",
            "Epoch 2/500\n",
            "556/556 - 1s - loss: 35.2223 - mse: 35.2223 - mae: 1.6730 - val_loss: 8.1396 - val_mse: 8.1396 - val_mae: 2.5288 - lr: 2.8857e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 3/500\n",
            "556/556 - 1s - loss: 34.4136 - mse: 34.4136 - mae: 1.6519 - val_loss: 8.6158 - val_mse: 8.6158 - val_mae: 2.5874 - lr: 2.8857e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 4/500\n",
            "556/556 - 1s - loss: 34.0413 - mse: 34.0413 - mae: 1.6296 - val_loss: 8.6030 - val_mse: 8.6030 - val_mae: 2.5553 - lr: 2.8857e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 5/500\n",
            "556/556 - 1s - loss: 33.7965 - mse: 33.7965 - mae: 1.6134 - val_loss: 9.0534 - val_mse: 9.0534 - val_mae: 2.5986 - lr: 2.8857e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 6/500\n",
            "556/556 - 1s - loss: 33.6297 - mse: 33.6297 - mae: 1.6082 - val_loss: 9.4816 - val_mse: 9.4816 - val_mae: 2.6414 - lr: 2.8857e-04 - 1s/epoch - 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-10-10 09:51:08,800]\u001b[0m Finished trial#28 resulted in value: 9.360000000000001. Current best value is 8.626666666666667 with parameters: {'activation': 'tanh', 'num_hidden_layer': 3, 'i': 4, 'learning_rate': 0.00018336981375442575}.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Score for inner fold : loss of 9.481575965881348\n",
            "Trial Number:29\n",
            "-----Fold:0--------\n",
            "Epoch 1/500\n",
            "556/556 - 2s - loss: 41.3975 - mse: 41.3975 - mae: 2.4394 - val_loss: 2.6153 - val_mse: 2.6153 - val_mae: 1.1167 - lr: 1.5288e-04 - 2s/epoch - 3ms/step\n",
            "Epoch 2/500\n",
            "556/556 - 1s - loss: 36.3074 - mse: 36.3074 - mae: 1.8141 - val_loss: 5.5903 - val_mse: 5.5903 - val_mae: 1.7958 - lr: 1.5288e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 3/500\n",
            "556/556 - 1s - loss: 33.3118 - mse: 33.3118 - mae: 1.5440 - val_loss: 9.6191 - val_mse: 9.6191 - val_mae: 2.5219 - lr: 1.5288e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 4/500\n",
            "556/556 - 1s - loss: 32.5889 - mse: 32.5889 - mae: 1.6180 - val_loss: 10.9310 - val_mse: 10.9310 - val_mae: 2.6501 - lr: 1.5288e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 5/500\n",
            "556/556 - 1s - loss: 32.3707 - mse: 32.3707 - mae: 1.6270 - val_loss: 10.8657 - val_mse: 10.8657 - val_mae: 2.6198 - lr: 1.5288e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 6/500\n",
            "556/556 - 1s - loss: 32.2639 - mse: 32.2639 - mae: 1.6154 - val_loss: 11.5532 - val_mse: 11.5532 - val_mae: 2.6736 - lr: 1.5288e-04 - 1s/epoch - 2ms/step\n",
            "Score for inner fold : loss of 11.553186416625977\n",
            "-----Fold:1--------\n",
            "Epoch 1/500\n",
            "556/556 - 2s - loss: 19.9105 - mse: 19.9105 - mae: 2.4199 - val_loss: 2.8303 - val_mse: 2.8303 - val_mae: 1.1581 - lr: 1.5288e-04 - 2s/epoch - 3ms/step\n",
            "Epoch 2/500\n",
            "556/556 - 1s - loss: 14.6776 - mse: 14.6776 - mae: 1.8292 - val_loss: 5.9987 - val_mse: 5.9987 - val_mae: 1.7721 - lr: 1.5288e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 3/500\n",
            "556/556 - 1s - loss: 12.1836 - mse: 12.1836 - mae: 1.5067 - val_loss: 10.2472 - val_mse: 10.2472 - val_mae: 2.5261 - lr: 1.5288e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 4/500\n",
            "556/556 - 1s - loss: 11.6049 - mse: 11.6049 - mae: 1.5670 - val_loss: 10.9462 - val_mse: 10.9462 - val_mae: 2.6322 - lr: 1.5288e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 5/500\n",
            "556/556 - 1s - loss: 11.5752 - mse: 11.5752 - mae: 1.5820 - val_loss: 11.6727 - val_mse: 11.6727 - val_mae: 2.6718 - lr: 1.5288e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 6/500\n",
            "556/556 - 1s - loss: 11.5180 - mse: 11.5180 - mae: 1.5814 - val_loss: 11.7415 - val_mse: 11.7415 - val_mae: 2.6820 - lr: 1.5288e-04 - 1s/epoch - 2ms/step\n",
            "Score for inner fold : loss of 11.74145221710205\n",
            "-----Fold:2--------\n",
            "Epoch 1/500\n",
            "556/556 - 2s - loss: 39.8015 - mse: 39.8015 - mae: 2.4594 - val_loss: 3.6520 - val_mse: 3.6520 - val_mae: 1.1238 - lr: 1.5288e-04 - 2s/epoch - 3ms/step\n",
            "Epoch 2/500\n",
            "556/556 - 1s - loss: 35.2637 - mse: 35.2637 - mae: 1.7979 - val_loss: 7.3258 - val_mse: 7.3258 - val_mae: 1.8137 - lr: 1.5288e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 3/500\n",
            "556/556 - 1s - loss: 33.0335 - mse: 33.0335 - mae: 1.5266 - val_loss: 11.5328 - val_mse: 11.5328 - val_mae: 2.5221 - lr: 1.5288e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 4/500\n",
            "556/556 - 1s - loss: 32.5930 - mse: 32.5930 - mae: 1.5895 - val_loss: 12.7021 - val_mse: 12.7021 - val_mae: 2.6632 - lr: 1.5288e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 5/500\n",
            "556/556 - 1s - loss: 32.4816 - mse: 32.4816 - mae: 1.6074 - val_loss: 13.2564 - val_mse: 13.2564 - val_mae: 2.6759 - lr: 1.5288e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 6/500\n",
            "556/556 - 1s - loss: 32.4378 - mse: 32.4378 - mae: 1.5974 - val_loss: 13.4896 - val_mse: 13.4896 - val_mae: 2.6907 - lr: 1.5288e-04 - 1s/epoch - 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-10-10 09:51:32,955]\u001b[0m Finished trial#29 resulted in value: 12.26. Current best value is 8.626666666666667 with parameters: {'activation': 'tanh', 'num_hidden_layer': 3, 'i': 4, 'learning_rate': 0.00018336981375442575}.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Score for inner fold : loss of 13.489577293395996\n",
            "Trial Number:30\n",
            "-----Fold:0--------\n",
            "Epoch 1/500\n",
            "556/556 - 2s - loss: 44.0355 - mse: 44.0355 - mae: 2.4707 - val_loss: 1.3856 - val_mse: 1.3856 - val_mae: 0.8904 - lr: 1.0583e-04 - 2s/epoch - 3ms/step\n",
            "Epoch 2/500\n",
            "556/556 - 1s - loss: 40.3592 - mse: 40.3592 - mae: 1.9126 - val_loss: 3.1047 - val_mse: 3.1047 - val_mae: 1.4880 - lr: 1.0583e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 3/500\n",
            "556/556 - 1s - loss: 37.3761 - mse: 37.3761 - mae: 1.6298 - val_loss: 6.6824 - val_mse: 6.6824 - val_mae: 2.3082 - lr: 1.0583e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 4/500\n",
            "556/556 - 1s - loss: 36.1539 - mse: 36.1539 - mae: 1.6875 - val_loss: 7.9238 - val_mse: 7.9238 - val_mae: 2.5122 - lr: 1.0583e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 5/500\n",
            "556/556 - 1s - loss: 35.7533 - mse: 35.7533 - mae: 1.6883 - val_loss: 8.3348 - val_mse: 8.3348 - val_mae: 2.5625 - lr: 1.0583e-04 - 998ms/epoch - 2ms/step\n",
            "Epoch 6/500\n",
            "556/556 - 1s - loss: 35.4836 - mse: 35.4836 - mae: 1.6805 - val_loss: 8.4494 - val_mse: 8.4494 - val_mae: 2.5620 - lr: 1.0583e-04 - 1s/epoch - 2ms/step\n",
            "Score for inner fold : loss of 8.449383735656738\n",
            "-----Fold:1--------\n",
            "Epoch 1/500\n",
            "556/556 - 2s - loss: 43.4684 - mse: 43.4684 - mae: 2.5332 - val_loss: 1.6133 - val_mse: 1.6133 - val_mae: 0.9176 - lr: 1.0583e-04 - 2s/epoch - 3ms/step\n",
            "Epoch 2/500\n",
            "556/556 - 1s - loss: 40.5495 - mse: 40.5495 - mae: 2.0930 - val_loss: 2.6771 - val_mse: 2.6771 - val_mae: 1.2964 - lr: 1.0583e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 3/500\n",
            "556/556 - 1s - loss: 37.5007 - mse: 37.5007 - mae: 1.7014 - val_loss: 5.6312 - val_mse: 5.6312 - val_mae: 2.0655 - lr: 1.0583e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 4/500\n",
            "556/556 - 1s - loss: 35.7370 - mse: 35.7370 - mae: 1.6374 - val_loss: 7.7414 - val_mse: 7.7414 - val_mae: 2.4876 - lr: 1.0583e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 5/500\n",
            "556/556 - 1s - loss: 35.2612 - mse: 35.2612 - mae: 1.6579 - val_loss: 8.1815 - val_mse: 8.1815 - val_mae: 2.5509 - lr: 1.0583e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 6/500\n",
            "556/556 - 1s - loss: 35.0048 - mse: 35.0048 - mae: 1.6518 - val_loss: 8.3399 - val_mse: 8.3399 - val_mae: 2.5594 - lr: 1.0583e-04 - 1s/epoch - 2ms/step\n",
            "Score for inner fold : loss of 8.339932441711426\n",
            "-----Fold:2--------\n",
            "Epoch 1/500\n",
            "556/556 - 2s - loss: 22.6052 - mse: 22.6052 - mae: 2.4797 - val_loss: 1.5325 - val_mse: 1.5325 - val_mae: 0.9326 - lr: 1.0583e-04 - 2s/epoch - 3ms/step\n",
            "Epoch 2/500\n",
            "556/556 - 1s - loss: 19.6371 - mse: 19.6371 - mae: 2.0817 - val_loss: 2.3875 - val_mse: 2.3875 - val_mae: 1.2618 - lr: 1.0583e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 3/500\n",
            "556/556 - 1s - loss: 16.6713 - mse: 16.6713 - mae: 1.7060 - val_loss: 4.9331 - val_mse: 4.9331 - val_mae: 1.9534 - lr: 1.0583e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 4/500\n",
            "556/556 - 1s - loss: 14.7100 - mse: 14.7100 - mae: 1.6194 - val_loss: 7.5236 - val_mse: 7.5236 - val_mae: 2.4683 - lr: 1.0583e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 5/500\n",
            "556/556 - 1s - loss: 14.0212 - mse: 14.0212 - mae: 1.6464 - val_loss: 8.2225 - val_mse: 8.2225 - val_mae: 2.5600 - lr: 1.0583e-04 - 999ms/epoch - 2ms/step\n",
            "Epoch 6/500\n",
            "556/556 - 1s - loss: 13.7239 - mse: 13.7239 - mae: 1.6420 - val_loss: 8.4329 - val_mse: 8.4329 - val_mae: 2.5786 - lr: 1.0583e-04 - 1s/epoch - 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-10-10 09:51:57,087]\u001b[0m Finished trial#30 resulted in value: 8.406666666666666. Current best value is 8.406666666666666 with parameters: {'activation': 'tanh', 'num_hidden_layer': 3, 'i': 4, 'learning_rate': 0.00010583102423243646}.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Score for inner fold : loss of 8.43288803100586\n",
            "Trial Number:31\n",
            "-----Fold:0--------\n",
            "Epoch 1/500\n",
            "556/556 - 2s - loss: 21.7776 - mse: 21.7776 - mae: 2.5077 - val_loss: 1.2250 - val_mse: 1.2250 - val_mae: 0.7952 - lr: 1.0295e-04 - 2s/epoch - 3ms/step\n",
            "Epoch 2/500\n",
            "556/556 - 1s - loss: 19.1036 - mse: 19.1036 - mae: 2.1206 - val_loss: 2.2453 - val_mse: 2.2453 - val_mae: 1.1914 - lr: 1.0295e-04 - 998ms/epoch - 2ms/step\n",
            "Epoch 3/500\n",
            "556/556 - 1s - loss: 16.2787 - mse: 16.2787 - mae: 1.7087 - val_loss: 4.8715 - val_mse: 4.8715 - val_mae: 1.9071 - lr: 1.0295e-04 - 996ms/epoch - 2ms/step\n",
            "Epoch 4/500\n",
            "556/556 - 1s - loss: 14.4267 - mse: 14.4267 - mae: 1.6093 - val_loss: 7.3739 - val_mse: 7.3739 - val_mae: 2.4219 - lr: 1.0295e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 5/500\n",
            "556/556 - 1s - loss: 13.8707 - mse: 13.8707 - mae: 1.6456 - val_loss: 8.1396 - val_mse: 8.1396 - val_mae: 2.5470 - lr: 1.0295e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 6/500\n",
            "556/556 - 1s - loss: 13.6298 - mse: 13.6298 - mae: 1.6479 - val_loss: 8.2356 - val_mse: 8.2356 - val_mae: 2.5527 - lr: 1.0295e-04 - 1s/epoch - 2ms/step\n",
            "Score for inner fold : loss of 8.23555850982666\n",
            "-----Fold:1--------\n",
            "Epoch 1/500\n",
            "556/556 - 2s - loss: 43.0333 - mse: 43.0333 - mae: 2.4750 - val_loss: 1.3384 - val_mse: 1.3384 - val_mae: 0.8561 - lr: 1.0295e-04 - 2s/epoch - 3ms/step\n",
            "Epoch 2/500\n",
            "556/556 - 1s - loss: 40.1522 - mse: 40.1522 - mae: 2.0064 - val_loss: 2.6806 - val_mse: 2.6806 - val_mae: 1.3490 - lr: 1.0295e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 3/500\n",
            "556/556 - 1s - loss: 37.2615 - mse: 37.2615 - mae: 1.6732 - val_loss: 5.6706 - val_mse: 5.6706 - val_mae: 2.1109 - lr: 1.0295e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 4/500\n",
            "556/556 - 1s - loss: 35.6563 - mse: 35.6563 - mae: 1.6561 - val_loss: 7.6492 - val_mse: 7.6492 - val_mae: 2.4884 - lr: 1.0295e-04 - 999ms/epoch - 2ms/step\n",
            "Epoch 5/500\n",
            "556/556 - 1s - loss: 35.1461 - mse: 35.1461 - mae: 1.6717 - val_loss: 8.0882 - val_mse: 8.0882 - val_mae: 2.5449 - lr: 1.0295e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 6/500\n",
            "556/556 - 1s - loss: 34.8698 - mse: 34.8698 - mae: 1.6629 - val_loss: 8.2249 - val_mse: 8.2249 - val_mae: 2.5527 - lr: 1.0295e-04 - 1s/epoch - 2ms/step\n",
            "Score for inner fold : loss of 8.224918365478516\n",
            "-----Fold:2--------\n",
            "Epoch 1/500\n",
            "556/556 - 2s - loss: 44.6654 - mse: 44.6654 - mae: 2.4808 - val_loss: 1.5027 - val_mse: 1.5027 - val_mae: 0.9203 - lr: 1.0295e-04 - 2s/epoch - 3ms/step\n",
            "Epoch 2/500\n",
            "556/556 - 1s - loss: 41.6240 - mse: 41.6240 - mae: 2.0715 - val_loss: 2.7513 - val_mse: 2.7513 - val_mae: 1.3541 - lr: 1.0295e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 3/500\n",
            "556/556 - 1s - loss: 38.7287 - mse: 38.7287 - mae: 1.7304 - val_loss: 5.4740 - val_mse: 5.4740 - val_mae: 2.0378 - lr: 1.0295e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 4/500\n",
            "556/556 - 1s - loss: 36.9587 - mse: 36.9587 - mae: 1.6741 - val_loss: 7.5671 - val_mse: 7.5671 - val_mae: 2.4611 - lr: 1.0295e-04 - 998ms/epoch - 2ms/step\n",
            "Epoch 5/500\n",
            "556/556 - 1s - loss: 36.3805 - mse: 36.3805 - mae: 1.6848 - val_loss: 8.2313 - val_mse: 8.2313 - val_mae: 2.5558 - lr: 1.0295e-04 - 998ms/epoch - 2ms/step\n",
            "Epoch 6/500\n",
            "556/556 - 1s - loss: 36.0815 - mse: 36.0815 - mae: 1.6852 - val_loss: 8.2974 - val_mse: 8.2974 - val_mae: 2.5585 - lr: 1.0295e-04 - 1s/epoch - 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-10-10 09:52:21,075]\u001b[0m Finished trial#31 resulted in value: 8.253333333333334. Current best value is 8.253333333333334 with parameters: {'activation': 'tanh', 'num_hidden_layer': 3, 'i': 4, 'learning_rate': 0.00010295062747318013}.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Score for inner fold : loss of 8.297428131103516\n",
            "Trial Number:32\n",
            "-----Fold:0--------\n",
            "Epoch 1/500\n",
            "556/556 - 2s - loss: 44.0484 - mse: 44.0484 - mae: 2.4192 - val_loss: 1.8445 - val_mse: 1.8445 - val_mae: 1.0469 - lr: 1.0271e-04 - 2s/epoch - 4ms/step\n",
            "Epoch 2/500\n",
            "556/556 - 1s - loss: 39.3463 - mse: 39.3463 - mae: 1.8233 - val_loss: 5.2677 - val_mse: 5.2677 - val_mae: 1.9772 - lr: 1.0271e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 3/500\n",
            "556/556 - 1s - loss: 36.3832 - mse: 36.3832 - mae: 1.6350 - val_loss: 7.9903 - val_mse: 7.9903 - val_mae: 2.5129 - lr: 1.0271e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 4/500\n",
            "556/556 - 1s - loss: 35.6483 - mse: 35.6483 - mae: 1.6404 - val_loss: 8.4271 - val_mse: 8.4271 - val_mae: 2.5474 - lr: 1.0271e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 5/500\n",
            "556/556 - 1s - loss: 35.2939 - mse: 35.2939 - mae: 1.6283 - val_loss: 8.4094 - val_mse: 8.4094 - val_mae: 2.5304 - lr: 1.0271e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 6/500\n",
            "556/556 - 1s - loss: 35.0781 - mse: 35.0781 - mae: 1.6112 - val_loss: 8.8524 - val_mse: 8.8524 - val_mae: 2.5735 - lr: 1.0271e-04 - 1s/epoch - 2ms/step\n",
            "Score for inner fold : loss of 8.852388381958008\n",
            "-----Fold:1--------\n",
            "Epoch 1/500\n",
            "556/556 - 2s - loss: 20.7912 - mse: 20.7912 - mae: 2.3763 - val_loss: 2.1130 - val_mse: 2.1130 - val_mae: 1.1159 - lr: 1.0271e-04 - 2s/epoch - 4ms/step\n",
            "Epoch 2/500\n",
            "556/556 - 1s - loss: 15.9462 - mse: 15.9462 - mae: 1.7457 - val_loss: 6.3795 - val_mse: 6.3795 - val_mae: 2.1840 - lr: 1.0271e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 3/500\n",
            "556/556 - 1s - loss: 13.4966 - mse: 13.4966 - mae: 1.6115 - val_loss: 8.4274 - val_mse: 8.4274 - val_mae: 2.5553 - lr: 1.0271e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 4/500\n",
            "556/556 - 1s - loss: 13.0115 - mse: 13.0115 - mae: 1.6145 - val_loss: 8.7613 - val_mse: 8.7613 - val_mae: 2.5767 - lr: 1.0271e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 5/500\n",
            "556/556 - 1s - loss: 12.7664 - mse: 12.7664 - mae: 1.6010 - val_loss: 9.1161 - val_mse: 9.1161 - val_mae: 2.6117 - lr: 1.0271e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 6/500\n",
            "556/556 - 1s - loss: 12.5956 - mse: 12.5956 - mae: 1.5930 - val_loss: 9.2164 - val_mse: 9.2164 - val_mae: 2.6188 - lr: 1.0271e-04 - 1s/epoch - 2ms/step\n",
            "Score for inner fold : loss of 9.216400146484375\n",
            "-----Fold:2--------\n",
            "Epoch 1/500\n",
            "556/556 - 2s - loss: 41.9733 - mse: 41.9733 - mae: 2.4426 - val_loss: 1.9335 - val_mse: 1.9335 - val_mae: 1.0747 - lr: 1.0271e-04 - 2s/epoch - 4ms/step\n",
            "Epoch 2/500\n",
            "556/556 - 1s - loss: 37.0377 - mse: 37.0377 - mae: 1.7610 - val_loss: 6.1098 - val_mse: 6.1098 - val_mae: 2.1372 - lr: 1.0271e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 3/500\n",
            "556/556 - 1s - loss: 34.6181 - mse: 34.6181 - mae: 1.6266 - val_loss: 8.2958 - val_mse: 8.2958 - val_mae: 2.5320 - lr: 1.0271e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 4/500\n",
            "556/556 - 1s - loss: 34.0971 - mse: 34.0971 - mae: 1.6335 - val_loss: 8.7277 - val_mse: 8.7277 - val_mae: 2.5787 - lr: 1.0271e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 5/500\n",
            "556/556 - 1s - loss: 33.8478 - mse: 33.8478 - mae: 1.6234 - val_loss: 8.9993 - val_mse: 8.9993 - val_mae: 2.5923 - lr: 1.0271e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 6/500\n",
            "556/556 - 1s - loss: 33.6767 - mse: 33.6767 - mae: 1.6179 - val_loss: 9.0403 - val_mse: 9.0403 - val_mae: 2.5944 - lr: 1.0271e-04 - 1s/epoch - 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-10-10 09:52:48,389]\u001b[0m Finished trial#32 resulted in value: 9.036666666666667. Current best value is 8.253333333333334 with parameters: {'activation': 'tanh', 'num_hidden_layer': 3, 'i': 4, 'learning_rate': 0.00010295062747318013}.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Score for inner fold : loss of 9.040271759033203\n",
            "Trial Number:33\n",
            "-----Fold:0--------\n",
            "Epoch 1/500\n",
            "556/556 - 2s - loss: 19.5295 - mse: 19.5295 - mae: 2.3677 - val_loss: 3.2291 - val_mse: 3.2291 - val_mae: 1.2517 - lr: 1.4228e-04 - 2s/epoch - 3ms/step\n",
            "Epoch 2/500\n",
            "556/556 - 1s - loss: 13.6633 - mse: 13.6633 - mae: 1.6723 - val_loss: 8.4726 - val_mse: 8.4726 - val_mae: 2.2880 - lr: 1.4228e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 3/500\n",
            "556/556 - 1s - loss: 11.4727 - mse: 11.4727 - mae: 1.5830 - val_loss: 10.9826 - val_mse: 10.9826 - val_mae: 2.6379 - lr: 1.4228e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 4/500\n",
            "556/556 - 1s - loss: 11.1111 - mse: 11.1111 - mae: 1.6003 - val_loss: 11.3918 - val_mse: 11.3918 - val_mae: 2.6368 - lr: 1.4228e-04 - 992ms/epoch - 2ms/step\n",
            "Epoch 5/500\n",
            "556/556 - 1s - loss: 10.9172 - mse: 10.9172 - mae: 1.5890 - val_loss: 11.7508 - val_mse: 11.7508 - val_mae: 2.6439 - lr: 1.4228e-04 - 964ms/epoch - 2ms/step\n",
            "Epoch 6/500\n",
            "556/556 - 1s - loss: 10.8334 - mse: 10.8334 - mae: 1.5878 - val_loss: 12.7571 - val_mse: 12.7571 - val_mae: 2.6777 - lr: 1.4228e-04 - 1s/epoch - 2ms/step\n",
            "Score for inner fold : loss of 12.757121086120605\n",
            "-----Fold:1--------\n",
            "Epoch 1/500\n",
            "556/556 - 2s - loss: 41.0560 - mse: 41.0560 - mae: 2.4303 - val_loss: 3.2986 - val_mse: 3.2986 - val_mae: 1.1811 - lr: 1.4228e-04 - 2s/epoch - 3ms/step\n",
            "Epoch 2/500\n",
            "556/556 - 1s - loss: 35.6807 - mse: 35.6807 - mae: 1.7475 - val_loss: 8.3880 - val_mse: 8.3880 - val_mae: 2.1518 - lr: 1.4228e-04 - 961ms/epoch - 2ms/step\n",
            "Epoch 3/500\n",
            "556/556 - 1s - loss: 33.6032 - mse: 33.6032 - mae: 1.5771 - val_loss: 11.4611 - val_mse: 11.4611 - val_mae: 2.6760 - lr: 1.4228e-04 - 997ms/epoch - 2ms/step\n",
            "Epoch 4/500\n",
            "556/556 - 1s - loss: 33.4051 - mse: 33.4051 - mae: 1.6184 - val_loss: 11.6068 - val_mse: 11.6068 - val_mae: 2.6888 - lr: 1.4228e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 5/500\n",
            "556/556 - 1s - loss: 33.3624 - mse: 33.3624 - mae: 1.6218 - val_loss: 11.5020 - val_mse: 11.5020 - val_mae: 2.6474 - lr: 1.4228e-04 - 987ms/epoch - 2ms/step\n",
            "Epoch 6/500\n",
            "556/556 - 1s - loss: 33.3615 - mse: 33.3615 - mae: 1.6175 - val_loss: 12.0748 - val_mse: 12.0748 - val_mae: 2.6970 - lr: 1.4228e-04 - 999ms/epoch - 2ms/step\n",
            "Score for inner fold : loss of 12.074790954589844\n",
            "-----Fold:2--------\n",
            "Epoch 1/500\n",
            "556/556 - 2s - loss: 40.2499 - mse: 40.2499 - mae: 2.3270 - val_loss: 3.5345 - val_mse: 3.5345 - val_mae: 1.3609 - lr: 1.4228e-04 - 2s/epoch - 3ms/step\n",
            "Epoch 2/500\n",
            "556/556 - 1s - loss: 34.4754 - mse: 34.4754 - mae: 1.6294 - val_loss: 9.1555 - val_mse: 9.1555 - val_mae: 2.5042 - lr: 1.4228e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 3/500\n",
            "556/556 - 1s - loss: 32.6011 - mse: 32.6011 - mae: 1.6116 - val_loss: 10.4203 - val_mse: 10.4203 - val_mae: 2.6409 - lr: 1.4228e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 4/500\n",
            "556/556 - 1s - loss: 32.2500 - mse: 32.2500 - mae: 1.6128 - val_loss: 10.6251 - val_mse: 10.6251 - val_mae: 2.6054 - lr: 1.4228e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 5/500\n",
            "556/556 - 1s - loss: 32.1110 - mse: 32.1110 - mae: 1.6002 - val_loss: 10.8872 - val_mse: 10.8872 - val_mae: 2.6187 - lr: 1.4228e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 6/500\n",
            "556/556 - 1s - loss: 32.0297 - mse: 32.0297 - mae: 1.6021 - val_loss: 11.3254 - val_mse: 11.3254 - val_mae: 2.6238 - lr: 1.4228e-04 - 1s/epoch - 2ms/step\n",
            "Score for inner fold : loss of 11.325450897216797\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-10-10 09:53:12,171]\u001b[0m Finished trial#33 resulted in value: 12.053333333333333. Current best value is 8.253333333333334 with parameters: {'activation': 'tanh', 'num_hidden_layer': 3, 'i': 4, 'learning_rate': 0.00010295062747318013}.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trial Number:34\n",
            "-----Fold:0--------\n",
            "Epoch 1/500\n",
            "556/556 - 2s - loss: 23.1054 - mse: 23.1054 - mae: 2.5247 - val_loss: 1.5389 - val_mse: 1.5389 - val_mae: 0.9224 - lr: 1.0034e-04 - 2s/epoch - 3ms/step\n",
            "Epoch 2/500\n",
            "556/556 - 1s - loss: 20.4691 - mse: 20.4691 - mae: 2.1366 - val_loss: 2.5976 - val_mse: 2.5976 - val_mae: 1.2888 - lr: 1.0034e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 3/500\n",
            "556/556 - 1s - loss: 17.7355 - mse: 17.7355 - mae: 1.7648 - val_loss: 4.9855 - val_mse: 4.9855 - val_mae: 1.9121 - lr: 1.0034e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 4/500\n",
            "556/556 - 1s - loss: 15.8708 - mse: 15.8708 - mae: 1.6443 - val_loss: 7.2997 - val_mse: 7.2997 - val_mae: 2.3971 - lr: 1.0034e-04 - 990ms/epoch - 2ms/step\n",
            "Epoch 5/500\n",
            "556/556 - 1s - loss: 15.2155 - mse: 15.2155 - mae: 1.6506 - val_loss: 7.9719 - val_mse: 7.9719 - val_mae: 2.5154 - lr: 1.0034e-04 - 991ms/epoch - 2ms/step\n",
            "Epoch 6/500\n",
            "556/556 - 1s - loss: 14.9215 - mse: 14.9215 - mae: 1.6417 - val_loss: 8.0807 - val_mse: 8.0807 - val_mae: 2.5289 - lr: 1.0034e-04 - 1s/epoch - 2ms/step\n",
            "Score for inner fold : loss of 8.080708503723145\n",
            "-----Fold:1--------\n",
            "Epoch 1/500\n",
            "556/556 - 2s - loss: 42.3691 - mse: 42.3691 - mae: 2.4705 - val_loss: 1.5767 - val_mse: 1.5767 - val_mae: 0.9383 - lr: 1.0034e-04 - 2s/epoch - 3ms/step\n",
            "Epoch 2/500\n",
            "556/556 - 1s - loss: 39.0570 - mse: 39.0570 - mae: 1.9816 - val_loss: 3.0221 - val_mse: 3.0221 - val_mae: 1.4341 - lr: 1.0034e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 3/500\n",
            "556/556 - 1s - loss: 36.2071 - mse: 36.2071 - mae: 1.6558 - val_loss: 6.0744 - val_mse: 6.0744 - val_mae: 2.1852 - lr: 1.0034e-04 - 997ms/epoch - 2ms/step\n",
            "Epoch 4/500\n",
            "556/556 - 1s - loss: 34.8044 - mse: 34.8044 - mae: 1.6532 - val_loss: 7.8127 - val_mse: 7.8127 - val_mae: 2.4994 - lr: 1.0034e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 5/500\n",
            "556/556 - 1s - loss: 34.3156 - mse: 34.3156 - mae: 1.6661 - val_loss: 8.2622 - val_mse: 8.2622 - val_mae: 2.5579 - lr: 1.0034e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 6/500\n",
            "556/556 - 1s - loss: 33.9995 - mse: 33.9995 - mae: 1.6568 - val_loss: 8.3173 - val_mse: 8.3173 - val_mae: 2.5507 - lr: 1.0034e-04 - 1s/epoch - 2ms/step\n",
            "Score for inner fold : loss of 8.31725025177002\n",
            "-----Fold:2--------\n",
            "Epoch 1/500\n",
            "556/556 - 2s - loss: 44.0103 - mse: 44.0103 - mae: 2.4891 - val_loss: 1.4271 - val_mse: 1.4271 - val_mae: 0.8958 - lr: 1.0034e-04 - 2s/epoch - 3ms/step\n",
            "Epoch 2/500\n",
            "556/556 - 1s - loss: 40.5450 - mse: 40.5450 - mae: 1.9891 - val_loss: 3.0136 - val_mse: 3.0136 - val_mae: 1.4477 - lr: 1.0034e-04 - 997ms/epoch - 2ms/step\n",
            "Epoch 3/500\n",
            "556/556 - 1s - loss: 37.7253 - mse: 37.7253 - mae: 1.6790 - val_loss: 6.1860 - val_mse: 6.1860 - val_mae: 2.2004 - lr: 1.0034e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 4/500\n",
            "556/556 - 1s - loss: 36.3125 - mse: 36.3125 - mae: 1.6808 - val_loss: 7.9771 - val_mse: 7.9771 - val_mae: 2.5254 - lr: 1.0034e-04 - 996ms/epoch - 2ms/step\n",
            "Epoch 5/500\n",
            "556/556 - 1s - loss: 35.8038 - mse: 35.8038 - mae: 1.6873 - val_loss: 8.2128 - val_mse: 8.2128 - val_mae: 2.5534 - lr: 1.0034e-04 - 984ms/epoch - 2ms/step\n",
            "Epoch 6/500\n",
            "556/556 - 1s - loss: 35.4853 - mse: 35.4853 - mae: 1.6757 - val_loss: 8.3713 - val_mse: 8.3713 - val_mae: 2.5710 - lr: 1.0034e-04 - 1s/epoch - 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-10-10 09:53:36,064]\u001b[0m Finished trial#34 resulted in value: 8.256666666666666. Current best value is 8.253333333333334 with parameters: {'activation': 'tanh', 'num_hidden_layer': 3, 'i': 4, 'learning_rate': 0.00010295062747318013}.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Score for inner fold : loss of 8.371289253234863\n",
            "Trial Number:35\n",
            "-----Fold:0--------\n",
            "Epoch 1/500\n",
            "556/556 - 2s - loss: 43.2234 - mse: 43.2234 - mae: 2.5943 - val_loss: 1.3049 - val_mse: 1.3049 - val_mae: 0.8102 - lr: 1.0021e-04 - 2s/epoch - 3ms/step\n",
            "Epoch 2/500\n",
            "556/556 - 1s - loss: 41.1892 - mse: 41.1892 - mae: 2.3338 - val_loss: 1.6236 - val_mse: 1.6236 - val_mae: 0.9543 - lr: 1.0021e-04 - 935ms/epoch - 2ms/step\n",
            "Epoch 3/500\n",
            "556/556 - 1s - loss: 39.3216 - mse: 39.3216 - mae: 2.0570 - val_loss: 2.2674 - val_mse: 2.2674 - val_mae: 1.1907 - lr: 1.0021e-04 - 960ms/epoch - 2ms/step\n",
            "Epoch 4/500\n",
            "556/556 - 1s - loss: 37.6180 - mse: 37.6180 - mae: 1.8008 - val_loss: 3.3069 - val_mse: 3.3069 - val_mae: 1.5075 - lr: 1.0021e-04 - 950ms/epoch - 2ms/step\n",
            "Epoch 5/500\n",
            "556/556 - 1s - loss: 36.1209 - mse: 36.1209 - mae: 1.6228 - val_loss: 4.8895 - val_mse: 4.8895 - val_mae: 1.9000 - lr: 1.0021e-04 - 959ms/epoch - 2ms/step\n",
            "Epoch 6/500\n",
            "556/556 - 1s - loss: 35.0612 - mse: 35.0612 - mae: 1.5709 - val_loss: 6.5804 - val_mse: 6.5804 - val_mae: 2.2590 - lr: 1.0021e-04 - 942ms/epoch - 2ms/step\n",
            "Score for inner fold : loss of 6.580367088317871\n",
            "-----Fold:1--------\n",
            "Epoch 1/500\n",
            "556/556 - 2s - loss: 21.8091 - mse: 21.8091 - mae: 2.5903 - val_loss: 1.3998 - val_mse: 1.3998 - val_mae: 0.8593 - lr: 1.0021e-04 - 2s/epoch - 3ms/step\n",
            "Epoch 2/500\n",
            "556/556 - 1s - loss: 19.7804 - mse: 19.7804 - mae: 2.3037 - val_loss: 1.6684 - val_mse: 1.6684 - val_mae: 0.9760 - lr: 1.0021e-04 - 999ms/epoch - 2ms/step\n",
            "Epoch 3/500\n",
            "556/556 - 1s - loss: 17.9283 - mse: 17.9283 - mae: 2.0138 - val_loss: 2.3531 - val_mse: 2.3531 - val_mae: 1.2176 - lr: 1.0021e-04 - 969ms/epoch - 2ms/step\n",
            "Epoch 4/500\n",
            "556/556 - 1s - loss: 16.2512 - mse: 16.2512 - mae: 1.7555 - val_loss: 3.4884 - val_mse: 3.4884 - val_mae: 1.5508 - lr: 1.0021e-04 - 978ms/epoch - 2ms/step\n",
            "Epoch 5/500\n",
            "556/556 - 1s - loss: 14.8221 - mse: 14.8221 - mae: 1.5862 - val_loss: 5.0986 - val_mse: 5.0986 - val_mae: 1.9499 - lr: 1.0021e-04 - 999ms/epoch - 2ms/step\n",
            "Epoch 6/500\n",
            "556/556 - 1s - loss: 13.8494 - mse: 13.8494 - mae: 1.5535 - val_loss: 6.7533 - val_mse: 6.7533 - val_mae: 2.2984 - lr: 1.0021e-04 - 940ms/epoch - 2ms/step\n",
            "Score for inner fold : loss of 6.753294944763184\n",
            "-----Fold:2--------\n",
            "Epoch 1/500\n",
            "556/556 - 2s - loss: 47.2651 - mse: 47.2651 - mae: 2.5470 - val_loss: 1.5055 - val_mse: 1.5055 - val_mae: 0.8913 - lr: 1.0021e-04 - 2s/epoch - 3ms/step\n",
            "Epoch 2/500\n",
            "556/556 - 1s - loss: 45.1693 - mse: 45.1693 - mae: 2.3106 - val_loss: 1.7384 - val_mse: 1.7384 - val_mae: 1.0059 - lr: 1.0021e-04 - 959ms/epoch - 2ms/step\n",
            "Epoch 3/500\n",
            "556/556 - 1s - loss: 43.3527 - mse: 43.3527 - mae: 2.0876 - val_loss: 2.2649 - val_mse: 2.2649 - val_mae: 1.1982 - lr: 1.0021e-04 - 968ms/epoch - 2ms/step\n",
            "Epoch 4/500\n",
            "556/556 - 1s - loss: 41.6716 - mse: 41.6716 - mae: 1.8815 - val_loss: 3.1672 - val_mse: 3.1672 - val_mae: 1.4685 - lr: 1.0021e-04 - 998ms/epoch - 2ms/step\n",
            "Epoch 5/500\n",
            "556/556 - 1s - loss: 40.1777 - mse: 40.1777 - mae: 1.7271 - val_loss: 4.4096 - val_mse: 4.4096 - val_mae: 1.7890 - lr: 1.0021e-04 - 953ms/epoch - 2ms/step\n",
            "Epoch 6/500\n",
            "556/556 - 1s - loss: 38.9481 - mse: 38.9481 - mae: 1.6550 - val_loss: 5.9044 - val_mse: 5.9044 - val_mae: 2.1253 - lr: 1.0021e-04 - 939ms/epoch - 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-10-10 09:53:58,858]\u001b[0m Finished trial#35 resulted in value: 6.41. Current best value is 6.41 with parameters: {'activation': 'tanh', 'num_hidden_layer': 2, 'i': 4, 'learning_rate': 0.00010020536598530568}.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Score for inner fold : loss of 5.904425621032715\n",
            "Trial Number:36\n",
            "-----Fold:0--------\n",
            "Epoch 1/500\n",
            "556/556 - 2s - loss: 43.8144 - mse: 43.8144 - mae: 2.6017 - val_loss: 1.4216 - val_mse: 1.4216 - val_mae: 0.8895 - lr: 1.0440e-04 - 2s/epoch - 3ms/step\n",
            "Epoch 2/500\n",
            "556/556 - 1s - loss: 41.0681 - mse: 41.0681 - mae: 2.1993 - val_loss: 2.0632 - val_mse: 2.0632 - val_mae: 1.1202 - lr: 1.0440e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 3/500\n",
            "556/556 - 1s - loss: 38.5832 - mse: 38.5832 - mae: 1.8153 - val_loss: 3.5923 - val_mse: 3.5923 - val_mae: 1.5648 - lr: 1.0440e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 4/500\n",
            "556/556 - 1s - loss: 36.5888 - mse: 36.5888 - mae: 1.5906 - val_loss: 5.9620 - val_mse: 5.9620 - val_mae: 2.1149 - lr: 1.0440e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 5/500\n",
            "556/556 - 1s - loss: 35.4526 - mse: 35.4526 - mae: 1.5864 - val_loss: 7.7583 - val_mse: 7.7583 - val_mae: 2.4561 - lr: 1.0440e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 6/500\n",
            "556/556 - 1s - loss: 34.9427 - mse: 34.9427 - mae: 1.6170 - val_loss: 8.4692 - val_mse: 8.4692 - val_mae: 2.5647 - lr: 1.0440e-04 - 1s/epoch - 2ms/step\n",
            "Score for inner fold : loss of 8.46920394897461\n",
            "-----Fold:1--------\n",
            "Epoch 1/500\n",
            "556/556 - 2s - loss: 44.1569 - mse: 44.1569 - mae: 2.5011 - val_loss: 1.5434 - val_mse: 1.5434 - val_mae: 0.9146 - lr: 1.0440e-04 - 2s/epoch - 3ms/step\n",
            "Epoch 2/500\n",
            "556/556 - 1s - loss: 41.2998 - mse: 41.2998 - mae: 2.1589 - val_loss: 2.2447 - val_mse: 2.2447 - val_mae: 1.1667 - lr: 1.0440e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 3/500\n",
            "556/556 - 1s - loss: 38.9346 - mse: 38.9346 - mae: 1.8324 - val_loss: 3.6362 - val_mse: 3.6362 - val_mae: 1.5674 - lr: 1.0440e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 4/500\n",
            "556/556 - 1s - loss: 36.9949 - mse: 36.9949 - mae: 1.6152 - val_loss: 5.8782 - val_mse: 5.8782 - val_mae: 2.0895 - lr: 1.0440e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 5/500\n",
            "556/556 - 1s - loss: 35.8322 - mse: 35.8322 - mae: 1.5989 - val_loss: 7.6569 - val_mse: 7.6569 - val_mae: 2.4262 - lr: 1.0440e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 6/500\n",
            "556/556 - 1s - loss: 35.2533 - mse: 35.2533 - mae: 1.6306 - val_loss: 8.3564 - val_mse: 8.3564 - val_mae: 2.5422 - lr: 1.0440e-04 - 1s/epoch - 2ms/step\n",
            "Score for inner fold : loss of 8.35641098022461\n",
            "-----Fold:2--------\n",
            "Epoch 1/500\n",
            "556/556 - 2s - loss: 22.2441 - mse: 22.2441 - mae: 2.5246 - val_loss: 1.3051 - val_mse: 1.3051 - val_mae: 0.8181 - lr: 1.0440e-04 - 2s/epoch - 3ms/step\n",
            "Epoch 2/500\n",
            "556/556 - 1s - loss: 19.6028 - mse: 19.6028 - mae: 2.1898 - val_loss: 1.8127 - val_mse: 1.8127 - val_mae: 1.0404 - lr: 1.0440e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 3/500\n",
            "556/556 - 1s - loss: 17.1053 - mse: 17.1053 - mae: 1.8479 - val_loss: 3.1123 - val_mse: 3.1123 - val_mae: 1.4393 - lr: 1.0440e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 4/500\n",
            "556/556 - 1s - loss: 14.9195 - mse: 14.9195 - mae: 1.5939 - val_loss: 5.3317 - val_mse: 5.3317 - val_mae: 1.9767 - lr: 1.0440e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 5/500\n",
            "556/556 - 1s - loss: 13.5541 - mse: 13.5541 - mae: 1.5404 - val_loss: 7.5306 - val_mse: 7.5306 - val_mae: 2.4023 - lr: 1.0440e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 6/500\n",
            "556/556 - 1s - loss: 12.9474 - mse: 12.9474 - mae: 1.5830 - val_loss: 8.4189 - val_mse: 8.4189 - val_mae: 2.5426 - lr: 1.0440e-04 - 1s/epoch - 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-10-10 09:54:23,178]\u001b[0m Finished trial#36 resulted in value: 8.416666666666666. Current best value is 6.41 with parameters: {'activation': 'tanh', 'num_hidden_layer': 2, 'i': 4, 'learning_rate': 0.00010020536598530568}.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Score for inner fold : loss of 8.41886043548584\n",
            "Trial Number:37\n",
            "-----Fold:0--------\n",
            "Epoch 1/500\n",
            "556/556 - 2s - loss: 41.6552 - mse: 41.6552 - mae: 2.5305 - val_loss: 2.0469 - val_mse: 2.0469 - val_mae: 1.0123 - lr: 1.2794e-04 - 2s/epoch - 3ms/step\n",
            "Epoch 2/500\n",
            "556/556 - 1s - loss: 37.9035 - mse: 37.9035 - mae: 2.1143 - val_loss: 3.1104 - val_mse: 3.1104 - val_mae: 1.2535 - lr: 1.2794e-04 - 949ms/epoch - 2ms/step\n",
            "Epoch 3/500\n",
            "556/556 - 1s - loss: 35.3608 - mse: 35.3608 - mae: 1.7363 - val_loss: 4.9484 - val_mse: 4.9484 - val_mae: 1.6844 - lr: 1.2794e-04 - 948ms/epoch - 2ms/step\n",
            "Epoch 4/500\n",
            "556/556 - 1s - loss: 33.5429 - mse: 33.5429 - mae: 1.5218 - val_loss: 7.7157 - val_mse: 7.7157 - val_mae: 2.2312 - lr: 1.2794e-04 - 974ms/epoch - 2ms/step\n",
            "Epoch 5/500\n",
            "556/556 - 1s - loss: 32.6719 - mse: 32.6719 - mae: 1.5522 - val_loss: 9.5405 - val_mse: 9.5405 - val_mae: 2.5344 - lr: 1.2794e-04 - 937ms/epoch - 2ms/step\n",
            "Epoch 6/500\n",
            "556/556 - 1s - loss: 32.4009 - mse: 32.4009 - mae: 1.5994 - val_loss: 10.4358 - val_mse: 10.4358 - val_mae: 2.6396 - lr: 1.2794e-04 - 950ms/epoch - 2ms/step\n",
            "Score for inner fold : loss of 10.435791969299316\n",
            "-----Fold:1--------\n",
            "Epoch 1/500\n",
            "556/556 - 2s - loss: 44.7194 - mse: 44.7194 - mae: 2.7486 - val_loss: 2.3315 - val_mse: 2.3315 - val_mae: 1.1620 - lr: 1.2794e-04 - 2s/epoch - 3ms/step\n",
            "Epoch 2/500\n",
            "556/556 - 1s - loss: 39.0869 - mse: 39.0869 - mae: 2.2127 - val_loss: 3.1331 - val_mse: 3.1331 - val_mae: 1.2923 - lr: 1.2794e-04 - 974ms/epoch - 2ms/step\n",
            "Epoch 3/500\n",
            "556/556 - 1s - loss: 35.8858 - mse: 35.8858 - mae: 1.8379 - val_loss: 4.9039 - val_mse: 4.9039 - val_mae: 1.6496 - lr: 1.2794e-04 - 998ms/epoch - 2ms/step\n",
            "Epoch 4/500\n",
            "556/556 - 1s - loss: 33.7295 - mse: 33.7295 - mae: 1.5767 - val_loss: 7.2147 - val_mse: 7.2147 - val_mae: 2.1396 - lr: 1.2794e-04 - 981ms/epoch - 2ms/step\n",
            "Epoch 5/500\n",
            "556/556 - 1s - loss: 32.6558 - mse: 32.6558 - mae: 1.5438 - val_loss: 9.3709 - val_mse: 9.3709 - val_mae: 2.5008 - lr: 1.2794e-04 - 965ms/epoch - 2ms/step\n",
            "Epoch 6/500\n",
            "556/556 - 1s - loss: 32.2886 - mse: 32.2886 - mae: 1.5990 - val_loss: 10.1929 - val_mse: 10.1929 - val_mae: 2.6085 - lr: 1.2794e-04 - 972ms/epoch - 2ms/step\n",
            "Score for inner fold : loss of 10.192885398864746\n",
            "-----Fold:2--------\n",
            "Epoch 1/500\n",
            "556/556 - 2s - loss: 22.6542 - mse: 22.6542 - mae: 2.6285 - val_loss: 2.6867 - val_mse: 2.6867 - val_mae: 1.0631 - lr: 1.2794e-04 - 2s/epoch - 3ms/step\n",
            "Epoch 2/500\n",
            "556/556 - 1s - loss: 17.9508 - mse: 17.9508 - mae: 2.1940 - val_loss: 4.2175 - val_mse: 4.2175 - val_mae: 1.2327 - lr: 1.2794e-04 - 974ms/epoch - 2ms/step\n",
            "Epoch 3/500\n",
            "556/556 - 1s - loss: 15.2498 - mse: 15.2498 - mae: 1.8038 - val_loss: 6.3606 - val_mse: 6.3606 - val_mae: 1.6399 - lr: 1.2794e-04 - 959ms/epoch - 2ms/step\n",
            "Epoch 4/500\n",
            "556/556 - 1s - loss: 13.2972 - mse: 13.2972 - mae: 1.5279 - val_loss: 9.3162 - val_mse: 9.3162 - val_mae: 2.1906 - lr: 1.2794e-04 - 986ms/epoch - 2ms/step\n",
            "Epoch 5/500\n",
            "556/556 - 1s - loss: 12.3176 - mse: 12.3176 - mae: 1.5225 - val_loss: 11.6040 - val_mse: 11.6040 - val_mae: 2.5694 - lr: 1.2794e-04 - 978ms/epoch - 2ms/step\n",
            "Epoch 6/500\n",
            "556/556 - 1s - loss: 12.0209 - mse: 12.0209 - mae: 1.5758 - val_loss: 12.7310 - val_mse: 12.7310 - val_mae: 2.6767 - lr: 1.2794e-04 - 944ms/epoch - 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-10-10 09:54:46,032]\u001b[0m Finished trial#37 resulted in value: 11.12. Current best value is 6.41 with parameters: {'activation': 'tanh', 'num_hidden_layer': 2, 'i': 4, 'learning_rate': 0.00010020536598530568}.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Score for inner fold : loss of 12.73100757598877\n",
            "Trial Number:38\n",
            "-----Fold:0--------\n",
            "Epoch 1/500\n",
            "556/556 - 2s - loss: 15.5179 - mse: 15.5179 - mae: 1.7221 - val_loss: 8.9124 - val_mse: 8.9124 - val_mae: 2.4819 - lr: 3.2533e-04 - 2s/epoch - 3ms/step\n",
            "Epoch 2/500\n",
            "556/556 - 1s - loss: 12.2580 - mse: 12.2580 - mae: 1.6126 - val_loss: 9.9891 - val_mse: 9.9891 - val_mae: 2.5313 - lr: 3.2533e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 3/500\n",
            "556/556 - 1s - loss: 11.4187 - mse: 11.4187 - mae: 1.5820 - val_loss: 10.5802 - val_mse: 10.5802 - val_mae: 2.5391 - lr: 3.2533e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 4/500\n",
            "556/556 - 1s - loss: 10.9508 - mse: 10.9508 - mae: 1.5628 - val_loss: 12.0347 - val_mse: 12.0347 - val_mae: 2.6409 - lr: 3.2533e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 5/500\n",
            "556/556 - 1s - loss: 10.7026 - mse: 10.7026 - mae: 1.5526 - val_loss: 11.9456 - val_mse: 11.9456 - val_mae: 2.5848 - lr: 3.2533e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 6/500\n",
            "556/556 - 1s - loss: 10.5735 - mse: 10.5735 - mae: 1.5383 - val_loss: 13.0040 - val_mse: 13.0040 - val_mae: 2.6683 - lr: 3.2533e-04 - 1s/epoch - 2ms/step\n",
            "Score for inner fold : loss of 13.003986358642578\n",
            "-----Fold:1--------\n",
            "Epoch 1/500\n",
            "556/556 - 2s - loss: 34.3751 - mse: 34.3751 - mae: 1.6836 - val_loss: 10.4605 - val_mse: 10.4605 - val_mae: 2.5991 - lr: 3.2533e-04 - 2s/epoch - 3ms/step\n",
            "Epoch 2/500\n",
            "556/556 - 1s - loss: 33.0667 - mse: 33.0667 - mae: 1.6486 - val_loss: 11.0133 - val_mse: 11.0133 - val_mae: 2.6030 - lr: 3.2533e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 3/500\n",
            "556/556 - 1s - loss: 32.8554 - mse: 32.8554 - mae: 1.6236 - val_loss: 11.5316 - val_mse: 11.5316 - val_mae: 2.6420 - lr: 3.2533e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 4/500\n",
            "556/556 - 1s - loss: 32.6571 - mse: 32.6571 - mae: 1.6221 - val_loss: 11.6245 - val_mse: 11.6245 - val_mae: 2.5857 - lr: 3.2533e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 5/500\n",
            "556/556 - 1s - loss: 32.5887 - mse: 32.5887 - mae: 1.6055 - val_loss: 12.5774 - val_mse: 12.5774 - val_mae: 2.6714 - lr: 3.2533e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 6/500\n",
            "556/556 - 1s - loss: 32.4901 - mse: 32.4901 - mae: 1.6069 - val_loss: 12.2776 - val_mse: 12.2776 - val_mae: 2.6232 - lr: 3.2533e-04 - 1s/epoch - 2ms/step\n",
            "Score for inner fold : loss of 12.27762508392334\n",
            "-----Fold:2--------\n",
            "Epoch 1/500\n",
            "556/556 - 2s - loss: 37.0772 - mse: 37.0772 - mae: 1.8566 - val_loss: 8.7344 - val_mse: 8.7344 - val_mae: 2.4162 - lr: 3.2533e-04 - 2s/epoch - 3ms/step\n",
            "Epoch 2/500\n",
            "556/556 - 1s - loss: 33.6067 - mse: 33.6067 - mae: 1.6320 - val_loss: 9.9983 - val_mse: 9.9983 - val_mae: 2.5824 - lr: 3.2533e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 3/500\n",
            "556/556 - 1s - loss: 32.9765 - mse: 32.9765 - mae: 1.6135 - val_loss: 10.3679 - val_mse: 10.3679 - val_mae: 2.5995 - lr: 3.2533e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 4/500\n",
            "556/556 - 1s - loss: 32.6594 - mse: 32.6594 - mae: 1.5984 - val_loss: 10.6193 - val_mse: 10.6193 - val_mae: 2.5897 - lr: 3.2533e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 5/500\n",
            "556/556 - 1s - loss: 32.4447 - mse: 32.4447 - mae: 1.5807 - val_loss: 11.0208 - val_mse: 11.0208 - val_mae: 2.6265 - lr: 3.2533e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 6/500\n",
            "556/556 - 1s - loss: 32.3494 - mse: 32.3494 - mae: 1.5822 - val_loss: 10.8439 - val_mse: 10.8439 - val_mae: 2.5648 - lr: 3.2533e-04 - 1s/epoch - 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-10-10 09:55:10,437]\u001b[0m Finished trial#38 resulted in value: 12.040000000000001. Current best value is 6.41 with parameters: {'activation': 'tanh', 'num_hidden_layer': 2, 'i': 4, 'learning_rate': 0.00010020536598530568}.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Score for inner fold : loss of 10.8439359664917\n",
            "Trial Number:39\n",
            "-----Fold:0--------\n",
            "Epoch 1/500\n",
            "556/556 - 2s - loss: 21.4884 - mse: 21.4884 - mae: 2.4818 - val_loss: 1.4146 - val_mse: 1.4146 - val_mae: 0.8637 - lr: 1.0651e-04 - 2s/epoch - 3ms/step\n",
            "Epoch 2/500\n",
            "556/556 - 1s - loss: 18.3977 - mse: 18.3977 - mae: 2.0264 - val_loss: 2.6439 - val_mse: 2.6439 - val_mae: 1.3231 - lr: 1.0651e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 3/500\n",
            "556/556 - 1s - loss: 15.3012 - mse: 15.3012 - mae: 1.6544 - val_loss: 5.7987 - val_mse: 5.7987 - val_mae: 2.1123 - lr: 1.0651e-04 - 975ms/epoch - 2ms/step\n",
            "Epoch 4/500\n",
            "556/556 - 1s - loss: 13.5904 - mse: 13.5904 - mae: 1.6279 - val_loss: 7.9231 - val_mse: 7.9231 - val_mae: 2.5056 - lr: 1.0651e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 5/500\n",
            "556/556 - 1s - loss: 13.0587 - mse: 13.0587 - mae: 1.6440 - val_loss: 8.3779 - val_mse: 8.3779 - val_mae: 2.5633 - lr: 1.0651e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 6/500\n",
            "556/556 - 1s - loss: 12.7862 - mse: 12.7862 - mae: 1.6346 - val_loss: 8.4058 - val_mse: 8.4058 - val_mae: 2.5541 - lr: 1.0651e-04 - 1s/epoch - 2ms/step\n",
            "Score for inner fold : loss of 8.405841827392578\n",
            "-----Fold:1--------\n",
            "Epoch 1/500\n",
            "556/556 - 2s - loss: 44.6325 - mse: 44.6325 - mae: 2.4891 - val_loss: 1.3396 - val_mse: 1.3396 - val_mae: 0.8670 - lr: 1.0651e-04 - 2s/epoch - 3ms/step\n",
            "Epoch 2/500\n",
            "556/556 - 1s - loss: 41.3056 - mse: 41.3056 - mae: 1.9971 - val_loss: 2.8964 - val_mse: 2.8964 - val_mae: 1.4157 - lr: 1.0651e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 3/500\n",
            "556/556 - 1s - loss: 38.2854 - mse: 38.2854 - mae: 1.6812 - val_loss: 6.1743 - val_mse: 6.1743 - val_mae: 2.2106 - lr: 1.0651e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 4/500\n",
            "556/556 - 1s - loss: 36.9095 - mse: 36.9095 - mae: 1.6873 - val_loss: 7.6730 - val_mse: 7.6730 - val_mae: 2.4852 - lr: 1.0651e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 5/500\n",
            "556/556 - 1s - loss: 36.4422 - mse: 36.4422 - mae: 1.6892 - val_loss: 8.1030 - val_mse: 8.1030 - val_mae: 2.5487 - lr: 1.0651e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 6/500\n",
            "556/556 - 1s - loss: 36.1488 - mse: 36.1488 - mae: 1.6734 - val_loss: 8.2869 - val_mse: 8.2869 - val_mae: 2.5620 - lr: 1.0651e-04 - 1s/epoch - 2ms/step\n",
            "Score for inner fold : loss of 8.28689193725586\n",
            "-----Fold:2--------\n",
            "Epoch 1/500\n",
            "556/556 - 2s - loss: 43.3421 - mse: 43.3421 - mae: 2.5657 - val_loss: 1.5151 - val_mse: 1.5151 - val_mae: 0.9354 - lr: 1.0651e-04 - 2s/epoch - 3ms/step\n",
            "Epoch 2/500\n",
            "556/556 - 1s - loss: 40.0954 - mse: 40.0954 - mae: 2.0362 - val_loss: 3.2682 - val_mse: 3.2682 - val_mae: 1.5006 - lr: 1.0651e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 3/500\n",
            "556/556 - 1s - loss: 37.1789 - mse: 37.1789 - mae: 1.6867 - val_loss: 6.7390 - val_mse: 6.7390 - val_mae: 2.2900 - lr: 1.0651e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 4/500\n",
            "556/556 - 1s - loss: 35.9774 - mse: 35.9774 - mae: 1.6905 - val_loss: 8.0899 - val_mse: 8.0899 - val_mae: 2.5346 - lr: 1.0651e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 5/500\n",
            "556/556 - 1s - loss: 35.5763 - mse: 35.5763 - mae: 1.6862 - val_loss: 8.2652 - val_mse: 8.2652 - val_mae: 2.5557 - lr: 1.0651e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 6/500\n",
            "556/556 - 1s - loss: 35.3031 - mse: 35.3031 - mae: 1.6683 - val_loss: 8.3369 - val_mse: 8.3369 - val_mae: 2.5552 - lr: 1.0651e-04 - 1s/epoch - 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-10-10 09:55:34,818]\u001b[0m Finished trial#39 resulted in value: 8.346666666666666. Current best value is 6.41 with parameters: {'activation': 'tanh', 'num_hidden_layer': 2, 'i': 4, 'learning_rate': 0.00010020536598530568}.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Score for inner fold : loss of 8.336869239807129\n",
            "Trial Number:40\n",
            "-----Fold:0--------\n",
            "Epoch 1/500\n",
            "556/556 - 2s - loss: 44.3500 - mse: 44.3500 - mae: 2.4927 - val_loss: 1.5645 - val_mse: 1.5645 - val_mae: 0.9195 - lr: 1.3330e-04 - 2s/epoch - 3ms/step\n",
            "Epoch 2/500\n",
            "556/556 - 1s - loss: 40.7073 - mse: 40.7073 - mae: 2.0228 - val_loss: 2.6193 - val_mse: 2.6193 - val_mae: 1.3070 - lr: 1.3330e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 3/500\n",
            "556/556 - 1s - loss: 37.5446 - mse: 37.5446 - mae: 1.6572 - val_loss: 5.2063 - val_mse: 5.2063 - val_mae: 1.9666 - lr: 1.3330e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 4/500\n",
            "556/556 - 1s - loss: 35.6515 - mse: 35.6515 - mae: 1.5878 - val_loss: 7.6646 - val_mse: 7.6646 - val_mae: 2.4550 - lr: 1.3330e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 5/500\n",
            "556/556 - 1s - loss: 34.9321 - mse: 34.9321 - mae: 1.6254 - val_loss: 8.6210 - val_mse: 8.6210 - val_mae: 2.5900 - lr: 1.3330e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 6/500\n",
            "556/556 - 1s - loss: 34.5872 - mse: 34.5872 - mae: 1.6298 - val_loss: 8.8244 - val_mse: 8.8244 - val_mae: 2.5950 - lr: 1.3330e-04 - 1s/epoch - 2ms/step\n",
            "Score for inner fold : loss of 8.824376106262207\n",
            "-----Fold:1--------\n",
            "Epoch 1/500\n",
            "556/556 - 2s - loss: 44.0295 - mse: 44.0295 - mae: 2.5040 - val_loss: 1.6934 - val_mse: 1.6934 - val_mae: 0.9739 - lr: 1.3330e-04 - 2s/epoch - 3ms/step\n",
            "Epoch 2/500\n",
            "556/556 - 1s - loss: 40.2968 - mse: 40.2968 - mae: 1.9803 - val_loss: 3.0872 - val_mse: 3.0872 - val_mae: 1.4220 - lr: 1.3330e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 3/500\n",
            "556/556 - 1s - loss: 37.3566 - mse: 37.3566 - mae: 1.6049 - val_loss: 6.2083 - val_mse: 6.2083 - val_mae: 2.1483 - lr: 1.3330e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 4/500\n",
            "556/556 - 1s - loss: 35.8887 - mse: 35.8887 - mae: 1.5960 - val_loss: 8.2229 - val_mse: 8.2229 - val_mae: 2.5129 - lr: 1.3330e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 5/500\n",
            "556/556 - 1s - loss: 35.3462 - mse: 35.3462 - mae: 1.6275 - val_loss: 8.8458 - val_mse: 8.8458 - val_mae: 2.5850 - lr: 1.3330e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 6/500\n",
            "556/556 - 1s - loss: 35.0701 - mse: 35.0701 - mae: 1.6303 - val_loss: 8.8916 - val_mse: 8.8916 - val_mae: 2.5833 - lr: 1.3330e-04 - 1s/epoch - 2ms/step\n",
            "Score for inner fold : loss of 8.891623497009277\n",
            "-----Fold:2--------\n",
            "Epoch 1/500\n",
            "556/556 - 2s - loss: 20.7361 - mse: 20.7361 - mae: 2.4892 - val_loss: 1.5326 - val_mse: 1.5326 - val_mae: 0.9346 - lr: 1.3330e-04 - 2s/epoch - 3ms/step\n",
            "Epoch 2/500\n",
            "556/556 - 1s - loss: 17.5001 - mse: 17.5001 - mae: 2.0163 - val_loss: 2.6981 - val_mse: 2.6981 - val_mae: 1.3290 - lr: 1.3330e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 3/500\n",
            "556/556 - 1s - loss: 14.7009 - mse: 14.7009 - mae: 1.6362 - val_loss: 5.2733 - val_mse: 5.2733 - val_mae: 1.9800 - lr: 1.3330e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 4/500\n",
            "556/556 - 1s - loss: 12.9666 - mse: 12.9666 - mae: 1.5568 - val_loss: 7.6386 - val_mse: 7.6386 - val_mae: 2.4482 - lr: 1.3330e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 5/500\n",
            "556/556 - 1s - loss: 12.2960 - mse: 12.2960 - mae: 1.5936 - val_loss: 8.4231 - val_mse: 8.4231 - val_mae: 2.5651 - lr: 1.3330e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 6/500\n",
            "556/556 - 1s - loss: 11.9591 - mse: 11.9591 - mae: 1.5925 - val_loss: 8.7808 - val_mse: 8.7808 - val_mae: 2.5904 - lr: 1.3330e-04 - 1s/epoch - 2ms/step\n",
            "Score for inner fold : loss of 8.780818939208984\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-10-10 09:55:59,351]\u001b[0m Finished trial#40 resulted in value: 8.83. Current best value is 6.41 with parameters: {'activation': 'tanh', 'num_hidden_layer': 2, 'i': 4, 'learning_rate': 0.00010020536598530568}.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trial Number:41\n",
            "-----Fold:0--------\n",
            "Epoch 1/500\n",
            "556/556 - 2s - loss: 42.5215 - mse: 42.5215 - mae: 2.5715 - val_loss: 1.4204 - val_mse: 1.4204 - val_mae: 0.8825 - lr: 1.0018e-04 - 2s/epoch - 3ms/step\n",
            "Epoch 2/500\n",
            "556/556 - 1s - loss: 39.7763 - mse: 39.7763 - mae: 2.1136 - val_loss: 2.5077 - val_mse: 2.5077 - val_mae: 1.2716 - lr: 1.0018e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 3/500\n",
            "556/556 - 1s - loss: 36.9457 - mse: 36.9457 - mae: 1.6840 - val_loss: 5.3522 - val_mse: 5.3522 - val_mae: 2.0152 - lr: 1.0018e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 4/500\n",
            "556/556 - 1s - loss: 35.3665 - mse: 35.3665 - mae: 1.6312 - val_loss: 7.5447 - val_mse: 7.5447 - val_mae: 2.4543 - lr: 1.0018e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 5/500\n",
            "556/556 - 1s - loss: 34.9310 - mse: 34.9310 - mae: 1.6653 - val_loss: 8.0925 - val_mse: 8.0925 - val_mae: 2.5462 - lr: 1.0018e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 6/500\n",
            "556/556 - 1s - loss: 34.7136 - mse: 34.7136 - mae: 1.6642 - val_loss: 8.1970 - val_mse: 8.1970 - val_mae: 2.5450 - lr: 1.0018e-04 - 1s/epoch - 2ms/step\n",
            "Score for inner fold : loss of 8.196950912475586\n",
            "-----Fold:1--------\n",
            "Epoch 1/500\n",
            "556/556 - 2s - loss: 20.6161 - mse: 20.6161 - mae: 2.4857 - val_loss: 1.5353 - val_mse: 1.5353 - val_mae: 0.9208 - lr: 1.0018e-04 - 2s/epoch - 3ms/step\n",
            "Epoch 2/500\n",
            "556/556 - 1s - loss: 17.6065 - mse: 17.6065 - mae: 2.0015 - val_loss: 2.8970 - val_mse: 2.8970 - val_mae: 1.3848 - lr: 1.0018e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 3/500\n",
            "556/556 - 1s - loss: 14.8459 - mse: 14.8459 - mae: 1.6475 - val_loss: 5.8147 - val_mse: 5.8147 - val_mae: 2.0951 - lr: 1.0018e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 4/500\n",
            "556/556 - 1s - loss: 13.3221 - mse: 13.3221 - mae: 1.6062 - val_loss: 7.9597 - val_mse: 7.9597 - val_mae: 2.4902 - lr: 1.0018e-04 - 995ms/epoch - 2ms/step\n",
            "Epoch 5/500\n",
            "556/556 - 1s - loss: 12.8766 - mse: 12.8766 - mae: 1.6202 - val_loss: 8.5065 - val_mse: 8.5065 - val_mae: 2.5719 - lr: 1.0018e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 6/500\n",
            "556/556 - 1s - loss: 12.6872 - mse: 12.6872 - mae: 1.6244 - val_loss: 8.6023 - val_mse: 8.6023 - val_mae: 2.5784 - lr: 1.0018e-04 - 1s/epoch - 2ms/step\n",
            "Score for inner fold : loss of 8.602337837219238\n",
            "-----Fold:2--------\n",
            "Epoch 1/500\n",
            "556/556 - 2s - loss: 44.9514 - mse: 44.9514 - mae: 2.4904 - val_loss: 1.5865 - val_mse: 1.5865 - val_mae: 0.9445 - lr: 1.0018e-04 - 2s/epoch - 3ms/step\n",
            "Epoch 2/500\n",
            "556/556 - 1s - loss: 41.5827 - mse: 41.5827 - mae: 2.0163 - val_loss: 2.9286 - val_mse: 2.9286 - val_mae: 1.4034 - lr: 1.0018e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 3/500\n",
            "556/556 - 1s - loss: 38.6444 - mse: 38.6444 - mae: 1.6737 - val_loss: 6.0352 - val_mse: 6.0352 - val_mae: 2.1544 - lr: 1.0018e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 4/500\n",
            "556/556 - 1s - loss: 37.1552 - mse: 37.1552 - mae: 1.6669 - val_loss: 7.8683 - val_mse: 7.8683 - val_mae: 2.4923 - lr: 1.0018e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 5/500\n",
            "556/556 - 1s - loss: 36.7193 - mse: 36.7193 - mae: 1.6905 - val_loss: 8.1357 - val_mse: 8.1357 - val_mae: 2.5222 - lr: 1.0018e-04 - 992ms/epoch - 2ms/step\n",
            "Epoch 6/500\n",
            "556/556 - 1s - loss: 36.5035 - mse: 36.5035 - mae: 1.6783 - val_loss: 8.2537 - val_mse: 8.2537 - val_mae: 2.5290 - lr: 1.0018e-04 - 1s/epoch - 2ms/step\n",
            "Score for inner fold : loss of 8.253722190856934\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-10-10 09:56:23,556]\u001b[0m Finished trial#41 resulted in value: 8.35. Current best value is 6.41 with parameters: {'activation': 'tanh', 'num_hidden_layer': 2, 'i': 4, 'learning_rate': 0.00010020536598530568}.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trial Number:42\n",
            "-----Fold:0--------\n",
            "Epoch 1/500\n",
            "556/556 - 2s - loss: 43.8405 - mse: 43.8405 - mae: 2.5182 - val_loss: 1.4111 - val_mse: 1.4111 - val_mae: 0.8914 - lr: 1.0910e-04 - 2s/epoch - 3ms/step\n",
            "Epoch 2/500\n",
            "556/556 - 1s - loss: 40.3147 - mse: 40.3147 - mae: 2.0140 - val_loss: 2.7621 - val_mse: 2.7621 - val_mae: 1.3767 - lr: 1.0910e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 3/500\n",
            "556/556 - 1s - loss: 37.0660 - mse: 37.0660 - mae: 1.6536 - val_loss: 6.1913 - val_mse: 6.1913 - val_mae: 2.2103 - lr: 1.0910e-04 - 977ms/epoch - 2ms/step\n",
            "Epoch 4/500\n",
            "556/556 - 1s - loss: 35.6624 - mse: 35.6624 - mae: 1.6619 - val_loss: 7.9091 - val_mse: 7.9091 - val_mae: 2.5124 - lr: 1.0910e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 5/500\n",
            "556/556 - 1s - loss: 35.3108 - mse: 35.3108 - mae: 1.6762 - val_loss: 8.1270 - val_mse: 8.1270 - val_mae: 2.5317 - lr: 1.0910e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 6/500\n",
            "556/556 - 1s - loss: 35.0673 - mse: 35.0673 - mae: 1.6623 - val_loss: 8.2954 - val_mse: 8.2954 - val_mae: 2.5417 - lr: 1.0910e-04 - 1s/epoch - 2ms/step\n",
            "Score for inner fold : loss of 8.295381546020508\n",
            "-----Fold:1--------\n",
            "Epoch 1/500\n",
            "556/556 - 2s - loss: 43.5532 - mse: 43.5532 - mae: 2.4992 - val_loss: 1.3269 - val_mse: 1.3269 - val_mae: 0.8539 - lr: 1.0910e-04 - 2s/epoch - 3ms/step\n",
            "Epoch 2/500\n",
            "556/556 - 1s - loss: 40.4113 - mse: 40.4113 - mae: 1.9863 - val_loss: 2.9771 - val_mse: 2.9771 - val_mae: 1.4288 - lr: 1.0910e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 3/500\n",
            "556/556 - 1s - loss: 37.4595 - mse: 37.4595 - mae: 1.6246 - val_loss: 6.5895 - val_mse: 6.5895 - val_mae: 2.2756 - lr: 1.0910e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 4/500\n",
            "556/556 - 1s - loss: 36.2640 - mse: 36.2640 - mae: 1.6465 - val_loss: 7.9368 - val_mse: 7.9368 - val_mae: 2.5141 - lr: 1.0910e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 5/500\n",
            "556/556 - 1s - loss: 35.8617 - mse: 35.8617 - mae: 1.6498 - val_loss: 8.3211 - val_mse: 8.3211 - val_mae: 2.5617 - lr: 1.0910e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 6/500\n",
            "556/556 - 1s - loss: 35.6058 - mse: 35.6058 - mae: 1.6421 - val_loss: 8.5407 - val_mse: 8.5407 - val_mae: 2.5712 - lr: 1.0910e-04 - 994ms/epoch - 2ms/step\n",
            "Score for inner fold : loss of 8.540693283081055\n",
            "-----Fold:2--------\n",
            "Epoch 1/500\n",
            "556/556 - 2s - loss: 21.7575 - mse: 21.7575 - mae: 2.4536 - val_loss: 1.6588 - val_mse: 1.6588 - val_mae: 0.9699 - lr: 1.0910e-04 - 2s/epoch - 3ms/step\n",
            "Epoch 2/500\n",
            "556/556 - 1s - loss: 18.4558 - mse: 18.4558 - mae: 1.9704 - val_loss: 3.2482 - val_mse: 3.2482 - val_mae: 1.5004 - lr: 1.0910e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 3/500\n",
            "556/556 - 1s - loss: 15.4199 - mse: 15.4199 - mae: 1.6453 - val_loss: 6.7181 - val_mse: 6.7181 - val_mae: 2.3026 - lr: 1.0910e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 4/500\n",
            "556/556 - 1s - loss: 14.0816 - mse: 14.0816 - mae: 1.6513 - val_loss: 7.9447 - val_mse: 7.9447 - val_mae: 2.5240 - lr: 1.0910e-04 - 976ms/epoch - 2ms/step\n",
            "Epoch 5/500\n",
            "556/556 - 1s - loss: 13.6229 - mse: 13.6229 - mae: 1.6470 - val_loss: 8.1884 - val_mse: 8.1884 - val_mae: 2.5559 - lr: 1.0910e-04 - 969ms/epoch - 2ms/step\n",
            "Epoch 6/500\n",
            "556/556 - 1s - loss: 13.3269 - mse: 13.3269 - mae: 1.6331 - val_loss: 8.3353 - val_mse: 8.3353 - val_mae: 2.5695 - lr: 1.0910e-04 - 1s/epoch - 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-10-10 09:56:47,712]\u001b[0m Finished trial#42 resulted in value: 8.393333333333333. Current best value is 6.41 with parameters: {'activation': 'tanh', 'num_hidden_layer': 2, 'i': 4, 'learning_rate': 0.00010020536598530568}.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Score for inner fold : loss of 8.335332870483398\n",
            "Trial Number:43\n",
            "-----Fold:0--------\n",
            "Epoch 1/500\n",
            "556/556 - 2s - loss: 42.8748 - mse: 42.8748 - mae: 2.3285 - val_loss: 1.9348 - val_mse: 1.9348 - val_mae: 1.1233 - lr: 2.5896e-04 - 2s/epoch - 3ms/step\n",
            "Epoch 2/500\n",
            "556/556 - 1s - loss: 38.3822 - mse: 38.3822 - mae: 1.7087 - val_loss: 6.0324 - val_mse: 6.0324 - val_mae: 2.2359 - lr: 2.5896e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 3/500\n",
            "556/556 - 1s - loss: 36.4715 - mse: 36.4715 - mae: 1.7050 - val_loss: 7.8580 - val_mse: 7.8580 - val_mae: 2.5362 - lr: 2.5896e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 4/500\n",
            "556/556 - 1s - loss: 36.0307 - mse: 36.0307 - mae: 1.7144 - val_loss: 8.1197 - val_mse: 8.1197 - val_mae: 2.5631 - lr: 2.5896e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 5/500\n",
            "556/556 - 1s - loss: 35.7338 - mse: 35.7338 - mae: 1.6981 - val_loss: 8.1533 - val_mse: 8.1533 - val_mae: 2.5482 - lr: 2.5896e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 6/500\n",
            "556/556 - 1s - loss: 35.4766 - mse: 35.4766 - mae: 1.6789 - val_loss: 8.1013 - val_mse: 8.1013 - val_mae: 2.5330 - lr: 2.5896e-04 - 1s/epoch - 2ms/step\n",
            "Score for inner fold : loss of 8.101272583007812\n",
            "-----Fold:1--------\n",
            "Epoch 1/500\n",
            "556/556 - 2s - loss: 43.3973 - mse: 43.3973 - mae: 2.4505 - val_loss: 1.8334 - val_mse: 1.8334 - val_mae: 1.0530 - lr: 2.5896e-04 - 2s/epoch - 3ms/step\n",
            "Epoch 2/500\n",
            "556/556 - 1s - loss: 38.7370 - mse: 38.7370 - mae: 1.7597 - val_loss: 6.0289 - val_mse: 6.0289 - val_mae: 2.2015 - lr: 2.5896e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 3/500\n",
            "556/556 - 1s - loss: 36.4901 - mse: 36.4901 - mae: 1.6947 - val_loss: 7.4985 - val_mse: 7.4985 - val_mae: 2.4776 - lr: 2.5896e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 4/500\n",
            "556/556 - 1s - loss: 36.0563 - mse: 36.0563 - mae: 1.6840 - val_loss: 8.0434 - val_mse: 8.0434 - val_mae: 2.5375 - lr: 2.5896e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 5/500\n",
            "556/556 - 1s - loss: 35.7630 - mse: 35.7630 - mae: 1.6681 - val_loss: 8.1304 - val_mse: 8.1304 - val_mae: 2.5414 - lr: 2.5896e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 6/500\n",
            "556/556 - 1s - loss: 35.5023 - mse: 35.5023 - mae: 1.6548 - val_loss: 8.1776 - val_mse: 8.1776 - val_mae: 2.5256 - lr: 2.5896e-04 - 987ms/epoch - 2ms/step\n",
            "Score for inner fold : loss of 8.177602767944336\n",
            "-----Fold:2--------\n",
            "Epoch 1/500\n",
            "556/556 - 2s - loss: 19.9244 - mse: 19.9244 - mae: 2.2196 - val_loss: 2.7161 - val_mse: 2.7161 - val_mae: 1.3196 - lr: 2.5896e-04 - 2s/epoch - 3ms/step\n",
            "Epoch 2/500\n",
            "556/556 - 1s - loss: 15.6096 - mse: 15.6096 - mae: 1.6446 - val_loss: 7.0683 - val_mse: 7.0683 - val_mae: 2.3745 - lr: 2.5896e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 3/500\n",
            "556/556 - 1s - loss: 14.1580 - mse: 14.1580 - mae: 1.6418 - val_loss: 8.0954 - val_mse: 8.0954 - val_mae: 2.5426 - lr: 2.5896e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 4/500\n",
            "556/556 - 1s - loss: 13.7278 - mse: 13.7278 - mae: 1.6252 - val_loss: 8.1947 - val_mse: 8.1947 - val_mae: 2.5428 - lr: 2.5896e-04 - 987ms/epoch - 2ms/step\n",
            "Epoch 5/500\n",
            "556/556 - 1s - loss: 13.4344 - mse: 13.4344 - mae: 1.5999 - val_loss: 8.3402 - val_mse: 8.3402 - val_mae: 2.5503 - lr: 2.5896e-04 - 988ms/epoch - 2ms/step\n",
            "Epoch 6/500\n",
            "556/556 - 1s - loss: 13.2094 - mse: 13.2094 - mae: 1.5867 - val_loss: 8.5408 - val_mse: 8.5408 - val_mae: 2.5674 - lr: 2.5896e-04 - 1s/epoch - 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-10-10 09:57:11,808]\u001b[0m Finished trial#43 resulted in value: 8.273333333333333. Current best value is 6.41 with parameters: {'activation': 'tanh', 'num_hidden_layer': 2, 'i': 4, 'learning_rate': 0.00010020536598530568}.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Score for inner fold : loss of 8.540799140930176\n",
            "Trial Number:44\n",
            "-----Fold:0--------\n",
            "Epoch 1/500\n",
            "556/556 - 2s - loss: 22.1661 - mse: 22.1661 - mae: 2.3197 - val_loss: 2.2224 - val_mse: 2.2224 - val_mae: 1.1745 - lr: 2.4376e-04 - 2s/epoch - 3ms/step\n",
            "Epoch 2/500\n",
            "556/556 - 1s - loss: 17.8927 - mse: 17.8927 - mae: 1.7136 - val_loss: 6.1818 - val_mse: 6.1818 - val_mae: 2.2284 - lr: 2.4376e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 3/500\n",
            "556/556 - 1s - loss: 15.7696 - mse: 15.7696 - mae: 1.6813 - val_loss: 7.8646 - val_mse: 7.8646 - val_mae: 2.5309 - lr: 2.4376e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 4/500\n",
            "556/556 - 1s - loss: 15.1732 - mse: 15.1732 - mae: 1.6753 - val_loss: 8.1308 - val_mse: 8.1308 - val_mae: 2.5548 - lr: 2.4376e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 5/500\n",
            "556/556 - 1s - loss: 14.7292 - mse: 14.7292 - mae: 1.6453 - val_loss: 8.3716 - val_mse: 8.3716 - val_mae: 2.5860 - lr: 2.4376e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 6/500\n",
            "556/556 - 1s - loss: 14.3786 - mse: 14.3786 - mae: 1.6296 - val_loss: 8.1388 - val_mse: 8.1388 - val_mae: 2.5199 - lr: 2.4376e-04 - 1s/epoch - 2ms/step\n",
            "Score for inner fold : loss of 8.138836860656738\n",
            "-----Fold:1--------\n",
            "Epoch 1/500\n",
            "556/556 - 2s - loss: 43.6757 - mse: 43.6757 - mae: 2.3467 - val_loss: 1.8315 - val_mse: 1.8315 - val_mae: 1.0471 - lr: 2.4376e-04 - 2s/epoch - 3ms/step\n",
            "Epoch 2/500\n",
            "556/556 - 1s - loss: 39.4259 - mse: 39.4259 - mae: 1.7640 - val_loss: 5.2025 - val_mse: 5.2025 - val_mae: 2.0398 - lr: 2.4376e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 3/500\n",
            "556/556 - 1s - loss: 36.7110 - mse: 36.7110 - mae: 1.6922 - val_loss: 7.7046 - val_mse: 7.7046 - val_mae: 2.5186 - lr: 2.4376e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 4/500\n",
            "556/556 - 1s - loss: 36.0968 - mse: 36.0968 - mae: 1.7121 - val_loss: 8.0309 - val_mse: 8.0309 - val_mae: 2.5590 - lr: 2.4376e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 5/500\n",
            "556/556 - 1s - loss: 35.7679 - mse: 35.7679 - mae: 1.6882 - val_loss: 8.0819 - val_mse: 8.0819 - val_mae: 2.5468 - lr: 2.4376e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 6/500\n",
            "556/556 - 1s - loss: 35.4986 - mse: 35.4986 - mae: 1.6635 - val_loss: 8.3458 - val_mse: 8.3458 - val_mae: 2.5714 - lr: 2.4376e-04 - 1s/epoch - 2ms/step\n",
            "Score for inner fold : loss of 8.345829963684082\n",
            "-----Fold:2--------\n",
            "Epoch 1/500\n",
            "556/556 - 2s - loss: 42.4621 - mse: 42.4621 - mae: 2.3551 - val_loss: 2.1656 - val_mse: 2.1656 - val_mae: 1.1730 - lr: 2.4376e-04 - 2s/epoch - 3ms/step\n",
            "Epoch 2/500\n",
            "556/556 - 1s - loss: 37.2419 - mse: 37.2419 - mae: 1.7270 - val_loss: 6.7333 - val_mse: 6.7333 - val_mae: 2.3203 - lr: 2.4376e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 3/500\n",
            "556/556 - 1s - loss: 35.3341 - mse: 35.3341 - mae: 1.6999 - val_loss: 7.9833 - val_mse: 7.9833 - val_mae: 2.5308 - lr: 2.4376e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 4/500\n",
            "556/556 - 1s - loss: 34.9301 - mse: 34.9301 - mae: 1.6897 - val_loss: 8.2162 - val_mse: 8.2162 - val_mae: 2.5544 - lr: 2.4376e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 5/500\n",
            "556/556 - 1s - loss: 34.6360 - mse: 34.6360 - mae: 1.6739 - val_loss: 8.2769 - val_mse: 8.2769 - val_mae: 2.5411 - lr: 2.4376e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 6/500\n",
            "556/556 - 1s - loss: 34.3713 - mse: 34.3713 - mae: 1.6534 - val_loss: 8.3677 - val_mse: 8.3677 - val_mae: 2.5486 - lr: 2.4376e-04 - 1s/epoch - 2ms/step\n",
            "Score for inner fold : loss of 8.367730140686035\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-10-10 09:57:36,122]\u001b[0m Finished trial#44 resulted in value: 8.286666666666667. Current best value is 6.41 with parameters: {'activation': 'tanh', 'num_hidden_layer': 2, 'i': 4, 'learning_rate': 0.00010020536598530568}.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trial Number:45\n",
            "-----Fold:0--------\n",
            "Epoch 1/500\n",
            "556/556 - 2s - loss: 36.6678 - mse: 36.6678 - mae: 1.8496 - val_loss: 8.5956 - val_mse: 8.5956 - val_mae: 2.2178 - lr: 2.4651e-04 - 2s/epoch - 3ms/step\n",
            "Epoch 2/500\n",
            "556/556 - 1s - loss: 34.7961 - mse: 34.7961 - mae: 1.7011 - val_loss: 9.1196 - val_mse: 9.1196 - val_mae: 2.4553 - lr: 2.4651e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 3/500\n",
            "556/556 - 1s - loss: 34.0466 - mse: 34.0466 - mae: 1.6617 - val_loss: 9.4345 - val_mse: 9.4345 - val_mae: 2.5138 - lr: 2.4651e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 4/500\n",
            "556/556 - 1s - loss: 33.6018 - mse: 33.6018 - mae: 1.6415 - val_loss: 9.6950 - val_mse: 9.6950 - val_mae: 2.5655 - lr: 2.4651e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 5/500\n",
            "556/556 - 1s - loss: 33.2816 - mse: 33.2816 - mae: 1.6346 - val_loss: 9.9318 - val_mse: 9.9318 - val_mae: 2.5801 - lr: 2.4651e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 6/500\n",
            "556/556 - 1s - loss: 33.0282 - mse: 33.0282 - mae: 1.6245 - val_loss: 10.2793 - val_mse: 10.2793 - val_mae: 2.6121 - lr: 2.4651e-04 - 1s/epoch - 2ms/step\n",
            "Score for inner fold : loss of 10.279253959655762\n",
            "-----Fold:1--------\n",
            "Epoch 1/500\n",
            "556/556 - 2s - loss: 44.2078 - mse: 44.2078 - mae: 2.4526 - val_loss: 3.8971 - val_mse: 3.8971 - val_mae: 1.5399 - lr: 2.4651e-04 - 2s/epoch - 3ms/step\n",
            "Epoch 2/500\n",
            "556/556 - 1s - loss: 35.7610 - mse: 35.7610 - mae: 1.7056 - val_loss: 9.2633 - val_mse: 9.2633 - val_mae: 2.4440 - lr: 2.4651e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 3/500\n",
            "556/556 - 1s - loss: 34.3471 - mse: 34.3471 - mae: 1.6622 - val_loss: 9.9889 - val_mse: 9.9889 - val_mae: 2.5642 - lr: 2.4651e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 4/500\n",
            "556/556 - 1s - loss: 33.9120 - mse: 33.9120 - mae: 1.6505 - val_loss: 10.2361 - val_mse: 10.2361 - val_mae: 2.6058 - lr: 2.4651e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 5/500\n",
            "556/556 - 1s - loss: 33.6475 - mse: 33.6475 - mae: 1.6387 - val_loss: 10.6044 - val_mse: 10.6044 - val_mae: 2.6541 - lr: 2.4651e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 6/500\n",
            "556/556 - 1s - loss: 33.4522 - mse: 33.4522 - mae: 1.6359 - val_loss: 10.5590 - val_mse: 10.5590 - val_mae: 2.6509 - lr: 2.4651e-04 - 997ms/epoch - 2ms/step\n",
            "Score for inner fold : loss of 10.558964729309082\n",
            "-----Fold:2--------\n",
            "Epoch 1/500\n",
            "556/556 - 2s - loss: 22.0406 - mse: 22.0406 - mae: 2.3129 - val_loss: 3.7089 - val_mse: 3.7089 - val_mae: 1.5122 - lr: 2.4651e-04 - 2s/epoch - 3ms/step\n",
            "Epoch 2/500\n",
            "556/556 - 1s - loss: 15.8834 - mse: 15.8834 - mae: 1.7584 - val_loss: 7.9533 - val_mse: 7.9533 - val_mae: 2.3773 - lr: 2.4651e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 3/500\n",
            "556/556 - 1s - loss: 13.8632 - mse: 13.8632 - mae: 1.7101 - val_loss: 9.1299 - val_mse: 9.1299 - val_mae: 2.5701 - lr: 2.4651e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 4/500\n",
            "556/556 - 1s - loss: 12.8261 - mse: 12.8261 - mae: 1.6868 - val_loss: 9.2559 - val_mse: 9.2559 - val_mae: 2.5555 - lr: 2.4651e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 5/500\n",
            "556/556 - 1s - loss: 12.1845 - mse: 12.1845 - mae: 1.6486 - val_loss: 9.7408 - val_mse: 9.7408 - val_mae: 2.5885 - lr: 2.4651e-04 - 996ms/epoch - 2ms/step\n",
            "Epoch 6/500\n",
            "556/556 - 1s - loss: 11.7360 - mse: 11.7360 - mae: 1.6249 - val_loss: 10.1258 - val_mse: 10.1258 - val_mae: 2.5970 - lr: 2.4651e-04 - 1s/epoch - 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-10-10 09:58:00,362]\u001b[0m Finished trial#45 resulted in value: 10.323333333333332. Current best value is 6.41 with parameters: {'activation': 'tanh', 'num_hidden_layer': 2, 'i': 4, 'learning_rate': 0.00010020536598530568}.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Score for inner fold : loss of 10.125778198242188\n",
            "Trial Number:46\n",
            "-----Fold:0--------\n",
            "Epoch 1/500\n",
            "556/556 - 2s - loss: 43.2451 - mse: 43.2451 - mae: 2.1705 - val_loss: 3.5233 - val_mse: 3.5233 - val_mae: 1.6029 - lr: 3.5348e-04 - 2s/epoch - 3ms/step\n",
            "Epoch 2/500\n",
            "556/556 - 1s - loss: 37.4321 - mse: 37.4321 - mae: 1.7123 - val_loss: 7.9226 - val_mse: 7.9226 - val_mae: 2.5513 - lr: 3.5348e-04 - 978ms/epoch - 2ms/step\n",
            "Epoch 3/500\n",
            "556/556 - 1s - loss: 36.3777 - mse: 36.3777 - mae: 1.7194 - val_loss: 8.0359 - val_mse: 8.0359 - val_mae: 2.5382 - lr: 3.5348e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 4/500\n",
            "556/556 - 1s - loss: 35.9671 - mse: 35.9671 - mae: 1.6867 - val_loss: 8.3854 - val_mse: 8.3854 - val_mae: 2.5765 - lr: 3.5348e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 5/500\n",
            "556/556 - 1s - loss: 35.6429 - mse: 35.6429 - mae: 1.6688 - val_loss: 8.4288 - val_mse: 8.4288 - val_mae: 2.5629 - lr: 3.5348e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 6/500\n",
            "556/556 - 1s - loss: 35.3752 - mse: 35.3752 - mae: 1.6501 - val_loss: 8.5289 - val_mse: 8.5289 - val_mae: 2.5695 - lr: 3.5348e-04 - 1s/epoch - 2ms/step\n",
            "Score for inner fold : loss of 8.528929710388184\n",
            "-----Fold:1--------\n",
            "Epoch 1/500\n",
            "556/556 - 2s - loss: 21.9090 - mse: 21.9090 - mae: 2.1424 - val_loss: 3.9299 - val_mse: 3.9299 - val_mae: 1.7129 - lr: 3.5348e-04 - 2s/epoch - 3ms/step\n",
            "Epoch 2/500\n",
            "556/556 - 1s - loss: 16.8308 - mse: 16.8308 - mae: 1.6832 - val_loss: 7.9832 - val_mse: 7.9832 - val_mae: 2.5305 - lr: 3.5348e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 3/500\n",
            "556/556 - 1s - loss: 15.8806 - mse: 15.8806 - mae: 1.6888 - val_loss: 8.2726 - val_mse: 8.2726 - val_mae: 2.5678 - lr: 3.5348e-04 - 993ms/epoch - 2ms/step\n",
            "Epoch 4/500\n",
            "556/556 - 1s - loss: 15.4560 - mse: 15.4560 - mae: 1.6608 - val_loss: 8.2559 - val_mse: 8.2559 - val_mae: 2.5305 - lr: 3.5348e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 5/500\n",
            "556/556 - 1s - loss: 15.1367 - mse: 15.1367 - mae: 1.6425 - val_loss: 8.4473 - val_mse: 8.4473 - val_mae: 2.5413 - lr: 3.5348e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 6/500\n",
            "556/556 - 1s - loss: 14.8875 - mse: 14.8875 - mae: 1.6251 - val_loss: 8.7083 - val_mse: 8.7083 - val_mae: 2.5600 - lr: 3.5348e-04 - 1s/epoch - 2ms/step\n",
            "Score for inner fold : loss of 8.708268165588379\n",
            "-----Fold:2--------\n",
            "Epoch 1/500\n",
            "556/556 - 2s - loss: 39.0432 - mse: 39.0432 - mae: 2.0539 - val_loss: 4.5235 - val_mse: 4.5235 - val_mae: 1.8684 - lr: 3.5348e-04 - 2s/epoch - 3ms/step\n",
            "Epoch 2/500\n",
            "556/556 - 1s - loss: 34.2478 - mse: 34.2478 - mae: 1.6395 - val_loss: 7.7782 - val_mse: 7.7782 - val_mae: 2.5017 - lr: 3.5348e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 3/500\n",
            "556/556 - 1s - loss: 33.5191 - mse: 33.5191 - mae: 1.6364 - val_loss: 8.0056 - val_mse: 8.0056 - val_mae: 2.5179 - lr: 3.5348e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 4/500\n",
            "556/556 - 1s - loss: 33.1978 - mse: 33.1978 - mae: 1.6153 - val_loss: 8.3508 - val_mse: 8.3508 - val_mae: 2.5513 - lr: 3.5348e-04 - 983ms/epoch - 2ms/step\n",
            "Epoch 5/500\n",
            "556/556 - 1s - loss: 32.9760 - mse: 32.9760 - mae: 1.6003 - val_loss: 8.4307 - val_mse: 8.4307 - val_mae: 2.5410 - lr: 3.5348e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 6/500\n",
            "556/556 - 1s - loss: 32.8020 - mse: 32.8020 - mae: 1.5916 - val_loss: 8.5057 - val_mse: 8.5057 - val_mae: 2.5373 - lr: 3.5348e-04 - 1s/epoch - 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-10-10 09:58:24,617]\u001b[0m Finished trial#46 resulted in value: 8.583333333333334. Current best value is 6.41 with parameters: {'activation': 'tanh', 'num_hidden_layer': 2, 'i': 4, 'learning_rate': 0.00010020536598530568}.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Score for inner fold : loss of 8.505728721618652\n",
            "Trial Number:47\n",
            "-----Fold:0--------\n",
            "Epoch 1/500\n",
            "556/556 - 2s - loss: 23.5395 - mse: 23.5395 - mae: 2.4354 - val_loss: 1.8881 - val_mse: 1.8881 - val_mae: 1.0799 - lr: 1.9812e-04 - 2s/epoch - 3ms/step\n",
            "Epoch 2/500\n",
            "556/556 - 1s - loss: 19.6247 - mse: 19.6247 - mae: 1.8801 - val_loss: 4.8205 - val_mse: 4.8205 - val_mae: 1.8980 - lr: 1.9812e-04 - 993ms/epoch - 2ms/step\n",
            "Epoch 3/500\n",
            "556/556 - 1s - loss: 17.0133 - mse: 17.0133 - mae: 1.6964 - val_loss: 7.4992 - val_mse: 7.4992 - val_mae: 2.4621 - lr: 1.9812e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 4/500\n",
            "556/556 - 1s - loss: 16.3044 - mse: 16.3044 - mae: 1.6911 - val_loss: 7.9187 - val_mse: 7.9187 - val_mae: 2.5251 - lr: 1.9812e-04 - 998ms/epoch - 2ms/step\n",
            "Epoch 5/500\n",
            "556/556 - 1s - loss: 15.9982 - mse: 15.9982 - mae: 1.6738 - val_loss: 8.1766 - val_mse: 8.1766 - val_mae: 2.5520 - lr: 1.9812e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 6/500\n",
            "556/556 - 1s - loss: 15.7731 - mse: 15.7731 - mae: 1.6622 - val_loss: 8.1855 - val_mse: 8.1855 - val_mae: 2.5412 - lr: 1.9812e-04 - 1s/epoch - 2ms/step\n",
            "Score for inner fold : loss of 8.185548782348633\n",
            "-----Fold:1--------\n",
            "Epoch 1/500\n",
            "556/556 - 2s - loss: 43.4426 - mse: 43.4426 - mae: 2.3921 - val_loss: 1.6316 - val_mse: 1.6316 - val_mae: 0.9926 - lr: 1.9812e-04 - 2s/epoch - 3ms/step\n",
            "Epoch 2/500\n",
            "556/556 - 1s - loss: 39.6768 - mse: 39.6768 - mae: 1.8504 - val_loss: 3.9580 - val_mse: 3.9580 - val_mae: 1.7307 - lr: 1.9812e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 3/500\n",
            "556/556 - 1s - loss: 36.7745 - mse: 36.7745 - mae: 1.6636 - val_loss: 7.2288 - val_mse: 7.2288 - val_mae: 2.4387 - lr: 1.9812e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 4/500\n",
            "556/556 - 1s - loss: 35.8338 - mse: 35.8338 - mae: 1.6947 - val_loss: 7.9065 - val_mse: 7.9065 - val_mae: 2.5319 - lr: 1.9812e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 5/500\n",
            "556/556 - 1s - loss: 35.4525 - mse: 35.4525 - mae: 1.6792 - val_loss: 8.0562 - val_mse: 8.0562 - val_mae: 2.5458 - lr: 1.9812e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 6/500\n",
            "556/556 - 1s - loss: 35.2060 - mse: 35.2060 - mae: 1.6616 - val_loss: 8.0836 - val_mse: 8.0836 - val_mae: 2.5328 - lr: 1.9812e-04 - 1s/epoch - 2ms/step\n",
            "Score for inner fold : loss of 8.083600044250488\n",
            "-----Fold:2--------\n",
            "Epoch 1/500\n",
            "556/556 - 2s - loss: 41.8878 - mse: 41.8878 - mae: 2.4163 - val_loss: 1.5114 - val_mse: 1.5114 - val_mae: 0.8924 - lr: 1.9812e-04 - 2s/epoch - 3ms/step\n",
            "Epoch 2/500\n",
            "556/556 - 1s - loss: 38.4647 - mse: 38.4647 - mae: 1.8620 - val_loss: 3.3735 - val_mse: 3.3735 - val_mae: 1.5528 - lr: 1.9812e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 3/500\n",
            "556/556 - 1s - loss: 35.4519 - mse: 35.4519 - mae: 1.6141 - val_loss: 6.7866 - val_mse: 6.7866 - val_mae: 2.3525 - lr: 1.9812e-04 - 993ms/epoch - 2ms/step\n",
            "Epoch 4/500\n",
            "556/556 - 1s - loss: 34.4202 - mse: 34.4202 - mae: 1.6662 - val_loss: 7.8327 - val_mse: 7.8327 - val_mae: 2.5193 - lr: 1.9812e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 5/500\n",
            "556/556 - 1s - loss: 34.1035 - mse: 34.1035 - mae: 1.6652 - val_loss: 8.0367 - val_mse: 8.0367 - val_mae: 2.5371 - lr: 1.9812e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 6/500\n",
            "556/556 - 1s - loss: 33.8812 - mse: 33.8812 - mae: 1.6474 - val_loss: 8.2792 - val_mse: 8.2792 - val_mae: 2.5657 - lr: 1.9812e-04 - 1s/epoch - 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-10-10 09:58:48,661]\u001b[0m Finished trial#47 resulted in value: 8.183333333333332. Current best value is 6.41 with parameters: {'activation': 'tanh', 'num_hidden_layer': 2, 'i': 4, 'learning_rate': 0.00010020536598530568}.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Score for inner fold : loss of 8.279170989990234\n",
            "Trial Number:48\n",
            "-----Fold:0--------\n",
            "Epoch 1/500\n",
            "556/556 - 2s - loss: 43.8413 - mse: 43.8413 - mae: 2.5157 - val_loss: 1.2447 - val_mse: 1.2447 - val_mae: 0.8147 - lr: 1.3084e-04 - 2s/epoch - 3ms/step\n",
            "Epoch 2/500\n",
            "556/556 - 1s - loss: 41.1163 - mse: 41.1163 - mae: 2.0756 - val_loss: 2.2012 - val_mse: 2.2012 - val_mae: 1.1995 - lr: 1.3084e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 3/500\n",
            "556/556 - 1s - loss: 38.5245 - mse: 38.5245 - mae: 1.7047 - val_loss: 4.5527 - val_mse: 4.5527 - val_mae: 1.8574 - lr: 1.3084e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 4/500\n",
            "556/556 - 1s - loss: 36.7643 - mse: 36.7643 - mae: 1.6414 - val_loss: 7.1307 - val_mse: 7.1307 - val_mae: 2.4000 - lr: 1.3084e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 5/500\n",
            "556/556 - 1s - loss: 36.2025 - mse: 36.2025 - mae: 1.6985 - val_loss: 7.9193 - val_mse: 7.9193 - val_mae: 2.5321 - lr: 1.3084e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 6/500\n",
            "556/556 - 1s - loss: 35.9711 - mse: 35.9711 - mae: 1.6974 - val_loss: 8.2223 - val_mse: 8.2223 - val_mae: 2.5732 - lr: 1.3084e-04 - 1s/epoch - 2ms/step\n",
            "Score for inner fold : loss of 8.222334861755371\n",
            "-----Fold:1--------\n",
            "Epoch 1/500\n",
            "556/556 - 2s - loss: 24.5278 - mse: 24.5278 - mae: 2.6131 - val_loss: 1.2563 - val_mse: 1.2563 - val_mae: 0.8122 - lr: 1.3084e-04 - 2s/epoch - 3ms/step\n",
            "Epoch 2/500\n",
            "556/556 - 1s - loss: 22.1165 - mse: 22.1165 - mae: 2.1843 - val_loss: 1.9320 - val_mse: 1.9320 - val_mae: 1.1046 - lr: 1.3084e-04 - 986ms/epoch - 2ms/step\n",
            "Epoch 3/500\n",
            "556/556 - 1s - loss: 19.5308 - mse: 19.5308 - mae: 1.7768 - val_loss: 3.8653 - val_mse: 3.8653 - val_mae: 1.7059 - lr: 1.3084e-04 - 988ms/epoch - 2ms/step\n",
            "Epoch 4/500\n",
            "556/556 - 1s - loss: 17.5419 - mse: 17.5419 - mae: 1.6405 - val_loss: 6.6516 - val_mse: 6.6516 - val_mae: 2.3372 - lr: 1.3084e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 5/500\n",
            "556/556 - 1s - loss: 16.7721 - mse: 16.7721 - mae: 1.6965 - val_loss: 7.7732 - val_mse: 7.7732 - val_mae: 2.5259 - lr: 1.3084e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 6/500\n",
            "556/556 - 1s - loss: 16.4543 - mse: 16.4543 - mae: 1.7041 - val_loss: 7.8743 - val_mse: 7.8743 - val_mae: 2.5325 - lr: 1.3084e-04 - 1s/epoch - 2ms/step\n",
            "Score for inner fold : loss of 7.874253749847412\n",
            "-----Fold:2--------\n",
            "Epoch 1/500\n",
            "556/556 - 2s - loss: 42.0110 - mse: 42.0110 - mae: 2.4510 - val_loss: 1.5963 - val_mse: 1.5963 - val_mae: 0.9274 - lr: 1.3084e-04 - 2s/epoch - 3ms/step\n",
            "Epoch 2/500\n",
            "556/556 - 1s - loss: 39.8007 - mse: 39.8007 - mae: 2.0807 - val_loss: 2.4285 - val_mse: 2.4285 - val_mae: 1.2357 - lr: 1.3084e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 3/500\n",
            "556/556 - 1s - loss: 37.5047 - mse: 37.5047 - mae: 1.7700 - val_loss: 4.3589 - val_mse: 4.3589 - val_mae: 1.7900 - lr: 1.3084e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 4/500\n",
            "556/556 - 1s - loss: 35.7156 - mse: 35.7156 - mae: 1.6684 - val_loss: 6.6227 - val_mse: 6.6227 - val_mae: 2.3053 - lr: 1.3084e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 5/500\n",
            "556/556 - 1s - loss: 34.8583 - mse: 34.8583 - mae: 1.6998 - val_loss: 7.6954 - val_mse: 7.6954 - val_mae: 2.5092 - lr: 1.3084e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 6/500\n",
            "556/556 - 1s - loss: 34.4382 - mse: 34.4382 - mae: 1.7033 - val_loss: 8.0037 - val_mse: 8.0037 - val_mae: 2.5495 - lr: 1.3084e-04 - 975ms/epoch - 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-10-10 09:59:12,971]\u001b[0m Finished trial#48 resulted in value: 8.03. Current best value is 6.41 with parameters: {'activation': 'tanh', 'num_hidden_layer': 2, 'i': 4, 'learning_rate': 0.00010020536598530568}.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Score for inner fold : loss of 8.003684997558594\n",
            "Trial Number:49\n",
            "-----Fold:0--------\n",
            "Epoch 1/500\n",
            "556/556 - 2s - loss: 40.5758 - mse: 40.5758 - mae: 2.1147 - val_loss: 5.7842 - val_mse: 5.7842 - val_mae: 2.0543 - lr: 1.2794e-04 - 2s/epoch - 4ms/step\n",
            "Epoch 2/500\n",
            "556/556 - 1s - loss: 34.8397 - mse: 34.8397 - mae: 1.6081 - val_loss: 9.3562 - val_mse: 9.3562 - val_mae: 2.6104 - lr: 1.2794e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 3/500\n",
            "556/556 - 1s - loss: 34.1675 - mse: 34.1675 - mae: 1.6070 - val_loss: 9.3825 - val_mse: 9.3825 - val_mae: 2.6076 - lr: 1.2794e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 4/500\n",
            "556/556 - 1s - loss: 33.9456 - mse: 33.9456 - mae: 1.5946 - val_loss: 9.4478 - val_mse: 9.4478 - val_mae: 2.5836 - lr: 1.2794e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 5/500\n",
            "556/556 - 1s - loss: 33.7735 - mse: 33.7735 - mae: 1.5873 - val_loss: 9.6319 - val_mse: 9.6319 - val_mae: 2.6021 - lr: 1.2794e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 6/500\n",
            "556/556 - 1s - loss: 33.6467 - mse: 33.6467 - mae: 1.5803 - val_loss: 9.6527 - val_mse: 9.6527 - val_mae: 2.5940 - lr: 1.2794e-04 - 1s/epoch - 2ms/step\n",
            "Score for inner fold : loss of 9.652661323547363\n",
            "-----Fold:1--------\n",
            "Epoch 1/500\n",
            "556/556 - 2s - loss: 39.8379 - mse: 39.8379 - mae: 2.1446 - val_loss: 5.6550 - val_mse: 5.6550 - val_mae: 2.0026 - lr: 1.2794e-04 - 2s/epoch - 4ms/step\n",
            "Epoch 2/500\n",
            "556/556 - 1s - loss: 34.1336 - mse: 34.1336 - mae: 1.6068 - val_loss: 9.1920 - val_mse: 9.1920 - val_mae: 2.6103 - lr: 1.2794e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 3/500\n",
            "556/556 - 1s - loss: 33.5527 - mse: 33.5527 - mae: 1.6085 - val_loss: 9.5266 - val_mse: 9.5266 - val_mae: 2.6074 - lr: 1.2794e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 4/500\n",
            "556/556 - 1s - loss: 33.2861 - mse: 33.2861 - mae: 1.5951 - val_loss: 9.6612 - val_mse: 9.6612 - val_mae: 2.6177 - lr: 1.2794e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 5/500\n",
            "556/556 - 1s - loss: 33.1374 - mse: 33.1374 - mae: 1.5821 - val_loss: 9.8676 - val_mse: 9.8676 - val_mae: 2.6334 - lr: 1.2794e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 6/500\n",
            "556/556 - 1s - loss: 32.9909 - mse: 32.9909 - mae: 1.5740 - val_loss: 10.0930 - val_mse: 10.0930 - val_mae: 2.6584 - lr: 1.2794e-04 - 1s/epoch - 2ms/step\n",
            "Score for inner fold : loss of 10.092970848083496\n",
            "-----Fold:2--------\n",
            "Epoch 1/500\n",
            "556/556 - 2s - loss: 18.4547 - mse: 18.4547 - mae: 2.0814 - val_loss: 5.8941 - val_mse: 5.8941 - val_mae: 2.0455 - lr: 1.2794e-04 - 2s/epoch - 4ms/step\n",
            "Epoch 2/500\n",
            "556/556 - 1s - loss: 13.1276 - mse: 13.1276 - mae: 1.5817 - val_loss: 8.7164 - val_mse: 8.7164 - val_mae: 2.5376 - lr: 1.2794e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 3/500\n",
            "556/556 - 1s - loss: 12.5112 - mse: 12.5112 - mae: 1.5657 - val_loss: 9.4245 - val_mse: 9.4245 - val_mae: 2.6154 - lr: 1.2794e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 4/500\n",
            "556/556 - 1s - loss: 12.2409 - mse: 12.2409 - mae: 1.5530 - val_loss: 9.5018 - val_mse: 9.5018 - val_mae: 2.6001 - lr: 1.2794e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 5/500\n",
            "556/556 - 1s - loss: 12.0499 - mse: 12.0499 - mae: 1.5419 - val_loss: 9.7241 - val_mse: 9.7241 - val_mae: 2.6088 - lr: 1.2794e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 6/500\n",
            "556/556 - 1s - loss: 11.9015 - mse: 11.9015 - mae: 1.5319 - val_loss: 9.8062 - val_mse: 9.8062 - val_mae: 2.6102 - lr: 1.2794e-04 - 1s/epoch - 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-10-10 09:59:40,985]\u001b[0m Finished trial#49 resulted in value: 9.850000000000001. Current best value is 6.41 with parameters: {'activation': 'tanh', 'num_hidden_layer': 2, 'i': 4, 'learning_rate': 0.00010020536598530568}.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Score for inner fold : loss of 9.806197166442871\n",
            "Number of finished trial : 50\n",
            "Best Trial Value : 6.41\n",
            "------ Outer Fold - 2 -------- \n",
            "Trial Number:0\n",
            "-----Fold:0--------\n",
            "Epoch 1/500\n",
            "556/556 - 2s - loss: 17.9951 - mse: 17.9951 - mae: 1.9250 - val_loss: 8.8374 - val_mse: 8.8374 - val_mae: 2.6023 - lr: 1.5034e-04 - 2s/epoch - 4ms/step\n",
            "Epoch 2/500\n",
            "556/556 - 1s - loss: 14.2034 - mse: 14.2034 - mae: 1.5911 - val_loss: 9.0039 - val_mse: 9.0039 - val_mae: 2.5549 - lr: 1.5034e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 3/500\n",
            "556/556 - 1s - loss: 13.8106 - mse: 13.8106 - mae: 1.5694 - val_loss: 9.3282 - val_mse: 9.3282 - val_mae: 2.6062 - lr: 1.5034e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 4/500\n",
            "556/556 - 1s - loss: 13.6035 - mse: 13.6035 - mae: 1.5534 - val_loss: 9.6174 - val_mse: 9.6174 - val_mae: 2.6043 - lr: 1.5034e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 5/500\n",
            "556/556 - 1s - loss: 13.4150 - mse: 13.4150 - mae: 1.5488 - val_loss: 9.1775 - val_mse: 9.1775 - val_mae: 2.5572 - lr: 1.5034e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 6/500\n",
            "556/556 - 1s - loss: 13.2815 - mse: 13.2815 - mae: 1.5372 - val_loss: 10.0265 - val_mse: 10.0265 - val_mae: 2.6205 - lr: 1.5034e-04 - 1s/epoch - 2ms/step\n",
            "Score for inner fold : loss of 10.026520729064941\n",
            "-----Fold:1--------\n",
            "Epoch 1/500\n",
            "556/556 - 2s - loss: 15.9798 - mse: 15.9798 - mae: 1.9158 - val_loss: 9.1000 - val_mse: 9.1000 - val_mae: 2.6152 - lr: 1.5034e-04 - 2s/epoch - 4ms/step\n",
            "Epoch 2/500\n",
            "556/556 - 1s - loss: 12.3016 - mse: 12.3016 - mae: 1.5980 - val_loss: 9.6331 - val_mse: 9.6331 - val_mae: 2.6380 - lr: 1.5034e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 3/500\n",
            "556/556 - 1s - loss: 11.9535 - mse: 11.9535 - mae: 1.5789 - val_loss: 9.7824 - val_mse: 9.7824 - val_mae: 2.6401 - lr: 1.5034e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 4/500\n",
            "556/556 - 1s - loss: 11.7339 - mse: 11.7339 - mae: 1.5636 - val_loss: 9.6513 - val_mse: 9.6513 - val_mae: 2.5900 - lr: 1.5034e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 5/500\n",
            "556/556 - 1s - loss: 11.5393 - mse: 11.5393 - mae: 1.5505 - val_loss: 9.6542 - val_mse: 9.6542 - val_mae: 2.5722 - lr: 1.5034e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 6/500\n",
            "556/556 - 1s - loss: 11.3646 - mse: 11.3646 - mae: 1.5429 - val_loss: 9.7021 - val_mse: 9.7021 - val_mae: 2.5627 - lr: 1.5034e-04 - 1s/epoch - 2ms/step\n",
            "Score for inner fold : loss of 9.702134132385254\n",
            "-----Fold:2--------\n",
            "Epoch 1/500\n",
            "556/556 - 2s - loss: 16.1478 - mse: 16.1478 - mae: 1.9099 - val_loss: 8.7463 - val_mse: 8.7463 - val_mae: 2.5777 - lr: 1.5034e-04 - 2s/epoch - 4ms/step\n",
            "Epoch 2/500\n",
            "556/556 - 1s - loss: 12.5696 - mse: 12.5696 - mae: 1.5802 - val_loss: 9.0278 - val_mse: 9.0278 - val_mae: 2.5650 - lr: 1.5034e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 3/500\n",
            "556/556 - 1s - loss: 12.2611 - mse: 12.2611 - mae: 1.5602 - val_loss: 9.4197 - val_mse: 9.4197 - val_mae: 2.5907 - lr: 1.5034e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 4/500\n",
            "556/556 - 1s - loss: 12.0336 - mse: 12.0336 - mae: 1.5458 - val_loss: 9.2514 - val_mse: 9.2514 - val_mae: 2.5495 - lr: 1.5034e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 5/500\n",
            "556/556 - 1s - loss: 11.8514 - mse: 11.8514 - mae: 1.5376 - val_loss: 9.2606 - val_mse: 9.2606 - val_mae: 2.5426 - lr: 1.5034e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 6/500\n",
            "556/556 - 1s - loss: 11.6986 - mse: 11.6986 - mae: 1.5264 - val_loss: 10.2091 - val_mse: 10.2091 - val_mae: 2.6541 - lr: 1.5034e-04 - 1s/epoch - 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-10-10 10:00:09,919]\u001b[0m Finished trial#0 resulted in value: 9.979999999999999. Current best value is 9.979999999999999 with parameters: {'activation': 'tanh', 'num_hidden_layer': 4, 'i': 6, 'learning_rate': 0.00015033774387638956}.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Score for inner fold : loss of 10.209054946899414\n",
            "Trial Number:1\n",
            "-----Fold:0--------\n",
            "Epoch 1/500\n",
            "556/556 - 2s - loss: 12.4837 - mse: 12.4837 - mae: 1.6214 - val_loss: 11.0731 - val_mse: 11.0731 - val_mae: 2.5548 - lr: 0.0055 - 2s/epoch - 4ms/step\n",
            "Epoch 2/500\n",
            "556/556 - 1s - loss: 11.6657 - mse: 11.6657 - mae: 1.5788 - val_loss: 13.5897 - val_mse: 13.5897 - val_mae: 2.8881 - lr: 0.0055 - 1s/epoch - 2ms/step\n",
            "Epoch 3/500\n",
            "556/556 - 1s - loss: 11.2616 - mse: 11.2616 - mae: 1.5741 - val_loss: 11.6928 - val_mse: 11.6928 - val_mae: 2.6831 - lr: 0.0055 - 1s/epoch - 2ms/step\n",
            "Epoch 4/500\n",
            "556/556 - 1s - loss: 10.9461 - mse: 10.9461 - mae: 1.5558 - val_loss: 13.4798 - val_mse: 13.4798 - val_mae: 2.7966 - lr: 0.0055 - 1s/epoch - 2ms/step\n",
            "Epoch 5/500\n",
            "556/556 - 1s - loss: 10.8550 - mse: 10.8550 - mae: 1.5502 - val_loss: 12.3587 - val_mse: 12.3587 - val_mae: 2.7567 - lr: 0.0055 - 1s/epoch - 2ms/step\n",
            "Epoch 6/500\n",
            "556/556 - 1s - loss: 10.5457 - mse: 10.5457 - mae: 1.5431 - val_loss: 11.4277 - val_mse: 11.4277 - val_mae: 2.7330 - lr: 0.0055 - 1s/epoch - 2ms/step\n",
            "Score for inner fold : loss of 11.427651405334473\n",
            "-----Fold:1--------\n",
            "Epoch 1/500\n",
            "556/556 - 2s - loss: 13.0595 - mse: 13.0595 - mae: 1.6667 - val_loss: 10.1177 - val_mse: 10.1177 - val_mae: 2.6593 - lr: 0.0055 - 2s/epoch - 4ms/step\n",
            "Epoch 2/500\n",
            "556/556 - 1s - loss: 12.0720 - mse: 12.0720 - mae: 1.6062 - val_loss: 10.8543 - val_mse: 10.8543 - val_mae: 2.6531 - lr: 0.0055 - 1s/epoch - 2ms/step\n",
            "Epoch 3/500\n",
            "556/556 - 1s - loss: 11.9054 - mse: 11.9054 - mae: 1.5882 - val_loss: 10.7043 - val_mse: 10.7043 - val_mae: 2.5705 - lr: 0.0055 - 1s/epoch - 2ms/step\n",
            "Epoch 4/500\n",
            "556/556 - 1s - loss: 11.5042 - mse: 11.5042 - mae: 1.5738 - val_loss: 10.8158 - val_mse: 10.8158 - val_mae: 2.4515 - lr: 0.0055 - 1s/epoch - 2ms/step\n",
            "Epoch 5/500\n",
            "556/556 - 1s - loss: 11.4677 - mse: 11.4677 - mae: 1.5646 - val_loss: 11.8822 - val_mse: 11.8822 - val_mae: 2.7162 - lr: 0.0055 - 1s/epoch - 2ms/step\n",
            "Epoch 6/500\n",
            "556/556 - 1s - loss: 11.3372 - mse: 11.3372 - mae: 1.5668 - val_loss: 13.8637 - val_mse: 13.8637 - val_mae: 2.9810 - lr: 0.0055 - 1s/epoch - 2ms/step\n",
            "Score for inner fold : loss of 13.863724708557129\n",
            "-----Fold:2--------\n",
            "Epoch 1/500\n",
            "556/556 - 2s - loss: 12.3692 - mse: 12.3692 - mae: 1.6483 - val_loss: 10.8053 - val_mse: 10.8053 - val_mae: 2.4785 - lr: 0.0055 - 2s/epoch - 4ms/step\n",
            "Epoch 2/500\n",
            "556/556 - 1s - loss: 11.4212 - mse: 11.4212 - mae: 1.5961 - val_loss: 12.7752 - val_mse: 12.7752 - val_mae: 2.9563 - lr: 0.0055 - 1s/epoch - 2ms/step\n",
            "Epoch 3/500\n",
            "556/556 - 1s - loss: 11.0767 - mse: 11.0767 - mae: 1.5745 - val_loss: 11.2942 - val_mse: 11.2942 - val_mae: 2.6815 - lr: 0.0055 - 1s/epoch - 2ms/step\n",
            "Epoch 4/500\n",
            "556/556 - 1s - loss: 10.8281 - mse: 10.8281 - mae: 1.5575 - val_loss: 12.8264 - val_mse: 12.8264 - val_mae: 2.7894 - lr: 0.0055 - 1s/epoch - 2ms/step\n",
            "Epoch 5/500\n",
            "556/556 - 1s - loss: 10.6708 - mse: 10.6708 - mae: 1.5522 - val_loss: 12.3957 - val_mse: 12.3957 - val_mae: 2.7220 - lr: 0.0055 - 1s/epoch - 2ms/step\n",
            "Epoch 6/500\n",
            "556/556 - 1s - loss: 10.5372 - mse: 10.5372 - mae: 1.5413 - val_loss: 11.4755 - val_mse: 11.4755 - val_mae: 2.5998 - lr: 0.0055 - 1s/epoch - 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-10-10 10:00:38,012]\u001b[0m Finished trial#1 resulted in value: 12.256666666666666. Current best value is 9.979999999999999 with parameters: {'activation': 'tanh', 'num_hidden_layer': 4, 'i': 6, 'learning_rate': 0.00015033774387638956}.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Score for inner fold : loss of 11.47546100616455\n",
            "Trial Number:2\n",
            "-----Fold:0--------\n",
            "Epoch 1/500\n",
            "556/556 - 2s - loss: 17.6055 - mse: 17.6055 - mae: 1.9210 - val_loss: 7.3560 - val_mse: 7.3560 - val_mae: 2.1955 - lr: 1.5143e-04 - 2s/epoch - 3ms/step\n",
            "Epoch 2/500\n",
            "556/556 - 1s - loss: 12.9143 - mse: 12.9143 - mae: 1.6111 - val_loss: 10.1298 - val_mse: 10.1298 - val_mae: 2.5617 - lr: 1.5143e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 3/500\n",
            "556/556 - 1s - loss: 12.3363 - mse: 12.3363 - mae: 1.6059 - val_loss: 10.2639 - val_mse: 10.2639 - val_mae: 2.5493 - lr: 1.5143e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 4/500\n",
            "556/556 - 1s - loss: 11.9814 - mse: 11.9814 - mae: 1.5858 - val_loss: 11.1081 - val_mse: 11.1081 - val_mae: 2.6144 - lr: 1.5143e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 5/500\n",
            "556/556 - 1s - loss: 11.8260 - mse: 11.8260 - mae: 1.5800 - val_loss: 11.5025 - val_mse: 11.5025 - val_mae: 2.6184 - lr: 1.5143e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 6/500\n",
            "556/556 - 1s - loss: 11.6914 - mse: 11.6914 - mae: 1.5705 - val_loss: 12.0495 - val_mse: 12.0495 - val_mae: 2.6364 - lr: 1.5143e-04 - 1s/epoch - 2ms/step\n",
            "Score for inner fold : loss of 12.049543380737305\n",
            "-----Fold:1--------\n",
            "Epoch 1/500\n",
            "556/556 - 2s - loss: 19.5364 - mse: 19.5364 - mae: 2.0993 - val_loss: 6.1368 - val_mse: 6.1368 - val_mae: 1.9844 - lr: 1.5143e-04 - 2s/epoch - 3ms/step\n",
            "Epoch 2/500\n",
            "556/556 - 1s - loss: 13.3327 - mse: 13.3327 - mae: 1.6005 - val_loss: 9.6840 - val_mse: 9.6840 - val_mae: 2.5394 - lr: 1.5143e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 3/500\n",
            "556/556 - 1s - loss: 12.7039 - mse: 12.7039 - mae: 1.6075 - val_loss: 9.9557 - val_mse: 9.9557 - val_mae: 2.5660 - lr: 1.5143e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 4/500\n",
            "556/556 - 1s - loss: 12.4580 - mse: 12.4580 - mae: 1.5958 - val_loss: 9.9706 - val_mse: 9.9706 - val_mae: 2.5571 - lr: 1.5143e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 5/500\n",
            "556/556 - 1s - loss: 12.2587 - mse: 12.2587 - mae: 1.5787 - val_loss: 10.5767 - val_mse: 10.5767 - val_mae: 2.6171 - lr: 1.5143e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 6/500\n",
            "556/556 - 1s - loss: 12.1226 - mse: 12.1226 - mae: 1.5721 - val_loss: 10.6865 - val_mse: 10.6865 - val_mae: 2.6090 - lr: 1.5143e-04 - 1s/epoch - 2ms/step\n",
            "Score for inner fold : loss of 10.686452865600586\n",
            "-----Fold:2--------\n",
            "Epoch 1/500\n",
            "556/556 - 2s - loss: 17.8830 - mse: 17.8830 - mae: 2.0327 - val_loss: 6.0101 - val_mse: 6.0101 - val_mae: 1.9680 - lr: 1.5143e-04 - 2s/epoch - 3ms/step\n",
            "Epoch 2/500\n",
            "556/556 - 1s - loss: 12.8306 - mse: 12.8306 - mae: 1.6166 - val_loss: 9.6283 - val_mse: 9.6283 - val_mae: 2.5184 - lr: 1.5143e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 3/500\n",
            "556/556 - 1s - loss: 12.0179 - mse: 12.0179 - mae: 1.6148 - val_loss: 10.3247 - val_mse: 10.3247 - val_mae: 2.5889 - lr: 1.5143e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 4/500\n",
            "556/556 - 1s - loss: 11.5288 - mse: 11.5288 - mae: 1.6035 - val_loss: 10.3273 - val_mse: 10.3273 - val_mae: 2.5660 - lr: 1.5143e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 5/500\n",
            "556/556 - 1s - loss: 11.1730 - mse: 11.1730 - mae: 1.5834 - val_loss: 11.2111 - val_mse: 11.2111 - val_mae: 2.6097 - lr: 1.5143e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 6/500\n",
            "556/556 - 1s - loss: 10.9352 - mse: 10.9352 - mae: 1.5702 - val_loss: 11.8538 - val_mse: 11.8538 - val_mae: 2.6223 - lr: 1.5143e-04 - 1s/epoch - 2ms/step\n",
            "Score for inner fold : loss of 11.853785514831543\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-10-10 10:01:04,162]\u001b[0m Finished trial#2 resulted in value: 11.530000000000001. Current best value is 9.979999999999999 with parameters: {'activation': 'tanh', 'num_hidden_layer': 4, 'i': 6, 'learning_rate': 0.00015033774387638956}.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trial Number:3\n",
            "-----Fold:0--------\n",
            "Epoch 1/500\n",
            "556/556 - 2s - loss: 12.3958 - mse: 12.3958 - mae: 1.6454 - val_loss: 10.4511 - val_mse: 10.4511 - val_mae: 2.5716 - lr: 2.7996e-04 - 2s/epoch - 4ms/step\n",
            "Epoch 2/500\n",
            "556/556 - 1s - loss: 10.6394 - mse: 10.6394 - mae: 1.5456 - val_loss: 11.6092 - val_mse: 11.6092 - val_mae: 2.5427 - lr: 2.7996e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 3/500\n",
            "556/556 - 1s - loss: 10.5436 - mse: 10.5436 - mae: 1.5235 - val_loss: 14.0860 - val_mse: 14.0860 - val_mae: 2.7536 - lr: 2.7996e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 4/500\n",
            "556/556 - 1s - loss: 10.3756 - mse: 10.3756 - mae: 1.5188 - val_loss: 13.7054 - val_mse: 13.7054 - val_mae: 2.6971 - lr: 2.7996e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 5/500\n",
            "556/556 - 1s - loss: 10.2507 - mse: 10.2507 - mae: 1.5084 - val_loss: 13.5524 - val_mse: 13.5524 - val_mae: 2.6799 - lr: 2.7996e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 6/500\n",
            "556/556 - 1s - loss: 10.1361 - mse: 10.1361 - mae: 1.5062 - val_loss: 13.4337 - val_mse: 13.4337 - val_mae: 2.6712 - lr: 2.7996e-04 - 1s/epoch - 2ms/step\n",
            "Score for inner fold : loss of 13.433675765991211\n",
            "-----Fold:1--------\n",
            "Epoch 1/500\n",
            "556/556 - 2s - loss: 13.9792 - mse: 13.9792 - mae: 1.6617 - val_loss: 10.4255 - val_mse: 10.4255 - val_mae: 2.5509 - lr: 2.7996e-04 - 2s/epoch - 4ms/step\n",
            "Epoch 2/500\n",
            "556/556 - 1s - loss: 12.3417 - mse: 12.3417 - mae: 1.5664 - val_loss: 11.8994 - val_mse: 11.8994 - val_mae: 2.6631 - lr: 2.7996e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 3/500\n",
            "556/556 - 1s - loss: 12.0288 - mse: 12.0288 - mae: 1.5524 - val_loss: 12.5022 - val_mse: 12.5022 - val_mae: 2.6623 - lr: 2.7996e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 4/500\n",
            "556/556 - 1s - loss: 11.9196 - mse: 11.9196 - mae: 1.5376 - val_loss: 12.0168 - val_mse: 12.0168 - val_mae: 2.6418 - lr: 2.7996e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 5/500\n",
            "556/556 - 1s - loss: 11.7961 - mse: 11.7961 - mae: 1.5330 - val_loss: 12.6686 - val_mse: 12.6686 - val_mae: 2.6317 - lr: 2.7996e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 6/500\n",
            "556/556 - 1s - loss: 11.7376 - mse: 11.7376 - mae: 1.5281 - val_loss: 12.5901 - val_mse: 12.5901 - val_mae: 2.6272 - lr: 2.7996e-04 - 1s/epoch - 2ms/step\n",
            "Score for inner fold : loss of 12.59009075164795\n",
            "-----Fold:2--------\n",
            "Epoch 1/500\n",
            "556/556 - 2s - loss: 13.5712 - mse: 13.5712 - mae: 1.6438 - val_loss: 12.1373 - val_mse: 12.1373 - val_mae: 2.6248 - lr: 2.7996e-04 - 2s/epoch - 4ms/step\n",
            "Epoch 2/500\n",
            "556/556 - 1s - loss: 11.5309 - mse: 11.5309 - mae: 1.5536 - val_loss: 12.1932 - val_mse: 12.1932 - val_mae: 2.5766 - lr: 2.7996e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 3/500\n",
            "556/556 - 1s - loss: 11.1971 - mse: 11.1971 - mae: 1.5365 - val_loss: 13.8047 - val_mse: 13.8047 - val_mae: 2.6269 - lr: 2.7996e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 4/500\n",
            "556/556 - 1s - loss: 11.0274 - mse: 11.0274 - mae: 1.5225 - val_loss: 12.7916 - val_mse: 12.7916 - val_mae: 2.6010 - lr: 2.7996e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 5/500\n",
            "556/556 - 1s - loss: 11.0335 - mse: 11.0335 - mae: 1.5148 - val_loss: 14.8020 - val_mse: 14.8020 - val_mae: 2.7740 - lr: 2.7996e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 6/500\n",
            "556/556 - 1s - loss: 10.8674 - mse: 10.8674 - mae: 1.5132 - val_loss: 12.9826 - val_mse: 12.9826 - val_mae: 2.5918 - lr: 2.7996e-04 - 1s/epoch - 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-10-10 10:01:34,009]\u001b[0m Finished trial#3 resulted in value: 13.0. Current best value is 9.979999999999999 with parameters: {'activation': 'tanh', 'num_hidden_layer': 4, 'i': 6, 'learning_rate': 0.00015033774387638956}.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Score for inner fold : loss of 12.98261547088623\n",
            "Trial Number:4\n",
            "-----Fold:0--------\n",
            "Epoch 1/500\n",
            "556/556 - 5s - loss: 13.8166 - mse: 13.8166 - mae: 1.7073 - val_loss: 8.3193 - val_mse: 8.3193 - val_mae: 2.2503 - lr: 7.0712e-04 - 5s/epoch - 8ms/step\n",
            "Epoch 2/500\n",
            "556/556 - 4s - loss: 12.7518 - mse: 12.7518 - mae: 1.6296 - val_loss: 14.0775 - val_mse: 14.0775 - val_mae: 2.7289 - lr: 7.0712e-04 - 4s/epoch - 7ms/step\n",
            "Epoch 3/500\n",
            "556/556 - 4s - loss: 12.8282 - mse: 12.8282 - mae: 1.6218 - val_loss: 13.1025 - val_mse: 13.1025 - val_mae: 2.5974 - lr: 7.0712e-04 - 4s/epoch - 7ms/step\n",
            "Epoch 4/500\n",
            "556/556 - 4s - loss: 12.9558 - mse: 12.9558 - mae: 1.6150 - val_loss: 13.2959 - val_mse: 13.2959 - val_mae: 2.6065 - lr: 7.0712e-04 - 4s/epoch - 7ms/step\n",
            "Epoch 5/500\n",
            "556/556 - 4s - loss: 12.5695 - mse: 12.5695 - mae: 1.6095 - val_loss: 15.0559 - val_mse: 15.0559 - val_mae: 2.6427 - lr: 7.0712e-04 - 4s/epoch - 7ms/step\n",
            "Epoch 6/500\n",
            "556/556 - 4s - loss: 12.5553 - mse: 12.5553 - mae: 1.6072 - val_loss: 12.0182 - val_mse: 12.0182 - val_mae: 2.4931 - lr: 7.0712e-04 - 4s/epoch - 7ms/step\n",
            "Score for inner fold : loss of 12.01822280883789\n",
            "-----Fold:1--------\n",
            "Epoch 1/500\n",
            "556/556 - 4s - loss: 13.5532 - mse: 13.5532 - mae: 1.7074 - val_loss: 14.2840 - val_mse: 14.2840 - val_mae: 2.7578 - lr: 7.0712e-04 - 4s/epoch - 8ms/step\n",
            "Epoch 2/500\n",
            "556/556 - 4s - loss: 13.5122 - mse: 13.5122 - mae: 1.6365 - val_loss: 12.3965 - val_mse: 12.3965 - val_mae: 2.6327 - lr: 7.0712e-04 - 4s/epoch - 7ms/step\n",
            "Epoch 3/500\n",
            "556/556 - 4s - loss: 13.4347 - mse: 13.4347 - mae: 1.6239 - val_loss: 10.9881 - val_mse: 10.9881 - val_mae: 2.4678 - lr: 7.0712e-04 - 4s/epoch - 7ms/step\n",
            "Epoch 4/500\n",
            "556/556 - 4s - loss: 12.7686 - mse: 12.7686 - mae: 1.6180 - val_loss: 12.6152 - val_mse: 12.6152 - val_mae: 2.6975 - lr: 7.0712e-04 - 4s/epoch - 7ms/step\n",
            "Epoch 5/500\n",
            "556/556 - 4s - loss: 13.1031 - mse: 13.1031 - mae: 1.6146 - val_loss: 12.1437 - val_mse: 12.1437 - val_mae: 2.7617 - lr: 7.0712e-04 - 4s/epoch - 7ms/step\n",
            "Epoch 6/500\n",
            "556/556 - 4s - loss: 13.1186 - mse: 13.1186 - mae: 1.6176 - val_loss: 12.7223 - val_mse: 12.7223 - val_mae: 2.6851 - lr: 7.0712e-04 - 4s/epoch - 7ms/step\n",
            "Epoch 7/500\n",
            "556/556 - 4s - loss: 12.7108 - mse: 12.7108 - mae: 1.6067 - val_loss: 11.9037 - val_mse: 11.9037 - val_mae: 2.6622 - lr: 7.0712e-04 - 4s/epoch - 7ms/step\n",
            "Epoch 8/500\n",
            "556/556 - 4s - loss: 12.9073 - mse: 12.9073 - mae: 1.6126 - val_loss: 13.7647 - val_mse: 13.7647 - val_mae: 2.7698 - lr: 7.0712e-04 - 4s/epoch - 7ms/step\n",
            "Score for inner fold : loss of 13.764683723449707\n",
            "-----Fold:2--------\n",
            "Epoch 1/500\n",
            "556/556 - 4s - loss: 10.8328 - mse: 10.8328 - mae: 1.6617 - val_loss: 12.3462 - val_mse: 12.3462 - val_mae: 2.5094 - lr: 7.0712e-04 - 4s/epoch - 8ms/step\n",
            "Epoch 2/500\n",
            "556/556 - 4s - loss: 10.6018 - mse: 10.6018 - mae: 1.5940 - val_loss: 14.9177 - val_mse: 14.9177 - val_mae: 2.8729 - lr: 7.0712e-04 - 4s/epoch - 6ms/step\n",
            "Epoch 3/500\n",
            "556/556 - 4s - loss: 10.3185 - mse: 10.3185 - mae: 1.5908 - val_loss: 11.7552 - val_mse: 11.7552 - val_mae: 2.4893 - lr: 7.0712e-04 - 4s/epoch - 7ms/step\n",
            "Epoch 4/500\n",
            "556/556 - 4s - loss: 10.0684 - mse: 10.0684 - mae: 1.5703 - val_loss: 10.3864 - val_mse: 10.3864 - val_mae: 2.5005 - lr: 7.0712e-04 - 4s/epoch - 7ms/step\n",
            "Epoch 5/500\n",
            "556/556 - 4s - loss: 10.5286 - mse: 10.5286 - mae: 1.5756 - val_loss: 10.4812 - val_mse: 10.4812 - val_mae: 2.5009 - lr: 7.0712e-04 - 4s/epoch - 7ms/step\n",
            "Epoch 6/500\n",
            "556/556 - 4s - loss: 10.1500 - mse: 10.1500 - mae: 1.5743 - val_loss: 10.9156 - val_mse: 10.9156 - val_mae: 2.6087 - lr: 7.0712e-04 - 4s/epoch - 7ms/step\n",
            "Epoch 7/500\n",
            "556/556 - 4s - loss: 9.9509 - mse: 9.9509 - mae: 1.5662 - val_loss: 12.4626 - val_mse: 12.4626 - val_mae: 2.5490 - lr: 7.0712e-04 - 4s/epoch - 7ms/step\n",
            "Epoch 8/500\n",
            "556/556 - 4s - loss: 10.1403 - mse: 10.1403 - mae: 1.5665 - val_loss: 11.1185 - val_mse: 11.1185 - val_mae: 2.4759 - lr: 7.0712e-04 - 4s/epoch - 7ms/step\n",
            "Epoch 9/500\n",
            "556/556 - 4s - loss: 10.0240 - mse: 10.0240 - mae: 1.5706 - val_loss: 13.5504 - val_mse: 13.5504 - val_mae: 2.5977 - lr: 7.0712e-04 - 4s/epoch - 7ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-10-10 10:03:07,714]\u001b[0m Finished trial#4 resulted in value: 13.11. Current best value is 9.979999999999999 with parameters: {'activation': 'tanh', 'num_hidden_layer': 4, 'i': 6, 'learning_rate': 0.00015033774387638956}.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Score for inner fold : loss of 13.550370216369629\n",
            "Trial Number:5\n",
            "-----Fold:0--------\n",
            "Epoch 1/500\n",
            "556/556 - 2s - loss: 12.4417 - mse: 12.4417 - mae: 1.6399 - val_loss: 11.7207 - val_mse: 11.7207 - val_mae: 2.6268 - lr: 2.7126e-04 - 2s/epoch - 4ms/step\n",
            "Epoch 2/500\n",
            "556/556 - 1s - loss: 11.6286 - mse: 11.6286 - mae: 1.5624 - val_loss: 11.7838 - val_mse: 11.7838 - val_mae: 2.5751 - lr: 2.7126e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 3/500\n",
            "556/556 - 1s - loss: 11.4854 - mse: 11.4854 - mae: 1.5469 - val_loss: 12.4309 - val_mse: 12.4309 - val_mae: 2.6312 - lr: 2.7126e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 4/500\n",
            "556/556 - 1s - loss: 11.2277 - mse: 11.2277 - mae: 1.5357 - val_loss: 10.3566 - val_mse: 10.3566 - val_mae: 2.4737 - lr: 2.7126e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 5/500\n",
            "556/556 - 1s - loss: 11.0659 - mse: 11.0659 - mae: 1.5296 - val_loss: 11.2783 - val_mse: 11.2783 - val_mae: 2.5575 - lr: 2.7126e-04 - 1s/epoch - 3ms/step\n",
            "Epoch 6/500\n",
            "556/556 - 1s - loss: 10.8506 - mse: 10.8506 - mae: 1.5234 - val_loss: 12.1654 - val_mse: 12.1654 - val_mae: 2.5899 - lr: 2.7126e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 7/500\n",
            "556/556 - 1s - loss: 10.4806 - mse: 10.4806 - mae: 1.5123 - val_loss: 11.7270 - val_mse: 11.7270 - val_mae: 2.7250 - lr: 2.7126e-04 - 1s/epoch - 3ms/step\n",
            "Epoch 8/500\n",
            "556/556 - 1s - loss: 10.7717 - mse: 10.7717 - mae: 1.5091 - val_loss: 11.6883 - val_mse: 11.6883 - val_mae: 2.6381 - lr: 2.7126e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 9/500\n",
            "556/556 - 1s - loss: 10.6999 - mse: 10.6999 - mae: 1.5053 - val_loss: 9.9648 - val_mse: 9.9648 - val_mae: 2.4866 - lr: 2.7126e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 10/500\n",
            "556/556 - 1s - loss: 10.7993 - mse: 10.7993 - mae: 1.5049 - val_loss: 12.2710 - val_mse: 12.2710 - val_mae: 2.6588 - lr: 2.7126e-04 - 1s/epoch - 3ms/step\n",
            "Epoch 11/500\n",
            "556/556 - 1s - loss: 10.7401 - mse: 10.7401 - mae: 1.5034 - val_loss: 12.0533 - val_mse: 12.0533 - val_mae: 2.6454 - lr: 2.7126e-04 - 1s/epoch - 3ms/step\n",
            "Epoch 12/500\n",
            "556/556 - 1s - loss: 10.6160 - mse: 10.6160 - mae: 1.4964 - val_loss: 12.3879 - val_mse: 12.3879 - val_mae: 2.6871 - lr: 2.7126e-04 - 1s/epoch - 3ms/step\n",
            "Epoch 13/500\n",
            "556/556 - 1s - loss: 10.5730 - mse: 10.5730 - mae: 1.4924 - val_loss: 13.0082 - val_mse: 13.0082 - val_mae: 2.6616 - lr: 2.7126e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 14/500\n",
            "556/556 - 1s - loss: 10.5068 - mse: 10.5068 - mae: 1.4885 - val_loss: 13.2316 - val_mse: 13.2316 - val_mae: 2.6215 - lr: 2.7126e-04 - 1s/epoch - 3ms/step\n",
            "Score for inner fold : loss of 13.231583595275879\n",
            "-----Fold:1--------\n",
            "Epoch 1/500\n",
            "556/556 - 2s - loss: 14.7057 - mse: 14.7057 - mae: 1.6754 - val_loss: 13.0388 - val_mse: 13.0388 - val_mae: 2.6753 - lr: 2.7126e-04 - 2s/epoch - 4ms/step\n",
            "Epoch 2/500\n",
            "556/556 - 1s - loss: 12.7120 - mse: 12.7120 - mae: 1.5846 - val_loss: 13.4745 - val_mse: 13.4745 - val_mae: 2.6684 - lr: 2.7126e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 3/500\n",
            "556/556 - 1s - loss: 12.5466 - mse: 12.5466 - mae: 1.5647 - val_loss: 14.0388 - val_mse: 14.0388 - val_mae: 2.6908 - lr: 2.7126e-04 - 1s/epoch - 3ms/step\n",
            "Epoch 4/500\n",
            "556/556 - 1s - loss: 12.2765 - mse: 12.2765 - mae: 1.5491 - val_loss: 12.3906 - val_mse: 12.3906 - val_mae: 2.5332 - lr: 2.7126e-04 - 1s/epoch - 3ms/step\n",
            "Epoch 5/500\n",
            "556/556 - 1s - loss: 12.0338 - mse: 12.0338 - mae: 1.5392 - val_loss: 13.6213 - val_mse: 13.6213 - val_mae: 2.6838 - lr: 2.7126e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 6/500\n",
            "556/556 - 1s - loss: 12.0909 - mse: 12.0909 - mae: 1.5384 - val_loss: 13.5593 - val_mse: 13.5593 - val_mae: 2.6441 - lr: 2.7126e-04 - 1s/epoch - 3ms/step\n",
            "Epoch 7/500\n",
            "556/556 - 1s - loss: 11.9637 - mse: 11.9637 - mae: 1.5283 - val_loss: 14.0575 - val_mse: 14.0575 - val_mae: 2.5919 - lr: 2.7126e-04 - 1s/epoch - 3ms/step\n",
            "Epoch 8/500\n",
            "556/556 - 1s - loss: 11.7293 - mse: 11.7293 - mae: 1.5188 - val_loss: 14.1641 - val_mse: 14.1641 - val_mae: 2.6256 - lr: 2.7126e-04 - 1s/epoch - 3ms/step\n",
            "Epoch 9/500\n",
            "556/556 - 1s - loss: 11.7247 - mse: 11.7247 - mae: 1.5139 - val_loss: 13.0665 - val_mse: 13.0665 - val_mae: 2.5826 - lr: 2.7126e-04 - 1s/epoch - 2ms/step\n",
            "Score for inner fold : loss of 13.066481590270996\n",
            "-----Fold:2--------\n",
            "Epoch 1/500\n",
            "556/556 - 2s - loss: 11.5242 - mse: 11.5242 - mae: 1.6200 - val_loss: 11.1196 - val_mse: 11.1196 - val_mae: 2.5119 - lr: 2.7126e-04 - 2s/epoch - 4ms/step\n",
            "Epoch 2/500\n",
            "556/556 - 1s - loss: 10.2678 - mse: 10.2678 - mae: 1.5196 - val_loss: 14.4912 - val_mse: 14.4912 - val_mae: 2.7572 - lr: 2.7126e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 3/500\n",
            "556/556 - 1s - loss: 10.0436 - mse: 10.0436 - mae: 1.5030 - val_loss: 13.5130 - val_mse: 13.5130 - val_mae: 2.6677 - lr: 2.7126e-04 - 1s/epoch - 3ms/step\n",
            "Epoch 4/500\n",
            "556/556 - 1s - loss: 9.8616 - mse: 9.8616 - mae: 1.4932 - val_loss: 14.6813 - val_mse: 14.6813 - val_mae: 2.7124 - lr: 2.7126e-04 - 1s/epoch - 3ms/step\n",
            "Epoch 5/500\n",
            "556/556 - 1s - loss: 9.6131 - mse: 9.6131 - mae: 1.4830 - val_loss: 15.3647 - val_mse: 15.3647 - val_mae: 2.8890 - lr: 2.7126e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 6/500\n",
            "556/556 - 1s - loss: 9.6598 - mse: 9.6598 - mae: 1.4772 - val_loss: 13.1429 - val_mse: 13.1429 - val_mae: 2.7295 - lr: 2.7126e-04 - 1s/epoch - 3ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-10-10 10:03:53,977]\u001b[0m Finished trial#5 resulted in value: 13.146666666666667. Current best value is 9.979999999999999 with parameters: {'activation': 'tanh', 'num_hidden_layer': 4, 'i': 6, 'learning_rate': 0.00015033774387638956}.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Score for inner fold : loss of 13.142905235290527\n",
            "Trial Number:6\n",
            "-----Fold:0--------\n",
            "Epoch 1/500\n",
            "556/556 - 3s - loss: 11.7198 - mse: 11.7198 - mae: 1.6058 - val_loss: 10.9211 - val_mse: 10.9211 - val_mae: 2.4218 - lr: 0.0044 - 3s/epoch - 5ms/step\n",
            "Epoch 2/500\n",
            "556/556 - 2s - loss: 11.3392 - mse: 11.3392 - mae: 1.5541 - val_loss: 11.2349 - val_mse: 11.2349 - val_mae: 2.6036 - lr: 0.0044 - 2s/epoch - 3ms/step\n",
            "Epoch 3/500\n",
            "556/556 - 2s - loss: 10.8894 - mse: 10.8894 - mae: 1.5400 - val_loss: 17.3996 - val_mse: 17.3996 - val_mae: 2.6127 - lr: 0.0044 - 2s/epoch - 3ms/step\n",
            "Epoch 4/500\n",
            "556/556 - 2s - loss: 10.7836 - mse: 10.7836 - mae: 1.5238 - val_loss: 10.2923 - val_mse: 10.2923 - val_mae: 2.4048 - lr: 0.0044 - 2s/epoch - 3ms/step\n",
            "Epoch 5/500\n",
            "556/556 - 2s - loss: 11.1823 - mse: 11.1823 - mae: 1.5201 - val_loss: 12.2808 - val_mse: 12.2808 - val_mae: 2.5638 - lr: 0.0044 - 2s/epoch - 3ms/step\n",
            "Epoch 6/500\n",
            "556/556 - 2s - loss: 10.7756 - mse: 10.7756 - mae: 1.5083 - val_loss: 16.4927 - val_mse: 16.4927 - val_mae: 2.9370 - lr: 0.0044 - 2s/epoch - 3ms/step\n",
            "Epoch 7/500\n",
            "556/556 - 2s - loss: 10.6179 - mse: 10.6179 - mae: 1.5095 - val_loss: 9.4056 - val_mse: 9.4056 - val_mae: 2.4624 - lr: 0.0044 - 2s/epoch - 3ms/step\n",
            "Epoch 8/500\n",
            "556/556 - 2s - loss: 10.7685 - mse: 10.7685 - mae: 1.5128 - val_loss: 12.2733 - val_mse: 12.2733 - val_mae: 2.7570 - lr: 0.0044 - 2s/epoch - 3ms/step\n",
            "Epoch 9/500\n",
            "556/556 - 2s - loss: 10.2396 - mse: 10.2396 - mae: 1.5115 - val_loss: 10.7221 - val_mse: 10.7221 - val_mae: 2.5952 - lr: 0.0044 - 2s/epoch - 3ms/step\n",
            "Epoch 10/500\n",
            "556/556 - 2s - loss: 11.5331 - mse: 11.5331 - mae: 1.5114 - val_loss: 10.5735 - val_mse: 10.5735 - val_mae: 2.6572 - lr: 0.0044 - 2s/epoch - 3ms/step\n",
            "Epoch 11/500\n",
            "556/556 - 2s - loss: 10.2325 - mse: 10.2325 - mae: 1.5016 - val_loss: 11.8465 - val_mse: 11.8465 - val_mae: 2.6162 - lr: 0.0044 - 2s/epoch - 3ms/step\n",
            "Epoch 12/500\n",
            "556/556 - 2s - loss: 10.7379 - mse: 10.7379 - mae: 1.4984 - val_loss: 11.2798 - val_mse: 11.2798 - val_mae: 2.6307 - lr: 0.0044 - 2s/epoch - 3ms/step\n",
            "Score for inner fold : loss of 11.279806137084961\n",
            "-----Fold:1--------\n",
            "Epoch 1/500\n",
            "556/556 - 3s - loss: 11.9883 - mse: 11.9883 - mae: 1.5972 - val_loss: 15.9433 - val_mse: 15.9433 - val_mae: 3.0739 - lr: 0.0044 - 3s/epoch - 5ms/step\n",
            "Epoch 2/500\n",
            "556/556 - 2s - loss: 10.9916 - mse: 10.9916 - mae: 1.5311 - val_loss: 12.2628 - val_mse: 12.2628 - val_mae: 2.4475 - lr: 0.0044 - 2s/epoch - 3ms/step\n",
            "Epoch 3/500\n",
            "556/556 - 2s - loss: 10.4469 - mse: 10.4469 - mae: 1.5180 - val_loss: 13.3041 - val_mse: 13.3041 - val_mae: 2.5469 - lr: 0.0044 - 2s/epoch - 3ms/step\n",
            "Epoch 4/500\n",
            "556/556 - 2s - loss: 10.3478 - mse: 10.3478 - mae: 1.5048 - val_loss: 22.4890 - val_mse: 22.4890 - val_mae: 2.7954 - lr: 0.0044 - 2s/epoch - 3ms/step\n",
            "Epoch 5/500\n",
            "556/556 - 2s - loss: 10.8260 - mse: 10.8260 - mae: 1.5011 - val_loss: 25.1661 - val_mse: 25.1661 - val_mae: 2.6163 - lr: 0.0044 - 2s/epoch - 3ms/step\n",
            "Epoch 6/500\n",
            "556/556 - 2s - loss: 10.4256 - mse: 10.4256 - mae: 1.4889 - val_loss: 13.1107 - val_mse: 13.1107 - val_mae: 2.5369 - lr: 0.0044 - 2s/epoch - 3ms/step\n",
            "Epoch 7/500\n",
            "556/556 - 2s - loss: 10.6968 - mse: 10.6968 - mae: 1.4947 - val_loss: 23.9093 - val_mse: 23.9093 - val_mae: 2.7804 - lr: 0.0044 - 2s/epoch - 3ms/step\n",
            "Score for inner fold : loss of 23.909252166748047\n",
            "-----Fold:2--------\n",
            "Epoch 1/500\n",
            "556/556 - 2s - loss: 12.1275 - mse: 12.1275 - mae: 1.6062 - val_loss: 19.5254 - val_mse: 19.5254 - val_mae: 3.2920 - lr: 0.0044 - 2s/epoch - 4ms/step\n",
            "Epoch 2/500\n",
            "556/556 - 2s - loss: 12.7850 - mse: 12.7850 - mae: 1.5515 - val_loss: 14.2859 - val_mse: 14.2859 - val_mae: 2.5443 - lr: 0.0044 - 2s/epoch - 3ms/step\n",
            "Epoch 3/500\n",
            "556/556 - 2s - loss: 11.5740 - mse: 11.5740 - mae: 1.5276 - val_loss: 12.2981 - val_mse: 12.2981 - val_mae: 2.4448 - lr: 0.0044 - 2s/epoch - 3ms/step\n",
            "Epoch 4/500\n",
            "556/556 - 2s - loss: 12.1345 - mse: 12.1345 - mae: 1.5272 - val_loss: 15.2715 - val_mse: 15.2715 - val_mae: 2.8097 - lr: 0.0044 - 2s/epoch - 3ms/step\n",
            "Epoch 5/500\n",
            "556/556 - 2s - loss: 11.2045 - mse: 11.2045 - mae: 1.5096 - val_loss: 15.4831 - val_mse: 15.4831 - val_mae: 2.7856 - lr: 0.0044 - 2s/epoch - 3ms/step\n",
            "Epoch 6/500\n",
            "556/556 - 2s - loss: 11.3441 - mse: 11.3441 - mae: 1.5053 - val_loss: 13.2686 - val_mse: 13.2686 - val_mae: 2.5237 - lr: 0.0044 - 2s/epoch - 3ms/step\n",
            "Epoch 7/500\n",
            "556/556 - 2s - loss: 11.2007 - mse: 11.2007 - mae: 1.5075 - val_loss: 15.8061 - val_mse: 15.8061 - val_mae: 2.5221 - lr: 0.0044 - 2s/epoch - 3ms/step\n",
            "Epoch 8/500\n",
            "556/556 - 2s - loss: 11.5090 - mse: 11.5090 - mae: 1.5072 - val_loss: 12.2705 - val_mse: 12.2705 - val_mae: 2.5080 - lr: 0.0044 - 2s/epoch - 3ms/step\n",
            "Epoch 9/500\n",
            "556/556 - 2s - loss: 10.9442 - mse: 10.9442 - mae: 1.5049 - val_loss: 14.0278 - val_mse: 14.0278 - val_mae: 2.5025 - lr: 0.0044 - 2s/epoch - 3ms/step\n",
            "Epoch 10/500\n",
            "556/556 - 2s - loss: 10.6418 - mse: 10.6418 - mae: 1.5017 - val_loss: 19.0951 - val_mse: 19.0951 - val_mae: 2.6871 - lr: 0.0044 - 2s/epoch - 3ms/step\n",
            "Epoch 11/500\n",
            "556/556 - 2s - loss: 10.2286 - mse: 10.2286 - mae: 1.4938 - val_loss: 15.2753 - val_mse: 15.2753 - val_mae: 2.8493 - lr: 0.0044 - 2s/epoch - 3ms/step\n",
            "Epoch 12/500\n",
            "556/556 - 2s - loss: 10.0910 - mse: 10.0910 - mae: 1.4939 - val_loss: 15.0514 - val_mse: 15.0514 - val_mae: 2.8104 - lr: 0.0044 - 2s/epoch - 3ms/step\n",
            "Epoch 13/500\n",
            "556/556 - 2s - loss: 10.0426 - mse: 10.0426 - mae: 1.4781 - val_loss: 14.3075 - val_mse: 14.3075 - val_mae: 2.6183 - lr: 0.0044 - 2s/epoch - 3ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-10-10 10:04:58,140]\u001b[0m Finished trial#6 resulted in value: 16.5. Current best value is 9.979999999999999 with parameters: {'activation': 'tanh', 'num_hidden_layer': 4, 'i': 6, 'learning_rate': 0.00015033774387638956}.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Score for inner fold : loss of 14.307525634765625\n",
            "Trial Number:7\n",
            "-----Fold:0--------\n",
            "Epoch 1/500\n",
            "556/556 - 2s - loss: 14.2424 - mse: 14.2424 - mae: 1.7656 - val_loss: 9.8600 - val_mse: 9.8600 - val_mae: 2.5775 - lr: 1.8988e-04 - 2s/epoch - 4ms/step\n",
            "Epoch 2/500\n",
            "556/556 - 2s - loss: 12.0297 - mse: 12.0297 - mae: 1.5652 - val_loss: 11.2270 - val_mse: 11.2270 - val_mae: 2.6859 - lr: 1.8988e-04 - 2s/epoch - 3ms/step\n",
            "Epoch 3/500\n",
            "556/556 - 2s - loss: 11.6892 - mse: 11.6892 - mae: 1.5476 - val_loss: 10.4302 - val_mse: 10.4302 - val_mae: 2.6401 - lr: 1.8988e-04 - 2s/epoch - 3ms/step\n",
            "Epoch 4/500\n",
            "556/556 - 2s - loss: 11.4402 - mse: 11.4402 - mae: 1.5371 - val_loss: 9.8969 - val_mse: 9.8969 - val_mae: 2.5416 - lr: 1.8988e-04 - 2s/epoch - 3ms/step\n",
            "Epoch 5/500\n",
            "556/556 - 2s - loss: 11.3144 - mse: 11.3144 - mae: 1.5261 - val_loss: 11.5740 - val_mse: 11.5740 - val_mae: 2.7605 - lr: 1.8988e-04 - 2s/epoch - 3ms/step\n",
            "Epoch 6/500\n",
            "556/556 - 2s - loss: 11.1776 - mse: 11.1776 - mae: 1.5195 - val_loss: 11.5523 - val_mse: 11.5523 - val_mae: 2.7262 - lr: 1.8988e-04 - 2s/epoch - 3ms/step\n",
            "Score for inner fold : loss of 11.552276611328125\n",
            "-----Fold:1--------\n",
            "Epoch 1/500\n",
            "556/556 - 2s - loss: 14.8791 - mse: 14.8791 - mae: 1.7481 - val_loss: 10.7935 - val_mse: 10.7935 - val_mae: 2.7252 - lr: 1.8988e-04 - 2s/epoch - 4ms/step\n",
            "Epoch 2/500\n",
            "556/556 - 2s - loss: 12.7260 - mse: 12.7260 - mae: 1.5597 - val_loss: 9.9181 - val_mse: 9.9181 - val_mae: 2.5496 - lr: 1.8988e-04 - 2s/epoch - 3ms/step\n",
            "Epoch 3/500\n",
            "556/556 - 2s - loss: 12.4029 - mse: 12.4029 - mae: 1.5426 - val_loss: 9.5457 - val_mse: 9.5457 - val_mae: 2.5308 - lr: 1.8988e-04 - 2s/epoch - 3ms/step\n",
            "Epoch 4/500\n",
            "556/556 - 2s - loss: 12.1647 - mse: 12.1647 - mae: 1.5282 - val_loss: 11.0934 - val_mse: 11.0934 - val_mae: 2.6412 - lr: 1.8988e-04 - 2s/epoch - 3ms/step\n",
            "Epoch 5/500\n",
            "556/556 - 2s - loss: 11.9774 - mse: 11.9774 - mae: 1.5252 - val_loss: 11.0624 - val_mse: 11.0624 - val_mae: 2.6623 - lr: 1.8988e-04 - 2s/epoch - 3ms/step\n",
            "Epoch 6/500\n",
            "556/556 - 2s - loss: 11.8590 - mse: 11.8590 - mae: 1.5154 - val_loss: 11.3945 - val_mse: 11.3945 - val_mae: 2.6741 - lr: 1.8988e-04 - 2s/epoch - 3ms/step\n",
            "Epoch 7/500\n",
            "556/556 - 2s - loss: 11.7501 - mse: 11.7501 - mae: 1.5105 - val_loss: 10.6462 - val_mse: 10.6462 - val_mae: 2.6104 - lr: 1.8988e-04 - 2s/epoch - 3ms/step\n",
            "Epoch 8/500\n",
            "556/556 - 2s - loss: 11.6260 - mse: 11.6260 - mae: 1.5091 - val_loss: 11.0306 - val_mse: 11.0306 - val_mae: 2.5834 - lr: 1.8988e-04 - 2s/epoch - 3ms/step\n",
            "Score for inner fold : loss of 11.030563354492188\n",
            "-----Fold:2--------\n",
            "Epoch 1/500\n",
            "556/556 - 2s - loss: 13.8583 - mse: 13.8583 - mae: 1.7215 - val_loss: 8.9324 - val_mse: 8.9324 - val_mae: 2.5019 - lr: 1.8988e-04 - 2s/epoch - 4ms/step\n",
            "Epoch 2/500\n",
            "556/556 - 2s - loss: 11.7150 - mse: 11.7150 - mae: 1.5396 - val_loss: 10.3043 - val_mse: 10.3043 - val_mae: 2.6572 - lr: 1.8988e-04 - 2s/epoch - 3ms/step\n",
            "Epoch 3/500\n",
            "556/556 - 2s - loss: 11.3856 - mse: 11.3856 - mae: 1.5268 - val_loss: 9.1976 - val_mse: 9.1976 - val_mae: 2.4785 - lr: 1.8988e-04 - 2s/epoch - 3ms/step\n",
            "Epoch 4/500\n",
            "556/556 - 2s - loss: 11.1819 - mse: 11.1819 - mae: 1.5095 - val_loss: 9.8598 - val_mse: 9.8598 - val_mae: 2.5468 - lr: 1.8988e-04 - 2s/epoch - 3ms/step\n",
            "Epoch 5/500\n",
            "556/556 - 2s - loss: 11.0184 - mse: 11.0184 - mae: 1.5046 - val_loss: 9.7797 - val_mse: 9.7797 - val_mae: 2.5145 - lr: 1.8988e-04 - 2s/epoch - 3ms/step\n",
            "Epoch 6/500\n",
            "556/556 - 2s - loss: 10.9190 - mse: 10.9190 - mae: 1.4990 - val_loss: 10.3481 - val_mse: 10.3481 - val_mae: 2.5962 - lr: 1.8988e-04 - 2s/epoch - 3ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-10-10 10:05:36,260]\u001b[0m Finished trial#7 resulted in value: 10.976666666666667. Current best value is 9.979999999999999 with parameters: {'activation': 'tanh', 'num_hidden_layer': 4, 'i': 6, 'learning_rate': 0.00015033774387638956}.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Score for inner fold : loss of 10.34813404083252\n",
            "Trial Number:8\n",
            "-----Fold:0--------\n",
            "Epoch 1/500\n",
            "556/556 - 3s - loss: 12.1181 - mse: 12.1181 - mae: 1.7745 - val_loss: 11.1147 - val_mse: 11.1147 - val_mae: 2.4571 - lr: 0.0078 - 3s/epoch - 6ms/step\n",
            "Epoch 2/500\n",
            "556/556 - 3s - loss: 11.0682 - mse: 11.0682 - mae: 1.7095 - val_loss: 13.0823 - val_mse: 13.0823 - val_mae: 2.9255 - lr: 0.0078 - 3s/epoch - 5ms/step\n",
            "Epoch 3/500\n",
            "556/556 - 3s - loss: 10.8000 - mse: 10.8000 - mae: 1.7260 - val_loss: 15.3055 - val_mse: 15.3055 - val_mae: 3.1544 - lr: 0.0078 - 3s/epoch - 5ms/step\n",
            "Epoch 4/500\n",
            "556/556 - 3s - loss: 10.5355 - mse: 10.5355 - mae: 1.6679 - val_loss: 7.5832 - val_mse: 7.5832 - val_mae: 1.8152 - lr: 0.0078 - 3s/epoch - 5ms/step\n",
            "Epoch 5/500\n",
            "556/556 - 3s - loss: 10.8108 - mse: 10.8108 - mae: 1.7181 - val_loss: 9.7332 - val_mse: 9.7332 - val_mae: 2.2412 - lr: 0.0078 - 3s/epoch - 5ms/step\n",
            "Epoch 6/500\n",
            "556/556 - 3s - loss: 10.4362 - mse: 10.4362 - mae: 1.6762 - val_loss: 9.6943 - val_mse: 9.6943 - val_mae: 2.2876 - lr: 0.0078 - 3s/epoch - 5ms/step\n",
            "Epoch 7/500\n",
            "556/556 - 3s - loss: 10.3406 - mse: 10.3406 - mae: 1.6747 - val_loss: 8.4047 - val_mse: 8.4047 - val_mae: 1.9820 - lr: 0.0078 - 3s/epoch - 5ms/step\n",
            "Epoch 8/500\n",
            "556/556 - 3s - loss: 10.3103 - mse: 10.3103 - mae: 1.6872 - val_loss: 11.9023 - val_mse: 11.9023 - val_mae: 2.4937 - lr: 0.0078 - 3s/epoch - 5ms/step\n",
            "Epoch 9/500\n",
            "556/556 - 3s - loss: 10.2214 - mse: 10.2214 - mae: 1.6678 - val_loss: 8.9010 - val_mse: 8.9010 - val_mae: 1.9123 - lr: 0.0078 - 3s/epoch - 5ms/step\n",
            "Score for inner fold : loss of 8.900955200195312\n",
            "-----Fold:1--------\n",
            "Epoch 1/500\n",
            "556/556 - 4s - loss: 13.3493 - mse: 13.3493 - mae: 1.7559 - val_loss: 13.3319 - val_mse: 13.3319 - val_mae: 2.9206 - lr: 0.0078 - 4s/epoch - 6ms/step\n",
            "Epoch 2/500\n",
            "556/556 - 3s - loss: 12.1965 - mse: 12.1965 - mae: 1.6972 - val_loss: 13.0930 - val_mse: 13.0930 - val_mae: 2.7412 - lr: 0.0078 - 3s/epoch - 5ms/step\n",
            "Epoch 3/500\n",
            "556/556 - 3s - loss: 11.5275 - mse: 11.5275 - mae: 1.7112 - val_loss: 4.5196 - val_mse: 4.5196 - val_mae: 1.4054 - lr: 0.0078 - 3s/epoch - 5ms/step\n",
            "Epoch 4/500\n",
            "556/556 - 3s - loss: 11.5080 - mse: 11.5080 - mae: 1.7089 - val_loss: 16.9146 - val_mse: 16.9146 - val_mae: 3.2562 - lr: 0.0078 - 3s/epoch - 5ms/step\n",
            "Epoch 5/500\n",
            "556/556 - 3s - loss: 11.4926 - mse: 11.4926 - mae: 1.7203 - val_loss: 12.4868 - val_mse: 12.4868 - val_mae: 3.0254 - lr: 0.0078 - 3s/epoch - 5ms/step\n",
            "Epoch 6/500\n",
            "556/556 - 3s - loss: 11.3591 - mse: 11.3591 - mae: 1.6981 - val_loss: 10.3333 - val_mse: 10.3333 - val_mae: 2.0639 - lr: 0.0078 - 3s/epoch - 5ms/step\n",
            "Epoch 7/500\n",
            "556/556 - 3s - loss: 11.2272 - mse: 11.2272 - mae: 1.6883 - val_loss: 13.4186 - val_mse: 13.4186 - val_mae: 2.9166 - lr: 0.0078 - 3s/epoch - 5ms/step\n",
            "Epoch 8/500\n",
            "556/556 - 3s - loss: 11.4606 - mse: 11.4606 - mae: 1.7251 - val_loss: 13.3352 - val_mse: 13.3352 - val_mae: 3.1062 - lr: 0.0078 - 3s/epoch - 5ms/step\n",
            "Score for inner fold : loss of 13.335149765014648\n",
            "-----Fold:2--------\n",
            "Epoch 1/500\n",
            "556/556 - 3s - loss: 13.8383 - mse: 13.8383 - mae: 1.7804 - val_loss: 9.3159 - val_mse: 9.3159 - val_mae: 1.9917 - lr: 0.0078 - 3s/epoch - 6ms/step\n",
            "Epoch 2/500\n",
            "556/556 - 3s - loss: 12.3148 - mse: 12.3148 - mae: 1.6935 - val_loss: 12.5292 - val_mse: 12.5292 - val_mae: 2.8558 - lr: 0.0078 - 3s/epoch - 5ms/step\n",
            "Epoch 3/500\n",
            "556/556 - 3s - loss: 11.8040 - mse: 11.8040 - mae: 1.6767 - val_loss: 12.3037 - val_mse: 12.3037 - val_mae: 2.8888 - lr: 0.0078 - 3s/epoch - 5ms/step\n",
            "Epoch 4/500\n",
            "556/556 - 3s - loss: 11.8750 - mse: 11.8750 - mae: 1.7147 - val_loss: 21.8443 - val_mse: 21.8443 - val_mae: 4.0935 - lr: 0.0078 - 3s/epoch - 5ms/step\n",
            "Epoch 5/500\n",
            "556/556 - 3s - loss: 11.8446 - mse: 11.8446 - mae: 1.7111 - val_loss: 21.1473 - val_mse: 21.1473 - val_mae: 3.6325 - lr: 0.0078 - 3s/epoch - 5ms/step\n",
            "Epoch 6/500\n",
            "556/556 - 3s - loss: 11.9262 - mse: 11.9262 - mae: 1.7593 - val_loss: 19.1266 - val_mse: 19.1266 - val_mae: 3.2851 - lr: 0.0078 - 3s/epoch - 5ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-10-10 10:06:47,109]\u001b[0m Finished trial#8 resulted in value: 13.790000000000001. Current best value is 9.979999999999999 with parameters: {'activation': 'tanh', 'num_hidden_layer': 4, 'i': 6, 'learning_rate': 0.00015033774387638956}.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Score for inner fold : loss of 19.126558303833008\n",
            "Trial Number:9\n",
            "-----Fold:0--------\n",
            "Epoch 1/500\n",
            "556/556 - 2s - loss: 17.0314 - mse: 17.0314 - mae: 1.7773 - val_loss: 8.0370 - val_mse: 8.0370 - val_mae: 2.5191 - lr: 0.0018 - 2s/epoch - 3ms/step\n",
            "Epoch 2/500\n",
            "556/556 - 1s - loss: 14.4966 - mse: 14.4966 - mae: 1.6237 - val_loss: 8.9156 - val_mse: 8.9156 - val_mae: 2.6143 - lr: 0.0018 - 1s/epoch - 2ms/step\n",
            "Epoch 3/500\n",
            "556/556 - 1s - loss: 13.8174 - mse: 13.8174 - mae: 1.5957 - val_loss: 9.8694 - val_mse: 9.8694 - val_mae: 2.6573 - lr: 0.0018 - 1s/epoch - 2ms/step\n",
            "Epoch 4/500\n",
            "556/556 - 1s - loss: 13.3358 - mse: 13.3358 - mae: 1.5767 - val_loss: 9.7771 - val_mse: 9.7771 - val_mae: 2.5898 - lr: 0.0018 - 1s/epoch - 2ms/step\n",
            "Epoch 5/500\n",
            "556/556 - 1s - loss: 12.9815 - mse: 12.9815 - mae: 1.5575 - val_loss: 9.0522 - val_mse: 9.0522 - val_mae: 2.4381 - lr: 0.0018 - 1s/epoch - 2ms/step\n",
            "Epoch 6/500\n",
            "556/556 - 1s - loss: 12.6950 - mse: 12.6950 - mae: 1.5418 - val_loss: 9.4738 - val_mse: 9.4738 - val_mae: 2.5186 - lr: 0.0018 - 1s/epoch - 2ms/step\n",
            "Score for inner fold : loss of 9.473817825317383\n",
            "-----Fold:1--------\n",
            "Epoch 1/500\n",
            "556/556 - 2s - loss: 15.4487 - mse: 15.4487 - mae: 1.7370 - val_loss: 8.4828 - val_mse: 8.4828 - val_mae: 2.5325 - lr: 0.0018 - 2s/epoch - 3ms/step\n",
            "Epoch 2/500\n",
            "556/556 - 1s - loss: 13.3271 - mse: 13.3271 - mae: 1.6084 - val_loss: 9.9498 - val_mse: 9.9498 - val_mae: 2.6993 - lr: 0.0018 - 1s/epoch - 2ms/step\n",
            "Epoch 3/500\n",
            "556/556 - 1s - loss: 12.8383 - mse: 12.8383 - mae: 1.5882 - val_loss: 9.8303 - val_mse: 9.8303 - val_mae: 2.6466 - lr: 0.0018 - 1s/epoch - 2ms/step\n",
            "Epoch 4/500\n",
            "556/556 - 1s - loss: 12.4410 - mse: 12.4410 - mae: 1.5700 - val_loss: 10.4050 - val_mse: 10.4050 - val_mae: 2.7090 - lr: 0.0018 - 1s/epoch - 2ms/step\n",
            "Epoch 5/500\n",
            "556/556 - 1s - loss: 12.1233 - mse: 12.1233 - mae: 1.5629 - val_loss: 11.2453 - val_mse: 11.2453 - val_mae: 2.7731 - lr: 0.0018 - 1s/epoch - 2ms/step\n",
            "Epoch 6/500\n",
            "556/556 - 1s - loss: 11.9019 - mse: 11.9019 - mae: 1.5499 - val_loss: 9.6532 - val_mse: 9.6532 - val_mae: 2.5266 - lr: 0.0018 - 1s/epoch - 2ms/step\n",
            "Score for inner fold : loss of 9.65322494506836\n",
            "-----Fold:2--------\n",
            "Epoch 1/500\n",
            "556/556 - 2s - loss: 15.2802 - mse: 15.2802 - mae: 1.7453 - val_loss: 7.5007 - val_mse: 7.5007 - val_mae: 2.3743 - lr: 0.0018 - 2s/epoch - 4ms/step\n",
            "Epoch 2/500\n",
            "556/556 - 1s - loss: 13.0869 - mse: 13.0869 - mae: 1.5938 - val_loss: 8.8826 - val_mse: 8.8826 - val_mae: 2.5425 - lr: 0.0018 - 1s/epoch - 2ms/step\n",
            "Epoch 3/500\n",
            "556/556 - 1s - loss: 12.3981 - mse: 12.3981 - mae: 1.5627 - val_loss: 8.5225 - val_mse: 8.5225 - val_mae: 2.4131 - lr: 0.0018 - 1s/epoch - 2ms/step\n",
            "Epoch 4/500\n",
            "556/556 - 1s - loss: 11.9736 - mse: 11.9736 - mae: 1.5444 - val_loss: 10.1454 - val_mse: 10.1454 - val_mae: 2.6550 - lr: 0.0018 - 1s/epoch - 2ms/step\n",
            "Epoch 5/500\n",
            "556/556 - 1s - loss: 11.6425 - mse: 11.6425 - mae: 1.5385 - val_loss: 9.1228 - val_mse: 9.1228 - val_mae: 2.4552 - lr: 0.0018 - 1s/epoch - 2ms/step\n",
            "Epoch 6/500\n",
            "556/556 - 1s - loss: 11.4124 - mse: 11.4124 - mae: 1.5290 - val_loss: 9.3302 - val_mse: 9.3302 - val_mae: 2.4894 - lr: 0.0018 - 1s/epoch - 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-10-10 10:07:12,153]\u001b[0m Finished trial#9 resulted in value: 9.483333333333334. Current best value is 9.483333333333334 with parameters: {'activation': 'tanh', 'num_hidden_layer': 4, 'i': 3, 'learning_rate': 0.0018363784259603326}.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Score for inner fold : loss of 9.330215454101562\n",
            "Trial Number:10\n",
            "-----Fold:0--------\n",
            "Epoch 1/500\n",
            "556/556 - 2s - loss: 12.3442 - mse: 12.3442 - mae: 1.6797 - val_loss: 12.2086 - val_mse: 12.2086 - val_mae: 2.5975 - lr: 0.0020 - 2s/epoch - 3ms/step\n",
            "Epoch 2/500\n",
            "556/556 - 1s - loss: 11.4915 - mse: 11.4915 - mae: 1.5867 - val_loss: 11.0328 - val_mse: 11.0328 - val_mae: 2.5048 - lr: 0.0020 - 1s/epoch - 2ms/step\n",
            "Epoch 3/500\n",
            "556/556 - 1s - loss: 11.6037 - mse: 11.6037 - mae: 1.5831 - val_loss: 13.5179 - val_mse: 13.5179 - val_mae: 2.6941 - lr: 0.0020 - 1s/epoch - 2ms/step\n",
            "Epoch 4/500\n",
            "556/556 - 1s - loss: 11.5513 - mse: 11.5513 - mae: 1.5726 - val_loss: 11.1821 - val_mse: 11.1821 - val_mae: 2.5473 - lr: 0.0020 - 1s/epoch - 2ms/step\n",
            "Epoch 5/500\n",
            "556/556 - 1s - loss: 11.6338 - mse: 11.6338 - mae: 1.5772 - val_loss: 12.2971 - val_mse: 12.2971 - val_mae: 2.5944 - lr: 0.0020 - 1s/epoch - 2ms/step\n",
            "Epoch 6/500\n",
            "556/556 - 1s - loss: 11.7252 - mse: 11.7252 - mae: 1.5782 - val_loss: 12.6762 - val_mse: 12.6762 - val_mae: 2.6259 - lr: 0.0020 - 1s/epoch - 2ms/step\n",
            "Epoch 7/500\n",
            "556/556 - 1s - loss: 11.4998 - mse: 11.4998 - mae: 1.5762 - val_loss: 11.7183 - val_mse: 11.7183 - val_mae: 2.5714 - lr: 0.0020 - 1s/epoch - 2ms/step\n",
            "Score for inner fold : loss of 11.718290328979492\n",
            "-----Fold:1--------\n",
            "Epoch 1/500\n",
            "556/556 - 2s - loss: 13.5311 - mse: 13.5311 - mae: 1.7069 - val_loss: 11.2300 - val_mse: 11.2300 - val_mae: 2.6183 - lr: 0.0020 - 2s/epoch - 3ms/step\n",
            "Epoch 2/500\n",
            "556/556 - 1s - loss: 12.2042 - mse: 12.2042 - mae: 1.5984 - val_loss: 15.6032 - val_mse: 15.6032 - val_mae: 2.9761 - lr: 0.0020 - 1s/epoch - 2ms/step\n",
            "Epoch 3/500\n",
            "556/556 - 1s - loss: 11.9613 - mse: 11.9613 - mae: 1.5936 - val_loss: 11.3133 - val_mse: 11.3133 - val_mae: 2.5535 - lr: 0.0020 - 1s/epoch - 2ms/step\n",
            "Epoch 4/500\n",
            "556/556 - 1s - loss: 12.1242 - mse: 12.1242 - mae: 1.5903 - val_loss: 12.0106 - val_mse: 12.0106 - val_mae: 2.7235 - lr: 0.0020 - 1s/epoch - 2ms/step\n",
            "Epoch 5/500\n",
            "556/556 - 1s - loss: 12.1616 - mse: 12.1616 - mae: 1.5918 - val_loss: 12.1315 - val_mse: 12.1315 - val_mae: 2.7066 - lr: 0.0020 - 1s/epoch - 2ms/step\n",
            "Epoch 6/500\n",
            "556/556 - 1s - loss: 11.9611 - mse: 11.9611 - mae: 1.5927 - val_loss: 12.5596 - val_mse: 12.5596 - val_mae: 2.7050 - lr: 0.0020 - 1s/epoch - 2ms/step\n",
            "Score for inner fold : loss of 12.559558868408203\n",
            "-----Fold:2--------\n",
            "Epoch 1/500\n",
            "556/556 - 2s - loss: 13.2211 - mse: 13.2211 - mae: 1.6728 - val_loss: 13.3378 - val_mse: 13.3378 - val_mae: 2.5178 - lr: 0.0020 - 2s/epoch - 3ms/step\n",
            "Epoch 2/500\n",
            "556/556 - 1s - loss: 11.8851 - mse: 11.8851 - mae: 1.5947 - val_loss: 13.2047 - val_mse: 13.2047 - val_mae: 2.6975 - lr: 0.0020 - 1s/epoch - 2ms/step\n",
            "Epoch 3/500\n",
            "556/556 - 1s - loss: 11.9100 - mse: 11.9100 - mae: 1.5979 - val_loss: 12.2596 - val_mse: 12.2596 - val_mae: 2.6028 - lr: 0.0020 - 1s/epoch - 2ms/step\n",
            "Epoch 4/500\n",
            "556/556 - 1s - loss: 11.9646 - mse: 11.9646 - mae: 1.5924 - val_loss: 11.4895 - val_mse: 11.4895 - val_mae: 2.6106 - lr: 0.0020 - 1s/epoch - 2ms/step\n",
            "Epoch 5/500\n",
            "556/556 - 1s - loss: 11.8717 - mse: 11.8717 - mae: 1.5981 - val_loss: 13.3953 - val_mse: 13.3953 - val_mae: 2.7053 - lr: 0.0020 - 1s/epoch - 2ms/step\n",
            "Epoch 6/500\n",
            "556/556 - 1s - loss: 11.8724 - mse: 11.8724 - mae: 1.5931 - val_loss: 14.7744 - val_mse: 14.7744 - val_mae: 2.7117 - lr: 0.0020 - 1s/epoch - 2ms/step\n",
            "Epoch 7/500\n",
            "556/556 - 1s - loss: 11.9580 - mse: 11.9580 - mae: 1.5900 - val_loss: 12.9532 - val_mse: 12.9532 - val_mae: 2.6607 - lr: 0.0020 - 1s/epoch - 2ms/step\n",
            "Epoch 8/500\n",
            "556/556 - 1s - loss: 11.9105 - mse: 11.9105 - mae: 1.5935 - val_loss: 15.3968 - val_mse: 15.3968 - val_mae: 2.8098 - lr: 0.0020 - 1s/epoch - 2ms/step\n",
            "Epoch 9/500\n",
            "556/556 - 1s - loss: 11.9838 - mse: 11.9838 - mae: 1.5940 - val_loss: 13.8610 - val_mse: 13.8610 - val_mae: 2.6401 - lr: 0.0020 - 1s/epoch - 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-10-10 10:07:41,143]\u001b[0m Finished trial#10 resulted in value: 12.713333333333333. Current best value is 9.483333333333334 with parameters: {'activation': 'tanh', 'num_hidden_layer': 4, 'i': 3, 'learning_rate': 0.0018363784259603326}.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Score for inner fold : loss of 13.861029624938965\n",
            "Trial Number:11\n",
            "-----Fold:0--------\n",
            "Epoch 1/500\n",
            "556/556 - 2s - loss: 17.3998 - mse: 17.3998 - mae: 1.8158 - val_loss: 7.7173 - val_mse: 7.7173 - val_mae: 2.4765 - lr: 0.0011 - 2s/epoch - 3ms/step\n",
            "Epoch 2/500\n",
            "556/556 - 1s - loss: 14.8666 - mse: 14.8666 - mae: 1.6356 - val_loss: 8.6443 - val_mse: 8.6443 - val_mae: 2.5653 - lr: 0.0011 - 1s/epoch - 2ms/step\n",
            "Epoch 3/500\n",
            "556/556 - 1s - loss: 14.1777 - mse: 14.1777 - mae: 1.6070 - val_loss: 9.5026 - val_mse: 9.5026 - val_mae: 2.6451 - lr: 0.0011 - 1s/epoch - 2ms/step\n",
            "Epoch 4/500\n",
            "556/556 - 1s - loss: 13.7763 - mse: 13.7763 - mae: 1.5876 - val_loss: 8.6276 - val_mse: 8.6276 - val_mae: 2.4776 - lr: 0.0011 - 1s/epoch - 2ms/step\n",
            "Epoch 5/500\n",
            "556/556 - 1s - loss: 13.4478 - mse: 13.4478 - mae: 1.5723 - val_loss: 9.6327 - val_mse: 9.6327 - val_mae: 2.6333 - lr: 0.0011 - 1s/epoch - 2ms/step\n",
            "Epoch 6/500\n",
            "556/556 - 1s - loss: 13.1824 - mse: 13.1824 - mae: 1.5587 - val_loss: 9.2716 - val_mse: 9.2716 - val_mae: 2.5566 - lr: 0.0011 - 1s/epoch - 2ms/step\n",
            "Score for inner fold : loss of 9.271583557128906\n",
            "-----Fold:1--------\n",
            "Epoch 1/500\n",
            "556/556 - 2s - loss: 15.8492 - mse: 15.8492 - mae: 1.7837 - val_loss: 8.2850 - val_mse: 8.2850 - val_mae: 2.5300 - lr: 0.0011 - 2s/epoch - 3ms/step\n",
            "Epoch 2/500\n",
            "556/556 - 1s - loss: 13.6632 - mse: 13.6632 - mae: 1.6327 - val_loss: 8.6759 - val_mse: 8.6759 - val_mae: 2.5515 - lr: 0.0011 - 1s/epoch - 2ms/step\n",
            "Epoch 3/500\n",
            "556/556 - 1s - loss: 13.1264 - mse: 13.1264 - mae: 1.6036 - val_loss: 9.2698 - val_mse: 9.2698 - val_mae: 2.6152 - lr: 0.0011 - 1s/epoch - 2ms/step\n",
            "Epoch 4/500\n",
            "556/556 - 1s - loss: 12.7353 - mse: 12.7353 - mae: 1.5864 - val_loss: 8.6844 - val_mse: 8.6844 - val_mae: 2.4537 - lr: 0.0011 - 1s/epoch - 2ms/step\n",
            "Epoch 5/500\n",
            "556/556 - 1s - loss: 12.4124 - mse: 12.4124 - mae: 1.5770 - val_loss: 10.1262 - val_mse: 10.1262 - val_mae: 2.6569 - lr: 0.0011 - 1s/epoch - 2ms/step\n",
            "Epoch 6/500\n",
            "556/556 - 1s - loss: 12.1906 - mse: 12.1906 - mae: 1.5678 - val_loss: 10.4683 - val_mse: 10.4683 - val_mae: 2.6714 - lr: 0.0011 - 1s/epoch - 2ms/step\n",
            "Score for inner fold : loss of 10.468273162841797\n",
            "-----Fold:2--------\n",
            "Epoch 1/500\n",
            "556/556 - 2s - loss: 16.3344 - mse: 16.3344 - mae: 1.8124 - val_loss: 7.7450 - val_mse: 7.7450 - val_mae: 2.4803 - lr: 0.0011 - 2s/epoch - 3ms/step\n",
            "Epoch 2/500\n",
            "556/556 - 1s - loss: 13.6044 - mse: 13.6044 - mae: 1.6110 - val_loss: 8.8028 - val_mse: 8.8028 - val_mae: 2.5741 - lr: 0.0011 - 1s/epoch - 2ms/step\n",
            "Epoch 3/500\n",
            "556/556 - 1s - loss: 12.9358 - mse: 12.9358 - mae: 1.5755 - val_loss: 8.9093 - val_mse: 8.9093 - val_mae: 2.5582 - lr: 0.0011 - 1s/epoch - 2ms/step\n",
            "Epoch 4/500\n",
            "556/556 - 1s - loss: 12.4887 - mse: 12.4887 - mae: 1.5549 - val_loss: 8.6412 - val_mse: 8.6412 - val_mae: 2.4951 - lr: 0.0011 - 1s/epoch - 2ms/step\n",
            "Epoch 5/500\n",
            "556/556 - 1s - loss: 12.1705 - mse: 12.1705 - mae: 1.5447 - val_loss: 9.8884 - val_mse: 9.8884 - val_mae: 2.6561 - lr: 0.0011 - 1s/epoch - 2ms/step\n",
            "Epoch 6/500\n",
            "556/556 - 1s - loss: 11.9123 - mse: 11.9123 - mae: 1.5430 - val_loss: 8.8245 - val_mse: 8.8245 - val_mae: 2.4822 - lr: 0.0011 - 1s/epoch - 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-10-10 10:08:06,206]\u001b[0m Finished trial#11 resulted in value: 9.520000000000001. Current best value is 9.483333333333334 with parameters: {'activation': 'tanh', 'num_hidden_layer': 4, 'i': 3, 'learning_rate': 0.0018363784259603326}.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Score for inner fold : loss of 8.824467658996582\n",
            "Trial Number:12\n",
            "-----Fold:0--------\n",
            "Epoch 1/500\n",
            "556/556 - 2s - loss: 17.1819 - mse: 17.1819 - mae: 1.8089 - val_loss: 8.4842 - val_mse: 8.4842 - val_mae: 2.5615 - lr: 0.0014 - 2s/epoch - 3ms/step\n",
            "Epoch 2/500\n",
            "556/556 - 1s - loss: 14.7698 - mse: 14.7698 - mae: 1.6391 - val_loss: 9.9209 - val_mse: 9.9209 - val_mae: 2.7413 - lr: 0.0014 - 1s/epoch - 2ms/step\n",
            "Epoch 3/500\n",
            "556/556 - 1s - loss: 14.1062 - mse: 14.1062 - mae: 1.5980 - val_loss: 9.4125 - val_mse: 9.4125 - val_mae: 2.5939 - lr: 0.0014 - 1s/epoch - 2ms/step\n",
            "Epoch 4/500\n",
            "556/556 - 1s - loss: 13.6039 - mse: 13.6039 - mae: 1.5701 - val_loss: 10.1321 - val_mse: 10.1321 - val_mae: 2.7225 - lr: 0.0014 - 1s/epoch - 2ms/step\n",
            "Epoch 5/500\n",
            "556/556 - 1s - loss: 13.2709 - mse: 13.2709 - mae: 1.5653 - val_loss: 9.7457 - val_mse: 9.7457 - val_mae: 2.5687 - lr: 0.0014 - 1s/epoch - 2ms/step\n",
            "Epoch 6/500\n",
            "556/556 - 1s - loss: 13.0181 - mse: 13.0181 - mae: 1.5490 - val_loss: 9.7405 - val_mse: 9.7405 - val_mae: 2.5592 - lr: 0.0014 - 1s/epoch - 2ms/step\n",
            "Score for inner fold : loss of 9.740522384643555\n",
            "-----Fold:1--------\n",
            "Epoch 1/500\n",
            "556/556 - 2s - loss: 16.7762 - mse: 16.7762 - mae: 1.7959 - val_loss: 8.0783 - val_mse: 8.0783 - val_mae: 2.4973 - lr: 0.0014 - 2s/epoch - 3ms/step\n",
            "Epoch 2/500\n",
            "556/556 - 1s - loss: 14.4887 - mse: 14.4887 - mae: 1.6363 - val_loss: 8.0856 - val_mse: 8.0856 - val_mae: 2.4398 - lr: 0.0014 - 1s/epoch - 2ms/step\n",
            "Epoch 3/500\n",
            "556/556 - 1s - loss: 13.8672 - mse: 13.8672 - mae: 1.6123 - val_loss: 8.8544 - val_mse: 8.8544 - val_mae: 2.5289 - lr: 0.0014 - 1s/epoch - 2ms/step\n",
            "Epoch 4/500\n",
            "556/556 - 1s - loss: 13.4531 - mse: 13.4531 - mae: 1.5874 - val_loss: 9.8986 - val_mse: 9.8986 - val_mae: 2.6630 - lr: 0.0014 - 1s/epoch - 2ms/step\n",
            "Epoch 5/500\n",
            "556/556 - 1s - loss: 13.1488 - mse: 13.1488 - mae: 1.5755 - val_loss: 9.5859 - val_mse: 9.5859 - val_mae: 2.6499 - lr: 0.0014 - 1s/epoch - 2ms/step\n",
            "Epoch 6/500\n",
            "556/556 - 1s - loss: 12.8865 - mse: 12.8865 - mae: 1.5626 - val_loss: 10.8159 - val_mse: 10.8159 - val_mae: 2.7750 - lr: 0.0014 - 1s/epoch - 2ms/step\n",
            "Score for inner fold : loss of 10.815946578979492\n",
            "-----Fold:2--------\n",
            "Epoch 1/500\n",
            "556/556 - 2s - loss: 14.6044 - mse: 14.6044 - mae: 1.7618 - val_loss: 7.6074 - val_mse: 7.6074 - val_mae: 2.4150 - lr: 0.0014 - 2s/epoch - 3ms/step\n",
            "Epoch 2/500\n",
            "556/556 - 1s - loss: 12.3521 - mse: 12.3521 - mae: 1.5758 - val_loss: 8.8124 - val_mse: 8.8124 - val_mae: 2.5521 - lr: 0.0014 - 1s/epoch - 2ms/step\n",
            "Epoch 3/500\n",
            "556/556 - 1s - loss: 11.8048 - mse: 11.8048 - mae: 1.5687 - val_loss: 8.9510 - val_mse: 8.9510 - val_mae: 2.5334 - lr: 0.0014 - 1s/epoch - 2ms/step\n",
            "Epoch 4/500\n",
            "556/556 - 1s - loss: 11.4764 - mse: 11.4764 - mae: 1.5588 - val_loss: 8.6570 - val_mse: 8.6570 - val_mae: 2.4779 - lr: 0.0014 - 1s/epoch - 2ms/step\n",
            "Epoch 5/500\n",
            "556/556 - 1s - loss: 11.2318 - mse: 11.2318 - mae: 1.5461 - val_loss: 9.7418 - val_mse: 9.7418 - val_mae: 2.6790 - lr: 0.0014 - 1s/epoch - 2ms/step\n",
            "Epoch 6/500\n",
            "556/556 - 1s - loss: 11.0101 - mse: 11.0101 - mae: 1.5393 - val_loss: 10.0781 - val_mse: 10.0781 - val_mae: 2.6422 - lr: 0.0014 - 1s/epoch - 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-10-10 10:08:31,294]\u001b[0m Finished trial#12 resulted in value: 10.213333333333333. Current best value is 9.483333333333334 with parameters: {'activation': 'tanh', 'num_hidden_layer': 4, 'i': 3, 'learning_rate': 0.0018363784259603326}.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Score for inner fold : loss of 10.078063011169434\n",
            "Trial Number:13\n",
            "-----Fold:0--------\n",
            "Epoch 1/500\n",
            "556/556 - 2s - loss: 15.3959 - mse: 15.3959 - mae: 1.7826 - val_loss: 8.6785 - val_mse: 8.6785 - val_mae: 2.5922 - lr: 6.1644e-04 - 2s/epoch - 3ms/step\n",
            "Epoch 2/500\n",
            "556/556 - 1s - loss: 12.9326 - mse: 12.9326 - mae: 1.5982 - val_loss: 8.7133 - val_mse: 8.7133 - val_mae: 2.5477 - lr: 6.1644e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 3/500\n",
            "556/556 - 1s - loss: 12.4197 - mse: 12.4197 - mae: 1.5685 - val_loss: 9.0773 - val_mse: 9.0773 - val_mae: 2.5440 - lr: 6.1644e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 4/500\n",
            "556/556 - 1s - loss: 12.1157 - mse: 12.1157 - mae: 1.5531 - val_loss: 9.0547 - val_mse: 9.0547 - val_mae: 2.5156 - lr: 6.1644e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 5/500\n",
            "556/556 - 1s - loss: 11.8636 - mse: 11.8636 - mae: 1.5445 - val_loss: 10.2426 - val_mse: 10.2426 - val_mae: 2.6568 - lr: 6.1644e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 6/500\n",
            "556/556 - 1s - loss: 11.6750 - mse: 11.6750 - mae: 1.5418 - val_loss: 10.3851 - val_mse: 10.3851 - val_mae: 2.6958 - lr: 6.1644e-04 - 1s/epoch - 2ms/step\n",
            "Score for inner fold : loss of 10.385123252868652\n",
            "-----Fold:1--------\n",
            "Epoch 1/500\n",
            "556/556 - 2s - loss: 17.6124 - mse: 17.6124 - mae: 1.8109 - val_loss: 8.7190 - val_mse: 8.7190 - val_mae: 2.6448 - lr: 6.1644e-04 - 2s/epoch - 3ms/step\n",
            "Epoch 2/500\n",
            "556/556 - 1s - loss: 14.6997 - mse: 14.6997 - mae: 1.6273 - val_loss: 9.4066 - val_mse: 9.4066 - val_mae: 2.6610 - lr: 6.1644e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 3/500\n",
            "556/556 - 1s - loss: 14.0346 - mse: 14.0346 - mae: 1.5965 - val_loss: 9.6922 - val_mse: 9.6922 - val_mae: 2.6531 - lr: 6.1644e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 4/500\n",
            "556/556 - 1s - loss: 13.6903 - mse: 13.6903 - mae: 1.5766 - val_loss: 10.1041 - val_mse: 10.1041 - val_mae: 2.6873 - lr: 6.1644e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 5/500\n",
            "556/556 - 1s - loss: 13.4097 - mse: 13.4097 - mae: 1.5645 - val_loss: 9.6539 - val_mse: 9.6539 - val_mae: 2.6037 - lr: 6.1644e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 6/500\n",
            "556/556 - 1s - loss: 13.1898 - mse: 13.1898 - mae: 1.5567 - val_loss: 9.7535 - val_mse: 9.7535 - val_mae: 2.5691 - lr: 6.1644e-04 - 1s/epoch - 2ms/step\n",
            "Score for inner fold : loss of 9.753491401672363\n",
            "-----Fold:2--------\n",
            "Epoch 1/500\n",
            "556/556 - 2s - loss: 16.0959 - mse: 16.0959 - mae: 1.8181 - val_loss: 8.6405 - val_mse: 8.6405 - val_mae: 2.5848 - lr: 6.1644e-04 - 2s/epoch - 3ms/step\n",
            "Epoch 2/500\n",
            "556/556 - 1s - loss: 13.5765 - mse: 13.5765 - mae: 1.6297 - val_loss: 8.6315 - val_mse: 8.6315 - val_mae: 2.5284 - lr: 6.1644e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 3/500\n",
            "556/556 - 1s - loss: 13.1178 - mse: 13.1178 - mae: 1.6038 - val_loss: 9.5392 - val_mse: 9.5392 - val_mae: 2.6380 - lr: 6.1644e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 4/500\n",
            "556/556 - 1s - loss: 12.7802 - mse: 12.7802 - mae: 1.5850 - val_loss: 9.3048 - val_mse: 9.3048 - val_mae: 2.5825 - lr: 6.1644e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 5/500\n",
            "556/556 - 1s - loss: 12.5195 - mse: 12.5195 - mae: 1.5739 - val_loss: 9.2319 - val_mse: 9.2319 - val_mae: 2.5597 - lr: 6.1644e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 6/500\n",
            "556/556 - 1s - loss: 12.3113 - mse: 12.3113 - mae: 1.5676 - val_loss: 9.3407 - val_mse: 9.3407 - val_mae: 2.5249 - lr: 6.1644e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 7/500\n",
            "556/556 - 1s - loss: 12.1402 - mse: 12.1402 - mae: 1.5562 - val_loss: 10.0671 - val_mse: 10.0671 - val_mae: 2.6183 - lr: 6.1644e-04 - 1s/epoch - 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-10-10 10:08:57,380]\u001b[0m Finished trial#13 resulted in value: 10.07. Current best value is 9.483333333333334 with parameters: {'activation': 'tanh', 'num_hidden_layer': 4, 'i': 3, 'learning_rate': 0.0018363784259603326}.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Score for inner fold : loss of 10.067070007324219\n",
            "Trial Number:14\n",
            "-----Fold:0--------\n",
            "Epoch 1/500\n",
            "556/556 - 2s - loss: 13.5175 - mse: 13.5175 - mae: 1.6465 - val_loss: 8.7740 - val_mse: 8.7740 - val_mae: 2.5326 - lr: 0.0028 - 2s/epoch - 3ms/step\n",
            "Epoch 2/500\n",
            "556/556 - 1s - loss: 11.9302 - mse: 11.9302 - mae: 1.5461 - val_loss: 10.3367 - val_mse: 10.3367 - val_mae: 2.6375 - lr: 0.0028 - 1s/epoch - 2ms/step\n",
            "Epoch 3/500\n",
            "556/556 - 1s - loss: 11.4285 - mse: 11.4285 - mae: 1.5258 - val_loss: 12.8556 - val_mse: 12.8556 - val_mae: 2.9208 - lr: 0.0028 - 1s/epoch - 2ms/step\n",
            "Epoch 4/500\n",
            "556/556 - 1s - loss: 11.1539 - mse: 11.1539 - mae: 1.5278 - val_loss: 9.8592 - val_mse: 9.8592 - val_mae: 2.5015 - lr: 0.0028 - 1s/epoch - 2ms/step\n",
            "Epoch 5/500\n",
            "556/556 - 1s - loss: 10.9115 - mse: 10.9115 - mae: 1.5144 - val_loss: 11.9246 - val_mse: 11.9246 - val_mae: 2.7546 - lr: 0.0028 - 1s/epoch - 2ms/step\n",
            "Epoch 6/500\n",
            "556/556 - 1s - loss: 10.7297 - mse: 10.7297 - mae: 1.5033 - val_loss: 10.6904 - val_mse: 10.6904 - val_mae: 2.6420 - lr: 0.0028 - 1s/epoch - 2ms/step\n",
            "Score for inner fold : loss of 10.690445899963379\n",
            "-----Fold:1--------\n",
            "Epoch 1/500\n",
            "556/556 - 2s - loss: 14.1394 - mse: 14.1394 - mae: 1.6816 - val_loss: 9.3278 - val_mse: 9.3278 - val_mae: 2.5334 - lr: 0.0028 - 2s/epoch - 3ms/step\n",
            "Epoch 2/500\n",
            "556/556 - 1s - loss: 12.4732 - mse: 12.4732 - mae: 1.5936 - val_loss: 10.7623 - val_mse: 10.7623 - val_mae: 2.6782 - lr: 0.0028 - 1s/epoch - 2ms/step\n",
            "Epoch 3/500\n",
            "556/556 - 1s - loss: 11.9462 - mse: 11.9462 - mae: 1.5728 - val_loss: 9.8025 - val_mse: 9.8025 - val_mae: 2.6122 - lr: 0.0028 - 1s/epoch - 2ms/step\n",
            "Epoch 4/500\n",
            "556/556 - 1s - loss: 11.6436 - mse: 11.6436 - mae: 1.5641 - val_loss: 10.2266 - val_mse: 10.2266 - val_mae: 2.5221 - lr: 0.0028 - 1s/epoch - 2ms/step\n",
            "Epoch 5/500\n",
            "556/556 - 1s - loss: 11.4046 - mse: 11.4046 - mae: 1.5567 - val_loss: 11.6800 - val_mse: 11.6800 - val_mae: 2.7347 - lr: 0.0028 - 1s/epoch - 2ms/step\n",
            "Epoch 6/500\n",
            "556/556 - 1s - loss: 11.2246 - mse: 11.2246 - mae: 1.5476 - val_loss: 10.3755 - val_mse: 10.3755 - val_mae: 2.5623 - lr: 0.0028 - 1s/epoch - 2ms/step\n",
            "Score for inner fold : loss of 10.375447273254395\n",
            "-----Fold:2--------\n",
            "Epoch 1/500\n",
            "556/556 - 2s - loss: 14.8562 - mse: 14.8562 - mae: 1.6825 - val_loss: 8.9366 - val_mse: 8.9366 - val_mae: 2.5786 - lr: 0.0028 - 2s/epoch - 4ms/step\n",
            "Epoch 2/500\n",
            "556/556 - 1s - loss: 13.2686 - mse: 13.2686 - mae: 1.5915 - val_loss: 10.3727 - val_mse: 10.3727 - val_mae: 2.6619 - lr: 0.0028 - 1s/epoch - 2ms/step\n",
            "Epoch 3/500\n",
            "556/556 - 1s - loss: 12.7092 - mse: 12.7092 - mae: 1.5721 - val_loss: 9.7942 - val_mse: 9.7942 - val_mae: 2.5079 - lr: 0.0028 - 1s/epoch - 2ms/step\n",
            "Epoch 4/500\n",
            "556/556 - 1s - loss: 12.3536 - mse: 12.3536 - mae: 1.5616 - val_loss: 9.9364 - val_mse: 9.9364 - val_mae: 2.5068 - lr: 0.0028 - 1s/epoch - 2ms/step\n",
            "Epoch 5/500\n",
            "556/556 - 1s - loss: 12.0645 - mse: 12.0645 - mae: 1.5442 - val_loss: 9.8240 - val_mse: 9.8240 - val_mae: 2.6081 - lr: 0.0028 - 1s/epoch - 2ms/step\n",
            "Epoch 6/500\n",
            "556/556 - 1s - loss: 11.9021 - mse: 11.9021 - mae: 1.5374 - val_loss: 12.6314 - val_mse: 12.6314 - val_mae: 2.7827 - lr: 0.0028 - 993ms/epoch - 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-10-10 10:09:22,558]\u001b[0m Finished trial#14 resulted in value: 11.233333333333334. Current best value is 9.483333333333334 with parameters: {'activation': 'tanh', 'num_hidden_layer': 4, 'i': 3, 'learning_rate': 0.0018363784259603326}.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Score for inner fold : loss of 12.631404876708984\n",
            "Trial Number:15\n",
            "-----Fold:0--------\n",
            "Epoch 1/500\n",
            "556/556 - 2s - loss: 16.4892 - mse: 16.4892 - mae: 1.7619 - val_loss: 8.9295 - val_mse: 8.9295 - val_mae: 2.5764 - lr: 0.0011 - 2s/epoch - 3ms/step\n",
            "Epoch 2/500\n",
            "556/556 - 1s - loss: 14.1342 - mse: 14.1342 - mae: 1.5991 - val_loss: 9.3960 - val_mse: 9.3960 - val_mae: 2.5652 - lr: 0.0011 - 996ms/epoch - 2ms/step\n",
            "Epoch 3/500\n",
            "556/556 - 1s - loss: 13.6999 - mse: 13.6999 - mae: 1.5785 - val_loss: 9.3761 - val_mse: 9.3761 - val_mae: 2.5935 - lr: 0.0011 - 972ms/epoch - 2ms/step\n",
            "Epoch 4/500\n",
            "556/556 - 1s - loss: 13.3821 - mse: 13.3821 - mae: 1.5691 - val_loss: 10.7892 - val_mse: 10.7892 - val_mae: 2.7289 - lr: 0.0011 - 969ms/epoch - 2ms/step\n",
            "Epoch 5/500\n",
            "556/556 - 1s - loss: 13.1354 - mse: 13.1354 - mae: 1.5629 - val_loss: 10.4523 - val_mse: 10.4523 - val_mae: 2.6973 - lr: 0.0011 - 1s/epoch - 2ms/step\n",
            "Epoch 6/500\n",
            "556/556 - 1s - loss: 12.9661 - mse: 12.9661 - mae: 1.5550 - val_loss: 11.0422 - val_mse: 11.0422 - val_mae: 2.7365 - lr: 0.0011 - 986ms/epoch - 2ms/step\n",
            "Score for inner fold : loss of 11.042232513427734\n",
            "-----Fold:1--------\n",
            "Epoch 1/500\n",
            "556/556 - 2s - loss: 15.3967 - mse: 15.3967 - mae: 1.7506 - val_loss: 9.2978 - val_mse: 9.2978 - val_mae: 2.6473 - lr: 0.0011 - 2s/epoch - 3ms/step\n",
            "Epoch 2/500\n",
            "556/556 - 1s - loss: 12.7681 - mse: 12.7681 - mae: 1.5857 - val_loss: 8.9063 - val_mse: 8.9063 - val_mae: 2.5210 - lr: 0.0011 - 1s/epoch - 2ms/step\n",
            "Epoch 3/500\n",
            "556/556 - 1s - loss: 12.2831 - mse: 12.2831 - mae: 1.5547 - val_loss: 9.3699 - val_mse: 9.3699 - val_mae: 2.5517 - lr: 0.0011 - 1s/epoch - 2ms/step\n",
            "Epoch 4/500\n",
            "556/556 - 1s - loss: 11.9263 - mse: 11.9263 - mae: 1.5412 - val_loss: 9.4217 - val_mse: 9.4217 - val_mae: 2.4942 - lr: 0.0011 - 995ms/epoch - 2ms/step\n",
            "Epoch 5/500\n",
            "556/556 - 1s - loss: 11.6278 - mse: 11.6278 - mae: 1.5337 - val_loss: 10.6944 - val_mse: 10.6944 - val_mae: 2.6592 - lr: 0.0011 - 992ms/epoch - 2ms/step\n",
            "Epoch 6/500\n",
            "556/556 - 1s - loss: 11.4125 - mse: 11.4125 - mae: 1.5270 - val_loss: 9.6493 - val_mse: 9.6493 - val_mae: 2.5515 - lr: 0.0011 - 1s/epoch - 2ms/step\n",
            "Epoch 7/500\n",
            "556/556 - 1s - loss: 11.2593 - mse: 11.2593 - mae: 1.5184 - val_loss: 10.0495 - val_mse: 10.0495 - val_mae: 2.5889 - lr: 0.0011 - 1s/epoch - 2ms/step\n",
            "Score for inner fold : loss of 10.049500465393066\n",
            "-----Fold:2--------\n",
            "Epoch 1/500\n",
            "556/556 - 2s - loss: 14.8529 - mse: 14.8529 - mae: 1.7498 - val_loss: 8.7642 - val_mse: 8.7642 - val_mae: 2.5755 - lr: 0.0011 - 2s/epoch - 3ms/step\n",
            "Epoch 2/500\n",
            "556/556 - 1s - loss: 12.3524 - mse: 12.3524 - mae: 1.5724 - val_loss: 8.6794 - val_mse: 8.6794 - val_mae: 2.4747 - lr: 0.0011 - 1s/epoch - 2ms/step\n",
            "Epoch 3/500\n",
            "556/556 - 1s - loss: 11.9119 - mse: 11.9119 - mae: 1.5458 - val_loss: 9.9718 - val_mse: 9.9718 - val_mae: 2.7019 - lr: 0.0011 - 971ms/epoch - 2ms/step\n",
            "Epoch 4/500\n",
            "556/556 - 1s - loss: 11.5914 - mse: 11.5914 - mae: 1.5351 - val_loss: 10.1610 - val_mse: 10.1610 - val_mae: 2.6502 - lr: 0.0011 - 1s/epoch - 2ms/step\n",
            "Epoch 5/500\n",
            "556/556 - 1s - loss: 11.3456 - mse: 11.3456 - mae: 1.5267 - val_loss: 9.5465 - val_mse: 9.5465 - val_mae: 2.5358 - lr: 0.0011 - 1s/epoch - 2ms/step\n",
            "Epoch 6/500\n",
            "556/556 - 1s - loss: 11.1711 - mse: 11.1711 - mae: 1.5184 - val_loss: 10.5466 - val_mse: 10.5466 - val_mae: 2.6523 - lr: 0.0011 - 1s/epoch - 2ms/step\n",
            "Epoch 7/500\n",
            "556/556 - 1s - loss: 11.0126 - mse: 11.0126 - mae: 1.5151 - val_loss: 10.0419 - val_mse: 10.0419 - val_mae: 2.5791 - lr: 0.0011 - 1s/epoch - 2ms/step\n",
            "Score for inner fold : loss of 10.041912078857422\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-10-10 10:09:48,557]\u001b[0m Finished trial#15 resulted in value: 10.376666666666667. Current best value is 9.483333333333334 with parameters: {'activation': 'tanh', 'num_hidden_layer': 4, 'i': 3, 'learning_rate': 0.0018363784259603326}.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trial Number:16\n",
            "-----Fold:0--------\n",
            "Epoch 1/500\n",
            "556/556 - 2s - loss: 14.8531 - mse: 14.8531 - mae: 1.9267 - val_loss: 9.9280 - val_mse: 9.9280 - val_mae: 2.6464 - lr: 4.4462e-04 - 2s/epoch - 3ms/step\n",
            "Epoch 2/500\n",
            "556/556 - 1s - loss: 11.0441 - mse: 11.0441 - mae: 1.6193 - val_loss: 10.6888 - val_mse: 10.6888 - val_mae: 2.6952 - lr: 4.4462e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 3/500\n",
            "556/556 - 1s - loss: 10.7122 - mse: 10.7122 - mae: 1.6076 - val_loss: 10.8892 - val_mse: 10.8892 - val_mae: 2.6481 - lr: 4.4462e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 4/500\n",
            "556/556 - 1s - loss: 10.6265 - mse: 10.6265 - mae: 1.5904 - val_loss: 11.6163 - val_mse: 11.6163 - val_mae: 2.6525 - lr: 4.4462e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 5/500\n",
            "556/556 - 1s - loss: 10.6395 - mse: 10.6395 - mae: 1.5860 - val_loss: 12.6417 - val_mse: 12.6417 - val_mae: 2.7028 - lr: 4.4462e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 6/500\n",
            "556/556 - 1s - loss: 10.6326 - mse: 10.6326 - mae: 1.5798 - val_loss: 13.1070 - val_mse: 13.1070 - val_mae: 2.7020 - lr: 4.4462e-04 - 1s/epoch - 2ms/step\n",
            "Score for inner fold : loss of 13.107036590576172\n",
            "-----Fold:1--------\n",
            "Epoch 1/500\n",
            "556/556 - 2s - loss: 16.2523 - mse: 16.2523 - mae: 1.9120 - val_loss: 11.3565 - val_mse: 11.3565 - val_mae: 2.6792 - lr: 4.4462e-04 - 2s/epoch - 3ms/step\n",
            "Epoch 2/500\n",
            "556/556 - 1s - loss: 12.0626 - mse: 12.0626 - mae: 1.6090 - val_loss: 11.9217 - val_mse: 11.9217 - val_mae: 2.6285 - lr: 4.4462e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 3/500\n",
            "556/556 - 1s - loss: 11.8678 - mse: 11.8678 - mae: 1.5869 - val_loss: 13.4135 - val_mse: 13.4135 - val_mae: 2.6631 - lr: 4.4462e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 4/500\n",
            "556/556 - 1s - loss: 11.7604 - mse: 11.7604 - mae: 1.5878 - val_loss: 14.2848 - val_mse: 14.2848 - val_mae: 2.6710 - lr: 4.4462e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 5/500\n",
            "556/556 - 1s - loss: 11.8149 - mse: 11.8149 - mae: 1.5807 - val_loss: 14.3531 - val_mse: 14.3531 - val_mae: 2.6785 - lr: 4.4462e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 6/500\n",
            "556/556 - 1s - loss: 11.7328 - mse: 11.7328 - mae: 1.5824 - val_loss: 14.8342 - val_mse: 14.8342 - val_mae: 2.6959 - lr: 4.4462e-04 - 1s/epoch - 2ms/step\n",
            "Score for inner fold : loss of 14.834178924560547\n",
            "-----Fold:2--------\n",
            "Epoch 1/500\n",
            "556/556 - 2s - loss: 16.6398 - mse: 16.6398 - mae: 1.8758 - val_loss: 10.2624 - val_mse: 10.2624 - val_mae: 2.6935 - lr: 4.4462e-04 - 2s/epoch - 3ms/step\n",
            "Epoch 2/500\n",
            "556/556 - 1s - loss: 13.1980 - mse: 13.1980 - mae: 1.6301 - val_loss: 9.6902 - val_mse: 9.6902 - val_mae: 2.5873 - lr: 4.4462e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 3/500\n",
            "556/556 - 1s - loss: 12.7193 - mse: 12.7193 - mae: 1.6088 - val_loss: 10.6649 - val_mse: 10.6649 - val_mae: 2.6513 - lr: 4.4462e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 4/500\n",
            "556/556 - 1s - loss: 12.6706 - mse: 12.6706 - mae: 1.5996 - val_loss: 11.4119 - val_mse: 11.4119 - val_mae: 2.6872 - lr: 4.4462e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 5/500\n",
            "556/556 - 1s - loss: 12.7382 - mse: 12.7382 - mae: 1.5995 - val_loss: 10.9888 - val_mse: 10.9888 - val_mae: 2.6096 - lr: 4.4462e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 6/500\n",
            "556/556 - 1s - loss: 12.6414 - mse: 12.6414 - mae: 1.5965 - val_loss: 12.1318 - val_mse: 12.1318 - val_mae: 2.6825 - lr: 4.4462e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 7/500\n",
            "556/556 - 1s - loss: 12.7523 - mse: 12.7523 - mae: 1.5879 - val_loss: 12.5782 - val_mse: 12.5782 - val_mae: 2.7136 - lr: 4.4462e-04 - 1s/epoch - 2ms/step\n",
            "Score for inner fold : loss of 12.578222274780273\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-10-10 10:10:14,587]\u001b[0m Finished trial#16 resulted in value: 13.506666666666666. Current best value is 9.483333333333334 with parameters: {'activation': 'tanh', 'num_hidden_layer': 4, 'i': 3, 'learning_rate': 0.0018363784259603326}.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trial Number:17\n",
            "-----Fold:0--------\n",
            "Epoch 1/500\n",
            "556/556 - 2s - loss: 13.8920 - mse: 13.8920 - mae: 1.6360 - val_loss: 9.1055 - val_mse: 9.1055 - val_mae: 2.4562 - lr: 0.0027 - 2s/epoch - 4ms/step\n",
            "Epoch 2/500\n",
            "556/556 - 1s - loss: 12.6034 - mse: 12.6034 - mae: 1.5596 - val_loss: 9.1143 - val_mse: 9.1143 - val_mae: 2.5297 - lr: 0.0027 - 1s/epoch - 2ms/step\n",
            "Epoch 3/500\n",
            "556/556 - 1s - loss: 12.1729 - mse: 12.1729 - mae: 1.5406 - val_loss: 10.7386 - val_mse: 10.7386 - val_mae: 2.6723 - lr: 0.0027 - 1s/epoch - 2ms/step\n",
            "Epoch 4/500\n",
            "556/556 - 1s - loss: 11.8351 - mse: 11.8351 - mae: 1.5292 - val_loss: 11.5786 - val_mse: 11.5786 - val_mae: 2.8406 - lr: 0.0027 - 1s/epoch - 2ms/step\n",
            "Epoch 5/500\n",
            "556/556 - 1s - loss: 11.6288 - mse: 11.6288 - mae: 1.5187 - val_loss: 10.5295 - val_mse: 10.5295 - val_mae: 2.5416 - lr: 0.0027 - 1s/epoch - 2ms/step\n",
            "Epoch 6/500\n",
            "556/556 - 1s - loss: 11.4922 - mse: 11.4922 - mae: 1.5262 - val_loss: 12.3132 - val_mse: 12.3132 - val_mae: 2.8677 - lr: 0.0027 - 1s/epoch - 2ms/step\n",
            "Score for inner fold : loss of 12.313161849975586\n",
            "-----Fold:1--------\n",
            "Epoch 1/500\n",
            "556/556 - 2s - loss: 11.7761 - mse: 11.7761 - mae: 1.6518 - val_loss: 11.0385 - val_mse: 11.0385 - val_mae: 2.7663 - lr: 0.0027 - 2s/epoch - 4ms/step\n",
            "Epoch 2/500\n",
            "556/556 - 1s - loss: 10.3511 - mse: 10.3511 - mae: 1.5675 - val_loss: 10.1004 - val_mse: 10.1004 - val_mae: 2.5361 - lr: 0.0027 - 1s/epoch - 2ms/step\n",
            "Epoch 3/500\n",
            "556/556 - 1s - loss: 9.9504 - mse: 9.9504 - mae: 1.5553 - val_loss: 10.0130 - val_mse: 10.0130 - val_mae: 2.4820 - lr: 0.0027 - 1s/epoch - 2ms/step\n",
            "Epoch 4/500\n",
            "556/556 - 1s - loss: 9.6658 - mse: 9.6658 - mae: 1.5380 - val_loss: 10.9537 - val_mse: 10.9537 - val_mae: 2.5642 - lr: 0.0027 - 1s/epoch - 2ms/step\n",
            "Epoch 5/500\n",
            "556/556 - 1s - loss: 9.4175 - mse: 9.4175 - mae: 1.5360 - val_loss: 8.9309 - val_mse: 8.9309 - val_mae: 2.2752 - lr: 0.0027 - 1s/epoch - 2ms/step\n",
            "Epoch 6/500\n",
            "556/556 - 1s - loss: 9.2977 - mse: 9.2977 - mae: 1.5238 - val_loss: 12.8875 - val_mse: 12.8875 - val_mae: 2.8499 - lr: 0.0027 - 1s/epoch - 2ms/step\n",
            "Epoch 7/500\n",
            "556/556 - 1s - loss: 9.1935 - mse: 9.1935 - mae: 1.5271 - val_loss: 10.8273 - val_mse: 10.8273 - val_mae: 2.5342 - lr: 0.0027 - 1s/epoch - 2ms/step\n",
            "Epoch 8/500\n",
            "556/556 - 1s - loss: 9.1091 - mse: 9.1091 - mae: 1.5223 - val_loss: 11.6509 - val_mse: 11.6509 - val_mae: 2.7305 - lr: 0.0027 - 1s/epoch - 2ms/step\n",
            "Epoch 9/500\n",
            "556/556 - 1s - loss: 8.9924 - mse: 8.9924 - mae: 1.5202 - val_loss: 11.7175 - val_mse: 11.7175 - val_mae: 2.7072 - lr: 0.0027 - 1s/epoch - 2ms/step\n",
            "Epoch 10/500\n",
            "556/556 - 1s - loss: 8.8615 - mse: 8.8615 - mae: 1.5122 - val_loss: 12.5991 - val_mse: 12.5991 - val_mae: 2.7389 - lr: 0.0027 - 1s/epoch - 2ms/step\n",
            "Score for inner fold : loss of 12.599087715148926\n",
            "-----Fold:2--------\n",
            "Epoch 1/500\n",
            "556/556 - 2s - loss: 14.5926 - mse: 14.5926 - mae: 1.6479 - val_loss: 10.5246 - val_mse: 10.5246 - val_mae: 2.6923 - lr: 0.0027 - 2s/epoch - 4ms/step\n",
            "Epoch 2/500\n",
            "556/556 - 1s - loss: 13.2091 - mse: 13.2091 - mae: 1.5737 - val_loss: 10.6689 - val_mse: 10.6689 - val_mae: 2.5783 - lr: 0.0027 - 1s/epoch - 2ms/step\n",
            "Epoch 3/500\n",
            "556/556 - 1s - loss: 12.7226 - mse: 12.7226 - mae: 1.5526 - val_loss: 12.0504 - val_mse: 12.0504 - val_mae: 2.7705 - lr: 0.0027 - 1s/epoch - 2ms/step\n",
            "Epoch 4/500\n",
            "556/556 - 1s - loss: 12.3957 - mse: 12.3957 - mae: 1.5453 - val_loss: 10.7975 - val_mse: 10.7975 - val_mae: 2.5735 - lr: 0.0027 - 1s/epoch - 2ms/step\n",
            "Epoch 5/500\n",
            "556/556 - 1s - loss: 12.0747 - mse: 12.0747 - mae: 1.5252 - val_loss: 12.1781 - val_mse: 12.1781 - val_mae: 2.8256 - lr: 0.0027 - 1s/epoch - 2ms/step\n",
            "Epoch 6/500\n",
            "556/556 - 1s - loss: 11.8545 - mse: 11.8545 - mae: 1.5280 - val_loss: 11.4443 - val_mse: 11.4443 - val_mae: 2.6953 - lr: 0.0027 - 1s/epoch - 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-10-10 10:10:47,390]\u001b[0m Finished trial#17 resulted in value: 12.116666666666667. Current best value is 9.483333333333334 with parameters: {'activation': 'tanh', 'num_hidden_layer': 4, 'i': 3, 'learning_rate': 0.0018363784259603326}.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Score for inner fold : loss of 11.44424819946289\n",
            "Trial Number:18\n",
            "-----Fold:0--------\n",
            "Epoch 1/500\n",
            "556/556 - 2s - loss: 13.7656 - mse: 13.7656 - mae: 1.6835 - val_loss: 9.4937 - val_mse: 9.4937 - val_mae: 2.6560 - lr: 0.0016 - 2s/epoch - 3ms/step\n",
            "Epoch 2/500\n",
            "556/556 - 1s - loss: 12.0171 - mse: 12.0171 - mae: 1.5681 - val_loss: 10.1159 - val_mse: 10.1159 - val_mae: 2.6404 - lr: 0.0016 - 1s/epoch - 2ms/step\n",
            "Epoch 3/500\n",
            "556/556 - 1s - loss: 11.5219 - mse: 11.5219 - mae: 1.5493 - val_loss: 10.4755 - val_mse: 10.4755 - val_mae: 2.6128 - lr: 0.0016 - 1s/epoch - 2ms/step\n",
            "Epoch 4/500\n",
            "556/556 - 1s - loss: 11.2238 - mse: 11.2238 - mae: 1.5407 - val_loss: 10.7524 - val_mse: 10.7524 - val_mae: 2.6058 - lr: 0.0016 - 1s/epoch - 2ms/step\n",
            "Epoch 5/500\n",
            "556/556 - 1s - loss: 10.9607 - mse: 10.9607 - mae: 1.5312 - val_loss: 9.9827 - val_mse: 9.9827 - val_mae: 2.5155 - lr: 0.0016 - 1s/epoch - 2ms/step\n",
            "Epoch 6/500\n",
            "556/556 - 1s - loss: 10.8334 - mse: 10.8334 - mae: 1.5261 - val_loss: 10.1198 - val_mse: 10.1198 - val_mae: 2.4982 - lr: 0.0016 - 1s/epoch - 2ms/step\n",
            "Score for inner fold : loss of 10.119806289672852\n",
            "-----Fold:1--------\n",
            "Epoch 1/500\n",
            "556/556 - 2s - loss: 14.6471 - mse: 14.6471 - mae: 1.6682 - val_loss: 8.6842 - val_mse: 8.6842 - val_mae: 2.4912 - lr: 0.0016 - 2s/epoch - 4ms/step\n",
            "Epoch 2/500\n",
            "556/556 - 1s - loss: 12.6807 - mse: 12.6807 - mae: 1.5575 - val_loss: 10.2661 - val_mse: 10.2661 - val_mae: 2.6458 - lr: 0.0016 - 1s/epoch - 2ms/step\n",
            "Epoch 3/500\n",
            "556/556 - 1s - loss: 12.2338 - mse: 12.2338 - mae: 1.5347 - val_loss: 11.3038 - val_mse: 11.3038 - val_mae: 2.7525 - lr: 0.0016 - 1s/epoch - 2ms/step\n",
            "Epoch 4/500\n",
            "556/556 - 1s - loss: 11.9404 - mse: 11.9404 - mae: 1.5355 - val_loss: 12.0309 - val_mse: 12.0309 - val_mae: 2.8043 - lr: 0.0016 - 1s/epoch - 2ms/step\n",
            "Epoch 5/500\n",
            "556/556 - 1s - loss: 11.7009 - mse: 11.7009 - mae: 1.5220 - val_loss: 11.4507 - val_mse: 11.4507 - val_mae: 2.7327 - lr: 0.0016 - 1s/epoch - 2ms/step\n",
            "Epoch 6/500\n",
            "556/556 - 1s - loss: 11.5718 - mse: 11.5718 - mae: 1.5217 - val_loss: 12.1671 - val_mse: 12.1671 - val_mae: 2.7553 - lr: 0.0016 - 1s/epoch - 2ms/step\n",
            "Score for inner fold : loss of 12.16712474822998\n",
            "-----Fold:2--------\n",
            "Epoch 1/500\n",
            "556/556 - 2s - loss: 13.6368 - mse: 13.6368 - mae: 1.6439 - val_loss: 9.7955 - val_mse: 9.7955 - val_mae: 2.6513 - lr: 0.0016 - 2s/epoch - 4ms/step\n",
            "Epoch 2/500\n",
            "556/556 - 1s - loss: 12.0219 - mse: 12.0219 - mae: 1.5380 - val_loss: 9.7470 - val_mse: 9.7470 - val_mae: 2.5723 - lr: 0.0016 - 1s/epoch - 2ms/step\n",
            "Epoch 3/500\n",
            "556/556 - 1s - loss: 11.5716 - mse: 11.5716 - mae: 1.5229 - val_loss: 10.8537 - val_mse: 10.8537 - val_mae: 2.6785 - lr: 0.0016 - 1s/epoch - 2ms/step\n",
            "Epoch 4/500\n",
            "556/556 - 1s - loss: 11.3207 - mse: 11.3207 - mae: 1.5194 - val_loss: 9.4599 - val_mse: 9.4599 - val_mae: 2.5045 - lr: 0.0016 - 1s/epoch - 2ms/step\n",
            "Epoch 5/500\n",
            "556/556 - 1s - loss: 11.2010 - mse: 11.2010 - mae: 1.5096 - val_loss: 10.9964 - val_mse: 10.9964 - val_mae: 2.6396 - lr: 0.0016 - 1s/epoch - 2ms/step\n",
            "Epoch 6/500\n",
            "556/556 - 1s - loss: 11.0388 - mse: 11.0388 - mae: 1.5079 - val_loss: 10.8451 - val_mse: 10.8451 - val_mae: 2.5904 - lr: 0.0016 - 1s/epoch - 2ms/step\n",
            "Epoch 7/500\n",
            "556/556 - 1s - loss: 10.9248 - mse: 10.9248 - mae: 1.5036 - val_loss: 10.1048 - val_mse: 10.1048 - val_mae: 2.5962 - lr: 0.0016 - 1s/epoch - 2ms/step\n",
            "Epoch 8/500\n",
            "556/556 - 1s - loss: 10.8844 - mse: 10.8844 - mae: 1.4988 - val_loss: 10.0615 - val_mse: 10.0615 - val_mae: 2.4917 - lr: 0.0016 - 1s/epoch - 2ms/step\n",
            "Epoch 9/500\n",
            "556/556 - 1s - loss: 10.7980 - mse: 10.7980 - mae: 1.4967 - val_loss: 10.5550 - val_mse: 10.5550 - val_mae: 2.5143 - lr: 0.0016 - 1s/epoch - 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-10-10 10:11:18,560]\u001b[0m Finished trial#18 resulted in value: 10.950000000000001. Current best value is 9.483333333333334 with parameters: {'activation': 'tanh', 'num_hidden_layer': 4, 'i': 3, 'learning_rate': 0.0018363784259603326}.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Score for inner fold : loss of 10.55504035949707\n",
            "Trial Number:19\n",
            "-----Fold:0--------\n",
            "Epoch 1/500\n",
            "556/556 - 2s - loss: 19.6888 - mse: 19.6888 - mae: 2.0927 - val_loss: 4.3733 - val_mse: 4.3733 - val_mae: 1.8075 - lr: 7.1335e-04 - 2s/epoch - 3ms/step\n",
            "Epoch 2/500\n",
            "556/556 - 1s - loss: 14.4115 - mse: 14.4115 - mae: 1.6010 - val_loss: 8.6411 - val_mse: 8.6411 - val_mae: 2.6172 - lr: 7.1335e-04 - 947ms/epoch - 2ms/step\n",
            "Epoch 3/500\n",
            "556/556 - 1s - loss: 13.7451 - mse: 13.7451 - mae: 1.6100 - val_loss: 8.7536 - val_mse: 8.7536 - val_mae: 2.5880 - lr: 7.1335e-04 - 952ms/epoch - 2ms/step\n",
            "Epoch 4/500\n",
            "556/556 - 1s - loss: 13.4038 - mse: 13.4038 - mae: 1.5894 - val_loss: 8.7137 - val_mse: 8.7137 - val_mae: 2.5581 - lr: 7.1335e-04 - 945ms/epoch - 2ms/step\n",
            "Epoch 5/500\n",
            "556/556 - 1s - loss: 13.1760 - mse: 13.1760 - mae: 1.5768 - val_loss: 9.1635 - val_mse: 9.1635 - val_mae: 2.6259 - lr: 7.1335e-04 - 943ms/epoch - 2ms/step\n",
            "Epoch 6/500\n",
            "556/556 - 1s - loss: 12.9906 - mse: 12.9906 - mae: 1.5721 - val_loss: 8.9132 - val_mse: 8.9132 - val_mae: 2.5412 - lr: 7.1335e-04 - 945ms/epoch - 2ms/step\n",
            "Score for inner fold : loss of 8.913207054138184\n",
            "-----Fold:1--------\n",
            "Epoch 1/500\n",
            "556/556 - 2s - loss: 18.9207 - mse: 18.9207 - mae: 2.0631 - val_loss: 5.1481 - val_mse: 5.1481 - val_mae: 1.9584 - lr: 7.1335e-04 - 2s/epoch - 3ms/step\n",
            "Epoch 2/500\n",
            "556/556 - 1s - loss: 14.3751 - mse: 14.3751 - mae: 1.6215 - val_loss: 8.3263 - val_mse: 8.3263 - val_mae: 2.5624 - lr: 7.1335e-04 - 941ms/epoch - 2ms/step\n",
            "Epoch 3/500\n",
            "556/556 - 1s - loss: 13.5506 - mse: 13.5506 - mae: 1.6197 - val_loss: 8.6149 - val_mse: 8.6149 - val_mae: 2.5752 - lr: 7.1335e-04 - 959ms/epoch - 2ms/step\n",
            "Epoch 4/500\n",
            "556/556 - 1s - loss: 13.1646 - mse: 13.1646 - mae: 1.6047 - val_loss: 8.9639 - val_mse: 8.9639 - val_mae: 2.5957 - lr: 7.1335e-04 - 933ms/epoch - 2ms/step\n",
            "Epoch 5/500\n",
            "556/556 - 1s - loss: 12.9036 - mse: 12.9036 - mae: 1.5921 - val_loss: 9.3534 - val_mse: 9.3534 - val_mae: 2.6385 - lr: 7.1335e-04 - 923ms/epoch - 2ms/step\n",
            "Epoch 6/500\n",
            "556/556 - 1s - loss: 12.7049 - mse: 12.7049 - mae: 1.5866 - val_loss: 9.5859 - val_mse: 9.5859 - val_mae: 2.6476 - lr: 7.1335e-04 - 930ms/epoch - 2ms/step\n",
            "Score for inner fold : loss of 9.585945129394531\n",
            "-----Fold:2--------\n",
            "Epoch 1/500\n",
            "556/556 - 2s - loss: 20.4863 - mse: 20.4863 - mae: 2.0806 - val_loss: 4.4736 - val_mse: 4.4736 - val_mae: 1.8563 - lr: 7.1335e-04 - 2s/epoch - 3ms/step\n",
            "Epoch 2/500\n",
            "556/556 - 1s - loss: 15.5181 - mse: 15.5181 - mae: 1.6430 - val_loss: 7.8105 - val_mse: 7.8105 - val_mae: 2.5142 - lr: 7.1335e-04 - 959ms/epoch - 2ms/step\n",
            "Epoch 3/500\n",
            "556/556 - 1s - loss: 14.5297 - mse: 14.5297 - mae: 1.6341 - val_loss: 7.9737 - val_mse: 7.9737 - val_mae: 2.4934 - lr: 7.1335e-04 - 941ms/epoch - 2ms/step\n",
            "Epoch 4/500\n",
            "556/556 - 1s - loss: 14.0065 - mse: 14.0065 - mae: 1.6003 - val_loss: 8.5669 - val_mse: 8.5669 - val_mae: 2.5668 - lr: 7.1335e-04 - 958ms/epoch - 2ms/step\n",
            "Epoch 5/500\n",
            "556/556 - 1s - loss: 13.6173 - mse: 13.6173 - mae: 1.5805 - val_loss: 8.8063 - val_mse: 8.8063 - val_mae: 2.5530 - lr: 7.1335e-04 - 914ms/epoch - 2ms/step\n",
            "Epoch 6/500\n",
            "556/556 - 1s - loss: 13.3039 - mse: 13.3039 - mae: 1.5680 - val_loss: 8.9178 - val_mse: 8.9178 - val_mae: 2.5544 - lr: 7.1335e-04 - 911ms/epoch - 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-10-10 10:11:41,308]\u001b[0m Finished trial#19 resulted in value: 9.14. Current best value is 9.14 with parameters: {'activation': 'tanh', 'num_hidden_layer': 2, 'i': 3, 'learning_rate': 0.0007133476189480918}.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Score for inner fold : loss of 8.917820930480957\n",
            "Trial Number:20\n",
            "-----Fold:0--------\n",
            "Epoch 1/500\n",
            "556/556 - 2s - loss: 14.1128 - mse: 14.1128 - mae: 1.8676 - val_loss: 11.0692 - val_mse: 11.0692 - val_mae: 2.6531 - lr: 5.8944e-04 - 2s/epoch - 3ms/step\n",
            "Epoch 2/500\n",
            "556/556 - 1s - loss: 11.1360 - mse: 11.1360 - mae: 1.5815 - val_loss: 12.4333 - val_mse: 12.4333 - val_mae: 2.7118 - lr: 5.8944e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 3/500\n",
            "556/556 - 1s - loss: 11.2805 - mse: 11.2805 - mae: 1.5791 - val_loss: 12.1769 - val_mse: 12.1769 - val_mae: 2.6517 - lr: 5.8944e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 4/500\n",
            "556/556 - 1s - loss: 11.3300 - mse: 11.3300 - mae: 1.5771 - val_loss: 13.2073 - val_mse: 13.2073 - val_mae: 2.7092 - lr: 5.8944e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 5/500\n",
            "556/556 - 1s - loss: 11.2760 - mse: 11.2760 - mae: 1.5746 - val_loss: 12.6748 - val_mse: 12.6748 - val_mae: 2.6780 - lr: 5.8944e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 6/500\n",
            "556/556 - 1s - loss: 11.2993 - mse: 11.2993 - mae: 1.5816 - val_loss: 13.1108 - val_mse: 13.1108 - val_mae: 2.6752 - lr: 5.8944e-04 - 1s/epoch - 2ms/step\n",
            "Score for inner fold : loss of 13.110782623291016\n",
            "-----Fold:1--------\n",
            "Epoch 1/500\n",
            "556/556 - 2s - loss: 16.2419 - mse: 16.2419 - mae: 1.8399 - val_loss: 11.0883 - val_mse: 11.0883 - val_mae: 2.6742 - lr: 5.8944e-04 - 2s/epoch - 3ms/step\n",
            "Epoch 2/500\n",
            "556/556 - 1s - loss: 11.4572 - mse: 11.4572 - mae: 1.5935 - val_loss: 13.4654 - val_mse: 13.4654 - val_mae: 2.6930 - lr: 5.8944e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 3/500\n",
            "556/556 - 1s - loss: 11.1974 - mse: 11.1974 - mae: 1.5859 - val_loss: 13.8025 - val_mse: 13.8025 - val_mae: 2.6342 - lr: 5.8944e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 4/500\n",
            "556/556 - 1s - loss: 11.1290 - mse: 11.1290 - mae: 1.5723 - val_loss: 15.1660 - val_mse: 15.1660 - val_mae: 2.7412 - lr: 5.8944e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 5/500\n",
            "556/556 - 1s - loss: 11.0946 - mse: 11.0946 - mae: 1.5767 - val_loss: 16.1457 - val_mse: 16.1457 - val_mae: 2.7393 - lr: 5.8944e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 6/500\n",
            "556/556 - 1s - loss: 11.1730 - mse: 11.1730 - mae: 1.5753 - val_loss: 15.3296 - val_mse: 15.3296 - val_mae: 2.6937 - lr: 5.8944e-04 - 1s/epoch - 2ms/step\n",
            "Score for inner fold : loss of 15.329578399658203\n",
            "-----Fold:2--------\n",
            "Epoch 1/500\n",
            "556/556 - 2s - loss: 15.5496 - mse: 15.5496 - mae: 1.8378 - val_loss: 10.7149 - val_mse: 10.7149 - val_mae: 2.6514 - lr: 5.8944e-04 - 2s/epoch - 3ms/step\n",
            "Epoch 2/500\n",
            "556/556 - 1s - loss: 12.6775 - mse: 12.6775 - mae: 1.5971 - val_loss: 11.2226 - val_mse: 11.2226 - val_mae: 2.6370 - lr: 5.8944e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 3/500\n",
            "556/556 - 1s - loss: 12.6258 - mse: 12.6258 - mae: 1.5944 - val_loss: 11.3019 - val_mse: 11.3019 - val_mae: 2.6037 - lr: 5.8944e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 4/500\n",
            "556/556 - 1s - loss: 12.7047 - mse: 12.7047 - mae: 1.5901 - val_loss: 12.5481 - val_mse: 12.5481 - val_mae: 2.7109 - lr: 5.8944e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 5/500\n",
            "556/556 - 1s - loss: 12.7859 - mse: 12.7859 - mae: 1.5930 - val_loss: 11.5314 - val_mse: 11.5314 - val_mae: 2.6534 - lr: 5.8944e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 6/500\n",
            "556/556 - 1s - loss: 12.7489 - mse: 12.7489 - mae: 1.5949 - val_loss: 11.5226 - val_mse: 11.5226 - val_mae: 2.6429 - lr: 5.8944e-04 - 1s/epoch - 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-10-10 10:12:05,915]\u001b[0m Finished trial#20 resulted in value: 13.319999999999999. Current best value is 9.14 with parameters: {'activation': 'tanh', 'num_hidden_layer': 2, 'i': 3, 'learning_rate': 0.0007133476189480918}.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Score for inner fold : loss of 11.522554397583008\n",
            "Trial Number:21\n",
            "-----Fold:0--------\n",
            "Epoch 1/500\n",
            "556/556 - 2s - loss: 16.6493 - mse: 16.6493 - mae: 1.9282 - val_loss: 8.0093 - val_mse: 8.0093 - val_mae: 2.5296 - lr: 0.0011 - 2s/epoch - 3ms/step\n",
            "Epoch 2/500\n",
            "556/556 - 1s - loss: 12.9258 - mse: 12.9258 - mae: 1.6063 - val_loss: 8.8564 - val_mse: 8.8564 - val_mae: 2.6104 - lr: 0.0011 - 946ms/epoch - 2ms/step\n",
            "Epoch 3/500\n",
            "556/556 - 1s - loss: 12.4279 - mse: 12.4279 - mae: 1.5749 - val_loss: 8.9620 - val_mse: 8.9620 - val_mae: 2.5736 - lr: 0.0011 - 950ms/epoch - 2ms/step\n",
            "Epoch 4/500\n",
            "556/556 - 1s - loss: 12.1000 - mse: 12.1000 - mae: 1.5637 - val_loss: 9.2666 - val_mse: 9.2666 - val_mae: 2.6117 - lr: 0.0011 - 979ms/epoch - 2ms/step\n",
            "Epoch 5/500\n",
            "556/556 - 1s - loss: 11.8488 - mse: 11.8488 - mae: 1.5471 - val_loss: 9.4969 - val_mse: 9.4969 - val_mae: 2.6161 - lr: 0.0011 - 948ms/epoch - 2ms/step\n",
            "Epoch 6/500\n",
            "556/556 - 1s - loss: 11.6370 - mse: 11.6370 - mae: 1.5407 - val_loss: 9.7131 - val_mse: 9.7131 - val_mae: 2.6238 - lr: 0.0011 - 945ms/epoch - 2ms/step\n",
            "Score for inner fold : loss of 9.713114738464355\n",
            "-----Fold:1--------\n",
            "Epoch 1/500\n",
            "556/556 - 2s - loss: 18.2693 - mse: 18.2693 - mae: 1.9403 - val_loss: 7.7212 - val_mse: 7.7212 - val_mae: 2.4660 - lr: 0.0011 - 2s/epoch - 3ms/step\n",
            "Epoch 2/500\n",
            "556/556 - 1s - loss: 14.1352 - mse: 14.1352 - mae: 1.6188 - val_loss: 8.7956 - val_mse: 8.7956 - val_mae: 2.5995 - lr: 0.0011 - 944ms/epoch - 2ms/step\n",
            "Epoch 3/500\n",
            "556/556 - 1s - loss: 13.6830 - mse: 13.6830 - mae: 1.5995 - val_loss: 8.9885 - val_mse: 8.9885 - val_mae: 2.5825 - lr: 0.0011 - 937ms/epoch - 2ms/step\n",
            "Epoch 4/500\n",
            "556/556 - 1s - loss: 13.3904 - mse: 13.3904 - mae: 1.5817 - val_loss: 9.2120 - val_mse: 9.2120 - val_mae: 2.5899 - lr: 0.0011 - 939ms/epoch - 2ms/step\n",
            "Epoch 5/500\n",
            "556/556 - 1s - loss: 13.1759 - mse: 13.1759 - mae: 1.5724 - val_loss: 9.3970 - val_mse: 9.3970 - val_mae: 2.6006 - lr: 0.0011 - 959ms/epoch - 2ms/step\n",
            "Epoch 6/500\n",
            "556/556 - 1s - loss: 12.9884 - mse: 12.9884 - mae: 1.5630 - val_loss: 9.8084 - val_mse: 9.8084 - val_mae: 2.6440 - lr: 0.0011 - 958ms/epoch - 2ms/step\n",
            "Score for inner fold : loss of 9.808359146118164\n",
            "-----Fold:2--------\n",
            "Epoch 1/500\n",
            "556/556 - 2s - loss: 19.3661 - mse: 19.3661 - mae: 1.9905 - val_loss: 7.2841 - val_mse: 7.2841 - val_mae: 2.4234 - lr: 0.0011 - 2s/epoch - 3ms/step\n",
            "Epoch 2/500\n",
            "556/556 - 1s - loss: 14.7148 - mse: 14.7148 - mae: 1.6358 - val_loss: 8.4701 - val_mse: 8.4701 - val_mae: 2.5767 - lr: 0.0011 - 943ms/epoch - 2ms/step\n",
            "Epoch 3/500\n",
            "556/556 - 1s - loss: 14.0408 - mse: 14.0408 - mae: 1.6028 - val_loss: 8.6485 - val_mse: 8.6485 - val_mae: 2.5552 - lr: 0.0011 - 929ms/epoch - 2ms/step\n",
            "Epoch 4/500\n",
            "556/556 - 1s - loss: 13.6247 - mse: 13.6247 - mae: 1.5830 - val_loss: 9.0539 - val_mse: 9.0539 - val_mae: 2.5663 - lr: 0.0011 - 953ms/epoch - 2ms/step\n",
            "Epoch 5/500\n",
            "556/556 - 1s - loss: 13.3103 - mse: 13.3103 - mae: 1.5726 - val_loss: 9.5778 - val_mse: 9.5778 - val_mae: 2.6435 - lr: 0.0011 - 959ms/epoch - 2ms/step\n",
            "Epoch 6/500\n",
            "556/556 - 1s - loss: 13.0570 - mse: 13.0570 - mae: 1.5645 - val_loss: 9.5279 - val_mse: 9.5279 - val_mae: 2.5858 - lr: 0.0011 - 945ms/epoch - 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-10-10 10:12:28,725]\u001b[0m Finished trial#21 resulted in value: 9.683333333333335. Current best value is 9.14 with parameters: {'activation': 'tanh', 'num_hidden_layer': 2, 'i': 3, 'learning_rate': 0.0007133476189480918}.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Score for inner fold : loss of 9.527902603149414\n",
            "Trial Number:22\n",
            "-----Fold:0--------\n",
            "Epoch 1/500\n",
            "556/556 - 2s - loss: 16.1916 - mse: 16.1916 - mae: 1.8867 - val_loss: 8.0303 - val_mse: 8.0303 - val_mae: 2.5121 - lr: 8.4382e-04 - 2s/epoch - 3ms/step\n",
            "Epoch 2/500\n",
            "556/556 - 1s - loss: 11.7397 - mse: 11.7397 - mae: 1.6047 - val_loss: 9.0087 - val_mse: 9.0087 - val_mae: 2.5957 - lr: 8.4382e-04 - 962ms/epoch - 2ms/step\n",
            "Epoch 3/500\n",
            "556/556 - 1s - loss: 11.0490 - mse: 11.0490 - mae: 1.5733 - val_loss: 9.2550 - val_mse: 9.2550 - val_mae: 2.5872 - lr: 8.4382e-04 - 929ms/epoch - 2ms/step\n",
            "Epoch 4/500\n",
            "556/556 - 1s - loss: 10.7009 - mse: 10.7009 - mae: 1.5510 - val_loss: 9.3389 - val_mse: 9.3389 - val_mae: 2.5573 - lr: 8.4382e-04 - 944ms/epoch - 2ms/step\n",
            "Epoch 5/500\n",
            "556/556 - 1s - loss: 10.4684 - mse: 10.4684 - mae: 1.5442 - val_loss: 9.7417 - val_mse: 9.7417 - val_mae: 2.5991 - lr: 8.4382e-04 - 984ms/epoch - 2ms/step\n",
            "Epoch 6/500\n",
            "556/556 - 1s - loss: 10.2623 - mse: 10.2623 - mae: 1.5311 - val_loss: 9.9608 - val_mse: 9.9608 - val_mae: 2.6157 - lr: 8.4382e-04 - 936ms/epoch - 2ms/step\n",
            "Score for inner fold : loss of 9.960768699645996\n",
            "-----Fold:1--------\n",
            "Epoch 1/500\n",
            "556/556 - 2s - loss: 18.1162 - mse: 18.1162 - mae: 1.8894 - val_loss: 7.8734 - val_mse: 7.8734 - val_mae: 2.4843 - lr: 8.4382e-04 - 2s/epoch - 3ms/step\n",
            "Epoch 2/500\n",
            "556/556 - 1s - loss: 14.1807 - mse: 14.1807 - mae: 1.6038 - val_loss: 9.2043 - val_mse: 9.2043 - val_mae: 2.6317 - lr: 8.4382e-04 - 955ms/epoch - 2ms/step\n",
            "Epoch 3/500\n",
            "556/556 - 1s - loss: 13.5968 - mse: 13.5968 - mae: 1.5775 - val_loss: 9.4630 - val_mse: 9.4630 - val_mae: 2.6144 - lr: 8.4382e-04 - 938ms/epoch - 2ms/step\n",
            "Epoch 4/500\n",
            "556/556 - 1s - loss: 13.2947 - mse: 13.2947 - mae: 1.5569 - val_loss: 9.4632 - val_mse: 9.4632 - val_mae: 2.5672 - lr: 8.4382e-04 - 980ms/epoch - 2ms/step\n",
            "Epoch 5/500\n",
            "556/556 - 1s - loss: 13.0803 - mse: 13.0803 - mae: 1.5470 - val_loss: 9.7973 - val_mse: 9.7973 - val_mae: 2.6315 - lr: 8.4382e-04 - 994ms/epoch - 2ms/step\n",
            "Epoch 6/500\n",
            "556/556 - 1s - loss: 12.9060 - mse: 12.9060 - mae: 1.5394 - val_loss: 10.1478 - val_mse: 10.1478 - val_mae: 2.6405 - lr: 8.4382e-04 - 952ms/epoch - 2ms/step\n",
            "Score for inner fold : loss of 10.14775562286377\n",
            "-----Fold:2--------\n",
            "Epoch 1/500\n",
            "556/556 - 2s - loss: 19.3197 - mse: 19.3197 - mae: 1.8929 - val_loss: 8.0229 - val_mse: 8.0229 - val_mae: 2.5338 - lr: 8.4382e-04 - 2s/epoch - 3ms/step\n",
            "Epoch 2/500\n",
            "556/556 - 1s - loss: 14.6734 - mse: 14.6734 - mae: 1.6133 - val_loss: 9.2937 - val_mse: 9.2937 - val_mae: 2.6297 - lr: 8.4382e-04 - 955ms/epoch - 2ms/step\n",
            "Epoch 3/500\n",
            "556/556 - 1s - loss: 14.0285 - mse: 14.0285 - mae: 1.5854 - val_loss: 9.2687 - val_mse: 9.2687 - val_mae: 2.5839 - lr: 8.4382e-04 - 957ms/epoch - 2ms/step\n",
            "Epoch 4/500\n",
            "556/556 - 1s - loss: 13.7264 - mse: 13.7264 - mae: 1.5659 - val_loss: 9.6037 - val_mse: 9.6037 - val_mae: 2.5931 - lr: 8.4382e-04 - 952ms/epoch - 2ms/step\n",
            "Epoch 5/500\n",
            "556/556 - 1s - loss: 13.4873 - mse: 13.4873 - mae: 1.5561 - val_loss: 9.4760 - val_mse: 9.4760 - val_mae: 2.5759 - lr: 8.4382e-04 - 930ms/epoch - 2ms/step\n",
            "Epoch 6/500\n",
            "556/556 - 1s - loss: 13.3031 - mse: 13.3031 - mae: 1.5475 - val_loss: 9.8500 - val_mse: 9.8500 - val_mae: 2.6075 - lr: 8.4382e-04 - 955ms/epoch - 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-10-10 10:12:51,723]\u001b[0m Finished trial#22 resulted in value: 9.986666666666666. Current best value is 9.14 with parameters: {'activation': 'tanh', 'num_hidden_layer': 2, 'i': 3, 'learning_rate': 0.0007133476189480918}.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Score for inner fold : loss of 9.850032806396484\n",
            "Trial Number:23\n",
            "-----Fold:0--------\n",
            "Epoch 1/500\n",
            "556/556 - 2s - loss: 15.7117 - mse: 15.7117 - mae: 1.7647 - val_loss: 8.0741 - val_mse: 8.0741 - val_mae: 2.4932 - lr: 0.0024 - 2s/epoch - 3ms/step\n",
            "Epoch 2/500\n",
            "556/556 - 1s - loss: 13.4206 - mse: 13.4206 - mae: 1.5867 - val_loss: 9.2264 - val_mse: 9.2264 - val_mae: 2.5602 - lr: 0.0024 - 999ms/epoch - 2ms/step\n",
            "Epoch 3/500\n",
            "556/556 - 1s - loss: 12.8537 - mse: 12.8537 - mae: 1.5685 - val_loss: 9.6374 - val_mse: 9.6374 - val_mae: 2.6338 - lr: 0.0024 - 1s/epoch - 2ms/step\n",
            "Epoch 4/500\n",
            "556/556 - 1s - loss: 12.5512 - mse: 12.5512 - mae: 1.5642 - val_loss: 12.0000 - val_mse: 12.0000 - val_mae: 2.8956 - lr: 0.0024 - 1s/epoch - 2ms/step\n",
            "Epoch 5/500\n",
            "556/556 - 1s - loss: 12.3087 - mse: 12.3087 - mae: 1.5559 - val_loss: 11.1750 - val_mse: 11.1750 - val_mae: 2.7631 - lr: 0.0024 - 988ms/epoch - 2ms/step\n",
            "Epoch 6/500\n",
            "556/556 - 1s - loss: 12.1366 - mse: 12.1366 - mae: 1.5520 - val_loss: 11.0333 - val_mse: 11.0333 - val_mae: 2.6981 - lr: 0.0024 - 982ms/epoch - 2ms/step\n",
            "Score for inner fold : loss of 11.033303260803223\n",
            "-----Fold:1--------\n",
            "Epoch 1/500\n",
            "556/556 - 2s - loss: 14.9647 - mse: 14.9647 - mae: 1.7391 - val_loss: 8.5321 - val_mse: 8.5321 - val_mae: 2.5478 - lr: 0.0024 - 2s/epoch - 3ms/step\n",
            "Epoch 2/500\n",
            "556/556 - 1s - loss: 12.7337 - mse: 12.7337 - mae: 1.5881 - val_loss: 9.0551 - val_mse: 9.0551 - val_mae: 2.5639 - lr: 0.0024 - 984ms/epoch - 2ms/step\n",
            "Epoch 3/500\n",
            "556/556 - 1s - loss: 12.1012 - mse: 12.1012 - mae: 1.5658 - val_loss: 9.4509 - val_mse: 9.4509 - val_mae: 2.5899 - lr: 0.0024 - 1s/epoch - 2ms/step\n",
            "Epoch 4/500\n",
            "556/556 - 1s - loss: 11.7121 - mse: 11.7121 - mae: 1.5502 - val_loss: 9.8721 - val_mse: 9.8721 - val_mae: 2.5373 - lr: 0.0024 - 997ms/epoch - 2ms/step\n",
            "Epoch 5/500\n",
            "556/556 - 1s - loss: 11.4367 - mse: 11.4367 - mae: 1.5395 - val_loss: 10.1716 - val_mse: 10.1716 - val_mae: 2.6633 - lr: 0.0024 - 1s/epoch - 2ms/step\n",
            "Epoch 6/500\n",
            "556/556 - 1s - loss: 11.2029 - mse: 11.2029 - mae: 1.5337 - val_loss: 9.9342 - val_mse: 9.9342 - val_mae: 2.4993 - lr: 0.0024 - 1000ms/epoch - 2ms/step\n",
            "Score for inner fold : loss of 9.934225082397461\n",
            "-----Fold:2--------\n",
            "Epoch 1/500\n",
            "556/556 - 2s - loss: 15.7985 - mse: 15.7985 - mae: 1.7373 - val_loss: 8.5443 - val_mse: 8.5443 - val_mae: 2.5002 - lr: 0.0024 - 2s/epoch - 3ms/step\n",
            "Epoch 2/500\n",
            "556/556 - 1s - loss: 13.5041 - mse: 13.5041 - mae: 1.5889 - val_loss: 10.3331 - val_mse: 10.3331 - val_mae: 2.7284 - lr: 0.0024 - 1s/epoch - 2ms/step\n",
            "Epoch 3/500\n",
            "556/556 - 1s - loss: 12.9200 - mse: 12.9200 - mae: 1.5670 - val_loss: 11.6683 - val_mse: 11.6683 - val_mae: 2.8683 - lr: 0.0024 - 984ms/epoch - 2ms/step\n",
            "Epoch 4/500\n",
            "556/556 - 1s - loss: 12.5762 - mse: 12.5762 - mae: 1.5625 - val_loss: 10.4386 - val_mse: 10.4386 - val_mae: 2.6753 - lr: 0.0024 - 1s/epoch - 2ms/step\n",
            "Epoch 5/500\n",
            "556/556 - 1s - loss: 12.3087 - mse: 12.3087 - mae: 1.5445 - val_loss: 12.0010 - val_mse: 12.0010 - val_mae: 2.8228 - lr: 0.0024 - 1s/epoch - 2ms/step\n",
            "Epoch 6/500\n",
            "556/556 - 1s - loss: 12.1014 - mse: 12.1014 - mae: 1.5416 - val_loss: 10.5935 - val_mse: 10.5935 - val_mae: 2.6817 - lr: 0.0024 - 1s/epoch - 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-10-10 10:13:15,911]\u001b[0m Finished trial#23 resulted in value: 10.516666666666667. Current best value is 9.14 with parameters: {'activation': 'tanh', 'num_hidden_layer': 2, 'i': 3, 'learning_rate': 0.0007133476189480918}.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Score for inner fold : loss of 10.59345531463623\n",
            "Trial Number:24\n",
            "-----Fold:0--------\n",
            "Epoch 1/500\n",
            "556/556 - 2s - loss: 18.5397 - mse: 18.5397 - mae: 1.9602 - val_loss: 7.8433 - val_mse: 7.8433 - val_mae: 2.5276 - lr: 4.1028e-04 - 2s/epoch - 3ms/step\n",
            "Epoch 2/500\n",
            "556/556 - 1s - loss: 14.8701 - mse: 14.8701 - mae: 1.6807 - val_loss: 7.9625 - val_mse: 7.9625 - val_mae: 2.4876 - lr: 4.1028e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 3/500\n",
            "556/556 - 1s - loss: 14.1803 - mse: 14.1803 - mae: 1.6338 - val_loss: 8.8526 - val_mse: 8.8526 - val_mae: 2.6168 - lr: 4.1028e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 4/500\n",
            "556/556 - 1s - loss: 13.7346 - mse: 13.7346 - mae: 1.6152 - val_loss: 8.7613 - val_mse: 8.7613 - val_mae: 2.5547 - lr: 4.1028e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 5/500\n",
            "556/556 - 1s - loss: 13.4132 - mse: 13.4132 - mae: 1.6011 - val_loss: 9.8758 - val_mse: 9.8758 - val_mae: 2.7085 - lr: 4.1028e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 6/500\n",
            "556/556 - 1s - loss: 13.1542 - mse: 13.1542 - mae: 1.5909 - val_loss: 9.2535 - val_mse: 9.2535 - val_mae: 2.6010 - lr: 4.1028e-04 - 1s/epoch - 2ms/step\n",
            "Score for inner fold : loss of 9.253532409667969\n",
            "-----Fold:1--------\n",
            "Epoch 1/500\n",
            "556/556 - 2s - loss: 18.2611 - mse: 18.2611 - mae: 1.8930 - val_loss: 8.6739 - val_mse: 8.6739 - val_mae: 2.6304 - lr: 4.1028e-04 - 2s/epoch - 3ms/step\n",
            "Epoch 2/500\n",
            "556/556 - 1s - loss: 14.9422 - mse: 14.9422 - mae: 1.6371 - val_loss: 8.0085 - val_mse: 8.0085 - val_mae: 2.4733 - lr: 4.1028e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 3/500\n",
            "556/556 - 1s - loss: 14.4618 - mse: 14.4618 - mae: 1.6047 - val_loss: 8.7031 - val_mse: 8.7031 - val_mae: 2.5521 - lr: 4.1028e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 4/500\n",
            "556/556 - 1s - loss: 14.1575 - mse: 14.1575 - mae: 1.5924 - val_loss: 9.3896 - val_mse: 9.3896 - val_mae: 2.6148 - lr: 4.1028e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 5/500\n",
            "556/556 - 1s - loss: 13.9196 - mse: 13.9196 - mae: 1.5774 - val_loss: 8.7276 - val_mse: 8.7276 - val_mae: 2.4986 - lr: 4.1028e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 6/500\n",
            "556/556 - 1s - loss: 13.7340 - mse: 13.7340 - mae: 1.5719 - val_loss: 9.5285 - val_mse: 9.5285 - val_mae: 2.6219 - lr: 4.1028e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 7/500\n",
            "556/556 - 1s - loss: 13.5675 - mse: 13.5675 - mae: 1.5646 - val_loss: 9.3994 - val_mse: 9.3994 - val_mae: 2.5746 - lr: 4.1028e-04 - 1s/epoch - 2ms/step\n",
            "Score for inner fold : loss of 9.399415016174316\n",
            "-----Fold:2--------\n",
            "Epoch 1/500\n",
            "556/556 - 2s - loss: 16.0242 - mse: 16.0242 - mae: 1.8402 - val_loss: 8.4166 - val_mse: 8.4166 - val_mae: 2.5925 - lr: 4.1028e-04 - 2s/epoch - 3ms/step\n",
            "Epoch 2/500\n",
            "556/556 - 1s - loss: 12.8846 - mse: 12.8846 - mae: 1.6136 - val_loss: 8.8103 - val_mse: 8.8103 - val_mae: 2.5897 - lr: 4.1028e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 3/500\n",
            "556/556 - 1s - loss: 12.4338 - mse: 12.4338 - mae: 1.5810 - val_loss: 9.2182 - val_mse: 9.2182 - val_mae: 2.6292 - lr: 4.1028e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 4/500\n",
            "556/556 - 1s - loss: 12.1326 - mse: 12.1326 - mae: 1.5655 - val_loss: 9.1499 - val_mse: 9.1499 - val_mae: 2.5839 - lr: 4.1028e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 5/500\n",
            "556/556 - 1s - loss: 11.9128 - mse: 11.9128 - mae: 1.5538 - val_loss: 9.3607 - val_mse: 9.3607 - val_mae: 2.5812 - lr: 4.1028e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 6/500\n",
            "556/556 - 1s - loss: 11.7143 - mse: 11.7143 - mae: 1.5399 - val_loss: 9.5558 - val_mse: 9.5558 - val_mae: 2.5776 - lr: 4.1028e-04 - 1s/epoch - 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-10-10 10:13:42,180]\u001b[0m Finished trial#24 resulted in value: 9.403333333333334. Current best value is 9.14 with parameters: {'activation': 'tanh', 'num_hidden_layer': 2, 'i': 3, 'learning_rate': 0.0007133476189480918}.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Score for inner fold : loss of 9.555743217468262\n",
            "Trial Number:25\n",
            "-----Fold:0--------\n",
            "Epoch 1/500\n",
            "556/556 - 2s - loss: 20.9838 - mse: 20.9838 - mae: 2.2137 - val_loss: 2.6893 - val_mse: 2.6893 - val_mae: 1.3571 - lr: 4.0014e-04 - 2s/epoch - 3ms/step\n",
            "Epoch 2/500\n",
            "556/556 - 1s - loss: 15.3032 - mse: 15.3032 - mae: 1.6169 - val_loss: 7.6590 - val_mse: 7.6590 - val_mae: 2.4803 - lr: 4.0014e-04 - 945ms/epoch - 2ms/step\n",
            "Epoch 3/500\n",
            "556/556 - 1s - loss: 13.7090 - mse: 13.7090 - mae: 1.6238 - val_loss: 8.6175 - val_mse: 8.6175 - val_mae: 2.5910 - lr: 4.0014e-04 - 961ms/epoch - 2ms/step\n",
            "Epoch 4/500\n",
            "556/556 - 1s - loss: 13.2905 - mse: 13.2905 - mae: 1.6026 - val_loss: 8.7240 - val_mse: 8.7240 - val_mae: 2.5916 - lr: 4.0014e-04 - 931ms/epoch - 2ms/step\n",
            "Epoch 5/500\n",
            "556/556 - 1s - loss: 13.0048 - mse: 13.0048 - mae: 1.5856 - val_loss: 8.9298 - val_mse: 8.9298 - val_mae: 2.5876 - lr: 4.0014e-04 - 930ms/epoch - 2ms/step\n",
            "Epoch 6/500\n",
            "556/556 - 1s - loss: 12.7826 - mse: 12.7826 - mae: 1.5748 - val_loss: 9.1483 - val_mse: 9.1483 - val_mae: 2.5902 - lr: 4.0014e-04 - 930ms/epoch - 2ms/step\n",
            "Score for inner fold : loss of 9.148322105407715\n",
            "-----Fold:1--------\n",
            "Epoch 1/500\n",
            "556/556 - 2s - loss: 20.6890 - mse: 20.6890 - mae: 2.2009 - val_loss: 3.0310 - val_mse: 3.0310 - val_mae: 1.4336 - lr: 4.0014e-04 - 2s/epoch - 3ms/step\n",
            "Epoch 2/500\n",
            "556/556 - 1s - loss: 15.5090 - mse: 15.5090 - mae: 1.5875 - val_loss: 8.0751 - val_mse: 8.0751 - val_mae: 2.5181 - lr: 4.0014e-04 - 969ms/epoch - 2ms/step\n",
            "Epoch 3/500\n",
            "556/556 - 1s - loss: 14.2509 - mse: 14.2509 - mae: 1.6065 - val_loss: 8.7383 - val_mse: 8.7383 - val_mae: 2.5924 - lr: 4.0014e-04 - 939ms/epoch - 2ms/step\n",
            "Epoch 4/500\n",
            "556/556 - 1s - loss: 13.9018 - mse: 13.9018 - mae: 1.5982 - val_loss: 8.7571 - val_mse: 8.7571 - val_mae: 2.5814 - lr: 4.0014e-04 - 918ms/epoch - 2ms/step\n",
            "Epoch 5/500\n",
            "556/556 - 1s - loss: 13.6842 - mse: 13.6842 - mae: 1.5831 - val_loss: 9.0698 - val_mse: 9.0698 - val_mae: 2.6095 - lr: 4.0014e-04 - 976ms/epoch - 2ms/step\n",
            "Epoch 6/500\n",
            "556/556 - 1s - loss: 13.5121 - mse: 13.5121 - mae: 1.5757 - val_loss: 9.0644 - val_mse: 9.0644 - val_mae: 2.5730 - lr: 4.0014e-04 - 959ms/epoch - 2ms/step\n",
            "Score for inner fold : loss of 9.064397811889648\n",
            "-----Fold:2--------\n",
            "Epoch 1/500\n",
            "556/556 - 2s - loss: 20.5267 - mse: 20.5267 - mae: 2.2373 - val_loss: 3.1028 - val_mse: 3.1028 - val_mae: 1.4516 - lr: 4.0014e-04 - 2s/epoch - 3ms/step\n",
            "Epoch 2/500\n",
            "556/556 - 1s - loss: 14.8220 - mse: 14.8220 - mae: 1.6087 - val_loss: 8.1808 - val_mse: 8.1808 - val_mae: 2.5292 - lr: 4.0014e-04 - 948ms/epoch - 2ms/step\n",
            "Epoch 3/500\n",
            "556/556 - 1s - loss: 13.3815 - mse: 13.3815 - mae: 1.6109 - val_loss: 8.8992 - val_mse: 8.8992 - val_mae: 2.6038 - lr: 4.0014e-04 - 940ms/epoch - 2ms/step\n",
            "Epoch 4/500\n",
            "556/556 - 1s - loss: 12.9149 - mse: 12.9149 - mae: 1.5975 - val_loss: 9.1093 - val_mse: 9.1093 - val_mae: 2.6034 - lr: 4.0014e-04 - 934ms/epoch - 2ms/step\n",
            "Epoch 5/500\n",
            "556/556 - 1s - loss: 12.6425 - mse: 12.6425 - mae: 1.5821 - val_loss: 9.2296 - val_mse: 9.2296 - val_mae: 2.6036 - lr: 4.0014e-04 - 926ms/epoch - 2ms/step\n",
            "Epoch 6/500\n",
            "556/556 - 1s - loss: 12.4425 - mse: 12.4425 - mae: 1.5701 - val_loss: 9.4939 - val_mse: 9.4939 - val_mae: 2.6225 - lr: 4.0014e-04 - 946ms/epoch - 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-10-10 10:14:04,994]\u001b[0m Finished trial#25 resulted in value: 9.233333333333334. Current best value is 9.14 with parameters: {'activation': 'tanh', 'num_hidden_layer': 2, 'i': 3, 'learning_rate': 0.0007133476189480918}.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Score for inner fold : loss of 9.493913650512695\n",
            "Trial Number:26\n",
            "-----Fold:0--------\n",
            "Epoch 1/500\n",
            "556/556 - 2s - loss: 21.5344 - mse: 21.5344 - mae: 2.3055 - val_loss: 2.7632 - val_mse: 2.7632 - val_mae: 1.3425 - lr: 3.7985e-04 - 2s/epoch - 3ms/step\n",
            "Epoch 2/500\n",
            "556/556 - 1s - loss: 16.2721 - mse: 16.2721 - mae: 1.6343 - val_loss: 7.7077 - val_mse: 7.7077 - val_mae: 2.4762 - lr: 3.7985e-04 - 928ms/epoch - 2ms/step\n",
            "Epoch 3/500\n",
            "556/556 - 1s - loss: 14.4856 - mse: 14.4856 - mae: 1.6239 - val_loss: 8.5960 - val_mse: 8.5960 - val_mae: 2.5776 - lr: 3.7985e-04 - 941ms/epoch - 2ms/step\n",
            "Epoch 4/500\n",
            "556/556 - 1s - loss: 13.9657 - mse: 13.9657 - mae: 1.6041 - val_loss: 8.9358 - val_mse: 8.9358 - val_mae: 2.5972 - lr: 3.7985e-04 - 948ms/epoch - 2ms/step\n",
            "Epoch 5/500\n",
            "556/556 - 1s - loss: 13.6787 - mse: 13.6787 - mae: 1.5891 - val_loss: 8.9004 - val_mse: 8.9004 - val_mae: 2.5764 - lr: 3.7985e-04 - 951ms/epoch - 2ms/step\n",
            "Epoch 6/500\n",
            "556/556 - 1s - loss: 13.4840 - mse: 13.4840 - mae: 1.5743 - val_loss: 9.2809 - val_mse: 9.2809 - val_mae: 2.6171 - lr: 3.7985e-04 - 940ms/epoch - 2ms/step\n",
            "Score for inner fold : loss of 9.280915260314941\n",
            "-----Fold:1--------\n",
            "Epoch 1/500\n",
            "556/556 - 2s - loss: 21.5656 - mse: 21.5656 - mae: 2.2808 - val_loss: 2.4908 - val_mse: 2.4908 - val_mae: 1.2652 - lr: 3.7985e-04 - 2s/epoch - 3ms/step\n",
            "Epoch 2/500\n",
            "556/556 - 1s - loss: 16.4577 - mse: 16.4577 - mae: 1.6481 - val_loss: 7.5195 - val_mse: 7.5195 - val_mae: 2.4378 - lr: 3.7985e-04 - 951ms/epoch - 2ms/step\n",
            "Epoch 3/500\n",
            "556/556 - 1s - loss: 14.4543 - mse: 14.4543 - mae: 1.6293 - val_loss: 8.6000 - val_mse: 8.6000 - val_mae: 2.5790 - lr: 3.7985e-04 - 982ms/epoch - 2ms/step\n",
            "Epoch 4/500\n",
            "556/556 - 1s - loss: 13.9195 - mse: 13.9195 - mae: 1.6116 - val_loss: 8.8913 - val_mse: 8.8913 - val_mae: 2.5998 - lr: 3.7985e-04 - 945ms/epoch - 2ms/step\n",
            "Epoch 5/500\n",
            "556/556 - 1s - loss: 13.6170 - mse: 13.6170 - mae: 1.5929 - val_loss: 9.1844 - val_mse: 9.1844 - val_mae: 2.6126 - lr: 3.7985e-04 - 934ms/epoch - 2ms/step\n",
            "Epoch 6/500\n",
            "556/556 - 1s - loss: 13.3639 - mse: 13.3639 - mae: 1.5820 - val_loss: 9.2999 - val_mse: 9.2999 - val_mae: 2.6029 - lr: 3.7985e-04 - 952ms/epoch - 2ms/step\n",
            "Score for inner fold : loss of 9.299863815307617\n",
            "-----Fold:2--------\n",
            "Epoch 1/500\n",
            "556/556 - 2s - loss: 19.5259 - mse: 19.5259 - mae: 2.2144 - val_loss: 2.7263 - val_mse: 2.7263 - val_mae: 1.3354 - lr: 3.7985e-04 - 2s/epoch - 3ms/step\n",
            "Epoch 2/500\n",
            "556/556 - 1s - loss: 14.1952 - mse: 14.1952 - mae: 1.5744 - val_loss: 7.7734 - val_mse: 7.7734 - val_mae: 2.4740 - lr: 3.7985e-04 - 980ms/epoch - 2ms/step\n",
            "Epoch 3/500\n",
            "556/556 - 1s - loss: 12.7558 - mse: 12.7558 - mae: 1.5945 - val_loss: 8.4944 - val_mse: 8.4944 - val_mae: 2.5740 - lr: 3.7985e-04 - 964ms/epoch - 2ms/step\n",
            "Epoch 4/500\n",
            "556/556 - 1s - loss: 12.3278 - mse: 12.3278 - mae: 1.5792 - val_loss: 8.6952 - val_mse: 8.6952 - val_mae: 2.5571 - lr: 3.7985e-04 - 949ms/epoch - 2ms/step\n",
            "Epoch 5/500\n",
            "556/556 - 1s - loss: 12.0847 - mse: 12.0847 - mae: 1.5653 - val_loss: 8.8956 - val_mse: 8.8956 - val_mae: 2.5671 - lr: 3.7985e-04 - 990ms/epoch - 2ms/step\n",
            "Epoch 6/500\n",
            "556/556 - 1s - loss: 11.9092 - mse: 11.9092 - mae: 1.5539 - val_loss: 9.2378 - val_mse: 9.2378 - val_mae: 2.5976 - lr: 3.7985e-04 - 969ms/epoch - 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-10-10 10:14:27,981]\u001b[0m Finished trial#26 resulted in value: 9.273333333333333. Current best value is 9.14 with parameters: {'activation': 'tanh', 'num_hidden_layer': 2, 'i': 3, 'learning_rate': 0.0007133476189480918}.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Score for inner fold : loss of 9.23776626586914\n",
            "Trial Number:27\n",
            "-----Fold:0--------\n",
            "Epoch 1/500\n",
            "556/556 - 2s - loss: 23.9653 - mse: 23.9653 - mae: 2.5290 - val_loss: 1.4204 - val_mse: 1.4204 - val_mae: 0.8477 - lr: 1.0036e-04 - 2s/epoch - 3ms/step\n",
            "Epoch 2/500\n",
            "556/556 - 1s - loss: 21.3058 - mse: 21.3058 - mae: 2.1904 - val_loss: 1.9956 - val_mse: 1.9956 - val_mae: 1.0863 - lr: 1.0036e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 3/500\n",
            "556/556 - 1s - loss: 18.9595 - mse: 18.9595 - mae: 1.8637 - val_loss: 3.2305 - val_mse: 3.2305 - val_mae: 1.4725 - lr: 1.0036e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 4/500\n",
            "556/556 - 1s - loss: 16.9365 - mse: 16.9365 - mae: 1.6263 - val_loss: 5.3152 - val_mse: 5.3152 - val_mae: 1.9802 - lr: 1.0036e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 5/500\n",
            "556/556 - 1s - loss: 15.6234 - mse: 15.6234 - mae: 1.5817 - val_loss: 7.2050 - val_mse: 7.2050 - val_mae: 2.3682 - lr: 1.0036e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 6/500\n",
            "556/556 - 1s - loss: 14.9596 - mse: 14.9596 - mae: 1.6112 - val_loss: 8.0862 - val_mse: 8.0862 - val_mae: 2.5093 - lr: 1.0036e-04 - 1s/epoch - 2ms/step\n",
            "Score for inner fold : loss of 8.086227416992188\n",
            "-----Fold:1--------\n",
            "Epoch 1/500\n",
            "556/556 - 2s - loss: 23.4722 - mse: 23.4722 - mae: 2.4636 - val_loss: 1.2635 - val_mse: 1.2635 - val_mae: 0.8331 - lr: 1.0036e-04 - 2s/epoch - 3ms/step\n",
            "Epoch 2/500\n",
            "556/556 - 1s - loss: 20.3291 - mse: 20.3291 - mae: 2.0900 - val_loss: 2.1202 - val_mse: 2.1202 - val_mae: 1.1565 - lr: 1.0036e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 3/500\n",
            "556/556 - 1s - loss: 17.7804 - mse: 17.7804 - mae: 1.7424 - val_loss: 3.8121 - val_mse: 3.8121 - val_mae: 1.6262 - lr: 1.0036e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 4/500\n",
            "556/556 - 1s - loss: 15.8257 - mse: 15.8257 - mae: 1.5608 - val_loss: 6.1638 - val_mse: 6.1638 - val_mae: 2.1555 - lr: 1.0036e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 5/500\n",
            "556/556 - 1s - loss: 14.7191 - mse: 14.7191 - mae: 1.5680 - val_loss: 7.9168 - val_mse: 7.9168 - val_mae: 2.4749 - lr: 1.0036e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 6/500\n",
            "556/556 - 1s - loss: 14.2107 - mse: 14.2107 - mae: 1.5990 - val_loss: 8.4574 - val_mse: 8.4574 - val_mae: 2.5459 - lr: 1.0036e-04 - 1s/epoch - 2ms/step\n",
            "Score for inner fold : loss of 8.457406997680664\n",
            "-----Fold:2--------\n",
            "Epoch 1/500\n",
            "556/556 - 2s - loss: 21.5123 - mse: 21.5123 - mae: 2.5585 - val_loss: 1.2568 - val_mse: 1.2568 - val_mae: 0.8172 - lr: 1.0036e-04 - 2s/epoch - 3ms/step\n",
            "Epoch 2/500\n",
            "556/556 - 1s - loss: 18.8978 - mse: 18.8978 - mae: 2.2333 - val_loss: 1.7755 - val_mse: 1.7755 - val_mae: 1.0300 - lr: 1.0036e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 3/500\n",
            "556/556 - 1s - loss: 16.4908 - mse: 16.4908 - mae: 1.8711 - val_loss: 2.9356 - val_mse: 2.9356 - val_mae: 1.3953 - lr: 1.0036e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 4/500\n",
            "556/556 - 1s - loss: 14.3973 - mse: 14.3973 - mae: 1.5991 - val_loss: 5.0013 - val_mse: 5.0013 - val_mae: 1.9137 - lr: 1.0036e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 5/500\n",
            "556/556 - 1s - loss: 13.0053 - mse: 13.0053 - mae: 1.5309 - val_loss: 7.2270 - val_mse: 7.2270 - val_mae: 2.3629 - lr: 1.0036e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 6/500\n",
            "556/556 - 1s - loss: 12.4067 - mse: 12.4067 - mae: 1.5695 - val_loss: 8.3024 - val_mse: 8.3024 - val_mae: 2.5355 - lr: 1.0036e-04 - 1s/epoch - 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-10-10 10:14:52,613]\u001b[0m Finished trial#27 resulted in value: 8.283333333333333. Current best value is 8.283333333333333 with parameters: {'activation': 'tanh', 'num_hidden_layer': 2, 'i': 5, 'learning_rate': 0.00010035947429195473}.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Score for inner fold : loss of 8.302416801452637\n",
            "Trial Number:28\n",
            "-----Fold:0--------\n",
            "Epoch 1/500\n",
            "556/556 - 2s - loss: 22.3115 - mse: 22.3115 - mae: 2.5913 - val_loss: 1.3540 - val_mse: 1.3540 - val_mae: 0.8437 - lr: 1.0717e-04 - 2s/epoch - 3ms/step\n",
            "Epoch 2/500\n",
            "556/556 - 1s - loss: 19.5544 - mse: 19.5544 - mae: 2.2227 - val_loss: 1.9708 - val_mse: 1.9708 - val_mae: 1.0763 - lr: 1.0717e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 3/500\n",
            "556/556 - 1s - loss: 16.8867 - mse: 16.8867 - mae: 1.8378 - val_loss: 3.4570 - val_mse: 3.4570 - val_mae: 1.5128 - lr: 1.0717e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 4/500\n",
            "556/556 - 1s - loss: 14.7454 - mse: 14.7454 - mae: 1.5838 - val_loss: 5.9145 - val_mse: 5.9145 - val_mae: 2.0869 - lr: 1.0717e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 5/500\n",
            "556/556 - 1s - loss: 13.5264 - mse: 13.5264 - mae: 1.5702 - val_loss: 7.8464 - val_mse: 7.8464 - val_mae: 2.4556 - lr: 1.0717e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 6/500\n",
            "556/556 - 1s - loss: 13.0651 - mse: 13.0651 - mae: 1.5989 - val_loss: 8.6547 - val_mse: 8.6547 - val_mae: 2.5783 - lr: 1.0717e-04 - 1s/epoch - 2ms/step\n",
            "Score for inner fold : loss of 8.654667854309082\n",
            "-----Fold:1--------\n",
            "Epoch 1/500\n",
            "556/556 - 2s - loss: 22.5143 - mse: 22.5143 - mae: 2.5292 - val_loss: 1.2309 - val_mse: 1.2309 - val_mae: 0.8076 - lr: 1.0717e-04 - 2s/epoch - 3ms/step\n",
            "Epoch 2/500\n",
            "556/556 - 1s - loss: 19.7059 - mse: 19.7059 - mae: 2.1558 - val_loss: 1.9134 - val_mse: 1.9134 - val_mae: 1.0789 - lr: 1.0717e-04 - 995ms/epoch - 2ms/step\n",
            "Epoch 3/500\n",
            "556/556 - 1s - loss: 17.1520 - mse: 17.1520 - mae: 1.7733 - val_loss: 3.5018 - val_mse: 3.5018 - val_mae: 1.5480 - lr: 1.0717e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 4/500\n",
            "556/556 - 1s - loss: 15.1414 - mse: 15.1414 - mae: 1.5518 - val_loss: 6.0600 - val_mse: 6.0600 - val_mae: 2.1346 - lr: 1.0717e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 5/500\n",
            "556/556 - 1s - loss: 14.0487 - mse: 14.0487 - mae: 1.5597 - val_loss: 7.8535 - val_mse: 7.8535 - val_mae: 2.4698 - lr: 1.0717e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 6/500\n",
            "556/556 - 1s - loss: 13.6447 - mse: 13.6447 - mae: 1.5955 - val_loss: 8.4385 - val_mse: 8.4385 - val_mae: 2.5476 - lr: 1.0717e-04 - 1s/epoch - 2ms/step\n",
            "Score for inner fold : loss of 8.438525199890137\n",
            "-----Fold:2--------\n",
            "Epoch 1/500\n",
            "556/556 - 2s - loss: 22.6285 - mse: 22.6285 - mae: 2.5626 - val_loss: 1.3190 - val_mse: 1.3190 - val_mae: 0.8282 - lr: 1.0717e-04 - 2s/epoch - 3ms/step\n",
            "Epoch 2/500\n",
            "556/556 - 1s - loss: 19.9004 - mse: 19.9004 - mae: 2.1723 - val_loss: 2.0964 - val_mse: 2.0964 - val_mae: 1.1183 - lr: 1.0717e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 3/500\n",
            "556/556 - 1s - loss: 17.4493 - mse: 17.4493 - mae: 1.7819 - val_loss: 3.7462 - val_mse: 3.7462 - val_mae: 1.5897 - lr: 1.0717e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 4/500\n",
            "556/556 - 1s - loss: 15.4483 - mse: 15.4483 - mae: 1.5545 - val_loss: 6.3243 - val_mse: 6.3243 - val_mae: 2.1703 - lr: 1.0717e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 5/500\n",
            "556/556 - 1s - loss: 14.4340 - mse: 14.4340 - mae: 1.5600 - val_loss: 8.2254 - val_mse: 8.2254 - val_mae: 2.5035 - lr: 1.0717e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 6/500\n",
            "556/556 - 1s - loss: 14.0506 - mse: 14.0506 - mae: 1.5949 - val_loss: 8.8193 - val_mse: 8.8193 - val_mae: 2.5905 - lr: 1.0717e-04 - 1s/epoch - 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-10-10 10:15:17,264]\u001b[0m Finished trial#28 resulted in value: 8.636666666666667. Current best value is 8.283333333333333 with parameters: {'activation': 'tanh', 'num_hidden_layer': 2, 'i': 5, 'learning_rate': 0.00010035947429195473}.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Score for inner fold : loss of 8.819341659545898\n",
            "Trial Number:29\n",
            "-----Fold:0--------\n",
            "Epoch 1/500\n",
            "556/556 - 2s - loss: 21.3969 - mse: 21.3969 - mae: 2.5312 - val_loss: 1.5263 - val_mse: 1.5263 - val_mae: 0.8975 - lr: 1.2188e-04 - 2s/epoch - 3ms/step\n",
            "Epoch 2/500\n",
            "556/556 - 1s - loss: 18.3875 - mse: 18.3875 - mae: 2.1040 - val_loss: 2.4722 - val_mse: 2.4722 - val_mae: 1.2318 - lr: 1.2188e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 3/500\n",
            "556/556 - 1s - loss: 15.6962 - mse: 15.6962 - mae: 1.7010 - val_loss: 4.5491 - val_mse: 4.5491 - val_mae: 1.7908 - lr: 1.2188e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 4/500\n",
            "556/556 - 1s - loss: 13.7567 - mse: 13.7567 - mae: 1.5421 - val_loss: 7.3933 - val_mse: 7.3933 - val_mae: 2.3762 - lr: 1.2188e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 5/500\n",
            "556/556 - 1s - loss: 12.9469 - mse: 12.9469 - mae: 1.5791 - val_loss: 8.6154 - val_mse: 8.6154 - val_mae: 2.5673 - lr: 1.2188e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 6/500\n",
            "556/556 - 1s - loss: 12.6062 - mse: 12.6062 - mae: 1.5905 - val_loss: 9.0853 - val_mse: 9.0853 - val_mae: 2.6176 - lr: 1.2188e-04 - 981ms/epoch - 2ms/step\n",
            "Score for inner fold : loss of 9.085268020629883\n",
            "-----Fold:1--------\n",
            "Epoch 1/500\n",
            "556/556 - 2s - loss: 21.7569 - mse: 21.7569 - mae: 2.5678 - val_loss: 1.3413 - val_mse: 1.3413 - val_mae: 0.8441 - lr: 1.2188e-04 - 2s/epoch - 3ms/step\n",
            "Epoch 2/500\n",
            "556/556 - 1s - loss: 19.0391 - mse: 19.0391 - mae: 2.1787 - val_loss: 2.1699 - val_mse: 2.1699 - val_mae: 1.1341 - lr: 1.2188e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 3/500\n",
            "556/556 - 1s - loss: 16.3372 - mse: 16.3372 - mae: 1.7682 - val_loss: 4.0694 - val_mse: 4.0694 - val_mae: 1.6578 - lr: 1.2188e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 4/500\n",
            "556/556 - 1s - loss: 14.2165 - mse: 14.2165 - mae: 1.5380 - val_loss: 7.0559 - val_mse: 7.0559 - val_mae: 2.2943 - lr: 1.2188e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 5/500\n",
            "556/556 - 1s - loss: 13.2774 - mse: 13.2774 - mae: 1.5743 - val_loss: 8.5109 - val_mse: 8.5109 - val_mae: 2.5522 - lr: 1.2188e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 6/500\n",
            "556/556 - 1s - loss: 12.9643 - mse: 12.9643 - mae: 1.5952 - val_loss: 8.9105 - val_mse: 8.9105 - val_mae: 2.5910 - lr: 1.2188e-04 - 1s/epoch - 2ms/step\n",
            "Score for inner fold : loss of 8.91054630279541\n",
            "-----Fold:2--------\n",
            "Epoch 1/500\n",
            "556/556 - 2s - loss: 23.6606 - mse: 23.6606 - mae: 2.5306 - val_loss: 1.2171 - val_mse: 1.2171 - val_mae: 0.8103 - lr: 1.2188e-04 - 2s/epoch - 3ms/step\n",
            "Epoch 2/500\n",
            "556/556 - 1s - loss: 20.5377 - mse: 20.5377 - mae: 2.1172 - val_loss: 2.0693 - val_mse: 2.0693 - val_mae: 1.1429 - lr: 1.2188e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 3/500\n",
            "556/556 - 1s - loss: 17.6032 - mse: 17.6032 - mae: 1.7114 - val_loss: 4.1519 - val_mse: 4.1519 - val_mae: 1.7220 - lr: 1.2188e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 4/500\n",
            "556/556 - 1s - loss: 15.4225 - mse: 15.4225 - mae: 1.5419 - val_loss: 7.1567 - val_mse: 7.1567 - val_mae: 2.3528 - lr: 1.2188e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 5/500\n",
            "556/556 - 1s - loss: 14.5139 - mse: 14.5139 - mae: 1.5841 - val_loss: 8.4424 - val_mse: 8.4424 - val_mae: 2.5542 - lr: 1.2188e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 6/500\n",
            "556/556 - 1s - loss: 14.1514 - mse: 14.1514 - mae: 1.5944 - val_loss: 8.9525 - val_mse: 8.9525 - val_mae: 2.6110 - lr: 1.2188e-04 - 1s/epoch - 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-10-10 10:15:41,786]\u001b[0m Finished trial#29 resulted in value: 8.983333333333333. Current best value is 8.283333333333333 with parameters: {'activation': 'tanh', 'num_hidden_layer': 2, 'i': 5, 'learning_rate': 0.00010035947429195473}.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Score for inner fold : loss of 8.952460289001465\n",
            "Trial Number:30\n",
            "-----Fold:0--------\n",
            "Epoch 1/500\n",
            "556/556 - 2s - loss: 23.0647 - mse: 23.0647 - mae: 2.5511 - val_loss: 1.4691 - val_mse: 1.4691 - val_mae: 0.8839 - lr: 1.0699e-04 - 2s/epoch - 3ms/step\n",
            "Epoch 2/500\n",
            "556/556 - 1s - loss: 20.4726 - mse: 20.4726 - mae: 2.2023 - val_loss: 2.0573 - val_mse: 2.0573 - val_mae: 1.1168 - lr: 1.0699e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 3/500\n",
            "556/556 - 1s - loss: 18.0165 - mse: 18.0165 - mae: 1.8416 - val_loss: 3.4620 - val_mse: 3.4620 - val_mae: 1.5292 - lr: 1.0699e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 4/500\n",
            "556/556 - 1s - loss: 15.9298 - mse: 15.9298 - mae: 1.5888 - val_loss: 5.8068 - val_mse: 5.8068 - val_mae: 2.0780 - lr: 1.0699e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 5/500\n",
            "556/556 - 1s - loss: 14.6456 - mse: 14.6456 - mae: 1.5671 - val_loss: 7.7423 - val_mse: 7.7423 - val_mae: 2.4453 - lr: 1.0699e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 6/500\n",
            "556/556 - 1s - loss: 14.1150 - mse: 14.1150 - mae: 1.5961 - val_loss: 8.5082 - val_mse: 8.5082 - val_mae: 2.5628 - lr: 1.0699e-04 - 1s/epoch - 2ms/step\n",
            "Score for inner fold : loss of 8.508238792419434\n",
            "-----Fold:1--------\n",
            "Epoch 1/500\n",
            "556/556 - 2s - loss: 22.2462 - mse: 22.2462 - mae: 2.5493 - val_loss: 1.4376 - val_mse: 1.4376 - val_mae: 0.8780 - lr: 1.0699e-04 - 2s/epoch - 3ms/step\n",
            "Epoch 2/500\n",
            "556/556 - 1s - loss: 19.4538 - mse: 19.4538 - mae: 2.1343 - val_loss: 2.1579 - val_mse: 2.1579 - val_mae: 1.1506 - lr: 1.0699e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 3/500\n",
            "556/556 - 1s - loss: 17.0713 - mse: 17.0713 - mae: 1.7619 - val_loss: 3.8011 - val_mse: 3.8011 - val_mae: 1.6193 - lr: 1.0699e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 4/500\n",
            "556/556 - 1s - loss: 15.1467 - mse: 15.1467 - mae: 1.5625 - val_loss: 6.2415 - val_mse: 6.2415 - val_mae: 2.1742 - lr: 1.0699e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 5/500\n",
            "556/556 - 1s - loss: 14.1166 - mse: 14.1166 - mae: 1.5737 - val_loss: 7.8948 - val_mse: 7.8948 - val_mae: 2.4818 - lr: 1.0699e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 6/500\n",
            "556/556 - 1s - loss: 13.7089 - mse: 13.7089 - mae: 1.5984 - val_loss: 8.4464 - val_mse: 8.4464 - val_mae: 2.5551 - lr: 1.0699e-04 - 1s/epoch - 2ms/step\n",
            "Score for inner fold : loss of 8.446359634399414\n",
            "-----Fold:2--------\n",
            "Epoch 1/500\n",
            "556/556 - 2s - loss: 22.0656 - mse: 22.0656 - mae: 2.5564 - val_loss: 1.3860 - val_mse: 1.3860 - val_mae: 0.8619 - lr: 1.0699e-04 - 2s/epoch - 3ms/step\n",
            "Epoch 2/500\n",
            "556/556 - 1s - loss: 19.2973 - mse: 19.2973 - mae: 2.1925 - val_loss: 2.0392 - val_mse: 2.0392 - val_mae: 1.1002 - lr: 1.0699e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 3/500\n",
            "556/556 - 1s - loss: 16.8904 - mse: 16.8904 - mae: 1.8222 - val_loss: 3.4429 - val_mse: 3.4429 - val_mae: 1.5182 - lr: 1.0699e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 4/500\n",
            "556/556 - 1s - loss: 14.9013 - mse: 14.9013 - mae: 1.5739 - val_loss: 5.7479 - val_mse: 5.7479 - val_mae: 2.0613 - lr: 1.0699e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 5/500\n",
            "556/556 - 1s - loss: 13.7024 - mse: 13.7024 - mae: 1.5530 - val_loss: 7.7351 - val_mse: 7.7351 - val_mae: 2.4466 - lr: 1.0699e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 6/500\n",
            "556/556 - 1s - loss: 13.1937 - mse: 13.1937 - mae: 1.5874 - val_loss: 8.5737 - val_mse: 8.5737 - val_mae: 2.5744 - lr: 1.0699e-04 - 1s/epoch - 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-10-10 10:16:06,430]\u001b[0m Finished trial#30 resulted in value: 8.51. Current best value is 8.283333333333333 with parameters: {'activation': 'tanh', 'num_hidden_layer': 2, 'i': 5, 'learning_rate': 0.00010035947429195473}.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Score for inner fold : loss of 8.573684692382812\n",
            "Trial Number:31\n",
            "-----Fold:0--------\n",
            "Epoch 1/500\n",
            "556/556 - 2s - loss: 22.4400 - mse: 22.4400 - mae: 2.5894 - val_loss: 1.3069 - val_mse: 1.3069 - val_mae: 0.8211 - lr: 1.0523e-04 - 2s/epoch - 3ms/step\n",
            "Epoch 2/500\n",
            "556/556 - 1s - loss: 19.9561 - mse: 19.9561 - mae: 2.2302 - val_loss: 1.9625 - val_mse: 1.9625 - val_mae: 1.0683 - lr: 1.0523e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 3/500\n",
            "556/556 - 1s - loss: 17.6677 - mse: 17.6677 - mae: 1.8454 - val_loss: 3.3280 - val_mse: 3.3280 - val_mae: 1.4801 - lr: 1.0523e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 4/500\n",
            "556/556 - 1s - loss: 15.7664 - mse: 15.7664 - mae: 1.5797 - val_loss: 5.5791 - val_mse: 5.5791 - val_mae: 2.0291 - lr: 1.0523e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 5/500\n",
            "556/556 - 1s - loss: 14.6022 - mse: 14.6022 - mae: 1.5468 - val_loss: 7.6384 - val_mse: 7.6384 - val_mae: 2.4260 - lr: 1.0523e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 6/500\n",
            "556/556 - 1s - loss: 14.1175 - mse: 14.1175 - mae: 1.5892 - val_loss: 8.5107 - val_mse: 8.5107 - val_mae: 2.5632 - lr: 1.0523e-04 - 1s/epoch - 2ms/step\n",
            "Score for inner fold : loss of 8.510663032531738\n",
            "-----Fold:1--------\n",
            "Epoch 1/500\n",
            "556/556 - 2s - loss: 23.7425 - mse: 23.7425 - mae: 2.5857 - val_loss: 1.2209 - val_mse: 1.2209 - val_mae: 0.7766 - lr: 1.0523e-04 - 2s/epoch - 3ms/step\n",
            "Epoch 2/500\n",
            "556/556 - 1s - loss: 21.3074 - mse: 21.3074 - mae: 2.2632 - val_loss: 1.7807 - val_mse: 1.7807 - val_mae: 1.0079 - lr: 1.0523e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 3/500\n",
            "556/556 - 1s - loss: 18.9279 - mse: 18.9279 - mae: 1.8990 - val_loss: 3.0231 - val_mse: 3.0231 - val_mae: 1.4041 - lr: 1.0523e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 4/500\n",
            "556/556 - 1s - loss: 16.8057 - mse: 16.8057 - mae: 1.6169 - val_loss: 5.1658 - val_mse: 5.1658 - val_mae: 1.9411 - lr: 1.0523e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 5/500\n",
            "556/556 - 1s - loss: 15.3912 - mse: 15.3912 - mae: 1.5661 - val_loss: 7.3167 - val_mse: 7.3167 - val_mae: 2.3776 - lr: 1.0523e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 6/500\n",
            "556/556 - 1s - loss: 14.7638 - mse: 14.7638 - mae: 1.6065 - val_loss: 8.3296 - val_mse: 8.3296 - val_mae: 2.5368 - lr: 1.0523e-04 - 1s/epoch - 2ms/step\n",
            "Score for inner fold : loss of 8.329587936401367\n",
            "-----Fold:2--------\n",
            "Epoch 1/500\n",
            "556/556 - 2s - loss: 21.3185 - mse: 21.3185 - mae: 2.4970 - val_loss: 1.2602 - val_mse: 1.2602 - val_mae: 0.8180 - lr: 1.0523e-04 - 2s/epoch - 3ms/step\n",
            "Epoch 2/500\n",
            "556/556 - 1s - loss: 18.5593 - mse: 18.5593 - mae: 2.1469 - val_loss: 2.0415 - val_mse: 2.0415 - val_mae: 1.1203 - lr: 1.0523e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 3/500\n",
            "556/556 - 1s - loss: 16.0383 - mse: 16.0383 - mae: 1.7905 - val_loss: 3.5210 - val_mse: 3.5210 - val_mae: 1.5514 - lr: 1.0523e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 4/500\n",
            "556/556 - 1s - loss: 14.0061 - mse: 14.0061 - mae: 1.5639 - val_loss: 5.8986 - val_mse: 5.8986 - val_mae: 2.1012 - lr: 1.0523e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 5/500\n",
            "556/556 - 1s - loss: 12.8419 - mse: 12.8419 - mae: 1.5503 - val_loss: 7.7122 - val_mse: 7.7122 - val_mae: 2.4496 - lr: 1.0523e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 6/500\n",
            "556/556 - 1s - loss: 12.3645 - mse: 12.3645 - mae: 1.5805 - val_loss: 8.4304 - val_mse: 8.4304 - val_mae: 2.5503 - lr: 1.0523e-04 - 1s/epoch - 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-10-10 10:16:31,398]\u001b[0m Finished trial#31 resulted in value: 8.423333333333334. Current best value is 8.283333333333333 with parameters: {'activation': 'tanh', 'num_hidden_layer': 2, 'i': 5, 'learning_rate': 0.00010035947429195473}.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Score for inner fold : loss of 8.430356979370117\n",
            "Trial Number:32\n",
            "-----Fold:0--------\n",
            "Epoch 1/500\n",
            "556/556 - 2s - loss: 22.8272 - mse: 22.8272 - mae: 2.5525 - val_loss: 1.2915 - val_mse: 1.2915 - val_mae: 0.8235 - lr: 1.0138e-04 - 2s/epoch - 3ms/step\n",
            "Epoch 2/500\n",
            "556/556 - 1s - loss: 20.2332 - mse: 20.2332 - mae: 2.2121 - val_loss: 1.8331 - val_mse: 1.8331 - val_mae: 1.0441 - lr: 1.0138e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 3/500\n",
            "556/556 - 1s - loss: 17.8060 - mse: 17.8060 - mae: 1.8558 - val_loss: 3.1043 - val_mse: 3.1043 - val_mae: 1.4389 - lr: 1.0138e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 4/500\n",
            "556/556 - 1s - loss: 15.7147 - mse: 15.7147 - mae: 1.5882 - val_loss: 5.3104 - val_mse: 5.3104 - val_mae: 1.9781 - lr: 1.0138e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 5/500\n",
            "556/556 - 1s - loss: 14.3782 - mse: 14.3782 - mae: 1.5393 - val_loss: 7.5089 - val_mse: 7.5089 - val_mae: 2.4075 - lr: 1.0138e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 6/500\n",
            "556/556 - 1s - loss: 13.7962 - mse: 13.7962 - mae: 1.5796 - val_loss: 8.3406 - val_mse: 8.3406 - val_mae: 2.5443 - lr: 1.0138e-04 - 1s/epoch - 2ms/step\n",
            "Score for inner fold : loss of 8.34064769744873\n",
            "-----Fold:1--------\n",
            "Epoch 1/500\n",
            "556/556 - 2s - loss: 21.6499 - mse: 21.6499 - mae: 2.4991 - val_loss: 1.2151 - val_mse: 1.2151 - val_mae: 0.8057 - lr: 1.0138e-04 - 2s/epoch - 3ms/step\n",
            "Epoch 2/500\n",
            "556/556 - 1s - loss: 18.9619 - mse: 18.9619 - mae: 2.1514 - val_loss: 1.9917 - val_mse: 1.9917 - val_mae: 1.1038 - lr: 1.0138e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 3/500\n",
            "556/556 - 1s - loss: 16.6354 - mse: 16.6354 - mae: 1.8061 - val_loss: 3.4610 - val_mse: 3.4610 - val_mae: 1.5366 - lr: 1.0138e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 4/500\n",
            "556/556 - 1s - loss: 14.6964 - mse: 14.6964 - mae: 1.5779 - val_loss: 5.6995 - val_mse: 5.6995 - val_mae: 2.0687 - lr: 1.0138e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 5/500\n",
            "556/556 - 1s - loss: 13.5087 - mse: 13.5087 - mae: 1.5647 - val_loss: 7.5657 - val_mse: 7.5657 - val_mae: 2.4277 - lr: 1.0138e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 6/500\n",
            "556/556 - 1s - loss: 12.9651 - mse: 12.9651 - mae: 1.5935 - val_loss: 8.4252 - val_mse: 8.4252 - val_mae: 2.5549 - lr: 1.0138e-04 - 1s/epoch - 2ms/step\n",
            "Score for inner fold : loss of 8.42523193359375\n",
            "-----Fold:2--------\n",
            "Epoch 1/500\n",
            "556/556 - 2s - loss: 23.4754 - mse: 23.4754 - mae: 2.5766 - val_loss: 1.5663 - val_mse: 1.5663 - val_mae: 0.9052 - lr: 1.0138e-04 - 2s/epoch - 3ms/step\n",
            "Epoch 2/500\n",
            "556/556 - 1s - loss: 21.1724 - mse: 21.1724 - mae: 2.2620 - val_loss: 2.1034 - val_mse: 2.1034 - val_mae: 1.1102 - lr: 1.0138e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 3/500\n",
            "556/556 - 1s - loss: 18.8607 - mse: 18.8607 - mae: 1.9147 - val_loss: 3.2431 - val_mse: 3.2431 - val_mae: 1.4569 - lr: 1.0138e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 4/500\n",
            "556/556 - 1s - loss: 16.7172 - mse: 16.7172 - mae: 1.6423 - val_loss: 5.3498 - val_mse: 5.3498 - val_mae: 1.9640 - lr: 1.0138e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 5/500\n",
            "556/556 - 1s - loss: 15.2347 - mse: 15.2347 - mae: 1.5721 - val_loss: 7.5087 - val_mse: 7.5087 - val_mae: 2.3960 - lr: 1.0138e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 6/500\n",
            "556/556 - 1s - loss: 14.5706 - mse: 14.5706 - mae: 1.6055 - val_loss: 8.4466 - val_mse: 8.4466 - val_mae: 2.5473 - lr: 1.0138e-04 - 1s/epoch - 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-10-10 10:16:55,918]\u001b[0m Finished trial#32 resulted in value: 8.406666666666666. Current best value is 8.283333333333333 with parameters: {'activation': 'tanh', 'num_hidden_layer': 2, 'i': 5, 'learning_rate': 0.00010035947429195473}.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Score for inner fold : loss of 8.446603775024414\n",
            "Trial Number:33\n",
            "-----Fold:0--------\n",
            "Epoch 1/500\n",
            "556/556 - 2s - loss: 21.7961 - mse: 21.7961 - mae: 2.4631 - val_loss: 1.4400 - val_mse: 1.4400 - val_mae: 0.8753 - lr: 1.0098e-04 - 2s/epoch - 3ms/step\n",
            "Epoch 2/500\n",
            "556/556 - 1s - loss: 18.0801 - mse: 18.0801 - mae: 1.9770 - val_loss: 2.8744 - val_mse: 2.8744 - val_mae: 1.3450 - lr: 1.0098e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 3/500\n",
            "556/556 - 1s - loss: 15.0403 - mse: 15.0403 - mae: 1.5774 - val_loss: 5.9920 - val_mse: 5.9920 - val_mae: 2.0885 - lr: 1.0098e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 4/500\n",
            "556/556 - 1s - loss: 13.4025 - mse: 13.4025 - mae: 1.5368 - val_loss: 8.4730 - val_mse: 8.4730 - val_mae: 2.5277 - lr: 1.0098e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 5/500\n",
            "556/556 - 1s - loss: 12.8761 - mse: 12.8761 - mae: 1.5771 - val_loss: 9.0741 - val_mse: 9.0741 - val_mae: 2.6022 - lr: 1.0098e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 6/500\n",
            "556/556 - 1s - loss: 12.6293 - mse: 12.6293 - mae: 1.5714 - val_loss: 9.4477 - val_mse: 9.4477 - val_mae: 2.6240 - lr: 1.0098e-04 - 1s/epoch - 2ms/step\n",
            "Score for inner fold : loss of 9.447662353515625\n",
            "-----Fold:1--------\n",
            "Epoch 1/500\n",
            "556/556 - 2s - loss: 21.8625 - mse: 21.8625 - mae: 2.4832 - val_loss: 1.7081 - val_mse: 1.7081 - val_mae: 0.9722 - lr: 1.0098e-04 - 2s/epoch - 3ms/step\n",
            "Epoch 2/500\n",
            "556/556 - 1s - loss: 18.3994 - mse: 18.3994 - mae: 1.9956 - val_loss: 3.3136 - val_mse: 3.3136 - val_mae: 1.4514 - lr: 1.0098e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 3/500\n",
            "556/556 - 1s - loss: 15.5402 - mse: 15.5402 - mae: 1.6088 - val_loss: 6.5096 - val_mse: 6.5096 - val_mae: 2.1810 - lr: 1.0098e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 4/500\n",
            "556/556 - 1s - loss: 14.1163 - mse: 14.1163 - mae: 1.5750 - val_loss: 8.6675 - val_mse: 8.6675 - val_mae: 2.5549 - lr: 1.0098e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 5/500\n",
            "556/556 - 1s - loss: 13.6654 - mse: 13.6654 - mae: 1.6072 - val_loss: 9.2107 - val_mse: 9.2107 - val_mae: 2.6177 - lr: 1.0098e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 6/500\n",
            "556/556 - 1s - loss: 13.4300 - mse: 13.4300 - mae: 1.6069 - val_loss: 9.2656 - val_mse: 9.2656 - val_mae: 2.6115 - lr: 1.0098e-04 - 1s/epoch - 2ms/step\n",
            "Score for inner fold : loss of 9.265615463256836\n",
            "-----Fold:2--------\n",
            "Epoch 1/500\n",
            "556/556 - 2s - loss: 21.4669 - mse: 21.4669 - mae: 2.4400 - val_loss: 1.5658 - val_mse: 1.5658 - val_mae: 0.9538 - lr: 1.0098e-04 - 2s/epoch - 3ms/step\n",
            "Epoch 2/500\n",
            "556/556 - 1s - loss: 17.5537 - mse: 17.5537 - mae: 1.9054 - val_loss: 3.3417 - val_mse: 3.3417 - val_mae: 1.4882 - lr: 1.0098e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 3/500\n",
            "556/556 - 1s - loss: 14.6124 - mse: 14.6124 - mae: 1.5590 - val_loss: 6.7407 - val_mse: 6.7407 - val_mae: 2.2470 - lr: 1.0098e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 4/500\n",
            "556/556 - 1s - loss: 13.2911 - mse: 13.2911 - mae: 1.5655 - val_loss: 8.4943 - val_mse: 8.4943 - val_mae: 2.5491 - lr: 1.0098e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 5/500\n",
            "556/556 - 1s - loss: 12.8412 - mse: 12.8412 - mae: 1.5880 - val_loss: 8.9403 - val_mse: 8.9403 - val_mae: 2.5897 - lr: 1.0098e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 6/500\n",
            "556/556 - 1s - loss: 12.5713 - mse: 12.5713 - mae: 1.5785 - val_loss: 9.1832 - val_mse: 9.1832 - val_mae: 2.6040 - lr: 1.0098e-04 - 1s/epoch - 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-10-10 10:17:21,885]\u001b[0m Finished trial#33 resulted in value: 9.299999999999999. Current best value is 8.283333333333333 with parameters: {'activation': 'tanh', 'num_hidden_layer': 2, 'i': 5, 'learning_rate': 0.00010035947429195473}.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Score for inner fold : loss of 9.183248519897461\n",
            "Trial Number:34\n",
            "-----Fold:0--------\n",
            "Epoch 1/500\n",
            "556/556 - 2s - loss: 20.0764 - mse: 20.0764 - mae: 2.3177 - val_loss: 2.4880 - val_mse: 2.4880 - val_mae: 1.2273 - lr: 1.7790e-04 - 2s/epoch - 3ms/step\n",
            "Epoch 2/500\n",
            "556/556 - 1s - loss: 14.6078 - mse: 14.6078 - mae: 1.6045 - val_loss: 7.7389 - val_mse: 7.7389 - val_mae: 2.4148 - lr: 1.7790e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 3/500\n",
            "556/556 - 1s - loss: 12.6413 - mse: 12.6413 - mae: 1.5672 - val_loss: 9.4834 - val_mse: 9.4834 - val_mae: 2.6291 - lr: 1.7790e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 4/500\n",
            "556/556 - 1s - loss: 12.2004 - mse: 12.2004 - mae: 1.5678 - val_loss: 9.6912 - val_mse: 9.6912 - val_mae: 2.6340 - lr: 1.7790e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 5/500\n",
            "556/556 - 1s - loss: 11.9736 - mse: 11.9736 - mae: 1.5609 - val_loss: 9.7297 - val_mse: 9.7297 - val_mae: 2.6189 - lr: 1.7790e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 6/500\n",
            "556/556 - 1s - loss: 11.7945 - mse: 11.7945 - mae: 1.5528 - val_loss: 9.9251 - val_mse: 9.9251 - val_mae: 2.6270 - lr: 1.7790e-04 - 1s/epoch - 2ms/step\n",
            "Score for inner fold : loss of 9.925131797790527\n",
            "-----Fold:1--------\n",
            "Epoch 1/500\n",
            "556/556 - 2s - loss: 20.8775 - mse: 20.8775 - mae: 2.3506 - val_loss: 2.2969 - val_mse: 2.2969 - val_mae: 1.1791 - lr: 1.7790e-04 - 2s/epoch - 3ms/step\n",
            "Epoch 2/500\n",
            "556/556 - 1s - loss: 15.5224 - mse: 15.5224 - mae: 1.6427 - val_loss: 7.3190 - val_mse: 7.3190 - val_mae: 2.3438 - lr: 1.7790e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 3/500\n",
            "556/556 - 1s - loss: 13.2955 - mse: 13.2955 - mae: 1.5934 - val_loss: 9.0546 - val_mse: 9.0546 - val_mae: 2.5949 - lr: 1.7790e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 4/500\n",
            "556/556 - 1s - loss: 12.8016 - mse: 12.8016 - mae: 1.5920 - val_loss: 9.4360 - val_mse: 9.4360 - val_mae: 2.6097 - lr: 1.7790e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 5/500\n",
            "556/556 - 1s - loss: 12.5378 - mse: 12.5378 - mae: 1.5842 - val_loss: 9.5569 - val_mse: 9.5569 - val_mae: 2.6081 - lr: 1.7790e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 6/500\n",
            "556/556 - 1s - loss: 12.3555 - mse: 12.3555 - mae: 1.5727 - val_loss: 9.8443 - val_mse: 9.8443 - val_mae: 2.6374 - lr: 1.7790e-04 - 1s/epoch - 2ms/step\n",
            "Score for inner fold : loss of 9.84429931640625\n",
            "-----Fold:2--------\n",
            "Epoch 1/500\n",
            "556/556 - 2s - loss: 19.9967 - mse: 19.9967 - mae: 2.2691 - val_loss: 2.8080 - val_mse: 2.8080 - val_mae: 1.3228 - lr: 1.7790e-04 - 2s/epoch - 3ms/step\n",
            "Epoch 2/500\n",
            "556/556 - 1s - loss: 14.8703 - mse: 14.8703 - mae: 1.5762 - val_loss: 8.0363 - val_mse: 8.0363 - val_mae: 2.4669 - lr: 1.7790e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 3/500\n",
            "556/556 - 1s - loss: 13.4059 - mse: 13.4059 - mae: 1.5789 - val_loss: 9.3373 - val_mse: 9.3373 - val_mae: 2.6162 - lr: 1.7790e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 4/500\n",
            "556/556 - 1s - loss: 13.0152 - mse: 13.0152 - mae: 1.5710 - val_loss: 9.4052 - val_mse: 9.4052 - val_mae: 2.6124 - lr: 1.7790e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 5/500\n",
            "556/556 - 1s - loss: 12.8077 - mse: 12.8077 - mae: 1.5625 - val_loss: 9.5910 - val_mse: 9.5910 - val_mae: 2.6246 - lr: 1.7790e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 6/500\n",
            "556/556 - 1s - loss: 12.6280 - mse: 12.6280 - mae: 1.5527 - val_loss: 9.8762 - val_mse: 9.8762 - val_mae: 2.6449 - lr: 1.7790e-04 - 1s/epoch - 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-10-10 10:17:47,901]\u001b[0m Finished trial#34 resulted in value: 9.883333333333333. Current best value is 8.283333333333333 with parameters: {'activation': 'tanh', 'num_hidden_layer': 2, 'i': 5, 'learning_rate': 0.00010035947429195473}.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Score for inner fold : loss of 9.876178741455078\n",
            "Trial Number:35\n",
            "-----Fold:0--------\n",
            "Epoch 1/500\n",
            "556/556 - 2s - loss: 15.4043 - mse: 15.4043 - mae: 1.7215 - val_loss: 8.5079 - val_mse: 8.5079 - val_mae: 2.3277 - lr: 1.3761e-04 - 2s/epoch - 3ms/step\n",
            "Epoch 2/500\n",
            "556/556 - 1s - loss: 13.0293 - mse: 13.0293 - mae: 1.6195 - val_loss: 10.0944 - val_mse: 10.0944 - val_mae: 2.5028 - lr: 1.3761e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 3/500\n",
            "556/556 - 1s - loss: 12.5256 - mse: 12.5256 - mae: 1.6026 - val_loss: 11.1377 - val_mse: 11.1377 - val_mae: 2.5673 - lr: 1.3761e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 4/500\n",
            "556/556 - 1s - loss: 12.2357 - mse: 12.2357 - mae: 1.5854 - val_loss: 11.8367 - val_mse: 11.8367 - val_mae: 2.6184 - lr: 1.3761e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 5/500\n",
            "556/556 - 1s - loss: 12.0828 - mse: 12.0828 - mae: 1.5791 - val_loss: 12.1064 - val_mse: 12.1064 - val_mae: 2.6049 - lr: 1.3761e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 6/500\n",
            "556/556 - 1s - loss: 11.9721 - mse: 11.9721 - mae: 1.5645 - val_loss: 12.3342 - val_mse: 12.3342 - val_mae: 2.6155 - lr: 1.3761e-04 - 1s/epoch - 2ms/step\n",
            "Score for inner fold : loss of 12.334228515625\n",
            "-----Fold:1--------\n",
            "Epoch 1/500\n",
            "556/556 - 2s - loss: 17.8326 - mse: 17.8326 - mae: 1.9109 - val_loss: 6.2961 - val_mse: 6.2961 - val_mae: 2.0721 - lr: 1.3761e-04 - 2s/epoch - 3ms/step\n",
            "Epoch 2/500\n",
            "556/556 - 1s - loss: 13.2442 - mse: 13.2442 - mae: 1.6244 - val_loss: 8.5485 - val_mse: 8.5485 - val_mae: 2.4445 - lr: 1.3761e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 3/500\n",
            "556/556 - 1s - loss: 12.4214 - mse: 12.4214 - mae: 1.6065 - val_loss: 9.3171 - val_mse: 9.3171 - val_mae: 2.5286 - lr: 1.3761e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 4/500\n",
            "556/556 - 1s - loss: 11.9827 - mse: 11.9827 - mae: 1.5935 - val_loss: 9.5639 - val_mse: 9.5639 - val_mae: 2.5465 - lr: 1.3761e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 5/500\n",
            "556/556 - 1s - loss: 11.7520 - mse: 11.7520 - mae: 1.5823 - val_loss: 9.9936 - val_mse: 9.9936 - val_mae: 2.5841 - lr: 1.3761e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 6/500\n",
            "556/556 - 1s - loss: 11.5539 - mse: 11.5539 - mae: 1.5726 - val_loss: 10.1895 - val_mse: 10.1895 - val_mae: 2.5832 - lr: 1.3761e-04 - 1s/epoch - 2ms/step\n",
            "Score for inner fold : loss of 10.189489364624023\n",
            "-----Fold:2--------\n",
            "Epoch 1/500\n",
            "556/556 - 2s - loss: 15.4001 - mse: 15.4001 - mae: 1.7164 - val_loss: 8.2491 - val_mse: 8.2491 - val_mae: 2.2894 - lr: 1.3761e-04 - 2s/epoch - 3ms/step\n",
            "Epoch 2/500\n",
            "556/556 - 1s - loss: 12.4072 - mse: 12.4072 - mae: 1.6175 - val_loss: 10.4943 - val_mse: 10.4943 - val_mae: 2.5469 - lr: 1.3761e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 3/500\n",
            "556/556 - 1s - loss: 11.8755 - mse: 11.8755 - mae: 1.6083 - val_loss: 11.0346 - val_mse: 11.0346 - val_mae: 2.5589 - lr: 1.3761e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 4/500\n",
            "556/556 - 1s - loss: 11.6194 - mse: 11.6194 - mae: 1.5885 - val_loss: 11.9888 - val_mse: 11.9888 - val_mae: 2.6143 - lr: 1.3761e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 5/500\n",
            "556/556 - 1s - loss: 11.3930 - mse: 11.3930 - mae: 1.5797 - val_loss: 12.5730 - val_mse: 12.5730 - val_mae: 2.6043 - lr: 1.3761e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 6/500\n",
            "556/556 - 1s - loss: 11.2780 - mse: 11.2780 - mae: 1.5701 - val_loss: 13.2551 - val_mse: 13.2551 - val_mae: 2.6374 - lr: 1.3761e-04 - 1s/epoch - 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-10-10 10:18:13,872]\u001b[0m Finished trial#35 resulted in value: 11.926666666666668. Current best value is 8.283333333333333 with parameters: {'activation': 'tanh', 'num_hidden_layer': 2, 'i': 5, 'learning_rate': 0.00010035947429195473}.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Score for inner fold : loss of 13.255101203918457\n",
            "Trial Number:36\n",
            "-----Fold:0--------\n",
            "Epoch 1/500\n",
            "556/556 - 2s - loss: 21.9359 - mse: 21.9359 - mae: 2.2893 - val_loss: 2.2949 - val_mse: 2.2949 - val_mae: 1.2026 - lr: 2.3099e-04 - 2s/epoch - 3ms/step\n",
            "Epoch 2/500\n",
            "556/556 - 1s - loss: 16.6152 - mse: 16.6152 - mae: 1.6377 - val_loss: 6.9118 - val_mse: 6.9118 - val_mae: 2.3060 - lr: 2.3099e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 3/500\n",
            "556/556 - 1s - loss: 14.4920 - mse: 14.4920 - mae: 1.5910 - val_loss: 8.6899 - val_mse: 8.6899 - val_mae: 2.5740 - lr: 2.3099e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 4/500\n",
            "556/556 - 1s - loss: 13.9348 - mse: 13.9348 - mae: 1.5899 - val_loss: 8.9150 - val_mse: 8.9150 - val_mae: 2.5715 - lr: 2.3099e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 5/500\n",
            "556/556 - 1s - loss: 13.6146 - mse: 13.6146 - mae: 1.5739 - val_loss: 9.1277 - val_mse: 9.1277 - val_mae: 2.5746 - lr: 2.3099e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 6/500\n",
            "556/556 - 1s - loss: 13.3744 - mse: 13.3744 - mae: 1.5649 - val_loss: 9.3429 - val_mse: 9.3429 - val_mae: 2.5877 - lr: 2.3099e-04 - 1s/epoch - 2ms/step\n",
            "Score for inner fold : loss of 9.342914581298828\n",
            "-----Fold:1--------\n",
            "Epoch 1/500\n",
            "556/556 - 2s - loss: 21.3094 - mse: 21.3094 - mae: 2.2696 - val_loss: 2.3271 - val_mse: 2.3271 - val_mae: 1.2206 - lr: 2.3099e-04 - 2s/epoch - 3ms/step\n",
            "Epoch 2/500\n",
            "556/556 - 1s - loss: 15.6823 - mse: 15.6823 - mae: 1.6188 - val_loss: 7.2425 - val_mse: 7.2425 - val_mae: 2.3709 - lr: 2.3099e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 3/500\n",
            "556/556 - 1s - loss: 13.5797 - mse: 13.5797 - mae: 1.6095 - val_loss: 8.8235 - val_mse: 8.8235 - val_mae: 2.5987 - lr: 2.3099e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 4/500\n",
            "556/556 - 1s - loss: 13.0266 - mse: 13.0266 - mae: 1.6014 - val_loss: 9.0992 - val_mse: 9.0992 - val_mae: 2.6084 - lr: 2.3099e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 5/500\n",
            "556/556 - 1s - loss: 12.7531 - mse: 12.7531 - mae: 1.5917 - val_loss: 9.1729 - val_mse: 9.1729 - val_mae: 2.5923 - lr: 2.3099e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 6/500\n",
            "556/556 - 1s - loss: 12.5504 - mse: 12.5504 - mae: 1.5834 - val_loss: 9.3162 - val_mse: 9.3162 - val_mae: 2.5825 - lr: 2.3099e-04 - 1s/epoch - 2ms/step\n",
            "Score for inner fold : loss of 9.316229820251465\n",
            "-----Fold:2--------\n",
            "Epoch 1/500\n",
            "556/556 - 2s - loss: 20.0152 - mse: 20.0152 - mae: 2.3094 - val_loss: 2.3851 - val_mse: 2.3851 - val_mae: 1.2229 - lr: 2.3099e-04 - 2s/epoch - 3ms/step\n",
            "Epoch 2/500\n",
            "556/556 - 1s - loss: 14.7722 - mse: 14.7722 - mae: 1.6257 - val_loss: 7.1052 - val_mse: 7.1052 - val_mae: 2.3357 - lr: 2.3099e-04 - 989ms/epoch - 2ms/step\n",
            "Epoch 3/500\n",
            "556/556 - 1s - loss: 12.8053 - mse: 12.8053 - mae: 1.5877 - val_loss: 8.6674 - val_mse: 8.6674 - val_mae: 2.5784 - lr: 2.3099e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 4/500\n",
            "556/556 - 1s - loss: 12.3857 - mse: 12.3857 - mae: 1.5829 - val_loss: 9.1409 - val_mse: 9.1409 - val_mae: 2.6099 - lr: 2.3099e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 5/500\n",
            "556/556 - 1s - loss: 12.1244 - mse: 12.1244 - mae: 1.5791 - val_loss: 9.1805 - val_mse: 9.1805 - val_mae: 2.5947 - lr: 2.3099e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 6/500\n",
            "556/556 - 1s - loss: 11.9558 - mse: 11.9558 - mae: 1.5659 - val_loss: 9.4292 - val_mse: 9.4292 - val_mae: 2.6062 - lr: 2.3099e-04 - 1s/epoch - 2ms/step\n",
            "Score for inner fold : loss of 9.42916488647461\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-10-10 10:18:38,552]\u001b[0m Finished trial#36 resulted in value: 9.363333333333333. Current best value is 8.283333333333333 with parameters: {'activation': 'tanh', 'num_hidden_layer': 2, 'i': 5, 'learning_rate': 0.00010035947429195473}.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trial Number:37\n",
            "-----Fold:0--------\n",
            "Epoch 1/500\n",
            "556/556 - 2s - loss: 20.8739 - mse: 20.8739 - mae: 2.2526 - val_loss: 2.4799 - val_mse: 2.4799 - val_mae: 1.2209 - lr: 1.0344e-04 - 2s/epoch - 3ms/step\n",
            "Epoch 2/500\n",
            "556/556 - 1s - loss: 15.4531 - mse: 15.4531 - mae: 1.6493 - val_loss: 6.2650 - val_mse: 6.2650 - val_mae: 2.0625 - lr: 1.0344e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 3/500\n",
            "556/556 - 1s - loss: 13.2829 - mse: 13.2829 - mae: 1.6172 - val_loss: 8.7831 - val_mse: 8.7831 - val_mae: 2.4361 - lr: 1.0344e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 4/500\n",
            "556/556 - 1s - loss: 12.6236 - mse: 12.6236 - mae: 1.6327 - val_loss: 9.7044 - val_mse: 9.7044 - val_mae: 2.5345 - lr: 1.0344e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 5/500\n",
            "556/556 - 1s - loss: 12.3355 - mse: 12.3355 - mae: 1.6321 - val_loss: 10.1510 - val_mse: 10.1510 - val_mae: 2.5636 - lr: 1.0344e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 6/500\n",
            "556/556 - 1s - loss: 12.1321 - mse: 12.1321 - mae: 1.6199 - val_loss: 10.3242 - val_mse: 10.3242 - val_mae: 2.5734 - lr: 1.0344e-04 - 1s/epoch - 2ms/step\n",
            "Score for inner fold : loss of 10.32417106628418\n",
            "-----Fold:1--------\n",
            "Epoch 1/500\n",
            "556/556 - 2s - loss: 17.1440 - mse: 17.1440 - mae: 1.8062 - val_loss: 4.9264 - val_mse: 4.9264 - val_mae: 1.8377 - lr: 1.0344e-04 - 2s/epoch - 3ms/step\n",
            "Epoch 2/500\n",
            "556/556 - 1s - loss: 13.6092 - mse: 13.6092 - mae: 1.6117 - val_loss: 8.1948 - val_mse: 8.1948 - val_mae: 2.3702 - lr: 1.0344e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 3/500\n",
            "556/556 - 1s - loss: 12.7476 - mse: 12.7476 - mae: 1.6219 - val_loss: 9.4531 - val_mse: 9.4531 - val_mae: 2.5181 - lr: 1.0344e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 4/500\n",
            "556/556 - 1s - loss: 12.3495 - mse: 12.3495 - mae: 1.6150 - val_loss: 9.8976 - val_mse: 9.8976 - val_mae: 2.5581 - lr: 1.0344e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 5/500\n",
            "556/556 - 1s - loss: 12.0863 - mse: 12.0863 - mae: 1.6045 - val_loss: 10.2320 - val_mse: 10.2320 - val_mae: 2.5802 - lr: 1.0344e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 6/500\n",
            "556/556 - 1s - loss: 11.8794 - mse: 11.8794 - mae: 1.5897 - val_loss: 10.6305 - val_mse: 10.6305 - val_mae: 2.5997 - lr: 1.0344e-04 - 1s/epoch - 2ms/step\n",
            "Score for inner fold : loss of 10.630474090576172\n",
            "-----Fold:2--------\n",
            "Epoch 1/500\n",
            "556/556 - 2s - loss: 19.8116 - mse: 19.8116 - mae: 2.0258 - val_loss: 3.6737 - val_mse: 3.6737 - val_mae: 1.5456 - lr: 1.0344e-04 - 2s/epoch - 3ms/step\n",
            "Epoch 2/500\n",
            "556/556 - 1s - loss: 15.4892 - mse: 15.4892 - mae: 1.6527 - val_loss: 7.4033 - val_mse: 7.4033 - val_mae: 2.2548 - lr: 1.0344e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 3/500\n",
            "556/556 - 1s - loss: 14.0156 - mse: 14.0156 - mae: 1.6480 - val_loss: 9.0606 - val_mse: 9.0606 - val_mae: 2.4730 - lr: 1.0344e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 4/500\n",
            "556/556 - 1s - loss: 13.4963 - mse: 13.4963 - mae: 1.6420 - val_loss: 9.6744 - val_mse: 9.6744 - val_mae: 2.5304 - lr: 1.0344e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 5/500\n",
            "556/556 - 1s - loss: 13.1447 - mse: 13.1447 - mae: 1.6323 - val_loss: 10.2375 - val_mse: 10.2375 - val_mae: 2.5720 - lr: 1.0344e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 6/500\n",
            "556/556 - 1s - loss: 12.8802 - mse: 12.8802 - mae: 1.6233 - val_loss: 10.4407 - val_mse: 10.4407 - val_mae: 2.5700 - lr: 1.0344e-04 - 1s/epoch - 2ms/step\n",
            "Score for inner fold : loss of 10.440719604492188\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-10-10 10:19:03,028]\u001b[0m Finished trial#37 resulted in value: 10.463333333333333. Current best value is 8.283333333333333 with parameters: {'activation': 'tanh', 'num_hidden_layer': 2, 'i': 5, 'learning_rate': 0.00010035947429195473}.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trial Number:38\n",
            "-----Fold:0--------\n",
            "Epoch 1/500\n",
            "556/556 - 2s - loss: 17.5271 - mse: 17.5271 - mae: 2.1308 - val_loss: 5.7028 - val_mse: 5.7028 - val_mae: 1.7565 - lr: 1.5903e-04 - 2s/epoch - 3ms/step\n",
            "Epoch 2/500\n",
            "556/556 - 1s - loss: 12.5700 - mse: 12.5700 - mae: 1.5610 - val_loss: 11.3095 - val_mse: 11.3095 - val_mae: 2.6510 - lr: 1.5903e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 3/500\n",
            "556/556 - 1s - loss: 12.2827 - mse: 12.2827 - mae: 1.6037 - val_loss: 12.2089 - val_mse: 12.2089 - val_mae: 2.6742 - lr: 1.5903e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 4/500\n",
            "556/556 - 1s - loss: 12.3641 - mse: 12.3641 - mae: 1.5988 - val_loss: 12.7309 - val_mse: 12.7309 - val_mae: 2.6792 - lr: 1.5903e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 5/500\n",
            "556/556 - 1s - loss: 12.3423 - mse: 12.3423 - mae: 1.5962 - val_loss: 12.2749 - val_mse: 12.2749 - val_mae: 2.6502 - lr: 1.5903e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 6/500\n",
            "556/556 - 1s - loss: 12.3829 - mse: 12.3829 - mae: 1.5913 - val_loss: 13.2907 - val_mse: 13.2907 - val_mae: 2.6773 - lr: 1.5903e-04 - 1s/epoch - 2ms/step\n",
            "Score for inner fold : loss of 13.290726661682129\n",
            "-----Fold:1--------\n",
            "Epoch 1/500\n",
            "556/556 - 2s - loss: 17.5611 - mse: 17.5611 - mae: 2.1145 - val_loss: 4.8405 - val_mse: 4.8405 - val_mae: 1.6656 - lr: 1.5903e-04 - 2s/epoch - 3ms/step\n",
            "Epoch 2/500\n",
            "556/556 - 1s - loss: 11.9955 - mse: 11.9955 - mae: 1.5372 - val_loss: 10.4558 - val_mse: 10.4558 - val_mae: 2.6529 - lr: 1.5903e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 3/500\n",
            "556/556 - 1s - loss: 11.2630 - mse: 11.2630 - mae: 1.5841 - val_loss: 11.2927 - val_mse: 11.2927 - val_mae: 2.6871 - lr: 1.5903e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 4/500\n",
            "556/556 - 1s - loss: 11.3010 - mse: 11.3010 - mae: 1.5775 - val_loss: 11.1686 - val_mse: 11.1686 - val_mae: 2.6447 - lr: 1.5903e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 5/500\n",
            "556/556 - 1s - loss: 11.3345 - mse: 11.3345 - mae: 1.5724 - val_loss: 11.4598 - val_mse: 11.4598 - val_mae: 2.6641 - lr: 1.5903e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 6/500\n",
            "556/556 - 1s - loss: 11.3736 - mse: 11.3736 - mae: 1.5732 - val_loss: 11.8776 - val_mse: 11.8776 - val_mae: 2.6795 - lr: 1.5903e-04 - 1s/epoch - 2ms/step\n",
            "Score for inner fold : loss of 11.877608299255371\n",
            "-----Fold:2--------\n",
            "Epoch 1/500\n",
            "556/556 - 2s - loss: 18.6475 - mse: 18.6475 - mae: 2.1368 - val_loss: 5.7035 - val_mse: 5.7035 - val_mae: 1.7094 - lr: 1.5903e-04 - 2s/epoch - 3ms/step\n",
            "Epoch 2/500\n",
            "556/556 - 1s - loss: 12.3586 - mse: 12.3586 - mae: 1.5534 - val_loss: 12.1529 - val_mse: 12.1529 - val_mae: 2.6716 - lr: 1.5903e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 3/500\n",
            "556/556 - 1s - loss: 11.4961 - mse: 11.4961 - mae: 1.5909 - val_loss: 12.8528 - val_mse: 12.8528 - val_mae: 2.6777 - lr: 1.5903e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 4/500\n",
            "556/556 - 1s - loss: 11.3694 - mse: 11.3694 - mae: 1.5855 - val_loss: 14.3042 - val_mse: 14.3042 - val_mae: 2.7174 - lr: 1.5903e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 5/500\n",
            "556/556 - 1s - loss: 11.3174 - mse: 11.3174 - mae: 1.5814 - val_loss: 13.6201 - val_mse: 13.6201 - val_mae: 2.6333 - lr: 1.5903e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 6/500\n",
            "556/556 - 1s - loss: 11.3131 - mse: 11.3131 - mae: 1.5751 - val_loss: 14.9345 - val_mse: 14.9345 - val_mae: 2.7078 - lr: 1.5903e-04 - 1s/epoch - 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-10-10 10:19:29,636]\u001b[0m Finished trial#38 resulted in value: 13.366666666666667. Current best value is 8.283333333333333 with parameters: {'activation': 'tanh', 'num_hidden_layer': 2, 'i': 5, 'learning_rate': 0.00010035947429195473}.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Score for inner fold : loss of 14.934501647949219\n",
            "Trial Number:39\n",
            "-----Fold:0--------\n",
            "Epoch 1/500\n",
            "556/556 - 2s - loss: 18.4142 - mse: 18.4142 - mae: 1.9743 - val_loss: 7.6417 - val_mse: 7.6417 - val_mae: 2.3560 - lr: 2.1938e-04 - 2s/epoch - 3ms/step\n",
            "Epoch 2/500\n",
            "556/556 - 1s - loss: 13.3194 - mse: 13.3194 - mae: 1.5911 - val_loss: 9.8841 - val_mse: 9.8841 - val_mae: 2.6503 - lr: 2.1938e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 3/500\n",
            "556/556 - 1s - loss: 12.7457 - mse: 12.7457 - mae: 1.5814 - val_loss: 10.0713 - val_mse: 10.0713 - val_mae: 2.6433 - lr: 2.1938e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 4/500\n",
            "556/556 - 1s - loss: 12.4642 - mse: 12.4642 - mae: 1.5665 - val_loss: 10.3645 - val_mse: 10.3645 - val_mae: 2.6559 - lr: 2.1938e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 5/500\n",
            "556/556 - 1s - loss: 12.2994 - mse: 12.2994 - mae: 1.5602 - val_loss: 10.3244 - val_mse: 10.3244 - val_mae: 2.6453 - lr: 2.1938e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 6/500\n",
            "556/556 - 1s - loss: 12.1772 - mse: 12.1772 - mae: 1.5478 - val_loss: 11.0600 - val_mse: 11.0600 - val_mae: 2.7197 - lr: 2.1938e-04 - 1s/epoch - 2ms/step\n",
            "Score for inner fold : loss of 11.060013771057129\n",
            "-----Fold:1--------\n",
            "Epoch 1/500\n",
            "556/556 - 2s - loss: 16.7784 - mse: 16.7784 - mae: 1.9958 - val_loss: 7.3224 - val_mse: 7.3224 - val_mae: 2.2533 - lr: 2.1938e-04 - 2s/epoch - 3ms/step\n",
            "Epoch 2/500\n",
            "556/556 - 1s - loss: 12.1051 - mse: 12.1051 - mae: 1.5569 - val_loss: 9.8898 - val_mse: 9.8898 - val_mae: 2.6404 - lr: 2.1938e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 3/500\n",
            "556/556 - 1s - loss: 11.7257 - mse: 11.7257 - mae: 1.5590 - val_loss: 9.8007 - val_mse: 9.8007 - val_mae: 2.6030 - lr: 2.1938e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 4/500\n",
            "556/556 - 1s - loss: 11.5131 - mse: 11.5131 - mae: 1.5435 - val_loss: 10.4731 - val_mse: 10.4731 - val_mae: 2.6708 - lr: 2.1938e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 5/500\n",
            "556/556 - 1s - loss: 11.4018 - mse: 11.4018 - mae: 1.5392 - val_loss: 10.1378 - val_mse: 10.1378 - val_mae: 2.6116 - lr: 2.1938e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 6/500\n",
            "556/556 - 1s - loss: 11.3036 - mse: 11.3036 - mae: 1.5288 - val_loss: 10.1330 - val_mse: 10.1330 - val_mae: 2.6200 - lr: 2.1938e-04 - 1s/epoch - 2ms/step\n",
            "Score for inner fold : loss of 10.133002281188965\n",
            "-----Fold:2--------\n",
            "Epoch 1/500\n",
            "556/556 - 2s - loss: 18.0918 - mse: 18.0918 - mae: 2.0190 - val_loss: 6.7475 - val_mse: 6.7475 - val_mae: 2.1995 - lr: 2.1938e-04 - 2s/epoch - 3ms/step\n",
            "Epoch 2/500\n",
            "556/556 - 1s - loss: 12.9084 - mse: 12.9084 - mae: 1.5824 - val_loss: 9.8645 - val_mse: 9.8645 - val_mae: 2.6872 - lr: 2.1938e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 3/500\n",
            "556/556 - 1s - loss: 12.2137 - mse: 12.2137 - mae: 1.5825 - val_loss: 9.7915 - val_mse: 9.7915 - val_mae: 2.6268 - lr: 2.1938e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 4/500\n",
            "556/556 - 1s - loss: 11.9498 - mse: 11.9498 - mae: 1.5703 - val_loss: 10.2573 - val_mse: 10.2573 - val_mae: 2.6562 - lr: 2.1938e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 5/500\n",
            "556/556 - 1s - loss: 11.7171 - mse: 11.7171 - mae: 1.5575 - val_loss: 10.1350 - val_mse: 10.1350 - val_mae: 2.6239 - lr: 2.1938e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 6/500\n",
            "556/556 - 1s - loss: 11.5556 - mse: 11.5556 - mae: 1.5515 - val_loss: 11.0125 - val_mse: 11.0125 - val_mae: 2.7342 - lr: 2.1938e-04 - 1s/epoch - 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-10-10 10:19:56,102]\u001b[0m Finished trial#39 resulted in value: 10.733333333333334. Current best value is 8.283333333333333 with parameters: {'activation': 'tanh', 'num_hidden_layer': 2, 'i': 5, 'learning_rate': 0.00010035947429195473}.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Score for inner fold : loss of 11.012480735778809\n",
            "Trial Number:40\n",
            "-----Fold:0--------\n",
            "Epoch 1/500\n",
            "556/556 - 2s - loss: 14.6275 - mse: 14.6275 - mae: 1.7500 - val_loss: 9.8686 - val_mse: 9.8686 - val_mae: 2.5477 - lr: 1.3212e-04 - 2s/epoch - 4ms/step\n",
            "Epoch 2/500\n",
            "556/556 - 1s - loss: 11.7052 - mse: 11.7052 - mae: 1.6270 - val_loss: 10.8202 - val_mse: 10.8202 - val_mae: 2.6476 - lr: 1.3212e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 3/500\n",
            "556/556 - 1s - loss: 11.2216 - mse: 11.2216 - mae: 1.6029 - val_loss: 11.0277 - val_mse: 11.0277 - val_mae: 2.6298 - lr: 1.3212e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 4/500\n",
            "556/556 - 1s - loss: 10.9217 - mse: 10.9217 - mae: 1.5821 - val_loss: 11.3029 - val_mse: 11.3029 - val_mae: 2.6494 - lr: 1.3212e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 5/500\n",
            "556/556 - 1s - loss: 10.7761 - mse: 10.7761 - mae: 1.5691 - val_loss: 11.1959 - val_mse: 11.1959 - val_mae: 2.6544 - lr: 1.3212e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 6/500\n",
            "556/556 - 1s - loss: 10.6913 - mse: 10.6913 - mae: 1.5623 - val_loss: 10.8409 - val_mse: 10.8409 - val_mae: 2.5873 - lr: 1.3212e-04 - 1s/epoch - 2ms/step\n",
            "Score for inner fold : loss of 10.840924263000488\n",
            "-----Fold:1--------\n",
            "Epoch 1/500\n",
            "556/556 - 2s - loss: 15.2178 - mse: 15.2178 - mae: 1.7381 - val_loss: 9.7724 - val_mse: 9.7724 - val_mae: 2.5738 - lr: 1.3212e-04 - 2s/epoch - 4ms/step\n",
            "Epoch 2/500\n",
            "556/556 - 1s - loss: 12.8767 - mse: 12.8767 - mae: 1.6184 - val_loss: 10.2215 - val_mse: 10.2215 - val_mae: 2.5646 - lr: 1.3212e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 3/500\n",
            "556/556 - 1s - loss: 12.4273 - mse: 12.4273 - mae: 1.5844 - val_loss: 10.6681 - val_mse: 10.6681 - val_mae: 2.5942 - lr: 1.3212e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 4/500\n",
            "556/556 - 1s - loss: 12.1821 - mse: 12.1821 - mae: 1.5719 - val_loss: 10.9089 - val_mse: 10.9089 - val_mae: 2.5926 - lr: 1.3212e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 5/500\n",
            "556/556 - 1s - loss: 12.0479 - mse: 12.0479 - mae: 1.5613 - val_loss: 10.8448 - val_mse: 10.8448 - val_mae: 2.5487 - lr: 1.3212e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 6/500\n",
            "556/556 - 1s - loss: 11.9177 - mse: 11.9177 - mae: 1.5492 - val_loss: 11.9193 - val_mse: 11.9193 - val_mae: 2.6532 - lr: 1.3212e-04 - 1s/epoch - 2ms/step\n",
            "Score for inner fold : loss of 11.919339179992676\n",
            "-----Fold:2--------\n",
            "Epoch 1/500\n",
            "556/556 - 2s - loss: 15.4141 - mse: 15.4141 - mae: 1.7304 - val_loss: 10.2017 - val_mse: 10.2017 - val_mae: 2.5598 - lr: 1.3212e-04 - 2s/epoch - 4ms/step\n",
            "Epoch 2/500\n",
            "556/556 - 1s - loss: 12.8948 - mse: 12.8948 - mae: 1.6282 - val_loss: 11.4294 - val_mse: 11.4294 - val_mae: 2.5528 - lr: 1.3212e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 3/500\n",
            "556/556 - 1s - loss: 12.2190 - mse: 12.2190 - mae: 1.5973 - val_loss: 12.1425 - val_mse: 12.1425 - val_mae: 2.5200 - lr: 1.3212e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 4/500\n",
            "556/556 - 1s - loss: 11.9149 - mse: 11.9149 - mae: 1.5774 - val_loss: 13.9441 - val_mse: 13.9441 - val_mae: 2.6658 - lr: 1.3212e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 5/500\n",
            "556/556 - 1s - loss: 11.6758 - mse: 11.6758 - mae: 1.5646 - val_loss: 15.2337 - val_mse: 15.2337 - val_mae: 2.7361 - lr: 1.3212e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 6/500\n",
            "556/556 - 1s - loss: 11.5353 - mse: 11.5353 - mae: 1.5588 - val_loss: 16.2359 - val_mse: 16.2359 - val_mae: 2.7534 - lr: 1.3212e-04 - 1s/epoch - 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-10-10 10:20:24,122]\u001b[0m Finished trial#40 resulted in value: 13.0. Current best value is 8.283333333333333 with parameters: {'activation': 'tanh', 'num_hidden_layer': 2, 'i': 5, 'learning_rate': 0.00010035947429195473}.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Score for inner fold : loss of 16.235855102539062\n",
            "Trial Number:41\n",
            "-----Fold:0--------\n",
            "Epoch 1/500\n",
            "556/556 - 2s - loss: 21.1965 - mse: 21.1965 - mae: 2.5541 - val_loss: 1.3024 - val_mse: 1.3024 - val_mae: 0.8210 - lr: 1.0140e-04 - 2s/epoch - 3ms/step\n",
            "Epoch 2/500\n",
            "556/556 - 1s - loss: 18.1904 - mse: 18.1904 - mae: 2.1408 - val_loss: 2.1613 - val_mse: 2.1613 - val_mae: 1.1369 - lr: 1.0140e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 3/500\n",
            "556/556 - 1s - loss: 15.6958 - mse: 15.6958 - mae: 1.7463 - val_loss: 3.9076 - val_mse: 3.9076 - val_mae: 1.6267 - lr: 1.0140e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 4/500\n",
            "556/556 - 1s - loss: 13.7914 - mse: 13.7914 - mae: 1.5347 - val_loss: 6.5371 - val_mse: 6.5371 - val_mae: 2.2084 - lr: 1.0140e-04 - 995ms/epoch - 2ms/step\n",
            "Epoch 5/500\n",
            "556/556 - 1s - loss: 12.8285 - mse: 12.8285 - mae: 1.5544 - val_loss: 8.2393 - val_mse: 8.2393 - val_mae: 2.5083 - lr: 1.0140e-04 - 996ms/epoch - 2ms/step\n",
            "Epoch 6/500\n",
            "556/556 - 1s - loss: 12.4612 - mse: 12.4612 - mae: 1.5848 - val_loss: 8.8413 - val_mse: 8.8413 - val_mae: 2.5867 - lr: 1.0140e-04 - 1s/epoch - 2ms/step\n",
            "Score for inner fold : loss of 8.841323852539062\n",
            "-----Fold:1--------\n",
            "Epoch 1/500\n",
            "556/556 - 2s - loss: 22.1822 - mse: 22.1822 - mae: 2.5258 - val_loss: 1.4022 - val_mse: 1.4022 - val_mae: 0.8781 - lr: 1.0140e-04 - 2s/epoch - 3ms/step\n",
            "Epoch 2/500\n",
            "556/556 - 1s - loss: 19.4578 - mse: 19.4578 - mae: 2.1450 - val_loss: 2.0007 - val_mse: 2.0007 - val_mae: 1.1007 - lr: 1.0140e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 3/500\n",
            "556/556 - 1s - loss: 17.1331 - mse: 17.1331 - mae: 1.7785 - val_loss: 3.4323 - val_mse: 3.4323 - val_mae: 1.5237 - lr: 1.0140e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 4/500\n",
            "556/556 - 1s - loss: 15.2816 - mse: 15.2816 - mae: 1.5531 - val_loss: 5.7269 - val_mse: 5.7269 - val_mae: 2.0643 - lr: 1.0140e-04 - 970ms/epoch - 2ms/step\n",
            "Epoch 5/500\n",
            "556/556 - 1s - loss: 14.2239 - mse: 14.2239 - mae: 1.5481 - val_loss: 7.5712 - val_mse: 7.5712 - val_mae: 2.4216 - lr: 1.0140e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 6/500\n",
            "556/556 - 1s - loss: 13.8056 - mse: 13.8056 - mae: 1.5899 - val_loss: 8.3269 - val_mse: 8.3269 - val_mae: 2.5406 - lr: 1.0140e-04 - 1s/epoch - 2ms/step\n",
            "Score for inner fold : loss of 8.326855659484863\n",
            "-----Fold:2--------\n",
            "Epoch 1/500\n",
            "556/556 - 2s - loss: 23.5540 - mse: 23.5540 - mae: 2.5241 - val_loss: 1.4269 - val_mse: 1.4269 - val_mae: 0.8749 - lr: 1.0140e-04 - 2s/epoch - 3ms/step\n",
            "Epoch 2/500\n",
            "556/556 - 1s - loss: 20.3285 - mse: 20.3285 - mae: 2.1090 - val_loss: 2.2756 - val_mse: 2.2756 - val_mae: 1.1877 - lr: 1.0140e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 3/500\n",
            "556/556 - 1s - loss: 17.7332 - mse: 17.7332 - mae: 1.7541 - val_loss: 3.9693 - val_mse: 3.9693 - val_mae: 1.6590 - lr: 1.0140e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 4/500\n",
            "556/556 - 1s - loss: 15.7429 - mse: 15.7429 - mae: 1.5634 - val_loss: 6.4907 - val_mse: 6.4907 - val_mae: 2.2093 - lr: 1.0140e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 5/500\n",
            "556/556 - 1s - loss: 14.7443 - mse: 14.7443 - mae: 1.5778 - val_loss: 8.1171 - val_mse: 8.1171 - val_mae: 2.4928 - lr: 1.0140e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 6/500\n",
            "556/556 - 1s - loss: 14.2965 - mse: 14.2965 - mae: 1.6024 - val_loss: 8.6568 - val_mse: 8.6568 - val_mae: 2.5624 - lr: 1.0140e-04 - 1s/epoch - 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-10-10 10:20:48,595]\u001b[0m Finished trial#41 resulted in value: 8.610000000000001. Current best value is 8.283333333333333 with parameters: {'activation': 'tanh', 'num_hidden_layer': 2, 'i': 5, 'learning_rate': 0.00010035947429195473}.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Score for inner fold : loss of 8.656805992126465\n",
            "Trial Number:42\n",
            "-----Fold:0--------\n",
            "Epoch 1/500\n",
            "556/556 - 2s - loss: 21.1222 - mse: 21.1222 - mae: 2.5526 - val_loss: 1.5709 - val_mse: 1.5709 - val_mae: 0.9174 - lr: 1.0326e-04 - 2s/epoch - 3ms/step\n",
            "Epoch 2/500\n",
            "556/556 - 1s - loss: 18.6633 - mse: 18.6633 - mae: 2.1598 - val_loss: 2.3170 - val_mse: 2.3170 - val_mae: 1.1682 - lr: 1.0326e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 3/500\n",
            "556/556 - 1s - loss: 16.4266 - mse: 16.4266 - mae: 1.7852 - val_loss: 3.8392 - val_mse: 3.8392 - val_mae: 1.5981 - lr: 1.0326e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 4/500\n",
            "556/556 - 1s - loss: 14.6087 - mse: 14.6087 - mae: 1.5506 - val_loss: 6.1281 - val_mse: 6.1281 - val_mae: 2.1228 - lr: 1.0326e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 5/500\n",
            "556/556 - 1s - loss: 13.5641 - mse: 13.5641 - mae: 1.5384 - val_loss: 7.9224 - val_mse: 7.9224 - val_mae: 2.4563 - lr: 1.0326e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 6/500\n",
            "556/556 - 1s - loss: 13.1148 - mse: 13.1148 - mae: 1.5697 - val_loss: 8.5849 - val_mse: 8.5849 - val_mae: 2.5486 - lr: 1.0326e-04 - 1s/epoch - 2ms/step\n",
            "Score for inner fold : loss of 8.584857940673828\n",
            "-----Fold:1--------\n",
            "Epoch 1/500\n",
            "556/556 - 2s - loss: 23.3175 - mse: 23.3175 - mae: 2.5793 - val_loss: 1.3301 - val_mse: 1.3301 - val_mae: 0.8329 - lr: 1.0326e-04 - 2s/epoch - 3ms/step\n",
            "Epoch 2/500\n",
            "556/556 - 1s - loss: 20.5658 - mse: 20.5658 - mae: 2.2271 - val_loss: 1.9697 - val_mse: 1.9697 - val_mae: 1.0714 - lr: 1.0326e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 3/500\n",
            "556/556 - 1s - loss: 18.0399 - mse: 18.0399 - mae: 1.8510 - val_loss: 3.4876 - val_mse: 3.4876 - val_mae: 1.5148 - lr: 1.0326e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 4/500\n",
            "556/556 - 1s - loss: 15.9465 - mse: 15.9465 - mae: 1.5895 - val_loss: 5.9348 - val_mse: 5.9348 - val_mae: 2.0814 - lr: 1.0326e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 5/500\n",
            "556/556 - 1s - loss: 14.7797 - mse: 14.7797 - mae: 1.5742 - val_loss: 7.8307 - val_mse: 7.8307 - val_mae: 2.4551 - lr: 1.0326e-04 - 992ms/epoch - 2ms/step\n",
            "Epoch 6/500\n",
            "556/556 - 1s - loss: 14.3077 - mse: 14.3077 - mae: 1.6108 - val_loss: 8.5944 - val_mse: 8.5944 - val_mae: 2.5716 - lr: 1.0326e-04 - 1s/epoch - 2ms/step\n",
            "Score for inner fold : loss of 8.594371795654297\n",
            "-----Fold:2--------\n",
            "Epoch 1/500\n",
            "556/556 - 2s - loss: 21.9560 - mse: 21.9560 - mae: 2.5285 - val_loss: 1.3041 - val_mse: 1.3041 - val_mae: 0.8097 - lr: 1.0326e-04 - 2s/epoch - 3ms/step\n",
            "Epoch 2/500\n",
            "556/556 - 1s - loss: 19.2106 - mse: 19.2106 - mae: 2.1542 - val_loss: 2.1220 - val_mse: 2.1220 - val_mae: 1.1113 - lr: 1.0326e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 3/500\n",
            "556/556 - 1s - loss: 16.8244 - mse: 16.8244 - mae: 1.7846 - val_loss: 3.7139 - val_mse: 3.7139 - val_mae: 1.5673 - lr: 1.0326e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 4/500\n",
            "556/556 - 1s - loss: 14.8705 - mse: 14.8705 - mae: 1.5564 - val_loss: 6.1282 - val_mse: 6.1282 - val_mae: 2.1273 - lr: 1.0326e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 5/500\n",
            "556/556 - 1s - loss: 13.8275 - mse: 13.8275 - mae: 1.5515 - val_loss: 7.9918 - val_mse: 7.9918 - val_mae: 2.4659 - lr: 1.0326e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 6/500\n",
            "556/556 - 1s - loss: 13.4084 - mse: 13.4084 - mae: 1.5900 - val_loss: 8.7469 - val_mse: 8.7469 - val_mae: 2.5721 - lr: 1.0326e-04 - 1s/epoch - 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-10-10 10:21:13,300]\u001b[0m Finished trial#42 resulted in value: 8.64. Current best value is 8.283333333333333 with parameters: {'activation': 'tanh', 'num_hidden_layer': 2, 'i': 5, 'learning_rate': 0.00010035947429195473}.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Score for inner fold : loss of 8.746925354003906\n",
            "Trial Number:43\n",
            "-----Fold:0--------\n",
            "Epoch 1/500\n",
            "556/556 - 2s - loss: 20.0711 - mse: 20.0711 - mae: 2.2155 - val_loss: 2.8832 - val_mse: 2.8832 - val_mae: 1.3801 - lr: 2.8344e-04 - 2s/epoch - 3ms/step\n",
            "Epoch 2/500\n",
            "556/556 - 1s - loss: 14.1412 - mse: 14.1412 - mae: 1.5798 - val_loss: 8.3199 - val_mse: 8.3199 - val_mae: 2.5474 - lr: 2.8344e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 3/500\n",
            "556/556 - 1s - loss: 12.7749 - mse: 12.7749 - mae: 1.5965 - val_loss: 8.9576 - val_mse: 8.9576 - val_mae: 2.5994 - lr: 2.8344e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 4/500\n",
            "556/556 - 1s - loss: 12.3902 - mse: 12.3902 - mae: 1.5858 - val_loss: 8.9012 - val_mse: 8.9012 - val_mae: 2.5661 - lr: 2.8344e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 5/500\n",
            "556/556 - 1s - loss: 12.1397 - mse: 12.1397 - mae: 1.5738 - val_loss: 9.5367 - val_mse: 9.5367 - val_mae: 2.6346 - lr: 2.8344e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 6/500\n",
            "556/556 - 1s - loss: 11.9584 - mse: 11.9584 - mae: 1.5655 - val_loss: 9.4362 - val_mse: 9.4362 - val_mae: 2.5990 - lr: 2.8344e-04 - 1s/epoch - 2ms/step\n",
            "Score for inner fold : loss of 9.436184883117676\n",
            "-----Fold:1--------\n",
            "Epoch 1/500\n",
            "556/556 - 2s - loss: 20.3995 - mse: 20.3995 - mae: 2.1867 - val_loss: 3.2103 - val_mse: 3.2103 - val_mae: 1.4733 - lr: 2.8344e-04 - 2s/epoch - 3ms/step\n",
            "Epoch 2/500\n",
            "556/556 - 1s - loss: 15.1828 - mse: 15.1828 - mae: 1.5836 - val_loss: 8.0758 - val_mse: 8.0758 - val_mae: 2.5114 - lr: 2.8344e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 3/500\n",
            "556/556 - 1s - loss: 13.7225 - mse: 13.7225 - mae: 1.5878 - val_loss: 9.1204 - val_mse: 9.1204 - val_mae: 2.6285 - lr: 2.8344e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 4/500\n",
            "556/556 - 1s - loss: 13.2580 - mse: 13.2580 - mae: 1.5750 - val_loss: 9.5448 - val_mse: 9.5448 - val_mae: 2.6405 - lr: 2.8344e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 5/500\n",
            "556/556 - 1s - loss: 12.9870 - mse: 12.9870 - mae: 1.5647 - val_loss: 9.6795 - val_mse: 9.6795 - val_mae: 2.6510 - lr: 2.8344e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 6/500\n",
            "556/556 - 1s - loss: 12.8003 - mse: 12.8003 - mae: 1.5557 - val_loss: 9.6541 - val_mse: 9.6541 - val_mae: 2.6303 - lr: 2.8344e-04 - 1s/epoch - 2ms/step\n",
            "Score for inner fold : loss of 9.65410041809082\n",
            "-----Fold:2--------\n",
            "Epoch 1/500\n",
            "556/556 - 2s - loss: 20.8922 - mse: 20.8922 - mae: 2.2286 - val_loss: 3.2389 - val_mse: 3.2389 - val_mae: 1.4606 - lr: 2.8344e-04 - 2s/epoch - 3ms/step\n",
            "Epoch 2/500\n",
            "556/556 - 1s - loss: 15.3416 - mse: 15.3416 - mae: 1.6123 - val_loss: 8.2460 - val_mse: 8.2460 - val_mae: 2.5280 - lr: 2.8344e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 3/500\n",
            "556/556 - 1s - loss: 13.8572 - mse: 13.8572 - mae: 1.6169 - val_loss: 8.7803 - val_mse: 8.7803 - val_mae: 2.5812 - lr: 2.8344e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 4/500\n",
            "556/556 - 1s - loss: 13.3341 - mse: 13.3341 - mae: 1.5948 - val_loss: 9.1867 - val_mse: 9.1867 - val_mae: 2.6109 - lr: 2.8344e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 5/500\n",
            "556/556 - 1s - loss: 12.9742 - mse: 12.9742 - mae: 1.5864 - val_loss: 9.2942 - val_mse: 9.2942 - val_mae: 2.5899 - lr: 2.8344e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 6/500\n",
            "556/556 - 1s - loss: 12.7452 - mse: 12.7452 - mae: 1.5721 - val_loss: 9.5399 - val_mse: 9.5399 - val_mae: 2.6210 - lr: 2.8344e-04 - 1s/epoch - 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-10-10 10:21:38,341]\u001b[0m Finished trial#43 resulted in value: 9.543333333333333. Current best value is 8.283333333333333 with parameters: {'activation': 'tanh', 'num_hidden_layer': 2, 'i': 5, 'learning_rate': 0.00010035947429195473}.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Score for inner fold : loss of 9.539886474609375\n",
            "Trial Number:44\n",
            "-----Fold:0--------\n",
            "Epoch 1/500\n",
            "556/556 - 2s - loss: 21.7088 - mse: 21.7088 - mae: 2.3190 - val_loss: 2.2278 - val_mse: 2.2278 - val_mae: 1.1734 - lr: 1.6553e-04 - 2s/epoch - 3ms/step\n",
            "Epoch 2/500\n",
            "556/556 - 1s - loss: 16.2484 - mse: 16.2484 - mae: 1.6141 - val_loss: 6.9566 - val_mse: 6.9566 - val_mae: 2.3010 - lr: 1.6553e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 3/500\n",
            "556/556 - 1s - loss: 14.0524 - mse: 14.0524 - mae: 1.5699 - val_loss: 8.8723 - val_mse: 8.8723 - val_mae: 2.5790 - lr: 1.6553e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 4/500\n",
            "556/556 - 1s - loss: 13.5464 - mse: 13.5464 - mae: 1.5677 - val_loss: 9.4084 - val_mse: 9.4084 - val_mae: 2.6176 - lr: 1.6553e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 5/500\n",
            "556/556 - 1s - loss: 13.2778 - mse: 13.2778 - mae: 1.5624 - val_loss: 9.4822 - val_mse: 9.4822 - val_mae: 2.5985 - lr: 1.6553e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 6/500\n",
            "556/556 - 1s - loss: 13.0803 - mse: 13.0803 - mae: 1.5544 - val_loss: 9.5915 - val_mse: 9.5915 - val_mae: 2.5926 - lr: 1.6553e-04 - 1s/epoch - 2ms/step\n",
            "Score for inner fold : loss of 9.591495513916016\n",
            "-----Fold:1--------\n",
            "Epoch 1/500\n",
            "556/556 - 2s - loss: 20.8366 - mse: 20.8366 - mae: 2.2792 - val_loss: 2.5720 - val_mse: 2.5720 - val_mae: 1.2554 - lr: 1.6553e-04 - 2s/epoch - 3ms/step\n",
            "Epoch 2/500\n",
            "556/556 - 1s - loss: 14.9712 - mse: 14.9712 - mae: 1.6334 - val_loss: 7.2879 - val_mse: 7.2879 - val_mae: 2.3431 - lr: 1.6553e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 3/500\n",
            "556/556 - 1s - loss: 12.8879 - mse: 12.8879 - mae: 1.5777 - val_loss: 9.0613 - val_mse: 9.0613 - val_mae: 2.5980 - lr: 1.6553e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 4/500\n",
            "556/556 - 1s - loss: 12.4345 - mse: 12.4345 - mae: 1.5807 - val_loss: 9.5545 - val_mse: 9.5545 - val_mae: 2.6358 - lr: 1.6553e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 5/500\n",
            "556/556 - 1s - loss: 12.1863 - mse: 12.1863 - mae: 1.5693 - val_loss: 9.7490 - val_mse: 9.7490 - val_mae: 2.6374 - lr: 1.6553e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 6/500\n",
            "556/556 - 1s - loss: 12.0089 - mse: 12.0089 - mae: 1.5643 - val_loss: 9.8351 - val_mse: 9.8351 - val_mae: 2.6333 - lr: 1.6553e-04 - 1s/epoch - 2ms/step\n",
            "Score for inner fold : loss of 9.835103034973145\n",
            "-----Fold:2--------\n",
            "Epoch 1/500\n",
            "556/556 - 2s - loss: 20.7690 - mse: 20.7690 - mae: 2.2954 - val_loss: 2.2999 - val_mse: 2.2999 - val_mae: 1.2098 - lr: 1.6553e-04 - 2s/epoch - 3ms/step\n",
            "Epoch 2/500\n",
            "556/556 - 1s - loss: 15.2757 - mse: 15.2757 - mae: 1.6372 - val_loss: 7.1258 - val_mse: 7.1258 - val_mae: 2.3305 - lr: 1.6553e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 3/500\n",
            "556/556 - 1s - loss: 13.1483 - mse: 13.1483 - mae: 1.6066 - val_loss: 9.0045 - val_mse: 9.0045 - val_mae: 2.5940 - lr: 1.6553e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 4/500\n",
            "556/556 - 1s - loss: 12.5375 - mse: 12.5375 - mae: 1.6076 - val_loss: 9.1723 - val_mse: 9.1723 - val_mae: 2.6040 - lr: 1.6553e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 5/500\n",
            "556/556 - 1s - loss: 12.2688 - mse: 12.2688 - mae: 1.5941 - val_loss: 9.4008 - val_mse: 9.4008 - val_mae: 2.6021 - lr: 1.6553e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 6/500\n",
            "556/556 - 1s - loss: 12.0556 - mse: 12.0556 - mae: 1.5841 - val_loss: 9.6059 - val_mse: 9.6059 - val_mae: 2.6153 - lr: 1.6553e-04 - 1s/epoch - 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-10-10 10:22:04,389]\u001b[0m Finished trial#44 resulted in value: 9.68. Current best value is 8.283333333333333 with parameters: {'activation': 'tanh', 'num_hidden_layer': 2, 'i': 5, 'learning_rate': 0.00010035947429195473}.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Score for inner fold : loss of 9.605941772460938\n",
            "Trial Number:45\n",
            "-----Fold:0--------\n",
            "Epoch 1/500\n",
            "556/556 - 2s - loss: 23.3806 - mse: 23.3806 - mae: 2.5202 - val_loss: 1.5007 - val_mse: 1.5007 - val_mae: 0.8980 - lr: 1.2236e-04 - 2s/epoch - 3ms/step\n",
            "Epoch 2/500\n",
            "556/556 - 1s - loss: 20.2801 - mse: 20.2801 - mae: 2.0958 - val_loss: 2.4994 - val_mse: 2.4994 - val_mae: 1.2487 - lr: 1.2236e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 3/500\n",
            "556/556 - 1s - loss: 17.5637 - mse: 17.5637 - mae: 1.7114 - val_loss: 4.6384 - val_mse: 4.6384 - val_mae: 1.8105 - lr: 1.2236e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 4/500\n",
            "556/556 - 1s - loss: 15.6433 - mse: 15.6433 - mae: 1.5757 - val_loss: 7.2531 - val_mse: 7.2531 - val_mae: 2.3584 - lr: 1.2236e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 5/500\n",
            "556/556 - 1s - loss: 14.8273 - mse: 14.8273 - mae: 1.6129 - val_loss: 8.4171 - val_mse: 8.4171 - val_mae: 2.5454 - lr: 1.2236e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 6/500\n",
            "556/556 - 1s - loss: 14.4402 - mse: 14.4402 - mae: 1.6214 - val_loss: 8.7934 - val_mse: 8.7934 - val_mae: 2.5877 - lr: 1.2236e-04 - 1s/epoch - 2ms/step\n",
            "Score for inner fold : loss of 8.793383598327637\n",
            "-----Fold:1--------\n",
            "Epoch 1/500\n",
            "556/556 - 2s - loss: 22.9417 - mse: 22.9417 - mae: 2.4536 - val_loss: 1.3790 - val_mse: 1.3790 - val_mae: 0.8740 - lr: 1.2236e-04 - 2s/epoch - 3ms/step\n",
            "Epoch 2/500\n",
            "556/556 - 1s - loss: 19.3697 - mse: 19.3697 - mae: 2.0153 - val_loss: 2.3433 - val_mse: 2.3433 - val_mae: 1.2276 - lr: 1.2236e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 3/500\n",
            "556/556 - 1s - loss: 16.5433 - mse: 16.5433 - mae: 1.6432 - val_loss: 4.6454 - val_mse: 4.6454 - val_mae: 1.8313 - lr: 1.2236e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 4/500\n",
            "556/556 - 1s - loss: 14.7298 - mse: 14.7298 - mae: 1.5462 - val_loss: 7.1974 - val_mse: 7.1974 - val_mae: 2.3582 - lr: 1.2236e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 5/500\n",
            "556/556 - 1s - loss: 13.9925 - mse: 13.9925 - mae: 1.5886 - val_loss: 8.2116 - val_mse: 8.2116 - val_mae: 2.5191 - lr: 1.2236e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 6/500\n",
            "556/556 - 1s - loss: 13.6591 - mse: 13.6591 - mae: 1.5965 - val_loss: 8.5847 - val_mse: 8.5847 - val_mae: 2.5669 - lr: 1.2236e-04 - 1s/epoch - 2ms/step\n",
            "Score for inner fold : loss of 8.584708213806152\n",
            "-----Fold:2--------\n",
            "Epoch 1/500\n",
            "556/556 - 2s - loss: 21.6469 - mse: 21.6469 - mae: 2.4904 - val_loss: 1.4243 - val_mse: 1.4243 - val_mae: 0.8780 - lr: 1.2236e-04 - 2s/epoch - 3ms/step\n",
            "Epoch 2/500\n",
            "556/556 - 1s - loss: 18.8547 - mse: 18.8547 - mae: 2.1361 - val_loss: 2.1431 - val_mse: 2.1431 - val_mae: 1.1583 - lr: 1.2236e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 3/500\n",
            "556/556 - 1s - loss: 16.1619 - mse: 16.1619 - mae: 1.7579 - val_loss: 3.9372 - val_mse: 3.9372 - val_mae: 1.6667 - lr: 1.2236e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 4/500\n",
            "556/556 - 1s - loss: 14.0122 - mse: 14.0122 - mae: 1.5520 - val_loss: 6.6630 - val_mse: 6.6630 - val_mae: 2.2674 - lr: 1.2236e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 5/500\n",
            "556/556 - 1s - loss: 12.9963 - mse: 12.9963 - mae: 1.5739 - val_loss: 8.0730 - val_mse: 8.0730 - val_mae: 2.5197 - lr: 1.2236e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 6/500\n",
            "556/556 - 1s - loss: 12.6298 - mse: 12.6298 - mae: 1.5862 - val_loss: 8.5366 - val_mse: 8.5366 - val_mae: 2.5715 - lr: 1.2236e-04 - 1s/epoch - 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-10-10 10:22:29,046]\u001b[0m Finished trial#45 resulted in value: 8.636666666666665. Current best value is 8.283333333333333 with parameters: {'activation': 'tanh', 'num_hidden_layer': 2, 'i': 5, 'learning_rate': 0.00010035947429195473}.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Score for inner fold : loss of 8.536605834960938\n",
            "Trial Number:46\n",
            "-----Fold:0--------\n",
            "Epoch 1/500\n",
            "556/556 - 2s - loss: 20.6473 - mse: 20.6473 - mae: 2.3246 - val_loss: 2.2956 - val_mse: 2.2956 - val_mae: 1.1871 - lr: 2.0893e-04 - 2s/epoch - 3ms/step\n",
            "Epoch 2/500\n",
            "556/556 - 1s - loss: 15.1678 - mse: 15.1678 - mae: 1.6287 - val_loss: 6.8049 - val_mse: 6.8049 - val_mae: 2.2416 - lr: 2.0893e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 3/500\n",
            "556/556 - 1s - loss: 13.1031 - mse: 13.1031 - mae: 1.5725 - val_loss: 8.7000 - val_mse: 8.7000 - val_mae: 2.5661 - lr: 2.0893e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 4/500\n",
            "556/556 - 1s - loss: 12.6366 - mse: 12.6366 - mae: 1.5813 - val_loss: 9.1245 - val_mse: 9.1245 - val_mae: 2.6059 - lr: 2.0893e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 5/500\n",
            "556/556 - 1s - loss: 12.3570 - mse: 12.3570 - mae: 1.5736 - val_loss: 9.1769 - val_mse: 9.1769 - val_mae: 2.5902 - lr: 2.0893e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 6/500\n",
            "556/556 - 1s - loss: 12.1534 - mse: 12.1534 - mae: 1.5656 - val_loss: 9.1680 - val_mse: 9.1680 - val_mae: 2.5800 - lr: 2.0893e-04 - 1s/epoch - 2ms/step\n",
            "Score for inner fold : loss of 9.168000221252441\n",
            "-----Fold:1--------\n",
            "Epoch 1/500\n",
            "556/556 - 2s - loss: 20.1346 - mse: 20.1346 - mae: 2.3051 - val_loss: 1.9548 - val_mse: 1.9548 - val_mae: 1.0847 - lr: 2.0893e-04 - 2s/epoch - 3ms/step\n",
            "Epoch 2/500\n",
            "556/556 - 1s - loss: 15.1224 - mse: 15.1224 - mae: 1.6629 - val_loss: 6.0721 - val_mse: 6.0721 - val_mae: 2.1433 - lr: 2.0893e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 3/500\n",
            "556/556 - 1s - loss: 12.7728 - mse: 12.7728 - mae: 1.5746 - val_loss: 8.4170 - val_mse: 8.4170 - val_mae: 2.5584 - lr: 2.0893e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 4/500\n",
            "556/556 - 1s - loss: 12.1949 - mse: 12.1949 - mae: 1.5878 - val_loss: 8.8565 - val_mse: 8.8565 - val_mae: 2.5986 - lr: 2.0893e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 5/500\n",
            "556/556 - 1s - loss: 11.8961 - mse: 11.8961 - mae: 1.5741 - val_loss: 9.3389 - val_mse: 9.3389 - val_mae: 2.6311 - lr: 2.0893e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 6/500\n",
            "556/556 - 1s - loss: 11.7022 - mse: 11.7022 - mae: 1.5673 - val_loss: 9.2330 - val_mse: 9.2330 - val_mae: 2.6031 - lr: 2.0893e-04 - 1s/epoch - 2ms/step\n",
            "Score for inner fold : loss of 9.233026504516602\n",
            "-----Fold:2--------\n",
            "Epoch 1/500\n",
            "556/556 - 2s - loss: 22.9382 - mse: 22.9382 - mae: 2.3903 - val_loss: 2.0254 - val_mse: 2.0254 - val_mae: 1.1087 - lr: 2.0893e-04 - 2s/epoch - 3ms/step\n",
            "Epoch 2/500\n",
            "556/556 - 1s - loss: 18.0390 - mse: 18.0390 - mae: 1.7152 - val_loss: 5.8655 - val_mse: 5.8655 - val_mae: 2.0976 - lr: 2.0893e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 3/500\n",
            "556/556 - 1s - loss: 15.5014 - mse: 15.5014 - mae: 1.6057 - val_loss: 8.7808 - val_mse: 8.7808 - val_mae: 2.5983 - lr: 2.0893e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 4/500\n",
            "556/556 - 1s - loss: 14.8988 - mse: 14.8988 - mae: 1.6274 - val_loss: 9.1803 - val_mse: 9.1803 - val_mae: 2.6303 - lr: 2.0893e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 5/500\n",
            "556/556 - 1s - loss: 14.5926 - mse: 14.5926 - mae: 1.6178 - val_loss: 9.3046 - val_mse: 9.3046 - val_mae: 2.6130 - lr: 2.0893e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 6/500\n",
            "556/556 - 1s - loss: 14.3810 - mse: 14.3810 - mae: 1.6039 - val_loss: 9.7147 - val_mse: 9.7147 - val_mae: 2.6486 - lr: 2.0893e-04 - 1s/epoch - 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-10-10 10:22:53,746]\u001b[0m Finished trial#46 resulted in value: 9.37. Current best value is 8.283333333333333 with parameters: {'activation': 'tanh', 'num_hidden_layer': 2, 'i': 5, 'learning_rate': 0.00010035947429195473}.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Score for inner fold : loss of 9.714680671691895\n",
            "Trial Number:47\n",
            "-----Fold:0--------\n",
            "Epoch 1/500\n",
            "556/556 - 2s - loss: 21.0049 - mse: 21.0049 - mae: 2.0519 - val_loss: 5.1517 - val_mse: 5.1517 - val_mae: 1.9005 - lr: 2.6996e-04 - 2s/epoch - 3ms/step\n",
            "Epoch 2/500\n",
            "556/556 - 1s - loss: 15.0085 - mse: 15.0085 - mae: 1.5774 - val_loss: 9.2890 - val_mse: 9.2890 - val_mae: 2.6193 - lr: 2.6996e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 3/500\n",
            "556/556 - 1s - loss: 14.2149 - mse: 14.2149 - mae: 1.5838 - val_loss: 9.5077 - val_mse: 9.5077 - val_mae: 2.6223 - lr: 2.6996e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 4/500\n",
            "556/556 - 1s - loss: 13.8558 - mse: 13.8558 - mae: 1.5755 - val_loss: 9.7903 - val_mse: 9.7903 - val_mae: 2.6175 - lr: 2.6996e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 5/500\n",
            "556/556 - 1s - loss: 13.6520 - mse: 13.6520 - mae: 1.5593 - val_loss: 9.6391 - val_mse: 9.6391 - val_mae: 2.5969 - lr: 2.6996e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 6/500\n",
            "556/556 - 1s - loss: 13.5064 - mse: 13.5064 - mae: 1.5508 - val_loss: 9.9239 - val_mse: 9.9239 - val_mae: 2.6085 - lr: 2.6996e-04 - 1s/epoch - 2ms/step\n",
            "Score for inner fold : loss of 9.923870086669922\n",
            "-----Fold:1--------\n",
            "Epoch 1/500\n",
            "556/556 - 2s - loss: 17.8139 - mse: 17.8139 - mae: 2.0667 - val_loss: 5.1152 - val_mse: 5.1152 - val_mae: 1.9098 - lr: 2.6996e-04 - 2s/epoch - 3ms/step\n",
            "Epoch 2/500\n",
            "556/556 - 1s - loss: 12.2405 - mse: 12.2405 - mae: 1.5734 - val_loss: 9.1961 - val_mse: 9.1961 - val_mae: 2.6021 - lr: 2.6996e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 3/500\n",
            "556/556 - 1s - loss: 11.3451 - mse: 11.3451 - mae: 1.5725 - val_loss: 9.9734 - val_mse: 9.9734 - val_mae: 2.6653 - lr: 2.6996e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 4/500\n",
            "556/556 - 1s - loss: 11.0527 - mse: 11.0527 - mae: 1.5679 - val_loss: 10.0551 - val_mse: 10.0551 - val_mae: 2.6417 - lr: 2.6996e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 5/500\n",
            "556/556 - 1s - loss: 10.8978 - mse: 10.8978 - mae: 1.5555 - val_loss: 10.1749 - val_mse: 10.1749 - val_mae: 2.6528 - lr: 2.6996e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 6/500\n",
            "556/556 - 1s - loss: 10.7829 - mse: 10.7829 - mae: 1.5494 - val_loss: 9.9354 - val_mse: 9.9354 - val_mae: 2.6252 - lr: 2.6996e-04 - 1s/epoch - 2ms/step\n",
            "Score for inner fold : loss of 9.935392379760742\n",
            "-----Fold:2--------\n",
            "Epoch 1/500\n",
            "556/556 - 2s - loss: 18.1363 - mse: 18.1363 - mae: 2.0654 - val_loss: 5.6550 - val_mse: 5.6550 - val_mae: 2.0123 - lr: 2.6996e-04 - 2s/epoch - 3ms/step\n",
            "Epoch 2/500\n",
            "556/556 - 1s - loss: 12.9986 - mse: 12.9986 - mae: 1.5805 - val_loss: 9.1322 - val_mse: 9.1322 - val_mae: 2.6097 - lr: 2.6996e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 3/500\n",
            "556/556 - 1s - loss: 12.3258 - mse: 12.3258 - mae: 1.5785 - val_loss: 9.5782 - val_mse: 9.5782 - val_mae: 2.6270 - lr: 2.6996e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 4/500\n",
            "556/556 - 1s - loss: 12.0477 - mse: 12.0477 - mae: 1.5641 - val_loss: 9.4260 - val_mse: 9.4260 - val_mae: 2.5855 - lr: 2.6996e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 5/500\n",
            "556/556 - 1s - loss: 11.8495 - mse: 11.8495 - mae: 1.5496 - val_loss: 9.9272 - val_mse: 9.9272 - val_mae: 2.6277 - lr: 2.6996e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 6/500\n",
            "556/556 - 1s - loss: 11.6749 - mse: 11.6749 - mae: 1.5431 - val_loss: 10.2264 - val_mse: 10.2264 - val_mae: 2.6598 - lr: 2.6996e-04 - 1s/epoch - 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-10-10 10:23:19,903]\u001b[0m Finished trial#47 resulted in value: 10.03. Current best value is 8.283333333333333 with parameters: {'activation': 'tanh', 'num_hidden_layer': 2, 'i': 5, 'learning_rate': 0.00010035947429195473}.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Score for inner fold : loss of 10.22641658782959\n",
            "Trial Number:48\n",
            "-----Fold:0--------\n",
            "Epoch 1/500\n",
            "556/556 - 2s - loss: 23.1271 - mse: 23.1271 - mae: 2.5215 - val_loss: 1.2553 - val_mse: 1.2553 - val_mae: 0.8057 - lr: 1.5024e-04 - 2s/epoch - 3ms/step\n",
            "Epoch 2/500\n",
            "556/556 - 1s - loss: 20.3370 - mse: 20.3370 - mae: 2.1956 - val_loss: 1.7353 - val_mse: 1.7353 - val_mae: 1.0294 - lr: 1.5024e-04 - 969ms/epoch - 2ms/step\n",
            "Epoch 3/500\n",
            "556/556 - 1s - loss: 17.7971 - mse: 17.7971 - mae: 1.8429 - val_loss: 3.0400 - val_mse: 3.0400 - val_mae: 1.4513 - lr: 1.5024e-04 - 943ms/epoch - 2ms/step\n",
            "Epoch 4/500\n",
            "556/556 - 1s - loss: 15.6142 - mse: 15.6142 - mae: 1.5951 - val_loss: 5.3522 - val_mse: 5.3522 - val_mae: 2.0225 - lr: 1.5024e-04 - 950ms/epoch - 2ms/step\n",
            "Epoch 5/500\n",
            "556/556 - 1s - loss: 14.2710 - mse: 14.2710 - mae: 1.5684 - val_loss: 7.4026 - val_mse: 7.4026 - val_mae: 2.4252 - lr: 1.5024e-04 - 974ms/epoch - 2ms/step\n",
            "Epoch 6/500\n",
            "556/556 - 1s - loss: 13.6456 - mse: 13.6456 - mae: 1.6008 - val_loss: 8.2704 - val_mse: 8.2704 - val_mae: 2.5577 - lr: 1.5024e-04 - 928ms/epoch - 2ms/step\n",
            "Score for inner fold : loss of 8.270355224609375\n",
            "-----Fold:1--------\n",
            "Epoch 1/500\n",
            "556/556 - 2s - loss: 21.6295 - mse: 21.6295 - mae: 2.5461 - val_loss: 1.4677 - val_mse: 1.4677 - val_mae: 0.8671 - lr: 1.5024e-04 - 2s/epoch - 3ms/step\n",
            "Epoch 2/500\n",
            "556/556 - 1s - loss: 19.0255 - mse: 19.0255 - mae: 2.1524 - val_loss: 2.1370 - val_mse: 2.1370 - val_mae: 1.1297 - lr: 1.5024e-04 - 957ms/epoch - 2ms/step\n",
            "Epoch 3/500\n",
            "556/556 - 1s - loss: 16.5065 - mse: 16.5065 - mae: 1.7778 - val_loss: 3.7133 - val_mse: 3.7133 - val_mae: 1.5957 - lr: 1.5024e-04 - 967ms/epoch - 2ms/step\n",
            "Epoch 4/500\n",
            "556/556 - 1s - loss: 14.4908 - mse: 14.4908 - mae: 1.5616 - val_loss: 6.2185 - val_mse: 6.2185 - val_mae: 2.1743 - lr: 1.5024e-04 - 930ms/epoch - 2ms/step\n",
            "Epoch 5/500\n",
            "556/556 - 1s - loss: 13.4800 - mse: 13.4800 - mae: 1.5790 - val_loss: 7.8406 - val_mse: 7.8406 - val_mae: 2.4729 - lr: 1.5024e-04 - 945ms/epoch - 2ms/step\n",
            "Epoch 6/500\n",
            "556/556 - 1s - loss: 13.0786 - mse: 13.0786 - mae: 1.6049 - val_loss: 8.3406 - val_mse: 8.3406 - val_mae: 2.5466 - lr: 1.5024e-04 - 951ms/epoch - 2ms/step\n",
            "Score for inner fold : loss of 8.340559005737305\n",
            "-----Fold:2--------\n",
            "Epoch 1/500\n",
            "556/556 - 2s - loss: 25.0535 - mse: 25.0535 - mae: 2.5159 - val_loss: 1.4054 - val_mse: 1.4054 - val_mae: 0.8738 - lr: 1.5024e-04 - 2s/epoch - 3ms/step\n",
            "Epoch 2/500\n",
            "556/556 - 1s - loss: 22.2450 - mse: 22.2450 - mae: 2.1937 - val_loss: 1.8180 - val_mse: 1.8180 - val_mae: 1.0593 - lr: 1.5024e-04 - 959ms/epoch - 2ms/step\n",
            "Epoch 3/500\n",
            "556/556 - 1s - loss: 19.6453 - mse: 19.6453 - mae: 1.8243 - val_loss: 3.1621 - val_mse: 3.1621 - val_mae: 1.4913 - lr: 1.5024e-04 - 909ms/epoch - 2ms/step\n",
            "Epoch 4/500\n",
            "556/556 - 1s - loss: 17.4604 - mse: 17.4604 - mae: 1.6014 - val_loss: 5.5874 - val_mse: 5.5874 - val_mae: 2.0754 - lr: 1.5024e-04 - 928ms/epoch - 2ms/step\n",
            "Epoch 5/500\n",
            "556/556 - 1s - loss: 16.1938 - mse: 16.1938 - mae: 1.6025 - val_loss: 7.4846 - val_mse: 7.4846 - val_mae: 2.4374 - lr: 1.5024e-04 - 945ms/epoch - 2ms/step\n",
            "Epoch 6/500\n",
            "556/556 - 1s - loss: 15.7010 - mse: 15.7010 - mae: 1.6387 - val_loss: 8.2285 - val_mse: 8.2285 - val_mae: 2.5507 - lr: 1.5024e-04 - 932ms/epoch - 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-10-10 10:23:43,054]\u001b[0m Finished trial#48 resulted in value: 8.28. Current best value is 8.28 with parameters: {'activation': 'tanh', 'num_hidden_layer': 2, 'i': 4, 'learning_rate': 0.00015023638006021693}.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Score for inner fold : loss of 8.228475570678711\n",
            "Trial Number:49\n",
            "-----Fold:0--------\n",
            "Epoch 1/500\n",
            "556/556 - 2s - loss: 22.0492 - mse: 22.0492 - mae: 2.4264 - val_loss: 1.6078 - val_mse: 1.6078 - val_mae: 0.9514 - lr: 1.3763e-04 - 2s/epoch - 3ms/step\n",
            "Epoch 2/500\n",
            "556/556 - 1s - loss: 18.1436 - mse: 18.1436 - mae: 1.8609 - val_loss: 4.0701 - val_mse: 4.0701 - val_mae: 1.7227 - lr: 1.3763e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 3/500\n",
            "556/556 - 1s - loss: 15.0814 - mse: 15.0814 - mae: 1.6118 - val_loss: 7.6401 - val_mse: 7.6401 - val_mae: 2.4816 - lr: 1.3763e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 4/500\n",
            "556/556 - 1s - loss: 14.1778 - mse: 14.1778 - mae: 1.6329 - val_loss: 8.2020 - val_mse: 8.2020 - val_mae: 2.5516 - lr: 1.3763e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 5/500\n",
            "556/556 - 1s - loss: 13.8127 - mse: 13.8127 - mae: 1.6141 - val_loss: 8.1829 - val_mse: 8.1829 - val_mae: 2.5344 - lr: 1.3763e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 6/500\n",
            "556/556 - 1s - loss: 13.5624 - mse: 13.5624 - mae: 1.5967 - val_loss: 8.4920 - val_mse: 8.4920 - val_mae: 2.5630 - lr: 1.3763e-04 - 1s/epoch - 2ms/step\n",
            "Score for inner fold : loss of 8.491972923278809\n",
            "-----Fold:1--------\n",
            "Epoch 1/500\n",
            "556/556 - 2s - loss: 21.2383 - mse: 21.2383 - mae: 2.5193 - val_loss: 1.6610 - val_mse: 1.6610 - val_mae: 0.9815 - lr: 1.3763e-04 - 2s/epoch - 3ms/step\n",
            "Epoch 2/500\n",
            "556/556 - 1s - loss: 17.1250 - mse: 17.1250 - mae: 1.8996 - val_loss: 4.3688 - val_mse: 4.3688 - val_mae: 1.7763 - lr: 1.3763e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 3/500\n",
            "556/556 - 1s - loss: 14.1855 - mse: 14.1855 - mae: 1.6368 - val_loss: 7.8342 - val_mse: 7.8342 - val_mae: 2.5023 - lr: 1.3763e-04 - 992ms/epoch - 2ms/step\n",
            "Epoch 4/500\n",
            "556/556 - 1s - loss: 13.4741 - mse: 13.4741 - mae: 1.6713 - val_loss: 8.4120 - val_mse: 8.4120 - val_mae: 2.5828 - lr: 1.3763e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 5/500\n",
            "556/556 - 1s - loss: 13.1831 - mse: 13.1831 - mae: 1.6646 - val_loss: 8.3648 - val_mse: 8.3648 - val_mae: 2.5583 - lr: 1.3763e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 6/500\n",
            "556/556 - 1s - loss: 12.9738 - mse: 12.9738 - mae: 1.6442 - val_loss: 8.6619 - val_mse: 8.6619 - val_mae: 2.5900 - lr: 1.3763e-04 - 961ms/epoch - 2ms/step\n",
            "Score for inner fold : loss of 8.661871910095215\n",
            "-----Fold:2--------\n",
            "Epoch 1/500\n",
            "556/556 - 2s - loss: 22.5437 - mse: 22.5437 - mae: 2.3884 - val_loss: 1.9094 - val_mse: 1.9094 - val_mae: 1.0752 - lr: 1.3763e-04 - 2s/epoch - 3ms/step\n",
            "Epoch 2/500\n",
            "556/556 - 1s - loss: 18.3527 - mse: 18.3527 - mae: 1.7827 - val_loss: 5.3362 - val_mse: 5.3362 - val_mae: 2.0025 - lr: 1.3763e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 3/500\n",
            "556/556 - 1s - loss: 15.9084 - mse: 15.9084 - mae: 1.6304 - val_loss: 7.8343 - val_mse: 7.8343 - val_mae: 2.4845 - lr: 1.3763e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 4/500\n",
            "556/556 - 1s - loss: 15.3688 - mse: 15.3688 - mae: 1.6269 - val_loss: 8.1682 - val_mse: 8.1682 - val_mae: 2.5303 - lr: 1.3763e-04 - 985ms/epoch - 2ms/step\n",
            "Epoch 5/500\n",
            "556/556 - 1s - loss: 15.1002 - mse: 15.1002 - mae: 1.6205 - val_loss: 8.3882 - val_mse: 8.3882 - val_mae: 2.5469 - lr: 1.3763e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 6/500\n",
            "556/556 - 1s - loss: 14.9105 - mse: 14.9105 - mae: 1.6082 - val_loss: 8.4136 - val_mse: 8.4136 - val_mae: 2.5412 - lr: 1.3763e-04 - 996ms/epoch - 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-10-10 10:24:07,493]\u001b[0m Finished trial#49 resulted in value: 8.52. Current best value is 8.28 with parameters: {'activation': 'tanh', 'num_hidden_layer': 2, 'i': 4, 'learning_rate': 0.00015023638006021693}.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Score for inner fold : loss of 8.413633346557617\n",
            "Number of finished trial : 50\n",
            "Best Trial Value : 8.28\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "2**6"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5Cj9zSovtv00",
        "outputId": "b1c50665-5c0e-47cb-c401-feacbc24c78e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "64"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#model_retrain for outer cv (best model 1)\n",
        "\n",
        "#  {'activation': 'tanh', 'num_hidden_layer': 2, 'i': 4, 'learning_rate': 0.00015023638006021693}\n",
        "history_list_cv1=[]\n",
        "optimizer = Adam(learning_rate=0.00015023638006021693,clipnorm=1.0)\n",
        "model = create_model(activation=\"tanh\", num_hidden_layer=2, num_hidden_unit=16)\n",
        "model.compile(loss='mse',optimizer=optimizer,metrics=['mse','mae'])\n",
        "kfold = KFold(n_splits=3,shuffle=True)\n",
        "fold_no=1\n",
        "loss_per_fold = []\n",
        "es = EarlyStopping(monitor='mse',mode='min',patience=10)\n",
        "for (train,test) in kfold.split(training,labelsForTrain):\n",
        "\n",
        "  print('------------------------------------------------------------------------')\n",
        "  print(f'Training for Outer fold {fold_no} ...')\n",
        "  \n",
        "  outer_cv_train=training.iloc[train]\n",
        "  outer_cv_trainlabel=labelsForTrain.iloc[train]\n",
        "\n",
        "  outer_cv_test=training.iloc[test]\n",
        "  outer_cv_testlabel=labelsForTrain.iloc[test]\n",
        "\n",
        "  reduce_lr = ReduceLROnPlateau(monitor='val_mse', factor=0.2,\n",
        "                              patience=5, min_lr=0.001)\n",
        "  # Fit data to model\n",
        "  history = model.fit(outer_cv_train, outer_cv_trainlabel,\n",
        "                  batch_size=64,\n",
        "                  epochs=500,\n",
        "                  verbose=2,\n",
        "                  validation_data=(outer_cv_test,outer_cv_testlabel),\n",
        "                  validation_batch_size=64,\n",
        "                  callbacks=[es,reduce_lr])\n",
        "  \n",
        "  scores=model.evaluate(outer_cv_test,outer_cv_testlabel,verbose=0)\n",
        "  print(scores)\n",
        "  print(f'Score for fold {fold_no}: {model.metrics_names[0]} of {scores[0]}')\n",
        "  loss_per_fold.append(scores[0])\n",
        "  # Increase fold number\n",
        "  fold_no = fold_no + 1 \n",
        "history_list_cv1.append(history)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7QOrVd6ss6Ze",
        "outputId": "165e6c76-de30-47ec-b517-c835dffacb59"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "------------------------------------------------------------------------\n",
            "Training for Outer fold 1 ...\n",
            "Epoch 1/500\n",
            "834/834 - 2s - loss: 21.2012 - mse: 21.2012 - mae: 2.4027 - val_loss: 48.1052 - val_mse: 48.1052 - val_mae: 2.1927 - lr: 1.5024e-04 - 2s/epoch - 3ms/step\n",
            "Epoch 2/500\n",
            "834/834 - 1s - loss: 17.5816 - mse: 17.5816 - mae: 1.8639 - val_loss: 44.6163 - val_mse: 44.6163 - val_mae: 1.6841 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 3/500\n",
            "834/834 - 1s - loss: 14.8383 - mse: 14.8383 - mae: 1.5634 - val_loss: 42.6120 - val_mse: 42.6120 - val_mae: 1.6119 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 4/500\n",
            "834/834 - 1s - loss: 13.7971 - mse: 13.7971 - mae: 1.5958 - val_loss: 42.0510 - val_mse: 42.0510 - val_mae: 1.6412 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 5/500\n",
            "834/834 - 1s - loss: 13.4445 - mse: 13.4445 - mae: 1.6026 - val_loss: 41.7488 - val_mse: 41.7488 - val_mae: 1.6392 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 6/500\n",
            "834/834 - 1s - loss: 13.2161 - mse: 13.2161 - mae: 1.5923 - val_loss: 41.5372 - val_mse: 41.5372 - val_mae: 1.6313 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 7/500\n",
            "834/834 - 1s - loss: 13.0273 - mse: 13.0273 - mae: 1.5852 - val_loss: 41.3572 - val_mse: 41.3572 - val_mae: 1.6214 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 8/500\n",
            "834/834 - 1s - loss: 12.8884 - mse: 12.8884 - mae: 1.5766 - val_loss: 41.2287 - val_mse: 41.2287 - val_mae: 1.6155 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 9/500\n",
            "834/834 - 1s - loss: 12.7667 - mse: 12.7667 - mae: 1.5674 - val_loss: 41.0887 - val_mse: 41.0887 - val_mae: 1.6127 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 10/500\n",
            "834/834 - 1s - loss: 12.6558 - mse: 12.6558 - mae: 1.5626 - val_loss: 40.9830 - val_mse: 40.9830 - val_mae: 1.6107 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 11/500\n",
            "834/834 - 1s - loss: 12.5666 - mse: 12.5666 - mae: 1.5588 - val_loss: 40.8757 - val_mse: 40.8757 - val_mae: 1.6019 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 12/500\n",
            "834/834 - 1s - loss: 12.4834 - mse: 12.4834 - mae: 1.5501 - val_loss: 40.7862 - val_mse: 40.7862 - val_mae: 1.5960 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 13/500\n",
            "834/834 - 1s - loss: 12.4053 - mse: 12.4053 - mae: 1.5486 - val_loss: 40.7059 - val_mse: 40.7059 - val_mae: 1.5940 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 14/500\n",
            "834/834 - 1s - loss: 12.3316 - mse: 12.3316 - mae: 1.5455 - val_loss: 40.6380 - val_mse: 40.6380 - val_mae: 1.5959 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 15/500\n",
            "834/834 - 1s - loss: 12.2720 - mse: 12.2720 - mae: 1.5414 - val_loss: 40.5589 - val_mse: 40.5589 - val_mae: 1.5887 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 16/500\n",
            "834/834 - 1s - loss: 12.2050 - mse: 12.2050 - mae: 1.5377 - val_loss: 40.4854 - val_mse: 40.4854 - val_mae: 1.5893 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 17/500\n",
            "834/834 - 1s - loss: 12.1515 - mse: 12.1515 - mae: 1.5350 - val_loss: 40.4195 - val_mse: 40.4195 - val_mae: 1.5875 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 18/500\n",
            "834/834 - 1s - loss: 12.0928 - mse: 12.0928 - mae: 1.5339 - val_loss: 40.3669 - val_mse: 40.3669 - val_mae: 1.5785 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 19/500\n",
            "834/834 - 1s - loss: 12.0431 - mse: 12.0431 - mae: 1.5285 - val_loss: 40.2867 - val_mse: 40.2867 - val_mae: 1.5845 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 20/500\n",
            "834/834 - 1s - loss: 11.9886 - mse: 11.9886 - mae: 1.5293 - val_loss: 40.2364 - val_mse: 40.2364 - val_mae: 1.5745 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 21/500\n",
            "834/834 - 1s - loss: 11.9400 - mse: 11.9400 - mae: 1.5263 - val_loss: 40.1959 - val_mse: 40.1959 - val_mae: 1.5781 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 22/500\n",
            "834/834 - 1s - loss: 11.8948 - mse: 11.8948 - mae: 1.5256 - val_loss: 40.1302 - val_mse: 40.1302 - val_mae: 1.5807 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 23/500\n",
            "834/834 - 1s - loss: 11.8466 - mse: 11.8466 - mae: 1.5267 - val_loss: 40.0891 - val_mse: 40.0891 - val_mae: 1.5669 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 24/500\n",
            "834/834 - 1s - loss: 11.8069 - mse: 11.8069 - mae: 1.5216 - val_loss: 40.0263 - val_mse: 40.0263 - val_mae: 1.5665 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 25/500\n",
            "834/834 - 1s - loss: 11.7629 - mse: 11.7629 - mae: 1.5202 - val_loss: 39.9914 - val_mse: 39.9914 - val_mae: 1.5644 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 26/500\n",
            "834/834 - 1s - loss: 11.7282 - mse: 11.7282 - mae: 1.5162 - val_loss: 39.9370 - val_mse: 39.9370 - val_mae: 1.5664 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 27/500\n",
            "834/834 - 1s - loss: 11.6850 - mse: 11.6850 - mae: 1.5180 - val_loss: 39.8869 - val_mse: 39.8869 - val_mae: 1.5668 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 28/500\n",
            "834/834 - 1s - loss: 11.6490 - mse: 11.6490 - mae: 1.5171 - val_loss: 39.8449 - val_mse: 39.8449 - val_mae: 1.5682 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 29/500\n",
            "834/834 - 1s - loss: 11.6167 - mse: 11.6167 - mae: 1.5143 - val_loss: 39.8100 - val_mse: 39.8100 - val_mae: 1.5667 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 30/500\n",
            "834/834 - 1s - loss: 11.5825 - mse: 11.5825 - mae: 1.5139 - val_loss: 39.7743 - val_mse: 39.7743 - val_mae: 1.5601 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 31/500\n",
            "834/834 - 1s - loss: 11.5496 - mse: 11.5496 - mae: 1.5141 - val_loss: 39.7399 - val_mse: 39.7399 - val_mae: 1.5618 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 32/500\n",
            "834/834 - 1s - loss: 11.5185 - mse: 11.5185 - mae: 1.5107 - val_loss: 39.6915 - val_mse: 39.6915 - val_mae: 1.5627 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 33/500\n",
            "834/834 - 1s - loss: 11.4875 - mse: 11.4875 - mae: 1.5112 - val_loss: 39.6498 - val_mse: 39.6498 - val_mae: 1.5624 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 34/500\n",
            "834/834 - 1s - loss: 11.4549 - mse: 11.4549 - mae: 1.5102 - val_loss: 39.6231 - val_mse: 39.6231 - val_mae: 1.5606 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 35/500\n",
            "834/834 - 1s - loss: 11.4278 - mse: 11.4278 - mae: 1.5081 - val_loss: 39.6018 - val_mse: 39.6018 - val_mae: 1.5623 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 36/500\n",
            "834/834 - 1s - loss: 11.3972 - mse: 11.3972 - mae: 1.5092 - val_loss: 39.5629 - val_mse: 39.5629 - val_mae: 1.5600 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 37/500\n",
            "834/834 - 1s - loss: 11.3773 - mse: 11.3773 - mae: 1.5086 - val_loss: 39.5316 - val_mse: 39.5316 - val_mae: 1.5571 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 38/500\n",
            "834/834 - 1s - loss: 11.3559 - mse: 11.3559 - mae: 1.5039 - val_loss: 39.4902 - val_mse: 39.4902 - val_mae: 1.5607 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 39/500\n",
            "834/834 - 1s - loss: 11.3259 - mse: 11.3259 - mae: 1.5085 - val_loss: 39.4635 - val_mse: 39.4635 - val_mae: 1.5585 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 40/500\n",
            "834/834 - 1s - loss: 11.3066 - mse: 11.3066 - mae: 1.5060 - val_loss: 39.4536 - val_mse: 39.4536 - val_mae: 1.5518 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 41/500\n",
            "834/834 - 1s - loss: 11.2879 - mse: 11.2879 - mae: 1.5049 - val_loss: 39.4109 - val_mse: 39.4109 - val_mae: 1.5581 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 42/500\n",
            "834/834 - 1s - loss: 11.2680 - mse: 11.2680 - mae: 1.5025 - val_loss: 39.3880 - val_mse: 39.3880 - val_mae: 1.5613 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 43/500\n",
            "834/834 - 1s - loss: 11.2408 - mse: 11.2408 - mae: 1.5055 - val_loss: 39.3639 - val_mse: 39.3639 - val_mae: 1.5539 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 44/500\n",
            "834/834 - 1s - loss: 11.2257 - mse: 11.2257 - mae: 1.5047 - val_loss: 39.3494 - val_mse: 39.3494 - val_mae: 1.5527 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 45/500\n",
            "834/834 - 1s - loss: 11.2057 - mse: 11.2057 - mae: 1.5006 - val_loss: 39.3176 - val_mse: 39.3176 - val_mae: 1.5532 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 46/500\n",
            "834/834 - 1s - loss: 11.1858 - mse: 11.1858 - mae: 1.5030 - val_loss: 39.3116 - val_mse: 39.3116 - val_mae: 1.5528 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 47/500\n",
            "834/834 - 1s - loss: 11.1685 - mse: 11.1685 - mae: 1.5038 - val_loss: 39.2872 - val_mse: 39.2872 - val_mae: 1.5557 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 48/500\n",
            "834/834 - 1s - loss: 11.1606 - mse: 11.1606 - mae: 1.4999 - val_loss: 39.2583 - val_mse: 39.2583 - val_mae: 1.5542 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 49/500\n",
            "834/834 - 1s - loss: 11.1393 - mse: 11.1393 - mae: 1.4998 - val_loss: 39.2471 - val_mse: 39.2471 - val_mae: 1.5597 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 50/500\n",
            "834/834 - 1s - loss: 11.1197 - mse: 11.1197 - mae: 1.5029 - val_loss: 39.2254 - val_mse: 39.2254 - val_mae: 1.5501 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 51/500\n",
            "834/834 - 1s - loss: 11.1121 - mse: 11.1121 - mae: 1.5006 - val_loss: 39.2164 - val_mse: 39.2164 - val_mae: 1.5488 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 52/500\n",
            "834/834 - 1s - loss: 11.0889 - mse: 11.0889 - mae: 1.4984 - val_loss: 39.1840 - val_mse: 39.1840 - val_mae: 1.5504 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 53/500\n",
            "834/834 - 1s - loss: 11.0697 - mse: 11.0697 - mae: 1.4994 - val_loss: 39.1761 - val_mse: 39.1761 - val_mae: 1.5475 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 54/500\n",
            "834/834 - 1s - loss: 11.0566 - mse: 11.0566 - mae: 1.4990 - val_loss: 39.1507 - val_mse: 39.1507 - val_mae: 1.5519 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 55/500\n",
            "834/834 - 1s - loss: 11.0501 - mse: 11.0501 - mae: 1.4973 - val_loss: 39.1268 - val_mse: 39.1268 - val_mae: 1.5545 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 56/500\n",
            "834/834 - 1s - loss: 11.0369 - mse: 11.0369 - mae: 1.4979 - val_loss: 39.1246 - val_mse: 39.1246 - val_mae: 1.5454 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 57/500\n",
            "834/834 - 1s - loss: 11.0186 - mse: 11.0186 - mae: 1.4971 - val_loss: 39.1000 - val_mse: 39.1000 - val_mae: 1.5469 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 58/500\n",
            "834/834 - 1s - loss: 11.0063 - mse: 11.0063 - mae: 1.4970 - val_loss: 39.0874 - val_mse: 39.0874 - val_mae: 1.5491 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 59/500\n",
            "834/834 - 1s - loss: 10.9964 - mse: 10.9964 - mae: 1.4960 - val_loss: 39.0668 - val_mse: 39.0668 - val_mae: 1.5463 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 60/500\n",
            "834/834 - 1s - loss: 10.9818 - mse: 10.9818 - mae: 1.4955 - val_loss: 39.0457 - val_mse: 39.0457 - val_mae: 1.5516 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 61/500\n",
            "834/834 - 1s - loss: 10.9675 - mse: 10.9675 - mae: 1.4962 - val_loss: 39.0529 - val_mse: 39.0529 - val_mae: 1.5379 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 62/500\n",
            "834/834 - 1s - loss: 10.9545 - mse: 10.9545 - mae: 1.4948 - val_loss: 39.0269 - val_mse: 39.0269 - val_mae: 1.5406 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 63/500\n",
            "834/834 - 1s - loss: 10.9447 - mse: 10.9447 - mae: 1.4934 - val_loss: 38.9942 - val_mse: 38.9942 - val_mae: 1.5495 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 64/500\n",
            "834/834 - 1s - loss: 10.9337 - mse: 10.9337 - mae: 1.4943 - val_loss: 38.9885 - val_mse: 38.9885 - val_mae: 1.5554 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 65/500\n",
            "834/834 - 1s - loss: 10.9217 - mse: 10.9217 - mae: 1.4962 - val_loss: 38.9751 - val_mse: 38.9751 - val_mae: 1.5483 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 66/500\n",
            "834/834 - 1s - loss: 10.9063 - mse: 10.9063 - mae: 1.4942 - val_loss: 38.9552 - val_mse: 38.9552 - val_mae: 1.5511 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 67/500\n",
            "834/834 - 1s - loss: 10.8996 - mse: 10.8996 - mae: 1.4956 - val_loss: 38.9436 - val_mse: 38.9436 - val_mae: 1.5441 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 68/500\n",
            "834/834 - 1s - loss: 10.8875 - mse: 10.8875 - mae: 1.4943 - val_loss: 38.9383 - val_mse: 38.9383 - val_mae: 1.5414 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 69/500\n",
            "834/834 - 1s - loss: 10.8822 - mse: 10.8822 - mae: 1.4907 - val_loss: 38.9206 - val_mse: 38.9206 - val_mae: 1.5468 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 70/500\n",
            "834/834 - 1s - loss: 10.8615 - mse: 10.8615 - mae: 1.4944 - val_loss: 38.9044 - val_mse: 38.9044 - val_mae: 1.5377 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 71/500\n",
            "834/834 - 1s - loss: 10.8529 - mse: 10.8529 - mae: 1.4921 - val_loss: 38.8894 - val_mse: 38.8894 - val_mae: 1.5436 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 72/500\n",
            "834/834 - 1s - loss: 10.8442 - mse: 10.8442 - mae: 1.4919 - val_loss: 38.8934 - val_mse: 38.8934 - val_mae: 1.5424 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 73/500\n",
            "834/834 - 1s - loss: 10.8334 - mse: 10.8334 - mae: 1.4939 - val_loss: 38.8732 - val_mse: 38.8732 - val_mae: 1.5424 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 74/500\n",
            "834/834 - 1s - loss: 10.8218 - mse: 10.8218 - mae: 1.4929 - val_loss: 38.8914 - val_mse: 38.8914 - val_mae: 1.5367 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 75/500\n",
            "834/834 - 1s - loss: 10.8160 - mse: 10.8160 - mae: 1.4900 - val_loss: 38.8422 - val_mse: 38.8422 - val_mae: 1.5414 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 76/500\n",
            "834/834 - 1s - loss: 10.8002 - mse: 10.8002 - mae: 1.4895 - val_loss: 38.8219 - val_mse: 38.8219 - val_mae: 1.5496 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 77/500\n",
            "834/834 - 1s - loss: 10.7888 - mse: 10.7888 - mae: 1.4941 - val_loss: 38.8298 - val_mse: 38.8298 - val_mae: 1.5394 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 78/500\n",
            "834/834 - 1s - loss: 10.7825 - mse: 10.7825 - mae: 1.4903 - val_loss: 38.8087 - val_mse: 38.8087 - val_mae: 1.5415 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 79/500\n",
            "834/834 - 1s - loss: 10.7756 - mse: 10.7756 - mae: 1.4917 - val_loss: 38.7941 - val_mse: 38.7941 - val_mae: 1.5425 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 80/500\n",
            "834/834 - 1s - loss: 10.7654 - mse: 10.7654 - mae: 1.4922 - val_loss: 38.7946 - val_mse: 38.7946 - val_mae: 1.5490 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 81/500\n",
            "834/834 - 1s - loss: 10.7549 - mse: 10.7549 - mae: 1.4933 - val_loss: 38.7879 - val_mse: 38.7879 - val_mae: 1.5409 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 82/500\n",
            "834/834 - 1s - loss: 10.7473 - mse: 10.7473 - mae: 1.4887 - val_loss: 38.7749 - val_mse: 38.7749 - val_mae: 1.5454 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 83/500\n",
            "834/834 - 1s - loss: 10.7364 - mse: 10.7364 - mae: 1.4899 - val_loss: 38.7548 - val_mse: 38.7548 - val_mae: 1.5369 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 84/500\n",
            "834/834 - 1s - loss: 10.7310 - mse: 10.7310 - mae: 1.4873 - val_loss: 38.7480 - val_mse: 38.7480 - val_mae: 1.5454 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 85/500\n",
            "834/834 - 1s - loss: 10.7181 - mse: 10.7181 - mae: 1.4903 - val_loss: 38.7425 - val_mse: 38.7425 - val_mae: 1.5376 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 86/500\n",
            "834/834 - 1s - loss: 10.7123 - mse: 10.7123 - mae: 1.4880 - val_loss: 38.7380 - val_mse: 38.7380 - val_mae: 1.5412 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 87/500\n",
            "834/834 - 1s - loss: 10.6991 - mse: 10.6991 - mae: 1.4898 - val_loss: 38.7208 - val_mse: 38.7208 - val_mae: 1.5384 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 88/500\n",
            "834/834 - 1s - loss: 10.6916 - mse: 10.6916 - mae: 1.4879 - val_loss: 38.7095 - val_mse: 38.7095 - val_mae: 1.5308 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 89/500\n",
            "834/834 - 1s - loss: 10.6853 - mse: 10.6853 - mae: 1.4877 - val_loss: 38.6903 - val_mse: 38.6903 - val_mae: 1.5376 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 90/500\n",
            "834/834 - 1s - loss: 10.6750 - mse: 10.6750 - mae: 1.4863 - val_loss: 38.6891 - val_mse: 38.6891 - val_mae: 1.5445 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 91/500\n",
            "834/834 - 1s - loss: 10.6651 - mse: 10.6651 - mae: 1.4898 - val_loss: 38.6909 - val_mse: 38.6909 - val_mae: 1.5328 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 92/500\n",
            "834/834 - 1s - loss: 10.6587 - mse: 10.6587 - mae: 1.4861 - val_loss: 38.6575 - val_mse: 38.6575 - val_mae: 1.5426 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 93/500\n",
            "834/834 - 1s - loss: 10.6516 - mse: 10.6516 - mae: 1.4882 - val_loss: 38.6487 - val_mse: 38.6487 - val_mae: 1.5361 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 94/500\n",
            "834/834 - 1s - loss: 10.6412 - mse: 10.6412 - mae: 1.4882 - val_loss: 38.6464 - val_mse: 38.6464 - val_mae: 1.5335 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 95/500\n",
            "834/834 - 1s - loss: 10.6336 - mse: 10.6336 - mae: 1.4861 - val_loss: 38.6331 - val_mse: 38.6331 - val_mae: 1.5418 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 96/500\n",
            "834/834 - 2s - loss: 10.6233 - mse: 10.6233 - mae: 1.4859 - val_loss: 38.6200 - val_mse: 38.6200 - val_mae: 1.5398 - lr: 1.5024e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 97/500\n",
            "834/834 - 1s - loss: 10.6154 - mse: 10.6154 - mae: 1.4858 - val_loss: 38.6300 - val_mse: 38.6300 - val_mae: 1.5274 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 98/500\n",
            "834/834 - 1s - loss: 10.6076 - mse: 10.6076 - mae: 1.4850 - val_loss: 38.6074 - val_mse: 38.6074 - val_mae: 1.5362 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 99/500\n",
            "834/834 - 1s - loss: 10.6035 - mse: 10.6035 - mae: 1.4847 - val_loss: 38.5994 - val_mse: 38.5994 - val_mae: 1.5358 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 100/500\n",
            "834/834 - 1s - loss: 10.5961 - mse: 10.5961 - mae: 1.4846 - val_loss: 38.5931 - val_mse: 38.5931 - val_mae: 1.5297 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 101/500\n",
            "834/834 - 1s - loss: 10.5850 - mse: 10.5850 - mae: 1.4842 - val_loss: 38.5791 - val_mse: 38.5791 - val_mae: 1.5410 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 102/500\n",
            "834/834 - 2s - loss: 10.5754 - mse: 10.5754 - mae: 1.4865 - val_loss: 38.5651 - val_mse: 38.5651 - val_mae: 1.5329 - lr: 1.5024e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 103/500\n",
            "834/834 - 1s - loss: 10.5646 - mse: 10.5646 - mae: 1.4832 - val_loss: 38.5541 - val_mse: 38.5541 - val_mae: 1.5351 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 104/500\n",
            "834/834 - 1s - loss: 10.5581 - mse: 10.5581 - mae: 1.4851 - val_loss: 38.5565 - val_mse: 38.5565 - val_mae: 1.5353 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 105/500\n",
            "834/834 - 1s - loss: 10.5523 - mse: 10.5523 - mae: 1.4821 - val_loss: 38.5405 - val_mse: 38.5405 - val_mae: 1.5398 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 106/500\n",
            "834/834 - 1s - loss: 10.5407 - mse: 10.5407 - mae: 1.4853 - val_loss: 38.5460 - val_mse: 38.5460 - val_mae: 1.5371 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 107/500\n",
            "834/834 - 1s - loss: 10.5399 - mse: 10.5399 - mae: 1.4839 - val_loss: 38.5221 - val_mse: 38.5221 - val_mae: 1.5382 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 108/500\n",
            "834/834 - 1s - loss: 10.5234 - mse: 10.5234 - mae: 1.4854 - val_loss: 38.5229 - val_mse: 38.5229 - val_mae: 1.5389 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 109/500\n",
            "834/834 - 1s - loss: 10.5230 - mse: 10.5230 - mae: 1.4838 - val_loss: 38.5114 - val_mse: 38.5114 - val_mae: 1.5320 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 110/500\n",
            "834/834 - 1s - loss: 10.5140 - mse: 10.5140 - mae: 1.4830 - val_loss: 38.4838 - val_mse: 38.4838 - val_mae: 1.5362 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 111/500\n",
            "834/834 - 1s - loss: 10.5053 - mse: 10.5053 - mae: 1.4808 - val_loss: 38.4781 - val_mse: 38.4781 - val_mae: 1.5446 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 112/500\n",
            "834/834 - 1s - loss: 10.4993 - mse: 10.4993 - mae: 1.4824 - val_loss: 38.4826 - val_mse: 38.4826 - val_mae: 1.5284 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 113/500\n",
            "834/834 - 1s - loss: 10.4921 - mse: 10.4921 - mae: 1.4812 - val_loss: 38.4815 - val_mse: 38.4815 - val_mae: 1.5257 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 114/500\n",
            "834/834 - 1s - loss: 10.4839 - mse: 10.4839 - mae: 1.4832 - val_loss: 38.4576 - val_mse: 38.4576 - val_mae: 1.5289 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 115/500\n",
            "834/834 - 1s - loss: 10.4757 - mse: 10.4757 - mae: 1.4839 - val_loss: 38.4588 - val_mse: 38.4588 - val_mae: 1.5353 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 116/500\n",
            "834/834 - 1s - loss: 10.4657 - mse: 10.4657 - mae: 1.4813 - val_loss: 38.4355 - val_mse: 38.4355 - val_mae: 1.5355 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 117/500\n",
            "834/834 - 1s - loss: 10.4669 - mse: 10.4669 - mae: 1.4829 - val_loss: 38.4340 - val_mse: 38.4340 - val_mae: 1.5301 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 118/500\n",
            "834/834 - 1s - loss: 10.4635 - mse: 10.4635 - mae: 1.4788 - val_loss: 38.4367 - val_mse: 38.4367 - val_mae: 1.5320 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 119/500\n",
            "834/834 - 1s - loss: 10.4495 - mse: 10.4495 - mae: 1.4830 - val_loss: 38.4281 - val_mse: 38.4281 - val_mae: 1.5255 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 120/500\n",
            "834/834 - 1s - loss: 10.4448 - mse: 10.4448 - mae: 1.4810 - val_loss: 38.4123 - val_mse: 38.4123 - val_mae: 1.5333 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 121/500\n",
            "834/834 - 1s - loss: 10.4395 - mse: 10.4395 - mae: 1.4818 - val_loss: 38.4111 - val_mse: 38.4111 - val_mae: 1.5352 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 122/500\n",
            "834/834 - 1s - loss: 10.4331 - mse: 10.4331 - mae: 1.4810 - val_loss: 38.4070 - val_mse: 38.4070 - val_mae: 1.5342 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 123/500\n",
            "834/834 - 1s - loss: 10.4229 - mse: 10.4229 - mae: 1.4804 - val_loss: 38.3969 - val_mse: 38.3969 - val_mae: 1.5388 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 124/500\n",
            "834/834 - 1s - loss: 10.4205 - mse: 10.4205 - mae: 1.4797 - val_loss: 38.3711 - val_mse: 38.3711 - val_mae: 1.5501 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 125/500\n",
            "834/834 - 1s - loss: 10.4086 - mse: 10.4086 - mae: 1.4838 - val_loss: 38.3843 - val_mse: 38.3843 - val_mae: 1.5296 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 126/500\n",
            "834/834 - 1s - loss: 10.4101 - mse: 10.4101 - mae: 1.4786 - val_loss: 38.3837 - val_mse: 38.3837 - val_mae: 1.5304 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 127/500\n",
            "834/834 - 1s - loss: 10.3986 - mse: 10.3986 - mae: 1.4817 - val_loss: 38.3850 - val_mse: 38.3850 - val_mae: 1.5347 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 128/500\n",
            "834/834 - 1s - loss: 10.3953 - mse: 10.3953 - mae: 1.4787 - val_loss: 38.3451 - val_mse: 38.3451 - val_mae: 1.5413 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 129/500\n",
            "834/834 - 1s - loss: 10.3906 - mse: 10.3906 - mae: 1.4794 - val_loss: 38.3413 - val_mse: 38.3413 - val_mae: 1.5317 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 130/500\n",
            "834/834 - 1s - loss: 10.3771 - mse: 10.3771 - mae: 1.4820 - val_loss: 38.3561 - val_mse: 38.3561 - val_mae: 1.5330 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 131/500\n",
            "834/834 - 1s - loss: 10.3769 - mse: 10.3769 - mae: 1.4776 - val_loss: 38.3213 - val_mse: 38.3213 - val_mae: 1.5417 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 132/500\n",
            "834/834 - 1s - loss: 10.3642 - mse: 10.3642 - mae: 1.4818 - val_loss: 38.3258 - val_mse: 38.3258 - val_mae: 1.5312 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 133/500\n",
            "834/834 - 1s - loss: 10.3640 - mse: 10.3640 - mae: 1.4793 - val_loss: 38.3243 - val_mse: 38.3243 - val_mae: 1.5283 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 134/500\n",
            "834/834 - 1s - loss: 10.3531 - mse: 10.3531 - mae: 1.4792 - val_loss: 38.3169 - val_mse: 38.3169 - val_mae: 1.5218 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 135/500\n",
            "834/834 - 1s - loss: 10.3574 - mse: 10.3574 - mae: 1.4784 - val_loss: 38.3065 - val_mse: 38.3065 - val_mae: 1.5240 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 136/500\n",
            "834/834 - 1s - loss: 10.3499 - mse: 10.3499 - mae: 1.4769 - val_loss: 38.3229 - val_mse: 38.3229 - val_mae: 1.5249 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 137/500\n",
            "834/834 - 1s - loss: 10.3415 - mse: 10.3415 - mae: 1.4780 - val_loss: 38.2956 - val_mse: 38.2956 - val_mae: 1.5321 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 138/500\n",
            "834/834 - 1s - loss: 10.3364 - mse: 10.3364 - mae: 1.4801 - val_loss: 38.2754 - val_mse: 38.2754 - val_mae: 1.5342 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 139/500\n",
            "834/834 - 1s - loss: 10.3260 - mse: 10.3260 - mae: 1.4797 - val_loss: 38.2673 - val_mse: 38.2673 - val_mae: 1.5438 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 140/500\n",
            "834/834 - 1s - loss: 10.3231 - mse: 10.3231 - mae: 1.4799 - val_loss: 38.2641 - val_mse: 38.2641 - val_mae: 1.5394 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 141/500\n",
            "834/834 - 1s - loss: 10.3173 - mse: 10.3173 - mae: 1.4795 - val_loss: 38.2646 - val_mse: 38.2646 - val_mae: 1.5258 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 142/500\n",
            "834/834 - 1s - loss: 10.3120 - mse: 10.3120 - mae: 1.4783 - val_loss: 38.2721 - val_mse: 38.2721 - val_mae: 1.5202 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 143/500\n",
            "834/834 - 1s - loss: 10.3104 - mse: 10.3104 - mae: 1.4767 - val_loss: 38.2398 - val_mse: 38.2398 - val_mae: 1.5378 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 144/500\n",
            "834/834 - 1s - loss: 10.3014 - mse: 10.3014 - mae: 1.4802 - val_loss: 38.2635 - val_mse: 38.2635 - val_mae: 1.5233 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 145/500\n",
            "834/834 - 1s - loss: 10.2981 - mse: 10.2981 - mae: 1.4770 - val_loss: 38.2454 - val_mse: 38.2454 - val_mae: 1.5290 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 146/500\n",
            "834/834 - 1s - loss: 10.2899 - mse: 10.2899 - mae: 1.4785 - val_loss: 38.2357 - val_mse: 38.2357 - val_mae: 1.5217 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 147/500\n",
            "834/834 - 1s - loss: 10.2881 - mse: 10.2881 - mae: 1.4752 - val_loss: 38.2263 - val_mse: 38.2263 - val_mae: 1.5381 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 148/500\n",
            "834/834 - 1s - loss: 10.2869 - mse: 10.2869 - mae: 1.4805 - val_loss: 38.2269 - val_mse: 38.2269 - val_mae: 1.5287 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 149/500\n",
            "834/834 - 1s - loss: 10.2746 - mse: 10.2746 - mae: 1.4776 - val_loss: 38.2100 - val_mse: 38.2100 - val_mae: 1.5229 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 150/500\n",
            "834/834 - 1s - loss: 10.2708 - mse: 10.2708 - mae: 1.4747 - val_loss: 38.2155 - val_mse: 38.2155 - val_mae: 1.5334 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 151/500\n",
            "834/834 - 1s - loss: 10.2610 - mse: 10.2610 - mae: 1.4775 - val_loss: 38.2065 - val_mse: 38.2065 - val_mae: 1.5345 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 152/500\n",
            "834/834 - 1s - loss: 10.2657 - mse: 10.2657 - mae: 1.4780 - val_loss: 38.1894 - val_mse: 38.1894 - val_mae: 1.5321 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 153/500\n",
            "834/834 - 1s - loss: 10.2575 - mse: 10.2575 - mae: 1.4764 - val_loss: 38.1960 - val_mse: 38.1960 - val_mae: 1.5309 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 154/500\n",
            "834/834 - 1s - loss: 10.2515 - mse: 10.2515 - mae: 1.4774 - val_loss: 38.1777 - val_mse: 38.1777 - val_mae: 1.5406 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 155/500\n",
            "834/834 - 1s - loss: 10.2441 - mse: 10.2441 - mae: 1.4794 - val_loss: 38.1793 - val_mse: 38.1793 - val_mae: 1.5207 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 156/500\n",
            "834/834 - 1s - loss: 10.2411 - mse: 10.2411 - mae: 1.4787 - val_loss: 38.1757 - val_mse: 38.1757 - val_mae: 1.5353 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 157/500\n",
            "834/834 - 1s - loss: 10.2435 - mse: 10.2435 - mae: 1.4740 - val_loss: 38.1613 - val_mse: 38.1613 - val_mae: 1.5393 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 158/500\n",
            "834/834 - 1s - loss: 10.2339 - mse: 10.2339 - mae: 1.4767 - val_loss: 38.1688 - val_mse: 38.1688 - val_mae: 1.5321 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 159/500\n",
            "834/834 - 1s - loss: 10.2263 - mse: 10.2263 - mae: 1.4763 - val_loss: 38.1578 - val_mse: 38.1578 - val_mae: 1.5340 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 160/500\n",
            "834/834 - 1s - loss: 10.2214 - mse: 10.2214 - mae: 1.4759 - val_loss: 38.1745 - val_mse: 38.1745 - val_mae: 1.5232 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 161/500\n",
            "834/834 - 1s - loss: 10.2139 - mse: 10.2139 - mae: 1.4753 - val_loss: 38.1588 - val_mse: 38.1588 - val_mae: 1.5234 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 162/500\n",
            "834/834 - 1s - loss: 10.2144 - mse: 10.2144 - mae: 1.4754 - val_loss: 38.1411 - val_mse: 38.1411 - val_mae: 1.5160 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 163/500\n",
            "834/834 - 1s - loss: 10.2086 - mse: 10.2086 - mae: 1.4748 - val_loss: 38.1401 - val_mse: 38.1401 - val_mae: 1.5263 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 164/500\n",
            "834/834 - 1s - loss: 10.2008 - mse: 10.2008 - mae: 1.4766 - val_loss: 38.1332 - val_mse: 38.1332 - val_mae: 1.5259 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 165/500\n",
            "834/834 - 1s - loss: 10.1982 - mse: 10.1982 - mae: 1.4748 - val_loss: 38.1342 - val_mse: 38.1342 - val_mae: 1.5264 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 166/500\n",
            "834/834 - 1s - loss: 10.1922 - mse: 10.1922 - mae: 1.4755 - val_loss: 38.1172 - val_mse: 38.1172 - val_mae: 1.5365 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 167/500\n",
            "834/834 - 1s - loss: 10.1896 - mse: 10.1896 - mae: 1.4776 - val_loss: 38.1179 - val_mse: 38.1179 - val_mae: 1.5241 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 168/500\n",
            "834/834 - 1s - loss: 10.1890 - mse: 10.1890 - mae: 1.4747 - val_loss: 38.1069 - val_mse: 38.1069 - val_mae: 1.5386 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 169/500\n",
            "834/834 - 1s - loss: 10.1811 - mse: 10.1811 - mae: 1.4777 - val_loss: 38.1076 - val_mse: 38.1076 - val_mae: 1.5230 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 170/500\n",
            "834/834 - 1s - loss: 10.1715 - mse: 10.1715 - mae: 1.4752 - val_loss: 38.0781 - val_mse: 38.0781 - val_mae: 1.5271 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 171/500\n",
            "834/834 - 1s - loss: 10.1728 - mse: 10.1728 - mae: 1.4743 - val_loss: 38.1054 - val_mse: 38.1054 - val_mae: 1.5247 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 172/500\n",
            "834/834 - 1s - loss: 10.1644 - mse: 10.1644 - mae: 1.4749 - val_loss: 38.1016 - val_mse: 38.1016 - val_mae: 1.5293 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 173/500\n",
            "834/834 - 1s - loss: 10.1596 - mse: 10.1596 - mae: 1.4748 - val_loss: 38.0974 - val_mse: 38.0974 - val_mae: 1.5295 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 174/500\n",
            "834/834 - 1s - loss: 10.1558 - mse: 10.1558 - mae: 1.4746 - val_loss: 38.0709 - val_mse: 38.0709 - val_mae: 1.5229 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 175/500\n",
            "834/834 - 1s - loss: 10.1551 - mse: 10.1551 - mae: 1.4763 - val_loss: 38.0611 - val_mse: 38.0611 - val_mae: 1.5273 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 176/500\n",
            "834/834 - 1s - loss: 10.1521 - mse: 10.1521 - mae: 1.4734 - val_loss: 38.0687 - val_mse: 38.0687 - val_mae: 1.5242 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 177/500\n",
            "834/834 - 1s - loss: 10.1450 - mse: 10.1450 - mae: 1.4761 - val_loss: 38.0605 - val_mse: 38.0605 - val_mae: 1.5249 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 178/500\n",
            "834/834 - 1s - loss: 10.1468 - mse: 10.1468 - mae: 1.4745 - val_loss: 38.0459 - val_mse: 38.0459 - val_mae: 1.5244 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 179/500\n",
            "834/834 - 1s - loss: 10.1346 - mse: 10.1346 - mae: 1.4733 - val_loss: 38.0364 - val_mse: 38.0364 - val_mae: 1.5368 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 180/500\n",
            "834/834 - 1s - loss: 10.1260 - mse: 10.1260 - mae: 1.4769 - val_loss: 38.0416 - val_mse: 38.0416 - val_mae: 1.5275 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 181/500\n",
            "834/834 - 1s - loss: 10.1305 - mse: 10.1305 - mae: 1.4737 - val_loss: 38.0324 - val_mse: 38.0324 - val_mae: 1.5248 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 182/500\n",
            "834/834 - 1s - loss: 10.1232 - mse: 10.1232 - mae: 1.4766 - val_loss: 38.0436 - val_mse: 38.0436 - val_mae: 1.5180 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 183/500\n",
            "834/834 - 1s - loss: 10.1226 - mse: 10.1226 - mae: 1.4737 - val_loss: 38.0237 - val_mse: 38.0237 - val_mae: 1.5287 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 184/500\n",
            "834/834 - 1s - loss: 10.1195 - mse: 10.1195 - mae: 1.4745 - val_loss: 38.0350 - val_mse: 38.0350 - val_mae: 1.5256 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 185/500\n",
            "834/834 - 1s - loss: 10.1126 - mse: 10.1126 - mae: 1.4740 - val_loss: 38.0319 - val_mse: 38.0319 - val_mae: 1.5238 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 186/500\n",
            "834/834 - 1s - loss: 10.1110 - mse: 10.1110 - mae: 1.4739 - val_loss: 38.0093 - val_mse: 38.0093 - val_mae: 1.5172 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 187/500\n",
            "834/834 - 1s - loss: 10.1054 - mse: 10.1054 - mae: 1.4743 - val_loss: 38.0003 - val_mse: 38.0003 - val_mae: 1.5334 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 188/500\n",
            "834/834 - 1s - loss: 10.0985 - mse: 10.0985 - mae: 1.4765 - val_loss: 38.0100 - val_mse: 38.0100 - val_mae: 1.5193 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 189/500\n",
            "834/834 - 1s - loss: 10.0983 - mse: 10.0983 - mae: 1.4749 - val_loss: 37.9997 - val_mse: 37.9997 - val_mae: 1.5287 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 190/500\n",
            "834/834 - 1s - loss: 10.0924 - mse: 10.0924 - mae: 1.4734 - val_loss: 38.0010 - val_mse: 38.0010 - val_mae: 1.5233 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 191/500\n",
            "834/834 - 1s - loss: 10.0851 - mse: 10.0851 - mae: 1.4735 - val_loss: 37.9759 - val_mse: 37.9759 - val_mae: 1.5314 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 192/500\n",
            "834/834 - 1s - loss: 10.0845 - mse: 10.0845 - mae: 1.4743 - val_loss: 37.9929 - val_mse: 37.9929 - val_mae: 1.5310 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 193/500\n",
            "834/834 - 1s - loss: 10.0758 - mse: 10.0758 - mae: 1.4749 - val_loss: 37.9979 - val_mse: 37.9979 - val_mae: 1.5228 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 194/500\n",
            "834/834 - 1s - loss: 10.0808 - mse: 10.0808 - mae: 1.4725 - val_loss: 37.9843 - val_mse: 37.9843 - val_mae: 1.5219 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 195/500\n",
            "834/834 - 1s - loss: 10.0780 - mse: 10.0780 - mae: 1.4746 - val_loss: 37.9743 - val_mse: 37.9743 - val_mae: 1.5193 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 196/500\n",
            "834/834 - 1s - loss: 10.0747 - mse: 10.0747 - mae: 1.4731 - val_loss: 37.9962 - val_mse: 37.9962 - val_mae: 1.5243 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 197/500\n",
            "834/834 - 1s - loss: 10.0669 - mse: 10.0669 - mae: 1.4732 - val_loss: 37.9608 - val_mse: 37.9608 - val_mae: 1.5274 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 198/500\n",
            "834/834 - 1s - loss: 10.0631 - mse: 10.0631 - mae: 1.4724 - val_loss: 37.9739 - val_mse: 37.9739 - val_mae: 1.5259 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 199/500\n",
            "834/834 - 1s - loss: 10.0690 - mse: 10.0690 - mae: 1.4715 - val_loss: 37.9559 - val_mse: 37.9559 - val_mae: 1.5279 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 200/500\n",
            "834/834 - 1s - loss: 10.0608 - mse: 10.0608 - mae: 1.4709 - val_loss: 37.9502 - val_mse: 37.9502 - val_mae: 1.5410 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 201/500\n",
            "834/834 - 1s - loss: 10.0535 - mse: 10.0535 - mae: 1.4731 - val_loss: 37.9559 - val_mse: 37.9559 - val_mae: 1.5227 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 202/500\n",
            "834/834 - 1s - loss: 10.0477 - mse: 10.0477 - mae: 1.4727 - val_loss: 37.9561 - val_mse: 37.9561 - val_mae: 1.5184 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 203/500\n",
            "834/834 - 1s - loss: 10.0462 - mse: 10.0462 - mae: 1.4711 - val_loss: 37.9326 - val_mse: 37.9326 - val_mae: 1.5351 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 204/500\n",
            "834/834 - 1s - loss: 10.0434 - mse: 10.0434 - mae: 1.4727 - val_loss: 37.9350 - val_mse: 37.9350 - val_mae: 1.5261 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 205/500\n",
            "834/834 - 1s - loss: 10.0341 - mse: 10.0341 - mae: 1.4732 - val_loss: 37.9327 - val_mse: 37.9327 - val_mae: 1.5219 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 206/500\n",
            "834/834 - 1s - loss: 10.0312 - mse: 10.0312 - mae: 1.4726 - val_loss: 37.9347 - val_mse: 37.9347 - val_mae: 1.5275 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 207/500\n",
            "834/834 - 1s - loss: 10.0332 - mse: 10.0332 - mae: 1.4738 - val_loss: 37.9267 - val_mse: 37.9267 - val_mae: 1.5220 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 208/500\n",
            "834/834 - 1s - loss: 10.0294 - mse: 10.0294 - mae: 1.4716 - val_loss: 37.9271 - val_mse: 37.9271 - val_mae: 1.5207 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 209/500\n",
            "834/834 - 1s - loss: 10.0245 - mse: 10.0245 - mae: 1.4717 - val_loss: 37.8980 - val_mse: 37.8980 - val_mae: 1.5229 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 210/500\n",
            "834/834 - 1s - loss: 10.0179 - mse: 10.0179 - mae: 1.4736 - val_loss: 37.9126 - val_mse: 37.9126 - val_mae: 1.5218 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 211/500\n",
            "834/834 - 1s - loss: 10.0192 - mse: 10.0192 - mae: 1.4731 - val_loss: 37.9281 - val_mse: 37.9281 - val_mae: 1.5169 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 212/500\n",
            "834/834 - 1s - loss: 10.0150 - mse: 10.0150 - mae: 1.4700 - val_loss: 37.9083 - val_mse: 37.9083 - val_mae: 1.5219 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 213/500\n",
            "834/834 - 1s - loss: 10.0104 - mse: 10.0104 - mae: 1.4728 - val_loss: 37.9103 - val_mse: 37.9103 - val_mae: 1.5131 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 214/500\n",
            "834/834 - 1s - loss: 10.0077 - mse: 10.0077 - mae: 1.4718 - val_loss: 37.9142 - val_mse: 37.9142 - val_mae: 1.5166 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 215/500\n",
            "834/834 - 1s - loss: 10.0031 - mse: 10.0031 - mae: 1.4707 - val_loss: 37.8777 - val_mse: 37.8777 - val_mae: 1.5223 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 216/500\n",
            "834/834 - 1s - loss: 10.0004 - mse: 10.0004 - mae: 1.4709 - val_loss: 37.8786 - val_mse: 37.8786 - val_mae: 1.5354 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 217/500\n",
            "834/834 - 1s - loss: 10.0013 - mse: 10.0013 - mae: 1.4723 - val_loss: 37.8801 - val_mse: 37.8801 - val_mae: 1.5314 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 218/500\n",
            "834/834 - 1s - loss: 9.9927 - mse: 9.9927 - mae: 1.4720 - val_loss: 37.8918 - val_mse: 37.8918 - val_mae: 1.5184 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 219/500\n",
            "834/834 - 1s - loss: 9.9929 - mse: 9.9929 - mae: 1.4708 - val_loss: 37.8653 - val_mse: 37.8653 - val_mae: 1.5332 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 220/500\n",
            "834/834 - 1s - loss: 9.9906 - mse: 9.9906 - mae: 1.4705 - val_loss: 37.8798 - val_mse: 37.8798 - val_mae: 1.5182 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 221/500\n",
            "834/834 - 1s - loss: 9.9805 - mse: 9.9805 - mae: 1.4729 - val_loss: 37.8612 - val_mse: 37.8612 - val_mae: 1.5177 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 222/500\n",
            "834/834 - 1s - loss: 9.9850 - mse: 9.9850 - mae: 1.4713 - val_loss: 37.8456 - val_mse: 37.8456 - val_mae: 1.5254 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 223/500\n",
            "834/834 - 1s - loss: 9.9819 - mse: 9.9819 - mae: 1.4712 - val_loss: 37.8591 - val_mse: 37.8591 - val_mae: 1.5196 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 224/500\n",
            "834/834 - 1s - loss: 9.9780 - mse: 9.9780 - mae: 1.4712 - val_loss: 37.8578 - val_mse: 37.8578 - val_mae: 1.5153 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 225/500\n",
            "834/834 - 1s - loss: 9.9687 - mse: 9.9687 - mae: 1.4704 - val_loss: 37.8471 - val_mse: 37.8471 - val_mae: 1.5256 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 226/500\n",
            "834/834 - 1s - loss: 9.9680 - mse: 9.9680 - mae: 1.4725 - val_loss: 37.8434 - val_mse: 37.8434 - val_mae: 1.5129 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 227/500\n",
            "834/834 - 1s - loss: 9.9572 - mse: 9.9572 - mae: 1.4680 - val_loss: 37.8757 - val_mse: 37.8757 - val_mae: 1.5024 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 228/500\n",
            "834/834 - 1s - loss: 9.9618 - mse: 9.9618 - mae: 1.4703 - val_loss: 37.8244 - val_mse: 37.8244 - val_mae: 1.5221 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 229/500\n",
            "834/834 - 1s - loss: 9.9643 - mse: 9.9643 - mae: 1.4713 - val_loss: 37.8387 - val_mse: 37.8387 - val_mae: 1.5145 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 230/500\n",
            "834/834 - 1s - loss: 9.9618 - mse: 9.9618 - mae: 1.4700 - val_loss: 37.8183 - val_mse: 37.8183 - val_mae: 1.5190 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 231/500\n",
            "834/834 - 1s - loss: 9.9502 - mse: 9.9502 - mae: 1.4708 - val_loss: 37.8131 - val_mse: 37.8131 - val_mae: 1.5160 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 232/500\n",
            "834/834 - 1s - loss: 9.9478 - mse: 9.9478 - mae: 1.4696 - val_loss: 37.8064 - val_mse: 37.8064 - val_mae: 1.5201 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 233/500\n",
            "834/834 - 1s - loss: 9.9425 - mse: 9.9425 - mae: 1.4705 - val_loss: 37.8220 - val_mse: 37.8220 - val_mae: 1.5124 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 234/500\n",
            "834/834 - 1s - loss: 9.9393 - mse: 9.9393 - mae: 1.4694 - val_loss: 37.8146 - val_mse: 37.8146 - val_mae: 1.5253 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 235/500\n",
            "834/834 - 1s - loss: 9.9319 - mse: 9.9319 - mae: 1.4706 - val_loss: 37.8121 - val_mse: 37.8121 - val_mae: 1.5161 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 236/500\n",
            "834/834 - 1s - loss: 9.9338 - mse: 9.9338 - mae: 1.4704 - val_loss: 37.7866 - val_mse: 37.7866 - val_mae: 1.5271 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 237/500\n",
            "834/834 - 1s - loss: 9.9311 - mse: 9.9311 - mae: 1.4693 - val_loss: 37.8081 - val_mse: 37.8081 - val_mae: 1.5145 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 238/500\n",
            "834/834 - 1s - loss: 9.9287 - mse: 9.9287 - mae: 1.4686 - val_loss: 37.8052 - val_mse: 37.8052 - val_mae: 1.5290 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 239/500\n",
            "834/834 - 1s - loss: 9.9226 - mse: 9.9226 - mae: 1.4702 - val_loss: 37.8138 - val_mse: 37.8138 - val_mae: 1.5288 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 240/500\n",
            "834/834 - 1s - loss: 9.9261 - mse: 9.9261 - mae: 1.4716 - val_loss: 37.7963 - val_mse: 37.7963 - val_mae: 1.5238 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 241/500\n",
            "834/834 - 1s - loss: 9.9278 - mse: 9.9278 - mae: 1.4687 - val_loss: 37.7888 - val_mse: 37.7888 - val_mae: 1.5141 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 242/500\n",
            "834/834 - 1s - loss: 9.9209 - mse: 9.9209 - mae: 1.4701 - val_loss: 37.7837 - val_mse: 37.7837 - val_mae: 1.5259 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 243/500\n",
            "834/834 - 1s - loss: 9.9201 - mse: 9.9201 - mae: 1.4705 - val_loss: 37.7805 - val_mse: 37.7805 - val_mae: 1.5208 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 244/500\n",
            "834/834 - 1s - loss: 9.9111 - mse: 9.9111 - mae: 1.4678 - val_loss: 37.7787 - val_mse: 37.7787 - val_mae: 1.5192 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 245/500\n",
            "834/834 - 1s - loss: 9.9075 - mse: 9.9075 - mae: 1.4701 - val_loss: 37.7993 - val_mse: 37.7993 - val_mae: 1.5179 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 246/500\n",
            "834/834 - 1s - loss: 9.9091 - mse: 9.9091 - mae: 1.4694 - val_loss: 37.7627 - val_mse: 37.7627 - val_mae: 1.5267 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 247/500\n",
            "834/834 - 1s - loss: 9.9036 - mse: 9.9036 - mae: 1.4708 - val_loss: 37.7737 - val_mse: 37.7737 - val_mae: 1.5092 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 248/500\n",
            "834/834 - 1s - loss: 9.9022 - mse: 9.9022 - mae: 1.4700 - val_loss: 37.7618 - val_mse: 37.7618 - val_mae: 1.5294 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 249/500\n",
            "834/834 - 1s - loss: 9.8933 - mse: 9.8933 - mae: 1.4710 - val_loss: 37.7706 - val_mse: 37.7706 - val_mae: 1.5145 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 250/500\n",
            "834/834 - 1s - loss: 9.8999 - mse: 9.8999 - mae: 1.4685 - val_loss: 37.7523 - val_mse: 37.7523 - val_mae: 1.5293 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 251/500\n",
            "834/834 - 1s - loss: 9.8896 - mse: 9.8896 - mae: 1.4713 - val_loss: 37.7470 - val_mse: 37.7470 - val_mae: 1.5181 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 252/500\n",
            "834/834 - 1s - loss: 9.8885 - mse: 9.8885 - mae: 1.4696 - val_loss: 37.7460 - val_mse: 37.7460 - val_mae: 1.5110 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 253/500\n",
            "834/834 - 1s - loss: 9.8810 - mse: 9.8810 - mae: 1.4660 - val_loss: 37.7486 - val_mse: 37.7486 - val_mae: 1.5146 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 254/500\n",
            "834/834 - 1s - loss: 9.8791 - mse: 9.8791 - mae: 1.4677 - val_loss: 37.7496 - val_mse: 37.7496 - val_mae: 1.5204 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 255/500\n",
            "834/834 - 1s - loss: 9.8857 - mse: 9.8857 - mae: 1.4690 - val_loss: 37.7352 - val_mse: 37.7352 - val_mae: 1.5187 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 256/500\n",
            "834/834 - 1s - loss: 9.8756 - mse: 9.8756 - mae: 1.4710 - val_loss: 37.7528 - val_mse: 37.7528 - val_mae: 1.5175 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 257/500\n",
            "834/834 - 1s - loss: 9.8869 - mse: 9.8869 - mae: 1.4698 - val_loss: 37.7391 - val_mse: 37.7391 - val_mae: 1.5147 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 258/500\n",
            "834/834 - 1s - loss: 9.8811 - mse: 9.8811 - mae: 1.4670 - val_loss: 37.7276 - val_mse: 37.7276 - val_mae: 1.5238 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 259/500\n",
            "834/834 - 1s - loss: 9.8705 - mse: 9.8705 - mae: 1.4723 - val_loss: 37.7434 - val_mse: 37.7434 - val_mae: 1.5081 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 260/500\n",
            "834/834 - 1s - loss: 9.8755 - mse: 9.8755 - mae: 1.4674 - val_loss: 37.7171 - val_mse: 37.7171 - val_mae: 1.5276 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 261/500\n",
            "834/834 - 1s - loss: 9.8718 - mse: 9.8718 - mae: 1.4680 - val_loss: 37.7475 - val_mse: 37.7475 - val_mae: 1.5216 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 262/500\n",
            "834/834 - 1s - loss: 9.8708 - mse: 9.8708 - mae: 1.4667 - val_loss: 37.7265 - val_mse: 37.7265 - val_mae: 1.5206 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 263/500\n",
            "834/834 - 1s - loss: 9.8646 - mse: 9.8646 - mae: 1.4670 - val_loss: 37.7273 - val_mse: 37.7273 - val_mae: 1.5131 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 264/500\n",
            "834/834 - 1s - loss: 9.8606 - mse: 9.8606 - mae: 1.4671 - val_loss: 37.7306 - val_mse: 37.7306 - val_mae: 1.5089 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 265/500\n",
            "834/834 - 1s - loss: 9.8707 - mse: 9.8707 - mae: 1.4669 - val_loss: 37.7096 - val_mse: 37.7096 - val_mae: 1.5302 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 266/500\n",
            "834/834 - 1s - loss: 9.8484 - mse: 9.8484 - mae: 1.4699 - val_loss: 37.7129 - val_mse: 37.7129 - val_mae: 1.5185 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 267/500\n",
            "834/834 - 1s - loss: 9.8568 - mse: 9.8568 - mae: 1.4671 - val_loss: 37.7041 - val_mse: 37.7041 - val_mae: 1.5196 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 268/500\n",
            "834/834 - 1s - loss: 9.8575 - mse: 9.8575 - mae: 1.4683 - val_loss: 37.7137 - val_mse: 37.7137 - val_mae: 1.5249 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 269/500\n",
            "834/834 - 1s - loss: 9.8445 - mse: 9.8445 - mae: 1.4700 - val_loss: 37.7039 - val_mse: 37.7039 - val_mae: 1.5214 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 270/500\n",
            "834/834 - 1s - loss: 9.8499 - mse: 9.8499 - mae: 1.4655 - val_loss: 37.6968 - val_mse: 37.6968 - val_mae: 1.5369 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 271/500\n",
            "834/834 - 1s - loss: 9.8429 - mse: 9.8429 - mae: 1.4713 - val_loss: 37.7137 - val_mse: 37.7137 - val_mae: 1.5197 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 272/500\n",
            "834/834 - 1s - loss: 9.8387 - mse: 9.8387 - mae: 1.4670 - val_loss: 37.6855 - val_mse: 37.6855 - val_mae: 1.5222 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 273/500\n",
            "834/834 - 1s - loss: 9.8434 - mse: 9.8434 - mae: 1.4701 - val_loss: 37.6945 - val_mse: 37.6945 - val_mae: 1.5080 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 274/500\n",
            "834/834 - 1s - loss: 9.8364 - mse: 9.8364 - mae: 1.4684 - val_loss: 37.6722 - val_mse: 37.6722 - val_mae: 1.5155 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 275/500\n",
            "834/834 - 1s - loss: 9.8381 - mse: 9.8381 - mae: 1.4681 - val_loss: 37.6680 - val_mse: 37.6680 - val_mae: 1.5125 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 276/500\n",
            "834/834 - 1s - loss: 9.8303 - mse: 9.8303 - mae: 1.4682 - val_loss: 37.6643 - val_mse: 37.6643 - val_mae: 1.5187 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 277/500\n",
            "834/834 - 1s - loss: 9.8326 - mse: 9.8326 - mae: 1.4640 - val_loss: 37.6547 - val_mse: 37.6547 - val_mae: 1.5204 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 278/500\n",
            "834/834 - 1s - loss: 9.8331 - mse: 9.8331 - mae: 1.4646 - val_loss: 37.6588 - val_mse: 37.6588 - val_mae: 1.5210 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 279/500\n",
            "834/834 - 1s - loss: 9.8229 - mse: 9.8229 - mae: 1.4665 - val_loss: 37.6668 - val_mse: 37.6668 - val_mae: 1.5237 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 280/500\n",
            "834/834 - 1s - loss: 9.8299 - mse: 9.8299 - mae: 1.4681 - val_loss: 37.6677 - val_mse: 37.6677 - val_mae: 1.5205 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 281/500\n",
            "834/834 - 1s - loss: 9.8264 - mse: 9.8264 - mae: 1.4683 - val_loss: 37.6713 - val_mse: 37.6713 - val_mae: 1.5115 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 282/500\n",
            "834/834 - 1s - loss: 9.8171 - mse: 9.8171 - mae: 1.4689 - val_loss: 37.6630 - val_mse: 37.6630 - val_mae: 1.5119 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 283/500\n",
            "834/834 - 1s - loss: 9.8200 - mse: 9.8200 - mae: 1.4646 - val_loss: 37.6407 - val_mse: 37.6407 - val_mae: 1.5300 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 284/500\n",
            "834/834 - 1s - loss: 9.8115 - mse: 9.8115 - mae: 1.4683 - val_loss: 37.6520 - val_mse: 37.6520 - val_mae: 1.5153 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 285/500\n",
            "834/834 - 1s - loss: 9.8165 - mse: 9.8165 - mae: 1.4667 - val_loss: 37.6746 - val_mse: 37.6746 - val_mae: 1.5231 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 286/500\n",
            "834/834 - 1s - loss: 9.8191 - mse: 9.8191 - mae: 1.4661 - val_loss: 37.6446 - val_mse: 37.6446 - val_mae: 1.5202 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 287/500\n",
            "834/834 - 1s - loss: 9.8108 - mse: 9.8108 - mae: 1.4689 - val_loss: 37.6459 - val_mse: 37.6459 - val_mae: 1.5102 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 288/500\n",
            "834/834 - 1s - loss: 9.8121 - mse: 9.8121 - mae: 1.4657 - val_loss: 37.6417 - val_mse: 37.6417 - val_mae: 1.5197 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 289/500\n",
            "834/834 - 1s - loss: 9.8036 - mse: 9.8036 - mae: 1.4652 - val_loss: 37.6505 - val_mse: 37.6505 - val_mae: 1.5214 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 290/500\n",
            "834/834 - 1s - loss: 9.8096 - mse: 9.8096 - mae: 1.4652 - val_loss: 37.6362 - val_mse: 37.6362 - val_mae: 1.5117 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 291/500\n",
            "834/834 - 1s - loss: 9.8005 - mse: 9.8005 - mae: 1.4672 - val_loss: 37.6686 - val_mse: 37.6686 - val_mae: 1.5088 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 292/500\n",
            "834/834 - 1s - loss: 9.8054 - mse: 9.8054 - mae: 1.4657 - val_loss: 37.6216 - val_mse: 37.6216 - val_mae: 1.5188 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 293/500\n",
            "834/834 - 1s - loss: 9.7941 - mse: 9.7941 - mae: 1.4681 - val_loss: 37.6305 - val_mse: 37.6305 - val_mae: 1.5005 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 294/500\n",
            "834/834 - 1s - loss: 9.7886 - mse: 9.7886 - mae: 1.4646 - val_loss: 37.6149 - val_mse: 37.6149 - val_mae: 1.5317 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 295/500\n",
            "834/834 - 1s - loss: 9.7870 - mse: 9.7870 - mae: 1.4710 - val_loss: 37.6270 - val_mse: 37.6270 - val_mae: 1.5172 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 296/500\n",
            "834/834 - 1s - loss: 9.7958 - mse: 9.7958 - mae: 1.4679 - val_loss: 37.6321 - val_mse: 37.6321 - val_mae: 1.5147 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 297/500\n",
            "834/834 - 1s - loss: 9.7919 - mse: 9.7919 - mae: 1.4676 - val_loss: 37.6227 - val_mse: 37.6227 - val_mae: 1.5215 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 298/500\n",
            "834/834 - 1s - loss: 9.7831 - mse: 9.7831 - mae: 1.4655 - val_loss: 37.6187 - val_mse: 37.6187 - val_mae: 1.5236 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 299/500\n",
            "834/834 - 1s - loss: 9.7864 - mse: 9.7864 - mae: 1.4666 - val_loss: 37.6064 - val_mse: 37.6064 - val_mae: 1.5160 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 300/500\n",
            "834/834 - 1s - loss: 9.7854 - mse: 9.7854 - mae: 1.4649 - val_loss: 37.6141 - val_mse: 37.6141 - val_mae: 1.5138 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 301/500\n",
            "834/834 - 1s - loss: 9.7816 - mse: 9.7816 - mae: 1.4667 - val_loss: 37.6103 - val_mse: 37.6103 - val_mae: 1.5229 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 302/500\n",
            "834/834 - 1s - loss: 9.7810 - mse: 9.7810 - mae: 1.4677 - val_loss: 37.6188 - val_mse: 37.6188 - val_mae: 1.5013 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 303/500\n",
            "834/834 - 1s - loss: 9.7715 - mse: 9.7715 - mae: 1.4660 - val_loss: 37.6192 - val_mse: 37.6192 - val_mae: 1.5009 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 304/500\n",
            "834/834 - 1s - loss: 9.7801 - mse: 9.7801 - mae: 1.4669 - val_loss: 37.6172 - val_mse: 37.6172 - val_mae: 1.5176 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 305/500\n",
            "834/834 - 1s - loss: 9.7738 - mse: 9.7738 - mae: 1.4662 - val_loss: 37.5931 - val_mse: 37.5931 - val_mae: 1.5191 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 306/500\n",
            "834/834 - 1s - loss: 9.7700 - mse: 9.7700 - mae: 1.4655 - val_loss: 37.6003 - val_mse: 37.6003 - val_mae: 1.5066 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 307/500\n",
            "834/834 - 1s - loss: 9.7698 - mse: 9.7698 - mae: 1.4654 - val_loss: 37.5890 - val_mse: 37.5890 - val_mae: 1.5207 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 308/500\n",
            "834/834 - 1s - loss: 9.7662 - mse: 9.7662 - mae: 1.4674 - val_loss: 37.6101 - val_mse: 37.6101 - val_mae: 1.5091 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 309/500\n",
            "834/834 - 1s - loss: 9.7732 - mse: 9.7732 - mae: 1.4640 - val_loss: 37.6013 - val_mse: 37.6013 - val_mae: 1.5081 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 310/500\n",
            "834/834 - 1s - loss: 9.7642 - mse: 9.7642 - mae: 1.4638 - val_loss: 37.6164 - val_mse: 37.6164 - val_mae: 1.5099 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 311/500\n",
            "834/834 - 1s - loss: 9.7702 - mse: 9.7702 - mae: 1.4620 - val_loss: 37.6037 - val_mse: 37.6037 - val_mae: 1.5158 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 312/500\n",
            "834/834 - 1s - loss: 9.7657 - mse: 9.7657 - mae: 1.4655 - val_loss: 37.5815 - val_mse: 37.5815 - val_mae: 1.5089 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 313/500\n",
            "834/834 - 2s - loss: 9.7593 - mse: 9.7593 - mae: 1.4661 - val_loss: 37.5839 - val_mse: 37.5839 - val_mae: 1.5094 - lr: 1.5024e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 314/500\n",
            "834/834 - 1s - loss: 9.7556 - mse: 9.7556 - mae: 1.4635 - val_loss: 37.5870 - val_mse: 37.5870 - val_mae: 1.5141 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 315/500\n",
            "834/834 - 1s - loss: 9.7613 - mse: 9.7613 - mae: 1.4647 - val_loss: 37.5695 - val_mse: 37.5695 - val_mae: 1.5287 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 316/500\n",
            "834/834 - 1s - loss: 9.7621 - mse: 9.7621 - mae: 1.4665 - val_loss: 37.5800 - val_mse: 37.5800 - val_mae: 1.5230 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 317/500\n",
            "834/834 - 1s - loss: 9.7555 - mse: 9.7555 - mae: 1.4673 - val_loss: 37.5771 - val_mse: 37.5771 - val_mae: 1.5182 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 318/500\n",
            "834/834 - 1s - loss: 9.7512 - mse: 9.7512 - mae: 1.4637 - val_loss: 37.5725 - val_mse: 37.5725 - val_mae: 1.5146 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 319/500\n",
            "834/834 - 1s - loss: 9.7596 - mse: 9.7596 - mae: 1.4651 - val_loss: 37.5576 - val_mse: 37.5576 - val_mae: 1.5180 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 320/500\n",
            "834/834 - 1s - loss: 9.7558 - mse: 9.7558 - mae: 1.4680 - val_loss: 37.5752 - val_mse: 37.5752 - val_mae: 1.5133 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 321/500\n",
            "834/834 - 1s - loss: 9.7458 - mse: 9.7458 - mae: 1.4652 - val_loss: 37.5623 - val_mse: 37.5623 - val_mae: 1.5162 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 322/500\n",
            "834/834 - 1s - loss: 9.7440 - mse: 9.7440 - mae: 1.4641 - val_loss: 37.5538 - val_mse: 37.5538 - val_mae: 1.5179 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 323/500\n",
            "834/834 - 1s - loss: 9.7422 - mse: 9.7422 - mae: 1.4654 - val_loss: 37.5610 - val_mse: 37.5610 - val_mae: 1.5130 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 324/500\n",
            "834/834 - 1s - loss: 9.7434 - mse: 9.7434 - mae: 1.4636 - val_loss: 37.5577 - val_mse: 37.5577 - val_mae: 1.5113 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 325/500\n",
            "834/834 - 1s - loss: 9.7462 - mse: 9.7462 - mae: 1.4635 - val_loss: 37.5790 - val_mse: 37.5790 - val_mae: 1.5024 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 326/500\n",
            "834/834 - 1s - loss: 9.7453 - mse: 9.7453 - mae: 1.4652 - val_loss: 37.5487 - val_mse: 37.5487 - val_mae: 1.5092 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 327/500\n",
            "834/834 - 1s - loss: 9.7364 - mse: 9.7364 - mae: 1.4644 - val_loss: 37.5343 - val_mse: 37.5343 - val_mae: 1.5179 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 328/500\n",
            "834/834 - 1s - loss: 9.7361 - mse: 9.7361 - mae: 1.4662 - val_loss: 37.5264 - val_mse: 37.5264 - val_mae: 1.5179 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 329/500\n",
            "834/834 - 1s - loss: 9.7337 - mse: 9.7337 - mae: 1.4672 - val_loss: 37.5549 - val_mse: 37.5549 - val_mae: 1.5078 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 330/500\n",
            "834/834 - 1s - loss: 9.7349 - mse: 9.7349 - mae: 1.4648 - val_loss: 37.5729 - val_mse: 37.5729 - val_mae: 1.5090 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 331/500\n",
            "834/834 - 1s - loss: 9.7299 - mse: 9.7299 - mae: 1.4649 - val_loss: 37.5359 - val_mse: 37.5359 - val_mae: 1.5152 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 332/500\n",
            "834/834 - 1s - loss: 9.7240 - mse: 9.7240 - mae: 1.4656 - val_loss: 37.5508 - val_mse: 37.5508 - val_mae: 1.5131 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 333/500\n",
            "834/834 - 1s - loss: 9.7295 - mse: 9.7295 - mae: 1.4643 - val_loss: 37.5502 - val_mse: 37.5502 - val_mae: 1.5022 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 334/500\n",
            "834/834 - 1s - loss: 9.7281 - mse: 9.7281 - mae: 1.4636 - val_loss: 37.5618 - val_mse: 37.5618 - val_mae: 1.5080 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 335/500\n",
            "834/834 - 1s - loss: 9.7235 - mse: 9.7235 - mae: 1.4630 - val_loss: 37.5312 - val_mse: 37.5312 - val_mae: 1.5170 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 336/500\n",
            "834/834 - 1s - loss: 9.7181 - mse: 9.7181 - mae: 1.4649 - val_loss: 37.5491 - val_mse: 37.5491 - val_mae: 1.5030 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 337/500\n",
            "834/834 - 1s - loss: 9.7266 - mse: 9.7266 - mae: 1.4624 - val_loss: 37.5426 - val_mse: 37.5426 - val_mae: 1.5126 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 338/500\n",
            "834/834 - 1s - loss: 9.7270 - mse: 9.7270 - mae: 1.4620 - val_loss: 37.5258 - val_mse: 37.5258 - val_mae: 1.5022 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 339/500\n",
            "834/834 - 1s - loss: 9.7154 - mse: 9.7154 - mae: 1.4631 - val_loss: 37.5377 - val_mse: 37.5377 - val_mae: 1.5124 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 340/500\n",
            "834/834 - 1s - loss: 9.7213 - mse: 9.7213 - mae: 1.4648 - val_loss: 37.5263 - val_mse: 37.5263 - val_mae: 1.5238 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 341/500\n",
            "834/834 - 1s - loss: 9.7107 - mse: 9.7107 - mae: 1.4630 - val_loss: 37.5315 - val_mse: 37.5315 - val_mae: 1.5201 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 342/500\n",
            "834/834 - 1s - loss: 9.7148 - mse: 9.7148 - mae: 1.4641 - val_loss: 37.5203 - val_mse: 37.5203 - val_mae: 1.5183 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 343/500\n",
            "834/834 - 1s - loss: 9.7088 - mse: 9.7088 - mae: 1.4633 - val_loss: 37.5152 - val_mse: 37.5152 - val_mae: 1.5159 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 344/500\n",
            "834/834 - 1s - loss: 9.7044 - mse: 9.7044 - mae: 1.4664 - val_loss: 37.5255 - val_mse: 37.5255 - val_mae: 1.5029 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 345/500\n",
            "834/834 - 1s - loss: 9.7120 - mse: 9.7120 - mae: 1.4604 - val_loss: 37.5170 - val_mse: 37.5170 - val_mae: 1.5146 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 346/500\n",
            "834/834 - 1s - loss: 9.7136 - mse: 9.7136 - mae: 1.4645 - val_loss: 37.5048 - val_mse: 37.5048 - val_mae: 1.5294 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 347/500\n",
            "834/834 - 1s - loss: 9.7096 - mse: 9.7096 - mae: 1.4649 - val_loss: 37.5095 - val_mse: 37.5095 - val_mae: 1.5057 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 348/500\n",
            "834/834 - 1s - loss: 9.6967 - mse: 9.6967 - mae: 1.4656 - val_loss: 37.5120 - val_mse: 37.5120 - val_mae: 1.5069 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 349/500\n",
            "834/834 - 1s - loss: 9.7072 - mse: 9.7072 - mae: 1.4619 - val_loss: 37.4916 - val_mse: 37.4916 - val_mae: 1.5222 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 350/500\n",
            "834/834 - 1s - loss: 9.7038 - mse: 9.7038 - mae: 1.4667 - val_loss: 37.5182 - val_mse: 37.5182 - val_mae: 1.4988 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 351/500\n",
            "834/834 - 1s - loss: 9.6956 - mse: 9.6956 - mae: 1.4621 - val_loss: 37.5447 - val_mse: 37.5447 - val_mae: 1.5047 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 352/500\n",
            "834/834 - 1s - loss: 9.7014 - mse: 9.7014 - mae: 1.4633 - val_loss: 37.4964 - val_mse: 37.4964 - val_mae: 1.5211 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 353/500\n",
            "834/834 - 1s - loss: 9.6861 - mse: 9.6861 - mae: 1.4668 - val_loss: 37.4804 - val_mse: 37.4804 - val_mae: 1.5184 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 354/500\n",
            "834/834 - 1s - loss: 9.6921 - mse: 9.6921 - mae: 1.4650 - val_loss: 37.5187 - val_mse: 37.5187 - val_mae: 1.5077 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 355/500\n",
            "834/834 - 1s - loss: 9.6900 - mse: 9.6900 - mae: 1.4647 - val_loss: 37.4878 - val_mse: 37.4878 - val_mae: 1.5139 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 356/500\n",
            "834/834 - 1s - loss: 9.6820 - mse: 9.6820 - mae: 1.4637 - val_loss: 37.4647 - val_mse: 37.4647 - val_mae: 1.5205 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 357/500\n",
            "834/834 - 1s - loss: 9.6852 - mse: 9.6852 - mae: 1.4640 - val_loss: 37.4799 - val_mse: 37.4799 - val_mae: 1.5172 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 358/500\n",
            "834/834 - 1s - loss: 9.6886 - mse: 9.6886 - mae: 1.4628 - val_loss: 37.4922 - val_mse: 37.4922 - val_mae: 1.5111 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 359/500\n",
            "834/834 - 1s - loss: 9.6910 - mse: 9.6910 - mae: 1.4617 - val_loss: 37.4840 - val_mse: 37.4840 - val_mae: 1.5030 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 360/500\n",
            "834/834 - 1s - loss: 9.6836 - mse: 9.6836 - mae: 1.4631 - val_loss: 37.4976 - val_mse: 37.4976 - val_mae: 1.4958 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 361/500\n",
            "834/834 - 1s - loss: 9.6857 - mse: 9.6857 - mae: 1.4642 - val_loss: 37.4672 - val_mse: 37.4672 - val_mae: 1.5186 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 362/500\n",
            "834/834 - 1s - loss: 9.6839 - mse: 9.6839 - mae: 1.4646 - val_loss: 37.4745 - val_mse: 37.4745 - val_mae: 1.5049 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 363/500\n",
            "834/834 - 1s - loss: 9.6820 - mse: 9.6820 - mae: 1.4618 - val_loss: 37.4648 - val_mse: 37.4648 - val_mae: 1.5199 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 364/500\n",
            "834/834 - 1s - loss: 9.6766 - mse: 9.6766 - mae: 1.4641 - val_loss: 37.4802 - val_mse: 37.4802 - val_mae: 1.5068 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 365/500\n",
            "834/834 - 1s - loss: 9.6851 - mse: 9.6851 - mae: 1.4629 - val_loss: 37.4901 - val_mse: 37.4901 - val_mae: 1.5168 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 366/500\n",
            "834/834 - 1s - loss: 9.6734 - mse: 9.6734 - mae: 1.4641 - val_loss: 37.4629 - val_mse: 37.4629 - val_mae: 1.5117 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 367/500\n",
            "834/834 - 1s - loss: 9.6750 - mse: 9.6750 - mae: 1.4635 - val_loss: 37.4504 - val_mse: 37.4504 - val_mae: 1.5175 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 368/500\n",
            "834/834 - 1s - loss: 9.6723 - mse: 9.6723 - mae: 1.4639 - val_loss: 37.4659 - val_mse: 37.4659 - val_mae: 1.5068 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 369/500\n",
            "834/834 - 1s - loss: 9.6684 - mse: 9.6684 - mae: 1.4623 - val_loss: 37.4650 - val_mse: 37.4650 - val_mae: 1.5128 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 370/500\n",
            "834/834 - 1s - loss: 9.6748 - mse: 9.6748 - mae: 1.4637 - val_loss: 37.4547 - val_mse: 37.4547 - val_mae: 1.5090 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 371/500\n",
            "834/834 - 1s - loss: 9.6702 - mse: 9.6702 - mae: 1.4628 - val_loss: 37.4553 - val_mse: 37.4553 - val_mae: 1.5232 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 372/500\n",
            "834/834 - 1s - loss: 9.6687 - mse: 9.6687 - mae: 1.4644 - val_loss: 37.4503 - val_mse: 37.4503 - val_mae: 1.5087 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 373/500\n",
            "834/834 - 1s - loss: 9.6656 - mse: 9.6656 - mae: 1.4622 - val_loss: 37.4613 - val_mse: 37.4613 - val_mae: 1.5078 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 374/500\n",
            "834/834 - 1s - loss: 9.6651 - mse: 9.6651 - mae: 1.4614 - val_loss: 37.4628 - val_mse: 37.4628 - val_mae: 1.5061 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 375/500\n",
            "834/834 - 1s - loss: 9.6584 - mse: 9.6584 - mae: 1.4622 - val_loss: 37.4669 - val_mse: 37.4669 - val_mae: 1.5115 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 376/500\n",
            "834/834 - 1s - loss: 9.6597 - mse: 9.6597 - mae: 1.4619 - val_loss: 37.4498 - val_mse: 37.4498 - val_mae: 1.5138 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 377/500\n",
            "834/834 - 1s - loss: 9.6537 - mse: 9.6537 - mae: 1.4647 - val_loss: 37.4591 - val_mse: 37.4591 - val_mae: 1.5022 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 378/500\n",
            "834/834 - 1s - loss: 9.6528 - mse: 9.6528 - mae: 1.4648 - val_loss: 37.4464 - val_mse: 37.4464 - val_mae: 1.5233 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 379/500\n",
            "834/834 - 1s - loss: 9.6556 - mse: 9.6556 - mae: 1.4643 - val_loss: 37.4426 - val_mse: 37.4426 - val_mae: 1.5117 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 380/500\n",
            "834/834 - 1s - loss: 9.6470 - mse: 9.6470 - mae: 1.4632 - val_loss: 37.4403 - val_mse: 37.4403 - val_mae: 1.5118 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 381/500\n",
            "834/834 - 1s - loss: 9.6485 - mse: 9.6485 - mae: 1.4642 - val_loss: 37.4602 - val_mse: 37.4602 - val_mae: 1.4967 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 382/500\n",
            "834/834 - 1s - loss: 9.6444 - mse: 9.6444 - mae: 1.4629 - val_loss: 37.4671 - val_mse: 37.4671 - val_mae: 1.5090 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 383/500\n",
            "834/834 - 1s - loss: 9.6547 - mse: 9.6547 - mae: 1.4614 - val_loss: 37.4487 - val_mse: 37.4487 - val_mae: 1.5150 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 384/500\n",
            "834/834 - 1s - loss: 9.6509 - mse: 9.6509 - mae: 1.4632 - val_loss: 37.4422 - val_mse: 37.4422 - val_mae: 1.5053 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 385/500\n",
            "834/834 - 1s - loss: 9.6560 - mse: 9.6560 - mae: 1.4601 - val_loss: 37.4386 - val_mse: 37.4386 - val_mae: 1.5038 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 386/500\n",
            "834/834 - 1s - loss: 9.6439 - mse: 9.6439 - mae: 1.4633 - val_loss: 37.4226 - val_mse: 37.4226 - val_mae: 1.5162 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 387/500\n",
            "834/834 - 1s - loss: 9.6422 - mse: 9.6422 - mae: 1.4637 - val_loss: 37.4258 - val_mse: 37.4258 - val_mae: 1.5032 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 388/500\n",
            "834/834 - 1s - loss: 9.6476 - mse: 9.6476 - mae: 1.4605 - val_loss: 37.4341 - val_mse: 37.4341 - val_mae: 1.5127 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 389/500\n",
            "834/834 - 1s - loss: 9.6408 - mse: 9.6408 - mae: 1.4634 - val_loss: 37.4305 - val_mse: 37.4305 - val_mae: 1.5240 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 390/500\n",
            "834/834 - 1s - loss: 9.6434 - mse: 9.6434 - mae: 1.4645 - val_loss: 37.4492 - val_mse: 37.4492 - val_mae: 1.5010 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 391/500\n",
            "834/834 - 1s - loss: 9.6402 - mse: 9.6402 - mae: 1.4610 - val_loss: 37.4726 - val_mse: 37.4726 - val_mae: 1.5124 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 392/500\n",
            "834/834 - 1s - loss: 9.6447 - mse: 9.6447 - mae: 1.4613 - val_loss: 37.4421 - val_mse: 37.4421 - val_mae: 1.5071 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 393/500\n",
            "834/834 - 1s - loss: 9.6295 - mse: 9.6295 - mae: 1.4625 - val_loss: 37.4267 - val_mse: 37.4267 - val_mae: 1.5068 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 394/500\n",
            "834/834 - 1s - loss: 9.6381 - mse: 9.6381 - mae: 1.4615 - val_loss: 37.4271 - val_mse: 37.4271 - val_mae: 1.5039 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 395/500\n",
            "834/834 - 1s - loss: 9.6278 - mse: 9.6278 - mae: 1.4618 - val_loss: 37.4084 - val_mse: 37.4084 - val_mae: 1.5200 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 396/500\n",
            "834/834 - 1s - loss: 9.6326 - mse: 9.6326 - mae: 1.4621 - val_loss: 37.4225 - val_mse: 37.4225 - val_mae: 1.5109 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 397/500\n",
            "834/834 - 1s - loss: 9.6266 - mse: 9.6266 - mae: 1.4628 - val_loss: 37.4152 - val_mse: 37.4152 - val_mae: 1.5071 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 398/500\n",
            "834/834 - 1s - loss: 9.6272 - mse: 9.6272 - mae: 1.4636 - val_loss: 37.4034 - val_mse: 37.4034 - val_mae: 1.5096 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 399/500\n",
            "834/834 - 1s - loss: 9.6308 - mse: 9.6308 - mae: 1.4592 - val_loss: 37.4206 - val_mse: 37.4206 - val_mae: 1.5285 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 400/500\n",
            "834/834 - 1s - loss: 9.6256 - mse: 9.6256 - mae: 1.4631 - val_loss: 37.4003 - val_mse: 37.4003 - val_mae: 1.5048 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 401/500\n",
            "834/834 - 1s - loss: 9.6231 - mse: 9.6231 - mae: 1.4614 - val_loss: 37.4072 - val_mse: 37.4072 - val_mae: 1.5140 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 402/500\n",
            "834/834 - 1s - loss: 9.6203 - mse: 9.6203 - mae: 1.4623 - val_loss: 37.4001 - val_mse: 37.4001 - val_mae: 1.5158 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 403/500\n",
            "834/834 - 1s - loss: 9.6250 - mse: 9.6250 - mae: 1.4626 - val_loss: 37.4072 - val_mse: 37.4072 - val_mae: 1.5017 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 404/500\n",
            "834/834 - 1s - loss: 9.6238 - mse: 9.6238 - mae: 1.4621 - val_loss: 37.4092 - val_mse: 37.4092 - val_mae: 1.5029 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 405/500\n",
            "834/834 - 1s - loss: 9.6194 - mse: 9.6194 - mae: 1.4620 - val_loss: 37.4097 - val_mse: 37.4097 - val_mae: 1.4977 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 406/500\n",
            "834/834 - 1s - loss: 9.6208 - mse: 9.6208 - mae: 1.4609 - val_loss: 37.3921 - val_mse: 37.3921 - val_mae: 1.5061 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 407/500\n",
            "834/834 - 1s - loss: 9.6161 - mse: 9.6161 - mae: 1.4604 - val_loss: 37.3930 - val_mse: 37.3930 - val_mae: 1.5069 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 408/500\n",
            "834/834 - 1s - loss: 9.6174 - mse: 9.6174 - mae: 1.4641 - val_loss: 37.3836 - val_mse: 37.3836 - val_mae: 1.5031 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 409/500\n",
            "834/834 - 1s - loss: 9.6065 - mse: 9.6065 - mae: 1.4616 - val_loss: 37.3932 - val_mse: 37.3932 - val_mae: 1.5083 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 410/500\n",
            "834/834 - 1s - loss: 9.6197 - mse: 9.6197 - mae: 1.4592 - val_loss: 37.3898 - val_mse: 37.3898 - val_mae: 1.5073 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 411/500\n",
            "834/834 - 1s - loss: 9.6118 - mse: 9.6118 - mae: 1.4623 - val_loss: 37.3766 - val_mse: 37.3766 - val_mae: 1.5076 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 412/500\n",
            "834/834 - 1s - loss: 9.6063 - mse: 9.6063 - mae: 1.4629 - val_loss: 37.3766 - val_mse: 37.3766 - val_mae: 1.5106 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 413/500\n",
            "834/834 - 1s - loss: 9.6055 - mse: 9.6055 - mae: 1.4626 - val_loss: 37.3903 - val_mse: 37.3903 - val_mae: 1.5108 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 414/500\n",
            "834/834 - 1s - loss: 9.6089 - mse: 9.6089 - mae: 1.4583 - val_loss: 37.4335 - val_mse: 37.4335 - val_mae: 1.5140 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 415/500\n",
            "834/834 - 1s - loss: 9.6086 - mse: 9.6086 - mae: 1.4612 - val_loss: 37.3741 - val_mse: 37.3741 - val_mae: 1.5088 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 416/500\n",
            "834/834 - 1s - loss: 9.6064 - mse: 9.6064 - mae: 1.4645 - val_loss: 37.3715 - val_mse: 37.3715 - val_mae: 1.5037 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 417/500\n",
            "834/834 - 1s - loss: 9.6012 - mse: 9.6012 - mae: 1.4611 - val_loss: 37.3978 - val_mse: 37.3978 - val_mae: 1.5071 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 418/500\n",
            "834/834 - 1s - loss: 9.6090 - mse: 9.6090 - mae: 1.4623 - val_loss: 37.3908 - val_mse: 37.3908 - val_mae: 1.4974 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 419/500\n",
            "834/834 - 1s - loss: 9.6024 - mse: 9.6024 - mae: 1.4648 - val_loss: 37.3753 - val_mse: 37.3753 - val_mae: 1.4968 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 420/500\n",
            "834/834 - 1s - loss: 9.5969 - mse: 9.5969 - mae: 1.4597 - val_loss: 37.3721 - val_mse: 37.3721 - val_mae: 1.4995 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 421/500\n",
            "834/834 - 1s - loss: 9.5976 - mse: 9.5976 - mae: 1.4628 - val_loss: 37.3783 - val_mse: 37.3783 - val_mae: 1.5097 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 422/500\n",
            "834/834 - 1s - loss: 9.5972 - mse: 9.5972 - mae: 1.4621 - val_loss: 37.3727 - val_mse: 37.3727 - val_mae: 1.5096 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 423/500\n",
            "834/834 - 1s - loss: 9.6031 - mse: 9.6031 - mae: 1.4598 - val_loss: 37.3742 - val_mse: 37.3742 - val_mae: 1.5088 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 424/500\n",
            "834/834 - 1s - loss: 9.5902 - mse: 9.5902 - mae: 1.4622 - val_loss: 37.3796 - val_mse: 37.3796 - val_mae: 1.5021 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 425/500\n",
            "834/834 - 1s - loss: 9.5990 - mse: 9.5990 - mae: 1.4607 - val_loss: 37.3784 - val_mse: 37.3784 - val_mae: 1.5054 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 426/500\n",
            "834/834 - 1s - loss: 9.5797 - mse: 9.5797 - mae: 1.4617 - val_loss: 37.3721 - val_mse: 37.3721 - val_mae: 1.5141 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 427/500\n",
            "834/834 - 1s - loss: 9.5908 - mse: 9.5908 - mae: 1.4645 - val_loss: 37.3708 - val_mse: 37.3708 - val_mae: 1.4978 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 428/500\n",
            "834/834 - 1s - loss: 9.5954 - mse: 9.5954 - mae: 1.4607 - val_loss: 37.3689 - val_mse: 37.3689 - val_mae: 1.5067 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 429/500\n",
            "834/834 - 1s - loss: 9.5962 - mse: 9.5962 - mae: 1.4616 - val_loss: 37.3597 - val_mse: 37.3597 - val_mae: 1.5187 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 430/500\n",
            "834/834 - 1s - loss: 9.5969 - mse: 9.5969 - mae: 1.4631 - val_loss: 37.3624 - val_mse: 37.3624 - val_mae: 1.5014 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 431/500\n",
            "834/834 - 1s - loss: 9.5907 - mse: 9.5907 - mae: 1.4637 - val_loss: 37.3389 - val_mse: 37.3389 - val_mae: 1.5089 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 432/500\n",
            "834/834 - 1s - loss: 9.5829 - mse: 9.5829 - mae: 1.4615 - val_loss: 37.3435 - val_mse: 37.3435 - val_mae: 1.5248 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 433/500\n",
            "834/834 - 1s - loss: 9.5889 - mse: 9.5889 - mae: 1.4642 - val_loss: 37.3578 - val_mse: 37.3578 - val_mae: 1.5126 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 434/500\n",
            "834/834 - 1s - loss: 9.5902 - mse: 9.5902 - mae: 1.4613 - val_loss: 37.3719 - val_mse: 37.3719 - val_mae: 1.5014 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 435/500\n",
            "834/834 - 1s - loss: 9.5974 - mse: 9.5974 - mae: 1.4597 - val_loss: 37.3645 - val_mse: 37.3645 - val_mae: 1.5163 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 436/500\n",
            "834/834 - 1s - loss: 9.5821 - mse: 9.5821 - mae: 1.4637 - val_loss: 37.3592 - val_mse: 37.3592 - val_mae: 1.5107 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "[37.359153747558594, 37.359153747558594, 1.5106968879699707]\n",
            "Score for fold 1: loss of 37.359153747558594\n",
            "------------------------------------------------------------------------\n",
            "Training for Outer fold 2 ...\n",
            "Epoch 1/500\n",
            "834/834 - 1s - loss: 23.1559 - mse: 23.1559 - mae: 1.4816 - val_loss: 10.1962 - val_mse: 10.1962 - val_mae: 1.4719 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 2/500\n",
            "834/834 - 1s - loss: 23.1510 - mse: 23.1510 - mae: 1.4806 - val_loss: 10.1890 - val_mse: 10.1890 - val_mae: 1.4761 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 3/500\n",
            "834/834 - 1s - loss: 23.1404 - mse: 23.1404 - mae: 1.4809 - val_loss: 10.2038 - val_mse: 10.2038 - val_mae: 1.4596 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 4/500\n",
            "834/834 - 1s - loss: 23.1336 - mse: 23.1336 - mae: 1.4798 - val_loss: 10.2102 - val_mse: 10.2102 - val_mae: 1.4566 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 5/500\n",
            "834/834 - 1s - loss: 23.1371 - mse: 23.1371 - mae: 1.4785 - val_loss: 10.2157 - val_mse: 10.2157 - val_mae: 1.4708 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 6/500\n",
            "834/834 - 1s - loss: 23.1321 - mse: 23.1321 - mae: 1.4776 - val_loss: 10.2124 - val_mse: 10.2124 - val_mae: 1.4651 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 7/500\n",
            "834/834 - 1s - loss: 23.1276 - mse: 23.1276 - mae: 1.4789 - val_loss: 10.2146 - val_mse: 10.2146 - val_mae: 1.4628 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 8/500\n",
            "834/834 - 1s - loss: 23.1114 - mse: 23.1114 - mae: 1.4795 - val_loss: 10.2302 - val_mse: 10.2302 - val_mae: 1.4632 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 9/500\n",
            "834/834 - 1s - loss: 23.1170 - mse: 23.1170 - mae: 1.4795 - val_loss: 10.2156 - val_mse: 10.2156 - val_mae: 1.4722 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 10/500\n",
            "834/834 - 1s - loss: 23.1095 - mse: 23.1095 - mae: 1.4819 - val_loss: 10.2226 - val_mse: 10.2226 - val_mae: 1.4662 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 11/500\n",
            "834/834 - 1s - loss: 23.1083 - mse: 23.1083 - mae: 1.4799 - val_loss: 10.2257 - val_mse: 10.2257 - val_mae: 1.4519 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 12/500\n",
            "834/834 - 1s - loss: 23.0950 - mse: 23.0950 - mae: 1.4795 - val_loss: 10.2384 - val_mse: 10.2384 - val_mae: 1.4588 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 13/500\n",
            "834/834 - 1s - loss: 23.0945 - mse: 23.0945 - mae: 1.4778 - val_loss: 10.2176 - val_mse: 10.2176 - val_mae: 1.4517 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 14/500\n",
            "834/834 - 1s - loss: 23.0904 - mse: 23.0904 - mae: 1.4774 - val_loss: 10.2141 - val_mse: 10.2141 - val_mae: 1.4620 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 15/500\n",
            "834/834 - 1s - loss: 23.0896 - mse: 23.0896 - mae: 1.4779 - val_loss: 10.2157 - val_mse: 10.2157 - val_mae: 1.4609 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 16/500\n",
            "834/834 - 1s - loss: 23.0929 - mse: 23.0929 - mae: 1.4774 - val_loss: 10.2072 - val_mse: 10.2072 - val_mae: 1.4560 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 17/500\n",
            "834/834 - 1s - loss: 23.0838 - mse: 23.0838 - mae: 1.4765 - val_loss: 10.2135 - val_mse: 10.2135 - val_mae: 1.4684 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 18/500\n",
            "834/834 - 1s - loss: 23.0878 - mse: 23.0878 - mae: 1.4779 - val_loss: 10.2247 - val_mse: 10.2247 - val_mae: 1.4704 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 19/500\n",
            "834/834 - 1s - loss: 23.0824 - mse: 23.0824 - mae: 1.4790 - val_loss: 10.2202 - val_mse: 10.2202 - val_mae: 1.4682 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 20/500\n",
            "834/834 - 1s - loss: 23.0767 - mse: 23.0767 - mae: 1.4786 - val_loss: 10.2244 - val_mse: 10.2244 - val_mae: 1.4649 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 21/500\n",
            "834/834 - 1s - loss: 23.0716 - mse: 23.0716 - mae: 1.4810 - val_loss: 10.2204 - val_mse: 10.2204 - val_mae: 1.4648 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 22/500\n",
            "834/834 - 1s - loss: 23.0784 - mse: 23.0784 - mae: 1.4753 - val_loss: 10.2112 - val_mse: 10.2112 - val_mae: 1.4714 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 23/500\n",
            "834/834 - 1s - loss: 23.0787 - mse: 23.0787 - mae: 1.4788 - val_loss: 10.2223 - val_mse: 10.2223 - val_mae: 1.4733 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 24/500\n",
            "834/834 - 1s - loss: 23.0695 - mse: 23.0695 - mae: 1.4771 - val_loss: 10.2076 - val_mse: 10.2076 - val_mae: 1.4608 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 25/500\n",
            "834/834 - 1s - loss: 23.0625 - mse: 23.0625 - mae: 1.4747 - val_loss: 10.2138 - val_mse: 10.2138 - val_mae: 1.4760 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 26/500\n",
            "834/834 - 1s - loss: 23.0660 - mse: 23.0660 - mae: 1.4802 - val_loss: 10.2212 - val_mse: 10.2212 - val_mae: 1.4612 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 27/500\n",
            "834/834 - 1s - loss: 23.0642 - mse: 23.0642 - mae: 1.4750 - val_loss: 10.2068 - val_mse: 10.2068 - val_mae: 1.4689 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 28/500\n",
            "834/834 - 1s - loss: 23.0514 - mse: 23.0514 - mae: 1.4761 - val_loss: 10.2101 - val_mse: 10.2101 - val_mae: 1.4679 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 29/500\n",
            "834/834 - 1s - loss: 23.0630 - mse: 23.0630 - mae: 1.4773 - val_loss: 10.2199 - val_mse: 10.2199 - val_mae: 1.4642 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 30/500\n",
            "834/834 - 1s - loss: 23.0620 - mse: 23.0620 - mae: 1.4759 - val_loss: 10.2171 - val_mse: 10.2171 - val_mae: 1.4620 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 31/500\n",
            "834/834 - 1s - loss: 23.0436 - mse: 23.0436 - mae: 1.4791 - val_loss: 10.2416 - val_mse: 10.2416 - val_mae: 1.4514 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 32/500\n",
            "834/834 - 1s - loss: 23.0604 - mse: 23.0604 - mae: 1.4753 - val_loss: 10.2260 - val_mse: 10.2260 - val_mae: 1.4643 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 33/500\n",
            "834/834 - 1s - loss: 23.0457 - mse: 23.0457 - mae: 1.4773 - val_loss: 10.2307 - val_mse: 10.2307 - val_mae: 1.4658 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 34/500\n",
            "834/834 - 1s - loss: 23.0533 - mse: 23.0533 - mae: 1.4762 - val_loss: 10.2266 - val_mse: 10.2266 - val_mae: 1.4609 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 35/500\n",
            "834/834 - 1s - loss: 23.0356 - mse: 23.0356 - mae: 1.4780 - val_loss: 10.2156 - val_mse: 10.2156 - val_mae: 1.4573 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 36/500\n",
            "834/834 - 1s - loss: 23.0441 - mse: 23.0441 - mae: 1.4747 - val_loss: 10.2000 - val_mse: 10.2000 - val_mae: 1.4674 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 37/500\n",
            "834/834 - 1s - loss: 23.0393 - mse: 23.0393 - mae: 1.4776 - val_loss: 10.2205 - val_mse: 10.2205 - val_mae: 1.4697 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 38/500\n",
            "834/834 - 1s - loss: 23.0383 - mse: 23.0383 - mae: 1.4753 - val_loss: 10.2042 - val_mse: 10.2042 - val_mae: 1.4525 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 39/500\n",
            "834/834 - 1s - loss: 23.0332 - mse: 23.0332 - mae: 1.4738 - val_loss: 10.2070 - val_mse: 10.2070 - val_mae: 1.4677 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 40/500\n",
            "834/834 - 1s - loss: 23.0470 - mse: 23.0470 - mae: 1.4744 - val_loss: 10.2406 - val_mse: 10.2406 - val_mae: 1.4651 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 41/500\n",
            "834/834 - 1s - loss: 23.0379 - mse: 23.0379 - mae: 1.4771 - val_loss: 10.2118 - val_mse: 10.2118 - val_mae: 1.4676 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 42/500\n",
            "834/834 - 1s - loss: 23.0261 - mse: 23.0261 - mae: 1.4768 - val_loss: 10.1998 - val_mse: 10.1998 - val_mae: 1.4634 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 43/500\n",
            "834/834 - 1s - loss: 23.0300 - mse: 23.0300 - mae: 1.4748 - val_loss: 10.2176 - val_mse: 10.2176 - val_mae: 1.4637 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 44/500\n",
            "834/834 - 1s - loss: 23.0303 - mse: 23.0303 - mae: 1.4751 - val_loss: 10.2205 - val_mse: 10.2205 - val_mae: 1.4656 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 45/500\n",
            "834/834 - 1s - loss: 23.0363 - mse: 23.0363 - mae: 1.4733 - val_loss: 10.1956 - val_mse: 10.1956 - val_mae: 1.4669 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 46/500\n",
            "834/834 - 1s - loss: 23.0200 - mse: 23.0200 - mae: 1.4773 - val_loss: 10.2265 - val_mse: 10.2265 - val_mae: 1.4612 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 47/500\n",
            "834/834 - 1s - loss: 23.0238 - mse: 23.0238 - mae: 1.4742 - val_loss: 10.2236 - val_mse: 10.2236 - val_mae: 1.4500 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 48/500\n",
            "834/834 - 1s - loss: 23.0233 - mse: 23.0233 - mae: 1.4734 - val_loss: 10.2133 - val_mse: 10.2133 - val_mae: 1.4655 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 49/500\n",
            "834/834 - 1s - loss: 23.0219 - mse: 23.0219 - mae: 1.4754 - val_loss: 10.1989 - val_mse: 10.1989 - val_mae: 1.4700 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 50/500\n",
            "834/834 - 1s - loss: 23.0143 - mse: 23.0143 - mae: 1.4767 - val_loss: 10.2035 - val_mse: 10.2035 - val_mae: 1.4741 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 51/500\n",
            "834/834 - 1s - loss: 23.0244 - mse: 23.0244 - mae: 1.4754 - val_loss: 10.2167 - val_mse: 10.2167 - val_mae: 1.4664 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 52/500\n",
            "834/834 - 1s - loss: 23.0082 - mse: 23.0082 - mae: 1.4771 - val_loss: 10.2183 - val_mse: 10.2183 - val_mae: 1.4683 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 53/500\n",
            "834/834 - 1s - loss: 23.0158 - mse: 23.0158 - mae: 1.4744 - val_loss: 10.2128 - val_mse: 10.2128 - val_mae: 1.4658 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 54/500\n",
            "834/834 - 1s - loss: 23.0138 - mse: 23.0138 - mae: 1.4743 - val_loss: 10.2047 - val_mse: 10.2047 - val_mae: 1.4705 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 55/500\n",
            "834/834 - 1s - loss: 23.0083 - mse: 23.0083 - mae: 1.4749 - val_loss: 10.2116 - val_mse: 10.2116 - val_mae: 1.4615 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 56/500\n",
            "834/834 - 1s - loss: 23.0154 - mse: 23.0154 - mae: 1.4739 - val_loss: 10.2243 - val_mse: 10.2243 - val_mae: 1.4615 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 57/500\n",
            "834/834 - 1s - loss: 23.0090 - mse: 23.0090 - mae: 1.4756 - val_loss: 10.2125 - val_mse: 10.2125 - val_mae: 1.4672 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 58/500\n",
            "834/834 - 1s - loss: 23.0041 - mse: 23.0041 - mae: 1.4741 - val_loss: 10.2099 - val_mse: 10.2099 - val_mae: 1.4666 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 59/500\n",
            "834/834 - 1s - loss: 23.0108 - mse: 23.0108 - mae: 1.4750 - val_loss: 10.2038 - val_mse: 10.2038 - val_mae: 1.4695 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 60/500\n",
            "834/834 - 1s - loss: 23.0041 - mse: 23.0041 - mae: 1.4716 - val_loss: 10.2031 - val_mse: 10.2031 - val_mae: 1.4794 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 61/500\n",
            "834/834 - 1s - loss: 23.0056 - mse: 23.0056 - mae: 1.4760 - val_loss: 10.1952 - val_mse: 10.1952 - val_mae: 1.4744 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 62/500\n",
            "834/834 - 1s - loss: 23.0061 - mse: 23.0061 - mae: 1.4757 - val_loss: 10.1924 - val_mse: 10.1924 - val_mae: 1.4584 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 63/500\n",
            "834/834 - 1s - loss: 22.9985 - mse: 22.9985 - mae: 1.4750 - val_loss: 10.2005 - val_mse: 10.2005 - val_mae: 1.4673 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 64/500\n",
            "834/834 - 1s - loss: 22.9914 - mse: 22.9914 - mae: 1.4740 - val_loss: 10.1938 - val_mse: 10.1938 - val_mae: 1.4589 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 65/500\n",
            "834/834 - 1s - loss: 22.9948 - mse: 22.9948 - mae: 1.4735 - val_loss: 10.1917 - val_mse: 10.1917 - val_mae: 1.4627 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 66/500\n",
            "834/834 - 1s - loss: 22.9924 - mse: 22.9924 - mae: 1.4743 - val_loss: 10.1877 - val_mse: 10.1877 - val_mae: 1.4802 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 67/500\n",
            "834/834 - 1s - loss: 22.9921 - mse: 22.9921 - mae: 1.4734 - val_loss: 10.2053 - val_mse: 10.2053 - val_mae: 1.4620 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 68/500\n",
            "834/834 - 1s - loss: 22.9902 - mse: 22.9902 - mae: 1.4745 - val_loss: 10.1981 - val_mse: 10.1981 - val_mae: 1.4660 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 69/500\n",
            "834/834 - 1s - loss: 22.9981 - mse: 22.9981 - mae: 1.4746 - val_loss: 10.2048 - val_mse: 10.2048 - val_mae: 1.4623 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 70/500\n",
            "834/834 - 1s - loss: 22.9956 - mse: 22.9956 - mae: 1.4738 - val_loss: 10.1918 - val_mse: 10.1918 - val_mae: 1.4622 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 71/500\n",
            "834/834 - 1s - loss: 22.9964 - mse: 22.9964 - mae: 1.4736 - val_loss: 10.1964 - val_mse: 10.1964 - val_mae: 1.4586 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 72/500\n",
            "834/834 - 1s - loss: 22.9838 - mse: 22.9838 - mae: 1.4734 - val_loss: 10.2041 - val_mse: 10.2041 - val_mae: 1.4649 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 73/500\n",
            "834/834 - 1s - loss: 22.9940 - mse: 22.9940 - mae: 1.4745 - val_loss: 10.2125 - val_mse: 10.2125 - val_mae: 1.4526 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 74/500\n",
            "834/834 - 1s - loss: 22.9906 - mse: 22.9906 - mae: 1.4695 - val_loss: 10.1760 - val_mse: 10.1760 - val_mae: 1.4730 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 75/500\n",
            "834/834 - 1s - loss: 22.9899 - mse: 22.9899 - mae: 1.4771 - val_loss: 10.1916 - val_mse: 10.1916 - val_mae: 1.4654 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 76/500\n",
            "834/834 - 1s - loss: 22.9953 - mse: 22.9953 - mae: 1.4739 - val_loss: 10.1757 - val_mse: 10.1757 - val_mae: 1.4658 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 77/500\n",
            "834/834 - 1s - loss: 22.9741 - mse: 22.9741 - mae: 1.4766 - val_loss: 10.2023 - val_mse: 10.2023 - val_mae: 1.4687 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 78/500\n",
            "834/834 - 1s - loss: 22.9838 - mse: 22.9838 - mae: 1.4727 - val_loss: 10.2256 - val_mse: 10.2256 - val_mae: 1.4514 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 79/500\n",
            "834/834 - 1s - loss: 22.9865 - mse: 22.9865 - mae: 1.4783 - val_loss: 10.2124 - val_mse: 10.2124 - val_mae: 1.4572 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 80/500\n",
            "834/834 - 1s - loss: 22.9825 - mse: 22.9825 - mae: 1.4735 - val_loss: 10.1778 - val_mse: 10.1778 - val_mae: 1.4619 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 81/500\n",
            "834/834 - 1s - loss: 22.9716 - mse: 22.9716 - mae: 1.4728 - val_loss: 10.2024 - val_mse: 10.2024 - val_mae: 1.4684 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 82/500\n",
            "834/834 - 1s - loss: 22.9845 - mse: 22.9845 - mae: 1.4733 - val_loss: 10.1883 - val_mse: 10.1883 - val_mae: 1.4601 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 83/500\n",
            "834/834 - 1s - loss: 22.9782 - mse: 22.9782 - mae: 1.4754 - val_loss: 10.1885 - val_mse: 10.1885 - val_mae: 1.4647 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 84/500\n",
            "834/834 - 1s - loss: 22.9728 - mse: 22.9728 - mae: 1.4738 - val_loss: 10.1926 - val_mse: 10.1926 - val_mae: 1.4664 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 85/500\n",
            "834/834 - 1s - loss: 22.9679 - mse: 22.9679 - mae: 1.4747 - val_loss: 10.1853 - val_mse: 10.1853 - val_mae: 1.4673 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 86/500\n",
            "834/834 - 1s - loss: 22.9773 - mse: 22.9773 - mae: 1.4743 - val_loss: 10.1862 - val_mse: 10.1862 - val_mae: 1.4788 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 87/500\n",
            "834/834 - 1s - loss: 22.9721 - mse: 22.9721 - mae: 1.4762 - val_loss: 10.1941 - val_mse: 10.1941 - val_mae: 1.4696 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 88/500\n",
            "834/834 - 1s - loss: 22.9694 - mse: 22.9694 - mae: 1.4735 - val_loss: 10.1970 - val_mse: 10.1970 - val_mae: 1.4615 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 89/500\n",
            "834/834 - 1s - loss: 22.9802 - mse: 22.9802 - mae: 1.4722 - val_loss: 10.1880 - val_mse: 10.1880 - val_mae: 1.4736 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 90/500\n",
            "834/834 - 1s - loss: 22.9768 - mse: 22.9768 - mae: 1.4738 - val_loss: 10.1871 - val_mse: 10.1871 - val_mae: 1.4614 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 91/500\n",
            "834/834 - 1s - loss: 22.9641 - mse: 22.9641 - mae: 1.4762 - val_loss: 10.1861 - val_mse: 10.1861 - val_mae: 1.4689 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 92/500\n",
            "834/834 - 1s - loss: 22.9678 - mse: 22.9678 - mae: 1.4731 - val_loss: 10.1926 - val_mse: 10.1926 - val_mae: 1.4821 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 93/500\n",
            "834/834 - 1s - loss: 22.9580 - mse: 22.9580 - mae: 1.4748 - val_loss: 10.1799 - val_mse: 10.1799 - val_mae: 1.4632 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 94/500\n",
            "834/834 - 1s - loss: 22.9599 - mse: 22.9599 - mae: 1.4749 - val_loss: 10.1703 - val_mse: 10.1703 - val_mae: 1.4606 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 95/500\n",
            "834/834 - 1s - loss: 22.9605 - mse: 22.9605 - mae: 1.4730 - val_loss: 10.1869 - val_mse: 10.1869 - val_mae: 1.4690 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 96/500\n",
            "834/834 - 1s - loss: 22.9635 - mse: 22.9635 - mae: 1.4722 - val_loss: 10.1686 - val_mse: 10.1686 - val_mae: 1.4716 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 97/500\n",
            "834/834 - 1s - loss: 22.9580 - mse: 22.9580 - mae: 1.4745 - val_loss: 10.2226 - val_mse: 10.2226 - val_mae: 1.4673 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 98/500\n",
            "834/834 - 1s - loss: 22.9739 - mse: 22.9739 - mae: 1.4728 - val_loss: 10.1866 - val_mse: 10.1866 - val_mae: 1.4495 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 99/500\n",
            "834/834 - 1s - loss: 22.9570 - mse: 22.9570 - mae: 1.4704 - val_loss: 10.1676 - val_mse: 10.1676 - val_mae: 1.4837 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 100/500\n",
            "834/834 - 1s - loss: 22.9565 - mse: 22.9565 - mae: 1.4757 - val_loss: 10.1733 - val_mse: 10.1733 - val_mae: 1.4603 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 101/500\n",
            "834/834 - 1s - loss: 22.9551 - mse: 22.9551 - mae: 1.4708 - val_loss: 10.1655 - val_mse: 10.1655 - val_mae: 1.4558 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 102/500\n",
            "834/834 - 1s - loss: 22.9505 - mse: 22.9505 - mae: 1.4726 - val_loss: 10.1773 - val_mse: 10.1773 - val_mae: 1.4678 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 103/500\n",
            "834/834 - 1s - loss: 22.9484 - mse: 22.9484 - mae: 1.4731 - val_loss: 10.1665 - val_mse: 10.1665 - val_mae: 1.4621 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 104/500\n",
            "834/834 - 1s - loss: 22.9509 - mse: 22.9509 - mae: 1.4752 - val_loss: 10.1738 - val_mse: 10.1738 - val_mae: 1.4620 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 105/500\n",
            "834/834 - 1s - loss: 22.9558 - mse: 22.9558 - mae: 1.4720 - val_loss: 10.1779 - val_mse: 10.1779 - val_mae: 1.4738 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 106/500\n",
            "834/834 - 1s - loss: 22.9532 - mse: 22.9532 - mae: 1.4719 - val_loss: 10.1768 - val_mse: 10.1768 - val_mae: 1.4632 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 107/500\n",
            "834/834 - 1s - loss: 22.9460 - mse: 22.9460 - mae: 1.4728 - val_loss: 10.1641 - val_mse: 10.1641 - val_mae: 1.4608 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 108/500\n",
            "834/834 - 1s - loss: 22.9488 - mse: 22.9488 - mae: 1.4707 - val_loss: 10.1738 - val_mse: 10.1738 - val_mae: 1.4681 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 109/500\n",
            "834/834 - 1s - loss: 22.9500 - mse: 22.9500 - mae: 1.4732 - val_loss: 10.1719 - val_mse: 10.1719 - val_mae: 1.4635 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 110/500\n",
            "834/834 - 1s - loss: 22.9458 - mse: 22.9458 - mae: 1.4704 - val_loss: 10.1368 - val_mse: 10.1368 - val_mae: 1.4826 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 111/500\n",
            "834/834 - 1s - loss: 22.9458 - mse: 22.9458 - mae: 1.4731 - val_loss: 10.1554 - val_mse: 10.1554 - val_mae: 1.4610 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 112/500\n",
            "834/834 - 1s - loss: 22.9459 - mse: 22.9459 - mae: 1.4723 - val_loss: 10.1634 - val_mse: 10.1634 - val_mae: 1.4659 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 113/500\n",
            "834/834 - 1s - loss: 22.9376 - mse: 22.9376 - mae: 1.4730 - val_loss: 10.1882 - val_mse: 10.1882 - val_mae: 1.4604 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 114/500\n",
            "834/834 - 1s - loss: 22.9472 - mse: 22.9472 - mae: 1.4704 - val_loss: 10.1733 - val_mse: 10.1733 - val_mae: 1.4744 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 115/500\n",
            "834/834 - 1s - loss: 22.9412 - mse: 22.9412 - mae: 1.4709 - val_loss: 10.1571 - val_mse: 10.1571 - val_mae: 1.4774 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 116/500\n",
            "834/834 - 1s - loss: 22.9428 - mse: 22.9428 - mae: 1.4746 - val_loss: 10.1571 - val_mse: 10.1571 - val_mae: 1.4702 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 117/500\n",
            "834/834 - 1s - loss: 22.9311 - mse: 22.9311 - mae: 1.4717 - val_loss: 10.1643 - val_mse: 10.1643 - val_mae: 1.4638 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 118/500\n",
            "834/834 - 1s - loss: 22.9292 - mse: 22.9292 - mae: 1.4726 - val_loss: 10.1553 - val_mse: 10.1553 - val_mae: 1.4623 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 119/500\n",
            "834/834 - 1s - loss: 22.9285 - mse: 22.9285 - mae: 1.4700 - val_loss: 10.1758 - val_mse: 10.1758 - val_mae: 1.4752 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 120/500\n",
            "834/834 - 1s - loss: 22.9384 - mse: 22.9384 - mae: 1.4723 - val_loss: 10.1592 - val_mse: 10.1592 - val_mae: 1.4694 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 121/500\n",
            "834/834 - 1s - loss: 22.9285 - mse: 22.9285 - mae: 1.4730 - val_loss: 10.1617 - val_mse: 10.1617 - val_mae: 1.4609 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 122/500\n",
            "834/834 - 1s - loss: 22.9378 - mse: 22.9378 - mae: 1.4748 - val_loss: 10.1739 - val_mse: 10.1739 - val_mae: 1.4752 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 123/500\n",
            "834/834 - 1s - loss: 22.9219 - mse: 22.9219 - mae: 1.4746 - val_loss: 10.1586 - val_mse: 10.1586 - val_mae: 1.4699 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 124/500\n",
            "834/834 - 1s - loss: 22.9323 - mse: 22.9323 - mae: 1.4715 - val_loss: 10.1612 - val_mse: 10.1612 - val_mae: 1.4673 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 125/500\n",
            "834/834 - 1s - loss: 22.9281 - mse: 22.9281 - mae: 1.4710 - val_loss: 10.1439 - val_mse: 10.1439 - val_mae: 1.4665 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 126/500\n",
            "834/834 - 1s - loss: 22.9156 - mse: 22.9156 - mae: 1.4721 - val_loss: 10.1675 - val_mse: 10.1675 - val_mae: 1.4609 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 127/500\n",
            "834/834 - 1s - loss: 22.9238 - mse: 22.9238 - mae: 1.4726 - val_loss: 10.1505 - val_mse: 10.1505 - val_mae: 1.4652 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 128/500\n",
            "834/834 - 1s - loss: 22.9199 - mse: 22.9199 - mae: 1.4744 - val_loss: 10.1750 - val_mse: 10.1750 - val_mae: 1.4571 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 129/500\n",
            "834/834 - 1s - loss: 22.9335 - mse: 22.9335 - mae: 1.4706 - val_loss: 10.1627 - val_mse: 10.1627 - val_mae: 1.4614 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 130/500\n",
            "834/834 - 1s - loss: 22.9240 - mse: 22.9240 - mae: 1.4689 - val_loss: 10.1556 - val_mse: 10.1556 - val_mae: 1.4694 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 131/500\n",
            "834/834 - 1s - loss: 22.9222 - mse: 22.9222 - mae: 1.4729 - val_loss: 10.1595 - val_mse: 10.1595 - val_mae: 1.4755 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 132/500\n",
            "834/834 - 1s - loss: 22.9146 - mse: 22.9146 - mae: 1.4745 - val_loss: 10.1620 - val_mse: 10.1620 - val_mae: 1.4635 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 133/500\n",
            "834/834 - 1s - loss: 22.9124 - mse: 22.9124 - mae: 1.4709 - val_loss: 10.1761 - val_mse: 10.1761 - val_mae: 1.4721 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 134/500\n",
            "834/834 - 1s - loss: 22.9268 - mse: 22.9268 - mae: 1.4719 - val_loss: 10.1718 - val_mse: 10.1718 - val_mae: 1.4615 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 135/500\n",
            "834/834 - 1s - loss: 22.9159 - mse: 22.9159 - mae: 1.4700 - val_loss: 10.1447 - val_mse: 10.1447 - val_mae: 1.4664 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 136/500\n",
            "834/834 - 1s - loss: 22.9205 - mse: 22.9205 - mae: 1.4711 - val_loss: 10.1505 - val_mse: 10.1505 - val_mae: 1.4700 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 137/500\n",
            "834/834 - 1s - loss: 22.9225 - mse: 22.9225 - mae: 1.4716 - val_loss: 10.1380 - val_mse: 10.1380 - val_mae: 1.4772 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 138/500\n",
            "834/834 - 1s - loss: 22.9171 - mse: 22.9171 - mae: 1.4721 - val_loss: 10.1429 - val_mse: 10.1429 - val_mae: 1.4624 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 139/500\n",
            "834/834 - 1s - loss: 22.9032 - mse: 22.9032 - mae: 1.4740 - val_loss: 10.1385 - val_mse: 10.1385 - val_mae: 1.4683 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 140/500\n",
            "834/834 - 1s - loss: 22.9054 - mse: 22.9054 - mae: 1.4714 - val_loss: 10.1610 - val_mse: 10.1610 - val_mae: 1.4632 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 141/500\n",
            "834/834 - 1s - loss: 22.9108 - mse: 22.9108 - mae: 1.4718 - val_loss: 10.1645 - val_mse: 10.1645 - val_mae: 1.4580 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 142/500\n",
            "834/834 - 1s - loss: 22.9141 - mse: 22.9141 - mae: 1.4699 - val_loss: 10.1516 - val_mse: 10.1516 - val_mae: 1.4613 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 143/500\n",
            "834/834 - 1s - loss: 22.9172 - mse: 22.9172 - mae: 1.4700 - val_loss: 10.1427 - val_mse: 10.1427 - val_mae: 1.4726 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 144/500\n",
            "834/834 - 1s - loss: 22.8972 - mse: 22.8972 - mae: 1.4700 - val_loss: 10.1637 - val_mse: 10.1637 - val_mae: 1.4633 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 145/500\n",
            "834/834 - 1s - loss: 22.9110 - mse: 22.9110 - mae: 1.4702 - val_loss: 10.1694 - val_mse: 10.1694 - val_mae: 1.4558 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 146/500\n",
            "834/834 - 1s - loss: 22.8978 - mse: 22.8978 - mae: 1.4706 - val_loss: 10.1592 - val_mse: 10.1592 - val_mae: 1.4603 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 147/500\n",
            "834/834 - 1s - loss: 22.9178 - mse: 22.9178 - mae: 1.4704 - val_loss: 10.1335 - val_mse: 10.1335 - val_mae: 1.4618 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 148/500\n",
            "834/834 - 1s - loss: 22.9125 - mse: 22.9125 - mae: 1.4732 - val_loss: 10.1293 - val_mse: 10.1293 - val_mae: 1.4751 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 149/500\n",
            "834/834 - 1s - loss: 22.9082 - mse: 22.9082 - mae: 1.4743 - val_loss: 10.1708 - val_mse: 10.1708 - val_mae: 1.4818 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 150/500\n",
            "834/834 - 1s - loss: 22.8998 - mse: 22.8998 - mae: 1.4749 - val_loss: 10.1757 - val_mse: 10.1757 - val_mae: 1.4531 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 151/500\n",
            "834/834 - 1s - loss: 22.9066 - mse: 22.9066 - mae: 1.4689 - val_loss: 10.1375 - val_mse: 10.1375 - val_mae: 1.4603 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 152/500\n",
            "834/834 - 1s - loss: 22.8960 - mse: 22.8960 - mae: 1.4682 - val_loss: 10.1303 - val_mse: 10.1303 - val_mae: 1.4789 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 153/500\n",
            "834/834 - 1s - loss: 22.8962 - mse: 22.8962 - mae: 1.4737 - val_loss: 10.1454 - val_mse: 10.1454 - val_mae: 1.4673 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 154/500\n",
            "834/834 - 1s - loss: 22.9050 - mse: 22.9050 - mae: 1.4743 - val_loss: 10.1396 - val_mse: 10.1396 - val_mae: 1.4643 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 155/500\n",
            "834/834 - 1s - loss: 22.9003 - mse: 22.9003 - mae: 1.4708 - val_loss: 10.1454 - val_mse: 10.1454 - val_mae: 1.4583 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 156/500\n",
            "834/834 - 1s - loss: 22.9055 - mse: 22.9055 - mae: 1.4676 - val_loss: 10.1398 - val_mse: 10.1398 - val_mae: 1.4648 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 157/500\n",
            "834/834 - 1s - loss: 22.8993 - mse: 22.8993 - mae: 1.4737 - val_loss: 10.1370 - val_mse: 10.1370 - val_mae: 1.4593 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 158/500\n",
            "834/834 - 1s - loss: 22.8882 - mse: 22.8882 - mae: 1.4705 - val_loss: 10.1379 - val_mse: 10.1379 - val_mae: 1.4692 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 159/500\n",
            "834/834 - 1s - loss: 22.8949 - mse: 22.8949 - mae: 1.4709 - val_loss: 10.1292 - val_mse: 10.1292 - val_mae: 1.4802 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 160/500\n",
            "834/834 - 1s - loss: 22.8828 - mse: 22.8828 - mae: 1.4725 - val_loss: 10.1481 - val_mse: 10.1481 - val_mae: 1.4701 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 161/500\n",
            "834/834 - 1s - loss: 22.8926 - mse: 22.8926 - mae: 1.4699 - val_loss: 10.1401 - val_mse: 10.1401 - val_mae: 1.4720 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 162/500\n",
            "834/834 - 1s - loss: 22.8904 - mse: 22.8904 - mae: 1.4695 - val_loss: 10.1460 - val_mse: 10.1460 - val_mae: 1.4849 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 163/500\n",
            "834/834 - 1s - loss: 22.8970 - mse: 22.8970 - mae: 1.4723 - val_loss: 10.1428 - val_mse: 10.1428 - val_mae: 1.4632 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 164/500\n",
            "834/834 - 1s - loss: 22.8921 - mse: 22.8921 - mae: 1.4699 - val_loss: 10.1367 - val_mse: 10.1367 - val_mae: 1.4669 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 165/500\n",
            "834/834 - 1s - loss: 22.8843 - mse: 22.8843 - mae: 1.4701 - val_loss: 10.1434 - val_mse: 10.1434 - val_mae: 1.4692 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 166/500\n",
            "834/834 - 1s - loss: 22.8932 - mse: 22.8932 - mae: 1.4718 - val_loss: 10.1533 - val_mse: 10.1533 - val_mae: 1.4627 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 167/500\n",
            "834/834 - 1s - loss: 22.8850 - mse: 22.8850 - mae: 1.4710 - val_loss: 10.1835 - val_mse: 10.1835 - val_mae: 1.4548 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 168/500\n",
            "834/834 - 1s - loss: 22.8995 - mse: 22.8995 - mae: 1.4695 - val_loss: 10.1399 - val_mse: 10.1399 - val_mae: 1.4659 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 169/500\n",
            "834/834 - 1s - loss: 22.8880 - mse: 22.8880 - mae: 1.4725 - val_loss: 10.1467 - val_mse: 10.1467 - val_mae: 1.4631 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 170/500\n",
            "834/834 - 1s - loss: 22.8845 - mse: 22.8845 - mae: 1.4710 - val_loss: 10.1595 - val_mse: 10.1595 - val_mae: 1.4600 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "[10.159497261047363, 10.159497261047363, 1.4600040912628174]\n",
            "Score for fold 2: loss of 10.159497261047363\n",
            "------------------------------------------------------------------------\n",
            "Training for Outer fold 3 ...\n",
            "Epoch 1/500\n",
            "834/834 - 1s - loss: 23.5301 - mse: 23.5301 - mae: 1.4752 - val_loss: 8.8666 - val_mse: 8.8666 - val_mae: 1.4535 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 2/500\n",
            "834/834 - 1s - loss: 23.5326 - mse: 23.5326 - mae: 1.4753 - val_loss: 8.8787 - val_mse: 8.8787 - val_mae: 1.4494 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 3/500\n",
            "834/834 - 1s - loss: 23.5423 - mse: 23.5423 - mae: 1.4728 - val_loss: 8.8886 - val_mse: 8.8886 - val_mae: 1.4552 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 4/500\n",
            "834/834 - 1s - loss: 23.5175 - mse: 23.5175 - mae: 1.4742 - val_loss: 8.8836 - val_mse: 8.8836 - val_mae: 1.4659 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 5/500\n",
            "834/834 - 1s - loss: 23.5238 - mse: 23.5238 - mae: 1.4760 - val_loss: 8.9022 - val_mse: 8.9022 - val_mae: 1.4513 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 6/500\n",
            "834/834 - 1s - loss: 23.5237 - mse: 23.5237 - mae: 1.4750 - val_loss: 8.9276 - val_mse: 8.9276 - val_mae: 1.4432 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 7/500\n",
            "834/834 - 1s - loss: 23.5352 - mse: 23.5352 - mae: 1.4709 - val_loss: 8.9043 - val_mse: 8.9043 - val_mae: 1.4529 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 8/500\n",
            "834/834 - 1s - loss: 23.5169 - mse: 23.5169 - mae: 1.4727 - val_loss: 8.9102 - val_mse: 8.9102 - val_mae: 1.4655 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 9/500\n",
            "834/834 - 1s - loss: 23.5277 - mse: 23.5277 - mae: 1.4726 - val_loss: 8.9195 - val_mse: 8.9195 - val_mae: 1.4608 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 10/500\n",
            "834/834 - 1s - loss: 23.5224 - mse: 23.5224 - mae: 1.4746 - val_loss: 8.9240 - val_mse: 8.9240 - val_mae: 1.4518 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 11/500\n",
            "834/834 - 1s - loss: 23.5213 - mse: 23.5213 - mae: 1.4749 - val_loss: 8.9332 - val_mse: 8.9332 - val_mae: 1.4452 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 12/500\n",
            "834/834 - 1s - loss: 23.5031 - mse: 23.5031 - mae: 1.4713 - val_loss: 8.9259 - val_mse: 8.9259 - val_mae: 1.4509 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 13/500\n",
            "834/834 - 1s - loss: 23.5072 - mse: 23.5072 - mae: 1.4735 - val_loss: 8.9231 - val_mse: 8.9231 - val_mae: 1.4629 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 14/500\n",
            "834/834 - 1s - loss: 23.5078 - mse: 23.5078 - mae: 1.4759 - val_loss: 8.9352 - val_mse: 8.9352 - val_mae: 1.4573 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 15/500\n",
            "834/834 - 1s - loss: 23.5173 - mse: 23.5173 - mae: 1.4736 - val_loss: 8.9487 - val_mse: 8.9487 - val_mae: 1.4539 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 16/500\n",
            "834/834 - 1s - loss: 23.5046 - mse: 23.5046 - mae: 1.4730 - val_loss: 8.9422 - val_mse: 8.9422 - val_mae: 1.4727 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 17/500\n",
            "834/834 - 1s - loss: 23.5046 - mse: 23.5046 - mae: 1.4746 - val_loss: 8.9420 - val_mse: 8.9420 - val_mae: 1.4640 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 18/500\n",
            "834/834 - 1s - loss: 23.4940 - mse: 23.4940 - mae: 1.4725 - val_loss: 8.9505 - val_mse: 8.9505 - val_mae: 1.4580 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 19/500\n",
            "834/834 - 1s - loss: 23.5051 - mse: 23.5051 - mae: 1.4744 - val_loss: 8.9417 - val_mse: 8.9417 - val_mae: 1.4526 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 20/500\n",
            "834/834 - 1s - loss: 23.5021 - mse: 23.5021 - mae: 1.4714 - val_loss: 8.9481 - val_mse: 8.9481 - val_mae: 1.4539 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 21/500\n",
            "834/834 - 1s - loss: 23.4990 - mse: 23.4990 - mae: 1.4735 - val_loss: 8.9418 - val_mse: 8.9418 - val_mae: 1.4499 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 22/500\n",
            "834/834 - 1s - loss: 23.4919 - mse: 23.4919 - mae: 1.4713 - val_loss: 8.9408 - val_mse: 8.9408 - val_mae: 1.4715 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 23/500\n",
            "834/834 - 1s - loss: 23.5023 - mse: 23.5023 - mae: 1.4709 - val_loss: 8.9395 - val_mse: 8.9395 - val_mae: 1.4589 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 24/500\n",
            "834/834 - 1s - loss: 23.4935 - mse: 23.4935 - mae: 1.4724 - val_loss: 8.9456 - val_mse: 8.9456 - val_mae: 1.4577 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 25/500\n",
            "834/834 - 1s - loss: 23.5057 - mse: 23.5057 - mae: 1.4700 - val_loss: 8.9362 - val_mse: 8.9362 - val_mae: 1.4632 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 26/500\n",
            "834/834 - 1s - loss: 23.4899 - mse: 23.4899 - mae: 1.4720 - val_loss: 8.9433 - val_mse: 8.9433 - val_mae: 1.4581 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 27/500\n",
            "834/834 - 1s - loss: 23.4872 - mse: 23.4872 - mae: 1.4723 - val_loss: 8.9519 - val_mse: 8.9519 - val_mae: 1.4672 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 28/500\n",
            "834/834 - 1s - loss: 23.4942 - mse: 23.4942 - mae: 1.4720 - val_loss: 8.9471 - val_mse: 8.9471 - val_mae: 1.4553 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 29/500\n",
            "834/834 - 1s - loss: 23.4851 - mse: 23.4851 - mae: 1.4731 - val_loss: 8.9477 - val_mse: 8.9477 - val_mae: 1.4625 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 30/500\n",
            "834/834 - 1s - loss: 23.4935 - mse: 23.4935 - mae: 1.4716 - val_loss: 8.9504 - val_mse: 8.9504 - val_mae: 1.4718 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 31/500\n",
            "834/834 - 1s - loss: 23.4760 - mse: 23.4760 - mae: 1.4724 - val_loss: 8.9535 - val_mse: 8.9535 - val_mae: 1.4537 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 32/500\n",
            "834/834 - 1s - loss: 23.4880 - mse: 23.4880 - mae: 1.4689 - val_loss: 8.9447 - val_mse: 8.9447 - val_mae: 1.4540 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 33/500\n",
            "834/834 - 1s - loss: 23.4948 - mse: 23.4948 - mae: 1.4706 - val_loss: 8.9529 - val_mse: 8.9529 - val_mae: 1.4590 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 34/500\n",
            "834/834 - 1s - loss: 23.5040 - mse: 23.5040 - mae: 1.4732 - val_loss: 8.9502 - val_mse: 8.9502 - val_mae: 1.4662 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 35/500\n",
            "834/834 - 1s - loss: 23.4981 - mse: 23.4981 - mae: 1.4718 - val_loss: 8.9535 - val_mse: 8.9535 - val_mae: 1.4627 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 36/500\n",
            "834/834 - 1s - loss: 23.4879 - mse: 23.4879 - mae: 1.4727 - val_loss: 8.9670 - val_mse: 8.9670 - val_mae: 1.4606 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 37/500\n",
            "834/834 - 1s - loss: 23.4833 - mse: 23.4833 - mae: 1.4715 - val_loss: 8.9699 - val_mse: 8.9699 - val_mae: 1.4617 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 38/500\n",
            "834/834 - 1s - loss: 23.4931 - mse: 23.4931 - mae: 1.4703 - val_loss: 8.9568 - val_mse: 8.9568 - val_mae: 1.4723 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 39/500\n",
            "834/834 - 1s - loss: 23.4870 - mse: 23.4870 - mae: 1.4710 - val_loss: 8.9448 - val_mse: 8.9448 - val_mae: 1.4801 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 40/500\n",
            "834/834 - 1s - loss: 23.4652 - mse: 23.4652 - mae: 1.4751 - val_loss: 8.9788 - val_mse: 8.9788 - val_mae: 1.4532 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 41/500\n",
            "834/834 - 1s - loss: 23.4858 - mse: 23.4858 - mae: 1.4699 - val_loss: 8.9918 - val_mse: 8.9918 - val_mae: 1.4518 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 42/500\n",
            "834/834 - 1s - loss: 23.4959 - mse: 23.4959 - mae: 1.4717 - val_loss: 8.9741 - val_mse: 8.9741 - val_mae: 1.4505 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 43/500\n",
            "834/834 - 1s - loss: 23.4685 - mse: 23.4685 - mae: 1.4707 - val_loss: 8.9657 - val_mse: 8.9657 - val_mae: 1.4544 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 44/500\n",
            "834/834 - 1s - loss: 23.4725 - mse: 23.4725 - mae: 1.4669 - val_loss: 8.9518 - val_mse: 8.9518 - val_mae: 1.4647 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 45/500\n",
            "834/834 - 1s - loss: 23.4891 - mse: 23.4891 - mae: 1.4723 - val_loss: 8.9695 - val_mse: 8.9695 - val_mae: 1.4502 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 46/500\n",
            "834/834 - 1s - loss: 23.4717 - mse: 23.4717 - mae: 1.4692 - val_loss: 8.9563 - val_mse: 8.9563 - val_mae: 1.4588 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 47/500\n",
            "834/834 - 1s - loss: 23.4743 - mse: 23.4743 - mae: 1.4690 - val_loss: 8.9696 - val_mse: 8.9696 - val_mae: 1.4547 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 48/500\n",
            "834/834 - 1s - loss: 23.4581 - mse: 23.4581 - mae: 1.4699 - val_loss: 8.9590 - val_mse: 8.9590 - val_mae: 1.4574 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 49/500\n",
            "834/834 - 1s - loss: 23.4750 - mse: 23.4750 - mae: 1.4677 - val_loss: 8.9524 - val_mse: 8.9524 - val_mae: 1.4724 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 50/500\n",
            "834/834 - 1s - loss: 23.4740 - mse: 23.4740 - mae: 1.4696 - val_loss: 8.9581 - val_mse: 8.9581 - val_mae: 1.4580 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 51/500\n",
            "834/834 - 1s - loss: 23.4715 - mse: 23.4715 - mae: 1.4717 - val_loss: 8.9569 - val_mse: 8.9569 - val_mae: 1.4538 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 52/500\n",
            "834/834 - 1s - loss: 23.4679 - mse: 23.4679 - mae: 1.4720 - val_loss: 8.9702 - val_mse: 8.9702 - val_mae: 1.4520 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 53/500\n",
            "834/834 - 1s - loss: 23.4488 - mse: 23.4488 - mae: 1.4714 - val_loss: 8.9821 - val_mse: 8.9821 - val_mae: 1.4434 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 54/500\n",
            "834/834 - 1s - loss: 23.4751 - mse: 23.4751 - mae: 1.4667 - val_loss: 8.9531 - val_mse: 8.9531 - val_mae: 1.4728 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 55/500\n",
            "834/834 - 1s - loss: 23.4583 - mse: 23.4583 - mae: 1.4710 - val_loss: 8.9578 - val_mse: 8.9578 - val_mae: 1.4567 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 56/500\n",
            "834/834 - 1s - loss: 23.4698 - mse: 23.4698 - mae: 1.4720 - val_loss: 8.9695 - val_mse: 8.9695 - val_mae: 1.4544 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 57/500\n",
            "834/834 - 1s - loss: 23.4593 - mse: 23.4593 - mae: 1.4705 - val_loss: 8.9533 - val_mse: 8.9533 - val_mae: 1.4627 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 58/500\n",
            "834/834 - 1s - loss: 23.4517 - mse: 23.4517 - mae: 1.4693 - val_loss: 8.9472 - val_mse: 8.9472 - val_mae: 1.4694 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 59/500\n",
            "834/834 - 1s - loss: 23.4483 - mse: 23.4483 - mae: 1.4704 - val_loss: 8.9797 - val_mse: 8.9797 - val_mae: 1.4522 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 60/500\n",
            "834/834 - 1s - loss: 23.4718 - mse: 23.4718 - mae: 1.4687 - val_loss: 8.9594 - val_mse: 8.9594 - val_mae: 1.4676 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 61/500\n",
            "834/834 - 1s - loss: 23.4726 - mse: 23.4726 - mae: 1.4717 - val_loss: 8.9550 - val_mse: 8.9550 - val_mae: 1.4573 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 62/500\n",
            "834/834 - 1s - loss: 23.4548 - mse: 23.4548 - mae: 1.4707 - val_loss: 8.9488 - val_mse: 8.9488 - val_mae: 1.4609 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 63/500\n",
            "834/834 - 1s - loss: 23.4466 - mse: 23.4466 - mae: 1.4678 - val_loss: 8.9697 - val_mse: 8.9697 - val_mae: 1.4564 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 64/500\n",
            "834/834 - 1s - loss: 23.4521 - mse: 23.4521 - mae: 1.4717 - val_loss: 8.9643 - val_mse: 8.9643 - val_mae: 1.4562 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 65/500\n",
            "834/834 - 1s - loss: 23.4526 - mse: 23.4526 - mae: 1.4709 - val_loss: 8.9531 - val_mse: 8.9531 - val_mae: 1.4597 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 66/500\n",
            "834/834 - 1s - loss: 23.4649 - mse: 23.4649 - mae: 1.4710 - val_loss: 8.9397 - val_mse: 8.9397 - val_mae: 1.4632 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 67/500\n",
            "834/834 - 1s - loss: 23.4425 - mse: 23.4425 - mae: 1.4707 - val_loss: 8.9554 - val_mse: 8.9554 - val_mae: 1.4676 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 68/500\n",
            "834/834 - 1s - loss: 23.4291 - mse: 23.4291 - mae: 1.4709 - val_loss: 8.9598 - val_mse: 8.9598 - val_mae: 1.4480 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 69/500\n",
            "834/834 - 1s - loss: 23.4388 - mse: 23.4388 - mae: 1.4694 - val_loss: 8.9406 - val_mse: 8.9406 - val_mae: 1.4701 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 70/500\n",
            "834/834 - 1s - loss: 23.4397 - mse: 23.4397 - mae: 1.4722 - val_loss: 8.9650 - val_mse: 8.9650 - val_mae: 1.4540 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 71/500\n",
            "834/834 - 1s - loss: 23.4421 - mse: 23.4421 - mae: 1.4709 - val_loss: 8.9723 - val_mse: 8.9723 - val_mae: 1.4553 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 72/500\n",
            "834/834 - 1s - loss: 23.4496 - mse: 23.4496 - mae: 1.4703 - val_loss: 8.9320 - val_mse: 8.9320 - val_mae: 1.4625 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 73/500\n",
            "834/834 - 1s - loss: 23.4060 - mse: 23.4060 - mae: 1.4679 - val_loss: 8.9400 - val_mse: 8.9400 - val_mae: 1.4633 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 74/500\n",
            "834/834 - 1s - loss: 23.4398 - mse: 23.4398 - mae: 1.4707 - val_loss: 8.9534 - val_mse: 8.9534 - val_mae: 1.4621 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 75/500\n",
            "834/834 - 1s - loss: 23.4409 - mse: 23.4409 - mae: 1.4685 - val_loss: 8.9576 - val_mse: 8.9576 - val_mae: 1.4663 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 76/500\n",
            "834/834 - 1s - loss: 23.4371 - mse: 23.4371 - mae: 1.4680 - val_loss: 8.9738 - val_mse: 8.9738 - val_mae: 1.4649 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 77/500\n",
            "834/834 - 1s - loss: 23.4509 - mse: 23.4509 - mae: 1.4682 - val_loss: 8.9484 - val_mse: 8.9484 - val_mae: 1.4599 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 78/500\n",
            "834/834 - 1s - loss: 23.4359 - mse: 23.4359 - mae: 1.4710 - val_loss: 8.9566 - val_mse: 8.9566 - val_mae: 1.4550 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 79/500\n",
            "834/834 - 1s - loss: 23.4280 - mse: 23.4280 - mae: 1.4702 - val_loss: 8.9438 - val_mse: 8.9438 - val_mae: 1.4892 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 80/500\n",
            "834/834 - 1s - loss: 23.4215 - mse: 23.4215 - mae: 1.4718 - val_loss: 8.9613 - val_mse: 8.9613 - val_mae: 1.4701 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 81/500\n",
            "834/834 - 1s - loss: 23.4449 - mse: 23.4449 - mae: 1.4693 - val_loss: 8.9714 - val_mse: 8.9714 - val_mae: 1.4492 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 82/500\n",
            "834/834 - 1s - loss: 23.4280 - mse: 23.4280 - mae: 1.4692 - val_loss: 8.9591 - val_mse: 8.9591 - val_mae: 1.4728 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 83/500\n",
            "834/834 - 1s - loss: 23.4315 - mse: 23.4315 - mae: 1.4684 - val_loss: 8.9473 - val_mse: 8.9473 - val_mae: 1.4733 - lr: 1.5024e-04 - 1s/epoch - 2ms/step\n",
            "[8.947280883789062, 8.947280883789062, 1.4733502864837646]\n",
            "Score for fold 3: loss of 8.947280883789062\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "results = model.evaluate(testing, labelsForTest, batch_size=10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4IvDe-I_0JFA",
        "outputId": "641ffc76-9ed7-40e2-a634-59d06e3a3a6c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2000/2000 [==============================] - 3s 2ms/step - loss: 9.0166 - mse: 9.0166 - mae: 1.4660\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#model_retrain for outer cv (best model 2), similar with model1\n",
        "\n",
        "# {'activation': 'tanh', 'num_hidden_layer': 2, 'i': 4, 'learning_rate': 0.00010020536598530568}\n",
        "history_list_cv2=[]\n",
        "optimizer = Adam(learning_rate=0.00010020536598530568,clipnorm=1.0)\n",
        "model2 = create_model(activation=\"tanh\", num_hidden_layer=2, num_hidden_unit=16)\n",
        "model2.compile(loss='mse',optimizer=optimizer,metrics=['mse','mae'])\n",
        "kfold = KFold(n_splits=3,shuffle=True)\n",
        "fold_no=1\n",
        "loss_per_fold = []\n",
        "es = EarlyStopping(monitor='mse',mode='min',patience=10)\n",
        "for (train,test) in kfold.split(training,labelsForTrain):\n",
        "\n",
        "  print('------------------------------------------------------------------------')\n",
        "  print(f'Training for Outer fold {fold_no} ...')\n",
        "  \n",
        "  outer_cv_train=training.iloc[train]\n",
        "  outer_cv_trainlabel=labelsForTrain.iloc[train]\n",
        "\n",
        "  outer_cv_test=training.iloc[test]\n",
        "  outer_cv_testlabel=labelsForTrain.iloc[test]\n",
        "\n",
        "  reduce_lr = ReduceLROnPlateau(monitor='val_mse', factor=0.2,\n",
        "                              patience=5, min_lr=0.001)\n",
        "  # Fit data to model\n",
        "  history = model2.fit(outer_cv_train, outer_cv_trainlabel,\n",
        "                  batch_size=64,\n",
        "                  epochs=500,\n",
        "                  verbose=2,\n",
        "                  validation_data=(outer_cv_test,outer_cv_testlabel),\n",
        "                  validation_batch_size=64,\n",
        "                  callbacks=[es,reduce_lr])\n",
        "  \n",
        "  scores=model2.evaluate(outer_cv_test,outer_cv_testlabel,verbose=0)\n",
        "  print(scores)\n",
        "  print(f'Score for fold {fold_no}: {model2.metrics_names[0]} of {scores[0]}')\n",
        "  loss_per_fold.append(scores[0])\n",
        "  # Increase fold number\n",
        "  fold_no = fold_no + 1 \n",
        "history_list_cv2.append(history)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 709
        },
        "id": "l0zZmRSU59nN",
        "outputId": "52c64e2a-ffab-4397-ca07-4612164c7918"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "------------------------------------------------------------------------\n",
            "Training for Outer fold 1 ...\n",
            "Epoch 1/1000\n",
            "4000/4000 - 11s - loss: 21.5003 - mse: 21.5003 - mae: 1.5880 - 11s/epoch - 3ms/step\n",
            "Epoch 2/1000\n",
            "4000/4000 - 10s - loss: 20.4470 - mse: 20.4470 - mae: 1.5312 - 10s/epoch - 3ms/step\n",
            "Epoch 3/1000\n",
            "4000/4000 - 10s - loss: 20.1788 - mse: 20.1788 - mae: 1.5167 - 10s/epoch - 2ms/step\n",
            "Epoch 4/1000\n",
            "4000/4000 - 10s - loss: 19.9602 - mse: 19.9602 - mae: 1.5045 - 10s/epoch - 2ms/step\n",
            "Epoch 5/1000\n",
            "4000/4000 - 10s - loss: 19.8158 - mse: 19.8158 - mae: 1.4948 - 10s/epoch - 3ms/step\n",
            "Epoch 6/1000\n",
            "4000/4000 - 10s - loss: 19.6828 - mse: 19.6828 - mae: 1.4866 - 10s/epoch - 2ms/step\n",
            "Epoch 7/1000\n",
            "4000/4000 - 10s - loss: 19.6309 - mse: 19.6309 - mae: 1.4812 - 10s/epoch - 2ms/step\n",
            "Epoch 8/1000\n",
            "4000/4000 - 10s - loss: 19.6343 - mse: 19.6343 - mae: 1.4812 - 10s/epoch - 2ms/step\n",
            "Epoch 9/1000\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-46-128103057dca>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     20\u001b[0m                 \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m                 \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m                 callbacks=[es])\n\u001b[0m\u001b[1;32m     23\u001b[0m   \u001b[0mscores\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel_2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouter_cv_val\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mouter_cv_label\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'Score for fold {fold_no}: {model_2.metrics_names[0]} of {scores[0]}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1382\u001b[0m                 _r=1):\n\u001b[1;32m   1383\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1384\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1385\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1386\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    913\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    914\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 915\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    916\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    917\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    945\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    946\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 947\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    948\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2955\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m   2956\u001b[0m     return graph_function._call_flat(\n\u001b[0;32m-> 2957\u001b[0;31m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m   2958\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2959\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1852\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1853\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1854\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1855\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1856\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    502\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    503\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 504\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    505\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    506\u001b[0m           outputs = execute.execute_with_cancellation(\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 55\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     56\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "results = model_2.evaluate(testing, labelsForTest, batch_size=10)"
      ],
      "metadata": {
        "id": "S1kWPGdW6udO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#model_retrain for outer cv (best model 3)\n",
        "#{'activation': 'tanh', 'num_hidden_layer': 4, 'i': 3, 'learning_rate': 0.00010292007837467995}\n",
        "history_list_cv3=[]\n",
        "optimizer = Adam(learning_rate=0.00010292007837467995,clipnorm=1.0)\n",
        "model3 = create_model(activation=\"tanh\", num_hidden_layer=4, num_hidden_unit=8)\n",
        "model3.compile(loss='mse',optimizer=optimizer,metrics=['mse','mae'])\n",
        "kfold = KFold(n_splits=3,shuffle=True)\n",
        "fold_no=1\n",
        "loss_per_fold = []\n",
        "es = EarlyStopping(monitor='mse',mode='min',patience=10)\n",
        "for (train,test) in kfold.split(training,labelsForTrain):\n",
        "\n",
        "  print('------------------------------------------------------------------------')\n",
        "  print(f'Training for Outer fold {fold_no} ...')\n",
        "  \n",
        "  outer_cv_train=training.iloc[train]\n",
        "  outer_cv_trainlabel=labelsForTrain.iloc[train]\n",
        "\n",
        "  outer_cv_test=training.iloc[test]\n",
        "  outer_cv_testlabel=labelsForTrain.iloc[test]\n",
        "\n",
        "  reduce_lr = ReduceLROnPlateau(monitor='val_mse', factor=0.2,\n",
        "                              patience=5, min_lr=0.001)\n",
        "  # Fit data to model\n",
        "  history = model3.fit(outer_cv_train, outer_cv_trainlabel,\n",
        "                  batch_size=64,\n",
        "                  epochs=500,\n",
        "                  verbose=2,\n",
        "                  validation_data=(outer_cv_test,outer_cv_testlabel),\n",
        "                  validation_batch_size=64,\n",
        "                  callbacks=[es,reduce_lr])\n",
        "  \n",
        "  scores=model3.evaluate(outer_cv_test,outer_cv_testlabel,verbose=0)\n",
        "  print(scores)\n",
        "  print(f'Score for fold {fold_no}: {model3.metrics_names[0]} of {scores[0]}')\n",
        "  loss_per_fold.append(scores[0])\n",
        "  # Increase fold number\n",
        "  fold_no = fold_no + 1 \n",
        "history_list_cv3.append(history)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YCg8pOzH6aZK",
        "outputId": "69ff09eb-90d2-4f5c-ab02-46739d441019"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "------------------------------------------------------------------------\n",
            "Training for Outer fold 1 ...\n",
            "Epoch 1/500\n",
            "834/834 - 2s - loss: 36.4972 - mse: 36.4972 - mae: 2.3267 - val_loss: 17.4396 - val_mse: 17.4396 - val_mae: 1.8788 - lr: 1.0292e-04 - 2s/epoch - 3ms/step\n",
            "Epoch 2/500\n",
            "834/834 - 2s - loss: 31.9206 - mse: 31.9206 - mae: 1.7468 - val_loss: 14.1429 - val_mse: 14.1429 - val_mae: 1.6416 - lr: 1.0292e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 3/500\n",
            "834/834 - 2s - loss: 29.9957 - mse: 29.9957 - mae: 1.7306 - val_loss: 13.5027 - val_mse: 13.5027 - val_mae: 1.6827 - lr: 1.0292e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 4/500\n",
            "834/834 - 2s - loss: 29.5711 - mse: 29.5711 - mae: 1.7270 - val_loss: 13.2361 - val_mse: 13.2361 - val_mae: 1.6572 - lr: 1.0292e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 5/500\n",
            "834/834 - 2s - loss: 29.3235 - mse: 29.3235 - mae: 1.7041 - val_loss: 13.0107 - val_mse: 13.0107 - val_mae: 1.6428 - lr: 1.0292e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 6/500\n",
            "834/834 - 2s - loss: 29.1044 - mse: 29.1044 - mae: 1.6863 - val_loss: 12.8055 - val_mse: 12.8055 - val_mae: 1.6210 - lr: 1.0292e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 7/500\n",
            "834/834 - 2s - loss: 28.8921 - mse: 28.8921 - mae: 1.6701 - val_loss: 12.6358 - val_mse: 12.6358 - val_mae: 1.5970 - lr: 1.0292e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 8/500\n",
            "834/834 - 2s - loss: 28.7210 - mse: 28.7210 - mae: 1.6537 - val_loss: 12.4879 - val_mse: 12.4879 - val_mae: 1.5958 - lr: 1.0292e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 9/500\n",
            "834/834 - 2s - loss: 28.5604 - mse: 28.5604 - mae: 1.6481 - val_loss: 12.3593 - val_mse: 12.3593 - val_mae: 1.5961 - lr: 1.0292e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 10/500\n",
            "834/834 - 2s - loss: 28.4121 - mse: 28.4121 - mae: 1.6433 - val_loss: 12.2499 - val_mse: 12.2499 - val_mae: 1.5876 - lr: 1.0292e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 11/500\n",
            "834/834 - 2s - loss: 28.2924 - mse: 28.2924 - mae: 1.6355 - val_loss: 12.1564 - val_mse: 12.1564 - val_mae: 1.5785 - lr: 1.0292e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 12/500\n",
            "834/834 - 2s - loss: 28.1791 - mse: 28.1791 - mae: 1.6317 - val_loss: 12.0723 - val_mse: 12.0723 - val_mae: 1.5833 - lr: 1.0292e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 13/500\n",
            "834/834 - 2s - loss: 28.0915 - mse: 28.0915 - mae: 1.6294 - val_loss: 12.0038 - val_mse: 12.0038 - val_mae: 1.5840 - lr: 1.0292e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 14/500\n",
            "834/834 - 2s - loss: 28.0117 - mse: 28.0117 - mae: 1.6262 - val_loss: 11.9397 - val_mse: 11.9397 - val_mae: 1.5757 - lr: 1.0292e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 15/500\n",
            "834/834 - 2s - loss: 27.9368 - mse: 27.9368 - mae: 1.6240 - val_loss: 11.8845 - val_mse: 11.8845 - val_mae: 1.5662 - lr: 1.0292e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 16/500\n",
            "834/834 - 2s - loss: 27.8646 - mse: 27.8646 - mae: 1.6203 - val_loss: 11.8271 - val_mse: 11.8271 - val_mae: 1.5765 - lr: 1.0292e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 17/500\n",
            "834/834 - 2s - loss: 27.8008 - mse: 27.8008 - mae: 1.6194 - val_loss: 11.7711 - val_mse: 11.7711 - val_mae: 1.5668 - lr: 1.0292e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 18/500\n",
            "834/834 - 2s - loss: 27.7346 - mse: 27.7346 - mae: 1.6153 - val_loss: 11.7232 - val_mse: 11.7232 - val_mae: 1.5575 - lr: 1.0292e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 19/500\n",
            "834/834 - 2s - loss: 27.6682 - mse: 27.6682 - mae: 1.6143 - val_loss: 11.6788 - val_mse: 11.6788 - val_mae: 1.5654 - lr: 1.0292e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 20/500\n",
            "834/834 - 2s - loss: 27.6180 - mse: 27.6180 - mae: 1.6119 - val_loss: 11.6353 - val_mse: 11.6353 - val_mae: 1.5612 - lr: 1.0292e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 21/500\n",
            "834/834 - 2s - loss: 27.5592 - mse: 27.5592 - mae: 1.6119 - val_loss: 11.5947 - val_mse: 11.5947 - val_mae: 1.5484 - lr: 1.0292e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 22/500\n",
            "834/834 - 2s - loss: 27.5057 - mse: 27.5057 - mae: 1.6061 - val_loss: 11.5432 - val_mse: 11.5432 - val_mae: 1.5598 - lr: 1.0292e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 23/500\n",
            "834/834 - 2s - loss: 27.4489 - mse: 27.4489 - mae: 1.6057 - val_loss: 11.5085 - val_mse: 11.5085 - val_mae: 1.5613 - lr: 1.0292e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 24/500\n",
            "834/834 - 2s - loss: 27.3966 - mse: 27.3966 - mae: 1.6024 - val_loss: 11.4710 - val_mse: 11.4710 - val_mae: 1.5559 - lr: 1.0292e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 25/500\n",
            "834/834 - 2s - loss: 27.3572 - mse: 27.3572 - mae: 1.5998 - val_loss: 11.4302 - val_mse: 11.4302 - val_mae: 1.5529 - lr: 1.0292e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 26/500\n",
            "834/834 - 2s - loss: 27.2991 - mse: 27.2991 - mae: 1.6005 - val_loss: 11.3921 - val_mse: 11.3921 - val_mae: 1.5642 - lr: 1.0292e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 27/500\n",
            "834/834 - 2s - loss: 27.2586 - mse: 27.2586 - mae: 1.5984 - val_loss: 11.3539 - val_mse: 11.3539 - val_mae: 1.5485 - lr: 1.0292e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 28/500\n",
            "834/834 - 2s - loss: 27.2087 - mse: 27.2087 - mae: 1.5973 - val_loss: 11.3314 - val_mse: 11.3314 - val_mae: 1.5417 - lr: 1.0292e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 29/500\n",
            "834/834 - 2s - loss: 27.1745 - mse: 27.1745 - mae: 1.5947 - val_loss: 11.2941 - val_mse: 11.2941 - val_mae: 1.5387 - lr: 1.0292e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 30/500\n",
            "834/834 - 2s - loss: 27.1180 - mse: 27.1180 - mae: 1.5912 - val_loss: 11.2587 - val_mse: 11.2587 - val_mae: 1.5422 - lr: 1.0292e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 31/500\n",
            "834/834 - 2s - loss: 27.0867 - mse: 27.0867 - mae: 1.5930 - val_loss: 11.2263 - val_mse: 11.2263 - val_mae: 1.5382 - lr: 1.0292e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 32/500\n",
            "834/834 - 2s - loss: 27.0510 - mse: 27.0510 - mae: 1.5887 - val_loss: 11.1912 - val_mse: 11.1912 - val_mae: 1.5460 - lr: 1.0292e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 33/500\n",
            "834/834 - 2s - loss: 27.0137 - mse: 27.0137 - mae: 1.5872 - val_loss: 11.1630 - val_mse: 11.1630 - val_mae: 1.5350 - lr: 1.0292e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 34/500\n",
            "834/834 - 2s - loss: 26.9726 - mse: 26.9726 - mae: 1.5839 - val_loss: 11.1429 - val_mse: 11.1429 - val_mae: 1.5326 - lr: 1.0292e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 35/500\n",
            "834/834 - 2s - loss: 26.9338 - mse: 26.9338 - mae: 1.5856 - val_loss: 11.1108 - val_mse: 11.1108 - val_mae: 1.5298 - lr: 1.0292e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 36/500\n",
            "834/834 - 2s - loss: 26.8953 - mse: 26.8953 - mae: 1.5837 - val_loss: 11.0851 - val_mse: 11.0851 - val_mae: 1.5450 - lr: 1.0292e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 37/500\n",
            "834/834 - 2s - loss: 26.8576 - mse: 26.8576 - mae: 1.5870 - val_loss: 11.0545 - val_mse: 11.0545 - val_mae: 1.5251 - lr: 1.0292e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 38/500\n",
            "834/834 - 2s - loss: 26.8250 - mse: 26.8250 - mae: 1.5787 - val_loss: 11.0234 - val_mse: 11.0234 - val_mae: 1.5361 - lr: 1.0292e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 39/500\n",
            "834/834 - 2s - loss: 26.7834 - mse: 26.7834 - mae: 1.5788 - val_loss: 11.0031 - val_mse: 11.0031 - val_mae: 1.5421 - lr: 1.0292e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 40/500\n",
            "834/834 - 2s - loss: 26.7518 - mse: 26.7518 - mae: 1.5809 - val_loss: 10.9803 - val_mse: 10.9803 - val_mae: 1.5259 - lr: 1.0292e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 41/500\n",
            "834/834 - 2s - loss: 26.7147 - mse: 26.7147 - mae: 1.5788 - val_loss: 10.9531 - val_mse: 10.9531 - val_mae: 1.5238 - lr: 1.0292e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 42/500\n",
            "834/834 - 2s - loss: 26.6874 - mse: 26.6874 - mae: 1.5714 - val_loss: 10.9216 - val_mse: 10.9216 - val_mae: 1.5347 - lr: 1.0292e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 43/500\n",
            "834/834 - 2s - loss: 26.6500 - mse: 26.6500 - mae: 1.5753 - val_loss: 10.9053 - val_mse: 10.9053 - val_mae: 1.5128 - lr: 1.0292e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 44/500\n",
            "834/834 - 2s - loss: 26.6106 - mse: 26.6106 - mae: 1.5697 - val_loss: 10.8768 - val_mse: 10.8768 - val_mae: 1.5278 - lr: 1.0292e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 45/500\n",
            "834/834 - 2s - loss: 26.5815 - mse: 26.5815 - mae: 1.5701 - val_loss: 10.8507 - val_mse: 10.8507 - val_mae: 1.5178 - lr: 1.0292e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 46/500\n",
            "834/834 - 2s - loss: 26.5564 - mse: 26.5564 - mae: 1.5702 - val_loss: 10.8218 - val_mse: 10.8218 - val_mae: 1.5314 - lr: 1.0292e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 47/500\n",
            "834/834 - 2s - loss: 26.5230 - mse: 26.5230 - mae: 1.5666 - val_loss: 10.7986 - val_mse: 10.7986 - val_mae: 1.5427 - lr: 1.0292e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 48/500\n",
            "834/834 - 2s - loss: 26.4880 - mse: 26.4880 - mae: 1.5688 - val_loss: 10.7815 - val_mse: 10.7815 - val_mae: 1.5225 - lr: 1.0292e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 49/500\n",
            "834/834 - 2s - loss: 26.4491 - mse: 26.4491 - mae: 1.5650 - val_loss: 10.7512 - val_mse: 10.7512 - val_mae: 1.5248 - lr: 1.0292e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 50/500\n",
            "834/834 - 2s - loss: 26.4207 - mse: 26.4207 - mae: 1.5656 - val_loss: 10.7312 - val_mse: 10.7312 - val_mae: 1.5116 - lr: 1.0292e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 51/500\n",
            "834/834 - 2s - loss: 26.3873 - mse: 26.3873 - mae: 1.5618 - val_loss: 10.7121 - val_mse: 10.7121 - val_mae: 1.5218 - lr: 1.0292e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 52/500\n",
            "834/834 - 2s - loss: 26.3583 - mse: 26.3583 - mae: 1.5612 - val_loss: 10.6805 - val_mse: 10.6805 - val_mae: 1.5165 - lr: 1.0292e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 53/500\n",
            "834/834 - 2s - loss: 26.3188 - mse: 26.3188 - mae: 1.5608 - val_loss: 10.6685 - val_mse: 10.6685 - val_mae: 1.5108 - lr: 1.0292e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 54/500\n",
            "834/834 - 2s - loss: 26.2954 - mse: 26.2954 - mae: 1.5567 - val_loss: 10.6404 - val_mse: 10.6404 - val_mae: 1.5216 - lr: 1.0292e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 55/500\n",
            "834/834 - 2s - loss: 26.2542 - mse: 26.2542 - mae: 1.5577 - val_loss: 10.6327 - val_mse: 10.6327 - val_mae: 1.4953 - lr: 1.0292e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 56/500\n",
            "834/834 - 2s - loss: 26.2258 - mse: 26.2258 - mae: 1.5554 - val_loss: 10.6018 - val_mse: 10.6018 - val_mae: 1.5028 - lr: 1.0292e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 57/500\n",
            "834/834 - 2s - loss: 26.1989 - mse: 26.1989 - mae: 1.5533 - val_loss: 10.5795 - val_mse: 10.5795 - val_mae: 1.5105 - lr: 1.0292e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 58/500\n",
            "834/834 - 2s - loss: 26.1830 - mse: 26.1830 - mae: 1.5546 - val_loss: 10.5705 - val_mse: 10.5705 - val_mae: 1.5101 - lr: 1.0292e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 59/500\n",
            "834/834 - 2s - loss: 26.1468 - mse: 26.1468 - mae: 1.5511 - val_loss: 10.5551 - val_mse: 10.5551 - val_mae: 1.5088 - lr: 1.0292e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 60/500\n",
            "834/834 - 2s - loss: 26.1119 - mse: 26.1119 - mae: 1.5488 - val_loss: 10.5324 - val_mse: 10.5324 - val_mae: 1.4916 - lr: 1.0292e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 61/500\n",
            "834/834 - 2s - loss: 26.0878 - mse: 26.0878 - mae: 1.5496 - val_loss: 10.5018 - val_mse: 10.5018 - val_mae: 1.5060 - lr: 1.0292e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 62/500\n",
            "834/834 - 2s - loss: 26.0569 - mse: 26.0569 - mae: 1.5490 - val_loss: 10.4824 - val_mse: 10.4824 - val_mae: 1.5112 - lr: 1.0292e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 63/500\n",
            "834/834 - 2s - loss: 26.0228 - mse: 26.0228 - mae: 1.5451 - val_loss: 10.4719 - val_mse: 10.4719 - val_mae: 1.5004 - lr: 1.0292e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 64/500\n",
            "834/834 - 2s - loss: 26.0049 - mse: 26.0049 - mae: 1.5464 - val_loss: 10.4536 - val_mse: 10.4536 - val_mae: 1.4996 - lr: 1.0292e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 65/500\n",
            "834/834 - 2s - loss: 25.9804 - mse: 25.9804 - mae: 1.5460 - val_loss: 10.4493 - val_mse: 10.4493 - val_mae: 1.4853 - lr: 1.0292e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 66/500\n",
            "834/834 - 2s - loss: 25.9624 - mse: 25.9624 - mae: 1.5419 - val_loss: 10.4323 - val_mse: 10.4323 - val_mae: 1.5061 - lr: 1.0292e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 67/500\n",
            "834/834 - 2s - loss: 25.9382 - mse: 25.9382 - mae: 1.5446 - val_loss: 10.4160 - val_mse: 10.4160 - val_mae: 1.4895 - lr: 1.0292e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 68/500\n",
            "834/834 - 2s - loss: 25.9180 - mse: 25.9180 - mae: 1.5392 - val_loss: 10.3857 - val_mse: 10.3857 - val_mae: 1.5081 - lr: 1.0292e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 69/500\n",
            "834/834 - 2s - loss: 25.8865 - mse: 25.8865 - mae: 1.5417 - val_loss: 10.3690 - val_mse: 10.3690 - val_mae: 1.4966 - lr: 1.0292e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 70/500\n",
            "834/834 - 2s - loss: 25.8637 - mse: 25.8637 - mae: 1.5411 - val_loss: 10.3791 - val_mse: 10.3791 - val_mae: 1.4807 - lr: 1.0292e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 71/500\n",
            "834/834 - 2s - loss: 25.8458 - mse: 25.8458 - mae: 1.5349 - val_loss: 10.3521 - val_mse: 10.3521 - val_mae: 1.5079 - lr: 1.0292e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 72/500\n",
            "834/834 - 2s - loss: 25.8234 - mse: 25.8234 - mae: 1.5405 - val_loss: 10.3422 - val_mse: 10.3422 - val_mae: 1.4802 - lr: 1.0292e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 73/500\n",
            "834/834 - 2s - loss: 25.8038 - mse: 25.8038 - mae: 1.5330 - val_loss: 10.3432 - val_mse: 10.3432 - val_mae: 1.4964 - lr: 1.0292e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 74/500\n",
            "834/834 - 2s - loss: 25.7900 - mse: 25.7900 - mae: 1.5382 - val_loss: 10.3144 - val_mse: 10.3144 - val_mae: 1.5006 - lr: 1.0292e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 75/500\n",
            "834/834 - 2s - loss: 25.7643 - mse: 25.7643 - mae: 1.5356 - val_loss: 10.3049 - val_mse: 10.3049 - val_mae: 1.4879 - lr: 1.0292e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 76/500\n",
            "834/834 - 2s - loss: 25.7384 - mse: 25.7384 - mae: 1.5342 - val_loss: 10.2858 - val_mse: 10.2858 - val_mae: 1.4876 - lr: 1.0292e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 77/500\n",
            "834/834 - 2s - loss: 25.7200 - mse: 25.7200 - mae: 1.5329 - val_loss: 10.2731 - val_mse: 10.2731 - val_mae: 1.5016 - lr: 1.0292e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 78/500\n",
            "834/834 - 2s - loss: 25.6971 - mse: 25.6971 - mae: 1.5339 - val_loss: 10.2629 - val_mse: 10.2629 - val_mae: 1.5055 - lr: 1.0292e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 79/500\n",
            "834/834 - 2s - loss: 25.6793 - mse: 25.6793 - mae: 1.5339 - val_loss: 10.2595 - val_mse: 10.2595 - val_mae: 1.4806 - lr: 1.0292e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 80/500\n",
            "834/834 - 2s - loss: 25.6662 - mse: 25.6662 - mae: 1.5346 - val_loss: 10.2390 - val_mse: 10.2390 - val_mae: 1.4839 - lr: 1.0292e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 81/500\n",
            "834/834 - 2s - loss: 25.6367 - mse: 25.6367 - mae: 1.5325 - val_loss: 10.2455 - val_mse: 10.2455 - val_mae: 1.4786 - lr: 1.0292e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 82/500\n",
            "834/834 - 2s - loss: 25.6291 - mse: 25.6291 - mae: 1.5308 - val_loss: 10.2085 - val_mse: 10.2085 - val_mae: 1.4873 - lr: 1.0292e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 83/500\n",
            "834/834 - 2s - loss: 25.6049 - mse: 25.6049 - mae: 1.5323 - val_loss: 10.2095 - val_mse: 10.2095 - val_mae: 1.5084 - lr: 1.0292e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 84/500\n",
            "834/834 - 2s - loss: 25.5836 - mse: 25.5836 - mae: 1.5304 - val_loss: 10.2102 - val_mse: 10.2102 - val_mae: 1.4872 - lr: 1.0292e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 85/500\n",
            "834/834 - 2s - loss: 25.5790 - mse: 25.5790 - mae: 1.5292 - val_loss: 10.1860 - val_mse: 10.1860 - val_mae: 1.4988 - lr: 1.0292e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 86/500\n",
            "834/834 - 2s - loss: 25.5524 - mse: 25.5524 - mae: 1.5309 - val_loss: 10.1807 - val_mse: 10.1807 - val_mae: 1.4938 - lr: 1.0292e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 87/500\n",
            "834/834 - 2s - loss: 25.5366 - mse: 25.5366 - mae: 1.5281 - val_loss: 10.1604 - val_mse: 10.1604 - val_mae: 1.4895 - lr: 1.0292e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 88/500\n",
            "834/834 - 2s - loss: 25.5289 - mse: 25.5289 - mae: 1.5275 - val_loss: 10.1622 - val_mse: 10.1622 - val_mae: 1.4912 - lr: 1.0292e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 89/500\n",
            "834/834 - 2s - loss: 25.5049 - mse: 25.5049 - mae: 1.5294 - val_loss: 10.1499 - val_mse: 10.1499 - val_mae: 1.4886 - lr: 1.0292e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 90/500\n",
            "834/834 - 2s - loss: 25.4842 - mse: 25.4842 - mae: 1.5246 - val_loss: 10.1299 - val_mse: 10.1299 - val_mae: 1.4952 - lr: 1.0292e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 91/500\n",
            "834/834 - 2s - loss: 25.4728 - mse: 25.4728 - mae: 1.5277 - val_loss: 10.1400 - val_mse: 10.1400 - val_mae: 1.4833 - lr: 1.0292e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 92/500\n",
            "834/834 - 2s - loss: 25.4559 - mse: 25.4559 - mae: 1.5273 - val_loss: 10.1388 - val_mse: 10.1388 - val_mae: 1.4823 - lr: 1.0292e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 93/500\n",
            "834/834 - 2s - loss: 25.4394 - mse: 25.4394 - mae: 1.5234 - val_loss: 10.1423 - val_mse: 10.1423 - val_mae: 1.4759 - lr: 1.0292e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 94/500\n",
            "834/834 - 2s - loss: 25.4307 - mse: 25.4307 - mae: 1.5242 - val_loss: 10.1079 - val_mse: 10.1079 - val_mae: 1.4868 - lr: 1.0292e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 95/500\n",
            "834/834 - 2s - loss: 25.4078 - mse: 25.4078 - mae: 1.5220 - val_loss: 10.1141 - val_mse: 10.1141 - val_mae: 1.4861 - lr: 1.0292e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 96/500\n",
            "834/834 - 2s - loss: 25.3979 - mse: 25.3979 - mae: 1.5224 - val_loss: 10.1016 - val_mse: 10.1016 - val_mae: 1.4822 - lr: 1.0292e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 97/500\n",
            "834/834 - 2s - loss: 25.3722 - mse: 25.3722 - mae: 1.5263 - val_loss: 10.1214 - val_mse: 10.1214 - val_mae: 1.4606 - lr: 1.0292e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 98/500\n",
            "834/834 - 2s - loss: 25.3687 - mse: 25.3687 - mae: 1.5215 - val_loss: 10.0806 - val_mse: 10.0806 - val_mae: 1.4791 - lr: 1.0292e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 99/500\n",
            "834/834 - 2s - loss: 25.3372 - mse: 25.3372 - mae: 1.5242 - val_loss: 10.1084 - val_mse: 10.1084 - val_mae: 1.4729 - lr: 1.0292e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 100/500\n",
            "834/834 - 2s - loss: 25.3405 - mse: 25.3405 - mae: 1.5220 - val_loss: 10.0525 - val_mse: 10.0525 - val_mae: 1.4854 - lr: 1.0292e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 101/500\n",
            "834/834 - 2s - loss: 25.3224 - mse: 25.3224 - mae: 1.5207 - val_loss: 10.0573 - val_mse: 10.0573 - val_mae: 1.4854 - lr: 1.0292e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 102/500\n",
            "834/834 - 2s - loss: 25.3082 - mse: 25.3082 - mae: 1.5228 - val_loss: 10.0394 - val_mse: 10.0394 - val_mae: 1.4876 - lr: 1.0292e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 103/500\n",
            "834/834 - 2s - loss: 25.2927 - mse: 25.2927 - mae: 1.5212 - val_loss: 10.0312 - val_mse: 10.0312 - val_mae: 1.4939 - lr: 1.0292e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 104/500\n",
            "834/834 - 2s - loss: 25.2795 - mse: 25.2795 - mae: 1.5191 - val_loss: 10.0195 - val_mse: 10.0195 - val_mae: 1.4911 - lr: 1.0292e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 105/500\n",
            "834/834 - 2s - loss: 25.2720 - mse: 25.2720 - mae: 1.5231 - val_loss: 10.0636 - val_mse: 10.0636 - val_mae: 1.4665 - lr: 1.0292e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 106/500\n",
            "834/834 - 2s - loss: 25.2501 - mse: 25.2501 - mae: 1.5188 - val_loss: 10.0078 - val_mse: 10.0078 - val_mae: 1.4912 - lr: 1.0292e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 107/500\n",
            "834/834 - 2s - loss: 25.2297 - mse: 25.2297 - mae: 1.5197 - val_loss: 10.0108 - val_mse: 10.0108 - val_mae: 1.4727 - lr: 1.0292e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 108/500\n",
            "834/834 - 2s - loss: 25.2265 - mse: 25.2265 - mae: 1.5173 - val_loss: 9.9985 - val_mse: 9.9985 - val_mae: 1.4977 - lr: 1.0292e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 109/500\n",
            "834/834 - 2s - loss: 25.2210 - mse: 25.2210 - mae: 1.5198 - val_loss: 9.9962 - val_mse: 9.9962 - val_mae: 1.4877 - lr: 1.0292e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 110/500\n",
            "834/834 - 2s - loss: 25.2003 - mse: 25.2003 - mae: 1.5190 - val_loss: 9.9871 - val_mse: 9.9871 - val_mae: 1.4767 - lr: 1.0292e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 111/500\n",
            "834/834 - 2s - loss: 25.1823 - mse: 25.1823 - mae: 1.5170 - val_loss: 9.9860 - val_mse: 9.9860 - val_mae: 1.4681 - lr: 1.0292e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 112/500\n",
            "834/834 - 2s - loss: 25.1754 - mse: 25.1754 - mae: 1.5142 - val_loss: 9.9807 - val_mse: 9.9807 - val_mae: 1.4914 - lr: 1.0292e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 113/500\n",
            "834/834 - 2s - loss: 25.1642 - mse: 25.1642 - mae: 1.5164 - val_loss: 9.9983 - val_mse: 9.9983 - val_mae: 1.4669 - lr: 1.0292e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 114/500\n",
            "834/834 - 2s - loss: 25.1556 - mse: 25.1556 - mae: 1.5130 - val_loss: 9.9746 - val_mse: 9.9746 - val_mae: 1.4686 - lr: 1.0292e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 115/500\n",
            "834/834 - 2s - loss: 25.1507 - mse: 25.1507 - mae: 1.5165 - val_loss: 9.9476 - val_mse: 9.9476 - val_mae: 1.4798 - lr: 1.0292e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 116/500\n",
            "834/834 - 2s - loss: 25.1305 - mse: 25.1305 - mae: 1.5151 - val_loss: 9.9495 - val_mse: 9.9495 - val_mae: 1.4693 - lr: 1.0292e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 117/500\n",
            "834/834 - 2s - loss: 25.1065 - mse: 25.1065 - mae: 1.5144 - val_loss: 9.9498 - val_mse: 9.9498 - val_mae: 1.4594 - lr: 1.0292e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 118/500\n",
            "834/834 - 2s - loss: 25.1014 - mse: 25.1014 - mae: 1.5128 - val_loss: 9.9288 - val_mse: 9.9288 - val_mae: 1.4952 - lr: 1.0292e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 119/500\n",
            "834/834 - 2s - loss: 25.0838 - mse: 25.0838 - mae: 1.5169 - val_loss: 9.9437 - val_mse: 9.9437 - val_mae: 1.4726 - lr: 1.0292e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 120/500\n",
            "834/834 - 2s - loss: 25.0797 - mse: 25.0797 - mae: 1.5130 - val_loss: 9.9237 - val_mse: 9.9237 - val_mae: 1.4852 - lr: 1.0292e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 121/500\n",
            "834/834 - 2s - loss: 25.0659 - mse: 25.0659 - mae: 1.5161 - val_loss: 9.9386 - val_mse: 9.9386 - val_mae: 1.4765 - lr: 1.0292e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 122/500\n",
            "834/834 - 2s - loss: 25.0601 - mse: 25.0601 - mae: 1.5145 - val_loss: 9.9210 - val_mse: 9.9210 - val_mae: 1.4577 - lr: 1.0292e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 123/500\n",
            "834/834 - 2s - loss: 25.0527 - mse: 25.0527 - mae: 1.5135 - val_loss: 9.9085 - val_mse: 9.9085 - val_mae: 1.4720 - lr: 1.0292e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 124/500\n",
            "834/834 - 2s - loss: 25.0470 - mse: 25.0470 - mae: 1.5134 - val_loss: 9.8949 - val_mse: 9.8949 - val_mae: 1.4882 - lr: 1.0292e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 125/500\n",
            "834/834 - 2s - loss: 25.0212 - mse: 25.0212 - mae: 1.5100 - val_loss: 9.9039 - val_mse: 9.9039 - val_mae: 1.4614 - lr: 1.0292e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 126/500\n",
            "834/834 - 2s - loss: 25.0025 - mse: 25.0025 - mae: 1.5135 - val_loss: 9.8867 - val_mse: 9.8867 - val_mae: 1.4787 - lr: 1.0292e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 127/500\n",
            "834/834 - 2s - loss: 24.9984 - mse: 24.9984 - mae: 1.5117 - val_loss: 9.8827 - val_mse: 9.8827 - val_mae: 1.4904 - lr: 1.0292e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 128/500\n",
            "834/834 - 2s - loss: 25.0034 - mse: 25.0034 - mae: 1.5125 - val_loss: 9.8645 - val_mse: 9.8645 - val_mae: 1.4837 - lr: 1.0292e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 129/500\n",
            "834/834 - 2s - loss: 24.9805 - mse: 24.9805 - mae: 1.5124 - val_loss: 9.8810 - val_mse: 9.8810 - val_mae: 1.4719 - lr: 1.0292e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 130/500\n",
            "834/834 - 2s - loss: 24.9741 - mse: 24.9741 - mae: 1.5114 - val_loss: 9.8601 - val_mse: 9.8601 - val_mae: 1.4866 - lr: 1.0292e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 131/500\n",
            "834/834 - 2s - loss: 24.9631 - mse: 24.9631 - mae: 1.5114 - val_loss: 9.8524 - val_mse: 9.8524 - val_mae: 1.4789 - lr: 1.0292e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 132/500\n",
            "834/834 - 2s - loss: 24.9459 - mse: 24.9459 - mae: 1.5140 - val_loss: 9.8674 - val_mse: 9.8674 - val_mae: 1.4553 - lr: 1.0292e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 133/500\n",
            "834/834 - 2s - loss: 24.9288 - mse: 24.9288 - mae: 1.5123 - val_loss: 9.8559 - val_mse: 9.8559 - val_mae: 1.4726 - lr: 1.0292e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 134/500\n",
            "834/834 - 2s - loss: 24.9326 - mse: 24.9326 - mae: 1.5101 - val_loss: 9.8493 - val_mse: 9.8493 - val_mae: 1.4612 - lr: 1.0292e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 135/500\n",
            "834/834 - 2s - loss: 24.9077 - mse: 24.9077 - mae: 1.5107 - val_loss: 9.8412 - val_mse: 9.8412 - val_mae: 1.4789 - lr: 1.0292e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 136/500\n",
            "834/834 - 2s - loss: 24.9105 - mse: 24.9105 - mae: 1.5105 - val_loss: 9.8348 - val_mse: 9.8348 - val_mae: 1.4873 - lr: 1.0292e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 137/500\n",
            "834/834 - 2s - loss: 24.9080 - mse: 24.9080 - mae: 1.5103 - val_loss: 9.8326 - val_mse: 9.8326 - val_mae: 1.4708 - lr: 1.0292e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 138/500\n",
            "834/834 - 2s - loss: 24.8643 - mse: 24.8643 - mae: 1.5075 - val_loss: 9.8690 - val_mse: 9.8690 - val_mae: 1.4575 - lr: 1.0292e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 139/500\n",
            "834/834 - 2s - loss: 24.8636 - mse: 24.8636 - mae: 1.5085 - val_loss: 9.8093 - val_mse: 9.8093 - val_mae: 1.4685 - lr: 1.0292e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 140/500\n",
            "834/834 - 2s - loss: 24.8544 - mse: 24.8544 - mae: 1.5083 - val_loss: 9.8189 - val_mse: 9.8189 - val_mae: 1.4664 - lr: 1.0292e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 141/500\n",
            "834/834 - 2s - loss: 24.8534 - mse: 24.8534 - mae: 1.5052 - val_loss: 9.7980 - val_mse: 9.7980 - val_mae: 1.4930 - lr: 1.0292e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 142/500\n",
            "834/834 - 2s - loss: 24.8374 - mse: 24.8374 - mae: 1.5107 - val_loss: 9.8081 - val_mse: 9.8081 - val_mae: 1.4689 - lr: 1.0292e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 143/500\n",
            "834/834 - 2s - loss: 24.8276 - mse: 24.8276 - mae: 1.5072 - val_loss: 9.8274 - val_mse: 9.8274 - val_mae: 1.4647 - lr: 1.0292e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 144/500\n",
            "834/834 - 2s - loss: 24.8259 - mse: 24.8259 - mae: 1.5087 - val_loss: 9.7837 - val_mse: 9.7837 - val_mae: 1.4804 - lr: 1.0292e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 145/500\n",
            "834/834 - 2s - loss: 24.8289 - mse: 24.8289 - mae: 1.5057 - val_loss: 9.8086 - val_mse: 9.8086 - val_mae: 1.4582 - lr: 1.0292e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 146/500\n",
            "834/834 - 2s - loss: 24.8096 - mse: 24.8096 - mae: 1.5031 - val_loss: 9.7685 - val_mse: 9.7685 - val_mae: 1.4871 - lr: 1.0292e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 147/500\n",
            "834/834 - 2s - loss: 24.7997 - mse: 24.7997 - mae: 1.5072 - val_loss: 9.7719 - val_mse: 9.7719 - val_mae: 1.4786 - lr: 1.0292e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 148/500\n",
            "834/834 - 2s - loss: 24.7757 - mse: 24.7757 - mae: 1.5075 - val_loss: 9.7773 - val_mse: 9.7773 - val_mae: 1.4852 - lr: 1.0292e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 149/500\n",
            "834/834 - 2s - loss: 24.7923 - mse: 24.7923 - mae: 1.5060 - val_loss: 9.7619 - val_mse: 9.7619 - val_mae: 1.4880 - lr: 1.0292e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 150/500\n",
            "834/834 - 2s - loss: 24.7534 - mse: 24.7534 - mae: 1.5076 - val_loss: 9.7451 - val_mse: 9.7451 - val_mae: 1.4892 - lr: 1.0292e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 151/500\n",
            "834/834 - 2s - loss: 24.7621 - mse: 24.7621 - mae: 1.5042 - val_loss: 9.7519 - val_mse: 9.7519 - val_mae: 1.4872 - lr: 1.0292e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 152/500\n",
            "834/834 - 2s - loss: 24.7550 - mse: 24.7550 - mae: 1.5052 - val_loss: 9.7698 - val_mse: 9.7698 - val_mae: 1.4553 - lr: 1.0292e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 153/500\n",
            "834/834 - 2s - loss: 24.7344 - mse: 24.7344 - mae: 1.5054 - val_loss: 9.7504 - val_mse: 9.7504 - val_mae: 1.4667 - lr: 1.0292e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 154/500\n",
            "834/834 - 2s - loss: 24.7343 - mse: 24.7343 - mae: 1.5048 - val_loss: 9.7349 - val_mse: 9.7349 - val_mae: 1.4898 - lr: 1.0292e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 155/500\n",
            "834/834 - 2s - loss: 24.7303 - mse: 24.7303 - mae: 1.5025 - val_loss: 9.7575 - val_mse: 9.7575 - val_mae: 1.4726 - lr: 1.0292e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 156/500\n",
            "834/834 - 2s - loss: 24.7003 - mse: 24.7003 - mae: 1.5049 - val_loss: 9.7372 - val_mse: 9.7372 - val_mae: 1.4555 - lr: 1.0292e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 157/500\n",
            "834/834 - 2s - loss: 24.7147 - mse: 24.7147 - mae: 1.5053 - val_loss: 9.7222 - val_mse: 9.7222 - val_mae: 1.4875 - lr: 1.0292e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 158/500\n",
            "834/834 - 2s - loss: 24.6880 - mse: 24.6880 - mae: 1.5054 - val_loss: 9.7598 - val_mse: 9.7598 - val_mae: 1.4431 - lr: 1.0292e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 159/500\n",
            "834/834 - 2s - loss: 24.6810 - mse: 24.6810 - mae: 1.5040 - val_loss: 9.7376 - val_mse: 9.7376 - val_mae: 1.4693 - lr: 1.0292e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 160/500\n",
            "834/834 - 2s - loss: 24.6873 - mse: 24.6873 - mae: 1.5016 - val_loss: 9.7256 - val_mse: 9.7256 - val_mae: 1.4627 - lr: 1.0292e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 161/500\n",
            "834/834 - 2s - loss: 24.6788 - mse: 24.6788 - mae: 1.5037 - val_loss: 9.7361 - val_mse: 9.7361 - val_mae: 1.4663 - lr: 1.0292e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 162/500\n",
            "834/834 - 2s - loss: 24.6589 - mse: 24.6589 - mae: 1.5048 - val_loss: 9.7224 - val_mse: 9.7224 - val_mae: 1.4635 - lr: 1.0292e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 163/500\n",
            "834/834 - 2s - loss: 24.6581 - mse: 24.6581 - mae: 1.5027 - val_loss: 9.7158 - val_mse: 9.7158 - val_mae: 1.4816 - lr: 1.0292e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 164/500\n",
            "834/834 - 2s - loss: 24.6481 - mse: 24.6481 - mae: 1.5056 - val_loss: 9.7016 - val_mse: 9.7016 - val_mae: 1.4781 - lr: 1.0292e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 165/500\n",
            "834/834 - 2s - loss: 24.6604 - mse: 24.6604 - mae: 1.5047 - val_loss: 9.6899 - val_mse: 9.6899 - val_mae: 1.4919 - lr: 1.0292e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 166/500\n",
            "834/834 - 2s - loss: 24.6509 - mse: 24.6509 - mae: 1.5022 - val_loss: 9.6895 - val_mse: 9.6895 - val_mae: 1.4960 - lr: 1.0292e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 167/500\n",
            "834/834 - 2s - loss: 24.6286 - mse: 24.6286 - mae: 1.5017 - val_loss: 9.7144 - val_mse: 9.7144 - val_mae: 1.4559 - lr: 1.0292e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 168/500\n",
            "834/834 - 2s - loss: 24.6293 - mse: 24.6293 - mae: 1.5014 - val_loss: 9.6695 - val_mse: 9.6695 - val_mae: 1.4773 - lr: 1.0292e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 169/500\n",
            "834/834 - 2s - loss: 24.6153 - mse: 24.6153 - mae: 1.5021 - val_loss: 9.6739 - val_mse: 9.6739 - val_mae: 1.4841 - lr: 1.0292e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 170/500\n",
            "834/834 - 2s - loss: 24.6091 - mse: 24.6091 - mae: 1.5040 - val_loss: 9.6961 - val_mse: 9.6961 - val_mae: 1.4685 - lr: 1.0292e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 171/500\n",
            "834/834 - 2s - loss: 24.6117 - mse: 24.6117 - mae: 1.5036 - val_loss: 9.6781 - val_mse: 9.6781 - val_mae: 1.4622 - lr: 1.0292e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 172/500\n",
            "834/834 - 2s - loss: 24.5828 - mse: 24.5828 - mae: 1.5019 - val_loss: 9.6871 - val_mse: 9.6871 - val_mae: 1.4604 - lr: 1.0292e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 173/500\n",
            "834/834 - 2s - loss: 24.5879 - mse: 24.5879 - mae: 1.5020 - val_loss: 9.6822 - val_mse: 9.6822 - val_mae: 1.4718 - lr: 1.0292e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 174/500\n",
            "834/834 - 2s - loss: 24.5760 - mse: 24.5760 - mae: 1.5019 - val_loss: 9.6623 - val_mse: 9.6623 - val_mae: 1.4808 - lr: 1.0292e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 175/500\n",
            "834/834 - 2s - loss: 24.5667 - mse: 24.5667 - mae: 1.5035 - val_loss: 9.6596 - val_mse: 9.6596 - val_mae: 1.4638 - lr: 1.0292e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 176/500\n",
            "834/834 - 2s - loss: 24.5719 - mse: 24.5719 - mae: 1.5015 - val_loss: 9.6528 - val_mse: 9.6528 - val_mae: 1.4723 - lr: 1.0292e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 177/500\n",
            "834/834 - 2s - loss: 24.5488 - mse: 24.5488 - mae: 1.5014 - val_loss: 9.6650 - val_mse: 9.6650 - val_mae: 1.4586 - lr: 1.0292e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 178/500\n",
            "834/834 - 2s - loss: 24.5450 - mse: 24.5450 - mae: 1.4998 - val_loss: 9.6598 - val_mse: 9.6598 - val_mae: 1.4699 - lr: 1.0292e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 179/500\n",
            "834/834 - 2s - loss: 24.5437 - mse: 24.5437 - mae: 1.4999 - val_loss: 9.6609 - val_mse: 9.6609 - val_mae: 1.4676 - lr: 1.0292e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 180/500\n",
            "834/834 - 2s - loss: 24.5306 - mse: 24.5306 - mae: 1.5000 - val_loss: 9.6312 - val_mse: 9.6312 - val_mae: 1.4770 - lr: 1.0292e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 181/500\n",
            "834/834 - 2s - loss: 24.5133 - mse: 24.5133 - mae: 1.4988 - val_loss: 9.6359 - val_mse: 9.6359 - val_mae: 1.4740 - lr: 1.0292e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 182/500\n",
            "834/834 - 2s - loss: 24.5371 - mse: 24.5371 - mae: 1.4996 - val_loss: 9.6215 - val_mse: 9.6215 - val_mae: 1.4997 - lr: 1.0292e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 183/500\n",
            "834/834 - 2s - loss: 24.5142 - mse: 24.5142 - mae: 1.5021 - val_loss: 9.6310 - val_mse: 9.6310 - val_mae: 1.4741 - lr: 1.0292e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 184/500\n",
            "834/834 - 2s - loss: 24.5014 - mse: 24.5014 - mae: 1.5014 - val_loss: 9.6614 - val_mse: 9.6614 - val_mae: 1.4607 - lr: 1.0292e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 185/500\n",
            "834/834 - 2s - loss: 24.5121 - mse: 24.5121 - mae: 1.4998 - val_loss: 9.6231 - val_mse: 9.6231 - val_mae: 1.4686 - lr: 1.0292e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 186/500\n",
            "834/834 - 2s - loss: 24.5026 - mse: 24.5026 - mae: 1.4991 - val_loss: 9.6201 - val_mse: 9.6201 - val_mae: 1.4900 - lr: 1.0292e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 187/500\n",
            "834/834 - 2s - loss: 24.4893 - mse: 24.4893 - mae: 1.4996 - val_loss: 9.6410 - val_mse: 9.6410 - val_mae: 1.4599 - lr: 1.0292e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 188/500\n",
            "834/834 - 2s - loss: 24.4931 - mse: 24.4931 - mae: 1.4988 - val_loss: 9.6195 - val_mse: 9.6195 - val_mae: 1.4647 - lr: 1.0292e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 189/500\n",
            "834/834 - 2s - loss: 24.4781 - mse: 24.4781 - mae: 1.4985 - val_loss: 9.6088 - val_mse: 9.6088 - val_mae: 1.4732 - lr: 1.0292e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 190/500\n",
            "834/834 - 1s - loss: 24.4986 - mse: 24.4986 - mae: 1.4988 - val_loss: 9.6086 - val_mse: 9.6086 - val_mae: 1.4677 - lr: 1.0292e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 191/500\n",
            "834/834 - 2s - loss: 24.4747 - mse: 24.4747 - mae: 1.4993 - val_loss: 9.5949 - val_mse: 9.5949 - val_mae: 1.4617 - lr: 1.0292e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 192/500\n",
            "834/834 - 2s - loss: 24.4703 - mse: 24.4703 - mae: 1.5000 - val_loss: 9.5999 - val_mse: 9.5999 - val_mae: 1.5023 - lr: 1.0292e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 193/500\n",
            "834/834 - 2s - loss: 24.4512 - mse: 24.4512 - mae: 1.4995 - val_loss: 9.6137 - val_mse: 9.6137 - val_mae: 1.4647 - lr: 1.0292e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 194/500\n",
            "834/834 - 2s - loss: 24.4382 - mse: 24.4382 - mae: 1.4993 - val_loss: 9.6057 - val_mse: 9.6057 - val_mae: 1.4614 - lr: 1.0292e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 195/500\n",
            "834/834 - 2s - loss: 24.4441 - mse: 24.4441 - mae: 1.4980 - val_loss: 9.5910 - val_mse: 9.5910 - val_mae: 1.4904 - lr: 1.0292e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 196/500\n",
            "834/834 - 2s - loss: 24.4552 - mse: 24.4552 - mae: 1.5001 - val_loss: 9.5904 - val_mse: 9.5904 - val_mae: 1.4537 - lr: 1.0292e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 197/500\n",
            "834/834 - 1s - loss: 24.4402 - mse: 24.4402 - mae: 1.4963 - val_loss: 9.6013 - val_mse: 9.6013 - val_mae: 1.4805 - lr: 1.0292e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 198/500\n",
            "834/834 - 2s - loss: 24.4149 - mse: 24.4149 - mae: 1.4991 - val_loss: 9.5851 - val_mse: 9.5851 - val_mae: 1.5073 - lr: 1.0292e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 199/500\n",
            "834/834 - 2s - loss: 24.4248 - mse: 24.4248 - mae: 1.4997 - val_loss: 9.6238 - val_mse: 9.6238 - val_mae: 1.4467 - lr: 1.0292e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 200/500\n",
            "834/834 - 2s - loss: 24.4284 - mse: 24.4284 - mae: 1.4973 - val_loss: 9.6230 - val_mse: 9.6230 - val_mae: 1.4456 - lr: 1.0292e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 201/500\n",
            "834/834 - 2s - loss: 24.4261 - mse: 24.4261 - mae: 1.4960 - val_loss: 9.5743 - val_mse: 9.5743 - val_mae: 1.4748 - lr: 1.0292e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 202/500\n",
            "834/834 - 2s - loss: 24.4126 - mse: 24.4126 - mae: 1.4982 - val_loss: 9.5915 - val_mse: 9.5915 - val_mae: 1.4452 - lr: 1.0292e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 203/500\n",
            "834/834 - 2s - loss: 24.3975 - mse: 24.3975 - mae: 1.4985 - val_loss: 9.5521 - val_mse: 9.5521 - val_mae: 1.4881 - lr: 1.0292e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 204/500\n",
            "834/834 - 2s - loss: 24.3863 - mse: 24.3863 - mae: 1.4999 - val_loss: 9.5660 - val_mse: 9.5660 - val_mae: 1.4659 - lr: 1.0292e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 205/500\n",
            "834/834 - 2s - loss: 24.3822 - mse: 24.3822 - mae: 1.4979 - val_loss: 9.5598 - val_mse: 9.5598 - val_mae: 1.4726 - lr: 1.0292e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 206/500\n",
            "834/834 - 2s - loss: 24.3716 - mse: 24.3716 - mae: 1.4983 - val_loss: 9.5674 - val_mse: 9.5674 - val_mae: 1.4730 - lr: 1.0292e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 207/500\n",
            "834/834 - 2s - loss: 24.3759 - mse: 24.3759 - mae: 1.5012 - val_loss: 9.5594 - val_mse: 9.5594 - val_mae: 1.4568 - lr: 1.0292e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 208/500\n",
            "834/834 - 2s - loss: 24.3599 - mse: 24.3599 - mae: 1.4960 - val_loss: 9.5959 - val_mse: 9.5959 - val_mae: 1.4585 - lr: 1.0292e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 209/500\n",
            "834/834 - 1s - loss: 24.3638 - mse: 24.3638 - mae: 1.4974 - val_loss: 9.5485 - val_mse: 9.5485 - val_mae: 1.4785 - lr: 1.0292e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 210/500\n",
            "834/834 - 2s - loss: 24.3679 - mse: 24.3679 - mae: 1.4977 - val_loss: 9.5516 - val_mse: 9.5516 - val_mae: 1.4654 - lr: 1.0292e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 211/500\n",
            "834/834 - 2s - loss: 24.3595 - mse: 24.3595 - mae: 1.4967 - val_loss: 9.5523 - val_mse: 9.5523 - val_mae: 1.4560 - lr: 1.0292e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 212/500\n",
            "834/834 - 2s - loss: 24.3484 - mse: 24.3484 - mae: 1.4963 - val_loss: 9.5502 - val_mse: 9.5502 - val_mae: 1.4644 - lr: 1.0292e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 213/500\n",
            "834/834 - 2s - loss: 24.3615 - mse: 24.3615 - mae: 1.4975 - val_loss: 9.5373 - val_mse: 9.5373 - val_mae: 1.4610 - lr: 1.0292e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 214/500\n",
            "834/834 - 2s - loss: 24.3461 - mse: 24.3461 - mae: 1.4984 - val_loss: 9.5431 - val_mse: 9.5431 - val_mae: 1.4559 - lr: 1.0292e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 215/500\n",
            "834/834 - 2s - loss: 24.3344 - mse: 24.3344 - mae: 1.4980 - val_loss: 9.5383 - val_mse: 9.5383 - val_mae: 1.4839 - lr: 1.0292e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 216/500\n",
            "834/834 - 2s - loss: 24.3315 - mse: 24.3315 - mae: 1.4981 - val_loss: 9.5332 - val_mse: 9.5332 - val_mae: 1.4669 - lr: 1.0292e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 217/500\n",
            "834/834 - 1s - loss: 24.3297 - mse: 24.3297 - mae: 1.4961 - val_loss: 9.5427 - val_mse: 9.5427 - val_mae: 1.4578 - lr: 1.0292e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 218/500\n",
            "834/834 - 2s - loss: 24.3149 - mse: 24.3149 - mae: 1.4962 - val_loss: 9.5260 - val_mse: 9.5260 - val_mae: 1.4775 - lr: 1.0292e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 219/500\n",
            "834/834 - 2s - loss: 24.3098 - mse: 24.3098 - mae: 1.4985 - val_loss: 9.5212 - val_mse: 9.5212 - val_mae: 1.4599 - lr: 1.0292e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 220/500\n",
            "834/834 - 2s - loss: 24.2842 - mse: 24.2842 - mae: 1.4940 - val_loss: 9.5306 - val_mse: 9.5306 - val_mae: 1.4671 - lr: 1.0292e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 221/500\n",
            "834/834 - 2s - loss: 24.2992 - mse: 24.2992 - mae: 1.4978 - val_loss: 9.5296 - val_mse: 9.5296 - val_mae: 1.4896 - lr: 1.0292e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 222/500\n",
            "834/834 - 2s - loss: 24.2775 - mse: 24.2775 - mae: 1.4954 - val_loss: 9.5052 - val_mse: 9.5052 - val_mae: 1.4780 - lr: 1.0292e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 223/500\n",
            "834/834 - 2s - loss: 24.2859 - mse: 24.2859 - mae: 1.4934 - val_loss: 9.5006 - val_mse: 9.5006 - val_mae: 1.4795 - lr: 1.0292e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 224/500\n",
            "834/834 - 2s - loss: 24.2800 - mse: 24.2800 - mae: 1.4984 - val_loss: 9.5689 - val_mse: 9.5689 - val_mae: 1.4757 - lr: 1.0292e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 225/500\n",
            "834/834 - 2s - loss: 24.2731 - mse: 24.2731 - mae: 1.4964 - val_loss: 9.5174 - val_mse: 9.5174 - val_mae: 1.4644 - lr: 1.0292e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 226/500\n",
            "834/834 - 2s - loss: 24.2887 - mse: 24.2887 - mae: 1.4928 - val_loss: 9.4981 - val_mse: 9.4981 - val_mae: 1.4947 - lr: 1.0292e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 227/500\n",
            "834/834 - 2s - loss: 24.2779 - mse: 24.2779 - mae: 1.4954 - val_loss: 9.5060 - val_mse: 9.5060 - val_mae: 1.4608 - lr: 1.0292e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 228/500\n",
            "834/834 - 2s - loss: 24.2801 - mse: 24.2801 - mae: 1.4935 - val_loss: 9.5140 - val_mse: 9.5140 - val_mae: 1.4718 - lr: 1.0292e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 229/500\n",
            "834/834 - 2s - loss: 24.2665 - mse: 24.2665 - mae: 1.4951 - val_loss: 9.4940 - val_mse: 9.4940 - val_mae: 1.4582 - lr: 1.0292e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 230/500\n",
            "834/834 - 2s - loss: 24.2578 - mse: 24.2578 - mae: 1.4961 - val_loss: 9.4881 - val_mse: 9.4881 - val_mae: 1.4921 - lr: 1.0292e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 231/500\n",
            "834/834 - 2s - loss: 24.2563 - mse: 24.2563 - mae: 1.4939 - val_loss: 9.4920 - val_mse: 9.4920 - val_mae: 1.5050 - lr: 1.0292e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 232/500\n",
            "834/834 - 2s - loss: 24.2321 - mse: 24.2321 - mae: 1.4970 - val_loss: 9.4664 - val_mse: 9.4664 - val_mae: 1.4793 - lr: 1.0292e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 233/500\n",
            "834/834 - 2s - loss: 24.2343 - mse: 24.2343 - mae: 1.4985 - val_loss: 9.4776 - val_mse: 9.4776 - val_mae: 1.4800 - lr: 1.0292e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 234/500\n",
            "834/834 - 2s - loss: 24.2234 - mse: 24.2234 - mae: 1.4924 - val_loss: 9.4718 - val_mse: 9.4718 - val_mae: 1.4882 - lr: 1.0292e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 235/500\n",
            "834/834 - 2s - loss: 24.2096 - mse: 24.2096 - mae: 1.4954 - val_loss: 9.4627 - val_mse: 9.4627 - val_mae: 1.4815 - lr: 1.0292e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 236/500\n",
            "834/834 - 2s - loss: 24.2234 - mse: 24.2234 - mae: 1.4973 - val_loss: 9.4872 - val_mse: 9.4872 - val_mae: 1.4594 - lr: 1.0292e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 237/500\n",
            "834/834 - 1s - loss: 24.2277 - mse: 24.2277 - mae: 1.4925 - val_loss: 9.5048 - val_mse: 9.5048 - val_mae: 1.4614 - lr: 1.0292e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 238/500\n",
            "834/834 - 2s - loss: 24.2091 - mse: 24.2091 - mae: 1.4942 - val_loss: 9.4938 - val_mse: 9.4938 - val_mae: 1.4759 - lr: 1.0292e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 239/500\n",
            "834/834 - 2s - loss: 24.1940 - mse: 24.1940 - mae: 1.4964 - val_loss: 9.4828 - val_mse: 9.4828 - val_mae: 1.4831 - lr: 1.0292e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 240/500\n",
            "834/834 - 2s - loss: 24.2093 - mse: 24.2093 - mae: 1.4955 - val_loss: 9.5250 - val_mse: 9.5250 - val_mae: 1.4510 - lr: 1.0292e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 241/500\n",
            "834/834 - 2s - loss: 24.2084 - mse: 24.2084 - mae: 1.4930 - val_loss: 9.4922 - val_mse: 9.4922 - val_mae: 1.4721 - lr: 1.0292e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 242/500\n",
            "834/834 - 2s - loss: 24.1972 - mse: 24.1972 - mae: 1.4954 - val_loss: 9.4857 - val_mse: 9.4857 - val_mae: 1.4686 - lr: 1.0292e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 243/500\n",
            "834/834 - 2s - loss: 24.1819 - mse: 24.1819 - mae: 1.4953 - val_loss: 9.4773 - val_mse: 9.4773 - val_mae: 1.4674 - lr: 1.0292e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 244/500\n",
            "834/834 - 2s - loss: 24.1917 - mse: 24.1917 - mae: 1.4950 - val_loss: 9.4973 - val_mse: 9.4973 - val_mae: 1.4652 - lr: 1.0292e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 245/500\n",
            "834/834 - 2s - loss: 24.1890 - mse: 24.1890 - mae: 1.4936 - val_loss: 9.4758 - val_mse: 9.4758 - val_mae: 1.4816 - lr: 1.0292e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 246/500\n",
            "834/834 - 2s - loss: 24.1975 - mse: 24.1975 - mae: 1.4953 - val_loss: 9.4949 - val_mse: 9.4949 - val_mae: 1.4661 - lr: 1.0292e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 247/500\n",
            "834/834 - 2s - loss: 24.1731 - mse: 24.1731 - mae: 1.4954 - val_loss: 9.5091 - val_mse: 9.5091 - val_mae: 1.4593 - lr: 1.0292e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 248/500\n",
            "834/834 - 2s - loss: 24.1789 - mse: 24.1789 - mae: 1.4997 - val_loss: 9.5035 - val_mse: 9.5035 - val_mae: 1.4588 - lr: 1.0292e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 249/500\n",
            "834/834 - 2s - loss: 24.1765 - mse: 24.1765 - mae: 1.4968 - val_loss: 9.4821 - val_mse: 9.4821 - val_mae: 1.4698 - lr: 1.0292e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 250/500\n",
            "834/834 - 2s - loss: 24.1638 - mse: 24.1638 - mae: 1.4976 - val_loss: 9.4979 - val_mse: 9.4979 - val_mae: 1.4481 - lr: 1.0292e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 251/500\n",
            "834/834 - 2s - loss: 24.1630 - mse: 24.1630 - mae: 1.4916 - val_loss: 9.4749 - val_mse: 9.4749 - val_mae: 1.4557 - lr: 1.0292e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 252/500\n",
            "834/834 - 2s - loss: 24.1571 - mse: 24.1571 - mae: 1.4937 - val_loss: 9.4552 - val_mse: 9.4552 - val_mae: 1.4805 - lr: 1.0292e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 253/500\n",
            "834/834 - 2s - loss: 24.1645 - mse: 24.1645 - mae: 1.4942 - val_loss: 9.4539 - val_mse: 9.4539 - val_mae: 1.4868 - lr: 1.0292e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 254/500\n",
            "834/834 - 2s - loss: 24.1378 - mse: 24.1378 - mae: 1.4942 - val_loss: 9.4727 - val_mse: 9.4727 - val_mae: 1.4525 - lr: 1.0292e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 255/500\n",
            "834/834 - 2s - loss: 24.1518 - mse: 24.1518 - mae: 1.4933 - val_loss: 9.4708 - val_mse: 9.4708 - val_mae: 1.4403 - lr: 1.0292e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 256/500\n",
            "834/834 - 2s - loss: 24.1408 - mse: 24.1408 - mae: 1.4954 - val_loss: 9.4561 - val_mse: 9.4561 - val_mae: 1.4632 - lr: 1.0292e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 257/500\n",
            "834/834 - 2s - loss: 24.1369 - mse: 24.1369 - mae: 1.4940 - val_loss: 9.4525 - val_mse: 9.4525 - val_mae: 1.4746 - lr: 1.0292e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 258/500\n",
            "834/834 - 2s - loss: 24.1333 - mse: 24.1333 - mae: 1.4941 - val_loss: 9.4476 - val_mse: 9.4476 - val_mae: 1.4565 - lr: 1.0292e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 259/500\n",
            "834/834 - 2s - loss: 24.1322 - mse: 24.1322 - mae: 1.4917 - val_loss: 9.4599 - val_mse: 9.4599 - val_mae: 1.4650 - lr: 1.0292e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 260/500\n",
            "834/834 - 2s - loss: 24.1202 - mse: 24.1202 - mae: 1.4936 - val_loss: 9.4497 - val_mse: 9.4497 - val_mae: 1.4844 - lr: 1.0292e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 261/500\n",
            "834/834 - 2s - loss: 24.1105 - mse: 24.1105 - mae: 1.4964 - val_loss: 9.4465 - val_mse: 9.4465 - val_mae: 1.4630 - lr: 1.0292e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 262/500\n",
            "834/834 - 2s - loss: 24.1193 - mse: 24.1193 - mae: 1.4957 - val_loss: 9.4784 - val_mse: 9.4784 - val_mae: 1.4652 - lr: 1.0292e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 263/500\n",
            "834/834 - 2s - loss: 24.1138 - mse: 24.1138 - mae: 1.4944 - val_loss: 9.4496 - val_mse: 9.4496 - val_mae: 1.4600 - lr: 1.0292e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 264/500\n",
            "834/834 - 2s - loss: 24.1078 - mse: 24.1078 - mae: 1.4953 - val_loss: 9.4343 - val_mse: 9.4343 - val_mae: 1.4736 - lr: 1.0292e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 265/500\n",
            "834/834 - 2s - loss: 24.1103 - mse: 24.1103 - mae: 1.4957 - val_loss: 9.4391 - val_mse: 9.4391 - val_mae: 1.4822 - lr: 1.0292e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 266/500\n",
            "834/834 - 2s - loss: 24.0944 - mse: 24.0944 - mae: 1.4961 - val_loss: 9.4312 - val_mse: 9.4312 - val_mae: 1.4631 - lr: 1.0292e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 267/500\n",
            "834/834 - 2s - loss: 24.1041 - mse: 24.1041 - mae: 1.4932 - val_loss: 9.4400 - val_mse: 9.4400 - val_mae: 1.4506 - lr: 1.0292e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 268/500\n",
            "834/834 - 2s - loss: 24.0981 - mse: 24.0981 - mae: 1.4921 - val_loss: 9.4175 - val_mse: 9.4175 - val_mae: 1.4873 - lr: 1.0292e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 269/500\n",
            "834/834 - 2s - loss: 24.0930 - mse: 24.0930 - mae: 1.4948 - val_loss: 9.4403 - val_mse: 9.4403 - val_mae: 1.4648 - lr: 1.0292e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 270/500\n",
            "834/834 - 2s - loss: 24.0967 - mse: 24.0967 - mae: 1.4927 - val_loss: 9.4360 - val_mse: 9.4360 - val_mae: 1.5172 - lr: 1.0292e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 271/500\n",
            "834/834 - 2s - loss: 24.0759 - mse: 24.0759 - mae: 1.4990 - val_loss: 9.4394 - val_mse: 9.4394 - val_mae: 1.4560 - lr: 1.0292e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 272/500\n",
            "834/834 - 2s - loss: 24.0688 - mse: 24.0688 - mae: 1.4956 - val_loss: 9.4236 - val_mse: 9.4236 - val_mae: 1.4759 - lr: 1.0292e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 273/500\n",
            "834/834 - 2s - loss: 24.0698 - mse: 24.0698 - mae: 1.4954 - val_loss: 9.4190 - val_mse: 9.4190 - val_mae: 1.4682 - lr: 1.0292e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 274/500\n",
            "834/834 - 2s - loss: 24.0855 - mse: 24.0855 - mae: 1.4963 - val_loss: 9.4222 - val_mse: 9.4222 - val_mae: 1.4702 - lr: 1.0292e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 275/500\n",
            "834/834 - 2s - loss: 24.0814 - mse: 24.0814 - mae: 1.4961 - val_loss: 9.4400 - val_mse: 9.4400 - val_mae: 1.4475 - lr: 1.0292e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 276/500\n",
            "834/834 - 2s - loss: 24.0581 - mse: 24.0581 - mae: 1.4933 - val_loss: 9.3982 - val_mse: 9.3982 - val_mae: 1.4960 - lr: 1.0292e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 277/500\n",
            "834/834 - 2s - loss: 24.0653 - mse: 24.0653 - mae: 1.4937 - val_loss: 9.4189 - val_mse: 9.4189 - val_mae: 1.4669 - lr: 1.0292e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 278/500\n",
            "834/834 - 2s - loss: 24.0445 - mse: 24.0445 - mae: 1.4971 - val_loss: 9.4037 - val_mse: 9.4037 - val_mae: 1.4648 - lr: 1.0292e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 279/500\n",
            "834/834 - 2s - loss: 24.0551 - mse: 24.0551 - mae: 1.4919 - val_loss: 9.4022 - val_mse: 9.4022 - val_mae: 1.4772 - lr: 1.0292e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 280/500\n",
            "834/834 - 2s - loss: 24.0491 - mse: 24.0491 - mae: 1.4950 - val_loss: 9.4249 - val_mse: 9.4249 - val_mae: 1.4520 - lr: 1.0292e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 281/500\n",
            "834/834 - 2s - loss: 24.0574 - mse: 24.0574 - mae: 1.4949 - val_loss: 9.4487 - val_mse: 9.4487 - val_mae: 1.4500 - lr: 1.0292e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 282/500\n",
            "834/834 - 2s - loss: 24.0489 - mse: 24.0489 - mae: 1.4932 - val_loss: 9.4130 - val_mse: 9.4130 - val_mae: 1.4580 - lr: 1.0292e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 283/500\n",
            "834/834 - 2s - loss: 24.0376 - mse: 24.0376 - mae: 1.4940 - val_loss: 9.3803 - val_mse: 9.3803 - val_mae: 1.4731 - lr: 1.0292e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 284/500\n",
            "834/834 - 2s - loss: 24.0213 - mse: 24.0213 - mae: 1.4909 - val_loss: 9.3930 - val_mse: 9.3930 - val_mae: 1.4989 - lr: 1.0292e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 285/500\n",
            "834/834 - 2s - loss: 24.0277 - mse: 24.0277 - mae: 1.4937 - val_loss: 9.4189 - val_mse: 9.4189 - val_mae: 1.4527 - lr: 1.0292e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 286/500\n",
            "834/834 - 2s - loss: 24.0402 - mse: 24.0402 - mae: 1.4939 - val_loss: 9.3974 - val_mse: 9.3974 - val_mae: 1.4754 - lr: 1.0292e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 287/500\n",
            "834/834 - 2s - loss: 24.0109 - mse: 24.0109 - mae: 1.4956 - val_loss: 9.4046 - val_mse: 9.4046 - val_mae: 1.4956 - lr: 1.0292e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 288/500\n",
            "834/834 - 2s - loss: 24.0310 - mse: 24.0310 - mae: 1.4971 - val_loss: 9.4560 - val_mse: 9.4560 - val_mae: 1.4389 - lr: 1.0292e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 289/500\n",
            "834/834 - 2s - loss: 24.0307 - mse: 24.0307 - mae: 1.4943 - val_loss: 9.4744 - val_mse: 9.4744 - val_mae: 1.4406 - lr: 1.0292e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 290/500\n",
            "834/834 - 2s - loss: 24.0253 - mse: 24.0253 - mae: 1.4948 - val_loss: 9.4343 - val_mse: 9.4343 - val_mae: 1.4426 - lr: 1.0292e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 291/500\n",
            "834/834 - 2s - loss: 24.0333 - mse: 24.0333 - mae: 1.4948 - val_loss: 9.4492 - val_mse: 9.4492 - val_mae: 1.4424 - lr: 1.0292e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 292/500\n",
            "834/834 - 2s - loss: 24.0110 - mse: 24.0110 - mae: 1.4922 - val_loss: 9.3837 - val_mse: 9.3837 - val_mae: 1.4873 - lr: 1.0292e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 293/500\n",
            "834/834 - 2s - loss: 23.9954 - mse: 23.9954 - mae: 1.4941 - val_loss: 9.4163 - val_mse: 9.4163 - val_mae: 1.4466 - lr: 1.0292e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 294/500\n",
            "834/834 - 2s - loss: 24.0069 - mse: 24.0069 - mae: 1.4919 - val_loss: 9.3897 - val_mse: 9.3897 - val_mae: 1.4597 - lr: 1.0292e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 295/500\n",
            "834/834 - 2s - loss: 23.9933 - mse: 23.9933 - mae: 1.4950 - val_loss: 9.4652 - val_mse: 9.4652 - val_mae: 1.4387 - lr: 1.0292e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 296/500\n",
            "834/834 - 2s - loss: 24.0280 - mse: 24.0280 - mae: 1.4967 - val_loss: 9.3746 - val_mse: 9.3746 - val_mae: 1.4648 - lr: 1.0292e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 297/500\n",
            "834/834 - 2s - loss: 24.0205 - mse: 24.0205 - mae: 1.4959 - val_loss: 9.3786 - val_mse: 9.3786 - val_mae: 1.4643 - lr: 1.0292e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 298/500\n",
            "834/834 - 2s - loss: 23.9842 - mse: 23.9842 - mae: 1.4938 - val_loss: 9.3728 - val_mse: 9.3728 - val_mae: 1.4755 - lr: 1.0292e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 299/500\n",
            "834/834 - 1s - loss: 24.0132 - mse: 24.0132 - mae: 1.4957 - val_loss: 9.3721 - val_mse: 9.3721 - val_mae: 1.4651 - lr: 1.0292e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 300/500\n",
            "834/834 - 2s - loss: 24.0169 - mse: 24.0169 - mae: 1.4952 - val_loss: 9.3621 - val_mse: 9.3621 - val_mae: 1.4765 - lr: 1.0292e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 301/500\n",
            "834/834 - 2s - loss: 23.9911 - mse: 23.9911 - mae: 1.4953 - val_loss: 9.4036 - val_mse: 9.4036 - val_mae: 1.4628 - lr: 1.0292e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 302/500\n",
            "834/834 - 2s - loss: 23.9903 - mse: 23.9903 - mae: 1.4956 - val_loss: 9.3850 - val_mse: 9.3850 - val_mae: 1.4697 - lr: 1.0292e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 303/500\n",
            "834/834 - 2s - loss: 23.9840 - mse: 23.9840 - mae: 1.4942 - val_loss: 9.3568 - val_mse: 9.3568 - val_mae: 1.5000 - lr: 1.0292e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 304/500\n",
            "834/834 - 2s - loss: 23.9812 - mse: 23.9812 - mae: 1.4955 - val_loss: 9.3651 - val_mse: 9.3651 - val_mae: 1.4673 - lr: 1.0292e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 305/500\n",
            "834/834 - 2s - loss: 23.9906 - mse: 23.9906 - mae: 1.4950 - val_loss: 9.3559 - val_mse: 9.3559 - val_mae: 1.4570 - lr: 1.0292e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 306/500\n",
            "834/834 - 2s - loss: 23.9745 - mse: 23.9745 - mae: 1.4950 - val_loss: 9.3331 - val_mse: 9.3331 - val_mae: 1.4747 - lr: 1.0292e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 307/500\n",
            "834/834 - 2s - loss: 23.9781 - mse: 23.9781 - mae: 1.4946 - val_loss: 9.3787 - val_mse: 9.3787 - val_mae: 1.4755 - lr: 1.0292e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 308/500\n",
            "834/834 - 2s - loss: 23.9791 - mse: 23.9791 - mae: 1.4919 - val_loss: 9.3747 - val_mse: 9.3747 - val_mae: 1.4525 - lr: 1.0292e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 309/500\n",
            "834/834 - 2s - loss: 23.9817 - mse: 23.9817 - mae: 1.4965 - val_loss: 9.3322 - val_mse: 9.3322 - val_mae: 1.4832 - lr: 1.0292e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 310/500\n",
            "834/834 - 2s - loss: 23.9762 - mse: 23.9762 - mae: 1.4926 - val_loss: 9.3370 - val_mse: 9.3370 - val_mae: 1.4579 - lr: 1.0292e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 311/500\n",
            "834/834 - 2s - loss: 23.9681 - mse: 23.9681 - mae: 1.4939 - val_loss: 9.3361 - val_mse: 9.3361 - val_mae: 1.4655 - lr: 1.0292e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 312/500\n",
            "834/834 - 2s - loss: 23.9990 - mse: 23.9990 - mae: 1.4923 - val_loss: 9.3209 - val_mse: 9.3209 - val_mae: 1.4812 - lr: 1.0292e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 313/500\n",
            "834/834 - 2s - loss: 23.9791 - mse: 23.9791 - mae: 1.4960 - val_loss: 9.3274 - val_mse: 9.3274 - val_mae: 1.5084 - lr: 1.0292e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 314/500\n",
            "834/834 - 2s - loss: 23.9458 - mse: 23.9458 - mae: 1.4969 - val_loss: 9.3461 - val_mse: 9.3461 - val_mae: 1.4847 - lr: 1.0292e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 315/500\n",
            "834/834 - 2s - loss: 23.9835 - mse: 23.9835 - mae: 1.4917 - val_loss: 9.3414 - val_mse: 9.3414 - val_mae: 1.4701 - lr: 1.0292e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 316/500\n",
            "834/834 - 2s - loss: 23.9752 - mse: 23.9752 - mae: 1.4963 - val_loss: 9.3620 - val_mse: 9.3620 - val_mae: 1.4601 - lr: 1.0292e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 317/500\n",
            "834/834 - 2s - loss: 23.9508 - mse: 23.9508 - mae: 1.4946 - val_loss: 9.3272 - val_mse: 9.3272 - val_mae: 1.4700 - lr: 1.0292e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 318/500\n",
            "834/834 - 2s - loss: 23.9537 - mse: 23.9537 - mae: 1.4920 - val_loss: 9.3175 - val_mse: 9.3175 - val_mae: 1.5015 - lr: 1.0292e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 319/500\n",
            "834/834 - 2s - loss: 23.9749 - mse: 23.9749 - mae: 1.4974 - val_loss: 9.3280 - val_mse: 9.3280 - val_mae: 1.4845 - lr: 1.0292e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 320/500\n",
            "834/834 - 2s - loss: 23.9602 - mse: 23.9602 - mae: 1.4931 - val_loss: 9.3593 - val_mse: 9.3593 - val_mae: 1.4460 - lr: 1.0292e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 321/500\n",
            "834/834 - 2s - loss: 23.9584 - mse: 23.9584 - mae: 1.4927 - val_loss: 9.3408 - val_mse: 9.3408 - val_mae: 1.4530 - lr: 1.0292e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 322/500\n",
            "834/834 - 2s - loss: 23.9253 - mse: 23.9253 - mae: 1.4982 - val_loss: 9.3243 - val_mse: 9.3243 - val_mae: 1.4644 - lr: 1.0292e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 323/500\n",
            "834/834 - 2s - loss: 23.9570 - mse: 23.9570 - mae: 1.4938 - val_loss: 9.3258 - val_mse: 9.3258 - val_mae: 1.5050 - lr: 1.0292e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 324/500\n",
            "834/834 - 2s - loss: 23.9593 - mse: 23.9593 - mae: 1.4964 - val_loss: 9.3612 - val_mse: 9.3612 - val_mae: 1.4775 - lr: 1.0292e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 325/500\n",
            "834/834 - 2s - loss: 23.9462 - mse: 23.9462 - mae: 1.4926 - val_loss: 9.3221 - val_mse: 9.3221 - val_mae: 1.4913 - lr: 1.0292e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 326/500\n",
            "834/834 - 2s - loss: 23.9272 - mse: 23.9272 - mae: 1.4950 - val_loss: 9.3387 - val_mse: 9.3387 - val_mae: 1.4914 - lr: 1.0292e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 327/500\n",
            "834/834 - 2s - loss: 23.9324 - mse: 23.9324 - mae: 1.4948 - val_loss: 9.3130 - val_mse: 9.3130 - val_mae: 1.4550 - lr: 1.0292e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 328/500\n",
            "834/834 - 2s - loss: 23.9288 - mse: 23.9288 - mae: 1.4963 - val_loss: 9.3348 - val_mse: 9.3348 - val_mae: 1.4463 - lr: 1.0292e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 329/500\n",
            "834/834 - 2s - loss: 23.9728 - mse: 23.9728 - mae: 1.4948 - val_loss: 9.3097 - val_mse: 9.3097 - val_mae: 1.4577 - lr: 1.0292e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 330/500\n",
            "834/834 - 2s - loss: 23.9541 - mse: 23.9541 - mae: 1.4968 - val_loss: 9.3263 - val_mse: 9.3263 - val_mae: 1.4780 - lr: 1.0292e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 331/500\n",
            "834/834 - 2s - loss: 23.9638 - mse: 23.9638 - mae: 1.4949 - val_loss: 9.3295 - val_mse: 9.3295 - val_mae: 1.4875 - lr: 1.0292e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 332/500\n",
            "834/834 - 2s - loss: 23.9511 - mse: 23.9511 - mae: 1.4963 - val_loss: 9.3550 - val_mse: 9.3550 - val_mae: 1.4798 - lr: 1.0292e-04 - 2s/epoch - 2ms/step\n",
            "[9.354945182800293, 9.354945182800293, 1.4798080921173096]\n",
            "Score for fold 1: loss of 9.354945182800293\n",
            "------------------------------------------------------------------------\n",
            "Training for Outer fold 2 ...\n",
            "Epoch 1/500\n",
            "834/834 - 2s - loss: 23.9479 - mse: 23.9479 - mae: 1.4924 - val_loss: 9.3111 - val_mse: 9.3111 - val_mae: 1.4746 - lr: 1.0292e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 2/500\n",
            "834/834 - 2s - loss: 23.9476 - mse: 23.9476 - mae: 1.4908 - val_loss: 9.3055 - val_mse: 9.3055 - val_mae: 1.4576 - lr: 1.0292e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 3/500\n",
            "834/834 - 2s - loss: 23.9362 - mse: 23.9362 - mae: 1.4939 - val_loss: 9.3318 - val_mse: 9.3318 - val_mae: 1.4704 - lr: 1.0292e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 4/500\n",
            "834/834 - 2s - loss: 23.9423 - mse: 23.9423 - mae: 1.4910 - val_loss: 9.3251 - val_mse: 9.3251 - val_mae: 1.4652 - lr: 1.0292e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 5/500\n",
            "834/834 - 2s - loss: 23.9387 - mse: 23.9387 - mae: 1.4905 - val_loss: 9.3678 - val_mse: 9.3678 - val_mae: 1.5133 - lr: 1.0292e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 6/500\n",
            "834/834 - 2s - loss: 23.9447 - mse: 23.9447 - mae: 1.4900 - val_loss: 9.3312 - val_mse: 9.3312 - val_mae: 1.4853 - lr: 1.0292e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 7/500\n",
            "834/834 - 2s - loss: 23.9326 - mse: 23.9326 - mae: 1.4942 - val_loss: 9.3766 - val_mse: 9.3766 - val_mae: 1.4481 - lr: 1.0292e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 8/500\n",
            "834/834 - 2s - loss: 23.9273 - mse: 23.9273 - mae: 1.4931 - val_loss: 9.3513 - val_mse: 9.3513 - val_mae: 1.4504 - lr: 1.0292e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 9/500\n",
            "834/834 - 2s - loss: 23.9430 - mse: 23.9430 - mae: 1.4893 - val_loss: 9.3544 - val_mse: 9.3544 - val_mae: 1.4592 - lr: 1.0292e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 10/500\n",
            "834/834 - 2s - loss: 23.9282 - mse: 23.9282 - mae: 1.4876 - val_loss: 9.3402 - val_mse: 9.3402 - val_mae: 1.4882 - lr: 1.0292e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 11/500\n",
            "834/834 - 2s - loss: 23.9209 - mse: 23.9209 - mae: 1.4920 - val_loss: 9.3463 - val_mse: 9.3463 - val_mae: 1.4805 - lr: 1.0292e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 12/500\n",
            "834/834 - 2s - loss: 23.9501 - mse: 23.9501 - mae: 1.4891 - val_loss: 9.3470 - val_mse: 9.3470 - val_mae: 1.4934 - lr: 1.0292e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 13/500\n",
            "834/834 - 2s - loss: 23.9285 - mse: 23.9285 - mae: 1.4899 - val_loss: 9.3627 - val_mse: 9.3627 - val_mae: 1.4622 - lr: 1.0292e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 14/500\n",
            "834/834 - 2s - loss: 23.9015 - mse: 23.9015 - mae: 1.4889 - val_loss: 9.4319 - val_mse: 9.4319 - val_mae: 1.4333 - lr: 1.0292e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 15/500\n",
            "834/834 - 1s - loss: 23.9235 - mse: 23.9235 - mae: 1.4922 - val_loss: 9.3368 - val_mse: 9.3368 - val_mae: 1.4859 - lr: 1.0292e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 16/500\n",
            "834/834 - 2s - loss: 23.9151 - mse: 23.9151 - mae: 1.4901 - val_loss: 9.3589 - val_mse: 9.3589 - val_mae: 1.4556 - lr: 1.0292e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 17/500\n",
            "834/834 - 2s - loss: 23.8985 - mse: 23.8985 - mae: 1.4895 - val_loss: 9.3781 - val_mse: 9.3781 - val_mae: 1.4564 - lr: 1.0292e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 18/500\n",
            "834/834 - 2s - loss: 23.8982 - mse: 23.8982 - mae: 1.4902 - val_loss: 9.3453 - val_mse: 9.3453 - val_mae: 1.4635 - lr: 1.0292e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 19/500\n",
            "834/834 - 2s - loss: 23.9061 - mse: 23.9061 - mae: 1.4911 - val_loss: 9.3449 - val_mse: 9.3449 - val_mae: 1.4775 - lr: 1.0292e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 20/500\n",
            "834/834 - 2s - loss: 23.9098 - mse: 23.9098 - mae: 1.4917 - val_loss: 9.3458 - val_mse: 9.3458 - val_mae: 1.4738 - lr: 1.0292e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 21/500\n",
            "834/834 - 2s - loss: 23.8923 - mse: 23.8923 - mae: 1.4900 - val_loss: 9.3618 - val_mse: 9.3618 - val_mae: 1.4661 - lr: 1.0292e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 22/500\n",
            "834/834 - 2s - loss: 23.9013 - mse: 23.9013 - mae: 1.4946 - val_loss: 9.3511 - val_mse: 9.3511 - val_mae: 1.4815 - lr: 1.0292e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 23/500\n",
            "834/834 - 2s - loss: 23.9068 - mse: 23.9068 - mae: 1.4888 - val_loss: 9.3719 - val_mse: 9.3719 - val_mae: 1.4539 - lr: 1.0292e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 24/500\n",
            "834/834 - 2s - loss: 23.9160 - mse: 23.9160 - mae: 1.4935 - val_loss: 9.3575 - val_mse: 9.3575 - val_mae: 1.4521 - lr: 1.0292e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 25/500\n",
            "834/834 - 2s - loss: 23.9107 - mse: 23.9107 - mae: 1.4914 - val_loss: 9.3682 - val_mse: 9.3682 - val_mae: 1.4803 - lr: 1.0292e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 26/500\n",
            "834/834 - 2s - loss: 23.9247 - mse: 23.9247 - mae: 1.4903 - val_loss: 9.3592 - val_mse: 9.3592 - val_mae: 1.4901 - lr: 1.0292e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 27/500\n",
            "834/834 - 2s - loss: 23.9367 - mse: 23.9367 - mae: 1.4892 - val_loss: 9.3486 - val_mse: 9.3486 - val_mae: 1.4980 - lr: 1.0292e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 28/500\n",
            "834/834 - 2s - loss: 23.8994 - mse: 23.8994 - mae: 1.4909 - val_loss: 9.3830 - val_mse: 9.3830 - val_mae: 1.4651 - lr: 1.0292e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 29/500\n",
            "834/834 - 2s - loss: 23.8927 - mse: 23.8927 - mae: 1.4909 - val_loss: 9.3452 - val_mse: 9.3452 - val_mae: 1.4853 - lr: 1.0292e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 30/500\n",
            "834/834 - 2s - loss: 23.9005 - mse: 23.9005 - mae: 1.4890 - val_loss: 9.3524 - val_mse: 9.3524 - val_mae: 1.4747 - lr: 1.0292e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 31/500\n",
            "834/834 - 2s - loss: 23.9095 - mse: 23.9095 - mae: 1.4888 - val_loss: 9.3484 - val_mse: 9.3484 - val_mae: 1.4671 - lr: 1.0292e-04 - 2s/epoch - 2ms/step\n",
            "[9.34839916229248, 9.34839916229248, 1.4670828580856323]\n",
            "Score for fold 2: loss of 9.34839916229248\n",
            "------------------------------------------------------------------------\n",
            "Training for Outer fold 3 ...\n",
            "Epoch 1/500\n",
            "834/834 - 2s - loss: 9.3014 - mse: 9.3014 - mae: 1.4624 - val_loss: 38.6582 - val_mse: 38.6582 - val_mae: 1.5055 - lr: 1.0292e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 2/500\n",
            "834/834 - 2s - loss: 9.2928 - mse: 9.2928 - mae: 1.4615 - val_loss: 38.5490 - val_mse: 38.5490 - val_mae: 1.5356 - lr: 1.0292e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 3/500\n",
            "834/834 - 2s - loss: 9.3044 - mse: 9.3044 - mae: 1.4664 - val_loss: 38.5983 - val_mse: 38.5983 - val_mae: 1.5268 - lr: 1.0292e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 4/500\n",
            "834/834 - 2s - loss: 9.2898 - mse: 9.2898 - mae: 1.4685 - val_loss: 38.7256 - val_mse: 38.7256 - val_mae: 1.5091 - lr: 1.0292e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 5/500\n",
            "834/834 - 2s - loss: 9.3014 - mse: 9.3014 - mae: 1.4628 - val_loss: 38.7180 - val_mse: 38.7180 - val_mae: 1.4963 - lr: 1.0292e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 6/500\n",
            "834/834 - 2s - loss: 9.2852 - mse: 9.2852 - mae: 1.4646 - val_loss: 38.6332 - val_mse: 38.6332 - val_mae: 1.5196 - lr: 1.0292e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 7/500\n",
            "834/834 - 2s - loss: 9.2904 - mse: 9.2904 - mae: 1.4666 - val_loss: 38.7210 - val_mse: 38.7210 - val_mae: 1.5011 - lr: 1.0292e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 8/500\n",
            "834/834 - 2s - loss: 9.2878 - mse: 9.2878 - mae: 1.4639 - val_loss: 38.7473 - val_mse: 38.7473 - val_mae: 1.5063 - lr: 1.0292e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 9/500\n",
            "834/834 - 2s - loss: 9.2775 - mse: 9.2775 - mae: 1.4663 - val_loss: 38.7438 - val_mse: 38.7438 - val_mae: 1.5193 - lr: 1.0292e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 10/500\n",
            "834/834 - 2s - loss: 9.2763 - mse: 9.2763 - mae: 1.4646 - val_loss: 38.7600 - val_mse: 38.7600 - val_mae: 1.5199 - lr: 1.0292e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 11/500\n",
            "834/834 - 2s - loss: 9.2930 - mse: 9.2930 - mae: 1.4651 - val_loss: 38.6619 - val_mse: 38.6619 - val_mae: 1.5305 - lr: 1.0292e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 12/500\n",
            "834/834 - 2s - loss: 9.2847 - mse: 9.2847 - mae: 1.4641 - val_loss: 38.7142 - val_mse: 38.7142 - val_mae: 1.5271 - lr: 1.0292e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 13/500\n",
            "834/834 - 2s - loss: 9.2678 - mse: 9.2678 - mae: 1.4620 - val_loss: 38.7371 - val_mse: 38.7371 - val_mae: 1.5153 - lr: 1.0292e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 14/500\n",
            "834/834 - 2s - loss: 9.2693 - mse: 9.2693 - mae: 1.4613 - val_loss: 38.7552 - val_mse: 38.7552 - val_mae: 1.5172 - lr: 1.0292e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 15/500\n",
            "834/834 - 2s - loss: 9.2824 - mse: 9.2824 - mae: 1.4631 - val_loss: 38.7203 - val_mse: 38.7203 - val_mae: 1.5423 - lr: 1.0292e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 16/500\n",
            "834/834 - 2s - loss: 9.2743 - mse: 9.2743 - mae: 1.4637 - val_loss: 38.6868 - val_mse: 38.6868 - val_mae: 1.5237 - lr: 1.0292e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 17/500\n",
            "834/834 - 2s - loss: 9.2615 - mse: 9.2615 - mae: 1.4652 - val_loss: 38.6746 - val_mse: 38.6746 - val_mae: 1.5246 - lr: 1.0292e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 18/500\n",
            "834/834 - 2s - loss: 9.2621 - mse: 9.2621 - mae: 1.4642 - val_loss: 38.7160 - val_mse: 38.7160 - val_mae: 1.5295 - lr: 1.0292e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 19/500\n",
            "834/834 - 2s - loss: 9.2464 - mse: 9.2464 - mae: 1.4626 - val_loss: 38.6262 - val_mse: 38.6262 - val_mae: 1.5504 - lr: 1.0292e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 20/500\n",
            "834/834 - 2s - loss: 9.2541 - mse: 9.2541 - mae: 1.4671 - val_loss: 38.7469 - val_mse: 38.7469 - val_mae: 1.5052 - lr: 1.0292e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 21/500\n",
            "834/834 - 2s - loss: 9.2623 - mse: 9.2623 - mae: 1.4623 - val_loss: 38.7209 - val_mse: 38.7209 - val_mae: 1.4919 - lr: 1.0292e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 22/500\n",
            "834/834 - 2s - loss: 9.2509 - mse: 9.2509 - mae: 1.4641 - val_loss: 38.7372 - val_mse: 38.7372 - val_mae: 1.4973 - lr: 1.0292e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 23/500\n",
            "834/834 - 2s - loss: 9.2400 - mse: 9.2400 - mae: 1.4633 - val_loss: 38.7114 - val_mse: 38.7114 - val_mae: 1.5346 - lr: 1.0292e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 24/500\n",
            "834/834 - 2s - loss: 9.2475 - mse: 9.2475 - mae: 1.4637 - val_loss: 38.6884 - val_mse: 38.6884 - val_mae: 1.5438 - lr: 1.0292e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 25/500\n",
            "834/834 - 2s - loss: 9.2404 - mse: 9.2404 - mae: 1.4660 - val_loss: 38.6993 - val_mse: 38.6993 - val_mae: 1.5222 - lr: 1.0292e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 26/500\n",
            "834/834 - 2s - loss: 9.2414 - mse: 9.2414 - mae: 1.4618 - val_loss: 38.7769 - val_mse: 38.7769 - val_mae: 1.5338 - lr: 1.0292e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 27/500\n",
            "834/834 - 2s - loss: 9.2409 - mse: 9.2409 - mae: 1.4650 - val_loss: 38.6943 - val_mse: 38.6943 - val_mae: 1.5287 - lr: 1.0292e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 28/500\n",
            "834/834 - 2s - loss: 9.2413 - mse: 9.2413 - mae: 1.4632 - val_loss: 38.7006 - val_mse: 38.7006 - val_mae: 1.5048 - lr: 1.0292e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 29/500\n",
            "834/834 - 2s - loss: 9.2386 - mse: 9.2386 - mae: 1.4583 - val_loss: 38.7173 - val_mse: 38.7173 - val_mae: 1.5241 - lr: 1.0292e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 30/500\n",
            "834/834 - 2s - loss: 9.2370 - mse: 9.2370 - mae: 1.4643 - val_loss: 38.7136 - val_mse: 38.7136 - val_mae: 1.5131 - lr: 1.0292e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 31/500\n",
            "834/834 - 2s - loss: 9.2354 - mse: 9.2354 - mae: 1.4626 - val_loss: 38.7044 - val_mse: 38.7044 - val_mae: 1.5234 - lr: 1.0292e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 32/500\n",
            "834/834 - 2s - loss: 9.2285 - mse: 9.2285 - mae: 1.4655 - val_loss: 38.6546 - val_mse: 38.6546 - val_mae: 1.5399 - lr: 1.0292e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 33/500\n",
            "834/834 - 2s - loss: 9.2246 - mse: 9.2246 - mae: 1.4607 - val_loss: 38.7053 - val_mse: 38.7053 - val_mae: 1.5341 - lr: 1.0292e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 34/500\n",
            "834/834 - 2s - loss: 9.2258 - mse: 9.2258 - mae: 1.4635 - val_loss: 38.7023 - val_mse: 38.7023 - val_mae: 1.5128 - lr: 1.0292e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 35/500\n",
            "834/834 - 2s - loss: 9.2359 - mse: 9.2359 - mae: 1.4627 - val_loss: 38.7387 - val_mse: 38.7387 - val_mae: 1.5087 - lr: 1.0292e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 36/500\n",
            "834/834 - 2s - loss: 9.2317 - mse: 9.2317 - mae: 1.4658 - val_loss: 38.6718 - val_mse: 38.6718 - val_mae: 1.5306 - lr: 1.0292e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 37/500\n",
            "834/834 - 2s - loss: 9.2335 - mse: 9.2335 - mae: 1.4628 - val_loss: 38.7426 - val_mse: 38.7426 - val_mae: 1.5069 - lr: 1.0292e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 38/500\n",
            "834/834 - 2s - loss: 9.2214 - mse: 9.2214 - mae: 1.4637 - val_loss: 38.7417 - val_mse: 38.7417 - val_mae: 1.5107 - lr: 1.0292e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 39/500\n",
            "834/834 - 2s - loss: 9.2273 - mse: 9.2273 - mae: 1.4659 - val_loss: 38.7182 - val_mse: 38.7182 - val_mae: 1.5372 - lr: 1.0292e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 40/500\n",
            "834/834 - 2s - loss: 9.2142 - mse: 9.2142 - mae: 1.4619 - val_loss: 38.7346 - val_mse: 38.7346 - val_mae: 1.5079 - lr: 1.0292e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 41/500\n",
            "834/834 - 2s - loss: 9.2311 - mse: 9.2311 - mae: 1.4627 - val_loss: 38.7507 - val_mse: 38.7507 - val_mae: 1.5152 - lr: 1.0292e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 42/500\n",
            "834/834 - 2s - loss: 9.2351 - mse: 9.2351 - mae: 1.4602 - val_loss: 38.6923 - val_mse: 38.6923 - val_mae: 1.5354 - lr: 1.0292e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 43/500\n",
            "834/834 - 2s - loss: 9.2225 - mse: 9.2225 - mae: 1.4627 - val_loss: 38.7090 - val_mse: 38.7090 - val_mae: 1.5117 - lr: 1.0292e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 44/500\n",
            "834/834 - 2s - loss: 9.2144 - mse: 9.2144 - mae: 1.4650 - val_loss: 38.6607 - val_mse: 38.6607 - val_mae: 1.5452 - lr: 1.0292e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 45/500\n",
            "834/834 - 2s - loss: 9.2172 - mse: 9.2172 - mae: 1.4670 - val_loss: 38.6906 - val_mse: 38.6906 - val_mae: 1.5348 - lr: 1.0292e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 46/500\n",
            "834/834 - 2s - loss: 9.2104 - mse: 9.2104 - mae: 1.4640 - val_loss: 38.7155 - val_mse: 38.7155 - val_mae: 1.5290 - lr: 1.0292e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 47/500\n",
            "834/834 - 2s - loss: 9.2089 - mse: 9.2089 - mae: 1.4649 - val_loss: 38.7295 - val_mse: 38.7295 - val_mae: 1.5133 - lr: 1.0292e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 48/500\n",
            "834/834 - 2s - loss: 9.2148 - mse: 9.2148 - mae: 1.4632 - val_loss: 38.7948 - val_mse: 38.7948 - val_mae: 1.5331 - lr: 1.0292e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 49/500\n",
            "834/834 - 2s - loss: 9.2094 - mse: 9.2094 - mae: 1.4650 - val_loss: 38.8314 - val_mse: 38.8314 - val_mae: 1.4870 - lr: 1.0292e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 50/500\n",
            "834/834 - 2s - loss: 9.2250 - mse: 9.2250 - mae: 1.4646 - val_loss: 38.7252 - val_mse: 38.7252 - val_mae: 1.5549 - lr: 1.0292e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 51/500\n",
            "834/834 - 2s - loss: 9.2050 - mse: 9.2050 - mae: 1.4634 - val_loss: 38.7544 - val_mse: 38.7544 - val_mae: 1.5279 - lr: 1.0292e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 52/500\n",
            "834/834 - 2s - loss: 9.2210 - mse: 9.2210 - mae: 1.4624 - val_loss: 38.7298 - val_mse: 38.7298 - val_mae: 1.5062 - lr: 1.0292e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 53/500\n",
            "834/834 - 2s - loss: 9.2087 - mse: 9.2087 - mae: 1.4649 - val_loss: 38.7814 - val_mse: 38.7814 - val_mae: 1.5134 - lr: 1.0292e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 54/500\n",
            "834/834 - 2s - loss: 9.2408 - mse: 9.2408 - mae: 1.4612 - val_loss: 38.7007 - val_mse: 38.7007 - val_mae: 1.5459 - lr: 1.0292e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 55/500\n",
            "834/834 - 2s - loss: 9.2215 - mse: 9.2215 - mae: 1.4635 - val_loss: 38.7925 - val_mse: 38.7925 - val_mae: 1.5031 - lr: 1.0292e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 56/500\n",
            "834/834 - 2s - loss: 9.2249 - mse: 9.2249 - mae: 1.4634 - val_loss: 38.7507 - val_mse: 38.7507 - val_mae: 1.5088 - lr: 1.0292e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 57/500\n",
            "834/834 - 2s - loss: 9.2100 - mse: 9.2100 - mae: 1.4646 - val_loss: 38.6640 - val_mse: 38.6640 - val_mae: 1.5117 - lr: 1.0292e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 58/500\n",
            "834/834 - 2s - loss: 9.2213 - mse: 9.2213 - mae: 1.4639 - val_loss: 38.8096 - val_mse: 38.8096 - val_mae: 1.4989 - lr: 1.0292e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 59/500\n",
            "834/834 - 2s - loss: 9.2171 - mse: 9.2171 - mae: 1.4627 - val_loss: 38.7066 - val_mse: 38.7066 - val_mae: 1.5405 - lr: 1.0292e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 60/500\n",
            "834/834 - 2s - loss: 9.2115 - mse: 9.2115 - mae: 1.4632 - val_loss: 38.7184 - val_mse: 38.7184 - val_mae: 1.5429 - lr: 1.0292e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 61/500\n",
            "834/834 - 2s - loss: 9.2074 - mse: 9.2074 - mae: 1.4607 - val_loss: 38.7016 - val_mse: 38.7016 - val_mae: 1.5535 - lr: 1.0292e-04 - 2s/epoch - 2ms/step\n",
            "[38.70158004760742, 38.70158004760742, 1.5535184144973755]\n",
            "Score for fold 3: loss of 38.70158004760742\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "results = model3.evaluate(testing, labelsForTest, batch_size=10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o5tqHl6_6yeT",
        "outputId": "1d524d2f-6226-4680-a6e4-c96da0e5a7c8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2000/2000 [==============================] - 3s 2ms/step - loss: 8.8159 - mse: 8.8159 - mae: 1.4581\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 468
        },
        "id": "amWqAJBQCklm",
        "outputId": "f3651f07-aa25-4234-cba7-39fb91db3efd"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "             0         1         2         3         4         5         6   \\\n",
              "0      0.594081  0.904101  0.998045 -0.004595  0.394529  0.926423  0.880260   \n",
              "1      1.219960 -1.688930 -1.491378  2.535186 -0.172440 -0.366368  0.423633   \n",
              "2      0.395851  0.255843  0.064511 -0.482978  0.319514  0.664912  0.609103   \n",
              "3     -0.555657  0.255843  0.064511 -0.169854 -0.219542  0.011135  0.976132   \n",
              "4      0.584975 -1.688930 -1.491378  0.795610 -0.306768 -0.400111 -0.386756   \n",
              "...         ...       ...       ...       ...       ...       ...       ...   \n",
              "79995  0.133394  0.904101  0.998045 -0.761310 -0.327702 -0.201869  0.646128   \n",
              "79996 -0.152830 -1.688930 -1.491378 -0.900476  0.089238 -0.389566 -1.231812   \n",
              "79997 -0.126048  0.904101  0.998045  0.317227 -0.006711  0.517285  0.463527   \n",
              "79998  0.418950 -1.040672 -1.180200  1.273993 -0.154995 -0.113293 -0.744071   \n",
              "79999 -1.000029 -1.040672 -1.180200 -0.152459 -0.332936 -0.383239 -0.518778   \n",
              "\n",
              "             7         8         9         10        11        12         13  \\\n",
              "0     -0.053977  1.976635  0.173799  0.402216  0.152305 -0.299098  -0.407369   \n",
              "1     -0.190553 -0.611129 -0.194835  0.814666 -0.181531  0.329854  10.726693   \n",
              "2      0.391782  1.976635  0.327396 -0.107280  0.264463  0.958806  -0.056630   \n",
              "3     -0.007639  0.359283 -0.141076 -0.374160 -0.066294  0.958806  -0.483286   \n",
              "4     -0.190553 -0.611129 -0.194835 -3.297700 -0.182851 -0.928051   0.087614   \n",
              "...         ...       ...       ...       ...       ...       ...        ...   \n",
              "79995  0.111351 -0.287658 -0.164116  1.154330 -0.154921  0.958806   0.599298   \n",
              "79996 -0.190553 -0.611129 -0.194835 -0.058757 -0.182851  0.329854   1.232450   \n",
              "79997  0.125043  1.653164 -0.018198 -0.046626  0.063678  0.958806  -0.223648   \n",
              "79998  0.018797  0.035812 -0.033558 -0.471207 -0.097962 -0.299098   4.288888   \n",
              "79999 -0.190553 -0.611129 -0.194835 -0.264982 -0.182191 -0.299098  -0.360300   \n",
              "\n",
              "             14        15        16        17  \n",
              "0      0.617311 -0.643731 -0.239689  1.452094  \n",
              "1      0.617311  1.475200 -1.087631 -0.473122  \n",
              "2     -1.818900 -1.382147  1.037042  0.823739  \n",
              "3      0.617311 -0.386891 -0.086498 -0.218550  \n",
              "4      0.617311  0.126789 -1.863465 -0.920809  \n",
              "...         ...       ...       ...       ...  \n",
              "79995 -0.600795 -0.980834  1.641533  1.645053  \n",
              "79996  0.617311  0.608365 -1.519767 -0.329103  \n",
              "79997  0.617311 -0.563469  0.257628  0.543124  \n",
              "79998  0.617311  0.126789 -0.882083  0.210619  \n",
              "79999  0.617311  0.512050 -1.101744 -1.064248  \n",
              "\n",
              "[80000 rows x 18 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-6ef91776-2415-4f77-b914-061e888005b9\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.594081</td>\n",
              "      <td>0.904101</td>\n",
              "      <td>0.998045</td>\n",
              "      <td>-0.004595</td>\n",
              "      <td>0.394529</td>\n",
              "      <td>0.926423</td>\n",
              "      <td>0.880260</td>\n",
              "      <td>-0.053977</td>\n",
              "      <td>1.976635</td>\n",
              "      <td>0.173799</td>\n",
              "      <td>0.402216</td>\n",
              "      <td>0.152305</td>\n",
              "      <td>-0.299098</td>\n",
              "      <td>-0.407369</td>\n",
              "      <td>0.617311</td>\n",
              "      <td>-0.643731</td>\n",
              "      <td>-0.239689</td>\n",
              "      <td>1.452094</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1.219960</td>\n",
              "      <td>-1.688930</td>\n",
              "      <td>-1.491378</td>\n",
              "      <td>2.535186</td>\n",
              "      <td>-0.172440</td>\n",
              "      <td>-0.366368</td>\n",
              "      <td>0.423633</td>\n",
              "      <td>-0.190553</td>\n",
              "      <td>-0.611129</td>\n",
              "      <td>-0.194835</td>\n",
              "      <td>0.814666</td>\n",
              "      <td>-0.181531</td>\n",
              "      <td>0.329854</td>\n",
              "      <td>10.726693</td>\n",
              "      <td>0.617311</td>\n",
              "      <td>1.475200</td>\n",
              "      <td>-1.087631</td>\n",
              "      <td>-0.473122</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.395851</td>\n",
              "      <td>0.255843</td>\n",
              "      <td>0.064511</td>\n",
              "      <td>-0.482978</td>\n",
              "      <td>0.319514</td>\n",
              "      <td>0.664912</td>\n",
              "      <td>0.609103</td>\n",
              "      <td>0.391782</td>\n",
              "      <td>1.976635</td>\n",
              "      <td>0.327396</td>\n",
              "      <td>-0.107280</td>\n",
              "      <td>0.264463</td>\n",
              "      <td>0.958806</td>\n",
              "      <td>-0.056630</td>\n",
              "      <td>-1.818900</td>\n",
              "      <td>-1.382147</td>\n",
              "      <td>1.037042</td>\n",
              "      <td>0.823739</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>-0.555657</td>\n",
              "      <td>0.255843</td>\n",
              "      <td>0.064511</td>\n",
              "      <td>-0.169854</td>\n",
              "      <td>-0.219542</td>\n",
              "      <td>0.011135</td>\n",
              "      <td>0.976132</td>\n",
              "      <td>-0.007639</td>\n",
              "      <td>0.359283</td>\n",
              "      <td>-0.141076</td>\n",
              "      <td>-0.374160</td>\n",
              "      <td>-0.066294</td>\n",
              "      <td>0.958806</td>\n",
              "      <td>-0.483286</td>\n",
              "      <td>0.617311</td>\n",
              "      <td>-0.386891</td>\n",
              "      <td>-0.086498</td>\n",
              "      <td>-0.218550</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.584975</td>\n",
              "      <td>-1.688930</td>\n",
              "      <td>-1.491378</td>\n",
              "      <td>0.795610</td>\n",
              "      <td>-0.306768</td>\n",
              "      <td>-0.400111</td>\n",
              "      <td>-0.386756</td>\n",
              "      <td>-0.190553</td>\n",
              "      <td>-0.611129</td>\n",
              "      <td>-0.194835</td>\n",
              "      <td>-3.297700</td>\n",
              "      <td>-0.182851</td>\n",
              "      <td>-0.928051</td>\n",
              "      <td>0.087614</td>\n",
              "      <td>0.617311</td>\n",
              "      <td>0.126789</td>\n",
              "      <td>-1.863465</td>\n",
              "      <td>-0.920809</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>79995</th>\n",
              "      <td>0.133394</td>\n",
              "      <td>0.904101</td>\n",
              "      <td>0.998045</td>\n",
              "      <td>-0.761310</td>\n",
              "      <td>-0.327702</td>\n",
              "      <td>-0.201869</td>\n",
              "      <td>0.646128</td>\n",
              "      <td>0.111351</td>\n",
              "      <td>-0.287658</td>\n",
              "      <td>-0.164116</td>\n",
              "      <td>1.154330</td>\n",
              "      <td>-0.154921</td>\n",
              "      <td>0.958806</td>\n",
              "      <td>0.599298</td>\n",
              "      <td>-0.600795</td>\n",
              "      <td>-0.980834</td>\n",
              "      <td>1.641533</td>\n",
              "      <td>1.645053</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>79996</th>\n",
              "      <td>-0.152830</td>\n",
              "      <td>-1.688930</td>\n",
              "      <td>-1.491378</td>\n",
              "      <td>-0.900476</td>\n",
              "      <td>0.089238</td>\n",
              "      <td>-0.389566</td>\n",
              "      <td>-1.231812</td>\n",
              "      <td>-0.190553</td>\n",
              "      <td>-0.611129</td>\n",
              "      <td>-0.194835</td>\n",
              "      <td>-0.058757</td>\n",
              "      <td>-0.182851</td>\n",
              "      <td>0.329854</td>\n",
              "      <td>1.232450</td>\n",
              "      <td>0.617311</td>\n",
              "      <td>0.608365</td>\n",
              "      <td>-1.519767</td>\n",
              "      <td>-0.329103</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>79997</th>\n",
              "      <td>-0.126048</td>\n",
              "      <td>0.904101</td>\n",
              "      <td>0.998045</td>\n",
              "      <td>0.317227</td>\n",
              "      <td>-0.006711</td>\n",
              "      <td>0.517285</td>\n",
              "      <td>0.463527</td>\n",
              "      <td>0.125043</td>\n",
              "      <td>1.653164</td>\n",
              "      <td>-0.018198</td>\n",
              "      <td>-0.046626</td>\n",
              "      <td>0.063678</td>\n",
              "      <td>0.958806</td>\n",
              "      <td>-0.223648</td>\n",
              "      <td>0.617311</td>\n",
              "      <td>-0.563469</td>\n",
              "      <td>0.257628</td>\n",
              "      <td>0.543124</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>79998</th>\n",
              "      <td>0.418950</td>\n",
              "      <td>-1.040672</td>\n",
              "      <td>-1.180200</td>\n",
              "      <td>1.273993</td>\n",
              "      <td>-0.154995</td>\n",
              "      <td>-0.113293</td>\n",
              "      <td>-0.744071</td>\n",
              "      <td>0.018797</td>\n",
              "      <td>0.035812</td>\n",
              "      <td>-0.033558</td>\n",
              "      <td>-0.471207</td>\n",
              "      <td>-0.097962</td>\n",
              "      <td>-0.299098</td>\n",
              "      <td>4.288888</td>\n",
              "      <td>0.617311</td>\n",
              "      <td>0.126789</td>\n",
              "      <td>-0.882083</td>\n",
              "      <td>0.210619</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>79999</th>\n",
              "      <td>-1.000029</td>\n",
              "      <td>-1.040672</td>\n",
              "      <td>-1.180200</td>\n",
              "      <td>-0.152459</td>\n",
              "      <td>-0.332936</td>\n",
              "      <td>-0.383239</td>\n",
              "      <td>-0.518778</td>\n",
              "      <td>-0.190553</td>\n",
              "      <td>-0.611129</td>\n",
              "      <td>-0.194835</td>\n",
              "      <td>-0.264982</td>\n",
              "      <td>-0.182191</td>\n",
              "      <td>-0.299098</td>\n",
              "      <td>-0.360300</td>\n",
              "      <td>0.617311</td>\n",
              "      <td>0.512050</td>\n",
              "      <td>-1.101744</td>\n",
              "      <td>-1.064248</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>80000 rows × 18 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-6ef91776-2415-4f77-b914-061e888005b9')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-6ef91776-2415-4f77-b914-061e888005b9 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-6ef91776-2415-4f77-b914-061e888005b9');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 397
        },
        "id": "ddVyvAhB9Jo9",
        "outputId": "7d40e71b-5149-41be-a3a4-bd0ddefedf7d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "4000/4000 - 5s - loss: 22.9177 - mse: 22.9177 - mae: 1.6737 - lr: 0.0024 - 5s/epoch - 1ms/step\n",
            "Epoch 2/10\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-22-0fb3082d5884>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mstudy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptuna\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_study\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstudy_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstudy_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mload_if_exists\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m#15.1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mstudy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobjective\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_trials\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/optuna/study.py\u001b[0m in \u001b[0;36moptimize\u001b[0;34m(self, func, n_trials, timeout, n_jobs, catch)\u001b[0m\n\u001b[1;32m    278\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    279\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mn_jobs\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 280\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_optimize_sequential\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_trials\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    281\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    282\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_optimize_parallel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_trials\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/optuna/study.py\u001b[0m in \u001b[0;36m_optimize_sequential\u001b[0;34m(self, func, n_trials, timeout, catch)\u001b[0m\n\u001b[1;32m    393\u001b[0m                     \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    394\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 395\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_trial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    396\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    397\u001b[0m     def _optimize_parallel(\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/optuna/study.py\u001b[0m in \u001b[0;36m_run_trial\u001b[0;34m(self, func, catch)\u001b[0m\n\u001b[1;32m    467\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    468\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 469\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    470\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mstructs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrialPruned\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    471\u001b[0m             message = 'Setting status of trial#{} as {}. {}'.format(trial_number,\n",
            "\u001b[0;32m<ipython-input-21-7f01bdddf488>\u001b[0m in \u001b[0;36mobjective\u001b[0;34m(trial)\u001b[0m\n\u001b[1;32m     40\u001b[0m                   \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m                   \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m                   callbacks=[reduce_lr])\n\u001b[0m\u001b[1;32m     43\u001b[0m   \u001b[0mscores_inner\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtesting\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlabelsForTest\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1382\u001b[0m                 _r=1):\n\u001b[1;32m   1383\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1384\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1385\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1386\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    913\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    914\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 915\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    916\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    917\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    945\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    946\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 947\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    948\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2955\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m   2956\u001b[0m     return graph_function._call_flat(\n\u001b[0;32m-> 2957\u001b[0;31m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m   2958\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2959\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1852\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1853\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1854\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1855\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1856\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    502\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    503\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 504\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    505\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    506\u001b[0m           outputs = execute.execute_with_cancellation(\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 55\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     56\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "model_list=[]\n",
        "history_list=[]\n",
        "study_name = 'NN_study'\n",
        "study = optuna.create_study(study_name=study_name, load_if_exists=True)\n",
        "#15.1\n",
        "study.optimize(objective, n_trials=50, )"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}