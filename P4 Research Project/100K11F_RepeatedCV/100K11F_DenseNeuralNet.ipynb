{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "_4bmjRCyDcQK",
        "outputId": "192e1fc8-3596-4dd7-b599-94ead28f3772"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting optuna==0.14.0\n",
            "  Downloading optuna-0.14.0.tar.gz (91 kB)\n",
            "\u001b[K     |████████████████████████████████| 91 kB 4.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: sqlalchemy>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from optuna==0.14.0) (1.4.41)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from optuna==0.14.0) (1.21.6)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from optuna==0.14.0) (1.7.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from optuna==0.14.0) (1.15.0)\n",
            "Collecting cliff\n",
            "  Downloading cliff-3.10.1-py3-none-any.whl (81 kB)\n",
            "\u001b[K     |████████████████████████████████| 81 kB 7.6 MB/s \n",
            "\u001b[?25hCollecting colorlog\n",
            "  Downloading colorlog-6.7.0-py2.py3-none-any.whl (11 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from optuna==0.14.0) (1.3.5)\n",
            "Collecting alembic\n",
            "  Downloading alembic-1.8.1-py3-none-any.whl (209 kB)\n",
            "\u001b[K     |████████████████████████████████| 209 kB 66.8 MB/s \n",
            "\u001b[?25hCollecting typing\n",
            "  Downloading typing-3.7.4.3.tar.gz (78 kB)\n",
            "\u001b[K     |████████████████████████████████| 78 kB 8.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from sqlalchemy>=1.1.0->optuna==0.14.0) (5.0.0)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.7/dist-packages (from sqlalchemy>=1.1.0->optuna==0.14.0) (1.1.3.post0)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.7/dist-packages (from alembic->optuna==0.14.0) (5.10.0)\n",
            "Collecting Mako\n",
            "  Downloading Mako-1.2.3-py3-none-any.whl (78 kB)\n",
            "\u001b[K     |████████████████████████████████| 78 kB 8.3 MB/s \n",
            "\u001b[?25hCollecting autopage>=0.4.0\n",
            "  Downloading autopage-0.5.1-py3-none-any.whl (29 kB)\n",
            "Requirement already satisfied: pyparsing>=2.1.0 in /usr/local/lib/python3.7/dist-packages (from cliff->optuna==0.14.0) (3.0.9)\n",
            "Collecting stevedore>=2.0.1\n",
            "  Downloading stevedore-3.5.1-py3-none-any.whl (50 kB)\n",
            "\u001b[K     |████████████████████████████████| 50 kB 5.9 MB/s \n",
            "\u001b[?25hCollecting pbr!=2.1.0,>=2.0.0\n",
            "  Downloading pbr-5.10.0-py2.py3-none-any.whl (112 kB)\n",
            "\u001b[K     |████████████████████████████████| 112 kB 87.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: PyYAML>=3.12 in /usr/local/lib/python3.7/dist-packages (from cliff->optuna==0.14.0) (6.0)\n",
            "Collecting cmd2>=1.0.0\n",
            "  Downloading cmd2-2.4.2-py3-none-any.whl (147 kB)\n",
            "\u001b[K     |████████████████████████████████| 147 kB 80.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: PrettyTable>=0.7.2 in /usr/local/lib/python3.7/dist-packages (from cliff->optuna==0.14.0) (3.4.1)\n",
            "Requirement already satisfied: wcwidth>=0.1.7 in /usr/local/lib/python3.7/dist-packages (from cmd2>=1.0.0->cliff->optuna==0.14.0) (0.2.5)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from cmd2>=1.0.0->cliff->optuna==0.14.0) (4.1.1)\n",
            "Requirement already satisfied: attrs>=16.3.0 in /usr/local/lib/python3.7/dist-packages (from cmd2>=1.0.0->cliff->optuna==0.14.0) (22.1.0)\n",
            "Collecting pyperclip>=1.6\n",
            "  Downloading pyperclip-1.8.2.tar.gz (20 kB)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->sqlalchemy>=1.1.0->optuna==0.14.0) (3.9.0)\n",
            "Requirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.7/dist-packages (from Mako->alembic->optuna==0.14.0) (2.0.1)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas->optuna==0.14.0) (2022.4)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->optuna==0.14.0) (2.8.2)\n",
            "Building wheels for collected packages: optuna, pyperclip, typing\n",
            "  Building wheel for optuna (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for optuna: filename=optuna-0.14.0-py3-none-any.whl size=125709 sha256=136b264809c54a3f78f6e68580a63829fd0bac33230742867ff8da9ad70b6ee5\n",
            "  Stored in directory: /root/.cache/pip/wheels/c1/41/64/03b183676c5d5e978de160cab6268d5b4fb095dff63f720e01\n",
            "  Building wheel for pyperclip (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyperclip: filename=pyperclip-1.8.2-py3-none-any.whl size=11137 sha256=ea5abc6289b04e04b8baabb7de877115f27cbbd0061b85e5f0315c485fcbfbe3\n",
            "  Stored in directory: /root/.cache/pip/wheels/9f/18/84/8f69f8b08169c7bae2dde6bd7daf0c19fca8c8e500ee620a28\n",
            "  Building wheel for typing (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for typing: filename=typing-3.7.4.3-py3-none-any.whl size=26325 sha256=03f7690f704b19b82b93c540039bad95b30264beac089e2f66b731af222d97e8\n",
            "  Stored in directory: /root/.cache/pip/wheels/35/f3/15/01aa6571f0a72ee6ae7b827c1491c37a1f72d686fd22b43b0e\n",
            "Successfully built optuna pyperclip typing\n",
            "Installing collected packages: pyperclip, pbr, stevedore, Mako, cmd2, autopage, typing, colorlog, cliff, alembic, optuna\n",
            "Successfully installed Mako-1.2.3 alembic-1.8.1 autopage-0.5.1 cliff-3.10.1 cmd2-2.4.2 colorlog-6.7.0 optuna-0.14.0 pbr-5.10.0 pyperclip-1.8.2 stevedore-3.5.1 typing-3.7.4.3\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "typing"
                ]
              }
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting fastparquet\n",
            "  Downloading fastparquet-0.8.1-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.5 MB 5.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: fsspec in /usr/local/lib/python3.7/dist-packages (from fastparquet) (2022.8.2)\n",
            "Collecting cramjam>=2.3.0\n",
            "  Downloading cramjam-2.5.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.whl (1.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.6 MB 82.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pandas>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from fastparquet) (1.3.5)\n",
            "Requirement already satisfied: numpy>=1.18 in /usr/local/lib/python3.7/dist-packages (from fastparquet) (1.21.6)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=1.1.0->fastparquet) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=1.1.0->fastparquet) (2022.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas>=1.1.0->fastparquet) (1.15.0)\n",
            "Installing collected packages: cramjam, fastparquet\n",
            "Successfully installed cramjam-2.5.0 fastparquet-0.8.1\n"
          ]
        }
      ],
      "source": [
        "!pip3 install optuna==0.14.0\n",
        "!pip install fastparquet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mIDHemXODo7d"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import keras\n",
        "import tensorflow as tf\n",
        "\n",
        "from keras.datasets import mnist\n",
        "from keras.models import Sequential,Model\n",
        "from keras.layers import Activation, Flatten, Dense, Conv2D, Conv1D,Input\n",
        "from keras.layers import MaxPooling1D, Dropout, BatchNormalization\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.optimizers import SGD, Adagrad, RMSprop, Adam, Adadelta, Adamax, Nadam\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.impute import SimpleImputer\n",
        "from keras.layers.core import *\n",
        "from keras.callbacks import EarlyStopping\n",
        "from keras.callbacks import ReduceLROnPlateau\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "import keras.backend as K\n",
        "import optuna\n",
        "import math\n",
        "import random\n",
        "from copy import deepcopy\n",
        "from sklearn.utils import shuffle\n",
        "\n",
        "from scipy.stats import sem\n",
        "from numpy import mean\n",
        "from numpy import std\n",
        "from sklearn.model_selection import RepeatedKFold\n",
        "from sklearn.model_selection import cross_val_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aM7NmKnGDqs0",
        "outputId": "5bb5ff04-7581-4a77-e929-dc2ad750b14a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Keras 2.9.0\n",
            "TensorFlow 2.9.2\n",
            "Optuna 0.14.0\n"
          ]
        }
      ],
      "source": [
        "print('Keras', keras.__version__)\n",
        "print('TensorFlow', tf.__version__)\n",
        "# import Optuna and OptKeras after Keras\n",
        "print('Optuna', optuna.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LuIA_2Iqbie2",
        "outputId": "2a4e5d78-89cd-4555-e166-24d05848207c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iK4fWJp4Dstt"
      },
      "outputs": [],
      "source": [
        "train_df = pd.read_parquet(\"/content/drive/MyDrive/New Data/11features/100K11F_train_main.parquet.snappy\",engine='fastparquet')\n",
        "val_df = pd.read_parquet(\"/content/drive/MyDrive/New Data/11features/100K11F_val_main.parquet.snappy\",engine='fastparquet')\n",
        "test_df = pd.read_parquet(\"/content/drive/MyDrive/New Data/11features/100K11F_test_main.parquet.snappy\",engine='fastparquet')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "DzJVbvkxFP92",
        "outputId": "1a8e4b20-2e8c-42dd-efab-301381e9c736"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "          r_id  r_stars  r_stars_square  r_length  u_friends_count  \\\n",
              "0      2486117        5              25        41                1   \n",
              "1      3238107        5              25        64              174   \n",
              "2      5581800        5              25       239              525   \n",
              "3      5953680        4              16       129              130   \n",
              "4      2208283        1               1       537              223   \n",
              "...        ...      ...             ...       ...              ...   \n",
              "79995  2065766        2               4        61              165   \n",
              "79996  4640630        4              16        74               78   \n",
              "79997  4218981        5              25        98               12   \n",
              "79998  4633311        4              16       656              173   \n",
              "79999   531591        3               9       190                9   \n",
              "\n",
              "       u_review_count  u_month_age  b_stars  b_review_count     r_sen  \\\n",
              "0                   8    35.657224      1.5              13  0.099375   \n",
              "1                 247     1.024218      3.5              77  0.300000   \n",
              "2                 603   125.863690      4.0            1646  0.202032   \n",
              "3                 183   129.975625      4.5             356  0.129097   \n",
              "4                   1     0.000111      3.5             375  0.014266   \n",
              "...               ...          ...      ...             ...       ...   \n",
              "79995             108     0.640257      2.5               9 -0.008333   \n",
              "79996             467    47.038862      2.5              40  0.238016   \n",
              "79997              88    79.607721      4.0              30  0.123629   \n",
              "79998             206    41.955031      2.5             485  0.069350   \n",
              "79999             187    32.499059      2.5              55  0.000208   \n",
              "\n",
              "          r_sub  r_rea  r_useful  \n",
              "0      0.460000  74.49         2  \n",
              "1      0.260000  75.61         2  \n",
              "2      0.444942  84.17         3  \n",
              "3      0.444861  83.76         4  \n",
              "4      0.300989  76.86         6  \n",
              "...         ...    ...       ...  \n",
              "79995  0.354167  86.91         2  \n",
              "79996  0.480476  84.37         2  \n",
              "79997  0.247150  74.49         2  \n",
              "79998  0.415728  82.54         6  \n",
              "79999  0.171181  78.18         2  \n",
              "\n",
              "[80000 rows x 13 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-e3892bed-39c5-4a79-aa38-adb90f9e308e\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>r_id</th>\n",
              "      <th>r_stars</th>\n",
              "      <th>r_stars_square</th>\n",
              "      <th>r_length</th>\n",
              "      <th>u_friends_count</th>\n",
              "      <th>u_review_count</th>\n",
              "      <th>u_month_age</th>\n",
              "      <th>b_stars</th>\n",
              "      <th>b_review_count</th>\n",
              "      <th>r_sen</th>\n",
              "      <th>r_sub</th>\n",
              "      <th>r_rea</th>\n",
              "      <th>r_useful</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2486117</td>\n",
              "      <td>5</td>\n",
              "      <td>25</td>\n",
              "      <td>41</td>\n",
              "      <td>1</td>\n",
              "      <td>8</td>\n",
              "      <td>35.657224</td>\n",
              "      <td>1.5</td>\n",
              "      <td>13</td>\n",
              "      <td>0.099375</td>\n",
              "      <td>0.460000</td>\n",
              "      <td>74.49</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>3238107</td>\n",
              "      <td>5</td>\n",
              "      <td>25</td>\n",
              "      <td>64</td>\n",
              "      <td>174</td>\n",
              "      <td>247</td>\n",
              "      <td>1.024218</td>\n",
              "      <td>3.5</td>\n",
              "      <td>77</td>\n",
              "      <td>0.300000</td>\n",
              "      <td>0.260000</td>\n",
              "      <td>75.61</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>5581800</td>\n",
              "      <td>5</td>\n",
              "      <td>25</td>\n",
              "      <td>239</td>\n",
              "      <td>525</td>\n",
              "      <td>603</td>\n",
              "      <td>125.863690</td>\n",
              "      <td>4.0</td>\n",
              "      <td>1646</td>\n",
              "      <td>0.202032</td>\n",
              "      <td>0.444942</td>\n",
              "      <td>84.17</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>5953680</td>\n",
              "      <td>4</td>\n",
              "      <td>16</td>\n",
              "      <td>129</td>\n",
              "      <td>130</td>\n",
              "      <td>183</td>\n",
              "      <td>129.975625</td>\n",
              "      <td>4.5</td>\n",
              "      <td>356</td>\n",
              "      <td>0.129097</td>\n",
              "      <td>0.444861</td>\n",
              "      <td>83.76</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2208283</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>537</td>\n",
              "      <td>223</td>\n",
              "      <td>1</td>\n",
              "      <td>0.000111</td>\n",
              "      <td>3.5</td>\n",
              "      <td>375</td>\n",
              "      <td>0.014266</td>\n",
              "      <td>0.300989</td>\n",
              "      <td>76.86</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>79995</th>\n",
              "      <td>2065766</td>\n",
              "      <td>2</td>\n",
              "      <td>4</td>\n",
              "      <td>61</td>\n",
              "      <td>165</td>\n",
              "      <td>108</td>\n",
              "      <td>0.640257</td>\n",
              "      <td>2.5</td>\n",
              "      <td>9</td>\n",
              "      <td>-0.008333</td>\n",
              "      <td>0.354167</td>\n",
              "      <td>86.91</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>79996</th>\n",
              "      <td>4640630</td>\n",
              "      <td>4</td>\n",
              "      <td>16</td>\n",
              "      <td>74</td>\n",
              "      <td>78</td>\n",
              "      <td>467</td>\n",
              "      <td>47.038862</td>\n",
              "      <td>2.5</td>\n",
              "      <td>40</td>\n",
              "      <td>0.238016</td>\n",
              "      <td>0.480476</td>\n",
              "      <td>84.37</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>79997</th>\n",
              "      <td>4218981</td>\n",
              "      <td>5</td>\n",
              "      <td>25</td>\n",
              "      <td>98</td>\n",
              "      <td>12</td>\n",
              "      <td>88</td>\n",
              "      <td>79.607721</td>\n",
              "      <td>4.0</td>\n",
              "      <td>30</td>\n",
              "      <td>0.123629</td>\n",
              "      <td>0.247150</td>\n",
              "      <td>74.49</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>79998</th>\n",
              "      <td>4633311</td>\n",
              "      <td>4</td>\n",
              "      <td>16</td>\n",
              "      <td>656</td>\n",
              "      <td>173</td>\n",
              "      <td>206</td>\n",
              "      <td>41.955031</td>\n",
              "      <td>2.5</td>\n",
              "      <td>485</td>\n",
              "      <td>0.069350</td>\n",
              "      <td>0.415728</td>\n",
              "      <td>82.54</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>79999</th>\n",
              "      <td>531591</td>\n",
              "      <td>3</td>\n",
              "      <td>9</td>\n",
              "      <td>190</td>\n",
              "      <td>9</td>\n",
              "      <td>187</td>\n",
              "      <td>32.499059</td>\n",
              "      <td>2.5</td>\n",
              "      <td>55</td>\n",
              "      <td>0.000208</td>\n",
              "      <td>0.171181</td>\n",
              "      <td>78.18</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>80000 rows × 13 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-e3892bed-39c5-4a79-aa38-adb90f9e308e')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-e3892bed-39c5-4a79-aa38-adb90f9e308e button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-e3892bed-39c5-4a79-aa38-adb90f9e308e');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "train_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r1F2B-gBEMiM"
      },
      "outputs": [],
      "source": [
        "training_withaim=train_df.drop(labels=\"r_id\", axis=1)\n",
        "valing_withaim=val_df.drop(labels=\"r_id\",axis=1)\n",
        "testing_withaim=test_df.drop(labels=\"r_id\", axis=1)\n",
        "\n",
        "#Check the NaN in data and drop them\n",
        "imp_train=SimpleImputer(missing_values=np.NaN)\n",
        "training=pd.DataFrame(imp_train.fit_transform(training_withaim))\n",
        "\n",
        "imp_test=SimpleImputer(missing_values=np.NaN)\n",
        "testing=pd.DataFrame(imp_test.fit_transform(testing_withaim))\n",
        "\n",
        "imp_val=SimpleImputer(missing_values=np.NaN)\n",
        "valing=pd.DataFrame(imp_val.fit_transform(valing_withaim))\n",
        "\n",
        "# There aren't any nan data in the dataframe \n",
        "training = training.iloc[: , 0:11]\n",
        "testing = testing.iloc[: , 0:11]\n",
        "valing=valing.iloc[:,0:11]\n",
        "scaler = StandardScaler()\n",
        "\n",
        "# scale skewed features\n",
        "std_train_df = train_df.copy(deep=True)\n",
        "std_train_df = scaler.fit_transform(training)\n",
        "std_test_df = test_df.copy(deep=True)\n",
        "std_test_df = scaler.transform(testing)\n",
        "std_val_df = val_df.copy(deep=True)\n",
        "std_val_df = scaler.transform(valing)\n",
        "#std_test_df[['r_stars','r_stars_square','r_length', 'u_friends_count', 'u_review_count', 'u_month_age', 'b_stars','b_review_count','r_rea']] = scaler.transform(test_df[['r_stars','r_stars_square','r_length', 'u_friends_count', 'u_review_count', 'u_month_age', 'b_stars','b_review_count','r_rea']])\n",
        "\n",
        "std_train_df = pd.DataFrame(std_train_df)\n",
        "std_val_df = pd.DataFrame(std_val_df)\n",
        "std_test_df = pd.DataFrame(std_test_df)\n",
        "\n",
        "training = std_train_df.iloc[: , 0:12]\n",
        "valing = std_val_df.iloc[: , 0:12]\n",
        "testing = std_test_df.iloc[: , 0:12]\n",
        "\n",
        "labelsForTrain=training_withaim.iloc[: , -1]\n",
        "labelsForVal=valing_withaim.iloc[: , -1]\n",
        "labelsForTest=testing_withaim.iloc[: , -1]\n",
        "input_shape = training.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kt9gM3cRieQ9",
        "outputId": "2d78aff7-f57d-41b3-9578-11f6062bacb3"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0        2\n",
              "1        2\n",
              "2        3\n",
              "3        4\n",
              "4        6\n",
              "        ..\n",
              "79995    2\n",
              "79996    2\n",
              "79997    2\n",
              "79998    6\n",
              "79999    2\n",
              "Name: r_useful, Length: 80000, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "labelsForTrain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "Ohik52yFigXK",
        "outputId": "e3e8a02f-d8fa-4049-81c1-e99d48260983"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "             0         1         2         3         4         5         6   \\\n",
              "0      0.902431  0.996586 -0.805509 -0.329454 -0.380837 -0.205818 -2.842089   \n",
              "1      0.902431  0.996586 -0.603075 -0.026023  0.120685 -1.203206 -0.314664   \n",
              "2      0.902431  0.996586  0.937184  0.589607  0.867722  2.392019  0.317192   \n",
              "3      0.253649  0.062198 -0.030979 -0.103196 -0.013614  2.510438  0.949048   \n",
              "4     -1.692696 -1.495115  3.560026  0.059919 -0.395526 -1.232699 -0.314664   \n",
              "...         ...       ...       ...       ...       ...       ...       ...   \n",
              "79995 -1.043914 -1.183653 -0.629480 -0.041809 -0.170995 -1.214264 -1.578376   \n",
              "79996  0.253649  0.062198 -0.515060 -0.194401  0.582337  0.121960 -1.578376   \n",
              "79997  0.902431  0.996586 -0.303825 -0.310161 -0.212964  1.059903  0.317192   \n",
              "79998  0.253649  0.062198  4.607402 -0.027777  0.034650 -0.024449 -1.578376   \n",
              "79999 -0.395132 -0.664548  0.505912 -0.315422 -0.005220 -0.296769 -1.578376   \n",
              "\n",
              "             7         8         9         10  \n",
              "0     -0.481997 -0.428646  0.106826 -0.509433  \n",
              "1     -0.387357  0.611427 -1.250932 -0.406493  \n",
              "2      1.932788  0.103544  0.004600  0.380261  \n",
              "3      0.025212 -0.274562  0.004050  0.342578  \n",
              "4      0.053308 -0.869865 -0.972666 -0.291605  \n",
              "...         ...       ...       ...       ...  \n",
              "79995 -0.487912 -0.987022 -0.611652  0.632096  \n",
              "79996 -0.442071  0.290091  0.245833  0.398643  \n",
              "79997 -0.456858 -0.302909 -1.338167 -0.509433  \n",
              "79998  0.215969 -0.584301 -0.193727  0.230447  \n",
              "79999 -0.419890 -0.942744 -1.853905 -0.170283  \n",
              "\n",
              "[80000 rows x 11 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-3a1b9fc3-1584-4633-b490-260dfd56ad57\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.902431</td>\n",
              "      <td>0.996586</td>\n",
              "      <td>-0.805509</td>\n",
              "      <td>-0.329454</td>\n",
              "      <td>-0.380837</td>\n",
              "      <td>-0.205818</td>\n",
              "      <td>-2.842089</td>\n",
              "      <td>-0.481997</td>\n",
              "      <td>-0.428646</td>\n",
              "      <td>0.106826</td>\n",
              "      <td>-0.509433</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.902431</td>\n",
              "      <td>0.996586</td>\n",
              "      <td>-0.603075</td>\n",
              "      <td>-0.026023</td>\n",
              "      <td>0.120685</td>\n",
              "      <td>-1.203206</td>\n",
              "      <td>-0.314664</td>\n",
              "      <td>-0.387357</td>\n",
              "      <td>0.611427</td>\n",
              "      <td>-1.250932</td>\n",
              "      <td>-0.406493</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.902431</td>\n",
              "      <td>0.996586</td>\n",
              "      <td>0.937184</td>\n",
              "      <td>0.589607</td>\n",
              "      <td>0.867722</td>\n",
              "      <td>2.392019</td>\n",
              "      <td>0.317192</td>\n",
              "      <td>1.932788</td>\n",
              "      <td>0.103544</td>\n",
              "      <td>0.004600</td>\n",
              "      <td>0.380261</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.253649</td>\n",
              "      <td>0.062198</td>\n",
              "      <td>-0.030979</td>\n",
              "      <td>-0.103196</td>\n",
              "      <td>-0.013614</td>\n",
              "      <td>2.510438</td>\n",
              "      <td>0.949048</td>\n",
              "      <td>0.025212</td>\n",
              "      <td>-0.274562</td>\n",
              "      <td>0.004050</td>\n",
              "      <td>0.342578</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>-1.692696</td>\n",
              "      <td>-1.495115</td>\n",
              "      <td>3.560026</td>\n",
              "      <td>0.059919</td>\n",
              "      <td>-0.395526</td>\n",
              "      <td>-1.232699</td>\n",
              "      <td>-0.314664</td>\n",
              "      <td>0.053308</td>\n",
              "      <td>-0.869865</td>\n",
              "      <td>-0.972666</td>\n",
              "      <td>-0.291605</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>79995</th>\n",
              "      <td>-1.043914</td>\n",
              "      <td>-1.183653</td>\n",
              "      <td>-0.629480</td>\n",
              "      <td>-0.041809</td>\n",
              "      <td>-0.170995</td>\n",
              "      <td>-1.214264</td>\n",
              "      <td>-1.578376</td>\n",
              "      <td>-0.487912</td>\n",
              "      <td>-0.987022</td>\n",
              "      <td>-0.611652</td>\n",
              "      <td>0.632096</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>79996</th>\n",
              "      <td>0.253649</td>\n",
              "      <td>0.062198</td>\n",
              "      <td>-0.515060</td>\n",
              "      <td>-0.194401</td>\n",
              "      <td>0.582337</td>\n",
              "      <td>0.121960</td>\n",
              "      <td>-1.578376</td>\n",
              "      <td>-0.442071</td>\n",
              "      <td>0.290091</td>\n",
              "      <td>0.245833</td>\n",
              "      <td>0.398643</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>79997</th>\n",
              "      <td>0.902431</td>\n",
              "      <td>0.996586</td>\n",
              "      <td>-0.303825</td>\n",
              "      <td>-0.310161</td>\n",
              "      <td>-0.212964</td>\n",
              "      <td>1.059903</td>\n",
              "      <td>0.317192</td>\n",
              "      <td>-0.456858</td>\n",
              "      <td>-0.302909</td>\n",
              "      <td>-1.338167</td>\n",
              "      <td>-0.509433</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>79998</th>\n",
              "      <td>0.253649</td>\n",
              "      <td>0.062198</td>\n",
              "      <td>4.607402</td>\n",
              "      <td>-0.027777</td>\n",
              "      <td>0.034650</td>\n",
              "      <td>-0.024449</td>\n",
              "      <td>-1.578376</td>\n",
              "      <td>0.215969</td>\n",
              "      <td>-0.584301</td>\n",
              "      <td>-0.193727</td>\n",
              "      <td>0.230447</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>79999</th>\n",
              "      <td>-0.395132</td>\n",
              "      <td>-0.664548</td>\n",
              "      <td>0.505912</td>\n",
              "      <td>-0.315422</td>\n",
              "      <td>-0.005220</td>\n",
              "      <td>-0.296769</td>\n",
              "      <td>-1.578376</td>\n",
              "      <td>-0.419890</td>\n",
              "      <td>-0.942744</td>\n",
              "      <td>-1.853905</td>\n",
              "      <td>-0.170283</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>80000 rows × 11 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-3a1b9fc3-1584-4633-b490-260dfd56ad57')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-3a1b9fc3-1584-4633-b490-260dfd56ad57 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-3a1b9fc3-1584-4633-b490-260dfd56ad57');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bz522wVVHDEh"
      },
      "outputs": [],
      "source": [
        "def create_model(activation, num_hidden_layer, num_hidden_unit):\n",
        "  inputs = Input(shape=(training.shape[1],))\n",
        "  model = inputs\n",
        "  for i in range(1,num_hidden_layer):\n",
        "    model = Dense(num_hidden_unit, activation=activation,)(model)\n",
        "        \n",
        "        \n",
        "  model = Dense(1,)(model)\n",
        "  model = Model(inputs, model)\n",
        "\n",
        "  return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kDK6LaeoHXd6"
      },
      "outputs": [],
      "source": [
        "kfold = KFold(n_splits=5, shuffle=True)\n",
        "def objective(trial:optuna.Trial,data_train,result_train):\n",
        "  K.clear_session()\n",
        "    \n",
        "  activation = trial.suggest_categorical('activation',['relu','tanh','linear'])\n",
        "  #Leave fewer optimizer\n",
        "    \n",
        "  num_hidden_layer = trial.suggest_int('num_hidden_layer',2,4)\n",
        "  #get more features per layer, add num of hidden unit if have time\n",
        "\n",
        "  #define the number of unit with 2^n\n",
        "  i = trial.suggest_int('i',6,10)\n",
        "  num_hidden_unit = 2**i\n",
        "  \n",
        "  #Try to adjust learning_rate\n",
        "\n",
        "  learning_rate = trial.suggest_loguniform('learning_rate', 0.0001,0.01)\n",
        "  \n",
        "  # Gradient Clipping\n",
        "\n",
        "  optimizer = Adam(learning_rate=learning_rate,clipnorm=1.0)\n",
        "    \n",
        "  num_folds = 3\n",
        "\n",
        "  loss_per_fold = []\n",
        "  es = EarlyStopping(monitor='val_mse', patience=5)\n",
        "\n",
        "  model = create_model(activation,num_hidden_layer,num_hidden_unit)\n",
        "  model_list.append(model)\n",
        "  model.compile(loss='mse',optimizer=optimizer,metrics=['mse','mae'])\n",
        "  #learning scheduler\n",
        "  reduce_lr = ReduceLROnPlateau(monitor='val_mse', factor=0.2,\n",
        "                              patience=5, min_lr=0.001)\n",
        "  fold_no=1\n",
        "      # Fit data to model\n",
        "  score_int=[]\n",
        "\n",
        "  for train,test in kfold.split(data_train,result_train):\n",
        "\n",
        "    print('------------------------------------------------------------------------')\n",
        "    print(f'Training for fold {fold_no} ...')\n",
        "\n",
        "    new_train_CV=data_train.iloc[train]\n",
        "    new_label=result_train.iloc[train]\n",
        "\n",
        "    val_CV=data_train.iloc[test]\n",
        "    val_result=result_train.iloc[test]\n",
        "\n",
        "    # Fit data to model\n",
        "    history = model.fit(new_train_CV,new_label,\n",
        "                batch_size=64,\n",
        "                epochs=100,\n",
        "                verbose=2,\n",
        "                validation_data=(val_CV,val_result),\n",
        "                validation_batch_size=64,\n",
        "                callbacks=[es,reduce_lr])\n",
        "    \n",
        "    scores=model.evaluate(val_CV,val_result,verbose=0)\n",
        "    print(f'Score for fold {fold_no}: {model.metrics_names[0]} of {scores[0]}')\n",
        "    loss_per_fold.append(scores[0])\n",
        "    # Increase fold number\n",
        "    score_int.append(round(scores[1],2))\n",
        "    fold_no = fold_no + 1  \n",
        "    \n",
        "  mse = np.array(history.history['mse'])\n",
        "\n",
        "  #loss_per_fold.append(scores[0])\n",
        "\n",
        "  history_list.append(history)\n",
        "  return np.mean(score_int)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-svTHldV9Wi5",
        "outputId": "c83cccd3-6ce1-4c66-d8ab-81cc85bd4e5d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 3s - loss: 16.2763 - mse: 16.2763 - mae: 1.6251 - val_loss: 12.0089 - val_mse: 12.0089 - val_mae: 1.6946 - lr: 0.0019 - 3s/epoch - 3ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 15.9967 - mse: 15.9967 - mae: 1.6034 - val_loss: 11.9670 - val_mse: 11.9670 - val_mae: 1.5578 - lr: 0.0019 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 15.9137 - mse: 15.9137 - mae: 1.6024 - val_loss: 11.8501 - val_mse: 11.8501 - val_mae: 1.5811 - lr: 0.0019 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 15.8924 - mse: 15.8924 - mae: 1.5940 - val_loss: 11.8098 - val_mse: 11.8098 - val_mae: 1.6091 - lr: 0.0019 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 15.7952 - mse: 15.7952 - mae: 1.5942 - val_loss: 11.7593 - val_mse: 11.7593 - val_mae: 1.6272 - lr: 0.0019 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 15.7921 - mse: 15.7921 - mae: 1.5897 - val_loss: 11.7735 - val_mse: 11.7735 - val_mae: 1.5365 - lr: 0.0019 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 15.7657 - mse: 15.7657 - mae: 1.5848 - val_loss: 11.7645 - val_mse: 11.7645 - val_mae: 1.6384 - lr: 0.0019 - 2s/epoch - 2ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 2s - loss: 15.7623 - mse: 15.7623 - mae: 1.5890 - val_loss: 11.7917 - val_mse: 11.7917 - val_mae: 1.5827 - lr: 0.0019 - 2s/epoch - 2ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 2s - loss: 15.7196 - mse: 15.7196 - mae: 1.5899 - val_loss: 12.0391 - val_mse: 12.0391 - val_mae: 1.5548 - lr: 0.0019 - 2s/epoch - 2ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 2s - loss: 15.7924 - mse: 15.7924 - mae: 1.5862 - val_loss: 11.7407 - val_mse: 11.7407 - val_mae: 1.6108 - lr: 0.0019 - 2s/epoch - 2ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 2s - loss: 15.7741 - mse: 15.7741 - mae: 1.5818 - val_loss: 11.7094 - val_mse: 11.7094 - val_mae: 1.5875 - lr: 0.0019 - 2s/epoch - 2ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 2s - loss: 15.6545 - mse: 15.6545 - mae: 1.5771 - val_loss: 11.7444 - val_mse: 11.7444 - val_mae: 1.5857 - lr: 0.0019 - 2s/epoch - 2ms/step\n",
            "Epoch 13/100\n",
            "1000/1000 - 2s - loss: 15.6065 - mse: 15.6065 - mae: 1.5766 - val_loss: 11.8321 - val_mse: 11.8321 - val_mae: 1.5954 - lr: 0.0019 - 2s/epoch - 2ms/step\n",
            "Epoch 14/100\n",
            "1000/1000 - 2s - loss: 15.7325 - mse: 15.7325 - mae: 1.5785 - val_loss: 11.7322 - val_mse: 11.7322 - val_mae: 1.5339 - lr: 0.0019 - 2s/epoch - 2ms/step\n",
            "Epoch 15/100\n",
            "1000/1000 - 2s - loss: 15.6227 - mse: 15.6227 - mae: 1.5759 - val_loss: 11.7725 - val_mse: 11.7725 - val_mae: 1.5689 - lr: 0.0019 - 2s/epoch - 2ms/step\n",
            "Epoch 16/100\n",
            "1000/1000 - 2s - loss: 15.6672 - mse: 15.6672 - mae: 1.5762 - val_loss: 11.6646 - val_mse: 11.6646 - val_mae: 1.6446 - lr: 0.0019 - 2s/epoch - 2ms/step\n",
            "Epoch 17/100\n",
            "1000/1000 - 2s - loss: 15.6869 - mse: 15.6869 - mae: 1.5793 - val_loss: 11.7688 - val_mse: 11.7688 - val_mae: 1.5444 - lr: 0.0019 - 2s/epoch - 2ms/step\n",
            "Epoch 18/100\n",
            "1000/1000 - 2s - loss: 15.6846 - mse: 15.6846 - mae: 1.5758 - val_loss: 11.7876 - val_mse: 11.7876 - val_mae: 1.5285 - lr: 0.0019 - 2s/epoch - 2ms/step\n",
            "Epoch 19/100\n",
            "1000/1000 - 2s - loss: 15.6305 - mse: 15.6305 - mae: 1.5792 - val_loss: 11.5907 - val_mse: 11.5907 - val_mae: 1.5709 - lr: 0.0019 - 2s/epoch - 2ms/step\n",
            "Epoch 20/100\n",
            "1000/1000 - 2s - loss: 15.4805 - mse: 15.4805 - mae: 1.5765 - val_loss: 11.6068 - val_mse: 11.6068 - val_mae: 1.5587 - lr: 0.0019 - 2s/epoch - 2ms/step\n",
            "Epoch 21/100\n",
            "1000/1000 - 2s - loss: 15.6926 - mse: 15.6926 - mae: 1.5777 - val_loss: 11.8075 - val_mse: 11.8075 - val_mae: 1.5880 - lr: 0.0019 - 2s/epoch - 2ms/step\n",
            "Epoch 22/100\n",
            "1000/1000 - 2s - loss: 15.6087 - mse: 15.6087 - mae: 1.5787 - val_loss: 11.6567 - val_mse: 11.6567 - val_mae: 1.6241 - lr: 0.0019 - 2s/epoch - 2ms/step\n",
            "Epoch 23/100\n",
            "1000/1000 - 2s - loss: 15.6069 - mse: 15.6069 - mae: 1.5763 - val_loss: 11.6367 - val_mse: 11.6367 - val_mae: 1.5811 - lr: 0.0019 - 2s/epoch - 2ms/step\n",
            "Epoch 24/100\n",
            "1000/1000 - 2s - loss: 15.5838 - mse: 15.5838 - mae: 1.5765 - val_loss: 11.7172 - val_mse: 11.7172 - val_mae: 1.5374 - lr: 0.0019 - 2s/epoch - 2ms/step\n",
            "Score for fold 1: loss of 11.717238426208496\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 14.2860 - mse: 14.2860 - mae: 1.5542 - val_loss: 16.3013 - val_mse: 16.3013 - val_mae: 1.6055 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 14.1682 - mse: 14.1682 - mae: 1.5498 - val_loss: 16.4413 - val_mse: 16.4413 - val_mae: 1.5944 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 14.1607 - mse: 14.1607 - mae: 1.5536 - val_loss: 16.3307 - val_mse: 16.3307 - val_mae: 1.5889 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 14.2003 - mse: 14.2003 - mae: 1.5461 - val_loss: 16.4842 - val_mse: 16.4842 - val_mae: 1.6124 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 14.1904 - mse: 14.1904 - mae: 1.5520 - val_loss: 16.4819 - val_mse: 16.4819 - val_mae: 1.5858 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 14.1086 - mse: 14.1086 - mae: 1.5401 - val_loss: 16.4919 - val_mse: 16.4919 - val_mae: 1.6074 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Score for fold 2: loss of 16.491933822631836\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 15.6123 - mse: 15.6123 - mae: 1.5735 - val_loss: 10.6324 - val_mse: 10.6324 - val_mae: 1.5364 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 15.5393 - mse: 15.5393 - mae: 1.5709 - val_loss: 10.8532 - val_mse: 10.8532 - val_mae: 1.4938 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 15.5570 - mse: 15.5570 - mae: 1.5751 - val_loss: 10.8982 - val_mse: 10.8982 - val_mae: 1.5154 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 15.4753 - mse: 15.4753 - mae: 1.5701 - val_loss: 10.6811 - val_mse: 10.6811 - val_mae: 1.5002 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 15.4558 - mse: 15.4558 - mae: 1.5708 - val_loss: 10.8050 - val_mse: 10.8050 - val_mae: 1.5242 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 15.4611 - mse: 15.4611 - mae: 1.5690 - val_loss: 11.1192 - val_mse: 11.1192 - val_mae: 1.5259 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Score for fold 3: loss of 11.119240760803223\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 12.4798 - mse: 12.4798 - mae: 1.5509 - val_loss: 22.4116 - val_mse: 22.4116 - val_mae: 1.5937 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 12.4360 - mse: 12.4360 - mae: 1.5488 - val_loss: 22.3212 - val_mse: 22.3212 - val_mae: 1.5711 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 12.3571 - mse: 12.3571 - mae: 1.5461 - val_loss: 22.5875 - val_mse: 22.5875 - val_mae: 1.6027 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 12.4415 - mse: 12.4415 - mae: 1.5439 - val_loss: 22.6345 - val_mse: 22.6345 - val_mae: 1.5745 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 12.3943 - mse: 12.3943 - mae: 1.5425 - val_loss: 22.4832 - val_mse: 22.4832 - val_mae: 1.6456 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 12.4121 - mse: 12.4121 - mae: 1.5465 - val_loss: 22.4167 - val_mse: 22.4167 - val_mae: 1.6213 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 12.3763 - mse: 12.3763 - mae: 1.5450 - val_loss: 22.5678 - val_mse: 22.5678 - val_mae: 1.6009 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Score for fold 4: loss of 22.567808151245117\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 15.1688 - mse: 15.1688 - mae: 1.5569 - val_loss: 11.4693 - val_mse: 11.4693 - val_mae: 1.5828 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 15.0948 - mse: 15.0948 - mae: 1.5554 - val_loss: 11.7833 - val_mse: 11.7833 - val_mae: 1.5572 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 15.1138 - mse: 15.1138 - mae: 1.5536 - val_loss: 11.8112 - val_mse: 11.8112 - val_mae: 1.5994 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 15.0681 - mse: 15.0681 - mae: 1.5558 - val_loss: 11.8350 - val_mse: 11.8350 - val_mae: 1.5043 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 15.0593 - mse: 15.0593 - mae: 1.5539 - val_loss: 11.8270 - val_mse: 11.8270 - val_mae: 1.5364 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 15.0119 - mse: 15.0119 - mae: 1.5492 - val_loss: 11.8577 - val_mse: 11.8577 - val_mae: 1.5774 - lr: 0.0010 - 2s/epoch - 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-10-14 11:11:54,696]\u001b[0m Finished trial#0 resulted in value: 14.751999999999999. Current best value is 14.751999999999999 with parameters: {'activation': 'relu', 'num_hidden_layer': 3, 'i': 7, 'learning_rate': 0.0018797852452371259}.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Score for fold 5: loss of 11.857675552368164\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 20.7118 - mse: 20.7118 - mae: 2.0586 - val_loss: 16.6573 - val_mse: 16.6573 - val_mae: 1.5344 - lr: 1.3816e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 16.0298 - mse: 16.0298 - mae: 1.5819 - val_loss: 15.4783 - val_mse: 15.4783 - val_mae: 1.6289 - lr: 1.3816e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 15.7339 - mse: 15.7339 - mae: 1.6238 - val_loss: 15.3959 - val_mse: 15.3959 - val_mae: 1.6246 - lr: 1.3816e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 15.6711 - mse: 15.6711 - mae: 1.6212 - val_loss: 15.3487 - val_mse: 15.3487 - val_mae: 1.6163 - lr: 1.3816e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 15.6302 - mse: 15.6302 - mae: 1.6195 - val_loss: 15.3098 - val_mse: 15.3098 - val_mae: 1.6184 - lr: 1.3816e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 15.6054 - mse: 15.6054 - mae: 1.6163 - val_loss: 15.2833 - val_mse: 15.2833 - val_mae: 1.6214 - lr: 1.3816e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 15.5894 - mse: 15.5894 - mae: 1.6143 - val_loss: 15.2565 - val_mse: 15.2565 - val_mae: 1.6146 - lr: 1.3816e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 2s - loss: 15.5608 - mse: 15.5608 - mae: 1.6114 - val_loss: 15.2477 - val_mse: 15.2477 - val_mae: 1.6074 - lr: 1.3816e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 2s - loss: 15.5478 - mse: 15.5478 - mae: 1.6066 - val_loss: 15.2216 - val_mse: 15.2216 - val_mae: 1.6117 - lr: 1.3816e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 2s - loss: 15.5229 - mse: 15.5229 - mae: 1.6106 - val_loss: 15.2036 - val_mse: 15.2036 - val_mae: 1.6167 - lr: 1.3816e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 2s - loss: 15.5125 - mse: 15.5125 - mae: 1.6067 - val_loss: 15.1998 - val_mse: 15.1998 - val_mae: 1.6077 - lr: 1.3816e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 2s - loss: 15.4992 - mse: 15.4992 - mae: 1.6062 - val_loss: 15.1758 - val_mse: 15.1758 - val_mae: 1.6156 - lr: 1.3816e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 13/100\n",
            "1000/1000 - 2s - loss: 15.4768 - mse: 15.4768 - mae: 1.6089 - val_loss: 15.1621 - val_mse: 15.1621 - val_mae: 1.6098 - lr: 1.3816e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 14/100\n",
            "1000/1000 - 2s - loss: 15.4633 - mse: 15.4633 - mae: 1.6073 - val_loss: 15.1616 - val_mse: 15.1616 - val_mae: 1.5973 - lr: 1.3816e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 15/100\n",
            "1000/1000 - 2s - loss: 15.4616 - mse: 15.4616 - mae: 1.6014 - val_loss: 15.1375 - val_mse: 15.1375 - val_mae: 1.6071 - lr: 1.3816e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 16/100\n",
            "1000/1000 - 2s - loss: 15.4434 - mse: 15.4434 - mae: 1.6041 - val_loss: 15.1398 - val_mse: 15.1398 - val_mae: 1.5964 - lr: 1.3816e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 17/100\n",
            "1000/1000 - 2s - loss: 15.4367 - mse: 15.4367 - mae: 1.6009 - val_loss: 15.1148 - val_mse: 15.1148 - val_mae: 1.6132 - lr: 1.3816e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 18/100\n",
            "1000/1000 - 2s - loss: 15.4176 - mse: 15.4176 - mae: 1.6056 - val_loss: 15.1167 - val_mse: 15.1167 - val_mae: 1.5933 - lr: 1.3816e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 19/100\n",
            "1000/1000 - 2s - loss: 15.4050 - mse: 15.4050 - mae: 1.6010 - val_loss: 15.1090 - val_mse: 15.1090 - val_mae: 1.5885 - lr: 1.3816e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 20/100\n",
            "1000/1000 - 2s - loss: 15.4168 - mse: 15.4168 - mae: 1.5968 - val_loss: 15.0839 - val_mse: 15.0839 - val_mae: 1.6078 - lr: 1.3816e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 21/100\n",
            "1000/1000 - 2s - loss: 15.4035 - mse: 15.4035 - mae: 1.5996 - val_loss: 15.0702 - val_mse: 15.0702 - val_mae: 1.6021 - lr: 1.3816e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 22/100\n",
            "1000/1000 - 2s - loss: 15.3813 - mse: 15.3813 - mae: 1.5991 - val_loss: 15.0773 - val_mse: 15.0773 - val_mae: 1.5924 - lr: 1.3816e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 23/100\n",
            "1000/1000 - 2s - loss: 15.3783 - mse: 15.3783 - mae: 1.5987 - val_loss: 15.0546 - val_mse: 15.0546 - val_mae: 1.6135 - lr: 1.3816e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 24/100\n",
            "1000/1000 - 2s - loss: 15.3711 - mse: 15.3711 - mae: 1.5994 - val_loss: 15.0408 - val_mse: 15.0408 - val_mae: 1.6065 - lr: 1.3816e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 25/100\n",
            "1000/1000 - 2s - loss: 15.3586 - mse: 15.3586 - mae: 1.5958 - val_loss: 15.0372 - val_mse: 15.0372 - val_mae: 1.6001 - lr: 1.3816e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 26/100\n",
            "1000/1000 - 2s - loss: 15.3432 - mse: 15.3432 - mae: 1.5966 - val_loss: 15.0434 - val_mse: 15.0434 - val_mae: 1.5976 - lr: 1.3816e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 27/100\n",
            "1000/1000 - 2s - loss: 15.3412 - mse: 15.3412 - mae: 1.5965 - val_loss: 15.0193 - val_mse: 15.0193 - val_mae: 1.6004 - lr: 1.3816e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 28/100\n",
            "1000/1000 - 2s - loss: 15.3287 - mse: 15.3287 - mae: 1.5988 - val_loss: 15.0208 - val_mse: 15.0208 - val_mae: 1.5935 - lr: 1.3816e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 29/100\n",
            "1000/1000 - 2s - loss: 15.3319 - mse: 15.3319 - mae: 1.5905 - val_loss: 14.9985 - val_mse: 14.9985 - val_mae: 1.6092 - lr: 1.3816e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 30/100\n",
            "1000/1000 - 2s - loss: 15.3253 - mse: 15.3253 - mae: 1.5924 - val_loss: 14.9916 - val_mse: 14.9916 - val_mae: 1.6110 - lr: 1.3816e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 31/100\n",
            "1000/1000 - 2s - loss: 15.3056 - mse: 15.3056 - mae: 1.5982 - val_loss: 14.9916 - val_mse: 14.9916 - val_mae: 1.5941 - lr: 1.3816e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 32/100\n",
            "1000/1000 - 2s - loss: 15.3015 - mse: 15.3015 - mae: 1.5905 - val_loss: 14.9942 - val_mse: 14.9942 - val_mae: 1.5966 - lr: 1.3816e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 33/100\n",
            "1000/1000 - 2s - loss: 15.3004 - mse: 15.3004 - mae: 1.5921 - val_loss: 14.9924 - val_mse: 14.9924 - val_mae: 1.5851 - lr: 1.3816e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 34/100\n",
            "1000/1000 - 2s - loss: 15.2852 - mse: 15.2852 - mae: 1.5905 - val_loss: 14.9876 - val_mse: 14.9876 - val_mae: 1.5880 - lr: 1.3816e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 35/100\n",
            "1000/1000 - 2s - loss: 15.2857 - mse: 15.2857 - mae: 1.5922 - val_loss: 14.9809 - val_mse: 14.9809 - val_mae: 1.5887 - lr: 1.3816e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 36/100\n",
            "1000/1000 - 2s - loss: 15.2826 - mse: 15.2826 - mae: 1.5923 - val_loss: 14.9712 - val_mse: 14.9712 - val_mae: 1.5885 - lr: 1.3816e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 37/100\n",
            "1000/1000 - 2s - loss: 15.2750 - mse: 15.2750 - mae: 1.5888 - val_loss: 14.9736 - val_mse: 14.9736 - val_mae: 1.5904 - lr: 1.3816e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 38/100\n",
            "1000/1000 - 2s - loss: 15.2692 - mse: 15.2692 - mae: 1.5904 - val_loss: 14.9554 - val_mse: 14.9554 - val_mae: 1.5998 - lr: 1.3816e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 39/100\n",
            "1000/1000 - 2s - loss: 15.2579 - mse: 15.2579 - mae: 1.5904 - val_loss: 14.9601 - val_mse: 14.9601 - val_mae: 1.5953 - lr: 1.3816e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 40/100\n",
            "1000/1000 - 2s - loss: 15.2611 - mse: 15.2611 - mae: 1.5903 - val_loss: 14.9483 - val_mse: 14.9483 - val_mae: 1.5934 - lr: 1.3816e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 41/100\n",
            "1000/1000 - 2s - loss: 15.2538 - mse: 15.2538 - mae: 1.5879 - val_loss: 14.9528 - val_mse: 14.9528 - val_mae: 1.5812 - lr: 1.3816e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 42/100\n",
            "1000/1000 - 2s - loss: 15.2523 - mse: 15.2523 - mae: 1.5873 - val_loss: 14.9375 - val_mse: 14.9375 - val_mae: 1.5857 - lr: 1.3816e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 43/100\n",
            "1000/1000 - 2s - loss: 15.2379 - mse: 15.2379 - mae: 1.5863 - val_loss: 14.9178 - val_mse: 14.9178 - val_mae: 1.6012 - lr: 1.3816e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 44/100\n",
            "1000/1000 - 2s - loss: 15.2348 - mse: 15.2348 - mae: 1.5863 - val_loss: 14.9318 - val_mse: 14.9318 - val_mae: 1.5945 - lr: 1.3816e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 45/100\n",
            "1000/1000 - 2s - loss: 15.2359 - mse: 15.2359 - mae: 1.5880 - val_loss: 14.9187 - val_mse: 14.9187 - val_mae: 1.5923 - lr: 1.3816e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 46/100\n",
            "1000/1000 - 2s - loss: 15.2309 - mse: 15.2309 - mae: 1.5877 - val_loss: 14.9149 - val_mse: 14.9149 - val_mae: 1.5875 - lr: 1.3816e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 47/100\n",
            "1000/1000 - 2s - loss: 15.2255 - mse: 15.2255 - mae: 1.5859 - val_loss: 14.9129 - val_mse: 14.9129 - val_mae: 1.5845 - lr: 1.3816e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 48/100\n",
            "1000/1000 - 2s - loss: 15.2181 - mse: 15.2181 - mae: 1.5845 - val_loss: 14.9128 - val_mse: 14.9128 - val_mae: 1.5900 - lr: 1.3816e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 49/100\n",
            "1000/1000 - 2s - loss: 15.2259 - mse: 15.2259 - mae: 1.5864 - val_loss: 14.9150 - val_mse: 14.9150 - val_mae: 1.5842 - lr: 1.3816e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 50/100\n",
            "1000/1000 - 2s - loss: 15.2184 - mse: 15.2184 - mae: 1.5843 - val_loss: 14.8930 - val_mse: 14.8930 - val_mae: 1.5843 - lr: 1.3816e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 51/100\n",
            "1000/1000 - 2s - loss: 15.2131 - mse: 15.2131 - mae: 1.5837 - val_loss: 14.8973 - val_mse: 14.8973 - val_mae: 1.5889 - lr: 1.3816e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 52/100\n",
            "1000/1000 - 2s - loss: 15.2096 - mse: 15.2096 - mae: 1.5865 - val_loss: 14.8959 - val_mse: 14.8959 - val_mae: 1.5863 - lr: 1.3816e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 53/100\n",
            "1000/1000 - 2s - loss: 15.1979 - mse: 15.1979 - mae: 1.5870 - val_loss: 14.8981 - val_mse: 14.8981 - val_mae: 1.5865 - lr: 1.3816e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 54/100\n",
            "1000/1000 - 2s - loss: 15.1998 - mse: 15.1998 - mae: 1.5830 - val_loss: 14.8973 - val_mse: 14.8973 - val_mae: 1.5848 - lr: 1.3816e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 55/100\n",
            "1000/1000 - 2s - loss: 15.1996 - mse: 15.1996 - mae: 1.5827 - val_loss: 14.8810 - val_mse: 14.8810 - val_mae: 1.5923 - lr: 1.3816e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 56/100\n",
            "1000/1000 - 2s - loss: 15.1912 - mse: 15.1912 - mae: 1.5878 - val_loss: 14.8915 - val_mse: 14.8915 - val_mae: 1.5819 - lr: 1.3816e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 57/100\n",
            "1000/1000 - 2s - loss: 15.1908 - mse: 15.1908 - mae: 1.5840 - val_loss: 14.8876 - val_mse: 14.8876 - val_mae: 1.5765 - lr: 1.3816e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 58/100\n",
            "1000/1000 - 2s - loss: 15.1934 - mse: 15.1934 - mae: 1.5802 - val_loss: 14.8754 - val_mse: 14.8754 - val_mae: 1.5804 - lr: 1.3816e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 59/100\n",
            "1000/1000 - 2s - loss: 15.1837 - mse: 15.1837 - mae: 1.5817 - val_loss: 14.8697 - val_mse: 14.8697 - val_mae: 1.5910 - lr: 1.3816e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 60/100\n",
            "1000/1000 - 2s - loss: 15.1780 - mse: 15.1780 - mae: 1.5831 - val_loss: 14.8572 - val_mse: 14.8572 - val_mae: 1.6016 - lr: 1.3816e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 61/100\n",
            "1000/1000 - 2s - loss: 15.1772 - mse: 15.1772 - mae: 1.5853 - val_loss: 14.8639 - val_mse: 14.8639 - val_mae: 1.5910 - lr: 1.3816e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 62/100\n",
            "1000/1000 - 2s - loss: 15.1719 - mse: 15.1719 - mae: 1.5819 - val_loss: 14.8756 - val_mse: 14.8756 - val_mae: 1.5846 - lr: 1.3816e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 63/100\n",
            "1000/1000 - 2s - loss: 15.1737 - mse: 15.1737 - mae: 1.5845 - val_loss: 14.8590 - val_mse: 14.8590 - val_mae: 1.5905 - lr: 1.3816e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 64/100\n",
            "1000/1000 - 2s - loss: 15.1656 - mse: 15.1656 - mae: 1.5835 - val_loss: 14.8648 - val_mse: 14.8648 - val_mae: 1.5871 - lr: 1.3816e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 65/100\n",
            "1000/1000 - 2s - loss: 15.1671 - mse: 15.1671 - mae: 1.5828 - val_loss: 14.8705 - val_mse: 14.8705 - val_mae: 1.5794 - lr: 1.3816e-04 - 2s/epoch - 2ms/step\n",
            "Score for fold 1: loss of 14.870537757873535\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 12.5328 - mse: 12.5328 - mae: 1.5666 - val_loss: 25.4235 - val_mse: 25.4235 - val_mae: 1.6370 - lr: 1.3816e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 12.5253 - mse: 12.5253 - mae: 1.5673 - val_loss: 25.4319 - val_mse: 25.4319 - val_mae: 1.6360 - lr: 1.3816e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 12.5231 - mse: 12.5231 - mae: 1.5695 - val_loss: 25.4066 - val_mse: 25.4066 - val_mae: 1.6272 - lr: 1.3816e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 12.5157 - mse: 12.5157 - mae: 1.5664 - val_loss: 25.4130 - val_mse: 25.4130 - val_mae: 1.6371 - lr: 1.3816e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 12.5141 - mse: 12.5141 - mae: 1.5690 - val_loss: 25.4281 - val_mse: 25.4281 - val_mae: 1.6339 - lr: 1.3816e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 12.5172 - mse: 12.5172 - mae: 1.5685 - val_loss: 25.4492 - val_mse: 25.4492 - val_mae: 1.6319 - lr: 1.3816e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 12.5160 - mse: 12.5160 - mae: 1.5670 - val_loss: 25.4331 - val_mse: 25.4331 - val_mae: 1.6203 - lr: 1.3816e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 2s - loss: 12.5146 - mse: 12.5146 - mae: 1.5640 - val_loss: 25.4119 - val_mse: 25.4119 - val_mae: 1.6405 - lr: 1.3816e-04 - 2s/epoch - 2ms/step\n",
            "Score for fold 2: loss of 25.411903381347656\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 16.1677 - mse: 16.1677 - mae: 1.5918 - val_loss: 10.7206 - val_mse: 10.7206 - val_mae: 1.5594 - lr: 1.3816e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 16.1634 - mse: 16.1634 - mae: 1.5905 - val_loss: 10.7100 - val_mse: 10.7100 - val_mae: 1.5546 - lr: 1.3816e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 16.1525 - mse: 16.1525 - mae: 1.5907 - val_loss: 10.7120 - val_mse: 10.7120 - val_mae: 1.5577 - lr: 1.3816e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 16.1554 - mse: 16.1554 - mae: 1.5880 - val_loss: 10.7151 - val_mse: 10.7151 - val_mae: 1.5572 - lr: 1.3816e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 16.1463 - mse: 16.1463 - mae: 1.5904 - val_loss: 10.7308 - val_mse: 10.7308 - val_mae: 1.5462 - lr: 1.3816e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 16.1531 - mse: 16.1531 - mae: 1.5854 - val_loss: 10.7113 - val_mse: 10.7113 - val_mae: 1.5713 - lr: 1.3816e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 16.1423 - mse: 16.1423 - mae: 1.5914 - val_loss: 10.7225 - val_mse: 10.7225 - val_mae: 1.5553 - lr: 1.3816e-04 - 2s/epoch - 2ms/step\n",
            "Score for fold 3: loss of 10.722450256347656\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 15.1592 - mse: 15.1592 - mae: 1.5838 - val_loss: 14.6550 - val_mse: 14.6550 - val_mae: 1.5677 - lr: 1.3816e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 15.1592 - mse: 15.1592 - mae: 1.5818 - val_loss: 14.6693 - val_mse: 14.6693 - val_mae: 1.5729 - lr: 1.3816e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 15.1586 - mse: 15.1586 - mae: 1.5828 - val_loss: 14.6624 - val_mse: 14.6624 - val_mae: 1.5693 - lr: 1.3816e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 15.1557 - mse: 15.1557 - mae: 1.5814 - val_loss: 14.6524 - val_mse: 14.6524 - val_mae: 1.5794 - lr: 1.3816e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 15.1532 - mse: 15.1532 - mae: 1.5826 - val_loss: 14.6429 - val_mse: 14.6429 - val_mae: 1.5829 - lr: 1.3816e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 15.1443 - mse: 15.1443 - mae: 1.5858 - val_loss: 14.6591 - val_mse: 14.6591 - val_mae: 1.5706 - lr: 1.3816e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 15.1439 - mse: 15.1439 - mae: 1.5835 - val_loss: 14.6634 - val_mse: 14.6634 - val_mae: 1.5702 - lr: 1.3816e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 2s - loss: 15.1335 - mse: 15.1335 - mae: 1.5872 - val_loss: 14.6732 - val_mse: 14.6732 - val_mae: 1.5560 - lr: 1.3816e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 2s - loss: 15.1456 - mse: 15.1456 - mae: 1.5820 - val_loss: 14.6472 - val_mse: 14.6472 - val_mae: 1.5731 - lr: 1.3816e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 2s - loss: 15.1285 - mse: 15.1285 - mae: 1.5835 - val_loss: 14.6458 - val_mse: 14.6458 - val_mae: 1.5675 - lr: 1.3816e-04 - 2s/epoch - 2ms/step\n",
            "Score for fold 4: loss of 14.645833015441895\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 16.3686 - mse: 16.3686 - mae: 1.5795 - val_loss: 9.7318 - val_mse: 9.7318 - val_mae: 1.5614 - lr: 1.3816e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 16.3648 - mse: 16.3648 - mae: 1.5802 - val_loss: 9.7397 - val_mse: 9.7397 - val_mae: 1.5557 - lr: 1.3816e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 16.3672 - mse: 16.3672 - mae: 1.5806 - val_loss: 9.7571 - val_mse: 9.7571 - val_mae: 1.5533 - lr: 1.3816e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 16.3710 - mse: 16.3710 - mae: 1.5816 - val_loss: 9.7435 - val_mse: 9.7435 - val_mae: 1.5547 - lr: 1.3816e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 16.3576 - mse: 16.3576 - mae: 1.5843 - val_loss: 9.7389 - val_mse: 9.7389 - val_mae: 1.5614 - lr: 1.3816e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 16.3695 - mse: 16.3695 - mae: 1.5804 - val_loss: 9.7319 - val_mse: 9.7319 - val_mae: 1.5616 - lr: 1.3816e-04 - 2s/epoch - 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-10-14 11:14:42,777]\u001b[0m Finished trial#1 resulted in value: 15.076000000000002. Current best value is 14.751999999999999 with parameters: {'activation': 'relu', 'num_hidden_layer': 3, 'i': 7, 'learning_rate': 0.0018797852452371259}.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Score for fold 5: loss of 9.731948852539062\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 3s - loss: 16.6013 - mse: 16.6013 - mae: 1.6632 - val_loss: 12.7807 - val_mse: 12.7807 - val_mae: 1.6318 - lr: 0.0080 - 3s/epoch - 3ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 16.2661 - mse: 16.2661 - mae: 1.6331 - val_loss: 12.6939 - val_mse: 12.6939 - val_mae: 1.5776 - lr: 0.0080 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 16.1153 - mse: 16.1153 - mae: 1.6307 - val_loss: 12.8158 - val_mse: 12.8158 - val_mae: 1.5792 - lr: 0.0080 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 16.0705 - mse: 16.0705 - mae: 1.6348 - val_loss: 12.7401 - val_mse: 12.7401 - val_mae: 1.5930 - lr: 0.0080 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 16.0351 - mse: 16.0351 - mae: 1.6264 - val_loss: 12.5556 - val_mse: 12.5556 - val_mae: 1.6409 - lr: 0.0080 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 16.0739 - mse: 16.0739 - mae: 1.6275 - val_loss: 12.8071 - val_mse: 12.8071 - val_mae: 1.6566 - lr: 0.0080 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 16.0886 - mse: 16.0886 - mae: 1.6326 - val_loss: 12.6753 - val_mse: 12.6753 - val_mae: 1.6155 - lr: 0.0080 - 2s/epoch - 2ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 2s - loss: 16.0188 - mse: 16.0188 - mae: 1.6278 - val_loss: 12.8134 - val_mse: 12.8134 - val_mae: 1.7085 - lr: 0.0080 - 2s/epoch - 2ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 2s - loss: 16.0577 - mse: 16.0577 - mae: 1.6281 - val_loss: 12.7417 - val_mse: 12.7417 - val_mae: 1.8874 - lr: 0.0080 - 2s/epoch - 2ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 2s - loss: 15.9968 - mse: 15.9968 - mae: 1.6360 - val_loss: 12.5655 - val_mse: 12.5655 - val_mae: 1.5466 - lr: 0.0080 - 2s/epoch - 2ms/step\n",
            "Score for fold 1: loss of 12.56554126739502\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 13.7672 - mse: 13.7672 - mae: 1.5897 - val_loss: 20.3602 - val_mse: 20.3602 - val_mae: 1.6684 - lr: 0.0016 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 13.6653 - mse: 13.6653 - mae: 1.5899 - val_loss: 20.3728 - val_mse: 20.3728 - val_mae: 1.6289 - lr: 0.0016 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 13.6198 - mse: 13.6198 - mae: 1.5850 - val_loss: 20.5106 - val_mse: 20.5106 - val_mae: 1.5829 - lr: 0.0016 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 13.5760 - mse: 13.5760 - mae: 1.5832 - val_loss: 20.3419 - val_mse: 20.3419 - val_mae: 1.6843 - lr: 0.0016 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 13.6614 - mse: 13.6614 - mae: 1.5851 - val_loss: 20.2130 - val_mse: 20.2130 - val_mae: 1.6567 - lr: 0.0016 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 13.5688 - mse: 13.5688 - mae: 1.5766 - val_loss: 20.3127 - val_mse: 20.3127 - val_mae: 1.6563 - lr: 0.0016 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 13.5653 - mse: 13.5653 - mae: 1.5774 - val_loss: 20.2364 - val_mse: 20.2364 - val_mae: 1.6519 - lr: 0.0016 - 2s/epoch - 2ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 2s - loss: 13.5467 - mse: 13.5467 - mae: 1.5763 - val_loss: 20.2504 - val_mse: 20.2504 - val_mae: 1.6554 - lr: 0.0016 - 2s/epoch - 2ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 2s - loss: 13.6152 - mse: 13.6152 - mae: 1.5797 - val_loss: 20.3011 - val_mse: 20.3011 - val_mae: 1.6462 - lr: 0.0016 - 2s/epoch - 2ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 2s - loss: 13.5537 - mse: 13.5537 - mae: 1.5791 - val_loss: 20.3708 - val_mse: 20.3708 - val_mae: 1.5807 - lr: 0.0016 - 2s/epoch - 2ms/step\n",
            "Score for fold 2: loss of 20.370832443237305\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 13.3651 - mse: 13.3651 - mae: 1.5839 - val_loss: 20.6664 - val_mse: 20.6664 - val_mae: 1.5579 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 13.3208 - mse: 13.3208 - mae: 1.5816 - val_loss: 20.5220 - val_mse: 20.5220 - val_mae: 1.5773 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 13.2532 - mse: 13.2532 - mae: 1.5800 - val_loss: 20.7260 - val_mse: 20.7260 - val_mae: 1.5761 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 13.2983 - mse: 13.2983 - mae: 1.5746 - val_loss: 20.5861 - val_mse: 20.5861 - val_mae: 1.6023 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 13.2605 - mse: 13.2605 - mae: 1.5756 - val_loss: 20.7572 - val_mse: 20.7572 - val_mae: 1.5709 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 13.2691 - mse: 13.2691 - mae: 1.5796 - val_loss: 20.7655 - val_mse: 20.7655 - val_mae: 1.5925 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 13.2405 - mse: 13.2405 - mae: 1.5795 - val_loss: 20.6818 - val_mse: 20.6818 - val_mae: 1.5403 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Score for fold 3: loss of 20.68173599243164\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 15.8667 - mse: 15.8667 - mae: 1.5875 - val_loss: 10.2699 - val_mse: 10.2699 - val_mae: 1.5561 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 15.9192 - mse: 15.9192 - mae: 1.5874 - val_loss: 10.1765 - val_mse: 10.1765 - val_mae: 1.5469 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 15.8913 - mse: 15.8913 - mae: 1.5895 - val_loss: 10.1177 - val_mse: 10.1177 - val_mae: 1.5003 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 15.8696 - mse: 15.8696 - mae: 1.5915 - val_loss: 10.1774 - val_mse: 10.1774 - val_mae: 1.5503 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 15.8725 - mse: 15.8725 - mae: 1.5846 - val_loss: 10.1043 - val_mse: 10.1043 - val_mae: 1.5573 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 15.8134 - mse: 15.8134 - mae: 1.5837 - val_loss: 10.3101 - val_mse: 10.3101 - val_mae: 1.5289 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 15.8403 - mse: 15.8403 - mae: 1.5829 - val_loss: 10.0817 - val_mse: 10.0817 - val_mae: 1.5510 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 2s - loss: 15.7871 - mse: 15.7871 - mae: 1.5818 - val_loss: 10.1095 - val_mse: 10.1095 - val_mae: 1.5857 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 2s - loss: 15.7935 - mse: 15.7935 - mae: 1.5866 - val_loss: 10.3633 - val_mse: 10.3633 - val_mae: 1.5716 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 2s - loss: 15.7943 - mse: 15.7943 - mae: 1.5765 - val_loss: 10.2356 - val_mse: 10.2356 - val_mae: 1.5632 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 2s - loss: 15.7846 - mse: 15.7846 - mae: 1.5835 - val_loss: 10.2968 - val_mse: 10.2968 - val_mae: 1.5383 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 2s - loss: 15.7975 - mse: 15.7975 - mae: 1.5841 - val_loss: 10.2422 - val_mse: 10.2422 - val_mae: 1.5650 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Score for fold 4: loss of 10.242168426513672\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 15.6273 - mse: 15.6273 - mae: 1.5864 - val_loss: 11.0774 - val_mse: 11.0774 - val_mae: 1.5246 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 15.6483 - mse: 15.6483 - mae: 1.5840 - val_loss: 11.0585 - val_mse: 11.0585 - val_mae: 1.5804 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 15.5499 - mse: 15.5499 - mae: 1.5821 - val_loss: 11.0457 - val_mse: 11.0457 - val_mae: 1.5493 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 15.5866 - mse: 15.5866 - mae: 1.5821 - val_loss: 11.1454 - val_mse: 11.1454 - val_mae: 1.5684 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 15.5124 - mse: 15.5124 - mae: 1.5841 - val_loss: 11.1417 - val_mse: 11.1417 - val_mae: 1.5401 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 15.5481 - mse: 15.5481 - mae: 1.5820 - val_loss: 11.1094 - val_mse: 11.1094 - val_mae: 1.5263 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 15.5416 - mse: 15.5416 - mae: 1.5798 - val_loss: 11.1223 - val_mse: 11.1223 - val_mae: 1.6002 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 2s - loss: 15.4967 - mse: 15.4967 - mae: 1.5831 - val_loss: 11.1010 - val_mse: 11.1010 - val_mae: 1.5539 - lr: 0.0010 - 2s/epoch - 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-10-14 11:16:17,947]\u001b[0m Finished trial#2 resulted in value: 14.991999999999999. Current best value is 14.751999999999999 with parameters: {'activation': 'relu', 'num_hidden_layer': 3, 'i': 7, 'learning_rate': 0.0018797852452371259}.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Score for fold 5: loss of 11.100991249084473\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 3s - loss: 17.6357 - mse: 17.6357 - mae: 1.8629 - val_loss: 19.0649 - val_mse: 19.0649 - val_mae: 1.6146 - lr: 1.0160e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 14.8132 - mse: 14.8132 - mae: 1.6261 - val_loss: 19.0384 - val_mse: 19.0384 - val_mae: 1.6355 - lr: 1.0160e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 14.8067 - mse: 14.8067 - mae: 1.6286 - val_loss: 18.9972 - val_mse: 18.9972 - val_mae: 1.6486 - lr: 1.0160e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 14.7940 - mse: 14.7940 - mae: 1.6279 - val_loss: 18.9961 - val_mse: 18.9961 - val_mae: 1.6292 - lr: 1.0160e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 14.7879 - mse: 14.7879 - mae: 1.6273 - val_loss: 19.0749 - val_mse: 19.0749 - val_mae: 1.6286 - lr: 1.0160e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 14.7902 - mse: 14.7902 - mae: 1.6261 - val_loss: 18.9739 - val_mse: 18.9739 - val_mae: 1.6384 - lr: 1.0160e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 14.7802 - mse: 14.7802 - mae: 1.6277 - val_loss: 18.9442 - val_mse: 18.9442 - val_mae: 1.6294 - lr: 1.0160e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 2s - loss: 14.7790 - mse: 14.7790 - mae: 1.6277 - val_loss: 18.9716 - val_mse: 18.9716 - val_mae: 1.6288 - lr: 1.0160e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 2s - loss: 14.7817 - mse: 14.7817 - mae: 1.6236 - val_loss: 18.9504 - val_mse: 18.9504 - val_mae: 1.6369 - lr: 1.0160e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 2s - loss: 14.7789 - mse: 14.7789 - mae: 1.6259 - val_loss: 19.0023 - val_mse: 19.0023 - val_mae: 1.6189 - lr: 1.0160e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 2s - loss: 14.7790 - mse: 14.7790 - mae: 1.6246 - val_loss: 19.0162 - val_mse: 19.0162 - val_mae: 1.6306 - lr: 1.0160e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 2s - loss: 14.7822 - mse: 14.7822 - mae: 1.6252 - val_loss: 19.0127 - val_mse: 19.0127 - val_mae: 1.6372 - lr: 1.0160e-04 - 2s/epoch - 2ms/step\n",
            "Score for fold 1: loss of 19.012683868408203\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 14.7324 - mse: 14.7324 - mae: 1.6269 - val_loss: 19.1988 - val_mse: 19.1988 - val_mae: 1.6425 - lr: 1.0160e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 14.7279 - mse: 14.7279 - mae: 1.6259 - val_loss: 19.2121 - val_mse: 19.2121 - val_mae: 1.6225 - lr: 1.0160e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 14.7189 - mse: 14.7189 - mae: 1.6263 - val_loss: 19.2225 - val_mse: 19.2225 - val_mae: 1.6322 - lr: 1.0160e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 14.7378 - mse: 14.7378 - mae: 1.6251 - val_loss: 19.2155 - val_mse: 19.2155 - val_mae: 1.6317 - lr: 1.0160e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 14.7294 - mse: 14.7294 - mae: 1.6280 - val_loss: 19.2044 - val_mse: 19.2044 - val_mae: 1.6322 - lr: 1.0160e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 14.7171 - mse: 14.7171 - mae: 1.6280 - val_loss: 19.2219 - val_mse: 19.2219 - val_mae: 1.6213 - lr: 1.0160e-04 - 2s/epoch - 2ms/step\n",
            "Score for fold 2: loss of 19.221940994262695\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 16.3498 - mse: 16.3498 - mae: 1.6277 - val_loss: 12.7147 - val_mse: 12.7147 - val_mae: 1.6005 - lr: 1.0160e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 16.3640 - mse: 16.3640 - mae: 1.6266 - val_loss: 12.6804 - val_mse: 12.6804 - val_mae: 1.6155 - lr: 1.0160e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 16.3446 - mse: 16.3446 - mae: 1.6305 - val_loss: 12.6889 - val_mse: 12.6889 - val_mae: 1.6037 - lr: 1.0160e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 16.3504 - mse: 16.3504 - mae: 1.6284 - val_loss: 12.6888 - val_mse: 12.6888 - val_mae: 1.6124 - lr: 1.0160e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 16.3488 - mse: 16.3488 - mae: 1.6317 - val_loss: 12.6913 - val_mse: 12.6913 - val_mae: 1.6027 - lr: 1.0160e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 16.3501 - mse: 16.3501 - mae: 1.6285 - val_loss: 12.6952 - val_mse: 12.6952 - val_mae: 1.6096 - lr: 1.0160e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 16.3555 - mse: 16.3555 - mae: 1.6300 - val_loss: 12.6797 - val_mse: 12.6797 - val_mae: 1.6163 - lr: 1.0160e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 2s - loss: 16.3562 - mse: 16.3562 - mae: 1.6313 - val_loss: 12.6883 - val_mse: 12.6883 - val_mae: 1.6044 - lr: 1.0160e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 2s - loss: 16.3492 - mse: 16.3492 - mae: 1.6308 - val_loss: 12.6969 - val_mse: 12.6969 - val_mae: 1.5994 - lr: 1.0160e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 2s - loss: 16.3406 - mse: 16.3406 - mae: 1.6304 - val_loss: 12.6948 - val_mse: 12.6948 - val_mae: 1.5994 - lr: 1.0160e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 2s - loss: 16.3548 - mse: 16.3548 - mae: 1.6317 - val_loss: 12.7068 - val_mse: 12.7068 - val_mae: 1.5940 - lr: 1.0160e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 2s - loss: 16.3584 - mse: 16.3584 - mae: 1.6288 - val_loss: 12.6981 - val_mse: 12.6981 - val_mae: 1.5997 - lr: 1.0160e-04 - 2s/epoch - 2ms/step\n",
            "Score for fold 3: loss of 12.698084831237793\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 16.4974 - mse: 16.4974 - mae: 1.6228 - val_loss: 12.1854 - val_mse: 12.1854 - val_mae: 1.6159 - lr: 1.0160e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 16.4782 - mse: 16.4782 - mae: 1.6285 - val_loss: 12.1825 - val_mse: 12.1825 - val_mae: 1.6160 - lr: 1.0160e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 16.4880 - mse: 16.4880 - mae: 1.6274 - val_loss: 12.1738 - val_mse: 12.1738 - val_mae: 1.6233 - lr: 1.0160e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 16.4892 - mse: 16.4892 - mae: 1.6258 - val_loss: 12.1757 - val_mse: 12.1757 - val_mae: 1.6204 - lr: 1.0160e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 16.4807 - mse: 16.4807 - mae: 1.6281 - val_loss: 12.1851 - val_mse: 12.1851 - val_mae: 1.6126 - lr: 1.0160e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 16.4969 - mse: 16.4969 - mae: 1.6261 - val_loss: 12.1880 - val_mse: 12.1880 - val_mae: 1.6199 - lr: 1.0160e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 16.4753 - mse: 16.4753 - mae: 1.6262 - val_loss: 12.1957 - val_mse: 12.1957 - val_mae: 1.6093 - lr: 1.0160e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 2s - loss: 16.4927 - mse: 16.4927 - mae: 1.6228 - val_loss: 12.1866 - val_mse: 12.1866 - val_mae: 1.6171 - lr: 1.0160e-04 - 2s/epoch - 2ms/step\n",
            "Score for fold 4: loss of 12.186562538146973\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 15.7934 - mse: 15.7934 - mae: 1.6177 - val_loss: 15.0476 - val_mse: 15.0476 - val_mae: 1.6416 - lr: 1.0160e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 15.7891 - mse: 15.7891 - mae: 1.6215 - val_loss: 15.0269 - val_mse: 15.0269 - val_mae: 1.6498 - lr: 1.0160e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 15.7778 - mse: 15.7778 - mae: 1.6221 - val_loss: 15.0376 - val_mse: 15.0376 - val_mae: 1.6456 - lr: 1.0160e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 15.7909 - mse: 15.7909 - mae: 1.6247 - val_loss: 15.0552 - val_mse: 15.0552 - val_mae: 1.6344 - lr: 1.0160e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 15.7892 - mse: 15.7892 - mae: 1.6237 - val_loss: 15.0572 - val_mse: 15.0572 - val_mae: 1.6328 - lr: 1.0160e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 15.7831 - mse: 15.7831 - mae: 1.6214 - val_loss: 15.0573 - val_mse: 15.0573 - val_mae: 1.6237 - lr: 1.0160e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 15.7842 - mse: 15.7842 - mae: 1.6217 - val_loss: 15.0609 - val_mse: 15.0609 - val_mae: 1.6307 - lr: 1.0160e-04 - 2s/epoch - 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-10-14 11:17:46,916]\u001b[0m Finished trial#3 resulted in value: 15.636000000000001. Current best value is 14.751999999999999 with parameters: {'activation': 'relu', 'num_hidden_layer': 3, 'i': 7, 'learning_rate': 0.0018797852452371259}.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Score for fold 5: loss of 15.060919761657715\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 7s - loss: 15.3915 - mse: 15.3915 - mae: 1.6243 - val_loss: 15.5871 - val_mse: 15.5871 - val_mae: 1.5915 - lr: 0.0010 - 7s/epoch - 7ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 6s - loss: 15.0283 - mse: 15.0283 - mae: 1.6084 - val_loss: 15.6880 - val_mse: 15.6880 - val_mae: 1.5355 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 6s - loss: 15.0433 - mse: 15.0433 - mae: 1.5980 - val_loss: 15.6051 - val_mse: 15.6051 - val_mae: 1.6872 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 6s - loss: 14.9442 - mse: 14.9442 - mae: 1.5969 - val_loss: 15.4228 - val_mse: 15.4228 - val_mae: 1.5706 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 6s - loss: 15.0084 - mse: 15.0084 - mae: 1.5975 - val_loss: 15.4494 - val_mse: 15.4494 - val_mae: 1.6060 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 6s - loss: 14.9543 - mse: 14.9543 - mae: 1.5931 - val_loss: 15.4545 - val_mse: 15.4545 - val_mae: 1.7004 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 6s - loss: 14.8964 - mse: 14.8964 - mae: 1.5884 - val_loss: 15.2709 - val_mse: 15.2709 - val_mae: 1.5783 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 6s - loss: 14.9240 - mse: 14.9240 - mae: 1.5877 - val_loss: 15.3359 - val_mse: 15.3359 - val_mae: 1.6038 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 6s - loss: 14.8028 - mse: 14.8028 - mae: 1.5864 - val_loss: 15.2840 - val_mse: 15.2840 - val_mae: 1.7030 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 6s - loss: 14.7010 - mse: 14.7010 - mae: 1.5727 - val_loss: 15.6964 - val_mse: 15.6964 - val_mae: 1.5689 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 6s - loss: 14.7639 - mse: 14.7639 - mae: 1.5732 - val_loss: 15.1839 - val_mse: 15.1839 - val_mae: 1.6119 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 6s - loss: 14.6761 - mse: 14.6761 - mae: 1.5763 - val_loss: 15.1297 - val_mse: 15.1297 - val_mae: 1.5860 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
            "Epoch 13/100\n",
            "1000/1000 - 6s - loss: 14.7339 - mse: 14.7339 - mae: 1.5703 - val_loss: 15.4392 - val_mse: 15.4392 - val_mae: 1.5383 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
            "Epoch 14/100\n",
            "1000/1000 - 6s - loss: 14.6394 - mse: 14.6394 - mae: 1.5678 - val_loss: 15.2933 - val_mse: 15.2933 - val_mae: 1.5966 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
            "Epoch 15/100\n",
            "1000/1000 - 6s - loss: 14.5830 - mse: 14.5830 - mae: 1.5674 - val_loss: 15.4734 - val_mse: 15.4734 - val_mae: 1.6062 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
            "Epoch 16/100\n",
            "1000/1000 - 6s - loss: 14.5938 - mse: 14.5938 - mae: 1.5648 - val_loss: 15.2448 - val_mse: 15.2448 - val_mae: 1.6086 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
            "Epoch 17/100\n",
            "1000/1000 - 6s - loss: 14.5182 - mse: 14.5182 - mae: 1.5681 - val_loss: 15.2954 - val_mse: 15.2954 - val_mae: 1.5606 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
            "Score for fold 1: loss of 15.295413970947266\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 6s - loss: 15.3261 - mse: 15.3261 - mae: 1.5716 - val_loss: 12.0356 - val_mse: 12.0356 - val_mae: 1.5197 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 6s - loss: 15.1966 - mse: 15.1966 - mae: 1.5652 - val_loss: 12.4240 - val_mse: 12.4240 - val_mae: 1.5622 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 6s - loss: 15.2017 - mse: 15.2017 - mae: 1.5614 - val_loss: 12.5676 - val_mse: 12.5676 - val_mae: 1.5200 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 6s - loss: 15.0973 - mse: 15.0973 - mae: 1.5599 - val_loss: 12.4164 - val_mse: 12.4164 - val_mae: 1.6236 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 6s - loss: 15.1230 - mse: 15.1230 - mae: 1.5603 - val_loss: 12.5751 - val_mse: 12.5751 - val_mae: 1.5782 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 6s - loss: 14.9756 - mse: 14.9756 - mae: 1.5530 - val_loss: 12.4029 - val_mse: 12.4029 - val_mae: 1.5532 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
            "Score for fold 2: loss of 12.402953147888184\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 6s - loss: 13.6557 - mse: 13.6557 - mae: 1.5600 - val_loss: 17.9908 - val_mse: 17.9908 - val_mae: 1.5443 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 6s - loss: 13.5849 - mse: 13.5849 - mae: 1.5598 - val_loss: 17.9141 - val_mse: 17.9141 - val_mae: 1.5074 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 6s - loss: 13.6018 - mse: 13.6018 - mae: 1.5553 - val_loss: 17.9935 - val_mse: 17.9935 - val_mae: 1.5387 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 6s - loss: 13.5830 - mse: 13.5830 - mae: 1.5506 - val_loss: 17.9222 - val_mse: 17.9222 - val_mae: 1.5915 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 6s - loss: 13.5209 - mse: 13.5209 - mae: 1.5537 - val_loss: 18.1462 - val_mse: 18.1462 - val_mae: 1.5011 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 6s - loss: 13.4275 - mse: 13.4275 - mae: 1.5480 - val_loss: 18.0836 - val_mse: 18.0836 - val_mae: 1.5639 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 6s - loss: 13.5526 - mse: 13.5526 - mae: 1.5458 - val_loss: 18.2138 - val_mse: 18.2138 - val_mae: 1.5863 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
            "Score for fold 3: loss of 18.21381950378418\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 6s - loss: 14.7030 - mse: 14.7030 - mae: 1.5529 - val_loss: 13.3022 - val_mse: 13.3022 - val_mae: 1.5406 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 6s - loss: 14.5573 - mse: 14.5573 - mae: 1.5468 - val_loss: 13.3540 - val_mse: 13.3540 - val_mae: 1.5595 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 6s - loss: 14.4801 - mse: 14.4801 - mae: 1.5440 - val_loss: 13.5757 - val_mse: 13.5757 - val_mae: 1.5945 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 6s - loss: 14.4216 - mse: 14.4216 - mae: 1.5408 - val_loss: 13.5654 - val_mse: 13.5654 - val_mae: 1.5366 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 6s - loss: 14.4750 - mse: 14.4750 - mae: 1.5376 - val_loss: 13.5361 - val_mse: 13.5361 - val_mae: 1.6029 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 6s - loss: 14.4472 - mse: 14.4472 - mae: 1.5355 - val_loss: 13.7188 - val_mse: 13.7188 - val_mae: 1.6017 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
            "Score for fold 4: loss of 13.718836784362793\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 6s - loss: 14.4377 - mse: 14.4377 - mae: 1.5428 - val_loss: 13.8466 - val_mse: 13.8466 - val_mae: 1.5599 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 6s - loss: 14.2373 - mse: 14.2373 - mae: 1.5415 - val_loss: 13.5701 - val_mse: 13.5701 - val_mae: 1.5636 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 6s - loss: 14.2654 - mse: 14.2654 - mae: 1.5332 - val_loss: 13.6469 - val_mse: 13.6469 - val_mae: 1.5467 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 6s - loss: 14.1606 - mse: 14.1606 - mae: 1.5297 - val_loss: 13.5295 - val_mse: 13.5295 - val_mae: 1.5463 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 6s - loss: 14.1475 - mse: 14.1475 - mae: 1.5250 - val_loss: 14.0654 - val_mse: 14.0654 - val_mae: 1.5456 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 6s - loss: 14.0871 - mse: 14.0871 - mae: 1.5226 - val_loss: 13.8641 - val_mse: 13.8641 - val_mae: 1.5852 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 6s - loss: 13.8758 - mse: 13.8758 - mae: 1.5206 - val_loss: 13.9049 - val_mse: 13.9049 - val_mae: 1.6014 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 6s - loss: 13.9589 - mse: 13.9589 - mae: 1.5205 - val_loss: 13.9698 - val_mse: 13.9698 - val_mae: 1.6010 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 6s - loss: 13.8665 - mse: 13.8665 - mae: 1.5204 - val_loss: 14.0303 - val_mse: 14.0303 - val_mae: 1.5217 - lr: 0.0010 - 6s/epoch - 6ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-10-14 11:22:24,036]\u001b[0m Finished trial#4 resulted in value: 14.732. Current best value is 14.732 with parameters: {'activation': 'relu', 'num_hidden_layer': 3, 'i': 10, 'learning_rate': 0.001046125302603454}.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Score for fold 5: loss of 14.030271530151367\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 3s - loss: 16.2449 - mse: 16.2449 - mae: 1.6838 - val_loss: 14.2817 - val_mse: 14.2817 - val_mae: 1.5500 - lr: 0.0080 - 3s/epoch - 3ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 15.8802 - mse: 15.8802 - mae: 1.6485 - val_loss: 14.3561 - val_mse: 14.3561 - val_mae: 1.6561 - lr: 0.0080 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 15.7356 - mse: 15.7356 - mae: 1.6482 - val_loss: 13.9253 - val_mse: 13.9253 - val_mae: 1.5576 - lr: 0.0080 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 15.7123 - mse: 15.7123 - mae: 1.6390 - val_loss: 13.9183 - val_mse: 13.9183 - val_mae: 1.5118 - lr: 0.0080 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 15.7047 - mse: 15.7047 - mae: 1.6437 - val_loss: 14.0669 - val_mse: 14.0669 - val_mae: 1.5649 - lr: 0.0080 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 15.6255 - mse: 15.6255 - mae: 1.6382 - val_loss: 14.0482 - val_mse: 14.0482 - val_mae: 1.5536 - lr: 0.0080 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 15.7227 - mse: 15.7227 - mae: 1.6375 - val_loss: 14.1378 - val_mse: 14.1378 - val_mae: 1.7458 - lr: 0.0080 - 2s/epoch - 2ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 2s - loss: 15.6741 - mse: 15.6741 - mae: 1.6325 - val_loss: 14.0472 - val_mse: 14.0472 - val_mae: 1.6855 - lr: 0.0080 - 2s/epoch - 2ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 2s - loss: 15.6568 - mse: 15.6568 - mae: 1.6488 - val_loss: 14.0412 - val_mse: 14.0412 - val_mae: 1.6034 - lr: 0.0080 - 2s/epoch - 2ms/step\n",
            "Score for fold 1: loss of 14.041153907775879\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 15.0887 - mse: 15.0887 - mae: 1.5996 - val_loss: 15.2649 - val_mse: 15.2649 - val_mae: 1.5690 - lr: 0.0016 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 15.0023 - mse: 15.0023 - mae: 1.5921 - val_loss: 15.1897 - val_mse: 15.1897 - val_mae: 1.7119 - lr: 0.0016 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 14.9328 - mse: 14.9328 - mae: 1.5892 - val_loss: 15.2092 - val_mse: 15.2092 - val_mae: 1.6093 - lr: 0.0016 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 14.8985 - mse: 14.8985 - mae: 1.5892 - val_loss: 15.1956 - val_mse: 15.1956 - val_mae: 1.5485 - lr: 0.0016 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 14.8207 - mse: 14.8207 - mae: 1.5815 - val_loss: 15.1641 - val_mse: 15.1641 - val_mae: 1.7243 - lr: 0.0016 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 14.8138 - mse: 14.8138 - mae: 1.5795 - val_loss: 15.1518 - val_mse: 15.1518 - val_mae: 1.6092 - lr: 0.0016 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 14.8031 - mse: 14.8031 - mae: 1.5849 - val_loss: 15.2672 - val_mse: 15.2672 - val_mae: 1.5803 - lr: 0.0016 - 2s/epoch - 2ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 2s - loss: 14.8319 - mse: 14.8319 - mae: 1.5864 - val_loss: 15.1823 - val_mse: 15.1823 - val_mae: 1.6107 - lr: 0.0016 - 2s/epoch - 2ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 2s - loss: 14.8197 - mse: 14.8197 - mae: 1.5760 - val_loss: 15.1159 - val_mse: 15.1159 - val_mae: 1.6115 - lr: 0.0016 - 2s/epoch - 2ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 2s - loss: 14.7355 - mse: 14.7355 - mae: 1.5757 - val_loss: 15.1998 - val_mse: 15.1998 - val_mae: 1.5962 - lr: 0.0016 - 2s/epoch - 2ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 2s - loss: 14.7255 - mse: 14.7255 - mae: 1.5771 - val_loss: 15.1009 - val_mse: 15.1009 - val_mae: 1.5866 - lr: 0.0016 - 2s/epoch - 2ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 2s - loss: 14.7527 - mse: 14.7527 - mae: 1.5800 - val_loss: 15.1908 - val_mse: 15.1908 - val_mae: 1.5927 - lr: 0.0016 - 2s/epoch - 2ms/step\n",
            "Epoch 13/100\n",
            "1000/1000 - 2s - loss: 14.7084 - mse: 14.7084 - mae: 1.5727 - val_loss: 15.2326 - val_mse: 15.2326 - val_mae: 1.5982 - lr: 0.0016 - 2s/epoch - 2ms/step\n",
            "Epoch 14/100\n",
            "1000/1000 - 2s - loss: 14.6714 - mse: 14.6714 - mae: 1.5807 - val_loss: 15.3397 - val_mse: 15.3397 - val_mae: 1.5561 - lr: 0.0016 - 2s/epoch - 2ms/step\n",
            "Epoch 15/100\n",
            "1000/1000 - 2s - loss: 14.6563 - mse: 14.6563 - mae: 1.5752 - val_loss: 15.2782 - val_mse: 15.2782 - val_mae: 1.5918 - lr: 0.0016 - 2s/epoch - 2ms/step\n",
            "Epoch 16/100\n",
            "1000/1000 - 2s - loss: 14.5065 - mse: 14.5065 - mae: 1.5743 - val_loss: 15.3267 - val_mse: 15.3267 - val_mae: 1.6065 - lr: 0.0016 - 2s/epoch - 2ms/step\n",
            "Score for fold 2: loss of 15.326728820800781\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 16.2094 - mse: 16.2094 - mae: 1.5884 - val_loss: 8.8971 - val_mse: 8.8971 - val_mae: 1.5253 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 16.1127 - mse: 16.1127 - mae: 1.5815 - val_loss: 8.9858 - val_mse: 8.9858 - val_mae: 1.6176 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 16.1752 - mse: 16.1752 - mae: 1.5870 - val_loss: 8.9317 - val_mse: 8.9317 - val_mae: 1.5585 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 16.1826 - mse: 16.1826 - mae: 1.5832 - val_loss: 9.0830 - val_mse: 9.0830 - val_mae: 1.5380 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 16.1375 - mse: 16.1375 - mae: 1.5873 - val_loss: 9.0622 - val_mse: 9.0622 - val_mae: 1.5584 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 16.1335 - mse: 16.1335 - mae: 1.5797 - val_loss: 9.0757 - val_mse: 9.0757 - val_mae: 1.5168 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Score for fold 3: loss of 9.075740814208984\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 12.7721 - mse: 12.7721 - mae: 1.5739 - val_loss: 22.7484 - val_mse: 22.7484 - val_mae: 1.5422 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 12.7127 - mse: 12.7127 - mae: 1.5659 - val_loss: 22.7687 - val_mse: 22.7687 - val_mae: 1.5507 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 12.6826 - mse: 12.6826 - mae: 1.5646 - val_loss: 22.7709 - val_mse: 22.7709 - val_mae: 1.6383 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 12.6891 - mse: 12.6891 - mae: 1.5650 - val_loss: 22.7421 - val_mse: 22.7421 - val_mae: 1.6292 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 12.6247 - mse: 12.6247 - mae: 1.5649 - val_loss: 22.8217 - val_mse: 22.8217 - val_mae: 1.6027 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 12.6203 - mse: 12.6203 - mae: 1.5614 - val_loss: 22.9629 - val_mse: 22.9629 - val_mae: 1.6076 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 12.6447 - mse: 12.6447 - mae: 1.5635 - val_loss: 22.7828 - val_mse: 22.7828 - val_mae: 1.5884 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 2s - loss: 12.5923 - mse: 12.5923 - mae: 1.5668 - val_loss: 22.9042 - val_mse: 22.9042 - val_mae: 1.6106 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 2s - loss: 12.6130 - mse: 12.6130 - mae: 1.5617 - val_loss: 23.0057 - val_mse: 23.0057 - val_mae: 1.5927 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Score for fold 4: loss of 23.005741119384766\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 14.9997 - mse: 14.9997 - mae: 1.5799 - val_loss: 13.3180 - val_mse: 13.3180 - val_mae: 1.6950 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 14.9831 - mse: 14.9831 - mae: 1.5736 - val_loss: 13.3852 - val_mse: 13.3852 - val_mae: 1.5816 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 14.8878 - mse: 14.8878 - mae: 1.5690 - val_loss: 13.7907 - val_mse: 13.7907 - val_mae: 1.5762 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 14.9415 - mse: 14.9415 - mae: 1.5712 - val_loss: 13.4357 - val_mse: 13.4357 - val_mae: 1.5535 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 14.9231 - mse: 14.9231 - mae: 1.5669 - val_loss: 13.5494 - val_mse: 13.5494 - val_mae: 1.6503 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 14.9108 - mse: 14.9108 - mae: 1.5677 - val_loss: 13.6323 - val_mse: 13.6323 - val_mae: 1.6306 - lr: 0.0010 - 2s/epoch - 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-10-14 11:23:56,836]\u001b[0m Finished trial#5 resulted in value: 15.017999999999997. Current best value is 14.732 with parameters: {'activation': 'relu', 'num_hidden_layer': 3, 'i': 10, 'learning_rate': 0.001046125302603454}.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Score for fold 5: loss of 13.632319450378418\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 11s - loss: 16.3497 - mse: 16.3497 - mae: 1.6788 - val_loss: 13.2077 - val_mse: 13.2077 - val_mae: 1.6569 - lr: 0.0026 - 11s/epoch - 11ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 10s - loss: 15.8477 - mse: 15.8477 - mae: 1.6303 - val_loss: 13.0922 - val_mse: 13.0922 - val_mae: 1.6284 - lr: 0.0026 - 10s/epoch - 10ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 10s - loss: 15.8829 - mse: 15.8829 - mae: 1.6280 - val_loss: 13.3438 - val_mse: 13.3438 - val_mae: 1.5620 - lr: 0.0026 - 10s/epoch - 10ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 10s - loss: 15.8974 - mse: 15.8974 - mae: 1.6377 - val_loss: 13.4008 - val_mse: 13.4008 - val_mae: 1.7663 - lr: 0.0026 - 10s/epoch - 10ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 10s - loss: 16.0491 - mse: 16.0491 - mae: 1.6446 - val_loss: 13.7749 - val_mse: 13.7749 - val_mae: 1.6216 - lr: 0.0026 - 10s/epoch - 10ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 10s - loss: 15.7478 - mse: 15.7478 - mae: 1.6181 - val_loss: 13.1083 - val_mse: 13.1083 - val_mae: 1.5442 - lr: 0.0026 - 10s/epoch - 10ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 10s - loss: 15.7544 - mse: 15.7544 - mae: 1.6015 - val_loss: 13.5546 - val_mse: 13.5546 - val_mae: 1.5893 - lr: 0.0026 - 10s/epoch - 10ms/step\n",
            "Score for fold 1: loss of 13.55457592010498\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 10s - loss: 15.6181 - mse: 15.6181 - mae: 1.5894 - val_loss: 12.5925 - val_mse: 12.5925 - val_mae: 1.5832 - lr: 0.0010 - 10s/epoch - 10ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 10s - loss: 15.5830 - mse: 15.5830 - mae: 1.5893 - val_loss: 12.3340 - val_mse: 12.3340 - val_mae: 1.5724 - lr: 0.0010 - 10s/epoch - 10ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 10s - loss: 15.5571 - mse: 15.5571 - mae: 1.5908 - val_loss: 12.5369 - val_mse: 12.5369 - val_mae: 1.5903 - lr: 0.0010 - 10s/epoch - 10ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 10s - loss: 15.5209 - mse: 15.5209 - mae: 1.5935 - val_loss: 12.4236 - val_mse: 12.4236 - val_mae: 1.6726 - lr: 0.0010 - 10s/epoch - 10ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 10s - loss: 15.5959 - mse: 15.5959 - mae: 1.5936 - val_loss: 12.4332 - val_mse: 12.4332 - val_mae: 1.5245 - lr: 0.0010 - 10s/epoch - 10ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 10s - loss: 15.4374 - mse: 15.4374 - mae: 1.5985 - val_loss: 12.2794 - val_mse: 12.2794 - val_mae: 1.6002 - lr: 0.0010 - 10s/epoch - 10ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 10s - loss: 15.4953 - mse: 15.4953 - mae: 1.6009 - val_loss: 12.7215 - val_mse: 12.7215 - val_mae: 1.5544 - lr: 0.0010 - 10s/epoch - 10ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 10s - loss: 15.5123 - mse: 15.5123 - mae: 1.6003 - val_loss: 12.9412 - val_mse: 12.9412 - val_mae: 1.5558 - lr: 0.0010 - 10s/epoch - 10ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 10s - loss: 15.5322 - mse: 15.5322 - mae: 1.6008 - val_loss: 12.5781 - val_mse: 12.5781 - val_mae: 1.7123 - lr: 0.0010 - 10s/epoch - 10ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 10s - loss: 15.5270 - mse: 15.5270 - mae: 1.5970 - val_loss: 12.3648 - val_mse: 12.3648 - val_mae: 1.6808 - lr: 0.0010 - 10s/epoch - 10ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 10s - loss: 15.5356 - mse: 15.5356 - mae: 1.6088 - val_loss: 12.8070 - val_mse: 12.8070 - val_mae: 1.4957 - lr: 0.0010 - 10s/epoch - 10ms/step\n",
            "Score for fold 2: loss of 12.807037353515625\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 10s - loss: 15.7750 - mse: 15.7750 - mae: 1.6095 - val_loss: 12.2099 - val_mse: 12.2099 - val_mae: 1.5176 - lr: 0.0010 - 10s/epoch - 10ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 10s - loss: 15.6542 - mse: 15.6542 - mae: 1.6044 - val_loss: 11.8902 - val_mse: 11.8902 - val_mae: 1.5214 - lr: 0.0010 - 10s/epoch - 10ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 10s - loss: 15.6845 - mse: 15.6845 - mae: 1.5975 - val_loss: 12.1516 - val_mse: 12.1516 - val_mae: 1.7641 - lr: 0.0010 - 10s/epoch - 10ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 10s - loss: 15.6242 - mse: 15.6242 - mae: 1.5995 - val_loss: 12.0564 - val_mse: 12.0564 - val_mae: 1.5909 - lr: 0.0010 - 10s/epoch - 10ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 10s - loss: 15.7425 - mse: 15.7425 - mae: 1.6046 - val_loss: 12.1995 - val_mse: 12.1995 - val_mae: 1.5218 - lr: 0.0010 - 10s/epoch - 10ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 10s - loss: 15.6618 - mse: 15.6618 - mae: 1.6008 - val_loss: 12.1637 - val_mse: 12.1637 - val_mae: 1.6569 - lr: 0.0010 - 10s/epoch - 10ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 10s - loss: 15.5537 - mse: 15.5537 - mae: 1.5995 - val_loss: 11.9880 - val_mse: 11.9880 - val_mae: 1.5669 - lr: 0.0010 - 10s/epoch - 10ms/step\n",
            "Score for fold 3: loss of 11.9879732131958\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 10s - loss: 13.6700 - mse: 13.6700 - mae: 1.5697 - val_loss: 19.5057 - val_mse: 19.5057 - val_mae: 1.6547 - lr: 0.0010 - 10s/epoch - 10ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 10s - loss: 13.7382 - mse: 13.7382 - mae: 1.5710 - val_loss: 19.3115 - val_mse: 19.3115 - val_mae: 1.6411 - lr: 0.0010 - 10s/epoch - 10ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 10s - loss: 13.5875 - mse: 13.5875 - mae: 1.5724 - val_loss: 19.2744 - val_mse: 19.2744 - val_mae: 1.6895 - lr: 0.0010 - 10s/epoch - 10ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 10s - loss: 13.6419 - mse: 13.6419 - mae: 1.5724 - val_loss: 19.6489 - val_mse: 19.6489 - val_mae: 1.5982 - lr: 0.0010 - 10s/epoch - 10ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 10s - loss: 13.6382 - mse: 13.6382 - mae: 1.5686 - val_loss: 20.0520 - val_mse: 20.0520 - val_mae: 1.5898 - lr: 0.0010 - 10s/epoch - 10ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 10s - loss: 13.6862 - mse: 13.6862 - mae: 1.5697 - val_loss: 19.4149 - val_mse: 19.4149 - val_mae: 1.6451 - lr: 0.0010 - 10s/epoch - 10ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 10s - loss: 13.6348 - mse: 13.6348 - mae: 1.5686 - val_loss: 19.6752 - val_mse: 19.6752 - val_mae: 1.6644 - lr: 0.0010 - 10s/epoch - 10ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 10s - loss: 13.5361 - mse: 13.5361 - mae: 1.5644 - val_loss: 19.5164 - val_mse: 19.5164 - val_mae: 1.6135 - lr: 0.0010 - 10s/epoch - 10ms/step\n",
            "Score for fold 4: loss of 19.516407012939453\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 10s - loss: 14.0815 - mse: 14.0815 - mae: 1.5997 - val_loss: 17.4339 - val_mse: 17.4339 - val_mae: 1.5648 - lr: 0.0010 - 10s/epoch - 10ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 10s - loss: 14.2483 - mse: 14.2483 - mae: 1.6006 - val_loss: 17.5453 - val_mse: 17.5453 - val_mae: 1.4699 - lr: 0.0010 - 10s/epoch - 10ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 10s - loss: 14.1324 - mse: 14.1324 - mae: 1.5966 - val_loss: 17.6996 - val_mse: 17.6996 - val_mae: 1.4467 - lr: 0.0010 - 10s/epoch - 10ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 10s - loss: 14.1424 - mse: 14.1424 - mae: 1.5995 - val_loss: 17.7250 - val_mse: 17.7250 - val_mae: 1.6294 - lr: 0.0010 - 10s/epoch - 10ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 10s - loss: 14.1329 - mse: 14.1329 - mae: 1.5976 - val_loss: 17.6173 - val_mse: 17.6173 - val_mae: 1.6600 - lr: 0.0010 - 10s/epoch - 10ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 10s - loss: 14.1755 - mse: 14.1755 - mae: 1.6006 - val_loss: 17.5886 - val_mse: 17.5886 - val_mae: 1.5187 - lr: 0.0010 - 10s/epoch - 10ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-10-14 11:30:33,491]\u001b[0m Finished trial#6 resulted in value: 15.092000000000002. Current best value is 14.732 with parameters: {'activation': 'relu', 'num_hidden_layer': 3, 'i': 10, 'learning_rate': 0.001046125302603454}.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Score for fold 5: loss of 17.588594436645508\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 10s - loss: 16.6688 - mse: 16.6688 - mae: 1.6986 - val_loss: 14.0703 - val_mse: 14.0703 - val_mae: 1.6609 - lr: 1.4430e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 9s - loss: 16.1893 - mse: 16.1893 - mae: 1.6517 - val_loss: 14.1591 - val_mse: 14.1591 - val_mae: 1.5815 - lr: 1.4430e-04 - 9s/epoch - 9ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 9s - loss: 16.1285 - mse: 16.1285 - mae: 1.6461 - val_loss: 14.0795 - val_mse: 14.0795 - val_mae: 1.6322 - lr: 1.4430e-04 - 9s/epoch - 9ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 9s - loss: 16.1125 - mse: 16.1125 - mae: 1.6411 - val_loss: 14.0428 - val_mse: 14.0428 - val_mae: 1.6520 - lr: 1.4430e-04 - 9s/epoch - 9ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 9s - loss: 16.1273 - mse: 16.1273 - mae: 1.6409 - val_loss: 14.2115 - val_mse: 14.2115 - val_mae: 1.5894 - lr: 1.4430e-04 - 9s/epoch - 9ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 9s - loss: 16.0647 - mse: 16.0647 - mae: 1.6428 - val_loss: 14.1846 - val_mse: 14.1846 - val_mae: 1.5866 - lr: 1.4430e-04 - 9s/epoch - 9ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 9s - loss: 16.0878 - mse: 16.0878 - mae: 1.6404 - val_loss: 14.0733 - val_mse: 14.0733 - val_mae: 1.5838 - lr: 1.4430e-04 - 9s/epoch - 9ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 9s - loss: 16.0746 - mse: 16.0746 - mae: 1.6389 - val_loss: 14.0384 - val_mse: 14.0384 - val_mae: 1.5814 - lr: 1.4430e-04 - 9s/epoch - 9ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 9s - loss: 16.0888 - mse: 16.0888 - mae: 1.6406 - val_loss: 13.9920 - val_mse: 13.9920 - val_mae: 1.6491 - lr: 1.4430e-04 - 9s/epoch - 9ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 9s - loss: 16.0809 - mse: 16.0809 - mae: 1.6427 - val_loss: 14.1156 - val_mse: 14.1156 - val_mae: 1.5985 - lr: 1.4430e-04 - 9s/epoch - 9ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 9s - loss: 16.0543 - mse: 16.0543 - mae: 1.6410 - val_loss: 14.0464 - val_mse: 14.0464 - val_mae: 1.6043 - lr: 1.4430e-04 - 9s/epoch - 9ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 10s - loss: 16.0348 - mse: 16.0348 - mae: 1.6417 - val_loss: 14.0035 - val_mse: 14.0035 - val_mae: 1.6256 - lr: 1.4430e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 13/100\n",
            "1000/1000 - 10s - loss: 16.0727 - mse: 16.0727 - mae: 1.6445 - val_loss: 14.0267 - val_mse: 14.0267 - val_mae: 1.7142 - lr: 1.4430e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 14/100\n",
            "1000/1000 - 9s - loss: 16.1124 - mse: 16.1124 - mae: 1.6432 - val_loss: 14.1457 - val_mse: 14.1457 - val_mae: 1.6278 - lr: 1.4430e-04 - 9s/epoch - 9ms/step\n",
            "Score for fold 1: loss of 14.145722389221191\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 9s - loss: 14.8000 - mse: 14.8000 - mae: 1.6377 - val_loss: 19.4450 - val_mse: 19.4450 - val_mae: 1.8133 - lr: 1.4430e-04 - 9s/epoch - 9ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 9s - loss: 14.8059 - mse: 14.8059 - mae: 1.6389 - val_loss: 19.0809 - val_mse: 19.0809 - val_mae: 1.6033 - lr: 1.4430e-04 - 9s/epoch - 9ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 10s - loss: 14.8366 - mse: 14.8366 - mae: 1.6355 - val_loss: 19.0359 - val_mse: 19.0359 - val_mae: 1.6457 - lr: 1.4430e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 10s - loss: 14.7922 - mse: 14.7922 - mae: 1.6404 - val_loss: 19.0333 - val_mse: 19.0333 - val_mae: 1.6229 - lr: 1.4430e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 10s - loss: 14.8076 - mse: 14.8076 - mae: 1.6404 - val_loss: 19.0989 - val_mse: 19.0989 - val_mae: 1.5911 - lr: 1.4430e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 9s - loss: 14.8182 - mse: 14.8182 - mae: 1.6370 - val_loss: 19.0280 - val_mse: 19.0280 - val_mae: 1.6060 - lr: 1.4430e-04 - 9s/epoch - 9ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 9s - loss: 14.8138 - mse: 14.8138 - mae: 1.6361 - val_loss: 19.0155 - val_mse: 19.0155 - val_mae: 1.6857 - lr: 1.4430e-04 - 9s/epoch - 9ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 10s - loss: 14.8218 - mse: 14.8218 - mae: 1.6398 - val_loss: 19.0088 - val_mse: 19.0088 - val_mae: 1.5985 - lr: 1.4430e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 10s - loss: 14.7991 - mse: 14.7991 - mae: 1.6380 - val_loss: 19.0591 - val_mse: 19.0591 - val_mae: 1.5812 - lr: 1.4430e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 10s - loss: 14.7938 - mse: 14.7938 - mae: 1.6368 - val_loss: 19.0260 - val_mse: 19.0260 - val_mae: 1.6190 - lr: 1.4430e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 10s - loss: 14.7796 - mse: 14.7796 - mae: 1.6378 - val_loss: 19.0343 - val_mse: 19.0343 - val_mae: 1.6479 - lr: 1.4430e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 10s - loss: 14.7742 - mse: 14.7742 - mae: 1.6378 - val_loss: 18.9968 - val_mse: 18.9968 - val_mae: 1.6427 - lr: 1.4430e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 13/100\n",
            "1000/1000 - 10s - loss: 14.8174 - mse: 14.8174 - mae: 1.6347 - val_loss: 19.0115 - val_mse: 19.0115 - val_mae: 1.6108 - lr: 1.4430e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 14/100\n",
            "1000/1000 - 9s - loss: 14.7821 - mse: 14.7821 - mae: 1.6347 - val_loss: 19.0309 - val_mse: 19.0309 - val_mae: 1.6356 - lr: 1.4430e-04 - 9s/epoch - 9ms/step\n",
            "Epoch 15/100\n",
            "1000/1000 - 10s - loss: 14.8001 - mse: 14.8001 - mae: 1.6403 - val_loss: 19.0029 - val_mse: 19.0029 - val_mae: 1.6697 - lr: 1.4430e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 16/100\n",
            "1000/1000 - 10s - loss: 14.7924 - mse: 14.7924 - mae: 1.6405 - val_loss: 19.0566 - val_mse: 19.0566 - val_mae: 1.5995 - lr: 1.4430e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 17/100\n",
            "1000/1000 - 10s - loss: 14.7992 - mse: 14.7992 - mae: 1.6335 - val_loss: 19.0349 - val_mse: 19.0349 - val_mae: 1.6100 - lr: 1.4430e-04 - 10s/epoch - 10ms/step\n",
            "Score for fold 2: loss of 19.034931182861328\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 9s - loss: 16.9182 - mse: 16.9182 - mae: 1.6399 - val_loss: 10.7109 - val_mse: 10.7109 - val_mae: 1.6216 - lr: 1.4430e-04 - 9s/epoch - 9ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 9s - loss: 16.9057 - mse: 16.9057 - mae: 1.6408 - val_loss: 10.6734 - val_mse: 10.6734 - val_mae: 1.6144 - lr: 1.4430e-04 - 9s/epoch - 9ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 10s - loss: 16.8674 - mse: 16.8674 - mae: 1.6421 - val_loss: 10.6872 - val_mse: 10.6872 - val_mae: 1.6121 - lr: 1.4430e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 9s - loss: 16.8796 - mse: 16.8796 - mae: 1.6430 - val_loss: 10.7122 - val_mse: 10.7122 - val_mae: 1.5872 - lr: 1.4430e-04 - 9s/epoch - 9ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 10s - loss: 16.8582 - mse: 16.8582 - mae: 1.6466 - val_loss: 10.6768 - val_mse: 10.6768 - val_mae: 1.6265 - lr: 1.4430e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 10s - loss: 16.9149 - mse: 16.9149 - mae: 1.6432 - val_loss: 10.6804 - val_mse: 10.6804 - val_mae: 1.6395 - lr: 1.4430e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 10s - loss: 16.8685 - mse: 16.8685 - mae: 1.6458 - val_loss: 10.7144 - val_mse: 10.7144 - val_mae: 1.5797 - lr: 1.4430e-04 - 10s/epoch - 10ms/step\n",
            "Score for fold 3: loss of 10.714345932006836\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 9s - loss: 14.8300 - mse: 14.8300 - mae: 1.6180 - val_loss: 18.7971 - val_mse: 18.7971 - val_mae: 1.6862 - lr: 1.4430e-04 - 9s/epoch - 9ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 9s - loss: 14.8663 - mse: 14.8663 - mae: 1.6227 - val_loss: 19.0155 - val_mse: 19.0155 - val_mae: 1.6067 - lr: 1.4430e-04 - 9s/epoch - 9ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 9s - loss: 14.8563 - mse: 14.8563 - mae: 1.6224 - val_loss: 18.7836 - val_mse: 18.7836 - val_mae: 1.7658 - lr: 1.4430e-04 - 9s/epoch - 9ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 9s - loss: 14.8537 - mse: 14.8537 - mae: 1.6264 - val_loss: 18.7672 - val_mse: 18.7672 - val_mae: 1.7092 - lr: 1.4430e-04 - 9s/epoch - 9ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 9s - loss: 14.8515 - mse: 14.8515 - mae: 1.6290 - val_loss: 18.7805 - val_mse: 18.7805 - val_mae: 1.6966 - lr: 1.4430e-04 - 9s/epoch - 9ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 10s - loss: 14.8776 - mse: 14.8776 - mae: 1.6191 - val_loss: 18.7471 - val_mse: 18.7471 - val_mae: 1.7248 - lr: 1.4430e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 9s - loss: 14.8656 - mse: 14.8656 - mae: 1.6257 - val_loss: 19.0248 - val_mse: 19.0248 - val_mae: 1.6042 - lr: 1.4430e-04 - 9s/epoch - 9ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 9s - loss: 14.8096 - mse: 14.8096 - mae: 1.6323 - val_loss: 18.8330 - val_mse: 18.8330 - val_mae: 1.6339 - lr: 1.4430e-04 - 9s/epoch - 9ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 10s - loss: 14.8541 - mse: 14.8541 - mae: 1.6267 - val_loss: 18.9117 - val_mse: 18.9117 - val_mae: 1.6135 - lr: 1.4430e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 9s - loss: 14.8360 - mse: 14.8360 - mae: 1.6205 - val_loss: 18.7857 - val_mse: 18.7857 - val_mae: 1.6992 - lr: 1.4430e-04 - 9s/epoch - 9ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 10s - loss: 14.8525 - mse: 14.8525 - mae: 1.6252 - val_loss: 18.8682 - val_mse: 18.8682 - val_mae: 1.6396 - lr: 1.4430e-04 - 10s/epoch - 10ms/step\n",
            "Score for fold 4: loss of 18.86821746826172\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 10s - loss: 15.6913 - mse: 15.6913 - mae: 1.6313 - val_loss: 15.5045 - val_mse: 15.5045 - val_mae: 1.6889 - lr: 1.4430e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 9s - loss: 15.7128 - mse: 15.7128 - mae: 1.6328 - val_loss: 15.5234 - val_mse: 15.5234 - val_mae: 1.6616 - lr: 1.4430e-04 - 9s/epoch - 9ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 9s - loss: 15.7075 - mse: 15.7075 - mae: 1.6397 - val_loss: 15.8509 - val_mse: 15.8509 - val_mae: 1.5477 - lr: 1.4430e-04 - 9s/epoch - 9ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 9s - loss: 15.7101 - mse: 15.7101 - mae: 1.6396 - val_loss: 15.5383 - val_mse: 15.5383 - val_mae: 1.6900 - lr: 1.4430e-04 - 9s/epoch - 9ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 10s - loss: 15.7026 - mse: 15.7026 - mae: 1.6391 - val_loss: 15.5297 - val_mse: 15.5297 - val_mae: 1.7691 - lr: 1.4430e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 10s - loss: 15.7242 - mse: 15.7242 - mae: 1.6401 - val_loss: 15.5095 - val_mse: 15.5095 - val_mae: 1.6867 - lr: 1.4430e-04 - 10s/epoch - 10ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-10-14 11:39:24,431]\u001b[0m Finished trial#7 resulted in value: 15.654000000000002. Current best value is 14.732 with parameters: {'activation': 'relu', 'num_hidden_layer': 3, 'i': 10, 'learning_rate': 0.001046125302603454}.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Score for fold 5: loss of 15.509513854980469\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 7s - loss: 16.7846 - mse: 16.7846 - mae: 1.6928 - val_loss: 11.9680 - val_mse: 11.9680 - val_mae: 1.5801 - lr: 2.8852e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 6s - loss: 16.3124 - mse: 16.3124 - mae: 1.6367 - val_loss: 11.7185 - val_mse: 11.7185 - val_mae: 1.6213 - lr: 2.8852e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 6s - loss: 16.2529 - mse: 16.2529 - mae: 1.6275 - val_loss: 11.8038 - val_mse: 11.8038 - val_mae: 1.7209 - lr: 2.8852e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 6s - loss: 16.2537 - mse: 16.2537 - mae: 1.6223 - val_loss: 11.7599 - val_mse: 11.7599 - val_mae: 1.5973 - lr: 2.8852e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 6s - loss: 16.2087 - mse: 16.2087 - mae: 1.6150 - val_loss: 11.7137 - val_mse: 11.7137 - val_mae: 1.5901 - lr: 2.8852e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 6s - loss: 16.2026 - mse: 16.2026 - mae: 1.6138 - val_loss: 11.8497 - val_mse: 11.8497 - val_mae: 1.6102 - lr: 2.8852e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 6s - loss: 16.2082 - mse: 16.2082 - mae: 1.6102 - val_loss: 11.8114 - val_mse: 11.8114 - val_mae: 1.5324 - lr: 2.8852e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 6s - loss: 16.1606 - mse: 16.1606 - mae: 1.6096 - val_loss: 11.7271 - val_mse: 11.7271 - val_mae: 1.6775 - lr: 2.8852e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 6s - loss: 16.1680 - mse: 16.1680 - mae: 1.6060 - val_loss: 11.7758 - val_mse: 11.7758 - val_mae: 1.5769 - lr: 2.8852e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 6s - loss: 16.1559 - mse: 16.1559 - mae: 1.6089 - val_loss: 11.7318 - val_mse: 11.7318 - val_mae: 1.5275 - lr: 2.8852e-04 - 6s/epoch - 6ms/step\n",
            "Score for fold 1: loss of 11.731837272644043\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 6s - loss: 15.8496 - mse: 15.8496 - mae: 1.6074 - val_loss: 12.7749 - val_mse: 12.7749 - val_mae: 1.5983 - lr: 2.8852e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 6s - loss: 15.8346 - mse: 15.8346 - mae: 1.6068 - val_loss: 12.8815 - val_mse: 12.8815 - val_mae: 1.5641 - lr: 2.8852e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 6s - loss: 15.8639 - mse: 15.8639 - mae: 1.6034 - val_loss: 12.7465 - val_mse: 12.7465 - val_mae: 1.5613 - lr: 2.8852e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 6s - loss: 15.7946 - mse: 15.7946 - mae: 1.5966 - val_loss: 12.7878 - val_mse: 12.7878 - val_mae: 1.6227 - lr: 2.8852e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 6s - loss: 15.8075 - mse: 15.8075 - mae: 1.6003 - val_loss: 12.6974 - val_mse: 12.6974 - val_mae: 1.5750 - lr: 2.8852e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 7s - loss: 15.7809 - mse: 15.7809 - mae: 1.6013 - val_loss: 12.7462 - val_mse: 12.7462 - val_mae: 1.5997 - lr: 2.8852e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 7s - loss: 15.7719 - mse: 15.7719 - mae: 1.5979 - val_loss: 12.6618 - val_mse: 12.6618 - val_mae: 1.5673 - lr: 2.8852e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 6s - loss: 15.7790 - mse: 15.7790 - mae: 1.5982 - val_loss: 12.7029 - val_mse: 12.7029 - val_mae: 1.5934 - lr: 2.8852e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 6s - loss: 15.7438 - mse: 15.7438 - mae: 1.5971 - val_loss: 12.6927 - val_mse: 12.6927 - val_mae: 1.5354 - lr: 2.8852e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 6s - loss: 15.7300 - mse: 15.7300 - mae: 1.5964 - val_loss: 12.6955 - val_mse: 12.6955 - val_mae: 1.6126 - lr: 2.8852e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 6s - loss: 15.7168 - mse: 15.7168 - mae: 1.5965 - val_loss: 12.7194 - val_mse: 12.7194 - val_mae: 1.5270 - lr: 2.8852e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 6s - loss: 15.6992 - mse: 15.6992 - mae: 1.5921 - val_loss: 12.6170 - val_mse: 12.6170 - val_mae: 1.5663 - lr: 2.8852e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 13/100\n",
            "1000/1000 - 6s - loss: 15.6350 - mse: 15.6350 - mae: 1.5908 - val_loss: 12.7352 - val_mse: 12.7352 - val_mae: 1.5609 - lr: 2.8852e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 14/100\n",
            "1000/1000 - 6s - loss: 15.6465 - mse: 15.6465 - mae: 1.5950 - val_loss: 12.7097 - val_mse: 12.7097 - val_mae: 1.5454 - lr: 2.8852e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 15/100\n",
            "1000/1000 - 6s - loss: 15.6652 - mse: 15.6652 - mae: 1.5901 - val_loss: 12.7524 - val_mse: 12.7524 - val_mae: 1.5955 - lr: 2.8852e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 16/100\n",
            "1000/1000 - 6s - loss: 15.6207 - mse: 15.6207 - mae: 1.5880 - val_loss: 12.5621 - val_mse: 12.5621 - val_mae: 1.5601 - lr: 2.8852e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 17/100\n",
            "1000/1000 - 7s - loss: 15.6116 - mse: 15.6116 - mae: 1.5862 - val_loss: 12.6044 - val_mse: 12.6044 - val_mae: 1.5927 - lr: 2.8852e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 18/100\n",
            "1000/1000 - 7s - loss: 15.6026 - mse: 15.6026 - mae: 1.5867 - val_loss: 12.6585 - val_mse: 12.6585 - val_mae: 1.5852 - lr: 2.8852e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 19/100\n",
            "1000/1000 - 7s - loss: 15.5620 - mse: 15.5620 - mae: 1.5863 - val_loss: 12.6582 - val_mse: 12.6582 - val_mae: 1.6174 - lr: 2.8852e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 20/100\n",
            "1000/1000 - 7s - loss: 15.5768 - mse: 15.5768 - mae: 1.5853 - val_loss: 12.5999 - val_mse: 12.5999 - val_mae: 1.5197 - lr: 2.8852e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 21/100\n",
            "1000/1000 - 7s - loss: 15.5341 - mse: 15.5341 - mae: 1.5853 - val_loss: 12.6816 - val_mse: 12.6816 - val_mae: 1.5108 - lr: 2.8852e-04 - 7s/epoch - 7ms/step\n",
            "Score for fold 2: loss of 12.681591987609863\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 6s - loss: 15.2782 - mse: 15.2782 - mae: 1.5790 - val_loss: 13.6829 - val_mse: 13.6829 - val_mae: 1.6158 - lr: 2.8852e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 6s - loss: 15.2364 - mse: 15.2364 - mae: 1.5784 - val_loss: 13.8890 - val_mse: 13.8890 - val_mae: 1.5628 - lr: 2.8852e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 6s - loss: 15.2052 - mse: 15.2052 - mae: 1.5788 - val_loss: 13.8658 - val_mse: 13.8658 - val_mae: 1.5698 - lr: 2.8852e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 6s - loss: 15.2223 - mse: 15.2223 - mae: 1.5796 - val_loss: 14.0456 - val_mse: 14.0456 - val_mae: 1.5518 - lr: 2.8852e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 6s - loss: 15.1898 - mse: 15.1898 - mae: 1.5783 - val_loss: 13.8797 - val_mse: 13.8797 - val_mae: 1.6169 - lr: 2.8852e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 6s - loss: 15.1548 - mse: 15.1548 - mae: 1.5751 - val_loss: 13.7670 - val_mse: 13.7670 - val_mae: 1.6419 - lr: 2.8852e-04 - 6s/epoch - 6ms/step\n",
            "Score for fold 3: loss of 13.766971588134766\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 6s - loss: 14.5833 - mse: 14.5833 - mae: 1.5735 - val_loss: 16.1729 - val_mse: 16.1729 - val_mae: 1.6312 - lr: 2.8852e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 6s - loss: 14.5374 - mse: 14.5374 - mae: 1.5722 - val_loss: 16.5023 - val_mse: 16.5023 - val_mae: 1.5799 - lr: 2.8852e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 6s - loss: 14.5429 - mse: 14.5429 - mae: 1.5692 - val_loss: 16.1764 - val_mse: 16.1764 - val_mae: 1.5974 - lr: 2.8852e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 6s - loss: 14.5243 - mse: 14.5243 - mae: 1.5662 - val_loss: 16.1669 - val_mse: 16.1669 - val_mae: 1.6313 - lr: 2.8852e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 6s - loss: 14.4809 - mse: 14.4809 - mae: 1.5672 - val_loss: 16.2495 - val_mse: 16.2495 - val_mae: 1.5971 - lr: 2.8852e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 6s - loss: 14.4839 - mse: 14.4839 - mae: 1.5669 - val_loss: 16.3329 - val_mse: 16.3329 - val_mae: 1.5573 - lr: 2.8852e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 6s - loss: 14.4532 - mse: 14.4532 - mae: 1.5621 - val_loss: 16.1742 - val_mse: 16.1742 - val_mae: 1.5997 - lr: 2.8852e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 6s - loss: 14.4487 - mse: 14.4487 - mae: 1.5625 - val_loss: 16.1293 - val_mse: 16.1293 - val_mae: 1.6454 - lr: 2.8852e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 6s - loss: 14.4551 - mse: 14.4551 - mae: 1.5660 - val_loss: 16.3458 - val_mse: 16.3458 - val_mae: 1.6304 - lr: 2.8852e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 6s - loss: 14.4128 - mse: 14.4128 - mae: 1.5642 - val_loss: 16.3601 - val_mse: 16.3601 - val_mae: 1.6921 - lr: 2.8852e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 6s - loss: 14.4385 - mse: 14.4385 - mae: 1.5631 - val_loss: 16.2706 - val_mse: 16.2706 - val_mae: 1.6102 - lr: 2.8852e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 6s - loss: 14.4218 - mse: 14.4218 - mae: 1.5616 - val_loss: 16.2978 - val_mse: 16.2978 - val_mae: 1.5764 - lr: 2.8852e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 13/100\n",
            "1000/1000 - 6s - loss: 14.3832 - mse: 14.3832 - mae: 1.5613 - val_loss: 16.1853 - val_mse: 16.1853 - val_mae: 1.7273 - lr: 2.8852e-04 - 6s/epoch - 6ms/step\n",
            "Score for fold 4: loss of 16.185306549072266\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 6s - loss: 13.3024 - mse: 13.3024 - mae: 1.5748 - val_loss: 20.3991 - val_mse: 20.3991 - val_mae: 1.5629 - lr: 2.8852e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 6s - loss: 13.3047 - mse: 13.3047 - mae: 1.5722 - val_loss: 20.4583 - val_mse: 20.4583 - val_mae: 1.5860 - lr: 2.8852e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 6s - loss: 13.2716 - mse: 13.2716 - mae: 1.5743 - val_loss: 20.4626 - val_mse: 20.4626 - val_mae: 1.6331 - lr: 2.8852e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 6s - loss: 13.3019 - mse: 13.3019 - mae: 1.5745 - val_loss: 20.5475 - val_mse: 20.5475 - val_mae: 1.5547 - lr: 2.8852e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 6s - loss: 13.2776 - mse: 13.2776 - mae: 1.5706 - val_loss: 20.6707 - val_mse: 20.6707 - val_mae: 1.7020 - lr: 2.8852e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 6s - loss: 13.2751 - mse: 13.2751 - mae: 1.5718 - val_loss: 20.4914 - val_mse: 20.4914 - val_mae: 1.6034 - lr: 2.8852e-04 - 6s/epoch - 6ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-10-14 11:45:24,522]\u001b[0m Finished trial#8 resulted in value: 14.972. Current best value is 14.732 with parameters: {'activation': 'relu', 'num_hidden_layer': 3, 'i': 10, 'learning_rate': 0.001046125302603454}.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Score for fold 5: loss of 20.491418838500977\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 3s - loss: 16.7046 - mse: 16.7046 - mae: 1.6839 - val_loss: 12.6254 - val_mse: 12.6254 - val_mae: 1.6594 - lr: 0.0023 - 3s/epoch - 3ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 3s - loss: 16.5535 - mse: 16.5535 - mae: 1.6585 - val_loss: 12.6228 - val_mse: 12.6228 - val_mae: 1.5602 - lr: 0.0023 - 3s/epoch - 3ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 3s - loss: 16.5848 - mse: 16.5848 - mae: 1.6601 - val_loss: 12.5797 - val_mse: 12.5797 - val_mae: 1.6204 - lr: 0.0023 - 3s/epoch - 3ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 3s - loss: 16.5196 - mse: 16.5196 - mae: 1.6545 - val_loss: 12.6548 - val_mse: 12.6548 - val_mae: 1.5652 - lr: 0.0023 - 3s/epoch - 3ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 3s - loss: 16.6371 - mse: 16.6371 - mae: 1.6676 - val_loss: 12.6298 - val_mse: 12.6298 - val_mae: 1.6769 - lr: 0.0023 - 3s/epoch - 3ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 3s - loss: 16.4852 - mse: 16.4852 - mae: 1.6616 - val_loss: 12.6603 - val_mse: 12.6603 - val_mae: 1.5513 - lr: 0.0023 - 3s/epoch - 3ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 3s - loss: 16.5734 - mse: 16.5734 - mae: 1.6670 - val_loss: 12.5837 - val_mse: 12.5837 - val_mae: 1.6506 - lr: 0.0023 - 3s/epoch - 3ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 3s - loss: 16.5563 - mse: 16.5563 - mae: 1.6595 - val_loss: 12.6445 - val_mse: 12.6445 - val_mae: 1.5497 - lr: 0.0023 - 3s/epoch - 3ms/step\n",
            "Score for fold 1: loss of 12.644492149353027\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 3s - loss: 16.1000 - mse: 16.1000 - mae: 1.6402 - val_loss: 14.0433 - val_mse: 14.0433 - val_mae: 1.5653 - lr: 0.0010 - 3s/epoch - 3ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 3s - loss: 16.1053 - mse: 16.1053 - mae: 1.6396 - val_loss: 13.8527 - val_mse: 13.8527 - val_mae: 1.6140 - lr: 0.0010 - 3s/epoch - 3ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 3s - loss: 16.1460 - mse: 16.1460 - mae: 1.6427 - val_loss: 13.8167 - val_mse: 13.8167 - val_mae: 1.6416 - lr: 0.0010 - 3s/epoch - 3ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 3s - loss: 16.2098 - mse: 16.2098 - mae: 1.6462 - val_loss: 14.3956 - val_mse: 14.3956 - val_mae: 1.9115 - lr: 0.0010 - 3s/epoch - 3ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 3s - loss: 16.1932 - mse: 16.1932 - mae: 1.6495 - val_loss: 14.3090 - val_mse: 14.3090 - val_mae: 1.5420 - lr: 0.0010 - 3s/epoch - 3ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 3s - loss: 16.1764 - mse: 16.1764 - mae: 1.6466 - val_loss: 14.0234 - val_mse: 14.0234 - val_mae: 1.5715 - lr: 0.0010 - 3s/epoch - 3ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 3s - loss: 16.1176 - mse: 16.1176 - mae: 1.6482 - val_loss: 13.9316 - val_mse: 13.9316 - val_mae: 1.5870 - lr: 0.0010 - 3s/epoch - 3ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 3s - loss: 16.1702 - mse: 16.1702 - mae: 1.6394 - val_loss: 13.9167 - val_mse: 13.9167 - val_mae: 1.5873 - lr: 0.0010 - 3s/epoch - 3ms/step\n",
            "Score for fold 2: loss of 13.9166898727417\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 3s - loss: 16.1777 - mse: 16.1777 - mae: 1.6527 - val_loss: 13.7355 - val_mse: 13.7355 - val_mae: 1.6453 - lr: 0.0010 - 3s/epoch - 3ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 3s - loss: 16.1923 - mse: 16.1923 - mae: 1.6595 - val_loss: 14.0144 - val_mse: 14.0144 - val_mae: 1.5339 - lr: 0.0010 - 3s/epoch - 3ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 3s - loss: 16.1787 - mse: 16.1787 - mae: 1.6499 - val_loss: 13.7566 - val_mse: 13.7566 - val_mae: 1.7168 - lr: 0.0010 - 3s/epoch - 3ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 3s - loss: 16.2642 - mse: 16.2642 - mae: 1.6498 - val_loss: 13.7122 - val_mse: 13.7122 - val_mae: 1.6440 - lr: 0.0010 - 3s/epoch - 3ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 3s - loss: 16.1887 - mse: 16.1887 - mae: 1.6532 - val_loss: 14.1630 - val_mse: 14.1630 - val_mae: 1.5204 - lr: 0.0010 - 3s/epoch - 3ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 3s - loss: 16.2639 - mse: 16.2639 - mae: 1.6493 - val_loss: 13.7303 - val_mse: 13.7303 - val_mae: 1.6532 - lr: 0.0010 - 3s/epoch - 3ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 3s - loss: 16.2578 - mse: 16.2578 - mae: 1.6609 - val_loss: 14.0664 - val_mse: 14.0664 - val_mae: 1.5307 - lr: 0.0010 - 3s/epoch - 3ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 3s - loss: 16.2122 - mse: 16.2122 - mae: 1.6528 - val_loss: 13.7289 - val_mse: 13.7289 - val_mae: 1.6711 - lr: 0.0010 - 3s/epoch - 3ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 3s - loss: 16.2259 - mse: 16.2259 - mae: 1.6560 - val_loss: 13.8164 - val_mse: 13.8164 - val_mae: 1.5653 - lr: 0.0010 - 3s/epoch - 3ms/step\n",
            "Score for fold 3: loss of 13.816436767578125\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 3s - loss: 15.7037 - mse: 15.7037 - mae: 1.6467 - val_loss: 16.0773 - val_mse: 16.0773 - val_mae: 1.6674 - lr: 0.0010 - 3s/epoch - 3ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 3s - loss: 15.6396 - mse: 15.6396 - mae: 1.6437 - val_loss: 16.0781 - val_mse: 16.0781 - val_mae: 1.6833 - lr: 0.0010 - 3s/epoch - 3ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 3s - loss: 15.7261 - mse: 15.7261 - mae: 1.6525 - val_loss: 16.0855 - val_mse: 16.0855 - val_mae: 1.6184 - lr: 0.0010 - 3s/epoch - 3ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 3s - loss: 15.6044 - mse: 15.6044 - mae: 1.6513 - val_loss: 16.2565 - val_mse: 16.2565 - val_mae: 1.6239 - lr: 0.0010 - 3s/epoch - 3ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 3s - loss: 15.6683 - mse: 15.6683 - mae: 1.6419 - val_loss: 16.1018 - val_mse: 16.1018 - val_mae: 1.6403 - lr: 0.0010 - 3s/epoch - 3ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 3s - loss: 15.6356 - mse: 15.6356 - mae: 1.6492 - val_loss: 17.0456 - val_mse: 17.0456 - val_mae: 1.5309 - lr: 0.0010 - 3s/epoch - 3ms/step\n",
            "Score for fold 4: loss of 17.04559326171875\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 3s - loss: 14.1924 - mse: 14.1924 - mae: 1.6338 - val_loss: 21.8178 - val_mse: 21.8178 - val_mae: 1.6789 - lr: 0.0010 - 3s/epoch - 3ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 3s - loss: 14.2665 - mse: 14.2665 - mae: 1.6299 - val_loss: 21.8324 - val_mse: 21.8324 - val_mae: 1.6780 - lr: 0.0010 - 3s/epoch - 3ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 3s - loss: 14.2240 - mse: 14.2240 - mae: 1.6351 - val_loss: 22.4445 - val_mse: 22.4445 - val_mae: 1.5992 - lr: 0.0010 - 3s/epoch - 3ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 3s - loss: 14.2348 - mse: 14.2348 - mae: 1.6294 - val_loss: 21.8017 - val_mse: 21.8017 - val_mae: 1.6797 - lr: 0.0010 - 3s/epoch - 3ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 3s - loss: 14.2027 - mse: 14.2027 - mae: 1.6321 - val_loss: 23.0261 - val_mse: 23.0261 - val_mae: 1.5892 - lr: 0.0010 - 3s/epoch - 3ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 3s - loss: 14.1909 - mse: 14.1909 - mae: 1.6289 - val_loss: 22.2707 - val_mse: 22.2707 - val_mae: 1.6177 - lr: 0.0010 - 3s/epoch - 3ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 3s - loss: 14.2158 - mse: 14.2158 - mae: 1.6329 - val_loss: 21.8904 - val_mse: 21.8904 - val_mae: 1.7984 - lr: 0.0010 - 3s/epoch - 3ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 3s - loss: 14.1608 - mse: 14.1608 - mae: 1.6351 - val_loss: 21.8218 - val_mse: 21.8218 - val_mae: 1.6594 - lr: 0.0010 - 3s/epoch - 3ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 3s - loss: 14.1289 - mse: 14.1289 - mae: 1.6336 - val_loss: 21.7537 - val_mse: 21.7537 - val_mae: 1.6737 - lr: 0.0010 - 3s/epoch - 3ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 3s - loss: 14.2330 - mse: 14.2330 - mae: 1.6316 - val_loss: 22.3455 - val_mse: 22.3455 - val_mae: 1.6112 - lr: 0.0010 - 3s/epoch - 3ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 3s - loss: 14.1928 - mse: 14.1928 - mae: 1.6311 - val_loss: 21.8828 - val_mse: 21.8828 - val_mae: 1.8120 - lr: 0.0010 - 3s/epoch - 3ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 3s - loss: 14.1539 - mse: 14.1539 - mae: 1.6353 - val_loss: 22.6399 - val_mse: 22.6399 - val_mae: 1.5959 - lr: 0.0010 - 3s/epoch - 3ms/step\n",
            "Epoch 13/100\n",
            "1000/1000 - 3s - loss: 14.2367 - mse: 14.2367 - mae: 1.6305 - val_loss: 21.7446 - val_mse: 21.7446 - val_mae: 1.7165 - lr: 0.0010 - 3s/epoch - 3ms/step\n",
            "Epoch 14/100\n",
            "1000/1000 - 3s - loss: 14.3275 - mse: 14.3275 - mae: 1.6338 - val_loss: 21.7741 - val_mse: 21.7741 - val_mae: 1.7238 - lr: 0.0010 - 3s/epoch - 3ms/step\n",
            "Epoch 15/100\n",
            "1000/1000 - 3s - loss: 14.2253 - mse: 14.2253 - mae: 1.6347 - val_loss: 21.8498 - val_mse: 21.8498 - val_mae: 1.6617 - lr: 0.0010 - 3s/epoch - 3ms/step\n",
            "Epoch 16/100\n",
            "1000/1000 - 3s - loss: 14.2006 - mse: 14.2006 - mae: 1.6345 - val_loss: 21.8369 - val_mse: 21.8369 - val_mae: 1.6580 - lr: 0.0010 - 3s/epoch - 3ms/step\n",
            "Epoch 17/100\n",
            "1000/1000 - 3s - loss: 14.2238 - mse: 14.2238 - mae: 1.6303 - val_loss: 21.8410 - val_mse: 21.8410 - val_mae: 1.7817 - lr: 0.0010 - 3s/epoch - 3ms/step\n",
            "Epoch 18/100\n",
            "1000/1000 - 3s - loss: 14.2491 - mse: 14.2491 - mae: 1.6362 - val_loss: 21.7886 - val_mse: 21.7886 - val_mae: 1.7468 - lr: 0.0010 - 3s/epoch - 3ms/step\n",
            "Score for fold 5: loss of 21.788558959960938\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-10-14 11:47:40,227]\u001b[0m Finished trial#9 resulted in value: 15.844. Current best value is 14.732 with parameters: {'activation': 'relu', 'num_hidden_layer': 3, 'i': 10, 'learning_rate': 0.001046125302603454}.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 3s - loss: 16.2871 - mse: 16.2871 - mae: 1.6111 - val_loss: 12.6132 - val_mse: 12.6132 - val_mae: 1.6085 - lr: 6.5345e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 15.8352 - mse: 15.8352 - mae: 1.5925 - val_loss: 12.4344 - val_mse: 12.4344 - val_mae: 1.6122 - lr: 6.5345e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 15.8225 - mse: 15.8225 - mae: 1.5885 - val_loss: 12.3188 - val_mse: 12.3188 - val_mae: 1.6416 - lr: 6.5345e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 15.7722 - mse: 15.7722 - mae: 1.5834 - val_loss: 12.4442 - val_mse: 12.4442 - val_mae: 1.6099 - lr: 6.5345e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 15.7705 - mse: 15.7705 - mae: 1.5848 - val_loss: 12.3886 - val_mse: 12.3886 - val_mae: 1.6245 - lr: 6.5345e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 15.7501 - mse: 15.7501 - mae: 1.5797 - val_loss: 12.5707 - val_mse: 12.5707 - val_mae: 1.5849 - lr: 6.5345e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 15.7126 - mse: 15.7126 - mae: 1.5800 - val_loss: 12.3886 - val_mse: 12.3886 - val_mae: 1.5827 - lr: 6.5345e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 2s - loss: 15.6825 - mse: 15.6825 - mae: 1.5791 - val_loss: 12.2551 - val_mse: 12.2551 - val_mae: 1.5897 - lr: 6.5345e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 2s - loss: 15.6890 - mse: 15.6890 - mae: 1.5798 - val_loss: 12.3887 - val_mse: 12.3887 - val_mae: 1.5806 - lr: 6.5345e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 2s - loss: 15.6768 - mse: 15.6768 - mae: 1.5769 - val_loss: 12.3173 - val_mse: 12.3173 - val_mae: 1.6033 - lr: 6.5345e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 2s - loss: 15.6561 - mse: 15.6561 - mae: 1.5750 - val_loss: 12.4122 - val_mse: 12.4122 - val_mae: 1.6594 - lr: 6.5345e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 2s - loss: 15.6569 - mse: 15.6569 - mae: 1.5767 - val_loss: 12.5366 - val_mse: 12.5366 - val_mae: 1.5667 - lr: 6.5345e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 13/100\n",
            "1000/1000 - 2s - loss: 15.6322 - mse: 15.6322 - mae: 1.5740 - val_loss: 12.2935 - val_mse: 12.2935 - val_mae: 1.5883 - lr: 6.5345e-04 - 2s/epoch - 2ms/step\n",
            "Score for fold 1: loss of 12.29348087310791\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 14.2769 - mse: 14.2769 - mae: 1.5843 - val_loss: 17.8210 - val_mse: 17.8210 - val_mae: 1.5608 - lr: 6.5345e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 14.2755 - mse: 14.2755 - mae: 1.5879 - val_loss: 17.6738 - val_mse: 17.6738 - val_mae: 1.6382 - lr: 6.5345e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 14.2678 - mse: 14.2678 - mae: 1.5796 - val_loss: 17.6606 - val_mse: 17.6606 - val_mae: 1.5868 - lr: 6.5345e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 14.2396 - mse: 14.2396 - mae: 1.5768 - val_loss: 17.6523 - val_mse: 17.6523 - val_mae: 1.5609 - lr: 6.5345e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 14.2505 - mse: 14.2505 - mae: 1.5774 - val_loss: 17.6511 - val_mse: 17.6511 - val_mae: 1.6299 - lr: 6.5345e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 14.2187 - mse: 14.2187 - mae: 1.5812 - val_loss: 17.7122 - val_mse: 17.7122 - val_mae: 1.6109 - lr: 6.5345e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 14.2144 - mse: 14.2144 - mae: 1.5769 - val_loss: 17.6900 - val_mse: 17.6900 - val_mae: 1.5898 - lr: 6.5345e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 2s - loss: 14.2113 - mse: 14.2113 - mae: 1.5773 - val_loss: 17.7012 - val_mse: 17.7012 - val_mae: 1.5495 - lr: 6.5345e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 2s - loss: 14.1977 - mse: 14.1977 - mae: 1.5742 - val_loss: 17.7834 - val_mse: 17.7834 - val_mae: 1.5276 - lr: 6.5345e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 2s - loss: 14.2111 - mse: 14.2111 - mae: 1.5711 - val_loss: 17.6140 - val_mse: 17.6140 - val_mae: 1.6089 - lr: 6.5345e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 2s - loss: 14.2017 - mse: 14.2017 - mae: 1.5740 - val_loss: 17.7213 - val_mse: 17.7213 - val_mae: 1.5496 - lr: 6.5345e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 2s - loss: 14.1834 - mse: 14.1834 - mae: 1.5727 - val_loss: 17.6903 - val_mse: 17.6903 - val_mae: 1.6148 - lr: 6.5345e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 13/100\n",
            "1000/1000 - 2s - loss: 14.1825 - mse: 14.1825 - mae: 1.5755 - val_loss: 17.6111 - val_mse: 17.6111 - val_mae: 1.5669 - lr: 6.5345e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 14/100\n",
            "1000/1000 - 2s - loss: 14.1832 - mse: 14.1832 - mae: 1.5753 - val_loss: 17.7391 - val_mse: 17.7391 - val_mae: 1.5543 - lr: 6.5345e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 15/100\n",
            "1000/1000 - 2s - loss: 14.1692 - mse: 14.1692 - mae: 1.5724 - val_loss: 17.6445 - val_mse: 17.6445 - val_mae: 1.6195 - lr: 6.5345e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 16/100\n",
            "1000/1000 - 2s - loss: 14.1851 - mse: 14.1851 - mae: 1.5716 - val_loss: 17.6460 - val_mse: 17.6460 - val_mae: 1.5656 - lr: 6.5345e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 17/100\n",
            "1000/1000 - 2s - loss: 14.1758 - mse: 14.1758 - mae: 1.5695 - val_loss: 17.6635 - val_mse: 17.6635 - val_mae: 1.5853 - lr: 6.5345e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 18/100\n",
            "1000/1000 - 2s - loss: 14.1694 - mse: 14.1694 - mae: 1.5692 - val_loss: 17.5617 - val_mse: 17.5617 - val_mae: 1.5914 - lr: 6.5345e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 19/100\n",
            "1000/1000 - 2s - loss: 14.1698 - mse: 14.1698 - mae: 1.5747 - val_loss: 17.5965 - val_mse: 17.5965 - val_mae: 1.5796 - lr: 6.5345e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 20/100\n",
            "1000/1000 - 2s - loss: 14.1627 - mse: 14.1627 - mae: 1.5681 - val_loss: 17.6353 - val_mse: 17.6353 - val_mae: 1.6034 - lr: 6.5345e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 21/100\n",
            "1000/1000 - 2s - loss: 14.1408 - mse: 14.1408 - mae: 1.5715 - val_loss: 17.6311 - val_mse: 17.6311 - val_mae: 1.6145 - lr: 6.5345e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 22/100\n",
            "1000/1000 - 2s - loss: 14.1204 - mse: 14.1204 - mae: 1.5735 - val_loss: 17.7589 - val_mse: 17.7589 - val_mae: 1.5615 - lr: 6.5345e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 23/100\n",
            "1000/1000 - 2s - loss: 14.1227 - mse: 14.1227 - mae: 1.5734 - val_loss: 17.6894 - val_mse: 17.6894 - val_mae: 1.5771 - lr: 6.5345e-04 - 2s/epoch - 2ms/step\n",
            "Score for fold 2: loss of 17.689434051513672\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 13.5176 - mse: 13.5176 - mae: 1.5748 - val_loss: 20.1793 - val_mse: 20.1793 - val_mae: 1.5898 - lr: 6.5345e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 13.4716 - mse: 13.4716 - mae: 1.5713 - val_loss: 20.1998 - val_mse: 20.1998 - val_mae: 1.5869 - lr: 6.5345e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 13.4810 - mse: 13.4810 - mae: 1.5745 - val_loss: 20.2044 - val_mse: 20.2044 - val_mae: 1.5983 - lr: 6.5345e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 13.4893 - mse: 13.4893 - mae: 1.5736 - val_loss: 20.2280 - val_mse: 20.2280 - val_mae: 1.6189 - lr: 6.5345e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 13.4627 - mse: 13.4627 - mae: 1.5724 - val_loss: 20.2865 - val_mse: 20.2865 - val_mae: 1.5657 - lr: 6.5345e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 13.4691 - mse: 13.4691 - mae: 1.5690 - val_loss: 20.2760 - val_mse: 20.2760 - val_mae: 1.5992 - lr: 6.5345e-04 - 2s/epoch - 2ms/step\n",
            "Score for fold 3: loss of 20.275955200195312\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 15.5725 - mse: 15.5725 - mae: 1.5828 - val_loss: 11.8794 - val_mse: 11.8794 - val_mae: 1.5238 - lr: 6.5345e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 15.5692 - mse: 15.5692 - mae: 1.5797 - val_loss: 11.8546 - val_mse: 11.8546 - val_mae: 1.5328 - lr: 6.5345e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 15.5477 - mse: 15.5477 - mae: 1.5804 - val_loss: 11.8707 - val_mse: 11.8707 - val_mae: 1.5324 - lr: 6.5345e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 15.5170 - mse: 15.5170 - mae: 1.5824 - val_loss: 11.9203 - val_mse: 11.9203 - val_mae: 1.5160 - lr: 6.5345e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 15.5419 - mse: 15.5419 - mae: 1.5813 - val_loss: 11.8380 - val_mse: 11.8380 - val_mae: 1.5953 - lr: 6.5345e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 15.5131 - mse: 15.5131 - mae: 1.5814 - val_loss: 11.8641 - val_mse: 11.8641 - val_mae: 1.5769 - lr: 6.5345e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 15.5438 - mse: 15.5438 - mae: 1.5813 - val_loss: 11.8933 - val_mse: 11.8933 - val_mae: 1.5492 - lr: 6.5345e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 2s - loss: 15.5546 - mse: 15.5546 - mae: 1.5813 - val_loss: 11.8701 - val_mse: 11.8701 - val_mae: 1.5939 - lr: 6.5345e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 2s - loss: 15.4988 - mse: 15.4988 - mae: 1.5834 - val_loss: 11.9342 - val_mse: 11.9342 - val_mae: 1.5383 - lr: 6.5345e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 2s - loss: 15.4958 - mse: 15.4958 - mae: 1.5811 - val_loss: 11.8868 - val_mse: 11.8868 - val_mae: 1.5527 - lr: 6.5345e-04 - 2s/epoch - 2ms/step\n",
            "Score for fold 4: loss of 11.886800765991211\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 15.3738 - mse: 15.3738 - mae: 1.5734 - val_loss: 12.3994 - val_mse: 12.3994 - val_mae: 1.5781 - lr: 6.5345e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 15.3568 - mse: 15.3568 - mae: 1.5713 - val_loss: 12.4212 - val_mse: 12.4212 - val_mae: 1.5561 - lr: 6.5345e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 15.3932 - mse: 15.3932 - mae: 1.5738 - val_loss: 12.3623 - val_mse: 12.3623 - val_mae: 1.5946 - lr: 6.5345e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 15.4003 - mse: 15.4003 - mae: 1.5702 - val_loss: 12.4138 - val_mse: 12.4138 - val_mae: 1.5876 - lr: 6.5345e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 15.3921 - mse: 15.3921 - mae: 1.5716 - val_loss: 12.4418 - val_mse: 12.4418 - val_mae: 1.5621 - lr: 6.5345e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 15.3801 - mse: 15.3801 - mae: 1.5718 - val_loss: 12.5133 - val_mse: 12.5133 - val_mae: 1.5426 - lr: 6.5345e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 15.4030 - mse: 15.4030 - mae: 1.5713 - val_loss: 12.5002 - val_mse: 12.5002 - val_mae: 1.5522 - lr: 6.5345e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 2s - loss: 15.3715 - mse: 15.3715 - mae: 1.5718 - val_loss: 12.4295 - val_mse: 12.4295 - val_mae: 1.5393 - lr: 6.5345e-04 - 2s/epoch - 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-10-14 11:49:43,866]\u001b[0m Finished trial#10 resulted in value: 14.916000000000002. Current best value is 14.732 with parameters: {'activation': 'relu', 'num_hidden_layer': 3, 'i': 10, 'learning_rate': 0.001046125302603454}.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Score for fold 5: loss of 12.429461479187012\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 3s - loss: 14.9420 - mse: 14.9420 - mae: 1.6156 - val_loss: 17.2335 - val_mse: 17.2335 - val_mae: 1.5552 - lr: 0.0014 - 3s/epoch - 3ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 14.6819 - mse: 14.6819 - mae: 1.5979 - val_loss: 16.9414 - val_mse: 16.9414 - val_mae: 1.6323 - lr: 0.0014 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 14.5786 - mse: 14.5786 - mae: 1.5917 - val_loss: 16.9625 - val_mse: 16.9625 - val_mae: 1.6334 - lr: 0.0014 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 14.5084 - mse: 14.5084 - mae: 1.5842 - val_loss: 16.9746 - val_mse: 16.9746 - val_mae: 1.5859 - lr: 0.0014 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 14.5065 - mse: 14.5065 - mae: 1.5838 - val_loss: 17.0976 - val_mse: 17.0976 - val_mae: 1.5863 - lr: 0.0014 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 14.5073 - mse: 14.5073 - mae: 1.5813 - val_loss: 16.7217 - val_mse: 16.7217 - val_mae: 1.6358 - lr: 0.0014 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 14.4725 - mse: 14.4725 - mae: 1.5785 - val_loss: 17.2299 - val_mse: 17.2299 - val_mae: 1.6330 - lr: 0.0014 - 2s/epoch - 2ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 2s - loss: 14.3996 - mse: 14.3996 - mae: 1.5817 - val_loss: 17.5391 - val_mse: 17.5391 - val_mae: 1.5997 - lr: 0.0014 - 2s/epoch - 2ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 2s - loss: 14.4489 - mse: 14.4489 - mae: 1.5829 - val_loss: 16.8756 - val_mse: 16.8756 - val_mae: 1.6498 - lr: 0.0014 - 2s/epoch - 2ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 2s - loss: 14.4754 - mse: 14.4754 - mae: 1.5817 - val_loss: 16.8792 - val_mse: 16.8792 - val_mae: 1.5846 - lr: 0.0014 - 2s/epoch - 2ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 2s - loss: 14.5211 - mse: 14.5211 - mae: 1.5815 - val_loss: 16.9419 - val_mse: 16.9419 - val_mae: 1.6756 - lr: 0.0014 - 2s/epoch - 2ms/step\n",
            "Score for fold 1: loss of 16.941938400268555\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 15.3646 - mse: 15.3646 - mae: 1.6049 - val_loss: 12.9052 - val_mse: 12.9052 - val_mae: 1.5809 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 15.3450 - mse: 15.3450 - mae: 1.6041 - val_loss: 13.1299 - val_mse: 13.1299 - val_mae: 1.5856 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 15.3078 - mse: 15.3078 - mae: 1.6010 - val_loss: 13.1014 - val_mse: 13.1014 - val_mae: 1.5685 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 15.3188 - mse: 15.3188 - mae: 1.6011 - val_loss: 13.2970 - val_mse: 13.2970 - val_mae: 1.5108 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 15.3061 - mse: 15.3061 - mae: 1.6000 - val_loss: 13.1746 - val_mse: 13.1746 - val_mae: 1.5610 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 15.2607 - mse: 15.2607 - mae: 1.5986 - val_loss: 13.0410 - val_mse: 13.0410 - val_mae: 1.5175 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Score for fold 2: loss of 13.041028022766113\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 15.3947 - mse: 15.3947 - mae: 1.6002 - val_loss: 12.9111 - val_mse: 12.9111 - val_mae: 1.4811 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 15.4464 - mse: 15.4464 - mae: 1.5958 - val_loss: 12.6358 - val_mse: 12.6358 - val_mae: 1.6792 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 15.3640 - mse: 15.3640 - mae: 1.5966 - val_loss: 12.5383 - val_mse: 12.5383 - val_mae: 1.5511 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 15.4217 - mse: 15.4217 - mae: 1.5903 - val_loss: 12.5818 - val_mse: 12.5818 - val_mae: 1.6121 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 15.4250 - mse: 15.4250 - mae: 1.5855 - val_loss: 12.6737 - val_mse: 12.6737 - val_mae: 1.6351 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 15.3775 - mse: 15.3775 - mae: 1.5905 - val_loss: 12.5799 - val_mse: 12.5799 - val_mae: 1.5987 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 15.3653 - mse: 15.3653 - mae: 1.5917 - val_loss: 12.6170 - val_mse: 12.6170 - val_mae: 1.5826 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 2s - loss: 15.3971 - mse: 15.3971 - mae: 1.5856 - val_loss: 12.6363 - val_mse: 12.6363 - val_mae: 1.6382 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Score for fold 3: loss of 12.636289596557617\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 15.6303 - mse: 15.6303 - mae: 1.5813 - val_loss: 11.5372 - val_mse: 11.5372 - val_mae: 1.6313 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 15.5640 - mse: 15.5640 - mae: 1.5731 - val_loss: 11.8856 - val_mse: 11.8856 - val_mae: 1.5766 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 15.5563 - mse: 15.5563 - mae: 1.5781 - val_loss: 11.6192 - val_mse: 11.6192 - val_mae: 1.5449 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 15.5142 - mse: 15.5142 - mae: 1.5728 - val_loss: 11.5728 - val_mse: 11.5728 - val_mae: 1.6911 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 15.4286 - mse: 15.4286 - mae: 1.5694 - val_loss: 11.9235 - val_mse: 11.9235 - val_mae: 1.5748 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 15.5383 - mse: 15.5383 - mae: 1.5712 - val_loss: 11.7843 - val_mse: 11.7843 - val_mae: 1.6054 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Score for fold 4: loss of 11.784296035766602\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 13.5101 - mse: 13.5101 - mae: 1.5827 - val_loss: 19.9136 - val_mse: 19.9136 - val_mae: 1.5558 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 13.4856 - mse: 13.4856 - mae: 1.5788 - val_loss: 19.7568 - val_mse: 19.7568 - val_mae: 1.6203 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 13.4183 - mse: 13.4183 - mae: 1.5766 - val_loss: 19.5370 - val_mse: 19.5370 - val_mae: 1.5537 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 13.3583 - mse: 13.3583 - mae: 1.5779 - val_loss: 19.8397 - val_mse: 19.8397 - val_mae: 1.5599 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 13.3719 - mse: 13.3719 - mae: 1.5815 - val_loss: 20.0012 - val_mse: 20.0012 - val_mae: 1.6292 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 13.3589 - mse: 13.3589 - mae: 1.5811 - val_loss: 20.2888 - val_mse: 20.2888 - val_mae: 1.6102 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 13.3969 - mse: 13.3969 - mae: 1.5792 - val_loss: 20.0736 - val_mse: 20.0736 - val_mae: 1.5269 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 2s - loss: 13.3556 - mse: 13.3556 - mae: 1.5782 - val_loss: 20.0491 - val_mse: 20.0491 - val_mae: 1.5572 - lr: 0.0010 - 2s/epoch - 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-10-14 11:51:18,376]\u001b[0m Finished trial#11 resulted in value: 14.89. Current best value is 14.732 with parameters: {'activation': 'relu', 'num_hidden_layer': 3, 'i': 10, 'learning_rate': 0.001046125302603454}.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Score for fold 5: loss of 20.04912567138672\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 3s - loss: 16.5280 - mse: 16.5280 - mae: 1.6210 - val_loss: 11.1755 - val_mse: 11.1755 - val_mae: 1.5920 - lr: 5.9903e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 16.1245 - mse: 16.1245 - mae: 1.6025 - val_loss: 11.2867 - val_mse: 11.2867 - val_mae: 1.5241 - lr: 5.9903e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 16.1128 - mse: 16.1128 - mae: 1.5972 - val_loss: 11.0977 - val_mse: 11.0977 - val_mae: 1.6376 - lr: 5.9903e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 16.0458 - mse: 16.0458 - mae: 1.5942 - val_loss: 11.2091 - val_mse: 11.2091 - val_mae: 1.4868 - lr: 5.9903e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 16.0200 - mse: 16.0200 - mae: 1.5912 - val_loss: 11.0180 - val_mse: 11.0180 - val_mae: 1.5150 - lr: 5.9903e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 15.9593 - mse: 15.9593 - mae: 1.5895 - val_loss: 11.0853 - val_mse: 11.0853 - val_mae: 1.5846 - lr: 5.9903e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 15.9132 - mse: 15.9132 - mae: 1.5853 - val_loss: 11.1523 - val_mse: 11.1523 - val_mae: 1.5314 - lr: 5.9903e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 2s - loss: 15.9385 - mse: 15.9385 - mae: 1.5854 - val_loss: 10.9706 - val_mse: 10.9706 - val_mae: 1.5325 - lr: 5.9903e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 2s - loss: 15.9218 - mse: 15.9218 - mae: 1.5829 - val_loss: 10.9798 - val_mse: 10.9798 - val_mae: 1.5861 - lr: 5.9903e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 2s - loss: 15.8605 - mse: 15.8605 - mae: 1.5846 - val_loss: 11.1703 - val_mse: 11.1703 - val_mae: 1.5229 - lr: 5.9903e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 2s - loss: 15.8380 - mse: 15.8380 - mae: 1.5790 - val_loss: 10.9647 - val_mse: 10.9647 - val_mae: 1.5181 - lr: 5.9903e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 2s - loss: 15.8380 - mse: 15.8380 - mae: 1.5764 - val_loss: 11.2416 - val_mse: 11.2416 - val_mae: 1.5329 - lr: 5.9903e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 13/100\n",
            "1000/1000 - 2s - loss: 15.8256 - mse: 15.8256 - mae: 1.5795 - val_loss: 11.1969 - val_mse: 11.1969 - val_mae: 1.5515 - lr: 5.9903e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 14/100\n",
            "1000/1000 - 2s - loss: 15.8235 - mse: 15.8235 - mae: 1.5783 - val_loss: 11.1756 - val_mse: 11.1756 - val_mae: 1.5598 - lr: 5.9903e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 15/100\n",
            "1000/1000 - 2s - loss: 15.8124 - mse: 15.8124 - mae: 1.5800 - val_loss: 11.1027 - val_mse: 11.1027 - val_mae: 1.5417 - lr: 5.9903e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 16/100\n",
            "1000/1000 - 2s - loss: 15.7399 - mse: 15.7399 - mae: 1.5750 - val_loss: 11.0616 - val_mse: 11.0616 - val_mae: 1.5558 - lr: 5.9903e-04 - 2s/epoch - 2ms/step\n",
            "Score for fold 1: loss of 11.061554908752441\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 14.8245 - mse: 14.8245 - mae: 1.5693 - val_loss: 14.8079 - val_mse: 14.8079 - val_mae: 1.5298 - lr: 5.9903e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 14.9041 - mse: 14.9041 - mae: 1.5715 - val_loss: 14.6546 - val_mse: 14.6546 - val_mae: 1.5629 - lr: 5.9903e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 14.8305 - mse: 14.8305 - mae: 1.5675 - val_loss: 14.5132 - val_mse: 14.5132 - val_mae: 1.5914 - lr: 5.9903e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 14.8376 - mse: 14.8376 - mae: 1.5656 - val_loss: 14.9682 - val_mse: 14.9682 - val_mae: 1.5489 - lr: 5.9903e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 14.8107 - mse: 14.8107 - mae: 1.5673 - val_loss: 14.6390 - val_mse: 14.6390 - val_mae: 1.6239 - lr: 5.9903e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 14.8584 - mse: 14.8584 - mae: 1.5667 - val_loss: 14.7084 - val_mse: 14.7084 - val_mae: 1.5198 - lr: 5.9903e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 14.8144 - mse: 14.8144 - mae: 1.5668 - val_loss: 14.8817 - val_mse: 14.8817 - val_mae: 1.5716 - lr: 5.9903e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 2s - loss: 14.7741 - mse: 14.7741 - mae: 1.5633 - val_loss: 14.6667 - val_mse: 14.6667 - val_mae: 1.5507 - lr: 5.9903e-04 - 2s/epoch - 2ms/step\n",
            "Score for fold 2: loss of 14.666729927062988\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 13.7149 - mse: 13.7149 - mae: 1.5705 - val_loss: 19.0770 - val_mse: 19.0770 - val_mae: 1.5833 - lr: 5.9903e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 13.7665 - mse: 13.7665 - mae: 1.5676 - val_loss: 19.2437 - val_mse: 19.2437 - val_mae: 1.5565 - lr: 5.9903e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 13.7460 - mse: 13.7460 - mae: 1.5660 - val_loss: 19.1458 - val_mse: 19.1458 - val_mae: 1.5826 - lr: 5.9903e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 13.6988 - mse: 13.6988 - mae: 1.5656 - val_loss: 19.2994 - val_mse: 19.2994 - val_mae: 1.5669 - lr: 5.9903e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 13.6848 - mse: 13.6848 - mae: 1.5619 - val_loss: 19.1231 - val_mse: 19.1231 - val_mae: 1.5816 - lr: 5.9903e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 13.6710 - mse: 13.6710 - mae: 1.5592 - val_loss: 19.0624 - val_mse: 19.0624 - val_mae: 1.6194 - lr: 5.9903e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 13.6253 - mse: 13.6253 - mae: 1.5605 - val_loss: 19.4151 - val_mse: 19.4151 - val_mae: 1.5818 - lr: 5.9903e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 2s - loss: 13.6276 - mse: 13.6276 - mae: 1.5588 - val_loss: 19.3561 - val_mse: 19.3561 - val_mae: 1.5387 - lr: 5.9903e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 2s - loss: 13.5332 - mse: 13.5332 - mae: 1.5593 - val_loss: 19.2180 - val_mse: 19.2180 - val_mae: 1.5652 - lr: 5.9903e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 2s - loss: 13.5343 - mse: 13.5343 - mae: 1.5561 - val_loss: 19.5120 - val_mse: 19.5120 - val_mae: 1.5724 - lr: 5.9903e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 2s - loss: 13.4423 - mse: 13.4423 - mae: 1.5538 - val_loss: 19.3184 - val_mse: 19.3184 - val_mae: 1.5594 - lr: 5.9903e-04 - 2s/epoch - 2ms/step\n",
            "Score for fold 3: loss of 19.318422317504883\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 14.9485 - mse: 14.9485 - mae: 1.5620 - val_loss: 13.4642 - val_mse: 13.4642 - val_mae: 1.5452 - lr: 5.9903e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 14.9468 - mse: 14.9468 - mae: 1.5598 - val_loss: 13.3757 - val_mse: 13.3757 - val_mae: 1.5522 - lr: 5.9903e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 14.9265 - mse: 14.9265 - mae: 1.5577 - val_loss: 13.3678 - val_mse: 13.3678 - val_mae: 1.6136 - lr: 5.9903e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 14.8694 - mse: 14.8694 - mae: 1.5573 - val_loss: 13.8070 - val_mse: 13.8070 - val_mae: 1.5771 - lr: 5.9903e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 14.9389 - mse: 14.9389 - mae: 1.5559 - val_loss: 13.6607 - val_mse: 13.6607 - val_mae: 1.5482 - lr: 5.9903e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 14.8058 - mse: 14.8058 - mae: 1.5545 - val_loss: 13.5266 - val_mse: 13.5266 - val_mae: 1.5409 - lr: 5.9903e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 14.7940 - mse: 14.7940 - mae: 1.5539 - val_loss: 13.6169 - val_mse: 13.6169 - val_mae: 1.5405 - lr: 5.9903e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 2s - loss: 14.7797 - mse: 14.7797 - mae: 1.5522 - val_loss: 13.6026 - val_mse: 13.6026 - val_mae: 1.5611 - lr: 5.9903e-04 - 2s/epoch - 2ms/step\n",
            "Score for fold 4: loss of 13.602614402770996\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 14.3697 - mse: 14.3697 - mae: 1.5475 - val_loss: 15.2842 - val_mse: 15.2842 - val_mae: 1.5882 - lr: 5.9903e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 14.4088 - mse: 14.4088 - mae: 1.5442 - val_loss: 15.3009 - val_mse: 15.3009 - val_mae: 1.5938 - lr: 5.9903e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 14.2992 - mse: 14.2992 - mae: 1.5418 - val_loss: 15.4225 - val_mse: 15.4225 - val_mae: 1.5874 - lr: 5.9903e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 14.1966 - mse: 14.1966 - mae: 1.5361 - val_loss: 15.5712 - val_mse: 15.5712 - val_mae: 1.5872 - lr: 5.9903e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 14.1523 - mse: 14.1523 - mae: 1.5394 - val_loss: 15.4239 - val_mse: 15.4239 - val_mae: 1.6113 - lr: 5.9903e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 14.2726 - mse: 14.2726 - mae: 1.5399 - val_loss: 15.4553 - val_mse: 15.4553 - val_mae: 1.5946 - lr: 5.9903e-04 - 2s/epoch - 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-10-14 11:53:02,498]\u001b[0m Finished trial#12 resulted in value: 14.822. Current best value is 14.732 with parameters: {'activation': 'relu', 'num_hidden_layer': 3, 'i': 10, 'learning_rate': 0.001046125302603454}.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Score for fold 5: loss of 15.45529842376709\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 3s - loss: 15.5758 - mse: 15.5758 - mae: 1.6269 - val_loss: 14.6181 - val_mse: 14.6181 - val_mae: 1.6847 - lr: 0.0036 - 3s/epoch - 3ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 15.2653 - mse: 15.2653 - mae: 1.6010 - val_loss: 14.9541 - val_mse: 14.9541 - val_mae: 1.6393 - lr: 0.0036 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 15.1952 - mse: 15.1952 - mae: 1.6005 - val_loss: 14.6462 - val_mse: 14.6462 - val_mae: 1.5849 - lr: 0.0036 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 15.1458 - mse: 15.1458 - mae: 1.5893 - val_loss: 15.5010 - val_mse: 15.5010 - val_mae: 1.5909 - lr: 0.0036 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 15.1977 - mse: 15.1977 - mae: 1.6015 - val_loss: 15.2498 - val_mse: 15.2498 - val_mae: 1.6401 - lr: 0.0036 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 15.1429 - mse: 15.1429 - mae: 1.6091 - val_loss: 14.5835 - val_mse: 14.5835 - val_mae: 1.5950 - lr: 0.0036 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 15.1181 - mse: 15.1181 - mae: 1.6007 - val_loss: 15.2901 - val_mse: 15.2901 - val_mae: 1.5479 - lr: 0.0036 - 2s/epoch - 2ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 2s - loss: 15.1258 - mse: 15.1258 - mae: 1.5880 - val_loss: 15.1974 - val_mse: 15.1974 - val_mae: 1.6187 - lr: 0.0036 - 2s/epoch - 2ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 2s - loss: 15.1419 - mse: 15.1419 - mae: 1.5906 - val_loss: 14.5883 - val_mse: 14.5883 - val_mae: 1.5803 - lr: 0.0036 - 2s/epoch - 2ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 2s - loss: 15.1238 - mse: 15.1238 - mae: 1.5869 - val_loss: 15.0778 - val_mse: 15.0778 - val_mae: 1.6472 - lr: 0.0036 - 2s/epoch - 2ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 2s - loss: 15.0519 - mse: 15.0519 - mae: 1.5865 - val_loss: 14.6872 - val_mse: 14.6872 - val_mae: 1.5444 - lr: 0.0036 - 2s/epoch - 2ms/step\n",
            "Score for fold 1: loss of 14.687255859375\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 15.5738 - mse: 15.5738 - mae: 1.5866 - val_loss: 12.7509 - val_mse: 12.7509 - val_mae: 1.5032 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 15.6270 - mse: 15.6270 - mae: 1.5876 - val_loss: 12.5533 - val_mse: 12.5533 - val_mae: 1.5322 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 15.5922 - mse: 15.5922 - mae: 1.5845 - val_loss: 12.5335 - val_mse: 12.5335 - val_mae: 1.5815 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 15.5894 - mse: 15.5894 - mae: 1.5885 - val_loss: 12.6399 - val_mse: 12.6399 - val_mae: 1.5673 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 15.5880 - mse: 15.5880 - mae: 1.5858 - val_loss: 12.7258 - val_mse: 12.7258 - val_mae: 1.5511 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 15.6594 - mse: 15.6594 - mae: 1.5873 - val_loss: 12.8966 - val_mse: 12.8966 - val_mae: 1.4962 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 15.6462 - mse: 15.6462 - mae: 1.5906 - val_loss: 12.7168 - val_mse: 12.7168 - val_mae: 1.5180 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 2s - loss: 15.6742 - mse: 15.6742 - mae: 1.5886 - val_loss: 12.6102 - val_mse: 12.6102 - val_mae: 1.5442 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Score for fold 2: loss of 12.610188484191895\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 15.0725 - mse: 15.0725 - mae: 1.5716 - val_loss: 15.1779 - val_mse: 15.1779 - val_mae: 1.5859 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 15.0054 - mse: 15.0054 - mae: 1.5698 - val_loss: 15.2479 - val_mse: 15.2479 - val_mae: 1.5870 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 14.9671 - mse: 14.9671 - mae: 1.5722 - val_loss: 14.9359 - val_mse: 14.9359 - val_mae: 1.6087 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 14.9893 - mse: 14.9893 - mae: 1.5694 - val_loss: 15.1611 - val_mse: 15.1611 - val_mae: 1.6215 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 15.0270 - mse: 15.0270 - mae: 1.5722 - val_loss: 15.0364 - val_mse: 15.0364 - val_mae: 1.6503 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 15.0936 - mse: 15.0936 - mae: 1.5749 - val_loss: 15.2414 - val_mse: 15.2414 - val_mae: 1.5832 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 14.9778 - mse: 14.9778 - mae: 1.5683 - val_loss: 15.3630 - val_mse: 15.3630 - val_mae: 1.5882 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 2s - loss: 15.0653 - mse: 15.0653 - mae: 1.5720 - val_loss: 15.3899 - val_mse: 15.3899 - val_mae: 1.5877 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Score for fold 3: loss of 15.389945030212402\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 14.0890 - mse: 14.0890 - mae: 1.5750 - val_loss: 18.9088 - val_mse: 18.9088 - val_mae: 1.5988 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 14.0015 - mse: 14.0015 - mae: 1.5756 - val_loss: 18.9633 - val_mse: 18.9633 - val_mae: 1.5812 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 14.1804 - mse: 14.1804 - mae: 1.5734 - val_loss: 18.7693 - val_mse: 18.7693 - val_mae: 1.6150 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 14.0701 - mse: 14.0701 - mae: 1.5771 - val_loss: 18.7183 - val_mse: 18.7183 - val_mae: 1.6434 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 14.0867 - mse: 14.0867 - mae: 1.5778 - val_loss: 18.7702 - val_mse: 18.7702 - val_mae: 1.6307 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 14.0758 - mse: 14.0758 - mae: 1.5714 - val_loss: 19.1243 - val_mse: 19.1243 - val_mae: 1.5920 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 14.1405 - mse: 14.1405 - mae: 1.5762 - val_loss: 18.7781 - val_mse: 18.7781 - val_mae: 1.6169 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 2s - loss: 14.1453 - mse: 14.1453 - mae: 1.5751 - val_loss: 18.9009 - val_mse: 18.9009 - val_mae: 1.5858 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 2s - loss: 14.1449 - mse: 14.1449 - mae: 1.5742 - val_loss: 19.1432 - val_mse: 19.1432 - val_mae: 1.5967 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Score for fold 4: loss of 19.1431941986084\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 15.5290 - mse: 15.5290 - mae: 1.5961 - val_loss: 13.6685 - val_mse: 13.6685 - val_mae: 1.5191 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 15.3148 - mse: 15.3148 - mae: 1.5907 - val_loss: 13.6616 - val_mse: 13.6616 - val_mae: 1.5381 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 15.4383 - mse: 15.4383 - mae: 1.5893 - val_loss: 14.1125 - val_mse: 14.1125 - val_mae: 1.5678 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 15.4956 - mse: 15.4956 - mae: 1.5927 - val_loss: 14.1474 - val_mse: 14.1474 - val_mae: 1.5575 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 15.5196 - mse: 15.5196 - mae: 1.5934 - val_loss: 14.1180 - val_mse: 14.1180 - val_mae: 1.5748 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 15.4945 - mse: 15.4945 - mae: 1.5950 - val_loss: 13.5837 - val_mse: 13.5837 - val_mae: 1.5457 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 15.4790 - mse: 15.4790 - mae: 1.5927 - val_loss: 13.5639 - val_mse: 13.5639 - val_mae: 1.5803 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 2s - loss: 15.4260 - mse: 15.4260 - mae: 1.5909 - val_loss: 14.1103 - val_mse: 14.1103 - val_mae: 1.5569 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 2s - loss: 15.4599 - mse: 15.4599 - mae: 1.5932 - val_loss: 13.4888 - val_mse: 13.4888 - val_mae: 1.5846 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 2s - loss: 15.3930 - mse: 15.3930 - mae: 1.5934 - val_loss: 13.6161 - val_mse: 13.6161 - val_mae: 1.5326 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 2s - loss: 15.4023 - mse: 15.4023 - mae: 1.5914 - val_loss: 13.6922 - val_mse: 13.6922 - val_mae: 1.5827 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 2s - loss: 15.3954 - mse: 15.3954 - mae: 1.5906 - val_loss: 13.5684 - val_mse: 13.5684 - val_mae: 1.5726 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 13/100\n",
            "1000/1000 - 2s - loss: 15.6705 - mse: 15.6705 - mae: 1.5884 - val_loss: 13.5515 - val_mse: 13.5515 - val_mae: 1.5328 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 14/100\n",
            "1000/1000 - 2s - loss: 15.5012 - mse: 15.5012 - mae: 1.5939 - val_loss: 13.5947 - val_mse: 13.5947 - val_mae: 1.5508 - lr: 0.0010 - 2s/epoch - 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-10-14 11:54:49,202]\u001b[0m Finished trial#13 resulted in value: 15.084. Current best value is 14.732 with parameters: {'activation': 'relu', 'num_hidden_layer': 3, 'i': 10, 'learning_rate': 0.001046125302603454}.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Score for fold 5: loss of 13.594669342041016\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 3s - loss: 15.5760 - mse: 15.5760 - mae: 1.6085 - val_loss: 15.0803 - val_mse: 15.0803 - val_mae: 1.6470 - lr: 9.3042e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 15.2402 - mse: 15.2402 - mae: 1.5902 - val_loss: 14.9791 - val_mse: 14.9791 - val_mae: 1.5966 - lr: 9.3042e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 15.1788 - mse: 15.1788 - mae: 1.5883 - val_loss: 14.8369 - val_mse: 14.8369 - val_mae: 1.6523 - lr: 9.3042e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 15.1636 - mse: 15.1636 - mae: 1.5870 - val_loss: 14.9191 - val_mse: 14.9191 - val_mae: 1.5620 - lr: 9.3042e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 15.1159 - mse: 15.1159 - mae: 1.5791 - val_loss: 14.8477 - val_mse: 14.8477 - val_mae: 1.5819 - lr: 9.3042e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 15.1039 - mse: 15.1039 - mae: 1.5834 - val_loss: 14.8572 - val_mse: 14.8572 - val_mae: 1.6357 - lr: 9.3042e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 15.0931 - mse: 15.0931 - mae: 1.5810 - val_loss: 14.8453 - val_mse: 14.8453 - val_mae: 1.5795 - lr: 9.3042e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 2s - loss: 15.0771 - mse: 15.0771 - mae: 1.5780 - val_loss: 14.8714 - val_mse: 14.8714 - val_mae: 1.5931 - lr: 9.3042e-04 - 2s/epoch - 2ms/step\n",
            "Score for fold 1: loss of 14.871413230895996\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 15.9686 - mse: 15.9686 - mae: 1.5933 - val_loss: 10.9318 - val_mse: 10.9318 - val_mae: 1.5722 - lr: 9.3042e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 15.9936 - mse: 15.9936 - mae: 1.5877 - val_loss: 10.8854 - val_mse: 10.8854 - val_mae: 1.6181 - lr: 9.3042e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 15.9710 - mse: 15.9710 - mae: 1.5908 - val_loss: 10.9641 - val_mse: 10.9641 - val_mae: 1.5516 - lr: 9.3042e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 15.9662 - mse: 15.9662 - mae: 1.5882 - val_loss: 10.9307 - val_mse: 10.9307 - val_mae: 1.5257 - lr: 9.3042e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 15.9829 - mse: 15.9829 - mae: 1.5855 - val_loss: 10.9976 - val_mse: 10.9976 - val_mae: 1.5537 - lr: 9.3042e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 15.9683 - mse: 15.9683 - mae: 1.5845 - val_loss: 10.8658 - val_mse: 10.8658 - val_mae: 1.5441 - lr: 9.3042e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 15.9581 - mse: 15.9581 - mae: 1.5854 - val_loss: 10.8477 - val_mse: 10.8477 - val_mae: 1.5951 - lr: 9.3042e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 2s - loss: 15.9283 - mse: 15.9283 - mae: 1.5858 - val_loss: 10.8749 - val_mse: 10.8749 - val_mae: 1.5807 - lr: 9.3042e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 2s - loss: 15.9239 - mse: 15.9239 - mae: 1.5841 - val_loss: 11.0523 - val_mse: 11.0523 - val_mae: 1.5060 - lr: 9.3042e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 2s - loss: 15.9512 - mse: 15.9512 - mae: 1.5845 - val_loss: 10.9377 - val_mse: 10.9377 - val_mae: 1.5304 - lr: 9.3042e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 2s - loss: 15.9145 - mse: 15.9145 - mae: 1.5810 - val_loss: 10.8966 - val_mse: 10.8966 - val_mae: 1.5847 - lr: 9.3042e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 2s - loss: 15.9149 - mse: 15.9149 - mae: 1.5834 - val_loss: 11.0360 - val_mse: 11.0360 - val_mae: 1.5684 - lr: 9.3042e-04 - 2s/epoch - 2ms/step\n",
            "Score for fold 2: loss of 11.035966873168945\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 15.6701 - mse: 15.6701 - mae: 1.5920 - val_loss: 11.9430 - val_mse: 11.9430 - val_mae: 1.5298 - lr: 9.3042e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 15.6380 - mse: 15.6380 - mae: 1.5885 - val_loss: 11.8722 - val_mse: 11.8722 - val_mae: 1.5252 - lr: 9.3042e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 15.6316 - mse: 15.6316 - mae: 1.5929 - val_loss: 11.8763 - val_mse: 11.8763 - val_mae: 1.5300 - lr: 9.3042e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 15.6280 - mse: 15.6280 - mae: 1.5926 - val_loss: 11.9525 - val_mse: 11.9525 - val_mae: 1.5101 - lr: 9.3042e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 15.6203 - mse: 15.6203 - mae: 1.5886 - val_loss: 11.9531 - val_mse: 11.9531 - val_mae: 1.6049 - lr: 9.3042e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 15.6107 - mse: 15.6107 - mae: 1.5899 - val_loss: 11.9919 - val_mse: 11.9919 - val_mae: 1.6301 - lr: 9.3042e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 15.5870 - mse: 15.5870 - mae: 1.5880 - val_loss: 11.9081 - val_mse: 11.9081 - val_mae: 1.5610 - lr: 9.3042e-04 - 2s/epoch - 2ms/step\n",
            "Score for fold 3: loss of 11.908129692077637\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 15.2953 - mse: 15.2953 - mae: 1.5832 - val_loss: 13.1831 - val_mse: 13.1831 - val_mae: 1.5306 - lr: 9.3042e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 15.2921 - mse: 15.2921 - mae: 1.5786 - val_loss: 13.2193 - val_mse: 13.2193 - val_mae: 1.6070 - lr: 9.3042e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 15.2938 - mse: 15.2938 - mae: 1.5768 - val_loss: 13.3095 - val_mse: 13.3095 - val_mae: 1.6024 - lr: 9.3042e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 15.2808 - mse: 15.2808 - mae: 1.5755 - val_loss: 13.1338 - val_mse: 13.1338 - val_mae: 1.5661 - lr: 9.3042e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 15.3001 - mse: 15.3001 - mae: 1.5783 - val_loss: 13.2626 - val_mse: 13.2626 - val_mae: 1.5525 - lr: 9.3042e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 15.2728 - mse: 15.2728 - mae: 1.5800 - val_loss: 13.1944 - val_mse: 13.1944 - val_mae: 1.5840 - lr: 9.3042e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 15.2353 - mse: 15.2353 - mae: 1.5761 - val_loss: 13.2737 - val_mse: 13.2737 - val_mae: 1.5850 - lr: 9.3042e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 2s - loss: 15.2661 - mse: 15.2661 - mae: 1.5751 - val_loss: 13.3433 - val_mse: 13.3433 - val_mae: 1.5921 - lr: 9.3042e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 2s - loss: 15.2478 - mse: 15.2478 - mae: 1.5803 - val_loss: 13.2959 - val_mse: 13.2959 - val_mae: 1.6010 - lr: 9.3042e-04 - 2s/epoch - 2ms/step\n",
            "Score for fold 4: loss of 13.295905113220215\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 12.6264 - mse: 12.6264 - mae: 1.5609 - val_loss: 23.7373 - val_mse: 23.7373 - val_mae: 1.6320 - lr: 9.3042e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 12.6137 - mse: 12.6137 - mae: 1.5639 - val_loss: 23.8435 - val_mse: 23.8435 - val_mae: 1.6446 - lr: 9.3042e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 12.6321 - mse: 12.6321 - mae: 1.5619 - val_loss: 23.9406 - val_mse: 23.9406 - val_mae: 1.5775 - lr: 9.3042e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 12.6113 - mse: 12.6113 - mae: 1.5635 - val_loss: 23.8178 - val_mse: 23.8178 - val_mae: 1.6253 - lr: 9.3042e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 12.5974 - mse: 12.5974 - mae: 1.5594 - val_loss: 23.7944 - val_mse: 23.7944 - val_mae: 1.6276 - lr: 9.3042e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 12.5815 - mse: 12.5815 - mae: 1.5604 - val_loss: 23.8098 - val_mse: 23.8098 - val_mae: 1.7275 - lr: 9.3042e-04 - 2s/epoch - 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-10-14 11:56:17,067]\u001b[0m Finished trial#14 resulted in value: 14.985999999999999. Current best value is 14.732 with parameters: {'activation': 'relu', 'num_hidden_layer': 3, 'i': 10, 'learning_rate': 0.001046125302603454}.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Score for fold 5: loss of 23.80983543395996\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 6s - loss: 15.7637 - mse: 15.7637 - mae: 1.6099 - val_loss: 14.1273 - val_mse: 14.1273 - val_mae: 1.5873 - lr: 3.3701e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 5s - loss: 15.3594 - mse: 15.3594 - mae: 1.5941 - val_loss: 14.0490 - val_mse: 14.0490 - val_mae: 1.5311 - lr: 3.3701e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 5s - loss: 15.3283 - mse: 15.3283 - mae: 1.5920 - val_loss: 13.8840 - val_mse: 13.8840 - val_mae: 1.6218 - lr: 3.3701e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 5s - loss: 15.2748 - mse: 15.2748 - mae: 1.5897 - val_loss: 13.8283 - val_mse: 13.8283 - val_mae: 1.6040 - lr: 3.3701e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 5s - loss: 15.1807 - mse: 15.1807 - mae: 1.5845 - val_loss: 13.7403 - val_mse: 13.7403 - val_mae: 1.6208 - lr: 3.3701e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 5s - loss: 15.2399 - mse: 15.2399 - mae: 1.5822 - val_loss: 13.8453 - val_mse: 13.8453 - val_mae: 1.6033 - lr: 3.3701e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 5s - loss: 15.1745 - mse: 15.1745 - mae: 1.5804 - val_loss: 13.9808 - val_mse: 13.9808 - val_mae: 1.5682 - lr: 3.3701e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 5s - loss: 15.1827 - mse: 15.1827 - mae: 1.5801 - val_loss: 13.9139 - val_mse: 13.9139 - val_mae: 1.5936 - lr: 3.3701e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 5s - loss: 15.0486 - mse: 15.0486 - mae: 1.5752 - val_loss: 13.7550 - val_mse: 13.7550 - val_mae: 1.5214 - lr: 3.3701e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 5s - loss: 15.0153 - mse: 15.0153 - mae: 1.5751 - val_loss: 13.5925 - val_mse: 13.5925 - val_mae: 1.5792 - lr: 3.3701e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 5s - loss: 15.0413 - mse: 15.0413 - mae: 1.5682 - val_loss: 13.7642 - val_mse: 13.7642 - val_mae: 1.5641 - lr: 3.3701e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 5s - loss: 15.0308 - mse: 15.0308 - mae: 1.5743 - val_loss: 14.0925 - val_mse: 14.0925 - val_mae: 1.5046 - lr: 3.3701e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 13/100\n",
            "1000/1000 - 5s - loss: 14.8798 - mse: 14.8798 - mae: 1.5702 - val_loss: 13.7473 - val_mse: 13.7473 - val_mae: 1.6174 - lr: 3.3701e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 14/100\n",
            "1000/1000 - 5s - loss: 14.7633 - mse: 14.7633 - mae: 1.5609 - val_loss: 13.5719 - val_mse: 13.5719 - val_mae: 1.7356 - lr: 3.3701e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 15/100\n",
            "1000/1000 - 5s - loss: 14.8096 - mse: 14.8096 - mae: 1.5665 - val_loss: 13.9691 - val_mse: 13.9691 - val_mae: 1.7194 - lr: 3.3701e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 16/100\n",
            "1000/1000 - 5s - loss: 14.7355 - mse: 14.7355 - mae: 1.5643 - val_loss: 13.5500 - val_mse: 13.5500 - val_mae: 1.6550 - lr: 3.3701e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 17/100\n",
            "1000/1000 - 5s - loss: 14.7422 - mse: 14.7422 - mae: 1.5661 - val_loss: 13.8647 - val_mse: 13.8647 - val_mae: 1.5217 - lr: 3.3701e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 18/100\n",
            "1000/1000 - 5s - loss: 14.6841 - mse: 14.6841 - mae: 1.5682 - val_loss: 13.7099 - val_mse: 13.7099 - val_mae: 1.7079 - lr: 3.3701e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 19/100\n",
            "1000/1000 - 5s - loss: 14.5045 - mse: 14.5045 - mae: 1.5677 - val_loss: 13.7822 - val_mse: 13.7822 - val_mae: 1.5601 - lr: 3.3701e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 20/100\n",
            "1000/1000 - 5s - loss: 14.5387 - mse: 14.5387 - mae: 1.5803 - val_loss: 13.6656 - val_mse: 13.6656 - val_mae: 1.5355 - lr: 3.3701e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 21/100\n",
            "1000/1000 - 5s - loss: 14.4789 - mse: 14.4789 - mae: 1.5719 - val_loss: 13.8604 - val_mse: 13.8604 - val_mae: 1.5709 - lr: 3.3701e-04 - 5s/epoch - 5ms/step\n",
            "Score for fold 1: loss of 13.86044979095459\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 5s - loss: 15.5206 - mse: 15.5206 - mae: 1.6002 - val_loss: 10.5422 - val_mse: 10.5422 - val_mae: 1.4723 - lr: 3.3701e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 5s - loss: 15.3369 - mse: 15.3369 - mae: 1.5998 - val_loss: 10.9681 - val_mse: 10.9681 - val_mae: 1.4510 - lr: 3.3701e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 5s - loss: 15.1002 - mse: 15.1002 - mae: 1.6023 - val_loss: 10.6855 - val_mse: 10.6855 - val_mae: 1.6203 - lr: 3.3701e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 5s - loss: 15.0822 - mse: 15.0822 - mae: 1.5953 - val_loss: 10.8492 - val_mse: 10.8492 - val_mae: 1.8336 - lr: 3.3701e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 5s - loss: 15.1537 - mse: 15.1537 - mae: 1.5866 - val_loss: 10.5494 - val_mse: 10.5494 - val_mae: 1.5341 - lr: 3.3701e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 5s - loss: 14.9922 - mse: 14.9922 - mae: 1.6085 - val_loss: 11.5321 - val_mse: 11.5321 - val_mae: 1.6019 - lr: 3.3701e-04 - 5s/epoch - 5ms/step\n",
            "Score for fold 2: loss of 11.532099723815918\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 5s - loss: 11.9653 - mse: 11.9653 - mae: 1.6171 - val_loss: 22.8298 - val_mse: 22.8298 - val_mae: 1.4916 - lr: 3.3701e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 5s - loss: 11.8219 - mse: 11.8219 - mae: 1.6077 - val_loss: 22.6524 - val_mse: 22.6524 - val_mae: 1.7920 - lr: 3.3701e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 5s - loss: 11.7117 - mse: 11.7117 - mae: 1.5890 - val_loss: 22.6707 - val_mse: 22.6707 - val_mae: 1.7369 - lr: 3.3701e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 5s - loss: 11.7927 - mse: 11.7927 - mae: 1.6173 - val_loss: 23.1140 - val_mse: 23.1140 - val_mae: 1.5288 - lr: 3.3701e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 5s - loss: 11.5216 - mse: 11.5216 - mae: 1.5989 - val_loss: 24.1988 - val_mse: 24.1988 - val_mae: 1.5942 - lr: 3.3701e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 5s - loss: 11.7270 - mse: 11.7270 - mae: 1.6065 - val_loss: 22.8537 - val_mse: 22.8537 - val_mae: 1.5862 - lr: 3.3701e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 5s - loss: 11.7226 - mse: 11.7226 - mae: 1.6195 - val_loss: 23.1492 - val_mse: 23.1492 - val_mae: 1.8034 - lr: 3.3701e-04 - 5s/epoch - 5ms/step\n",
            "Score for fold 3: loss of 23.14922523498535\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 5s - loss: 14.8011 - mse: 14.8011 - mae: 1.6419 - val_loss: 10.1671 - val_mse: 10.1671 - val_mae: 1.5989 - lr: 3.3701e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 5s - loss: 14.8393 - mse: 14.8393 - mae: 1.6587 - val_loss: 10.4027 - val_mse: 10.4027 - val_mae: 1.4732 - lr: 3.3701e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 5s - loss: 14.7153 - mse: 14.7153 - mae: 1.6500 - val_loss: 10.0242 - val_mse: 10.0242 - val_mae: 1.4904 - lr: 3.3701e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 5s - loss: 14.6708 - mse: 14.6708 - mae: 1.6526 - val_loss: 10.5707 - val_mse: 10.5707 - val_mae: 1.8031 - lr: 3.3701e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 5s - loss: 14.6375 - mse: 14.6375 - mae: 1.6478 - val_loss: 10.6315 - val_mse: 10.6315 - val_mae: 1.8561 - lr: 3.3701e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 5s - loss: 14.6967 - mse: 14.6967 - mae: 1.6715 - val_loss: 10.3442 - val_mse: 10.3442 - val_mae: 1.4689 - lr: 3.3701e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 5s - loss: 14.7941 - mse: 14.7941 - mae: 1.6792 - val_loss: 12.2871 - val_mse: 12.2871 - val_mae: 2.1938 - lr: 3.3701e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 5s - loss: 14.4643 - mse: 14.4643 - mae: 1.6168 - val_loss: 10.4428 - val_mse: 10.4428 - val_mae: 1.6713 - lr: 3.3701e-04 - 5s/epoch - 5ms/step\n",
            "Score for fold 4: loss of 10.442776679992676\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 5s - loss: 14.1519 - mse: 14.1519 - mae: 1.6552 - val_loss: 13.4243 - val_mse: 13.4243 - val_mae: 1.5781 - lr: 3.3701e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 5s - loss: 14.0546 - mse: 14.0546 - mae: 1.6578 - val_loss: 12.9421 - val_mse: 12.9421 - val_mae: 1.8976 - lr: 3.3701e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 5s - loss: 13.8780 - mse: 13.8780 - mae: 1.6440 - val_loss: 13.5569 - val_mse: 13.5569 - val_mae: 1.4564 - lr: 3.3701e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 5s - loss: 13.8696 - mse: 13.8696 - mae: 1.6257 - val_loss: 13.5471 - val_mse: 13.5471 - val_mae: 1.4760 - lr: 3.3701e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 5s - loss: 13.7104 - mse: 13.7104 - mae: 1.5997 - val_loss: 12.2775 - val_mse: 12.2775 - val_mae: 1.6176 - lr: 3.3701e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 5s - loss: 13.6575 - mse: 13.6575 - mae: 1.6013 - val_loss: 14.8484 - val_mse: 14.8484 - val_mae: 1.6113 - lr: 3.3701e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 5s - loss: 13.5697 - mse: 13.5697 - mae: 1.5974 - val_loss: 12.8765 - val_mse: 12.8765 - val_mae: 1.5621 - lr: 3.3701e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 5s - loss: 13.3573 - mse: 13.3573 - mae: 1.5940 - val_loss: 13.1124 - val_mse: 13.1124 - val_mae: 1.4641 - lr: 3.3701e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 5s - loss: 13.2219 - mse: 13.2219 - mae: 1.5937 - val_loss: 13.1990 - val_mse: 13.1990 - val_mae: 1.6758 - lr: 3.3701e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 5s - loss: 13.3398 - mse: 13.3398 - mae: 1.5957 - val_loss: 13.6400 - val_mse: 13.6400 - val_mae: 1.7564 - lr: 3.3701e-04 - 5s/epoch - 5ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-10-14 12:00:41,828]\u001b[0m Finished trial#15 resulted in value: 14.524000000000001. Current best value is 14.524000000000001 with parameters: {'activation': 'relu', 'num_hidden_layer': 4, 'i': 9, 'learning_rate': 0.00033700551736380043}.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Score for fold 5: loss of 13.640035629272461\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 6s - loss: 16.0916 - mse: 16.0916 - mae: 1.6082 - val_loss: 12.6586 - val_mse: 12.6586 - val_mae: 1.6673 - lr: 3.1743e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 5s - loss: 15.7608 - mse: 15.7608 - mae: 1.5876 - val_loss: 12.7302 - val_mse: 12.7302 - val_mae: 1.5865 - lr: 3.1743e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 5s - loss: 15.6666 - mse: 15.6666 - mae: 1.5922 - val_loss: 12.9286 - val_mse: 12.9286 - val_mae: 1.5802 - lr: 3.1743e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 5s - loss: 15.6262 - mse: 15.6262 - mae: 1.5811 - val_loss: 12.7641 - val_mse: 12.7641 - val_mae: 1.6279 - lr: 3.1743e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 5s - loss: 15.5627 - mse: 15.5627 - mae: 1.5779 - val_loss: 12.5689 - val_mse: 12.5689 - val_mae: 1.6352 - lr: 3.1743e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 5s - loss: 15.5435 - mse: 15.5435 - mae: 1.5782 - val_loss: 12.5837 - val_mse: 12.5837 - val_mae: 1.5914 - lr: 3.1743e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 5s - loss: 15.4672 - mse: 15.4672 - mae: 1.5695 - val_loss: 12.7402 - val_mse: 12.7402 - val_mae: 1.5866 - lr: 3.1743e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 5s - loss: 15.3769 - mse: 15.3769 - mae: 1.5688 - val_loss: 12.5965 - val_mse: 12.5965 - val_mae: 1.5920 - lr: 3.1743e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 5s - loss: 15.3873 - mse: 15.3873 - mae: 1.5716 - val_loss: 12.5422 - val_mse: 12.5422 - val_mae: 1.5476 - lr: 3.1743e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 5s - loss: 15.3420 - mse: 15.3420 - mae: 1.5684 - val_loss: 12.6061 - val_mse: 12.6061 - val_mae: 1.7046 - lr: 3.1743e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 5s - loss: 15.2528 - mse: 15.2528 - mae: 1.5660 - val_loss: 12.5264 - val_mse: 12.5264 - val_mae: 1.6410 - lr: 3.1743e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 5s - loss: 15.2395 - mse: 15.2395 - mae: 1.5653 - val_loss: 12.6043 - val_mse: 12.6043 - val_mae: 1.5799 - lr: 3.1743e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 13/100\n",
            "1000/1000 - 5s - loss: 15.2824 - mse: 15.2824 - mae: 1.5611 - val_loss: 12.6402 - val_mse: 12.6402 - val_mae: 1.6098 - lr: 3.1743e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 14/100\n",
            "1000/1000 - 5s - loss: 15.2134 - mse: 15.2134 - mae: 1.5633 - val_loss: 12.8200 - val_mse: 12.8200 - val_mae: 1.6069 - lr: 3.1743e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 15/100\n",
            "1000/1000 - 5s - loss: 15.1477 - mse: 15.1477 - mae: 1.5611 - val_loss: 12.8131 - val_mse: 12.8131 - val_mae: 1.5843 - lr: 3.1743e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 16/100\n",
            "1000/1000 - 5s - loss: 15.1344 - mse: 15.1344 - mae: 1.5592 - val_loss: 12.6976 - val_mse: 12.6976 - val_mae: 1.6822 - lr: 3.1743e-04 - 5s/epoch - 5ms/step\n",
            "Score for fold 1: loss of 12.697653770446777\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 5s - loss: 14.1190 - mse: 14.1190 - mae: 1.5934 - val_loss: 16.3110 - val_mse: 16.3110 - val_mae: 1.6581 - lr: 3.1743e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 5s - loss: 14.0084 - mse: 14.0084 - mae: 1.5851 - val_loss: 16.3653 - val_mse: 16.3653 - val_mae: 1.4738 - lr: 3.1743e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 5s - loss: 13.9967 - mse: 13.9967 - mae: 1.5810 - val_loss: 16.4958 - val_mse: 16.4958 - val_mae: 1.4388 - lr: 3.1743e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 5s - loss: 13.7629 - mse: 13.7629 - mae: 1.5838 - val_loss: 16.5641 - val_mse: 16.5641 - val_mae: 1.4510 - lr: 3.1743e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 5s - loss: 13.7688 - mse: 13.7688 - mae: 1.5737 - val_loss: 16.4209 - val_mse: 16.4209 - val_mae: 1.4742 - lr: 3.1743e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 5s - loss: 13.6753 - mse: 13.6753 - mae: 1.5754 - val_loss: 16.7047 - val_mse: 16.7047 - val_mae: 1.4611 - lr: 3.1743e-04 - 5s/epoch - 5ms/step\n",
            "Score for fold 2: loss of 16.704675674438477\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 5s - loss: 13.7166 - mse: 13.7166 - mae: 1.5642 - val_loss: 16.6038 - val_mse: 16.6038 - val_mae: 1.5308 - lr: 3.1743e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 5s - loss: 13.4770 - mse: 13.4770 - mae: 1.5586 - val_loss: 16.5553 - val_mse: 16.5553 - val_mae: 1.5724 - lr: 3.1743e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 5s - loss: 13.4375 - mse: 13.4375 - mae: 1.5511 - val_loss: 16.7110 - val_mse: 16.7110 - val_mae: 1.7374 - lr: 3.1743e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 5s - loss: 13.3876 - mse: 13.3876 - mae: 1.5609 - val_loss: 16.8289 - val_mse: 16.8289 - val_mae: 1.5945 - lr: 3.1743e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 5s - loss: 13.5625 - mse: 13.5625 - mae: 1.5557 - val_loss: 17.3351 - val_mse: 17.3351 - val_mae: 1.4890 - lr: 3.1743e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 5s - loss: 13.5362 - mse: 13.5362 - mae: 1.5658 - val_loss: 17.0767 - val_mse: 17.0767 - val_mae: 1.5044 - lr: 3.1743e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 5s - loss: 13.3268 - mse: 13.3268 - mae: 1.5523 - val_loss: 16.9494 - val_mse: 16.9494 - val_mae: 1.8444 - lr: 3.1743e-04 - 5s/epoch - 5ms/step\n",
            "Score for fold 3: loss of 16.94938850402832\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 5s - loss: 14.7740 - mse: 14.7740 - mae: 1.5948 - val_loss: 11.2581 - val_mse: 11.2581 - val_mae: 1.5654 - lr: 3.1743e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 5s - loss: 14.7549 - mse: 14.7549 - mae: 1.5961 - val_loss: 11.7528 - val_mse: 11.7528 - val_mae: 1.4640 - lr: 3.1743e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 5s - loss: 14.7579 - mse: 14.7579 - mae: 1.5905 - val_loss: 11.5139 - val_mse: 11.5139 - val_mae: 1.6139 - lr: 3.1743e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 5s - loss: 14.5995 - mse: 14.5995 - mae: 1.5950 - val_loss: 11.7804 - val_mse: 11.7804 - val_mae: 1.7292 - lr: 3.1743e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 5s - loss: 14.4802 - mse: 14.4802 - mae: 1.5794 - val_loss: 11.9399 - val_mse: 11.9399 - val_mae: 1.4974 - lr: 3.1743e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 5s - loss: 14.4263 - mse: 14.4263 - mae: 1.5894 - val_loss: 12.2571 - val_mse: 12.2571 - val_mae: 1.7032 - lr: 3.1743e-04 - 5s/epoch - 5ms/step\n",
            "Score for fold 4: loss of 12.257063865661621\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 5s - loss: 13.7662 - mse: 13.7662 - mae: 1.5862 - val_loss: 14.1835 - val_mse: 14.1835 - val_mae: 1.6620 - lr: 3.1743e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 5s - loss: 13.7378 - mse: 13.7378 - mae: 1.6018 - val_loss: 15.3773 - val_mse: 15.3773 - val_mae: 1.4563 - lr: 3.1743e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 5s - loss: 13.7260 - mse: 13.7260 - mae: 1.5950 - val_loss: 14.9821 - val_mse: 14.9821 - val_mae: 1.4532 - lr: 3.1743e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 5s - loss: 13.5061 - mse: 13.5061 - mae: 1.6050 - val_loss: 15.1325 - val_mse: 15.1325 - val_mae: 1.7128 - lr: 3.1743e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 5s - loss: 13.4936 - mse: 13.4936 - mae: 1.5925 - val_loss: 15.1422 - val_mse: 15.1422 - val_mae: 1.4662 - lr: 3.1743e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 5s - loss: 13.5936 - mse: 13.5936 - mae: 1.5934 - val_loss: 15.7166 - val_mse: 15.7166 - val_mae: 2.1056 - lr: 3.1743e-04 - 5s/epoch - 5ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-10-14 12:04:14,148]\u001b[0m Finished trial#16 resulted in value: 14.866. Current best value is 14.524000000000001 with parameters: {'activation': 'relu', 'num_hidden_layer': 4, 'i': 9, 'learning_rate': 0.00033700551736380043}.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Score for fold 5: loss of 15.716573715209961\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 6s - loss: 15.3646 - mse: 15.3646 - mae: 1.6136 - val_loss: 15.4233 - val_mse: 15.4233 - val_mae: 1.6780 - lr: 3.3188e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 5s - loss: 15.0302 - mse: 15.0302 - mae: 1.5973 - val_loss: 15.5864 - val_mse: 15.5864 - val_mae: 1.5519 - lr: 3.3188e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 5s - loss: 14.9150 - mse: 14.9150 - mae: 1.5992 - val_loss: 15.5883 - val_mse: 15.5883 - val_mae: 1.5607 - lr: 3.3188e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 5s - loss: 14.8948 - mse: 14.8948 - mae: 1.5960 - val_loss: 15.3825 - val_mse: 15.3825 - val_mae: 1.6608 - lr: 3.3188e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 5s - loss: 14.8439 - mse: 14.8439 - mae: 1.5889 - val_loss: 15.6704 - val_mse: 15.6704 - val_mae: 1.6229 - lr: 3.3188e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 5s - loss: 14.7879 - mse: 14.7879 - mae: 1.5831 - val_loss: 15.2167 - val_mse: 15.2167 - val_mae: 1.5978 - lr: 3.3188e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 5s - loss: 14.7159 - mse: 14.7159 - mae: 1.5774 - val_loss: 15.4104 - val_mse: 15.4104 - val_mae: 1.5755 - lr: 3.3188e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 5s - loss: 14.7171 - mse: 14.7171 - mae: 1.5790 - val_loss: 16.0417 - val_mse: 16.0417 - val_mae: 1.5305 - lr: 3.3188e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 5s - loss: 14.6506 - mse: 14.6506 - mae: 1.5796 - val_loss: 15.8438 - val_mse: 15.8438 - val_mae: 1.5875 - lr: 3.3188e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 5s - loss: 14.6631 - mse: 14.6631 - mae: 1.5794 - val_loss: 15.9672 - val_mse: 15.9672 - val_mae: 1.5147 - lr: 3.3188e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 5s - loss: 14.5566 - mse: 14.5566 - mae: 1.5822 - val_loss: 15.4763 - val_mse: 15.4763 - val_mae: 1.6116 - lr: 3.3188e-04 - 5s/epoch - 5ms/step\n",
            "Score for fold 1: loss of 15.476346969604492\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 5s - loss: 14.6919 - mse: 14.6919 - mae: 1.5794 - val_loss: 15.0493 - val_mse: 15.0493 - val_mae: 1.7137 - lr: 3.3188e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 5s - loss: 14.6788 - mse: 14.6788 - mae: 1.5748 - val_loss: 15.1874 - val_mse: 15.1874 - val_mae: 1.6367 - lr: 3.3188e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 5s - loss: 14.6012 - mse: 14.6012 - mae: 1.5753 - val_loss: 14.9563 - val_mse: 14.9563 - val_mae: 1.5825 - lr: 3.3188e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 5s - loss: 14.5376 - mse: 14.5376 - mae: 1.5732 - val_loss: 15.1259 - val_mse: 15.1259 - val_mae: 1.5986 - lr: 3.3188e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 5s - loss: 14.3679 - mse: 14.3679 - mae: 1.5661 - val_loss: 15.0513 - val_mse: 15.0513 - val_mae: 1.5836 - lr: 3.3188e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 5s - loss: 14.3890 - mse: 14.3890 - mae: 1.5648 - val_loss: 15.3094 - val_mse: 15.3094 - val_mae: 1.6769 - lr: 3.3188e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 5s - loss: 14.4017 - mse: 14.4017 - mae: 1.5746 - val_loss: 15.3982 - val_mse: 15.3982 - val_mae: 1.5340 - lr: 3.3188e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 5s - loss: 14.3714 - mse: 14.3714 - mae: 1.5776 - val_loss: 15.4073 - val_mse: 15.4073 - val_mae: 1.5698 - lr: 3.3188e-04 - 5s/epoch - 5ms/step\n",
            "Score for fold 2: loss of 15.407257080078125\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 5s - loss: 15.2938 - mse: 15.2938 - mae: 1.6022 - val_loss: 11.8131 - val_mse: 11.8131 - val_mae: 1.4834 - lr: 3.3188e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 5s - loss: 15.0947 - mse: 15.0947 - mae: 1.5940 - val_loss: 11.6468 - val_mse: 11.6468 - val_mae: 1.4901 - lr: 3.3188e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 5s - loss: 14.9411 - mse: 14.9411 - mae: 1.5939 - val_loss: 11.9758 - val_mse: 11.9758 - val_mae: 1.4979 - lr: 3.3188e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 5s - loss: 14.9712 - mse: 14.9712 - mae: 1.5908 - val_loss: 11.8476 - val_mse: 11.8476 - val_mae: 1.6183 - lr: 3.3188e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 5s - loss: 14.9428 - mse: 14.9428 - mae: 1.5875 - val_loss: 11.9564 - val_mse: 11.9564 - val_mae: 1.7770 - lr: 3.3188e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 5s - loss: 14.9096 - mse: 14.9096 - mae: 1.5872 - val_loss: 11.7518 - val_mse: 11.7518 - val_mae: 1.6158 - lr: 3.3188e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 5s - loss: 14.7677 - mse: 14.7677 - mae: 1.5961 - val_loss: 11.9958 - val_mse: 11.9958 - val_mae: 1.7779 - lr: 3.3188e-04 - 5s/epoch - 5ms/step\n",
            "Score for fold 3: loss of 11.995793342590332\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 5s - loss: 15.1203 - mse: 15.1203 - mae: 1.6150 - val_loss: 10.6092 - val_mse: 10.6092 - val_mae: 1.4268 - lr: 3.3188e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 5s - loss: 15.0789 - mse: 15.0789 - mae: 1.6021 - val_loss: 10.4698 - val_mse: 10.4698 - val_mae: 1.6344 - lr: 3.3188e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 5s - loss: 14.9312 - mse: 14.9312 - mae: 1.5981 - val_loss: 11.3543 - val_mse: 11.3543 - val_mae: 1.5730 - lr: 3.3188e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 5s - loss: 14.9308 - mse: 14.9308 - mae: 1.6135 - val_loss: 11.1586 - val_mse: 11.1586 - val_mae: 1.4989 - lr: 3.3188e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 5s - loss: 14.9559 - mse: 14.9559 - mae: 1.6220 - val_loss: 11.0716 - val_mse: 11.0716 - val_mae: 1.7967 - lr: 3.3188e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 5s - loss: 14.8905 - mse: 14.8905 - mae: 1.6369 - val_loss: 11.1927 - val_mse: 11.1927 - val_mae: 1.5649 - lr: 3.3188e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 5s - loss: 15.0675 - mse: 15.0675 - mae: 1.6752 - val_loss: 11.2114 - val_mse: 11.2114 - val_mae: 1.5147 - lr: 3.3188e-04 - 5s/epoch - 5ms/step\n",
            "Score for fold 4: loss of 11.211419105529785\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 5s - loss: 12.9006 - mse: 12.9006 - mae: 1.6710 - val_loss: 19.4840 - val_mse: 19.4840 - val_mae: 1.4807 - lr: 3.3188e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 5s - loss: 12.9446 - mse: 12.9446 - mae: 1.6503 - val_loss: 19.3589 - val_mse: 19.3589 - val_mae: 1.7227 - lr: 3.3188e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 5s - loss: 12.8395 - mse: 12.8395 - mae: 1.6842 - val_loss: 19.8726 - val_mse: 19.8726 - val_mae: 1.9342 - lr: 3.3188e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 5s - loss: 12.8105 - mse: 12.8105 - mae: 1.6951 - val_loss: 19.8805 - val_mse: 19.8805 - val_mae: 1.4807 - lr: 3.3188e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 5s - loss: 12.5924 - mse: 12.5924 - mae: 1.6657 - val_loss: 21.4900 - val_mse: 21.4900 - val_mae: 1.8157 - lr: 3.3188e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 5s - loss: 12.5923 - mse: 12.5923 - mae: 1.6621 - val_loss: 19.5688 - val_mse: 19.5688 - val_mae: 1.5181 - lr: 3.3188e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 5s - loss: 12.6587 - mse: 12.6587 - mae: 1.6772 - val_loss: 19.9203 - val_mse: 19.9203 - val_mae: 1.9429 - lr: 3.3188e-04 - 5s/epoch - 5ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-10-14 12:07:43,128]\u001b[0m Finished trial#17 resulted in value: 14.804000000000002. Current best value is 14.524000000000001 with parameters: {'activation': 'relu', 'num_hidden_layer': 4, 'i': 9, 'learning_rate': 0.00033700551736380043}.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Score for fold 5: loss of 19.920251846313477\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 11s - loss: 14.3477 - mse: 14.3477 - mae: 1.6180 - val_loss: 19.7160 - val_mse: 19.7160 - val_mae: 1.5669 - lr: 4.7879e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 10s - loss: 14.0023 - mse: 14.0023 - mae: 1.5959 - val_loss: 19.5871 - val_mse: 19.5871 - val_mae: 1.6805 - lr: 4.7879e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 10s - loss: 14.0172 - mse: 14.0172 - mae: 1.5921 - val_loss: 19.3064 - val_mse: 19.3064 - val_mae: 1.6480 - lr: 4.7879e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 10s - loss: 13.9493 - mse: 13.9493 - mae: 1.5828 - val_loss: 19.4035 - val_mse: 19.4035 - val_mae: 1.6121 - lr: 4.7879e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 10s - loss: 13.9449 - mse: 13.9449 - mae: 1.5788 - val_loss: 19.6493 - val_mse: 19.6493 - val_mae: 1.5397 - lr: 4.7879e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 10s - loss: 14.0048 - mse: 14.0048 - mae: 1.5776 - val_loss: 19.2815 - val_mse: 19.2815 - val_mae: 1.6151 - lr: 4.7879e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 10s - loss: 13.9605 - mse: 13.9605 - mae: 1.5820 - val_loss: 19.3222 - val_mse: 19.3222 - val_mae: 1.5569 - lr: 4.7879e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 10s - loss: 13.8215 - mse: 13.8215 - mae: 1.5812 - val_loss: 19.2325 - val_mse: 19.2325 - val_mae: 1.5771 - lr: 4.7879e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 10s - loss: 13.8174 - mse: 13.8174 - mae: 1.5783 - val_loss: 19.3354 - val_mse: 19.3354 - val_mae: 1.5786 - lr: 4.7879e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 10s - loss: 13.7141 - mse: 13.7141 - mae: 1.5725 - val_loss: 19.2887 - val_mse: 19.2887 - val_mae: 1.6280 - lr: 4.7879e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 11s - loss: 13.8011 - mse: 13.8011 - mae: 1.5885 - val_loss: 19.2937 - val_mse: 19.2937 - val_mae: 1.7197 - lr: 4.7879e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 10s - loss: 13.7004 - mse: 13.7004 - mae: 1.5849 - val_loss: 19.1741 - val_mse: 19.1741 - val_mae: 1.7025 - lr: 4.7879e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 13/100\n",
            "1000/1000 - 10s - loss: 13.6275 - mse: 13.6275 - mae: 1.5828 - val_loss: 19.3886 - val_mse: 19.3886 - val_mae: 1.6049 - lr: 4.7879e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 14/100\n",
            "1000/1000 - 10s - loss: 13.6760 - mse: 13.6760 - mae: 1.5913 - val_loss: 19.4658 - val_mse: 19.4658 - val_mae: 1.5606 - lr: 4.7879e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 15/100\n",
            "1000/1000 - 10s - loss: 13.6411 - mse: 13.6411 - mae: 1.5893 - val_loss: 19.2838 - val_mse: 19.2838 - val_mae: 1.5451 - lr: 4.7879e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 16/100\n",
            "1000/1000 - 10s - loss: 13.4961 - mse: 13.4961 - mae: 1.5856 - val_loss: 19.3566 - val_mse: 19.3566 - val_mae: 1.5652 - lr: 4.7879e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 17/100\n",
            "1000/1000 - 10s - loss: 13.4768 - mse: 13.4768 - mae: 1.5731 - val_loss: 19.4200 - val_mse: 19.4200 - val_mae: 1.8554 - lr: 4.7879e-04 - 10s/epoch - 10ms/step\n",
            "Score for fold 1: loss of 19.42002296447754\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 10s - loss: 15.8446 - mse: 15.8446 - mae: 1.6021 - val_loss: 10.9339 - val_mse: 10.9339 - val_mae: 1.5259 - lr: 4.7879e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 10s - loss: 15.4739 - mse: 15.4739 - mae: 1.5901 - val_loss: 10.6552 - val_mse: 10.6552 - val_mae: 1.4828 - lr: 4.7879e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 10s - loss: 15.6276 - mse: 15.6276 - mae: 1.5879 - val_loss: 11.0243 - val_mse: 11.0243 - val_mae: 1.5010 - lr: 4.7879e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 11s - loss: 15.7732 - mse: 15.7732 - mae: 1.5930 - val_loss: 10.8443 - val_mse: 10.8443 - val_mae: 1.5905 - lr: 4.7879e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 11s - loss: 15.5094 - mse: 15.5094 - mae: 1.5921 - val_loss: 11.2930 - val_mse: 11.2930 - val_mae: 1.4826 - lr: 4.7879e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 11s - loss: 15.5586 - mse: 15.5586 - mae: 1.5967 - val_loss: 10.8880 - val_mse: 10.8880 - val_mae: 1.5656 - lr: 4.7879e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 11s - loss: 15.4597 - mse: 15.4597 - mae: 1.5933 - val_loss: 10.8261 - val_mse: 10.8261 - val_mae: 1.5801 - lr: 4.7879e-04 - 11s/epoch - 11ms/step\n",
            "Score for fold 2: loss of 10.826096534729004\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 11s - loss: 14.6954 - mse: 14.6954 - mae: 1.6079 - val_loss: 14.0539 - val_mse: 14.0539 - val_mae: 1.6350 - lr: 4.7879e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 11s - loss: 14.6316 - mse: 14.6316 - mae: 1.5976 - val_loss: 14.5181 - val_mse: 14.5181 - val_mae: 1.4834 - lr: 4.7879e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 11s - loss: 14.6271 - mse: 14.6271 - mae: 1.6043 - val_loss: 13.8571 - val_mse: 13.8571 - val_mae: 1.4550 - lr: 4.7879e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 10s - loss: 14.7320 - mse: 14.7320 - mae: 1.5939 - val_loss: 13.7169 - val_mse: 13.7169 - val_mae: 1.5415 - lr: 4.7879e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 10s - loss: 14.5793 - mse: 14.5793 - mae: 1.6036 - val_loss: 13.8752 - val_mse: 13.8752 - val_mae: 1.5994 - lr: 4.7879e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 10s - loss: 14.3953 - mse: 14.3953 - mae: 1.6031 - val_loss: 14.3004 - val_mse: 14.3004 - val_mae: 1.4817 - lr: 4.7879e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 10s - loss: 14.4605 - mse: 14.4605 - mae: 1.5992 - val_loss: 14.2389 - val_mse: 14.2389 - val_mae: 1.5289 - lr: 4.7879e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 10s - loss: 14.6468 - mse: 14.6468 - mae: 1.6028 - val_loss: 14.6002 - val_mse: 14.6002 - val_mae: 1.8542 - lr: 4.7879e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 10s - loss: 14.3567 - mse: 14.3567 - mae: 1.6048 - val_loss: 14.3793 - val_mse: 14.3793 - val_mae: 1.4379 - lr: 4.7879e-04 - 10s/epoch - 10ms/step\n",
            "Score for fold 3: loss of 14.379316329956055\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 10s - loss: 14.9949 - mse: 14.9949 - mae: 1.5934 - val_loss: 12.3867 - val_mse: 12.3867 - val_mae: 1.5454 - lr: 4.7879e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 10s - loss: 14.8625 - mse: 14.8625 - mae: 1.5782 - val_loss: 12.1559 - val_mse: 12.1559 - val_mae: 1.6466 - lr: 4.7879e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 10s - loss: 14.7835 - mse: 14.7835 - mae: 1.5808 - val_loss: 12.2567 - val_mse: 12.2567 - val_mae: 1.7336 - lr: 4.7879e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 10s - loss: 14.8575 - mse: 14.8575 - mae: 1.5824 - val_loss: 12.6073 - val_mse: 12.6073 - val_mae: 1.5120 - lr: 4.7879e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 10s - loss: 14.7182 - mse: 14.7182 - mae: 1.5782 - val_loss: 11.7911 - val_mse: 11.7911 - val_mae: 1.6883 - lr: 4.7879e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 11s - loss: 14.8068 - mse: 14.8068 - mae: 1.5882 - val_loss: 12.1652 - val_mse: 12.1652 - val_mae: 1.5451 - lr: 4.7879e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 11s - loss: 14.7048 - mse: 14.7048 - mae: 1.5792 - val_loss: 12.4503 - val_mse: 12.4503 - val_mae: 1.7295 - lr: 4.7879e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 11s - loss: 14.5652 - mse: 14.5652 - mae: 1.5757 - val_loss: 12.3369 - val_mse: 12.3369 - val_mae: 1.5582 - lr: 4.7879e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 10s - loss: 14.5342 - mse: 14.5342 - mae: 1.5792 - val_loss: 12.3843 - val_mse: 12.3843 - val_mae: 1.8349 - lr: 4.7879e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 11s - loss: 14.5600 - mse: 14.5600 - mae: 1.5825 - val_loss: 12.3739 - val_mse: 12.3739 - val_mae: 1.5623 - lr: 4.7879e-04 - 11s/epoch - 11ms/step\n",
            "Score for fold 4: loss of 12.373905181884766\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 11s - loss: 13.6238 - mse: 13.6238 - mae: 1.5864 - val_loss: 16.6668 - val_mse: 16.6668 - val_mae: 1.6898 - lr: 4.7879e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 10s - loss: 13.4739 - mse: 13.4739 - mae: 1.5733 - val_loss: 16.3057 - val_mse: 16.3057 - val_mae: 1.5798 - lr: 4.7879e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 10s - loss: 13.4136 - mse: 13.4136 - mae: 1.5797 - val_loss: 16.5146 - val_mse: 16.5146 - val_mae: 1.5315 - lr: 4.7879e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 10s - loss: 13.4564 - mse: 13.4564 - mae: 1.5900 - val_loss: 17.0442 - val_mse: 17.0442 - val_mae: 1.6377 - lr: 4.7879e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 10s - loss: 13.2022 - mse: 13.2022 - mae: 1.5901 - val_loss: 17.1588 - val_mse: 17.1588 - val_mae: 1.4921 - lr: 4.7879e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 10s - loss: 13.3091 - mse: 13.3091 - mae: 1.5897 - val_loss: 17.2766 - val_mse: 17.2766 - val_mae: 1.5030 - lr: 4.7879e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 11s - loss: 13.4022 - mse: 13.4022 - mae: 1.6033 - val_loss: 17.0015 - val_mse: 17.0015 - val_mae: 1.6480 - lr: 4.7879e-04 - 11s/epoch - 11ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-10-14 12:16:29,980]\u001b[0m Finished trial#18 resulted in value: 14.8. Current best value is 14.524000000000001 with parameters: {'activation': 'relu', 'num_hidden_layer': 4, 'i': 9, 'learning_rate': 0.00033700551736380043}.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Score for fold 5: loss of 17.00149917602539\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 6s - loss: 16.1349 - mse: 16.1349 - mae: 1.6124 - val_loss: 13.0071 - val_mse: 13.0071 - val_mae: 1.5506 - lr: 2.0734e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 5s - loss: 15.7519 - mse: 15.7519 - mae: 1.5940 - val_loss: 12.5444 - val_mse: 12.5444 - val_mae: 1.5935 - lr: 2.0734e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 5s - loss: 15.6917 - mse: 15.6917 - mae: 1.5902 - val_loss: 12.8383 - val_mse: 12.8383 - val_mae: 1.6530 - lr: 2.0734e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 5s - loss: 15.5857 - mse: 15.5857 - mae: 1.5881 - val_loss: 12.5680 - val_mse: 12.5680 - val_mae: 1.6010 - lr: 2.0734e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 5s - loss: 15.5989 - mse: 15.5989 - mae: 1.5849 - val_loss: 12.8512 - val_mse: 12.8512 - val_mae: 1.5853 - lr: 2.0734e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 5s - loss: 15.5832 - mse: 15.5832 - mae: 1.5829 - val_loss: 12.2791 - val_mse: 12.2791 - val_mae: 1.6808 - lr: 2.0734e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 5s - loss: 15.5034 - mse: 15.5034 - mae: 1.5788 - val_loss: 12.5869 - val_mse: 12.5869 - val_mae: 1.5447 - lr: 2.0734e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 5s - loss: 15.5091 - mse: 15.5091 - mae: 1.5742 - val_loss: 12.6148 - val_mse: 12.6148 - val_mae: 1.5972 - lr: 2.0734e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 5s - loss: 15.3995 - mse: 15.3995 - mae: 1.5721 - val_loss: 12.5558 - val_mse: 12.5558 - val_mae: 1.6319 - lr: 2.0734e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 5s - loss: 15.3930 - mse: 15.3930 - mae: 1.5744 - val_loss: 12.4791 - val_mse: 12.4791 - val_mae: 1.5963 - lr: 2.0734e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 5s - loss: 15.3519 - mse: 15.3519 - mae: 1.5680 - val_loss: 12.5158 - val_mse: 12.5158 - val_mae: 1.5368 - lr: 2.0734e-04 - 5s/epoch - 5ms/step\n",
            "Score for fold 1: loss of 12.515788078308105\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 5s - loss: 14.7029 - mse: 14.7029 - mae: 1.5768 - val_loss: 14.8890 - val_mse: 14.8890 - val_mae: 1.4689 - lr: 2.0734e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 5s - loss: 14.5809 - mse: 14.5809 - mae: 1.5770 - val_loss: 14.7298 - val_mse: 14.7298 - val_mae: 1.5942 - lr: 2.0734e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 5s - loss: 14.6085 - mse: 14.6085 - mae: 1.5746 - val_loss: 15.1315 - val_mse: 15.1315 - val_mae: 1.7346 - lr: 2.0734e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 5s - loss: 14.4568 - mse: 14.4568 - mae: 1.5684 - val_loss: 14.8862 - val_mse: 14.8862 - val_mae: 1.5106 - lr: 2.0734e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 5s - loss: 14.4085 - mse: 14.4085 - mae: 1.5662 - val_loss: 14.8636 - val_mse: 14.8636 - val_mae: 1.5467 - lr: 2.0734e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 5s - loss: 14.3064 - mse: 14.3064 - mae: 1.5622 - val_loss: 15.1083 - val_mse: 15.1083 - val_mae: 1.7069 - lr: 2.0734e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 5s - loss: 14.2456 - mse: 14.2456 - mae: 1.5668 - val_loss: 14.9844 - val_mse: 14.9844 - val_mae: 1.5026 - lr: 2.0734e-04 - 5s/epoch - 5ms/step\n",
            "Score for fold 2: loss of 14.984444618225098\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 5s - loss: 15.1657 - mse: 15.1657 - mae: 1.5803 - val_loss: 11.1433 - val_mse: 11.1433 - val_mae: 1.5255 - lr: 2.0734e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 5s - loss: 15.0201 - mse: 15.0201 - mae: 1.5717 - val_loss: 11.3203 - val_mse: 11.3203 - val_mae: 1.6238 - lr: 2.0734e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 5s - loss: 14.9787 - mse: 14.9787 - mae: 1.5729 - val_loss: 11.6300 - val_mse: 11.6300 - val_mae: 1.5367 - lr: 2.0734e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 5s - loss: 14.8352 - mse: 14.8352 - mae: 1.5641 - val_loss: 11.6820 - val_mse: 11.6820 - val_mae: 1.7427 - lr: 2.0734e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 5s - loss: 14.7000 - mse: 14.7000 - mae: 1.5659 - val_loss: 11.5555 - val_mse: 11.5555 - val_mae: 1.5651 - lr: 2.0734e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 5s - loss: 14.5298 - mse: 14.5298 - mae: 1.5511 - val_loss: 11.5934 - val_mse: 11.5934 - val_mae: 1.5481 - lr: 2.0734e-04 - 5s/epoch - 5ms/step\n",
            "Score for fold 3: loss of 11.593417167663574\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 5s - loss: 15.0985 - mse: 15.0985 - mae: 1.5694 - val_loss: 10.7365 - val_mse: 10.7365 - val_mae: 1.5275 - lr: 2.0734e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 5s - loss: 15.0190 - mse: 15.0190 - mae: 1.5610 - val_loss: 10.3091 - val_mse: 10.3091 - val_mae: 1.6066 - lr: 2.0734e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 5s - loss: 14.8912 - mse: 14.8912 - mae: 1.5596 - val_loss: 9.9422 - val_mse: 9.9422 - val_mae: 1.5911 - lr: 2.0734e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 5s - loss: 14.7776 - mse: 14.7776 - mae: 1.5622 - val_loss: 10.0200 - val_mse: 10.0200 - val_mae: 1.5651 - lr: 2.0734e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 5s - loss: 14.6120 - mse: 14.6120 - mae: 1.5561 - val_loss: 10.6129 - val_mse: 10.6129 - val_mae: 1.7483 - lr: 2.0734e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 5s - loss: 14.6704 - mse: 14.6704 - mae: 1.5498 - val_loss: 10.1772 - val_mse: 10.1772 - val_mae: 1.5958 - lr: 2.0734e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 5s - loss: 14.6478 - mse: 14.6478 - mae: 1.5577 - val_loss: 10.5953 - val_mse: 10.5953 - val_mae: 1.5712 - lr: 2.0734e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 5s - loss: 14.5034 - mse: 14.5034 - mae: 1.5664 - val_loss: 10.6594 - val_mse: 10.6594 - val_mae: 1.6192 - lr: 2.0734e-04 - 5s/epoch - 5ms/step\n",
            "Score for fold 4: loss of 10.659378051757812\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 5s - loss: 11.4556 - mse: 11.4556 - mae: 1.5719 - val_loss: 22.9094 - val_mse: 22.9094 - val_mae: 1.6473 - lr: 2.0734e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 5s - loss: 11.6489 - mse: 11.6489 - mae: 1.5916 - val_loss: 23.1566 - val_mse: 23.1566 - val_mae: 1.5496 - lr: 2.0734e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 5s - loss: 11.3070 - mse: 11.3070 - mae: 1.5753 - val_loss: 22.8623 - val_mse: 22.8623 - val_mae: 1.5231 - lr: 2.0734e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 5s - loss: 11.3481 - mse: 11.3481 - mae: 1.5615 - val_loss: 23.1633 - val_mse: 23.1633 - val_mae: 1.7760 - lr: 2.0734e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 5s - loss: 11.3109 - mse: 11.3109 - mae: 1.5866 - val_loss: 23.3935 - val_mse: 23.3935 - val_mae: 1.5632 - lr: 2.0734e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 5s - loss: 11.2387 - mse: 11.2387 - mae: 1.5814 - val_loss: 24.0105 - val_mse: 24.0105 - val_mae: 2.0607 - lr: 2.0734e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 5s - loss: 11.1848 - mse: 11.1848 - mae: 1.6062 - val_loss: 23.9581 - val_mse: 23.9581 - val_mae: 2.0237 - lr: 2.0734e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 5s - loss: 11.1284 - mse: 11.1284 - mae: 1.5871 - val_loss: 23.3329 - val_mse: 23.3329 - val_mae: 1.7444 - lr: 2.0734e-04 - 5s/epoch - 5ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-10-14 12:20:00,604]\u001b[0m Finished trial#19 resulted in value: 14.616. Current best value is 14.524000000000001 with parameters: {'activation': 'relu', 'num_hidden_layer': 4, 'i': 9, 'learning_rate': 0.00033700551736380043}.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Score for fold 5: loss of 23.332942962646484\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 4s - loss: 16.7361 - mse: 16.7361 - mae: 1.6907 - val_loss: 13.5926 - val_mse: 13.5926 - val_mae: 1.6080 - lr: 1.8268e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 3s - loss: 16.2726 - mse: 16.2726 - mae: 1.6349 - val_loss: 13.5443 - val_mse: 13.5443 - val_mae: 1.6058 - lr: 1.8268e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 3s - loss: 16.2471 - mse: 16.2471 - mae: 1.6352 - val_loss: 13.5313 - val_mse: 13.5313 - val_mae: 1.6666 - lr: 1.8268e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 3s - loss: 16.2506 - mse: 16.2506 - mae: 1.6419 - val_loss: 13.4745 - val_mse: 13.4745 - val_mae: 1.6515 - lr: 1.8268e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 3s - loss: 16.1957 - mse: 16.1957 - mae: 1.6367 - val_loss: 13.6520 - val_mse: 13.6520 - val_mae: 1.5957 - lr: 1.8268e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 3s - loss: 16.2230 - mse: 16.2230 - mae: 1.6356 - val_loss: 13.4801 - val_mse: 13.4801 - val_mae: 1.6469 - lr: 1.8268e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 3s - loss: 16.2204 - mse: 16.2204 - mae: 1.6328 - val_loss: 13.5798 - val_mse: 13.5798 - val_mae: 1.6187 - lr: 1.8268e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 3s - loss: 16.1776 - mse: 16.1776 - mae: 1.6335 - val_loss: 13.5197 - val_mse: 13.5197 - val_mae: 1.6797 - lr: 1.8268e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 3s - loss: 16.1867 - mse: 16.1867 - mae: 1.6372 - val_loss: 13.5696 - val_mse: 13.5696 - val_mae: 1.6171 - lr: 1.8268e-04 - 3s/epoch - 3ms/step\n",
            "Score for fold 1: loss of 13.569615364074707\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 3s - loss: 15.8372 - mse: 15.8372 - mae: 1.6407 - val_loss: 15.0156 - val_mse: 15.0156 - val_mae: 1.5682 - lr: 1.8268e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 3s - loss: 15.8771 - mse: 15.8771 - mae: 1.6376 - val_loss: 14.9276 - val_mse: 14.9276 - val_mae: 1.6046 - lr: 1.8268e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 3s - loss: 15.8541 - mse: 15.8541 - mae: 1.6391 - val_loss: 15.0718 - val_mse: 15.0718 - val_mae: 1.5930 - lr: 1.8268e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 3s - loss: 15.8680 - mse: 15.8680 - mae: 1.6392 - val_loss: 14.9038 - val_mse: 14.9038 - val_mae: 1.5992 - lr: 1.8268e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 3s - loss: 15.8360 - mse: 15.8360 - mae: 1.6418 - val_loss: 14.8844 - val_mse: 14.8844 - val_mae: 1.6253 - lr: 1.8268e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 3s - loss: 15.8494 - mse: 15.8494 - mae: 1.6411 - val_loss: 15.0371 - val_mse: 15.0371 - val_mae: 1.5712 - lr: 1.8268e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 3s - loss: 15.8614 - mse: 15.8614 - mae: 1.6409 - val_loss: 14.9319 - val_mse: 14.9319 - val_mae: 1.5877 - lr: 1.8268e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 3s - loss: 15.8374 - mse: 15.8374 - mae: 1.6414 - val_loss: 14.9271 - val_mse: 14.9271 - val_mae: 1.5892 - lr: 1.8268e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 3s - loss: 15.8366 - mse: 15.8366 - mae: 1.6382 - val_loss: 14.9629 - val_mse: 14.9629 - val_mae: 1.6360 - lr: 1.8268e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 3s - loss: 15.8596 - mse: 15.8596 - mae: 1.6421 - val_loss: 14.9183 - val_mse: 14.9183 - val_mae: 1.6203 - lr: 1.8268e-04 - 3s/epoch - 3ms/step\n",
            "Score for fold 2: loss of 14.91833782196045\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 3s - loss: 16.6720 - mse: 16.6720 - mae: 1.6385 - val_loss: 11.6570 - val_mse: 11.6570 - val_mae: 1.6445 - lr: 1.8268e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 3s - loss: 16.6243 - mse: 16.6243 - mae: 1.6376 - val_loss: 11.6639 - val_mse: 11.6639 - val_mae: 1.6549 - lr: 1.8268e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 3s - loss: 16.6515 - mse: 16.6515 - mae: 1.6361 - val_loss: 11.6946 - val_mse: 11.6946 - val_mae: 1.6084 - lr: 1.8268e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 3s - loss: 16.6477 - mse: 16.6477 - mae: 1.6356 - val_loss: 11.7417 - val_mse: 11.7417 - val_mae: 1.6106 - lr: 1.8268e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 3s - loss: 16.6610 - mse: 16.6610 - mae: 1.6385 - val_loss: 11.7143 - val_mse: 11.7143 - val_mae: 1.5996 - lr: 1.8268e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 3s - loss: 16.6355 - mse: 16.6355 - mae: 1.6377 - val_loss: 11.6856 - val_mse: 11.6856 - val_mae: 1.6029 - lr: 1.8268e-04 - 3s/epoch - 3ms/step\n",
            "Score for fold 3: loss of 11.685608863830566\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 3s - loss: 15.9055 - mse: 15.9055 - mae: 1.6361 - val_loss: 14.6753 - val_mse: 14.6753 - val_mae: 1.6654 - lr: 1.8268e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 3s - loss: 15.8566 - mse: 15.8566 - mae: 1.6384 - val_loss: 14.6873 - val_mse: 14.6873 - val_mae: 1.6223 - lr: 1.8268e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 3s - loss: 15.8874 - mse: 15.8874 - mae: 1.6382 - val_loss: 14.7602 - val_mse: 14.7602 - val_mae: 1.6186 - lr: 1.8268e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 3s - loss: 15.8867 - mse: 15.8867 - mae: 1.6387 - val_loss: 14.7227 - val_mse: 14.7227 - val_mae: 1.6509 - lr: 1.8268e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 3s - loss: 15.8945 - mse: 15.8945 - mae: 1.6380 - val_loss: 14.6259 - val_mse: 14.6259 - val_mae: 1.6830 - lr: 1.8268e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 3s - loss: 15.8804 - mse: 15.8804 - mae: 1.6433 - val_loss: 14.6818 - val_mse: 14.6818 - val_mae: 1.6535 - lr: 1.8268e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 3s - loss: 15.9008 - mse: 15.9008 - mae: 1.6332 - val_loss: 14.7092 - val_mse: 14.7092 - val_mae: 1.6308 - lr: 1.8268e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 3s - loss: 15.8581 - mse: 15.8581 - mae: 1.6399 - val_loss: 14.7676 - val_mse: 14.7676 - val_mae: 1.6055 - lr: 1.8268e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 3s - loss: 15.8646 - mse: 15.8646 - mae: 1.6396 - val_loss: 14.6669 - val_mse: 14.6669 - val_mae: 1.7304 - lr: 1.8268e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 3s - loss: 15.8819 - mse: 15.8819 - mae: 1.6397 - val_loss: 14.7126 - val_mse: 14.7126 - val_mae: 1.6587 - lr: 1.8268e-04 - 3s/epoch - 3ms/step\n",
            "Score for fold 4: loss of 14.71261978149414\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 3s - loss: 13.7334 - mse: 13.7334 - mae: 1.6236 - val_loss: 23.4149 - val_mse: 23.4149 - val_mae: 1.6607 - lr: 1.8268e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 3s - loss: 13.7560 - mse: 13.7560 - mae: 1.6236 - val_loss: 23.3205 - val_mse: 23.3205 - val_mae: 1.6925 - lr: 1.8268e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 3s - loss: 13.7129 - mse: 13.7129 - mae: 1.6263 - val_loss: 23.3156 - val_mse: 23.3156 - val_mae: 1.7006 - lr: 1.8268e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 3s - loss: 13.7469 - mse: 13.7469 - mae: 1.6260 - val_loss: 23.3808 - val_mse: 23.3808 - val_mae: 1.6639 - lr: 1.8268e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 3s - loss: 13.7562 - mse: 13.7562 - mae: 1.6234 - val_loss: 23.3458 - val_mse: 23.3458 - val_mae: 1.6763 - lr: 1.8268e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 3s - loss: 13.7494 - mse: 13.7494 - mae: 1.6260 - val_loss: 23.3551 - val_mse: 23.3551 - val_mae: 1.7111 - lr: 1.8268e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 3s - loss: 13.7539 - mse: 13.7539 - mae: 1.6236 - val_loss: 23.5268 - val_mse: 23.5268 - val_mae: 1.6271 - lr: 1.8268e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 3s - loss: 13.7248 - mse: 13.7248 - mae: 1.6252 - val_loss: 23.4641 - val_mse: 23.4641 - val_mae: 1.6305 - lr: 1.8268e-04 - 3s/epoch - 3ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-10-14 12:22:31,134]\u001b[0m Finished trial#20 resulted in value: 15.669999999999998. Current best value is 14.524000000000001 with parameters: {'activation': 'relu', 'num_hidden_layer': 4, 'i': 9, 'learning_rate': 0.00033700551736380043}.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Score for fold 5: loss of 23.464107513427734\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 6s - loss: 16.6025 - mse: 16.6025 - mae: 1.6358 - val_loss: 11.2009 - val_mse: 11.2009 - val_mae: 1.6036 - lr: 9.4497e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 5s - loss: 16.2114 - mse: 16.2114 - mae: 1.6106 - val_loss: 10.9007 - val_mse: 10.9007 - val_mae: 1.5383 - lr: 9.4497e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 5s - loss: 16.2535 - mse: 16.2535 - mae: 1.6023 - val_loss: 10.9352 - val_mse: 10.9352 - val_mae: 1.5826 - lr: 9.4497e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 5s - loss: 16.1984 - mse: 16.1984 - mae: 1.5999 - val_loss: 11.0462 - val_mse: 11.0462 - val_mae: 1.5405 - lr: 9.4497e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 5s - loss: 16.1486 - mse: 16.1486 - mae: 1.5927 - val_loss: 10.7707 - val_mse: 10.7707 - val_mae: 1.5947 - lr: 9.4497e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 5s - loss: 16.0375 - mse: 16.0375 - mae: 1.5948 - val_loss: 10.9269 - val_mse: 10.9269 - val_mae: 1.5332 - lr: 9.4497e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 5s - loss: 16.0257 - mse: 16.0257 - mae: 1.5953 - val_loss: 10.6565 - val_mse: 10.6565 - val_mae: 1.5710 - lr: 9.4497e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 5s - loss: 15.9995 - mse: 15.9995 - mae: 1.5977 - val_loss: 10.9814 - val_mse: 10.9814 - val_mae: 1.7842 - lr: 9.4497e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 5s - loss: 16.0019 - mse: 16.0019 - mae: 1.5993 - val_loss: 10.9497 - val_mse: 10.9497 - val_mae: 1.5330 - lr: 9.4497e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 5s - loss: 15.9544 - mse: 15.9544 - mae: 1.5974 - val_loss: 10.7609 - val_mse: 10.7609 - val_mae: 1.5761 - lr: 9.4497e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 5s - loss: 16.0050 - mse: 16.0050 - mae: 1.6012 - val_loss: 11.0635 - val_mse: 11.0635 - val_mae: 1.5518 - lr: 9.4497e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 5s - loss: 15.9756 - mse: 15.9756 - mae: 1.6022 - val_loss: 11.2188 - val_mse: 11.2188 - val_mae: 1.6511 - lr: 9.4497e-04 - 5s/epoch - 5ms/step\n",
            "Score for fold 1: loss of 11.218841552734375\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 5s - loss: 12.4339 - mse: 12.4339 - mae: 1.5922 - val_loss: 25.4423 - val_mse: 25.4423 - val_mae: 1.5507 - lr: 9.4497e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 5s - loss: 12.3468 - mse: 12.3468 - mae: 1.5925 - val_loss: 25.5792 - val_mse: 25.5792 - val_mae: 1.6814 - lr: 9.4497e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 5s - loss: 12.3766 - mse: 12.3766 - mae: 1.5932 - val_loss: 25.4485 - val_mse: 25.4485 - val_mae: 1.6393 - lr: 9.4497e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 5s - loss: 12.4313 - mse: 12.4313 - mae: 1.5919 - val_loss: 25.4716 - val_mse: 25.4716 - val_mae: 1.7639 - lr: 9.4497e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 5s - loss: 12.4768 - mse: 12.4768 - mae: 1.5909 - val_loss: 24.9671 - val_mse: 24.9671 - val_mae: 1.6916 - lr: 9.4497e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 5s - loss: 12.2843 - mse: 12.2843 - mae: 1.5881 - val_loss: 25.4270 - val_mse: 25.4270 - val_mae: 1.7936 - lr: 9.4497e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 5s - loss: 12.2965 - mse: 12.2965 - mae: 1.5898 - val_loss: 25.3632 - val_mse: 25.3632 - val_mae: 1.6854 - lr: 9.4497e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 5s - loss: 12.3823 - mse: 12.3823 - mae: 1.5937 - val_loss: 25.3870 - val_mse: 25.3870 - val_mae: 1.6085 - lr: 9.4497e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 5s - loss: 12.2665 - mse: 12.2665 - mae: 1.5975 - val_loss: 25.1825 - val_mse: 25.1825 - val_mae: 1.5908 - lr: 9.4497e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 5s - loss: 12.3645 - mse: 12.3645 - mae: 1.6028 - val_loss: 25.5938 - val_mse: 25.5938 - val_mae: 1.6030 - lr: 9.4497e-04 - 5s/epoch - 5ms/step\n",
            "Score for fold 2: loss of 25.59375762939453\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 5s - loss: 16.2918 - mse: 16.2918 - mae: 1.6215 - val_loss: 9.4285 - val_mse: 9.4285 - val_mae: 1.6689 - lr: 9.4497e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 5s - loss: 16.2397 - mse: 16.2397 - mae: 1.6315 - val_loss: 9.4040 - val_mse: 9.4040 - val_mae: 1.5623 - lr: 9.4497e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 5s - loss: 16.3119 - mse: 16.3119 - mae: 1.6241 - val_loss: 9.1918 - val_mse: 9.1918 - val_mae: 1.5293 - lr: 9.4497e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 5s - loss: 16.2827 - mse: 16.2827 - mae: 1.6204 - val_loss: 9.4866 - val_mse: 9.4866 - val_mae: 1.5152 - lr: 9.4497e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 5s - loss: 16.2463 - mse: 16.2463 - mae: 1.6259 - val_loss: 9.3308 - val_mse: 9.3308 - val_mae: 1.5598 - lr: 9.4497e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 5s - loss: 16.2360 - mse: 16.2360 - mae: 1.6180 - val_loss: 9.7556 - val_mse: 9.7556 - val_mae: 1.4091 - lr: 9.4497e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 5s - loss: 16.1192 - mse: 16.1192 - mae: 1.6093 - val_loss: 9.5853 - val_mse: 9.5853 - val_mae: 1.5664 - lr: 9.4497e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 5s - loss: 16.2008 - mse: 16.2008 - mae: 1.6167 - val_loss: 9.6660 - val_mse: 9.6660 - val_mae: 1.6321 - lr: 9.4497e-04 - 5s/epoch - 5ms/step\n",
            "Score for fold 3: loss of 9.666036605834961\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 5s - loss: 14.8848 - mse: 14.8848 - mae: 1.5837 - val_loss: 15.3899 - val_mse: 15.3899 - val_mae: 1.5526 - lr: 9.4497e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 5s - loss: 14.8610 - mse: 14.8610 - mae: 1.6039 - val_loss: 14.9942 - val_mse: 14.9942 - val_mae: 1.6490 - lr: 9.4497e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 5s - loss: 14.7638 - mse: 14.7638 - mae: 1.5934 - val_loss: 14.7508 - val_mse: 14.7508 - val_mae: 1.6643 - lr: 9.4497e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 5s - loss: 14.8575 - mse: 14.8575 - mae: 1.6130 - val_loss: 15.2962 - val_mse: 15.2962 - val_mae: 1.4857 - lr: 9.4497e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 5s - loss: 14.7141 - mse: 14.7141 - mae: 1.6139 - val_loss: 15.2124 - val_mse: 15.2124 - val_mae: 1.7761 - lr: 9.4497e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 5s - loss: 14.5452 - mse: 14.5452 - mae: 1.6097 - val_loss: 15.2967 - val_mse: 15.2967 - val_mae: 1.5203 - lr: 9.4497e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 5s - loss: 14.6390 - mse: 14.6390 - mae: 1.5943 - val_loss: 15.3000 - val_mse: 15.3000 - val_mae: 1.6701 - lr: 9.4497e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 5s - loss: 14.7943 - mse: 14.7943 - mae: 1.6067 - val_loss: 15.2257 - val_mse: 15.2257 - val_mae: 1.8801 - lr: 9.4497e-04 - 5s/epoch - 5ms/step\n",
            "Score for fold 4: loss of 15.225707054138184\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 5s - loss: 14.9319 - mse: 14.9319 - mae: 1.6047 - val_loss: 13.7265 - val_mse: 13.7265 - val_mae: 1.7299 - lr: 9.4497e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 5s - loss: 14.8828 - mse: 14.8828 - mae: 1.6218 - val_loss: 14.0553 - val_mse: 14.0553 - val_mae: 1.5766 - lr: 9.4497e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 5s - loss: 14.8846 - mse: 14.8846 - mae: 1.6256 - val_loss: 14.1684 - val_mse: 14.1684 - val_mae: 1.7397 - lr: 9.4497e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 5s - loss: 14.9154 - mse: 14.9154 - mae: 1.6224 - val_loss: 13.7874 - val_mse: 13.7874 - val_mae: 1.5711 - lr: 9.4497e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 5s - loss: 14.9351 - mse: 14.9351 - mae: 1.6340 - val_loss: 13.9503 - val_mse: 13.9503 - val_mae: 1.6267 - lr: 9.4497e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 5s - loss: 14.8575 - mse: 14.8575 - mae: 1.6559 - val_loss: 14.1388 - val_mse: 14.1388 - val_mae: 1.6217 - lr: 9.4497e-04 - 5s/epoch - 5ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-10-14 12:26:23,115]\u001b[0m Finished trial#21 resulted in value: 15.170000000000002. Current best value is 14.524000000000001 with parameters: {'activation': 'relu', 'num_hidden_layer': 4, 'i': 9, 'learning_rate': 0.00033700551736380043}.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Score for fold 5: loss of 14.138823509216309\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 6s - loss: 16.4956 - mse: 16.4956 - mae: 1.6225 - val_loss: 11.0936 - val_mse: 11.0936 - val_mae: 1.5437 - lr: 2.2175e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 5s - loss: 16.0457 - mse: 16.0457 - mae: 1.5929 - val_loss: 11.1515 - val_mse: 11.1515 - val_mae: 1.5357 - lr: 2.2175e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 5s - loss: 16.0786 - mse: 16.0786 - mae: 1.5954 - val_loss: 11.0628 - val_mse: 11.0628 - val_mae: 1.6288 - lr: 2.2175e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 5s - loss: 15.9727 - mse: 15.9727 - mae: 1.5898 - val_loss: 10.9876 - val_mse: 10.9876 - val_mae: 1.6247 - lr: 2.2175e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 5s - loss: 15.9132 - mse: 15.9132 - mae: 1.5879 - val_loss: 11.0406 - val_mse: 11.0406 - val_mae: 1.5948 - lr: 2.2175e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 5s - loss: 15.8372 - mse: 15.8372 - mae: 1.5873 - val_loss: 11.0589 - val_mse: 11.0589 - val_mae: 1.6085 - lr: 2.2175e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 5s - loss: 15.8634 - mse: 15.8634 - mae: 1.5846 - val_loss: 11.0522 - val_mse: 11.0522 - val_mae: 1.5759 - lr: 2.2175e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 5s - loss: 15.7785 - mse: 15.7785 - mae: 1.5771 - val_loss: 11.2354 - val_mse: 11.2354 - val_mae: 1.5418 - lr: 2.2175e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 5s - loss: 15.7306 - mse: 15.7306 - mae: 1.5754 - val_loss: 11.0742 - val_mse: 11.0742 - val_mae: 1.5920 - lr: 2.2175e-04 - 5s/epoch - 5ms/step\n",
            "Score for fold 1: loss of 11.074196815490723\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 5s - loss: 14.2016 - mse: 14.2016 - mae: 1.5713 - val_loss: 17.4801 - val_mse: 17.4801 - val_mae: 1.5561 - lr: 2.2175e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 5s - loss: 14.1365 - mse: 14.1365 - mae: 1.5652 - val_loss: 17.6687 - val_mse: 17.6687 - val_mae: 1.6499 - lr: 2.2175e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 5s - loss: 14.0892 - mse: 14.0892 - mae: 1.5657 - val_loss: 17.6613 - val_mse: 17.6613 - val_mae: 1.5898 - lr: 2.2175e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 5s - loss: 13.9856 - mse: 13.9856 - mae: 1.5654 - val_loss: 17.1605 - val_mse: 17.1605 - val_mae: 1.6545 - lr: 2.2175e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 5s - loss: 13.9431 - mse: 13.9431 - mae: 1.5572 - val_loss: 18.0625 - val_mse: 18.0625 - val_mae: 1.5259 - lr: 2.2175e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 5s - loss: 13.9727 - mse: 13.9727 - mae: 1.5606 - val_loss: 17.1702 - val_mse: 17.1702 - val_mae: 1.6242 - lr: 2.2175e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 5s - loss: 13.8320 - mse: 13.8320 - mae: 1.5536 - val_loss: 17.0459 - val_mse: 17.0459 - val_mae: 1.6155 - lr: 2.2175e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 5s - loss: 13.6332 - mse: 13.6332 - mae: 1.5533 - val_loss: 17.3392 - val_mse: 17.3392 - val_mae: 1.5704 - lr: 2.2175e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 5s - loss: 13.6533 - mse: 13.6533 - mae: 1.5546 - val_loss: 17.3958 - val_mse: 17.3958 - val_mae: 1.6511 - lr: 2.2175e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 5s - loss: 13.5848 - mse: 13.5848 - mae: 1.5547 - val_loss: 17.5366 - val_mse: 17.5366 - val_mae: 1.6452 - lr: 2.2175e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 5s - loss: 13.5188 - mse: 13.5188 - mae: 1.5453 - val_loss: 17.0868 - val_mse: 17.0868 - val_mae: 1.7220 - lr: 2.2175e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 5s - loss: 13.4604 - mse: 13.4604 - mae: 1.5470 - val_loss: 17.7080 - val_mse: 17.7080 - val_mae: 1.7526 - lr: 2.2175e-04 - 5s/epoch - 5ms/step\n",
            "Score for fold 2: loss of 17.70798110961914\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 5s - loss: 12.2571 - mse: 12.2571 - mae: 1.5604 - val_loss: 21.7159 - val_mse: 21.7159 - val_mae: 1.6129 - lr: 2.2175e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 5s - loss: 12.2164 - mse: 12.2164 - mae: 1.5554 - val_loss: 21.6510 - val_mse: 21.6510 - val_mae: 1.5972 - lr: 2.2175e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 5s - loss: 12.1041 - mse: 12.1041 - mae: 1.5494 - val_loss: 22.0399 - val_mse: 22.0399 - val_mae: 1.6025 - lr: 2.2175e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 5s - loss: 12.0608 - mse: 12.0608 - mae: 1.5497 - val_loss: 22.4166 - val_mse: 22.4166 - val_mae: 1.7366 - lr: 2.2175e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 5s - loss: 11.9137 - mse: 11.9137 - mae: 1.5539 - val_loss: 22.5895 - val_mse: 22.5895 - val_mae: 1.6364 - lr: 2.2175e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 5s - loss: 11.8599 - mse: 11.8599 - mae: 1.5511 - val_loss: 22.2311 - val_mse: 22.2311 - val_mae: 1.7242 - lr: 2.2175e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 5s - loss: 11.7967 - mse: 11.7967 - mae: 1.5519 - val_loss: 22.1126 - val_mse: 22.1126 - val_mae: 1.5288 - lr: 2.2175e-04 - 5s/epoch - 5ms/step\n",
            "Score for fold 3: loss of 22.112590789794922\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 5s - loss: 14.9699 - mse: 14.9699 - mae: 1.5850 - val_loss: 10.0401 - val_mse: 10.0401 - val_mae: 1.4700 - lr: 2.2175e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 5s - loss: 14.8286 - mse: 14.8286 - mae: 1.5742 - val_loss: 10.1348 - val_mse: 10.1348 - val_mae: 1.5718 - lr: 2.2175e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 5s - loss: 14.7468 - mse: 14.7468 - mae: 1.5743 - val_loss: 10.4734 - val_mse: 10.4734 - val_mae: 1.4811 - lr: 2.2175e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 5s - loss: 14.6710 - mse: 14.6710 - mae: 1.5802 - val_loss: 10.6525 - val_mse: 10.6525 - val_mae: 1.9046 - lr: 2.2175e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 5s - loss: 14.3970 - mse: 14.3970 - mae: 1.5987 - val_loss: 10.4154 - val_mse: 10.4154 - val_mae: 1.6079 - lr: 2.2175e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 5s - loss: 14.6560 - mse: 14.6560 - mae: 1.6111 - val_loss: 10.5089 - val_mse: 10.5089 - val_mae: 1.4782 - lr: 2.2175e-04 - 5s/epoch - 5ms/step\n",
            "Score for fold 4: loss of 10.508934020996094\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 5s - loss: 14.5402 - mse: 14.5402 - mae: 1.6340 - val_loss: 10.8949 - val_mse: 10.8949 - val_mae: 1.6631 - lr: 2.2175e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 5s - loss: 14.2894 - mse: 14.2894 - mae: 1.6064 - val_loss: 10.9732 - val_mse: 10.9732 - val_mae: 1.4052 - lr: 2.2175e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 5s - loss: 14.1159 - mse: 14.1159 - mae: 1.6031 - val_loss: 10.9470 - val_mse: 10.9470 - val_mae: 1.5054 - lr: 2.2175e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 5s - loss: 14.1360 - mse: 14.1360 - mae: 1.6187 - val_loss: 11.2918 - val_mse: 11.2918 - val_mae: 1.4309 - lr: 2.2175e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 5s - loss: 13.9373 - mse: 13.9373 - mae: 1.6174 - val_loss: 11.4740 - val_mse: 11.4740 - val_mae: 1.4981 - lr: 2.2175e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 5s - loss: 13.8527 - mse: 13.8527 - mae: 1.6422 - val_loss: 12.1095 - val_mse: 12.1095 - val_mae: 1.9757 - lr: 2.2175e-04 - 5s/epoch - 5ms/step\n",
            "Score for fold 5: loss of 12.10949420928955\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-10-14 12:29:51,158]\u001b[0m Finished trial#22 resulted in value: 14.701999999999998. Current best value is 14.524000000000001 with parameters: {'activation': 'relu', 'num_hidden_layer': 4, 'i': 9, 'learning_rate': 0.00033700551736380043}.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 6s - loss: 13.1023 - mse: 13.1023 - mae: 1.6005 - val_loss: 24.3563 - val_mse: 24.3563 - val_mae: 1.6621 - lr: 2.0230e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 5s - loss: 12.7320 - mse: 12.7320 - mae: 1.5782 - val_loss: 24.8092 - val_mse: 24.8092 - val_mae: 1.6546 - lr: 2.0230e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 5s - loss: 12.6451 - mse: 12.6451 - mae: 1.5753 - val_loss: 24.6899 - val_mse: 24.6899 - val_mae: 1.6716 - lr: 2.0230e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 5s - loss: 12.6593 - mse: 12.6593 - mae: 1.5718 - val_loss: 24.1623 - val_mse: 24.1623 - val_mae: 1.6242 - lr: 2.0230e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 5s - loss: 12.5963 - mse: 12.5963 - mae: 1.5685 - val_loss: 24.6966 - val_mse: 24.6966 - val_mae: 1.6202 - lr: 2.0230e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 5s - loss: 12.5411 - mse: 12.5411 - mae: 1.5668 - val_loss: 24.4529 - val_mse: 24.4529 - val_mae: 1.5826 - lr: 2.0230e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 5s - loss: 12.4890 - mse: 12.4890 - mae: 1.5636 - val_loss: 24.3569 - val_mse: 24.3569 - val_mae: 1.6346 - lr: 2.0230e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 5s - loss: 12.4929 - mse: 12.4929 - mae: 1.5629 - val_loss: 24.3963 - val_mse: 24.3963 - val_mae: 1.6201 - lr: 2.0230e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 5s - loss: 12.4348 - mse: 12.4348 - mae: 1.5564 - val_loss: 24.4389 - val_mse: 24.4389 - val_mae: 1.6753 - lr: 2.0230e-04 - 5s/epoch - 5ms/step\n",
            "Score for fold 1: loss of 24.438854217529297\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 5s - loss: 16.2845 - mse: 16.2845 - mae: 1.5869 - val_loss: 8.9173 - val_mse: 8.9173 - val_mae: 1.5806 - lr: 2.0230e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 5s - loss: 16.2510 - mse: 16.2510 - mae: 1.5796 - val_loss: 9.0658 - val_mse: 9.0658 - val_mae: 1.6330 - lr: 2.0230e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 5s - loss: 16.0497 - mse: 16.0497 - mae: 1.5755 - val_loss: 9.0093 - val_mse: 9.0093 - val_mae: 1.5877 - lr: 2.0230e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 5s - loss: 16.0212 - mse: 16.0212 - mae: 1.5766 - val_loss: 8.9538 - val_mse: 8.9538 - val_mae: 1.5676 - lr: 2.0230e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 5s - loss: 15.9513 - mse: 15.9513 - mae: 1.5724 - val_loss: 9.0645 - val_mse: 9.0645 - val_mae: 1.6063 - lr: 2.0230e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 5s - loss: 15.8871 - mse: 15.8871 - mae: 1.5705 - val_loss: 9.0694 - val_mse: 9.0694 - val_mae: 1.6070 - lr: 2.0230e-04 - 5s/epoch - 5ms/step\n",
            "Score for fold 2: loss of 9.069416999816895\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 5s - loss: 15.0183 - mse: 15.0183 - mae: 1.5616 - val_loss: 12.0794 - val_mse: 12.0794 - val_mae: 1.5045 - lr: 2.0230e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 5s - loss: 14.9845 - mse: 14.9845 - mae: 1.5632 - val_loss: 12.6047 - val_mse: 12.6047 - val_mae: 1.5107 - lr: 2.0230e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 5s - loss: 14.9040 - mse: 14.9040 - mae: 1.5593 - val_loss: 12.5297 - val_mse: 12.5297 - val_mae: 1.5600 - lr: 2.0230e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 5s - loss: 14.8227 - mse: 14.8227 - mae: 1.5587 - val_loss: 12.4936 - val_mse: 12.4936 - val_mae: 1.5146 - lr: 2.0230e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 5s - loss: 14.8288 - mse: 14.8288 - mae: 1.5582 - val_loss: 12.4984 - val_mse: 12.4984 - val_mae: 1.5173 - lr: 2.0230e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 5s - loss: 14.6960 - mse: 14.6960 - mae: 1.5514 - val_loss: 12.4128 - val_mse: 12.4128 - val_mae: 1.5385 - lr: 2.0230e-04 - 5s/epoch - 5ms/step\n",
            "Score for fold 3: loss of 12.412814140319824\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 5s - loss: 14.7891 - mse: 14.7891 - mae: 1.5751 - val_loss: 12.0870 - val_mse: 12.0870 - val_mae: 1.5473 - lr: 2.0230e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 5s - loss: 14.6184 - mse: 14.6184 - mae: 1.5764 - val_loss: 12.2723 - val_mse: 12.2723 - val_mae: 1.6890 - lr: 2.0230e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 5s - loss: 14.5285 - mse: 14.5285 - mae: 1.5622 - val_loss: 12.1811 - val_mse: 12.1811 - val_mae: 1.5617 - lr: 2.0230e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 5s - loss: 14.5054 - mse: 14.5054 - mae: 1.5613 - val_loss: 12.2797 - val_mse: 12.2797 - val_mae: 1.5788 - lr: 2.0230e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 5s - loss: 14.4055 - mse: 14.4055 - mae: 1.5641 - val_loss: 12.4600 - val_mse: 12.4600 - val_mae: 1.4281 - lr: 2.0230e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 5s - loss: 14.2526 - mse: 14.2526 - mae: 1.5601 - val_loss: 12.4687 - val_mse: 12.4687 - val_mae: 1.7547 - lr: 2.0230e-04 - 5s/epoch - 5ms/step\n",
            "Score for fold 4: loss of 12.468697547912598\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 5s - loss: 13.9105 - mse: 13.9105 - mae: 1.5590 - val_loss: 14.0593 - val_mse: 14.0593 - val_mae: 1.4673 - lr: 2.0230e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 5s - loss: 13.9002 - mse: 13.9002 - mae: 1.5633 - val_loss: 13.9362 - val_mse: 13.9362 - val_mae: 1.4279 - lr: 2.0230e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 5s - loss: 13.6997 - mse: 13.6997 - mae: 1.5538 - val_loss: 14.4991 - val_mse: 14.4991 - val_mae: 1.5181 - lr: 2.0230e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 5s - loss: 13.6604 - mse: 13.6604 - mae: 1.5521 - val_loss: 14.4947 - val_mse: 14.4947 - val_mae: 1.6539 - lr: 2.0230e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 5s - loss: 13.6081 - mse: 13.6081 - mae: 1.5529 - val_loss: 14.2905 - val_mse: 14.2905 - val_mae: 1.5856 - lr: 2.0230e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 5s - loss: 13.4848 - mse: 13.4848 - mae: 1.5432 - val_loss: 14.6772 - val_mse: 14.6772 - val_mae: 1.6376 - lr: 2.0230e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 5s - loss: 13.4001 - mse: 13.4001 - mae: 1.5498 - val_loss: 14.6095 - val_mse: 14.6095 - val_mae: 1.4914 - lr: 2.0230e-04 - 5s/epoch - 5ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-10-14 12:32:51,236]\u001b[0m Finished trial#23 resulted in value: 14.6. Current best value is 14.524000000000001 with parameters: {'activation': 'relu', 'num_hidden_layer': 4, 'i': 9, 'learning_rate': 0.00033700551736380043}.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Score for fold 5: loss of 14.609536170959473\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 6s - loss: 17.2152 - mse: 17.2152 - mae: 1.6382 - val_loss: 8.7111 - val_mse: 8.7111 - val_mae: 1.5065 - lr: 1.0251e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 5s - loss: 16.6867 - mse: 16.6867 - mae: 1.6048 - val_loss: 8.9133 - val_mse: 8.9133 - val_mae: 1.7280 - lr: 1.0251e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 5s - loss: 16.6120 - mse: 16.6120 - mae: 1.6084 - val_loss: 8.7985 - val_mse: 8.7985 - val_mae: 1.4698 - lr: 1.0251e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 5s - loss: 16.4971 - mse: 16.4971 - mae: 1.6011 - val_loss: 8.6683 - val_mse: 8.6683 - val_mae: 1.5832 - lr: 1.0251e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 5s - loss: 16.5008 - mse: 16.5008 - mae: 1.6010 - val_loss: 8.6431 - val_mse: 8.6431 - val_mae: 1.5272 - lr: 1.0251e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 5s - loss: 16.5143 - mse: 16.5143 - mae: 1.5973 - val_loss: 8.6570 - val_mse: 8.6570 - val_mae: 1.5226 - lr: 1.0251e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 5s - loss: 16.4113 - mse: 16.4113 - mae: 1.5961 - val_loss: 8.8767 - val_mse: 8.8767 - val_mae: 1.5046 - lr: 1.0251e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 5s - loss: 16.3890 - mse: 16.3890 - mae: 1.5936 - val_loss: 8.6737 - val_mse: 8.6737 - val_mae: 1.4941 - lr: 1.0251e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 5s - loss: 16.3078 - mse: 16.3078 - mae: 1.5876 - val_loss: 8.6267 - val_mse: 8.6267 - val_mae: 1.5594 - lr: 1.0251e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 5s - loss: 16.2733 - mse: 16.2733 - mae: 1.5876 - val_loss: 8.6161 - val_mse: 8.6161 - val_mae: 1.5390 - lr: 1.0251e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 5s - loss: 16.3129 - mse: 16.3129 - mae: 1.5852 - val_loss: 8.5614 - val_mse: 8.5614 - val_mae: 1.5190 - lr: 1.0251e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 5s - loss: 16.2523 - mse: 16.2523 - mae: 1.5850 - val_loss: 8.5582 - val_mse: 8.5582 - val_mae: 1.5409 - lr: 1.0251e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 13/100\n",
            "1000/1000 - 5s - loss: 16.2302 - mse: 16.2302 - mae: 1.5823 - val_loss: 8.5527 - val_mse: 8.5527 - val_mae: 1.5286 - lr: 1.0251e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 14/100\n",
            "1000/1000 - 5s - loss: 16.1687 - mse: 16.1687 - mae: 1.5768 - val_loss: 8.5386 - val_mse: 8.5386 - val_mae: 1.5277 - lr: 1.0251e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 15/100\n",
            "1000/1000 - 5s - loss: 16.0908 - mse: 16.0908 - mae: 1.5777 - val_loss: 8.5643 - val_mse: 8.5643 - val_mae: 1.5316 - lr: 1.0251e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 16/100\n",
            "1000/1000 - 5s - loss: 16.1551 - mse: 16.1551 - mae: 1.5734 - val_loss: 8.6429 - val_mse: 8.6429 - val_mae: 1.4890 - lr: 1.0251e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 17/100\n",
            "1000/1000 - 5s - loss: 16.0737 - mse: 16.0737 - mae: 1.5723 - val_loss: 8.7118 - val_mse: 8.7118 - val_mae: 1.5316 - lr: 1.0251e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 18/100\n",
            "1000/1000 - 5s - loss: 16.0912 - mse: 16.0912 - mae: 1.5716 - val_loss: 8.5813 - val_mse: 8.5813 - val_mae: 1.4991 - lr: 1.0251e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 19/100\n",
            "1000/1000 - 5s - loss: 16.0339 - mse: 16.0339 - mae: 1.5698 - val_loss: 8.6642 - val_mse: 8.6642 - val_mae: 1.4792 - lr: 1.0251e-04 - 5s/epoch - 5ms/step\n",
            "Score for fold 1: loss of 8.664237022399902\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 5s - loss: 13.7090 - mse: 13.7090 - mae: 1.5457 - val_loss: 17.7016 - val_mse: 17.7016 - val_mae: 1.5800 - lr: 1.0251e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 5s - loss: 13.6698 - mse: 13.6698 - mae: 1.5390 - val_loss: 17.7003 - val_mse: 17.7003 - val_mae: 1.6614 - lr: 1.0251e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 5s - loss: 13.7206 - mse: 13.7206 - mae: 1.5401 - val_loss: 17.5848 - val_mse: 17.5848 - val_mae: 1.6157 - lr: 1.0251e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 5s - loss: 13.6451 - mse: 13.6451 - mae: 1.5371 - val_loss: 17.5889 - val_mse: 17.5889 - val_mae: 1.6246 - lr: 1.0251e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 5s - loss: 13.5699 - mse: 13.5699 - mae: 1.5326 - val_loss: 17.8949 - val_mse: 17.8949 - val_mae: 1.5847 - lr: 1.0251e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 5s - loss: 13.4996 - mse: 13.4996 - mae: 1.5337 - val_loss: 17.9398 - val_mse: 17.9398 - val_mae: 1.5641 - lr: 1.0251e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 5s - loss: 13.4613 - mse: 13.4613 - mae: 1.5278 - val_loss: 17.7569 - val_mse: 17.7569 - val_mae: 1.6735 - lr: 1.0251e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 5s - loss: 13.4734 - mse: 13.4734 - mae: 1.5292 - val_loss: 17.6766 - val_mse: 17.6766 - val_mae: 1.6301 - lr: 1.0251e-04 - 5s/epoch - 5ms/step\n",
            "Score for fold 2: loss of 17.676603317260742\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 5s - loss: 14.8987 - mse: 14.8987 - mae: 1.5586 - val_loss: 11.5021 - val_mse: 11.5021 - val_mae: 1.5547 - lr: 1.0251e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 5s - loss: 14.8682 - mse: 14.8682 - mae: 1.5465 - val_loss: 11.4143 - val_mse: 11.4143 - val_mae: 1.5592 - lr: 1.0251e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 5s - loss: 14.7195 - mse: 14.7195 - mae: 1.5456 - val_loss: 12.2145 - val_mse: 12.2145 - val_mae: 1.5080 - lr: 1.0251e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 5s - loss: 14.7463 - mse: 14.7463 - mae: 1.5414 - val_loss: 11.8761 - val_mse: 11.8761 - val_mae: 1.5838 - lr: 1.0251e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 5s - loss: 14.6848 - mse: 14.6848 - mae: 1.5389 - val_loss: 11.8436 - val_mse: 11.8436 - val_mae: 1.5571 - lr: 1.0251e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 5s - loss: 14.5895 - mse: 14.5895 - mae: 1.5344 - val_loss: 11.8152 - val_mse: 11.8152 - val_mae: 1.5361 - lr: 1.0251e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 5s - loss: 14.4787 - mse: 14.4787 - mae: 1.5333 - val_loss: 12.0509 - val_mse: 12.0509 - val_mae: 1.6025 - lr: 1.0251e-04 - 5s/epoch - 5ms/step\n",
            "Score for fold 3: loss of 12.050858497619629\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 5s - loss: 14.5524 - mse: 14.5524 - mae: 1.5332 - val_loss: 11.9408 - val_mse: 11.9408 - val_mae: 1.5074 - lr: 1.0251e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 5s - loss: 14.4235 - mse: 14.4235 - mae: 1.5235 - val_loss: 12.1406 - val_mse: 12.1406 - val_mae: 1.5988 - lr: 1.0251e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 5s - loss: 14.2492 - mse: 14.2492 - mae: 1.5181 - val_loss: 11.8964 - val_mse: 11.8964 - val_mae: 1.5619 - lr: 1.0251e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 5s - loss: 14.2700 - mse: 14.2700 - mae: 1.5101 - val_loss: 12.1038 - val_mse: 12.1038 - val_mae: 1.6184 - lr: 1.0251e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 5s - loss: 14.2454 - mse: 14.2454 - mae: 1.5133 - val_loss: 12.2923 - val_mse: 12.2923 - val_mae: 1.5422 - lr: 1.0251e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 5s - loss: 14.0149 - mse: 14.0149 - mae: 1.5079 - val_loss: 12.1610 - val_mse: 12.1610 - val_mae: 1.6116 - lr: 1.0251e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 5s - loss: 14.0839 - mse: 14.0839 - mae: 1.5077 - val_loss: 12.2971 - val_mse: 12.2971 - val_mae: 1.5607 - lr: 1.0251e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 5s - loss: 13.8555 - mse: 13.8555 - mae: 1.4960 - val_loss: 12.3098 - val_mse: 12.3098 - val_mae: 1.5589 - lr: 1.0251e-04 - 5s/epoch - 5ms/step\n",
            "Score for fold 4: loss of 12.309813499450684\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 5s - loss: 11.7254 - mse: 11.7254 - mae: 1.5156 - val_loss: 21.2039 - val_mse: 21.2039 - val_mae: 1.5957 - lr: 1.0251e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 5s - loss: 11.6357 - mse: 11.6357 - mae: 1.5109 - val_loss: 21.1680 - val_mse: 21.1680 - val_mae: 1.5647 - lr: 1.0251e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 5s - loss: 11.5276 - mse: 11.5276 - mae: 1.5011 - val_loss: 21.1823 - val_mse: 21.1823 - val_mae: 1.6088 - lr: 1.0251e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 5s - loss: 11.3458 - mse: 11.3458 - mae: 1.4990 - val_loss: 21.0826 - val_mse: 21.0826 - val_mae: 1.5640 - lr: 1.0251e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 5s - loss: 11.2055 - mse: 11.2055 - mae: 1.4936 - val_loss: 21.2227 - val_mse: 21.2227 - val_mae: 1.5563 - lr: 1.0251e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 5s - loss: 11.2055 - mse: 11.2055 - mae: 1.4939 - val_loss: 21.6149 - val_mse: 21.6149 - val_mae: 1.5118 - lr: 1.0251e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 5s - loss: 11.0713 - mse: 11.0713 - mae: 1.4862 - val_loss: 21.7441 - val_mse: 21.7441 - val_mae: 1.5099 - lr: 1.0251e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 5s - loss: 11.0358 - mse: 11.0358 - mae: 1.4856 - val_loss: 21.5322 - val_mse: 21.5322 - val_mae: 1.6233 - lr: 1.0251e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 5s - loss: 11.0359 - mse: 11.0359 - mae: 1.4749 - val_loss: 21.4415 - val_mse: 21.4415 - val_mae: 1.5695 - lr: 1.0251e-04 - 5s/epoch - 5ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-10-14 12:37:15,713]\u001b[0m Finished trial#24 resulted in value: 14.428. Current best value is 14.428 with parameters: {'activation': 'relu', 'num_hidden_layer': 4, 'i': 9, 'learning_rate': 0.00010250848120373353}.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Score for fold 5: loss of 21.441543579101562\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 4s - loss: 15.6170 - mse: 15.6170 - mae: 1.6349 - val_loss: 16.2251 - val_mse: 16.2251 - val_mae: 1.6344 - lr: 1.1021e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 3s - loss: 14.8660 - mse: 14.8660 - mae: 1.5931 - val_loss: 16.0735 - val_mse: 16.0735 - val_mae: 1.6248 - lr: 1.1021e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 3s - loss: 14.7814 - mse: 14.7814 - mae: 1.5914 - val_loss: 16.0144 - val_mse: 16.0144 - val_mae: 1.5757 - lr: 1.1021e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 4s - loss: 14.7594 - mse: 14.7594 - mae: 1.5892 - val_loss: 16.0610 - val_mse: 16.0610 - val_mae: 1.5944 - lr: 1.1021e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 4s - loss: 14.6878 - mse: 14.6878 - mae: 1.5884 - val_loss: 16.0082 - val_mse: 16.0082 - val_mae: 1.5993 - lr: 1.1021e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 4s - loss: 14.6399 - mse: 14.6399 - mae: 1.5862 - val_loss: 16.0971 - val_mse: 16.0971 - val_mae: 1.5372 - lr: 1.1021e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 4s - loss: 14.6617 - mse: 14.6617 - mae: 1.5833 - val_loss: 16.0627 - val_mse: 16.0627 - val_mae: 1.5486 - lr: 1.1021e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 4s - loss: 14.6155 - mse: 14.6155 - mae: 1.5835 - val_loss: 16.1068 - val_mse: 16.1068 - val_mae: 1.5852 - lr: 1.1021e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 4s - loss: 14.5932 - mse: 14.5932 - mae: 1.5796 - val_loss: 16.0110 - val_mse: 16.0110 - val_mae: 1.5688 - lr: 1.1021e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 4s - loss: 14.5377 - mse: 14.5377 - mae: 1.5793 - val_loss: 16.1522 - val_mse: 16.1522 - val_mae: 1.5309 - lr: 1.1021e-04 - 4s/epoch - 4ms/step\n",
            "Score for fold 1: loss of 16.152223587036133\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 4s - loss: 12.9249 - mse: 12.9249 - mae: 1.5667 - val_loss: 22.5681 - val_mse: 22.5681 - val_mae: 1.5863 - lr: 1.1021e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 3s - loss: 12.9421 - mse: 12.9421 - mae: 1.5644 - val_loss: 22.5727 - val_mse: 22.5727 - val_mae: 1.5692 - lr: 1.1021e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 3s - loss: 12.8818 - mse: 12.8818 - mae: 1.5629 - val_loss: 22.5351 - val_mse: 22.5351 - val_mae: 1.5836 - lr: 1.1021e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 3s - loss: 12.8470 - mse: 12.8470 - mae: 1.5610 - val_loss: 22.4437 - val_mse: 22.4437 - val_mae: 1.6162 - lr: 1.1021e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 3s - loss: 12.8293 - mse: 12.8293 - mae: 1.5596 - val_loss: 22.4819 - val_mse: 22.4819 - val_mae: 1.5657 - lr: 1.1021e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 3s - loss: 12.7972 - mse: 12.7972 - mae: 1.5569 - val_loss: 22.5708 - val_mse: 22.5708 - val_mae: 1.5664 - lr: 1.1021e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 3s - loss: 12.8420 - mse: 12.8420 - mae: 1.5573 - val_loss: 22.4613 - val_mse: 22.4613 - val_mae: 1.6563 - lr: 1.1021e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 3s - loss: 12.7626 - mse: 12.7626 - mae: 1.5540 - val_loss: 22.6528 - val_mse: 22.6528 - val_mae: 1.5944 - lr: 1.1021e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 3s - loss: 12.7541 - mse: 12.7541 - mae: 1.5551 - val_loss: 22.4756 - val_mse: 22.4756 - val_mae: 1.6318 - lr: 1.1021e-04 - 3s/epoch - 3ms/step\n",
            "Score for fold 2: loss of 22.475576400756836\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 3s - loss: 15.1367 - mse: 15.1367 - mae: 1.5664 - val_loss: 13.1832 - val_mse: 13.1832 - val_mae: 1.5299 - lr: 1.1021e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 4s - loss: 15.1279 - mse: 15.1279 - mae: 1.5650 - val_loss: 13.0769 - val_mse: 13.0769 - val_mae: 1.5420 - lr: 1.1021e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 3s - loss: 15.0654 - mse: 15.0654 - mae: 1.5654 - val_loss: 13.1300 - val_mse: 13.1300 - val_mae: 1.5423 - lr: 1.1021e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 3s - loss: 15.0197 - mse: 15.0197 - mae: 1.5643 - val_loss: 13.1563 - val_mse: 13.1563 - val_mae: 1.5299 - lr: 1.1021e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 3s - loss: 15.0115 - mse: 15.0115 - mae: 1.5642 - val_loss: 13.1343 - val_mse: 13.1343 - val_mae: 1.5708 - lr: 1.1021e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 3s - loss: 15.0340 - mse: 15.0340 - mae: 1.5616 - val_loss: 13.3462 - val_mse: 13.3462 - val_mae: 1.5325 - lr: 1.1021e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 3s - loss: 14.9558 - mse: 14.9558 - mae: 1.5632 - val_loss: 13.2377 - val_mse: 13.2377 - val_mae: 1.5043 - lr: 1.1021e-04 - 3s/epoch - 3ms/step\n",
            "Score for fold 3: loss of 13.237713813781738\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 3s - loss: 15.3920 - mse: 15.3920 - mae: 1.5591 - val_loss: 11.5134 - val_mse: 11.5134 - val_mae: 1.5957 - lr: 1.1021e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 3s - loss: 15.3313 - mse: 15.3313 - mae: 1.5592 - val_loss: 11.7043 - val_mse: 11.7043 - val_mae: 1.5629 - lr: 1.1021e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 3s - loss: 15.3020 - mse: 15.3020 - mae: 1.5574 - val_loss: 11.6892 - val_mse: 11.6892 - val_mae: 1.5889 - lr: 1.1021e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 3s - loss: 15.2530 - mse: 15.2530 - mae: 1.5561 - val_loss: 11.9524 - val_mse: 11.9524 - val_mae: 1.6296 - lr: 1.1021e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 3s - loss: 15.2081 - mse: 15.2081 - mae: 1.5576 - val_loss: 11.7676 - val_mse: 11.7676 - val_mae: 1.5249 - lr: 1.1021e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 3s - loss: 15.1693 - mse: 15.1693 - mae: 1.5515 - val_loss: 11.7628 - val_mse: 11.7628 - val_mae: 1.6144 - lr: 1.1021e-04 - 3s/epoch - 3ms/step\n",
            "Score for fold 4: loss of 11.762786865234375\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 3s - loss: 15.6341 - mse: 15.6341 - mae: 1.5534 - val_loss: 10.2019 - val_mse: 10.2019 - val_mae: 1.5438 - lr: 1.1021e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 3s - loss: 15.5970 - mse: 15.5970 - mae: 1.5512 - val_loss: 10.2321 - val_mse: 10.2321 - val_mae: 1.5181 - lr: 1.1021e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 3s - loss: 15.5574 - mse: 15.5574 - mae: 1.5509 - val_loss: 10.2828 - val_mse: 10.2828 - val_mae: 1.5778 - lr: 1.1021e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 3s - loss: 15.4722 - mse: 15.4722 - mae: 1.5449 - val_loss: 10.3061 - val_mse: 10.3061 - val_mae: 1.5666 - lr: 1.1021e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 3s - loss: 15.5062 - mse: 15.5062 - mae: 1.5444 - val_loss: 10.2169 - val_mse: 10.2169 - val_mae: 1.5553 - lr: 1.1021e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 3s - loss: 15.3890 - mse: 15.3890 - mae: 1.5389 - val_loss: 10.1359 - val_mse: 10.1359 - val_mae: 1.5891 - lr: 1.1021e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 3s - loss: 15.3553 - mse: 15.3553 - mae: 1.5402 - val_loss: 10.3212 - val_mse: 10.3212 - val_mae: 1.5887 - lr: 1.1021e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 3s - loss: 15.2442 - mse: 15.2442 - mae: 1.5407 - val_loss: 10.2359 - val_mse: 10.2359 - val_mae: 1.5730 - lr: 1.1021e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 3s - loss: 15.2522 - mse: 15.2522 - mae: 1.5358 - val_loss: 10.2297 - val_mse: 10.2297 - val_mae: 1.6159 - lr: 1.1021e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 3s - loss: 15.1894 - mse: 15.1894 - mae: 1.5365 - val_loss: 10.4477 - val_mse: 10.4477 - val_mae: 1.5500 - lr: 1.1021e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 3s - loss: 15.1915 - mse: 15.1915 - mae: 1.5334 - val_loss: 10.3550 - val_mse: 10.3550 - val_mae: 1.5567 - lr: 1.1021e-04 - 3s/epoch - 3ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-10-14 12:39:51,070]\u001b[0m Finished trial#25 resulted in value: 14.797999999999998. Current best value is 14.428 with parameters: {'activation': 'relu', 'num_hidden_layer': 4, 'i': 9, 'learning_rate': 0.00010250848120373353}.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Score for fold 5: loss of 10.355024337768555\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 6s - loss: 16.2680 - mse: 16.2680 - mae: 1.6217 - val_loss: 11.9966 - val_mse: 11.9966 - val_mae: 1.7166 - lr: 3.9935e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 5s - loss: 15.9202 - mse: 15.9202 - mae: 1.6118 - val_loss: 11.7965 - val_mse: 11.7965 - val_mae: 1.5455 - lr: 3.9935e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 5s - loss: 15.8158 - mse: 15.8158 - mae: 1.6041 - val_loss: 11.8117 - val_mse: 11.8117 - val_mae: 1.5842 - lr: 3.9935e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 5s - loss: 15.7321 - mse: 15.7321 - mae: 1.6018 - val_loss: 11.8484 - val_mse: 11.8484 - val_mae: 1.5492 - lr: 3.9935e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 5s - loss: 15.7483 - mse: 15.7483 - mae: 1.5962 - val_loss: 11.9187 - val_mse: 11.9187 - val_mae: 1.5676 - lr: 3.9935e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 5s - loss: 15.7102 - mse: 15.7102 - mae: 1.5961 - val_loss: 11.8646 - val_mse: 11.8646 - val_mae: 1.5718 - lr: 3.9935e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 5s - loss: 15.6228 - mse: 15.6228 - mae: 1.5885 - val_loss: 11.7716 - val_mse: 11.7716 - val_mae: 1.5438 - lr: 3.9935e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 5s - loss: 15.6438 - mse: 15.6438 - mae: 1.5888 - val_loss: 11.6288 - val_mse: 11.6288 - val_mae: 1.5631 - lr: 3.9935e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 5s - loss: 15.5078 - mse: 15.5078 - mae: 1.5883 - val_loss: 11.7815 - val_mse: 11.7815 - val_mae: 1.5270 - lr: 3.9935e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 5s - loss: 15.4747 - mse: 15.4747 - mae: 1.5865 - val_loss: 11.6910 - val_mse: 11.6910 - val_mae: 1.5463 - lr: 3.9935e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 5s - loss: 15.4207 - mse: 15.4207 - mae: 1.5865 - val_loss: 11.8612 - val_mse: 11.8612 - val_mae: 1.4968 - lr: 3.9935e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 5s - loss: 15.4230 - mse: 15.4230 - mae: 1.5819 - val_loss: 11.7666 - val_mse: 11.7666 - val_mae: 1.4803 - lr: 3.9935e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 13/100\n",
            "1000/1000 - 5s - loss: 15.2884 - mse: 15.2884 - mae: 1.5792 - val_loss: 11.9674 - val_mse: 11.9674 - val_mae: 1.6009 - lr: 3.9935e-04 - 5s/epoch - 5ms/step\n",
            "Score for fold 1: loss of 11.967401504516602\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 5s - loss: 15.0152 - mse: 15.0152 - mae: 1.5694 - val_loss: 13.4574 - val_mse: 13.4574 - val_mae: 1.5284 - lr: 3.9935e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 5s - loss: 15.0013 - mse: 15.0013 - mae: 1.5692 - val_loss: 13.2480 - val_mse: 13.2480 - val_mae: 1.6180 - lr: 3.9935e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 5s - loss: 14.9400 - mse: 14.9400 - mae: 1.5688 - val_loss: 13.2686 - val_mse: 13.2686 - val_mae: 1.5543 - lr: 3.9935e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 5s - loss: 14.8033 - mse: 14.8033 - mae: 1.5661 - val_loss: 13.1429 - val_mse: 13.1429 - val_mae: 1.7310 - lr: 3.9935e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 5s - loss: 14.8138 - mse: 14.8138 - mae: 1.5676 - val_loss: 13.3438 - val_mse: 13.3438 - val_mae: 1.5171 - lr: 3.9935e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 5s - loss: 14.7895 - mse: 14.7895 - mae: 1.5674 - val_loss: 13.6353 - val_mse: 13.6353 - val_mae: 1.5601 - lr: 3.9935e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 5s - loss: 14.7140 - mse: 14.7140 - mae: 1.5749 - val_loss: 13.8869 - val_mse: 13.8869 - val_mae: 1.9647 - lr: 3.9935e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 5s - loss: 14.7619 - mse: 14.7619 - mae: 1.5704 - val_loss: 13.4589 - val_mse: 13.4589 - val_mae: 1.5146 - lr: 3.9935e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 5s - loss: 14.6207 - mse: 14.6207 - mae: 1.5606 - val_loss: 13.3020 - val_mse: 13.3020 - val_mae: 1.7020 - lr: 3.9935e-04 - 5s/epoch - 5ms/step\n",
            "Score for fold 2: loss of 13.30200481414795\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 5s - loss: 14.0878 - mse: 14.0878 - mae: 1.5804 - val_loss: 16.0240 - val_mse: 16.0240 - val_mae: 1.5430 - lr: 3.9935e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 5s - loss: 13.9598 - mse: 13.9598 - mae: 1.5811 - val_loss: 16.0360 - val_mse: 16.0360 - val_mae: 1.6106 - lr: 3.9935e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 5s - loss: 13.7048 - mse: 13.7048 - mae: 1.5869 - val_loss: 16.1057 - val_mse: 16.1057 - val_mae: 1.6307 - lr: 3.9935e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 5s - loss: 13.8133 - mse: 13.8133 - mae: 1.5799 - val_loss: 16.1386 - val_mse: 16.1386 - val_mae: 1.6822 - lr: 3.9935e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 5s - loss: 13.8220 - mse: 13.8220 - mae: 1.5775 - val_loss: 16.2328 - val_mse: 16.2328 - val_mae: 1.6353 - lr: 3.9935e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 5s - loss: 13.7439 - mse: 13.7439 - mae: 1.5826 - val_loss: 16.3644 - val_mse: 16.3644 - val_mae: 1.5635 - lr: 3.9935e-04 - 5s/epoch - 5ms/step\n",
            "Score for fold 3: loss of 16.36436653137207\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 5s - loss: 12.4544 - mse: 12.4544 - mae: 1.5913 - val_loss: 20.9639 - val_mse: 20.9639 - val_mae: 1.6661 - lr: 3.9935e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 5s - loss: 12.3737 - mse: 12.3737 - mae: 1.6030 - val_loss: 21.8034 - val_mse: 21.8034 - val_mae: 1.5994 - lr: 3.9935e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 5s - loss: 12.3249 - mse: 12.3249 - mae: 1.5976 - val_loss: 21.4817 - val_mse: 21.4817 - val_mae: 1.5447 - lr: 3.9935e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 5s - loss: 12.2253 - mse: 12.2253 - mae: 1.6031 - val_loss: 21.9811 - val_mse: 21.9811 - val_mae: 1.4865 - lr: 3.9935e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 5s - loss: 12.1459 - mse: 12.1459 - mae: 1.5996 - val_loss: 22.0742 - val_mse: 22.0742 - val_mae: 1.9117 - lr: 3.9935e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 5s - loss: 12.1877 - mse: 12.1877 - mae: 1.6067 - val_loss: 21.6944 - val_mse: 21.6944 - val_mae: 1.7351 - lr: 3.9935e-04 - 5s/epoch - 5ms/step\n",
            "Score for fold 4: loss of 21.69443702697754\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 5s - loss: 15.3117 - mse: 15.3117 - mae: 1.6124 - val_loss: 9.2030 - val_mse: 9.2030 - val_mae: 1.6185 - lr: 3.9935e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 5s - loss: 15.2541 - mse: 15.2541 - mae: 1.6407 - val_loss: 9.4933 - val_mse: 9.4933 - val_mae: 1.5216 - lr: 3.9935e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 5s - loss: 15.2529 - mse: 15.2529 - mae: 1.6453 - val_loss: 9.5976 - val_mse: 9.5976 - val_mae: 1.6583 - lr: 3.9935e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 5s - loss: 14.8671 - mse: 14.8671 - mae: 1.6147 - val_loss: 9.8124 - val_mse: 9.8124 - val_mae: 1.4562 - lr: 3.9935e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 5s - loss: 14.9671 - mse: 14.9671 - mae: 1.6344 - val_loss: 9.7986 - val_mse: 9.7986 - val_mae: 1.7336 - lr: 3.9935e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 5s - loss: 14.9599 - mse: 14.9599 - mae: 1.6348 - val_loss: 10.2272 - val_mse: 10.2272 - val_mae: 1.5959 - lr: 3.9935e-04 - 5s/epoch - 5ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-10-14 12:43:19,724]\u001b[0m Finished trial#26 resulted in value: 14.710000000000003. Current best value is 14.428 with parameters: {'activation': 'relu', 'num_hidden_layer': 4, 'i': 9, 'learning_rate': 0.00010250848120373353}.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Score for fold 5: loss of 10.227238655090332\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 4s - loss: 12.7424 - mse: 12.7424 - mae: 1.6116 - val_loss: 27.4896 - val_mse: 27.4896 - val_mae: 1.6074 - lr: 1.4667e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 3s - loss: 11.9518 - mse: 11.9518 - mae: 1.5796 - val_loss: 27.4246 - val_mse: 27.4246 - val_mae: 1.6306 - lr: 1.4667e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 3s - loss: 11.8796 - mse: 11.8796 - mae: 1.5777 - val_loss: 27.3868 - val_mse: 27.3868 - val_mae: 1.6337 - lr: 1.4667e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 3s - loss: 11.8668 - mse: 11.8668 - mae: 1.5775 - val_loss: 27.3938 - val_mse: 27.3938 - val_mae: 1.6354 - lr: 1.4667e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 4s - loss: 11.7831 - mse: 11.7831 - mae: 1.5742 - val_loss: 27.3965 - val_mse: 27.3965 - val_mae: 1.6254 - lr: 1.4667e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 4s - loss: 11.7793 - mse: 11.7793 - mae: 1.5718 - val_loss: 27.3701 - val_mse: 27.3701 - val_mae: 1.6042 - lr: 1.4667e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 3s - loss: 11.7520 - mse: 11.7520 - mae: 1.5682 - val_loss: 27.4239 - val_mse: 27.4239 - val_mae: 1.5988 - lr: 1.4667e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 3s - loss: 11.7197 - mse: 11.7197 - mae: 1.5673 - val_loss: 27.3938 - val_mse: 27.3938 - val_mae: 1.5906 - lr: 1.4667e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 3s - loss: 11.6844 - mse: 11.6844 - mae: 1.5639 - val_loss: 27.3967 - val_mse: 27.3967 - val_mae: 1.5910 - lr: 1.4667e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 3s - loss: 11.6407 - mse: 11.6407 - mae: 1.5609 - val_loss: 27.3526 - val_mse: 27.3526 - val_mae: 1.6418 - lr: 1.4667e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 3s - loss: 11.5928 - mse: 11.5928 - mae: 1.5608 - val_loss: 27.3397 - val_mse: 27.3397 - val_mae: 1.5975 - lr: 1.4667e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 4s - loss: 11.5477 - mse: 11.5477 - mae: 1.5570 - val_loss: 27.3296 - val_mse: 27.3296 - val_mae: 1.6383 - lr: 1.4667e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 13/100\n",
            "1000/1000 - 4s - loss: 11.5475 - mse: 11.5475 - mae: 1.5541 - val_loss: 27.4312 - val_mse: 27.4312 - val_mae: 1.5867 - lr: 1.4667e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 14/100\n",
            "1000/1000 - 3s - loss: 11.5222 - mse: 11.5222 - mae: 1.5545 - val_loss: 27.3847 - val_mse: 27.3847 - val_mae: 1.5796 - lr: 1.4667e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 15/100\n",
            "1000/1000 - 4s - loss: 11.4784 - mse: 11.4784 - mae: 1.5549 - val_loss: 27.4887 - val_mse: 27.4887 - val_mae: 1.6362 - lr: 1.4667e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 16/100\n",
            "1000/1000 - 4s - loss: 11.4613 - mse: 11.4613 - mae: 1.5538 - val_loss: 27.5395 - val_mse: 27.5395 - val_mae: 1.6106 - lr: 1.4667e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 17/100\n",
            "1000/1000 - 4s - loss: 11.4566 - mse: 11.4566 - mae: 1.5509 - val_loss: 27.3926 - val_mse: 27.3926 - val_mae: 1.6518 - lr: 1.4667e-04 - 4s/epoch - 4ms/step\n",
            "Score for fold 1: loss of 27.39263343811035\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 3s - loss: 15.1765 - mse: 15.1765 - mae: 1.5631 - val_loss: 12.5043 - val_mse: 12.5043 - val_mae: 1.5703 - lr: 1.4667e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 4s - loss: 15.1122 - mse: 15.1122 - mae: 1.5635 - val_loss: 12.5831 - val_mse: 12.5831 - val_mae: 1.5996 - lr: 1.4667e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 4s - loss: 15.1599 - mse: 15.1599 - mae: 1.5620 - val_loss: 12.5484 - val_mse: 12.5484 - val_mae: 1.5735 - lr: 1.4667e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 3s - loss: 15.0789 - mse: 15.0789 - mae: 1.5583 - val_loss: 12.7351 - val_mse: 12.7351 - val_mae: 1.5263 - lr: 1.4667e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 4s - loss: 14.9656 - mse: 14.9656 - mae: 1.5597 - val_loss: 12.5369 - val_mse: 12.5369 - val_mae: 1.5880 - lr: 1.4667e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 4s - loss: 15.0663 - mse: 15.0663 - mae: 1.5522 - val_loss: 12.5721 - val_mse: 12.5721 - val_mae: 1.6053 - lr: 1.4667e-04 - 4s/epoch - 4ms/step\n",
            "Score for fold 2: loss of 12.57213020324707\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 3s - loss: 15.9409 - mse: 15.9409 - mae: 1.5709 - val_loss: 9.0575 - val_mse: 9.0575 - val_mae: 1.4984 - lr: 1.4667e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 3s - loss: 15.9127 - mse: 15.9127 - mae: 1.5685 - val_loss: 8.9911 - val_mse: 8.9911 - val_mae: 1.5019 - lr: 1.4667e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 3s - loss: 15.8112 - mse: 15.8112 - mae: 1.5634 - val_loss: 9.3729 - val_mse: 9.3729 - val_mae: 1.5357 - lr: 1.4667e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 3s - loss: 15.8096 - mse: 15.8096 - mae: 1.5615 - val_loss: 9.1566 - val_mse: 9.1566 - val_mae: 1.5256 - lr: 1.4667e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 4s - loss: 15.7115 - mse: 15.7115 - mae: 1.5639 - val_loss: 9.1737 - val_mse: 9.1737 - val_mae: 1.6132 - lr: 1.4667e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 3s - loss: 15.6357 - mse: 15.6357 - mae: 1.5602 - val_loss: 9.2094 - val_mse: 9.2094 - val_mae: 1.5539 - lr: 1.4667e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 3s - loss: 15.6343 - mse: 15.6343 - mae: 1.5533 - val_loss: 9.2695 - val_mse: 9.2695 - val_mae: 1.5065 - lr: 1.4667e-04 - 3s/epoch - 3ms/step\n",
            "Score for fold 3: loss of 9.269490242004395\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 4s - loss: 15.0265 - mse: 15.0265 - mae: 1.5575 - val_loss: 11.4151 - val_mse: 11.4151 - val_mae: 1.5335 - lr: 1.4667e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 3s - loss: 14.9649 - mse: 14.9649 - mae: 1.5557 - val_loss: 11.4761 - val_mse: 11.4761 - val_mae: 1.5389 - lr: 1.4667e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 3s - loss: 14.8904 - mse: 14.8904 - mae: 1.5482 - val_loss: 11.3169 - val_mse: 11.3169 - val_mae: 1.5551 - lr: 1.4667e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 3s - loss: 14.8065 - mse: 14.8065 - mae: 1.5456 - val_loss: 11.7132 - val_mse: 11.7132 - val_mae: 1.5240 - lr: 1.4667e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 3s - loss: 14.7401 - mse: 14.7401 - mae: 1.5421 - val_loss: 11.7114 - val_mse: 11.7114 - val_mae: 1.5374 - lr: 1.4667e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 3s - loss: 14.7716 - mse: 14.7716 - mae: 1.5446 - val_loss: 11.6348 - val_mse: 11.6348 - val_mae: 1.5416 - lr: 1.4667e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 4s - loss: 14.6317 - mse: 14.6317 - mae: 1.5417 - val_loss: 11.3227 - val_mse: 11.3227 - val_mae: 1.4650 - lr: 1.4667e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 4s - loss: 14.5475 - mse: 14.5475 - mae: 1.5302 - val_loss: 11.5536 - val_mse: 11.5536 - val_mae: 1.5191 - lr: 1.4667e-04 - 4s/epoch - 4ms/step\n",
            "Score for fold 4: loss of 11.55359172821045\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 4s - loss: 14.6108 - mse: 14.6108 - mae: 1.5445 - val_loss: 11.5502 - val_mse: 11.5502 - val_mae: 1.5943 - lr: 1.4667e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 4s - loss: 14.4858 - mse: 14.4858 - mae: 1.5399 - val_loss: 12.2989 - val_mse: 12.2989 - val_mae: 1.5179 - lr: 1.4667e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 4s - loss: 14.4098 - mse: 14.4098 - mae: 1.5405 - val_loss: 12.3661 - val_mse: 12.3661 - val_mae: 1.4939 - lr: 1.4667e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 4s - loss: 14.3765 - mse: 14.3765 - mae: 1.5326 - val_loss: 11.8720 - val_mse: 11.8720 - val_mae: 1.5870 - lr: 1.4667e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 4s - loss: 14.3368 - mse: 14.3368 - mae: 1.5301 - val_loss: 12.2286 - val_mse: 12.2286 - val_mae: 1.6515 - lr: 1.4667e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 3s - loss: 14.2311 - mse: 14.2311 - mae: 1.5207 - val_loss: 11.7443 - val_mse: 11.7443 - val_mae: 1.6021 - lr: 1.4667e-04 - 3s/epoch - 3ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-10-14 12:45:59,640]\u001b[0m Finished trial#27 resulted in value: 14.504. Current best value is 14.428 with parameters: {'activation': 'relu', 'num_hidden_layer': 4, 'i': 9, 'learning_rate': 0.00010250848120373353}.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Score for fold 5: loss of 11.74429988861084\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 4s - loss: 13.6983 - mse: 13.6983 - mae: 1.6301 - val_loss: 23.8183 - val_mse: 23.8183 - val_mae: 1.6187 - lr: 1.0565e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 3s - loss: 12.9192 - mse: 12.9192 - mae: 1.5847 - val_loss: 23.6630 - val_mse: 23.6630 - val_mae: 1.6885 - lr: 1.0565e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 3s - loss: 12.8720 - mse: 12.8720 - mae: 1.5780 - val_loss: 23.6319 - val_mse: 23.6319 - val_mae: 1.6472 - lr: 1.0565e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 3s - loss: 12.7915 - mse: 12.7915 - mae: 1.5770 - val_loss: 23.7453 - val_mse: 23.7453 - val_mae: 1.5845 - lr: 1.0565e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 3s - loss: 12.7875 - mse: 12.7875 - mae: 1.5779 - val_loss: 23.5621 - val_mse: 23.5621 - val_mae: 1.5945 - lr: 1.0565e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 3s - loss: 12.7815 - mse: 12.7815 - mae: 1.5728 - val_loss: 23.6353 - val_mse: 23.6353 - val_mae: 1.6014 - lr: 1.0565e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 4s - loss: 12.7273 - mse: 12.7273 - mae: 1.5742 - val_loss: 23.6263 - val_mse: 23.6263 - val_mae: 1.6005 - lr: 1.0565e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 4s - loss: 12.7236 - mse: 12.7236 - mae: 1.5710 - val_loss: 23.5202 - val_mse: 23.5202 - val_mae: 1.6547 - lr: 1.0565e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 3s - loss: 12.6823 - mse: 12.6823 - mae: 1.5713 - val_loss: 23.4818 - val_mse: 23.4818 - val_mae: 1.6773 - lr: 1.0565e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 3s - loss: 12.6998 - mse: 12.6998 - mae: 1.5693 - val_loss: 23.4765 - val_mse: 23.4765 - val_mae: 1.6072 - lr: 1.0565e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 3s - loss: 12.6418 - mse: 12.6418 - mae: 1.5673 - val_loss: 23.4142 - val_mse: 23.4142 - val_mae: 1.6164 - lr: 1.0565e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 3s - loss: 12.6290 - mse: 12.6290 - mae: 1.5685 - val_loss: 23.5949 - val_mse: 23.5949 - val_mae: 1.6713 - lr: 1.0565e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 13/100\n",
            "1000/1000 - 4s - loss: 12.6125 - mse: 12.6125 - mae: 1.5633 - val_loss: 23.4328 - val_mse: 23.4328 - val_mae: 1.6131 - lr: 1.0565e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 14/100\n",
            "1000/1000 - 3s - loss: 12.5553 - mse: 12.5553 - mae: 1.5635 - val_loss: 23.6532 - val_mse: 23.6532 - val_mae: 1.6109 - lr: 1.0565e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 15/100\n",
            "1000/1000 - 3s - loss: 12.5280 - mse: 12.5280 - mae: 1.5603 - val_loss: 23.4173 - val_mse: 23.4173 - val_mae: 1.6850 - lr: 1.0565e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 16/100\n",
            "1000/1000 - 3s - loss: 12.5298 - mse: 12.5298 - mae: 1.5591 - val_loss: 23.4491 - val_mse: 23.4491 - val_mae: 1.6785 - lr: 1.0565e-04 - 3s/epoch - 3ms/step\n",
            "Score for fold 1: loss of 23.449077606201172\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 3s - loss: 15.3830 - mse: 15.3830 - mae: 1.5718 - val_loss: 12.1154 - val_mse: 12.1154 - val_mae: 1.5769 - lr: 1.0565e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 3s - loss: 15.4132 - mse: 15.4132 - mae: 1.5748 - val_loss: 11.9774 - val_mse: 11.9774 - val_mae: 1.5090 - lr: 1.0565e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 3s - loss: 15.3726 - mse: 15.3726 - mae: 1.5730 - val_loss: 12.0442 - val_mse: 12.0442 - val_mae: 1.5073 - lr: 1.0565e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 3s - loss: 15.3255 - mse: 15.3255 - mae: 1.5724 - val_loss: 12.1401 - val_mse: 12.1401 - val_mae: 1.5524 - lr: 1.0565e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 3s - loss: 15.2855 - mse: 15.2855 - mae: 1.5710 - val_loss: 12.0863 - val_mse: 12.0863 - val_mae: 1.5698 - lr: 1.0565e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 3s - loss: 15.2538 - mse: 15.2538 - mae: 1.5671 - val_loss: 11.9883 - val_mse: 11.9883 - val_mae: 1.5328 - lr: 1.0565e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 3s - loss: 15.2647 - mse: 15.2647 - mae: 1.5696 - val_loss: 11.9814 - val_mse: 11.9814 - val_mae: 1.5357 - lr: 1.0565e-04 - 3s/epoch - 3ms/step\n",
            "Score for fold 2: loss of 11.981412887573242\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 3s - loss: 14.8862 - mse: 14.8862 - mae: 1.5675 - val_loss: 13.5599 - val_mse: 13.5599 - val_mae: 1.5120 - lr: 1.0565e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 3s - loss: 14.8715 - mse: 14.8715 - mae: 1.5641 - val_loss: 13.4892 - val_mse: 13.4892 - val_mae: 1.5884 - lr: 1.0565e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 3s - loss: 14.7812 - mse: 14.7812 - mae: 1.5639 - val_loss: 13.4163 - val_mse: 13.4163 - val_mae: 1.5643 - lr: 1.0565e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 3s - loss: 14.7995 - mse: 14.7995 - mae: 1.5644 - val_loss: 13.4599 - val_mse: 13.4599 - val_mae: 1.5595 - lr: 1.0565e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 3s - loss: 14.8298 - mse: 14.8298 - mae: 1.5634 - val_loss: 13.5134 - val_mse: 13.5134 - val_mae: 1.6204 - lr: 1.0565e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 3s - loss: 14.7534 - mse: 14.7534 - mae: 1.5606 - val_loss: 13.5426 - val_mse: 13.5426 - val_mae: 1.5546 - lr: 1.0565e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 3s - loss: 14.7354 - mse: 14.7354 - mae: 1.5579 - val_loss: 13.5639 - val_mse: 13.5639 - val_mae: 1.5278 - lr: 1.0565e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 3s - loss: 14.7069 - mse: 14.7069 - mae: 1.5578 - val_loss: 13.6824 - val_mse: 13.6824 - val_mae: 1.5426 - lr: 1.0565e-04 - 3s/epoch - 3ms/step\n",
            "Score for fold 3: loss of 13.682454109191895\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 3s - loss: 14.8763 - mse: 14.8763 - mae: 1.5534 - val_loss: 13.1527 - val_mse: 13.1527 - val_mae: 1.5590 - lr: 1.0565e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 3s - loss: 14.8386 - mse: 14.8386 - mae: 1.5521 - val_loss: 13.1531 - val_mse: 13.1531 - val_mae: 1.5792 - lr: 1.0565e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 3s - loss: 14.8321 - mse: 14.8321 - mae: 1.5489 - val_loss: 13.2915 - val_mse: 13.2915 - val_mae: 1.5410 - lr: 1.0565e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 3s - loss: 14.7561 - mse: 14.7561 - mae: 1.5474 - val_loss: 13.4372 - val_mse: 13.4372 - val_mae: 1.6116 - lr: 1.0565e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 3s - loss: 14.7015 - mse: 14.7015 - mae: 1.5437 - val_loss: 13.1110 - val_mse: 13.1110 - val_mae: 1.5938 - lr: 1.0565e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 3s - loss: 14.7055 - mse: 14.7055 - mae: 1.5440 - val_loss: 13.3705 - val_mse: 13.3705 - val_mae: 1.5454 - lr: 1.0565e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 3s - loss: 14.6389 - mse: 14.6389 - mae: 1.5385 - val_loss: 13.0559 - val_mse: 13.0559 - val_mae: 1.5839 - lr: 1.0565e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 3s - loss: 14.5587 - mse: 14.5587 - mae: 1.5383 - val_loss: 13.0555 - val_mse: 13.0555 - val_mae: 1.6249 - lr: 1.0565e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 3s - loss: 14.5586 - mse: 14.5586 - mae: 1.5353 - val_loss: 13.5321 - val_mse: 13.5321 - val_mae: 1.5297 - lr: 1.0565e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 3s - loss: 14.5358 - mse: 14.5358 - mae: 1.5355 - val_loss: 13.3624 - val_mse: 13.3624 - val_mae: 1.5317 - lr: 1.0565e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 3s - loss: 14.5116 - mse: 14.5116 - mae: 1.5392 - val_loss: 13.3286 - val_mse: 13.3286 - val_mae: 1.5362 - lr: 1.0565e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 3s - loss: 14.4395 - mse: 14.4395 - mae: 1.5327 - val_loss: 13.5169 - val_mse: 13.5169 - val_mae: 1.5760 - lr: 1.0565e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 13/100\n",
            "1000/1000 - 3s - loss: 14.4307 - mse: 14.4307 - mae: 1.5344 - val_loss: 13.2897 - val_mse: 13.2897 - val_mae: 1.5467 - lr: 1.0565e-04 - 3s/epoch - 3ms/step\n",
            "Score for fold 4: loss of 13.289713859558105\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 4s - loss: 15.1123 - mse: 15.1123 - mae: 1.5399 - val_loss: 10.9119 - val_mse: 10.9119 - val_mae: 1.5037 - lr: 1.0565e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 3s - loss: 14.9902 - mse: 14.9902 - mae: 1.5371 - val_loss: 10.5561 - val_mse: 10.5561 - val_mae: 1.5984 - lr: 1.0565e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 3s - loss: 15.0011 - mse: 15.0011 - mae: 1.5354 - val_loss: 10.9145 - val_mse: 10.9145 - val_mae: 1.5449 - lr: 1.0565e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 3s - loss: 14.9008 - mse: 14.9008 - mae: 1.5320 - val_loss: 10.9598 - val_mse: 10.9598 - val_mae: 1.5357 - lr: 1.0565e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 3s - loss: 15.0303 - mse: 15.0303 - mae: 1.5324 - val_loss: 10.6995 - val_mse: 10.6995 - val_mae: 1.5497 - lr: 1.0565e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 3s - loss: 14.8417 - mse: 14.8417 - mae: 1.5236 - val_loss: 10.6926 - val_mse: 10.6926 - val_mae: 1.6029 - lr: 1.0565e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 3s - loss: 14.8899 - mse: 14.8899 - mae: 1.5265 - val_loss: 10.9402 - val_mse: 10.9402 - val_mae: 1.4944 - lr: 1.0565e-04 - 3s/epoch - 3ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-10-14 12:49:02,043]\u001b[0m Finished trial#28 resulted in value: 14.668000000000001. Current best value is 14.428 with parameters: {'activation': 'relu', 'num_hidden_layer': 4, 'i': 9, 'learning_rate': 0.00010250848120373353}.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Score for fold 5: loss of 10.94020938873291\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 4s - loss: 15.8386 - mse: 15.8386 - mae: 1.6216 - val_loss: 15.1144 - val_mse: 15.1144 - val_mae: 1.5472 - lr: 1.3698e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 3s - loss: 15.1965 - mse: 15.1965 - mae: 1.5825 - val_loss: 14.9567 - val_mse: 14.9567 - val_mae: 1.5510 - lr: 1.3698e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 3s - loss: 15.1046 - mse: 15.1046 - mae: 1.5861 - val_loss: 14.6516 - val_mse: 14.6516 - val_mae: 1.5817 - lr: 1.3698e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 3s - loss: 15.0606 - mse: 15.0606 - mae: 1.5813 - val_loss: 14.9851 - val_mse: 14.9851 - val_mae: 1.5357 - lr: 1.3698e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 3s - loss: 15.0294 - mse: 15.0294 - mae: 1.5784 - val_loss: 14.6676 - val_mse: 14.6676 - val_mae: 1.5478 - lr: 1.3698e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 3s - loss: 15.0123 - mse: 15.0123 - mae: 1.5746 - val_loss: 14.6921 - val_mse: 14.6921 - val_mae: 1.5590 - lr: 1.3698e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 4s - loss: 14.9786 - mse: 14.9786 - mae: 1.5768 - val_loss: 14.5791 - val_mse: 14.5791 - val_mae: 1.5861 - lr: 1.3698e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 4s - loss: 14.9517 - mse: 14.9517 - mae: 1.5737 - val_loss: 14.8624 - val_mse: 14.8624 - val_mae: 1.5686 - lr: 1.3698e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 3s - loss: 14.9331 - mse: 14.9331 - mae: 1.5744 - val_loss: 14.7671 - val_mse: 14.7671 - val_mae: 1.5418 - lr: 1.3698e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 3s - loss: 14.9174 - mse: 14.9174 - mae: 1.5710 - val_loss: 14.7503 - val_mse: 14.7503 - val_mae: 1.5436 - lr: 1.3698e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 3s - loss: 14.9284 - mse: 14.9284 - mae: 1.5681 - val_loss: 14.6144 - val_mse: 14.6144 - val_mae: 1.5811 - lr: 1.3698e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 3s - loss: 14.8591 - mse: 14.8591 - mae: 1.5669 - val_loss: 14.7396 - val_mse: 14.7396 - val_mae: 1.5364 - lr: 1.3698e-04 - 3s/epoch - 3ms/step\n",
            "Score for fold 1: loss of 14.739607810974121\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 3s - loss: 15.4808 - mse: 15.4808 - mae: 1.5728 - val_loss: 12.2065 - val_mse: 12.2065 - val_mae: 1.5511 - lr: 1.3698e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 3s - loss: 15.4434 - mse: 15.4434 - mae: 1.5739 - val_loss: 12.1713 - val_mse: 12.1713 - val_mae: 1.5281 - lr: 1.3698e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 3s - loss: 15.4506 - mse: 15.4506 - mae: 1.5736 - val_loss: 12.4115 - val_mse: 12.4115 - val_mae: 1.5154 - lr: 1.3698e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 3s - loss: 15.3651 - mse: 15.3651 - mae: 1.5672 - val_loss: 12.4103 - val_mse: 12.4103 - val_mae: 1.5522 - lr: 1.3698e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 4s - loss: 15.2682 - mse: 15.2682 - mae: 1.5709 - val_loss: 12.2637 - val_mse: 12.2637 - val_mae: 1.5917 - lr: 1.3698e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 4s - loss: 15.3422 - mse: 15.3422 - mae: 1.5696 - val_loss: 12.3221 - val_mse: 12.3221 - val_mae: 1.5906 - lr: 1.3698e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 3s - loss: 15.3467 - mse: 15.3467 - mae: 1.5674 - val_loss: 12.2398 - val_mse: 12.2398 - val_mae: 1.5868 - lr: 1.3698e-04 - 3s/epoch - 3ms/step\n",
            "Score for fold 2: loss of 12.239776611328125\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 3s - loss: 13.8874 - mse: 13.8874 - mae: 1.5742 - val_loss: 17.8478 - val_mse: 17.8478 - val_mae: 1.5481 - lr: 1.3698e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 3s - loss: 13.8017 - mse: 13.8017 - mae: 1.5697 - val_loss: 17.9345 - val_mse: 17.9345 - val_mae: 1.5275 - lr: 1.3698e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 3s - loss: 13.6885 - mse: 13.6885 - mae: 1.5705 - val_loss: 17.9948 - val_mse: 17.9948 - val_mae: 1.5331 - lr: 1.3698e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 3s - loss: 13.7435 - mse: 13.7435 - mae: 1.5639 - val_loss: 17.8364 - val_mse: 17.8364 - val_mae: 1.5930 - lr: 1.3698e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 3s - loss: 13.6949 - mse: 13.6949 - mae: 1.5578 - val_loss: 18.0371 - val_mse: 18.0371 - val_mae: 1.5296 - lr: 1.3698e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 3s - loss: 13.5931 - mse: 13.5931 - mae: 1.5595 - val_loss: 17.9254 - val_mse: 17.9254 - val_mae: 1.5752 - lr: 1.3698e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 3s - loss: 13.5166 - mse: 13.5166 - mae: 1.5594 - val_loss: 18.2001 - val_mse: 18.2001 - val_mae: 1.6583 - lr: 1.3698e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 3s - loss: 13.4851 - mse: 13.4851 - mae: 1.5586 - val_loss: 17.9857 - val_mse: 17.9857 - val_mae: 1.5890 - lr: 1.3698e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 3s - loss: 13.5031 - mse: 13.5031 - mae: 1.5554 - val_loss: 17.9825 - val_mse: 17.9825 - val_mae: 1.5914 - lr: 1.3698e-04 - 3s/epoch - 3ms/step\n",
            "Score for fold 3: loss of 17.982479095458984\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 4s - loss: 14.2699 - mse: 14.2699 - mae: 1.5570 - val_loss: 14.7809 - val_mse: 14.7809 - val_mae: 1.5791 - lr: 1.3698e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 4s - loss: 14.2698 - mse: 14.2698 - mae: 1.5526 - val_loss: 14.9065 - val_mse: 14.9065 - val_mae: 1.5289 - lr: 1.3698e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 4s - loss: 14.1609 - mse: 14.1609 - mae: 1.5514 - val_loss: 15.0425 - val_mse: 15.0425 - val_mae: 1.4987 - lr: 1.3698e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 4s - loss: 14.1255 - mse: 14.1255 - mae: 1.5509 - val_loss: 14.9847 - val_mse: 14.9847 - val_mae: 1.5415 - lr: 1.3698e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 4s - loss: 14.0990 - mse: 14.0990 - mae: 1.5472 - val_loss: 14.8115 - val_mse: 14.8115 - val_mae: 1.5571 - lr: 1.3698e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 4s - loss: 13.8803 - mse: 13.8803 - mae: 1.5428 - val_loss: 14.9587 - val_mse: 14.9587 - val_mae: 1.6249 - lr: 1.3698e-04 - 4s/epoch - 4ms/step\n",
            "Score for fold 4: loss of 14.95870590209961\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 4s - loss: 14.5029 - mse: 14.5029 - mae: 1.5572 - val_loss: 13.6595 - val_mse: 13.6595 - val_mae: 1.4616 - lr: 1.3698e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 3s - loss: 14.3338 - mse: 14.3338 - mae: 1.5510 - val_loss: 13.3036 - val_mse: 13.3036 - val_mae: 1.5245 - lr: 1.3698e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 3s - loss: 14.1825 - mse: 14.1825 - mae: 1.5482 - val_loss: 13.4538 - val_mse: 13.4538 - val_mae: 1.5430 - lr: 1.3698e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 3s - loss: 14.2704 - mse: 14.2704 - mae: 1.5414 - val_loss: 13.4505 - val_mse: 13.4505 - val_mae: 1.5811 - lr: 1.3698e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 3s - loss: 14.1677 - mse: 14.1677 - mae: 1.5391 - val_loss: 13.6297 - val_mse: 13.6297 - val_mae: 1.5559 - lr: 1.3698e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 4s - loss: 14.1795 - mse: 14.1795 - mae: 1.5395 - val_loss: 13.5575 - val_mse: 13.5575 - val_mae: 1.5413 - lr: 1.3698e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 3s - loss: 14.0781 - mse: 14.0781 - mae: 1.5349 - val_loss: 13.5216 - val_mse: 13.5216 - val_mae: 1.6348 - lr: 1.3698e-04 - 3s/epoch - 3ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-10-14 12:51:31,163]\u001b[0m Finished trial#29 resulted in value: 14.687999999999999. Current best value is 14.428 with parameters: {'activation': 'relu', 'num_hidden_layer': 4, 'i': 9, 'learning_rate': 0.00010250848120373353}.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Score for fold 5: loss of 13.521551132202148\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 4s - loss: 16.3687 - mse: 16.3687 - mae: 1.6181 - val_loss: 11.6962 - val_mse: 11.6962 - val_mae: 1.5838 - lr: 2.5651e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 3s - loss: 15.9071 - mse: 15.9071 - mae: 1.5929 - val_loss: 11.7326 - val_mse: 11.7326 - val_mae: 1.5950 - lr: 2.5651e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 4s - loss: 15.8745 - mse: 15.8745 - mae: 1.5935 - val_loss: 11.6984 - val_mse: 11.6984 - val_mae: 1.5333 - lr: 2.5651e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 4s - loss: 15.8326 - mse: 15.8326 - mae: 1.5887 - val_loss: 11.8740 - val_mse: 11.8740 - val_mae: 1.5227 - lr: 2.5651e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 4s - loss: 15.8167 - mse: 15.8167 - mae: 1.5876 - val_loss: 11.5793 - val_mse: 11.5793 - val_mae: 1.5843 - lr: 2.5651e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 4s - loss: 15.6800 - mse: 15.6800 - mae: 1.5845 - val_loss: 11.7404 - val_mse: 11.7404 - val_mae: 1.5429 - lr: 2.5651e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 4s - loss: 15.6761 - mse: 15.6761 - mae: 1.5805 - val_loss: 11.7340 - val_mse: 11.7340 - val_mae: 1.5828 - lr: 2.5651e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 4s - loss: 15.7094 - mse: 15.7094 - mae: 1.5810 - val_loss: 11.6612 - val_mse: 11.6612 - val_mae: 1.6369 - lr: 2.5651e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 4s - loss: 15.6429 - mse: 15.6429 - mae: 1.5745 - val_loss: 11.6808 - val_mse: 11.6808 - val_mae: 1.5145 - lr: 2.5651e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 4s - loss: 15.6253 - mse: 15.6253 - mae: 1.5708 - val_loss: 11.6172 - val_mse: 11.6172 - val_mae: 1.5532 - lr: 2.5651e-04 - 4s/epoch - 4ms/step\n",
            "Score for fold 1: loss of 11.617226600646973\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 4s - loss: 15.8481 - mse: 15.8481 - mae: 1.5812 - val_loss: 10.4626 - val_mse: 10.4626 - val_mae: 1.5871 - lr: 2.5651e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 3s - loss: 15.8108 - mse: 15.8108 - mae: 1.5779 - val_loss: 10.4007 - val_mse: 10.4007 - val_mae: 1.5871 - lr: 2.5651e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 3s - loss: 15.6911 - mse: 15.6911 - mae: 1.5747 - val_loss: 10.7309 - val_mse: 10.7309 - val_mae: 1.4854 - lr: 2.5651e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 4s - loss: 15.7554 - mse: 15.7554 - mae: 1.5780 - val_loss: 10.6892 - val_mse: 10.6892 - val_mae: 1.4947 - lr: 2.5651e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 3s - loss: 15.6514 - mse: 15.6514 - mae: 1.5725 - val_loss: 10.5797 - val_mse: 10.5797 - val_mae: 1.5285 - lr: 2.5651e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 4s - loss: 15.5454 - mse: 15.5454 - mae: 1.5656 - val_loss: 10.5765 - val_mse: 10.5765 - val_mae: 1.5469 - lr: 2.5651e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 3s - loss: 15.5523 - mse: 15.5523 - mae: 1.5654 - val_loss: 10.6404 - val_mse: 10.6404 - val_mae: 1.5693 - lr: 2.5651e-04 - 3s/epoch - 3ms/step\n",
            "Score for fold 2: loss of 10.640409469604492\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 3s - loss: 14.0952 - mse: 14.0952 - mae: 1.5657 - val_loss: 16.0174 - val_mse: 16.0174 - val_mae: 1.6165 - lr: 2.5651e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 3s - loss: 14.0260 - mse: 14.0260 - mae: 1.5608 - val_loss: 16.0806 - val_mse: 16.0806 - val_mae: 1.5769 - lr: 2.5651e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 4s - loss: 13.9104 - mse: 13.9104 - mae: 1.5613 - val_loss: 16.2644 - val_mse: 16.2644 - val_mae: 1.6643 - lr: 2.5651e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 3s - loss: 13.8691 - mse: 13.8691 - mae: 1.5527 - val_loss: 16.2724 - val_mse: 16.2724 - val_mae: 1.5586 - lr: 2.5651e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 3s - loss: 13.8080 - mse: 13.8080 - mae: 1.5474 - val_loss: 16.7270 - val_mse: 16.7270 - val_mae: 1.5767 - lr: 2.5651e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 4s - loss: 13.8420 - mse: 13.8420 - mae: 1.5528 - val_loss: 16.3217 - val_mse: 16.3217 - val_mae: 1.6041 - lr: 2.5651e-04 - 4s/epoch - 4ms/step\n",
            "Score for fold 3: loss of 16.321731567382812\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 3s - loss: 13.8861 - mse: 13.8861 - mae: 1.5568 - val_loss: 16.0231 - val_mse: 16.0231 - val_mae: 1.6612 - lr: 2.5651e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 4s - loss: 13.8075 - mse: 13.8075 - mae: 1.5586 - val_loss: 16.1013 - val_mse: 16.1013 - val_mae: 1.7071 - lr: 2.5651e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 4s - loss: 13.6891 - mse: 13.6891 - mae: 1.5474 - val_loss: 16.2229 - val_mse: 16.2229 - val_mae: 1.5370 - lr: 2.5651e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 4s - loss: 13.5983 - mse: 13.5983 - mae: 1.5438 - val_loss: 16.2564 - val_mse: 16.2564 - val_mae: 1.4964 - lr: 2.5651e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 4s - loss: 13.5394 - mse: 13.5394 - mae: 1.5448 - val_loss: 16.4967 - val_mse: 16.4967 - val_mae: 1.6174 - lr: 2.5651e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 4s - loss: 13.5721 - mse: 13.5721 - mae: 1.5489 - val_loss: 16.5370 - val_mse: 16.5370 - val_mae: 1.6800 - lr: 2.5651e-04 - 4s/epoch - 4ms/step\n",
            "Score for fold 4: loss of 16.53696632385254\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 3s - loss: 13.0147 - mse: 13.0147 - mae: 1.5535 - val_loss: 17.9196 - val_mse: 17.9196 - val_mae: 1.5195 - lr: 2.5651e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 3s - loss: 13.0058 - mse: 13.0058 - mae: 1.5522 - val_loss: 18.0332 - val_mse: 18.0332 - val_mae: 1.7341 - lr: 2.5651e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 3s - loss: 12.9004 - mse: 12.9004 - mae: 1.5471 - val_loss: 18.2404 - val_mse: 18.2404 - val_mae: 1.5090 - lr: 2.5651e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 3s - loss: 12.8007 - mse: 12.8007 - mae: 1.5443 - val_loss: 18.2125 - val_mse: 18.2125 - val_mae: 1.4592 - lr: 2.5651e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 3s - loss: 12.7256 - mse: 12.7256 - mae: 1.5419 - val_loss: 18.3473 - val_mse: 18.3473 - val_mae: 1.4592 - lr: 2.5651e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 3s - loss: 12.6968 - mse: 12.6968 - mae: 1.5384 - val_loss: 18.1460 - val_mse: 18.1460 - val_mae: 1.5771 - lr: 2.5651e-04 - 3s/epoch - 3ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-10-14 12:53:39,779]\u001b[0m Finished trial#30 resulted in value: 14.654. Current best value is 14.428 with parameters: {'activation': 'relu', 'num_hidden_layer': 4, 'i': 9, 'learning_rate': 0.00010250848120373353}.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Score for fold 5: loss of 18.145980834960938\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 6s - loss: 15.3892 - mse: 15.3892 - mae: 1.6065 - val_loss: 15.7069 - val_mse: 15.7069 - val_mae: 1.6738 - lr: 1.7463e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 5s - loss: 14.9642 - mse: 14.9642 - mae: 1.5839 - val_loss: 15.5283 - val_mse: 15.5283 - val_mae: 1.5884 - lr: 1.7463e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 5s - loss: 14.8569 - mse: 14.8569 - mae: 1.5786 - val_loss: 15.8073 - val_mse: 15.8073 - val_mae: 1.6341 - lr: 1.7463e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 5s - loss: 14.8317 - mse: 14.8317 - mae: 1.5842 - val_loss: 15.4524 - val_mse: 15.4524 - val_mae: 1.6999 - lr: 1.7463e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 5s - loss: 14.7721 - mse: 14.7721 - mae: 1.5797 - val_loss: 15.7517 - val_mse: 15.7517 - val_mae: 1.5404 - lr: 1.7463e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 5s - loss: 14.6725 - mse: 14.6725 - mae: 1.5760 - val_loss: 15.4126 - val_mse: 15.4126 - val_mae: 1.6765 - lr: 1.7463e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 5s - loss: 14.6527 - mse: 14.6527 - mae: 1.5696 - val_loss: 15.7816 - val_mse: 15.7816 - val_mae: 1.5766 - lr: 1.7463e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 5s - loss: 14.6374 - mse: 14.6374 - mae: 1.5687 - val_loss: 15.8315 - val_mse: 15.8315 - val_mae: 1.5790 - lr: 1.7463e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 5s - loss: 14.5747 - mse: 14.5747 - mae: 1.5651 - val_loss: 15.4703 - val_mse: 15.4703 - val_mae: 1.6287 - lr: 1.7463e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 5s - loss: 14.4772 - mse: 14.4772 - mae: 1.5632 - val_loss: 15.8953 - val_mse: 15.8953 - val_mae: 1.5796 - lr: 1.7463e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 5s - loss: 14.4074 - mse: 14.4074 - mae: 1.5568 - val_loss: 15.3548 - val_mse: 15.3548 - val_mae: 1.6617 - lr: 1.7463e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 5s - loss: 14.3878 - mse: 14.3878 - mae: 1.5556 - val_loss: 15.5778 - val_mse: 15.5778 - val_mae: 1.5902 - lr: 1.7463e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 13/100\n",
            "1000/1000 - 5s - loss: 14.3419 - mse: 14.3419 - mae: 1.5555 - val_loss: 15.8464 - val_mse: 15.8464 - val_mae: 1.5593 - lr: 1.7463e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 14/100\n",
            "1000/1000 - 5s - loss: 14.3264 - mse: 14.3264 - mae: 1.5517 - val_loss: 15.9137 - val_mse: 15.9137 - val_mae: 1.6187 - lr: 1.7463e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 15/100\n",
            "1000/1000 - 5s - loss: 14.1600 - mse: 14.1600 - mae: 1.5478 - val_loss: 15.8840 - val_mse: 15.8840 - val_mae: 1.6000 - lr: 1.7463e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 16/100\n",
            "1000/1000 - 5s - loss: 14.1862 - mse: 14.1862 - mae: 1.5490 - val_loss: 15.8063 - val_mse: 15.8063 - val_mae: 1.7023 - lr: 1.7463e-04 - 5s/epoch - 5ms/step\n",
            "Score for fold 1: loss of 15.806318283081055\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 5s - loss: 14.9988 - mse: 14.9988 - mae: 1.5639 - val_loss: 12.1057 - val_mse: 12.1057 - val_mae: 1.5409 - lr: 1.7463e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 5s - loss: 14.9980 - mse: 14.9980 - mae: 1.5648 - val_loss: 12.2090 - val_mse: 12.2090 - val_mae: 1.5986 - lr: 1.7463e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 5s - loss: 14.9427 - mse: 14.9427 - mae: 1.5633 - val_loss: 12.1286 - val_mse: 12.1286 - val_mae: 1.5660 - lr: 1.7463e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 5s - loss: 14.7966 - mse: 14.7966 - mae: 1.5508 - val_loss: 12.1829 - val_mse: 12.1829 - val_mae: 1.5626 - lr: 1.7463e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 5s - loss: 14.7710 - mse: 14.7710 - mae: 1.5524 - val_loss: 12.3208 - val_mse: 12.3208 - val_mae: 1.6025 - lr: 1.7463e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 5s - loss: 14.6078 - mse: 14.6078 - mae: 1.5450 - val_loss: 12.3053 - val_mse: 12.3053 - val_mae: 1.5456 - lr: 1.7463e-04 - 5s/epoch - 5ms/step\n",
            "Score for fold 2: loss of 12.30528736114502\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 5s - loss: 12.7445 - mse: 12.7445 - mae: 1.5489 - val_loss: 19.4049 - val_mse: 19.4049 - val_mae: 1.4750 - lr: 1.7463e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 5s - loss: 12.6035 - mse: 12.6035 - mae: 1.5450 - val_loss: 20.1323 - val_mse: 20.1323 - val_mae: 1.5047 - lr: 1.7463e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 5s - loss: 12.5567 - mse: 12.5567 - mae: 1.5476 - val_loss: 19.7993 - val_mse: 19.7993 - val_mae: 1.5836 - lr: 1.7463e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 5s - loss: 12.5883 - mse: 12.5883 - mae: 1.5400 - val_loss: 19.7744 - val_mse: 19.7744 - val_mae: 1.4966 - lr: 1.7463e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 5s - loss: 12.3154 - mse: 12.3154 - mae: 1.5379 - val_loss: 19.9175 - val_mse: 19.9175 - val_mae: 1.6174 - lr: 1.7463e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 5s - loss: 12.2248 - mse: 12.2248 - mae: 1.5426 - val_loss: 20.0636 - val_mse: 20.0636 - val_mae: 1.5192 - lr: 1.7463e-04 - 5s/epoch - 5ms/step\n",
            "Score for fold 3: loss of 20.063644409179688\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 5s - loss: 14.7252 - mse: 14.7252 - mae: 1.5603 - val_loss: 10.4280 - val_mse: 10.4280 - val_mae: 1.3926 - lr: 1.7463e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 5s - loss: 14.6054 - mse: 14.6054 - mae: 1.5494 - val_loss: 10.5716 - val_mse: 10.5716 - val_mae: 1.3976 - lr: 1.7463e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 5s - loss: 14.5949 - mse: 14.5949 - mae: 1.5509 - val_loss: 10.1463 - val_mse: 10.1463 - val_mae: 1.5525 - lr: 1.7463e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 5s - loss: 14.4618 - mse: 14.4618 - mae: 1.5546 - val_loss: 10.5963 - val_mse: 10.5963 - val_mae: 1.4513 - lr: 1.7463e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 5s - loss: 14.3385 - mse: 14.3385 - mae: 1.5507 - val_loss: 10.5689 - val_mse: 10.5689 - val_mae: 1.5358 - lr: 1.7463e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 5s - loss: 14.2945 - mse: 14.2945 - mae: 1.5500 - val_loss: 10.3165 - val_mse: 10.3165 - val_mae: 1.5942 - lr: 1.7463e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 5s - loss: 14.2439 - mse: 14.2439 - mae: 1.5487 - val_loss: 11.2496 - val_mse: 11.2496 - val_mae: 1.8798 - lr: 1.7463e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 5s - loss: 14.3351 - mse: 14.3351 - mae: 1.5572 - val_loss: 10.7444 - val_mse: 10.7444 - val_mae: 1.4671 - lr: 1.7463e-04 - 5s/epoch - 5ms/step\n",
            "Score for fold 4: loss of 10.744423866271973\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 5s - loss: 13.5891 - mse: 13.5891 - mae: 1.5578 - val_loss: 12.4275 - val_mse: 12.4275 - val_mae: 1.6512 - lr: 1.7463e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 5s - loss: 13.5903 - mse: 13.5903 - mae: 1.5695 - val_loss: 12.9727 - val_mse: 12.9727 - val_mae: 1.4681 - lr: 1.7463e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 5s - loss: 13.3921 - mse: 13.3921 - mae: 1.5653 - val_loss: 12.7199 - val_mse: 12.7199 - val_mae: 1.4112 - lr: 1.7463e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 5s - loss: 13.3757 - mse: 13.3757 - mae: 1.5467 - val_loss: 12.6312 - val_mse: 12.6312 - val_mae: 1.4950 - lr: 1.7463e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 5s - loss: 13.3466 - mse: 13.3466 - mae: 1.5758 - val_loss: 12.9572 - val_mse: 12.9572 - val_mae: 1.5313 - lr: 1.7463e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 5s - loss: 13.2509 - mse: 13.2509 - mae: 1.5587 - val_loss: 13.7206 - val_mse: 13.7206 - val_mae: 1.4354 - lr: 1.7463e-04 - 5s/epoch - 5ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-10-14 12:57:21,455]\u001b[0m Finished trial#31 resulted in value: 14.528. Current best value is 14.428 with parameters: {'activation': 'relu', 'num_hidden_layer': 4, 'i': 9, 'learning_rate': 0.00010250848120373353}.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Score for fold 5: loss of 13.720637321472168\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 6s - loss: 14.7337 - mse: 14.7337 - mae: 1.6032 - val_loss: 17.7909 - val_mse: 17.7909 - val_mae: 1.6998 - lr: 1.5343e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 5s - loss: 14.3901 - mse: 14.3901 - mae: 1.5776 - val_loss: 17.7146 - val_mse: 17.7146 - val_mae: 1.6095 - lr: 1.5343e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 5s - loss: 14.3299 - mse: 14.3299 - mae: 1.5757 - val_loss: 18.0428 - val_mse: 18.0428 - val_mae: 1.5581 - lr: 1.5343e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 5s - loss: 14.3244 - mse: 14.3244 - mae: 1.5760 - val_loss: 17.7803 - val_mse: 17.7803 - val_mae: 1.6337 - lr: 1.5343e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 5s - loss: 14.2999 - mse: 14.2999 - mae: 1.5701 - val_loss: 17.8227 - val_mse: 17.8227 - val_mae: 1.6281 - lr: 1.5343e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 5s - loss: 14.2486 - mse: 14.2486 - mae: 1.5678 - val_loss: 17.7563 - val_mse: 17.7563 - val_mae: 1.6251 - lr: 1.5343e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 5s - loss: 14.1345 - mse: 14.1345 - mae: 1.5674 - val_loss: 17.6355 - val_mse: 17.6355 - val_mae: 1.5870 - lr: 1.5343e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 5s - loss: 14.1335 - mse: 14.1335 - mae: 1.5628 - val_loss: 17.9066 - val_mse: 17.9066 - val_mae: 1.5925 - lr: 1.5343e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 5s - loss: 14.1112 - mse: 14.1112 - mae: 1.5585 - val_loss: 17.5670 - val_mse: 17.5670 - val_mae: 1.6250 - lr: 1.5343e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 5s - loss: 14.0928 - mse: 14.0928 - mae: 1.5575 - val_loss: 17.6671 - val_mse: 17.6671 - val_mae: 1.6286 - lr: 1.5343e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 5s - loss: 14.0365 - mse: 14.0365 - mae: 1.5572 - val_loss: 17.7409 - val_mse: 17.7409 - val_mae: 1.6203 - lr: 1.5343e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 5s - loss: 13.9693 - mse: 13.9693 - mae: 1.5527 - val_loss: 17.8010 - val_mse: 17.8010 - val_mae: 1.5704 - lr: 1.5343e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 13/100\n",
            "1000/1000 - 5s - loss: 13.9215 - mse: 13.9215 - mae: 1.5517 - val_loss: 17.6485 - val_mse: 17.6485 - val_mae: 1.6220 - lr: 1.5343e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 14/100\n",
            "1000/1000 - 5s - loss: 13.8579 - mse: 13.8579 - mae: 1.5479 - val_loss: 17.4292 - val_mse: 17.4292 - val_mae: 1.5825 - lr: 1.5343e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 15/100\n",
            "1000/1000 - 5s - loss: 13.7945 - mse: 13.7945 - mae: 1.5503 - val_loss: 17.9639 - val_mse: 17.9639 - val_mae: 1.6849 - lr: 1.5343e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 16/100\n",
            "1000/1000 - 5s - loss: 13.8051 - mse: 13.8051 - mae: 1.5469 - val_loss: 18.0347 - val_mse: 18.0347 - val_mae: 1.5765 - lr: 1.5343e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 17/100\n",
            "1000/1000 - 5s - loss: 13.7024 - mse: 13.7024 - mae: 1.5435 - val_loss: 18.2700 - val_mse: 18.2700 - val_mae: 1.6064 - lr: 1.5343e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 18/100\n",
            "1000/1000 - 5s - loss: 13.6514 - mse: 13.6514 - mae: 1.5409 - val_loss: 18.1078 - val_mse: 18.1078 - val_mae: 1.6388 - lr: 1.5343e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 19/100\n",
            "1000/1000 - 5s - loss: 13.6825 - mse: 13.6825 - mae: 1.5404 - val_loss: 17.7265 - val_mse: 17.7265 - val_mae: 1.5855 - lr: 1.5343e-04 - 5s/epoch - 5ms/step\n",
            "Score for fold 1: loss of 17.726490020751953\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 5s - loss: 14.8926 - mse: 14.8926 - mae: 1.5635 - val_loss: 12.9860 - val_mse: 12.9860 - val_mae: 1.6470 - lr: 1.5343e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 5s - loss: 14.7956 - mse: 14.7956 - mae: 1.5584 - val_loss: 12.9596 - val_mse: 12.9596 - val_mae: 1.5793 - lr: 1.5343e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 5s - loss: 14.6885 - mse: 14.6885 - mae: 1.5536 - val_loss: 13.1305 - val_mse: 13.1305 - val_mae: 1.5434 - lr: 1.5343e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 5s - loss: 14.5562 - mse: 14.5562 - mae: 1.5518 - val_loss: 13.2204 - val_mse: 13.2204 - val_mae: 1.5139 - lr: 1.5343e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 5s - loss: 14.4763 - mse: 14.4763 - mae: 1.5496 - val_loss: 13.0565 - val_mse: 13.0565 - val_mae: 1.5401 - lr: 1.5343e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 5s - loss: 14.2879 - mse: 14.2879 - mae: 1.5458 - val_loss: 13.0707 - val_mse: 13.0707 - val_mae: 1.6531 - lr: 1.5343e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 5s - loss: 14.3077 - mse: 14.3077 - mae: 1.5439 - val_loss: 13.4019 - val_mse: 13.4019 - val_mae: 1.6338 - lr: 1.5343e-04 - 5s/epoch - 5ms/step\n",
            "Score for fold 2: loss of 13.401939392089844\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 5s - loss: 14.9259 - mse: 14.9259 - mae: 1.5493 - val_loss: 11.5590 - val_mse: 11.5590 - val_mae: 1.5598 - lr: 1.5343e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 5s - loss: 14.7065 - mse: 14.7065 - mae: 1.5454 - val_loss: 11.8107 - val_mse: 11.8107 - val_mae: 1.4971 - lr: 1.5343e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 5s - loss: 14.6957 - mse: 14.6957 - mae: 1.5411 - val_loss: 11.8619 - val_mse: 11.8619 - val_mae: 1.5383 - lr: 1.5343e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 5s - loss: 14.4838 - mse: 14.4838 - mae: 1.5367 - val_loss: 11.9315 - val_mse: 11.9315 - val_mae: 1.5781 - lr: 1.5343e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 5s - loss: 14.5170 - mse: 14.5170 - mae: 1.5275 - val_loss: 12.0166 - val_mse: 12.0166 - val_mae: 1.6010 - lr: 1.5343e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 5s - loss: 14.2555 - mse: 14.2555 - mae: 1.5254 - val_loss: 12.0590 - val_mse: 12.0590 - val_mae: 1.7753 - lr: 1.5343e-04 - 5s/epoch - 5ms/step\n",
            "Score for fold 3: loss of 12.059003829956055\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 5s - loss: 14.7285 - mse: 14.7285 - mae: 1.5659 - val_loss: 10.6723 - val_mse: 10.6723 - val_mae: 1.5545 - lr: 1.5343e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 5s - loss: 14.5573 - mse: 14.5573 - mae: 1.5628 - val_loss: 11.0031 - val_mse: 11.0031 - val_mae: 1.6664 - lr: 1.5343e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 5s - loss: 14.4985 - mse: 14.4985 - mae: 1.5446 - val_loss: 11.1331 - val_mse: 11.1331 - val_mae: 1.5314 - lr: 1.5343e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 5s - loss: 14.3800 - mse: 14.3800 - mae: 1.5540 - val_loss: 10.9936 - val_mse: 10.9936 - val_mae: 1.5240 - lr: 1.5343e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 5s - loss: 14.3595 - mse: 14.3595 - mae: 1.5527 - val_loss: 11.0274 - val_mse: 11.0274 - val_mae: 1.6066 - lr: 1.5343e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 5s - loss: 14.2387 - mse: 14.2387 - mae: 1.5458 - val_loss: 11.2149 - val_mse: 11.2149 - val_mae: 1.5133 - lr: 1.5343e-04 - 5s/epoch - 5ms/step\n",
            "Score for fold 4: loss of 11.214914321899414\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 5s - loss: 12.6288 - mse: 12.6288 - mae: 1.5606 - val_loss: 17.3902 - val_mse: 17.3902 - val_mae: 1.4449 - lr: 1.5343e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 5s - loss: 12.5587 - mse: 12.5587 - mae: 1.5518 - val_loss: 17.4023 - val_mse: 17.4023 - val_mae: 1.4387 - lr: 1.5343e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 5s - loss: 12.4751 - mse: 12.4751 - mae: 1.5500 - val_loss: 17.5062 - val_mse: 17.5062 - val_mae: 1.5724 - lr: 1.5343e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 5s - loss: 12.4059 - mse: 12.4059 - mae: 1.5534 - val_loss: 17.6258 - val_mse: 17.6258 - val_mae: 1.6096 - lr: 1.5343e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 5s - loss: 12.3837 - mse: 12.3837 - mae: 1.5746 - val_loss: 17.7126 - val_mse: 17.7126 - val_mae: 1.5675 - lr: 1.5343e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 5s - loss: 12.2943 - mse: 12.2943 - mae: 1.5645 - val_loss: 17.9265 - val_mse: 17.9265 - val_mae: 1.4865 - lr: 1.5343e-04 - 5s/epoch - 5ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-10-14 13:01:16,301]\u001b[0m Finished trial#32 resulted in value: 14.466000000000003. Current best value is 14.428 with parameters: {'activation': 'relu', 'num_hidden_layer': 4, 'i': 9, 'learning_rate': 0.00010250848120373353}.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Score for fold 5: loss of 17.926496505737305\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 6s - loss: 16.8990 - mse: 16.8990 - mae: 1.6956 - val_loss: 11.9861 - val_mse: 11.9861 - val_mae: 1.5876 - lr: 1.2845e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 5s - loss: 16.3180 - mse: 16.3180 - mae: 1.6239 - val_loss: 12.0158 - val_mse: 12.0158 - val_mae: 1.6185 - lr: 1.2845e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 5s - loss: 16.1989 - mse: 16.1989 - mae: 1.6210 - val_loss: 11.8763 - val_mse: 11.8763 - val_mae: 1.5206 - lr: 1.2845e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 5s - loss: 16.1473 - mse: 16.1473 - mae: 1.6152 - val_loss: 11.9959 - val_mse: 11.9959 - val_mae: 1.5753 - lr: 1.2845e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 5s - loss: 16.1130 - mse: 16.1130 - mae: 1.6077 - val_loss: 11.8651 - val_mse: 11.8651 - val_mae: 1.5202 - lr: 1.2845e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 5s - loss: 16.0802 - mse: 16.0802 - mae: 1.6031 - val_loss: 11.7963 - val_mse: 11.7963 - val_mae: 1.5142 - lr: 1.2845e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 5s - loss: 16.0634 - mse: 16.0634 - mae: 1.6055 - val_loss: 11.8946 - val_mse: 11.8946 - val_mae: 1.4890 - lr: 1.2845e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 5s - loss: 16.0475 - mse: 16.0475 - mae: 1.6027 - val_loss: 11.8158 - val_mse: 11.8158 - val_mae: 1.5284 - lr: 1.2845e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 5s - loss: 16.0221 - mse: 16.0221 - mae: 1.6033 - val_loss: 11.9586 - val_mse: 11.9586 - val_mae: 1.5856 - lr: 1.2845e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 5s - loss: 16.0234 - mse: 16.0234 - mae: 1.6032 - val_loss: 11.7162 - val_mse: 11.7162 - val_mae: 1.5832 - lr: 1.2845e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 5s - loss: 16.0267 - mse: 16.0267 - mae: 1.6029 - val_loss: 11.7145 - val_mse: 11.7145 - val_mae: 1.5759 - lr: 1.2845e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 5s - loss: 16.0313 - mse: 16.0313 - mae: 1.6040 - val_loss: 11.8158 - val_mse: 11.8158 - val_mae: 1.6064 - lr: 1.2845e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 13/100\n",
            "1000/1000 - 5s - loss: 15.9513 - mse: 15.9513 - mae: 1.5997 - val_loss: 11.8777 - val_mse: 11.8777 - val_mae: 1.6700 - lr: 1.2845e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 14/100\n",
            "1000/1000 - 5s - loss: 15.9836 - mse: 15.9836 - mae: 1.6004 - val_loss: 11.7800 - val_mse: 11.7800 - val_mae: 1.5572 - lr: 1.2845e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 15/100\n",
            "1000/1000 - 5s - loss: 15.9919 - mse: 15.9919 - mae: 1.5988 - val_loss: 11.6613 - val_mse: 11.6613 - val_mae: 1.5694 - lr: 1.2845e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 16/100\n",
            "1000/1000 - 5s - loss: 15.9553 - mse: 15.9553 - mae: 1.5946 - val_loss: 11.6879 - val_mse: 11.6879 - val_mae: 1.5424 - lr: 1.2845e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 17/100\n",
            "1000/1000 - 5s - loss: 15.8980 - mse: 15.8980 - mae: 1.5976 - val_loss: 11.6211 - val_mse: 11.6211 - val_mae: 1.5612 - lr: 1.2845e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 18/100\n",
            "1000/1000 - 5s - loss: 15.9270 - mse: 15.9270 - mae: 1.5957 - val_loss: 11.6353 - val_mse: 11.6353 - val_mae: 1.5647 - lr: 1.2845e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 19/100\n",
            "1000/1000 - 5s - loss: 15.9268 - mse: 15.9268 - mae: 1.5934 - val_loss: 11.7573 - val_mse: 11.7573 - val_mae: 1.5062 - lr: 1.2845e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 20/100\n",
            "1000/1000 - 5s - loss: 15.8849 - mse: 15.8849 - mae: 1.5914 - val_loss: 11.7365 - val_mse: 11.7365 - val_mae: 1.5460 - lr: 1.2845e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 21/100\n",
            "1000/1000 - 5s - loss: 15.8438 - mse: 15.8438 - mae: 1.5932 - val_loss: 11.6647 - val_mse: 11.6647 - val_mae: 1.5401 - lr: 1.2845e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 22/100\n",
            "1000/1000 - 5s - loss: 15.8416 - mse: 15.8416 - mae: 1.5957 - val_loss: 11.6863 - val_mse: 11.6863 - val_mae: 1.6160 - lr: 1.2845e-04 - 5s/epoch - 5ms/step\n",
            "Score for fold 1: loss of 11.686271667480469\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 5s - loss: 15.4725 - mse: 15.4725 - mae: 1.5851 - val_loss: 13.0053 - val_mse: 13.0053 - val_mae: 1.5900 - lr: 1.2845e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 5s - loss: 15.4693 - mse: 15.4693 - mae: 1.5818 - val_loss: 13.1332 - val_mse: 13.1332 - val_mae: 1.5673 - lr: 1.2845e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 5s - loss: 15.4732 - mse: 15.4732 - mae: 1.5858 - val_loss: 12.9990 - val_mse: 12.9990 - val_mae: 1.5861 - lr: 1.2845e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 5s - loss: 15.4123 - mse: 15.4123 - mae: 1.5816 - val_loss: 12.9069 - val_mse: 12.9069 - val_mae: 1.5278 - lr: 1.2845e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 5s - loss: 15.3845 - mse: 15.3845 - mae: 1.5819 - val_loss: 12.9553 - val_mse: 12.9553 - val_mae: 1.5923 - lr: 1.2845e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 5s - loss: 15.4086 - mse: 15.4086 - mae: 1.5808 - val_loss: 13.0395 - val_mse: 13.0395 - val_mae: 1.5633 - lr: 1.2845e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 5s - loss: 15.3627 - mse: 15.3627 - mae: 1.5783 - val_loss: 13.3368 - val_mse: 13.3368 - val_mae: 1.6176 - lr: 1.2845e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 5s - loss: 15.3418 - mse: 15.3418 - mae: 1.5759 - val_loss: 12.9803 - val_mse: 12.9803 - val_mae: 1.5638 - lr: 1.2845e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 5s - loss: 15.3534 - mse: 15.3534 - mae: 1.5768 - val_loss: 13.1570 - val_mse: 13.1570 - val_mae: 1.5430 - lr: 1.2845e-04 - 5s/epoch - 5ms/step\n",
            "Score for fold 2: loss of 13.156991004943848\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 5s - loss: 13.7799 - mse: 13.7799 - mae: 1.5699 - val_loss: 19.2062 - val_mse: 19.2062 - val_mae: 1.6567 - lr: 1.2845e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 5s - loss: 13.7555 - mse: 13.7555 - mae: 1.5694 - val_loss: 19.7829 - val_mse: 19.7829 - val_mae: 1.6514 - lr: 1.2845e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 5s - loss: 13.7394 - mse: 13.7394 - mae: 1.5680 - val_loss: 19.4776 - val_mse: 19.4776 - val_mae: 1.5869 - lr: 1.2845e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 5s - loss: 13.7216 - mse: 13.7216 - mae: 1.5603 - val_loss: 19.3763 - val_mse: 19.3763 - val_mae: 1.6028 - lr: 1.2845e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 5s - loss: 13.6938 - mse: 13.6938 - mae: 1.5612 - val_loss: 19.2739 - val_mse: 19.2739 - val_mae: 1.6147 - lr: 1.2845e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 5s - loss: 13.6981 - mse: 13.6981 - mae: 1.5593 - val_loss: 19.3364 - val_mse: 19.3364 - val_mae: 1.6858 - lr: 1.2845e-04 - 5s/epoch - 5ms/step\n",
            "Score for fold 3: loss of 19.336427688598633\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 5s - loss: 15.5097 - mse: 15.5097 - mae: 1.5779 - val_loss: 11.9682 - val_mse: 11.9682 - val_mae: 1.5501 - lr: 1.2845e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 5s - loss: 15.5080 - mse: 15.5080 - mae: 1.5737 - val_loss: 11.8715 - val_mse: 11.8715 - val_mae: 1.5655 - lr: 1.2845e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 5s - loss: 15.4805 - mse: 15.4805 - mae: 1.5723 - val_loss: 11.8539 - val_mse: 11.8539 - val_mae: 1.5856 - lr: 1.2845e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 5s - loss: 15.4154 - mse: 15.4154 - mae: 1.5695 - val_loss: 12.0289 - val_mse: 12.0289 - val_mae: 1.5194 - lr: 1.2845e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 5s - loss: 15.3894 - mse: 15.3894 - mae: 1.5730 - val_loss: 11.9622 - val_mse: 11.9622 - val_mae: 1.5637 - lr: 1.2845e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 5s - loss: 15.3872 - mse: 15.3872 - mae: 1.5680 - val_loss: 12.0291 - val_mse: 12.0291 - val_mae: 1.5717 - lr: 1.2845e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 5s - loss: 15.3623 - mse: 15.3623 - mae: 1.5646 - val_loss: 11.8506 - val_mse: 11.8506 - val_mae: 1.5691 - lr: 1.2845e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 5s - loss: 15.3355 - mse: 15.3355 - mae: 1.5630 - val_loss: 11.9277 - val_mse: 11.9277 - val_mae: 1.6031 - lr: 1.2845e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 5s - loss: 15.3228 - mse: 15.3228 - mae: 1.5641 - val_loss: 12.1501 - val_mse: 12.1501 - val_mae: 1.5102 - lr: 1.2845e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 5s - loss: 15.2932 - mse: 15.2932 - mae: 1.5616 - val_loss: 11.9683 - val_mse: 11.9683 - val_mae: 1.6251 - lr: 1.2845e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 5s - loss: 15.2725 - mse: 15.2725 - mae: 1.5635 - val_loss: 11.8957 - val_mse: 11.8957 - val_mae: 1.6124 - lr: 1.2845e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 5s - loss: 15.2856 - mse: 15.2856 - mae: 1.5646 - val_loss: 11.9475 - val_mse: 11.9475 - val_mae: 1.5784 - lr: 1.2845e-04 - 5s/epoch - 5ms/step\n",
            "Score for fold 4: loss of 11.94751262664795\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 5s - loss: 13.6946 - mse: 13.6946 - mae: 1.5643 - val_loss: 18.3356 - val_mse: 18.3356 - val_mae: 1.5373 - lr: 1.2845e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 5s - loss: 13.6517 - mse: 13.6517 - mae: 1.5599 - val_loss: 18.3517 - val_mse: 18.3517 - val_mae: 1.5874 - lr: 1.2845e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 5s - loss: 13.6158 - mse: 13.6158 - mae: 1.5610 - val_loss: 18.2824 - val_mse: 18.2824 - val_mae: 1.5520 - lr: 1.2845e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 5s - loss: 13.5711 - mse: 13.5711 - mae: 1.5618 - val_loss: 18.4943 - val_mse: 18.4943 - val_mae: 1.5643 - lr: 1.2845e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 5s - loss: 13.5447 - mse: 13.5447 - mae: 1.5560 - val_loss: 18.4983 - val_mse: 18.4983 - val_mae: 1.5514 - lr: 1.2845e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 5s - loss: 13.5534 - mse: 13.5534 - mae: 1.5529 - val_loss: 18.3237 - val_mse: 18.3237 - val_mae: 1.5999 - lr: 1.2845e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 5s - loss: 13.5094 - mse: 13.5094 - mae: 1.5536 - val_loss: 18.5250 - val_mse: 18.5250 - val_mae: 1.5813 - lr: 1.2845e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 5s - loss: 13.4864 - mse: 13.4864 - mae: 1.5571 - val_loss: 18.3600 - val_mse: 18.3600 - val_mae: 1.5928 - lr: 1.2845e-04 - 5s/epoch - 5ms/step\n",
            "Score for fold 5: loss of 18.360004425048828\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-10-14 13:06:19,794]\u001b[0m Finished trial#33 resulted in value: 14.9. Current best value is 14.428 with parameters: {'activation': 'relu', 'num_hidden_layer': 4, 'i': 9, 'learning_rate': 0.00010250848120373353}.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 11s - loss: 14.1384 - mse: 14.1384 - mae: 1.6207 - val_loss: 20.7271 - val_mse: 20.7271 - val_mae: 1.5088 - lr: 1.0333e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 10s - loss: 13.7714 - mse: 13.7714 - mae: 1.5914 - val_loss: 20.7852 - val_mse: 20.7852 - val_mae: 1.5349 - lr: 1.0333e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 10s - loss: 13.6994 - mse: 13.6994 - mae: 1.5887 - val_loss: 20.5708 - val_mse: 20.5708 - val_mae: 1.6152 - lr: 1.0333e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 10s - loss: 13.5049 - mse: 13.5049 - mae: 1.5960 - val_loss: 20.5814 - val_mse: 20.5814 - val_mae: 1.5816 - lr: 1.0333e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 10s - loss: 13.5457 - mse: 13.5457 - mae: 1.5900 - val_loss: 20.5360 - val_mse: 20.5360 - val_mae: 1.5902 - lr: 1.0333e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 10s - loss: 13.4860 - mse: 13.4860 - mae: 1.5867 - val_loss: 20.6536 - val_mse: 20.6536 - val_mae: 1.6085 - lr: 1.0333e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 10s - loss: 13.4572 - mse: 13.4572 - mae: 1.5773 - val_loss: 20.6175 - val_mse: 20.6175 - val_mae: 1.6440 - lr: 1.0333e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 10s - loss: 13.3506 - mse: 13.3506 - mae: 1.5808 - val_loss: 20.5631 - val_mse: 20.5631 - val_mae: 1.5713 - lr: 1.0333e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 10s - loss: 13.3398 - mse: 13.3398 - mae: 1.5700 - val_loss: 20.4989 - val_mse: 20.4989 - val_mae: 1.5913 - lr: 1.0333e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 10s - loss: 13.2935 - mse: 13.2935 - mae: 1.5689 - val_loss: 20.6359 - val_mse: 20.6359 - val_mae: 1.5654 - lr: 1.0333e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 10s - loss: 13.2708 - mse: 13.2708 - mae: 1.5701 - val_loss: 20.4687 - val_mse: 20.4687 - val_mae: 1.5890 - lr: 1.0333e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 10s - loss: 13.2054 - mse: 13.2054 - mae: 1.5657 - val_loss: 20.4993 - val_mse: 20.4993 - val_mae: 1.5762 - lr: 1.0333e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 13/100\n",
            "1000/1000 - 10s - loss: 13.1074 - mse: 13.1074 - mae: 1.5637 - val_loss: 20.5123 - val_mse: 20.5123 - val_mae: 1.6140 - lr: 1.0333e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 14/100\n",
            "1000/1000 - 10s - loss: 13.1603 - mse: 13.1603 - mae: 1.5618 - val_loss: 20.7502 - val_mse: 20.7502 - val_mae: 1.5818 - lr: 1.0333e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 15/100\n",
            "1000/1000 - 10s - loss: 12.9602 - mse: 12.9602 - mae: 1.5643 - val_loss: 20.6279 - val_mse: 20.6279 - val_mae: 1.7110 - lr: 1.0333e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 16/100\n",
            "1000/1000 - 11s - loss: 12.9331 - mse: 12.9331 - mae: 1.5623 - val_loss: 20.5540 - val_mse: 20.5540 - val_mae: 1.6028 - lr: 1.0333e-04 - 11s/epoch - 11ms/step\n",
            "Score for fold 1: loss of 20.553956985473633\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 10s - loss: 15.8752 - mse: 15.8752 - mae: 1.5748 - val_loss: 9.0387 - val_mse: 9.0387 - val_mae: 1.4626 - lr: 1.0333e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 10s - loss: 15.8156 - mse: 15.8156 - mae: 1.5700 - val_loss: 8.9184 - val_mse: 8.9184 - val_mae: 1.4993 - lr: 1.0333e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 10s - loss: 15.7122 - mse: 15.7122 - mae: 1.5645 - val_loss: 8.9303 - val_mse: 8.9303 - val_mae: 1.4994 - lr: 1.0333e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 11s - loss: 15.4528 - mse: 15.4528 - mae: 1.5630 - val_loss: 9.0288 - val_mse: 9.0288 - val_mae: 1.5230 - lr: 1.0333e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 11s - loss: 15.5701 - mse: 15.5701 - mae: 1.5589 - val_loss: 9.0630 - val_mse: 9.0630 - val_mae: 1.4970 - lr: 1.0333e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 11s - loss: 15.4423 - mse: 15.4423 - mae: 1.5525 - val_loss: 9.0585 - val_mse: 9.0585 - val_mae: 1.5920 - lr: 1.0333e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 11s - loss: 15.2670 - mse: 15.2670 - mae: 1.5449 - val_loss: 9.6235 - val_mse: 9.6235 - val_mae: 1.8529 - lr: 1.0333e-04 - 11s/epoch - 11ms/step\n",
            "Score for fold 2: loss of 9.623476028442383\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 10s - loss: 14.4216 - mse: 14.4216 - mae: 1.5610 - val_loss: 12.7686 - val_mse: 12.7686 - val_mae: 1.5644 - lr: 1.0333e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 10s - loss: 14.2333 - mse: 14.2333 - mae: 1.5487 - val_loss: 12.6084 - val_mse: 12.6084 - val_mae: 1.5638 - lr: 1.0333e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 10s - loss: 14.2478 - mse: 14.2478 - mae: 1.5459 - val_loss: 13.1303 - val_mse: 13.1303 - val_mae: 1.6530 - lr: 1.0333e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 11s - loss: 14.1426 - mse: 14.1426 - mae: 1.5435 - val_loss: 12.5882 - val_mse: 12.5882 - val_mae: 1.7026 - lr: 1.0333e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 11s - loss: 13.9525 - mse: 13.9525 - mae: 1.5417 - val_loss: 13.0743 - val_mse: 13.0743 - val_mae: 1.6098 - lr: 1.0333e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 11s - loss: 13.8424 - mse: 13.8424 - mae: 1.5440 - val_loss: 13.2284 - val_mse: 13.2284 - val_mae: 1.8501 - lr: 1.0333e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 11s - loss: 13.8681 - mse: 13.8681 - mae: 1.5399 - val_loss: 13.2069 - val_mse: 13.2069 - val_mae: 1.4721 - lr: 1.0333e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 11s - loss: 13.6861 - mse: 13.6861 - mae: 1.5348 - val_loss: 12.9200 - val_mse: 12.9200 - val_mae: 1.5560 - lr: 1.0333e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 11s - loss: 13.6239 - mse: 13.6239 - mae: 1.5369 - val_loss: 13.1052 - val_mse: 13.1052 - val_mae: 1.5762 - lr: 1.0333e-04 - 11s/epoch - 11ms/step\n",
            "Score for fold 3: loss of 13.105207443237305\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 10s - loss: 13.2138 - mse: 13.2138 - mae: 1.5420 - val_loss: 14.8778 - val_mse: 14.8778 - val_mae: 1.4475 - lr: 1.0333e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 10s - loss: 13.0466 - mse: 13.0466 - mae: 1.5394 - val_loss: 15.1358 - val_mse: 15.1358 - val_mae: 1.5315 - lr: 1.0333e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 11s - loss: 12.9374 - mse: 12.9374 - mae: 1.5465 - val_loss: 15.0990 - val_mse: 15.0990 - val_mae: 1.6031 - lr: 1.0333e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 11s - loss: 12.8805 - mse: 12.8805 - mae: 1.5382 - val_loss: 15.5116 - val_mse: 15.5116 - val_mae: 1.6715 - lr: 1.0333e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 11s - loss: 12.7881 - mse: 12.7881 - mae: 1.5335 - val_loss: 15.5520 - val_mse: 15.5520 - val_mae: 1.8431 - lr: 1.0333e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 11s - loss: 12.7643 - mse: 12.7643 - mae: 1.5342 - val_loss: 15.6533 - val_mse: 15.6533 - val_mae: 1.4830 - lr: 1.0333e-04 - 11s/epoch - 11ms/step\n",
            "Score for fold 4: loss of 15.653268814086914\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 10s - loss: 13.6077 - mse: 13.6077 - mae: 1.5450 - val_loss: 12.6266 - val_mse: 12.6266 - val_mae: 1.8142 - lr: 1.0333e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 11s - loss: 13.4712 - mse: 13.4712 - mae: 1.5540 - val_loss: 12.4034 - val_mse: 12.4034 - val_mae: 1.4305 - lr: 1.0333e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 11s - loss: 13.2997 - mse: 13.2997 - mae: 1.5538 - val_loss: 11.7844 - val_mse: 11.7844 - val_mae: 1.4830 - lr: 1.0333e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 11s - loss: 13.2383 - mse: 13.2383 - mae: 1.5434 - val_loss: 12.2320 - val_mse: 12.2320 - val_mae: 1.4448 - lr: 1.0333e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 11s - loss: 13.1212 - mse: 13.1212 - mae: 1.5289 - val_loss: 13.1968 - val_mse: 13.1968 - val_mae: 1.4889 - lr: 1.0333e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 11s - loss: 13.0725 - mse: 13.0725 - mae: 1.5346 - val_loss: 12.3922 - val_mse: 12.3922 - val_mae: 1.5151 - lr: 1.0333e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 11s - loss: 12.9278 - mse: 12.9278 - mae: 1.5343 - val_loss: 13.0354 - val_mse: 13.0354 - val_mae: 1.7675 - lr: 1.0333e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 11s - loss: 12.7489 - mse: 12.7489 - mae: 1.5303 - val_loss: 12.3953 - val_mse: 12.3953 - val_mae: 1.5706 - lr: 1.0333e-04 - 11s/epoch - 11ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-10-14 13:14:33,585]\u001b[0m Finished trial#34 resulted in value: 14.266. Current best value is 14.266 with parameters: {'activation': 'relu', 'num_hidden_layer': 4, 'i': 10, 'learning_rate': 0.00010332846452133283}.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Score for fold 5: loss of 12.395268440246582\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 11s - loss: 15.4370 - mse: 15.4370 - mae: 1.6107 - val_loss: 15.3565 - val_mse: 15.3565 - val_mae: 1.6880 - lr: 1.0093e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 10s - loss: 14.9951 - mse: 14.9951 - mae: 1.5840 - val_loss: 15.5393 - val_mse: 15.5393 - val_mae: 1.5588 - lr: 1.0093e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 10s - loss: 14.9052 - mse: 14.9052 - mae: 1.5834 - val_loss: 15.7916 - val_mse: 15.7916 - val_mae: 1.5715 - lr: 1.0093e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 10s - loss: 14.9145 - mse: 14.9145 - mae: 1.5799 - val_loss: 15.3318 - val_mse: 15.3318 - val_mae: 1.6702 - lr: 1.0093e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 11s - loss: 14.8731 - mse: 14.8731 - mae: 1.5772 - val_loss: 15.3063 - val_mse: 15.3063 - val_mae: 1.6566 - lr: 1.0093e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 10s - loss: 14.8102 - mse: 14.8102 - mae: 1.5767 - val_loss: 15.3160 - val_mse: 15.3160 - val_mae: 1.6383 - lr: 1.0093e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 10s - loss: 14.7660 - mse: 14.7660 - mae: 1.5729 - val_loss: 15.3944 - val_mse: 15.3944 - val_mae: 1.6330 - lr: 1.0093e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 10s - loss: 14.7704 - mse: 14.7704 - mae: 1.5712 - val_loss: 15.4250 - val_mse: 15.4250 - val_mae: 1.6141 - lr: 1.0093e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 10s - loss: 14.6748 - mse: 14.6748 - mae: 1.5652 - val_loss: 15.2517 - val_mse: 15.2517 - val_mae: 1.6602 - lr: 1.0093e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 11s - loss: 14.6392 - mse: 14.6392 - mae: 1.5588 - val_loss: 15.2472 - val_mse: 15.2472 - val_mae: 1.5806 - lr: 1.0093e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 11s - loss: 14.6191 - mse: 14.6191 - mae: 1.5635 - val_loss: 15.1766 - val_mse: 15.1766 - val_mae: 1.6154 - lr: 1.0093e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 11s - loss: 14.5275 - mse: 14.5275 - mae: 1.5589 - val_loss: 15.4755 - val_mse: 15.4755 - val_mae: 1.6193 - lr: 1.0093e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 13/100\n",
            "1000/1000 - 11s - loss: 14.4852 - mse: 14.4852 - mae: 1.5550 - val_loss: 15.4211 - val_mse: 15.4211 - val_mae: 1.6343 - lr: 1.0093e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 14/100\n",
            "1000/1000 - 11s - loss: 14.4616 - mse: 14.4616 - mae: 1.5547 - val_loss: 15.2231 - val_mse: 15.2231 - val_mae: 1.6695 - lr: 1.0093e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 15/100\n",
            "1000/1000 - 12s - loss: 14.3392 - mse: 14.3392 - mae: 1.5491 - val_loss: 14.9814 - val_mse: 14.9814 - val_mae: 1.6550 - lr: 1.0093e-04 - 12s/epoch - 12ms/step\n",
            "Epoch 16/100\n",
            "1000/1000 - 12s - loss: 14.3088 - mse: 14.3088 - mae: 1.5517 - val_loss: 15.3919 - val_mse: 15.3919 - val_mae: 1.5821 - lr: 1.0093e-04 - 12s/epoch - 12ms/step\n",
            "Epoch 17/100\n",
            "1000/1000 - 12s - loss: 14.2008 - mse: 14.2008 - mae: 1.5437 - val_loss: 15.3447 - val_mse: 15.3447 - val_mae: 1.5868 - lr: 1.0093e-04 - 12s/epoch - 12ms/step\n",
            "Epoch 18/100\n",
            "1000/1000 - 12s - loss: 14.1310 - mse: 14.1310 - mae: 1.5436 - val_loss: 15.3090 - val_mse: 15.3090 - val_mae: 1.5904 - lr: 1.0093e-04 - 12s/epoch - 12ms/step\n",
            "Epoch 19/100\n",
            "1000/1000 - 12s - loss: 14.0215 - mse: 14.0215 - mae: 1.5407 - val_loss: 15.5298 - val_mse: 15.5298 - val_mae: 1.6238 - lr: 1.0093e-04 - 12s/epoch - 12ms/step\n",
            "Epoch 20/100\n",
            "1000/1000 - 12s - loss: 14.1026 - mse: 14.1026 - mae: 1.5385 - val_loss: 15.5970 - val_mse: 15.5970 - val_mae: 1.6053 - lr: 1.0093e-04 - 12s/epoch - 12ms/step\n",
            "Score for fold 1: loss of 15.597041130065918\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 12s - loss: 14.8302 - mse: 14.8302 - mae: 1.5585 - val_loss: 12.1282 - val_mse: 12.1282 - val_mae: 1.5433 - lr: 1.0093e-04 - 12s/epoch - 12ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 12s - loss: 14.7876 - mse: 14.7876 - mae: 1.5553 - val_loss: 11.9986 - val_mse: 11.9986 - val_mae: 1.6562 - lr: 1.0093e-04 - 12s/epoch - 12ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 12s - loss: 14.6464 - mse: 14.6464 - mae: 1.5468 - val_loss: 12.3429 - val_mse: 12.3429 - val_mae: 1.6055 - lr: 1.0093e-04 - 12s/epoch - 12ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 12s - loss: 14.5788 - mse: 14.5788 - mae: 1.5486 - val_loss: 12.5390 - val_mse: 12.5390 - val_mae: 1.5550 - lr: 1.0093e-04 - 12s/epoch - 12ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 13s - loss: 14.4333 - mse: 14.4333 - mae: 1.5451 - val_loss: 12.2987 - val_mse: 12.2987 - val_mae: 1.5218 - lr: 1.0093e-04 - 13s/epoch - 13ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 12s - loss: 14.3450 - mse: 14.3450 - mae: 1.5418 - val_loss: 12.3314 - val_mse: 12.3314 - val_mae: 1.4860 - lr: 1.0093e-04 - 12s/epoch - 12ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 13s - loss: 14.2003 - mse: 14.2003 - mae: 1.5334 - val_loss: 12.8388 - val_mse: 12.8388 - val_mae: 1.5531 - lr: 1.0093e-04 - 13s/epoch - 13ms/step\n",
            "Score for fold 2: loss of 12.838789939880371\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 12s - loss: 15.1095 - mse: 15.1095 - mae: 1.5519 - val_loss: 8.8572 - val_mse: 8.8572 - val_mae: 1.5259 - lr: 1.0093e-04 - 12s/epoch - 12ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 12s - loss: 15.0413 - mse: 15.0413 - mae: 1.5567 - val_loss: 8.9740 - val_mse: 8.9740 - val_mae: 1.4806 - lr: 1.0093e-04 - 12s/epoch - 12ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 13s - loss: 14.8664 - mse: 14.8664 - mae: 1.5450 - val_loss: 9.3485 - val_mse: 9.3485 - val_mae: 1.4557 - lr: 1.0093e-04 - 13s/epoch - 13ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 13s - loss: 14.8703 - mse: 14.8703 - mae: 1.5369 - val_loss: 9.3603 - val_mse: 9.3603 - val_mae: 1.5186 - lr: 1.0093e-04 - 13s/epoch - 13ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 12s - loss: 14.6161 - mse: 14.6161 - mae: 1.5408 - val_loss: 9.1920 - val_mse: 9.1920 - val_mae: 1.5517 - lr: 1.0093e-04 - 12s/epoch - 12ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 14s - loss: 14.5149 - mse: 14.5149 - mae: 1.5381 - val_loss: 9.5046 - val_mse: 9.5046 - val_mae: 1.5522 - lr: 1.0093e-04 - 14s/epoch - 14ms/step\n",
            "Score for fold 3: loss of 9.50463581085205\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 12s - loss: 14.4767 - mse: 14.4767 - mae: 1.5455 - val_loss: 9.5226 - val_mse: 9.5226 - val_mae: 1.4310 - lr: 1.0093e-04 - 12s/epoch - 12ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 12s - loss: 14.3949 - mse: 14.3949 - mae: 1.5371 - val_loss: 9.7243 - val_mse: 9.7243 - val_mae: 1.6051 - lr: 1.0093e-04 - 12s/epoch - 12ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 12s - loss: 14.1371 - mse: 14.1371 - mae: 1.5430 - val_loss: 9.6694 - val_mse: 9.6694 - val_mae: 1.5224 - lr: 1.0093e-04 - 12s/epoch - 12ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 12s - loss: 14.1616 - mse: 14.1616 - mae: 1.5292 - val_loss: 10.0055 - val_mse: 10.0055 - val_mae: 1.4609 - lr: 1.0093e-04 - 12s/epoch - 12ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 13s - loss: 13.7625 - mse: 13.7625 - mae: 1.5367 - val_loss: 10.3520 - val_mse: 10.3520 - val_mae: 1.7613 - lr: 1.0093e-04 - 13s/epoch - 13ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 12s - loss: 13.8387 - mse: 13.8387 - mae: 1.5267 - val_loss: 10.5836 - val_mse: 10.5836 - val_mae: 1.8074 - lr: 1.0093e-04 - 12s/epoch - 12ms/step\n",
            "Score for fold 4: loss of 10.583579063415527\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 12s - loss: 10.6268 - mse: 10.6268 - mae: 1.5487 - val_loss: 23.0998 - val_mse: 23.0998 - val_mae: 1.6844 - lr: 1.0093e-04 - 12s/epoch - 12ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 12s - loss: 10.6458 - mse: 10.6458 - mae: 1.5626 - val_loss: 23.5967 - val_mse: 23.5967 - val_mae: 1.4418 - lr: 1.0093e-04 - 12s/epoch - 12ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 11s - loss: 10.4733 - mse: 10.4733 - mae: 1.5502 - val_loss: 23.7681 - val_mse: 23.7681 - val_mae: 1.4489 - lr: 1.0093e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 12s - loss: 10.3465 - mse: 10.3465 - mae: 1.5380 - val_loss: 23.1873 - val_mse: 23.1873 - val_mae: 1.4433 - lr: 1.0093e-04 - 12s/epoch - 12ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 12s - loss: 10.2710 - mse: 10.2710 - mae: 1.5538 - val_loss: 23.2965 - val_mse: 23.2965 - val_mae: 1.6654 - lr: 1.0093e-04 - 12s/epoch - 12ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 12s - loss: 10.2640 - mse: 10.2640 - mae: 1.5631 - val_loss: 24.6815 - val_mse: 24.6815 - val_mae: 1.5994 - lr: 1.0093e-04 - 12s/epoch - 12ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-10-14 13:23:29,253]\u001b[0m Finished trial#35 resulted in value: 14.639999999999997. Current best value is 14.266 with parameters: {'activation': 'relu', 'num_hidden_layer': 4, 'i': 10, 'learning_rate': 0.00010332846452133283}.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Score for fold 5: loss of 24.681537628173828\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 8s - loss: 15.4683 - mse: 15.4683 - mae: 1.6722 - val_loss: 17.8976 - val_mse: 17.8976 - val_mae: 1.6321 - lr: 1.5258e-04 - 8s/epoch - 8ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 7s - loss: 14.8985 - mse: 14.8985 - mae: 1.6152 - val_loss: 17.6089 - val_mse: 17.6089 - val_mae: 1.6509 - lr: 1.5258e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 7s - loss: 14.7993 - mse: 14.7993 - mae: 1.6023 - val_loss: 17.7740 - val_mse: 17.7740 - val_mae: 1.6374 - lr: 1.5258e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 7s - loss: 14.6967 - mse: 14.6967 - mae: 1.5961 - val_loss: 17.5223 - val_mse: 17.5223 - val_mae: 1.6552 - lr: 1.5258e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 7s - loss: 14.6598 - mse: 14.6598 - mae: 1.5908 - val_loss: 17.5147 - val_mse: 17.5147 - val_mae: 1.6233 - lr: 1.5258e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 7s - loss: 14.6408 - mse: 14.6408 - mae: 1.5881 - val_loss: 17.4073 - val_mse: 17.4073 - val_mae: 1.7009 - lr: 1.5258e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 7s - loss: 14.6051 - mse: 14.6051 - mae: 1.5906 - val_loss: 17.4110 - val_mse: 17.4110 - val_mae: 1.6755 - lr: 1.5258e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 7s - loss: 14.6161 - mse: 14.6161 - mae: 1.5860 - val_loss: 17.4834 - val_mse: 17.4834 - val_mae: 1.5943 - lr: 1.5258e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 7s - loss: 14.6115 - mse: 14.6115 - mae: 1.5845 - val_loss: 17.4420 - val_mse: 17.4420 - val_mae: 1.6782 - lr: 1.5258e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 7s - loss: 14.6160 - mse: 14.6160 - mae: 1.5817 - val_loss: 17.3709 - val_mse: 17.3709 - val_mae: 1.6761 - lr: 1.5258e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 7s - loss: 14.6139 - mse: 14.6139 - mae: 1.5799 - val_loss: 17.3489 - val_mse: 17.3489 - val_mae: 1.6382 - lr: 1.5258e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 7s - loss: 14.5994 - mse: 14.5994 - mae: 1.5819 - val_loss: 17.4041 - val_mse: 17.4041 - val_mae: 1.6585 - lr: 1.5258e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 13/100\n",
            "1000/1000 - 7s - loss: 14.6044 - mse: 14.6044 - mae: 1.5807 - val_loss: 17.4710 - val_mse: 17.4710 - val_mae: 1.7181 - lr: 1.5258e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 14/100\n",
            "1000/1000 - 7s - loss: 14.6014 - mse: 14.6014 - mae: 1.5815 - val_loss: 17.5392 - val_mse: 17.5392 - val_mae: 1.6291 - lr: 1.5258e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 15/100\n",
            "1000/1000 - 7s - loss: 14.5823 - mse: 14.5823 - mae: 1.5794 - val_loss: 17.6685 - val_mse: 17.6685 - val_mae: 1.6191 - lr: 1.5258e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 16/100\n",
            "1000/1000 - 7s - loss: 14.5595 - mse: 14.5595 - mae: 1.5812 - val_loss: 17.5990 - val_mse: 17.5990 - val_mae: 1.6010 - lr: 1.5258e-04 - 7s/epoch - 7ms/step\n",
            "Score for fold 1: loss of 17.599040985107422\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 6s - loss: 15.4117 - mse: 15.4117 - mae: 1.5836 - val_loss: 14.1226 - val_mse: 14.1226 - val_mae: 1.5900 - lr: 1.5258e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 6s - loss: 15.4014 - mse: 15.4014 - mae: 1.5817 - val_loss: 14.1341 - val_mse: 14.1341 - val_mae: 1.6421 - lr: 1.5258e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 7s - loss: 15.4081 - mse: 15.4081 - mae: 1.5829 - val_loss: 14.3284 - val_mse: 14.3284 - val_mae: 1.5460 - lr: 1.5258e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 7s - loss: 15.4093 - mse: 15.4093 - mae: 1.5833 - val_loss: 14.1575 - val_mse: 14.1575 - val_mae: 1.6389 - lr: 1.5258e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 7s - loss: 15.3983 - mse: 15.3983 - mae: 1.5869 - val_loss: 14.2033 - val_mse: 14.2033 - val_mae: 1.5763 - lr: 1.5258e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 7s - loss: 15.4222 - mse: 15.4222 - mae: 1.5816 - val_loss: 14.1637 - val_mse: 14.1637 - val_mae: 1.6834 - lr: 1.5258e-04 - 7s/epoch - 7ms/step\n",
            "Score for fold 2: loss of 14.163737297058105\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 6s - loss: 15.4763 - mse: 15.4763 - mae: 1.5980 - val_loss: 13.7357 - val_mse: 13.7357 - val_mae: 1.5690 - lr: 1.5258e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 6s - loss: 15.4718 - mse: 15.4718 - mae: 1.6000 - val_loss: 13.8138 - val_mse: 13.8138 - val_mae: 1.5509 - lr: 1.5258e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 7s - loss: 15.4673 - mse: 15.4673 - mae: 1.5982 - val_loss: 13.6679 - val_mse: 13.6679 - val_mae: 1.6017 - lr: 1.5258e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 7s - loss: 15.4552 - mse: 15.4552 - mae: 1.5971 - val_loss: 13.8043 - val_mse: 13.8043 - val_mae: 1.5837 - lr: 1.5258e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 7s - loss: 15.4402 - mse: 15.4402 - mae: 1.5968 - val_loss: 13.7104 - val_mse: 13.7104 - val_mae: 1.5778 - lr: 1.5258e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 7s - loss: 15.4444 - mse: 15.4444 - mae: 1.5978 - val_loss: 13.7481 - val_mse: 13.7481 - val_mae: 1.5791 - lr: 1.5258e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 7s - loss: 15.4464 - mse: 15.4464 - mae: 1.5965 - val_loss: 13.7665 - val_mse: 13.7665 - val_mae: 1.5107 - lr: 1.5258e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 7s - loss: 15.4344 - mse: 15.4344 - mae: 1.5946 - val_loss: 13.7883 - val_mse: 13.7883 - val_mae: 1.7002 - lr: 1.5258e-04 - 7s/epoch - 7ms/step\n",
            "Score for fold 3: loss of 13.788341522216797\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 6s - loss: 14.4925 - mse: 14.4925 - mae: 1.5958 - val_loss: 17.5140 - val_mse: 17.5140 - val_mae: 1.6179 - lr: 1.5258e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 7s - loss: 14.5051 - mse: 14.5051 - mae: 1.5942 - val_loss: 17.5543 - val_mse: 17.5543 - val_mae: 1.5476 - lr: 1.5258e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 7s - loss: 14.5095 - mse: 14.5095 - mae: 1.5917 - val_loss: 17.5522 - val_mse: 17.5522 - val_mae: 1.5806 - lr: 1.5258e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 7s - loss: 14.4957 - mse: 14.4957 - mae: 1.5973 - val_loss: 17.4944 - val_mse: 17.4944 - val_mae: 1.5598 - lr: 1.5258e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 7s - loss: 14.4719 - mse: 14.4719 - mae: 1.5968 - val_loss: 17.5223 - val_mse: 17.5223 - val_mae: 1.5955 - lr: 1.5258e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 7s - loss: 14.4451 - mse: 14.4451 - mae: 1.5925 - val_loss: 17.5780 - val_mse: 17.5780 - val_mae: 1.5727 - lr: 1.5258e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 7s - loss: 14.4606 - mse: 14.4606 - mae: 1.5961 - val_loss: 17.5220 - val_mse: 17.5220 - val_mae: 1.5565 - lr: 1.5258e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 7s - loss: 14.4491 - mse: 14.4491 - mae: 1.5962 - val_loss: 17.5592 - val_mse: 17.5592 - val_mae: 1.5578 - lr: 1.5258e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 7s - loss: 14.4497 - mse: 14.4497 - mae: 1.5950 - val_loss: 17.5632 - val_mse: 17.5632 - val_mae: 1.6103 - lr: 1.5258e-04 - 7s/epoch - 7ms/step\n",
            "Score for fold 4: loss of 17.56320571899414\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 6s - loss: 15.6454 - mse: 15.6454 - mae: 1.5989 - val_loss: 12.7417 - val_mse: 12.7417 - val_mae: 1.6204 - lr: 1.5258e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 6s - loss: 15.6131 - mse: 15.6131 - mae: 1.6024 - val_loss: 12.7726 - val_mse: 12.7726 - val_mae: 1.5373 - lr: 1.5258e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 7s - loss: 15.6175 - mse: 15.6175 - mae: 1.5966 - val_loss: 12.7319 - val_mse: 12.7319 - val_mae: 1.5494 - lr: 1.5258e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 7s - loss: 15.6098 - mse: 15.6098 - mae: 1.5977 - val_loss: 12.7236 - val_mse: 12.7236 - val_mae: 1.5692 - lr: 1.5258e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 7s - loss: 15.5926 - mse: 15.5926 - mae: 1.5998 - val_loss: 12.9103 - val_mse: 12.9103 - val_mae: 1.5477 - lr: 1.5258e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 7s - loss: 15.6120 - mse: 15.6120 - mae: 1.5983 - val_loss: 12.9156 - val_mse: 12.9156 - val_mae: 1.6413 - lr: 1.5258e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 7s - loss: 15.5975 - mse: 15.5975 - mae: 1.5965 - val_loss: 12.8258 - val_mse: 12.8258 - val_mae: 1.5521 - lr: 1.5258e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 7s - loss: 15.5709 - mse: 15.5709 - mae: 1.5912 - val_loss: 12.7166 - val_mse: 12.7166 - val_mae: 1.5281 - lr: 1.5258e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 7s - loss: 15.5625 - mse: 15.5625 - mae: 1.5949 - val_loss: 12.7325 - val_mse: 12.7325 - val_mae: 1.5589 - lr: 1.5258e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 7s - loss: 15.5578 - mse: 15.5578 - mae: 1.5947 - val_loss: 12.8126 - val_mse: 12.8126 - val_mae: 1.5875 - lr: 1.5258e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 7s - loss: 15.5391 - mse: 15.5391 - mae: 1.5950 - val_loss: 12.8287 - val_mse: 12.8287 - val_mae: 1.5305 - lr: 1.5258e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 7s - loss: 15.5170 - mse: 15.5170 - mae: 1.5953 - val_loss: 12.7417 - val_mse: 12.7417 - val_mae: 1.5209 - lr: 1.5258e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 13/100\n",
            "1000/1000 - 7s - loss: 15.5183 - mse: 15.5183 - mae: 1.5908 - val_loss: 12.8848 - val_mse: 12.8848 - val_mae: 1.5457 - lr: 1.5258e-04 - 7s/epoch - 7ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-10-14 13:29:26,542]\u001b[0m Finished trial#36 resulted in value: 15.197999999999999. Current best value is 14.266 with parameters: {'activation': 'relu', 'num_hidden_layer': 4, 'i': 10, 'learning_rate': 0.00010332846452133283}.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Score for fold 5: loss of 12.88479995727539\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 11s - loss: 17.1945 - mse: 17.1945 - mae: 1.7037 - val_loss: 11.8112 - val_mse: 11.8112 - val_mae: 1.7054 - lr: 1.1016e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 10s - loss: 16.7572 - mse: 16.7572 - mae: 1.6491 - val_loss: 11.7214 - val_mse: 11.7214 - val_mae: 1.5711 - lr: 1.1016e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 10s - loss: 16.7629 - mse: 16.7629 - mae: 1.6428 - val_loss: 11.6373 - val_mse: 11.6373 - val_mae: 1.6158 - lr: 1.1016e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 10s - loss: 16.7159 - mse: 16.7159 - mae: 1.6406 - val_loss: 11.7201 - val_mse: 11.7201 - val_mae: 1.5916 - lr: 1.1016e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 10s - loss: 16.7086 - mse: 16.7086 - mae: 1.6446 - val_loss: 11.6793 - val_mse: 11.6793 - val_mae: 1.6034 - lr: 1.1016e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 10s - loss: 16.6867 - mse: 16.6867 - mae: 1.6442 - val_loss: 11.8973 - val_mse: 11.8973 - val_mae: 1.7071 - lr: 1.1016e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 10s - loss: 16.6769 - mse: 16.6769 - mae: 1.6443 - val_loss: 11.6501 - val_mse: 11.6501 - val_mae: 1.6089 - lr: 1.1016e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 10s - loss: 16.6556 - mse: 16.6556 - mae: 1.6442 - val_loss: 11.6174 - val_mse: 11.6174 - val_mae: 1.6378 - lr: 1.1016e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 10s - loss: 16.6856 - mse: 16.6856 - mae: 1.6423 - val_loss: 11.6724 - val_mse: 11.6724 - val_mae: 1.6033 - lr: 1.1016e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 10s - loss: 16.7131 - mse: 16.7131 - mae: 1.6422 - val_loss: 11.7697 - val_mse: 11.7697 - val_mae: 1.5965 - lr: 1.1016e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 10s - loss: 16.6692 - mse: 16.6692 - mae: 1.6428 - val_loss: 11.6327 - val_mse: 11.6327 - val_mae: 1.6038 - lr: 1.1016e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 10s - loss: 16.6860 - mse: 16.6860 - mae: 1.6441 - val_loss: 11.6999 - val_mse: 11.6999 - val_mae: 1.5995 - lr: 1.1016e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 13/100\n",
            "1000/1000 - 10s - loss: 16.6627 - mse: 16.6627 - mae: 1.6418 - val_loss: 11.6780 - val_mse: 11.6780 - val_mae: 1.5981 - lr: 1.1016e-04 - 10s/epoch - 10ms/step\n",
            "Score for fold 1: loss of 11.677992820739746\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 10s - loss: 14.1552 - mse: 14.1552 - mae: 1.6291 - val_loss: 21.6604 - val_mse: 21.6604 - val_mae: 1.6055 - lr: 1.1016e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 10s - loss: 14.1774 - mse: 14.1774 - mae: 1.6337 - val_loss: 21.6208 - val_mse: 21.6208 - val_mae: 1.6389 - lr: 1.1016e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 10s - loss: 14.1735 - mse: 14.1735 - mae: 1.6331 - val_loss: 21.5924 - val_mse: 21.5924 - val_mae: 1.7137 - lr: 1.1016e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 11s - loss: 14.2042 - mse: 14.2042 - mae: 1.6354 - val_loss: 21.6163 - val_mse: 21.6163 - val_mae: 1.6540 - lr: 1.1016e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 11s - loss: 14.1445 - mse: 14.1445 - mae: 1.6369 - val_loss: 21.6369 - val_mse: 21.6369 - val_mae: 1.6593 - lr: 1.1016e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 11s - loss: 14.1594 - mse: 14.1594 - mae: 1.6331 - val_loss: 21.7925 - val_mse: 21.7925 - val_mae: 1.6699 - lr: 1.1016e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 10s - loss: 14.1574 - mse: 14.1574 - mae: 1.6379 - val_loss: 21.6447 - val_mse: 21.6447 - val_mae: 1.6777 - lr: 1.1016e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 11s - loss: 14.1313 - mse: 14.1313 - mae: 1.6386 - val_loss: 21.6245 - val_mse: 21.6245 - val_mae: 1.6716 - lr: 1.1016e-04 - 11s/epoch - 11ms/step\n",
            "Score for fold 2: loss of 21.624467849731445\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 10s - loss: 15.6226 - mse: 15.6226 - mae: 1.6365 - val_loss: 15.6679 - val_mse: 15.6679 - val_mae: 1.6411 - lr: 1.1016e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 10s - loss: 15.6799 - mse: 15.6799 - mae: 1.6353 - val_loss: 15.6350 - val_mse: 15.6350 - val_mae: 1.6425 - lr: 1.1016e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 10s - loss: 15.6371 - mse: 15.6371 - mae: 1.6352 - val_loss: 15.6406 - val_mse: 15.6406 - val_mae: 1.6522 - lr: 1.1016e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 10s - loss: 15.6332 - mse: 15.6332 - mae: 1.6383 - val_loss: 15.7294 - val_mse: 15.7294 - val_mae: 1.6115 - lr: 1.1016e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 10s - loss: 15.6515 - mse: 15.6515 - mae: 1.6341 - val_loss: 15.6466 - val_mse: 15.6466 - val_mae: 1.6458 - lr: 1.1016e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 10s - loss: 15.6447 - mse: 15.6447 - mae: 1.6310 - val_loss: 15.6674 - val_mse: 15.6674 - val_mae: 1.6248 - lr: 1.1016e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 10s - loss: 15.6779 - mse: 15.6779 - mae: 1.6341 - val_loss: 15.6609 - val_mse: 15.6609 - val_mae: 1.6379 - lr: 1.1016e-04 - 10s/epoch - 10ms/step\n",
            "Score for fold 3: loss of 15.660926818847656\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 10s - loss: 15.7785 - mse: 15.7785 - mae: 1.6313 - val_loss: 15.2906 - val_mse: 15.2906 - val_mae: 1.6116 - lr: 1.1016e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 10s - loss: 15.7612 - mse: 15.7612 - mae: 1.6280 - val_loss: 15.2359 - val_mse: 15.2359 - val_mae: 1.6537 - lr: 1.1016e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 10s - loss: 15.7454 - mse: 15.7454 - mae: 1.6256 - val_loss: 15.2806 - val_mse: 15.2806 - val_mae: 1.6388 - lr: 1.1016e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 10s - loss: 15.7584 - mse: 15.7584 - mae: 1.6232 - val_loss: 15.2574 - val_mse: 15.2574 - val_mae: 1.6363 - lr: 1.1016e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 10s - loss: 15.7682 - mse: 15.7682 - mae: 1.6247 - val_loss: 15.2335 - val_mse: 15.2335 - val_mae: 1.6374 - lr: 1.1016e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 10s - loss: 15.7395 - mse: 15.7395 - mae: 1.6244 - val_loss: 15.3129 - val_mse: 15.3129 - val_mae: 1.6178 - lr: 1.1016e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 10s - loss: 15.7797 - mse: 15.7797 - mae: 1.6249 - val_loss: 15.2335 - val_mse: 15.2335 - val_mae: 1.6213 - lr: 1.1016e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 10s - loss: 15.7747 - mse: 15.7747 - mae: 1.6343 - val_loss: 15.1234 - val_mse: 15.1234 - val_mae: 1.6845 - lr: 1.1016e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 10s - loss: 15.7486 - mse: 15.7486 - mae: 1.6254 - val_loss: 15.2094 - val_mse: 15.2094 - val_mae: 1.6260 - lr: 1.1016e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 10s - loss: 15.7361 - mse: 15.7361 - mae: 1.6235 - val_loss: 15.4435 - val_mse: 15.4435 - val_mae: 1.6232 - lr: 1.1016e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 10s - loss: 15.7223 - mse: 15.7223 - mae: 1.6241 - val_loss: 15.2031 - val_mse: 15.2031 - val_mae: 1.6225 - lr: 1.1016e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 10s - loss: 15.7791 - mse: 15.7791 - mae: 1.6258 - val_loss: 15.3114 - val_mse: 15.3114 - val_mae: 1.6028 - lr: 1.1016e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 13/100\n",
            "1000/1000 - 10s - loss: 15.7934 - mse: 15.7934 - mae: 1.6272 - val_loss: 15.3994 - val_mse: 15.3994 - val_mae: 1.5718 - lr: 1.1016e-04 - 10s/epoch - 10ms/step\n",
            "Score for fold 4: loss of 15.399435997009277\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 10s - loss: 16.0955 - mse: 16.0955 - mae: 1.6280 - val_loss: 14.0423 - val_mse: 14.0423 - val_mae: 1.6250 - lr: 1.1016e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 10s - loss: 16.0805 - mse: 16.0805 - mae: 1.6331 - val_loss: 13.9815 - val_mse: 13.9815 - val_mae: 1.6971 - lr: 1.1016e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 10s - loss: 16.0898 - mse: 16.0898 - mae: 1.6364 - val_loss: 14.0948 - val_mse: 14.0948 - val_mae: 1.7156 - lr: 1.1016e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 10s - loss: 16.0746 - mse: 16.0746 - mae: 1.6324 - val_loss: 13.9518 - val_mse: 13.9518 - val_mae: 1.6576 - lr: 1.1016e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 10s - loss: 16.0968 - mse: 16.0968 - mae: 1.6316 - val_loss: 13.9515 - val_mse: 13.9515 - val_mae: 1.7044 - lr: 1.1016e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 11s - loss: 16.1087 - mse: 16.1087 - mae: 1.6354 - val_loss: 14.0003 - val_mse: 14.0003 - val_mae: 1.6180 - lr: 1.1016e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 10s - loss: 16.0929 - mse: 16.0929 - mae: 1.6322 - val_loss: 13.9626 - val_mse: 13.9626 - val_mae: 1.6900 - lr: 1.1016e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 10s - loss: 16.0724 - mse: 16.0724 - mae: 1.6320 - val_loss: 13.9954 - val_mse: 13.9954 - val_mae: 1.6226 - lr: 1.1016e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 10s - loss: 16.1290 - mse: 16.1290 - mae: 1.6300 - val_loss: 14.0412 - val_mse: 14.0412 - val_mae: 1.5753 - lr: 1.1016e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 10s - loss: 16.1349 - mse: 16.1349 - mae: 1.6335 - val_loss: 13.9874 - val_mse: 13.9874 - val_mae: 1.6352 - lr: 1.1016e-04 - 10s/epoch - 10ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-10-14 13:38:14,842]\u001b[0m Finished trial#37 resulted in value: 15.669999999999998. Current best value is 14.266 with parameters: {'activation': 'relu', 'num_hidden_layer': 4, 'i': 10, 'learning_rate': 0.00010332846452133283}.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Score for fold 5: loss of 13.987383842468262\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 3s - loss: 13.9427 - mse: 13.9427 - mae: 1.6226 - val_loss: 23.3196 - val_mse: 23.3196 - val_mae: 1.5994 - lr: 1.5789e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 3s - loss: 13.2136 - mse: 13.2136 - mae: 1.5768 - val_loss: 23.3350 - val_mse: 23.3350 - val_mae: 1.5610 - lr: 1.5789e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 3s - loss: 13.0778 - mse: 13.0778 - mae: 1.5766 - val_loss: 23.1225 - val_mse: 23.1225 - val_mae: 1.6098 - lr: 1.5789e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 3s - loss: 13.0676 - mse: 13.0676 - mae: 1.5770 - val_loss: 23.0775 - val_mse: 23.0775 - val_mae: 1.6182 - lr: 1.5789e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 3s - loss: 13.0107 - mse: 13.0107 - mae: 1.5720 - val_loss: 23.0892 - val_mse: 23.0892 - val_mae: 1.6111 - lr: 1.5789e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 3s - loss: 12.9977 - mse: 12.9977 - mae: 1.5724 - val_loss: 23.1273 - val_mse: 23.1273 - val_mae: 1.6310 - lr: 1.5789e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 3s - loss: 12.9468 - mse: 12.9468 - mae: 1.5707 - val_loss: 23.1032 - val_mse: 23.1032 - val_mae: 1.5785 - lr: 1.5789e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 3s - loss: 12.9562 - mse: 12.9562 - mae: 1.5714 - val_loss: 23.0405 - val_mse: 23.0405 - val_mae: 1.5916 - lr: 1.5789e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 3s - loss: 12.9304 - mse: 12.9304 - mae: 1.5690 - val_loss: 23.0407 - val_mse: 23.0407 - val_mae: 1.5993 - lr: 1.5789e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 3s - loss: 12.9181 - mse: 12.9181 - mae: 1.5667 - val_loss: 23.0083 - val_mse: 23.0083 - val_mae: 1.6003 - lr: 1.5789e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 3s - loss: 12.9028 - mse: 12.9028 - mae: 1.5669 - val_loss: 22.9910 - val_mse: 22.9910 - val_mae: 1.5958 - lr: 1.5789e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 3s - loss: 12.8633 - mse: 12.8633 - mae: 1.5683 - val_loss: 22.9864 - val_mse: 22.9864 - val_mae: 1.6398 - lr: 1.5789e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 13/100\n",
            "1000/1000 - 3s - loss: 12.8524 - mse: 12.8524 - mae: 1.5657 - val_loss: 22.9518 - val_mse: 22.9518 - val_mae: 1.6536 - lr: 1.5789e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 14/100\n",
            "1000/1000 - 3s - loss: 12.8972 - mse: 12.8972 - mae: 1.5644 - val_loss: 22.9621 - val_mse: 22.9621 - val_mae: 1.5862 - lr: 1.5789e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 15/100\n",
            "1000/1000 - 3s - loss: 12.8320 - mse: 12.8320 - mae: 1.5647 - val_loss: 22.9692 - val_mse: 22.9692 - val_mae: 1.5996 - lr: 1.5789e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 16/100\n",
            "1000/1000 - 3s - loss: 12.8398 - mse: 12.8398 - mae: 1.5633 - val_loss: 22.8824 - val_mse: 22.8824 - val_mae: 1.6360 - lr: 1.5789e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 17/100\n",
            "1000/1000 - 3s - loss: 12.7802 - mse: 12.7802 - mae: 1.5621 - val_loss: 22.9551 - val_mse: 22.9551 - val_mae: 1.6223 - lr: 1.5789e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 18/100\n",
            "1000/1000 - 3s - loss: 12.8268 - mse: 12.8268 - mae: 1.5610 - val_loss: 22.9169 - val_mse: 22.9169 - val_mae: 1.5844 - lr: 1.5789e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 19/100\n",
            "1000/1000 - 3s - loss: 12.7714 - mse: 12.7714 - mae: 1.5591 - val_loss: 22.9284 - val_mse: 22.9284 - val_mae: 1.6413 - lr: 1.5789e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 20/100\n",
            "1000/1000 - 3s - loss: 12.7814 - mse: 12.7814 - mae: 1.5619 - val_loss: 22.9361 - val_mse: 22.9361 - val_mae: 1.5911 - lr: 1.5789e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 21/100\n",
            "1000/1000 - 3s - loss: 12.7547 - mse: 12.7547 - mae: 1.5580 - val_loss: 22.9464 - val_mse: 22.9464 - val_mae: 1.5938 - lr: 1.5789e-04 - 3s/epoch - 3ms/step\n",
            "Score for fold 1: loss of 22.94642448425293\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 3s - loss: 14.0737 - mse: 14.0737 - mae: 1.5628 - val_loss: 17.5475 - val_mse: 17.5475 - val_mae: 1.6216 - lr: 1.5789e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 3s - loss: 14.0716 - mse: 14.0716 - mae: 1.5599 - val_loss: 17.6793 - val_mse: 17.6793 - val_mae: 1.5939 - lr: 1.5789e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 3s - loss: 14.0668 - mse: 14.0668 - mae: 1.5575 - val_loss: 17.6792 - val_mse: 17.6792 - val_mae: 1.5875 - lr: 1.5789e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 3s - loss: 14.0563 - mse: 14.0563 - mae: 1.5593 - val_loss: 17.5699 - val_mse: 17.5699 - val_mae: 1.6236 - lr: 1.5789e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 3s - loss: 14.0141 - mse: 14.0141 - mae: 1.5574 - val_loss: 17.7596 - val_mse: 17.7596 - val_mae: 1.5789 - lr: 1.5789e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 3s - loss: 14.0273 - mse: 14.0273 - mae: 1.5559 - val_loss: 17.7928 - val_mse: 17.7928 - val_mae: 1.5815 - lr: 1.5789e-04 - 3s/epoch - 3ms/step\n",
            "Score for fold 2: loss of 17.792755126953125\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 3s - loss: 15.4016 - mse: 15.4016 - mae: 1.5681 - val_loss: 12.1709 - val_mse: 12.1709 - val_mae: 1.5307 - lr: 1.5789e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 3s - loss: 15.3783 - mse: 15.3783 - mae: 1.5630 - val_loss: 12.2651 - val_mse: 12.2651 - val_mae: 1.5641 - lr: 1.5789e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 3s - loss: 15.3583 - mse: 15.3583 - mae: 1.5662 - val_loss: 12.2102 - val_mse: 12.2102 - val_mae: 1.5205 - lr: 1.5789e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 3s - loss: 15.3545 - mse: 15.3545 - mae: 1.5642 - val_loss: 12.3440 - val_mse: 12.3440 - val_mae: 1.5532 - lr: 1.5789e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 3s - loss: 15.3395 - mse: 15.3395 - mae: 1.5637 - val_loss: 12.3658 - val_mse: 12.3658 - val_mae: 1.5123 - lr: 1.5789e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 3s - loss: 15.3109 - mse: 15.3109 - mae: 1.5650 - val_loss: 12.3206 - val_mse: 12.3206 - val_mae: 1.5870 - lr: 1.5789e-04 - 3s/epoch - 3ms/step\n",
            "Score for fold 3: loss of 12.320643424987793\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 3s - loss: 15.6161 - mse: 15.6161 - mae: 1.5647 - val_loss: 11.1883 - val_mse: 11.1883 - val_mae: 1.4835 - lr: 1.5789e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 3s - loss: 15.6387 - mse: 15.6387 - mae: 1.5602 - val_loss: 11.0131 - val_mse: 11.0131 - val_mae: 1.5932 - lr: 1.5789e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 3s - loss: 15.6090 - mse: 15.6090 - mae: 1.5626 - val_loss: 11.1090 - val_mse: 11.1090 - val_mae: 1.5925 - lr: 1.5789e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 3s - loss: 15.6249 - mse: 15.6249 - mae: 1.5651 - val_loss: 11.0836 - val_mse: 11.0836 - val_mae: 1.5546 - lr: 1.5789e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 3s - loss: 15.5986 - mse: 15.5986 - mae: 1.5584 - val_loss: 11.0983 - val_mse: 11.0983 - val_mae: 1.5536 - lr: 1.5789e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 3s - loss: 15.5958 - mse: 15.5958 - mae: 1.5592 - val_loss: 11.1274 - val_mse: 11.1274 - val_mae: 1.5245 - lr: 1.5789e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 3s - loss: 15.5569 - mse: 15.5569 - mae: 1.5581 - val_loss: 11.2832 - val_mse: 11.2832 - val_mae: 1.5005 - lr: 1.5789e-04 - 3s/epoch - 3ms/step\n",
            "Score for fold 4: loss of 11.283193588256836\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 3s - loss: 15.8462 - mse: 15.8462 - mae: 1.5640 - val_loss: 9.9040 - val_mse: 9.9040 - val_mae: 1.5031 - lr: 1.5789e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 3s - loss: 15.8154 - mse: 15.8154 - mae: 1.5636 - val_loss: 9.9719 - val_mse: 9.9719 - val_mae: 1.5389 - lr: 1.5789e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 3s - loss: 15.8105 - mse: 15.8105 - mae: 1.5632 - val_loss: 9.9429 - val_mse: 9.9429 - val_mae: 1.4942 - lr: 1.5789e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 3s - loss: 15.7884 - mse: 15.7884 - mae: 1.5645 - val_loss: 10.0009 - val_mse: 10.0009 - val_mae: 1.5282 - lr: 1.5789e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 3s - loss: 15.7765 - mse: 15.7765 - mae: 1.5636 - val_loss: 9.9684 - val_mse: 9.9684 - val_mae: 1.5095 - lr: 1.5789e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 3s - loss: 15.7554 - mse: 15.7554 - mae: 1.5638 - val_loss: 9.9904 - val_mse: 9.9904 - val_mae: 1.5235 - lr: 1.5789e-04 - 3s/epoch - 3ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-10-14 13:40:31,890]\u001b[0m Finished trial#38 resulted in value: 14.865999999999996. Current best value is 14.266 with parameters: {'activation': 'relu', 'num_hidden_layer': 4, 'i': 10, 'learning_rate': 0.00010332846452133283}.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Score for fold 5: loss of 9.990413665771484\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 3s - loss: 16.4589 - mse: 16.4589 - mae: 1.6443 - val_loss: 11.7078 - val_mse: 11.7078 - val_mae: 1.6814 - lr: 0.0053 - 3s/epoch - 3ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 16.0563 - mse: 16.0563 - mae: 1.6151 - val_loss: 11.9554 - val_mse: 11.9554 - val_mae: 1.4923 - lr: 0.0053 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 3s - loss: 16.1206 - mse: 16.1206 - mae: 1.6055 - val_loss: 11.6829 - val_mse: 11.6829 - val_mae: 1.6041 - lr: 0.0053 - 3s/epoch - 3ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 3s - loss: 16.0765 - mse: 16.0765 - mae: 1.6063 - val_loss: 11.8927 - val_mse: 11.8927 - val_mae: 1.5298 - lr: 0.0053 - 3s/epoch - 3ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 3s - loss: 16.0382 - mse: 16.0382 - mae: 1.6166 - val_loss: 11.6787 - val_mse: 11.6787 - val_mae: 1.5722 - lr: 0.0053 - 3s/epoch - 3ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 3s - loss: 15.9993 - mse: 15.9993 - mae: 1.5999 - val_loss: 11.6287 - val_mse: 11.6287 - val_mae: 1.6355 - lr: 0.0053 - 3s/epoch - 3ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 3s - loss: 15.9772 - mse: 15.9772 - mae: 1.5932 - val_loss: 11.8345 - val_mse: 11.8345 - val_mae: 1.6618 - lr: 0.0053 - 3s/epoch - 3ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 3s - loss: 15.8818 - mse: 15.8818 - mae: 1.5936 - val_loss: 11.6608 - val_mse: 11.6608 - val_mae: 1.5918 - lr: 0.0053 - 3s/epoch - 3ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 3s - loss: 15.9045 - mse: 15.9045 - mae: 1.6026 - val_loss: 11.9058 - val_mse: 11.9058 - val_mae: 1.6560 - lr: 0.0053 - 3s/epoch - 3ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 3s - loss: 16.1206 - mse: 16.1206 - mae: 1.6075 - val_loss: 11.5791 - val_mse: 11.5791 - val_mae: 1.5934 - lr: 0.0053 - 3s/epoch - 3ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 3s - loss: 16.0751 - mse: 16.0751 - mae: 1.6054 - val_loss: 11.6729 - val_mse: 11.6729 - val_mae: 1.6971 - lr: 0.0053 - 3s/epoch - 3ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 3s - loss: 16.0511 - mse: 16.0511 - mae: 1.6124 - val_loss: 11.6582 - val_mse: 11.6582 - val_mae: 1.5539 - lr: 0.0053 - 3s/epoch - 3ms/step\n",
            "Epoch 13/100\n",
            "1000/1000 - 3s - loss: 15.9574 - mse: 15.9574 - mae: 1.6074 - val_loss: 11.7354 - val_mse: 11.7354 - val_mae: 1.5988 - lr: 0.0053 - 3s/epoch - 3ms/step\n",
            "Epoch 14/100\n",
            "1000/1000 - 3s - loss: 15.9329 - mse: 15.9329 - mae: 1.6037 - val_loss: 11.8070 - val_mse: 11.8070 - val_mae: 1.6431 - lr: 0.0053 - 3s/epoch - 3ms/step\n",
            "Epoch 15/100\n",
            "1000/1000 - 3s - loss: 15.9970 - mse: 15.9970 - mae: 1.6136 - val_loss: 11.4403 - val_mse: 11.4403 - val_mae: 1.6650 - lr: 0.0053 - 3s/epoch - 3ms/step\n",
            "Epoch 16/100\n",
            "1000/1000 - 3s - loss: 15.9444 - mse: 15.9444 - mae: 1.6080 - val_loss: 11.7218 - val_mse: 11.7218 - val_mae: 1.5286 - lr: 0.0053 - 3s/epoch - 3ms/step\n",
            "Epoch 17/100\n",
            "1000/1000 - 3s - loss: 16.0535 - mse: 16.0535 - mae: 1.6036 - val_loss: 11.6205 - val_mse: 11.6205 - val_mae: 1.6444 - lr: 0.0053 - 3s/epoch - 3ms/step\n",
            "Epoch 18/100\n",
            "1000/1000 - 3s - loss: 15.9545 - mse: 15.9545 - mae: 1.6114 - val_loss: 11.7718 - val_mse: 11.7718 - val_mae: 1.5322 - lr: 0.0053 - 3s/epoch - 3ms/step\n",
            "Epoch 19/100\n",
            "1000/1000 - 3s - loss: 15.9773 - mse: 15.9773 - mae: 1.6123 - val_loss: 11.8000 - val_mse: 11.8000 - val_mae: 1.6562 - lr: 0.0053 - 3s/epoch - 3ms/step\n",
            "Epoch 20/100\n",
            "1000/1000 - 3s - loss: 16.1871 - mse: 16.1871 - mae: 1.6182 - val_loss: 11.7326 - val_mse: 11.7326 - val_mae: 1.6948 - lr: 0.0053 - 3s/epoch - 3ms/step\n",
            "Score for fold 1: loss of 11.732647895812988\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 3s - loss: 15.5231 - mse: 15.5231 - mae: 1.5816 - val_loss: 12.7168 - val_mse: 12.7168 - val_mae: 1.5937 - lr: 0.0011 - 3s/epoch - 3ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 15.4545 - mse: 15.4545 - mae: 1.5761 - val_loss: 12.7049 - val_mse: 12.7049 - val_mae: 1.6537 - lr: 0.0011 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 15.4563 - mse: 15.4563 - mae: 1.5739 - val_loss: 12.6474 - val_mse: 12.6474 - val_mae: 1.5261 - lr: 0.0011 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 15.4036 - mse: 15.4036 - mae: 1.5725 - val_loss: 12.7514 - val_mse: 12.7514 - val_mae: 1.5987 - lr: 0.0011 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 15.4550 - mse: 15.4550 - mae: 1.5772 - val_loss: 12.7700 - val_mse: 12.7700 - val_mae: 1.5942 - lr: 0.0011 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 15.4077 - mse: 15.4077 - mae: 1.5722 - val_loss: 12.6873 - val_mse: 12.6873 - val_mae: 1.5158 - lr: 0.0011 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 15.4350 - mse: 15.4350 - mae: 1.5741 - val_loss: 12.6934 - val_mse: 12.6934 - val_mae: 1.5660 - lr: 0.0011 - 2s/epoch - 2ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 2s - loss: 15.4186 - mse: 15.4186 - mae: 1.5729 - val_loss: 12.6693 - val_mse: 12.6693 - val_mae: 1.6400 - lr: 0.0011 - 2s/epoch - 2ms/step\n",
            "Score for fold 2: loss of 12.669278144836426\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 3s - loss: 15.7750 - mse: 15.7750 - mae: 1.5831 - val_loss: 11.1675 - val_mse: 11.1675 - val_mae: 1.5458 - lr: 0.0010 - 3s/epoch - 3ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 15.7442 - mse: 15.7442 - mae: 1.5826 - val_loss: 11.2582 - val_mse: 11.2582 - val_mae: 1.5338 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 15.7792 - mse: 15.7792 - mae: 1.5792 - val_loss: 11.2660 - val_mse: 11.2660 - val_mae: 1.5579 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 3s - loss: 15.7648 - mse: 15.7648 - mae: 1.5844 - val_loss: 11.2130 - val_mse: 11.2130 - val_mae: 1.5225 - lr: 0.0010 - 3s/epoch - 3ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 15.7447 - mse: 15.7447 - mae: 1.5854 - val_loss: 11.3030 - val_mse: 11.3030 - val_mae: 1.5768 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 3s - loss: 15.7195 - mse: 15.7195 - mae: 1.5828 - val_loss: 11.2386 - val_mse: 11.2386 - val_mae: 1.5402 - lr: 0.0010 - 3s/epoch - 3ms/step\n",
            "Score for fold 3: loss of 11.238570213317871\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 3s - loss: 14.9148 - mse: 14.9148 - mae: 1.5723 - val_loss: 14.4583 - val_mse: 14.4583 - val_mae: 1.5687 - lr: 0.0010 - 3s/epoch - 3ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 14.9238 - mse: 14.9238 - mae: 1.5729 - val_loss: 14.5279 - val_mse: 14.5279 - val_mae: 1.5530 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 14.8927 - mse: 14.8927 - mae: 1.5726 - val_loss: 14.5764 - val_mse: 14.5764 - val_mae: 1.6003 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 3s - loss: 14.8461 - mse: 14.8461 - mae: 1.5707 - val_loss: 14.4233 - val_mse: 14.4233 - val_mae: 1.6322 - lr: 0.0010 - 3s/epoch - 3ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 14.8693 - mse: 14.8693 - mae: 1.5677 - val_loss: 14.4767 - val_mse: 14.4767 - val_mae: 1.5765 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 14.9256 - mse: 14.9256 - mae: 1.5732 - val_loss: 14.5253 - val_mse: 14.5253 - val_mae: 1.6028 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 3s - loss: 14.8768 - mse: 14.8768 - mae: 1.5688 - val_loss: 14.4962 - val_mse: 14.4962 - val_mae: 1.6082 - lr: 0.0010 - 3s/epoch - 3ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 3s - loss: 14.8123 - mse: 14.8123 - mae: 1.5750 - val_loss: 14.5344 - val_mse: 14.5344 - val_mae: 1.6646 - lr: 0.0010 - 3s/epoch - 3ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 3s - loss: 14.8347 - mse: 14.8347 - mae: 1.5669 - val_loss: 14.5787 - val_mse: 14.5787 - val_mae: 1.5790 - lr: 0.0010 - 3s/epoch - 3ms/step\n",
            "Score for fold 4: loss of 14.57866382598877\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 3s - loss: 12.4062 - mse: 12.4062 - mae: 1.5689 - val_loss: 24.1540 - val_mse: 24.1540 - val_mae: 1.6216 - lr: 0.0010 - 3s/epoch - 3ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 12.4471 - mse: 12.4471 - mae: 1.5692 - val_loss: 24.2091 - val_mse: 24.2091 - val_mae: 1.5741 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 3s - loss: 12.4575 - mse: 12.4575 - mae: 1.5715 - val_loss: 24.1905 - val_mse: 24.1905 - val_mae: 1.5624 - lr: 0.0010 - 3s/epoch - 3ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 3s - loss: 12.4001 - mse: 12.4001 - mae: 1.5712 - val_loss: 24.4192 - val_mse: 24.4192 - val_mae: 1.5618 - lr: 0.0010 - 3s/epoch - 3ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 3s - loss: 12.3435 - mse: 12.3435 - mae: 1.5708 - val_loss: 24.3624 - val_mse: 24.3624 - val_mae: 1.5849 - lr: 0.0010 - 3s/epoch - 3ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 3s - loss: 12.4154 - mse: 12.4154 - mae: 1.5717 - val_loss: 24.5848 - val_mse: 24.5848 - val_mae: 1.5390 - lr: 0.0010 - 3s/epoch - 3ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-10-14 13:42:42,500]\u001b[0m Finished trial#39 resulted in value: 14.959999999999999. Current best value is 14.266 with parameters: {'activation': 'relu', 'num_hidden_layer': 4, 'i': 10, 'learning_rate': 0.00010332846452133283}.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Score for fold 5: loss of 24.584774017333984\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 7s - loss: 16.4121 - mse: 16.4121 - mae: 1.6147 - val_loss: 11.6517 - val_mse: 11.6517 - val_mae: 1.6038 - lr: 2.4857e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 6s - loss: 15.9847 - mse: 15.9847 - mae: 1.5993 - val_loss: 11.4669 - val_mse: 11.4669 - val_mae: 1.5645 - lr: 2.4857e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 6s - loss: 15.9091 - mse: 15.9091 - mae: 1.5915 - val_loss: 11.5110 - val_mse: 11.5110 - val_mae: 1.5854 - lr: 2.4857e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 7s - loss: 15.8834 - mse: 15.8834 - mae: 1.5922 - val_loss: 11.3089 - val_mse: 11.3089 - val_mae: 1.6553 - lr: 2.4857e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 6s - loss: 15.8787 - mse: 15.8787 - mae: 1.5853 - val_loss: 11.3465 - val_mse: 11.3465 - val_mae: 1.6302 - lr: 2.4857e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 6s - loss: 15.8661 - mse: 15.8661 - mae: 1.5822 - val_loss: 11.4167 - val_mse: 11.4167 - val_mae: 1.5393 - lr: 2.4857e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 7s - loss: 15.8004 - mse: 15.8004 - mae: 1.5826 - val_loss: 11.4379 - val_mse: 11.4379 - val_mae: 1.5832 - lr: 2.4857e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 7s - loss: 15.7772 - mse: 15.7772 - mae: 1.5803 - val_loss: 11.4535 - val_mse: 11.4535 - val_mae: 1.5682 - lr: 2.4857e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 6s - loss: 15.7656 - mse: 15.7656 - mae: 1.5750 - val_loss: 11.2482 - val_mse: 11.2482 - val_mae: 1.5690 - lr: 2.4857e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 6s - loss: 15.6908 - mse: 15.6908 - mae: 1.5745 - val_loss: 11.6325 - val_mse: 11.6325 - val_mae: 1.5507 - lr: 2.4857e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 6s - loss: 15.6635 - mse: 15.6635 - mae: 1.5738 - val_loss: 11.4169 - val_mse: 11.4169 - val_mae: 1.5300 - lr: 2.4857e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 6s - loss: 15.7084 - mse: 15.7084 - mae: 1.5739 - val_loss: 11.2638 - val_mse: 11.2638 - val_mae: 1.6107 - lr: 2.4857e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 13/100\n",
            "1000/1000 - 6s - loss: 15.5880 - mse: 15.5880 - mae: 1.5696 - val_loss: 11.2903 - val_mse: 11.2903 - val_mae: 1.5224 - lr: 2.4857e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 14/100\n",
            "1000/1000 - 6s - loss: 15.6403 - mse: 15.6403 - mae: 1.5669 - val_loss: 11.2215 - val_mse: 11.2215 - val_mae: 1.5146 - lr: 2.4857e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 15/100\n",
            "1000/1000 - 6s - loss: 15.6288 - mse: 15.6288 - mae: 1.5661 - val_loss: 11.3014 - val_mse: 11.3014 - val_mae: 1.5925 - lr: 2.4857e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 16/100\n",
            "1000/1000 - 7s - loss: 15.5814 - mse: 15.5814 - mae: 1.5683 - val_loss: 11.2226 - val_mse: 11.2226 - val_mae: 1.5678 - lr: 2.4857e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 17/100\n",
            "1000/1000 - 7s - loss: 15.4997 - mse: 15.4997 - mae: 1.5640 - val_loss: 11.3190 - val_mse: 11.3190 - val_mae: 1.5645 - lr: 2.4857e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 18/100\n",
            "1000/1000 - 7s - loss: 15.5618 - mse: 15.5618 - mae: 1.5600 - val_loss: 11.2988 - val_mse: 11.2988 - val_mae: 1.5491 - lr: 2.4857e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 19/100\n",
            "1000/1000 - 7s - loss: 15.4777 - mse: 15.4777 - mae: 1.5599 - val_loss: 11.5034 - val_mse: 11.5034 - val_mae: 1.5889 - lr: 2.4857e-04 - 7s/epoch - 7ms/step\n",
            "Score for fold 1: loss of 11.503372192382812\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 7s - loss: 12.4956 - mse: 12.4956 - mae: 1.5513 - val_loss: 23.3647 - val_mse: 23.3647 - val_mae: 1.5976 - lr: 2.4857e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 7s - loss: 12.4076 - mse: 12.4076 - mae: 1.5512 - val_loss: 23.4762 - val_mse: 23.4762 - val_mae: 1.6237 - lr: 2.4857e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 7s - loss: 12.5495 - mse: 12.5495 - mae: 1.5479 - val_loss: 23.2896 - val_mse: 23.2896 - val_mae: 1.6108 - lr: 2.4857e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 7s - loss: 12.4130 - mse: 12.4130 - mae: 1.5443 - val_loss: 23.4802 - val_mse: 23.4802 - val_mae: 1.5767 - lr: 2.4857e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 7s - loss: 12.3808 - mse: 12.3808 - mae: 1.5463 - val_loss: 23.5888 - val_mse: 23.5888 - val_mae: 1.5721 - lr: 2.4857e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 7s - loss: 12.3586 - mse: 12.3586 - mae: 1.5443 - val_loss: 23.5074 - val_mse: 23.5074 - val_mae: 1.5913 - lr: 2.4857e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 7s - loss: 12.3671 - mse: 12.3671 - mae: 1.5428 - val_loss: 23.5130 - val_mse: 23.5130 - val_mae: 1.6156 - lr: 2.4857e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 7s - loss: 12.3024 - mse: 12.3024 - mae: 1.5422 - val_loss: 23.4889 - val_mse: 23.4889 - val_mae: 1.6970 - lr: 2.4857e-04 - 7s/epoch - 7ms/step\n",
            "Score for fold 2: loss of 23.488882064819336\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 6s - loss: 15.6400 - mse: 15.6400 - mae: 1.5631 - val_loss: 10.6173 - val_mse: 10.6173 - val_mae: 1.4921 - lr: 2.4857e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 6s - loss: 15.5483 - mse: 15.5483 - mae: 1.5630 - val_loss: 11.0210 - val_mse: 11.0210 - val_mae: 1.4806 - lr: 2.4857e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 7s - loss: 15.4105 - mse: 15.4105 - mae: 1.5566 - val_loss: 10.9881 - val_mse: 10.9881 - val_mae: 1.5307 - lr: 2.4857e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 7s - loss: 15.5073 - mse: 15.5073 - mae: 1.5543 - val_loss: 11.1020 - val_mse: 11.1020 - val_mae: 1.4893 - lr: 2.4857e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 7s - loss: 15.4297 - mse: 15.4297 - mae: 1.5530 - val_loss: 10.7574 - val_mse: 10.7574 - val_mae: 1.5312 - lr: 2.4857e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 7s - loss: 15.2840 - mse: 15.2840 - mae: 1.5502 - val_loss: 10.8422 - val_mse: 10.8422 - val_mae: 1.5327 - lr: 2.4857e-04 - 7s/epoch - 7ms/step\n",
            "Score for fold 3: loss of 10.842187881469727\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 7s - loss: 15.0690 - mse: 15.0690 - mae: 1.5532 - val_loss: 12.1583 - val_mse: 12.1583 - val_mae: 1.5434 - lr: 2.4857e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 7s - loss: 14.9822 - mse: 14.9822 - mae: 1.5538 - val_loss: 12.3656 - val_mse: 12.3656 - val_mae: 1.5233 - lr: 2.4857e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 7s - loss: 14.9099 - mse: 14.9099 - mae: 1.5427 - val_loss: 12.5436 - val_mse: 12.5436 - val_mae: 1.5378 - lr: 2.4857e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 7s - loss: 14.8295 - mse: 14.8295 - mae: 1.5420 - val_loss: 12.1935 - val_mse: 12.1935 - val_mae: 1.5311 - lr: 2.4857e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 7s - loss: 14.8642 - mse: 14.8642 - mae: 1.5442 - val_loss: 11.6323 - val_mse: 11.6323 - val_mae: 1.5572 - lr: 2.4857e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 7s - loss: 14.7989 - mse: 14.7989 - mae: 1.5385 - val_loss: 12.3430 - val_mse: 12.3430 - val_mae: 1.4990 - lr: 2.4857e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 7s - loss: 14.7261 - mse: 14.7261 - mae: 1.5349 - val_loss: 12.1292 - val_mse: 12.1292 - val_mae: 1.5270 - lr: 2.4857e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 7s - loss: 14.6756 - mse: 14.6756 - mae: 1.5358 - val_loss: 11.9506 - val_mse: 11.9506 - val_mae: 1.5135 - lr: 2.4857e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 7s - loss: 14.7307 - mse: 14.7307 - mae: 1.5359 - val_loss: 12.4878 - val_mse: 12.4878 - val_mae: 1.5250 - lr: 2.4857e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 7s - loss: 14.6437 - mse: 14.6437 - mae: 1.5307 - val_loss: 12.5932 - val_mse: 12.5932 - val_mae: 1.5428 - lr: 2.4857e-04 - 7s/epoch - 7ms/step\n",
            "Score for fold 4: loss of 12.593222618103027\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 7s - loss: 14.0514 - mse: 14.0514 - mae: 1.5264 - val_loss: 15.1272 - val_mse: 15.1272 - val_mae: 1.5621 - lr: 2.4857e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 7s - loss: 13.8458 - mse: 13.8458 - mae: 1.5223 - val_loss: 15.0523 - val_mse: 15.0523 - val_mae: 1.5583 - lr: 2.4857e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 7s - loss: 13.8934 - mse: 13.8934 - mae: 1.5141 - val_loss: 15.1361 - val_mse: 15.1361 - val_mae: 1.5483 - lr: 2.4857e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 7s - loss: 13.7023 - mse: 13.7023 - mae: 1.5129 - val_loss: 15.1250 - val_mse: 15.1250 - val_mae: 1.6259 - lr: 2.4857e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 7s - loss: 13.7291 - mse: 13.7291 - mae: 1.5120 - val_loss: 15.3242 - val_mse: 15.3242 - val_mae: 1.5487 - lr: 2.4857e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 7s - loss: 13.5940 - mse: 13.5940 - mae: 1.5055 - val_loss: 15.2633 - val_mse: 15.2633 - val_mae: 1.6015 - lr: 2.4857e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 7s - loss: 13.5928 - mse: 13.5928 - mae: 1.5040 - val_loss: 15.3589 - val_mse: 15.3589 - val_mae: 1.6092 - lr: 2.4857e-04 - 7s/epoch - 7ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-10-14 13:48:25,193]\u001b[0m Finished trial#40 resulted in value: 14.756. Current best value is 14.266 with parameters: {'activation': 'relu', 'num_hidden_layer': 4, 'i': 10, 'learning_rate': 0.00010332846452133283}.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Score for fold 5: loss of 15.358938217163086\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 6s - loss: 15.6089 - mse: 15.6089 - mae: 1.6261 - val_loss: 15.2233 - val_mse: 15.2233 - val_mae: 1.5247 - lr: 1.2303e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 5s - loss: 15.0359 - mse: 15.0359 - mae: 1.5957 - val_loss: 15.1839 - val_mse: 15.1839 - val_mae: 1.7083 - lr: 1.2303e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 5s - loss: 15.0107 - mse: 15.0107 - mae: 1.5902 - val_loss: 15.0760 - val_mse: 15.0760 - val_mae: 1.5554 - lr: 1.2303e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 6s - loss: 14.9506 - mse: 14.9506 - mae: 1.5875 - val_loss: 15.1077 - val_mse: 15.1077 - val_mae: 1.6088 - lr: 1.2303e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 5s - loss: 14.9143 - mse: 14.9143 - mae: 1.5852 - val_loss: 15.2276 - val_mse: 15.2276 - val_mae: 1.5665 - lr: 1.2303e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 6s - loss: 14.8968 - mse: 14.8968 - mae: 1.5867 - val_loss: 15.1420 - val_mse: 15.1420 - val_mae: 1.5220 - lr: 1.2303e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 6s - loss: 14.8632 - mse: 14.8632 - mae: 1.5812 - val_loss: 15.0920 - val_mse: 15.0920 - val_mae: 1.5271 - lr: 1.2303e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 5s - loss: 14.8372 - mse: 14.8372 - mae: 1.5822 - val_loss: 15.0514 - val_mse: 15.0514 - val_mae: 1.5267 - lr: 1.2303e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 5s - loss: 14.7501 - mse: 14.7501 - mae: 1.5777 - val_loss: 15.0143 - val_mse: 15.0143 - val_mae: 1.5766 - lr: 1.2303e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 5s - loss: 14.7230 - mse: 14.7230 - mae: 1.5733 - val_loss: 15.0883 - val_mse: 15.0883 - val_mae: 1.5434 - lr: 1.2303e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 5s - loss: 14.6550 - mse: 14.6550 - mae: 1.5694 - val_loss: 15.1252 - val_mse: 15.1252 - val_mae: 1.5520 - lr: 1.2303e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 5s - loss: 14.6351 - mse: 14.6351 - mae: 1.5682 - val_loss: 14.9595 - val_mse: 14.9595 - val_mae: 1.5702 - lr: 1.2303e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 13/100\n",
            "1000/1000 - 5s - loss: 14.5384 - mse: 14.5384 - mae: 1.5626 - val_loss: 15.2000 - val_mse: 15.2000 - val_mae: 1.6158 - lr: 1.2303e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 14/100\n",
            "1000/1000 - 5s - loss: 14.5115 - mse: 14.5115 - mae: 1.5652 - val_loss: 15.1685 - val_mse: 15.1685 - val_mae: 1.5307 - lr: 1.2303e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 15/100\n",
            "1000/1000 - 5s - loss: 14.5405 - mse: 14.5405 - mae: 1.5671 - val_loss: 15.1547 - val_mse: 15.1547 - val_mae: 1.5491 - lr: 1.2303e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 16/100\n",
            "1000/1000 - 5s - loss: 14.4569 - mse: 14.4569 - mae: 1.5639 - val_loss: 15.1843 - val_mse: 15.1843 - val_mae: 1.5522 - lr: 1.2303e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 17/100\n",
            "1000/1000 - 5s - loss: 14.3430 - mse: 14.3430 - mae: 1.5626 - val_loss: 15.2770 - val_mse: 15.2770 - val_mae: 1.5573 - lr: 1.2303e-04 - 5s/epoch - 5ms/step\n",
            "Score for fold 1: loss of 15.27702522277832\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 5s - loss: 14.6748 - mse: 14.6748 - mae: 1.5570 - val_loss: 14.2261 - val_mse: 14.2261 - val_mae: 1.5715 - lr: 1.2303e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 5s - loss: 14.5143 - mse: 14.5143 - mae: 1.5506 - val_loss: 14.4369 - val_mse: 14.4369 - val_mae: 1.5347 - lr: 1.2303e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 5s - loss: 14.4978 - mse: 14.4978 - mae: 1.5521 - val_loss: 14.6515 - val_mse: 14.6515 - val_mae: 1.5864 - lr: 1.2303e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 6s - loss: 14.4394 - mse: 14.4394 - mae: 1.5462 - val_loss: 14.7063 - val_mse: 14.7063 - val_mae: 1.5717 - lr: 1.2303e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 5s - loss: 14.4214 - mse: 14.4214 - mae: 1.5448 - val_loss: 14.4586 - val_mse: 14.4586 - val_mae: 1.5539 - lr: 1.2303e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 6s - loss: 14.3622 - mse: 14.3622 - mae: 1.5440 - val_loss: 14.6561 - val_mse: 14.6561 - val_mae: 1.6588 - lr: 1.2303e-04 - 6s/epoch - 6ms/step\n",
            "Score for fold 2: loss of 14.656098365783691\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 5s - loss: 13.3341 - mse: 13.3341 - mae: 1.5528 - val_loss: 18.3503 - val_mse: 18.3503 - val_mae: 1.5335 - lr: 1.2303e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 5s - loss: 13.2547 - mse: 13.2547 - mae: 1.5504 - val_loss: 18.5852 - val_mse: 18.5852 - val_mae: 1.5910 - lr: 1.2303e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 5s - loss: 13.0734 - mse: 13.0734 - mae: 1.5484 - val_loss: 18.6083 - val_mse: 18.6083 - val_mae: 1.4928 - lr: 1.2303e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 5s - loss: 13.0506 - mse: 13.0506 - mae: 1.5415 - val_loss: 18.7023 - val_mse: 18.7023 - val_mae: 1.6333 - lr: 1.2303e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 5s - loss: 12.9946 - mse: 12.9946 - mae: 1.5385 - val_loss: 18.6608 - val_mse: 18.6608 - val_mae: 1.5942 - lr: 1.2303e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 5s - loss: 12.8856 - mse: 12.8856 - mae: 1.5396 - val_loss: 18.7155 - val_mse: 18.7155 - val_mae: 1.6664 - lr: 1.2303e-04 - 5s/epoch - 5ms/step\n",
            "Score for fold 3: loss of 18.715532302856445\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 5s - loss: 15.0663 - mse: 15.0663 - mae: 1.5441 - val_loss: 9.8376 - val_mse: 9.8376 - val_mae: 1.5472 - lr: 1.2303e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 6s - loss: 15.0910 - mse: 15.0910 - mae: 1.5424 - val_loss: 9.9263 - val_mse: 9.9263 - val_mae: 1.5000 - lr: 1.2303e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 6s - loss: 14.8776 - mse: 14.8776 - mae: 1.5308 - val_loss: 10.1626 - val_mse: 10.1626 - val_mae: 1.5477 - lr: 1.2303e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 6s - loss: 14.8298 - mse: 14.8298 - mae: 1.5333 - val_loss: 10.1268 - val_mse: 10.1268 - val_mae: 1.6650 - lr: 1.2303e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 6s - loss: 14.7618 - mse: 14.7618 - mae: 1.5303 - val_loss: 10.2367 - val_mse: 10.2367 - val_mae: 1.5634 - lr: 1.2303e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 6s - loss: 14.6788 - mse: 14.6788 - mae: 1.5235 - val_loss: 10.3287 - val_mse: 10.3287 - val_mae: 1.5491 - lr: 1.2303e-04 - 6s/epoch - 6ms/step\n",
            "Score for fold 4: loss of 10.328725814819336\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 5s - loss: 13.9715 - mse: 13.9715 - mae: 1.5469 - val_loss: 13.4277 - val_mse: 13.4277 - val_mae: 1.4072 - lr: 1.2303e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 5s - loss: 13.7116 - mse: 13.7116 - mae: 1.5391 - val_loss: 13.3332 - val_mse: 13.3332 - val_mae: 1.5757 - lr: 1.2303e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 5s - loss: 13.7762 - mse: 13.7762 - mae: 1.5394 - val_loss: 13.3528 - val_mse: 13.3528 - val_mae: 1.5986 - lr: 1.2303e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 5s - loss: 13.5934 - mse: 13.5934 - mae: 1.5339 - val_loss: 13.5627 - val_mse: 13.5627 - val_mae: 1.4602 - lr: 1.2303e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 6s - loss: 13.5247 - mse: 13.5247 - mae: 1.5292 - val_loss: 13.4304 - val_mse: 13.4304 - val_mae: 1.5661 - lr: 1.2303e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 6s - loss: 13.4172 - mse: 13.4172 - mae: 1.5162 - val_loss: 14.3597 - val_mse: 14.3597 - val_mae: 1.4508 - lr: 1.2303e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 5s - loss: 13.3212 - mse: 13.3212 - mae: 1.5178 - val_loss: 14.1116 - val_mse: 14.1116 - val_mae: 1.4839 - lr: 1.2303e-04 - 5s/epoch - 5ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-10-14 13:52:19,965]\u001b[0m Finished trial#41 resulted in value: 14.62. Current best value is 14.266 with parameters: {'activation': 'relu', 'num_hidden_layer': 4, 'i': 10, 'learning_rate': 0.00010332846452133283}.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Score for fold 5: loss of 14.11158275604248\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 12s - loss: 15.7076 - mse: 15.7076 - mae: 1.6231 - val_loss: 14.2362 - val_mse: 14.2362 - val_mae: 1.5725 - lr: 4.1056e-04 - 12s/epoch - 12ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 12s - loss: 15.4096 - mse: 15.4096 - mae: 1.6075 - val_loss: 13.8408 - val_mse: 13.8408 - val_mae: 1.6126 - lr: 4.1056e-04 - 12s/epoch - 12ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 12s - loss: 15.3470 - mse: 15.3470 - mae: 1.5910 - val_loss: 14.1685 - val_mse: 14.1685 - val_mae: 1.5908 - lr: 4.1056e-04 - 12s/epoch - 12ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 12s - loss: 15.3246 - mse: 15.3246 - mae: 1.5916 - val_loss: 14.4015 - val_mse: 14.4015 - val_mae: 1.5212 - lr: 4.1056e-04 - 12s/epoch - 12ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 11s - loss: 15.2975 - mse: 15.2975 - mae: 1.5904 - val_loss: 13.8847 - val_mse: 13.8847 - val_mae: 1.6076 - lr: 4.1056e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 12s - loss: 15.2336 - mse: 15.2336 - mae: 1.5837 - val_loss: 14.5285 - val_mse: 14.5285 - val_mae: 1.6257 - lr: 4.1056e-04 - 12s/epoch - 12ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 11s - loss: 15.1244 - mse: 15.1244 - mae: 1.5824 - val_loss: 13.5791 - val_mse: 13.5791 - val_mae: 1.5851 - lr: 4.1056e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 12s - loss: 15.1264 - mse: 15.1264 - mae: 1.5809 - val_loss: 13.8822 - val_mse: 13.8822 - val_mae: 1.4934 - lr: 4.1056e-04 - 12s/epoch - 12ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 11s - loss: 15.1041 - mse: 15.1041 - mae: 1.5822 - val_loss: 13.8403 - val_mse: 13.8403 - val_mae: 1.5064 - lr: 4.1056e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 12s - loss: 15.1368 - mse: 15.1368 - mae: 1.5835 - val_loss: 13.7757 - val_mse: 13.7757 - val_mae: 1.5705 - lr: 4.1056e-04 - 12s/epoch - 12ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 12s - loss: 15.0193 - mse: 15.0193 - mae: 1.5838 - val_loss: 14.0265 - val_mse: 14.0265 - val_mae: 1.5412 - lr: 4.1056e-04 - 12s/epoch - 12ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 12s - loss: 14.9484 - mse: 14.9484 - mae: 1.5833 - val_loss: 13.8426 - val_mse: 13.8426 - val_mae: 1.5658 - lr: 4.1056e-04 - 12s/epoch - 12ms/step\n",
            "Score for fold 1: loss of 13.842589378356934\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 11s - loss: 15.6262 - mse: 15.6262 - mae: 1.6024 - val_loss: 10.6018 - val_mse: 10.6018 - val_mae: 1.4439 - lr: 4.1056e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 12s - loss: 15.6212 - mse: 15.6212 - mae: 1.6023 - val_loss: 10.9876 - val_mse: 10.9876 - val_mae: 1.5214 - lr: 4.1056e-04 - 12s/epoch - 12ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 12s - loss: 15.7458 - mse: 15.7458 - mae: 1.5948 - val_loss: 11.2626 - val_mse: 11.2626 - val_mae: 1.6481 - lr: 4.1056e-04 - 12s/epoch - 12ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 12s - loss: 15.5854 - mse: 15.5854 - mae: 1.5972 - val_loss: 10.9141 - val_mse: 10.9141 - val_mae: 1.4726 - lr: 4.1056e-04 - 12s/epoch - 12ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 12s - loss: 15.5392 - mse: 15.5392 - mae: 1.5952 - val_loss: 11.2150 - val_mse: 11.2150 - val_mae: 1.4639 - lr: 4.1056e-04 - 12s/epoch - 12ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 11s - loss: 15.7369 - mse: 15.7369 - mae: 1.6056 - val_loss: 10.9245 - val_mse: 10.9245 - val_mae: 1.5532 - lr: 4.1056e-04 - 11s/epoch - 11ms/step\n",
            "Score for fold 2: loss of 10.92448902130127\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 11s - loss: 12.7531 - mse: 12.7531 - mae: 1.5797 - val_loss: 22.0881 - val_mse: 22.0881 - val_mae: 1.5174 - lr: 4.1056e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 11s - loss: 12.6981 - mse: 12.6981 - mae: 1.5851 - val_loss: 22.3599 - val_mse: 22.3599 - val_mae: 1.6402 - lr: 4.1056e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 12s - loss: 12.7327 - mse: 12.7327 - mae: 1.5839 - val_loss: 22.5165 - val_mse: 22.5165 - val_mae: 1.5636 - lr: 4.1056e-04 - 12s/epoch - 12ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 12s - loss: 12.5983 - mse: 12.5983 - mae: 1.5880 - val_loss: 22.3370 - val_mse: 22.3370 - val_mae: 1.5692 - lr: 4.1056e-04 - 12s/epoch - 12ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 11s - loss: 12.6482 - mse: 12.6482 - mae: 1.5889 - val_loss: 22.6241 - val_mse: 22.6241 - val_mae: 1.6977 - lr: 4.1056e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 11s - loss: 12.4778 - mse: 12.4778 - mae: 1.5906 - val_loss: 22.5444 - val_mse: 22.5444 - val_mae: 1.6505 - lr: 4.1056e-04 - 11s/epoch - 11ms/step\n",
            "Score for fold 3: loss of 22.544424057006836\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 11s - loss: 14.7490 - mse: 14.7490 - mae: 1.6030 - val_loss: 14.1434 - val_mse: 14.1434 - val_mae: 1.6028 - lr: 4.1056e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 11s - loss: 14.5759 - mse: 14.5759 - mae: 1.5978 - val_loss: 14.7477 - val_mse: 14.7477 - val_mae: 1.6469 - lr: 4.1056e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 12s - loss: 14.5167 - mse: 14.5167 - mae: 1.5826 - val_loss: 14.0533 - val_mse: 14.0533 - val_mae: 1.5123 - lr: 4.1056e-04 - 12s/epoch - 12ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 12s - loss: 14.4582 - mse: 14.4582 - mae: 1.5946 - val_loss: 14.2451 - val_mse: 14.2451 - val_mae: 1.5231 - lr: 4.1056e-04 - 12s/epoch - 12ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 12s - loss: 14.4896 - mse: 14.4896 - mae: 1.6041 - val_loss: 14.2673 - val_mse: 14.2673 - val_mae: 1.5840 - lr: 4.1056e-04 - 12s/epoch - 12ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 12s - loss: 14.4555 - mse: 14.4555 - mae: 1.5935 - val_loss: 14.5732 - val_mse: 14.5732 - val_mae: 1.6279 - lr: 4.1056e-04 - 12s/epoch - 12ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 13s - loss: 14.5502 - mse: 14.5502 - mae: 1.6170 - val_loss: 14.7796 - val_mse: 14.7796 - val_mae: 1.7390 - lr: 4.1056e-04 - 13s/epoch - 13ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 12s - loss: 14.4817 - mse: 14.4817 - mae: 1.6157 - val_loss: 14.6329 - val_mse: 14.6329 - val_mae: 1.5804 - lr: 4.1056e-04 - 12s/epoch - 12ms/step\n",
            "Score for fold 4: loss of 14.6329345703125\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 11s - loss: 15.2110 - mse: 15.2110 - mae: 1.6181 - val_loss: 11.8744 - val_mse: 11.8744 - val_mae: 1.4625 - lr: 4.1056e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 11s - loss: 15.0575 - mse: 15.0575 - mae: 1.6137 - val_loss: 12.3402 - val_mse: 12.3402 - val_mae: 2.0398 - lr: 4.1056e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 11s - loss: 14.9434 - mse: 14.9434 - mae: 1.6277 - val_loss: 11.8021 - val_mse: 11.8021 - val_mae: 1.5511 - lr: 4.1056e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 11s - loss: 15.1727 - mse: 15.1727 - mae: 1.6553 - val_loss: 12.0137 - val_mse: 12.0137 - val_mae: 1.6048 - lr: 4.1056e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 11s - loss: 15.2134 - mse: 15.2134 - mae: 1.6630 - val_loss: 11.9388 - val_mse: 11.9388 - val_mae: 1.5220 - lr: 4.1056e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 11s - loss: 15.1046 - mse: 15.1046 - mae: 1.6780 - val_loss: 13.2629 - val_mse: 13.2629 - val_mae: 1.7027 - lr: 4.1056e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 11s - loss: 15.1721 - mse: 15.1721 - mae: 1.7104 - val_loss: 11.9220 - val_mse: 11.9220 - val_mae: 1.5194 - lr: 4.1056e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 11s - loss: 15.0766 - mse: 15.0766 - mae: 1.7011 - val_loss: 11.8707 - val_mse: 11.8707 - val_mae: 1.6009 - lr: 4.1056e-04 - 11s/epoch - 11ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-10-14 14:00:11,887]\u001b[0m Finished trial#42 resulted in value: 14.76. Current best value is 14.266 with parameters: {'activation': 'relu', 'num_hidden_layer': 4, 'i': 10, 'learning_rate': 0.00010332846452133283}.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Score for fold 5: loss of 11.870680809020996\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 3s - loss: 16.4631 - mse: 16.4631 - mae: 1.6537 - val_loss: 15.0997 - val_mse: 15.0997 - val_mae: 1.6118 - lr: 1.6536e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 15.4162 - mse: 15.4162 - mae: 1.5905 - val_loss: 14.8257 - val_mse: 14.8257 - val_mae: 1.6028 - lr: 1.6536e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 15.2969 - mse: 15.2969 - mae: 1.5865 - val_loss: 14.6179 - val_mse: 14.6179 - val_mae: 1.6084 - lr: 1.6536e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 15.2748 - mse: 15.2748 - mae: 1.5815 - val_loss: 14.5994 - val_mse: 14.5994 - val_mae: 1.5868 - lr: 1.6536e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 15.2035 - mse: 15.2035 - mae: 1.5820 - val_loss: 14.5382 - val_mse: 14.5382 - val_mae: 1.6315 - lr: 1.6536e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 15.1593 - mse: 15.1593 - mae: 1.5805 - val_loss: 14.5664 - val_mse: 14.5664 - val_mae: 1.5975 - lr: 1.6536e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 15.1519 - mse: 15.1519 - mae: 1.5830 - val_loss: 14.3642 - val_mse: 14.3642 - val_mae: 1.6112 - lr: 1.6536e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 2s - loss: 15.1264 - mse: 15.1264 - mae: 1.5812 - val_loss: 14.4277 - val_mse: 14.4277 - val_mae: 1.6006 - lr: 1.6536e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 2s - loss: 15.1254 - mse: 15.1254 - mae: 1.5773 - val_loss: 14.4325 - val_mse: 14.4325 - val_mae: 1.6320 - lr: 1.6536e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 2s - loss: 15.1051 - mse: 15.1051 - mae: 1.5753 - val_loss: 14.5135 - val_mse: 14.5135 - val_mae: 1.5703 - lr: 1.6536e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 2s - loss: 15.0849 - mse: 15.0849 - mae: 1.5768 - val_loss: 14.4272 - val_mse: 14.4272 - val_mae: 1.6009 - lr: 1.6536e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 2s - loss: 15.0623 - mse: 15.0623 - mae: 1.5800 - val_loss: 14.5431 - val_mse: 14.5431 - val_mae: 1.5654 - lr: 1.6536e-04 - 2s/epoch - 2ms/step\n",
            "Score for fold 1: loss of 14.543120384216309\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 14.1462 - mse: 14.1462 - mae: 1.5788 - val_loss: 18.1231 - val_mse: 18.1231 - val_mae: 1.5807 - lr: 1.6536e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 14.1120 - mse: 14.1120 - mae: 1.5748 - val_loss: 18.0998 - val_mse: 18.0998 - val_mae: 1.6181 - lr: 1.6536e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 14.0937 - mse: 14.0937 - mae: 1.5780 - val_loss: 18.1965 - val_mse: 18.1965 - val_mae: 1.6090 - lr: 1.6536e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 14.0738 - mse: 14.0738 - mae: 1.5748 - val_loss: 18.1293 - val_mse: 18.1293 - val_mae: 1.5958 - lr: 1.6536e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 14.0556 - mse: 14.0556 - mae: 1.5748 - val_loss: 18.3398 - val_mse: 18.3398 - val_mae: 1.5528 - lr: 1.6536e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 14.0874 - mse: 14.0874 - mae: 1.5715 - val_loss: 18.1217 - val_mse: 18.1217 - val_mae: 1.5915 - lr: 1.6536e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 14.0453 - mse: 14.0453 - mae: 1.5720 - val_loss: 18.1422 - val_mse: 18.1422 - val_mae: 1.5990 - lr: 1.6536e-04 - 2s/epoch - 2ms/step\n",
            "Score for fold 2: loss of 18.14215087890625\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 14.0439 - mse: 14.0439 - mae: 1.5653 - val_loss: 18.0851 - val_mse: 18.0851 - val_mae: 1.5959 - lr: 1.6536e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 14.0336 - mse: 14.0336 - mae: 1.5665 - val_loss: 18.0471 - val_mse: 18.0471 - val_mae: 1.5989 - lr: 1.6536e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 14.0169 - mse: 14.0169 - mae: 1.5626 - val_loss: 18.0087 - val_mse: 18.0087 - val_mae: 1.6502 - lr: 1.6536e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 14.0049 - mse: 14.0049 - mae: 1.5622 - val_loss: 18.0768 - val_mse: 18.0768 - val_mae: 1.6334 - lr: 1.6536e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 14.0226 - mse: 14.0226 - mae: 1.5588 - val_loss: 18.0375 - val_mse: 18.0375 - val_mae: 1.6085 - lr: 1.6536e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 14.0001 - mse: 14.0001 - mae: 1.5600 - val_loss: 18.0897 - val_mse: 18.0897 - val_mae: 1.5820 - lr: 1.6536e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 13.9566 - mse: 13.9566 - mae: 1.5581 - val_loss: 18.0453 - val_mse: 18.0453 - val_mae: 1.6100 - lr: 1.6536e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 2s - loss: 13.9702 - mse: 13.9702 - mae: 1.5603 - val_loss: 18.1569 - val_mse: 18.1569 - val_mae: 1.5767 - lr: 1.6536e-04 - 2s/epoch - 2ms/step\n",
            "Score for fold 3: loss of 18.156911849975586\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 15.3229 - mse: 15.3229 - mae: 1.5749 - val_loss: 12.6786 - val_mse: 12.6786 - val_mae: 1.5262 - lr: 1.6536e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 15.2889 - mse: 15.2889 - mae: 1.5691 - val_loss: 12.7605 - val_mse: 12.7605 - val_mae: 1.5558 - lr: 1.6536e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 15.3142 - mse: 15.3142 - mae: 1.5743 - val_loss: 12.7418 - val_mse: 12.7418 - val_mae: 1.5345 - lr: 1.6536e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 15.2970 - mse: 15.2970 - mae: 1.5691 - val_loss: 12.7003 - val_mse: 12.7003 - val_mae: 1.5405 - lr: 1.6536e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 15.2794 - mse: 15.2794 - mae: 1.5720 - val_loss: 12.7042 - val_mse: 12.7042 - val_mae: 1.5530 - lr: 1.6536e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 15.2856 - mse: 15.2856 - mae: 1.5702 - val_loss: 12.6690 - val_mse: 12.6690 - val_mae: 1.6088 - lr: 1.6536e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 15.2667 - mse: 15.2667 - mae: 1.5727 - val_loss: 12.7497 - val_mse: 12.7497 - val_mae: 1.5752 - lr: 1.6536e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 2s - loss: 15.2197 - mse: 15.2197 - mae: 1.5727 - val_loss: 12.7870 - val_mse: 12.7870 - val_mae: 1.5640 - lr: 1.6536e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 2s - loss: 15.2097 - mse: 15.2097 - mae: 1.5704 - val_loss: 12.8061 - val_mse: 12.8061 - val_mae: 1.5509 - lr: 1.6536e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 2s - loss: 15.2031 - mse: 15.2031 - mae: 1.5713 - val_loss: 12.6662 - val_mse: 12.6662 - val_mae: 1.5741 - lr: 1.6536e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 2s - loss: 15.2378 - mse: 15.2378 - mae: 1.5698 - val_loss: 12.7295 - val_mse: 12.7295 - val_mae: 1.6027 - lr: 1.6536e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 2s - loss: 15.2338 - mse: 15.2338 - mae: 1.5706 - val_loss: 12.6905 - val_mse: 12.6905 - val_mae: 1.5516 - lr: 1.6536e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 13/100\n",
            "1000/1000 - 2s - loss: 15.1968 - mse: 15.1968 - mae: 1.5707 - val_loss: 12.8090 - val_mse: 12.8090 - val_mae: 1.5825 - lr: 1.6536e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 14/100\n",
            "1000/1000 - 2s - loss: 15.1723 - mse: 15.1723 - mae: 1.5699 - val_loss: 12.6236 - val_mse: 12.6236 - val_mae: 1.5951 - lr: 1.6536e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 15/100\n",
            "1000/1000 - 2s - loss: 15.1991 - mse: 15.1991 - mae: 1.5666 - val_loss: 12.8117 - val_mse: 12.8117 - val_mae: 1.5787 - lr: 1.6536e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 16/100\n",
            "1000/1000 - 2s - loss: 15.1682 - mse: 15.1682 - mae: 1.5694 - val_loss: 12.7340 - val_mse: 12.7340 - val_mae: 1.5535 - lr: 1.6536e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 17/100\n",
            "1000/1000 - 2s - loss: 15.1927 - mse: 15.1927 - mae: 1.5686 - val_loss: 12.8427 - val_mse: 12.8427 - val_mae: 1.5629 - lr: 1.6536e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 18/100\n",
            "1000/1000 - 2s - loss: 15.1970 - mse: 15.1970 - mae: 1.5663 - val_loss: 12.8166 - val_mse: 12.8166 - val_mae: 1.5553 - lr: 1.6536e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 19/100\n",
            "1000/1000 - 2s - loss: 15.1647 - mse: 15.1647 - mae: 1.5654 - val_loss: 12.7354 - val_mse: 12.7354 - val_mae: 1.6167 - lr: 1.6536e-04 - 2s/epoch - 2ms/step\n",
            "Score for fold 4: loss of 12.735424041748047\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 15.6760 - mse: 15.6760 - mae: 1.5847 - val_loss: 10.6557 - val_mse: 10.6557 - val_mae: 1.5907 - lr: 1.6536e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 15.5874 - mse: 15.5874 - mae: 1.5832 - val_loss: 10.8512 - val_mse: 10.8512 - val_mae: 1.5160 - lr: 1.6536e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 15.6247 - mse: 15.6247 - mae: 1.5827 - val_loss: 10.8578 - val_mse: 10.8578 - val_mae: 1.5167 - lr: 1.6536e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 15.6228 - mse: 15.6228 - mae: 1.5802 - val_loss: 10.8864 - val_mse: 10.8864 - val_mae: 1.5276 - lr: 1.6536e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 15.5832 - mse: 15.5832 - mae: 1.5801 - val_loss: 10.8191 - val_mse: 10.8191 - val_mae: 1.5367 - lr: 1.6536e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 15.5544 - mse: 15.5544 - mae: 1.5841 - val_loss: 11.0854 - val_mse: 11.0854 - val_mae: 1.5044 - lr: 1.6536e-04 - 2s/epoch - 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-10-14 14:02:02,305]\u001b[0m Finished trial#43 resulted in value: 14.934000000000001. Current best value is 14.266 with parameters: {'activation': 'relu', 'num_hidden_layer': 4, 'i': 10, 'learning_rate': 0.00010332846452133283}.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Score for fold 5: loss of 11.085397720336914\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 4s - loss: 17.3907 - mse: 17.3907 - mae: 1.7121 - val_loss: 11.1331 - val_mse: 11.1331 - val_mae: 1.6509 - lr: 1.0158e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 4s - loss: 16.5372 - mse: 16.5372 - mae: 1.6170 - val_loss: 11.0834 - val_mse: 11.0834 - val_mae: 1.6349 - lr: 1.0158e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 4s - loss: 16.4658 - mse: 16.4658 - mae: 1.6110 - val_loss: 11.0293 - val_mse: 11.0293 - val_mae: 1.5940 - lr: 1.0158e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 4s - loss: 16.3948 - mse: 16.3948 - mae: 1.6035 - val_loss: 11.0043 - val_mse: 11.0043 - val_mae: 1.5912 - lr: 1.0158e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 4s - loss: 16.3414 - mse: 16.3414 - mae: 1.6033 - val_loss: 10.9277 - val_mse: 10.9277 - val_mae: 1.5885 - lr: 1.0158e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 4s - loss: 16.2948 - mse: 16.2948 - mae: 1.5979 - val_loss: 11.0186 - val_mse: 11.0186 - val_mae: 1.5747 - lr: 1.0158e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 4s - loss: 16.2297 - mse: 16.2297 - mae: 1.5903 - val_loss: 11.0333 - val_mse: 11.0333 - val_mae: 1.5892 - lr: 1.0158e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 4s - loss: 16.1903 - mse: 16.1903 - mae: 1.5945 - val_loss: 10.9030 - val_mse: 10.9030 - val_mae: 1.5882 - lr: 1.0158e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 4s - loss: 16.1810 - mse: 16.1810 - mae: 1.5948 - val_loss: 10.8130 - val_mse: 10.8130 - val_mae: 1.5726 - lr: 1.0158e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 4s - loss: 16.1642 - mse: 16.1642 - mae: 1.5906 - val_loss: 10.8398 - val_mse: 10.8398 - val_mae: 1.5744 - lr: 1.0158e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 4s - loss: 16.1504 - mse: 16.1504 - mae: 1.5879 - val_loss: 10.8831 - val_mse: 10.8831 - val_mae: 1.5611 - lr: 1.0158e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 4s - loss: 16.1458 - mse: 16.1458 - mae: 1.5865 - val_loss: 10.8729 - val_mse: 10.8729 - val_mae: 1.5479 - lr: 1.0158e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 13/100\n",
            "1000/1000 - 4s - loss: 16.1418 - mse: 16.1418 - mae: 1.5869 - val_loss: 10.8221 - val_mse: 10.8221 - val_mae: 1.5692 - lr: 1.0158e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 14/100\n",
            "1000/1000 - 4s - loss: 16.1197 - mse: 16.1197 - mae: 1.5873 - val_loss: 10.7728 - val_mse: 10.7728 - val_mae: 1.5661 - lr: 1.0158e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 15/100\n",
            "1000/1000 - 4s - loss: 16.0899 - mse: 16.0899 - mae: 1.5828 - val_loss: 10.8305 - val_mse: 10.8305 - val_mae: 1.5809 - lr: 1.0158e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 16/100\n",
            "1000/1000 - 4s - loss: 16.0812 - mse: 16.0812 - mae: 1.5869 - val_loss: 10.9137 - val_mse: 10.9137 - val_mae: 1.5341 - lr: 1.0158e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 17/100\n",
            "1000/1000 - 4s - loss: 16.0973 - mse: 16.0973 - mae: 1.5849 - val_loss: 10.7827 - val_mse: 10.7827 - val_mae: 1.5682 - lr: 1.0158e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 18/100\n",
            "1000/1000 - 4s - loss: 16.0736 - mse: 16.0736 - mae: 1.5831 - val_loss: 10.8325 - val_mse: 10.8325 - val_mae: 1.5697 - lr: 1.0158e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 19/100\n",
            "1000/1000 - 4s - loss: 16.0478 - mse: 16.0478 - mae: 1.5837 - val_loss: 10.7896 - val_mse: 10.7896 - val_mae: 1.6134 - lr: 1.0158e-04 - 4s/epoch - 4ms/step\n",
            "Score for fold 1: loss of 10.789606094360352\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 4s - loss: 14.1626 - mse: 14.1626 - mae: 1.5822 - val_loss: 18.3379 - val_mse: 18.3379 - val_mae: 1.5559 - lr: 1.0158e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 4s - loss: 14.1480 - mse: 14.1480 - mae: 1.5787 - val_loss: 18.4318 - val_mse: 18.4318 - val_mae: 1.6190 - lr: 1.0158e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 4s - loss: 14.1152 - mse: 14.1152 - mae: 1.5800 - val_loss: 18.6187 - val_mse: 18.6187 - val_mae: 1.6245 - lr: 1.0158e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 4s - loss: 14.1309 - mse: 14.1309 - mae: 1.5766 - val_loss: 18.4685 - val_mse: 18.4685 - val_mae: 1.5537 - lr: 1.0158e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 4s - loss: 14.1132 - mse: 14.1132 - mae: 1.5790 - val_loss: 18.3466 - val_mse: 18.3466 - val_mae: 1.6361 - lr: 1.0158e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 4s - loss: 14.1221 - mse: 14.1221 - mae: 1.5815 - val_loss: 18.3264 - val_mse: 18.3264 - val_mae: 1.6147 - lr: 1.0158e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 4s - loss: 14.0898 - mse: 14.0898 - mae: 1.5771 - val_loss: 18.2884 - val_mse: 18.2884 - val_mae: 1.6041 - lr: 1.0158e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 4s - loss: 14.0858 - mse: 14.0858 - mae: 1.5774 - val_loss: 18.3033 - val_mse: 18.3033 - val_mae: 1.6008 - lr: 1.0158e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 4s - loss: 14.1147 - mse: 14.1147 - mae: 1.5758 - val_loss: 18.2907 - val_mse: 18.2907 - val_mae: 1.6211 - lr: 1.0158e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 4s - loss: 14.0668 - mse: 14.0668 - mae: 1.5737 - val_loss: 18.6430 - val_mse: 18.6430 - val_mae: 1.5613 - lr: 1.0158e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 4s - loss: 14.0761 - mse: 14.0761 - mae: 1.5781 - val_loss: 18.3207 - val_mse: 18.3207 - val_mae: 1.5906 - lr: 1.0158e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 4s - loss: 14.0613 - mse: 14.0613 - mae: 1.5795 - val_loss: 18.3702 - val_mse: 18.3702 - val_mae: 1.5589 - lr: 1.0158e-04 - 4s/epoch - 4ms/step\n",
            "Score for fold 2: loss of 18.37021827697754\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 4s - loss: 16.2953 - mse: 16.2953 - mae: 1.5970 - val_loss: 9.5292 - val_mse: 9.5292 - val_mae: 1.5486 - lr: 1.0158e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 4s - loss: 16.2713 - mse: 16.2713 - mae: 1.5941 - val_loss: 9.5097 - val_mse: 9.5097 - val_mae: 1.4834 - lr: 1.0158e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 4s - loss: 16.2530 - mse: 16.2530 - mae: 1.5948 - val_loss: 9.5588 - val_mse: 9.5588 - val_mae: 1.4984 - lr: 1.0158e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 4s - loss: 16.2419 - mse: 16.2419 - mae: 1.5965 - val_loss: 9.5088 - val_mse: 9.5088 - val_mae: 1.5121 - lr: 1.0158e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 4s - loss: 16.2559 - mse: 16.2559 - mae: 1.5911 - val_loss: 9.4995 - val_mse: 9.4995 - val_mae: 1.5121 - lr: 1.0158e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 4s - loss: 16.2223 - mse: 16.2223 - mae: 1.5954 - val_loss: 9.4902 - val_mse: 9.4902 - val_mae: 1.4997 - lr: 1.0158e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 4s - loss: 16.2286 - mse: 16.2286 - mae: 1.5895 - val_loss: 9.5492 - val_mse: 9.5492 - val_mae: 1.5012 - lr: 1.0158e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 4s - loss: 16.2255 - mse: 16.2255 - mae: 1.5879 - val_loss: 9.5207 - val_mse: 9.5207 - val_mae: 1.5097 - lr: 1.0158e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 4s - loss: 16.2029 - mse: 16.2029 - mae: 1.5961 - val_loss: 9.5412 - val_mse: 9.5412 - val_mae: 1.5186 - lr: 1.0158e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 4s - loss: 16.1966 - mse: 16.1966 - mae: 1.5915 - val_loss: 9.5158 - val_mse: 9.5158 - val_mae: 1.5909 - lr: 1.0158e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 4s - loss: 16.1913 - mse: 16.1913 - mae: 1.5933 - val_loss: 9.4942 - val_mse: 9.4942 - val_mae: 1.5358 - lr: 1.0158e-04 - 4s/epoch - 4ms/step\n",
            "Score for fold 3: loss of 9.494211196899414\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 4s - loss: 14.9343 - mse: 14.9343 - mae: 1.5747 - val_loss: 14.4477 - val_mse: 14.4477 - val_mae: 1.5746 - lr: 1.0158e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 4s - loss: 14.9185 - mse: 14.9185 - mae: 1.5703 - val_loss: 14.4978 - val_mse: 14.4978 - val_mae: 1.5693 - lr: 1.0158e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 4s - loss: 14.8859 - mse: 14.8859 - mae: 1.5703 - val_loss: 14.4351 - val_mse: 14.4351 - val_mae: 1.5899 - lr: 1.0158e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 4s - loss: 14.8964 - mse: 14.8964 - mae: 1.5723 - val_loss: 14.4942 - val_mse: 14.4942 - val_mae: 1.5851 - lr: 1.0158e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 4s - loss: 14.8712 - mse: 14.8712 - mae: 1.5730 - val_loss: 14.5095 - val_mse: 14.5095 - val_mae: 1.5349 - lr: 1.0158e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 4s - loss: 14.8691 - mse: 14.8691 - mae: 1.5681 - val_loss: 14.5001 - val_mse: 14.5001 - val_mae: 1.5481 - lr: 1.0158e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 4s - loss: 14.8570 - mse: 14.8570 - mae: 1.5640 - val_loss: 14.4714 - val_mse: 14.4714 - val_mae: 1.5825 - lr: 1.0158e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 4s - loss: 14.8384 - mse: 14.8384 - mae: 1.5665 - val_loss: 14.5025 - val_mse: 14.5025 - val_mae: 1.5397 - lr: 1.0158e-04 - 4s/epoch - 4ms/step\n",
            "Score for fold 4: loss of 14.502538681030273\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 4s - loss: 13.1225 - mse: 13.1225 - mae: 1.5498 - val_loss: 21.4871 - val_mse: 21.4871 - val_mae: 1.6749 - lr: 1.0158e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 4s - loss: 13.1323 - mse: 13.1323 - mae: 1.5488 - val_loss: 21.3857 - val_mse: 21.3857 - val_mae: 1.6649 - lr: 1.0158e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 4s - loss: 13.1402 - mse: 13.1402 - mae: 1.5514 - val_loss: 21.4227 - val_mse: 21.4227 - val_mae: 1.6311 - lr: 1.0158e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 4s - loss: 13.1265 - mse: 13.1265 - mae: 1.5486 - val_loss: 21.3699 - val_mse: 21.3699 - val_mae: 1.6548 - lr: 1.0158e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 4s - loss: 13.1101 - mse: 13.1101 - mae: 1.5492 - val_loss: 21.2926 - val_mse: 21.2926 - val_mae: 1.6762 - lr: 1.0158e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 4s - loss: 13.0907 - mse: 13.0907 - mae: 1.5446 - val_loss: 21.2988 - val_mse: 21.2988 - val_mae: 1.6955 - lr: 1.0158e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 4s - loss: 13.0861 - mse: 13.0861 - mae: 1.5486 - val_loss: 21.3453 - val_mse: 21.3453 - val_mae: 1.6499 - lr: 1.0158e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 4s - loss: 13.0637 - mse: 13.0637 - mae: 1.5445 - val_loss: 21.5227 - val_mse: 21.5227 - val_mae: 1.6338 - lr: 1.0158e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 4s - loss: 13.0647 - mse: 13.0647 - mae: 1.5486 - val_loss: 21.3293 - val_mse: 21.3293 - val_mae: 1.6520 - lr: 1.0158e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 4s - loss: 13.0361 - mse: 13.0361 - mae: 1.5443 - val_loss: 21.4126 - val_mse: 21.4126 - val_mae: 1.6537 - lr: 1.0158e-04 - 4s/epoch - 4ms/step\n",
            "Score for fold 5: loss of 21.412633895874023\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-10-14 14:05:56,314]\u001b[0m Finished trial#44 resulted in value: 14.912. Current best value is 14.266 with parameters: {'activation': 'relu', 'num_hidden_layer': 4, 'i': 10, 'learning_rate': 0.00010332846452133283}.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 6s - loss: 16.2842 - mse: 16.2842 - mae: 1.6228 - val_loss: 12.3326 - val_mse: 12.3326 - val_mae: 1.5102 - lr: 1.3202e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 5s - loss: 15.8419 - mse: 15.8419 - mae: 1.5936 - val_loss: 12.3008 - val_mse: 12.3008 - val_mae: 1.5414 - lr: 1.3202e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 5s - loss: 15.8023 - mse: 15.8023 - mae: 1.5883 - val_loss: 12.3428 - val_mse: 12.3428 - val_mae: 1.5571 - lr: 1.3202e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 5s - loss: 15.6813 - mse: 15.6813 - mae: 1.5882 - val_loss: 12.4190 - val_mse: 12.4190 - val_mae: 1.5236 - lr: 1.3202e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 5s - loss: 15.6049 - mse: 15.6049 - mae: 1.5895 - val_loss: 12.3470 - val_mse: 12.3470 - val_mae: 1.6788 - lr: 1.3202e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 5s - loss: 15.5418 - mse: 15.5418 - mae: 1.5935 - val_loss: 12.1783 - val_mse: 12.1783 - val_mae: 1.5423 - lr: 1.3202e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 5s - loss: 15.5305 - mse: 15.5305 - mae: 1.5835 - val_loss: 12.2051 - val_mse: 12.2051 - val_mae: 1.5587 - lr: 1.3202e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 5s - loss: 15.4998 - mse: 15.4998 - mae: 1.5849 - val_loss: 12.2093 - val_mse: 12.2093 - val_mae: 1.5536 - lr: 1.3202e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 5s - loss: 15.5066 - mse: 15.5066 - mae: 1.5771 - val_loss: 12.2951 - val_mse: 12.2951 - val_mae: 1.5364 - lr: 1.3202e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 5s - loss: 15.4306 - mse: 15.4306 - mae: 1.5771 - val_loss: 12.2833 - val_mse: 12.2833 - val_mae: 1.4965 - lr: 1.3202e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 5s - loss: 15.4095 - mse: 15.4095 - mae: 1.5754 - val_loss: 12.1969 - val_mse: 12.1969 - val_mae: 1.5507 - lr: 1.3202e-04 - 5s/epoch - 5ms/step\n",
            "Score for fold 1: loss of 12.196873664855957\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 5s - loss: 12.5604 - mse: 12.5604 - mae: 1.5640 - val_loss: 23.4361 - val_mse: 23.4361 - val_mae: 1.5830 - lr: 1.3202e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 5s - loss: 12.4798 - mse: 12.4798 - mae: 1.5598 - val_loss: 23.5118 - val_mse: 23.5118 - val_mae: 1.5724 - lr: 1.3202e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 5s - loss: 12.4076 - mse: 12.4076 - mae: 1.5555 - val_loss: 23.5366 - val_mse: 23.5366 - val_mae: 1.5811 - lr: 1.3202e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 5s - loss: 12.4408 - mse: 12.4408 - mae: 1.5569 - val_loss: 23.5036 - val_mse: 23.5036 - val_mae: 1.5852 - lr: 1.3202e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 5s - loss: 12.3319 - mse: 12.3319 - mae: 1.5563 - val_loss: 23.4264 - val_mse: 23.4264 - val_mae: 1.5987 - lr: 1.3202e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 5s - loss: 12.2308 - mse: 12.2308 - mae: 1.5537 - val_loss: 23.7194 - val_mse: 23.7194 - val_mae: 1.6260 - lr: 1.3202e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 5s - loss: 12.2844 - mse: 12.2844 - mae: 1.5532 - val_loss: 23.6737 - val_mse: 23.6737 - val_mae: 1.6703 - lr: 1.3202e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 5s - loss: 12.2071 - mse: 12.2071 - mae: 1.5468 - val_loss: 23.5694 - val_mse: 23.5694 - val_mae: 1.6710 - lr: 1.3202e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 5s - loss: 12.0983 - mse: 12.0983 - mae: 1.5492 - val_loss: 23.6691 - val_mse: 23.6691 - val_mae: 1.6223 - lr: 1.3202e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 5s - loss: 12.1144 - mse: 12.1144 - mae: 1.5417 - val_loss: 23.5414 - val_mse: 23.5414 - val_mae: 1.5973 - lr: 1.3202e-04 - 5s/epoch - 5ms/step\n",
            "Score for fold 2: loss of 23.541399002075195\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 5s - loss: 15.3714 - mse: 15.3714 - mae: 1.5607 - val_loss: 10.2577 - val_mse: 10.2577 - val_mae: 1.4723 - lr: 1.3202e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 5s - loss: 15.2855 - mse: 15.2855 - mae: 1.5598 - val_loss: 10.2567 - val_mse: 10.2567 - val_mae: 1.5735 - lr: 1.3202e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 5s - loss: 15.2872 - mse: 15.2872 - mae: 1.5566 - val_loss: 10.4179 - val_mse: 10.4179 - val_mae: 1.5782 - lr: 1.3202e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 5s - loss: 15.2647 - mse: 15.2647 - mae: 1.5527 - val_loss: 10.3697 - val_mse: 10.3697 - val_mae: 1.6128 - lr: 1.3202e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 5s - loss: 15.1287 - mse: 15.1287 - mae: 1.5498 - val_loss: 10.6294 - val_mse: 10.6294 - val_mae: 1.5143 - lr: 1.3202e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 5s - loss: 15.1531 - mse: 15.1531 - mae: 1.5500 - val_loss: 10.5879 - val_mse: 10.5879 - val_mae: 1.5241 - lr: 1.3202e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 5s - loss: 14.9388 - mse: 14.9388 - mae: 1.5407 - val_loss: 10.5854 - val_mse: 10.5854 - val_mae: 1.5150 - lr: 1.3202e-04 - 5s/epoch - 5ms/step\n",
            "Score for fold 3: loss of 10.585363388061523\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 5s - loss: 13.9455 - mse: 13.9455 - mae: 1.5440 - val_loss: 15.1362 - val_mse: 15.1362 - val_mae: 1.6156 - lr: 1.3202e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 5s - loss: 13.8131 - mse: 13.8131 - mae: 1.5309 - val_loss: 14.9601 - val_mse: 14.9601 - val_mae: 1.5010 - lr: 1.3202e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 5s - loss: 13.7555 - mse: 13.7555 - mae: 1.5328 - val_loss: 15.2893 - val_mse: 15.2893 - val_mae: 1.5304 - lr: 1.3202e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 5s - loss: 13.6605 - mse: 13.6605 - mae: 1.5318 - val_loss: 15.1207 - val_mse: 15.1207 - val_mae: 1.6672 - lr: 1.3202e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 5s - loss: 13.5645 - mse: 13.5645 - mae: 1.5223 - val_loss: 15.7454 - val_mse: 15.7454 - val_mae: 1.5730 - lr: 1.3202e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 5s - loss: 13.3758 - mse: 13.3758 - mae: 1.5181 - val_loss: 15.5708 - val_mse: 15.5708 - val_mae: 1.7045 - lr: 1.3202e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 5s - loss: 13.3332 - mse: 13.3332 - mae: 1.5172 - val_loss: 15.4169 - val_mse: 15.4169 - val_mae: 1.5886 - lr: 1.3202e-04 - 5s/epoch - 5ms/step\n",
            "Score for fold 4: loss of 15.416885375976562\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 5s - loss: 14.5377 - mse: 14.5377 - mae: 1.5359 - val_loss: 10.5160 - val_mse: 10.5160 - val_mae: 1.4600 - lr: 1.3202e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 5s - loss: 14.4866 - mse: 14.4866 - mae: 1.5317 - val_loss: 10.5831 - val_mse: 10.5831 - val_mae: 1.5825 - lr: 1.3202e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 5s - loss: 14.3940 - mse: 14.3940 - mae: 1.5220 - val_loss: 10.7904 - val_mse: 10.7904 - val_mae: 1.7044 - lr: 1.3202e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 5s - loss: 14.3116 - mse: 14.3116 - mae: 1.5271 - val_loss: 10.4969 - val_mse: 10.4969 - val_mae: 1.5342 - lr: 1.3202e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 5s - loss: 14.2297 - mse: 14.2297 - mae: 1.5228 - val_loss: 10.8593 - val_mse: 10.8593 - val_mae: 1.6461 - lr: 1.3202e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 5s - loss: 14.0548 - mse: 14.0548 - mae: 1.5239 - val_loss: 10.7743 - val_mse: 10.7743 - val_mae: 1.6321 - lr: 1.3202e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 5s - loss: 13.9975 - mse: 13.9975 - mae: 1.5224 - val_loss: 11.1099 - val_mse: 11.1099 - val_mae: 1.4826 - lr: 1.3202e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 5s - loss: 13.9657 - mse: 13.9657 - mae: 1.5169 - val_loss: 10.7593 - val_mse: 10.7593 - val_mae: 1.5091 - lr: 1.3202e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 5s - loss: 13.8027 - mse: 13.8027 - mae: 1.5019 - val_loss: 11.0852 - val_mse: 11.0852 - val_mae: 1.6007 - lr: 1.3202e-04 - 5s/epoch - 5ms/step\n",
            "Score for fold 5: loss of 11.085190773010254\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-10-14 14:09:54,526]\u001b[0m Finished trial#45 resulted in value: 14.568000000000001. Current best value is 14.266 with parameters: {'activation': 'relu', 'num_hidden_layer': 4, 'i': 10, 'learning_rate': 0.00010332846452133283}.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 10s - loss: 17.1301 - mse: 17.1301 - mae: 1.7119 - val_loss: 12.2808 - val_mse: 12.2808 - val_mae: 1.5403 - lr: 7.0315e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 10s - loss: 16.7372 - mse: 16.7372 - mae: 1.6562 - val_loss: 12.1968 - val_mse: 12.1968 - val_mae: 1.7418 - lr: 7.0315e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 10s - loss: 16.6537 - mse: 16.6537 - mae: 1.6552 - val_loss: 12.3167 - val_mse: 12.3167 - val_mae: 1.7903 - lr: 7.0315e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 10s - loss: 16.6378 - mse: 16.6378 - mae: 1.6602 - val_loss: 12.5830 - val_mse: 12.5830 - val_mae: 1.4964 - lr: 7.0315e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 10s - loss: 16.5960 - mse: 16.5960 - mae: 1.6470 - val_loss: 12.2730 - val_mse: 12.2730 - val_mae: 1.5372 - lr: 7.0315e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 10s - loss: 16.6710 - mse: 16.6710 - mae: 1.6556 - val_loss: 12.1840 - val_mse: 12.1840 - val_mae: 1.7034 - lr: 7.0315e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 10s - loss: 16.5868 - mse: 16.5868 - mae: 1.6516 - val_loss: 12.1706 - val_mse: 12.1706 - val_mae: 1.7051 - lr: 7.0315e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 10s - loss: 16.6062 - mse: 16.6062 - mae: 1.6589 - val_loss: 12.3140 - val_mse: 12.3140 - val_mae: 1.5297 - lr: 7.0315e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 10s - loss: 16.6272 - mse: 16.6272 - mae: 1.6673 - val_loss: 12.0980 - val_mse: 12.0980 - val_mae: 1.6150 - lr: 7.0315e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 10s - loss: 16.6643 - mse: 16.6643 - mae: 1.6555 - val_loss: 12.1710 - val_mse: 12.1710 - val_mae: 1.7443 - lr: 7.0315e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 10s - loss: 16.6405 - mse: 16.6405 - mae: 1.6607 - val_loss: 12.3911 - val_mse: 12.3911 - val_mae: 1.5267 - lr: 7.0315e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 10s - loss: 16.6820 - mse: 16.6820 - mae: 1.6712 - val_loss: 12.1486 - val_mse: 12.1486 - val_mae: 1.5775 - lr: 7.0315e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 13/100\n",
            "1000/1000 - 10s - loss: 16.6915 - mse: 16.6915 - mae: 1.6599 - val_loss: 12.4388 - val_mse: 12.4388 - val_mae: 1.8544 - lr: 7.0315e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 14/100\n",
            "1000/1000 - 10s - loss: 16.7021 - mse: 16.7021 - mae: 1.6661 - val_loss: 12.1802 - val_mse: 12.1802 - val_mae: 1.7124 - lr: 7.0315e-04 - 10s/epoch - 10ms/step\n",
            "Score for fold 1: loss of 12.180221557617188\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 10s - loss: 16.4842 - mse: 16.4842 - mae: 1.6588 - val_loss: 12.7106 - val_mse: 12.7106 - val_mae: 1.6721 - lr: 7.0315e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 10s - loss: 16.5177 - mse: 16.5177 - mae: 1.6547 - val_loss: 12.7361 - val_mse: 12.7361 - val_mae: 1.7391 - lr: 7.0315e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 10s - loss: 16.5477 - mse: 16.5477 - mae: 1.6595 - val_loss: 12.7710 - val_mse: 12.7710 - val_mae: 1.6093 - lr: 7.0315e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 10s - loss: 16.5875 - mse: 16.5875 - mae: 1.6707 - val_loss: 13.3776 - val_mse: 13.3776 - val_mae: 1.4952 - lr: 7.0315e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 10s - loss: 16.4979 - mse: 16.4979 - mae: 1.6627 - val_loss: 12.7608 - val_mse: 12.7608 - val_mae: 1.6142 - lr: 7.0315e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 10s - loss: 16.5379 - mse: 16.5379 - mae: 1.6591 - val_loss: 13.3328 - val_mse: 13.3328 - val_mae: 1.5674 - lr: 7.0315e-04 - 10s/epoch - 10ms/step\n",
            "Score for fold 2: loss of 13.33276653289795\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 10s - loss: 14.5315 - mse: 14.5315 - mae: 1.6578 - val_loss: 21.4276 - val_mse: 21.4276 - val_mae: 1.5338 - lr: 7.0315e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 10s - loss: 14.5838 - mse: 14.5838 - mae: 1.6522 - val_loss: 21.1742 - val_mse: 21.1742 - val_mae: 1.5456 - lr: 7.0315e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 10s - loss: 14.4997 - mse: 14.4997 - mae: 1.6555 - val_loss: 20.7634 - val_mse: 20.7634 - val_mae: 1.8272 - lr: 7.0315e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 10s - loss: 14.5891 - mse: 14.5891 - mae: 1.6597 - val_loss: 20.8169 - val_mse: 20.8169 - val_mae: 1.7922 - lr: 7.0315e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 10s - loss: 14.4875 - mse: 14.4875 - mae: 1.6478 - val_loss: 20.7774 - val_mse: 20.7774 - val_mae: 1.6393 - lr: 7.0315e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 10s - loss: 14.5085 - mse: 14.5085 - mae: 1.6431 - val_loss: 20.6665 - val_mse: 20.6665 - val_mae: 1.6780 - lr: 7.0315e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 10s - loss: 14.5499 - mse: 14.5499 - mae: 1.6554 - val_loss: 22.2061 - val_mse: 22.2061 - val_mae: 1.5086 - lr: 7.0315e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 10s - loss: 14.5298 - mse: 14.5298 - mae: 1.6577 - val_loss: 20.6632 - val_mse: 20.6632 - val_mae: 1.6769 - lr: 7.0315e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 10s - loss: 14.5487 - mse: 14.5487 - mae: 1.6504 - val_loss: 20.7734 - val_mse: 20.7734 - val_mae: 1.8240 - lr: 7.0315e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 10s - loss: 14.5507 - mse: 14.5507 - mae: 1.6596 - val_loss: 20.6877 - val_mse: 20.6877 - val_mae: 1.6886 - lr: 7.0315e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 10s - loss: 14.5161 - mse: 14.5161 - mae: 1.6435 - val_loss: 21.2535 - val_mse: 21.2535 - val_mae: 1.5761 - lr: 7.0315e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 10s - loss: 14.5705 - mse: 14.5705 - mae: 1.6503 - val_loss: 20.6830 - val_mse: 20.6830 - val_mae: 1.7242 - lr: 7.0315e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 13/100\n",
            "1000/1000 - 10s - loss: 14.5767 - mse: 14.5767 - mae: 1.6555 - val_loss: 20.6899 - val_mse: 20.6899 - val_mae: 1.6797 - lr: 7.0315e-04 - 10s/epoch - 10ms/step\n",
            "Score for fold 3: loss of 20.68992042541504\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 10s - loss: 15.6370 - mse: 15.6370 - mae: 1.6558 - val_loss: 16.5647 - val_mse: 16.5647 - val_mae: 1.6062 - lr: 7.0315e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 10s - loss: 15.6023 - mse: 15.6023 - mae: 1.6541 - val_loss: 16.7588 - val_mse: 16.7588 - val_mae: 1.5427 - lr: 7.0315e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 10s - loss: 15.6530 - mse: 15.6530 - mae: 1.6545 - val_loss: 16.6602 - val_mse: 16.6602 - val_mae: 1.5781 - lr: 7.0315e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 10s - loss: 15.6903 - mse: 15.6903 - mae: 1.6593 - val_loss: 17.3550 - val_mse: 17.3550 - val_mae: 1.5270 - lr: 7.0315e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 10s - loss: 15.6509 - mse: 15.6509 - mae: 1.6543 - val_loss: 16.4700 - val_mse: 16.4700 - val_mae: 1.6362 - lr: 7.0315e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 10s - loss: 15.6260 - mse: 15.6260 - mae: 1.6555 - val_loss: 16.4662 - val_mse: 16.4662 - val_mae: 1.7883 - lr: 7.0315e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 10s - loss: 15.6266 - mse: 15.6266 - mae: 1.6510 - val_loss: 17.7667 - val_mse: 17.7667 - val_mae: 1.5348 - lr: 7.0315e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 10s - loss: 15.6476 - mse: 15.6476 - mae: 1.6601 - val_loss: 16.4162 - val_mse: 16.4162 - val_mae: 1.6095 - lr: 7.0315e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 10s - loss: 15.5963 - mse: 15.5963 - mae: 1.6530 - val_loss: 16.3365 - val_mse: 16.3365 - val_mae: 1.6714 - lr: 7.0315e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 10s - loss: 15.5983 - mse: 15.5983 - mae: 1.6541 - val_loss: 16.4032 - val_mse: 16.4032 - val_mae: 1.5843 - lr: 7.0315e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 10s - loss: 15.6978 - mse: 15.6978 - mae: 1.6528 - val_loss: 16.8299 - val_mse: 16.8299 - val_mae: 1.5865 - lr: 7.0315e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 10s - loss: 15.6257 - mse: 15.6257 - mae: 1.6558 - val_loss: 16.4671 - val_mse: 16.4671 - val_mae: 1.6529 - lr: 7.0315e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 13/100\n",
            "1000/1000 - 10s - loss: 15.6416 - mse: 15.6416 - mae: 1.6552 - val_loss: 17.8168 - val_mse: 17.8168 - val_mae: 1.5288 - lr: 7.0315e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 14/100\n",
            "1000/1000 - 10s - loss: 15.5963 - mse: 15.5963 - mae: 1.6470 - val_loss: 16.8986 - val_mse: 16.8986 - val_mae: 1.5460 - lr: 7.0315e-04 - 10s/epoch - 10ms/step\n",
            "Score for fold 4: loss of 16.898576736450195\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 10s - loss: 15.7376 - mse: 15.7376 - mae: 1.6416 - val_loss: 16.2478 - val_mse: 16.2478 - val_mae: 1.7562 - lr: 7.0315e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 10s - loss: 15.6817 - mse: 15.6817 - mae: 1.6406 - val_loss: 16.2467 - val_mse: 16.2467 - val_mae: 1.6433 - lr: 7.0315e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 10s - loss: 15.7329 - mse: 15.7329 - mae: 1.6420 - val_loss: 16.2352 - val_mse: 16.2352 - val_mae: 1.6172 - lr: 7.0315e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 10s - loss: 15.8058 - mse: 15.8058 - mae: 1.6449 - val_loss: 16.1733 - val_mse: 16.1733 - val_mae: 1.6181 - lr: 7.0315e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 10s - loss: 15.6658 - mse: 15.6658 - mae: 1.6461 - val_loss: 16.1408 - val_mse: 16.1408 - val_mae: 1.6594 - lr: 7.0315e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 10s - loss: 15.7090 - mse: 15.7090 - mae: 1.6396 - val_loss: 16.1819 - val_mse: 16.1819 - val_mae: 1.7100 - lr: 7.0315e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 10s - loss: 15.7840 - mse: 15.7840 - mae: 1.6403 - val_loss: 16.6817 - val_mse: 16.6817 - val_mae: 1.8400 - lr: 7.0315e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 10s - loss: 15.6487 - mse: 15.6487 - mae: 1.6417 - val_loss: 16.1563 - val_mse: 16.1563 - val_mae: 1.6820 - lr: 7.0315e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 10s - loss: 15.7049 - mse: 15.7049 - mae: 1.6431 - val_loss: 16.1546 - val_mse: 16.1546 - val_mae: 1.6492 - lr: 7.0315e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 10s - loss: 15.7231 - mse: 15.7231 - mae: 1.6482 - val_loss: 16.1713 - val_mse: 16.1713 - val_mae: 1.6379 - lr: 7.0315e-04 - 10s/epoch - 10ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-10-14 14:19:42,688]\u001b[0m Finished trial#46 resulted in value: 15.854000000000003. Current best value is 14.266 with parameters: {'activation': 'relu', 'num_hidden_layer': 4, 'i': 10, 'learning_rate': 0.00010332846452133283}.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Score for fold 5: loss of 16.171337127685547\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 4s - loss: 16.8926 - mse: 16.8926 - mae: 1.6279 - val_loss: 10.2426 - val_mse: 10.2426 - val_mae: 1.4787 - lr: 2.5768e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 3s - loss: 16.3545 - mse: 16.3545 - mae: 1.5975 - val_loss: 10.1436 - val_mse: 10.1436 - val_mae: 1.5348 - lr: 2.5768e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 4s - loss: 16.2781 - mse: 16.2781 - mae: 1.5964 - val_loss: 10.1293 - val_mse: 10.1293 - val_mae: 1.6141 - lr: 2.5768e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 4s - loss: 16.2578 - mse: 16.2578 - mae: 1.5979 - val_loss: 10.1262 - val_mse: 10.1262 - val_mae: 1.5506 - lr: 2.5768e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 4s - loss: 16.1208 - mse: 16.1208 - mae: 1.5932 - val_loss: 10.1066 - val_mse: 10.1066 - val_mae: 1.5598 - lr: 2.5768e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 4s - loss: 16.1105 - mse: 16.1105 - mae: 1.5894 - val_loss: 10.0947 - val_mse: 10.0947 - val_mae: 1.5509 - lr: 2.5768e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 4s - loss: 16.0778 - mse: 16.0778 - mae: 1.5834 - val_loss: 10.1797 - val_mse: 10.1797 - val_mae: 1.4917 - lr: 2.5768e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 4s - loss: 16.0505 - mse: 16.0505 - mae: 1.5830 - val_loss: 10.0599 - val_mse: 10.0599 - val_mae: 1.5890 - lr: 2.5768e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 4s - loss: 16.0281 - mse: 16.0281 - mae: 1.5843 - val_loss: 10.1590 - val_mse: 10.1590 - val_mae: 1.5739 - lr: 2.5768e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 4s - loss: 15.9529 - mse: 15.9529 - mae: 1.5778 - val_loss: 10.0685 - val_mse: 10.0685 - val_mae: 1.5348 - lr: 2.5768e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 4s - loss: 16.0166 - mse: 16.0166 - mae: 1.5769 - val_loss: 10.0826 - val_mse: 10.0826 - val_mae: 1.5638 - lr: 2.5768e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 4s - loss: 15.8222 - mse: 15.8222 - mae: 1.5716 - val_loss: 10.1870 - val_mse: 10.1870 - val_mae: 1.6307 - lr: 2.5768e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 13/100\n",
            "1000/1000 - 4s - loss: 15.8686 - mse: 15.8686 - mae: 1.5780 - val_loss: 10.0587 - val_mse: 10.0587 - val_mae: 1.5852 - lr: 2.5768e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 14/100\n",
            "1000/1000 - 4s - loss: 15.8944 - mse: 15.8944 - mae: 1.5715 - val_loss: 10.1118 - val_mse: 10.1118 - val_mae: 1.5159 - lr: 2.5768e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 15/100\n",
            "1000/1000 - 4s - loss: 15.7753 - mse: 15.7753 - mae: 1.5721 - val_loss: 10.0553 - val_mse: 10.0553 - val_mae: 1.5884 - lr: 2.5768e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 16/100\n",
            "1000/1000 - 4s - loss: 15.7185 - mse: 15.7185 - mae: 1.5745 - val_loss: 10.1694 - val_mse: 10.1694 - val_mae: 1.5131 - lr: 2.5768e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 17/100\n",
            "1000/1000 - 4s - loss: 15.7031 - mse: 15.7031 - mae: 1.5697 - val_loss: 10.0621 - val_mse: 10.0621 - val_mae: 1.6060 - lr: 2.5768e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 18/100\n",
            "1000/1000 - 4s - loss: 15.6630 - mse: 15.6630 - mae: 1.5730 - val_loss: 10.1199 - val_mse: 10.1199 - val_mae: 1.5295 - lr: 2.5768e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 19/100\n",
            "1000/1000 - 4s - loss: 15.5445 - mse: 15.5445 - mae: 1.5707 - val_loss: 10.1758 - val_mse: 10.1758 - val_mae: 1.5446 - lr: 2.5768e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 20/100\n",
            "1000/1000 - 4s - loss: 15.5725 - mse: 15.5725 - mae: 1.5754 - val_loss: 10.1546 - val_mse: 10.1546 - val_mae: 1.6154 - lr: 2.5768e-04 - 4s/epoch - 4ms/step\n",
            "Score for fold 1: loss of 10.154582023620605\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 4s - loss: 14.3270 - mse: 14.3270 - mae: 1.5603 - val_loss: 14.9679 - val_mse: 14.9679 - val_mae: 1.5575 - lr: 2.5768e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 4s - loss: 14.2375 - mse: 14.2375 - mae: 1.5537 - val_loss: 15.1931 - val_mse: 15.1931 - val_mae: 1.7018 - lr: 2.5768e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 4s - loss: 14.2048 - mse: 14.2048 - mae: 1.5499 - val_loss: 15.3107 - val_mse: 15.3107 - val_mae: 1.6104 - lr: 2.5768e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 4s - loss: 14.1621 - mse: 14.1621 - mae: 1.5600 - val_loss: 15.4405 - val_mse: 15.4405 - val_mae: 1.7301 - lr: 2.5768e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 4s - loss: 13.9663 - mse: 13.9663 - mae: 1.5574 - val_loss: 15.3252 - val_mse: 15.3252 - val_mae: 1.6401 - lr: 2.5768e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 4s - loss: 13.9764 - mse: 13.9764 - mae: 1.5498 - val_loss: 15.3575 - val_mse: 15.3575 - val_mae: 1.8651 - lr: 2.5768e-04 - 4s/epoch - 4ms/step\n",
            "Score for fold 2: loss of 15.357537269592285\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 4s - loss: 15.4903 - mse: 15.4903 - mae: 1.5890 - val_loss: 9.7967 - val_mse: 9.7967 - val_mae: 1.3860 - lr: 2.5768e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 4s - loss: 15.1857 - mse: 15.1857 - mae: 1.5799 - val_loss: 9.4471 - val_mse: 9.4471 - val_mae: 1.5932 - lr: 2.5768e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 4s - loss: 15.2837 - mse: 15.2837 - mae: 1.5814 - val_loss: 10.0761 - val_mse: 10.0761 - val_mae: 1.7807 - lr: 2.5768e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 4s - loss: 15.2196 - mse: 15.2196 - mae: 1.5834 - val_loss: 9.5117 - val_mse: 9.5117 - val_mae: 1.4742 - lr: 2.5768e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 4s - loss: 15.1865 - mse: 15.1865 - mae: 1.5898 - val_loss: 9.4870 - val_mse: 9.4870 - val_mae: 1.4830 - lr: 2.5768e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 4s - loss: 15.0743 - mse: 15.0743 - mae: 1.5926 - val_loss: 9.5032 - val_mse: 9.5032 - val_mae: 1.4837 - lr: 2.5768e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 4s - loss: 15.0925 - mse: 15.0925 - mae: 1.5764 - val_loss: 9.6147 - val_mse: 9.6147 - val_mae: 1.4928 - lr: 2.5768e-04 - 4s/epoch - 4ms/step\n",
            "Score for fold 3: loss of 9.614724159240723\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 4s - loss: 14.2116 - mse: 14.2116 - mae: 1.5874 - val_loss: 13.3764 - val_mse: 13.3764 - val_mae: 1.5133 - lr: 2.5768e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 4s - loss: 14.1688 - mse: 14.1688 - mae: 1.5964 - val_loss: 13.6954 - val_mse: 13.6954 - val_mae: 1.5175 - lr: 2.5768e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 4s - loss: 14.1187 - mse: 14.1187 - mae: 1.5916 - val_loss: 13.2881 - val_mse: 13.2881 - val_mae: 1.5551 - lr: 2.5768e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 4s - loss: 13.8514 - mse: 13.8514 - mae: 1.5730 - val_loss: 13.2828 - val_mse: 13.2828 - val_mae: 1.6046 - lr: 2.5768e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 4s - loss: 13.9453 - mse: 13.9453 - mae: 1.6040 - val_loss: 13.8734 - val_mse: 13.8734 - val_mae: 1.4714 - lr: 2.5768e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 4s - loss: 13.8737 - mse: 13.8737 - mae: 1.5891 - val_loss: 13.4240 - val_mse: 13.4240 - val_mae: 1.5769 - lr: 2.5768e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 4s - loss: 13.8336 - mse: 13.8336 - mae: 1.5970 - val_loss: 13.6714 - val_mse: 13.6714 - val_mae: 1.6044 - lr: 2.5768e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 4s - loss: 13.6780 - mse: 13.6780 - mae: 1.5909 - val_loss: 13.6495 - val_mse: 13.6495 - val_mae: 1.6243 - lr: 2.5768e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 4s - loss: 13.7064 - mse: 13.7064 - mae: 1.5881 - val_loss: 14.3221 - val_mse: 14.3221 - val_mae: 1.9494 - lr: 2.5768e-04 - 4s/epoch - 4ms/step\n",
            "Score for fold 4: loss of 14.32213306427002\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 4s - loss: 11.4433 - mse: 11.4433 - mae: 1.5999 - val_loss: 22.9129 - val_mse: 22.9129 - val_mae: 1.4912 - lr: 2.5768e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 4s - loss: 11.4189 - mse: 11.4189 - mae: 1.6080 - val_loss: 23.9469 - val_mse: 23.9469 - val_mae: 1.4916 - lr: 2.5768e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 4s - loss: 11.3510 - mse: 11.3510 - mae: 1.6108 - val_loss: 22.8829 - val_mse: 22.8829 - val_mae: 1.5131 - lr: 2.5768e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 4s - loss: 11.3690 - mse: 11.3690 - mae: 1.6357 - val_loss: 23.1370 - val_mse: 23.1370 - val_mae: 1.5830 - lr: 2.5768e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 4s - loss: 11.2073 - mse: 11.2073 - mae: 1.6189 - val_loss: 22.7942 - val_mse: 22.7942 - val_mae: 1.6359 - lr: 2.5768e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 4s - loss: 11.2458 - mse: 11.2458 - mae: 1.6262 - val_loss: 23.1341 - val_mse: 23.1341 - val_mae: 1.5303 - lr: 2.5768e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 4s - loss: 11.0175 - mse: 11.0175 - mae: 1.6011 - val_loss: 23.2576 - val_mse: 23.2576 - val_mae: 1.7631 - lr: 2.5768e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 4s - loss: 11.0056 - mse: 11.0056 - mae: 1.5968 - val_loss: 23.1871 - val_mse: 23.1871 - val_mae: 1.6827 - lr: 2.5768e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 4s - loss: 11.1280 - mse: 11.1280 - mae: 1.6199 - val_loss: 24.8187 - val_mse: 24.8187 - val_mae: 2.3084 - lr: 2.5768e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 4s - loss: 10.9630 - mse: 10.9630 - mae: 1.6208 - val_loss: 24.1618 - val_mse: 24.1618 - val_mae: 2.0387 - lr: 2.5768e-04 - 4s/epoch - 4ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-10-14 14:23:00,598]\u001b[0m Finished trial#47 resulted in value: 14.719999999999999. Current best value is 14.266 with parameters: {'activation': 'relu', 'num_hidden_layer': 4, 'i': 10, 'learning_rate': 0.00010332846452133283}.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Score for fold 5: loss of 24.161746978759766\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 6s - loss: 12.5974 - mse: 12.5974 - mae: 1.6039 - val_loss: 26.5185 - val_mse: 26.5185 - val_mae: 1.7608 - lr: 3.5237e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 5s - loss: 12.2798 - mse: 12.2798 - mae: 1.5870 - val_loss: 26.4772 - val_mse: 26.4772 - val_mae: 1.7252 - lr: 3.5237e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 5s - loss: 12.1792 - mse: 12.1792 - mae: 1.5814 - val_loss: 26.5550 - val_mse: 26.5550 - val_mae: 1.6256 - lr: 3.5237e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 5s - loss: 12.1734 - mse: 12.1734 - mae: 1.5827 - val_loss: 26.6399 - val_mse: 26.6399 - val_mae: 1.6935 - lr: 3.5237e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 5s - loss: 12.0885 - mse: 12.0885 - mae: 1.5730 - val_loss: 26.4851 - val_mse: 26.4851 - val_mae: 1.6398 - lr: 3.5237e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 5s - loss: 11.9980 - mse: 11.9980 - mae: 1.5678 - val_loss: 26.5221 - val_mse: 26.5221 - val_mae: 1.6579 - lr: 3.5237e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 5s - loss: 11.9393 - mse: 11.9393 - mae: 1.5665 - val_loss: 26.6112 - val_mse: 26.6112 - val_mae: 1.6074 - lr: 3.5237e-04 - 5s/epoch - 5ms/step\n",
            "Score for fold 1: loss of 26.611183166503906\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 5s - loss: 15.5310 - mse: 15.5310 - mae: 1.5804 - val_loss: 12.6959 - val_mse: 12.6959 - val_mae: 1.5500 - lr: 3.5237e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 5s - loss: 15.5052 - mse: 15.5052 - mae: 1.5853 - val_loss: 12.2031 - val_mse: 12.2031 - val_mae: 1.5345 - lr: 3.5237e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 5s - loss: 15.4849 - mse: 15.4849 - mae: 1.5822 - val_loss: 12.4192 - val_mse: 12.4192 - val_mae: 1.5656 - lr: 3.5237e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 6s - loss: 15.5010 - mse: 15.5010 - mae: 1.5784 - val_loss: 12.1210 - val_mse: 12.1210 - val_mae: 1.5781 - lr: 3.5237e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 6s - loss: 15.4138 - mse: 15.4138 - mae: 1.5754 - val_loss: 12.3045 - val_mse: 12.3045 - val_mae: 1.5781 - lr: 3.5237e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 5s - loss: 15.4242 - mse: 15.4242 - mae: 1.5774 - val_loss: 12.1914 - val_mse: 12.1914 - val_mae: 1.6973 - lr: 3.5237e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 5s - loss: 15.2924 - mse: 15.2924 - mae: 1.5731 - val_loss: 12.1825 - val_mse: 12.1825 - val_mae: 1.5497 - lr: 3.5237e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 5s - loss: 15.2579 - mse: 15.2579 - mae: 1.5723 - val_loss: 12.5520 - val_mse: 12.5520 - val_mae: 1.5289 - lr: 3.5237e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 5s - loss: 15.3006 - mse: 15.3006 - mae: 1.5740 - val_loss: 12.4100 - val_mse: 12.4100 - val_mae: 1.7655 - lr: 3.5237e-04 - 5s/epoch - 5ms/step\n",
            "Score for fold 2: loss of 12.410001754760742\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 5s - loss: 15.6373 - mse: 15.6373 - mae: 1.5887 - val_loss: 10.9394 - val_mse: 10.9394 - val_mae: 1.5512 - lr: 3.5237e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 5s - loss: 15.5342 - mse: 15.5342 - mae: 1.5926 - val_loss: 11.0070 - val_mse: 11.0070 - val_mae: 1.6373 - lr: 3.5237e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 5s - loss: 15.3900 - mse: 15.3900 - mae: 1.5849 - val_loss: 11.3383 - val_mse: 11.3383 - val_mae: 1.6339 - lr: 3.5237e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 5s - loss: 15.4753 - mse: 15.4753 - mae: 1.5871 - val_loss: 11.2198 - val_mse: 11.2198 - val_mae: 1.6592 - lr: 3.5237e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 5s - loss: 15.3197 - mse: 15.3197 - mae: 1.5812 - val_loss: 11.1651 - val_mse: 11.1651 - val_mae: 1.4844 - lr: 3.5237e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 5s - loss: 15.2268 - mse: 15.2268 - mae: 1.5749 - val_loss: 11.2856 - val_mse: 11.2856 - val_mae: 1.5498 - lr: 3.5237e-04 - 5s/epoch - 5ms/step\n",
            "Score for fold 3: loss of 11.285612106323242\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 5s - loss: 14.1875 - mse: 14.1875 - mae: 1.5549 - val_loss: 15.7570 - val_mse: 15.7570 - val_mae: 1.5828 - lr: 3.5237e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 5s - loss: 14.0113 - mse: 14.0113 - mae: 1.5483 - val_loss: 15.8754 - val_mse: 15.8754 - val_mae: 1.5842 - lr: 3.5237e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 5s - loss: 13.9808 - mse: 13.9808 - mae: 1.5412 - val_loss: 15.6216 - val_mse: 15.6216 - val_mae: 1.5839 - lr: 3.5237e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 5s - loss: 13.8815 - mse: 13.8815 - mae: 1.5427 - val_loss: 15.9464 - val_mse: 15.9464 - val_mae: 1.7438 - lr: 3.5237e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 5s - loss: 13.8632 - mse: 13.8632 - mae: 1.5484 - val_loss: 15.8870 - val_mse: 15.8870 - val_mae: 1.6063 - lr: 3.5237e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 5s - loss: 13.8864 - mse: 13.8864 - mae: 1.5458 - val_loss: 16.0593 - val_mse: 16.0593 - val_mae: 1.5984 - lr: 3.5237e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 5s - loss: 13.8527 - mse: 13.8527 - mae: 1.5492 - val_loss: 15.8878 - val_mse: 15.8878 - val_mae: 1.6121 - lr: 3.5237e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 5s - loss: 13.8344 - mse: 13.8344 - mae: 1.5560 - val_loss: 16.0634 - val_mse: 16.0634 - val_mae: 1.5570 - lr: 3.5237e-04 - 5s/epoch - 5ms/step\n",
            "Score for fold 4: loss of 16.063432693481445\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 5s - loss: 15.8902 - mse: 15.8902 - mae: 1.6020 - val_loss: 7.9299 - val_mse: 7.9299 - val_mae: 1.3831 - lr: 3.5237e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 5s - loss: 15.9469 - mse: 15.9469 - mae: 1.6100 - val_loss: 7.9771 - val_mse: 7.9771 - val_mae: 1.7589 - lr: 3.5237e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 5s - loss: 15.7018 - mse: 15.7018 - mae: 1.6097 - val_loss: 8.0676 - val_mse: 8.0676 - val_mae: 1.5723 - lr: 3.5237e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 5s - loss: 15.7240 - mse: 15.7240 - mae: 1.6230 - val_loss: 7.8798 - val_mse: 7.8798 - val_mae: 1.4450 - lr: 3.5237e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 5s - loss: 15.6584 - mse: 15.6584 - mae: 1.6233 - val_loss: 8.0315 - val_mse: 8.0315 - val_mae: 1.3947 - lr: 3.5237e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 5s - loss: 15.5200 - mse: 15.5200 - mae: 1.6283 - val_loss: 8.2635 - val_mse: 8.2635 - val_mae: 1.3874 - lr: 3.5237e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 5s - loss: 15.5304 - mse: 15.5304 - mae: 1.6206 - val_loss: 7.9476 - val_mse: 7.9476 - val_mae: 1.4808 - lr: 3.5237e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 5s - loss: 15.4324 - mse: 15.4324 - mae: 1.6408 - val_loss: 8.1034 - val_mse: 8.1034 - val_mae: 1.5330 - lr: 3.5237e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 5s - loss: 15.5156 - mse: 15.5156 - mae: 1.6511 - val_loss: 8.4054 - val_mse: 8.4054 - val_mae: 1.8092 - lr: 3.5237e-04 - 5s/epoch - 5ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-10-14 14:26:33,060]\u001b[0m Finished trial#48 resulted in value: 14.955999999999998. Current best value is 14.266 with parameters: {'activation': 'relu', 'num_hidden_layer': 4, 'i': 10, 'learning_rate': 0.00010332846452133283}.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Score for fold 5: loss of 8.405388832092285\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 4s - loss: 16.6705 - mse: 16.6705 - mae: 1.6170 - val_loss: 10.6649 - val_mse: 10.6649 - val_mae: 1.5443 - lr: 5.0769e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 4s - loss: 16.2626 - mse: 16.2626 - mae: 1.5970 - val_loss: 10.5537 - val_mse: 10.5537 - val_mae: 1.6244 - lr: 5.0769e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 4s - loss: 16.2118 - mse: 16.2118 - mae: 1.5943 - val_loss: 10.6541 - val_mse: 10.6541 - val_mae: 1.5515 - lr: 5.0769e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 4s - loss: 16.1474 - mse: 16.1474 - mae: 1.5892 - val_loss: 10.4382 - val_mse: 10.4382 - val_mae: 1.5828 - lr: 5.0769e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 4s - loss: 16.1347 - mse: 16.1347 - mae: 1.5873 - val_loss: 10.4356 - val_mse: 10.4356 - val_mae: 1.5418 - lr: 5.0769e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 4s - loss: 16.0855 - mse: 16.0855 - mae: 1.5832 - val_loss: 10.4887 - val_mse: 10.4887 - val_mae: 1.5287 - lr: 5.0769e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 4s - loss: 16.0269 - mse: 16.0269 - mae: 1.5803 - val_loss: 10.4492 - val_mse: 10.4492 - val_mae: 1.5918 - lr: 5.0769e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 4s - loss: 16.0634 - mse: 16.0634 - mae: 1.5780 - val_loss: 10.4447 - val_mse: 10.4447 - val_mae: 1.5646 - lr: 5.0769e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 4s - loss: 16.0610 - mse: 16.0610 - mae: 1.5797 - val_loss: 10.5665 - val_mse: 10.5665 - val_mae: 1.6011 - lr: 5.0769e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 4s - loss: 16.0118 - mse: 16.0118 - mae: 1.5770 - val_loss: 10.4967 - val_mse: 10.4967 - val_mae: 1.5175 - lr: 5.0769e-04 - 4s/epoch - 4ms/step\n",
            "Score for fold 1: loss of 10.496685028076172\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 4s - loss: 15.2427 - mse: 15.2427 - mae: 1.5760 - val_loss: 13.8023 - val_mse: 13.8023 - val_mae: 1.5612 - lr: 5.0769e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 4s - loss: 15.2043 - mse: 15.2043 - mae: 1.5739 - val_loss: 13.3915 - val_mse: 13.3915 - val_mae: 1.6063 - lr: 5.0769e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 4s - loss: 15.2461 - mse: 15.2461 - mae: 1.5695 - val_loss: 13.4528 - val_mse: 13.4528 - val_mae: 1.6203 - lr: 5.0769e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 4s - loss: 15.1294 - mse: 15.1294 - mae: 1.5711 - val_loss: 13.5430 - val_mse: 13.5430 - val_mae: 1.5737 - lr: 5.0769e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 4s - loss: 15.2483 - mse: 15.2483 - mae: 1.5694 - val_loss: 13.5198 - val_mse: 13.5198 - val_mae: 1.5593 - lr: 5.0769e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 4s - loss: 15.2387 - mse: 15.2387 - mae: 1.5684 - val_loss: 13.5353 - val_mse: 13.5353 - val_mae: 1.5587 - lr: 5.0769e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 4s - loss: 15.1714 - mse: 15.1714 - mae: 1.5671 - val_loss: 13.5617 - val_mse: 13.5617 - val_mae: 1.5731 - lr: 5.0769e-04 - 4s/epoch - 4ms/step\n",
            "Score for fold 2: loss of 13.561714172363281\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 4s - loss: 14.3399 - mse: 14.3399 - mae: 1.5632 - val_loss: 16.6799 - val_mse: 16.6799 - val_mae: 1.5916 - lr: 5.0769e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 4s - loss: 14.3291 - mse: 14.3291 - mae: 1.5594 - val_loss: 17.3011 - val_mse: 17.3011 - val_mae: 1.5433 - lr: 5.0769e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 4s - loss: 14.3087 - mse: 14.3087 - mae: 1.5507 - val_loss: 16.8307 - val_mse: 16.8307 - val_mae: 1.5921 - lr: 5.0769e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 4s - loss: 14.3593 - mse: 14.3593 - mae: 1.5547 - val_loss: 16.5449 - val_mse: 16.5449 - val_mae: 1.6111 - lr: 5.0769e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 4s - loss: 14.3202 - mse: 14.3202 - mae: 1.5564 - val_loss: 16.6785 - val_mse: 16.6785 - val_mae: 1.6111 - lr: 5.0769e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 4s - loss: 14.2412 - mse: 14.2412 - mae: 1.5537 - val_loss: 16.0642 - val_mse: 16.0642 - val_mae: 1.6114 - lr: 5.0769e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 4s - loss: 14.2444 - mse: 14.2444 - mae: 1.5513 - val_loss: 16.7736 - val_mse: 16.7736 - val_mae: 1.6241 - lr: 5.0769e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 4s - loss: 14.1648 - mse: 14.1648 - mae: 1.5508 - val_loss: 17.2644 - val_mse: 17.2644 - val_mae: 1.6579 - lr: 5.0769e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 4s - loss: 14.1956 - mse: 14.1956 - mae: 1.5443 - val_loss: 15.9383 - val_mse: 15.9383 - val_mae: 1.6751 - lr: 5.0769e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 4s - loss: 14.0879 - mse: 14.0879 - mae: 1.5454 - val_loss: 17.0998 - val_mse: 17.0998 - val_mae: 1.5803 - lr: 5.0769e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 4s - loss: 14.0888 - mse: 14.0888 - mae: 1.5449 - val_loss: 16.6230 - val_mse: 16.6230 - val_mae: 1.6118 - lr: 5.0769e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 4s - loss: 14.0166 - mse: 14.0166 - mae: 1.5448 - val_loss: 16.4139 - val_mse: 16.4139 - val_mae: 1.5941 - lr: 5.0769e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 13/100\n",
            "1000/1000 - 4s - loss: 13.9786 - mse: 13.9786 - mae: 1.5381 - val_loss: 16.3474 - val_mse: 16.3474 - val_mae: 1.6595 - lr: 5.0769e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 14/100\n",
            "1000/1000 - 4s - loss: 14.0023 - mse: 14.0023 - mae: 1.5367 - val_loss: 16.5570 - val_mse: 16.5570 - val_mae: 1.6419 - lr: 5.0769e-04 - 4s/epoch - 4ms/step\n",
            "Score for fold 3: loss of 16.55704116821289\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 4s - loss: 12.1982 - mse: 12.1982 - mae: 1.5464 - val_loss: 23.9026 - val_mse: 23.9026 - val_mae: 1.5627 - lr: 5.0769e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 4s - loss: 12.0470 - mse: 12.0470 - mae: 1.5423 - val_loss: 24.3206 - val_mse: 24.3206 - val_mae: 1.6061 - lr: 5.0769e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 4s - loss: 11.9779 - mse: 11.9779 - mae: 1.5376 - val_loss: 24.1623 - val_mse: 24.1623 - val_mae: 1.5487 - lr: 5.0769e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 4s - loss: 12.0243 - mse: 12.0243 - mae: 1.5344 - val_loss: 24.0979 - val_mse: 24.0979 - val_mae: 1.5990 - lr: 5.0769e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 4s - loss: 11.9930 - mse: 11.9930 - mae: 1.5317 - val_loss: 23.9610 - val_mse: 23.9610 - val_mae: 1.5877 - lr: 5.0769e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 4s - loss: 11.9622 - mse: 11.9622 - mae: 1.5288 - val_loss: 24.1100 - val_mse: 24.1100 - val_mae: 1.5405 - lr: 5.0769e-04 - 4s/epoch - 4ms/step\n",
            "Score for fold 4: loss of 24.109983444213867\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 4s - loss: 15.7411 - mse: 15.7411 - mae: 1.5535 - val_loss: 9.0488 - val_mse: 9.0488 - val_mae: 1.4915 - lr: 5.0769e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 4s - loss: 15.6582 - mse: 15.6582 - mae: 1.5500 - val_loss: 9.0048 - val_mse: 9.0048 - val_mae: 1.5295 - lr: 5.0769e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 4s - loss: 15.5273 - mse: 15.5273 - mae: 1.5426 - val_loss: 9.2121 - val_mse: 9.2121 - val_mae: 1.5095 - lr: 5.0769e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 4s - loss: 15.4895 - mse: 15.4895 - mae: 1.5430 - val_loss: 9.0122 - val_mse: 9.0122 - val_mae: 1.4909 - lr: 5.0769e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 4s - loss: 15.4312 - mse: 15.4312 - mae: 1.5390 - val_loss: 9.4025 - val_mse: 9.4025 - val_mae: 1.5493 - lr: 5.0769e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 4s - loss: 15.4008 - mse: 15.4008 - mae: 1.5389 - val_loss: 9.0982 - val_mse: 9.0982 - val_mae: 1.5417 - lr: 5.0769e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 4s - loss: 15.2734 - mse: 15.2734 - mae: 1.5373 - val_loss: 9.2209 - val_mse: 9.2209 - val_mae: 1.5522 - lr: 5.0769e-04 - 4s/epoch - 4ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-10-14 14:29:25,145]\u001b[0m Finished trial#49 resulted in value: 14.790000000000001. Current best value is 14.266 with parameters: {'activation': 'relu', 'num_hidden_layer': 4, 'i': 10, 'learning_rate': 0.00010332846452133283}.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Score for fold 5: loss of 9.220843315124512\n"
          ]
        }
      ],
      "source": [
        "model_list=[]\n",
        "history_list=[]\n",
        "study_name = 'NN_study'\n",
        "study = optuna.create_study(study_name=study_name, load_if_exists=True)\n",
        "func = lambda trial: objective(trial,training,labelsForTrain)\n",
        "study.optimize(func, n_trials=50,)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wvCnW6PlWjWe",
        "outputId": "a81c03b5-c75f-4f1c-e097-46997ea947a6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/500\n",
            "1250/1250 - 6s - loss: 15.9909 - mse: 15.9909 - mae: 1.6802 - val_loss: 13.9783 - val_mse: 13.9783 - val_mae: 1.6952 - lr: 1.0333e-04 - 6s/epoch - 5ms/step\n",
            "Epoch 2/500\n",
            "1250/1250 - 5s - loss: 15.4380 - mse: 15.4380 - mae: 1.6140 - val_loss: 13.7686 - val_mse: 13.7686 - val_mae: 1.6166 - lr: 1.0333e-04 - 5s/epoch - 4ms/step\n",
            "Epoch 3/500\n",
            "1250/1250 - 5s - loss: 15.3375 - mse: 15.3375 - mae: 1.6038 - val_loss: 13.5927 - val_mse: 13.5927 - val_mae: 1.7006 - lr: 1.0333e-04 - 5s/epoch - 4ms/step\n",
            "Epoch 4/500\n",
            "1250/1250 - 5s - loss: 15.2647 - mse: 15.2647 - mae: 1.6013 - val_loss: 13.6895 - val_mse: 13.6895 - val_mae: 1.7494 - lr: 1.0333e-04 - 5s/epoch - 4ms/step\n",
            "Epoch 5/500\n",
            "1250/1250 - 5s - loss: 15.2675 - mse: 15.2675 - mae: 1.5985 - val_loss: 13.5877 - val_mse: 13.5877 - val_mae: 1.5684 - lr: 1.0333e-04 - 5s/epoch - 4ms/step\n",
            "Epoch 6/500\n",
            "1250/1250 - 5s - loss: 15.2242 - mse: 15.2242 - mae: 1.5951 - val_loss: 13.5446 - val_mse: 13.5446 - val_mae: 1.6720 - lr: 1.0333e-04 - 5s/epoch - 4ms/step\n",
            "Epoch 7/500\n",
            "1250/1250 - 5s - loss: 15.2077 - mse: 15.2077 - mae: 1.5967 - val_loss: 13.5659 - val_mse: 13.5659 - val_mae: 1.6060 - lr: 1.0333e-04 - 5s/epoch - 4ms/step\n",
            "Epoch 8/500\n",
            "1250/1250 - 5s - loss: 15.2190 - mse: 15.2190 - mae: 1.5961 - val_loss: 13.6510 - val_mse: 13.6510 - val_mae: 1.6226 - lr: 1.0333e-04 - 5s/epoch - 4ms/step\n",
            "Epoch 9/500\n",
            "1250/1250 - 5s - loss: 15.1901 - mse: 15.1901 - mae: 1.5921 - val_loss: 13.4460 - val_mse: 13.4460 - val_mae: 1.6162 - lr: 1.0333e-04 - 5s/epoch - 4ms/step\n",
            "Epoch 10/500\n",
            "1250/1250 - 5s - loss: 15.1942 - mse: 15.1942 - mae: 1.5879 - val_loss: 13.5661 - val_mse: 13.5661 - val_mae: 1.6097 - lr: 1.0333e-04 - 5s/epoch - 4ms/step\n",
            "Epoch 11/500\n",
            "1250/1250 - 5s - loss: 15.1878 - mse: 15.1878 - mae: 1.5921 - val_loss: 13.6129 - val_mse: 13.6129 - val_mae: 1.6128 - lr: 1.0333e-04 - 5s/epoch - 4ms/step\n",
            "Epoch 12/500\n",
            "1250/1250 - 5s - loss: 15.1828 - mse: 15.1828 - mae: 1.5906 - val_loss: 13.4341 - val_mse: 13.4341 - val_mae: 1.5982 - lr: 1.0333e-04 - 5s/epoch - 4ms/step\n",
            "Epoch 13/500\n",
            "1250/1250 - 5s - loss: 15.1188 - mse: 15.1188 - mae: 1.5913 - val_loss: 13.4487 - val_mse: 13.4487 - val_mae: 1.5921 - lr: 1.0333e-04 - 5s/epoch - 4ms/step\n",
            "Epoch 14/500\n",
            "1250/1250 - 5s - loss: 15.1260 - mse: 15.1260 - mae: 1.5867 - val_loss: 13.7815 - val_mse: 13.7815 - val_mae: 1.5910 - lr: 1.0333e-04 - 5s/epoch - 4ms/step\n",
            "Epoch 15/500\n",
            "1250/1250 - 5s - loss: 15.1238 - mse: 15.1238 - mae: 1.5893 - val_loss: 13.6180 - val_mse: 13.6180 - val_mae: 1.5469 - lr: 1.0333e-04 - 5s/epoch - 4ms/step\n",
            "Epoch 16/500\n",
            "1250/1250 - 5s - loss: 15.0939 - mse: 15.0939 - mae: 1.5887 - val_loss: 13.5178 - val_mse: 13.5178 - val_mae: 1.6222 - lr: 1.0333e-04 - 5s/epoch - 4ms/step\n",
            "Epoch 17/500\n",
            "1250/1250 - 5s - loss: 15.0931 - mse: 15.0931 - mae: 1.5861 - val_loss: 13.3886 - val_mse: 13.3886 - val_mae: 1.6596 - lr: 1.0333e-04 - 5s/epoch - 4ms/step\n",
            "Epoch 18/500\n",
            "1250/1250 - 5s - loss: 15.0710 - mse: 15.0710 - mae: 1.5862 - val_loss: 13.6988 - val_mse: 13.6988 - val_mae: 1.5541 - lr: 1.0333e-04 - 5s/epoch - 4ms/step\n",
            "Epoch 19/500\n",
            "1250/1250 - 5s - loss: 15.1050 - mse: 15.1050 - mae: 1.5821 - val_loss: 13.5229 - val_mse: 13.5229 - val_mae: 1.7109 - lr: 1.0333e-04 - 5s/epoch - 4ms/step\n",
            "Epoch 20/500\n",
            "1250/1250 - 5s - loss: 15.0604 - mse: 15.0604 - mae: 1.5856 - val_loss: 13.4129 - val_mse: 13.4129 - val_mae: 1.6138 - lr: 1.0333e-04 - 5s/epoch - 4ms/step\n",
            "Epoch 21/500\n",
            "1250/1250 - 5s - loss: 15.0395 - mse: 15.0395 - mae: 1.5836 - val_loss: 13.3383 - val_mse: 13.3383 - val_mae: 1.5962 - lr: 1.0333e-04 - 5s/epoch - 4ms/step\n",
            "Epoch 22/500\n",
            "1250/1250 - 5s - loss: 15.0339 - mse: 15.0339 - mae: 1.5816 - val_loss: 13.4678 - val_mse: 13.4678 - val_mae: 1.5906 - lr: 1.0333e-04 - 5s/epoch - 4ms/step\n",
            "Epoch 23/500\n",
            "1250/1250 - 5s - loss: 15.0257 - mse: 15.0257 - mae: 1.5809 - val_loss: 13.3877 - val_mse: 13.3877 - val_mae: 1.5894 - lr: 1.0333e-04 - 5s/epoch - 4ms/step\n",
            "Epoch 24/500\n",
            "1250/1250 - 5s - loss: 15.0330 - mse: 15.0330 - mae: 1.5795 - val_loss: 13.3444 - val_mse: 13.3444 - val_mae: 1.6216 - lr: 1.0333e-04 - 5s/epoch - 4ms/step\n",
            "Epoch 25/500\n",
            "1250/1250 - 5s - loss: 14.9803 - mse: 14.9803 - mae: 1.5774 - val_loss: 13.3661 - val_mse: 13.3661 - val_mae: 1.5815 - lr: 1.0333e-04 - 5s/epoch - 4ms/step\n",
            "Epoch 26/500\n",
            "1250/1250 - 5s - loss: 14.9749 - mse: 14.9749 - mae: 1.5779 - val_loss: 13.4515 - val_mse: 13.4515 - val_mae: 1.6903 - lr: 1.0333e-04 - 5s/epoch - 4ms/step\n",
            "Epoch 27/500\n",
            "1250/1250 - 5s - loss: 14.9377 - mse: 14.9377 - mae: 1.5773 - val_loss: 13.4721 - val_mse: 13.4721 - val_mae: 1.5843 - lr: 1.0333e-04 - 5s/epoch - 4ms/step\n",
            "Epoch 28/500\n",
            "1250/1250 - 5s - loss: 14.9524 - mse: 14.9524 - mae: 1.5747 - val_loss: 13.2608 - val_mse: 13.2608 - val_mae: 1.6359 - lr: 1.0333e-04 - 5s/epoch - 4ms/step\n",
            "Epoch 29/500\n",
            "1250/1250 - 5s - loss: 14.9123 - mse: 14.9123 - mae: 1.5754 - val_loss: 13.3624 - val_mse: 13.3624 - val_mae: 1.6295 - lr: 1.0333e-04 - 5s/epoch - 4ms/step\n",
            "Epoch 30/500\n",
            "1250/1250 - 5s - loss: 14.9191 - mse: 14.9191 - mae: 1.5758 - val_loss: 13.3120 - val_mse: 13.3120 - val_mae: 1.6061 - lr: 1.0333e-04 - 5s/epoch - 4ms/step\n",
            "Epoch 31/500\n",
            "1250/1250 - 5s - loss: 14.8726 - mse: 14.8726 - mae: 1.5715 - val_loss: 13.3291 - val_mse: 13.3291 - val_mae: 1.6576 - lr: 1.0333e-04 - 5s/epoch - 4ms/step\n",
            "Epoch 32/500\n",
            "1250/1250 - 5s - loss: 14.8875 - mse: 14.8875 - mae: 1.5738 - val_loss: 13.4061 - val_mse: 13.4061 - val_mae: 1.6059 - lr: 1.0333e-04 - 5s/epoch - 4ms/step\n",
            "Epoch 33/500\n",
            "1250/1250 - 5s - loss: 14.8273 - mse: 14.8273 - mae: 1.5721 - val_loss: 13.5562 - val_mse: 13.5562 - val_mae: 1.6525 - lr: 1.0333e-04 - 5s/epoch - 4ms/step\n",
            "Epoch 34/500\n",
            "1250/1250 - 5s - loss: 14.8501 - mse: 14.8501 - mae: 1.5731 - val_loss: 13.3726 - val_mse: 13.3726 - val_mae: 1.5725 - lr: 1.0333e-04 - 5s/epoch - 4ms/step\n",
            "Epoch 35/500\n",
            "1250/1250 - 5s - loss: 14.7979 - mse: 14.7979 - mae: 1.5683 - val_loss: 13.4482 - val_mse: 13.4482 - val_mae: 1.5624 - lr: 1.0333e-04 - 5s/epoch - 4ms/step\n",
            "Epoch 36/500\n",
            "1250/1250 - 5s - loss: 14.7898 - mse: 14.7898 - mae: 1.5659 - val_loss: 13.4981 - val_mse: 13.4981 - val_mae: 1.5860 - lr: 1.0333e-04 - 5s/epoch - 4ms/step\n",
            "Epoch 37/500\n",
            "1250/1250 - 5s - loss: 14.8081 - mse: 14.8081 - mae: 1.5696 - val_loss: 13.3023 - val_mse: 13.3023 - val_mae: 1.5797 - lr: 1.0333e-04 - 5s/epoch - 4ms/step\n",
            "Epoch 38/500\n",
            "1250/1250 - 5s - loss: 14.7595 - mse: 14.7595 - mae: 1.5667 - val_loss: 13.2725 - val_mse: 13.2725 - val_mae: 1.5891 - lr: 1.0333e-04 - 5s/epoch - 4ms/step\n"
          ]
        }
      ],
      "source": [
        "#{'activation': 'tanh', 'num_hidden_layer': 4, 'i': 9, 'learning_rate': 0.00039524792654887494}.\n",
        "optimizer = Adam(learning_rate=0.00010332846452133283 ,clipnorm=1.0)\n",
        "model_1 = create_model(activation=\"tanh\",num_hidden_layer=4,num_hidden_unit=512)\n",
        "\n",
        "es = EarlyStopping(monitor='val_mse', patience=10)\n",
        "reduce_lr = ReduceLROnPlateau(monitor='val_mse', factor=0.2,\n",
        "                              patience=20, min_lr=0.001)\n",
        "model_1.compile(loss='mse',optimizer=optimizer,metrics=['mse','mae'])\n",
        "history = model_1.fit(training,labelsForTrain,\n",
        "                batch_size=64,\n",
        "                epochs=500,\n",
        "                verbose=2,\n",
        "                validation_data=(valing,labelsForVal),\n",
        "                validation_batch_size=64,\n",
        "                callbacks=[es,reduce_lr])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E7tRpJTHX7CY",
        "outputId": "168e5dcd-24d1-487a-d514-f33b58fff4c1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "313/313 [==============================] - 1s 2ms/step - loss: 11.6705 - mse: 11.6705 - mae: 1.5727\n"
          ]
        }
      ],
      "source": [
        "results_model1 = model_1.evaluate(testing, labelsForTest, batch_size=32)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "amvicsmJGgjt"
      },
      "source": [
        "## Shuffle & Repetation 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VzDLbL_7BIuK"
      },
      "outputs": [],
      "source": [
        "# Shuffle the data after train\n",
        "shuffled1 = shuffle(train_df, random_state=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DcTtPuUtjwCu"
      },
      "outputs": [],
      "source": [
        "def process_shuffle_dataset(dataset):\n",
        "  training_withaim=dataset.drop(labels=\"r_id\", axis=1)\n",
        "  imp_train=SimpleImputer(missing_values=np.NaN)\n",
        "  training_shuffled=pd.DataFrame(imp_train.fit_transform(training_withaim))\n",
        "  training_shuffled = training_shuffled.iloc[: , 0:11]\n",
        "  scaler = StandardScaler()\n",
        "  std_train_df = dataset.copy(deep=True)\n",
        "  std_train_df = scaler.fit_transform(training_shuffled)\n",
        "  std_train_df = pd.DataFrame(std_train_df)\n",
        "  training_shuffled = std_train_df.iloc[: , 0:12]\n",
        "  labelsForTrain_shuffled=training_withaim.iloc[: , -1]\n",
        "  return(training_shuffled,labelsForTrain_shuffled)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dm38hbJ3kF1t"
      },
      "outputs": [],
      "source": [
        "training_shuffled1,labelsForTrain_shuffled1=process_shuffle_dataset(shuffled1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "orNMxT13oi-z",
        "outputId": "2b6faa47-3504-48ab-f7af-12028d962765"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 3s - loss: 15.0282 - mse: 15.0282 - mae: 1.6655 - val_loss: 18.7563 - val_mse: 18.7563 - val_mae: 1.5745 - lr: 6.1633e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 14.5845 - mse: 14.5845 - mae: 1.6152 - val_loss: 18.6136 - val_mse: 18.6136 - val_mae: 1.5898 - lr: 6.1633e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 14.4450 - mse: 14.4450 - mae: 1.6062 - val_loss: 18.4968 - val_mse: 18.4968 - val_mae: 1.6463 - lr: 6.1633e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 14.4048 - mse: 14.4048 - mae: 1.6021 - val_loss: 18.4354 - val_mse: 18.4354 - val_mae: 1.5850 - lr: 6.1633e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 14.3877 - mse: 14.3877 - mae: 1.6041 - val_loss: 18.4718 - val_mse: 18.4718 - val_mae: 1.6301 - lr: 6.1633e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 14.3005 - mse: 14.3005 - mae: 1.6000 - val_loss: 18.4213 - val_mse: 18.4213 - val_mae: 1.5921 - lr: 6.1633e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 14.2932 - mse: 14.2932 - mae: 1.5970 - val_loss: 18.4758 - val_mse: 18.4758 - val_mae: 1.5585 - lr: 6.1633e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 2s - loss: 14.2375 - mse: 14.2375 - mae: 1.5939 - val_loss: 18.4804 - val_mse: 18.4804 - val_mae: 1.6212 - lr: 6.1633e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 2s - loss: 14.2392 - mse: 14.2392 - mae: 1.5937 - val_loss: 18.3739 - val_mse: 18.3739 - val_mae: 1.5775 - lr: 6.1633e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 2s - loss: 14.2335 - mse: 14.2335 - mae: 1.5928 - val_loss: 18.6571 - val_mse: 18.6571 - val_mae: 1.5536 - lr: 6.1633e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 2s - loss: 14.2325 - mse: 14.2325 - mae: 1.5946 - val_loss: 18.3258 - val_mse: 18.3258 - val_mae: 1.6039 - lr: 6.1633e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 2s - loss: 14.1598 - mse: 14.1598 - mae: 1.5872 - val_loss: 18.4018 - val_mse: 18.4018 - val_mae: 1.5502 - lr: 6.1633e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 13/100\n",
            "1000/1000 - 2s - loss: 14.1552 - mse: 14.1552 - mae: 1.5887 - val_loss: 18.2926 - val_mse: 18.2926 - val_mae: 1.5859 - lr: 6.1633e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 14/100\n",
            "1000/1000 - 2s - loss: 14.1189 - mse: 14.1189 - mae: 1.5875 - val_loss: 18.3166 - val_mse: 18.3166 - val_mae: 1.5861 - lr: 6.1633e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 15/100\n",
            "1000/1000 - 2s - loss: 14.1102 - mse: 14.1102 - mae: 1.5817 - val_loss: 18.4345 - val_mse: 18.4345 - val_mae: 1.6397 - lr: 6.1633e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 16/100\n",
            "1000/1000 - 2s - loss: 14.0839 - mse: 14.0839 - mae: 1.5840 - val_loss: 18.3844 - val_mse: 18.3844 - val_mae: 1.5351 - lr: 6.1633e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 17/100\n",
            "1000/1000 - 3s - loss: 14.0415 - mse: 14.0415 - mae: 1.5798 - val_loss: 18.5387 - val_mse: 18.5387 - val_mae: 1.5680 - lr: 6.1633e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 18/100\n",
            "1000/1000 - 2s - loss: 14.0683 - mse: 14.0683 - mae: 1.5792 - val_loss: 18.2920 - val_mse: 18.2920 - val_mae: 1.5759 - lr: 6.1633e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 19/100\n",
            "1000/1000 - 3s - loss: 14.0108 - mse: 14.0108 - mae: 1.5807 - val_loss: 18.4370 - val_mse: 18.4370 - val_mae: 1.6429 - lr: 6.1633e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 20/100\n",
            "1000/1000 - 3s - loss: 13.9738 - mse: 13.9738 - mae: 1.5783 - val_loss: 18.3959 - val_mse: 18.3959 - val_mae: 1.5620 - lr: 6.1633e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 21/100\n",
            "1000/1000 - 3s - loss: 13.9910 - mse: 13.9910 - mae: 1.5779 - val_loss: 18.5521 - val_mse: 18.5521 - val_mae: 1.5389 - lr: 6.1633e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 22/100\n",
            "1000/1000 - 3s - loss: 13.9550 - mse: 13.9550 - mae: 1.5740 - val_loss: 18.3798 - val_mse: 18.3798 - val_mae: 1.5923 - lr: 6.1633e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 23/100\n",
            "1000/1000 - 3s - loss: 13.8707 - mse: 13.8707 - mae: 1.5742 - val_loss: 18.3903 - val_mse: 18.3903 - val_mae: 1.6062 - lr: 6.1633e-04 - 3s/epoch - 3ms/step\n",
            "Score for fold 1: loss of 18.390338897705078\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 15.3064 - mse: 15.3064 - mae: 1.5781 - val_loss: 13.0367 - val_mse: 13.0367 - val_mae: 1.5757 - lr: 6.1633e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 15.2913 - mse: 15.2913 - mae: 1.5738 - val_loss: 12.9671 - val_mse: 12.9671 - val_mae: 1.5916 - lr: 6.1633e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 15.1987 - mse: 15.1987 - mae: 1.5759 - val_loss: 12.8687 - val_mse: 12.8687 - val_mae: 1.5984 - lr: 6.1633e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 15.1517 - mse: 15.1517 - mae: 1.5693 - val_loss: 13.1898 - val_mse: 13.1898 - val_mae: 1.5772 - lr: 6.1633e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 15.1819 - mse: 15.1819 - mae: 1.5689 - val_loss: 12.6510 - val_mse: 12.6510 - val_mae: 1.5974 - lr: 6.1633e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 15.1833 - mse: 15.1833 - mae: 1.5598 - val_loss: 12.8621 - val_mse: 12.8621 - val_mae: 1.6376 - lr: 6.1633e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 15.0800 - mse: 15.0800 - mae: 1.5651 - val_loss: 13.1441 - val_mse: 13.1441 - val_mae: 1.5630 - lr: 6.1633e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 2s - loss: 15.0848 - mse: 15.0848 - mae: 1.5602 - val_loss: 13.2756 - val_mse: 13.2756 - val_mae: 1.5507 - lr: 6.1633e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 2s - loss: 15.0717 - mse: 15.0717 - mae: 1.5615 - val_loss: 12.9799 - val_mse: 12.9799 - val_mae: 1.6070 - lr: 6.1633e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 2s - loss: 15.0789 - mse: 15.0789 - mae: 1.5626 - val_loss: 12.9454 - val_mse: 12.9454 - val_mae: 1.5663 - lr: 6.1633e-04 - 2s/epoch - 2ms/step\n",
            "Score for fold 2: loss of 12.945399284362793\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 14.9470 - mse: 14.9470 - mae: 1.5610 - val_loss: 13.2418 - val_mse: 13.2418 - val_mae: 1.5672 - lr: 6.1633e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 3s - loss: 14.8805 - mse: 14.8805 - mae: 1.5561 - val_loss: 13.4840 - val_mse: 13.4840 - val_mae: 1.5436 - lr: 6.1633e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 3s - loss: 14.8668 - mse: 14.8668 - mae: 1.5542 - val_loss: 13.4648 - val_mse: 13.4648 - val_mae: 1.5693 - lr: 6.1633e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 3s - loss: 14.8475 - mse: 14.8475 - mae: 1.5551 - val_loss: 13.2815 - val_mse: 13.2815 - val_mae: 1.5986 - lr: 6.1633e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 3s - loss: 14.8236 - mse: 14.8236 - mae: 1.5539 - val_loss: 13.4166 - val_mse: 13.4166 - val_mae: 1.5816 - lr: 6.1633e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 3s - loss: 14.7940 - mse: 14.7940 - mae: 1.5543 - val_loss: 13.4087 - val_mse: 13.4087 - val_mae: 1.5714 - lr: 6.1633e-04 - 3s/epoch - 3ms/step\n",
            "Score for fold 3: loss of 13.408699035644531\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 14.5590 - mse: 14.5590 - mae: 1.5644 - val_loss: 14.3929 - val_mse: 14.3929 - val_mae: 1.5858 - lr: 6.1633e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 3s - loss: 14.5533 - mse: 14.5533 - mae: 1.5629 - val_loss: 14.3747 - val_mse: 14.3747 - val_mae: 1.5510 - lr: 6.1633e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 3s - loss: 14.4323 - mse: 14.4323 - mae: 1.5587 - val_loss: 14.4641 - val_mse: 14.4641 - val_mae: 1.5966 - lr: 6.1633e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 3s - loss: 14.4258 - mse: 14.4258 - mae: 1.5640 - val_loss: 15.0640 - val_mse: 15.0640 - val_mae: 1.5589 - lr: 6.1633e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 3s - loss: 14.4421 - mse: 14.4421 - mae: 1.5540 - val_loss: 14.5201 - val_mse: 14.5201 - val_mae: 1.5944 - lr: 6.1633e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 3s - loss: 14.3960 - mse: 14.3960 - mae: 1.5547 - val_loss: 14.7741 - val_mse: 14.7741 - val_mae: 1.5922 - lr: 6.1633e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 3s - loss: 14.4141 - mse: 14.4141 - mae: 1.5555 - val_loss: 15.1940 - val_mse: 15.1940 - val_mae: 1.5991 - lr: 6.1633e-04 - 3s/epoch - 3ms/step\n",
            "Score for fold 4: loss of 15.193963050842285\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 14.5869 - mse: 14.5869 - mae: 1.5648 - val_loss: 14.0073 - val_mse: 14.0073 - val_mae: 1.5285 - lr: 6.1633e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 3s - loss: 14.5370 - mse: 14.5370 - mae: 1.5650 - val_loss: 14.0970 - val_mse: 14.0970 - val_mae: 1.5346 - lr: 6.1633e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 3s - loss: 14.4846 - mse: 14.4846 - mae: 1.5611 - val_loss: 14.1577 - val_mse: 14.1577 - val_mae: 1.5396 - lr: 6.1633e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 3s - loss: 14.4752 - mse: 14.4752 - mae: 1.5586 - val_loss: 14.0424 - val_mse: 14.0424 - val_mae: 1.5303 - lr: 6.1633e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 3s - loss: 14.4083 - mse: 14.4083 - mae: 1.5558 - val_loss: 14.1708 - val_mse: 14.1708 - val_mae: 1.5570 - lr: 6.1633e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 3s - loss: 14.3491 - mse: 14.3491 - mae: 1.5550 - val_loss: 14.2012 - val_mse: 14.2012 - val_mae: 1.5787 - lr: 6.1633e-04 - 3s/epoch - 3ms/step\n",
            "Score for fold 5: loss of 14.201173782348633\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-10-14 14:31:41,781]\u001b[0m Finished trial#0 resulted in value: 14.828. Current best value is 14.828 with parameters: {'activation': 'tanh', 'num_hidden_layer': 4, 'i': 7, 'learning_rate': 0.0006163259286815976}.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 3s - loss: 18.0802 - mse: 18.0802 - mae: 1.7739 - val_loss: 12.9341 - val_mse: 12.9341 - val_mae: 1.6154 - lr: 1.7552e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 16.1743 - mse: 16.1743 - mae: 1.6275 - val_loss: 12.9060 - val_mse: 12.9060 - val_mae: 1.6170 - lr: 1.7552e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 16.1437 - mse: 16.1437 - mae: 1.6236 - val_loss: 12.8711 - val_mse: 12.8711 - val_mae: 1.6199 - lr: 1.7552e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 16.1127 - mse: 16.1127 - mae: 1.6268 - val_loss: 12.8848 - val_mse: 12.8848 - val_mae: 1.5719 - lr: 1.7552e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 16.0885 - mse: 16.0885 - mae: 1.6186 - val_loss: 12.8518 - val_mse: 12.8518 - val_mae: 1.5754 - lr: 1.7552e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 16.0632 - mse: 16.0632 - mae: 1.6187 - val_loss: 12.8315 - val_mse: 12.8315 - val_mae: 1.5849 - lr: 1.7552e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 16.0557 - mse: 16.0557 - mae: 1.6159 - val_loss: 12.7963 - val_mse: 12.7963 - val_mae: 1.6015 - lr: 1.7552e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 2s - loss: 16.0150 - mse: 16.0150 - mae: 1.6153 - val_loss: 12.8055 - val_mse: 12.8055 - val_mae: 1.5725 - lr: 1.7552e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 2s - loss: 16.0101 - mse: 16.0101 - mae: 1.6111 - val_loss: 12.7844 - val_mse: 12.7844 - val_mae: 1.5851 - lr: 1.7552e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 2s - loss: 15.9787 - mse: 15.9787 - mae: 1.6154 - val_loss: 12.7775 - val_mse: 12.7775 - val_mae: 1.5762 - lr: 1.7552e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 2s - loss: 15.9691 - mse: 15.9691 - mae: 1.6109 - val_loss: 12.7764 - val_mse: 12.7764 - val_mae: 1.5676 - lr: 1.7552e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 2s - loss: 15.9558 - mse: 15.9558 - mae: 1.6109 - val_loss: 12.7462 - val_mse: 12.7462 - val_mae: 1.5724 - lr: 1.7552e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 13/100\n",
            "1000/1000 - 2s - loss: 15.9459 - mse: 15.9459 - mae: 1.6068 - val_loss: 12.7308 - val_mse: 12.7308 - val_mae: 1.5855 - lr: 1.7552e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 14/100\n",
            "1000/1000 - 2s - loss: 15.9273 - mse: 15.9273 - mae: 1.6085 - val_loss: 12.7388 - val_mse: 12.7388 - val_mae: 1.5662 - lr: 1.7552e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 15/100\n",
            "1000/1000 - 2s - loss: 15.9212 - mse: 15.9212 - mae: 1.6079 - val_loss: 12.7182 - val_mse: 12.7182 - val_mae: 1.5801 - lr: 1.7552e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 16/100\n",
            "1000/1000 - 2s - loss: 15.9131 - mse: 15.9131 - mae: 1.6091 - val_loss: 12.7346 - val_mse: 12.7346 - val_mae: 1.5659 - lr: 1.7552e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 17/100\n",
            "1000/1000 - 2s - loss: 15.8946 - mse: 15.8946 - mae: 1.6047 - val_loss: 12.7035 - val_mse: 12.7035 - val_mae: 1.5798 - lr: 1.7552e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 18/100\n",
            "1000/1000 - 2s - loss: 15.8602 - mse: 15.8602 - mae: 1.6091 - val_loss: 12.7116 - val_mse: 12.7116 - val_mae: 1.5658 - lr: 1.7552e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 19/100\n",
            "1000/1000 - 2s - loss: 15.8801 - mse: 15.8801 - mae: 1.6057 - val_loss: 12.6664 - val_mse: 12.6664 - val_mae: 1.5822 - lr: 1.7552e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 20/100\n",
            "1000/1000 - 2s - loss: 15.8526 - mse: 15.8526 - mae: 1.6067 - val_loss: 12.6846 - val_mse: 12.6846 - val_mae: 1.5601 - lr: 1.7552e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 21/100\n",
            "1000/1000 - 2s - loss: 15.8477 - mse: 15.8477 - mae: 1.6030 - val_loss: 12.6876 - val_mse: 12.6876 - val_mae: 1.5679 - lr: 1.7552e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 22/100\n",
            "1000/1000 - 2s - loss: 15.8432 - mse: 15.8432 - mae: 1.6017 - val_loss: 12.6869 - val_mse: 12.6869 - val_mae: 1.5546 - lr: 1.7552e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 23/100\n",
            "1000/1000 - 2s - loss: 15.8273 - mse: 15.8273 - mae: 1.6008 - val_loss: 12.6538 - val_mse: 12.6538 - val_mae: 1.5941 - lr: 1.7552e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 24/100\n",
            "1000/1000 - 2s - loss: 15.8098 - mse: 15.8098 - mae: 1.6036 - val_loss: 12.6546 - val_mse: 12.6546 - val_mae: 1.5676 - lr: 1.7552e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 25/100\n",
            "1000/1000 - 2s - loss: 15.8069 - mse: 15.8069 - mae: 1.6000 - val_loss: 12.6758 - val_mse: 12.6758 - val_mae: 1.5623 - lr: 1.7552e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 26/100\n",
            "1000/1000 - 2s - loss: 15.8097 - mse: 15.8097 - mae: 1.5987 - val_loss: 12.6469 - val_mse: 12.6469 - val_mae: 1.5783 - lr: 1.7552e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 27/100\n",
            "1000/1000 - 2s - loss: 15.8048 - mse: 15.8048 - mae: 1.5979 - val_loss: 12.6420 - val_mse: 12.6420 - val_mae: 1.5725 - lr: 1.7552e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 28/100\n",
            "1000/1000 - 2s - loss: 15.7853 - mse: 15.7853 - mae: 1.5997 - val_loss: 12.6256 - val_mse: 12.6256 - val_mae: 1.5785 - lr: 1.7552e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 29/100\n",
            "1000/1000 - 2s - loss: 15.7754 - mse: 15.7754 - mae: 1.5989 - val_loss: 12.6145 - val_mse: 12.6145 - val_mae: 1.6068 - lr: 1.7552e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 30/100\n",
            "1000/1000 - 2s - loss: 15.7804 - mse: 15.7804 - mae: 1.5970 - val_loss: 12.6414 - val_mse: 12.6414 - val_mae: 1.5624 - lr: 1.7552e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 31/100\n",
            "1000/1000 - 2s - loss: 15.7762 - mse: 15.7762 - mae: 1.5949 - val_loss: 12.6110 - val_mse: 12.6110 - val_mae: 1.6071 - lr: 1.7552e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 32/100\n",
            "1000/1000 - 2s - loss: 15.7568 - mse: 15.7568 - mae: 1.6001 - val_loss: 12.6373 - val_mse: 12.6373 - val_mae: 1.5678 - lr: 1.7552e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 33/100\n",
            "1000/1000 - 2s - loss: 15.7582 - mse: 15.7582 - mae: 1.5979 - val_loss: 12.6246 - val_mse: 12.6246 - val_mae: 1.5690 - lr: 1.7552e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 34/100\n",
            "1000/1000 - 2s - loss: 15.7592 - mse: 15.7592 - mae: 1.5976 - val_loss: 12.6396 - val_mse: 12.6396 - val_mae: 1.5544 - lr: 1.7552e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 35/100\n",
            "1000/1000 - 2s - loss: 15.7439 - mse: 15.7439 - mae: 1.5944 - val_loss: 12.6219 - val_mse: 12.6219 - val_mae: 1.5845 - lr: 1.7552e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 36/100\n",
            "1000/1000 - 2s - loss: 15.7578 - mse: 15.7578 - mae: 1.5970 - val_loss: 12.6116 - val_mse: 12.6116 - val_mae: 1.5668 - lr: 1.7552e-04 - 2s/epoch - 2ms/step\n",
            "Score for fold 1: loss of 12.611557960510254\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 14.1820 - mse: 14.1820 - mae: 1.5901 - val_loss: 18.8789 - val_mse: 18.8789 - val_mae: 1.6056 - lr: 1.7552e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 14.1600 - mse: 14.1600 - mae: 1.5908 - val_loss: 18.8879 - val_mse: 18.8879 - val_mae: 1.5949 - lr: 1.7552e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 14.1636 - mse: 14.1636 - mae: 1.5926 - val_loss: 18.8970 - val_mse: 18.8970 - val_mae: 1.5802 - lr: 1.7552e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 14.1594 - mse: 14.1594 - mae: 1.5895 - val_loss: 18.9089 - val_mse: 18.9089 - val_mae: 1.5796 - lr: 1.7552e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 14.1580 - mse: 14.1580 - mae: 1.5888 - val_loss: 18.8793 - val_mse: 18.8793 - val_mae: 1.6044 - lr: 1.7552e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 14.1556 - mse: 14.1556 - mae: 1.5899 - val_loss: 18.9117 - val_mse: 18.9117 - val_mae: 1.5830 - lr: 1.7552e-04 - 2s/epoch - 2ms/step\n",
            "Score for fold 2: loss of 18.91172218322754\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 14.8773 - mse: 14.8773 - mae: 1.5747 - val_loss: 16.0843 - val_mse: 16.0843 - val_mae: 1.6220 - lr: 1.7552e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 14.8643 - mse: 14.8643 - mae: 1.5765 - val_loss: 16.1267 - val_mse: 16.1267 - val_mae: 1.6102 - lr: 1.7552e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 14.8615 - mse: 14.8615 - mae: 1.5744 - val_loss: 16.1248 - val_mse: 16.1248 - val_mae: 1.6317 - lr: 1.7552e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 14.8747 - mse: 14.8747 - mae: 1.5760 - val_loss: 16.1110 - val_mse: 16.1110 - val_mae: 1.6104 - lr: 1.7552e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 14.8665 - mse: 14.8665 - mae: 1.5760 - val_loss: 16.1124 - val_mse: 16.1124 - val_mae: 1.6145 - lr: 1.7552e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 14.8714 - mse: 14.8714 - mae: 1.5743 - val_loss: 16.1129 - val_mse: 16.1129 - val_mae: 1.6251 - lr: 1.7552e-04 - 2s/epoch - 2ms/step\n",
            "Score for fold 3: loss of 16.112857818603516\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 15.6657 - mse: 15.6657 - mae: 1.5866 - val_loss: 12.8574 - val_mse: 12.8574 - val_mae: 1.5782 - lr: 1.7552e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 15.6567 - mse: 15.6567 - mae: 1.5829 - val_loss: 12.8256 - val_mse: 12.8256 - val_mae: 1.5936 - lr: 1.7552e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 15.6506 - mse: 15.6506 - mae: 1.5853 - val_loss: 12.8441 - val_mse: 12.8441 - val_mae: 1.5807 - lr: 1.7552e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 15.6439 - mse: 15.6439 - mae: 1.5821 - val_loss: 12.8475 - val_mse: 12.8475 - val_mae: 1.5812 - lr: 1.7552e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 15.6576 - mse: 15.6576 - mae: 1.5830 - val_loss: 12.8466 - val_mse: 12.8466 - val_mae: 1.5828 - lr: 1.7552e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 15.6380 - mse: 15.6380 - mae: 1.5836 - val_loss: 12.8390 - val_mse: 12.8390 - val_mae: 1.5888 - lr: 1.7552e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 15.6282 - mse: 15.6282 - mae: 1.5833 - val_loss: 12.8540 - val_mse: 12.8540 - val_mae: 1.5759 - lr: 1.7552e-04 - 2s/epoch - 2ms/step\n",
            "Score for fold 4: loss of 12.853984832763672\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 15.0905 - mse: 15.0905 - mae: 1.5826 - val_loss: 15.0412 - val_mse: 15.0412 - val_mae: 1.5951 - lr: 1.7552e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 15.0833 - mse: 15.0833 - mae: 1.5841 - val_loss: 15.0503 - val_mse: 15.0503 - val_mae: 1.5833 - lr: 1.7552e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 15.0787 - mse: 15.0787 - mae: 1.5823 - val_loss: 15.0511 - val_mse: 15.0511 - val_mae: 1.5929 - lr: 1.7552e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 15.0788 - mse: 15.0788 - mae: 1.5859 - val_loss: 15.0886 - val_mse: 15.0886 - val_mae: 1.5781 - lr: 1.7552e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 15.0770 - mse: 15.0770 - mae: 1.5815 - val_loss: 15.0363 - val_mse: 15.0363 - val_mae: 1.5847 - lr: 1.7552e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 15.0695 - mse: 15.0695 - mae: 1.5841 - val_loss: 15.0754 - val_mse: 15.0754 - val_mae: 1.5731 - lr: 1.7552e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 15.0618 - mse: 15.0618 - mae: 1.5804 - val_loss: 15.0874 - val_mse: 15.0874 - val_mae: 1.5675 - lr: 1.7552e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 2s - loss: 15.0695 - mse: 15.0695 - mae: 1.5815 - val_loss: 15.0449 - val_mse: 15.0449 - val_mae: 1.5858 - lr: 1.7552e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 2s - loss: 15.0599 - mse: 15.0599 - mae: 1.5832 - val_loss: 15.0551 - val_mse: 15.0551 - val_mae: 1.5786 - lr: 1.7552e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 2s - loss: 15.0530 - mse: 15.0530 - mae: 1.5825 - val_loss: 15.0418 - val_mse: 15.0418 - val_mae: 1.5756 - lr: 1.7552e-04 - 2s/epoch - 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-10-14 14:33:52,501]\u001b[0m Finished trial#1 resulted in value: 15.104. Current best value is 14.828 with parameters: {'activation': 'tanh', 'num_hidden_layer': 4, 'i': 7, 'learning_rate': 0.0006163259286815976}.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Score for fold 5: loss of 15.041754722595215\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 3s - loss: 17.5532 - mse: 17.5532 - mae: 1.7328 - val_loss: 11.9741 - val_mse: 11.9741 - val_mae: 1.6121 - lr: 2.6768e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 16.5715 - mse: 16.5715 - mae: 1.6271 - val_loss: 11.9287 - val_mse: 11.9287 - val_mae: 1.6335 - lr: 2.6768e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 16.6053 - mse: 16.6053 - mae: 1.6280 - val_loss: 11.8918 - val_mse: 11.8918 - val_mae: 1.6397 - lr: 2.6768e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 16.5827 - mse: 16.5827 - mae: 1.6283 - val_loss: 11.9450 - val_mse: 11.9450 - val_mae: 1.6245 - lr: 2.6768e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 16.5908 - mse: 16.5908 - mae: 1.6236 - val_loss: 11.9075 - val_mse: 11.9075 - val_mae: 1.6365 - lr: 2.6768e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 16.5795 - mse: 16.5795 - mae: 1.6290 - val_loss: 11.9777 - val_mse: 11.9777 - val_mae: 1.6238 - lr: 2.6768e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 16.5744 - mse: 16.5744 - mae: 1.6287 - val_loss: 12.0193 - val_mse: 12.0193 - val_mae: 1.6228 - lr: 2.6768e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 2s - loss: 16.6160 - mse: 16.6160 - mae: 1.6312 - val_loss: 11.9094 - val_mse: 11.9094 - val_mae: 1.6304 - lr: 2.6768e-04 - 2s/epoch - 2ms/step\n",
            "Score for fold 1: loss of 11.90937614440918\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 16.0005 - mse: 16.0005 - mae: 1.6198 - val_loss: 14.4446 - val_mse: 14.4446 - val_mae: 1.6335 - lr: 2.6768e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 16.0145 - mse: 16.0145 - mae: 1.6185 - val_loss: 14.3290 - val_mse: 14.3290 - val_mae: 1.6915 - lr: 2.6768e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 15.9905 - mse: 15.9905 - mae: 1.6224 - val_loss: 14.3240 - val_mse: 14.3240 - val_mae: 1.6681 - lr: 2.6768e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 16.0012 - mse: 16.0012 - mae: 1.6190 - val_loss: 14.2945 - val_mse: 14.2945 - val_mae: 1.6777 - lr: 2.6768e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 16.0119 - mse: 16.0119 - mae: 1.6183 - val_loss: 14.3650 - val_mse: 14.3650 - val_mae: 1.6598 - lr: 2.6768e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 15.9897 - mse: 15.9897 - mae: 1.6198 - val_loss: 14.3414 - val_mse: 14.3414 - val_mae: 1.6712 - lr: 2.6768e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 15.9953 - mse: 15.9953 - mae: 1.6169 - val_loss: 14.2836 - val_mse: 14.2836 - val_mae: 1.6964 - lr: 2.6768e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 2s - loss: 15.9742 - mse: 15.9742 - mae: 1.6181 - val_loss: 14.2844 - val_mse: 14.2844 - val_mae: 1.6882 - lr: 2.6768e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 2s - loss: 16.0029 - mse: 16.0029 - mae: 1.6204 - val_loss: 14.3706 - val_mse: 14.3706 - val_mae: 1.7119 - lr: 2.6768e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 2s - loss: 15.9888 - mse: 15.9888 - mae: 1.6214 - val_loss: 14.3333 - val_mse: 14.3333 - val_mae: 1.6526 - lr: 2.6768e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 2s - loss: 15.9941 - mse: 15.9941 - mae: 1.6205 - val_loss: 14.3689 - val_mse: 14.3689 - val_mae: 1.6602 - lr: 2.6768e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 2s - loss: 15.9695 - mse: 15.9695 - mae: 1.6227 - val_loss: 14.3430 - val_mse: 14.3430 - val_mae: 1.6491 - lr: 2.6768e-04 - 2s/epoch - 2ms/step\n",
            "Score for fold 2: loss of 14.343010902404785\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 13.8128 - mse: 13.8128 - mae: 1.6246 - val_loss: 23.0516 - val_mse: 23.0516 - val_mae: 1.6501 - lr: 2.6768e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 13.8089 - mse: 13.8089 - mae: 1.6320 - val_loss: 23.0975 - val_mse: 23.0975 - val_mae: 1.6223 - lr: 2.6768e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 13.8391 - mse: 13.8391 - mae: 1.6303 - val_loss: 22.9684 - val_mse: 22.9684 - val_mae: 1.6601 - lr: 2.6768e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 13.8326 - mse: 13.8326 - mae: 1.6282 - val_loss: 23.0885 - val_mse: 23.0885 - val_mae: 1.6543 - lr: 2.6768e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 13.8279 - mse: 13.8279 - mae: 1.6279 - val_loss: 23.0462 - val_mse: 23.0462 - val_mae: 1.6429 - lr: 2.6768e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 13.8276 - mse: 13.8276 - mae: 1.6308 - val_loss: 23.0239 - val_mse: 23.0239 - val_mae: 1.6355 - lr: 2.6768e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 13.8413 - mse: 13.8413 - mae: 1.6295 - val_loss: 22.9759 - val_mse: 22.9759 - val_mae: 1.6423 - lr: 2.6768e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 2s - loss: 13.8236 - mse: 13.8236 - mae: 1.6288 - val_loss: 22.9984 - val_mse: 22.9984 - val_mae: 1.6551 - lr: 2.6768e-04 - 2s/epoch - 2ms/step\n",
            "Score for fold 3: loss of 22.998428344726562\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 15.9555 - mse: 15.9555 - mae: 1.6404 - val_loss: 14.3680 - val_mse: 14.3680 - val_mae: 1.6054 - lr: 2.6768e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 15.9663 - mse: 15.9663 - mae: 1.6427 - val_loss: 14.3864 - val_mse: 14.3864 - val_mae: 1.5751 - lr: 2.6768e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 15.9761 - mse: 15.9761 - mae: 1.6409 - val_loss: 14.3774 - val_mse: 14.3774 - val_mae: 1.5968 - lr: 2.6768e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 15.9631 - mse: 15.9631 - mae: 1.6447 - val_loss: 14.4202 - val_mse: 14.4202 - val_mae: 1.5706 - lr: 2.6768e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 15.9550 - mse: 15.9550 - mae: 1.6468 - val_loss: 14.4063 - val_mse: 14.4063 - val_mae: 1.5674 - lr: 2.6768e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 15.9525 - mse: 15.9525 - mae: 1.6438 - val_loss: 14.3835 - val_mse: 14.3835 - val_mae: 1.6365 - lr: 2.6768e-04 - 2s/epoch - 2ms/step\n",
            "Score for fold 4: loss of 14.383482933044434\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 15.9182 - mse: 15.9182 - mae: 1.6375 - val_loss: 14.6447 - val_mse: 14.6447 - val_mae: 1.5956 - lr: 2.6768e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 15.9049 - mse: 15.9049 - mae: 1.6348 - val_loss: 14.6115 - val_mse: 14.6115 - val_mae: 1.6060 - lr: 2.6768e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 15.9038 - mse: 15.9038 - mae: 1.6360 - val_loss: 14.5224 - val_mse: 14.5224 - val_mae: 1.6621 - lr: 2.6768e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 15.9057 - mse: 15.9057 - mae: 1.6365 - val_loss: 14.6943 - val_mse: 14.6943 - val_mae: 1.5990 - lr: 2.6768e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 15.9097 - mse: 15.9097 - mae: 1.6343 - val_loss: 14.5716 - val_mse: 14.5716 - val_mae: 1.6281 - lr: 2.6768e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 15.9169 - mse: 15.9169 - mae: 1.6356 - val_loss: 14.5274 - val_mse: 14.5274 - val_mae: 1.6495 - lr: 2.6768e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 15.9031 - mse: 15.9031 - mae: 1.6385 - val_loss: 14.5129 - val_mse: 14.5129 - val_mae: 1.6562 - lr: 2.6768e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 2s - loss: 15.8971 - mse: 15.8971 - mae: 1.6381 - val_loss: 14.5671 - val_mse: 14.5671 - val_mae: 1.6274 - lr: 2.6768e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 2s - loss: 15.9210 - mse: 15.9210 - mae: 1.6338 - val_loss: 14.5706 - val_mse: 14.5706 - val_mae: 1.6208 - lr: 2.6768e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 2s - loss: 15.9041 - mse: 15.9041 - mae: 1.6350 - val_loss: 14.5687 - val_mse: 14.5687 - val_mae: 1.6212 - lr: 2.6768e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 2s - loss: 15.8960 - mse: 15.8960 - mae: 1.6360 - val_loss: 14.5225 - val_mse: 14.5225 - val_mae: 1.6596 - lr: 2.6768e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 2s - loss: 15.9118 - mse: 15.9118 - mae: 1.6358 - val_loss: 14.5675 - val_mse: 14.5675 - val_mae: 1.6274 - lr: 2.6768e-04 - 2s/epoch - 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-10-14 14:35:26,736]\u001b[0m Finished trial#2 resulted in value: 15.64. Current best value is 14.828 with parameters: {'activation': 'tanh', 'num_hidden_layer': 4, 'i': 7, 'learning_rate': 0.0006163259286815976}.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Score for fold 5: loss of 14.567462921142578\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 3s - loss: 15.2753 - mse: 15.2753 - mae: 1.6211 - val_loss: 16.1529 - val_mse: 16.1529 - val_mae: 1.6363 - lr: 0.0080 - 3s/epoch - 3ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 14.9895 - mse: 14.9895 - mae: 1.5966 - val_loss: 16.1927 - val_mse: 16.1927 - val_mae: 1.6908 - lr: 0.0080 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 14.9340 - mse: 14.9340 - mae: 1.6040 - val_loss: 16.1284 - val_mse: 16.1284 - val_mae: 1.6789 - lr: 0.0080 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 14.9548 - mse: 14.9548 - mae: 1.6062 - val_loss: 16.5288 - val_mse: 16.5288 - val_mae: 1.6594 - lr: 0.0080 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 14.8675 - mse: 14.8675 - mae: 1.6091 - val_loss: 16.1175 - val_mse: 16.1175 - val_mae: 1.6692 - lr: 0.0080 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 14.9024 - mse: 14.9024 - mae: 1.6061 - val_loss: 16.2790 - val_mse: 16.2790 - val_mae: 1.6041 - lr: 0.0080 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 15.0073 - mse: 15.0073 - mae: 1.6105 - val_loss: 16.2315 - val_mse: 16.2315 - val_mae: 1.7277 - lr: 0.0080 - 2s/epoch - 2ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 2s - loss: 14.9147 - mse: 14.9147 - mae: 1.6055 - val_loss: 15.9191 - val_mse: 15.9191 - val_mae: 1.6832 - lr: 0.0080 - 2s/epoch - 2ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 2s - loss: 14.9001 - mse: 14.9001 - mae: 1.6033 - val_loss: 16.2972 - val_mse: 16.2972 - val_mae: 1.6643 - lr: 0.0080 - 2s/epoch - 2ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 2s - loss: 14.9545 - mse: 14.9545 - mae: 1.6123 - val_loss: 16.2190 - val_mse: 16.2190 - val_mae: 1.5966 - lr: 0.0080 - 2s/epoch - 2ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 2s - loss: 14.8483 - mse: 14.8483 - mae: 1.6096 - val_loss: 15.9673 - val_mse: 15.9673 - val_mae: 1.6094 - lr: 0.0080 - 2s/epoch - 2ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 2s - loss: 14.9046 - mse: 14.9046 - mae: 1.6097 - val_loss: 17.5154 - val_mse: 17.5154 - val_mae: 1.5654 - lr: 0.0080 - 2s/epoch - 2ms/step\n",
            "Epoch 13/100\n",
            "1000/1000 - 2s - loss: 14.9396 - mse: 14.9396 - mae: 1.6089 - val_loss: 15.9756 - val_mse: 15.9756 - val_mae: 1.6160 - lr: 0.0080 - 2s/epoch - 2ms/step\n",
            "Score for fold 1: loss of 15.97558879852295\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 16.3832 - mse: 16.3832 - mae: 1.6190 - val_loss: 9.6107 - val_mse: 9.6107 - val_mae: 1.5241 - lr: 0.0016 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 16.3420 - mse: 16.3420 - mae: 1.6173 - val_loss: 9.4979 - val_mse: 9.4979 - val_mae: 1.5323 - lr: 0.0016 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 16.4324 - mse: 16.4324 - mae: 1.6202 - val_loss: 9.5436 - val_mse: 9.5436 - val_mae: 1.5266 - lr: 0.0016 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 16.4538 - mse: 16.4538 - mae: 1.6144 - val_loss: 9.5068 - val_mse: 9.5068 - val_mae: 1.5279 - lr: 0.0016 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 16.4211 - mse: 16.4211 - mae: 1.6158 - val_loss: 9.5609 - val_mse: 9.5609 - val_mae: 1.5680 - lr: 0.0016 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 16.3919 - mse: 16.3919 - mae: 1.6150 - val_loss: 9.4742 - val_mse: 9.4742 - val_mae: 1.5951 - lr: 0.0016 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 16.4767 - mse: 16.4767 - mae: 1.6166 - val_loss: 9.4909 - val_mse: 9.4909 - val_mae: 1.5492 - lr: 0.0016 - 2s/epoch - 2ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 2s - loss: 16.4510 - mse: 16.4510 - mae: 1.6183 - val_loss: 9.8956 - val_mse: 9.8956 - val_mae: 1.5051 - lr: 0.0016 - 2s/epoch - 2ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 2s - loss: 16.4336 - mse: 16.4336 - mae: 1.6236 - val_loss: 9.4966 - val_mse: 9.4966 - val_mae: 1.6112 - lr: 0.0016 - 2s/epoch - 2ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 2s - loss: 16.3341 - mse: 16.3341 - mae: 1.6195 - val_loss: 9.5224 - val_mse: 9.5224 - val_mae: 1.5147 - lr: 0.0016 - 2s/epoch - 2ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 2s - loss: 16.3581 - mse: 16.3581 - mae: 1.6133 - val_loss: 9.6275 - val_mse: 9.6275 - val_mae: 1.5390 - lr: 0.0016 - 2s/epoch - 2ms/step\n",
            "Score for fold 2: loss of 9.627494812011719\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 13.1118 - mse: 13.1118 - mae: 1.6030 - val_loss: 22.6750 - val_mse: 22.6750 - val_mae: 1.5939 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 13.0810 - mse: 13.0810 - mae: 1.6020 - val_loss: 22.7301 - val_mse: 22.7301 - val_mae: 1.5931 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 13.1339 - mse: 13.1339 - mae: 1.6035 - val_loss: 22.6516 - val_mse: 22.6516 - val_mae: 1.6921 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 13.1542 - mse: 13.1542 - mae: 1.6035 - val_loss: 22.8097 - val_mse: 22.8097 - val_mae: 1.5944 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 13.1509 - mse: 13.1509 - mae: 1.6065 - val_loss: 22.6595 - val_mse: 22.6595 - val_mae: 1.6301 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 13.0584 - mse: 13.0584 - mae: 1.6053 - val_loss: 22.6035 - val_mse: 22.6035 - val_mae: 1.6293 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 13.1281 - mse: 13.1281 - mae: 1.5927 - val_loss: 22.8871 - val_mse: 22.8871 - val_mae: 1.5625 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 2s - loss: 13.1370 - mse: 13.1370 - mae: 1.5928 - val_loss: 22.6535 - val_mse: 22.6535 - val_mae: 1.5893 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 2s - loss: 13.1200 - mse: 13.1200 - mae: 1.5943 - val_loss: 22.9329 - val_mse: 22.9329 - val_mae: 1.5692 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 2s - loss: 13.0848 - mse: 13.0848 - mae: 1.5872 - val_loss: 22.6787 - val_mse: 22.6787 - val_mae: 1.6416 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 2s - loss: 13.1805 - mse: 13.1805 - mae: 1.5943 - val_loss: 22.9041 - val_mse: 22.9041 - val_mae: 1.5424 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Score for fold 3: loss of 22.904142379760742\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 15.3455 - mse: 15.3455 - mae: 1.5879 - val_loss: 13.4731 - val_mse: 13.4731 - val_mae: 1.5778 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 15.4247 - mse: 15.4247 - mae: 1.5874 - val_loss: 13.6074 - val_mse: 13.6074 - val_mae: 1.5600 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 15.3800 - mse: 15.3800 - mae: 1.5884 - val_loss: 13.4269 - val_mse: 13.4269 - val_mae: 1.5979 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 15.4069 - mse: 15.4069 - mae: 1.5850 - val_loss: 13.4062 - val_mse: 13.4062 - val_mae: 1.5826 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 15.3383 - mse: 15.3383 - mae: 1.5862 - val_loss: 14.0204 - val_mse: 14.0204 - val_mae: 1.5218 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 15.3837 - mse: 15.3837 - mae: 1.5802 - val_loss: 14.1062 - val_mse: 14.1062 - val_mae: 1.5399 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 15.4072 - mse: 15.4072 - mae: 1.5805 - val_loss: 13.2977 - val_mse: 13.2977 - val_mae: 1.6940 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 2s - loss: 15.3395 - mse: 15.3395 - mae: 1.5802 - val_loss: 13.3247 - val_mse: 13.3247 - val_mae: 1.6115 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 2s - loss: 15.3697 - mse: 15.3697 - mae: 1.5883 - val_loss: 13.3029 - val_mse: 13.3029 - val_mae: 1.7068 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 2s - loss: 15.3604 - mse: 15.3604 - mae: 1.5815 - val_loss: 13.3315 - val_mse: 13.3315 - val_mae: 1.6226 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 2s - loss: 15.3819 - mse: 15.3819 - mae: 1.5772 - val_loss: 13.6152 - val_mse: 13.6152 - val_mae: 1.5986 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 2s - loss: 15.3946 - mse: 15.3946 - mae: 1.5755 - val_loss: 13.8584 - val_mse: 13.8584 - val_mae: 1.5764 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Score for fold 4: loss of 13.858407020568848\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 15.4456 - mse: 15.4456 - mae: 1.5996 - val_loss: 13.2147 - val_mse: 13.2147 - val_mae: 1.5572 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 15.4376 - mse: 15.4376 - mae: 1.5968 - val_loss: 14.1213 - val_mse: 14.1213 - val_mae: 1.5023 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 15.4853 - mse: 15.4853 - mae: 1.5956 - val_loss: 13.1628 - val_mse: 13.1628 - val_mae: 1.5656 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 15.4642 - mse: 15.4642 - mae: 1.5937 - val_loss: 13.7118 - val_mse: 13.7118 - val_mae: 1.5111 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 15.4515 - mse: 15.4515 - mae: 1.5935 - val_loss: 13.4774 - val_mse: 13.4774 - val_mae: 1.5162 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 15.4530 - mse: 15.4530 - mae: 1.5947 - val_loss: 13.1088 - val_mse: 13.1088 - val_mae: 1.6141 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 15.4430 - mse: 15.4430 - mae: 1.5900 - val_loss: 13.3106 - val_mse: 13.3106 - val_mae: 1.5293 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 2s - loss: 15.3909 - mse: 15.3909 - mae: 1.5963 - val_loss: 13.4852 - val_mse: 13.4852 - val_mae: 1.5290 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 2s - loss: 15.4277 - mse: 15.4277 - mae: 1.5932 - val_loss: 13.8504 - val_mse: 13.8504 - val_mae: 1.5330 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 2s - loss: 15.4302 - mse: 15.4302 - mae: 1.5891 - val_loss: 13.2548 - val_mse: 13.2548 - val_mae: 1.5449 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 2s - loss: 15.3851 - mse: 15.3851 - mae: 1.5925 - val_loss: 13.2943 - val_mse: 13.2943 - val_mae: 1.5579 - lr: 0.0010 - 2s/epoch - 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-10-14 14:37:40,618]\u001b[0m Finished trial#3 resulted in value: 15.132. Current best value is 14.828 with parameters: {'activation': 'tanh', 'num_hidden_layer': 4, 'i': 7, 'learning_rate': 0.0006163259286815976}.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Score for fold 5: loss of 13.29426097869873\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 11s - loss: 16.7693 - mse: 16.7693 - mae: 1.7448 - val_loss: 14.1766 - val_mse: 14.1766 - val_mae: 1.8618 - lr: 0.0021 - 11s/epoch - 11ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 11s - loss: 16.6528 - mse: 16.6528 - mae: 1.7645 - val_loss: 14.7229 - val_mse: 14.7229 - val_mae: 2.1021 - lr: 0.0021 - 11s/epoch - 11ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 11s - loss: 16.4288 - mse: 16.4288 - mae: 1.7414 - val_loss: 16.6588 - val_mse: 16.6588 - val_mae: 2.6680 - lr: 0.0021 - 11s/epoch - 11ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 11s - loss: 16.3621 - mse: 16.3621 - mae: 1.7414 - val_loss: 17.6510 - val_mse: 17.6510 - val_mae: 2.0289 - lr: 0.0021 - 11s/epoch - 11ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 11s - loss: 16.3882 - mse: 16.3882 - mae: 1.7528 - val_loss: 14.2689 - val_mse: 14.2689 - val_mae: 1.6428 - lr: 0.0021 - 11s/epoch - 11ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 11s - loss: 16.3828 - mse: 16.3828 - mae: 1.7318 - val_loss: 13.7692 - val_mse: 13.7692 - val_mae: 1.6068 - lr: 0.0021 - 11s/epoch - 11ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 11s - loss: 16.2708 - mse: 16.2708 - mae: 1.7303 - val_loss: 14.5688 - val_mse: 14.5688 - val_mae: 1.5003 - lr: 0.0021 - 11s/epoch - 11ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 11s - loss: 16.2373 - mse: 16.2373 - mae: 1.7238 - val_loss: 14.4437 - val_mse: 14.4437 - val_mae: 2.1442 - lr: 0.0021 - 11s/epoch - 11ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 11s - loss: 16.2771 - mse: 16.2771 - mae: 1.7366 - val_loss: 14.2359 - val_mse: 14.2359 - val_mae: 1.9679 - lr: 0.0021 - 11s/epoch - 11ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 11s - loss: 16.3112 - mse: 16.3112 - mae: 1.7359 - val_loss: 14.0382 - val_mse: 14.0382 - val_mae: 1.5690 - lr: 0.0021 - 11s/epoch - 11ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 11s - loss: 16.3644 - mse: 16.3644 - mae: 1.7638 - val_loss: 13.9728 - val_mse: 13.9728 - val_mae: 1.8779 - lr: 0.0021 - 11s/epoch - 11ms/step\n",
            "Score for fold 1: loss of 13.972855567932129\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 11s - loss: 15.4107 - mse: 15.4107 - mae: 1.6695 - val_loss: 15.5825 - val_mse: 15.5825 - val_mae: 1.7367 - lr: 0.0010 - 11s/epoch - 11ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 11s - loss: 15.3765 - mse: 15.3765 - mae: 1.6633 - val_loss: 15.6008 - val_mse: 15.6008 - val_mae: 1.5113 - lr: 0.0010 - 11s/epoch - 11ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 11s - loss: 15.3656 - mse: 15.3656 - mae: 1.6831 - val_loss: 15.5336 - val_mse: 15.5336 - val_mae: 1.5535 - lr: 0.0010 - 11s/epoch - 11ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 11s - loss: 15.4090 - mse: 15.4090 - mae: 1.6660 - val_loss: 15.8503 - val_mse: 15.8503 - val_mae: 1.4497 - lr: 0.0010 - 11s/epoch - 11ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 11s - loss: 15.3479 - mse: 15.3479 - mae: 1.6749 - val_loss: 15.5945 - val_mse: 15.5945 - val_mae: 1.5736 - lr: 0.0010 - 11s/epoch - 11ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 11s - loss: 15.2246 - mse: 15.2246 - mae: 1.6696 - val_loss: 15.8164 - val_mse: 15.8164 - val_mae: 1.4815 - lr: 0.0010 - 11s/epoch - 11ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 11s - loss: 15.3459 - mse: 15.3459 - mae: 1.6885 - val_loss: 15.5187 - val_mse: 15.5187 - val_mae: 1.7184 - lr: 0.0010 - 11s/epoch - 11ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 11s - loss: 15.2032 - mse: 15.2032 - mae: 1.6781 - val_loss: 16.2000 - val_mse: 16.2000 - val_mae: 1.9760 - lr: 0.0010 - 11s/epoch - 11ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 11s - loss: 15.3589 - mse: 15.3589 - mae: 1.6731 - val_loss: 15.7874 - val_mse: 15.7874 - val_mae: 1.4740 - lr: 0.0010 - 11s/epoch - 11ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 11s - loss: 15.3214 - mse: 15.3214 - mae: 1.6876 - val_loss: 16.2085 - val_mse: 16.2085 - val_mae: 2.0710 - lr: 0.0010 - 11s/epoch - 11ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 11s - loss: 15.3834 - mse: 15.3834 - mae: 1.6895 - val_loss: 15.5608 - val_mse: 15.5608 - val_mae: 1.6176 - lr: 0.0010 - 11s/epoch - 11ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 11s - loss: 15.3648 - mse: 15.3648 - mae: 1.6794 - val_loss: 15.9424 - val_mse: 15.9424 - val_mae: 1.8883 - lr: 0.0010 - 11s/epoch - 11ms/step\n",
            "Score for fold 2: loss of 15.94236946105957\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 11s - loss: 16.6214 - mse: 16.6214 - mae: 1.6881 - val_loss: 10.5882 - val_mse: 10.5882 - val_mae: 1.6577 - lr: 0.0010 - 11s/epoch - 11ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 11s - loss: 16.5942 - mse: 16.5942 - mae: 1.6837 - val_loss: 10.5308 - val_mse: 10.5308 - val_mae: 1.6226 - lr: 0.0010 - 11s/epoch - 11ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 11s - loss: 16.4375 - mse: 16.4375 - mae: 1.6788 - val_loss: 10.5230 - val_mse: 10.5230 - val_mae: 1.6619 - lr: 0.0010 - 11s/epoch - 11ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 11s - loss: 16.4498 - mse: 16.4498 - mae: 1.6750 - val_loss: 10.7368 - val_mse: 10.7368 - val_mae: 1.7332 - lr: 0.0010 - 11s/epoch - 11ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 11s - loss: 16.4958 - mse: 16.4958 - mae: 1.6659 - val_loss: 10.9605 - val_mse: 10.9605 - val_mae: 1.8377 - lr: 0.0010 - 11s/epoch - 11ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 11s - loss: 16.6320 - mse: 16.6320 - mae: 1.6934 - val_loss: 10.9573 - val_mse: 10.9573 - val_mae: 1.5100 - lr: 0.0010 - 11s/epoch - 11ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 11s - loss: 16.5979 - mse: 16.5979 - mae: 1.6765 - val_loss: 10.6656 - val_mse: 10.6656 - val_mae: 1.4912 - lr: 0.0010 - 11s/epoch - 11ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 11s - loss: 16.5579 - mse: 16.5579 - mae: 1.6823 - val_loss: 11.2938 - val_mse: 11.2938 - val_mae: 2.0338 - lr: 0.0010 - 11s/epoch - 11ms/step\n",
            "Score for fold 3: loss of 11.2937650680542\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 11s - loss: 15.7257 - mse: 15.7257 - mae: 1.6551 - val_loss: 14.2046 - val_mse: 14.2046 - val_mae: 1.4825 - lr: 0.0010 - 11s/epoch - 11ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 11s - loss: 15.7270 - mse: 15.7270 - mae: 1.6662 - val_loss: 14.5392 - val_mse: 14.5392 - val_mae: 1.6236 - lr: 0.0010 - 11s/epoch - 11ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 11s - loss: 15.6836 - mse: 15.6836 - mae: 1.6698 - val_loss: 14.8666 - val_mse: 14.8666 - val_mae: 1.6972 - lr: 0.0010 - 11s/epoch - 11ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 11s - loss: 15.7441 - mse: 15.7441 - mae: 1.6679 - val_loss: 13.9340 - val_mse: 13.9340 - val_mae: 1.7235 - lr: 0.0010 - 11s/epoch - 11ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 11s - loss: 15.6833 - mse: 15.6833 - mae: 1.6699 - val_loss: 15.4619 - val_mse: 15.4619 - val_mae: 2.3361 - lr: 0.0010 - 11s/epoch - 11ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 11s - loss: 15.6647 - mse: 15.6647 - mae: 1.6678 - val_loss: 14.3484 - val_mse: 14.3484 - val_mae: 1.5150 - lr: 0.0010 - 11s/epoch - 11ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 12s - loss: 15.6812 - mse: 15.6812 - mae: 1.6611 - val_loss: 14.2943 - val_mse: 14.2943 - val_mae: 2.0679 - lr: 0.0010 - 12s/epoch - 12ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 11s - loss: 15.8063 - mse: 15.8063 - mae: 1.6907 - val_loss: 14.2902 - val_mse: 14.2902 - val_mae: 1.6741 - lr: 0.0010 - 11s/epoch - 11ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 11s - loss: 15.6545 - mse: 15.6545 - mae: 1.6698 - val_loss: 14.7724 - val_mse: 14.7724 - val_mae: 1.5014 - lr: 0.0010 - 11s/epoch - 11ms/step\n",
            "Score for fold 4: loss of 14.772430419921875\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 11s - loss: 13.7165 - mse: 13.7165 - mae: 1.6650 - val_loss: 22.3742 - val_mse: 22.3742 - val_mae: 1.7286 - lr: 0.0010 - 11s/epoch - 11ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 11s - loss: 13.5853 - mse: 13.5853 - mae: 1.6544 - val_loss: 22.4024 - val_mse: 22.4024 - val_mae: 1.5675 - lr: 0.0010 - 11s/epoch - 11ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 11s - loss: 13.6194 - mse: 13.6194 - mae: 1.6632 - val_loss: 22.2848 - val_mse: 22.2848 - val_mae: 1.6626 - lr: 0.0010 - 11s/epoch - 11ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 11s - loss: 13.4562 - mse: 13.4562 - mae: 1.6435 - val_loss: 22.3325 - val_mse: 22.3325 - val_mae: 1.6409 - lr: 0.0010 - 11s/epoch - 11ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 11s - loss: 13.5225 - mse: 13.5225 - mae: 1.6541 - val_loss: 22.4060 - val_mse: 22.4060 - val_mae: 1.7660 - lr: 0.0010 - 11s/epoch - 11ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 11s - loss: 13.6207 - mse: 13.6207 - mae: 1.6578 - val_loss: 22.5067 - val_mse: 22.5067 - val_mae: 1.5990 - lr: 0.0010 - 11s/epoch - 11ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 11s - loss: 13.5036 - mse: 13.5036 - mae: 1.6717 - val_loss: 22.3316 - val_mse: 22.3316 - val_mae: 1.6630 - lr: 0.0010 - 11s/epoch - 11ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 11s - loss: 13.6241 - mse: 13.6241 - mae: 1.6606 - val_loss: 22.2793 - val_mse: 22.2793 - val_mae: 1.7286 - lr: 0.0010 - 11s/epoch - 11ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 11s - loss: 13.6401 - mse: 13.6401 - mae: 1.6719 - val_loss: 22.4942 - val_mse: 22.4942 - val_mae: 1.5578 - lr: 0.0010 - 11s/epoch - 11ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 11s - loss: 13.5986 - mse: 13.5986 - mae: 1.6613 - val_loss: 22.7587 - val_mse: 22.7587 - val_mae: 1.6420 - lr: 0.0010 - 11s/epoch - 11ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 11s - loss: 13.6601 - mse: 13.6601 - mae: 1.6604 - val_loss: 22.5255 - val_mse: 22.5255 - val_mae: 1.8860 - lr: 0.0010 - 11s/epoch - 11ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 12s - loss: 13.5914 - mse: 13.5914 - mae: 1.6641 - val_loss: 22.2295 - val_mse: 22.2295 - val_mae: 1.6353 - lr: 0.0010 - 12s/epoch - 12ms/step\n",
            "Epoch 13/100\n",
            "1000/1000 - 11s - loss: 13.5583 - mse: 13.5583 - mae: 1.6604 - val_loss: 22.9604 - val_mse: 22.9604 - val_mae: 1.5216 - lr: 0.0010 - 11s/epoch - 11ms/step\n",
            "Epoch 14/100\n",
            "1000/1000 - 11s - loss: 13.5609 - mse: 13.5609 - mae: 1.6740 - val_loss: 24.3746 - val_mse: 24.3746 - val_mae: 2.5211 - lr: 0.0010 - 11s/epoch - 11ms/step\n",
            "Epoch 15/100\n",
            "1000/1000 - 11s - loss: 13.5886 - mse: 13.5886 - mae: 1.6449 - val_loss: 22.6193 - val_mse: 22.6193 - val_mae: 1.9661 - lr: 0.0010 - 11s/epoch - 11ms/step\n",
            "Epoch 16/100\n",
            "1000/1000 - 11s - loss: 13.5633 - mse: 13.5633 - mae: 1.6649 - val_loss: 22.1543 - val_mse: 22.1543 - val_mae: 1.6667 - lr: 0.0010 - 11s/epoch - 11ms/step\n",
            "Epoch 17/100\n",
            "1000/1000 - 11s - loss: 13.5708 - mse: 13.5708 - mae: 1.6548 - val_loss: 22.9762 - val_mse: 22.9762 - val_mae: 1.5236 - lr: 0.0010 - 11s/epoch - 11ms/step\n",
            "Epoch 18/100\n",
            "1000/1000 - 11s - loss: 13.5120 - mse: 13.5120 - mae: 1.6464 - val_loss: 22.3441 - val_mse: 22.3441 - val_mae: 1.7408 - lr: 0.0010 - 11s/epoch - 11ms/step\n",
            "Epoch 19/100\n",
            "1000/1000 - 11s - loss: 13.5381 - mse: 13.5381 - mae: 1.6466 - val_loss: 22.5033 - val_mse: 22.5033 - val_mae: 1.5986 - lr: 0.0010 - 11s/epoch - 11ms/step\n",
            "Epoch 20/100\n",
            "1000/1000 - 11s - loss: 13.5442 - mse: 13.5442 - mae: 1.6536 - val_loss: 22.6629 - val_mse: 22.6629 - val_mae: 1.5158 - lr: 0.0010 - 11s/epoch - 11ms/step\n",
            "Epoch 21/100\n",
            "1000/1000 - 11s - loss: 13.6309 - mse: 13.6309 - mae: 1.6763 - val_loss: 22.6204 - val_mse: 22.6204 - val_mae: 1.9293 - lr: 0.0010 - 11s/epoch - 11ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-10-14 14:49:10,768]\u001b[0m Finished trial#4 resulted in value: 15.718. Current best value is 14.828 with parameters: {'activation': 'tanh', 'num_hidden_layer': 4, 'i': 7, 'learning_rate': 0.0006163259286815976}.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Score for fold 5: loss of 22.620431900024414\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 4s - loss: 14.9270 - mse: 14.9270 - mae: 1.6607 - val_loss: 19.6484 - val_mse: 19.6484 - val_mae: 1.6597 - lr: 2.6898e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 4s - loss: 14.4074 - mse: 14.4074 - mae: 1.6056 - val_loss: 19.7578 - val_mse: 19.7578 - val_mae: 1.6422 - lr: 2.6898e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 4s - loss: 14.3418 - mse: 14.3418 - mae: 1.6011 - val_loss: 19.4273 - val_mse: 19.4273 - val_mae: 1.6748 - lr: 2.6898e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 4s - loss: 14.2943 - mse: 14.2943 - mae: 1.5939 - val_loss: 19.5850 - val_mse: 19.5850 - val_mae: 1.6946 - lr: 2.6898e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 4s - loss: 14.2393 - mse: 14.2393 - mae: 1.5889 - val_loss: 19.5748 - val_mse: 19.5748 - val_mae: 1.6802 - lr: 2.6898e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 4s - loss: 14.2242 - mse: 14.2242 - mae: 1.5905 - val_loss: 19.6808 - val_mse: 19.6808 - val_mae: 1.6118 - lr: 2.6898e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 4s - loss: 14.2313 - mse: 14.2313 - mae: 1.5843 - val_loss: 19.3617 - val_mse: 19.3617 - val_mae: 1.7271 - lr: 2.6898e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 4s - loss: 14.2186 - mse: 14.2186 - mae: 1.5856 - val_loss: 19.5965 - val_mse: 19.5965 - val_mae: 1.6126 - lr: 2.6898e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 4s - loss: 14.2131 - mse: 14.2131 - mae: 1.5857 - val_loss: 19.4463 - val_mse: 19.4463 - val_mae: 1.6490 - lr: 2.6898e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 4s - loss: 14.1750 - mse: 14.1750 - mae: 1.5797 - val_loss: 19.3509 - val_mse: 19.3509 - val_mae: 1.6658 - lr: 2.6898e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 4s - loss: 14.1817 - mse: 14.1817 - mae: 1.5795 - val_loss: 19.3224 - val_mse: 19.3224 - val_mae: 1.6397 - lr: 2.6898e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 4s - loss: 14.1661 - mse: 14.1661 - mae: 1.5797 - val_loss: 19.3429 - val_mse: 19.3429 - val_mae: 1.6240 - lr: 2.6898e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 13/100\n",
            "1000/1000 - 4s - loss: 14.1519 - mse: 14.1519 - mae: 1.5823 - val_loss: 19.4503 - val_mse: 19.4503 - val_mae: 1.6321 - lr: 2.6898e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 14/100\n",
            "1000/1000 - 4s - loss: 14.1445 - mse: 14.1445 - mae: 1.5776 - val_loss: 19.3989 - val_mse: 19.3989 - val_mae: 1.6150 - lr: 2.6898e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 15/100\n",
            "1000/1000 - 4s - loss: 14.1294 - mse: 14.1294 - mae: 1.5779 - val_loss: 19.2863 - val_mse: 19.2863 - val_mae: 1.6475 - lr: 2.6898e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 16/100\n",
            "1000/1000 - 4s - loss: 14.1166 - mse: 14.1166 - mae: 1.5781 - val_loss: 19.5254 - val_mse: 19.5254 - val_mae: 1.6271 - lr: 2.6898e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 17/100\n",
            "1000/1000 - 4s - loss: 14.0815 - mse: 14.0815 - mae: 1.5798 - val_loss: 19.2469 - val_mse: 19.2469 - val_mae: 1.6935 - lr: 2.6898e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 18/100\n",
            "1000/1000 - 4s - loss: 14.0891 - mse: 14.0891 - mae: 1.5743 - val_loss: 19.3830 - val_mse: 19.3830 - val_mae: 1.6838 - lr: 2.6898e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 19/100\n",
            "1000/1000 - 4s - loss: 14.0934 - mse: 14.0934 - mae: 1.5758 - val_loss: 19.3394 - val_mse: 19.3394 - val_mae: 1.6642 - lr: 2.6898e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 20/100\n",
            "1000/1000 - 4s - loss: 14.0894 - mse: 14.0894 - mae: 1.5737 - val_loss: 19.4086 - val_mse: 19.4086 - val_mae: 1.6239 - lr: 2.6898e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 21/100\n",
            "1000/1000 - 4s - loss: 14.0595 - mse: 14.0595 - mae: 1.5717 - val_loss: 19.3303 - val_mse: 19.3303 - val_mae: 1.6115 - lr: 2.6898e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 22/100\n",
            "1000/1000 - 4s - loss: 14.0414 - mse: 14.0414 - mae: 1.5751 - val_loss: 19.3157 - val_mse: 19.3157 - val_mae: 1.7589 - lr: 2.6898e-04 - 4s/epoch - 4ms/step\n",
            "Score for fold 1: loss of 19.315698623657227\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 4s - loss: 15.9918 - mse: 15.9918 - mae: 1.5888 - val_loss: 11.5336 - val_mse: 11.5336 - val_mae: 1.5786 - lr: 2.6898e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 4s - loss: 15.9608 - mse: 15.9608 - mae: 1.5869 - val_loss: 11.7873 - val_mse: 11.7873 - val_mae: 1.5795 - lr: 2.6898e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 4s - loss: 15.9642 - mse: 15.9642 - mae: 1.5867 - val_loss: 11.7120 - val_mse: 11.7120 - val_mae: 1.5938 - lr: 2.6898e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 4s - loss: 15.9586 - mse: 15.9586 - mae: 1.5892 - val_loss: 11.7080 - val_mse: 11.7080 - val_mae: 1.6236 - lr: 2.6898e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 4s - loss: 15.9417 - mse: 15.9417 - mae: 1.5883 - val_loss: 11.6767 - val_mse: 11.6767 - val_mae: 1.5349 - lr: 2.6898e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 4s - loss: 15.8975 - mse: 15.8975 - mae: 1.5824 - val_loss: 11.4143 - val_mse: 11.4143 - val_mae: 1.6197 - lr: 2.6898e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 4s - loss: 15.9056 - mse: 15.9056 - mae: 1.5817 - val_loss: 11.6121 - val_mse: 11.6121 - val_mae: 1.6219 - lr: 2.6898e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 4s - loss: 15.8554 - mse: 15.8554 - mae: 1.5829 - val_loss: 11.6021 - val_mse: 11.6021 - val_mae: 1.5671 - lr: 2.6898e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 4s - loss: 15.8911 - mse: 15.8911 - mae: 1.5796 - val_loss: 11.6532 - val_mse: 11.6532 - val_mae: 1.5509 - lr: 2.6898e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 4s - loss: 15.8714 - mse: 15.8714 - mae: 1.5813 - val_loss: 11.7033 - val_mse: 11.7033 - val_mae: 1.5632 - lr: 2.6898e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 4s - loss: 15.8468 - mse: 15.8468 - mae: 1.5801 - val_loss: 11.5557 - val_mse: 11.5557 - val_mae: 1.5941 - lr: 2.6898e-04 - 4s/epoch - 4ms/step\n",
            "Score for fold 2: loss of 11.555731773376465\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 4s - loss: 16.0096 - mse: 16.0096 - mae: 1.5977 - val_loss: 10.8727 - val_mse: 10.8727 - val_mae: 1.4965 - lr: 2.6898e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 4s - loss: 15.9816 - mse: 15.9816 - mae: 1.5944 - val_loss: 10.8153 - val_mse: 10.8153 - val_mae: 1.5484 - lr: 2.6898e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 4s - loss: 15.9444 - mse: 15.9444 - mae: 1.5951 - val_loss: 10.8532 - val_mse: 10.8532 - val_mae: 1.5122 - lr: 2.6898e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 4s - loss: 15.9103 - mse: 15.9103 - mae: 1.5909 - val_loss: 10.8383 - val_mse: 10.8383 - val_mae: 1.5511 - lr: 2.6898e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 4s - loss: 15.9204 - mse: 15.9204 - mae: 1.5940 - val_loss: 11.0270 - val_mse: 11.0270 - val_mae: 1.6087 - lr: 2.6898e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 4s - loss: 15.9220 - mse: 15.9220 - mae: 1.5951 - val_loss: 10.8765 - val_mse: 10.8765 - val_mae: 1.5302 - lr: 2.6898e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 4s - loss: 15.9139 - mse: 15.9139 - mae: 1.5912 - val_loss: 10.8526 - val_mse: 10.8526 - val_mae: 1.5469 - lr: 2.6898e-04 - 4s/epoch - 4ms/step\n",
            "Score for fold 3: loss of 10.852617263793945\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 4s - loss: 15.6315 - mse: 15.6315 - mae: 1.5854 - val_loss: 11.8694 - val_mse: 11.8694 - val_mae: 1.5936 - lr: 2.6898e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 4s - loss: 15.5972 - mse: 15.5972 - mae: 1.5870 - val_loss: 11.8926 - val_mse: 11.8926 - val_mae: 1.5212 - lr: 2.6898e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 4s - loss: 15.5782 - mse: 15.5782 - mae: 1.5820 - val_loss: 11.9710 - val_mse: 11.9710 - val_mae: 1.5933 - lr: 2.6898e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 4s - loss: 15.5772 - mse: 15.5772 - mae: 1.5793 - val_loss: 12.1652 - val_mse: 12.1652 - val_mae: 1.5736 - lr: 2.6898e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 4s - loss: 15.5515 - mse: 15.5515 - mae: 1.5811 - val_loss: 12.0044 - val_mse: 12.0044 - val_mae: 1.5564 - lr: 2.6898e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 4s - loss: 15.5292 - mse: 15.5292 - mae: 1.5799 - val_loss: 11.9603 - val_mse: 11.9603 - val_mae: 1.5836 - lr: 2.6898e-04 - 4s/epoch - 4ms/step\n",
            "Score for fold 4: loss of 11.960288047790527\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 4s - loss: 13.2481 - mse: 13.2481 - mae: 1.5749 - val_loss: 21.3525 - val_mse: 21.3525 - val_mae: 1.6091 - lr: 2.6898e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 4s - loss: 13.2270 - mse: 13.2270 - mae: 1.5749 - val_loss: 21.1597 - val_mse: 21.1597 - val_mae: 1.5991 - lr: 2.6898e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 4s - loss: 13.2156 - mse: 13.2156 - mae: 1.5727 - val_loss: 21.1648 - val_mse: 21.1648 - val_mae: 1.5901 - lr: 2.6898e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 4s - loss: 13.1885 - mse: 13.1885 - mae: 1.5702 - val_loss: 21.2786 - val_mse: 21.2786 - val_mae: 1.5813 - lr: 2.6898e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 4s - loss: 13.1900 - mse: 13.1900 - mae: 1.5702 - val_loss: 21.2589 - val_mse: 21.2589 - val_mae: 1.6207 - lr: 2.6898e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 4s - loss: 13.1569 - mse: 13.1569 - mae: 1.5718 - val_loss: 21.3146 - val_mse: 21.3146 - val_mae: 1.5839 - lr: 2.6898e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 4s - loss: 13.1416 - mse: 13.1416 - mae: 1.5685 - val_loss: 21.2522 - val_mse: 21.2522 - val_mae: 1.6025 - lr: 2.6898e-04 - 4s/epoch - 4ms/step\n",
            "Score for fold 5: loss of 21.25220489501953\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-10-14 14:52:44,444]\u001b[0m Finished trial#5 resulted in value: 14.988. Current best value is 14.828 with parameters: {'activation': 'tanh', 'num_hidden_layer': 4, 'i': 7, 'learning_rate': 0.0006163259286815976}.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 3s - loss: 17.4537 - mse: 17.4537 - mae: 1.6983 - val_loss: 10.6852 - val_mse: 10.6852 - val_mae: 1.4742 - lr: 0.0025 - 3s/epoch - 3ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 17.1158 - mse: 17.1158 - mae: 1.6651 - val_loss: 10.2867 - val_mse: 10.2867 - val_mae: 1.6135 - lr: 0.0025 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 17.1787 - mse: 17.1787 - mae: 1.6640 - val_loss: 10.2868 - val_mse: 10.2868 - val_mae: 1.6135 - lr: 0.0025 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 3s - loss: 17.0833 - mse: 17.0833 - mae: 1.6626 - val_loss: 10.2897 - val_mse: 10.2897 - val_mae: 1.5894 - lr: 0.0025 - 3s/epoch - 3ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 3s - loss: 17.0566 - mse: 17.0566 - mae: 1.6597 - val_loss: 10.3132 - val_mse: 10.3132 - val_mae: 1.6642 - lr: 0.0025 - 3s/epoch - 3ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 3s - loss: 17.0572 - mse: 17.0572 - mae: 1.6643 - val_loss: 12.4375 - val_mse: 12.4375 - val_mae: 1.5296 - lr: 0.0025 - 3s/epoch - 3ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 3s - loss: 17.1270 - mse: 17.1270 - mae: 1.6662 - val_loss: 10.2863 - val_mse: 10.2863 - val_mae: 1.5335 - lr: 0.0025 - 3s/epoch - 3ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 3s - loss: 17.0672 - mse: 17.0672 - mae: 1.6664 - val_loss: 10.2828 - val_mse: 10.2828 - val_mae: 1.6283 - lr: 0.0025 - 3s/epoch - 3ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 3s - loss: 16.9638 - mse: 16.9638 - mae: 1.6680 - val_loss: 10.3042 - val_mse: 10.3042 - val_mae: 1.6065 - lr: 0.0025 - 3s/epoch - 3ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 3s - loss: 17.0480 - mse: 17.0480 - mae: 1.6616 - val_loss: 10.7075 - val_mse: 10.7075 - val_mae: 1.8118 - lr: 0.0025 - 3s/epoch - 3ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 3s - loss: 17.0628 - mse: 17.0628 - mae: 1.6674 - val_loss: 10.4286 - val_mse: 10.4286 - val_mae: 1.5136 - lr: 0.0025 - 3s/epoch - 3ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 3s - loss: 17.0493 - mse: 17.0493 - mae: 1.6540 - val_loss: 10.3393 - val_mse: 10.3393 - val_mae: 1.6340 - lr: 0.0025 - 3s/epoch - 3ms/step\n",
            "Epoch 13/100\n",
            "1000/1000 - 3s - loss: 17.0777 - mse: 17.0777 - mae: 1.6678 - val_loss: 10.2924 - val_mse: 10.2924 - val_mae: 1.6349 - lr: 0.0025 - 3s/epoch - 3ms/step\n",
            "Score for fold 1: loss of 10.292387962341309\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 16.7173 - mse: 16.7173 - mae: 1.6383 - val_loss: 11.3561 - val_mse: 11.3561 - val_mae: 1.7160 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 16.7304 - mse: 16.7304 - mae: 1.6350 - val_loss: 11.3657 - val_mse: 11.3657 - val_mae: 1.6802 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 16.7775 - mse: 16.7775 - mae: 1.6402 - val_loss: 11.9557 - val_mse: 11.9557 - val_mae: 1.5448 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 16.7729 - mse: 16.7729 - mae: 1.6361 - val_loss: 11.7379 - val_mse: 11.7379 - val_mae: 1.5869 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 16.7783 - mse: 16.7783 - mae: 1.6467 - val_loss: 11.3648 - val_mse: 11.3648 - val_mae: 1.6532 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 16.7973 - mse: 16.7973 - mae: 1.6388 - val_loss: 11.4619 - val_mse: 11.4619 - val_mae: 1.5983 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Score for fold 2: loss of 11.461864471435547\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 15.6860 - mse: 15.6860 - mae: 1.6407 - val_loss: 15.8901 - val_mse: 15.8901 - val_mae: 1.5816 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 15.6586 - mse: 15.6586 - mae: 1.6480 - val_loss: 15.7220 - val_mse: 15.7220 - val_mae: 1.6214 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 15.6974 - mse: 15.6974 - mae: 1.6407 - val_loss: 15.6610 - val_mse: 15.6610 - val_mae: 1.7255 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 15.7145 - mse: 15.7145 - mae: 1.6485 - val_loss: 15.7087 - val_mse: 15.7087 - val_mae: 1.7618 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 3s - loss: 15.6812 - mse: 15.6812 - mae: 1.6519 - val_loss: 15.7044 - val_mse: 15.7044 - val_mae: 1.6260 - lr: 0.0010 - 3s/epoch - 3ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 15.7239 - mse: 15.7239 - mae: 1.6479 - val_loss: 15.6641 - val_mse: 15.6641 - val_mae: 1.7410 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 3s - loss: 15.7042 - mse: 15.7042 - mae: 1.6439 - val_loss: 15.7652 - val_mse: 15.7652 - val_mae: 1.7784 - lr: 0.0010 - 3s/epoch - 3ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 3s - loss: 15.7015 - mse: 15.7015 - mae: 1.6508 - val_loss: 15.8119 - val_mse: 15.8119 - val_mae: 1.6012 - lr: 0.0010 - 3s/epoch - 3ms/step\n",
            "Score for fold 3: loss of 15.81193733215332\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 15.0272 - mse: 15.0272 - mae: 1.6432 - val_loss: 18.3502 - val_mse: 18.3502 - val_mae: 1.7165 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 3s - loss: 15.1655 - mse: 15.1655 - mae: 1.6434 - val_loss: 18.3574 - val_mse: 18.3574 - val_mae: 1.7213 - lr: 0.0010 - 3s/epoch - 3ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 3s - loss: 15.0258 - mse: 15.0258 - mae: 1.6396 - val_loss: 18.4865 - val_mse: 18.4865 - val_mae: 1.6360 - lr: 0.0010 - 3s/epoch - 3ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 3s - loss: 15.1062 - mse: 15.1062 - mae: 1.6375 - val_loss: 18.3561 - val_mse: 18.3561 - val_mae: 1.7018 - lr: 0.0010 - 3s/epoch - 3ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 3s - loss: 15.1385 - mse: 15.1385 - mae: 1.6419 - val_loss: 18.7723 - val_mse: 18.7723 - val_mae: 1.5995 - lr: 0.0010 - 3s/epoch - 3ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 3s - loss: 15.0387 - mse: 15.0387 - mae: 1.6357 - val_loss: 18.5422 - val_mse: 18.5422 - val_mae: 1.6394 - lr: 0.0010 - 3s/epoch - 3ms/step\n",
            "Score for fold 4: loss of 18.542198181152344\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 14.1003 - mse: 14.1003 - mae: 1.6422 - val_loss: 22.4183 - val_mse: 22.4183 - val_mae: 1.7507 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 14.0233 - mse: 14.0233 - mae: 1.6383 - val_loss: 22.2853 - val_mse: 22.2853 - val_mae: 1.6881 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 14.1288 - mse: 14.1288 - mae: 1.6336 - val_loss: 22.3132 - val_mse: 22.3132 - val_mae: 1.6689 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 3s - loss: 14.1277 - mse: 14.1277 - mae: 1.6383 - val_loss: 22.3387 - val_mse: 22.3387 - val_mae: 1.6503 - lr: 0.0010 - 3s/epoch - 3ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 3s - loss: 14.1352 - mse: 14.1352 - mae: 1.6441 - val_loss: 22.5360 - val_mse: 22.5360 - val_mae: 1.7569 - lr: 0.0010 - 3s/epoch - 3ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 3s - loss: 14.2076 - mse: 14.2076 - mae: 1.6277 - val_loss: 22.2850 - val_mse: 22.2850 - val_mae: 1.6795 - lr: 0.0010 - 3s/epoch - 3ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 3s - loss: 14.0839 - mse: 14.0839 - mae: 1.6349 - val_loss: 22.3182 - val_mse: 22.3182 - val_mae: 1.6676 - lr: 0.0010 - 3s/epoch - 3ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 3s - loss: 14.1258 - mse: 14.1258 - mae: 1.6402 - val_loss: 22.3094 - val_mse: 22.3094 - val_mae: 1.7036 - lr: 0.0010 - 3s/epoch - 3ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 3s - loss: 14.1215 - mse: 14.1215 - mae: 1.6324 - val_loss: 22.3015 - val_mse: 22.3015 - val_mae: 1.7054 - lr: 0.0010 - 3s/epoch - 3ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 3s - loss: 14.1440 - mse: 14.1440 - mae: 1.6391 - val_loss: 22.3541 - val_mse: 22.3541 - val_mae: 1.7208 - lr: 0.0010 - 3s/epoch - 3ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 3s - loss: 14.1047 - mse: 14.1047 - mae: 1.6460 - val_loss: 22.3776 - val_mse: 22.3776 - val_mae: 1.7335 - lr: 0.0010 - 3s/epoch - 3ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-10-14 14:54:39,405]\u001b[0m Finished trial#6 resulted in value: 15.696000000000002. Current best value is 14.828 with parameters: {'activation': 'tanh', 'num_hidden_layer': 4, 'i': 7, 'learning_rate': 0.0006163259286815976}.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Score for fold 5: loss of 22.377647399902344\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 4s - loss: 16.6010 - mse: 16.6010 - mae: 1.6486 - val_loss: 10.6019 - val_mse: 10.6019 - val_mae: 1.5024 - lr: 0.0026 - 4s/epoch - 4ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 4s - loss: 16.3349 - mse: 16.3349 - mae: 1.6293 - val_loss: 10.5475 - val_mse: 10.5475 - val_mae: 1.4868 - lr: 0.0026 - 4s/epoch - 4ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 4s - loss: 16.3385 - mse: 16.3385 - mae: 1.6164 - val_loss: 10.6961 - val_mse: 10.6961 - val_mae: 1.6836 - lr: 0.0026 - 4s/epoch - 4ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 4s - loss: 16.2990 - mse: 16.2990 - mae: 1.6133 - val_loss: 10.4325 - val_mse: 10.4325 - val_mae: 1.5548 - lr: 0.0026 - 4s/epoch - 4ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 4s - loss: 16.2421 - mse: 16.2421 - mae: 1.6104 - val_loss: 10.4527 - val_mse: 10.4527 - val_mae: 1.5910 - lr: 0.0026 - 4s/epoch - 4ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 4s - loss: 16.2380 - mse: 16.2380 - mae: 1.6114 - val_loss: 10.6485 - val_mse: 10.6485 - val_mae: 1.4918 - lr: 0.0026 - 4s/epoch - 4ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 4s - loss: 16.1561 - mse: 16.1561 - mae: 1.6095 - val_loss: 10.4968 - val_mse: 10.4968 - val_mae: 1.4844 - lr: 0.0026 - 4s/epoch - 4ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 4s - loss: 16.1933 - mse: 16.1933 - mae: 1.6082 - val_loss: 10.3394 - val_mse: 10.3394 - val_mae: 1.5035 - lr: 0.0026 - 4s/epoch - 4ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 4s - loss: 16.1751 - mse: 16.1751 - mae: 1.6034 - val_loss: 10.3860 - val_mse: 10.3860 - val_mae: 1.5360 - lr: 0.0026 - 4s/epoch - 4ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 4s - loss: 16.0635 - mse: 16.0635 - mae: 1.6040 - val_loss: 10.4302 - val_mse: 10.4302 - val_mae: 1.5538 - lr: 0.0026 - 4s/epoch - 4ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 4s - loss: 16.1313 - mse: 16.1313 - mae: 1.5958 - val_loss: 10.4067 - val_mse: 10.4067 - val_mae: 1.6271 - lr: 0.0026 - 4s/epoch - 4ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 4s - loss: 16.0213 - mse: 16.0213 - mae: 1.5970 - val_loss: 10.3569 - val_mse: 10.3569 - val_mae: 1.5069 - lr: 0.0026 - 4s/epoch - 4ms/step\n",
            "Epoch 13/100\n",
            "1000/1000 - 4s - loss: 15.9952 - mse: 15.9952 - mae: 1.5964 - val_loss: 10.3925 - val_mse: 10.3925 - val_mae: 1.4617 - lr: 0.0026 - 4s/epoch - 4ms/step\n",
            "Score for fold 1: loss of 10.392516136169434\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 4s - loss: 14.4472 - mse: 14.4472 - mae: 1.5545 - val_loss: 15.9388 - val_mse: 15.9388 - val_mae: 1.5721 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 4s - loss: 14.3964 - mse: 14.3964 - mae: 1.5536 - val_loss: 15.9953 - val_mse: 15.9953 - val_mae: 1.6377 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 4s - loss: 14.3817 - mse: 14.3817 - mae: 1.5580 - val_loss: 16.0858 - val_mse: 16.0858 - val_mae: 1.5153 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 4s - loss: 14.3662 - mse: 14.3662 - mae: 1.5521 - val_loss: 16.3078 - val_mse: 16.3078 - val_mae: 1.5805 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 4s - loss: 14.3467 - mse: 14.3467 - mae: 1.5497 - val_loss: 16.1093 - val_mse: 16.1093 - val_mae: 1.6349 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 4s - loss: 14.2874 - mse: 14.2874 - mae: 1.5514 - val_loss: 16.1832 - val_mse: 16.1832 - val_mae: 1.6449 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Score for fold 2: loss of 16.183197021484375\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 4s - loss: 15.4328 - mse: 15.4328 - mae: 1.5709 - val_loss: 11.6564 - val_mse: 11.6564 - val_mae: 1.5022 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 4s - loss: 15.3864 - mse: 15.3864 - mae: 1.5641 - val_loss: 11.6158 - val_mse: 11.6158 - val_mae: 1.5205 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 4s - loss: 15.3937 - mse: 15.3937 - mae: 1.5647 - val_loss: 11.4417 - val_mse: 11.4417 - val_mae: 1.5622 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 4s - loss: 15.3289 - mse: 15.3289 - mae: 1.5666 - val_loss: 11.6467 - val_mse: 11.6467 - val_mae: 1.5720 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 4s - loss: 15.2931 - mse: 15.2931 - mae: 1.5625 - val_loss: 11.7337 - val_mse: 11.7337 - val_mae: 1.5133 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 4s - loss: 15.2603 - mse: 15.2603 - mae: 1.5634 - val_loss: 11.8114 - val_mse: 11.8114 - val_mae: 1.5453 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 4s - loss: 15.3120 - mse: 15.3120 - mae: 1.5610 - val_loss: 11.7146 - val_mse: 11.7146 - val_mae: 1.5319 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 4s - loss: 15.2493 - mse: 15.2493 - mae: 1.5596 - val_loss: 11.8334 - val_mse: 11.8334 - val_mae: 1.5519 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Score for fold 3: loss of 11.833429336547852\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 4s - loss: 12.6523 - mse: 12.6523 - mae: 1.5385 - val_loss: 22.7898 - val_mse: 22.7898 - val_mae: 1.5693 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 4s - loss: 12.6113 - mse: 12.6113 - mae: 1.5353 - val_loss: 22.5374 - val_mse: 22.5374 - val_mae: 1.6113 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 4s - loss: 12.5844 - mse: 12.5844 - mae: 1.5353 - val_loss: 22.7788 - val_mse: 22.7788 - val_mae: 1.5916 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 4s - loss: 12.5441 - mse: 12.5441 - mae: 1.5338 - val_loss: 22.6126 - val_mse: 22.6126 - val_mae: 1.6212 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 4s - loss: 12.4578 - mse: 12.4578 - mae: 1.5370 - val_loss: 22.8204 - val_mse: 22.8204 - val_mae: 1.6312 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 4s - loss: 12.4639 - mse: 12.4639 - mae: 1.5302 - val_loss: 22.7364 - val_mse: 22.7364 - val_mae: 1.6319 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 4s - loss: 12.4813 - mse: 12.4813 - mae: 1.5273 - val_loss: 22.7527 - val_mse: 22.7527 - val_mae: 1.6320 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Score for fold 4: loss of 22.752674102783203\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 4s - loss: 14.9473 - mse: 14.9473 - mae: 1.5615 - val_loss: 12.6315 - val_mse: 12.6315 - val_mae: 1.6025 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 4s - loss: 14.9652 - mse: 14.9652 - mae: 1.5577 - val_loss: 12.6090 - val_mse: 12.6090 - val_mae: 1.5610 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 4s - loss: 14.8336 - mse: 14.8336 - mae: 1.5586 - val_loss: 12.6331 - val_mse: 12.6331 - val_mae: 1.5034 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 4s - loss: 14.7357 - mse: 14.7357 - mae: 1.5584 - val_loss: 12.7401 - val_mse: 12.7401 - val_mae: 1.4965 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 4s - loss: 14.7869 - mse: 14.7869 - mae: 1.5519 - val_loss: 13.0103 - val_mse: 13.0103 - val_mae: 1.5143 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 4s - loss: 14.6399 - mse: 14.6399 - mae: 1.5535 - val_loss: 12.9072 - val_mse: 12.9072 - val_mae: 1.5593 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 4s - loss: 14.7596 - mse: 14.7596 - mae: 1.5528 - val_loss: 12.8631 - val_mse: 12.8631 - val_mae: 1.5390 - lr: 0.0010 - 4s/epoch - 4ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-10-14 14:57:24,729]\u001b[0m Finished trial#7 resulted in value: 14.801999999999998. Current best value is 14.801999999999998 with parameters: {'activation': 'relu', 'num_hidden_layer': 3, 'i': 9, 'learning_rate': 0.002615735581594066}.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Score for fold 5: loss of 12.86313247680664\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 4s - loss: 16.3225 - mse: 16.3225 - mae: 1.6722 - val_loss: 14.8258 - val_mse: 14.8258 - val_mae: 1.6809 - lr: 7.2131e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 4s - loss: 15.9938 - mse: 15.9938 - mae: 1.6373 - val_loss: 14.7912 - val_mse: 14.7912 - val_mae: 1.6845 - lr: 7.2131e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 4s - loss: 15.9293 - mse: 15.9293 - mae: 1.6309 - val_loss: 14.7376 - val_mse: 14.7376 - val_mae: 1.6755 - lr: 7.2131e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 4s - loss: 15.9180 - mse: 15.9180 - mae: 1.6297 - val_loss: 14.7158 - val_mse: 14.7158 - val_mae: 1.7182 - lr: 7.2131e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 4s - loss: 15.9059 - mse: 15.9059 - mae: 1.6264 - val_loss: 14.8859 - val_mse: 14.8859 - val_mae: 1.6618 - lr: 7.2131e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 4s - loss: 15.9129 - mse: 15.9129 - mae: 1.6276 - val_loss: 14.8152 - val_mse: 14.8152 - val_mae: 1.7757 - lr: 7.2131e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 4s - loss: 15.8932 - mse: 15.8932 - mae: 1.6283 - val_loss: 15.0936 - val_mse: 15.0936 - val_mae: 1.9231 - lr: 7.2131e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 4s - loss: 15.8932 - mse: 15.8932 - mae: 1.6314 - val_loss: 14.6924 - val_mse: 14.6924 - val_mae: 1.7043 - lr: 7.2131e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 4s - loss: 15.9434 - mse: 15.9434 - mae: 1.6337 - val_loss: 14.7185 - val_mse: 14.7185 - val_mae: 1.7423 - lr: 7.2131e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 4s - loss: 15.9367 - mse: 15.9367 - mae: 1.6360 - val_loss: 14.6737 - val_mse: 14.6737 - val_mae: 1.7326 - lr: 7.2131e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 4s - loss: 15.9339 - mse: 15.9339 - mae: 1.6375 - val_loss: 14.7247 - val_mse: 14.7247 - val_mae: 1.7611 - lr: 7.2131e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 4s - loss: 15.9679 - mse: 15.9679 - mae: 1.6396 - val_loss: 14.7207 - val_mse: 14.7207 - val_mae: 1.7307 - lr: 7.2131e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 13/100\n",
            "1000/1000 - 4s - loss: 15.9891 - mse: 15.9891 - mae: 1.6400 - val_loss: 14.8365 - val_mse: 14.8365 - val_mae: 1.6197 - lr: 7.2131e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 14/100\n",
            "1000/1000 - 4s - loss: 15.9345 - mse: 15.9345 - mae: 1.6393 - val_loss: 14.7401 - val_mse: 14.7401 - val_mae: 1.7285 - lr: 7.2131e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 15/100\n",
            "1000/1000 - 4s - loss: 16.0354 - mse: 16.0354 - mae: 1.6421 - val_loss: 14.7206 - val_mse: 14.7206 - val_mae: 1.6782 - lr: 7.2131e-04 - 4s/epoch - 4ms/step\n",
            "Score for fold 1: loss of 14.720574378967285\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 4s - loss: 14.1930 - mse: 14.1930 - mae: 1.6573 - val_loss: 21.8185 - val_mse: 21.8185 - val_mae: 1.6457 - lr: 7.2131e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 4s - loss: 14.2485 - mse: 14.2485 - mae: 1.6635 - val_loss: 21.8311 - val_mse: 21.8311 - val_mae: 1.6954 - lr: 7.2131e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 4s - loss: 14.2841 - mse: 14.2841 - mae: 1.6686 - val_loss: 21.9501 - val_mse: 21.9501 - val_mae: 1.5436 - lr: 7.2131e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 4s - loss: 14.2381 - mse: 14.2381 - mae: 1.6688 - val_loss: 21.8486 - val_mse: 21.8486 - val_mae: 1.7435 - lr: 7.2131e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 4s - loss: 14.2209 - mse: 14.2209 - mae: 1.6580 - val_loss: 22.1602 - val_mse: 22.1602 - val_mae: 1.8549 - lr: 7.2131e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 4s - loss: 14.1758 - mse: 14.1758 - mae: 1.6588 - val_loss: 22.0170 - val_mse: 22.0170 - val_mae: 1.5813 - lr: 7.2131e-04 - 4s/epoch - 4ms/step\n",
            "Score for fold 2: loss of 22.017030715942383\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 4s - loss: 17.2529 - mse: 17.2529 - mae: 1.6879 - val_loss: 9.5994 - val_mse: 9.5994 - val_mae: 1.5497 - lr: 7.2131e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 4s - loss: 17.1946 - mse: 17.1946 - mae: 1.6789 - val_loss: 9.7512 - val_mse: 9.7512 - val_mae: 1.7356 - lr: 7.2131e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 4s - loss: 17.3554 - mse: 17.3554 - mae: 1.6700 - val_loss: 9.8363 - val_mse: 9.8363 - val_mae: 1.4705 - lr: 7.2131e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 4s - loss: 17.3300 - mse: 17.3300 - mae: 1.6790 - val_loss: 9.6093 - val_mse: 9.6093 - val_mae: 1.5219 - lr: 7.2131e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 4s - loss: 17.4094 - mse: 17.4094 - mae: 1.6921 - val_loss: 9.8263 - val_mse: 9.8263 - val_mae: 1.4573 - lr: 7.2131e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 4s - loss: 17.2885 - mse: 17.2885 - mae: 1.6753 - val_loss: 9.6334 - val_mse: 9.6334 - val_mae: 1.5333 - lr: 7.2131e-04 - 4s/epoch - 4ms/step\n",
            "Score for fold 3: loss of 9.633391380310059\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 4s - loss: 14.9725 - mse: 14.9725 - mae: 1.6519 - val_loss: 18.6793 - val_mse: 18.6793 - val_mae: 1.6402 - lr: 7.2131e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 4s - loss: 15.1217 - mse: 15.1217 - mae: 1.6569 - val_loss: 18.6204 - val_mse: 18.6204 - val_mae: 1.6479 - lr: 7.2131e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 4s - loss: 15.0029 - mse: 15.0029 - mae: 1.6516 - val_loss: 19.1462 - val_mse: 19.1462 - val_mae: 1.9992 - lr: 7.2131e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 4s - loss: 14.9894 - mse: 14.9894 - mae: 1.6471 - val_loss: 18.8727 - val_mse: 18.8727 - val_mae: 1.6093 - lr: 7.2131e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 4s - loss: 15.0732 - mse: 15.0732 - mae: 1.6416 - val_loss: 19.1198 - val_mse: 19.1198 - val_mae: 1.5483 - lr: 7.2131e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 4s - loss: 15.1850 - mse: 15.1850 - mae: 1.6528 - val_loss: 19.5444 - val_mse: 19.5444 - val_mae: 2.1080 - lr: 7.2131e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 4s - loss: 15.0406 - mse: 15.0406 - mae: 1.6451 - val_loss: 18.7144 - val_mse: 18.7144 - val_mae: 1.6079 - lr: 7.2131e-04 - 4s/epoch - 4ms/step\n",
            "Score for fold 4: loss of 18.71442222595215\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 4s - loss: 16.3760 - mse: 16.3760 - mae: 1.6500 - val_loss: 13.4502 - val_mse: 13.4502 - val_mae: 1.8449 - lr: 7.2131e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 4s - loss: 16.3930 - mse: 16.3930 - mae: 1.6439 - val_loss: 13.2735 - val_mse: 13.2735 - val_mae: 1.7683 - lr: 7.2131e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 4s - loss: 16.3758 - mse: 16.3758 - mae: 1.6412 - val_loss: 13.2733 - val_mse: 13.2733 - val_mae: 1.6425 - lr: 7.2131e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 4s - loss: 16.4256 - mse: 16.4256 - mae: 1.6439 - val_loss: 13.3360 - val_mse: 13.3360 - val_mae: 1.6229 - lr: 7.2131e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 4s - loss: 16.3409 - mse: 16.3409 - mae: 1.6416 - val_loss: 13.3345 - val_mse: 13.3345 - val_mae: 1.6365 - lr: 7.2131e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 4s - loss: 16.3409 - mse: 16.3409 - mae: 1.6386 - val_loss: 13.3209 - val_mse: 13.3209 - val_mae: 1.7983 - lr: 7.2131e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 4s - loss: 16.3836 - mse: 16.3836 - mae: 1.6432 - val_loss: 13.2508 - val_mse: 13.2508 - val_mae: 1.6827 - lr: 7.2131e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 4s - loss: 16.3552 - mse: 16.3552 - mae: 1.6470 - val_loss: 13.2893 - val_mse: 13.2893 - val_mae: 1.7144 - lr: 7.2131e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 4s - loss: 16.2791 - mse: 16.2791 - mae: 1.6344 - val_loss: 13.2448 - val_mse: 13.2448 - val_mae: 1.6700 - lr: 7.2131e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 4s - loss: 16.3032 - mse: 16.3032 - mae: 1.6427 - val_loss: 13.4565 - val_mse: 13.4565 - val_mae: 1.8653 - lr: 7.2131e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 4s - loss: 16.4394 - mse: 16.4394 - mae: 1.6406 - val_loss: 13.3590 - val_mse: 13.3590 - val_mae: 1.6034 - lr: 7.2131e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 4s - loss: 16.3774 - mse: 16.3774 - mae: 1.6464 - val_loss: 13.3080 - val_mse: 13.3080 - val_mae: 1.7666 - lr: 7.2131e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 13/100\n",
            "1000/1000 - 4s - loss: 16.2649 - mse: 16.2649 - mae: 1.6373 - val_loss: 13.2746 - val_mse: 13.2746 - val_mae: 1.6766 - lr: 7.2131e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 14/100\n",
            "1000/1000 - 4s - loss: 16.3233 - mse: 16.3233 - mae: 1.6394 - val_loss: 13.3541 - val_mse: 13.3541 - val_mae: 1.6099 - lr: 7.2131e-04 - 4s/epoch - 4ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-10-14 15:00:32,379]\u001b[0m Finished trial#8 resulted in value: 15.686000000000002. Current best value is 14.801999999999998 with parameters: {'activation': 'relu', 'num_hidden_layer': 3, 'i': 9, 'learning_rate': 0.002615735581594066}.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Score for fold 5: loss of 13.354122161865234\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 7s - loss: 16.0430 - mse: 16.0430 - mae: 1.7042 - val_loss: 15.3628 - val_mse: 15.3628 - val_mae: 1.6344 - lr: 1.3324e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 7s - loss: 15.4950 - mse: 15.4950 - mae: 1.6385 - val_loss: 15.2772 - val_mse: 15.2772 - val_mae: 1.5374 - lr: 1.3324e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 7s - loss: 15.3954 - mse: 15.3954 - mae: 1.6241 - val_loss: 15.0928 - val_mse: 15.0928 - val_mae: 1.6012 - lr: 1.3324e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 7s - loss: 15.3074 - mse: 15.3074 - mae: 1.6257 - val_loss: 15.0849 - val_mse: 15.0849 - val_mae: 1.6003 - lr: 1.3324e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 7s - loss: 15.2223 - mse: 15.2223 - mae: 1.6212 - val_loss: 15.2261 - val_mse: 15.2261 - val_mae: 1.5240 - lr: 1.3324e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 7s - loss: 15.2593 - mse: 15.2593 - mae: 1.6140 - val_loss: 15.0949 - val_mse: 15.0949 - val_mae: 1.5498 - lr: 1.3324e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 7s - loss: 15.1982 - mse: 15.1982 - mae: 1.6161 - val_loss: 15.0673 - val_mse: 15.0673 - val_mae: 1.5591 - lr: 1.3324e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 7s - loss: 15.2004 - mse: 15.2004 - mae: 1.6163 - val_loss: 15.2296 - val_mse: 15.2296 - val_mae: 1.5229 - lr: 1.3324e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 7s - loss: 15.1798 - mse: 15.1798 - mae: 1.6146 - val_loss: 15.1342 - val_mse: 15.1342 - val_mae: 1.5496 - lr: 1.3324e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 7s - loss: 15.1770 - mse: 15.1770 - mae: 1.6083 - val_loss: 15.0304 - val_mse: 15.0304 - val_mae: 1.5874 - lr: 1.3324e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 7s - loss: 15.1741 - mse: 15.1741 - mae: 1.6092 - val_loss: 15.1600 - val_mse: 15.1600 - val_mae: 1.5389 - lr: 1.3324e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 7s - loss: 15.1555 - mse: 15.1555 - mae: 1.6077 - val_loss: 15.0133 - val_mse: 15.0133 - val_mae: 1.5668 - lr: 1.3324e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 13/100\n",
            "1000/1000 - 7s - loss: 15.1709 - mse: 15.1709 - mae: 1.6058 - val_loss: 15.0918 - val_mse: 15.0918 - val_mae: 1.5208 - lr: 1.3324e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 14/100\n",
            "1000/1000 - 7s - loss: 15.1586 - mse: 15.1586 - mae: 1.6012 - val_loss: 15.0022 - val_mse: 15.0022 - val_mae: 1.5582 - lr: 1.3324e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 15/100\n",
            "1000/1000 - 7s - loss: 15.1584 - mse: 15.1584 - mae: 1.6040 - val_loss: 15.0222 - val_mse: 15.0222 - val_mae: 1.5634 - lr: 1.3324e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 16/100\n",
            "1000/1000 - 7s - loss: 15.1500 - mse: 15.1500 - mae: 1.6060 - val_loss: 15.0471 - val_mse: 15.0471 - val_mae: 1.5536 - lr: 1.3324e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 17/100\n",
            "1000/1000 - 7s - loss: 15.1287 - mse: 15.1287 - mae: 1.6062 - val_loss: 15.1576 - val_mse: 15.1576 - val_mae: 1.5297 - lr: 1.3324e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 18/100\n",
            "1000/1000 - 7s - loss: 15.1323 - mse: 15.1323 - mae: 1.6054 - val_loss: 15.0459 - val_mse: 15.0459 - val_mae: 1.6038 - lr: 1.3324e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 19/100\n",
            "1000/1000 - 7s - loss: 15.1278 - mse: 15.1278 - mae: 1.6032 - val_loss: 15.1361 - val_mse: 15.1361 - val_mae: 1.5342 - lr: 1.3324e-04 - 7s/epoch - 7ms/step\n",
            "Score for fold 1: loss of 15.136133193969727\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 7s - loss: 14.2705 - mse: 14.2705 - mae: 1.5858 - val_loss: 18.5349 - val_mse: 18.5349 - val_mae: 1.6153 - lr: 1.3324e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 6s - loss: 14.2493 - mse: 14.2493 - mae: 1.5820 - val_loss: 18.7590 - val_mse: 18.7590 - val_mae: 1.5886 - lr: 1.3324e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 7s - loss: 14.2439 - mse: 14.2439 - mae: 1.5826 - val_loss: 18.5975 - val_mse: 18.5975 - val_mae: 1.6080 - lr: 1.3324e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 7s - loss: 14.2429 - mse: 14.2429 - mae: 1.5873 - val_loss: 18.6426 - val_mse: 18.6426 - val_mae: 1.5923 - lr: 1.3324e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 7s - loss: 14.2445 - mse: 14.2445 - mae: 1.5809 - val_loss: 18.6263 - val_mse: 18.6263 - val_mae: 1.5864 - lr: 1.3324e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 7s - loss: 14.2348 - mse: 14.2348 - mae: 1.5820 - val_loss: 18.6662 - val_mse: 18.6662 - val_mae: 1.5678 - lr: 1.3324e-04 - 7s/epoch - 7ms/step\n",
            "Score for fold 2: loss of 18.66620635986328\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 7s - loss: 16.3812 - mse: 16.3812 - mae: 1.5947 - val_loss: 9.9133 - val_mse: 9.9133 - val_mae: 1.5621 - lr: 1.3324e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 7s - loss: 16.3858 - mse: 16.3858 - mae: 1.5960 - val_loss: 9.9261 - val_mse: 9.9261 - val_mae: 1.5350 - lr: 1.3324e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 7s - loss: 16.3860 - mse: 16.3860 - mae: 1.5923 - val_loss: 9.9946 - val_mse: 9.9946 - val_mae: 1.5370 - lr: 1.3324e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 7s - loss: 16.3641 - mse: 16.3641 - mae: 1.5968 - val_loss: 9.8854 - val_mse: 9.8854 - val_mae: 1.5667 - lr: 1.3324e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 7s - loss: 16.3683 - mse: 16.3683 - mae: 1.5969 - val_loss: 9.8695 - val_mse: 9.8695 - val_mae: 1.5432 - lr: 1.3324e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 7s - loss: 16.3800 - mse: 16.3800 - mae: 1.5939 - val_loss: 10.0031 - val_mse: 10.0031 - val_mae: 1.5474 - lr: 1.3324e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 7s - loss: 16.3851 - mse: 16.3851 - mae: 1.5944 - val_loss: 9.9638 - val_mse: 9.9638 - val_mae: 1.5433 - lr: 1.3324e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 7s - loss: 16.3493 - mse: 16.3493 - mae: 1.5911 - val_loss: 9.9476 - val_mse: 9.9476 - val_mae: 1.5383 - lr: 1.3324e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 7s - loss: 16.3533 - mse: 16.3533 - mae: 1.5963 - val_loss: 9.8828 - val_mse: 9.8828 - val_mae: 1.6123 - lr: 1.3324e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 7s - loss: 16.3417 - mse: 16.3417 - mae: 1.5997 - val_loss: 9.9657 - val_mse: 9.9657 - val_mae: 1.5244 - lr: 1.3324e-04 - 7s/epoch - 7ms/step\n",
            "Score for fold 3: loss of 9.965654373168945\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 7s - loss: 15.4452 - mse: 15.4452 - mae: 1.5754 - val_loss: 13.9035 - val_mse: 13.9035 - val_mae: 1.5830 - lr: 1.3324e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 7s - loss: 15.4229 - mse: 15.4229 - mae: 1.5723 - val_loss: 13.6154 - val_mse: 13.6154 - val_mae: 1.6052 - lr: 1.3324e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 7s - loss: 15.4343 - mse: 15.4343 - mae: 1.5760 - val_loss: 13.6686 - val_mse: 13.6686 - val_mae: 1.6698 - lr: 1.3324e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 7s - loss: 15.4172 - mse: 15.4172 - mae: 1.5731 - val_loss: 13.6531 - val_mse: 13.6531 - val_mae: 1.6067 - lr: 1.3324e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 7s - loss: 15.3957 - mse: 15.3957 - mae: 1.5737 - val_loss: 13.6127 - val_mse: 13.6127 - val_mae: 1.6315 - lr: 1.3324e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 7s - loss: 15.3897 - mse: 15.3897 - mae: 1.5761 - val_loss: 13.7268 - val_mse: 13.7268 - val_mae: 1.5768 - lr: 1.3324e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 7s - loss: 15.4085 - mse: 15.4085 - mae: 1.5671 - val_loss: 13.5018 - val_mse: 13.5018 - val_mae: 1.6786 - lr: 1.3324e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 7s - loss: 15.3829 - mse: 15.3829 - mae: 1.5715 - val_loss: 13.6484 - val_mse: 13.6484 - val_mae: 1.6291 - lr: 1.3324e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 7s - loss: 15.3722 - mse: 15.3722 - mae: 1.5696 - val_loss: 13.7020 - val_mse: 13.7020 - val_mae: 1.6341 - lr: 1.3324e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 7s - loss: 15.3676 - mse: 15.3676 - mae: 1.5685 - val_loss: 13.7694 - val_mse: 13.7694 - val_mae: 1.6571 - lr: 1.3324e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 7s - loss: 15.3756 - mse: 15.3756 - mae: 1.5718 - val_loss: 13.5155 - val_mse: 13.5155 - val_mae: 1.6315 - lr: 1.3324e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 7s - loss: 15.3485 - mse: 15.3485 - mae: 1.5669 - val_loss: 13.5072 - val_mse: 13.5072 - val_mae: 1.6688 - lr: 1.3324e-04 - 7s/epoch - 7ms/step\n",
            "Score for fold 4: loss of 13.507172584533691\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 7s - loss: 14.2335 - mse: 14.2335 - mae: 1.5781 - val_loss: 18.0524 - val_mse: 18.0524 - val_mae: 1.6480 - lr: 1.3324e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 7s - loss: 14.2070 - mse: 14.2070 - mae: 1.5752 - val_loss: 18.0338 - val_mse: 18.0338 - val_mae: 1.6477 - lr: 1.3324e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 7s - loss: 14.2123 - mse: 14.2123 - mae: 1.5775 - val_loss: 18.0911 - val_mse: 18.0911 - val_mae: 1.6615 - lr: 1.3324e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 8s - loss: 14.1936 - mse: 14.1936 - mae: 1.5738 - val_loss: 18.1594 - val_mse: 18.1594 - val_mae: 1.6082 - lr: 1.3324e-04 - 8s/epoch - 8ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 8s - loss: 14.1581 - mse: 14.1581 - mae: 1.5772 - val_loss: 18.1404 - val_mse: 18.1404 - val_mae: 1.5893 - lr: 1.3324e-04 - 8s/epoch - 8ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 7s - loss: 14.1738 - mse: 14.1738 - mae: 1.5772 - val_loss: 18.1222 - val_mse: 18.1222 - val_mae: 1.6600 - lr: 1.3324e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 7s - loss: 14.1493 - mse: 14.1493 - mae: 1.5753 - val_loss: 18.3584 - val_mse: 18.3584 - val_mae: 1.5898 - lr: 1.3324e-04 - 7s/epoch - 7ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-10-14 15:06:51,589]\u001b[0m Finished trial#9 resulted in value: 15.13. Current best value is 14.801999999999998 with parameters: {'activation': 'relu', 'num_hidden_layer': 3, 'i': 9, 'learning_rate': 0.002615735581594066}.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Score for fold 5: loss of 18.35843276977539\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 3s - loss: 15.2371 - mse: 15.2371 - mae: 1.6328 - val_loss: 16.4453 - val_mse: 16.4453 - val_mae: 1.7705 - lr: 0.0061 - 3s/epoch - 3ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 15.0281 - mse: 15.0281 - mae: 1.6204 - val_loss: 16.6666 - val_mse: 16.6666 - val_mae: 1.5714 - lr: 0.0061 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 14.9789 - mse: 14.9789 - mae: 1.6123 - val_loss: 16.2349 - val_mse: 16.2349 - val_mae: 1.6781 - lr: 0.0061 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 14.9560 - mse: 14.9560 - mae: 1.6118 - val_loss: 16.7996 - val_mse: 16.7996 - val_mae: 1.5843 - lr: 0.0061 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 14.8990 - mse: 14.8990 - mae: 1.6071 - val_loss: 16.3025 - val_mse: 16.3025 - val_mae: 1.6202 - lr: 0.0061 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 14.8496 - mse: 14.8496 - mae: 1.6062 - val_loss: 16.6775 - val_mse: 16.6775 - val_mae: 1.6293 - lr: 0.0061 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 14.8191 - mse: 14.8191 - mae: 1.6033 - val_loss: 16.5111 - val_mse: 16.5111 - val_mae: 1.6147 - lr: 0.0061 - 2s/epoch - 2ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 2s - loss: 14.8567 - mse: 14.8567 - mae: 1.6003 - val_loss: 16.1910 - val_mse: 16.1910 - val_mae: 1.6718 - lr: 0.0061 - 2s/epoch - 2ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 2s - loss: 14.8095 - mse: 14.8095 - mae: 1.6031 - val_loss: 16.4756 - val_mse: 16.4756 - val_mae: 1.5791 - lr: 0.0061 - 2s/epoch - 2ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 2s - loss: 14.8035 - mse: 14.8035 - mae: 1.6025 - val_loss: 16.2801 - val_mse: 16.2801 - val_mae: 1.6175 - lr: 0.0061 - 2s/epoch - 2ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 2s - loss: 14.8490 - mse: 14.8490 - mae: 1.6008 - val_loss: 16.2665 - val_mse: 16.2665 - val_mae: 1.6317 - lr: 0.0061 - 2s/epoch - 2ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 2s - loss: 14.8618 - mse: 14.8618 - mae: 1.6001 - val_loss: 16.4336 - val_mse: 16.4336 - val_mae: 1.6282 - lr: 0.0061 - 2s/epoch - 2ms/step\n",
            "Epoch 13/100\n",
            "1000/1000 - 2s - loss: 14.8486 - mse: 14.8486 - mae: 1.5994 - val_loss: 16.5969 - val_mse: 16.5969 - val_mae: 1.5884 - lr: 0.0061 - 2s/epoch - 2ms/step\n",
            "Score for fold 1: loss of 16.596864700317383\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 15.2435 - mse: 15.2435 - mae: 1.5849 - val_loss: 14.1696 - val_mse: 14.1696 - val_mae: 1.5739 - lr: 0.0012 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 15.1760 - mse: 15.1760 - mae: 1.5874 - val_loss: 14.2256 - val_mse: 14.2256 - val_mae: 1.5872 - lr: 0.0012 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 15.2047 - mse: 15.2047 - mae: 1.5833 - val_loss: 14.1896 - val_mse: 14.1896 - val_mae: 1.5847 - lr: 0.0012 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 15.2113 - mse: 15.2113 - mae: 1.5834 - val_loss: 14.1835 - val_mse: 14.1835 - val_mae: 1.6183 - lr: 0.0012 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 15.1627 - mse: 15.1627 - mae: 1.5837 - val_loss: 14.3432 - val_mse: 14.3432 - val_mae: 1.6058 - lr: 0.0012 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 15.1973 - mse: 15.1973 - mae: 1.5797 - val_loss: 14.3879 - val_mse: 14.3879 - val_mae: 1.5661 - lr: 0.0012 - 2s/epoch - 2ms/step\n",
            "Score for fold 2: loss of 14.38791561126709\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 15.3948 - mse: 15.3948 - mae: 1.5727 - val_loss: 13.4572 - val_mse: 13.4572 - val_mae: 1.6018 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 15.3716 - mse: 15.3716 - mae: 1.5758 - val_loss: 13.4171 - val_mse: 13.4171 - val_mae: 1.5752 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 15.3355 - mse: 15.3355 - mae: 1.5740 - val_loss: 13.4428 - val_mse: 13.4428 - val_mae: 1.5829 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 15.3047 - mse: 15.3047 - mae: 1.5765 - val_loss: 13.5785 - val_mse: 13.5785 - val_mae: 1.5802 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 15.3331 - mse: 15.3331 - mae: 1.5717 - val_loss: 13.4294 - val_mse: 13.4294 - val_mae: 1.6080 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 15.3010 - mse: 15.3010 - mae: 1.5725 - val_loss: 13.5649 - val_mse: 13.5649 - val_mae: 1.5827 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 15.3323 - mse: 15.3323 - mae: 1.5728 - val_loss: 13.5139 - val_mse: 13.5139 - val_mae: 1.5797 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Score for fold 3: loss of 13.513875961303711\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 15.6066 - mse: 15.6066 - mae: 1.5883 - val_loss: 12.2111 - val_mse: 12.2111 - val_mae: 1.5171 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 15.6155 - mse: 15.6155 - mae: 1.5840 - val_loss: 12.3434 - val_mse: 12.3434 - val_mae: 1.5314 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 15.6304 - mse: 15.6304 - mae: 1.5845 - val_loss: 12.3030 - val_mse: 12.3030 - val_mae: 1.5563 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 15.5880 - mse: 15.5880 - mae: 1.5868 - val_loss: 12.3030 - val_mse: 12.3030 - val_mae: 1.5419 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 15.5922 - mse: 15.5922 - mae: 1.5854 - val_loss: 12.2730 - val_mse: 12.2730 - val_mae: 1.5458 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 15.5806 - mse: 15.5806 - mae: 1.5812 - val_loss: 12.2251 - val_mse: 12.2251 - val_mae: 1.5498 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Score for fold 4: loss of 12.22508430480957\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 13.9560 - mse: 13.9560 - mae: 1.5758 - val_loss: 18.6810 - val_mse: 18.6810 - val_mae: 1.5522 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 13.9592 - mse: 13.9592 - mae: 1.5729 - val_loss: 18.6868 - val_mse: 18.6868 - val_mae: 1.5766 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 13.9362 - mse: 13.9362 - mae: 1.5734 - val_loss: 18.7159 - val_mse: 18.7159 - val_mae: 1.5529 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 13.9330 - mse: 13.9330 - mae: 1.5772 - val_loss: 18.7043 - val_mse: 18.7043 - val_mae: 1.5511 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 13.9174 - mse: 13.9174 - mae: 1.5720 - val_loss: 18.6451 - val_mse: 18.6451 - val_mae: 1.6085 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 13.9160 - mse: 13.9160 - mae: 1.5739 - val_loss: 18.7379 - val_mse: 18.7379 - val_mae: 1.5768 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 13.9010 - mse: 13.9010 - mae: 1.5685 - val_loss: 18.7920 - val_mse: 18.7920 - val_mae: 1.5820 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 2s - loss: 13.9223 - mse: 13.9223 - mae: 1.5734 - val_loss: 18.7237 - val_mse: 18.7237 - val_mae: 1.5797 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 2s - loss: 13.9524 - mse: 13.9524 - mae: 1.5745 - val_loss: 18.6828 - val_mse: 18.6828 - val_mae: 1.5828 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 2s - loss: 13.8936 - mse: 13.8936 - mae: 1.5773 - val_loss: 18.8263 - val_mse: 18.8263 - val_mae: 1.5481 - lr: 0.0010 - 2s/epoch - 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-10-14 15:08:27,607]\u001b[0m Finished trial#10 resulted in value: 15.112. Current best value is 14.801999999999998 with parameters: {'activation': 'relu', 'num_hidden_layer': 3, 'i': 9, 'learning_rate': 0.002615735581594066}.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Score for fold 5: loss of 18.82634162902832\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 3s - loss: 15.7055 - mse: 15.7055 - mae: 1.6184 - val_loss: 14.1465 - val_mse: 14.1465 - val_mae: 1.6923 - lr: 9.5841e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 15.4259 - mse: 15.4259 - mae: 1.6051 - val_loss: 13.9314 - val_mse: 13.9314 - val_mae: 1.5949 - lr: 9.5841e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 15.2998 - mse: 15.2998 - mae: 1.5994 - val_loss: 14.0678 - val_mse: 14.0678 - val_mae: 1.5686 - lr: 9.5841e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 15.3282 - mse: 15.3282 - mae: 1.5931 - val_loss: 13.9538 - val_mse: 13.9538 - val_mae: 1.6117 - lr: 9.5841e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 15.2867 - mse: 15.2867 - mae: 1.5955 - val_loss: 13.9038 - val_mse: 13.9038 - val_mae: 1.5513 - lr: 9.5841e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 15.2779 - mse: 15.2779 - mae: 1.5942 - val_loss: 13.8454 - val_mse: 13.8454 - val_mae: 1.6381 - lr: 9.5841e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 15.3468 - mse: 15.3468 - mae: 1.5925 - val_loss: 13.9700 - val_mse: 13.9700 - val_mae: 1.5142 - lr: 9.5841e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 2s - loss: 15.1906 - mse: 15.1906 - mae: 1.5889 - val_loss: 13.9604 - val_mse: 13.9604 - val_mae: 1.5044 - lr: 9.5841e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 2s - loss: 15.3225 - mse: 15.3225 - mae: 1.5903 - val_loss: 13.8702 - val_mse: 13.8702 - val_mae: 1.5273 - lr: 9.5841e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 2s - loss: 15.0891 - mse: 15.0891 - mae: 1.5880 - val_loss: 13.9389 - val_mse: 13.9389 - val_mae: 1.6063 - lr: 9.5841e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 2s - loss: 15.1748 - mse: 15.1748 - mae: 1.5875 - val_loss: 13.9948 - val_mse: 13.9948 - val_mae: 1.5280 - lr: 9.5841e-04 - 2s/epoch - 2ms/step\n",
            "Score for fold 1: loss of 13.994842529296875\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 15.4766 - mse: 15.4766 - mae: 1.5890 - val_loss: 12.4991 - val_mse: 12.4991 - val_mae: 1.5557 - lr: 9.5841e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 15.3403 - mse: 15.3403 - mae: 1.5866 - val_loss: 12.5535 - val_mse: 12.5535 - val_mae: 1.6650 - lr: 9.5841e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 15.4002 - mse: 15.4002 - mae: 1.5906 - val_loss: 12.6662 - val_mse: 12.6662 - val_mae: 1.5790 - lr: 9.5841e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 15.4959 - mse: 15.4959 - mae: 1.5891 - val_loss: 12.6416 - val_mse: 12.6416 - val_mae: 1.5288 - lr: 9.5841e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 15.3718 - mse: 15.3718 - mae: 1.5930 - val_loss: 12.6811 - val_mse: 12.6811 - val_mae: 1.6740 - lr: 9.5841e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 15.2788 - mse: 15.2788 - mae: 1.5954 - val_loss: 12.7722 - val_mse: 12.7722 - val_mae: 1.6271 - lr: 9.5841e-04 - 2s/epoch - 2ms/step\n",
            "Score for fold 2: loss of 12.772167205810547\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 15.3273 - mse: 15.3273 - mae: 1.5988 - val_loss: 12.9800 - val_mse: 12.9800 - val_mae: 1.5413 - lr: 9.5841e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 15.2940 - mse: 15.2940 - mae: 1.5966 - val_loss: 13.0759 - val_mse: 13.0759 - val_mae: 1.5666 - lr: 9.5841e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 15.2191 - mse: 15.2191 - mae: 1.5920 - val_loss: 13.0567 - val_mse: 13.0567 - val_mae: 1.6013 - lr: 9.5841e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 15.0728 - mse: 15.0728 - mae: 1.5933 - val_loss: 13.1275 - val_mse: 13.1275 - val_mae: 1.5166 - lr: 9.5841e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 3s - loss: 15.1599 - mse: 15.1599 - mae: 1.5889 - val_loss: 13.0574 - val_mse: 13.0574 - val_mae: 1.5244 - lr: 9.5841e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 3s - loss: 14.9019 - mse: 14.9019 - mae: 1.5878 - val_loss: 13.2104 - val_mse: 13.2104 - val_mae: 1.5275 - lr: 9.5841e-04 - 3s/epoch - 3ms/step\n",
            "Score for fold 3: loss of 13.210447311401367\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 13.6674 - mse: 13.6674 - mae: 1.5844 - val_loss: 19.3630 - val_mse: 19.3630 - val_mae: 1.6424 - lr: 9.5841e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 13.5517 - mse: 13.5517 - mae: 1.5781 - val_loss: 18.8396 - val_mse: 18.8396 - val_mae: 1.6294 - lr: 9.5841e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 13.5032 - mse: 13.5032 - mae: 1.5829 - val_loss: 18.7332 - val_mse: 18.7332 - val_mae: 1.6352 - lr: 9.5841e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 13.4075 - mse: 13.4075 - mae: 1.5816 - val_loss: 19.1185 - val_mse: 19.1185 - val_mae: 1.5571 - lr: 9.5841e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 13.4611 - mse: 13.4611 - mae: 1.5757 - val_loss: 19.2725 - val_mse: 19.2725 - val_mae: 1.6001 - lr: 9.5841e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 13.3114 - mse: 13.3114 - mae: 1.5756 - val_loss: 19.2708 - val_mse: 19.2708 - val_mae: 1.5678 - lr: 9.5841e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 3s - loss: 13.3341 - mse: 13.3341 - mae: 1.5711 - val_loss: 19.7409 - val_mse: 19.7409 - val_mae: 1.5442 - lr: 9.5841e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 2s - loss: 13.3177 - mse: 13.3177 - mae: 1.5675 - val_loss: 19.1935 - val_mse: 19.1935 - val_mae: 1.6419 - lr: 9.5841e-04 - 2s/epoch - 2ms/step\n",
            "Score for fold 4: loss of 19.193525314331055\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 14.6549 - mse: 14.6549 - mae: 1.5603 - val_loss: 14.6340 - val_mse: 14.6340 - val_mae: 1.7404 - lr: 9.5841e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 14.4687 - mse: 14.4687 - mae: 1.5611 - val_loss: 14.9184 - val_mse: 14.9184 - val_mae: 1.6008 - lr: 9.5841e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 14.4738 - mse: 14.4738 - mae: 1.5530 - val_loss: 14.6883 - val_mse: 14.6883 - val_mae: 1.6472 - lr: 9.5841e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 14.4842 - mse: 14.4842 - mae: 1.5558 - val_loss: 15.0143 - val_mse: 15.0143 - val_mae: 1.6094 - lr: 9.5841e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 14.4336 - mse: 14.4336 - mae: 1.5493 - val_loss: 15.4107 - val_mse: 15.4107 - val_mae: 1.5669 - lr: 9.5841e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 14.4417 - mse: 14.4417 - mae: 1.5513 - val_loss: 15.0110 - val_mse: 15.0110 - val_mae: 1.5985 - lr: 9.5841e-04 - 2s/epoch - 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-10-14 15:10:03,738]\u001b[0m Finished trial#11 resulted in value: 14.834. Current best value is 14.801999999999998 with parameters: {'activation': 'relu', 'num_hidden_layer': 3, 'i': 9, 'learning_rate': 0.002615735581594066}.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Score for fold 5: loss of 15.010979652404785\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 4s - loss: 15.1146 - mse: 15.1146 - mae: 1.6243 - val_loss: 16.8630 - val_mse: 16.8630 - val_mae: 1.7681 - lr: 0.0023 - 4s/epoch - 4ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 4s - loss: 14.7888 - mse: 14.7888 - mae: 1.6087 - val_loss: 17.5775 - val_mse: 17.5775 - val_mae: 1.5966 - lr: 0.0023 - 4s/epoch - 4ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 4s - loss: 14.7510 - mse: 14.7510 - mae: 1.5961 - val_loss: 17.5019 - val_mse: 17.5019 - val_mae: 1.6196 - lr: 0.0023 - 4s/epoch - 4ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 4s - loss: 14.7212 - mse: 14.7212 - mae: 1.5877 - val_loss: 16.5730 - val_mse: 16.5730 - val_mae: 1.5541 - lr: 0.0023 - 4s/epoch - 4ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 4s - loss: 14.7044 - mse: 14.7044 - mae: 1.5909 - val_loss: 16.8711 - val_mse: 16.8711 - val_mae: 1.6907 - lr: 0.0023 - 4s/epoch - 4ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 4s - loss: 14.6691 - mse: 14.6691 - mae: 1.5904 - val_loss: 16.9769 - val_mse: 16.9769 - val_mae: 1.6311 - lr: 0.0023 - 4s/epoch - 4ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 4s - loss: 14.6547 - mse: 14.6547 - mae: 1.6031 - val_loss: 17.0491 - val_mse: 17.0491 - val_mae: 1.5985 - lr: 0.0023 - 4s/epoch - 4ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 4s - loss: 14.6460 - mse: 14.6460 - mae: 1.5974 - val_loss: 17.0304 - val_mse: 17.0304 - val_mae: 1.5825 - lr: 0.0023 - 4s/epoch - 4ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 4s - loss: 14.5840 - mse: 14.5840 - mae: 1.5912 - val_loss: 17.1073 - val_mse: 17.1073 - val_mae: 1.5791 - lr: 0.0023 - 4s/epoch - 4ms/step\n",
            "Score for fold 1: loss of 17.107250213623047\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 4s - loss: 15.3117 - mse: 15.3117 - mae: 1.5886 - val_loss: 13.1894 - val_mse: 13.1894 - val_mae: 1.5742 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 4s - loss: 15.2662 - mse: 15.2662 - mae: 1.5865 - val_loss: 13.4547 - val_mse: 13.4547 - val_mae: 1.5535 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 4s - loss: 15.2659 - mse: 15.2659 - mae: 1.5891 - val_loss: 13.3134 - val_mse: 13.3134 - val_mae: 1.6677 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 4s - loss: 15.2800 - mse: 15.2800 - mae: 1.5840 - val_loss: 13.1795 - val_mse: 13.1795 - val_mae: 1.6354 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 4s - loss: 15.2613 - mse: 15.2613 - mae: 1.5858 - val_loss: 13.2127 - val_mse: 13.2127 - val_mae: 1.5681 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 4s - loss: 15.2784 - mse: 15.2784 - mae: 1.5855 - val_loss: 13.0170 - val_mse: 13.0170 - val_mae: 1.6055 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 4s - loss: 15.2317 - mse: 15.2317 - mae: 1.5884 - val_loss: 13.1898 - val_mse: 13.1898 - val_mae: 1.5125 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 4s - loss: 15.2314 - mse: 15.2314 - mae: 1.5864 - val_loss: 13.5165 - val_mse: 13.5165 - val_mae: 1.5150 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 4s - loss: 15.2031 - mse: 15.2031 - mae: 1.5807 - val_loss: 13.3421 - val_mse: 13.3421 - val_mae: 1.6524 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 4s - loss: 15.1883 - mse: 15.1883 - mae: 1.5829 - val_loss: 13.6768 - val_mse: 13.6768 - val_mae: 1.4770 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 4s - loss: 15.0763 - mse: 15.0763 - mae: 1.5795 - val_loss: 13.3212 - val_mse: 13.3212 - val_mae: 1.6673 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Score for fold 2: loss of 13.321242332458496\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 4s - loss: 13.0247 - mse: 13.0247 - mae: 1.5689 - val_loss: 21.8043 - val_mse: 21.8043 - val_mae: 1.5940 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 4s - loss: 13.0299 - mse: 13.0299 - mae: 1.5599 - val_loss: 21.9038 - val_mse: 21.9038 - val_mae: 1.6579 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 4s - loss: 12.9080 - mse: 12.9080 - mae: 1.5600 - val_loss: 22.1570 - val_mse: 22.1570 - val_mae: 1.5771 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 4s - loss: 12.8733 - mse: 12.8733 - mae: 1.5614 - val_loss: 21.9700 - val_mse: 21.9700 - val_mae: 1.7884 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 4s - loss: 12.9441 - mse: 12.9441 - mae: 1.5600 - val_loss: 21.9520 - val_mse: 21.9520 - val_mae: 1.6437 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 4s - loss: 12.9692 - mse: 12.9692 - mae: 1.5694 - val_loss: 21.9510 - val_mse: 21.9510 - val_mae: 1.6231 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Score for fold 3: loss of 21.95100975036621\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 4s - loss: 15.5112 - mse: 15.5112 - mae: 1.5958 - val_loss: 11.8609 - val_mse: 11.8609 - val_mae: 1.5098 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 4s - loss: 15.4435 - mse: 15.4435 - mae: 1.5957 - val_loss: 11.7917 - val_mse: 11.7917 - val_mae: 1.6398 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 4s - loss: 15.4270 - mse: 15.4270 - mae: 1.6015 - val_loss: 11.9220 - val_mse: 11.9220 - val_mae: 1.6456 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 4s - loss: 15.2540 - mse: 15.2540 - mae: 1.5919 - val_loss: 11.9390 - val_mse: 11.9390 - val_mae: 1.5291 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 4s - loss: 15.2691 - mse: 15.2691 - mae: 1.5890 - val_loss: 11.9352 - val_mse: 11.9352 - val_mae: 1.6089 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 4s - loss: 15.3269 - mse: 15.3269 - mae: 1.5961 - val_loss: 11.9445 - val_mse: 11.9445 - val_mae: 1.4879 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 4s - loss: 15.3235 - mse: 15.3235 - mae: 1.6011 - val_loss: 12.0336 - val_mse: 12.0336 - val_mae: 1.4584 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Score for fold 4: loss of 12.033564567565918\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 4s - loss: 15.8108 - mse: 15.8108 - mae: 1.5995 - val_loss: 10.0816 - val_mse: 10.0816 - val_mae: 1.4777 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 4s - loss: 15.7639 - mse: 15.7639 - mae: 1.5947 - val_loss: 10.3271 - val_mse: 10.3271 - val_mae: 1.5377 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 4s - loss: 15.7023 - mse: 15.7023 - mae: 1.5901 - val_loss: 10.2180 - val_mse: 10.2180 - val_mae: 1.5318 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 4s - loss: 15.6212 - mse: 15.6212 - mae: 1.5976 - val_loss: 10.3491 - val_mse: 10.3491 - val_mae: 1.6919 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 4s - loss: 15.6353 - mse: 15.6353 - mae: 1.5908 - val_loss: 10.5999 - val_mse: 10.5999 - val_mae: 1.5220 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 4s - loss: 15.6420 - mse: 15.6420 - mae: 1.5935 - val_loss: 10.3306 - val_mse: 10.3306 - val_mae: 1.5306 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Score for fold 5: loss of 10.330609321594238\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-10-14 15:12:37,267]\u001b[0m Finished trial#12 resulted in value: 14.947999999999999. Current best value is 14.801999999999998 with parameters: {'activation': 'relu', 'num_hidden_layer': 3, 'i': 9, 'learning_rate': 0.002615735581594066}.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 15.5957 - mse: 15.5957 - mae: 1.7194 - val_loss: 22.3370 - val_mse: 22.3370 - val_mae: 1.6387 - lr: 5.5676e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 13.8990 - mse: 13.8990 - mae: 1.6084 - val_loss: 22.2116 - val_mse: 22.2116 - val_mae: 1.6574 - lr: 5.5676e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 13.8040 - mse: 13.8040 - mae: 1.6061 - val_loss: 22.1897 - val_mse: 22.1897 - val_mae: 1.6431 - lr: 5.5676e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 13.7717 - mse: 13.7717 - mae: 1.5996 - val_loss: 22.1245 - val_mse: 22.1245 - val_mae: 1.6319 - lr: 5.5676e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 13.7238 - mse: 13.7238 - mae: 1.5972 - val_loss: 22.0843 - val_mse: 22.0843 - val_mae: 1.6301 - lr: 5.5676e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 13.6764 - mse: 13.6764 - mae: 1.5960 - val_loss: 22.0735 - val_mse: 22.0735 - val_mae: 1.6236 - lr: 5.5676e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 13.6502 - mse: 13.6502 - mae: 1.5900 - val_loss: 22.0047 - val_mse: 22.0047 - val_mae: 1.6449 - lr: 5.5676e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 2s - loss: 13.6031 - mse: 13.6031 - mae: 1.5918 - val_loss: 21.9747 - val_mse: 21.9747 - val_mae: 1.6192 - lr: 5.5676e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 2s - loss: 13.5808 - mse: 13.5808 - mae: 1.5861 - val_loss: 21.9520 - val_mse: 21.9520 - val_mae: 1.6386 - lr: 5.5676e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 2s - loss: 13.5566 - mse: 13.5566 - mae: 1.5855 - val_loss: 21.9119 - val_mse: 21.9119 - val_mae: 1.6425 - lr: 5.5676e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 2s - loss: 13.5447 - mse: 13.5447 - mae: 1.5841 - val_loss: 21.9067 - val_mse: 21.9067 - val_mae: 1.6358 - lr: 5.5676e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 2s - loss: 13.5142 - mse: 13.5142 - mae: 1.5847 - val_loss: 21.9493 - val_mse: 21.9493 - val_mae: 1.5920 - lr: 5.5676e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 13/100\n",
            "1000/1000 - 2s - loss: 13.4887 - mse: 13.4887 - mae: 1.5809 - val_loss: 21.8752 - val_mse: 21.8752 - val_mae: 1.6221 - lr: 5.5676e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 14/100\n",
            "1000/1000 - 2s - loss: 13.4744 - mse: 13.4744 - mae: 1.5808 - val_loss: 21.8646 - val_mse: 21.8646 - val_mae: 1.6272 - lr: 5.5676e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 15/100\n",
            "1000/1000 - 2s - loss: 13.4607 - mse: 13.4607 - mae: 1.5803 - val_loss: 21.8781 - val_mse: 21.8781 - val_mae: 1.6273 - lr: 5.5676e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 16/100\n",
            "1000/1000 - 2s - loss: 13.4653 - mse: 13.4653 - mae: 1.5775 - val_loss: 21.8603 - val_mse: 21.8603 - val_mae: 1.6271 - lr: 5.5676e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 17/100\n",
            "1000/1000 - 2s - loss: 13.4441 - mse: 13.4441 - mae: 1.5787 - val_loss: 21.8535 - val_mse: 21.8535 - val_mae: 1.6017 - lr: 5.5676e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 18/100\n",
            "1000/1000 - 2s - loss: 13.4237 - mse: 13.4237 - mae: 1.5750 - val_loss: 21.8622 - val_mse: 21.8622 - val_mae: 1.6482 - lr: 5.5676e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 19/100\n",
            "1000/1000 - 2s - loss: 13.4269 - mse: 13.4269 - mae: 1.5785 - val_loss: 21.8504 - val_mse: 21.8504 - val_mae: 1.6003 - lr: 5.5676e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 20/100\n",
            "1000/1000 - 2s - loss: 13.4157 - mse: 13.4157 - mae: 1.5736 - val_loss: 21.8369 - val_mse: 21.8369 - val_mae: 1.6031 - lr: 5.5676e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 21/100\n",
            "1000/1000 - 2s - loss: 13.3991 - mse: 13.3991 - mae: 1.5737 - val_loss: 21.8460 - val_mse: 21.8460 - val_mae: 1.6085 - lr: 5.5676e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 22/100\n",
            "1000/1000 - 2s - loss: 13.4050 - mse: 13.4050 - mae: 1.5727 - val_loss: 21.8408 - val_mse: 21.8408 - val_mae: 1.6089 - lr: 5.5676e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 23/100\n",
            "1000/1000 - 2s - loss: 13.3787 - mse: 13.3787 - mae: 1.5749 - val_loss: 21.8220 - val_mse: 21.8220 - val_mae: 1.6092 - lr: 5.5676e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 24/100\n",
            "1000/1000 - 2s - loss: 13.3902 - mse: 13.3902 - mae: 1.5724 - val_loss: 21.8059 - val_mse: 21.8059 - val_mae: 1.6277 - lr: 5.5676e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 25/100\n",
            "1000/1000 - 2s - loss: 13.3758 - mse: 13.3758 - mae: 1.5741 - val_loss: 21.8006 - val_mse: 21.8006 - val_mae: 1.6336 - lr: 5.5676e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 26/100\n",
            "1000/1000 - 2s - loss: 13.3648 - mse: 13.3648 - mae: 1.5729 - val_loss: 21.8034 - val_mse: 21.8034 - val_mae: 1.6340 - lr: 5.5676e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 27/100\n",
            "1000/1000 - 2s - loss: 13.3652 - mse: 13.3652 - mae: 1.5724 - val_loss: 21.8307 - val_mse: 21.8307 - val_mae: 1.6358 - lr: 5.5676e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 28/100\n",
            "1000/1000 - 2s - loss: 13.3662 - mse: 13.3662 - mae: 1.5702 - val_loss: 21.8161 - val_mse: 21.8161 - val_mae: 1.6276 - lr: 5.5676e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 29/100\n",
            "1000/1000 - 2s - loss: 13.3555 - mse: 13.3555 - mae: 1.5714 - val_loss: 21.7778 - val_mse: 21.7778 - val_mae: 1.6395 - lr: 5.5676e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 30/100\n",
            "1000/1000 - 2s - loss: 13.3421 - mse: 13.3421 - mae: 1.5735 - val_loss: 21.7929 - val_mse: 21.7929 - val_mae: 1.6031 - lr: 5.5676e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 31/100\n",
            "1000/1000 - 2s - loss: 13.3348 - mse: 13.3348 - mae: 1.5712 - val_loss: 21.8059 - val_mse: 21.8059 - val_mae: 1.5912 - lr: 5.5676e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 32/100\n",
            "1000/1000 - 2s - loss: 13.3352 - mse: 13.3352 - mae: 1.5675 - val_loss: 21.8011 - val_mse: 21.8011 - val_mae: 1.6319 - lr: 5.5676e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 33/100\n",
            "1000/1000 - 2s - loss: 13.3195 - mse: 13.3195 - mae: 1.5693 - val_loss: 21.7749 - val_mse: 21.7749 - val_mae: 1.6214 - lr: 5.5676e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 34/100\n",
            "1000/1000 - 2s - loss: 13.3249 - mse: 13.3249 - mae: 1.5708 - val_loss: 21.7876 - val_mse: 21.7876 - val_mae: 1.6167 - lr: 5.5676e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 35/100\n",
            "1000/1000 - 2s - loss: 13.3257 - mse: 13.3257 - mae: 1.5704 - val_loss: 21.8520 - val_mse: 21.8520 - val_mae: 1.5935 - lr: 5.5676e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 36/100\n",
            "1000/1000 - 2s - loss: 13.3238 - mse: 13.3238 - mae: 1.5712 - val_loss: 21.8145 - val_mse: 21.8145 - val_mae: 1.6046 - lr: 5.5676e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 37/100\n",
            "1000/1000 - 2s - loss: 13.3126 - mse: 13.3126 - mae: 1.5652 - val_loss: 21.8252 - val_mse: 21.8252 - val_mae: 1.5959 - lr: 5.5676e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 38/100\n",
            "1000/1000 - 2s - loss: 13.3042 - mse: 13.3042 - mae: 1.5706 - val_loss: 21.8160 - val_mse: 21.8160 - val_mae: 1.5949 - lr: 5.5676e-04 - 2s/epoch - 2ms/step\n",
            "Score for fold 1: loss of 21.815980911254883\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 15.6741 - mse: 15.6741 - mae: 1.5797 - val_loss: 12.3024 - val_mse: 12.3024 - val_mae: 1.5716 - lr: 5.5676e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 15.6503 - mse: 15.6503 - mae: 1.5787 - val_loss: 12.2890 - val_mse: 12.2890 - val_mae: 1.5850 - lr: 5.5676e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 15.6762 - mse: 15.6762 - mae: 1.5775 - val_loss: 12.2610 - val_mse: 12.2610 - val_mae: 1.5810 - lr: 5.5676e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 15.6576 - mse: 15.6576 - mae: 1.5780 - val_loss: 12.2767 - val_mse: 12.2767 - val_mae: 1.5599 - lr: 5.5676e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 15.6465 - mse: 15.6465 - mae: 1.5784 - val_loss: 12.3033 - val_mse: 12.3033 - val_mae: 1.5674 - lr: 5.5676e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 15.6431 - mse: 15.6431 - mae: 1.5765 - val_loss: 12.2761 - val_mse: 12.2761 - val_mae: 1.5777 - lr: 5.5676e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 15.6382 - mse: 15.6382 - mae: 1.5780 - val_loss: 12.3106 - val_mse: 12.3106 - val_mae: 1.5601 - lr: 5.5676e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 2s - loss: 15.6300 - mse: 15.6300 - mae: 1.5760 - val_loss: 12.3345 - val_mse: 12.3345 - val_mae: 1.5543 - lr: 5.5676e-04 - 2s/epoch - 2ms/step\n",
            "Score for fold 2: loss of 12.334481239318848\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 15.6957 - mse: 15.6957 - mae: 1.5783 - val_loss: 12.0224 - val_mse: 12.0224 - val_mae: 1.5662 - lr: 5.5676e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 15.6883 - mse: 15.6883 - mae: 1.5754 - val_loss: 12.1126 - val_mse: 12.1126 - val_mae: 1.5503 - lr: 5.5676e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 15.6956 - mse: 15.6956 - mae: 1.5757 - val_loss: 12.0166 - val_mse: 12.0166 - val_mae: 1.5800 - lr: 5.5676e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 15.6790 - mse: 15.6790 - mae: 1.5766 - val_loss: 12.0462 - val_mse: 12.0462 - val_mae: 1.5418 - lr: 5.5676e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 15.6796 - mse: 15.6796 - mae: 1.5737 - val_loss: 12.0062 - val_mse: 12.0062 - val_mae: 1.5738 - lr: 5.5676e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 15.6713 - mse: 15.6713 - mae: 1.5747 - val_loss: 12.0572 - val_mse: 12.0572 - val_mae: 1.5699 - lr: 5.5676e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 15.6679 - mse: 15.6679 - mae: 1.5774 - val_loss: 12.0558 - val_mse: 12.0558 - val_mae: 1.5446 - lr: 5.5676e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 2s - loss: 15.6575 - mse: 15.6575 - mae: 1.5774 - val_loss: 12.0335 - val_mse: 12.0335 - val_mae: 1.5688 - lr: 5.5676e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 2s - loss: 15.6663 - mse: 15.6663 - mae: 1.5757 - val_loss: 12.0034 - val_mse: 12.0034 - val_mae: 1.5508 - lr: 5.5676e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 2s - loss: 15.6569 - mse: 15.6569 - mae: 1.5760 - val_loss: 12.0488 - val_mse: 12.0488 - val_mae: 1.5630 - lr: 5.5676e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 2s - loss: 15.6725 - mse: 15.6725 - mae: 1.5740 - val_loss: 12.0082 - val_mse: 12.0082 - val_mae: 1.5729 - lr: 5.5676e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 2s - loss: 15.6474 - mse: 15.6474 - mae: 1.5747 - val_loss: 12.0285 - val_mse: 12.0285 - val_mae: 1.5675 - lr: 5.5676e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 13/100\n",
            "1000/1000 - 2s - loss: 15.6494 - mse: 15.6494 - mae: 1.5743 - val_loss: 12.0142 - val_mse: 12.0142 - val_mae: 1.5437 - lr: 5.5676e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 14/100\n",
            "1000/1000 - 2s - loss: 15.6499 - mse: 15.6499 - mae: 1.5738 - val_loss: 12.0078 - val_mse: 12.0078 - val_mae: 1.5669 - lr: 5.5676e-04 - 2s/epoch - 2ms/step\n",
            "Score for fold 3: loss of 12.00780200958252\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 14.8285 - mse: 14.8285 - mae: 1.5752 - val_loss: 15.2728 - val_mse: 15.2728 - val_mae: 1.5498 - lr: 5.5676e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 14.8128 - mse: 14.8128 - mae: 1.5746 - val_loss: 15.2465 - val_mse: 15.2465 - val_mae: 1.5779 - lr: 5.5676e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 14.8048 - mse: 14.8048 - mae: 1.5747 - val_loss: 15.2596 - val_mse: 15.2596 - val_mae: 1.5884 - lr: 5.5676e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 14.7981 - mse: 14.7981 - mae: 1.5744 - val_loss: 15.2507 - val_mse: 15.2507 - val_mae: 1.5871 - lr: 5.5676e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 14.7879 - mse: 14.7879 - mae: 1.5757 - val_loss: 15.3243 - val_mse: 15.3243 - val_mae: 1.5542 - lr: 5.5676e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 14.7985 - mse: 14.7985 - mae: 1.5733 - val_loss: 15.2786 - val_mse: 15.2786 - val_mae: 1.6128 - lr: 5.5676e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 14.7956 - mse: 14.7956 - mae: 1.5705 - val_loss: 15.2807 - val_mse: 15.2807 - val_mae: 1.5814 - lr: 5.5676e-04 - 2s/epoch - 2ms/step\n",
            "Score for fold 4: loss of 15.280656814575195\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 15.2654 - mse: 15.2654 - mae: 1.5764 - val_loss: 13.3917 - val_mse: 13.3917 - val_mae: 1.5704 - lr: 5.5676e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 15.2646 - mse: 15.2646 - mae: 1.5780 - val_loss: 13.3583 - val_mse: 13.3583 - val_mae: 1.5491 - lr: 5.5676e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 15.2642 - mse: 15.2642 - mae: 1.5773 - val_loss: 13.4485 - val_mse: 13.4485 - val_mae: 1.5907 - lr: 5.5676e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 15.2623 - mse: 15.2623 - mae: 1.5808 - val_loss: 13.4229 - val_mse: 13.4229 - val_mae: 1.5466 - lr: 5.5676e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 15.2472 - mse: 15.2472 - mae: 1.5806 - val_loss: 13.3986 - val_mse: 13.3986 - val_mae: 1.5363 - lr: 5.5676e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 15.2506 - mse: 15.2506 - mae: 1.5755 - val_loss: 13.5274 - val_mse: 13.5274 - val_mae: 1.5496 - lr: 5.5676e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 15.2594 - mse: 15.2594 - mae: 1.5806 - val_loss: 13.4257 - val_mse: 13.4257 - val_mae: 1.5813 - lr: 5.5676e-04 - 2s/epoch - 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-10-14 15:14:50,072]\u001b[0m Finished trial#13 resulted in value: 14.974. Current best value is 14.801999999999998 with parameters: {'activation': 'relu', 'num_hidden_layer': 3, 'i': 9, 'learning_rate': 0.002615735581594066}.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Score for fold 5: loss of 13.425674438476562\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 4s - loss: 13.7486 - mse: 13.7486 - mae: 1.6291 - val_loss: 22.6414 - val_mse: 22.6414 - val_mae: 1.6314 - lr: 0.0044 - 4s/epoch - 4ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 4s - loss: 13.4868 - mse: 13.4868 - mae: 1.5960 - val_loss: 22.6235 - val_mse: 22.6235 - val_mae: 1.6141 - lr: 0.0044 - 4s/epoch - 4ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 4s - loss: 13.4096 - mse: 13.4096 - mae: 1.5911 - val_loss: 22.5035 - val_mse: 22.5035 - val_mae: 1.6809 - lr: 0.0044 - 4s/epoch - 4ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 4s - loss: 13.3704 - mse: 13.3704 - mae: 1.5848 - val_loss: 22.5797 - val_mse: 22.5797 - val_mae: 1.6587 - lr: 0.0044 - 4s/epoch - 4ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 4s - loss: 13.3727 - mse: 13.3727 - mae: 1.5962 - val_loss: 22.0954 - val_mse: 22.0954 - val_mae: 1.6419 - lr: 0.0044 - 4s/epoch - 4ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 4s - loss: 13.3773 - mse: 13.3773 - mae: 1.6030 - val_loss: 22.4721 - val_mse: 22.4721 - val_mae: 1.7171 - lr: 0.0044 - 4s/epoch - 4ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 4s - loss: 13.3516 - mse: 13.3516 - mae: 1.5972 - val_loss: 22.2416 - val_mse: 22.2416 - val_mae: 1.6533 - lr: 0.0044 - 4s/epoch - 4ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 4s - loss: 13.2667 - mse: 13.2667 - mae: 1.5838 - val_loss: 22.5957 - val_mse: 22.5957 - val_mae: 1.7197 - lr: 0.0044 - 4s/epoch - 4ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 4s - loss: 13.2502 - mse: 13.2502 - mae: 1.5789 - val_loss: 22.8203 - val_mse: 22.8203 - val_mae: 1.6418 - lr: 0.0044 - 4s/epoch - 4ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 4s - loss: 13.2144 - mse: 13.2144 - mae: 1.5733 - val_loss: 22.9386 - val_mse: 22.9386 - val_mae: 1.6030 - lr: 0.0044 - 4s/epoch - 4ms/step\n",
            "Score for fold 1: loss of 22.938575744628906\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 4s - loss: 14.8701 - mse: 14.8701 - mae: 1.5715 - val_loss: 14.9684 - val_mse: 14.9684 - val_mae: 1.5633 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 4s - loss: 14.7359 - mse: 14.7359 - mae: 1.5671 - val_loss: 14.9730 - val_mse: 14.9730 - val_mae: 1.5876 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 4s - loss: 14.7342 - mse: 14.7342 - mae: 1.5667 - val_loss: 14.9446 - val_mse: 14.9446 - val_mae: 1.5918 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 4s - loss: 14.7269 - mse: 14.7269 - mae: 1.5621 - val_loss: 15.2693 - val_mse: 15.2693 - val_mae: 1.5801 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 4s - loss: 14.7045 - mse: 14.7045 - mae: 1.5642 - val_loss: 15.2683 - val_mse: 15.2683 - val_mae: 1.5581 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 4s - loss: 14.6794 - mse: 14.6794 - mae: 1.5642 - val_loss: 15.3458 - val_mse: 15.3458 - val_mae: 1.5855 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 4s - loss: 14.6546 - mse: 14.6546 - mae: 1.5649 - val_loss: 15.6908 - val_mse: 15.6908 - val_mae: 1.5555 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 4s - loss: 14.6408 - mse: 14.6408 - mae: 1.5663 - val_loss: 15.4275 - val_mse: 15.4275 - val_mae: 1.6366 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Score for fold 2: loss of 15.427509307861328\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 4s - loss: 15.6174 - mse: 15.6174 - mae: 1.5724 - val_loss: 11.5546 - val_mse: 11.5546 - val_mae: 1.5457 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 4s - loss: 15.5981 - mse: 15.5981 - mae: 1.5774 - val_loss: 11.8232 - val_mse: 11.8232 - val_mae: 1.5475 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 4s - loss: 15.5100 - mse: 15.5100 - mae: 1.5726 - val_loss: 11.7198 - val_mse: 11.7198 - val_mae: 1.6519 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 4s - loss: 15.5220 - mse: 15.5220 - mae: 1.5768 - val_loss: 11.9303 - val_mse: 11.9303 - val_mae: 1.5598 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 4s - loss: 15.3794 - mse: 15.3794 - mae: 1.5762 - val_loss: 12.0778 - val_mse: 12.0778 - val_mae: 1.5921 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 4s - loss: 15.4944 - mse: 15.4944 - mae: 1.5787 - val_loss: 11.7285 - val_mse: 11.7285 - val_mae: 1.5870 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Score for fold 3: loss of 11.728532791137695\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 4s - loss: 15.9971 - mse: 15.9971 - mae: 1.5826 - val_loss: 9.6928 - val_mse: 9.6928 - val_mae: 1.5714 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 4s - loss: 15.9284 - mse: 15.9284 - mae: 1.5840 - val_loss: 9.5978 - val_mse: 9.5978 - val_mae: 1.5436 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 4s - loss: 15.9920 - mse: 15.9920 - mae: 1.5869 - val_loss: 9.5589 - val_mse: 9.5589 - val_mae: 1.4722 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 4s - loss: 15.9467 - mse: 15.9467 - mae: 1.5819 - val_loss: 9.6829 - val_mse: 9.6829 - val_mae: 1.5648 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 4s - loss: 15.7904 - mse: 15.7904 - mae: 1.5850 - val_loss: 9.6684 - val_mse: 9.6684 - val_mae: 1.5740 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 4s - loss: 15.8615 - mse: 15.8615 - mae: 1.5808 - val_loss: 9.8077 - val_mse: 9.8077 - val_mae: 1.6035 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 4s - loss: 15.7814 - mse: 15.7814 - mae: 1.5762 - val_loss: 9.6844 - val_mse: 9.6844 - val_mae: 1.5498 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 4s - loss: 15.8505 - mse: 15.8505 - mae: 1.5783 - val_loss: 9.6828 - val_mse: 9.6828 - val_mae: 1.5129 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Score for fold 4: loss of 9.682779312133789\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 4s - loss: 14.4207 - mse: 14.4207 - mae: 1.5821 - val_loss: 15.3372 - val_mse: 15.3372 - val_mae: 1.5853 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 4s - loss: 14.2161 - mse: 14.2161 - mae: 1.5795 - val_loss: 15.5461 - val_mse: 15.5461 - val_mae: 1.5928 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 4s - loss: 14.4467 - mse: 14.4467 - mae: 1.5820 - val_loss: 15.4204 - val_mse: 15.4204 - val_mae: 1.5537 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 4s - loss: 14.3550 - mse: 14.3550 - mae: 1.5793 - val_loss: 15.4732 - val_mse: 15.4732 - val_mae: 1.5132 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 4s - loss: 14.5053 - mse: 14.5053 - mae: 1.5752 - val_loss: 15.5409 - val_mse: 15.5409 - val_mae: 1.5172 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 4s - loss: 14.3022 - mse: 14.3022 - mae: 1.5730 - val_loss: 15.4222 - val_mse: 15.4222 - val_mae: 1.6036 - lr: 0.0010 - 4s/epoch - 4ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-10-14 15:17:18,169]\u001b[0m Finished trial#14 resulted in value: 15.040000000000001. Current best value is 14.801999999999998 with parameters: {'activation': 'relu', 'num_hidden_layer': 3, 'i': 9, 'learning_rate': 0.002615735581594066}.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Score for fold 5: loss of 15.422147750854492\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 6s - loss: 14.2242 - mse: 14.2242 - mae: 1.6899 - val_loss: 23.4931 - val_mse: 23.4931 - val_mae: 1.6278 - lr: 0.0012 - 6s/epoch - 6ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 5s - loss: 13.9260 - mse: 13.9260 - mae: 1.6443 - val_loss: 23.3718 - val_mse: 23.3718 - val_mae: 1.8257 - lr: 0.0012 - 5s/epoch - 5ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 6s - loss: 13.6957 - mse: 13.6957 - mae: 1.6321 - val_loss: 23.3126 - val_mse: 23.3126 - val_mae: 1.6162 - lr: 0.0012 - 6s/epoch - 6ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 5s - loss: 13.6129 - mse: 13.6129 - mae: 1.6270 - val_loss: 22.9278 - val_mse: 22.9278 - val_mae: 1.6556 - lr: 0.0012 - 5s/epoch - 5ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 5s - loss: 13.5982 - mse: 13.5982 - mae: 1.6383 - val_loss: 23.5977 - val_mse: 23.5977 - val_mae: 1.7360 - lr: 0.0012 - 5s/epoch - 5ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 5s - loss: 13.6185 - mse: 13.6185 - mae: 1.6425 - val_loss: 23.1350 - val_mse: 23.1350 - val_mae: 1.7375 - lr: 0.0012 - 5s/epoch - 5ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 5s - loss: 13.4982 - mse: 13.4982 - mae: 1.6381 - val_loss: 23.1825 - val_mse: 23.1825 - val_mae: 1.7285 - lr: 0.0012 - 5s/epoch - 5ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 5s - loss: 13.5959 - mse: 13.5959 - mae: 1.6374 - val_loss: 23.2233 - val_mse: 23.2233 - val_mae: 1.6230 - lr: 0.0012 - 5s/epoch - 5ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 5s - loss: 13.6107 - mse: 13.6107 - mae: 1.6336 - val_loss: 23.0916 - val_mse: 23.0916 - val_mae: 1.6420 - lr: 0.0012 - 5s/epoch - 5ms/step\n",
            "Score for fold 1: loss of 23.09156608581543\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 5s - loss: 16.3602 - mse: 16.3602 - mae: 1.6374 - val_loss: 12.0221 - val_mse: 12.0221 - val_mae: 1.6557 - lr: 0.0010 - 5s/epoch - 5ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 5s - loss: 16.2400 - mse: 16.2400 - mae: 1.6459 - val_loss: 11.7764 - val_mse: 11.7764 - val_mae: 1.5951 - lr: 0.0010 - 5s/epoch - 5ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 5s - loss: 16.3367 - mse: 16.3367 - mae: 1.6442 - val_loss: 12.3511 - val_mse: 12.3511 - val_mae: 1.6943 - lr: 0.0010 - 5s/epoch - 5ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 6s - loss: 16.3011 - mse: 16.3011 - mae: 1.6409 - val_loss: 11.6876 - val_mse: 11.6876 - val_mae: 1.6501 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 6s - loss: 16.2461 - mse: 16.2461 - mae: 1.6385 - val_loss: 11.8634 - val_mse: 11.8634 - val_mae: 1.7376 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 5s - loss: 16.3587 - mse: 16.3587 - mae: 1.6411 - val_loss: 11.7891 - val_mse: 11.7891 - val_mae: 1.5705 - lr: 0.0010 - 5s/epoch - 5ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 6s - loss: 16.3662 - mse: 16.3662 - mae: 1.6458 - val_loss: 11.9255 - val_mse: 11.9255 - val_mae: 1.5651 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 5s - loss: 16.3322 - mse: 16.3322 - mae: 1.6448 - val_loss: 11.7556 - val_mse: 11.7556 - val_mae: 1.5829 - lr: 0.0010 - 5s/epoch - 5ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 6s - loss: 16.2806 - mse: 16.2806 - mae: 1.6425 - val_loss: 11.8801 - val_mse: 11.8801 - val_mae: 1.5513 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
            "Score for fold 2: loss of 11.880130767822266\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 5s - loss: 16.2401 - mse: 16.2401 - mae: 1.6301 - val_loss: 12.1975 - val_mse: 12.1975 - val_mae: 1.8356 - lr: 0.0010 - 5s/epoch - 5ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 5s - loss: 16.2625 - mse: 16.2625 - mae: 1.6505 - val_loss: 12.1495 - val_mse: 12.1495 - val_mae: 1.6531 - lr: 0.0010 - 5s/epoch - 5ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 5s - loss: 16.2508 - mse: 16.2508 - mae: 1.6369 - val_loss: 12.8816 - val_mse: 12.8816 - val_mae: 1.4899 - lr: 0.0010 - 5s/epoch - 5ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 6s - loss: 16.2424 - mse: 16.2424 - mae: 1.6505 - val_loss: 11.9479 - val_mse: 11.9479 - val_mae: 1.5202 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 6s - loss: 16.2975 - mse: 16.2975 - mae: 1.6421 - val_loss: 12.0221 - val_mse: 12.0221 - val_mae: 1.5823 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 6s - loss: 16.2858 - mse: 16.2858 - mae: 1.6361 - val_loss: 12.3243 - val_mse: 12.3243 - val_mae: 1.6915 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 6s - loss: 16.2566 - mse: 16.2566 - mae: 1.6521 - val_loss: 11.7892 - val_mse: 11.7892 - val_mae: 1.7863 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 5s - loss: 16.2505 - mse: 16.2505 - mae: 1.6571 - val_loss: 11.8591 - val_mse: 11.8591 - val_mae: 1.6607 - lr: 0.0010 - 5s/epoch - 5ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 6s - loss: 16.1686 - mse: 16.1686 - mae: 1.6379 - val_loss: 11.8040 - val_mse: 11.8040 - val_mae: 1.5734 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 5s - loss: 16.2603 - mse: 16.2603 - mae: 1.6417 - val_loss: 11.8657 - val_mse: 11.8657 - val_mae: 1.8340 - lr: 0.0010 - 5s/epoch - 5ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 5s - loss: 16.2754 - mse: 16.2754 - mae: 1.6463 - val_loss: 12.0385 - val_mse: 12.0385 - val_mae: 1.7392 - lr: 0.0010 - 5s/epoch - 5ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 5s - loss: 16.2356 - mse: 16.2356 - mae: 1.6324 - val_loss: 12.1602 - val_mse: 12.1602 - val_mae: 1.4915 - lr: 0.0010 - 5s/epoch - 5ms/step\n",
            "Score for fold 3: loss of 12.160171508789062\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 5s - loss: 15.2694 - mse: 15.2694 - mae: 1.6501 - val_loss: 15.6090 - val_mse: 15.6090 - val_mae: 1.8683 - lr: 0.0010 - 5s/epoch - 5ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 5s - loss: 15.2841 - mse: 15.2841 - mae: 1.6516 - val_loss: 15.5157 - val_mse: 15.5157 - val_mae: 1.7757 - lr: 0.0010 - 5s/epoch - 5ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 5s - loss: 15.2701 - mse: 15.2701 - mae: 1.6525 - val_loss: 15.3204 - val_mse: 15.3204 - val_mae: 1.5410 - lr: 0.0010 - 5s/epoch - 5ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 6s - loss: 15.2741 - mse: 15.2741 - mae: 1.6367 - val_loss: 15.8860 - val_mse: 15.8860 - val_mae: 1.5306 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 5s - loss: 15.2782 - mse: 15.2782 - mae: 1.6457 - val_loss: 15.5151 - val_mse: 15.5151 - val_mae: 1.5526 - lr: 0.0010 - 5s/epoch - 5ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 6s - loss: 15.3123 - mse: 15.3123 - mae: 1.6550 - val_loss: 16.4064 - val_mse: 16.4064 - val_mae: 2.1693 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 6s - loss: 15.3104 - mse: 15.3104 - mae: 1.6541 - val_loss: 15.5252 - val_mse: 15.5252 - val_mae: 1.5569 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 6s - loss: 15.3104 - mse: 15.3104 - mae: 1.6488 - val_loss: 15.7721 - val_mse: 15.7721 - val_mae: 1.5273 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
            "Score for fold 4: loss of 15.772144317626953\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 6s - loss: 15.6555 - mse: 15.6555 - mae: 1.6574 - val_loss: 13.9846 - val_mse: 13.9846 - val_mae: 1.6505 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 5s - loss: 15.6340 - mse: 15.6340 - mae: 1.6451 - val_loss: 14.2274 - val_mse: 14.2274 - val_mae: 1.6325 - lr: 0.0010 - 5s/epoch - 5ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 6s - loss: 15.5905 - mse: 15.5905 - mae: 1.6451 - val_loss: 14.1012 - val_mse: 14.1012 - val_mae: 1.5899 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 6s - loss: 15.5741 - mse: 15.5741 - mae: 1.6397 - val_loss: 14.3388 - val_mse: 14.3388 - val_mae: 1.6403 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 6s - loss: 15.5498 - mse: 15.5498 - mae: 1.6369 - val_loss: 14.1836 - val_mse: 14.1836 - val_mae: 1.6118 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 5s - loss: 15.5268 - mse: 15.5268 - mae: 1.6533 - val_loss: 14.7177 - val_mse: 14.7177 - val_mae: 1.7900 - lr: 0.0010 - 5s/epoch - 5ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-10-14 15:21:25,009]\u001b[0m Finished trial#15 resulted in value: 15.523999999999997. Current best value is 14.801999999999998 with parameters: {'activation': 'relu', 'num_hidden_layer': 3, 'i': 9, 'learning_rate': 0.002615735581594066}.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Score for fold 5: loss of 14.717697143554688\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 16.5883 - mse: 16.5883 - mae: 1.6247 - val_loss: 11.1530 - val_mse: 11.1530 - val_mae: 1.5800 - lr: 0.0014 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 16.1696 - mse: 16.1696 - mae: 1.6056 - val_loss: 11.1861 - val_mse: 11.1861 - val_mae: 1.5268 - lr: 0.0014 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 16.1104 - mse: 16.1104 - mae: 1.6013 - val_loss: 11.0845 - val_mse: 11.0845 - val_mae: 1.5764 - lr: 0.0014 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 16.0802 - mse: 16.0802 - mae: 1.5960 - val_loss: 11.1390 - val_mse: 11.1390 - val_mae: 1.6114 - lr: 0.0014 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 16.0709 - mse: 16.0709 - mae: 1.5994 - val_loss: 11.0921 - val_mse: 11.0921 - val_mae: 1.5396 - lr: 0.0014 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 16.0471 - mse: 16.0471 - mae: 1.5960 - val_loss: 11.0432 - val_mse: 11.0432 - val_mae: 1.5873 - lr: 0.0014 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 16.0066 - mse: 16.0066 - mae: 1.5922 - val_loss: 11.1274 - val_mse: 11.1274 - val_mae: 1.5412 - lr: 0.0014 - 2s/epoch - 2ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 2s - loss: 16.0144 - mse: 16.0144 - mae: 1.5933 - val_loss: 11.1177 - val_mse: 11.1177 - val_mae: 1.5405 - lr: 0.0014 - 2s/epoch - 2ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 2s - loss: 15.9548 - mse: 15.9548 - mae: 1.5962 - val_loss: 11.0246 - val_mse: 11.0246 - val_mae: 1.6343 - lr: 0.0014 - 2s/epoch - 2ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 2s - loss: 15.9759 - mse: 15.9759 - mae: 1.5937 - val_loss: 11.0555 - val_mse: 11.0555 - val_mae: 1.5412 - lr: 0.0014 - 2s/epoch - 2ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 2s - loss: 15.9725 - mse: 15.9725 - mae: 1.5933 - val_loss: 11.0061 - val_mse: 11.0061 - val_mae: 1.5870 - lr: 0.0014 - 2s/epoch - 2ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 2s - loss: 15.9443 - mse: 15.9443 - mae: 1.5926 - val_loss: 11.0784 - val_mse: 11.0784 - val_mae: 1.5135 - lr: 0.0014 - 2s/epoch - 2ms/step\n",
            "Epoch 13/100\n",
            "1000/1000 - 2s - loss: 15.9371 - mse: 15.9371 - mae: 1.5889 - val_loss: 11.0134 - val_mse: 11.0134 - val_mae: 1.5575 - lr: 0.0014 - 2s/epoch - 2ms/step\n",
            "Epoch 14/100\n",
            "1000/1000 - 2s - loss: 15.9868 - mse: 15.9868 - mae: 1.5892 - val_loss: 11.0601 - val_mse: 11.0601 - val_mae: 1.5342 - lr: 0.0014 - 2s/epoch - 2ms/step\n",
            "Epoch 15/100\n",
            "1000/1000 - 2s - loss: 15.9565 - mse: 15.9565 - mae: 1.5844 - val_loss: 11.0368 - val_mse: 11.0368 - val_mae: 1.5861 - lr: 0.0014 - 2s/epoch - 2ms/step\n",
            "Epoch 16/100\n",
            "1000/1000 - 2s - loss: 15.9523 - mse: 15.9523 - mae: 1.5880 - val_loss: 11.0454 - val_mse: 11.0454 - val_mae: 1.5870 - lr: 0.0014 - 2s/epoch - 2ms/step\n",
            "Score for fold 1: loss of 11.045365333557129\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 15.6135 - mse: 15.6135 - mae: 1.5883 - val_loss: 12.2331 - val_mse: 12.2331 - val_mae: 1.5301 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 15.5953 - mse: 15.5953 - mae: 1.5857 - val_loss: 12.3801 - val_mse: 12.3801 - val_mae: 1.5153 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 15.5986 - mse: 15.5986 - mae: 1.5833 - val_loss: 12.2186 - val_mse: 12.2186 - val_mae: 1.5373 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 15.5862 - mse: 15.5862 - mae: 1.5828 - val_loss: 12.2396 - val_mse: 12.2396 - val_mae: 1.5702 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 15.5627 - mse: 15.5627 - mae: 1.5815 - val_loss: 12.2332 - val_mse: 12.2332 - val_mae: 1.5905 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 15.5600 - mse: 15.5600 - mae: 1.5852 - val_loss: 12.2499 - val_mse: 12.2499 - val_mae: 1.5304 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 15.5543 - mse: 15.5543 - mae: 1.5841 - val_loss: 12.2600 - val_mse: 12.2600 - val_mae: 1.5229 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 2s - loss: 15.5233 - mse: 15.5233 - mae: 1.5846 - val_loss: 12.2638 - val_mse: 12.2638 - val_mae: 1.5490 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Score for fold 2: loss of 12.263809204101562\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 15.5658 - mse: 15.5658 - mae: 1.5725 - val_loss: 12.2520 - val_mse: 12.2520 - val_mae: 1.5853 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 15.5688 - mse: 15.5688 - mae: 1.5724 - val_loss: 12.2436 - val_mse: 12.2436 - val_mae: 1.6084 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 15.5605 - mse: 15.5605 - mae: 1.5711 - val_loss: 12.2368 - val_mse: 12.2368 - val_mae: 1.5825 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 15.5337 - mse: 15.5337 - mae: 1.5727 - val_loss: 12.3262 - val_mse: 12.3262 - val_mae: 1.5724 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 15.5809 - mse: 15.5809 - mae: 1.5718 - val_loss: 12.3187 - val_mse: 12.3187 - val_mae: 1.5546 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 15.5742 - mse: 15.5742 - mae: 1.5705 - val_loss: 12.3405 - val_mse: 12.3405 - val_mae: 1.6259 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 15.5276 - mse: 15.5276 - mae: 1.5732 - val_loss: 12.1690 - val_mse: 12.1690 - val_mae: 1.6376 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 2s - loss: 15.5393 - mse: 15.5393 - mae: 1.5733 - val_loss: 12.2897 - val_mse: 12.2897 - val_mae: 1.5655 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 2s - loss: 15.5416 - mse: 15.5416 - mae: 1.5714 - val_loss: 12.4153 - val_mse: 12.4153 - val_mae: 1.5628 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 2s - loss: 15.5273 - mse: 15.5273 - mae: 1.5704 - val_loss: 12.3780 - val_mse: 12.3780 - val_mae: 1.5727 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 2s - loss: 15.5409 - mse: 15.5409 - mae: 1.5717 - val_loss: 12.3954 - val_mse: 12.3954 - val_mae: 1.6695 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 2s - loss: 15.5550 - mse: 15.5550 - mae: 1.5715 - val_loss: 12.2880 - val_mse: 12.2880 - val_mae: 1.6166 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Score for fold 3: loss of 12.288052558898926\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 15.2320 - mse: 15.2320 - mae: 1.5742 - val_loss: 13.4026 - val_mse: 13.4026 - val_mae: 1.6083 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 15.2405 - mse: 15.2405 - mae: 1.5759 - val_loss: 13.4332 - val_mse: 13.4332 - val_mae: 1.5720 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 15.2425 - mse: 15.2425 - mae: 1.5733 - val_loss: 13.3891 - val_mse: 13.3891 - val_mae: 1.6197 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 15.2164 - mse: 15.2164 - mae: 1.5762 - val_loss: 13.5607 - val_mse: 13.5607 - val_mae: 1.5623 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 15.2540 - mse: 15.2540 - mae: 1.5709 - val_loss: 13.3672 - val_mse: 13.3672 - val_mae: 1.6292 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 15.1845 - mse: 15.1845 - mae: 1.5714 - val_loss: 13.4316 - val_mse: 13.4316 - val_mae: 1.6116 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 15.2263 - mse: 15.2263 - mae: 1.5713 - val_loss: 13.5143 - val_mse: 13.5143 - val_mae: 1.5837 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 2s - loss: 15.2068 - mse: 15.2068 - mae: 1.5712 - val_loss: 13.4073 - val_mse: 13.4073 - val_mae: 1.6122 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 2s - loss: 15.2011 - mse: 15.2011 - mae: 1.5706 - val_loss: 13.4758 - val_mse: 13.4758 - val_mae: 1.5787 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 2s - loss: 15.1877 - mse: 15.1877 - mae: 1.5727 - val_loss: 13.4326 - val_mse: 13.4326 - val_mae: 1.5955 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Score for fold 4: loss of 13.432635307312012\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 12.1888 - mse: 12.1888 - mae: 1.5793 - val_loss: 25.4805 - val_mse: 25.4805 - val_mae: 1.5988 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 12.1655 - mse: 12.1655 - mae: 1.5735 - val_loss: 25.4346 - val_mse: 25.4346 - val_mae: 1.6361 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 12.1720 - mse: 12.1720 - mae: 1.5741 - val_loss: 25.6799 - val_mse: 25.6799 - val_mae: 1.6007 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 12.1807 - mse: 12.1807 - mae: 1.5759 - val_loss: 25.4852 - val_mse: 25.4852 - val_mae: 1.6427 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 12.1820 - mse: 12.1820 - mae: 1.5744 - val_loss: 25.6319 - val_mse: 25.6319 - val_mae: 1.5862 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 12.1448 - mse: 12.1448 - mae: 1.5742 - val_loss: 26.0758 - val_mse: 26.0758 - val_mae: 1.6254 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 12.2011 - mse: 12.2011 - mae: 1.5716 - val_loss: 25.6851 - val_mse: 25.6851 - val_mae: 1.6158 - lr: 0.0010 - 2s/epoch - 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-10-14 15:23:06,792]\u001b[0m Finished trial#16 resulted in value: 14.943999999999999. Current best value is 14.801999999999998 with parameters: {'activation': 'relu', 'num_hidden_layer': 3, 'i': 9, 'learning_rate': 0.002615735581594066}.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Score for fold 5: loss of 25.68512725830078\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 3s - loss: 15.0280 - mse: 15.0280 - mae: 1.7050 - val_loss: 22.0556 - val_mse: 22.0556 - val_mae: 1.6522 - lr: 3.6073e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 13.9117 - mse: 13.9117 - mae: 1.6053 - val_loss: 21.9298 - val_mse: 21.9298 - val_mae: 1.6639 - lr: 3.6073e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 13.7889 - mse: 13.7889 - mae: 1.6002 - val_loss: 21.7865 - val_mse: 21.7865 - val_mae: 1.6542 - lr: 3.6073e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 13.7057 - mse: 13.7057 - mae: 1.5939 - val_loss: 21.7402 - val_mse: 21.7402 - val_mae: 1.5866 - lr: 3.6073e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 13.6483 - mse: 13.6483 - mae: 1.5871 - val_loss: 21.6701 - val_mse: 21.6701 - val_mae: 1.6055 - lr: 3.6073e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 13.5884 - mse: 13.5884 - mae: 1.5890 - val_loss: 21.6260 - val_mse: 21.6260 - val_mae: 1.6096 - lr: 3.6073e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 13.5615 - mse: 13.5615 - mae: 1.5866 - val_loss: 21.5953 - val_mse: 21.5953 - val_mae: 1.5893 - lr: 3.6073e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 2s - loss: 13.5397 - mse: 13.5397 - mae: 1.5835 - val_loss: 21.5950 - val_mse: 21.5950 - val_mae: 1.5982 - lr: 3.6073e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 2s - loss: 13.5080 - mse: 13.5080 - mae: 1.5803 - val_loss: 21.5340 - val_mse: 21.5340 - val_mae: 1.6136 - lr: 3.6073e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 2s - loss: 13.4834 - mse: 13.4834 - mae: 1.5820 - val_loss: 21.5320 - val_mse: 21.5320 - val_mae: 1.6137 - lr: 3.6073e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 2s - loss: 13.4613 - mse: 13.4613 - mae: 1.5833 - val_loss: 21.5659 - val_mse: 21.5659 - val_mae: 1.5911 - lr: 3.6073e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 2s - loss: 13.4671 - mse: 13.4671 - mae: 1.5830 - val_loss: 21.5640 - val_mse: 21.5640 - val_mae: 1.5743 - lr: 3.6073e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 13/100\n",
            "1000/1000 - 2s - loss: 13.4330 - mse: 13.4330 - mae: 1.5786 - val_loss: 21.5252 - val_mse: 21.5252 - val_mae: 1.6015 - lr: 3.6073e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 14/100\n",
            "1000/1000 - 2s - loss: 13.4282 - mse: 13.4282 - mae: 1.5785 - val_loss: 21.5017 - val_mse: 21.5017 - val_mae: 1.6092 - lr: 3.6073e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 15/100\n",
            "1000/1000 - 2s - loss: 13.4098 - mse: 13.4098 - mae: 1.5795 - val_loss: 21.5268 - val_mse: 21.5268 - val_mae: 1.5981 - lr: 3.6073e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 16/100\n",
            "1000/1000 - 2s - loss: 13.4171 - mse: 13.4171 - mae: 1.5782 - val_loss: 21.5167 - val_mse: 21.5167 - val_mae: 1.5940 - lr: 3.6073e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 17/100\n",
            "1000/1000 - 2s - loss: 13.4013 - mse: 13.4013 - mae: 1.5789 - val_loss: 21.5001 - val_mse: 21.5001 - val_mae: 1.6080 - lr: 3.6073e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 18/100\n",
            "1000/1000 - 2s - loss: 13.3855 - mse: 13.3855 - mae: 1.5789 - val_loss: 21.5397 - val_mse: 21.5397 - val_mae: 1.6508 - lr: 3.6073e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 19/100\n",
            "1000/1000 - 2s - loss: 13.3807 - mse: 13.3807 - mae: 1.5765 - val_loss: 21.4957 - val_mse: 21.4957 - val_mae: 1.5867 - lr: 3.6073e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 20/100\n",
            "1000/1000 - 2s - loss: 13.3713 - mse: 13.3713 - mae: 1.5730 - val_loss: 21.4930 - val_mse: 21.4930 - val_mae: 1.5985 - lr: 3.6073e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 21/100\n",
            "1000/1000 - 2s - loss: 13.3712 - mse: 13.3712 - mae: 1.5759 - val_loss: 21.4676 - val_mse: 21.4676 - val_mae: 1.6024 - lr: 3.6073e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 22/100\n",
            "1000/1000 - 2s - loss: 13.3550 - mse: 13.3550 - mae: 1.5767 - val_loss: 21.4815 - val_mse: 21.4815 - val_mae: 1.5806 - lr: 3.6073e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 23/100\n",
            "1000/1000 - 2s - loss: 13.3624 - mse: 13.3624 - mae: 1.5766 - val_loss: 21.4509 - val_mse: 21.4509 - val_mae: 1.5816 - lr: 3.6073e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 24/100\n",
            "1000/1000 - 2s - loss: 13.3488 - mse: 13.3488 - mae: 1.5754 - val_loss: 21.4502 - val_mse: 21.4502 - val_mae: 1.6240 - lr: 3.6073e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 25/100\n",
            "1000/1000 - 2s - loss: 13.3276 - mse: 13.3276 - mae: 1.5785 - val_loss: 21.4854 - val_mse: 21.4854 - val_mae: 1.5861 - lr: 3.6073e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 26/100\n",
            "1000/1000 - 2s - loss: 13.3486 - mse: 13.3486 - mae: 1.5708 - val_loss: 21.4261 - val_mse: 21.4261 - val_mae: 1.6238 - lr: 3.6073e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 27/100\n",
            "1000/1000 - 2s - loss: 13.3216 - mse: 13.3216 - mae: 1.5750 - val_loss: 21.4123 - val_mse: 21.4123 - val_mae: 1.6234 - lr: 3.6073e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 28/100\n",
            "1000/1000 - 2s - loss: 13.3206 - mse: 13.3206 - mae: 1.5721 - val_loss: 21.4156 - val_mse: 21.4156 - val_mae: 1.5921 - lr: 3.6073e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 29/100\n",
            "1000/1000 - 2s - loss: 13.3151 - mse: 13.3151 - mae: 1.5740 - val_loss: 21.4461 - val_mse: 21.4461 - val_mae: 1.6046 - lr: 3.6073e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 30/100\n",
            "1000/1000 - 2s - loss: 13.2965 - mse: 13.2965 - mae: 1.5771 - val_loss: 21.4175 - val_mse: 21.4175 - val_mae: 1.5869 - lr: 3.6073e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 31/100\n",
            "1000/1000 - 2s - loss: 13.3035 - mse: 13.3035 - mae: 1.5698 - val_loss: 21.3948 - val_mse: 21.3948 - val_mae: 1.5992 - lr: 3.6073e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 32/100\n",
            "1000/1000 - 2s - loss: 13.2926 - mse: 13.2926 - mae: 1.5712 - val_loss: 21.4141 - val_mse: 21.4141 - val_mae: 1.6376 - lr: 3.6073e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 33/100\n",
            "1000/1000 - 2s - loss: 13.2744 - mse: 13.2744 - mae: 1.5739 - val_loss: 21.3680 - val_mse: 21.3680 - val_mae: 1.6083 - lr: 3.6073e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 34/100\n",
            "1000/1000 - 2s - loss: 13.2642 - mse: 13.2642 - mae: 1.5721 - val_loss: 21.4089 - val_mse: 21.4089 - val_mae: 1.5850 - lr: 3.6073e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 35/100\n",
            "1000/1000 - 2s - loss: 13.2611 - mse: 13.2611 - mae: 1.5718 - val_loss: 21.3570 - val_mse: 21.3570 - val_mae: 1.6134 - lr: 3.6073e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 36/100\n",
            "1000/1000 - 2s - loss: 13.2497 - mse: 13.2497 - mae: 1.5710 - val_loss: 21.3822 - val_mse: 21.3822 - val_mae: 1.6010 - lr: 3.6073e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 37/100\n",
            "1000/1000 - 2s - loss: 13.2507 - mse: 13.2507 - mae: 1.5697 - val_loss: 21.3556 - val_mse: 21.3556 - val_mae: 1.6087 - lr: 3.6073e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 38/100\n",
            "1000/1000 - 2s - loss: 13.2527 - mse: 13.2527 - mae: 1.5664 - val_loss: 21.3604 - val_mse: 21.3604 - val_mae: 1.6465 - lr: 3.6073e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 39/100\n",
            "1000/1000 - 2s - loss: 13.2312 - mse: 13.2312 - mae: 1.5717 - val_loss: 21.3553 - val_mse: 21.3553 - val_mae: 1.6174 - lr: 3.6073e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 40/100\n",
            "1000/1000 - 2s - loss: 13.2314 - mse: 13.2314 - mae: 1.5695 - val_loss: 21.3823 - val_mse: 21.3823 - val_mae: 1.5752 - lr: 3.6073e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 41/100\n",
            "1000/1000 - 2s - loss: 13.2233 - mse: 13.2233 - mae: 1.5715 - val_loss: 21.3711 - val_mse: 21.3711 - val_mae: 1.5989 - lr: 3.6073e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 42/100\n",
            "1000/1000 - 2s - loss: 13.2089 - mse: 13.2089 - mae: 1.5695 - val_loss: 21.3378 - val_mse: 21.3378 - val_mae: 1.5971 - lr: 3.6073e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 43/100\n",
            "1000/1000 - 2s - loss: 13.2081 - mse: 13.2081 - mae: 1.5697 - val_loss: 21.3407 - val_mse: 21.3407 - val_mae: 1.5933 - lr: 3.6073e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 44/100\n",
            "1000/1000 - 2s - loss: 13.1893 - mse: 13.1893 - mae: 1.5669 - val_loss: 21.3196 - val_mse: 21.3196 - val_mae: 1.6197 - lr: 3.6073e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 45/100\n",
            "1000/1000 - 2s - loss: 13.1908 - mse: 13.1908 - mae: 1.5689 - val_loss: 21.4265 - val_mse: 21.4265 - val_mae: 1.5570 - lr: 3.6073e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 46/100\n",
            "1000/1000 - 2s - loss: 13.1985 - mse: 13.1985 - mae: 1.5681 - val_loss: 21.3672 - val_mse: 21.3672 - val_mae: 1.5809 - lr: 3.6073e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 47/100\n",
            "1000/1000 - 2s - loss: 13.1837 - mse: 13.1837 - mae: 1.5692 - val_loss: 21.4248 - val_mse: 21.4248 - val_mae: 1.5927 - lr: 3.6073e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 48/100\n",
            "1000/1000 - 2s - loss: 13.1842 - mse: 13.1842 - mae: 1.5639 - val_loss: 21.3870 - val_mse: 21.3870 - val_mae: 1.6319 - lr: 3.6073e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 49/100\n",
            "1000/1000 - 2s - loss: 13.1772 - mse: 13.1772 - mae: 1.5652 - val_loss: 21.3155 - val_mse: 21.3155 - val_mae: 1.5942 - lr: 3.6073e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 50/100\n",
            "1000/1000 - 2s - loss: 13.1755 - mse: 13.1755 - mae: 1.5709 - val_loss: 21.3797 - val_mse: 21.3797 - val_mae: 1.5805 - lr: 3.6073e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 51/100\n",
            "1000/1000 - 2s - loss: 13.1714 - mse: 13.1714 - mae: 1.5680 - val_loss: 21.3113 - val_mse: 21.3113 - val_mae: 1.5956 - lr: 3.6073e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 52/100\n",
            "1000/1000 - 2s - loss: 13.1694 - mse: 13.1694 - mae: 1.5657 - val_loss: 21.3556 - val_mse: 21.3556 - val_mae: 1.5731 - lr: 3.6073e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 53/100\n",
            "1000/1000 - 2s - loss: 13.1756 - mse: 13.1756 - mae: 1.5689 - val_loss: 21.3347 - val_mse: 21.3347 - val_mae: 1.5777 - lr: 3.6073e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 54/100\n",
            "1000/1000 - 2s - loss: 13.1606 - mse: 13.1606 - mae: 1.5692 - val_loss: 21.3126 - val_mse: 21.3126 - val_mae: 1.5956 - lr: 3.6073e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 55/100\n",
            "1000/1000 - 2s - loss: 13.1618 - mse: 13.1618 - mae: 1.5674 - val_loss: 21.3119 - val_mse: 21.3119 - val_mae: 1.6039 - lr: 3.6073e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 56/100\n",
            "1000/1000 - 2s - loss: 13.1570 - mse: 13.1570 - mae: 1.5665 - val_loss: 21.3260 - val_mse: 21.3260 - val_mae: 1.6058 - lr: 3.6073e-04 - 2s/epoch - 2ms/step\n",
            "Score for fold 1: loss of 21.325960159301758\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 15.3659 - mse: 15.3659 - mae: 1.5687 - val_loss: 12.5441 - val_mse: 12.5441 - val_mae: 1.5685 - lr: 3.6073e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 15.3695 - mse: 15.3695 - mae: 1.5655 - val_loss: 12.6550 - val_mse: 12.6550 - val_mae: 1.5740 - lr: 3.6073e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 15.3731 - mse: 15.3731 - mae: 1.5662 - val_loss: 12.5192 - val_mse: 12.5192 - val_mae: 1.5852 - lr: 3.6073e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 15.3575 - mse: 15.3575 - mae: 1.5687 - val_loss: 12.5876 - val_mse: 12.5876 - val_mae: 1.5503 - lr: 3.6073e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 15.3382 - mse: 15.3382 - mae: 1.5641 - val_loss: 12.6169 - val_mse: 12.6169 - val_mae: 1.5711 - lr: 3.6073e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 15.3433 - mse: 15.3433 - mae: 1.5656 - val_loss: 12.6506 - val_mse: 12.6506 - val_mae: 1.6064 - lr: 3.6073e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 15.3356 - mse: 15.3356 - mae: 1.5669 - val_loss: 12.6205 - val_mse: 12.6205 - val_mae: 1.5742 - lr: 3.6073e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 2s - loss: 15.3260 - mse: 15.3260 - mae: 1.5649 - val_loss: 12.6253 - val_mse: 12.6253 - val_mae: 1.5667 - lr: 3.6073e-04 - 2s/epoch - 2ms/step\n",
            "Score for fold 2: loss of 12.625273704528809\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 14.9986 - mse: 14.9986 - mae: 1.5649 - val_loss: 13.9792 - val_mse: 13.9792 - val_mae: 1.5778 - lr: 3.6073e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 15.0113 - mse: 15.0113 - mae: 1.5635 - val_loss: 13.8928 - val_mse: 13.8928 - val_mae: 1.6270 - lr: 3.6073e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 14.9961 - mse: 14.9961 - mae: 1.5691 - val_loss: 13.9904 - val_mse: 13.9904 - val_mae: 1.5953 - lr: 3.6073e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 14.9828 - mse: 14.9828 - mae: 1.5667 - val_loss: 14.0565 - val_mse: 14.0565 - val_mae: 1.6452 - lr: 3.6073e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 14.9763 - mse: 14.9763 - mae: 1.5627 - val_loss: 14.0559 - val_mse: 14.0559 - val_mae: 1.6011 - lr: 3.6073e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 14.9619 - mse: 14.9619 - mae: 1.5684 - val_loss: 14.0095 - val_mse: 14.0095 - val_mae: 1.5948 - lr: 3.6073e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 14.9798 - mse: 14.9798 - mae: 1.5669 - val_loss: 13.9828 - val_mse: 13.9828 - val_mae: 1.5755 - lr: 3.6073e-04 - 2s/epoch - 2ms/step\n",
            "Score for fold 3: loss of 13.98275375366211\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 15.5360 - mse: 15.5360 - mae: 1.5771 - val_loss: 11.6685 - val_mse: 11.6685 - val_mae: 1.5376 - lr: 3.6073e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 15.5560 - mse: 15.5560 - mae: 1.5721 - val_loss: 11.6164 - val_mse: 11.6164 - val_mae: 1.5355 - lr: 3.6073e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 15.5260 - mse: 15.5260 - mae: 1.5711 - val_loss: 11.7228 - val_mse: 11.7228 - val_mae: 1.5378 - lr: 3.6073e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 15.5075 - mse: 15.5075 - mae: 1.5724 - val_loss: 11.6083 - val_mse: 11.6083 - val_mae: 1.5632 - lr: 3.6073e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 15.5032 - mse: 15.5032 - mae: 1.5706 - val_loss: 11.6885 - val_mse: 11.6885 - val_mae: 1.5447 - lr: 3.6073e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 15.4817 - mse: 15.4817 - mae: 1.5728 - val_loss: 11.6295 - val_mse: 11.6295 - val_mae: 1.5431 - lr: 3.6073e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 15.4847 - mse: 15.4847 - mae: 1.5680 - val_loss: 11.6325 - val_mse: 11.6325 - val_mae: 1.5465 - lr: 3.6073e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 2s - loss: 15.4887 - mse: 15.4887 - mae: 1.5679 - val_loss: 11.6811 - val_mse: 11.6811 - val_mae: 1.5398 - lr: 3.6073e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 2s - loss: 15.4847 - mse: 15.4847 - mae: 1.5721 - val_loss: 11.7555 - val_mse: 11.7555 - val_mae: 1.5548 - lr: 3.6073e-04 - 2s/epoch - 2ms/step\n",
            "Score for fold 4: loss of 11.75551700592041\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 14.7783 - mse: 14.7783 - mae: 1.5752 - val_loss: 14.4444 - val_mse: 14.4444 - val_mae: 1.5548 - lr: 3.6073e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 14.7660 - mse: 14.7660 - mae: 1.5740 - val_loss: 14.4516 - val_mse: 14.4516 - val_mae: 1.5654 - lr: 3.6073e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 14.7447 - mse: 14.7447 - mae: 1.5767 - val_loss: 14.4965 - val_mse: 14.4965 - val_mae: 1.5516 - lr: 3.6073e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 14.7140 - mse: 14.7140 - mae: 1.5756 - val_loss: 14.5210 - val_mse: 14.5210 - val_mae: 1.5116 - lr: 3.6073e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 14.7240 - mse: 14.7240 - mae: 1.5755 - val_loss: 14.4958 - val_mse: 14.4958 - val_mae: 1.5501 - lr: 3.6073e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 14.7140 - mse: 14.7140 - mae: 1.5777 - val_loss: 14.5109 - val_mse: 14.5109 - val_mae: 1.5138 - lr: 3.6073e-04 - 2s/epoch - 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-10-14 15:25:58,415]\u001b[0m Finished trial#17 resulted in value: 14.841999999999999. Current best value is 14.801999999999998 with parameters: {'activation': 'relu', 'num_hidden_layer': 3, 'i': 9, 'learning_rate': 0.002615735581594066}.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Score for fold 5: loss of 14.510859489440918\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 12s - loss: 16.4427 - mse: 16.4427 - mae: 1.6759 - val_loss: 12.6418 - val_mse: 12.6418 - val_mae: 1.6185 - lr: 0.0036 - 12s/epoch - 12ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 11s - loss: 15.9452 - mse: 15.9452 - mae: 1.6309 - val_loss: 12.7612 - val_mse: 12.7612 - val_mae: 1.5728 - lr: 0.0036 - 11s/epoch - 11ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 12s - loss: 15.8549 - mse: 15.8549 - mae: 1.6281 - val_loss: 12.4889 - val_mse: 12.4889 - val_mae: 1.6529 - lr: 0.0036 - 12s/epoch - 12ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 11s - loss: 15.8755 - mse: 15.8755 - mae: 1.6183 - val_loss: 12.6269 - val_mse: 12.6269 - val_mae: 1.6569 - lr: 0.0036 - 11s/epoch - 11ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 12s - loss: 15.8303 - mse: 15.8303 - mae: 1.6171 - val_loss: 12.8479 - val_mse: 12.8479 - val_mae: 1.5683 - lr: 0.0036 - 12s/epoch - 12ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 11s - loss: 15.7935 - mse: 15.7935 - mae: 1.6174 - val_loss: 12.5954 - val_mse: 12.5954 - val_mae: 1.6518 - lr: 0.0036 - 11s/epoch - 11ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 11s - loss: 15.8357 - mse: 15.8357 - mae: 1.6228 - val_loss: 12.3659 - val_mse: 12.3659 - val_mae: 1.6082 - lr: 0.0036 - 11s/epoch - 11ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 11s - loss: 15.8091 - mse: 15.8091 - mae: 1.6156 - val_loss: 12.4664 - val_mse: 12.4664 - val_mae: 1.6853 - lr: 0.0036 - 11s/epoch - 11ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 11s - loss: 15.8681 - mse: 15.8681 - mae: 1.6258 - val_loss: 12.8749 - val_mse: 12.8749 - val_mae: 1.8783 - lr: 0.0036 - 11s/epoch - 11ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 11s - loss: 15.9633 - mse: 15.9633 - mae: 1.6275 - val_loss: 12.5240 - val_mse: 12.5240 - val_mae: 1.5839 - lr: 0.0036 - 11s/epoch - 11ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 11s - loss: 15.9056 - mse: 15.9056 - mae: 1.6310 - val_loss: 13.6630 - val_mse: 13.6630 - val_mae: 1.5218 - lr: 0.0036 - 11s/epoch - 11ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 11s - loss: 15.8922 - mse: 15.8922 - mae: 1.6347 - val_loss: 12.4725 - val_mse: 12.4725 - val_mae: 1.6052 - lr: 0.0036 - 11s/epoch - 11ms/step\n",
            "Score for fold 1: loss of 12.472458839416504\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 11s - loss: 12.3258 - mse: 12.3258 - mae: 1.5904 - val_loss: 26.2518 - val_mse: 26.2518 - val_mae: 1.6610 - lr: 0.0010 - 11s/epoch - 11ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 11s - loss: 12.4074 - mse: 12.4074 - mae: 1.5968 - val_loss: 25.9996 - val_mse: 25.9996 - val_mae: 1.6852 - lr: 0.0010 - 11s/epoch - 11ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 11s - loss: 12.3789 - mse: 12.3789 - mae: 1.5942 - val_loss: 25.9405 - val_mse: 25.9405 - val_mae: 1.7741 - lr: 0.0010 - 11s/epoch - 11ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 11s - loss: 12.4208 - mse: 12.4208 - mae: 1.6014 - val_loss: 25.9543 - val_mse: 25.9543 - val_mae: 1.7309 - lr: 0.0010 - 11s/epoch - 11ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 11s - loss: 12.4360 - mse: 12.4360 - mae: 1.6044 - val_loss: 26.7164 - val_mse: 26.7164 - val_mae: 1.6280 - lr: 0.0010 - 11s/epoch - 11ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 11s - loss: 12.4663 - mse: 12.4663 - mae: 1.5956 - val_loss: 26.0252 - val_mse: 26.0252 - val_mae: 1.7021 - lr: 0.0010 - 11s/epoch - 11ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 11s - loss: 12.4597 - mse: 12.4597 - mae: 1.6024 - val_loss: 26.3516 - val_mse: 26.3516 - val_mae: 1.6486 - lr: 0.0010 - 11s/epoch - 11ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 11s - loss: 12.4081 - mse: 12.4081 - mae: 1.6011 - val_loss: 26.4857 - val_mse: 26.4857 - val_mae: 1.6366 - lr: 0.0010 - 11s/epoch - 11ms/step\n",
            "Score for fold 2: loss of 26.485652923583984\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 11s - loss: 15.7291 - mse: 15.7291 - mae: 1.6264 - val_loss: 13.2155 - val_mse: 13.2155 - val_mae: 1.5628 - lr: 0.0010 - 11s/epoch - 11ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 11s - loss: 15.7544 - mse: 15.7544 - mae: 1.6317 - val_loss: 13.4993 - val_mse: 13.4993 - val_mae: 1.5282 - lr: 0.0010 - 11s/epoch - 11ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 12s - loss: 15.6487 - mse: 15.6487 - mae: 1.6257 - val_loss: 13.1562 - val_mse: 13.1562 - val_mae: 1.6792 - lr: 0.0010 - 12s/epoch - 12ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 13s - loss: 15.8863 - mse: 15.8863 - mae: 1.6274 - val_loss: 13.1105 - val_mse: 13.1105 - val_mae: 1.6818 - lr: 0.0010 - 13s/epoch - 13ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 12s - loss: 15.7768 - mse: 15.7768 - mae: 1.6294 - val_loss: 13.2019 - val_mse: 13.2019 - val_mae: 1.5763 - lr: 0.0010 - 12s/epoch - 12ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 12s - loss: 15.7776 - mse: 15.7776 - mae: 1.6304 - val_loss: 13.0412 - val_mse: 13.0412 - val_mae: 1.6204 - lr: 0.0010 - 12s/epoch - 12ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 13s - loss: 15.8228 - mse: 15.8228 - mae: 1.6355 - val_loss: 13.3408 - val_mse: 13.3408 - val_mae: 1.5647 - lr: 0.0010 - 13s/epoch - 13ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 13s - loss: 15.9894 - mse: 15.9894 - mae: 1.6375 - val_loss: 13.5625 - val_mse: 13.5625 - val_mae: 1.7478 - lr: 0.0010 - 13s/epoch - 13ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 12s - loss: 15.8988 - mse: 15.8988 - mae: 1.6353 - val_loss: 13.7217 - val_mse: 13.7217 - val_mae: 1.5425 - lr: 0.0010 - 12s/epoch - 12ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 12s - loss: 15.7630 - mse: 15.7630 - mae: 1.6338 - val_loss: 13.0533 - val_mse: 13.0533 - val_mae: 1.6100 - lr: 0.0010 - 12s/epoch - 12ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 12s - loss: 15.7071 - mse: 15.7071 - mae: 1.6319 - val_loss: 13.3077 - val_mse: 13.3077 - val_mae: 1.5725 - lr: 0.0010 - 12s/epoch - 12ms/step\n",
            "Score for fold 3: loss of 13.307710647583008\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 12s - loss: 15.3789 - mse: 15.3789 - mae: 1.6250 - val_loss: 14.6908 - val_mse: 14.6908 - val_mae: 1.6084 - lr: 0.0010 - 12s/epoch - 12ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 13s - loss: 15.5166 - mse: 15.5166 - mae: 1.6286 - val_loss: 14.6062 - val_mse: 14.6062 - val_mae: 1.6236 - lr: 0.0010 - 13s/epoch - 13ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 13s - loss: 15.4467 - mse: 15.4467 - mae: 1.6273 - val_loss: 14.4352 - val_mse: 14.4352 - val_mae: 1.6654 - lr: 0.0010 - 13s/epoch - 13ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 12s - loss: 15.4310 - mse: 15.4310 - mae: 1.6277 - val_loss: 15.1323 - val_mse: 15.1323 - val_mae: 1.8166 - lr: 0.0010 - 12s/epoch - 12ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 12s - loss: 15.4125 - mse: 15.4125 - mae: 1.6313 - val_loss: 15.8400 - val_mse: 15.8400 - val_mae: 1.5767 - lr: 0.0010 - 12s/epoch - 12ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 12s - loss: 15.4267 - mse: 15.4267 - mae: 1.6222 - val_loss: 14.9853 - val_mse: 14.9853 - val_mae: 1.7882 - lr: 0.0010 - 12s/epoch - 12ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 12s - loss: 15.3324 - mse: 15.3324 - mae: 1.6317 - val_loss: 14.7176 - val_mse: 14.7176 - val_mae: 1.6118 - lr: 0.0010 - 12s/epoch - 12ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 12s - loss: 15.5338 - mse: 15.5338 - mae: 1.6339 - val_loss: 14.9354 - val_mse: 14.9354 - val_mae: 1.5899 - lr: 0.0010 - 12s/epoch - 12ms/step\n",
            "Score for fold 4: loss of 14.935367584228516\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 11s - loss: 16.6222 - mse: 16.6222 - mae: 1.6454 - val_loss: 9.4519 - val_mse: 9.4519 - val_mae: 1.5298 - lr: 0.0010 - 11s/epoch - 11ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 11s - loss: 16.8194 - mse: 16.8194 - mae: 1.6474 - val_loss: 9.3057 - val_mse: 9.3057 - val_mae: 1.5862 - lr: 0.0010 - 11s/epoch - 11ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 12s - loss: 16.7266 - mse: 16.7266 - mae: 1.6425 - val_loss: 9.2891 - val_mse: 9.2891 - val_mae: 1.5689 - lr: 0.0010 - 12s/epoch - 12ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 12s - loss: 16.7474 - mse: 16.7474 - mae: 1.6478 - val_loss: 9.2946 - val_mse: 9.2946 - val_mae: 1.5894 - lr: 0.0010 - 12s/epoch - 12ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 12s - loss: 16.7558 - mse: 16.7558 - mae: 1.6365 - val_loss: 10.4392 - val_mse: 10.4392 - val_mae: 1.5350 - lr: 0.0010 - 12s/epoch - 12ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 12s - loss: 16.8243 - mse: 16.8243 - mae: 1.6392 - val_loss: 9.3097 - val_mse: 9.3097 - val_mae: 1.6078 - lr: 0.0010 - 12s/epoch - 12ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 11s - loss: 16.7868 - mse: 16.7868 - mae: 1.6433 - val_loss: 9.3103 - val_mse: 9.3103 - val_mae: 1.5634 - lr: 0.0010 - 11s/epoch - 11ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 11s - loss: 16.7939 - mse: 16.7939 - mae: 1.6378 - val_loss: 9.3539 - val_mse: 9.3539 - val_mae: 1.6175 - lr: 0.0010 - 11s/epoch - 11ms/step\n",
            "Score for fold 5: loss of 9.353930473327637\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-10-14 15:35:15,387]\u001b[0m Finished trial#18 resulted in value: 15.312000000000001. Current best value is 14.801999999999998 with parameters: {'activation': 'relu', 'num_hidden_layer': 3, 'i': 9, 'learning_rate': 0.002615735581594066}.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 4s - loss: 15.1270 - mse: 15.1270 - mae: 1.7296 - val_loss: 21.1150 - val_mse: 21.1150 - val_mae: 1.7777 - lr: 0.0099 - 4s/epoch - 4ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 3s - loss: 14.8063 - mse: 14.8063 - mae: 1.6936 - val_loss: 20.9306 - val_mse: 20.9306 - val_mae: 1.5889 - lr: 0.0099 - 3s/epoch - 3ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 3s - loss: 14.8786 - mse: 14.8786 - mae: 1.6937 - val_loss: 21.7731 - val_mse: 21.7731 - val_mae: 1.8973 - lr: 0.0099 - 3s/epoch - 3ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 3s - loss: 14.9258 - mse: 14.9258 - mae: 1.7005 - val_loss: 21.2090 - val_mse: 21.2090 - val_mae: 1.6005 - lr: 0.0099 - 3s/epoch - 3ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 3s - loss: 14.7004 - mse: 14.7004 - mae: 1.6872 - val_loss: 21.0323 - val_mse: 21.0323 - val_mae: 1.6483 - lr: 0.0099 - 3s/epoch - 3ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 3s - loss: 14.7678 - mse: 14.7678 - mae: 1.6919 - val_loss: 20.9527 - val_mse: 20.9527 - val_mae: 1.6810 - lr: 0.0099 - 3s/epoch - 3ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 3s - loss: 14.8417 - mse: 14.8417 - mae: 1.6956 - val_loss: 21.0989 - val_mse: 21.0989 - val_mae: 1.7319 - lr: 0.0099 - 3s/epoch - 3ms/step\n",
            "Score for fold 1: loss of 21.098886489868164\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 3s - loss: 16.6187 - mse: 16.6187 - mae: 1.6553 - val_loss: 11.9966 - val_mse: 11.9966 - val_mae: 1.6260 - lr: 0.0020 - 3s/epoch - 3ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 3s - loss: 16.5706 - mse: 16.5706 - mae: 1.6544 - val_loss: 11.9509 - val_mse: 11.9509 - val_mae: 1.6352 - lr: 0.0020 - 3s/epoch - 3ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 3s - loss: 16.6435 - mse: 16.6435 - mae: 1.6574 - val_loss: 12.1941 - val_mse: 12.1941 - val_mae: 1.5981 - lr: 0.0020 - 3s/epoch - 3ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 3s - loss: 16.6313 - mse: 16.6313 - mae: 1.6587 - val_loss: 12.0811 - val_mse: 12.0811 - val_mae: 1.7090 - lr: 0.0020 - 3s/epoch - 3ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 3s - loss: 16.6587 - mse: 16.6587 - mae: 1.6597 - val_loss: 12.3355 - val_mse: 12.3355 - val_mae: 1.5788 - lr: 0.0020 - 3s/epoch - 3ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 3s - loss: 16.6702 - mse: 16.6702 - mae: 1.6558 - val_loss: 12.3043 - val_mse: 12.3043 - val_mae: 1.5801 - lr: 0.0020 - 3s/epoch - 3ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 3s - loss: 16.6313 - mse: 16.6313 - mae: 1.6580 - val_loss: 12.0345 - val_mse: 12.0345 - val_mae: 1.6270 - lr: 0.0020 - 3s/epoch - 3ms/step\n",
            "Score for fold 2: loss of 12.034478187561035\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 3s - loss: 15.6017 - mse: 15.6017 - mae: 1.6485 - val_loss: 15.8332 - val_mse: 15.8332 - val_mae: 1.6425 - lr: 0.0010 - 3s/epoch - 3ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 3s - loss: 15.6509 - mse: 15.6509 - mae: 1.6457 - val_loss: 15.8232 - val_mse: 15.8232 - val_mae: 1.6191 - lr: 0.0010 - 3s/epoch - 3ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 3s - loss: 15.7167 - mse: 15.7167 - mae: 1.6481 - val_loss: 16.0382 - val_mse: 16.0382 - val_mae: 1.6065 - lr: 0.0010 - 3s/epoch - 3ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 3s - loss: 15.6544 - mse: 15.6544 - mae: 1.6471 - val_loss: 16.3753 - val_mse: 16.3753 - val_mae: 1.5913 - lr: 0.0010 - 3s/epoch - 3ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 3s - loss: 15.7198 - mse: 15.7198 - mae: 1.6517 - val_loss: 15.8004 - val_mse: 15.8004 - val_mae: 1.6491 - lr: 0.0010 - 3s/epoch - 3ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 3s - loss: 15.6192 - mse: 15.6192 - mae: 1.6500 - val_loss: 15.7989 - val_mse: 15.7989 - val_mae: 1.6644 - lr: 0.0010 - 3s/epoch - 3ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 3s - loss: 15.5979 - mse: 15.5979 - mae: 1.6471 - val_loss: 15.8390 - val_mse: 15.8390 - val_mae: 1.6329 - lr: 0.0010 - 3s/epoch - 3ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 3s - loss: 15.6632 - mse: 15.6632 - mae: 1.6511 - val_loss: 15.9276 - val_mse: 15.9276 - val_mae: 1.6005 - lr: 0.0010 - 3s/epoch - 3ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 3s - loss: 15.6759 - mse: 15.6759 - mae: 1.6500 - val_loss: 15.8818 - val_mse: 15.8818 - val_mae: 1.6453 - lr: 0.0010 - 3s/epoch - 3ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 3s - loss: 15.7105 - mse: 15.7105 - mae: 1.6432 - val_loss: 15.7819 - val_mse: 15.7819 - val_mae: 1.6383 - lr: 0.0010 - 3s/epoch - 3ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 3s - loss: 15.6960 - mse: 15.6960 - mae: 1.6467 - val_loss: 15.8946 - val_mse: 15.8946 - val_mae: 1.6085 - lr: 0.0010 - 3s/epoch - 3ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 3s - loss: 15.6618 - mse: 15.6618 - mae: 1.6484 - val_loss: 15.9237 - val_mse: 15.9237 - val_mae: 1.6173 - lr: 0.0010 - 3s/epoch - 3ms/step\n",
            "Epoch 13/100\n",
            "1000/1000 - 3s - loss: 15.6802 - mse: 15.6802 - mae: 1.6517 - val_loss: 15.8727 - val_mse: 15.8727 - val_mae: 1.6290 - lr: 0.0010 - 3s/epoch - 3ms/step\n",
            "Epoch 14/100\n",
            "1000/1000 - 3s - loss: 15.6534 - mse: 15.6534 - mae: 1.6531 - val_loss: 16.0037 - val_mse: 16.0037 - val_mae: 1.5927 - lr: 0.0010 - 3s/epoch - 3ms/step\n",
            "Epoch 15/100\n",
            "1000/1000 - 3s - loss: 15.6189 - mse: 15.6189 - mae: 1.6504 - val_loss: 15.8794 - val_mse: 15.8794 - val_mae: 1.6887 - lr: 0.0010 - 3s/epoch - 3ms/step\n",
            "Score for fold 3: loss of 15.879376411437988\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 3s - loss: 16.3133 - mse: 16.3133 - mae: 1.6555 - val_loss: 13.2956 - val_mse: 13.2956 - val_mae: 1.5697 - lr: 0.0010 - 3s/epoch - 3ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 3s - loss: 16.3590 - mse: 16.3590 - mae: 1.6525 - val_loss: 13.2319 - val_mse: 13.2319 - val_mae: 1.6135 - lr: 0.0010 - 3s/epoch - 3ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 3s - loss: 16.3460 - mse: 16.3460 - mae: 1.6491 - val_loss: 13.3724 - val_mse: 13.3724 - val_mae: 1.5770 - lr: 0.0010 - 3s/epoch - 3ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 3s - loss: 16.3266 - mse: 16.3266 - mae: 1.6542 - val_loss: 13.2027 - val_mse: 13.2027 - val_mae: 1.6045 - lr: 0.0010 - 3s/epoch - 3ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 3s - loss: 16.2897 - mse: 16.2897 - mae: 1.6553 - val_loss: 13.2138 - val_mse: 13.2138 - val_mae: 1.5767 - lr: 0.0010 - 3s/epoch - 3ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 3s - loss: 16.3578 - mse: 16.3578 - mae: 1.6573 - val_loss: 13.1758 - val_mse: 13.1758 - val_mae: 1.6183 - lr: 0.0010 - 3s/epoch - 3ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 3s - loss: 16.2859 - mse: 16.2859 - mae: 1.6532 - val_loss: 13.2728 - val_mse: 13.2728 - val_mae: 1.5665 - lr: 0.0010 - 3s/epoch - 3ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 3s - loss: 16.3331 - mse: 16.3331 - mae: 1.6536 - val_loss: 13.2163 - val_mse: 13.2163 - val_mae: 1.5793 - lr: 0.0010 - 3s/epoch - 3ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 3s - loss: 16.3942 - mse: 16.3942 - mae: 1.6530 - val_loss: 13.7689 - val_mse: 13.7689 - val_mae: 1.5692 - lr: 0.0010 - 3s/epoch - 3ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 3s - loss: 16.3472 - mse: 16.3472 - mae: 1.6542 - val_loss: 13.4875 - val_mse: 13.4875 - val_mae: 1.5693 - lr: 0.0010 - 3s/epoch - 3ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 3s - loss: 16.3956 - mse: 16.3956 - mae: 1.6608 - val_loss: 13.2607 - val_mse: 13.2607 - val_mae: 1.6447 - lr: 0.0010 - 3s/epoch - 3ms/step\n",
            "Score for fold 4: loss of 13.260700225830078\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 3s - loss: 15.5857 - mse: 15.5857 - mae: 1.6280 - val_loss: 16.2059 - val_mse: 16.2059 - val_mae: 1.7006 - lr: 0.0010 - 3s/epoch - 3ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 3s - loss: 15.5989 - mse: 15.5989 - mae: 1.6318 - val_loss: 16.9213 - val_mse: 16.9213 - val_mae: 1.6631 - lr: 0.0010 - 3s/epoch - 3ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 3s - loss: 15.5687 - mse: 15.5687 - mae: 1.6281 - val_loss: 17.1020 - val_mse: 17.1020 - val_mae: 1.6567 - lr: 0.0010 - 3s/epoch - 3ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 3s - loss: 15.5696 - mse: 15.5696 - mae: 1.6315 - val_loss: 16.2704 - val_mse: 16.2704 - val_mae: 1.6983 - lr: 0.0010 - 3s/epoch - 3ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 3s - loss: 15.5546 - mse: 15.5546 - mae: 1.6274 - val_loss: 16.2737 - val_mse: 16.2737 - val_mae: 1.7004 - lr: 0.0010 - 3s/epoch - 3ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 3s - loss: 15.5296 - mse: 15.5296 - mae: 1.6309 - val_loss: 16.2719 - val_mse: 16.2719 - val_mae: 1.8137 - lr: 0.0010 - 3s/epoch - 3ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-10-14 15:37:36,949]\u001b[0m Finished trial#19 resulted in value: 15.708000000000002. Current best value is 14.801999999999998 with parameters: {'activation': 'relu', 'num_hidden_layer': 3, 'i': 9, 'learning_rate': 0.002615735581594066}.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Score for fold 5: loss of 16.271936416625977\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 3s - loss: 17.0816 - mse: 17.0816 - mae: 1.6315 - val_loss: 9.5284 - val_mse: 9.5284 - val_mae: 1.5342 - lr: 5.0897e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 16.5787 - mse: 16.5787 - mae: 1.6058 - val_loss: 9.4678 - val_mse: 9.4678 - val_mae: 1.5836 - lr: 5.0897e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 16.5385 - mse: 16.5385 - mae: 1.6038 - val_loss: 9.4741 - val_mse: 9.4741 - val_mae: 1.5505 - lr: 5.0897e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 16.4891 - mse: 16.4891 - mae: 1.6023 - val_loss: 9.4547 - val_mse: 9.4547 - val_mae: 1.5379 - lr: 5.0897e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 16.4792 - mse: 16.4792 - mae: 1.6004 - val_loss: 9.4202 - val_mse: 9.4202 - val_mae: 1.5772 - lr: 5.0897e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 16.4258 - mse: 16.4258 - mae: 1.5995 - val_loss: 9.4533 - val_mse: 9.4533 - val_mae: 1.5119 - lr: 5.0897e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 16.3754 - mse: 16.3754 - mae: 1.5943 - val_loss: 9.4631 - val_mse: 9.4631 - val_mae: 1.5358 - lr: 5.0897e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 2s - loss: 16.4069 - mse: 16.4069 - mae: 1.5923 - val_loss: 9.4123 - val_mse: 9.4123 - val_mae: 1.5161 - lr: 5.0897e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 2s - loss: 16.3713 - mse: 16.3713 - mae: 1.5967 - val_loss: 9.3845 - val_mse: 9.3845 - val_mae: 1.5732 - lr: 5.0897e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 2s - loss: 16.3264 - mse: 16.3264 - mae: 1.5944 - val_loss: 9.4337 - val_mse: 9.4337 - val_mae: 1.5326 - lr: 5.0897e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 2s - loss: 16.3301 - mse: 16.3301 - mae: 1.5949 - val_loss: 9.3789 - val_mse: 9.3789 - val_mae: 1.5544 - lr: 5.0897e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 2s - loss: 16.3084 - mse: 16.3084 - mae: 1.5937 - val_loss: 9.3500 - val_mse: 9.3500 - val_mae: 1.5270 - lr: 5.0897e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 13/100\n",
            "1000/1000 - 2s - loss: 16.3159 - mse: 16.3159 - mae: 1.5952 - val_loss: 9.4160 - val_mse: 9.4160 - val_mae: 1.5461 - lr: 5.0897e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 14/100\n",
            "1000/1000 - 2s - loss: 16.2918 - mse: 16.2918 - mae: 1.5921 - val_loss: 9.4502 - val_mse: 9.4502 - val_mae: 1.5377 - lr: 5.0897e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 15/100\n",
            "1000/1000 - 2s - loss: 16.3030 - mse: 16.3030 - mae: 1.5905 - val_loss: 9.3797 - val_mse: 9.3797 - val_mae: 1.5671 - lr: 5.0897e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 16/100\n",
            "1000/1000 - 2s - loss: 16.2883 - mse: 16.2883 - mae: 1.5889 - val_loss: 9.4268 - val_mse: 9.4268 - val_mae: 1.5406 - lr: 5.0897e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 17/100\n",
            "1000/1000 - 2s - loss: 16.3087 - mse: 16.3087 - mae: 1.5881 - val_loss: 9.3811 - val_mse: 9.3811 - val_mae: 1.5681 - lr: 5.0897e-04 - 2s/epoch - 2ms/step\n",
            "Score for fold 1: loss of 9.381085395812988\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 15.5402 - mse: 15.5402 - mae: 1.5807 - val_loss: 12.4598 - val_mse: 12.4598 - val_mae: 1.5940 - lr: 5.0897e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 15.5159 - mse: 15.5159 - mae: 1.5783 - val_loss: 12.4723 - val_mse: 12.4723 - val_mae: 1.6140 - lr: 5.0897e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 15.5257 - mse: 15.5257 - mae: 1.5779 - val_loss: 12.5479 - val_mse: 12.5479 - val_mae: 1.5610 - lr: 5.0897e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 15.4999 - mse: 15.4999 - mae: 1.5772 - val_loss: 12.5356 - val_mse: 12.5356 - val_mae: 1.5830 - lr: 5.0897e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 15.4841 - mse: 15.4841 - mae: 1.5802 - val_loss: 12.6121 - val_mse: 12.6121 - val_mae: 1.5382 - lr: 5.0897e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 15.5005 - mse: 15.5005 - mae: 1.5793 - val_loss: 12.5409 - val_mse: 12.5409 - val_mae: 1.5624 - lr: 5.0897e-04 - 2s/epoch - 2ms/step\n",
            "Score for fold 2: loss of 12.540926933288574\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 14.8482 - mse: 14.8482 - mae: 1.5780 - val_loss: 15.0281 - val_mse: 15.0281 - val_mae: 1.5948 - lr: 5.0897e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 14.8331 - mse: 14.8331 - mae: 1.5707 - val_loss: 14.9665 - val_mse: 14.9665 - val_mae: 1.5851 - lr: 5.0897e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 14.8376 - mse: 14.8376 - mae: 1.5752 - val_loss: 15.0272 - val_mse: 15.0272 - val_mae: 1.5677 - lr: 5.0897e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 14.8285 - mse: 14.8285 - mae: 1.5745 - val_loss: 14.9491 - val_mse: 14.9491 - val_mae: 1.6204 - lr: 5.0897e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 14.8229 - mse: 14.8229 - mae: 1.5756 - val_loss: 14.9707 - val_mse: 14.9707 - val_mae: 1.5583 - lr: 5.0897e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 14.8279 - mse: 14.8279 - mae: 1.5736 - val_loss: 14.9757 - val_mse: 14.9757 - val_mae: 1.5798 - lr: 5.0897e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 14.8111 - mse: 14.8111 - mae: 1.5739 - val_loss: 14.9925 - val_mse: 14.9925 - val_mae: 1.5618 - lr: 5.0897e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 2s - loss: 14.8352 - mse: 14.8352 - mae: 1.5712 - val_loss: 15.0153 - val_mse: 15.0153 - val_mae: 1.6008 - lr: 5.0897e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 2s - loss: 14.8244 - mse: 14.8244 - mae: 1.5750 - val_loss: 15.0125 - val_mse: 15.0125 - val_mae: 1.5720 - lr: 5.0897e-04 - 2s/epoch - 2ms/step\n",
            "Score for fold 3: loss of 15.012534141540527\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 14.3343 - mse: 14.3343 - mae: 1.5609 - val_loss: 17.0297 - val_mse: 17.0297 - val_mae: 1.5985 - lr: 5.0897e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 14.3106 - mse: 14.3106 - mae: 1.5626 - val_loss: 17.0547 - val_mse: 17.0547 - val_mae: 1.5840 - lr: 5.0897e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 14.2986 - mse: 14.2986 - mae: 1.5613 - val_loss: 16.9527 - val_mse: 16.9527 - val_mae: 1.6109 - lr: 5.0897e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 14.3005 - mse: 14.3005 - mae: 1.5629 - val_loss: 16.9905 - val_mse: 16.9905 - val_mae: 1.5900 - lr: 5.0897e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 14.3050 - mse: 14.3050 - mae: 1.5581 - val_loss: 17.0452 - val_mse: 17.0452 - val_mae: 1.6263 - lr: 5.0897e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 14.2964 - mse: 14.2964 - mae: 1.5622 - val_loss: 17.0189 - val_mse: 17.0189 - val_mae: 1.6121 - lr: 5.0897e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 14.2908 - mse: 14.2908 - mae: 1.5632 - val_loss: 17.1724 - val_mse: 17.1724 - val_mae: 1.6333 - lr: 5.0897e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 2s - loss: 14.2747 - mse: 14.2747 - mae: 1.5621 - val_loss: 17.0328 - val_mse: 17.0328 - val_mae: 1.6385 - lr: 5.0897e-04 - 2s/epoch - 2ms/step\n",
            "Score for fold 4: loss of 17.0328311920166\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 13.4037 - mse: 13.4037 - mae: 1.5676 - val_loss: 20.6031 - val_mse: 20.6031 - val_mae: 1.5672 - lr: 5.0897e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 13.4070 - mse: 13.4070 - mae: 1.5703 - val_loss: 20.4948 - val_mse: 20.4948 - val_mae: 1.5889 - lr: 5.0897e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 13.3977 - mse: 13.3977 - mae: 1.5683 - val_loss: 20.4856 - val_mse: 20.4856 - val_mae: 1.6179 - lr: 5.0897e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 13.3955 - mse: 13.3955 - mae: 1.5677 - val_loss: 20.4914 - val_mse: 20.4914 - val_mae: 1.6269 - lr: 5.0897e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 13.3725 - mse: 13.3725 - mae: 1.5675 - val_loss: 20.4863 - val_mse: 20.4863 - val_mae: 1.6149 - lr: 5.0897e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 13.3905 - mse: 13.3905 - mae: 1.5665 - val_loss: 20.5720 - val_mse: 20.5720 - val_mae: 1.6310 - lr: 5.0897e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 13.3783 - mse: 13.3783 - mae: 1.5678 - val_loss: 20.5893 - val_mse: 20.5893 - val_mae: 1.5770 - lr: 5.0897e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 2s - loss: 13.4003 - mse: 13.4003 - mae: 1.5679 - val_loss: 20.5405 - val_mse: 20.5405 - val_mae: 1.5897 - lr: 5.0897e-04 - 2s/epoch - 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-10-14 15:39:23,378]\u001b[0m Finished trial#20 resulted in value: 14.9. Current best value is 14.801999999999998 with parameters: {'activation': 'relu', 'num_hidden_layer': 3, 'i': 9, 'learning_rate': 0.002615735581594066}.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Score for fold 5: loss of 20.54054832458496\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 3s - loss: 15.5922 - mse: 15.5922 - mae: 1.6228 - val_loss: 14.8499 - val_mse: 14.8499 - val_mae: 1.5849 - lr: 8.7595e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 15.1789 - mse: 15.1789 - mae: 1.6059 - val_loss: 14.7980 - val_mse: 14.7980 - val_mae: 1.6720 - lr: 8.7595e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 3s - loss: 15.1444 - mse: 15.1444 - mae: 1.5999 - val_loss: 14.8587 - val_mse: 14.8587 - val_mae: 1.5550 - lr: 8.7595e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 3s - loss: 15.0684 - mse: 15.0684 - mae: 1.5967 - val_loss: 14.7280 - val_mse: 14.7280 - val_mae: 1.5893 - lr: 8.7595e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 3s - loss: 14.9439 - mse: 14.9439 - mae: 1.5901 - val_loss: 14.8305 - val_mse: 14.8305 - val_mae: 1.6222 - lr: 8.7595e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 3s - loss: 14.9837 - mse: 14.9837 - mae: 1.5923 - val_loss: 14.7752 - val_mse: 14.7752 - val_mae: 1.5638 - lr: 8.7595e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 3s - loss: 14.9410 - mse: 14.9410 - mae: 1.5835 - val_loss: 14.7179 - val_mse: 14.7179 - val_mae: 1.6065 - lr: 8.7595e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 3s - loss: 14.8592 - mse: 14.8592 - mae: 1.5890 - val_loss: 14.7176 - val_mse: 14.7176 - val_mae: 1.5628 - lr: 8.7595e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 3s - loss: 14.9836 - mse: 14.9836 - mae: 1.5900 - val_loss: 14.7275 - val_mse: 14.7275 - val_mae: 1.5774 - lr: 8.7595e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 3s - loss: 14.8514 - mse: 14.8514 - mae: 1.5888 - val_loss: 14.6223 - val_mse: 14.6223 - val_mae: 1.5765 - lr: 8.7595e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 3s - loss: 14.9340 - mse: 14.9340 - mae: 1.5876 - val_loss: 14.7451 - val_mse: 14.7451 - val_mae: 1.6628 - lr: 8.7595e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 3s - loss: 14.8464 - mse: 14.8464 - mae: 1.5898 - val_loss: 14.9069 - val_mse: 14.9069 - val_mae: 1.5277 - lr: 8.7595e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 13/100\n",
            "1000/1000 - 3s - loss: 14.9122 - mse: 14.9122 - mae: 1.5849 - val_loss: 14.8980 - val_mse: 14.8980 - val_mae: 1.5057 - lr: 8.7595e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 14/100\n",
            "1000/1000 - 3s - loss: 14.6914 - mse: 14.6914 - mae: 1.5831 - val_loss: 15.0361 - val_mse: 15.0361 - val_mae: 1.4973 - lr: 8.7595e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 15/100\n",
            "1000/1000 - 3s - loss: 14.7386 - mse: 14.7386 - mae: 1.5859 - val_loss: 14.7356 - val_mse: 14.7356 - val_mae: 1.6860 - lr: 8.7595e-04 - 3s/epoch - 3ms/step\n",
            "Score for fold 1: loss of 14.735584259033203\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 3s - loss: 16.2671 - mse: 16.2671 - mae: 1.5928 - val_loss: 8.8562 - val_mse: 8.8562 - val_mae: 1.4954 - lr: 8.7595e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 3s - loss: 16.1258 - mse: 16.1258 - mae: 1.5936 - val_loss: 9.1707 - val_mse: 9.1707 - val_mae: 1.5111 - lr: 8.7595e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 3s - loss: 16.1193 - mse: 16.1193 - mae: 1.5923 - val_loss: 9.0440 - val_mse: 9.0440 - val_mae: 1.5184 - lr: 8.7595e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 3s - loss: 16.1095 - mse: 16.1095 - mae: 1.5901 - val_loss: 9.2843 - val_mse: 9.2843 - val_mae: 1.5046 - lr: 8.7595e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 3s - loss: 16.1818 - mse: 16.1818 - mae: 1.5869 - val_loss: 9.1602 - val_mse: 9.1602 - val_mae: 1.5215 - lr: 8.7595e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 3s - loss: 15.9821 - mse: 15.9821 - mae: 1.5907 - val_loss: 9.0097 - val_mse: 9.0097 - val_mae: 1.6123 - lr: 8.7595e-04 - 3s/epoch - 3ms/step\n",
            "Score for fold 2: loss of 9.009732246398926\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 3s - loss: 14.2667 - mse: 14.2667 - mae: 1.5832 - val_loss: 16.9248 - val_mse: 16.9248 - val_mae: 1.6775 - lr: 8.7595e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 14.1984 - mse: 14.1984 - mae: 1.5814 - val_loss: 16.7968 - val_mse: 16.7968 - val_mae: 1.5798 - lr: 8.7595e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 14.3167 - mse: 14.3167 - mae: 1.5745 - val_loss: 16.5668 - val_mse: 16.5668 - val_mae: 1.7432 - lr: 8.7595e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 3s - loss: 14.2146 - mse: 14.2146 - mae: 1.5697 - val_loss: 16.4778 - val_mse: 16.4778 - val_mae: 1.6538 - lr: 8.7595e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 3s - loss: 14.1184 - mse: 14.1184 - mae: 1.5727 - val_loss: 16.9104 - val_mse: 16.9104 - val_mae: 1.4818 - lr: 8.7595e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 3s - loss: 14.2042 - mse: 14.2042 - mae: 1.5774 - val_loss: 16.6052 - val_mse: 16.6052 - val_mae: 1.5646 - lr: 8.7595e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 3s - loss: 14.1722 - mse: 14.1722 - mae: 1.5775 - val_loss: 16.5935 - val_mse: 16.5935 - val_mae: 1.6813 - lr: 8.7595e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 3s - loss: 14.1996 - mse: 14.1996 - mae: 1.5806 - val_loss: 16.9769 - val_mse: 16.9769 - val_mae: 1.5797 - lr: 8.7595e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 3s - loss: 14.1958 - mse: 14.1958 - mae: 1.5799 - val_loss: 17.2392 - val_mse: 17.2392 - val_mae: 1.7827 - lr: 8.7595e-04 - 3s/epoch - 3ms/step\n",
            "Score for fold 3: loss of 17.239166259765625\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 12.8890 - mse: 12.8890 - mae: 1.5823 - val_loss: 22.2809 - val_mse: 22.2809 - val_mae: 1.6843 - lr: 8.7595e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 3s - loss: 12.8589 - mse: 12.8589 - mae: 1.5895 - val_loss: 22.3161 - val_mse: 22.3161 - val_mae: 1.6116 - lr: 8.7595e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 3s - loss: 12.7423 - mse: 12.7423 - mae: 1.5677 - val_loss: 21.9692 - val_mse: 21.9692 - val_mae: 1.5594 - lr: 8.7595e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 3s - loss: 12.6802 - mse: 12.6802 - mae: 1.5798 - val_loss: 22.8334 - val_mse: 22.8334 - val_mae: 1.4967 - lr: 8.7595e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 3s - loss: 12.6736 - mse: 12.6736 - mae: 1.5746 - val_loss: 22.1968 - val_mse: 22.1968 - val_mae: 1.6235 - lr: 8.7595e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 3s - loss: 12.5505 - mse: 12.5505 - mae: 1.5723 - val_loss: 22.4905 - val_mse: 22.4905 - val_mae: 1.7597 - lr: 8.7595e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 3s - loss: 12.6310 - mse: 12.6310 - mae: 1.5767 - val_loss: 22.8335 - val_mse: 22.8335 - val_mae: 1.5392 - lr: 8.7595e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 3s - loss: 12.6402 - mse: 12.6402 - mae: 1.5729 - val_loss: 22.4497 - val_mse: 22.4497 - val_mae: 1.6059 - lr: 8.7595e-04 - 3s/epoch - 3ms/step\n",
            "Score for fold 4: loss of 22.449665069580078\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 3s - loss: 15.5044 - mse: 15.5044 - mae: 1.5987 - val_loss: 11.1620 - val_mse: 11.1620 - val_mae: 1.6386 - lr: 8.7595e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 3s - loss: 15.3811 - mse: 15.3811 - mae: 1.5896 - val_loss: 11.0769 - val_mse: 11.0769 - val_mae: 1.5381 - lr: 8.7595e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 3s - loss: 15.2595 - mse: 15.2595 - mae: 1.5892 - val_loss: 11.6419 - val_mse: 11.6419 - val_mae: 1.4258 - lr: 8.7595e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 3s - loss: 15.2730 - mse: 15.2730 - mae: 1.5883 - val_loss: 11.4248 - val_mse: 11.4248 - val_mae: 1.4720 - lr: 8.7595e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 3s - loss: 15.2616 - mse: 15.2616 - mae: 1.5867 - val_loss: 11.3436 - val_mse: 11.3436 - val_mae: 1.5438 - lr: 8.7595e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 3s - loss: 15.1768 - mse: 15.1768 - mae: 1.5845 - val_loss: 11.2020 - val_mse: 11.2020 - val_mae: 1.5688 - lr: 8.7595e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 3s - loss: 15.1917 - mse: 15.1917 - mae: 1.5844 - val_loss: 11.4250 - val_mse: 11.4250 - val_mae: 1.6128 - lr: 8.7595e-04 - 3s/epoch - 3ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-10-14 15:41:25,580]\u001b[0m Finished trial#21 resulted in value: 14.974. Current best value is 14.801999999999998 with parameters: {'activation': 'relu', 'num_hidden_layer': 3, 'i': 9, 'learning_rate': 0.002615735581594066}.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Score for fold 5: loss of 11.425006866455078\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 3s - loss: 12.8520 - mse: 12.8520 - mae: 1.5980 - val_loss: 25.3868 - val_mse: 25.3868 - val_mae: 1.6981 - lr: 0.0015 - 3s/epoch - 3ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 12.5616 - mse: 12.5616 - mae: 1.5835 - val_loss: 25.6636 - val_mse: 25.6636 - val_mae: 1.6352 - lr: 0.0015 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 3s - loss: 12.4944 - mse: 12.4944 - mae: 1.5748 - val_loss: 25.6742 - val_mse: 25.6742 - val_mae: 1.6841 - lr: 0.0015 - 3s/epoch - 3ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 3s - loss: 12.4448 - mse: 12.4448 - mae: 1.5680 - val_loss: 25.6259 - val_mse: 25.6259 - val_mae: 1.7042 - lr: 0.0015 - 3s/epoch - 3ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 3s - loss: 12.4815 - mse: 12.4815 - mae: 1.5714 - val_loss: 26.0481 - val_mse: 26.0481 - val_mae: 1.6111 - lr: 0.0015 - 3s/epoch - 3ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 3s - loss: 12.4832 - mse: 12.4832 - mae: 1.5704 - val_loss: 25.3959 - val_mse: 25.3959 - val_mae: 1.6987 - lr: 0.0015 - 3s/epoch - 3ms/step\n",
            "Score for fold 1: loss of 25.395883560180664\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 3s - loss: 16.1818 - mse: 16.1818 - mae: 1.5903 - val_loss: 10.4245 - val_mse: 10.4245 - val_mae: 1.5495 - lr: 0.0010 - 3s/epoch - 3ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 16.1578 - mse: 16.1578 - mae: 1.5839 - val_loss: 10.3569 - val_mse: 10.3569 - val_mae: 1.5631 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 16.0401 - mse: 16.0401 - mae: 1.5882 - val_loss: 10.6398 - val_mse: 10.6398 - val_mae: 1.7290 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 16.1035 - mse: 16.1035 - mae: 1.5927 - val_loss: 10.2971 - val_mse: 10.2971 - val_mae: 1.4978 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 16.0507 - mse: 16.0507 - mae: 1.5928 - val_loss: 10.3662 - val_mse: 10.3662 - val_mae: 1.5173 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 16.1209 - mse: 16.1209 - mae: 1.5926 - val_loss: 10.5289 - val_mse: 10.5289 - val_mae: 1.5420 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 16.1249 - mse: 16.1249 - mae: 1.5905 - val_loss: 10.5627 - val_mse: 10.5627 - val_mae: 1.5302 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 2s - loss: 16.0834 - mse: 16.0834 - mae: 1.5965 - val_loss: 10.2572 - val_mse: 10.2572 - val_mae: 1.5844 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 3s - loss: 15.9404 - mse: 15.9404 - mae: 1.5926 - val_loss: 10.6965 - val_mse: 10.6965 - val_mae: 1.5251 - lr: 0.0010 - 3s/epoch - 3ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 2s - loss: 16.0926 - mse: 16.0926 - mae: 1.5905 - val_loss: 10.2685 - val_mse: 10.2685 - val_mae: 1.5403 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 2s - loss: 16.0970 - mse: 16.0970 - mae: 1.5938 - val_loss: 10.2887 - val_mse: 10.2887 - val_mae: 1.5060 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 3s - loss: 16.0511 - mse: 16.0511 - mae: 1.5912 - val_loss: 10.3679 - val_mse: 10.3679 - val_mae: 1.6020 - lr: 0.0010 - 3s/epoch - 3ms/step\n",
            "Epoch 13/100\n",
            "1000/1000 - 3s - loss: 15.9579 - mse: 15.9579 - mae: 1.5943 - val_loss: 10.4642 - val_mse: 10.4642 - val_mae: 1.5275 - lr: 0.0010 - 3s/epoch - 3ms/step\n",
            "Score for fold 2: loss of 10.464195251464844\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 15.5926 - mse: 15.5926 - mae: 1.5954 - val_loss: 12.1722 - val_mse: 12.1722 - val_mae: 1.6036 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 15.5177 - mse: 15.5177 - mae: 1.5909 - val_loss: 12.3174 - val_mse: 12.3174 - val_mae: 1.7079 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 15.4749 - mse: 15.4749 - mae: 1.5907 - val_loss: 12.3316 - val_mse: 12.3316 - val_mae: 1.4825 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 3s - loss: 15.4657 - mse: 15.4657 - mae: 1.5930 - val_loss: 12.3856 - val_mse: 12.3856 - val_mae: 1.4930 - lr: 0.0010 - 3s/epoch - 3ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 15.4059 - mse: 15.4059 - mae: 1.5915 - val_loss: 12.3696 - val_mse: 12.3696 - val_mae: 1.6713 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 15.3814 - mse: 15.3814 - mae: 1.5963 - val_loss: 12.4624 - val_mse: 12.4624 - val_mae: 1.5562 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Score for fold 3: loss of 12.462416648864746\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 15.6145 - mse: 15.6145 - mae: 1.5963 - val_loss: 11.8249 - val_mse: 11.8249 - val_mae: 1.4961 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 15.4883 - mse: 15.4883 - mae: 1.5977 - val_loss: 11.5613 - val_mse: 11.5613 - val_mae: 1.5521 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 15.4349 - mse: 15.4349 - mae: 1.5898 - val_loss: 11.6530 - val_mse: 11.6530 - val_mae: 1.6363 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 15.3555 - mse: 15.3555 - mae: 1.5899 - val_loss: 11.6951 - val_mse: 11.6951 - val_mae: 1.5918 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 15.2803 - mse: 15.2803 - mae: 1.5877 - val_loss: 11.7671 - val_mse: 11.7671 - val_mae: 1.5936 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 15.2446 - mse: 15.2446 - mae: 1.5868 - val_loss: 11.9657 - val_mse: 11.9657 - val_mae: 1.4856 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 15.2250 - mse: 15.2250 - mae: 1.5878 - val_loss: 11.7147 - val_mse: 11.7147 - val_mae: 1.4991 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Score for fold 4: loss of 11.714710235595703\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 14.5949 - mse: 14.5949 - mae: 1.5804 - val_loss: 15.2887 - val_mse: 15.2887 - val_mae: 1.7275 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 3s - loss: 14.5376 - mse: 14.5376 - mae: 1.5840 - val_loss: 14.9295 - val_mse: 14.9295 - val_mae: 1.6023 - lr: 0.0010 - 3s/epoch - 3ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 3s - loss: 14.5697 - mse: 14.5697 - mae: 1.5799 - val_loss: 15.0296 - val_mse: 15.0296 - val_mae: 1.6123 - lr: 0.0010 - 3s/epoch - 3ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 3s - loss: 14.3909 - mse: 14.3909 - mae: 1.5821 - val_loss: 15.0886 - val_mse: 15.0886 - val_mae: 1.4965 - lr: 0.0010 - 3s/epoch - 3ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 3s - loss: 14.4019 - mse: 14.4019 - mae: 1.5807 - val_loss: 14.9927 - val_mse: 14.9927 - val_mae: 1.6787 - lr: 0.0010 - 3s/epoch - 3ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 3s - loss: 14.4050 - mse: 14.4050 - mae: 1.5836 - val_loss: 15.5219 - val_mse: 15.5219 - val_mae: 1.6276 - lr: 0.0010 - 3s/epoch - 3ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 3s - loss: 14.4542 - mse: 14.4542 - mae: 1.5813 - val_loss: 15.2979 - val_mse: 15.2979 - val_mae: 1.5756 - lr: 0.0010 - 3s/epoch - 3ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-10-14 15:43:08,508]\u001b[0m Finished trial#22 resulted in value: 15.065999999999999. Current best value is 14.801999999999998 with parameters: {'activation': 'relu', 'num_hidden_layer': 3, 'i': 9, 'learning_rate': 0.002615735581594066}.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Score for fold 5: loss of 15.297883987426758\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 3s - loss: 16.6661 - mse: 16.6661 - mae: 1.6322 - val_loss: 10.3214 - val_mse: 10.3214 - val_mae: 1.5419 - lr: 8.9571e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 16.3348 - mse: 16.3348 - mae: 1.6187 - val_loss: 10.2026 - val_mse: 10.2026 - val_mae: 1.5973 - lr: 8.9571e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 3s - loss: 16.3688 - mse: 16.3688 - mae: 1.6166 - val_loss: 10.3886 - val_mse: 10.3886 - val_mae: 1.4919 - lr: 8.9571e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 3s - loss: 16.2359 - mse: 16.2359 - mae: 1.6102 - val_loss: 10.0962 - val_mse: 10.0962 - val_mae: 1.5206 - lr: 8.9571e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 3s - loss: 16.1474 - mse: 16.1474 - mae: 1.6043 - val_loss: 10.0745 - val_mse: 10.0745 - val_mae: 1.5292 - lr: 8.9571e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 3s - loss: 16.2094 - mse: 16.2094 - mae: 1.6016 - val_loss: 10.0645 - val_mse: 10.0645 - val_mae: 1.6316 - lr: 8.9571e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 3s - loss: 16.1215 - mse: 16.1215 - mae: 1.6036 - val_loss: 10.0905 - val_mse: 10.0905 - val_mae: 1.5464 - lr: 8.9571e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 3s - loss: 16.1530 - mse: 16.1530 - mae: 1.5959 - val_loss: 10.0658 - val_mse: 10.0658 - val_mae: 1.5543 - lr: 8.9571e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 3s - loss: 16.1169 - mse: 16.1169 - mae: 1.5967 - val_loss: 10.2348 - val_mse: 10.2348 - val_mae: 1.4907 - lr: 8.9571e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 3s - loss: 15.9711 - mse: 15.9711 - mae: 1.5980 - val_loss: 10.0890 - val_mse: 10.0890 - val_mae: 1.5661 - lr: 8.9571e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 3s - loss: 16.0773 - mse: 16.0773 - mae: 1.5996 - val_loss: 10.0724 - val_mse: 10.0724 - val_mae: 1.5496 - lr: 8.9571e-04 - 3s/epoch - 3ms/step\n",
            "Score for fold 1: loss of 10.0724458694458\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 13.8298 - mse: 13.8298 - mae: 1.5748 - val_loss: 18.6968 - val_mse: 18.6968 - val_mae: 1.5900 - lr: 8.9571e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 3s - loss: 13.7784 - mse: 13.7784 - mae: 1.5752 - val_loss: 18.9123 - val_mse: 18.9123 - val_mae: 1.5229 - lr: 8.9571e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 3s - loss: 13.6844 - mse: 13.6844 - mae: 1.5742 - val_loss: 18.9136 - val_mse: 18.9136 - val_mae: 1.5684 - lr: 8.9571e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 3s - loss: 13.5848 - mse: 13.5848 - mae: 1.5793 - val_loss: 18.7830 - val_mse: 18.7830 - val_mae: 1.5733 - lr: 8.9571e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 3s - loss: 13.5300 - mse: 13.5300 - mae: 1.5763 - val_loss: 18.8450 - val_mse: 18.8450 - val_mae: 1.5949 - lr: 8.9571e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 3s - loss: 13.6445 - mse: 13.6445 - mae: 1.5782 - val_loss: 18.8203 - val_mse: 18.8203 - val_mae: 1.6039 - lr: 8.9571e-04 - 3s/epoch - 3ms/step\n",
            "Score for fold 2: loss of 18.820335388183594\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 14.1485 - mse: 14.1485 - mae: 1.5766 - val_loss: 17.3556 - val_mse: 17.3556 - val_mae: 1.5453 - lr: 8.9571e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 3s - loss: 14.0686 - mse: 14.0686 - mae: 1.5768 - val_loss: 17.3908 - val_mse: 17.3908 - val_mae: 1.6343 - lr: 8.9571e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 3s - loss: 14.0433 - mse: 14.0433 - mae: 1.5738 - val_loss: 17.2327 - val_mse: 17.2327 - val_mae: 1.5595 - lr: 8.9571e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 3s - loss: 14.0956 - mse: 14.0956 - mae: 1.5692 - val_loss: 17.2905 - val_mse: 17.2905 - val_mae: 1.5405 - lr: 8.9571e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 3s - loss: 13.9968 - mse: 13.9968 - mae: 1.5650 - val_loss: 17.2271 - val_mse: 17.2271 - val_mae: 1.6122 - lr: 8.9571e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 3s - loss: 13.9760 - mse: 13.9760 - mae: 1.5653 - val_loss: 17.2005 - val_mse: 17.2005 - val_mae: 1.6381 - lr: 8.9571e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 3s - loss: 14.0464 - mse: 14.0464 - mae: 1.5633 - val_loss: 17.2782 - val_mse: 17.2782 - val_mae: 1.6406 - lr: 8.9571e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 3s - loss: 13.8932 - mse: 13.8932 - mae: 1.5656 - val_loss: 17.3678 - val_mse: 17.3678 - val_mae: 1.6551 - lr: 8.9571e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 3s - loss: 13.8040 - mse: 13.8040 - mae: 1.5613 - val_loss: 17.2834 - val_mse: 17.2834 - val_mae: 1.5763 - lr: 8.9571e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 3s - loss: 13.9414 - mse: 13.9414 - mae: 1.5600 - val_loss: 17.5243 - val_mse: 17.5243 - val_mae: 1.5667 - lr: 8.9571e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 3s - loss: 13.7691 - mse: 13.7691 - mae: 1.5612 - val_loss: 17.2383 - val_mse: 17.2383 - val_mae: 1.5577 - lr: 8.9571e-04 - 3s/epoch - 3ms/step\n",
            "Score for fold 3: loss of 17.238353729248047\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 3s - loss: 14.9825 - mse: 14.9825 - mae: 1.5595 - val_loss: 12.7472 - val_mse: 12.7472 - val_mae: 1.5147 - lr: 8.9571e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 3s - loss: 14.9884 - mse: 14.9884 - mae: 1.5590 - val_loss: 12.6000 - val_mse: 12.6000 - val_mae: 1.5756 - lr: 8.9571e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 3s - loss: 14.8040 - mse: 14.8040 - mae: 1.5536 - val_loss: 12.9953 - val_mse: 12.9953 - val_mae: 1.6260 - lr: 8.9571e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 3s - loss: 14.9086 - mse: 14.9086 - mae: 1.5546 - val_loss: 12.7491 - val_mse: 12.7491 - val_mae: 1.5973 - lr: 8.9571e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 3s - loss: 14.8599 - mse: 14.8599 - mae: 1.5589 - val_loss: 12.8371 - val_mse: 12.8371 - val_mae: 1.5528 - lr: 8.9571e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 3s - loss: 14.7340 - mse: 14.7340 - mae: 1.5520 - val_loss: 13.1024 - val_mse: 13.1024 - val_mae: 1.7719 - lr: 8.9571e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 3s - loss: 14.8838 - mse: 14.8838 - mae: 1.5545 - val_loss: 12.9655 - val_mse: 12.9655 - val_mae: 1.6595 - lr: 8.9571e-04 - 3s/epoch - 3ms/step\n",
            "Score for fold 4: loss of 12.96546459197998\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 14.4037 - mse: 14.4037 - mae: 1.5677 - val_loss: 14.5521 - val_mse: 14.5521 - val_mae: 1.6611 - lr: 8.9571e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 3s - loss: 14.2436 - mse: 14.2436 - mae: 1.5677 - val_loss: 14.4679 - val_mse: 14.4679 - val_mae: 1.5788 - lr: 8.9571e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 3s - loss: 14.3590 - mse: 14.3590 - mae: 1.5718 - val_loss: 14.3794 - val_mse: 14.3794 - val_mae: 1.5525 - lr: 8.9571e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 3s - loss: 14.3193 - mse: 14.3193 - mae: 1.5607 - val_loss: 14.7225 - val_mse: 14.7225 - val_mae: 1.5689 - lr: 8.9571e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 3s - loss: 14.2983 - mse: 14.2983 - mae: 1.5673 - val_loss: 14.7935 - val_mse: 14.7935 - val_mae: 1.5516 - lr: 8.9571e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 3s - loss: 14.1052 - mse: 14.1052 - mae: 1.5613 - val_loss: 14.7122 - val_mse: 14.7122 - val_mae: 1.5734 - lr: 8.9571e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 3s - loss: 14.1704 - mse: 14.1704 - mae: 1.5646 - val_loss: 14.6392 - val_mse: 14.6392 - val_mae: 1.6148 - lr: 8.9571e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 3s - loss: 14.0809 - mse: 14.0809 - mae: 1.5633 - val_loss: 14.6887 - val_mse: 14.6887 - val_mae: 1.5572 - lr: 8.9571e-04 - 3s/epoch - 3ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-10-14 15:45:06,196]\u001b[0m Finished trial#23 resulted in value: 14.758. Current best value is 14.758 with parameters: {'activation': 'relu', 'num_hidden_layer': 4, 'i': 7, 'learning_rate': 0.0008957084104345709}.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Score for fold 5: loss of 14.688702583312988\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 3s - loss: 15.6336 - mse: 15.6336 - mae: 1.6363 - val_loss: 15.6414 - val_mse: 15.6414 - val_mae: 1.6259 - lr: 4.9969e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 15.0002 - mse: 15.0002 - mae: 1.5936 - val_loss: 15.6317 - val_mse: 15.6317 - val_mae: 1.5651 - lr: 4.9969e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 14.9866 - mse: 14.9866 - mae: 1.5923 - val_loss: 15.3638 - val_mse: 15.3638 - val_mae: 1.6184 - lr: 4.9969e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 14.9363 - mse: 14.9363 - mae: 1.5886 - val_loss: 15.5110 - val_mse: 15.5110 - val_mae: 1.5949 - lr: 4.9969e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 14.9018 - mse: 14.9018 - mae: 1.5895 - val_loss: 15.6755 - val_mse: 15.6755 - val_mae: 1.5805 - lr: 4.9969e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 14.8926 - mse: 14.8926 - mae: 1.5875 - val_loss: 15.3637 - val_mse: 15.3637 - val_mae: 1.6320 - lr: 4.9969e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 14.8473 - mse: 14.8473 - mae: 1.5861 - val_loss: 15.3145 - val_mse: 15.3145 - val_mae: 1.6053 - lr: 4.9969e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 2s - loss: 14.8147 - mse: 14.8147 - mae: 1.5793 - val_loss: 15.4632 - val_mse: 15.4632 - val_mae: 1.5991 - lr: 4.9969e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 2s - loss: 14.7821 - mse: 14.7821 - mae: 1.5811 - val_loss: 15.3607 - val_mse: 15.3607 - val_mae: 1.5952 - lr: 4.9969e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 2s - loss: 14.7286 - mse: 14.7286 - mae: 1.5815 - val_loss: 15.4733 - val_mse: 15.4733 - val_mae: 1.5420 - lr: 4.9969e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 2s - loss: 14.7493 - mse: 14.7493 - mae: 1.5767 - val_loss: 15.3319 - val_mse: 15.3319 - val_mae: 1.5756 - lr: 4.9969e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 2s - loss: 14.7601 - mse: 14.7601 - mae: 1.5752 - val_loss: 15.2662 - val_mse: 15.2662 - val_mae: 1.6394 - lr: 4.9969e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 13/100\n",
            "1000/1000 - 2s - loss: 14.7217 - mse: 14.7217 - mae: 1.5771 - val_loss: 15.2853 - val_mse: 15.2853 - val_mae: 1.6119 - lr: 4.9969e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 14/100\n",
            "1000/1000 - 2s - loss: 14.7125 - mse: 14.7125 - mae: 1.5761 - val_loss: 15.3822 - val_mse: 15.3822 - val_mae: 1.5874 - lr: 4.9969e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 15/100\n",
            "1000/1000 - 2s - loss: 14.6697 - mse: 14.6697 - mae: 1.5778 - val_loss: 15.3702 - val_mse: 15.3702 - val_mae: 1.5407 - lr: 4.9969e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 16/100\n",
            "1000/1000 - 2s - loss: 14.6961 - mse: 14.6961 - mae: 1.5767 - val_loss: 15.2226 - val_mse: 15.2226 - val_mae: 1.6200 - lr: 4.9969e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 17/100\n",
            "1000/1000 - 2s - loss: 14.6283 - mse: 14.6283 - mae: 1.5720 - val_loss: 15.4386 - val_mse: 15.4386 - val_mae: 1.5571 - lr: 4.9969e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 18/100\n",
            "1000/1000 - 2s - loss: 14.6409 - mse: 14.6409 - mae: 1.5734 - val_loss: 15.3247 - val_mse: 15.3247 - val_mae: 1.5850 - lr: 4.9969e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 19/100\n",
            "1000/1000 - 2s - loss: 14.6097 - mse: 14.6097 - mae: 1.5740 - val_loss: 15.3210 - val_mse: 15.3210 - val_mae: 1.5752 - lr: 4.9969e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 20/100\n",
            "1000/1000 - 2s - loss: 14.5913 - mse: 14.5913 - mae: 1.5805 - val_loss: 15.3271 - val_mse: 15.3271 - val_mae: 1.6815 - lr: 4.9969e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 21/100\n",
            "1000/1000 - 2s - loss: 14.5504 - mse: 14.5504 - mae: 1.5736 - val_loss: 15.3622 - val_mse: 15.3622 - val_mae: 1.6235 - lr: 4.9969e-04 - 2s/epoch - 2ms/step\n",
            "Score for fold 1: loss of 15.362237930297852\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 13.8060 - mse: 13.8060 - mae: 1.5804 - val_loss: 18.7156 - val_mse: 18.7156 - val_mae: 1.5194 - lr: 4.9969e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 13.7404 - mse: 13.7404 - mae: 1.5748 - val_loss: 18.6288 - val_mse: 18.6288 - val_mae: 1.6135 - lr: 4.9969e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 13.6790 - mse: 13.6790 - mae: 1.5737 - val_loss: 18.6585 - val_mse: 18.6585 - val_mae: 1.6321 - lr: 4.9969e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 13.7439 - mse: 13.7439 - mae: 1.5728 - val_loss: 18.5997 - val_mse: 18.5997 - val_mae: 1.5710 - lr: 4.9969e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 13.6877 - mse: 13.6877 - mae: 1.5758 - val_loss: 18.7285 - val_mse: 18.7285 - val_mae: 1.5112 - lr: 4.9969e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 13.6036 - mse: 13.6036 - mae: 1.5743 - val_loss: 18.7412 - val_mse: 18.7412 - val_mae: 1.6311 - lr: 4.9969e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 13.5046 - mse: 13.5046 - mae: 1.5727 - val_loss: 18.6536 - val_mse: 18.6536 - val_mae: 1.5630 - lr: 4.9969e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 2s - loss: 13.5104 - mse: 13.5104 - mae: 1.5718 - val_loss: 18.7350 - val_mse: 18.7350 - val_mae: 1.5824 - lr: 4.9969e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 2s - loss: 13.3386 - mse: 13.3386 - mae: 1.5644 - val_loss: 18.7908 - val_mse: 18.7908 - val_mae: 1.6638 - lr: 4.9969e-04 - 2s/epoch - 2ms/step\n",
            "Score for fold 2: loss of 18.790802001953125\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 15.5145 - mse: 15.5145 - mae: 1.5779 - val_loss: 11.2048 - val_mse: 11.2048 - val_mae: 1.5266 - lr: 4.9969e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 15.3543 - mse: 15.3543 - mae: 1.5755 - val_loss: 11.3131 - val_mse: 11.3131 - val_mae: 1.5606 - lr: 4.9969e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 15.3875 - mse: 15.3875 - mae: 1.5698 - val_loss: 11.3961 - val_mse: 11.3961 - val_mae: 1.5234 - lr: 4.9969e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 15.4254 - mse: 15.4254 - mae: 1.5669 - val_loss: 11.4366 - val_mse: 11.4366 - val_mae: 1.5784 - lr: 4.9969e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 15.3296 - mse: 15.3296 - mae: 1.5630 - val_loss: 11.4095 - val_mse: 11.4095 - val_mae: 1.5971 - lr: 4.9969e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 15.2742 - mse: 15.2742 - mae: 1.5702 - val_loss: 11.5616 - val_mse: 11.5616 - val_mae: 1.5113 - lr: 4.9969e-04 - 2s/epoch - 2ms/step\n",
            "Score for fold 3: loss of 11.5615816116333\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 14.3713 - mse: 14.3713 - mae: 1.5711 - val_loss: 14.6711 - val_mse: 14.6711 - val_mae: 1.5416 - lr: 4.9969e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 14.2681 - mse: 14.2681 - mae: 1.5623 - val_loss: 15.3939 - val_mse: 15.3939 - val_mae: 1.5948 - lr: 4.9969e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 14.3268 - mse: 14.3268 - mae: 1.5623 - val_loss: 15.3259 - val_mse: 15.3259 - val_mae: 1.6345 - lr: 4.9969e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 14.2720 - mse: 14.2720 - mae: 1.5666 - val_loss: 15.0318 - val_mse: 15.0318 - val_mae: 1.5606 - lr: 4.9969e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 14.3254 - mse: 14.3254 - mae: 1.5618 - val_loss: 15.1047 - val_mse: 15.1047 - val_mae: 1.7542 - lr: 4.9969e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 14.2604 - mse: 14.2604 - mae: 1.5644 - val_loss: 15.4912 - val_mse: 15.4912 - val_mae: 1.5134 - lr: 4.9969e-04 - 2s/epoch - 2ms/step\n",
            "Score for fold 4: loss of 15.491155624389648\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 14.8528 - mse: 14.8528 - mae: 1.5710 - val_loss: 12.9763 - val_mse: 12.9763 - val_mae: 1.4673 - lr: 4.9969e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 14.7404 - mse: 14.7404 - mae: 1.5696 - val_loss: 13.1678 - val_mse: 13.1678 - val_mae: 1.6090 - lr: 4.9969e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 14.6831 - mse: 14.6831 - mae: 1.5741 - val_loss: 13.1831 - val_mse: 13.1831 - val_mae: 1.5770 - lr: 4.9969e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 14.7805 - mse: 14.7805 - mae: 1.5687 - val_loss: 13.0642 - val_mse: 13.0642 - val_mae: 1.4648 - lr: 4.9969e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 14.6995 - mse: 14.6995 - mae: 1.5614 - val_loss: 13.0274 - val_mse: 13.0274 - val_mae: 1.6093 - lr: 4.9969e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 14.7079 - mse: 14.7079 - mae: 1.5660 - val_loss: 13.1321 - val_mse: 13.1321 - val_mae: 1.6372 - lr: 4.9969e-04 - 2s/epoch - 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-10-14 15:46:48,098]\u001b[0m Finished trial#24 resulted in value: 14.866. Current best value is 14.758 with parameters: {'activation': 'relu', 'num_hidden_layer': 4, 'i': 7, 'learning_rate': 0.0008957084104345709}.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Score for fold 5: loss of 13.132078170776367\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 5s - loss: 16.1054 - mse: 16.1054 - mae: 1.6393 - val_loss: 13.2379 - val_mse: 13.2379 - val_mae: 1.5533 - lr: 0.0018 - 5s/epoch - 5ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 4s - loss: 15.8167 - mse: 15.8167 - mae: 1.6170 - val_loss: 12.8433 - val_mse: 12.8433 - val_mae: 1.6771 - lr: 0.0018 - 4s/epoch - 4ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 4s - loss: 15.6739 - mse: 15.6739 - mae: 1.6050 - val_loss: 12.7601 - val_mse: 12.7601 - val_mae: 1.5978 - lr: 0.0018 - 4s/epoch - 4ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 4s - loss: 15.6665 - mse: 15.6665 - mae: 1.5989 - val_loss: 12.7886 - val_mse: 12.7886 - val_mae: 1.5810 - lr: 0.0018 - 4s/epoch - 4ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 4s - loss: 15.5810 - mse: 15.5810 - mae: 1.5987 - val_loss: 13.0403 - val_mse: 13.0403 - val_mae: 1.5108 - lr: 0.0018 - 4s/epoch - 4ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 4s - loss: 15.5448 - mse: 15.5448 - mae: 1.5980 - val_loss: 12.9204 - val_mse: 12.9204 - val_mae: 1.5000 - lr: 0.0018 - 4s/epoch - 4ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 4s - loss: 15.6344 - mse: 15.6344 - mae: 1.5922 - val_loss: 12.8975 - val_mse: 12.8975 - val_mae: 1.5791 - lr: 0.0018 - 4s/epoch - 4ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 4s - loss: 15.5836 - mse: 15.5836 - mae: 1.5921 - val_loss: 12.8513 - val_mse: 12.8513 - val_mae: 1.5457 - lr: 0.0018 - 4s/epoch - 4ms/step\n",
            "Score for fold 1: loss of 12.851348876953125\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 4s - loss: 15.6542 - mse: 15.6542 - mae: 1.5905 - val_loss: 12.6419 - val_mse: 12.6419 - val_mae: 1.6590 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 4s - loss: 15.6024 - mse: 15.6024 - mae: 1.5902 - val_loss: 12.6266 - val_mse: 12.6266 - val_mae: 1.5406 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 4s - loss: 15.6002 - mse: 15.6002 - mae: 1.5933 - val_loss: 12.7187 - val_mse: 12.7187 - val_mae: 1.6314 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 4s - loss: 15.5667 - mse: 15.5667 - mae: 1.5904 - val_loss: 12.7294 - val_mse: 12.7294 - val_mae: 1.5275 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 4s - loss: 15.5174 - mse: 15.5174 - mae: 1.5998 - val_loss: 12.6073 - val_mse: 12.6073 - val_mae: 1.6290 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 4s - loss: 15.5446 - mse: 15.5446 - mae: 1.5960 - val_loss: 12.4301 - val_mse: 12.4301 - val_mae: 1.5653 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 4s - loss: 15.5668 - mse: 15.5668 - mae: 1.6007 - val_loss: 12.3454 - val_mse: 12.3454 - val_mae: 1.5919 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 4s - loss: 15.5415 - mse: 15.5415 - mae: 1.5957 - val_loss: 12.6048 - val_mse: 12.6048 - val_mae: 1.6267 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 4s - loss: 15.5642 - mse: 15.5642 - mae: 1.5947 - val_loss: 12.7128 - val_mse: 12.7128 - val_mae: 1.5041 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 4s - loss: 15.5494 - mse: 15.5494 - mae: 1.5972 - val_loss: 12.6699 - val_mse: 12.6699 - val_mae: 1.5519 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 4s - loss: 15.4477 - mse: 15.4477 - mae: 1.5886 - val_loss: 12.9842 - val_mse: 12.9842 - val_mae: 1.8773 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 4s - loss: 15.4919 - mse: 15.4919 - mae: 1.5868 - val_loss: 12.3450 - val_mse: 12.3450 - val_mae: 1.5626 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 13/100\n",
            "1000/1000 - 4s - loss: 15.4819 - mse: 15.4819 - mae: 1.5862 - val_loss: 12.5401 - val_mse: 12.5401 - val_mae: 1.6660 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 14/100\n",
            "1000/1000 - 4s - loss: 15.4057 - mse: 15.4057 - mae: 1.5789 - val_loss: 12.4813 - val_mse: 12.4813 - val_mae: 1.6508 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 15/100\n",
            "1000/1000 - 4s - loss: 15.4080 - mse: 15.4080 - mae: 1.5809 - val_loss: 12.8261 - val_mse: 12.8261 - val_mae: 1.5315 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 16/100\n",
            "1000/1000 - 4s - loss: 15.4407 - mse: 15.4407 - mae: 1.5806 - val_loss: 12.7398 - val_mse: 12.7398 - val_mae: 1.6200 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 17/100\n",
            "1000/1000 - 4s - loss: 15.3289 - mse: 15.3289 - mae: 1.5785 - val_loss: 12.6117 - val_mse: 12.6117 - val_mae: 1.5388 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Score for fold 2: loss of 12.611706733703613\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 4s - loss: 15.2234 - mse: 15.2234 - mae: 1.5714 - val_loss: 12.6821 - val_mse: 12.6821 - val_mae: 1.6512 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 4s - loss: 15.1463 - mse: 15.1463 - mae: 1.5686 - val_loss: 12.8127 - val_mse: 12.8127 - val_mae: 1.6136 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 4s - loss: 15.0657 - mse: 15.0657 - mae: 1.5676 - val_loss: 13.1052 - val_mse: 13.1052 - val_mae: 1.5547 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 4s - loss: 15.0979 - mse: 15.0979 - mae: 1.5709 - val_loss: 12.8127 - val_mse: 12.8127 - val_mae: 1.5349 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 4s - loss: 15.0633 - mse: 15.0633 - mae: 1.5628 - val_loss: 13.2283 - val_mse: 13.2283 - val_mae: 1.5981 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 4s - loss: 15.0875 - mse: 15.0875 - mae: 1.5645 - val_loss: 13.6565 - val_mse: 13.6565 - val_mae: 1.5369 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Score for fold 3: loss of 13.656550407409668\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 4s - loss: 15.4535 - mse: 15.4535 - mae: 1.5687 - val_loss: 11.3058 - val_mse: 11.3058 - val_mae: 1.5488 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 4s - loss: 15.3461 - mse: 15.3461 - mae: 1.5723 - val_loss: 11.5651 - val_mse: 11.5651 - val_mae: 1.6180 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 4s - loss: 15.1966 - mse: 15.1966 - mae: 1.5647 - val_loss: 11.4951 - val_mse: 11.4951 - val_mae: 1.5088 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 4s - loss: 15.1798 - mse: 15.1798 - mae: 1.5752 - val_loss: 11.6730 - val_mse: 11.6730 - val_mae: 1.5485 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 4s - loss: 15.2917 - mse: 15.2917 - mae: 1.5664 - val_loss: 11.7367 - val_mse: 11.7367 - val_mae: 1.5738 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 4s - loss: 15.3072 - mse: 15.3072 - mae: 1.5716 - val_loss: 12.0537 - val_mse: 12.0537 - val_mae: 1.6194 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Score for fold 4: loss of 12.053739547729492\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 4s - loss: 12.2648 - mse: 12.2648 - mae: 1.5782 - val_loss: 24.0563 - val_mse: 24.0563 - val_mae: 1.5310 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 4s - loss: 12.2790 - mse: 12.2790 - mae: 1.5750 - val_loss: 24.1232 - val_mse: 24.1232 - val_mae: 1.6321 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 4s - loss: 12.0520 - mse: 12.0520 - mae: 1.5801 - val_loss: 24.0260 - val_mse: 24.0260 - val_mae: 1.6310 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 4s - loss: 12.2816 - mse: 12.2816 - mae: 1.5836 - val_loss: 24.5006 - val_mse: 24.5006 - val_mae: 1.6356 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 4s - loss: 12.2812 - mse: 12.2812 - mae: 1.5753 - val_loss: 24.2549 - val_mse: 24.2549 - val_mae: 1.5811 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 4s - loss: 12.0224 - mse: 12.0224 - mae: 1.5676 - val_loss: 24.1130 - val_mse: 24.1130 - val_mae: 1.6728 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 4s - loss: 12.0360 - mse: 12.0360 - mae: 1.5695 - val_loss: 24.2557 - val_mse: 24.2557 - val_mae: 1.6574 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 4s - loss: 12.1535 - mse: 12.1535 - mae: 1.5784 - val_loss: 24.1844 - val_mse: 24.1844 - val_mae: 1.5971 - lr: 0.0010 - 4s/epoch - 4ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-10-14 15:49:41,326]\u001b[0m Finished trial#25 resulted in value: 15.069999999999999. Current best value is 14.758 with parameters: {'activation': 'relu', 'num_hidden_layer': 4, 'i': 7, 'learning_rate': 0.0008957084104345709}.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Score for fold 5: loss of 24.18444061279297\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 3s - loss: 17.0877 - mse: 17.0877 - mae: 1.6817 - val_loss: 10.5176 - val_mse: 10.5176 - val_mae: 1.5312 - lr: 0.0038 - 3s/epoch - 3ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 16.7218 - mse: 16.7218 - mae: 1.6382 - val_loss: 10.4617 - val_mse: 10.4617 - val_mae: 1.5354 - lr: 0.0038 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 16.6456 - mse: 16.6456 - mae: 1.6299 - val_loss: 10.7265 - val_mse: 10.7265 - val_mae: 1.4917 - lr: 0.0038 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 16.6129 - mse: 16.6129 - mae: 1.6279 - val_loss: 10.2238 - val_mse: 10.2238 - val_mae: 1.6233 - lr: 0.0038 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 16.4974 - mse: 16.4974 - mae: 1.6303 - val_loss: 10.6884 - val_mse: 10.6884 - val_mae: 1.7550 - lr: 0.0038 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 16.5155 - mse: 16.5155 - mae: 1.6296 - val_loss: 10.4430 - val_mse: 10.4430 - val_mae: 1.5070 - lr: 0.0038 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 16.4279 - mse: 16.4279 - mae: 1.6285 - val_loss: 10.5281 - val_mse: 10.5281 - val_mae: 1.4961 - lr: 0.0038 - 2s/epoch - 2ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 2s - loss: 16.4123 - mse: 16.4123 - mae: 1.6306 - val_loss: 10.6024 - val_mse: 10.6024 - val_mae: 1.6260 - lr: 0.0038 - 2s/epoch - 2ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 2s - loss: 16.3819 - mse: 16.3819 - mae: 1.6275 - val_loss: 10.2724 - val_mse: 10.2724 - val_mae: 1.5922 - lr: 0.0038 - 2s/epoch - 2ms/step\n",
            "Score for fold 1: loss of 10.272370338439941\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 15.5782 - mse: 15.5782 - mae: 1.5848 - val_loss: 12.1336 - val_mse: 12.1336 - val_mae: 1.6726 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 15.6003 - mse: 15.6003 - mae: 1.5820 - val_loss: 12.1047 - val_mse: 12.1047 - val_mae: 1.6022 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 15.5682 - mse: 15.5682 - mae: 1.5791 - val_loss: 12.2991 - val_mse: 12.2991 - val_mae: 1.5787 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 15.5081 - mse: 15.5081 - mae: 1.5736 - val_loss: 12.3115 - val_mse: 12.3115 - val_mae: 1.7002 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 15.5483 - mse: 15.5483 - mae: 1.5771 - val_loss: 12.0596 - val_mse: 12.0596 - val_mae: 1.5663 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 15.5246 - mse: 15.5246 - mae: 1.5737 - val_loss: 12.1019 - val_mse: 12.1019 - val_mae: 1.6295 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 15.5433 - mse: 15.5433 - mae: 1.5813 - val_loss: 12.3772 - val_mse: 12.3772 - val_mae: 1.5553 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 2s - loss: 15.5925 - mse: 15.5925 - mae: 1.5795 - val_loss: 12.1455 - val_mse: 12.1455 - val_mae: 1.5935 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 2s - loss: 15.5488 - mse: 15.5488 - mae: 1.5815 - val_loss: 12.1456 - val_mse: 12.1456 - val_mae: 1.6047 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 2s - loss: 15.5505 - mse: 15.5505 - mae: 1.5779 - val_loss: 12.2958 - val_mse: 12.2958 - val_mae: 1.6147 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Score for fold 2: loss of 12.295818328857422\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 13.5564 - mse: 13.5564 - mae: 1.5834 - val_loss: 20.7374 - val_mse: 20.7374 - val_mae: 1.5254 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 13.5308 - mse: 13.5308 - mae: 1.5796 - val_loss: 20.7189 - val_mse: 20.7189 - val_mae: 1.6186 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 13.4882 - mse: 13.4882 - mae: 1.5838 - val_loss: 20.6261 - val_mse: 20.6261 - val_mae: 1.7291 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 13.4497 - mse: 13.4497 - mae: 1.5854 - val_loss: 20.5896 - val_mse: 20.5896 - val_mae: 1.6139 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 13.4128 - mse: 13.4128 - mae: 1.5841 - val_loss: 20.6629 - val_mse: 20.6629 - val_mae: 1.6249 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 13.4592 - mse: 13.4592 - mae: 1.5759 - val_loss: 20.6317 - val_mse: 20.6317 - val_mae: 1.5966 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 13.4219 - mse: 13.4219 - mae: 1.5766 - val_loss: 20.7989 - val_mse: 20.7989 - val_mae: 1.5917 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 2s - loss: 13.4580 - mse: 13.4580 - mae: 1.5770 - val_loss: 20.6159 - val_mse: 20.6159 - val_mae: 1.6311 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 2s - loss: 13.4168 - mse: 13.4168 - mae: 1.5816 - val_loss: 20.5958 - val_mse: 20.5958 - val_mae: 1.6586 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Score for fold 3: loss of 20.59584617614746\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 14.9476 - mse: 14.9476 - mae: 1.5917 - val_loss: 14.6694 - val_mse: 14.6694 - val_mae: 1.5449 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 14.8585 - mse: 14.8585 - mae: 1.5930 - val_loss: 14.6886 - val_mse: 14.6886 - val_mae: 1.5556 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 14.8690 - mse: 14.8690 - mae: 1.5929 - val_loss: 14.8400 - val_mse: 14.8400 - val_mae: 1.5765 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 14.8270 - mse: 14.8270 - mae: 1.5841 - val_loss: 14.6776 - val_mse: 14.6776 - val_mae: 1.5858 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 14.7330 - mse: 14.7330 - mae: 1.5834 - val_loss: 14.7842 - val_mse: 14.7842 - val_mae: 1.6513 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 14.8008 - mse: 14.8008 - mae: 1.5872 - val_loss: 14.6766 - val_mse: 14.6766 - val_mae: 1.5422 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Score for fold 4: loss of 14.676590919494629\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 14.3568 - mse: 14.3568 - mae: 1.5819 - val_loss: 16.6431 - val_mse: 16.6431 - val_mae: 1.5932 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 14.3151 - mse: 14.3151 - mae: 1.5797 - val_loss: 16.6203 - val_mse: 16.6203 - val_mae: 1.5896 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 14.3035 - mse: 14.3035 - mae: 1.5781 - val_loss: 16.8060 - val_mse: 16.8060 - val_mae: 1.6419 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 14.2983 - mse: 14.2983 - mae: 1.5824 - val_loss: 16.8832 - val_mse: 16.8832 - val_mae: 1.5471 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 14.2944 - mse: 14.2944 - mae: 1.5799 - val_loss: 16.7751 - val_mse: 16.7751 - val_mae: 1.5870 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 14.2856 - mse: 14.2856 - mae: 1.5773 - val_loss: 16.5284 - val_mse: 16.5284 - val_mae: 1.6252 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 14.2476 - mse: 14.2476 - mae: 1.5795 - val_loss: 16.9671 - val_mse: 16.9671 - val_mae: 1.6161 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 2s - loss: 14.2695 - mse: 14.2695 - mae: 1.5758 - val_loss: 16.9477 - val_mse: 16.9477 - val_mae: 1.5603 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 2s - loss: 14.3010 - mse: 14.3010 - mae: 1.5728 - val_loss: 16.8063 - val_mse: 16.8063 - val_mae: 1.6319 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 2s - loss: 14.2385 - mse: 14.2385 - mae: 1.5797 - val_loss: 16.8899 - val_mse: 16.8899 - val_mae: 1.6045 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 2s - loss: 14.2374 - mse: 14.2374 - mae: 1.5765 - val_loss: 16.6313 - val_mse: 16.6313 - val_mae: 1.7079 - lr: 0.0010 - 2s/epoch - 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-10-14 15:51:24,562]\u001b[0m Finished trial#26 resulted in value: 14.896. Current best value is 14.758 with parameters: {'activation': 'relu', 'num_hidden_layer': 4, 'i': 7, 'learning_rate': 0.0008957084104345709}.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Score for fold 5: loss of 16.631336212158203\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 4s - loss: 16.1341 - mse: 16.1341 - mae: 1.6122 - val_loss: 12.3003 - val_mse: 12.3003 - val_mae: 1.6398 - lr: 7.2103e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 4s - loss: 15.8591 - mse: 15.8591 - mae: 1.6003 - val_loss: 12.1680 - val_mse: 12.1680 - val_mae: 1.6615 - lr: 7.2103e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 4s - loss: 15.6945 - mse: 15.6945 - mae: 1.5942 - val_loss: 12.2097 - val_mse: 12.2097 - val_mae: 1.5626 - lr: 7.2103e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 4s - loss: 15.7336 - mse: 15.7336 - mae: 1.5866 - val_loss: 12.3337 - val_mse: 12.3337 - val_mae: 1.5804 - lr: 7.2103e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 4s - loss: 15.6725 - mse: 15.6725 - mae: 1.5888 - val_loss: 12.2141 - val_mse: 12.2141 - val_mae: 1.6223 - lr: 7.2103e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 4s - loss: 15.6609 - mse: 15.6609 - mae: 1.5813 - val_loss: 12.4778 - val_mse: 12.4778 - val_mae: 1.5701 - lr: 7.2103e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 4s - loss: 15.6153 - mse: 15.6153 - mae: 1.5799 - val_loss: 12.7447 - val_mse: 12.7447 - val_mae: 1.5605 - lr: 7.2103e-04 - 4s/epoch - 4ms/step\n",
            "Score for fold 1: loss of 12.744660377502441\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 4s - loss: 16.4882 - mse: 16.4882 - mae: 1.6003 - val_loss: 8.8723 - val_mse: 8.8723 - val_mae: 1.4741 - lr: 7.2103e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 4s - loss: 16.3762 - mse: 16.3762 - mae: 1.6009 - val_loss: 9.0401 - val_mse: 9.0401 - val_mae: 1.4328 - lr: 7.2103e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 4s - loss: 16.3363 - mse: 16.3363 - mae: 1.6018 - val_loss: 8.8193 - val_mse: 8.8193 - val_mae: 1.5120 - lr: 7.2103e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 4s - loss: 16.3665 - mse: 16.3665 - mae: 1.6060 - val_loss: 9.0360 - val_mse: 9.0360 - val_mae: 1.5379 - lr: 7.2103e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 4s - loss: 16.4018 - mse: 16.4018 - mae: 1.6071 - val_loss: 9.0376 - val_mse: 9.0376 - val_mae: 1.4675 - lr: 7.2103e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 4s - loss: 16.2584 - mse: 16.2584 - mae: 1.6052 - val_loss: 8.9528 - val_mse: 8.9528 - val_mae: 1.5288 - lr: 7.2103e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 4s - loss: 16.2916 - mse: 16.2916 - mae: 1.6042 - val_loss: 9.0221 - val_mse: 9.0221 - val_mae: 1.5339 - lr: 7.2103e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 4s - loss: 16.3095 - mse: 16.3095 - mae: 1.6073 - val_loss: 8.9503 - val_mse: 8.9503 - val_mae: 1.5796 - lr: 7.2103e-04 - 4s/epoch - 4ms/step\n",
            "Score for fold 2: loss of 8.95032787322998\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 4s - loss: 15.6686 - mse: 15.6686 - mae: 1.6011 - val_loss: 11.3925 - val_mse: 11.3925 - val_mae: 1.4925 - lr: 7.2103e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 4s - loss: 15.5066 - mse: 15.5066 - mae: 1.6041 - val_loss: 11.5556 - val_mse: 11.5556 - val_mae: 1.5094 - lr: 7.2103e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 4s - loss: 15.4155 - mse: 15.4155 - mae: 1.6018 - val_loss: 11.7733 - val_mse: 11.7733 - val_mae: 1.6664 - lr: 7.2103e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 4s - loss: 15.3635 - mse: 15.3635 - mae: 1.6027 - val_loss: 11.6651 - val_mse: 11.6651 - val_mae: 1.6165 - lr: 7.2103e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 4s - loss: 15.5503 - mse: 15.5503 - mae: 1.6009 - val_loss: 11.7331 - val_mse: 11.7331 - val_mae: 1.4199 - lr: 7.2103e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 4s - loss: 15.4414 - mse: 15.4414 - mae: 1.6024 - val_loss: 11.8297 - val_mse: 11.8297 - val_mae: 1.4390 - lr: 7.2103e-04 - 4s/epoch - 4ms/step\n",
            "Score for fold 3: loss of 11.829672813415527\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 4s - loss: 12.3504 - mse: 12.3504 - mae: 1.5728 - val_loss: 23.7156 - val_mse: 23.7156 - val_mae: 1.6207 - lr: 7.2103e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 4s - loss: 12.3497 - mse: 12.3497 - mae: 1.5814 - val_loss: 24.0216 - val_mse: 24.0216 - val_mae: 1.5762 - lr: 7.2103e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 4s - loss: 12.3901 - mse: 12.3901 - mae: 1.5772 - val_loss: 24.0788 - val_mse: 24.0788 - val_mae: 1.6466 - lr: 7.2103e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 4s - loss: 12.2139 - mse: 12.2139 - mae: 1.5810 - val_loss: 24.0059 - val_mse: 24.0059 - val_mae: 1.8882 - lr: 7.2103e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 4s - loss: 12.3172 - mse: 12.3172 - mae: 1.5706 - val_loss: 24.1359 - val_mse: 24.1359 - val_mae: 1.6342 - lr: 7.2103e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 4s - loss: 12.1545 - mse: 12.1545 - mae: 1.5762 - val_loss: 24.1142 - val_mse: 24.1142 - val_mae: 1.6622 - lr: 7.2103e-04 - 4s/epoch - 4ms/step\n",
            "Score for fold 4: loss of 24.114213943481445\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 4s - loss: 13.9485 - mse: 13.9485 - mae: 1.5968 - val_loss: 16.8648 - val_mse: 16.8648 - val_mae: 1.6578 - lr: 7.2103e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 4s - loss: 13.9683 - mse: 13.9683 - mae: 1.5900 - val_loss: 17.0051 - val_mse: 17.0051 - val_mae: 1.5139 - lr: 7.2103e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 4s - loss: 14.0206 - mse: 14.0206 - mae: 1.5803 - val_loss: 17.1461 - val_mse: 17.1461 - val_mae: 1.6070 - lr: 7.2103e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 4s - loss: 13.9242 - mse: 13.9242 - mae: 1.5885 - val_loss: 17.2225 - val_mse: 17.2225 - val_mae: 1.5654 - lr: 7.2103e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 4s - loss: 13.7781 - mse: 13.7781 - mae: 1.5900 - val_loss: 17.3263 - val_mse: 17.3263 - val_mae: 1.5380 - lr: 7.2103e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 4s - loss: 13.8924 - mse: 13.8924 - mae: 1.5860 - val_loss: 17.1197 - val_mse: 17.1197 - val_mae: 1.6500 - lr: 7.2103e-04 - 4s/epoch - 4ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-10-14 15:53:33,112]\u001b[0m Finished trial#27 resulted in value: 14.95. Current best value is 14.758 with parameters: {'activation': 'relu', 'num_hidden_layer': 4, 'i': 7, 'learning_rate': 0.0008957084104345709}.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Score for fold 5: loss of 17.119747161865234\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 3s - loss: 16.4796 - mse: 16.4796 - mae: 1.7001 - val_loss: 16.6437 - val_mse: 16.6437 - val_mae: 1.6093 - lr: 3.3336e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 15.2983 - mse: 15.2983 - mae: 1.6068 - val_loss: 16.3928 - val_mse: 16.3928 - val_mae: 1.6354 - lr: 3.3336e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 15.1855 - mse: 15.1855 - mae: 1.6012 - val_loss: 16.2926 - val_mse: 16.2926 - val_mae: 1.6275 - lr: 3.3336e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 15.1015 - mse: 15.1015 - mae: 1.5906 - val_loss: 16.2913 - val_mse: 16.2913 - val_mae: 1.6325 - lr: 3.3336e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 15.0292 - mse: 15.0292 - mae: 1.5899 - val_loss: 16.2239 - val_mse: 16.2239 - val_mae: 1.6146 - lr: 3.3336e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 14.9824 - mse: 14.9824 - mae: 1.5845 - val_loss: 16.1288 - val_mse: 16.1288 - val_mae: 1.6315 - lr: 3.3336e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 14.9563 - mse: 14.9563 - mae: 1.5839 - val_loss: 16.0976 - val_mse: 16.0976 - val_mae: 1.6235 - lr: 3.3336e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 2s - loss: 14.9299 - mse: 14.9299 - mae: 1.5804 - val_loss: 16.0915 - val_mse: 16.0915 - val_mae: 1.6054 - lr: 3.3336e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 2s - loss: 14.9045 - mse: 14.9045 - mae: 1.5792 - val_loss: 16.0681 - val_mse: 16.0681 - val_mae: 1.6316 - lr: 3.3336e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 2s - loss: 14.8743 - mse: 14.8743 - mae: 1.5784 - val_loss: 16.0022 - val_mse: 16.0022 - val_mae: 1.6297 - lr: 3.3336e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 2s - loss: 14.8656 - mse: 14.8656 - mae: 1.5764 - val_loss: 15.9938 - val_mse: 15.9938 - val_mae: 1.6450 - lr: 3.3336e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 2s - loss: 14.8498 - mse: 14.8498 - mae: 1.5807 - val_loss: 15.9767 - val_mse: 15.9767 - val_mae: 1.6261 - lr: 3.3336e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 13/100\n",
            "1000/1000 - 2s - loss: 14.8231 - mse: 14.8231 - mae: 1.5764 - val_loss: 16.0066 - val_mse: 16.0066 - val_mae: 1.6079 - lr: 3.3336e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 14/100\n",
            "1000/1000 - 2s - loss: 14.8146 - mse: 14.8146 - mae: 1.5764 - val_loss: 15.9751 - val_mse: 15.9751 - val_mae: 1.6030 - lr: 3.3336e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 15/100\n",
            "1000/1000 - 2s - loss: 14.8067 - mse: 14.8067 - mae: 1.5756 - val_loss: 15.9279 - val_mse: 15.9279 - val_mae: 1.6419 - lr: 3.3336e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 16/100\n",
            "1000/1000 - 2s - loss: 14.8088 - mse: 14.8088 - mae: 1.5724 - val_loss: 16.0256 - val_mse: 16.0256 - val_mae: 1.5852 - lr: 3.3336e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 17/100\n",
            "1000/1000 - 2s - loss: 14.7867 - mse: 14.7867 - mae: 1.5768 - val_loss: 15.9695 - val_mse: 15.9695 - val_mae: 1.6090 - lr: 3.3336e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 18/100\n",
            "1000/1000 - 2s - loss: 14.7892 - mse: 14.7892 - mae: 1.5752 - val_loss: 15.9619 - val_mse: 15.9619 - val_mae: 1.6129 - lr: 3.3336e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 19/100\n",
            "1000/1000 - 2s - loss: 14.7610 - mse: 14.7610 - mae: 1.5746 - val_loss: 16.0220 - val_mse: 16.0220 - val_mae: 1.5732 - lr: 3.3336e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 20/100\n",
            "1000/1000 - 2s - loss: 14.7779 - mse: 14.7779 - mae: 1.5719 - val_loss: 15.9612 - val_mse: 15.9612 - val_mae: 1.6230 - lr: 3.3336e-04 - 2s/epoch - 2ms/step\n",
            "Score for fold 1: loss of 15.961167335510254\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 16.0300 - mse: 16.0300 - mae: 1.5974 - val_loss: 10.8708 - val_mse: 10.8708 - val_mae: 1.5274 - lr: 3.3336e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 16.0202 - mse: 16.0202 - mae: 1.5936 - val_loss: 10.9169 - val_mse: 10.9169 - val_mae: 1.5186 - lr: 3.3336e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 16.0089 - mse: 16.0089 - mae: 1.5962 - val_loss: 10.9233 - val_mse: 10.9233 - val_mae: 1.5146 - lr: 3.3336e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 16.0000 - mse: 16.0000 - mae: 1.5942 - val_loss: 10.9057 - val_mse: 10.9057 - val_mae: 1.5086 - lr: 3.3336e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 16.0060 - mse: 16.0060 - mae: 1.5940 - val_loss: 10.9435 - val_mse: 10.9435 - val_mae: 1.5480 - lr: 3.3336e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 15.9769 - mse: 15.9769 - mae: 1.5962 - val_loss: 10.9021 - val_mse: 10.9021 - val_mae: 1.5152 - lr: 3.3336e-04 - 2s/epoch - 2ms/step\n",
            "Score for fold 2: loss of 10.90211296081543\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 13.0590 - mse: 13.0590 - mae: 1.5661 - val_loss: 22.6400 - val_mse: 22.6400 - val_mae: 1.6398 - lr: 3.3336e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 13.0460 - mse: 13.0460 - mae: 1.5674 - val_loss: 22.6592 - val_mse: 22.6592 - val_mae: 1.6383 - lr: 3.3336e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 13.0346 - mse: 13.0346 - mae: 1.5660 - val_loss: 22.6783 - val_mse: 22.6783 - val_mae: 1.6009 - lr: 3.3336e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 13.0256 - mse: 13.0256 - mae: 1.5653 - val_loss: 22.6554 - val_mse: 22.6554 - val_mae: 1.6293 - lr: 3.3336e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 13.0159 - mse: 13.0159 - mae: 1.5687 - val_loss: 22.6995 - val_mse: 22.6995 - val_mae: 1.6288 - lr: 3.3336e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 13.0224 - mse: 13.0224 - mae: 1.5642 - val_loss: 22.6941 - val_mse: 22.6941 - val_mae: 1.6188 - lr: 3.3336e-04 - 2s/epoch - 2ms/step\n",
            "Score for fold 3: loss of 22.694150924682617\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 15.3011 - mse: 15.3011 - mae: 1.5772 - val_loss: 13.5445 - val_mse: 13.5445 - val_mae: 1.6247 - lr: 3.3336e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 15.2938 - mse: 15.2938 - mae: 1.5751 - val_loss: 13.4820 - val_mse: 13.4820 - val_mae: 1.5979 - lr: 3.3336e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 15.2772 - mse: 15.2772 - mae: 1.5759 - val_loss: 13.5943 - val_mse: 13.5943 - val_mae: 1.5712 - lr: 3.3336e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 15.2646 - mse: 15.2646 - mae: 1.5750 - val_loss: 13.5374 - val_mse: 13.5374 - val_mae: 1.6012 - lr: 3.3336e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 15.2584 - mse: 15.2584 - mae: 1.5742 - val_loss: 13.6372 - val_mse: 13.6372 - val_mae: 1.5461 - lr: 3.3336e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 15.2432 - mse: 15.2432 - mae: 1.5724 - val_loss: 13.7303 - val_mse: 13.7303 - val_mae: 1.5281 - lr: 3.3336e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 15.2522 - mse: 15.2522 - mae: 1.5709 - val_loss: 13.6071 - val_mse: 13.6071 - val_mae: 1.5398 - lr: 3.3336e-04 - 2s/epoch - 2ms/step\n",
            "Score for fold 4: loss of 13.607125282287598\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 15.6609 - mse: 15.6609 - mae: 1.5766 - val_loss: 11.7525 - val_mse: 11.7525 - val_mae: 1.5561 - lr: 3.3336e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 15.6489 - mse: 15.6489 - mae: 1.5783 - val_loss: 11.9486 - val_mse: 11.9486 - val_mae: 1.5542 - lr: 3.3336e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 15.6433 - mse: 15.6433 - mae: 1.5751 - val_loss: 11.8292 - val_mse: 11.8292 - val_mae: 1.5364 - lr: 3.3336e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 15.6420 - mse: 15.6420 - mae: 1.5776 - val_loss: 11.8278 - val_mse: 11.8278 - val_mae: 1.5432 - lr: 3.3336e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 15.6169 - mse: 15.6169 - mae: 1.5740 - val_loss: 11.7879 - val_mse: 11.7879 - val_mae: 1.5843 - lr: 3.3336e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 15.6159 - mse: 15.6159 - mae: 1.5773 - val_loss: 11.8252 - val_mse: 11.8252 - val_mae: 1.5597 - lr: 3.3336e-04 - 2s/epoch - 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-10-14 15:55:05,676]\u001b[0m Finished trial#28 resulted in value: 14.998. Current best value is 14.758 with parameters: {'activation': 'relu', 'num_hidden_layer': 4, 'i': 7, 'learning_rate': 0.0008957084104345709}.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Score for fold 5: loss of 11.825220108032227\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 4s - loss: 12.9322 - mse: 12.9322 - mae: 1.6952 - val_loss: 27.9474 - val_mse: 27.9474 - val_mae: 1.6907 - lr: 0.0030 - 4s/epoch - 4ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 4s - loss: 12.5938 - mse: 12.5938 - mae: 1.6682 - val_loss: 28.2703 - val_mse: 28.2703 - val_mae: 1.8293 - lr: 0.0030 - 4s/epoch - 4ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 4s - loss: 12.5019 - mse: 12.5019 - mae: 1.6531 - val_loss: 27.8677 - val_mse: 27.8677 - val_mae: 1.5952 - lr: 0.0030 - 4s/epoch - 4ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 4s - loss: 12.5396 - mse: 12.5396 - mae: 1.6680 - val_loss: 27.8983 - val_mse: 27.8983 - val_mae: 1.7514 - lr: 0.0030 - 4s/epoch - 4ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 4s - loss: 12.5461 - mse: 12.5461 - mae: 1.6630 - val_loss: 28.1097 - val_mse: 28.1097 - val_mae: 1.4885 - lr: 0.0030 - 4s/epoch - 4ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 4s - loss: 12.5122 - mse: 12.5122 - mae: 1.6693 - val_loss: 27.6128 - val_mse: 27.6128 - val_mae: 1.7388 - lr: 0.0030 - 4s/epoch - 4ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 4s - loss: 12.4867 - mse: 12.4867 - mae: 1.6610 - val_loss: 27.7247 - val_mse: 27.7247 - val_mae: 1.6325 - lr: 0.0030 - 4s/epoch - 4ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 4s - loss: 12.4724 - mse: 12.4724 - mae: 1.6652 - val_loss: 27.8728 - val_mse: 27.8728 - val_mae: 1.5107 - lr: 0.0030 - 4s/epoch - 4ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 4s - loss: 12.4122 - mse: 12.4122 - mae: 1.6581 - val_loss: 27.7187 - val_mse: 27.7187 - val_mae: 1.7409 - lr: 0.0030 - 4s/epoch - 4ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 4s - loss: 12.4712 - mse: 12.4712 - mae: 1.6652 - val_loss: 27.5946 - val_mse: 27.5946 - val_mae: 1.6033 - lr: 0.0030 - 4s/epoch - 4ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 4s - loss: 12.5437 - mse: 12.5437 - mae: 1.6898 - val_loss: 27.8972 - val_mse: 27.8972 - val_mae: 1.8770 - lr: 0.0030 - 4s/epoch - 4ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 4s - loss: 12.5855 - mse: 12.5855 - mae: 1.6770 - val_loss: 28.3298 - val_mse: 28.3298 - val_mae: 1.5276 - lr: 0.0030 - 4s/epoch - 4ms/step\n",
            "Epoch 13/100\n",
            "1000/1000 - 4s - loss: 12.6174 - mse: 12.6174 - mae: 1.6830 - val_loss: 27.8444 - val_mse: 27.8444 - val_mae: 1.8234 - lr: 0.0030 - 4s/epoch - 4ms/step\n",
            "Epoch 14/100\n",
            "1000/1000 - 4s - loss: 12.3748 - mse: 12.3748 - mae: 1.6659 - val_loss: 27.8743 - val_mse: 27.8743 - val_mae: 1.6925 - lr: 0.0030 - 4s/epoch - 4ms/step\n",
            "Epoch 15/100\n",
            "1000/1000 - 4s - loss: 12.5295 - mse: 12.5295 - mae: 1.6827 - val_loss: 27.7218 - val_mse: 27.7218 - val_mae: 1.6381 - lr: 0.0030 - 4s/epoch - 4ms/step\n",
            "Score for fold 1: loss of 27.721773147583008\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 4s - loss: 16.4857 - mse: 16.4857 - mae: 1.6377 - val_loss: 10.2728 - val_mse: 10.2728 - val_mae: 1.6003 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 4s - loss: 16.3197 - mse: 16.3197 - mae: 1.6184 - val_loss: 10.2223 - val_mse: 10.2223 - val_mae: 1.5536 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 4s - loss: 16.3091 - mse: 16.3091 - mae: 1.6246 - val_loss: 10.1909 - val_mse: 10.1909 - val_mae: 1.5921 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 4s - loss: 16.2745 - mse: 16.2745 - mae: 1.6163 - val_loss: 10.2356 - val_mse: 10.2356 - val_mae: 1.8202 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 4s - loss: 16.2208 - mse: 16.2208 - mae: 1.6111 - val_loss: 10.3475 - val_mse: 10.3475 - val_mae: 1.7364 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 4s - loss: 16.2635 - mse: 16.2635 - mae: 1.6108 - val_loss: 10.3631 - val_mse: 10.3631 - val_mae: 1.7273 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 4s - loss: 16.2183 - mse: 16.2183 - mae: 1.6060 - val_loss: 10.1713 - val_mse: 10.1713 - val_mae: 1.5965 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 4s - loss: 16.2163 - mse: 16.2163 - mae: 1.6159 - val_loss: 10.2582 - val_mse: 10.2582 - val_mae: 1.7521 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 4s - loss: 16.2825 - mse: 16.2825 - mae: 1.6127 - val_loss: 10.4889 - val_mse: 10.4889 - val_mae: 1.5251 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 4s - loss: 16.2590 - mse: 16.2590 - mae: 1.6135 - val_loss: 10.1963 - val_mse: 10.1963 - val_mae: 1.6260 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 4s - loss: 16.1772 - mse: 16.1772 - mae: 1.6127 - val_loss: 10.1777 - val_mse: 10.1777 - val_mae: 1.6743 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 4s - loss: 16.1394 - mse: 16.1394 - mae: 1.6052 - val_loss: 10.1889 - val_mse: 10.1889 - val_mae: 1.7525 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Score for fold 2: loss of 10.188915252685547\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 4s - loss: 16.0203 - mse: 16.0203 - mae: 1.6054 - val_loss: 11.0895 - val_mse: 11.0895 - val_mae: 1.5201 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 4s - loss: 15.9145 - mse: 15.9145 - mae: 1.6109 - val_loss: 11.2254 - val_mse: 11.2254 - val_mae: 1.8723 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 4s - loss: 15.9840 - mse: 15.9840 - mae: 1.5974 - val_loss: 10.9385 - val_mse: 10.9385 - val_mae: 1.5625 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 4s - loss: 15.7456 - mse: 15.7456 - mae: 1.5980 - val_loss: 11.0800 - val_mse: 11.0800 - val_mae: 1.7989 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 4s - loss: 15.8932 - mse: 15.8932 - mae: 1.6108 - val_loss: 11.0283 - val_mse: 11.0283 - val_mae: 1.6248 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 4s - loss: 15.6578 - mse: 15.6578 - mae: 1.5949 - val_loss: 11.3292 - val_mse: 11.3292 - val_mae: 1.5517 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 4s - loss: 15.7905 - mse: 15.7905 - mae: 1.6038 - val_loss: 10.9920 - val_mse: 10.9920 - val_mae: 1.5997 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 4s - loss: 15.7292 - mse: 15.7292 - mae: 1.6001 - val_loss: 11.0495 - val_mse: 11.0495 - val_mae: 1.6018 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Score for fold 3: loss of 11.049513816833496\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 4s - loss: 15.4034 - mse: 15.4034 - mae: 1.6054 - val_loss: 13.1006 - val_mse: 13.1006 - val_mae: 1.6728 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 4s - loss: 15.5004 - mse: 15.5004 - mae: 1.6080 - val_loss: 12.9891 - val_mse: 12.9891 - val_mae: 1.6423 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 4s - loss: 15.4696 - mse: 15.4696 - mae: 1.6035 - val_loss: 13.1277 - val_mse: 13.1277 - val_mae: 1.5740 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 4s - loss: 15.3796 - mse: 15.3796 - mae: 1.6024 - val_loss: 12.8816 - val_mse: 12.8816 - val_mae: 1.5610 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 4s - loss: 15.3749 - mse: 15.3749 - mae: 1.6104 - val_loss: 12.8822 - val_mse: 12.8822 - val_mae: 1.6808 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 4s - loss: 15.3052 - mse: 15.3052 - mae: 1.5970 - val_loss: 12.8275 - val_mse: 12.8275 - val_mae: 1.7201 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 4s - loss: 15.3241 - mse: 15.3241 - mae: 1.6113 - val_loss: 12.9716 - val_mse: 12.9716 - val_mae: 1.5889 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 4s - loss: 15.3413 - mse: 15.3413 - mae: 1.6056 - val_loss: 12.7838 - val_mse: 12.7838 - val_mae: 1.7048 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 4s - loss: 15.4138 - mse: 15.4138 - mae: 1.5993 - val_loss: 12.9280 - val_mse: 12.9280 - val_mae: 1.5633 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 4s - loss: 15.2968 - mse: 15.2968 - mae: 1.6023 - val_loss: 12.9461 - val_mse: 12.9461 - val_mae: 1.5970 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 4s - loss: 15.3994 - mse: 15.3994 - mae: 1.6000 - val_loss: 13.1218 - val_mse: 13.1218 - val_mae: 1.6481 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 4s - loss: 15.3676 - mse: 15.3676 - mae: 1.6068 - val_loss: 13.0022 - val_mse: 13.0022 - val_mae: 1.6586 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 13/100\n",
            "1000/1000 - 4s - loss: 15.3617 - mse: 15.3617 - mae: 1.5982 - val_loss: 13.0590 - val_mse: 13.0590 - val_mae: 1.6319 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Score for fold 4: loss of 13.059000968933105\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 4s - loss: 15.3428 - mse: 15.3428 - mae: 1.6252 - val_loss: 13.5871 - val_mse: 13.5871 - val_mae: 1.5160 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 4s - loss: 15.2506 - mse: 15.2506 - mae: 1.6209 - val_loss: 13.7149 - val_mse: 13.7149 - val_mae: 1.8207 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 4s - loss: 15.2668 - mse: 15.2668 - mae: 1.6249 - val_loss: 13.3052 - val_mse: 13.3052 - val_mae: 1.5107 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 4s - loss: 15.3096 - mse: 15.3096 - mae: 1.6262 - val_loss: 13.3829 - val_mse: 13.3829 - val_mae: 1.4968 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 4s - loss: 15.2682 - mse: 15.2682 - mae: 1.6231 - val_loss: 13.5296 - val_mse: 13.5296 - val_mae: 1.5636 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 4s - loss: 15.2239 - mse: 15.2239 - mae: 1.6117 - val_loss: 13.4293 - val_mse: 13.4293 - val_mae: 1.5870 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 4s - loss: 15.2013 - mse: 15.2013 - mae: 1.6156 - val_loss: 13.6256 - val_mse: 13.6256 - val_mae: 1.5384 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 4s - loss: 15.2490 - mse: 15.2490 - mae: 1.6281 - val_loss: 13.4586 - val_mse: 13.4586 - val_mae: 1.5193 - lr: 0.0010 - 4s/epoch - 4ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-10-14 15:58:45,160]\u001b[0m Finished trial#29 resulted in value: 15.095999999999998. Current best value is 14.758 with parameters: {'activation': 'relu', 'num_hidden_layer': 4, 'i': 7, 'learning_rate': 0.0008957084104345709}.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Score for fold 5: loss of 13.45861530303955\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 16.5308 - mse: 16.5308 - mae: 1.6287 - val_loss: 12.2153 - val_mse: 12.2153 - val_mae: 1.5583 - lr: 0.0011 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 15.9458 - mse: 15.9458 - mae: 1.6006 - val_loss: 12.1261 - val_mse: 12.1261 - val_mae: 1.5763 - lr: 0.0011 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 15.8862 - mse: 15.8862 - mae: 1.5968 - val_loss: 12.1705 - val_mse: 12.1705 - val_mae: 1.5617 - lr: 0.0011 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 15.8536 - mse: 15.8536 - mae: 1.5963 - val_loss: 12.0531 - val_mse: 12.0531 - val_mae: 1.5492 - lr: 0.0011 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 15.8222 - mse: 15.8222 - mae: 1.5951 - val_loss: 12.1480 - val_mse: 12.1480 - val_mae: 1.5576 - lr: 0.0011 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 15.8506 - mse: 15.8506 - mae: 1.5914 - val_loss: 12.0234 - val_mse: 12.0234 - val_mae: 1.6414 - lr: 0.0011 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 15.7896 - mse: 15.7896 - mae: 1.5956 - val_loss: 12.0124 - val_mse: 12.0124 - val_mae: 1.5673 - lr: 0.0011 - 2s/epoch - 2ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 2s - loss: 15.7703 - mse: 15.7703 - mae: 1.5900 - val_loss: 11.9626 - val_mse: 11.9626 - val_mae: 1.6152 - lr: 0.0011 - 2s/epoch - 2ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 2s - loss: 15.7857 - mse: 15.7857 - mae: 1.5883 - val_loss: 11.9398 - val_mse: 11.9398 - val_mae: 1.6314 - lr: 0.0011 - 2s/epoch - 2ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 2s - loss: 15.7380 - mse: 15.7380 - mae: 1.5915 - val_loss: 11.9782 - val_mse: 11.9782 - val_mae: 1.6053 - lr: 0.0011 - 2s/epoch - 2ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 2s - loss: 15.7073 - mse: 15.7073 - mae: 1.5896 - val_loss: 11.9512 - val_mse: 11.9512 - val_mae: 1.5525 - lr: 0.0011 - 2s/epoch - 2ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 2s - loss: 15.7027 - mse: 15.7027 - mae: 1.5880 - val_loss: 11.9556 - val_mse: 11.9556 - val_mae: 1.5839 - lr: 0.0011 - 2s/epoch - 2ms/step\n",
            "Epoch 13/100\n",
            "1000/1000 - 2s - loss: 15.7153 - mse: 15.7153 - mae: 1.5876 - val_loss: 11.9125 - val_mse: 11.9125 - val_mae: 1.5933 - lr: 0.0011 - 2s/epoch - 2ms/step\n",
            "Epoch 14/100\n",
            "1000/1000 - 2s - loss: 15.7040 - mse: 15.7040 - mae: 1.5861 - val_loss: 11.9365 - val_mse: 11.9365 - val_mae: 1.5695 - lr: 0.0011 - 2s/epoch - 2ms/step\n",
            "Epoch 15/100\n",
            "1000/1000 - 2s - loss: 15.7253 - mse: 15.7253 - mae: 1.5870 - val_loss: 11.9583 - val_mse: 11.9583 - val_mae: 1.5290 - lr: 0.0011 - 2s/epoch - 2ms/step\n",
            "Epoch 16/100\n",
            "1000/1000 - 2s - loss: 15.7053 - mse: 15.7053 - mae: 1.5823 - val_loss: 11.9231 - val_mse: 11.9231 - val_mae: 1.5972 - lr: 0.0011 - 2s/epoch - 2ms/step\n",
            "Epoch 17/100\n",
            "1000/1000 - 2s - loss: 15.6904 - mse: 15.6904 - mae: 1.5829 - val_loss: 11.8809 - val_mse: 11.8809 - val_mae: 1.5868 - lr: 0.0011 - 2s/epoch - 2ms/step\n",
            "Epoch 18/100\n",
            "1000/1000 - 2s - loss: 15.6619 - mse: 15.6619 - mae: 1.5818 - val_loss: 11.9125 - val_mse: 11.9125 - val_mae: 1.5840 - lr: 0.0011 - 2s/epoch - 2ms/step\n",
            "Epoch 19/100\n",
            "1000/1000 - 2s - loss: 15.6832 - mse: 15.6832 - mae: 1.5858 - val_loss: 11.9025 - val_mse: 11.9025 - val_mae: 1.5705 - lr: 0.0011 - 2s/epoch - 2ms/step\n",
            "Epoch 20/100\n",
            "1000/1000 - 2s - loss: 15.6575 - mse: 15.6575 - mae: 1.5844 - val_loss: 11.9903 - val_mse: 11.9903 - val_mae: 1.5626 - lr: 0.0011 - 2s/epoch - 2ms/step\n",
            "Epoch 21/100\n",
            "1000/1000 - 2s - loss: 15.6537 - mse: 15.6537 - mae: 1.5807 - val_loss: 11.9742 - val_mse: 11.9742 - val_mae: 1.6027 - lr: 0.0011 - 2s/epoch - 2ms/step\n",
            "Epoch 22/100\n",
            "1000/1000 - 2s - loss: 15.6689 - mse: 15.6689 - mae: 1.5855 - val_loss: 11.9180 - val_mse: 11.9180 - val_mae: 1.5389 - lr: 0.0011 - 2s/epoch - 2ms/step\n",
            "Score for fold 1: loss of 11.91800308227539\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 14.8681 - mse: 14.8681 - mae: 1.5766 - val_loss: 15.1648 - val_mse: 15.1648 - val_mae: 1.6144 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 14.8507 - mse: 14.8507 - mae: 1.5747 - val_loss: 15.1174 - val_mse: 15.1174 - val_mae: 1.6229 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 14.8735 - mse: 14.8735 - mae: 1.5718 - val_loss: 15.1647 - val_mse: 15.1647 - val_mae: 1.6083 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 14.8811 - mse: 14.8811 - mae: 1.5752 - val_loss: 15.1400 - val_mse: 15.1400 - val_mae: 1.5640 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 14.8610 - mse: 14.8610 - mae: 1.5755 - val_loss: 15.0706 - val_mse: 15.0706 - val_mae: 1.5843 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 14.8791 - mse: 14.8791 - mae: 1.5736 - val_loss: 15.0537 - val_mse: 15.0537 - val_mae: 1.6453 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 14.8389 - mse: 14.8389 - mae: 1.5740 - val_loss: 15.1519 - val_mse: 15.1519 - val_mae: 1.6057 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 2s - loss: 14.8459 - mse: 14.8459 - mae: 1.5747 - val_loss: 15.2005 - val_mse: 15.2005 - val_mae: 1.5777 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 2s - loss: 14.8284 - mse: 14.8284 - mae: 1.5698 - val_loss: 15.0192 - val_mse: 15.0192 - val_mae: 1.6492 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 2s - loss: 14.8520 - mse: 14.8520 - mae: 1.5755 - val_loss: 15.1517 - val_mse: 15.1517 - val_mae: 1.6021 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 2s - loss: 14.8396 - mse: 14.8396 - mae: 1.5718 - val_loss: 15.2214 - val_mse: 15.2214 - val_mae: 1.5822 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 2s - loss: 14.8452 - mse: 14.8452 - mae: 1.5696 - val_loss: 15.2848 - val_mse: 15.2848 - val_mae: 1.5651 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 13/100\n",
            "1000/1000 - 2s - loss: 14.8257 - mse: 14.8257 - mae: 1.5722 - val_loss: 15.2804 - val_mse: 15.2804 - val_mae: 1.6026 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 14/100\n",
            "1000/1000 - 2s - loss: 14.8472 - mse: 14.8472 - mae: 1.5748 - val_loss: 15.0990 - val_mse: 15.0990 - val_mae: 1.6015 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Score for fold 2: loss of 15.099000930786133\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 13.2918 - mse: 13.2918 - mae: 1.5713 - val_loss: 21.3520 - val_mse: 21.3520 - val_mae: 1.6692 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 13.2934 - mse: 13.2934 - mae: 1.5745 - val_loss: 21.3891 - val_mse: 21.3891 - val_mae: 1.6330 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 13.2903 - mse: 13.2903 - mae: 1.5738 - val_loss: 21.3136 - val_mse: 21.3136 - val_mae: 1.6398 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 13.2636 - mse: 13.2636 - mae: 1.5724 - val_loss: 21.4317 - val_mse: 21.4317 - val_mae: 1.5886 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 13.2779 - mse: 13.2779 - mae: 1.5725 - val_loss: 21.4570 - val_mse: 21.4570 - val_mae: 1.5593 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 13.2700 - mse: 13.2700 - mae: 1.5719 - val_loss: 21.5438 - val_mse: 21.5438 - val_mae: 1.5972 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 13.2888 - mse: 13.2888 - mae: 1.5759 - val_loss: 21.5047 - val_mse: 21.5047 - val_mae: 1.5923 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 2s - loss: 13.2685 - mse: 13.2685 - mae: 1.5730 - val_loss: 21.4818 - val_mse: 21.4818 - val_mae: 1.5975 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Score for fold 3: loss of 21.481840133666992\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 15.5763 - mse: 15.5763 - mae: 1.5906 - val_loss: 12.2677 - val_mse: 12.2677 - val_mae: 1.5454 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 15.5673 - mse: 15.5673 - mae: 1.5893 - val_loss: 12.2539 - val_mse: 12.2539 - val_mae: 1.5848 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 15.5748 - mse: 15.5748 - mae: 1.5897 - val_loss: 12.2166 - val_mse: 12.2166 - val_mae: 1.5584 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 15.5351 - mse: 15.5351 - mae: 1.5908 - val_loss: 12.2176 - val_mse: 12.2176 - val_mae: 1.5702 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 15.5397 - mse: 15.5397 - mae: 1.5919 - val_loss: 12.3165 - val_mse: 12.3165 - val_mae: 1.5275 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 15.5642 - mse: 15.5642 - mae: 1.5880 - val_loss: 12.2375 - val_mse: 12.2375 - val_mae: 1.5276 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 15.5107 - mse: 15.5107 - mae: 1.5902 - val_loss: 12.2568 - val_mse: 12.2568 - val_mae: 1.5423 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 2s - loss: 15.5472 - mse: 15.5472 - mae: 1.5888 - val_loss: 12.3012 - val_mse: 12.3012 - val_mae: 1.5758 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Score for fold 4: loss of 12.301214218139648\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 15.1548 - mse: 15.1548 - mae: 1.5730 - val_loss: 13.8780 - val_mse: 13.8780 - val_mae: 1.6392 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 15.1377 - mse: 15.1377 - mae: 1.5779 - val_loss: 13.9379 - val_mse: 13.9379 - val_mae: 1.5808 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 15.1388 - mse: 15.1388 - mae: 1.5729 - val_loss: 14.0084 - val_mse: 14.0084 - val_mae: 1.6133 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 15.0967 - mse: 15.0967 - mae: 1.5718 - val_loss: 14.0917 - val_mse: 14.0917 - val_mae: 1.6028 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 15.1226 - mse: 15.1226 - mae: 1.5765 - val_loss: 13.9092 - val_mse: 13.9092 - val_mae: 1.5918 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 15.0925 - mse: 15.0925 - mae: 1.5706 - val_loss: 13.9902 - val_mse: 13.9902 - val_mae: 1.5972 - lr: 0.0010 - 2s/epoch - 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-10-14 16:00:36,561]\u001b[0m Finished trial#30 resulted in value: 14.957999999999998. Current best value is 14.758 with parameters: {'activation': 'relu', 'num_hidden_layer': 4, 'i': 7, 'learning_rate': 0.0008957084104345709}.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Score for fold 5: loss of 13.990179061889648\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 3s - loss: 16.4616 - mse: 16.4616 - mae: 1.6293 - val_loss: 11.4867 - val_mse: 11.4867 - val_mae: 1.5010 - lr: 8.2136e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 16.0781 - mse: 16.0781 - mae: 1.6072 - val_loss: 11.3714 - val_mse: 11.3714 - val_mae: 1.5575 - lr: 8.2136e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 3s - loss: 16.0422 - mse: 16.0422 - mae: 1.6084 - val_loss: 11.3999 - val_mse: 11.3999 - val_mae: 1.5089 - lr: 8.2136e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 3s - loss: 15.9931 - mse: 15.9931 - mae: 1.6018 - val_loss: 11.3446 - val_mse: 11.3446 - val_mae: 1.5436 - lr: 8.2136e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 3s - loss: 15.9245 - mse: 15.9245 - mae: 1.5922 - val_loss: 11.0824 - val_mse: 11.0824 - val_mae: 1.5546 - lr: 8.2136e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 3s - loss: 15.9492 - mse: 15.9492 - mae: 1.5910 - val_loss: 11.2264 - val_mse: 11.2264 - val_mae: 1.5812 - lr: 8.2136e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 3s - loss: 15.9241 - mse: 15.9241 - mae: 1.5948 - val_loss: 11.1344 - val_mse: 11.1344 - val_mae: 1.5881 - lr: 8.2136e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 3s - loss: 15.9177 - mse: 15.9177 - mae: 1.5873 - val_loss: 11.0291 - val_mse: 11.0291 - val_mae: 1.5827 - lr: 8.2136e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 3s - loss: 15.8842 - mse: 15.8842 - mae: 1.5910 - val_loss: 11.1774 - val_mse: 11.1774 - val_mae: 1.5423 - lr: 8.2136e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 3s - loss: 15.8811 - mse: 15.8811 - mae: 1.5950 - val_loss: 11.1800 - val_mse: 11.1800 - val_mae: 1.6197 - lr: 8.2136e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 3s - loss: 15.7989 - mse: 15.7989 - mae: 1.5930 - val_loss: 11.1344 - val_mse: 11.1344 - val_mae: 1.5452 - lr: 8.2136e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 3s - loss: 15.7324 - mse: 15.7324 - mae: 1.5960 - val_loss: 11.0049 - val_mse: 11.0049 - val_mae: 1.5821 - lr: 8.2136e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 13/100\n",
            "1000/1000 - 3s - loss: 15.7382 - mse: 15.7382 - mae: 1.5964 - val_loss: 11.2506 - val_mse: 11.2506 - val_mae: 1.5687 - lr: 8.2136e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 14/100\n",
            "1000/1000 - 3s - loss: 15.6205 - mse: 15.6205 - mae: 1.5868 - val_loss: 11.4920 - val_mse: 11.4920 - val_mae: 1.5630 - lr: 8.2136e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 15/100\n",
            "1000/1000 - 3s - loss: 15.6876 - mse: 15.6876 - mae: 1.5932 - val_loss: 11.0085 - val_mse: 11.0085 - val_mae: 1.5652 - lr: 8.2136e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 16/100\n",
            "1000/1000 - 3s - loss: 15.6064 - mse: 15.6064 - mae: 1.5891 - val_loss: 10.9766 - val_mse: 10.9766 - val_mae: 1.5608 - lr: 8.2136e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 17/100\n",
            "1000/1000 - 3s - loss: 15.6260 - mse: 15.6260 - mae: 1.5887 - val_loss: 11.0604 - val_mse: 11.0604 - val_mae: 1.6695 - lr: 8.2136e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 18/100\n",
            "1000/1000 - 3s - loss: 15.5156 - mse: 15.5156 - mae: 1.5875 - val_loss: 11.0843 - val_mse: 11.0843 - val_mae: 1.5261 - lr: 8.2136e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 19/100\n",
            "1000/1000 - 3s - loss: 15.6647 - mse: 15.6647 - mae: 1.5873 - val_loss: 11.1378 - val_mse: 11.1378 - val_mae: 1.5924 - lr: 8.2136e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 20/100\n",
            "1000/1000 - 3s - loss: 15.4771 - mse: 15.4771 - mae: 1.5801 - val_loss: 11.0064 - val_mse: 11.0064 - val_mae: 1.6189 - lr: 8.2136e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 21/100\n",
            "1000/1000 - 3s - loss: 15.4738 - mse: 15.4738 - mae: 1.5784 - val_loss: 11.4277 - val_mse: 11.4277 - val_mae: 1.5039 - lr: 8.2136e-04 - 3s/epoch - 3ms/step\n",
            "Score for fold 1: loss of 11.427703857421875\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 3s - loss: 13.3511 - mse: 13.3511 - mae: 1.5913 - val_loss: 20.2154 - val_mse: 20.2154 - val_mae: 1.5245 - lr: 8.2136e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 3s - loss: 13.2857 - mse: 13.2857 - mae: 1.5833 - val_loss: 20.5011 - val_mse: 20.5011 - val_mae: 1.6750 - lr: 8.2136e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 3s - loss: 13.2557 - mse: 13.2557 - mae: 1.5795 - val_loss: 20.5562 - val_mse: 20.5562 - val_mae: 1.6160 - lr: 8.2136e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 3s - loss: 12.9264 - mse: 12.9264 - mae: 1.5873 - val_loss: 20.3612 - val_mse: 20.3612 - val_mae: 1.5503 - lr: 8.2136e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 3s - loss: 13.1157 - mse: 13.1157 - mae: 1.5875 - val_loss: 20.5661 - val_mse: 20.5661 - val_mae: 1.7986 - lr: 8.2136e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 3s - loss: 13.0675 - mse: 13.0675 - mae: 1.5974 - val_loss: 20.4855 - val_mse: 20.4855 - val_mae: 1.7336 - lr: 8.2136e-04 - 3s/epoch - 3ms/step\n",
            "Score for fold 2: loss of 20.485488891601562\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 14.1176 - mse: 14.1176 - mae: 1.5976 - val_loss: 16.7508 - val_mse: 16.7508 - val_mae: 1.6596 - lr: 8.2136e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 3s - loss: 14.0936 - mse: 14.0936 - mae: 1.6045 - val_loss: 16.8565 - val_mse: 16.8565 - val_mae: 1.6963 - lr: 8.2136e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 3s - loss: 14.0568 - mse: 14.0568 - mae: 1.6085 - val_loss: 17.5464 - val_mse: 17.5464 - val_mae: 1.5444 - lr: 8.2136e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 3s - loss: 13.9960 - mse: 13.9960 - mae: 1.6137 - val_loss: 17.4622 - val_mse: 17.4622 - val_mae: 1.6367 - lr: 8.2136e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 3s - loss: 14.0495 - mse: 14.0495 - mae: 1.6145 - val_loss: 17.0567 - val_mse: 17.0567 - val_mae: 1.6049 - lr: 8.2136e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 3s - loss: 13.9703 - mse: 13.9703 - mae: 1.6153 - val_loss: 17.2894 - val_mse: 17.2894 - val_mae: 1.6208 - lr: 8.2136e-04 - 3s/epoch - 3ms/step\n",
            "Score for fold 3: loss of 17.289365768432617\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 3s - loss: 15.5723 - mse: 15.5723 - mae: 1.6299 - val_loss: 11.5828 - val_mse: 11.5828 - val_mae: 1.5776 - lr: 8.2136e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 3s - loss: 15.4602 - mse: 15.4602 - mae: 1.6298 - val_loss: 11.4654 - val_mse: 11.4654 - val_mae: 1.6933 - lr: 8.2136e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 3s - loss: 15.3252 - mse: 15.3252 - mae: 1.6242 - val_loss: 11.4668 - val_mse: 11.4668 - val_mae: 1.5937 - lr: 8.2136e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 3s - loss: 15.3066 - mse: 15.3066 - mae: 1.6129 - val_loss: 11.6534 - val_mse: 11.6534 - val_mae: 1.4773 - lr: 8.2136e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 3s - loss: 15.2188 - mse: 15.2188 - mae: 1.6281 - val_loss: 11.4910 - val_mse: 11.4910 - val_mae: 1.5495 - lr: 8.2136e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 3s - loss: 15.2233 - mse: 15.2233 - mae: 1.6181 - val_loss: 12.2325 - val_mse: 12.2325 - val_mae: 1.9624 - lr: 8.2136e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 3s - loss: 15.0494 - mse: 15.0494 - mae: 1.6042 - val_loss: 11.8863 - val_mse: 11.8863 - val_mae: 1.4849 - lr: 8.2136e-04 - 3s/epoch - 3ms/step\n",
            "Score for fold 4: loss of 11.886274337768555\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 3s - loss: 14.8151 - mse: 14.8151 - mae: 1.6125 - val_loss: 13.2731 - val_mse: 13.2731 - val_mae: 1.4518 - lr: 8.2136e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 3s - loss: 14.7030 - mse: 14.7030 - mae: 1.6156 - val_loss: 13.1538 - val_mse: 13.1538 - val_mae: 1.5365 - lr: 8.2136e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 14.7492 - mse: 14.7492 - mae: 1.6197 - val_loss: 13.1132 - val_mse: 13.1132 - val_mae: 1.5498 - lr: 8.2136e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 3s - loss: 14.7503 - mse: 14.7503 - mae: 1.6136 - val_loss: 12.8279 - val_mse: 12.8279 - val_mae: 1.7033 - lr: 8.2136e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 3s - loss: 14.6119 - mse: 14.6119 - mae: 1.6117 - val_loss: 13.7255 - val_mse: 13.7255 - val_mae: 1.4980 - lr: 8.2136e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 3s - loss: 14.5741 - mse: 14.5741 - mae: 1.5932 - val_loss: 12.9381 - val_mse: 12.9381 - val_mae: 1.4819 - lr: 8.2136e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 14.4066 - mse: 14.4066 - mae: 1.5858 - val_loss: 13.2237 - val_mse: 13.2237 - val_mae: 1.6232 - lr: 8.2136e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 3s - loss: 14.4881 - mse: 14.4881 - mae: 1.6012 - val_loss: 13.9762 - val_mse: 13.9762 - val_mae: 1.8609 - lr: 8.2136e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 3s - loss: 14.4575 - mse: 14.4575 - mae: 1.5978 - val_loss: 13.5501 - val_mse: 13.5501 - val_mae: 1.4738 - lr: 8.2136e-04 - 3s/epoch - 3ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-10-14 16:02:49,771]\u001b[0m Finished trial#31 resulted in value: 14.929999999999998. Current best value is 14.758 with parameters: {'activation': 'relu', 'num_hidden_layer': 4, 'i': 7, 'learning_rate': 0.0008957084104345709}.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Score for fold 5: loss of 13.550098419189453\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 3s - loss: 15.8035 - mse: 15.8035 - mae: 1.6190 - val_loss: 13.9536 - val_mse: 13.9536 - val_mae: 1.5865 - lr: 0.0010 - 3s/epoch - 3ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 3s - loss: 15.4817 - mse: 15.4817 - mae: 1.6023 - val_loss: 13.7072 - val_mse: 13.7072 - val_mae: 1.5843 - lr: 0.0010 - 3s/epoch - 3ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 3s - loss: 15.3887 - mse: 15.3887 - mae: 1.5982 - val_loss: 13.6417 - val_mse: 13.6417 - val_mae: 1.5922 - lr: 0.0010 - 3s/epoch - 3ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 3s - loss: 15.3106 - mse: 15.3106 - mae: 1.5910 - val_loss: 13.8164 - val_mse: 13.8164 - val_mae: 1.6047 - lr: 0.0010 - 3s/epoch - 3ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 3s - loss: 15.3857 - mse: 15.3857 - mae: 1.5895 - val_loss: 14.0053 - val_mse: 14.0053 - val_mae: 1.4955 - lr: 0.0010 - 3s/epoch - 3ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 3s - loss: 15.3371 - mse: 15.3371 - mae: 1.5867 - val_loss: 13.5105 - val_mse: 13.5105 - val_mae: 1.6140 - lr: 0.0010 - 3s/epoch - 3ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 3s - loss: 15.4160 - mse: 15.4160 - mae: 1.5852 - val_loss: 13.6334 - val_mse: 13.6334 - val_mae: 1.5334 - lr: 0.0010 - 3s/epoch - 3ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 3s - loss: 15.2978 - mse: 15.2978 - mae: 1.5819 - val_loss: 13.5835 - val_mse: 13.5835 - val_mae: 1.5639 - lr: 0.0010 - 3s/epoch - 3ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 3s - loss: 15.2726 - mse: 15.2726 - mae: 1.5818 - val_loss: 13.6249 - val_mse: 13.6249 - val_mae: 1.6766 - lr: 0.0010 - 3s/epoch - 3ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 3s - loss: 15.2641 - mse: 15.2641 - mae: 1.5818 - val_loss: 13.6556 - val_mse: 13.6556 - val_mae: 1.6429 - lr: 0.0010 - 3s/epoch - 3ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 3s - loss: 15.1723 - mse: 15.1723 - mae: 1.5803 - val_loss: 13.6319 - val_mse: 13.6319 - val_mae: 1.5639 - lr: 0.0010 - 3s/epoch - 3ms/step\n",
            "Score for fold 1: loss of 13.631936073303223\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 12.8526 - mse: 12.8526 - mae: 1.5769 - val_loss: 22.8086 - val_mse: 22.8086 - val_mae: 1.5418 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 12.8483 - mse: 12.8483 - mae: 1.5815 - val_loss: 22.7398 - val_mse: 22.7398 - val_mae: 1.6220 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 12.8770 - mse: 12.8770 - mae: 1.5727 - val_loss: 22.8227 - val_mse: 22.8227 - val_mae: 1.5586 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 3s - loss: 12.8154 - mse: 12.8154 - mae: 1.5755 - val_loss: 22.9344 - val_mse: 22.9344 - val_mae: 1.6028 - lr: 0.0010 - 3s/epoch - 3ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 12.8239 - mse: 12.8239 - mae: 1.5795 - val_loss: 22.6168 - val_mse: 22.6168 - val_mae: 1.5471 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 12.6790 - mse: 12.6790 - mae: 1.5806 - val_loss: 22.6852 - val_mse: 22.6852 - val_mae: 1.5399 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 3s - loss: 12.5862 - mse: 12.5862 - mae: 1.5720 - val_loss: 23.1073 - val_mse: 23.1073 - val_mae: 1.6016 - lr: 0.0010 - 3s/epoch - 3ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 3s - loss: 12.6101 - mse: 12.6101 - mae: 1.5696 - val_loss: 22.9707 - val_mse: 22.9707 - val_mae: 1.6302 - lr: 0.0010 - 3s/epoch - 3ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 3s - loss: 12.7672 - mse: 12.7672 - mae: 1.5769 - val_loss: 22.8883 - val_mse: 22.8883 - val_mae: 1.5585 - lr: 0.0010 - 3s/epoch - 3ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 3s - loss: 12.7123 - mse: 12.7123 - mae: 1.5761 - val_loss: 22.7265 - val_mse: 22.7265 - val_mae: 1.5540 - lr: 0.0010 - 3s/epoch - 3ms/step\n",
            "Score for fold 2: loss of 22.726469039916992\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 15.9245 - mse: 15.9245 - mae: 1.6000 - val_loss: 10.0517 - val_mse: 10.0517 - val_mae: 1.4997 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 15.7890 - mse: 15.7890 - mae: 1.5913 - val_loss: 10.0543 - val_mse: 10.0543 - val_mae: 1.6822 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 15.5769 - mse: 15.5769 - mae: 1.5838 - val_loss: 9.9667 - val_mse: 9.9667 - val_mae: 1.5683 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 15.6776 - mse: 15.6776 - mae: 1.5829 - val_loss: 10.2144 - val_mse: 10.2144 - val_mae: 1.6006 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 15.6286 - mse: 15.6286 - mae: 1.5800 - val_loss: 9.9329 - val_mse: 9.9329 - val_mae: 1.6566 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 15.4044 - mse: 15.4044 - mae: 1.5896 - val_loss: 10.0595 - val_mse: 10.0595 - val_mae: 1.6820 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 15.4699 - mse: 15.4699 - mae: 1.5787 - val_loss: 9.8040 - val_mse: 9.8040 - val_mae: 1.5668 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 2s - loss: 15.5053 - mse: 15.5053 - mae: 1.5851 - val_loss: 10.3568 - val_mse: 10.3568 - val_mae: 1.5222 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 2s - loss: 15.5453 - mse: 15.5453 - mae: 1.5836 - val_loss: 9.9029 - val_mse: 9.9029 - val_mae: 1.5386 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 2s - loss: 15.5846 - mse: 15.5846 - mae: 1.5820 - val_loss: 9.9752 - val_mse: 9.9752 - val_mae: 1.6112 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 2s - loss: 15.3884 - mse: 15.3884 - mae: 1.5761 - val_loss: 10.4202 - val_mse: 10.4202 - val_mae: 1.4733 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 2s - loss: 15.2792 - mse: 15.2792 - mae: 1.5766 - val_loss: 10.1487 - val_mse: 10.1487 - val_mae: 1.5710 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Score for fold 3: loss of 10.148652076721191\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 14.4434 - mse: 14.4434 - mae: 1.5820 - val_loss: 14.1069 - val_mse: 14.1069 - val_mae: 1.5692 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 3s - loss: 14.3454 - mse: 14.3454 - mae: 1.5765 - val_loss: 14.4492 - val_mse: 14.4492 - val_mae: 1.5838 - lr: 0.0010 - 3s/epoch - 3ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 3s - loss: 14.4426 - mse: 14.4426 - mae: 1.5735 - val_loss: 14.4801 - val_mse: 14.4801 - val_mae: 1.5449 - lr: 0.0010 - 3s/epoch - 3ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 3s - loss: 14.2721 - mse: 14.2721 - mae: 1.5732 - val_loss: 14.5319 - val_mse: 14.5319 - val_mae: 1.5982 - lr: 0.0010 - 3s/epoch - 3ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 3s - loss: 14.1227 - mse: 14.1227 - mae: 1.5694 - val_loss: 14.3401 - val_mse: 14.3401 - val_mae: 1.5473 - lr: 0.0010 - 3s/epoch - 3ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 3s - loss: 14.1402 - mse: 14.1402 - mae: 1.5674 - val_loss: 14.6292 - val_mse: 14.6292 - val_mae: 1.5450 - lr: 0.0010 - 3s/epoch - 3ms/step\n",
            "Score for fold 4: loss of 14.629223823547363\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 14.8086 - mse: 14.8086 - mae: 1.5716 - val_loss: 12.4494 - val_mse: 12.4494 - val_mae: 1.5362 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 14.6553 - mse: 14.6553 - mae: 1.5826 - val_loss: 12.7833 - val_mse: 12.7833 - val_mae: 1.6130 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 14.7205 - mse: 14.7205 - mae: 1.5781 - val_loss: 13.0198 - val_mse: 13.0198 - val_mae: 1.9269 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 14.6541 - mse: 14.6541 - mae: 1.5735 - val_loss: 13.1301 - val_mse: 13.1301 - val_mae: 1.4698 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 14.5333 - mse: 14.5333 - mae: 1.5609 - val_loss: 12.8850 - val_mse: 12.8850 - val_mae: 1.5694 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 14.5533 - mse: 14.5533 - mae: 1.5667 - val_loss: 12.0382 - val_mse: 12.0382 - val_mae: 1.6400 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 14.5089 - mse: 14.5089 - mae: 1.5606 - val_loss: 12.9225 - val_mse: 12.9225 - val_mae: 1.5153 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 2s - loss: 14.6280 - mse: 14.6280 - mae: 1.5633 - val_loss: 12.9530 - val_mse: 12.9530 - val_mae: 1.5610 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 2s - loss: 14.5687 - mse: 14.5687 - mae: 1.5598 - val_loss: 12.0560 - val_mse: 12.0560 - val_mae: 1.5872 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 2s - loss: 14.4758 - mse: 14.4758 - mae: 1.5524 - val_loss: 12.6740 - val_mse: 12.6740 - val_mae: 1.5945 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 2s - loss: 14.5179 - mse: 14.5179 - mae: 1.5557 - val_loss: 12.8503 - val_mse: 12.8503 - val_mae: 1.5906 - lr: 0.0010 - 2s/epoch - 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-10-14 16:05:01,039]\u001b[0m Finished trial#32 resulted in value: 14.797999999999998. Current best value is 14.758 with parameters: {'activation': 'relu', 'num_hidden_layer': 4, 'i': 7, 'learning_rate': 0.0008957084104345709}.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Score for fold 5: loss of 12.850259780883789\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 3s - loss: 16.9890 - mse: 16.9890 - mae: 1.6281 - val_loss: 9.8379 - val_mse: 9.8379 - val_mae: 1.5335 - lr: 5.7768e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 16.4486 - mse: 16.4486 - mae: 1.5983 - val_loss: 9.8214 - val_mse: 9.8214 - val_mae: 1.5766 - lr: 5.7768e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 16.3875 - mse: 16.3875 - mae: 1.5961 - val_loss: 9.9722 - val_mse: 9.9722 - val_mae: 1.5008 - lr: 5.7768e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 16.3633 - mse: 16.3633 - mae: 1.5956 - val_loss: 9.8145 - val_mse: 9.8145 - val_mae: 1.5630 - lr: 5.7768e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 16.2297 - mse: 16.2297 - mae: 1.5932 - val_loss: 9.7458 - val_mse: 9.7458 - val_mae: 1.5887 - lr: 5.7768e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 16.2539 - mse: 16.2539 - mae: 1.5927 - val_loss: 9.7477 - val_mse: 9.7477 - val_mae: 1.5847 - lr: 5.7768e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 16.2888 - mse: 16.2888 - mae: 1.5884 - val_loss: 9.8522 - val_mse: 9.8522 - val_mae: 1.5149 - lr: 5.7768e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 2s - loss: 16.2255 - mse: 16.2255 - mae: 1.5849 - val_loss: 9.7195 - val_mse: 9.7195 - val_mae: 1.5444 - lr: 5.7768e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 2s - loss: 16.2068 - mse: 16.2068 - mae: 1.5842 - val_loss: 9.7573 - val_mse: 9.7573 - val_mae: 1.6241 - lr: 5.7768e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 2s - loss: 16.1418 - mse: 16.1418 - mae: 1.5828 - val_loss: 9.9404 - val_mse: 9.9404 - val_mae: 1.5638 - lr: 5.7768e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 2s - loss: 16.1478 - mse: 16.1478 - mae: 1.5835 - val_loss: 9.6966 - val_mse: 9.6966 - val_mae: 1.5758 - lr: 5.7768e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 2s - loss: 16.0770 - mse: 16.0770 - mae: 1.5846 - val_loss: 9.8421 - val_mse: 9.8421 - val_mae: 1.5579 - lr: 5.7768e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 13/100\n",
            "1000/1000 - 2s - loss: 16.1256 - mse: 16.1256 - mae: 1.5838 - val_loss: 9.7480 - val_mse: 9.7480 - val_mae: 1.5557 - lr: 5.7768e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 14/100\n",
            "1000/1000 - 2s - loss: 16.1213 - mse: 16.1213 - mae: 1.5776 - val_loss: 9.6864 - val_mse: 9.6864 - val_mae: 1.5995 - lr: 5.7768e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 15/100\n",
            "1000/1000 - 2s - loss: 16.0208 - mse: 16.0208 - mae: 1.5839 - val_loss: 9.8012 - val_mse: 9.8012 - val_mae: 1.5362 - lr: 5.7768e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 16/100\n",
            "1000/1000 - 2s - loss: 16.1292 - mse: 16.1292 - mae: 1.5804 - val_loss: 9.7239 - val_mse: 9.7239 - val_mae: 1.5500 - lr: 5.7768e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 17/100\n",
            "1000/1000 - 2s - loss: 16.1193 - mse: 16.1193 - mae: 1.5798 - val_loss: 9.6657 - val_mse: 9.6657 - val_mae: 1.5446 - lr: 5.7768e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 18/100\n",
            "1000/1000 - 2s - loss: 16.0541 - mse: 16.0541 - mae: 1.5837 - val_loss: 9.6903 - val_mse: 9.6903 - val_mae: 1.5467 - lr: 5.7768e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 19/100\n",
            "1000/1000 - 2s - loss: 15.9276 - mse: 15.9276 - mae: 1.5803 - val_loss: 9.7374 - val_mse: 9.7374 - val_mae: 1.6060 - lr: 5.7768e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 20/100\n",
            "1000/1000 - 2s - loss: 16.0580 - mse: 16.0580 - mae: 1.5815 - val_loss: 9.8640 - val_mse: 9.8640 - val_mae: 1.4939 - lr: 5.7768e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 21/100\n",
            "1000/1000 - 2s - loss: 15.9602 - mse: 15.9602 - mae: 1.5801 - val_loss: 9.6749 - val_mse: 9.6749 - val_mae: 1.5680 - lr: 5.7768e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 22/100\n",
            "1000/1000 - 2s - loss: 15.9426 - mse: 15.9426 - mae: 1.5754 - val_loss: 9.5773 - val_mse: 9.5773 - val_mae: 1.5765 - lr: 5.7768e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 23/100\n",
            "1000/1000 - 2s - loss: 15.9655 - mse: 15.9655 - mae: 1.5846 - val_loss: 9.7679 - val_mse: 9.7679 - val_mae: 1.5456 - lr: 5.7768e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 24/100\n",
            "1000/1000 - 2s - loss: 15.8633 - mse: 15.8633 - mae: 1.5793 - val_loss: 9.7535 - val_mse: 9.7535 - val_mae: 1.7063 - lr: 5.7768e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 25/100\n",
            "1000/1000 - 2s - loss: 15.8912 - mse: 15.8912 - mae: 1.5779 - val_loss: 9.5470 - val_mse: 9.5470 - val_mae: 1.5950 - lr: 5.7768e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 26/100\n",
            "1000/1000 - 2s - loss: 15.7776 - mse: 15.7776 - mae: 1.5805 - val_loss: 9.5213 - val_mse: 9.5213 - val_mae: 1.5572 - lr: 5.7768e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 27/100\n",
            "1000/1000 - 2s - loss: 15.7707 - mse: 15.7707 - mae: 1.5731 - val_loss: 9.4430 - val_mse: 9.4430 - val_mae: 1.5394 - lr: 5.7768e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 28/100\n",
            "1000/1000 - 2s - loss: 15.8143 - mse: 15.8143 - mae: 1.5788 - val_loss: 9.5698 - val_mse: 9.5698 - val_mae: 1.5675 - lr: 5.7768e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 29/100\n",
            "1000/1000 - 2s - loss: 15.7526 - mse: 15.7526 - mae: 1.5728 - val_loss: 9.5310 - val_mse: 9.5310 - val_mae: 1.5752 - lr: 5.7768e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 30/100\n",
            "1000/1000 - 2s - loss: 15.8423 - mse: 15.8423 - mae: 1.5734 - val_loss: 9.5222 - val_mse: 9.5222 - val_mae: 1.5752 - lr: 5.7768e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 31/100\n",
            "1000/1000 - 2s - loss: 15.7026 - mse: 15.7026 - mae: 1.5743 - val_loss: 9.7605 - val_mse: 9.7605 - val_mae: 1.4841 - lr: 5.7768e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 32/100\n",
            "1000/1000 - 2s - loss: 15.7539 - mse: 15.7539 - mae: 1.5727 - val_loss: 9.9364 - val_mse: 9.9364 - val_mae: 1.5101 - lr: 5.7768e-04 - 2s/epoch - 2ms/step\n",
            "Score for fold 1: loss of 9.936420440673828\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 12.3092 - mse: 12.3092 - mae: 1.5647 - val_loss: 23.3212 - val_mse: 23.3212 - val_mae: 1.6156 - lr: 5.7768e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 12.1974 - mse: 12.1974 - mae: 1.5683 - val_loss: 23.5411 - val_mse: 23.5411 - val_mae: 1.6046 - lr: 5.7768e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 12.1381 - mse: 12.1381 - mae: 1.5622 - val_loss: 23.4331 - val_mse: 23.4331 - val_mae: 1.5660 - lr: 5.7768e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 12.1290 - mse: 12.1290 - mae: 1.5627 - val_loss: 23.5623 - val_mse: 23.5623 - val_mae: 1.6401 - lr: 5.7768e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 12.1697 - mse: 12.1697 - mae: 1.5671 - val_loss: 23.2921 - val_mse: 23.2921 - val_mae: 1.6983 - lr: 5.7768e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 12.0283 - mse: 12.0283 - mae: 1.5581 - val_loss: 23.7300 - val_mse: 23.7300 - val_mae: 1.6129 - lr: 5.7768e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 12.0747 - mse: 12.0747 - mae: 1.5658 - val_loss: 23.2675 - val_mse: 23.2675 - val_mae: 1.7113 - lr: 5.7768e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 2s - loss: 11.9934 - mse: 11.9934 - mae: 1.5554 - val_loss: 23.1714 - val_mse: 23.1714 - val_mae: 1.7361 - lr: 5.7768e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 2s - loss: 12.0244 - mse: 12.0244 - mae: 1.5615 - val_loss: 23.7798 - val_mse: 23.7798 - val_mae: 1.5796 - lr: 5.7768e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 2s - loss: 11.9748 - mse: 11.9748 - mae: 1.5622 - val_loss: 23.5873 - val_mse: 23.5873 - val_mae: 1.6124 - lr: 5.7768e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 2s - loss: 11.8796 - mse: 11.8796 - mae: 1.5584 - val_loss: 23.6196 - val_mse: 23.6196 - val_mae: 1.5406 - lr: 5.7768e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 2s - loss: 11.9321 - mse: 11.9321 - mae: 1.5579 - val_loss: 23.9583 - val_mse: 23.9583 - val_mae: 1.5430 - lr: 5.7768e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 13/100\n",
            "1000/1000 - 2s - loss: 11.9744 - mse: 11.9744 - mae: 1.5620 - val_loss: 23.6769 - val_mse: 23.6769 - val_mae: 1.6614 - lr: 5.7768e-04 - 2s/epoch - 2ms/step\n",
            "Score for fold 2: loss of 23.67684555053711\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 15.0731 - mse: 15.0731 - mae: 1.5684 - val_loss: 11.1650 - val_mse: 11.1650 - val_mae: 1.6748 - lr: 5.7768e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 15.0840 - mse: 15.0840 - mae: 1.5711 - val_loss: 11.2144 - val_mse: 11.2144 - val_mae: 1.4971 - lr: 5.7768e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 15.0525 - mse: 15.0525 - mae: 1.5650 - val_loss: 11.2345 - val_mse: 11.2345 - val_mae: 1.6561 - lr: 5.7768e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 14.9713 - mse: 14.9713 - mae: 1.5674 - val_loss: 11.2173 - val_mse: 11.2173 - val_mae: 1.6707 - lr: 5.7768e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 14.9602 - mse: 14.9602 - mae: 1.5733 - val_loss: 11.2815 - val_mse: 11.2815 - val_mae: 1.5506 - lr: 5.7768e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 15.0284 - mse: 15.0284 - mae: 1.5640 - val_loss: 11.3719 - val_mse: 11.3719 - val_mae: 1.5562 - lr: 5.7768e-04 - 2s/epoch - 2ms/step\n",
            "Score for fold 3: loss of 11.371856689453125\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 13.5263 - mse: 13.5263 - mae: 1.5617 - val_loss: 16.9059 - val_mse: 16.9059 - val_mae: 1.5718 - lr: 5.7768e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 13.4155 - mse: 13.4155 - mae: 1.5484 - val_loss: 17.4789 - val_mse: 17.4789 - val_mae: 1.5517 - lr: 5.7768e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 13.3766 - mse: 13.3766 - mae: 1.5540 - val_loss: 17.2171 - val_mse: 17.2171 - val_mae: 1.5942 - lr: 5.7768e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 13.2948 - mse: 13.2948 - mae: 1.5493 - val_loss: 17.4204 - val_mse: 17.4204 - val_mae: 1.6410 - lr: 5.7768e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 13.2805 - mse: 13.2805 - mae: 1.5455 - val_loss: 17.2026 - val_mse: 17.2026 - val_mae: 1.5945 - lr: 5.7768e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 13.2984 - mse: 13.2984 - mae: 1.5503 - val_loss: 17.1907 - val_mse: 17.1907 - val_mae: 1.6857 - lr: 5.7768e-04 - 2s/epoch - 2ms/step\n",
            "Score for fold 4: loss of 17.190692901611328\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 15.1384 - mse: 15.1384 - mae: 1.5781 - val_loss: 10.6661 - val_mse: 10.6661 - val_mae: 1.4567 - lr: 5.7768e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 15.0548 - mse: 15.0548 - mae: 1.5710 - val_loss: 10.4735 - val_mse: 10.4735 - val_mae: 1.5424 - lr: 5.7768e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 14.9945 - mse: 14.9945 - mae: 1.5713 - val_loss: 10.5248 - val_mse: 10.5248 - val_mae: 1.5268 - lr: 5.7768e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 14.7768 - mse: 14.7768 - mae: 1.5716 - val_loss: 10.5612 - val_mse: 10.5612 - val_mae: 1.5185 - lr: 5.7768e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 14.9359 - mse: 14.9359 - mae: 1.5695 - val_loss: 10.7879 - val_mse: 10.7879 - val_mae: 1.5304 - lr: 5.7768e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 14.7975 - mse: 14.7975 - mae: 1.5649 - val_loss: 10.5860 - val_mse: 10.5860 - val_mae: 1.5230 - lr: 5.7768e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 14.8312 - mse: 14.8312 - mae: 1.5655 - val_loss: 10.7904 - val_mse: 10.7904 - val_mae: 1.5125 - lr: 5.7768e-04 - 2s/epoch - 2ms/step\n",
            "Score for fold 5: loss of 10.790366172790527\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-10-14 16:07:15,955]\u001b[0m Finished trial#33 resulted in value: 14.594. Current best value is 14.594 with parameters: {'activation': 'relu', 'num_hidden_layer': 4, 'i': 6, 'learning_rate': 0.000577682381030866}.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 3s - loss: 15.6583 - mse: 15.6583 - mae: 1.6265 - val_loss: 15.9042 - val_mse: 15.9042 - val_mae: 1.5277 - lr: 4.2845e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 14.9978 - mse: 14.9978 - mae: 1.5970 - val_loss: 15.7730 - val_mse: 15.7730 - val_mae: 1.6047 - lr: 4.2845e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 14.8909 - mse: 14.8909 - mae: 1.5936 - val_loss: 15.7002 - val_mse: 15.7002 - val_mae: 1.5706 - lr: 4.2845e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 14.8748 - mse: 14.8748 - mae: 1.5883 - val_loss: 15.6574 - val_mse: 15.6574 - val_mae: 1.5764 - lr: 4.2845e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 14.8491 - mse: 14.8491 - mae: 1.5840 - val_loss: 15.7160 - val_mse: 15.7160 - val_mae: 1.5387 - lr: 4.2845e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 14.8009 - mse: 14.8009 - mae: 1.5807 - val_loss: 15.6958 - val_mse: 15.6958 - val_mae: 1.5468 - lr: 4.2845e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 14.7630 - mse: 14.7630 - mae: 1.5799 - val_loss: 15.7179 - val_mse: 15.7179 - val_mae: 1.5543 - lr: 4.2845e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 2s - loss: 14.7843 - mse: 14.7843 - mae: 1.5808 - val_loss: 15.5686 - val_mse: 15.5686 - val_mae: 1.5705 - lr: 4.2845e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 2s - loss: 14.6951 - mse: 14.6951 - mae: 1.5752 - val_loss: 15.6447 - val_mse: 15.6447 - val_mae: 1.6111 - lr: 4.2845e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 2s - loss: 14.7257 - mse: 14.7257 - mae: 1.5777 - val_loss: 15.5921 - val_mse: 15.5921 - val_mae: 1.6028 - lr: 4.2845e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 2s - loss: 14.7073 - mse: 14.7073 - mae: 1.5735 - val_loss: 15.6077 - val_mse: 15.6077 - val_mae: 1.5982 - lr: 4.2845e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 2s - loss: 14.6466 - mse: 14.6466 - mae: 1.5743 - val_loss: 15.5683 - val_mse: 15.5683 - val_mae: 1.5999 - lr: 4.2845e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 13/100\n",
            "1000/1000 - 2s - loss: 14.5975 - mse: 14.5975 - mae: 1.5765 - val_loss: 15.6815 - val_mse: 15.6815 - val_mae: 1.6030 - lr: 4.2845e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 14/100\n",
            "1000/1000 - 2s - loss: 14.6163 - mse: 14.6163 - mae: 1.5721 - val_loss: 15.5720 - val_mse: 15.5720 - val_mae: 1.5645 - lr: 4.2845e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 15/100\n",
            "1000/1000 - 2s - loss: 14.6430 - mse: 14.6430 - mae: 1.5715 - val_loss: 15.5567 - val_mse: 15.5567 - val_mae: 1.5889 - lr: 4.2845e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 16/100\n",
            "1000/1000 - 2s - loss: 14.6396 - mse: 14.6396 - mae: 1.5766 - val_loss: 15.5048 - val_mse: 15.5048 - val_mae: 1.5997 - lr: 4.2845e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 17/100\n",
            "1000/1000 - 2s - loss: 14.5943 - mse: 14.5943 - mae: 1.5749 - val_loss: 15.5675 - val_mse: 15.5675 - val_mae: 1.5573 - lr: 4.2845e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 18/100\n",
            "1000/1000 - 2s - loss: 14.5805 - mse: 14.5805 - mae: 1.5705 - val_loss: 15.6273 - val_mse: 15.6273 - val_mae: 1.6016 - lr: 4.2845e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 19/100\n",
            "1000/1000 - 2s - loss: 14.5278 - mse: 14.5278 - mae: 1.5725 - val_loss: 15.5464 - val_mse: 15.5464 - val_mae: 1.5781 - lr: 4.2845e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 20/100\n",
            "1000/1000 - 2s - loss: 14.5698 - mse: 14.5698 - mae: 1.5707 - val_loss: 15.5868 - val_mse: 15.5868 - val_mae: 1.5798 - lr: 4.2845e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 21/100\n",
            "1000/1000 - 2s - loss: 14.5726 - mse: 14.5726 - mae: 1.5724 - val_loss: 15.4940 - val_mse: 15.4940 - val_mae: 1.5824 - lr: 4.2845e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 22/100\n",
            "1000/1000 - 2s - loss: 14.5714 - mse: 14.5714 - mae: 1.5715 - val_loss: 15.6351 - val_mse: 15.6351 - val_mae: 1.5764 - lr: 4.2845e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 23/100\n",
            "1000/1000 - 2s - loss: 14.5306 - mse: 14.5306 - mae: 1.5730 - val_loss: 15.6312 - val_mse: 15.6312 - val_mae: 1.5873 - lr: 4.2845e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 24/100\n",
            "1000/1000 - 2s - loss: 14.5487 - mse: 14.5487 - mae: 1.5785 - val_loss: 15.5716 - val_mse: 15.5716 - val_mae: 1.5771 - lr: 4.2845e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 25/100\n",
            "1000/1000 - 2s - loss: 14.4450 - mse: 14.4450 - mae: 1.5704 - val_loss: 15.6652 - val_mse: 15.6652 - val_mae: 1.5335 - lr: 4.2845e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 26/100\n",
            "1000/1000 - 2s - loss: 14.5626 - mse: 14.5626 - mae: 1.5725 - val_loss: 15.8195 - val_mse: 15.8195 - val_mae: 1.5368 - lr: 4.2845e-04 - 2s/epoch - 2ms/step\n",
            "Score for fold 1: loss of 15.819547653198242\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 15.2008 - mse: 15.2008 - mae: 1.5733 - val_loss: 13.2812 - val_mse: 13.2812 - val_mae: 1.5339 - lr: 4.2845e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 15.2445 - mse: 15.2445 - mae: 1.5747 - val_loss: 13.2178 - val_mse: 13.2178 - val_mae: 1.5800 - lr: 4.2845e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 15.1511 - mse: 15.1511 - mae: 1.5726 - val_loss: 13.0115 - val_mse: 13.0115 - val_mae: 1.5494 - lr: 4.2845e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 15.1419 - mse: 15.1419 - mae: 1.5727 - val_loss: 12.9813 - val_mse: 12.9813 - val_mae: 1.5983 - lr: 4.2845e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 15.1858 - mse: 15.1858 - mae: 1.5763 - val_loss: 13.3498 - val_mse: 13.3498 - val_mae: 1.5675 - lr: 4.2845e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 15.1525 - mse: 15.1525 - mae: 1.5715 - val_loss: 13.2212 - val_mse: 13.2212 - val_mae: 1.5946 - lr: 4.2845e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 15.0512 - mse: 15.0512 - mae: 1.5715 - val_loss: 13.0364 - val_mse: 13.0364 - val_mae: 1.6572 - lr: 4.2845e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 2s - loss: 15.0499 - mse: 15.0499 - mae: 1.5719 - val_loss: 13.4440 - val_mse: 13.4440 - val_mae: 1.5621 - lr: 4.2845e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 2s - loss: 15.0798 - mse: 15.0798 - mae: 1.5700 - val_loss: 13.5338 - val_mse: 13.5338 - val_mae: 1.5937 - lr: 4.2845e-04 - 2s/epoch - 2ms/step\n",
            "Score for fold 2: loss of 13.53376579284668\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 12.9784 - mse: 12.9784 - mae: 1.5635 - val_loss: 21.4982 - val_mse: 21.4982 - val_mae: 1.6938 - lr: 4.2845e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 12.9287 - mse: 12.9287 - mae: 1.5616 - val_loss: 21.3149 - val_mse: 21.3149 - val_mae: 1.6415 - lr: 4.2845e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 12.8466 - mse: 12.8466 - mae: 1.5637 - val_loss: 21.5283 - val_mse: 21.5283 - val_mae: 1.6173 - lr: 4.2845e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 12.8249 - mse: 12.8249 - mae: 1.5638 - val_loss: 21.4347 - val_mse: 21.4347 - val_mae: 1.6454 - lr: 4.2845e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 12.7471 - mse: 12.7471 - mae: 1.5581 - val_loss: 21.7431 - val_mse: 21.7431 - val_mae: 1.6920 - lr: 4.2845e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 12.8267 - mse: 12.8267 - mae: 1.5629 - val_loss: 21.7659 - val_mse: 21.7659 - val_mae: 1.5906 - lr: 4.2845e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 12.7749 - mse: 12.7749 - mae: 1.5562 - val_loss: 21.9355 - val_mse: 21.9355 - val_mae: 1.6273 - lr: 4.2845e-04 - 2s/epoch - 2ms/step\n",
            "Score for fold 3: loss of 21.935508728027344\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 15.6159 - mse: 15.6159 - mae: 1.5758 - val_loss: 10.5957 - val_mse: 10.5957 - val_mae: 1.5464 - lr: 4.2845e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 15.5533 - mse: 15.5533 - mae: 1.5707 - val_loss: 10.6772 - val_mse: 10.6772 - val_mae: 1.5003 - lr: 4.2845e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 15.5009 - mse: 15.5009 - mae: 1.5699 - val_loss: 10.8298 - val_mse: 10.8298 - val_mae: 1.5630 - lr: 4.2845e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 15.5019 - mse: 15.5019 - mae: 1.5726 - val_loss: 10.6283 - val_mse: 10.6283 - val_mae: 1.5609 - lr: 4.2845e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 15.3771 - mse: 15.3771 - mae: 1.5709 - val_loss: 11.0089 - val_mse: 11.0089 - val_mae: 1.7974 - lr: 4.2845e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 15.2281 - mse: 15.2281 - mae: 1.5646 - val_loss: 10.9699 - val_mse: 10.9699 - val_mae: 1.5034 - lr: 4.2845e-04 - 2s/epoch - 2ms/step\n",
            "Score for fold 4: loss of 10.969937324523926\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 14.9123 - mse: 14.9123 - mae: 1.5909 - val_loss: 12.7557 - val_mse: 12.7557 - val_mae: 1.5028 - lr: 4.2845e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 14.8808 - mse: 14.8808 - mae: 1.5840 - val_loss: 12.7351 - val_mse: 12.7351 - val_mae: 1.5113 - lr: 4.2845e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 14.7912 - mse: 14.7912 - mae: 1.5775 - val_loss: 12.7749 - val_mse: 12.7749 - val_mae: 1.5243 - lr: 4.2845e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 14.7547 - mse: 14.7547 - mae: 1.5798 - val_loss: 12.8342 - val_mse: 12.8342 - val_mae: 1.5344 - lr: 4.2845e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 14.6964 - mse: 14.6964 - mae: 1.5711 - val_loss: 12.7873 - val_mse: 12.7873 - val_mae: 1.5764 - lr: 4.2845e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 14.8062 - mse: 14.8062 - mae: 1.5731 - val_loss: 13.0074 - val_mse: 13.0074 - val_mae: 1.6079 - lr: 4.2845e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 14.6863 - mse: 14.6863 - mae: 1.5736 - val_loss: 12.9754 - val_mse: 12.9754 - val_mae: 1.5385 - lr: 4.2845e-04 - 2s/epoch - 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-10-14 16:09:13,600]\u001b[0m Finished trial#34 resulted in value: 15.048000000000002. Current best value is 14.594 with parameters: {'activation': 'relu', 'num_hidden_layer': 4, 'i': 6, 'learning_rate': 0.000577682381030866}.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Score for fold 5: loss of 12.975419998168945\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 3s - loss: 16.3066 - mse: 16.3066 - mae: 1.6781 - val_loss: 15.9770 - val_mse: 15.9770 - val_mae: 1.6081 - lr: 2.0354e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 15.2024 - mse: 15.2024 - mae: 1.5964 - val_loss: 15.7361 - val_mse: 15.7361 - val_mae: 1.6529 - lr: 2.0354e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 15.0810 - mse: 15.0810 - mae: 1.5918 - val_loss: 15.7238 - val_mse: 15.7238 - val_mae: 1.5485 - lr: 2.0354e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 14.9918 - mse: 14.9918 - mae: 1.5924 - val_loss: 15.8115 - val_mse: 15.8115 - val_mae: 1.5508 - lr: 2.0354e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 14.9432 - mse: 14.9432 - mae: 1.5870 - val_loss: 15.5351 - val_mse: 15.5351 - val_mae: 1.6248 - lr: 2.0354e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 14.9020 - mse: 14.9020 - mae: 1.5899 - val_loss: 15.5049 - val_mse: 15.5049 - val_mae: 1.5764 - lr: 2.0354e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 14.8678 - mse: 14.8678 - mae: 1.5827 - val_loss: 15.5678 - val_mse: 15.5678 - val_mae: 1.5945 - lr: 2.0354e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 2s - loss: 14.8768 - mse: 14.8768 - mae: 1.5865 - val_loss: 15.5634 - val_mse: 15.5634 - val_mae: 1.5922 - lr: 2.0354e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 2s - loss: 14.8583 - mse: 14.8583 - mae: 1.5845 - val_loss: 15.4286 - val_mse: 15.4286 - val_mae: 1.5827 - lr: 2.0354e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 2s - loss: 14.8284 - mse: 14.8284 - mae: 1.5825 - val_loss: 15.3851 - val_mse: 15.3851 - val_mae: 1.5917 - lr: 2.0354e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 2s - loss: 14.7973 - mse: 14.7973 - mae: 1.5807 - val_loss: 15.4473 - val_mse: 15.4473 - val_mae: 1.6000 - lr: 2.0354e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 2s - loss: 14.7939 - mse: 14.7939 - mae: 1.5811 - val_loss: 15.7265 - val_mse: 15.7265 - val_mae: 1.6069 - lr: 2.0354e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 13/100\n",
            "1000/1000 - 2s - loss: 14.7820 - mse: 14.7820 - mae: 1.5781 - val_loss: 15.4639 - val_mse: 15.4639 - val_mae: 1.5717 - lr: 2.0354e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 14/100\n",
            "1000/1000 - 2s - loss: 14.7359 - mse: 14.7359 - mae: 1.5792 - val_loss: 15.5830 - val_mse: 15.5830 - val_mae: 1.5543 - lr: 2.0354e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 15/100\n",
            "1000/1000 - 2s - loss: 14.7502 - mse: 14.7502 - mae: 1.5782 - val_loss: 15.3327 - val_mse: 15.3327 - val_mae: 1.5721 - lr: 2.0354e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 16/100\n",
            "1000/1000 - 2s - loss: 14.7364 - mse: 14.7364 - mae: 1.5731 - val_loss: 15.4221 - val_mse: 15.4221 - val_mae: 1.5867 - lr: 2.0354e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 17/100\n",
            "1000/1000 - 2s - loss: 14.6926 - mse: 14.6926 - mae: 1.5739 - val_loss: 15.6043 - val_mse: 15.6043 - val_mae: 1.6003 - lr: 2.0354e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 18/100\n",
            "1000/1000 - 2s - loss: 14.6779 - mse: 14.6779 - mae: 1.5754 - val_loss: 15.5815 - val_mse: 15.5815 - val_mae: 1.5738 - lr: 2.0354e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 19/100\n",
            "1000/1000 - 2s - loss: 14.6805 - mse: 14.6805 - mae: 1.5734 - val_loss: 15.4835 - val_mse: 15.4835 - val_mae: 1.5593 - lr: 2.0354e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 20/100\n",
            "1000/1000 - 2s - loss: 14.6432 - mse: 14.6432 - mae: 1.5750 - val_loss: 15.3543 - val_mse: 15.3543 - val_mae: 1.5751 - lr: 2.0354e-04 - 2s/epoch - 2ms/step\n",
            "Score for fold 1: loss of 15.354324340820312\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 15.0581 - mse: 15.0581 - mae: 1.5775 - val_loss: 13.7709 - val_mse: 13.7709 - val_mae: 1.5303 - lr: 2.0354e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 15.0506 - mse: 15.0506 - mae: 1.5736 - val_loss: 13.6861 - val_mse: 13.6861 - val_mae: 1.5638 - lr: 2.0354e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 15.0332 - mse: 15.0332 - mae: 1.5758 - val_loss: 13.7054 - val_mse: 13.7054 - val_mae: 1.5775 - lr: 2.0354e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 15.0422 - mse: 15.0422 - mae: 1.5734 - val_loss: 13.7380 - val_mse: 13.7380 - val_mae: 1.5674 - lr: 2.0354e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 15.0002 - mse: 15.0002 - mae: 1.5729 - val_loss: 13.7199 - val_mse: 13.7199 - val_mae: 1.5716 - lr: 2.0354e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 14.9613 - mse: 14.9613 - mae: 1.5706 - val_loss: 13.7573 - val_mse: 13.7573 - val_mae: 1.5863 - lr: 2.0354e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 15.0082 - mse: 15.0082 - mae: 1.5681 - val_loss: 13.7645 - val_mse: 13.7645 - val_mae: 1.6178 - lr: 2.0354e-04 - 2s/epoch - 2ms/step\n",
            "Score for fold 2: loss of 13.76450252532959\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 13.5583 - mse: 13.5583 - mae: 1.5695 - val_loss: 19.4562 - val_mse: 19.4562 - val_mae: 1.5558 - lr: 2.0354e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 13.5396 - mse: 13.5396 - mae: 1.5695 - val_loss: 19.4575 - val_mse: 19.4575 - val_mae: 1.5917 - lr: 2.0354e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 13.5121 - mse: 13.5121 - mae: 1.5707 - val_loss: 19.5804 - val_mse: 19.5804 - val_mae: 1.5320 - lr: 2.0354e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 13.5335 - mse: 13.5335 - mae: 1.5666 - val_loss: 19.5067 - val_mse: 19.5067 - val_mae: 1.5334 - lr: 2.0354e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 13.4717 - mse: 13.4717 - mae: 1.5704 - val_loss: 19.4997 - val_mse: 19.4997 - val_mae: 1.5959 - lr: 2.0354e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 13.4970 - mse: 13.4970 - mae: 1.5677 - val_loss: 19.4974 - val_mse: 19.4974 - val_mae: 1.5916 - lr: 2.0354e-04 - 2s/epoch - 2ms/step\n",
            "Score for fold 3: loss of 19.497385025024414\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 15.1638 - mse: 15.1638 - mae: 1.5750 - val_loss: 12.9256 - val_mse: 12.9256 - val_mae: 1.5601 - lr: 2.0354e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 15.1580 - mse: 15.1580 - mae: 1.5738 - val_loss: 12.8914 - val_mse: 12.8914 - val_mae: 1.5183 - lr: 2.0354e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 15.1377 - mse: 15.1377 - mae: 1.5744 - val_loss: 12.9854 - val_mse: 12.9854 - val_mae: 1.5462 - lr: 2.0354e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 15.1583 - mse: 15.1583 - mae: 1.5685 - val_loss: 12.9725 - val_mse: 12.9725 - val_mae: 1.5597 - lr: 2.0354e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 15.1060 - mse: 15.1060 - mae: 1.5686 - val_loss: 12.8034 - val_mse: 12.8034 - val_mae: 1.5777 - lr: 2.0354e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 15.0728 - mse: 15.0728 - mae: 1.5714 - val_loss: 12.8066 - val_mse: 12.8066 - val_mae: 1.5663 - lr: 2.0354e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 15.0927 - mse: 15.0927 - mae: 1.5660 - val_loss: 12.9139 - val_mse: 12.9139 - val_mae: 1.6090 - lr: 2.0354e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 2s - loss: 15.0673 - mse: 15.0673 - mae: 1.5692 - val_loss: 12.8864 - val_mse: 12.8864 - val_mae: 1.5865 - lr: 2.0354e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 2s - loss: 15.0372 - mse: 15.0372 - mae: 1.5683 - val_loss: 13.1281 - val_mse: 13.1281 - val_mae: 1.5357 - lr: 2.0354e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 2s - loss: 15.0325 - mse: 15.0325 - mae: 1.5652 - val_loss: 13.0383 - val_mse: 13.0383 - val_mae: 1.5582 - lr: 2.0354e-04 - 2s/epoch - 2ms/step\n",
            "Score for fold 4: loss of 13.038331031799316\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 15.2460 - mse: 15.2460 - mae: 1.5584 - val_loss: 12.3588 - val_mse: 12.3588 - val_mae: 1.5411 - lr: 2.0354e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 15.2168 - mse: 15.2168 - mae: 1.5568 - val_loss: 12.3294 - val_mse: 12.3294 - val_mae: 1.5977 - lr: 2.0354e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 15.2279 - mse: 15.2279 - mae: 1.5552 - val_loss: 12.2836 - val_mse: 12.2836 - val_mae: 1.5797 - lr: 2.0354e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 15.1814 - mse: 15.1814 - mae: 1.5546 - val_loss: 12.2556 - val_mse: 12.2556 - val_mae: 1.6118 - lr: 2.0354e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 15.1807 - mse: 15.1807 - mae: 1.5560 - val_loss: 12.3378 - val_mse: 12.3378 - val_mae: 1.5933 - lr: 2.0354e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 15.1934 - mse: 15.1934 - mae: 1.5535 - val_loss: 12.3411 - val_mse: 12.3411 - val_mae: 1.6125 - lr: 2.0354e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 15.1333 - mse: 15.1333 - mae: 1.5529 - val_loss: 12.3468 - val_mse: 12.3468 - val_mae: 1.6650 - lr: 2.0354e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 2s - loss: 15.0781 - mse: 15.0781 - mae: 1.5515 - val_loss: 12.3958 - val_mse: 12.3958 - val_mae: 1.6214 - lr: 2.0354e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 2s - loss: 15.1319 - mse: 15.1319 - mae: 1.5535 - val_loss: 12.4569 - val_mse: 12.4569 - val_mae: 1.5731 - lr: 2.0354e-04 - 2s/epoch - 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-10-14 16:11:03,819]\u001b[0m Finished trial#35 resulted in value: 14.822. Current best value is 14.594 with parameters: {'activation': 'relu', 'num_hidden_layer': 4, 'i': 6, 'learning_rate': 0.000577682381030866}.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Score for fold 5: loss of 12.456880569458008\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 3s - loss: 15.6653 - mse: 15.6653 - mae: 1.6181 - val_loss: 14.7510 - val_mse: 14.7510 - val_mae: 1.6495 - lr: 0.0016 - 3s/epoch - 3ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 15.2025 - mse: 15.2025 - mae: 1.5939 - val_loss: 14.7817 - val_mse: 14.7817 - val_mae: 1.5512 - lr: 0.0016 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 15.1526 - mse: 15.1526 - mae: 1.5895 - val_loss: 15.0779 - val_mse: 15.0779 - val_mae: 1.5304 - lr: 0.0016 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 15.1360 - mse: 15.1360 - mae: 1.5892 - val_loss: 14.7386 - val_mse: 14.7386 - val_mae: 1.5941 - lr: 0.0016 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 15.1126 - mse: 15.1126 - mae: 1.5838 - val_loss: 14.7276 - val_mse: 14.7276 - val_mae: 1.5809 - lr: 0.0016 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 15.0464 - mse: 15.0464 - mae: 1.5835 - val_loss: 14.6948 - val_mse: 14.6948 - val_mae: 1.5913 - lr: 0.0016 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 15.0944 - mse: 15.0944 - mae: 1.5831 - val_loss: 14.7418 - val_mse: 14.7418 - val_mae: 1.5968 - lr: 0.0016 - 2s/epoch - 2ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 2s - loss: 15.0407 - mse: 15.0407 - mae: 1.5809 - val_loss: 14.6973 - val_mse: 14.6973 - val_mae: 1.5768 - lr: 0.0016 - 2s/epoch - 2ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 2s - loss: 15.0112 - mse: 15.0112 - mae: 1.5796 - val_loss: 14.6435 - val_mse: 14.6435 - val_mae: 1.6270 - lr: 0.0016 - 2s/epoch - 2ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 2s - loss: 15.0622 - mse: 15.0622 - mae: 1.5764 - val_loss: 14.6681 - val_mse: 14.6681 - val_mae: 1.6520 - lr: 0.0016 - 2s/epoch - 2ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 2s - loss: 14.9584 - mse: 14.9584 - mae: 1.5759 - val_loss: 14.6620 - val_mse: 14.6620 - val_mae: 1.6426 - lr: 0.0016 - 2s/epoch - 2ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 2s - loss: 14.9872 - mse: 14.9872 - mae: 1.5759 - val_loss: 14.7447 - val_mse: 14.7447 - val_mae: 1.6148 - lr: 0.0016 - 2s/epoch - 2ms/step\n",
            "Epoch 13/100\n",
            "1000/1000 - 2s - loss: 14.9691 - mse: 14.9691 - mae: 1.5755 - val_loss: 14.6394 - val_mse: 14.6394 - val_mae: 1.5993 - lr: 0.0016 - 2s/epoch - 2ms/step\n",
            "Epoch 14/100\n",
            "1000/1000 - 2s - loss: 14.9620 - mse: 14.9620 - mae: 1.5723 - val_loss: 14.7120 - val_mse: 14.7120 - val_mae: 1.6034 - lr: 0.0016 - 2s/epoch - 2ms/step\n",
            "Epoch 15/100\n",
            "1000/1000 - 2s - loss: 14.9774 - mse: 14.9774 - mae: 1.5738 - val_loss: 14.9291 - val_mse: 14.9291 - val_mae: 1.5462 - lr: 0.0016 - 2s/epoch - 2ms/step\n",
            "Epoch 16/100\n",
            "1000/1000 - 2s - loss: 14.9546 - mse: 14.9546 - mae: 1.5717 - val_loss: 14.7511 - val_mse: 14.7511 - val_mae: 1.6089 - lr: 0.0016 - 2s/epoch - 2ms/step\n",
            "Epoch 17/100\n",
            "1000/1000 - 2s - loss: 14.9905 - mse: 14.9905 - mae: 1.5692 - val_loss: 14.6237 - val_mse: 14.6237 - val_mae: 1.5750 - lr: 0.0016 - 2s/epoch - 2ms/step\n",
            "Epoch 18/100\n",
            "1000/1000 - 2s - loss: 14.9327 - mse: 14.9327 - mae: 1.5642 - val_loss: 14.6384 - val_mse: 14.6384 - val_mae: 1.6255 - lr: 0.0016 - 2s/epoch - 2ms/step\n",
            "Epoch 19/100\n",
            "1000/1000 - 2s - loss: 14.9333 - mse: 14.9333 - mae: 1.5700 - val_loss: 14.6764 - val_mse: 14.6764 - val_mae: 1.5923 - lr: 0.0016 - 2s/epoch - 2ms/step\n",
            "Epoch 20/100\n",
            "1000/1000 - 2s - loss: 14.9054 - mse: 14.9054 - mae: 1.5703 - val_loss: 14.5500 - val_mse: 14.5500 - val_mae: 1.6121 - lr: 0.0016 - 2s/epoch - 2ms/step\n",
            "Epoch 21/100\n",
            "1000/1000 - 2s - loss: 14.9339 - mse: 14.9339 - mae: 1.5734 - val_loss: 14.5475 - val_mse: 14.5475 - val_mae: 1.6237 - lr: 0.0016 - 2s/epoch - 2ms/step\n",
            "Epoch 22/100\n",
            "1000/1000 - 2s - loss: 14.8420 - mse: 14.8420 - mae: 1.5670 - val_loss: 14.6274 - val_mse: 14.6274 - val_mae: 1.5712 - lr: 0.0016 - 2s/epoch - 2ms/step\n",
            "Epoch 23/100\n",
            "1000/1000 - 2s - loss: 14.7638 - mse: 14.7638 - mae: 1.5697 - val_loss: 14.7761 - val_mse: 14.7761 - val_mae: 1.6278 - lr: 0.0016 - 2s/epoch - 2ms/step\n",
            "Epoch 24/100\n",
            "1000/1000 - 2s - loss: 14.9189 - mse: 14.9189 - mae: 1.5716 - val_loss: 14.7309 - val_mse: 14.7309 - val_mae: 1.6094 - lr: 0.0016 - 2s/epoch - 2ms/step\n",
            "Epoch 25/100\n",
            "1000/1000 - 2s - loss: 14.8054 - mse: 14.8054 - mae: 1.5666 - val_loss: 14.7052 - val_mse: 14.7052 - val_mae: 1.5879 - lr: 0.0016 - 2s/epoch - 2ms/step\n",
            "Epoch 26/100\n",
            "1000/1000 - 2s - loss: 14.9434 - mse: 14.9434 - mae: 1.5698 - val_loss: 14.8602 - val_mse: 14.8602 - val_mae: 1.6153 - lr: 0.0016 - 2s/epoch - 2ms/step\n",
            "Score for fold 1: loss of 14.86023235321045\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 12.9253 - mse: 12.9253 - mae: 1.5642 - val_loss: 22.2181 - val_mse: 22.2181 - val_mae: 1.5587 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 12.9169 - mse: 12.9169 - mae: 1.5599 - val_loss: 22.1424 - val_mse: 22.1424 - val_mae: 1.5916 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 12.8856 - mse: 12.8856 - mae: 1.5582 - val_loss: 22.3583 - val_mse: 22.3583 - val_mae: 1.5890 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 12.8791 - mse: 12.8791 - mae: 1.5583 - val_loss: 22.0479 - val_mse: 22.0479 - val_mae: 1.6152 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 12.8461 - mse: 12.8461 - mae: 1.5593 - val_loss: 22.1846 - val_mse: 22.1846 - val_mae: 1.6142 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 12.8128 - mse: 12.8128 - mae: 1.5594 - val_loss: 22.2150 - val_mse: 22.2150 - val_mae: 1.5750 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 12.8322 - mse: 12.8322 - mae: 1.5566 - val_loss: 22.1584 - val_mse: 22.1584 - val_mae: 1.6044 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 2s - loss: 12.7703 - mse: 12.7703 - mae: 1.5582 - val_loss: 22.0787 - val_mse: 22.0787 - val_mae: 1.5779 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 2s - loss: 12.7460 - mse: 12.7460 - mae: 1.5575 - val_loss: 22.1725 - val_mse: 22.1725 - val_mae: 1.6009 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Score for fold 2: loss of 22.172536849975586\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 15.3407 - mse: 15.3407 - mae: 1.5650 - val_loss: 12.1922 - val_mse: 12.1922 - val_mae: 1.5886 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 15.2683 - mse: 15.2683 - mae: 1.5635 - val_loss: 11.9806 - val_mse: 11.9806 - val_mae: 1.5944 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 15.2530 - mse: 15.2530 - mae: 1.5628 - val_loss: 11.8283 - val_mse: 11.8283 - val_mae: 1.5702 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 15.2762 - mse: 15.2762 - mae: 1.5623 - val_loss: 11.7984 - val_mse: 11.7984 - val_mae: 1.5948 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 15.1805 - mse: 15.1805 - mae: 1.5633 - val_loss: 11.7867 - val_mse: 11.7867 - val_mae: 1.5320 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 15.1625 - mse: 15.1625 - mae: 1.5625 - val_loss: 12.1044 - val_mse: 12.1044 - val_mae: 1.5228 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 15.1693 - mse: 15.1693 - mae: 1.5626 - val_loss: 12.1432 - val_mse: 12.1432 - val_mae: 1.5504 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 2s - loss: 15.1727 - mse: 15.1727 - mae: 1.5617 - val_loss: 11.9137 - val_mse: 11.9137 - val_mae: 1.5683 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 2s - loss: 15.1650 - mse: 15.1650 - mae: 1.5589 - val_loss: 11.9018 - val_mse: 11.9018 - val_mae: 1.5858 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 2s - loss: 15.1599 - mse: 15.1599 - mae: 1.5602 - val_loss: 11.8850 - val_mse: 11.8850 - val_mae: 1.5665 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Score for fold 3: loss of 11.884961128234863\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 15.2133 - mse: 15.2133 - mae: 1.5699 - val_loss: 11.9588 - val_mse: 11.9588 - val_mae: 1.4945 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 15.2578 - mse: 15.2578 - mae: 1.5712 - val_loss: 11.8534 - val_mse: 11.8534 - val_mae: 1.5187 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 15.1854 - mse: 15.1854 - mae: 1.5646 - val_loss: 11.7713 - val_mse: 11.7713 - val_mae: 1.5356 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 15.2403 - mse: 15.2403 - mae: 1.5720 - val_loss: 11.7887 - val_mse: 11.7887 - val_mae: 1.5144 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 15.1936 - mse: 15.1936 - mae: 1.5704 - val_loss: 11.8735 - val_mse: 11.8735 - val_mae: 1.5454 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 15.1514 - mse: 15.1514 - mae: 1.5679 - val_loss: 11.8972 - val_mse: 11.8972 - val_mae: 1.5284 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 15.1951 - mse: 15.1951 - mae: 1.5670 - val_loss: 11.9173 - val_mse: 11.9173 - val_mae: 1.5622 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 2s - loss: 15.1416 - mse: 15.1416 - mae: 1.5690 - val_loss: 11.8283 - val_mse: 11.8283 - val_mae: 1.5566 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Score for fold 4: loss of 11.828332901000977\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 14.9605 - mse: 14.9605 - mae: 1.5698 - val_loss: 12.6643 - val_mse: 12.6643 - val_mae: 1.5408 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 14.8868 - mse: 14.8868 - mae: 1.5678 - val_loss: 12.8263 - val_mse: 12.8263 - val_mae: 1.5000 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 14.8927 - mse: 14.8927 - mae: 1.5660 - val_loss: 12.7747 - val_mse: 12.7747 - val_mae: 1.5547 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 14.8440 - mse: 14.8440 - mae: 1.5617 - val_loss: 12.7782 - val_mse: 12.7782 - val_mae: 1.5283 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 14.7587 - mse: 14.7587 - mae: 1.5613 - val_loss: 12.8910 - val_mse: 12.8910 - val_mae: 1.5674 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 14.7608 - mse: 14.7608 - mae: 1.5658 - val_loss: 12.8602 - val_mse: 12.8602 - val_mae: 1.5882 - lr: 0.0010 - 2s/epoch - 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-10-14 16:13:04,274]\u001b[0m Finished trial#36 resulted in value: 14.719999999999999. Current best value is 14.594 with parameters: {'activation': 'relu', 'num_hidden_layer': 4, 'i': 6, 'learning_rate': 0.000577682381030866}.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Score for fold 5: loss of 12.860162734985352\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 3s - loss: 15.7064 - mse: 15.7064 - mae: 1.6237 - val_loss: 14.5317 - val_mse: 14.5317 - val_mae: 1.6549 - lr: 0.0017 - 3s/epoch - 3ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 15.2992 - mse: 15.2992 - mae: 1.6168 - val_loss: 14.5069 - val_mse: 14.5069 - val_mae: 1.6005 - lr: 0.0017 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 15.2809 - mse: 15.2809 - mae: 1.5999 - val_loss: 14.4556 - val_mse: 14.4556 - val_mae: 1.5765 - lr: 0.0017 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 15.1408 - mse: 15.1408 - mae: 1.5987 - val_loss: 14.6246 - val_mse: 14.6246 - val_mae: 1.5914 - lr: 0.0017 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 15.1982 - mse: 15.1982 - mae: 1.5981 - val_loss: 14.4696 - val_mse: 14.4696 - val_mae: 1.5490 - lr: 0.0017 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 15.1169 - mse: 15.1169 - mae: 1.5907 - val_loss: 14.4786 - val_mse: 14.4786 - val_mae: 1.5031 - lr: 0.0017 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 15.0833 - mse: 15.0833 - mae: 1.5913 - val_loss: 14.4238 - val_mse: 14.4238 - val_mae: 1.5730 - lr: 0.0017 - 2s/epoch - 2ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 2s - loss: 15.0441 - mse: 15.0441 - mae: 1.5896 - val_loss: 14.4088 - val_mse: 14.4088 - val_mae: 1.5478 - lr: 0.0017 - 2s/epoch - 2ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 2s - loss: 15.1263 - mse: 15.1263 - mae: 1.5905 - val_loss: 14.4208 - val_mse: 14.4208 - val_mae: 1.5068 - lr: 0.0017 - 2s/epoch - 2ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 2s - loss: 15.0760 - mse: 15.0760 - mae: 1.5900 - val_loss: 14.3718 - val_mse: 14.3718 - val_mae: 1.6395 - lr: 0.0017 - 2s/epoch - 2ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 2s - loss: 15.0261 - mse: 15.0261 - mae: 1.5928 - val_loss: 14.5856 - val_mse: 14.5856 - val_mae: 1.7034 - lr: 0.0017 - 2s/epoch - 2ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 2s - loss: 14.9590 - mse: 14.9590 - mae: 1.5941 - val_loss: 14.5578 - val_mse: 14.5578 - val_mae: 1.6634 - lr: 0.0017 - 2s/epoch - 2ms/step\n",
            "Epoch 13/100\n",
            "1000/1000 - 2s - loss: 15.0198 - mse: 15.0198 - mae: 1.5952 - val_loss: 14.7116 - val_mse: 14.7116 - val_mae: 1.7459 - lr: 0.0017 - 2s/epoch - 2ms/step\n",
            "Epoch 14/100\n",
            "1000/1000 - 2s - loss: 14.9159 - mse: 14.9159 - mae: 1.5912 - val_loss: 14.4674 - val_mse: 14.4674 - val_mae: 1.6429 - lr: 0.0017 - 2s/epoch - 2ms/step\n",
            "Epoch 15/100\n",
            "1000/1000 - 2s - loss: 15.0542 - mse: 15.0542 - mae: 1.5996 - val_loss: 14.5894 - val_mse: 14.5894 - val_mae: 1.5349 - lr: 0.0017 - 2s/epoch - 2ms/step\n",
            "Score for fold 1: loss of 14.589427947998047\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 15.0834 - mse: 15.0834 - mae: 1.5799 - val_loss: 13.9599 - val_mse: 13.9599 - val_mae: 1.5466 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 14.9015 - mse: 14.9015 - mae: 1.5831 - val_loss: 13.9778 - val_mse: 13.9778 - val_mae: 1.5611 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 15.0417 - mse: 15.0417 - mae: 1.5833 - val_loss: 13.4801 - val_mse: 13.4801 - val_mae: 1.5951 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 15.0231 - mse: 15.0231 - mae: 1.5838 - val_loss: 14.0673 - val_mse: 14.0673 - val_mae: 1.5587 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 15.0073 - mse: 15.0073 - mae: 1.5806 - val_loss: 14.2016 - val_mse: 14.2016 - val_mae: 1.5115 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 14.9731 - mse: 14.9731 - mae: 1.5797 - val_loss: 13.9705 - val_mse: 13.9705 - val_mae: 1.5404 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 14.9583 - mse: 14.9583 - mae: 1.5849 - val_loss: 14.0914 - val_mse: 14.0914 - val_mae: 1.5724 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 2s - loss: 14.9583 - mse: 14.9583 - mae: 1.5827 - val_loss: 13.8653 - val_mse: 13.8653 - val_mae: 1.5782 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Score for fold 2: loss of 13.86535930633545\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 15.2944 - mse: 15.2944 - mae: 1.5904 - val_loss: 12.8457 - val_mse: 12.8457 - val_mae: 1.5824 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 15.2169 - mse: 15.2169 - mae: 1.5798 - val_loss: 12.7217 - val_mse: 12.7217 - val_mae: 1.5697 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 15.1951 - mse: 15.1951 - mae: 1.5836 - val_loss: 13.2589 - val_mse: 13.2589 - val_mae: 1.5968 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 15.1946 - mse: 15.1946 - mae: 1.5735 - val_loss: 12.7296 - val_mse: 12.7296 - val_mae: 1.5716 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 15.1496 - mse: 15.1496 - mae: 1.5741 - val_loss: 12.6713 - val_mse: 12.6713 - val_mae: 1.6655 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 15.0966 - mse: 15.0966 - mae: 1.5744 - val_loss: 12.9839 - val_mse: 12.9839 - val_mae: 1.4703 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 15.0617 - mse: 15.0617 - mae: 1.5771 - val_loss: 13.0844 - val_mse: 13.0844 - val_mae: 1.5599 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 2s - loss: 15.1090 - mse: 15.1090 - mae: 1.5753 - val_loss: 12.9604 - val_mse: 12.9604 - val_mae: 1.4921 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 2s - loss: 15.0619 - mse: 15.0619 - mae: 1.5704 - val_loss: 12.7596 - val_mse: 12.7596 - val_mae: 1.5450 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 2s - loss: 15.0533 - mse: 15.0533 - mae: 1.5722 - val_loss: 12.8632 - val_mse: 12.8632 - val_mae: 1.5009 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Score for fold 3: loss of 12.863158226013184\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 15.1483 - mse: 15.1483 - mae: 1.5735 - val_loss: 12.1574 - val_mse: 12.1574 - val_mae: 1.7261 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 14.8847 - mse: 14.8847 - mae: 1.5739 - val_loss: 12.5124 - val_mse: 12.5124 - val_mae: 1.5484 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 15.0091 - mse: 15.0091 - mae: 1.5711 - val_loss: 12.5507 - val_mse: 12.5507 - val_mae: 1.5929 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 14.8484 - mse: 14.8484 - mae: 1.5651 - val_loss: 12.3505 - val_mse: 12.3505 - val_mae: 1.5891 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 14.8189 - mse: 14.8189 - mae: 1.5713 - val_loss: 12.2862 - val_mse: 12.2862 - val_mae: 1.5113 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 14.9415 - mse: 14.9415 - mae: 1.5732 - val_loss: 12.5779 - val_mse: 12.5779 - val_mae: 1.6312 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Score for fold 4: loss of 12.57788372039795\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 12.8748 - mse: 12.8748 - mae: 1.5581 - val_loss: 20.2801 - val_mse: 20.2801 - val_mae: 1.6168 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 13.0005 - mse: 13.0005 - mae: 1.5536 - val_loss: 20.5883 - val_mse: 20.5883 - val_mae: 1.5597 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 12.8885 - mse: 12.8885 - mae: 1.5608 - val_loss: 20.5181 - val_mse: 20.5181 - val_mae: 1.6709 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 12.9364 - mse: 12.9364 - mae: 1.5619 - val_loss: 20.6229 - val_mse: 20.6229 - val_mae: 1.6186 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 13.0186 - mse: 13.0186 - mae: 1.5586 - val_loss: 20.5195 - val_mse: 20.5195 - val_mae: 1.6215 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 12.9406 - mse: 12.9406 - mae: 1.5624 - val_loss: 20.6804 - val_mse: 20.6804 - val_mae: 1.5600 - lr: 0.0010 - 2s/epoch - 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-10-14 16:14:41,742]\u001b[0m Finished trial#37 resulted in value: 14.916. Current best value is 14.594 with parameters: {'activation': 'relu', 'num_hidden_layer': 4, 'i': 6, 'learning_rate': 0.000577682381030866}.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Score for fold 5: loss of 20.680452346801758\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 3s - loss: 15.5132 - mse: 15.5132 - mae: 1.6346 - val_loss: 15.9529 - val_mse: 15.9529 - val_mae: 1.5598 - lr: 6.4085e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 14.9363 - mse: 14.9363 - mae: 1.6007 - val_loss: 15.7214 - val_mse: 15.7214 - val_mae: 1.5995 - lr: 6.4085e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 14.8517 - mse: 14.8517 - mae: 1.5946 - val_loss: 16.0918 - val_mse: 16.0918 - val_mae: 1.5467 - lr: 6.4085e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 14.8220 - mse: 14.8220 - mae: 1.5918 - val_loss: 16.0083 - val_mse: 16.0083 - val_mae: 1.5383 - lr: 6.4085e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 14.7744 - mse: 14.7744 - mae: 1.5870 - val_loss: 15.6655 - val_mse: 15.6655 - val_mae: 1.5826 - lr: 6.4085e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 14.7561 - mse: 14.7561 - mae: 1.5871 - val_loss: 15.8608 - val_mse: 15.8608 - val_mae: 1.5411 - lr: 6.4085e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 14.7466 - mse: 14.7466 - mae: 1.5825 - val_loss: 15.8478 - val_mse: 15.8478 - val_mae: 1.5542 - lr: 6.4085e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 2s - loss: 14.7019 - mse: 14.7019 - mae: 1.5810 - val_loss: 15.9001 - val_mse: 15.9001 - val_mae: 1.5536 - lr: 6.4085e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 2s - loss: 14.6817 - mse: 14.6817 - mae: 1.5782 - val_loss: 15.8856 - val_mse: 15.8856 - val_mae: 1.5682 - lr: 6.4085e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 2s - loss: 14.6819 - mse: 14.6819 - mae: 1.5789 - val_loss: 16.2125 - val_mse: 16.2125 - val_mae: 1.6048 - lr: 6.4085e-04 - 2s/epoch - 2ms/step\n",
            "Score for fold 1: loss of 16.212493896484375\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 11.9628 - mse: 11.9628 - mae: 1.5573 - val_loss: 26.7842 - val_mse: 26.7842 - val_mae: 1.6668 - lr: 6.4085e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 11.9545 - mse: 11.9545 - mae: 1.5556 - val_loss: 26.8753 - val_mse: 26.8753 - val_mae: 1.7247 - lr: 6.4085e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 11.8614 - mse: 11.8614 - mae: 1.5563 - val_loss: 26.6868 - val_mse: 26.6868 - val_mae: 1.6797 - lr: 6.4085e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 11.8013 - mse: 11.8013 - mae: 1.5528 - val_loss: 26.9183 - val_mse: 26.9183 - val_mae: 1.6456 - lr: 6.4085e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 11.9093 - mse: 11.9093 - mae: 1.5558 - val_loss: 26.7443 - val_mse: 26.7443 - val_mae: 1.7222 - lr: 6.4085e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 11.7956 - mse: 11.7956 - mae: 1.5573 - val_loss: 26.6864 - val_mse: 26.6864 - val_mae: 1.6382 - lr: 6.4085e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 11.8291 - mse: 11.8291 - mae: 1.5497 - val_loss: 26.7276 - val_mse: 26.7276 - val_mae: 1.6892 - lr: 6.4085e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 2s - loss: 11.7047 - mse: 11.7047 - mae: 1.5540 - val_loss: 26.7994 - val_mse: 26.7994 - val_mae: 1.6510 - lr: 6.4085e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 2s - loss: 11.8217 - mse: 11.8217 - mae: 1.5535 - val_loss: 26.8000 - val_mse: 26.8000 - val_mae: 1.6552 - lr: 6.4085e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 2s - loss: 11.7459 - mse: 11.7459 - mae: 1.5588 - val_loss: 26.6649 - val_mse: 26.6649 - val_mae: 1.6713 - lr: 6.4085e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 2s - loss: 11.7559 - mse: 11.7559 - mae: 1.5486 - val_loss: 26.7287 - val_mse: 26.7287 - val_mae: 1.7263 - lr: 6.4085e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 2s - loss: 11.6893 - mse: 11.6893 - mae: 1.5498 - val_loss: 26.8224 - val_mse: 26.8224 - val_mae: 1.6623 - lr: 6.4085e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 13/100\n",
            "1000/1000 - 2s - loss: 11.8609 - mse: 11.8609 - mae: 1.5511 - val_loss: 26.9150 - val_mse: 26.9150 - val_mae: 1.6159 - lr: 6.4085e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 14/100\n",
            "1000/1000 - 2s - loss: 11.6873 - mse: 11.6873 - mae: 1.5520 - val_loss: 27.2795 - val_mse: 27.2795 - val_mae: 1.5864 - lr: 6.4085e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 15/100\n",
            "1000/1000 - 2s - loss: 11.7758 - mse: 11.7758 - mae: 1.5499 - val_loss: 26.7006 - val_mse: 26.7006 - val_mae: 1.6850 - lr: 6.4085e-04 - 2s/epoch - 2ms/step\n",
            "Score for fold 2: loss of 26.700637817382812\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 15.6646 - mse: 15.6646 - mae: 1.5768 - val_loss: 11.1739 - val_mse: 11.1739 - val_mae: 1.7638 - lr: 6.4085e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 15.5765 - mse: 15.5765 - mae: 1.5673 - val_loss: 11.6525 - val_mse: 11.6525 - val_mae: 1.5276 - lr: 6.4085e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 15.6031 - mse: 15.6031 - mae: 1.5749 - val_loss: 11.0875 - val_mse: 11.0875 - val_mae: 1.5948 - lr: 6.4085e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 15.5328 - mse: 15.5328 - mae: 1.5680 - val_loss: 11.5570 - val_mse: 11.5570 - val_mae: 1.6771 - lr: 6.4085e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 15.5572 - mse: 15.5572 - mae: 1.5692 - val_loss: 11.4661 - val_mse: 11.4661 - val_mae: 1.7041 - lr: 6.4085e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 15.3262 - mse: 15.3262 - mae: 1.5677 - val_loss: 11.0986 - val_mse: 11.0986 - val_mae: 1.5181 - lr: 6.4085e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 15.5410 - mse: 15.5410 - mae: 1.5618 - val_loss: 11.2497 - val_mse: 11.2497 - val_mae: 1.6795 - lr: 6.4085e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 2s - loss: 15.4626 - mse: 15.4626 - mae: 1.5596 - val_loss: 11.2980 - val_mse: 11.2980 - val_mae: 1.5280 - lr: 6.4085e-04 - 2s/epoch - 2ms/step\n",
            "Score for fold 3: loss of 11.297957420349121\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 15.6321 - mse: 15.6321 - mae: 1.5840 - val_loss: 10.9237 - val_mse: 10.9237 - val_mae: 1.5027 - lr: 6.4085e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 15.5689 - mse: 15.5689 - mae: 1.5778 - val_loss: 11.0230 - val_mse: 11.0230 - val_mae: 1.5649 - lr: 6.4085e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 15.3748 - mse: 15.3748 - mae: 1.5754 - val_loss: 11.1181 - val_mse: 11.1181 - val_mae: 1.6992 - lr: 6.4085e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 15.4196 - mse: 15.4196 - mae: 1.5715 - val_loss: 11.2323 - val_mse: 11.2323 - val_mae: 1.6501 - lr: 6.4085e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 15.4592 - mse: 15.4592 - mae: 1.5727 - val_loss: 10.8579 - val_mse: 10.8579 - val_mae: 1.5498 - lr: 6.4085e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 15.3636 - mse: 15.3636 - mae: 1.5706 - val_loss: 11.0923 - val_mse: 11.0923 - val_mae: 1.5380 - lr: 6.4085e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 15.4665 - mse: 15.4665 - mae: 1.5755 - val_loss: 11.1175 - val_mse: 11.1175 - val_mae: 1.4795 - lr: 6.4085e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 2s - loss: 15.4534 - mse: 15.4534 - mae: 1.5728 - val_loss: 11.0429 - val_mse: 11.0429 - val_mae: 1.5925 - lr: 6.4085e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 2s - loss: 15.3704 - mse: 15.3704 - mae: 1.5729 - val_loss: 11.3260 - val_mse: 11.3260 - val_mae: 1.4749 - lr: 6.4085e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 2s - loss: 15.2627 - mse: 15.2627 - mae: 1.5672 - val_loss: 11.0460 - val_mse: 11.0460 - val_mae: 1.5317 - lr: 6.4085e-04 - 2s/epoch - 2ms/step\n",
            "Score for fold 4: loss of 11.04600715637207\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 15.8766 - mse: 15.8766 - mae: 1.5907 - val_loss: 8.9646 - val_mse: 8.9646 - val_mae: 1.6390 - lr: 6.4085e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 15.8656 - mse: 15.8656 - mae: 1.5831 - val_loss: 9.0248 - val_mse: 9.0248 - val_mae: 1.4610 - lr: 6.4085e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 15.8216 - mse: 15.8216 - mae: 1.5820 - val_loss: 8.9761 - val_mse: 8.9761 - val_mae: 1.4466 - lr: 6.4085e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 15.7447 - mse: 15.7447 - mae: 1.5819 - val_loss: 9.2668 - val_mse: 9.2668 - val_mae: 1.6574 - lr: 6.4085e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 15.6460 - mse: 15.6460 - mae: 1.5781 - val_loss: 9.0375 - val_mse: 9.0375 - val_mae: 1.4896 - lr: 6.4085e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 15.6982 - mse: 15.6982 - mae: 1.5776 - val_loss: 9.2987 - val_mse: 9.2987 - val_mae: 1.5775 - lr: 6.4085e-04 - 2s/epoch - 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-10-14 16:16:27,293]\u001b[0m Finished trial#38 resulted in value: 14.911999999999997. Current best value is 14.594 with parameters: {'activation': 'relu', 'num_hidden_layer': 4, 'i': 6, 'learning_rate': 0.000577682381030866}.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Score for fold 5: loss of 9.2987060546875\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 3s - loss: 15.9912 - mse: 15.9912 - mae: 1.6819 - val_loss: 15.7139 - val_mse: 15.7139 - val_mae: 1.6265 - lr: 0.0015 - 3s/epoch - 3ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 15.7481 - mse: 15.7481 - mae: 1.6587 - val_loss: 15.6963 - val_mse: 15.6963 - val_mae: 1.6642 - lr: 0.0015 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 15.7141 - mse: 15.7141 - mae: 1.6513 - val_loss: 15.6427 - val_mse: 15.6427 - val_mae: 1.6090 - lr: 0.0015 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 15.6948 - mse: 15.6948 - mae: 1.6491 - val_loss: 15.6174 - val_mse: 15.6174 - val_mae: 1.6110 - lr: 0.0015 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 15.6778 - mse: 15.6778 - mae: 1.6473 - val_loss: 15.6827 - val_mse: 15.6827 - val_mae: 1.6650 - lr: 0.0015 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 15.7146 - mse: 15.7146 - mae: 1.6451 - val_loss: 15.6024 - val_mse: 15.6024 - val_mae: 1.5951 - lr: 0.0015 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 15.6861 - mse: 15.6861 - mae: 1.6481 - val_loss: 15.6306 - val_mse: 15.6306 - val_mae: 1.6011 - lr: 0.0015 - 2s/epoch - 2ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 2s - loss: 15.7106 - mse: 15.7106 - mae: 1.6431 - val_loss: 15.6082 - val_mse: 15.6082 - val_mae: 1.6601 - lr: 0.0015 - 2s/epoch - 2ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 2s - loss: 15.6634 - mse: 15.6634 - mae: 1.6501 - val_loss: 15.5800 - val_mse: 15.5800 - val_mae: 1.6073 - lr: 0.0015 - 2s/epoch - 2ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 2s - loss: 15.6725 - mse: 15.6725 - mae: 1.6400 - val_loss: 15.5955 - val_mse: 15.5955 - val_mae: 1.6038 - lr: 0.0015 - 2s/epoch - 2ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 2s - loss: 15.6608 - mse: 15.6608 - mae: 1.6467 - val_loss: 15.6243 - val_mse: 15.6243 - val_mae: 1.6823 - lr: 0.0015 - 2s/epoch - 2ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 2s - loss: 15.6680 - mse: 15.6680 - mae: 1.6473 - val_loss: 15.5860 - val_mse: 15.5860 - val_mae: 1.6169 - lr: 0.0015 - 2s/epoch - 2ms/step\n",
            "Epoch 13/100\n",
            "1000/1000 - 2s - loss: 15.6779 - mse: 15.6779 - mae: 1.6535 - val_loss: 15.7188 - val_mse: 15.7188 - val_mae: 1.7643 - lr: 0.0015 - 2s/epoch - 2ms/step\n",
            "Epoch 14/100\n",
            "1000/1000 - 2s - loss: 15.7317 - mse: 15.7317 - mae: 1.6507 - val_loss: 15.6491 - val_mse: 15.6491 - val_mae: 1.5730 - lr: 0.0015 - 2s/epoch - 2ms/step\n",
            "Score for fold 1: loss of 15.649113655090332\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 16.0968 - mse: 16.0968 - mae: 1.6503 - val_loss: 14.2129 - val_mse: 14.2129 - val_mae: 1.5380 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 16.1056 - mse: 16.1056 - mae: 1.6404 - val_loss: 13.8586 - val_mse: 13.8586 - val_mae: 1.7400 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 16.1080 - mse: 16.1080 - mae: 1.6428 - val_loss: 13.8585 - val_mse: 13.8585 - val_mae: 1.6298 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 16.1409 - mse: 16.1409 - mae: 1.6399 - val_loss: 13.8349 - val_mse: 13.8349 - val_mae: 1.7327 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 16.1469 - mse: 16.1469 - mae: 1.6486 - val_loss: 13.8895 - val_mse: 13.8895 - val_mae: 1.7806 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 16.1599 - mse: 16.1599 - mae: 1.6373 - val_loss: 14.5599 - val_mse: 14.5599 - val_mae: 2.0277 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 16.2460 - mse: 16.2460 - mae: 1.6512 - val_loss: 13.9262 - val_mse: 13.9262 - val_mae: 1.6005 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 2s - loss: 16.1403 - mse: 16.1403 - mae: 1.6480 - val_loss: 13.8088 - val_mse: 13.8088 - val_mae: 1.6884 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 2s - loss: 16.1831 - mse: 16.1831 - mae: 1.6397 - val_loss: 13.8497 - val_mse: 13.8497 - val_mae: 1.6388 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 2s - loss: 16.2205 - mse: 16.2205 - mae: 1.6509 - val_loss: 13.9411 - val_mse: 13.9411 - val_mae: 1.8078 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 2s - loss: 16.2189 - mse: 16.2189 - mae: 1.6429 - val_loss: 14.0017 - val_mse: 14.0017 - val_mae: 1.5862 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 2s - loss: 16.1994 - mse: 16.1994 - mae: 1.6573 - val_loss: 13.8120 - val_mse: 13.8120 - val_mae: 1.6576 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 13/100\n",
            "1000/1000 - 2s - loss: 16.1715 - mse: 16.1715 - mae: 1.6427 - val_loss: 13.8740 - val_mse: 13.8740 - val_mae: 1.7561 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Score for fold 2: loss of 13.873963356018066\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 16.5285 - mse: 16.5285 - mae: 1.6419 - val_loss: 12.4279 - val_mse: 12.4279 - val_mae: 1.7495 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 16.5094 - mse: 16.5094 - mae: 1.6566 - val_loss: 12.3498 - val_mse: 12.3498 - val_mae: 1.6637 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 16.5909 - mse: 16.5909 - mae: 1.6408 - val_loss: 12.3551 - val_mse: 12.3551 - val_mae: 1.6213 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 16.5354 - mse: 16.5354 - mae: 1.6384 - val_loss: 12.3604 - val_mse: 12.3604 - val_mae: 1.6820 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 16.5458 - mse: 16.5458 - mae: 1.6476 - val_loss: 12.5180 - val_mse: 12.5180 - val_mae: 1.7606 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 16.5437 - mse: 16.5437 - mae: 1.6434 - val_loss: 12.6841 - val_mse: 12.6841 - val_mae: 1.5489 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 16.4856 - mse: 16.4856 - mae: 1.6478 - val_loss: 12.3742 - val_mse: 12.3742 - val_mae: 1.6212 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Score for fold 3: loss of 12.374207496643066\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 16.3476 - mse: 16.3476 - mae: 1.6519 - val_loss: 13.2611 - val_mse: 13.2611 - val_mae: 1.5699 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 16.3392 - mse: 16.3392 - mae: 1.6510 - val_loss: 13.2007 - val_mse: 13.2007 - val_mae: 1.6048 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 16.2777 - mse: 16.2777 - mae: 1.6533 - val_loss: 13.1739 - val_mse: 13.1739 - val_mae: 1.6408 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 16.4257 - mse: 16.4257 - mae: 1.6506 - val_loss: 13.1711 - val_mse: 13.1711 - val_mae: 1.6411 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 16.4098 - mse: 16.4098 - mae: 1.6605 - val_loss: 13.6101 - val_mse: 13.6101 - val_mae: 1.5234 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 16.4091 - mse: 16.4091 - mae: 1.6545 - val_loss: 13.1722 - val_mse: 13.1722 - val_mae: 1.6263 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 16.3663 - mse: 16.3663 - mae: 1.6496 - val_loss: 13.2959 - val_mse: 13.2959 - val_mae: 1.5587 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 2s - loss: 16.3477 - mse: 16.3477 - mae: 1.6538 - val_loss: 13.3733 - val_mse: 13.3733 - val_mae: 1.5576 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 2s - loss: 16.2450 - mse: 16.2450 - mae: 1.6538 - val_loss: 13.2036 - val_mse: 13.2036 - val_mae: 1.6626 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Score for fold 4: loss of 13.203570365905762\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 13.8354 - mse: 13.8354 - mae: 1.6244 - val_loss: 23.1988 - val_mse: 23.1988 - val_mae: 1.6498 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 13.8670 - mse: 13.8670 - mae: 1.6351 - val_loss: 23.0032 - val_mse: 23.0032 - val_mae: 1.6832 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 13.8373 - mse: 13.8373 - mae: 1.6297 - val_loss: 23.1191 - val_mse: 23.1191 - val_mae: 1.6533 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 13.8763 - mse: 13.8763 - mae: 1.6268 - val_loss: 22.9643 - val_mse: 22.9643 - val_mae: 1.7159 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 13.8615 - mse: 13.8615 - mae: 1.6347 - val_loss: 22.9091 - val_mse: 22.9091 - val_mae: 1.7906 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 13.8558 - mse: 13.8558 - mae: 1.6280 - val_loss: 23.0164 - val_mse: 23.0164 - val_mae: 1.8388 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 13.8336 - mse: 13.8336 - mae: 1.6321 - val_loss: 23.1220 - val_mse: 23.1220 - val_mae: 1.6617 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 2s - loss: 13.9351 - mse: 13.9351 - mae: 1.6288 - val_loss: 22.9259 - val_mse: 22.9259 - val_mae: 1.7260 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 2s - loss: 13.9290 - mse: 13.9290 - mae: 1.6345 - val_loss: 23.2657 - val_mse: 23.2657 - val_mae: 1.6327 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 2s - loss: 13.8863 - mse: 13.8863 - mae: 1.6247 - val_loss: 23.5243 - val_mse: 23.5243 - val_mae: 1.6370 - lr: 0.0010 - 2s/epoch - 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-10-14 16:18:26,412]\u001b[0m Finished trial#39 resulted in value: 15.722. Current best value is 14.594 with parameters: {'activation': 'relu', 'num_hidden_layer': 4, 'i': 6, 'learning_rate': 0.000577682381030866}.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Score for fold 5: loss of 23.52432632446289\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 3s - loss: 15.8221 - mse: 15.8221 - mae: 1.6316 - val_loss: 14.1047 - val_mse: 14.1047 - val_mae: 1.5575 - lr: 0.0012 - 3s/epoch - 3ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 15.3796 - mse: 15.3796 - mae: 1.6051 - val_loss: 13.8761 - val_mse: 13.8761 - val_mae: 1.6109 - lr: 0.0012 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 15.3421 - mse: 15.3421 - mae: 1.6041 - val_loss: 13.8135 - val_mse: 13.8135 - val_mae: 1.6034 - lr: 0.0012 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 15.2830 - mse: 15.2830 - mae: 1.5966 - val_loss: 13.8786 - val_mse: 13.8786 - val_mae: 1.5344 - lr: 0.0012 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 15.2634 - mse: 15.2634 - mae: 1.5984 - val_loss: 13.7958 - val_mse: 13.7958 - val_mae: 1.6147 - lr: 0.0012 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 15.2031 - mse: 15.2031 - mae: 1.5927 - val_loss: 13.7587 - val_mse: 13.7587 - val_mae: 1.6178 - lr: 0.0012 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 15.1998 - mse: 15.1998 - mae: 1.5923 - val_loss: 13.7415 - val_mse: 13.7415 - val_mae: 1.5535 - lr: 0.0012 - 2s/epoch - 2ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 2s - loss: 15.2039 - mse: 15.2039 - mae: 1.5912 - val_loss: 13.7781 - val_mse: 13.7781 - val_mae: 1.5690 - lr: 0.0012 - 2s/epoch - 2ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 2s - loss: 15.1653 - mse: 15.1653 - mae: 1.5893 - val_loss: 13.9083 - val_mse: 13.9083 - val_mae: 1.5549 - lr: 0.0012 - 2s/epoch - 2ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 2s - loss: 15.1730 - mse: 15.1730 - mae: 1.5885 - val_loss: 13.9182 - val_mse: 13.9182 - val_mae: 1.5793 - lr: 0.0012 - 2s/epoch - 2ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 2s - loss: 15.1735 - mse: 15.1735 - mae: 1.5874 - val_loss: 13.8967 - val_mse: 13.8967 - val_mae: 1.5906 - lr: 0.0012 - 2s/epoch - 2ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 2s - loss: 15.1752 - mse: 15.1752 - mae: 1.5887 - val_loss: 13.9209 - val_mse: 13.9209 - val_mae: 1.5527 - lr: 0.0012 - 2s/epoch - 2ms/step\n",
            "Score for fold 1: loss of 13.920891761779785\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 15.6000 - mse: 15.6000 - mae: 1.5911 - val_loss: 11.7475 - val_mse: 11.7475 - val_mae: 1.5437 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 15.6321 - mse: 15.6321 - mae: 1.5858 - val_loss: 11.7374 - val_mse: 11.7374 - val_mae: 1.5595 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 15.5585 - mse: 15.5585 - mae: 1.5859 - val_loss: 11.7342 - val_mse: 11.7342 - val_mae: 1.5734 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 15.5921 - mse: 15.5921 - mae: 1.5831 - val_loss: 11.9339 - val_mse: 11.9339 - val_mae: 1.5206 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 15.6278 - mse: 15.6278 - mae: 1.5838 - val_loss: 11.9279 - val_mse: 11.9279 - val_mae: 1.4874 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 15.6313 - mse: 15.6313 - mae: 1.5787 - val_loss: 11.7634 - val_mse: 11.7634 - val_mae: 1.5143 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 15.5172 - mse: 15.5172 - mae: 1.5791 - val_loss: 11.7312 - val_mse: 11.7312 - val_mae: 1.5625 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 2s - loss: 15.5646 - mse: 15.5646 - mae: 1.5773 - val_loss: 11.8002 - val_mse: 11.8002 - val_mae: 1.5269 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 2s - loss: 15.6238 - mse: 15.6238 - mae: 1.5754 - val_loss: 11.8697 - val_mse: 11.8697 - val_mae: 1.5340 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 2s - loss: 15.5625 - mse: 15.5625 - mae: 1.5775 - val_loss: 11.8148 - val_mse: 11.8148 - val_mae: 1.5351 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 2s - loss: 15.4703 - mse: 15.4703 - mae: 1.5719 - val_loss: 11.7449 - val_mse: 11.7449 - val_mae: 1.5735 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 2s - loss: 15.5762 - mse: 15.5762 - mae: 1.5699 - val_loss: 11.7623 - val_mse: 11.7623 - val_mae: 1.5192 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Score for fold 2: loss of 11.762332916259766\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 15.5237 - mse: 15.5237 - mae: 1.5584 - val_loss: 12.0091 - val_mse: 12.0091 - val_mae: 1.5637 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 15.4052 - mse: 15.4052 - mae: 1.5612 - val_loss: 11.8406 - val_mse: 11.8406 - val_mae: 1.6054 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 15.4345 - mse: 15.4345 - mae: 1.5654 - val_loss: 11.8852 - val_mse: 11.8852 - val_mae: 1.6029 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 15.4134 - mse: 15.4134 - mae: 1.5625 - val_loss: 12.0842 - val_mse: 12.0842 - val_mae: 1.5840 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 15.4517 - mse: 15.4517 - mae: 1.5573 - val_loss: 11.9141 - val_mse: 11.9141 - val_mae: 1.6020 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 15.4484 - mse: 15.4484 - mae: 1.5571 - val_loss: 12.1738 - val_mse: 12.1738 - val_mae: 1.6107 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 15.4210 - mse: 15.4210 - mae: 1.5588 - val_loss: 11.9995 - val_mse: 11.9995 - val_mae: 1.5635 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Score for fold 3: loss of 11.999481201171875\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 14.9355 - mse: 14.9355 - mae: 1.5588 - val_loss: 14.5424 - val_mse: 14.5424 - val_mae: 1.5384 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 14.8613 - mse: 14.8613 - mae: 1.5586 - val_loss: 14.2926 - val_mse: 14.2926 - val_mae: 1.6186 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 14.7467 - mse: 14.7467 - mae: 1.5566 - val_loss: 14.5600 - val_mse: 14.5600 - val_mae: 1.5810 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 14.8550 - mse: 14.8550 - mae: 1.5579 - val_loss: 14.1649 - val_mse: 14.1649 - val_mae: 1.5981 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 14.7372 - mse: 14.7372 - mae: 1.5541 - val_loss: 14.3614 - val_mse: 14.3614 - val_mae: 1.5742 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 14.7760 - mse: 14.7760 - mae: 1.5587 - val_loss: 14.1555 - val_mse: 14.1555 - val_mae: 1.5966 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 14.8257 - mse: 14.8257 - mae: 1.5575 - val_loss: 14.4028 - val_mse: 14.4028 - val_mae: 1.5834 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 2s - loss: 14.7285 - mse: 14.7285 - mae: 1.5542 - val_loss: 14.5846 - val_mse: 14.5846 - val_mae: 1.5390 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 2s - loss: 14.7339 - mse: 14.7339 - mae: 1.5562 - val_loss: 14.1189 - val_mse: 14.1189 - val_mae: 1.5985 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 2s - loss: 14.7282 - mse: 14.7282 - mae: 1.5533 - val_loss: 14.5425 - val_mse: 14.5425 - val_mae: 1.5905 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 2s - loss: 14.6933 - mse: 14.6933 - mae: 1.5518 - val_loss: 14.2598 - val_mse: 14.2598 - val_mae: 1.5515 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 2s - loss: 14.7645 - mse: 14.7645 - mae: 1.5582 - val_loss: 14.3944 - val_mse: 14.3944 - val_mae: 1.5464 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 13/100\n",
            "1000/1000 - 2s - loss: 14.7027 - mse: 14.7027 - mae: 1.5506 - val_loss: 14.3092 - val_mse: 14.3092 - val_mae: 1.5948 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 14/100\n",
            "1000/1000 - 2s - loss: 14.6878 - mse: 14.6878 - mae: 1.5498 - val_loss: 14.2633 - val_mse: 14.2633 - val_mae: 1.6031 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Score for fold 4: loss of 14.263257026672363\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 12.7309 - mse: 12.7309 - mae: 1.5545 - val_loss: 22.1154 - val_mse: 22.1154 - val_mae: 1.5886 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 12.6686 - mse: 12.6686 - mae: 1.5528 - val_loss: 22.2944 - val_mse: 22.2944 - val_mae: 1.5356 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 12.7146 - mse: 12.7146 - mae: 1.5524 - val_loss: 22.2636 - val_mse: 22.2636 - val_mae: 1.5910 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 12.6230 - mse: 12.6230 - mae: 1.5569 - val_loss: 22.2200 - val_mse: 22.2200 - val_mae: 1.5883 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 12.6375 - mse: 12.6375 - mae: 1.5503 - val_loss: 22.3576 - val_mse: 22.3576 - val_mae: 1.5622 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 12.5671 - mse: 12.5671 - mae: 1.5495 - val_loss: 22.1469 - val_mse: 22.1469 - val_mae: 1.6201 - lr: 0.0010 - 2s/epoch - 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-10-14 16:20:10,366]\u001b[0m Finished trial#40 resulted in value: 14.818000000000001. Current best value is 14.594 with parameters: {'activation': 'relu', 'num_hidden_layer': 4, 'i': 6, 'learning_rate': 0.000577682381030866}.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Score for fold 5: loss of 22.14687156677246\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 4s - loss: 16.1873 - mse: 16.1873 - mae: 1.6436 - val_loss: 12.4958 - val_mse: 12.4958 - val_mae: 1.5712 - lr: 0.0026 - 4s/epoch - 4ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 4s - loss: 15.9393 - mse: 15.9393 - mae: 1.6121 - val_loss: 12.5030 - val_mse: 12.5030 - val_mae: 1.6214 - lr: 0.0026 - 4s/epoch - 4ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 4s - loss: 15.8232 - mse: 15.8232 - mae: 1.6030 - val_loss: 12.3960 - val_mse: 12.3960 - val_mae: 1.6434 - lr: 0.0026 - 4s/epoch - 4ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 4s - loss: 15.7229 - mse: 15.7229 - mae: 1.6004 - val_loss: 12.5121 - val_mse: 12.5121 - val_mae: 1.6167 - lr: 0.0026 - 4s/epoch - 4ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 4s - loss: 15.7716 - mse: 15.7716 - mae: 1.6038 - val_loss: 12.5103 - val_mse: 12.5103 - val_mae: 1.5795 - lr: 0.0026 - 4s/epoch - 4ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 4s - loss: 15.8478 - mse: 15.8478 - mae: 1.6033 - val_loss: 12.3637 - val_mse: 12.3637 - val_mae: 1.5845 - lr: 0.0026 - 4s/epoch - 4ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 4s - loss: 15.7388 - mse: 15.7388 - mae: 1.6048 - val_loss: 12.4244 - val_mse: 12.4244 - val_mae: 1.6025 - lr: 0.0026 - 4s/epoch - 4ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 4s - loss: 15.8479 - mse: 15.8479 - mae: 1.6078 - val_loss: 12.4750 - val_mse: 12.4750 - val_mae: 1.5705 - lr: 0.0026 - 4s/epoch - 4ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 4s - loss: 15.7341 - mse: 15.7341 - mae: 1.6010 - val_loss: 12.4245 - val_mse: 12.4245 - val_mae: 1.6255 - lr: 0.0026 - 4s/epoch - 4ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 4s - loss: 15.7634 - mse: 15.7634 - mae: 1.5987 - val_loss: 12.3778 - val_mse: 12.3778 - val_mae: 1.5853 - lr: 0.0026 - 4s/epoch - 4ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 4s - loss: 15.6130 - mse: 15.6130 - mae: 1.5972 - val_loss: 12.4895 - val_mse: 12.4895 - val_mae: 1.5554 - lr: 0.0026 - 4s/epoch - 4ms/step\n",
            "Score for fold 1: loss of 12.489505767822266\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 4s - loss: 13.4940 - mse: 13.4940 - mae: 1.5748 - val_loss: 20.9500 - val_mse: 20.9500 - val_mae: 1.5985 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 4s - loss: 13.4628 - mse: 13.4628 - mae: 1.5717 - val_loss: 20.8835 - val_mse: 20.8835 - val_mae: 1.5934 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 4s - loss: 13.4339 - mse: 13.4339 - mae: 1.5725 - val_loss: 20.8961 - val_mse: 20.8961 - val_mae: 1.6387 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 4s - loss: 13.3950 - mse: 13.3950 - mae: 1.5710 - val_loss: 20.8814 - val_mse: 20.8814 - val_mae: 1.6007 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 4s - loss: 13.2830 - mse: 13.2830 - mae: 1.5721 - val_loss: 20.9789 - val_mse: 20.9789 - val_mae: 1.5519 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 4s - loss: 13.3085 - mse: 13.3085 - mae: 1.5663 - val_loss: 20.9272 - val_mse: 20.9272 - val_mae: 1.5861 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 4s - loss: 13.2280 - mse: 13.2280 - mae: 1.5639 - val_loss: 21.0809 - val_mse: 21.0809 - val_mae: 1.5564 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 4s - loss: 13.2550 - mse: 13.2550 - mae: 1.5608 - val_loss: 20.8873 - val_mse: 20.8873 - val_mae: 1.6372 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 4s - loss: 13.2337 - mse: 13.2337 - mae: 1.5649 - val_loss: 20.9531 - val_mse: 20.9531 - val_mae: 1.5727 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Score for fold 2: loss of 20.953144073486328\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 4s - loss: 14.1986 - mse: 14.1986 - mae: 1.5707 - val_loss: 17.0497 - val_mse: 17.0497 - val_mae: 1.5653 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 4s - loss: 14.1651 - mse: 14.1651 - mae: 1.5669 - val_loss: 17.0696 - val_mse: 17.0696 - val_mae: 1.5999 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 4s - loss: 14.1541 - mse: 14.1541 - mae: 1.5673 - val_loss: 17.0746 - val_mse: 17.0746 - val_mae: 1.6351 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 4s - loss: 14.1254 - mse: 14.1254 - mae: 1.5664 - val_loss: 16.9754 - val_mse: 16.9754 - val_mae: 1.5482 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 4s - loss: 14.1125 - mse: 14.1125 - mae: 1.5644 - val_loss: 17.1279 - val_mse: 17.1279 - val_mae: 1.5560 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 4s - loss: 14.0948 - mse: 14.0948 - mae: 1.5647 - val_loss: 17.1702 - val_mse: 17.1702 - val_mae: 1.5611 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 4s - loss: 14.0398 - mse: 14.0398 - mae: 1.5607 - val_loss: 17.1240 - val_mse: 17.1240 - val_mae: 1.6280 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 4s - loss: 14.0017 - mse: 14.0017 - mae: 1.5601 - val_loss: 16.9973 - val_mse: 16.9973 - val_mae: 1.6044 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 4s - loss: 13.9990 - mse: 13.9990 - mae: 1.5586 - val_loss: 17.1324 - val_mse: 17.1324 - val_mae: 1.6343 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Score for fold 3: loss of 17.13235855102539\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 4s - loss: 15.0946 - mse: 15.0946 - mae: 1.5636 - val_loss: 12.8306 - val_mse: 12.8306 - val_mae: 1.5494 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 4s - loss: 15.0158 - mse: 15.0158 - mae: 1.5691 - val_loss: 13.2136 - val_mse: 13.2136 - val_mae: 1.4982 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 4s - loss: 15.0162 - mse: 15.0162 - mae: 1.5602 - val_loss: 12.9782 - val_mse: 12.9782 - val_mae: 1.5461 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 4s - loss: 14.9279 - mse: 14.9279 - mae: 1.5572 - val_loss: 13.0269 - val_mse: 13.0269 - val_mae: 1.4973 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 4s - loss: 14.9067 - mse: 14.9067 - mae: 1.5555 - val_loss: 12.8235 - val_mse: 12.8235 - val_mae: 1.5913 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 4s - loss: 14.8823 - mse: 14.8823 - mae: 1.5527 - val_loss: 13.2245 - val_mse: 13.2245 - val_mae: 1.5403 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 4s - loss: 14.8518 - mse: 14.8518 - mae: 1.5556 - val_loss: 13.0452 - val_mse: 13.0452 - val_mae: 1.5653 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 4s - loss: 14.8398 - mse: 14.8398 - mae: 1.5563 - val_loss: 13.0160 - val_mse: 13.0160 - val_mae: 1.5341 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 4s - loss: 14.8223 - mse: 14.8223 - mae: 1.5488 - val_loss: 13.0796 - val_mse: 13.0796 - val_mae: 1.6263 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 4s - loss: 14.7656 - mse: 14.7656 - mae: 1.5504 - val_loss: 13.1976 - val_mse: 13.1976 - val_mae: 1.6075 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Score for fold 4: loss of 13.197590827941895\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 4s - loss: 15.4676 - mse: 15.4676 - mae: 1.5592 - val_loss: 10.3821 - val_mse: 10.3821 - val_mae: 1.5204 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 4s - loss: 15.4506 - mse: 15.4506 - mae: 1.5519 - val_loss: 10.2504 - val_mse: 10.2504 - val_mae: 1.5278 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 4s - loss: 15.2997 - mse: 15.2997 - mae: 1.5489 - val_loss: 10.2983 - val_mse: 10.2983 - val_mae: 1.5566 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 4s - loss: 15.2956 - mse: 15.2956 - mae: 1.5482 - val_loss: 10.4569 - val_mse: 10.4569 - val_mae: 1.5706 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 4s - loss: 15.2856 - mse: 15.2856 - mae: 1.5508 - val_loss: 10.3596 - val_mse: 10.3596 - val_mae: 1.5348 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 4s - loss: 15.2127 - mse: 15.2127 - mae: 1.5464 - val_loss: 10.5978 - val_mse: 10.5978 - val_mae: 1.5166 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 4s - loss: 15.1215 - mse: 15.1215 - mae: 1.5476 - val_loss: 10.5741 - val_mse: 10.5741 - val_mae: 1.6071 - lr: 0.0010 - 4s/epoch - 4ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-10-14 16:23:12,386]\u001b[0m Finished trial#41 resulted in value: 14.868. Current best value is 14.594 with parameters: {'activation': 'relu', 'num_hidden_layer': 4, 'i': 6, 'learning_rate': 0.000577682381030866}.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Score for fold 5: loss of 10.574140548706055\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 3s - loss: 16.7163 - mse: 16.7163 - mae: 1.6270 - val_loss: 9.9955 - val_mse: 9.9955 - val_mae: 1.5865 - lr: 0.0019 - 3s/epoch - 3ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 16.4598 - mse: 16.4598 - mae: 1.6096 - val_loss: 10.0191 - val_mse: 10.0191 - val_mae: 1.6024 - lr: 0.0019 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 16.3536 - mse: 16.3536 - mae: 1.6060 - val_loss: 9.9501 - val_mse: 9.9501 - val_mae: 1.5314 - lr: 0.0019 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 16.3131 - mse: 16.3131 - mae: 1.5986 - val_loss: 10.0124 - val_mse: 10.0124 - val_mae: 1.5525 - lr: 0.0019 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 16.2900 - mse: 16.2900 - mae: 1.5969 - val_loss: 9.8992 - val_mse: 9.8992 - val_mae: 1.5598 - lr: 0.0019 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 16.3208 - mse: 16.3208 - mae: 1.5940 - val_loss: 9.8600 - val_mse: 9.8600 - val_mae: 1.6149 - lr: 0.0019 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 16.2361 - mse: 16.2361 - mae: 1.5930 - val_loss: 9.8724 - val_mse: 9.8724 - val_mae: 1.5936 - lr: 0.0019 - 2s/epoch - 2ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 2s - loss: 16.2221 - mse: 16.2221 - mae: 1.5964 - val_loss: 9.8627 - val_mse: 9.8627 - val_mae: 1.5665 - lr: 0.0019 - 2s/epoch - 2ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 2s - loss: 16.2719 - mse: 16.2719 - mae: 1.5967 - val_loss: 9.8339 - val_mse: 9.8339 - val_mae: 1.5683 - lr: 0.0019 - 2s/epoch - 2ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 2s - loss: 16.2331 - mse: 16.2331 - mae: 1.5889 - val_loss: 9.9060 - val_mse: 9.9060 - val_mae: 1.6069 - lr: 0.0019 - 2s/epoch - 2ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 2s - loss: 16.2630 - mse: 16.2630 - mae: 1.5917 - val_loss: 10.0172 - val_mse: 10.0172 - val_mae: 1.5369 - lr: 0.0019 - 2s/epoch - 2ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 2s - loss: 16.2341 - mse: 16.2341 - mae: 1.5852 - val_loss: 9.8246 - val_mse: 9.8246 - val_mae: 1.5820 - lr: 0.0019 - 2s/epoch - 2ms/step\n",
            "Epoch 13/100\n",
            "1000/1000 - 2s - loss: 16.1557 - mse: 16.1557 - mae: 1.5827 - val_loss: 10.0140 - val_mse: 10.0140 - val_mae: 1.5335 - lr: 0.0019 - 2s/epoch - 2ms/step\n",
            "Epoch 14/100\n",
            "1000/1000 - 2s - loss: 16.1629 - mse: 16.1629 - mae: 1.5811 - val_loss: 9.8711 - val_mse: 9.8711 - val_mae: 1.5637 - lr: 0.0019 - 2s/epoch - 2ms/step\n",
            "Epoch 15/100\n",
            "1000/1000 - 2s - loss: 16.1359 - mse: 16.1359 - mae: 1.5795 - val_loss: 9.7750 - val_mse: 9.7750 - val_mae: 1.5727 - lr: 0.0019 - 2s/epoch - 2ms/step\n",
            "Epoch 16/100\n",
            "1000/1000 - 2s - loss: 16.1381 - mse: 16.1381 - mae: 1.5868 - val_loss: 9.7475 - val_mse: 9.7475 - val_mae: 1.5668 - lr: 0.0019 - 2s/epoch - 2ms/step\n",
            "Epoch 17/100\n",
            "1000/1000 - 2s - loss: 16.0935 - mse: 16.0935 - mae: 1.5783 - val_loss: 9.7470 - val_mse: 9.7470 - val_mae: 1.5831 - lr: 0.0019 - 2s/epoch - 2ms/step\n",
            "Epoch 18/100\n",
            "1000/1000 - 2s - loss: 16.0855 - mse: 16.0855 - mae: 1.5807 - val_loss: 10.0452 - val_mse: 10.0452 - val_mae: 1.4962 - lr: 0.0019 - 2s/epoch - 2ms/step\n",
            "Epoch 19/100\n",
            "1000/1000 - 2s - loss: 16.0829 - mse: 16.0829 - mae: 1.5808 - val_loss: 9.8770 - val_mse: 9.8770 - val_mae: 1.5922 - lr: 0.0019 - 2s/epoch - 2ms/step\n",
            "Epoch 20/100\n",
            "1000/1000 - 2s - loss: 16.1060 - mse: 16.1060 - mae: 1.5828 - val_loss: 9.9655 - val_mse: 9.9655 - val_mae: 1.5418 - lr: 0.0019 - 2s/epoch - 2ms/step\n",
            "Epoch 21/100\n",
            "1000/1000 - 2s - loss: 16.1400 - mse: 16.1400 - mae: 1.5849 - val_loss: 9.8686 - val_mse: 9.8686 - val_mae: 1.5057 - lr: 0.0019 - 2s/epoch - 2ms/step\n",
            "Epoch 22/100\n",
            "1000/1000 - 2s - loss: 16.0512 - mse: 16.0512 - mae: 1.5827 - val_loss: 10.0608 - val_mse: 10.0608 - val_mae: 1.5409 - lr: 0.0019 - 2s/epoch - 2ms/step\n",
            "Score for fold 1: loss of 10.060802459716797\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 15.6492 - mse: 15.6492 - mae: 1.5697 - val_loss: 11.0773 - val_mse: 11.0773 - val_mae: 1.5262 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 15.6685 - mse: 15.6685 - mae: 1.5690 - val_loss: 10.9249 - val_mse: 10.9249 - val_mae: 1.5210 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 15.6360 - mse: 15.6360 - mae: 1.5657 - val_loss: 10.9869 - val_mse: 10.9869 - val_mae: 1.5858 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 15.5697 - mse: 15.5697 - mae: 1.5674 - val_loss: 11.1253 - val_mse: 11.1253 - val_mae: 1.5465 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 15.6034 - mse: 15.6034 - mae: 1.5659 - val_loss: 10.9330 - val_mse: 10.9330 - val_mae: 1.5164 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 15.5852 - mse: 15.5852 - mae: 1.5628 - val_loss: 10.9153 - val_mse: 10.9153 - val_mae: 1.5567 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 15.5372 - mse: 15.5372 - mae: 1.5676 - val_loss: 10.8989 - val_mse: 10.8989 - val_mae: 1.5756 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 2s - loss: 15.5954 - mse: 15.5954 - mae: 1.5617 - val_loss: 10.9950 - val_mse: 10.9950 - val_mae: 1.5879 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 2s - loss: 15.4447 - mse: 15.4447 - mae: 1.5662 - val_loss: 10.9758 - val_mse: 10.9758 - val_mae: 1.5199 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 2s - loss: 15.4811 - mse: 15.4811 - mae: 1.5582 - val_loss: 10.8342 - val_mse: 10.8342 - val_mae: 1.5802 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 2s - loss: 15.5311 - mse: 15.5311 - mae: 1.5615 - val_loss: 11.0390 - val_mse: 11.0390 - val_mae: 1.5223 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 2s - loss: 15.4588 - mse: 15.4588 - mae: 1.5597 - val_loss: 10.9214 - val_mse: 10.9214 - val_mae: 1.5578 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 13/100\n",
            "1000/1000 - 2s - loss: 15.4504 - mse: 15.4504 - mae: 1.5571 - val_loss: 10.9466 - val_mse: 10.9466 - val_mae: 1.5352 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 14/100\n",
            "1000/1000 - 2s - loss: 15.3670 - mse: 15.3670 - mae: 1.5605 - val_loss: 10.9211 - val_mse: 10.9211 - val_mae: 1.5940 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 15/100\n",
            "1000/1000 - 2s - loss: 15.4614 - mse: 15.4614 - mae: 1.5567 - val_loss: 10.9518 - val_mse: 10.9518 - val_mae: 1.5965 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Score for fold 2: loss of 10.951787948608398\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 12.1023 - mse: 12.1023 - mae: 1.5443 - val_loss: 23.7453 - val_mse: 23.7453 - val_mae: 1.6190 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 12.0069 - mse: 12.0069 - mae: 1.5443 - val_loss: 24.0462 - val_mse: 24.0462 - val_mae: 1.5897 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 12.0874 - mse: 12.0874 - mae: 1.5433 - val_loss: 23.8896 - val_mse: 23.8896 - val_mae: 1.6305 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 12.0420 - mse: 12.0420 - mae: 1.5430 - val_loss: 23.7686 - val_mse: 23.7686 - val_mae: 1.6421 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 11.9912 - mse: 11.9912 - mae: 1.5401 - val_loss: 24.0858 - val_mse: 24.0858 - val_mae: 1.6236 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 11.9985 - mse: 11.9985 - mae: 1.5388 - val_loss: 23.8715 - val_mse: 23.8715 - val_mae: 1.6076 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Score for fold 3: loss of 23.871442794799805\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 15.4691 - mse: 15.4691 - mae: 1.5670 - val_loss: 10.4619 - val_mse: 10.4619 - val_mae: 1.5253 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 15.3765 - mse: 15.3765 - mae: 1.5613 - val_loss: 10.4883 - val_mse: 10.4883 - val_mae: 1.5396 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 15.3510 - mse: 15.3510 - mae: 1.5553 - val_loss: 10.3911 - val_mse: 10.3911 - val_mae: 1.6012 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 15.2860 - mse: 15.2860 - mae: 1.5606 - val_loss: 10.7066 - val_mse: 10.7066 - val_mae: 1.5489 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 15.3071 - mse: 15.3071 - mae: 1.5535 - val_loss: 10.7454 - val_mse: 10.7454 - val_mae: 1.5792 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 15.2673 - mse: 15.2673 - mae: 1.5535 - val_loss: 10.5356 - val_mse: 10.5356 - val_mae: 1.5534 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 15.2459 - mse: 15.2459 - mae: 1.5585 - val_loss: 10.7654 - val_mse: 10.7654 - val_mae: 1.5499 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 2s - loss: 15.3361 - mse: 15.3361 - mae: 1.5591 - val_loss: 10.7579 - val_mse: 10.7579 - val_mae: 1.5433 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Score for fold 4: loss of 10.75793170928955\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 13.4951 - mse: 13.4951 - mae: 1.5502 - val_loss: 17.1556 - val_mse: 17.1556 - val_mae: 1.5573 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 13.5540 - mse: 13.5540 - mae: 1.5501 - val_loss: 17.5778 - val_mse: 17.5778 - val_mae: 1.5278 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 13.4650 - mse: 13.4650 - mae: 1.5446 - val_loss: 17.0052 - val_mse: 17.0052 - val_mae: 1.5822 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 13.5025 - mse: 13.5025 - mae: 1.5469 - val_loss: 17.5877 - val_mse: 17.5877 - val_mae: 1.5685 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 13.4918 - mse: 13.4918 - mae: 1.5437 - val_loss: 17.8245 - val_mse: 17.8245 - val_mae: 1.5805 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 13.4327 - mse: 13.4327 - mae: 1.5417 - val_loss: 17.5100 - val_mse: 17.5100 - val_mae: 1.5653 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 13.4917 - mse: 13.4917 - mae: 1.5417 - val_loss: 17.5653 - val_mse: 17.5653 - val_mae: 1.6181 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 2s - loss: 13.3215 - mse: 13.3215 - mae: 1.5419 - val_loss: 17.6892 - val_mse: 17.6892 - val_mae: 1.5801 - lr: 0.0010 - 2s/epoch - 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-10-14 16:25:28,339]\u001b[0m Finished trial#42 resulted in value: 14.666. Current best value is 14.594 with parameters: {'activation': 'relu', 'num_hidden_layer': 4, 'i': 6, 'learning_rate': 0.000577682381030866}.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Score for fold 5: loss of 17.689233779907227\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 3s - loss: 16.1583 - mse: 16.1583 - mae: 1.6166 - val_loss: 12.2357 - val_mse: 12.2357 - val_mae: 1.6359 - lr: 9.9010e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 15.8314 - mse: 15.8314 - mae: 1.5952 - val_loss: 12.2670 - val_mse: 12.2670 - val_mae: 1.5012 - lr: 9.9010e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 15.7484 - mse: 15.7484 - mae: 1.5970 - val_loss: 12.1043 - val_mse: 12.1043 - val_mae: 1.5227 - lr: 9.9010e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 15.7637 - mse: 15.7637 - mae: 1.5914 - val_loss: 12.0683 - val_mse: 12.0683 - val_mae: 1.6192 - lr: 9.9010e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 15.7375 - mse: 15.7375 - mae: 1.5899 - val_loss: 11.9491 - val_mse: 11.9491 - val_mae: 1.6443 - lr: 9.9010e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 15.7193 - mse: 15.7193 - mae: 1.5861 - val_loss: 12.0724 - val_mse: 12.0724 - val_mae: 1.6345 - lr: 9.9010e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 15.6692 - mse: 15.6692 - mae: 1.5884 - val_loss: 11.9266 - val_mse: 11.9266 - val_mae: 1.5509 - lr: 9.9010e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 2s - loss: 15.6968 - mse: 15.6968 - mae: 1.5857 - val_loss: 12.0507 - val_mse: 12.0507 - val_mae: 1.5417 - lr: 9.9010e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 2s - loss: 15.6791 - mse: 15.6791 - mae: 1.5856 - val_loss: 11.9966 - val_mse: 11.9966 - val_mae: 1.6125 - lr: 9.9010e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 2s - loss: 15.6510 - mse: 15.6510 - mae: 1.5848 - val_loss: 11.9326 - val_mse: 11.9326 - val_mae: 1.5357 - lr: 9.9010e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 2s - loss: 15.6803 - mse: 15.6803 - mae: 1.5785 - val_loss: 11.8888 - val_mse: 11.8888 - val_mae: 1.6596 - lr: 9.9010e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 2s - loss: 15.6744 - mse: 15.6744 - mae: 1.5810 - val_loss: 11.9575 - val_mse: 11.9575 - val_mae: 1.5615 - lr: 9.9010e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 13/100\n",
            "1000/1000 - 2s - loss: 15.5745 - mse: 15.5745 - mae: 1.5806 - val_loss: 12.1391 - val_mse: 12.1391 - val_mae: 1.5552 - lr: 9.9010e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 14/100\n",
            "1000/1000 - 2s - loss: 15.6316 - mse: 15.6316 - mae: 1.5767 - val_loss: 11.9810 - val_mse: 11.9810 - val_mae: 1.5991 - lr: 9.9010e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 15/100\n",
            "1000/1000 - 2s - loss: 15.6119 - mse: 15.6119 - mae: 1.5811 - val_loss: 12.1334 - val_mse: 12.1334 - val_mae: 1.5374 - lr: 9.9010e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 16/100\n",
            "1000/1000 - 2s - loss: 15.6318 - mse: 15.6318 - mae: 1.5801 - val_loss: 11.8922 - val_mse: 11.8922 - val_mae: 1.6128 - lr: 9.9010e-04 - 2s/epoch - 2ms/step\n",
            "Score for fold 1: loss of 11.89220905303955\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 12.7361 - mse: 12.7361 - mae: 1.5743 - val_loss: 23.2377 - val_mse: 23.2377 - val_mae: 1.6026 - lr: 9.9010e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 12.7427 - mse: 12.7427 - mae: 1.5708 - val_loss: 23.1729 - val_mse: 23.1729 - val_mae: 1.6130 - lr: 9.9010e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 12.6742 - mse: 12.6742 - mae: 1.5646 - val_loss: 23.3195 - val_mse: 23.3195 - val_mae: 1.5661 - lr: 9.9010e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 12.6820 - mse: 12.6820 - mae: 1.5690 - val_loss: 23.2476 - val_mse: 23.2476 - val_mae: 1.6314 - lr: 9.9010e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 12.6548 - mse: 12.6548 - mae: 1.5678 - val_loss: 23.3903 - val_mse: 23.3903 - val_mae: 1.5566 - lr: 9.9010e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 12.5985 - mse: 12.5985 - mae: 1.5617 - val_loss: 23.4316 - val_mse: 23.4316 - val_mae: 1.6496 - lr: 9.9010e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 12.5946 - mse: 12.5946 - mae: 1.5622 - val_loss: 23.4038 - val_mse: 23.4038 - val_mae: 1.6262 - lr: 9.9010e-04 - 2s/epoch - 2ms/step\n",
            "Score for fold 2: loss of 23.40378189086914\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 15.4121 - mse: 15.4121 - mae: 1.5664 - val_loss: 12.2222 - val_mse: 12.2222 - val_mae: 1.5586 - lr: 9.9010e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 15.2848 - mse: 15.2848 - mae: 1.5658 - val_loss: 11.9041 - val_mse: 11.9041 - val_mae: 1.5714 - lr: 9.9010e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 15.2905 - mse: 15.2905 - mae: 1.5660 - val_loss: 12.7229 - val_mse: 12.7229 - val_mae: 1.5475 - lr: 9.9010e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 15.3361 - mse: 15.3361 - mae: 1.5627 - val_loss: 12.5339 - val_mse: 12.5339 - val_mae: 1.5017 - lr: 9.9010e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 15.2937 - mse: 15.2937 - mae: 1.5593 - val_loss: 12.1529 - val_mse: 12.1529 - val_mae: 1.5508 - lr: 9.9010e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 15.2566 - mse: 15.2566 - mae: 1.5597 - val_loss: 12.2984 - val_mse: 12.2984 - val_mae: 1.6064 - lr: 9.9010e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 15.1842 - mse: 15.1842 - mae: 1.5618 - val_loss: 12.6547 - val_mse: 12.6547 - val_mae: 1.5865 - lr: 9.9010e-04 - 2s/epoch - 2ms/step\n",
            "Score for fold 3: loss of 12.65469741821289\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 15.2913 - mse: 15.2913 - mae: 1.5551 - val_loss: 12.3166 - val_mse: 12.3166 - val_mae: 1.6310 - lr: 9.9010e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 15.2899 - mse: 15.2899 - mae: 1.5581 - val_loss: 12.5902 - val_mse: 12.5902 - val_mae: 1.5667 - lr: 9.9010e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 15.2104 - mse: 15.2104 - mae: 1.5545 - val_loss: 12.6946 - val_mse: 12.6946 - val_mae: 1.5576 - lr: 9.9010e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 15.1771 - mse: 15.1771 - mae: 1.5550 - val_loss: 12.6849 - val_mse: 12.6849 - val_mae: 1.5779 - lr: 9.9010e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 15.1832 - mse: 15.1832 - mae: 1.5493 - val_loss: 12.7317 - val_mse: 12.7317 - val_mae: 1.5505 - lr: 9.9010e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 15.0980 - mse: 15.0980 - mae: 1.5540 - val_loss: 12.7758 - val_mse: 12.7758 - val_mae: 1.5771 - lr: 9.9010e-04 - 2s/epoch - 2ms/step\n",
            "Score for fold 4: loss of 12.775765419006348\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 14.8538 - mse: 14.8538 - mae: 1.5615 - val_loss: 13.8787 - val_mse: 13.8787 - val_mae: 1.4989 - lr: 9.9010e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 14.8005 - mse: 14.8005 - mae: 1.5591 - val_loss: 13.9570 - val_mse: 13.9570 - val_mae: 1.5813 - lr: 9.9010e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 14.7422 - mse: 14.7422 - mae: 1.5588 - val_loss: 13.7653 - val_mse: 13.7653 - val_mae: 1.5741 - lr: 9.9010e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 14.7453 - mse: 14.7453 - mae: 1.5563 - val_loss: 13.8308 - val_mse: 13.8308 - val_mae: 1.6022 - lr: 9.9010e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 14.6544 - mse: 14.6544 - mae: 1.5545 - val_loss: 13.8473 - val_mse: 13.8473 - val_mae: 1.5516 - lr: 9.9010e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 14.5489 - mse: 14.5489 - mae: 1.5517 - val_loss: 14.0713 - val_mse: 14.0713 - val_mae: 1.5666 - lr: 9.9010e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 14.5735 - mse: 14.5735 - mae: 1.5500 - val_loss: 13.7250 - val_mse: 13.7250 - val_mae: 1.5596 - lr: 9.9010e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 2s - loss: 14.4262 - mse: 14.4262 - mae: 1.5501 - val_loss: 14.1229 - val_mse: 14.1229 - val_mae: 1.5630 - lr: 9.9010e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 2s - loss: 14.4000 - mse: 14.4000 - mae: 1.5466 - val_loss: 13.9356 - val_mse: 13.9356 - val_mae: 1.5817 - lr: 9.9010e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 2s - loss: 14.4827 - mse: 14.4827 - mae: 1.5472 - val_loss: 14.0071 - val_mse: 14.0071 - val_mae: 1.6106 - lr: 9.9010e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 2s - loss: 14.4876 - mse: 14.4876 - mae: 1.5469 - val_loss: 14.2980 - val_mse: 14.2980 - val_mae: 1.5552 - lr: 9.9010e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 2s - loss: 14.2944 - mse: 14.2944 - mae: 1.5455 - val_loss: 13.7233 - val_mse: 13.7233 - val_mae: 1.5889 - lr: 9.9010e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 13/100\n",
            "1000/1000 - 2s - loss: 14.3130 - mse: 14.3130 - mae: 1.5450 - val_loss: 14.1620 - val_mse: 14.1620 - val_mae: 1.5531 - lr: 9.9010e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 14/100\n",
            "1000/1000 - 2s - loss: 14.3167 - mse: 14.3167 - mae: 1.5407 - val_loss: 14.0091 - val_mse: 14.0091 - val_mae: 1.6118 - lr: 9.9010e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 15/100\n",
            "1000/1000 - 2s - loss: 14.1917 - mse: 14.1917 - mae: 1.5453 - val_loss: 13.9238 - val_mse: 13.9238 - val_mae: 1.5774 - lr: 9.9010e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 16/100\n",
            "1000/1000 - 2s - loss: 14.3223 - mse: 14.3223 - mae: 1.5407 - val_loss: 14.2134 - val_mse: 14.2134 - val_mae: 1.6634 - lr: 9.9010e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 17/100\n",
            "1000/1000 - 2s - loss: 14.2181 - mse: 14.2181 - mae: 1.5350 - val_loss: 14.0989 - val_mse: 14.0989 - val_mae: 1.6020 - lr: 9.9010e-04 - 2s/epoch - 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-10-14 16:27:32,079]\u001b[0m Finished trial#43 resulted in value: 14.963999999999999. Current best value is 14.594 with parameters: {'activation': 'relu', 'num_hidden_layer': 4, 'i': 6, 'learning_rate': 0.000577682381030866}.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Score for fold 5: loss of 14.098919868469238\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 3s - loss: 16.3868 - mse: 16.3868 - mae: 1.6296 - val_loss: 11.4299 - val_mse: 11.4299 - val_mae: 1.5652 - lr: 0.0019 - 3s/epoch - 3ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 16.0049 - mse: 16.0049 - mae: 1.6054 - val_loss: 11.4584 - val_mse: 11.4584 - val_mae: 1.5725 - lr: 0.0019 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 15.9274 - mse: 15.9274 - mae: 1.6044 - val_loss: 11.6252 - val_mse: 11.6252 - val_mae: 1.5975 - lr: 0.0019 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 15.9723 - mse: 15.9723 - mae: 1.5993 - val_loss: 11.6047 - val_mse: 11.6047 - val_mae: 1.5662 - lr: 0.0019 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 15.8778 - mse: 15.8778 - mae: 1.5937 - val_loss: 11.4322 - val_mse: 11.4322 - val_mae: 1.6069 - lr: 0.0019 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 15.8630 - mse: 15.8630 - mae: 1.5939 - val_loss: 11.4499 - val_mse: 11.4499 - val_mae: 1.5378 - lr: 0.0019 - 2s/epoch - 2ms/step\n",
            "Score for fold 1: loss of 11.449873924255371\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 14.3577 - mse: 14.3577 - mae: 1.5720 - val_loss: 17.0959 - val_mse: 17.0959 - val_mae: 1.5909 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 14.3396 - mse: 14.3396 - mae: 1.5693 - val_loss: 17.0293 - val_mse: 17.0293 - val_mae: 1.5737 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 14.3230 - mse: 14.3230 - mae: 1.5729 - val_loss: 17.1058 - val_mse: 17.1058 - val_mae: 1.5945 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 14.3036 - mse: 14.3036 - mae: 1.5685 - val_loss: 17.2817 - val_mse: 17.2817 - val_mae: 1.6143 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 14.2824 - mse: 14.2824 - mae: 1.5665 - val_loss: 17.2497 - val_mse: 17.2497 - val_mae: 1.5971 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 14.3744 - mse: 14.3744 - mae: 1.5720 - val_loss: 17.1609 - val_mse: 17.1609 - val_mae: 1.6757 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 14.2697 - mse: 14.2697 - mae: 1.5670 - val_loss: 17.3121 - val_mse: 17.3121 - val_mae: 1.6291 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Score for fold 2: loss of 17.312118530273438\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 15.7022 - mse: 15.7022 - mae: 1.5731 - val_loss: 11.6302 - val_mse: 11.6302 - val_mae: 1.5864 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 15.5454 - mse: 15.5454 - mae: 1.5614 - val_loss: 11.8292 - val_mse: 11.8292 - val_mae: 1.5691 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 15.5496 - mse: 15.5496 - mae: 1.5631 - val_loss: 12.0114 - val_mse: 12.0114 - val_mae: 1.5587 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 15.5376 - mse: 15.5376 - mae: 1.5585 - val_loss: 11.8896 - val_mse: 11.8896 - val_mae: 1.5925 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 15.5325 - mse: 15.5325 - mae: 1.5613 - val_loss: 11.6652 - val_mse: 11.6652 - val_mae: 1.6052 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 15.4378 - mse: 15.4378 - mae: 1.5596 - val_loss: 11.6820 - val_mse: 11.6820 - val_mae: 1.6352 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Score for fold 3: loss of 11.682040214538574\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 14.6124 - mse: 14.6124 - mae: 1.5634 - val_loss: 15.3741 - val_mse: 15.3741 - val_mae: 1.5731 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 14.5772 - mse: 14.5772 - mae: 1.5624 - val_loss: 14.9938 - val_mse: 14.9938 - val_mae: 1.6171 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 14.5715 - mse: 14.5715 - mae: 1.5629 - val_loss: 15.2674 - val_mse: 15.2674 - val_mae: 1.5510 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 14.5228 - mse: 14.5228 - mae: 1.5642 - val_loss: 15.4480 - val_mse: 15.4480 - val_mae: 1.5665 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 14.5411 - mse: 14.5411 - mae: 1.5611 - val_loss: 15.3156 - val_mse: 15.3156 - val_mae: 1.5375 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 14.4880 - mse: 14.4880 - mae: 1.5586 - val_loss: 15.2733 - val_mse: 15.2733 - val_mae: 1.5681 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 14.5165 - mse: 14.5165 - mae: 1.5576 - val_loss: 15.1906 - val_mse: 15.1906 - val_mae: 1.5997 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Score for fold 4: loss of 15.190641403198242\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 13.7267 - mse: 13.7267 - mae: 1.5646 - val_loss: 18.8128 - val_mse: 18.8128 - val_mae: 1.5536 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 13.5682 - mse: 13.5682 - mae: 1.5661 - val_loss: 18.8741 - val_mse: 18.8741 - val_mae: 1.5335 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 13.5864 - mse: 13.5864 - mae: 1.5641 - val_loss: 18.9063 - val_mse: 18.9063 - val_mae: 1.5684 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 13.4718 - mse: 13.4718 - mae: 1.5603 - val_loss: 18.9668 - val_mse: 18.9668 - val_mae: 1.5366 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 13.5414 - mse: 13.5414 - mae: 1.5634 - val_loss: 18.8332 - val_mse: 18.8332 - val_mae: 1.6235 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 13.4969 - mse: 13.4969 - mae: 1.5569 - val_loss: 18.8122 - val_mse: 18.8122 - val_mae: 1.5747 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 13.4762 - mse: 13.4762 - mae: 1.5613 - val_loss: 18.8994 - val_mse: 18.8994 - val_mae: 1.5900 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 2s - loss: 13.4041 - mse: 13.4041 - mae: 1.5570 - val_loss: 18.9542 - val_mse: 18.9542 - val_mae: 1.5471 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 2s - loss: 13.4470 - mse: 13.4470 - mae: 1.5555 - val_loss: 18.8401 - val_mse: 18.8401 - val_mae: 1.5513 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 2s - loss: 13.3458 - mse: 13.3458 - mae: 1.5511 - val_loss: 19.0647 - val_mse: 19.0647 - val_mae: 1.5841 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 2s - loss: 13.3658 - mse: 13.3658 - mae: 1.5545 - val_loss: 18.7484 - val_mse: 18.7484 - val_mae: 1.6204 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 2s - loss: 13.3551 - mse: 13.3551 - mae: 1.5572 - val_loss: 19.0794 - val_mse: 19.0794 - val_mae: 1.5317 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 13/100\n",
            "1000/1000 - 2s - loss: 13.3671 - mse: 13.3671 - mae: 1.5492 - val_loss: 18.7276 - val_mse: 18.7276 - val_mae: 1.5756 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 14/100\n",
            "1000/1000 - 2s - loss: 13.3849 - mse: 13.3849 - mae: 1.5542 - val_loss: 18.9597 - val_mse: 18.9597 - val_mae: 1.5615 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 15/100\n",
            "1000/1000 - 2s - loss: 13.3880 - mse: 13.3880 - mae: 1.5504 - val_loss: 19.0867 - val_mse: 19.0867 - val_mae: 1.6000 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 16/100\n",
            "1000/1000 - 2s - loss: 13.3903 - mse: 13.3903 - mae: 1.5504 - val_loss: 19.1676 - val_mse: 19.1676 - val_mae: 1.5712 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 17/100\n",
            "1000/1000 - 2s - loss: 13.2511 - mse: 13.2511 - mae: 1.5469 - val_loss: 19.1168 - val_mse: 19.1168 - val_mae: 1.5106 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 18/100\n",
            "1000/1000 - 2s - loss: 13.2561 - mse: 13.2561 - mae: 1.5447 - val_loss: 19.0036 - val_mse: 19.0036 - val_mae: 1.5412 - lr: 0.0010 - 2s/epoch - 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-10-14 16:29:14,020]\u001b[0m Finished trial#44 resulted in value: 14.925999999999998. Current best value is 14.594 with parameters: {'activation': 'relu', 'num_hidden_layer': 4, 'i': 6, 'learning_rate': 0.000577682381030866}.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Score for fold 5: loss of 19.00362777709961\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 3s - loss: 16.3237 - mse: 16.3237 - mae: 1.6410 - val_loss: 12.3590 - val_mse: 12.3590 - val_mae: 1.4645 - lr: 7.8365e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 15.8425 - mse: 15.8425 - mae: 1.6133 - val_loss: 12.3782 - val_mse: 12.3782 - val_mae: 1.4705 - lr: 7.8365e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 15.7313 - mse: 15.7313 - mae: 1.6095 - val_loss: 12.2223 - val_mse: 12.2223 - val_mae: 1.5577 - lr: 7.8365e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 15.7758 - mse: 15.7758 - mae: 1.6063 - val_loss: 12.2809 - val_mse: 12.2809 - val_mae: 1.4982 - lr: 7.8365e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 15.6830 - mse: 15.6830 - mae: 1.6026 - val_loss: 12.2429 - val_mse: 12.2429 - val_mae: 1.5173 - lr: 7.8365e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 15.6363 - mse: 15.6363 - mae: 1.5983 - val_loss: 12.2814 - val_mse: 12.2814 - val_mae: 1.6002 - lr: 7.8365e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 15.6120 - mse: 15.6120 - mae: 1.6005 - val_loss: 12.3334 - val_mse: 12.3334 - val_mae: 1.5407 - lr: 7.8365e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 2s - loss: 15.5670 - mse: 15.5670 - mae: 1.5948 - val_loss: 12.2458 - val_mse: 12.2458 - val_mae: 1.5200 - lr: 7.8365e-04 - 2s/epoch - 2ms/step\n",
            "Score for fold 1: loss of 12.245793342590332\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 14.7733 - mse: 14.7733 - mae: 1.5802 - val_loss: 15.8616 - val_mse: 15.8616 - val_mae: 1.6315 - lr: 7.8365e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 14.6864 - mse: 14.6864 - mae: 1.5748 - val_loss: 15.7037 - val_mse: 15.7037 - val_mae: 1.5828 - lr: 7.8365e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 14.6955 - mse: 14.6955 - mae: 1.5733 - val_loss: 15.5488 - val_mse: 15.5488 - val_mae: 1.5707 - lr: 7.8365e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 14.6146 - mse: 14.6146 - mae: 1.5670 - val_loss: 15.6620 - val_mse: 15.6620 - val_mae: 1.6320 - lr: 7.8365e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 14.6925 - mse: 14.6925 - mae: 1.5709 - val_loss: 15.9283 - val_mse: 15.9283 - val_mae: 1.5624 - lr: 7.8365e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 14.6247 - mse: 14.6247 - mae: 1.5720 - val_loss: 15.9759 - val_mse: 15.9759 - val_mae: 1.5680 - lr: 7.8365e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 14.6701 - mse: 14.6701 - mae: 1.5698 - val_loss: 15.8408 - val_mse: 15.8408 - val_mae: 1.6847 - lr: 7.8365e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 2s - loss: 14.5603 - mse: 14.5603 - mae: 1.5699 - val_loss: 15.4807 - val_mse: 15.4807 - val_mae: 1.6304 - lr: 7.8365e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 2s - loss: 14.5327 - mse: 14.5327 - mae: 1.5726 - val_loss: 15.6900 - val_mse: 15.6900 - val_mae: 1.5951 - lr: 7.8365e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 2s - loss: 14.5722 - mse: 14.5722 - mae: 1.5730 - val_loss: 15.8006 - val_mse: 15.8006 - val_mae: 1.5963 - lr: 7.8365e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 2s - loss: 14.5347 - mse: 14.5347 - mae: 1.5705 - val_loss: 15.8430 - val_mse: 15.8430 - val_mae: 1.5921 - lr: 7.8365e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 2s - loss: 14.5197 - mse: 14.5197 - mae: 1.5638 - val_loss: 15.6433 - val_mse: 15.6433 - val_mae: 1.5747 - lr: 7.8365e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 13/100\n",
            "1000/1000 - 2s - loss: 14.4987 - mse: 14.4987 - mae: 1.5689 - val_loss: 15.4353 - val_mse: 15.4353 - val_mae: 1.6430 - lr: 7.8365e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 14/100\n",
            "1000/1000 - 2s - loss: 14.3880 - mse: 14.3880 - mae: 1.5714 - val_loss: 15.9772 - val_mse: 15.9772 - val_mae: 1.5640 - lr: 7.8365e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 15/100\n",
            "1000/1000 - 2s - loss: 14.4423 - mse: 14.4423 - mae: 1.5707 - val_loss: 15.6887 - val_mse: 15.6887 - val_mae: 1.6367 - lr: 7.8365e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 16/100\n",
            "1000/1000 - 2s - loss: 14.4512 - mse: 14.4512 - mae: 1.5711 - val_loss: 15.6361 - val_mse: 15.6361 - val_mae: 1.6390 - lr: 7.8365e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 17/100\n",
            "1000/1000 - 2s - loss: 14.3621 - mse: 14.3621 - mae: 1.5641 - val_loss: 15.8491 - val_mse: 15.8491 - val_mae: 1.5440 - lr: 7.8365e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 18/100\n",
            "1000/1000 - 2s - loss: 14.2607 - mse: 14.2607 - mae: 1.5641 - val_loss: 15.7173 - val_mse: 15.7173 - val_mae: 1.5745 - lr: 7.8365e-04 - 2s/epoch - 2ms/step\n",
            "Score for fold 2: loss of 15.71731948852539\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 13.2616 - mse: 13.2616 - mae: 1.5805 - val_loss: 20.5212 - val_mse: 20.5212 - val_mae: 1.5843 - lr: 7.8365e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 13.1234 - mse: 13.1234 - mae: 1.5755 - val_loss: 20.7035 - val_mse: 20.7035 - val_mae: 1.7076 - lr: 7.8365e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 13.0041 - mse: 13.0041 - mae: 1.5734 - val_loss: 20.5797 - val_mse: 20.5797 - val_mae: 1.5965 - lr: 7.8365e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 12.9385 - mse: 12.9385 - mae: 1.5733 - val_loss: 20.6490 - val_mse: 20.6490 - val_mae: 1.6667 - lr: 7.8365e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 12.9434 - mse: 12.9434 - mae: 1.5759 - val_loss: 20.6307 - val_mse: 20.6307 - val_mae: 1.5931 - lr: 7.8365e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 12.9887 - mse: 12.9887 - mae: 1.5799 - val_loss: 20.9356 - val_mse: 20.9356 - val_mae: 1.5476 - lr: 7.8365e-04 - 2s/epoch - 2ms/step\n",
            "Score for fold 3: loss of 20.935585021972656\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 14.9611 - mse: 14.9611 - mae: 1.5844 - val_loss: 13.2142 - val_mse: 13.2142 - val_mae: 1.7249 - lr: 7.8365e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 15.0756 - mse: 15.0756 - mae: 1.5866 - val_loss: 12.8367 - val_mse: 12.8367 - val_mae: 1.6444 - lr: 7.8365e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 15.0218 - mse: 15.0218 - mae: 1.5759 - val_loss: 13.1810 - val_mse: 13.1810 - val_mae: 1.7357 - lr: 7.8365e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 14.9614 - mse: 14.9614 - mae: 1.5745 - val_loss: 12.6660 - val_mse: 12.6660 - val_mae: 1.6102 - lr: 7.8365e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 14.9065 - mse: 14.9065 - mae: 1.5728 - val_loss: 12.7358 - val_mse: 12.7358 - val_mae: 1.5990 - lr: 7.8365e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 14.8667 - mse: 14.8667 - mae: 1.5792 - val_loss: 13.0460 - val_mse: 13.0460 - val_mae: 1.6325 - lr: 7.8365e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 14.9147 - mse: 14.9147 - mae: 1.5760 - val_loss: 12.8622 - val_mse: 12.8622 - val_mae: 1.6036 - lr: 7.8365e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 2s - loss: 14.8274 - mse: 14.8274 - mae: 1.5702 - val_loss: 13.0959 - val_mse: 13.0959 - val_mae: 1.6088 - lr: 7.8365e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 2s - loss: 14.7675 - mse: 14.7675 - mae: 1.5660 - val_loss: 13.4097 - val_mse: 13.4097 - val_mae: 1.5259 - lr: 7.8365e-04 - 2s/epoch - 2ms/step\n",
            "Score for fold 4: loss of 13.409707069396973\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 15.1450 - mse: 15.1450 - mae: 1.5804 - val_loss: 11.9449 - val_mse: 11.9449 - val_mae: 1.4571 - lr: 7.8365e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 15.0101 - mse: 15.0101 - mae: 1.5794 - val_loss: 11.6309 - val_mse: 11.6309 - val_mae: 1.4895 - lr: 7.8365e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 14.9592 - mse: 14.9592 - mae: 1.5809 - val_loss: 11.7118 - val_mse: 11.7118 - val_mae: 1.5766 - lr: 7.8365e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 15.0364 - mse: 15.0364 - mae: 1.5837 - val_loss: 12.1843 - val_mse: 12.1843 - val_mae: 1.6247 - lr: 7.8365e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 15.0180 - mse: 15.0180 - mae: 1.5777 - val_loss: 11.7709 - val_mse: 11.7709 - val_mae: 1.6392 - lr: 7.8365e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 14.9923 - mse: 14.9923 - mae: 1.5744 - val_loss: 11.9611 - val_mse: 11.9611 - val_mae: 1.6027 - lr: 7.8365e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 15.1086 - mse: 15.1086 - mae: 1.5782 - val_loss: 12.1187 - val_mse: 12.1187 - val_mae: 1.7441 - lr: 7.8365e-04 - 2s/epoch - 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-10-14 16:30:56,655]\u001b[0m Finished trial#45 resulted in value: 14.888. Current best value is 14.594 with parameters: {'activation': 'relu', 'num_hidden_layer': 4, 'i': 6, 'learning_rate': 0.000577682381030866}.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Score for fold 5: loss of 12.118680953979492\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 3s - loss: 14.4088 - mse: 14.4088 - mae: 1.6115 - val_loss: 19.4538 - val_mse: 19.4538 - val_mae: 1.5556 - lr: 0.0013 - 3s/epoch - 3ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 14.1412 - mse: 14.1412 - mae: 1.5957 - val_loss: 19.0954 - val_mse: 19.0954 - val_mae: 1.7610 - lr: 0.0013 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 14.0878 - mse: 14.0878 - mae: 1.5915 - val_loss: 19.0482 - val_mse: 19.0482 - val_mae: 1.6371 - lr: 0.0013 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 14.0607 - mse: 14.0607 - mae: 1.5853 - val_loss: 19.1395 - val_mse: 19.1395 - val_mae: 1.5784 - lr: 0.0013 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 13.9825 - mse: 13.9825 - mae: 1.5808 - val_loss: 19.2532 - val_mse: 19.2532 - val_mae: 1.6320 - lr: 0.0013 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 13.9946 - mse: 13.9946 - mae: 1.5799 - val_loss: 19.1625 - val_mse: 19.1625 - val_mae: 1.5700 - lr: 0.0013 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 13.9745 - mse: 13.9745 - mae: 1.5826 - val_loss: 19.2526 - val_mse: 19.2526 - val_mae: 1.5846 - lr: 0.0013 - 2s/epoch - 2ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 2s - loss: 13.9807 - mse: 13.9807 - mae: 1.5782 - val_loss: 18.9886 - val_mse: 18.9886 - val_mae: 1.6214 - lr: 0.0013 - 2s/epoch - 2ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 2s - loss: 13.9705 - mse: 13.9705 - mae: 1.5757 - val_loss: 18.8946 - val_mse: 18.8946 - val_mae: 1.6241 - lr: 0.0013 - 2s/epoch - 2ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 2s - loss: 13.9508 - mse: 13.9508 - mae: 1.5762 - val_loss: 19.2719 - val_mse: 19.2719 - val_mae: 1.6149 - lr: 0.0013 - 2s/epoch - 2ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 2s - loss: 13.9952 - mse: 13.9952 - mae: 1.5785 - val_loss: 19.0957 - val_mse: 19.0957 - val_mae: 1.5918 - lr: 0.0013 - 2s/epoch - 2ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 2s - loss: 13.8957 - mse: 13.8957 - mae: 1.5777 - val_loss: 19.2496 - val_mse: 19.2496 - val_mae: 1.5490 - lr: 0.0013 - 2s/epoch - 2ms/step\n",
            "Epoch 13/100\n",
            "1000/1000 - 2s - loss: 13.9221 - mse: 13.9221 - mae: 1.5779 - val_loss: 18.9474 - val_mse: 18.9474 - val_mae: 1.5991 - lr: 0.0013 - 2s/epoch - 2ms/step\n",
            "Epoch 14/100\n",
            "1000/1000 - 2s - loss: 13.8716 - mse: 13.8716 - mae: 1.5732 - val_loss: 19.1148 - val_mse: 19.1148 - val_mae: 1.6049 - lr: 0.0013 - 2s/epoch - 2ms/step\n",
            "Score for fold 1: loss of 19.11480140686035\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 15.5890 - mse: 15.5890 - mae: 1.5817 - val_loss: 11.8873 - val_mse: 11.8873 - val_mae: 1.5558 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 15.4471 - mse: 15.4471 - mae: 1.5792 - val_loss: 12.1844 - val_mse: 12.1844 - val_mae: 1.5205 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 15.4487 - mse: 15.4487 - mae: 1.5798 - val_loss: 12.0066 - val_mse: 12.0066 - val_mae: 1.5905 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 15.3655 - mse: 15.3655 - mae: 1.5810 - val_loss: 12.0782 - val_mse: 12.0782 - val_mae: 1.5428 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 15.4406 - mse: 15.4406 - mae: 1.5767 - val_loss: 12.0087 - val_mse: 12.0087 - val_mae: 1.5607 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 15.4818 - mse: 15.4818 - mae: 1.5737 - val_loss: 12.0670 - val_mse: 12.0670 - val_mae: 1.5456 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Score for fold 2: loss of 12.06699275970459\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 14.7362 - mse: 14.7362 - mae: 1.5572 - val_loss: 15.0671 - val_mse: 15.0671 - val_mae: 1.6184 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 14.5910 - mse: 14.5910 - mae: 1.5567 - val_loss: 15.1226 - val_mse: 15.1226 - val_mae: 1.6061 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 14.4822 - mse: 14.4822 - mae: 1.5530 - val_loss: 15.0338 - val_mse: 15.0338 - val_mae: 1.6260 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 14.5921 - mse: 14.5921 - mae: 1.5531 - val_loss: 15.2683 - val_mse: 15.2683 - val_mae: 1.6494 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 14.4557 - mse: 14.4557 - mae: 1.5537 - val_loss: 15.0719 - val_mse: 15.0719 - val_mae: 1.6271 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 14.5068 - mse: 14.5068 - mae: 1.5478 - val_loss: 15.4440 - val_mse: 15.4440 - val_mae: 1.6170 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 14.5202 - mse: 14.5202 - mae: 1.5518 - val_loss: 15.5374 - val_mse: 15.5374 - val_mae: 1.5540 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 2s - loss: 14.4258 - mse: 14.4258 - mae: 1.5486 - val_loss: 15.3417 - val_mse: 15.3417 - val_mae: 1.6449 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Score for fold 3: loss of 15.341753005981445\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 15.1089 - mse: 15.1089 - mae: 1.5720 - val_loss: 12.7785 - val_mse: 12.7785 - val_mae: 1.5275 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 15.0877 - mse: 15.0877 - mae: 1.5702 - val_loss: 12.5271 - val_mse: 12.5271 - val_mae: 1.5482 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 15.0050 - mse: 15.0050 - mae: 1.5706 - val_loss: 12.9516 - val_mse: 12.9516 - val_mae: 1.5076 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 15.0440 - mse: 15.0440 - mae: 1.5634 - val_loss: 13.0642 - val_mse: 13.0642 - val_mae: 1.5624 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 14.9200 - mse: 14.9200 - mae: 1.5634 - val_loss: 12.9044 - val_mse: 12.9044 - val_mae: 1.5494 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 14.9100 - mse: 14.9100 - mae: 1.5637 - val_loss: 13.0435 - val_mse: 13.0435 - val_mae: 1.5433 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 14.8977 - mse: 14.8977 - mae: 1.5651 - val_loss: 12.7916 - val_mse: 12.7916 - val_mae: 1.5024 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Score for fold 4: loss of 12.791638374328613\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 14.4515 - mse: 14.4515 - mae: 1.5552 - val_loss: 14.6319 - val_mse: 14.6319 - val_mae: 1.5774 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 14.4120 - mse: 14.4120 - mae: 1.5490 - val_loss: 14.5331 - val_mse: 14.5331 - val_mae: 1.5355 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 14.3791 - mse: 14.3791 - mae: 1.5498 - val_loss: 14.8612 - val_mse: 14.8612 - val_mae: 1.5774 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 14.3313 - mse: 14.3313 - mae: 1.5485 - val_loss: 14.7762 - val_mse: 14.7762 - val_mae: 1.5542 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 14.3610 - mse: 14.3610 - mae: 1.5462 - val_loss: 14.7365 - val_mse: 14.7365 - val_mae: 1.5646 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 14.3284 - mse: 14.3284 - mae: 1.5465 - val_loss: 14.6942 - val_mse: 14.6942 - val_mae: 1.5774 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 14.2580 - mse: 14.2580 - mae: 1.5437 - val_loss: 15.0123 - val_mse: 15.0123 - val_mae: 1.6253 - lr: 0.0010 - 2s/epoch - 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-10-14 16:32:36,051]\u001b[0m Finished trial#46 resulted in value: 14.863999999999999. Current best value is 14.594 with parameters: {'activation': 'relu', 'num_hidden_layer': 4, 'i': 6, 'learning_rate': 0.000577682381030866}.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Score for fold 5: loss of 15.012300491333008\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 3s - loss: 16.8702 - mse: 16.8702 - mae: 1.6818 - val_loss: 12.5430 - val_mse: 12.5430 - val_mae: 1.6207 - lr: 6.4242e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 16.5285 - mse: 16.5285 - mae: 1.6486 - val_loss: 12.4606 - val_mse: 12.4606 - val_mae: 1.6308 - lr: 6.4242e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 3s - loss: 16.5029 - mse: 16.5029 - mae: 1.6524 - val_loss: 12.5795 - val_mse: 12.5795 - val_mae: 1.5434 - lr: 6.4242e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 3s - loss: 16.4676 - mse: 16.4676 - mae: 1.6498 - val_loss: 12.6356 - val_mse: 12.6356 - val_mae: 1.5661 - lr: 6.4242e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 3s - loss: 16.4964 - mse: 16.4964 - mae: 1.6463 - val_loss: 12.5104 - val_mse: 12.5104 - val_mae: 1.5770 - lr: 6.4242e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 3s - loss: 16.4981 - mse: 16.4981 - mae: 1.6471 - val_loss: 12.4998 - val_mse: 12.4998 - val_mae: 1.6147 - lr: 6.4242e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 3s - loss: 16.4792 - mse: 16.4792 - mae: 1.6469 - val_loss: 12.7238 - val_mse: 12.7238 - val_mae: 1.5432 - lr: 6.4242e-04 - 3s/epoch - 3ms/step\n",
            "Score for fold 1: loss of 12.723827362060547\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 14.7332 - mse: 14.7332 - mae: 1.6232 - val_loss: 19.7429 - val_mse: 19.7429 - val_mae: 1.6402 - lr: 6.4242e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 14.7031 - mse: 14.7031 - mae: 1.6281 - val_loss: 19.5412 - val_mse: 19.5412 - val_mae: 1.7331 - lr: 6.4242e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 3s - loss: 14.6833 - mse: 14.6833 - mae: 1.6276 - val_loss: 19.5537 - val_mse: 19.5537 - val_mae: 1.6782 - lr: 6.4242e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 3s - loss: 14.7066 - mse: 14.7066 - mae: 1.6224 - val_loss: 19.6893 - val_mse: 19.6893 - val_mae: 1.6427 - lr: 6.4242e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 3s - loss: 14.6585 - mse: 14.6585 - mae: 1.6296 - val_loss: 19.7089 - val_mse: 19.7089 - val_mae: 1.6424 - lr: 6.4242e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 3s - loss: 14.7059 - mse: 14.7059 - mae: 1.6289 - val_loss: 19.5875 - val_mse: 19.5875 - val_mae: 1.6747 - lr: 6.4242e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 3s - loss: 14.7083 - mse: 14.7083 - mae: 1.6279 - val_loss: 19.6614 - val_mse: 19.6614 - val_mae: 1.6466 - lr: 6.4242e-04 - 3s/epoch - 3ms/step\n",
            "Score for fold 2: loss of 19.66143226623535\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 3s - loss: 16.0562 - mse: 16.0562 - mae: 1.6389 - val_loss: 14.2036 - val_mse: 14.2036 - val_mae: 1.6101 - lr: 6.4242e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 3s - loss: 16.0226 - mse: 16.0226 - mae: 1.6367 - val_loss: 14.1903 - val_mse: 14.1903 - val_mae: 1.6255 - lr: 6.4242e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 3s - loss: 16.0448 - mse: 16.0448 - mae: 1.6391 - val_loss: 14.1767 - val_mse: 14.1767 - val_mae: 1.6634 - lr: 6.4242e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 3s - loss: 16.0285 - mse: 16.0285 - mae: 1.6377 - val_loss: 14.1845 - val_mse: 14.1845 - val_mae: 1.6750 - lr: 6.4242e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 3s - loss: 16.0313 - mse: 16.0313 - mae: 1.6390 - val_loss: 14.1919 - val_mse: 14.1919 - val_mae: 1.6836 - lr: 6.4242e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 3s - loss: 16.0300 - mse: 16.0300 - mae: 1.6446 - val_loss: 14.2288 - val_mse: 14.2288 - val_mae: 1.5882 - lr: 6.4242e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 3s - loss: 16.0242 - mse: 16.0242 - mae: 1.6407 - val_loss: 14.1914 - val_mse: 14.1914 - val_mae: 1.6161 - lr: 6.4242e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 3s - loss: 16.0222 - mse: 16.0222 - mae: 1.6449 - val_loss: 14.2411 - val_mse: 14.2411 - val_mae: 1.5751 - lr: 6.4242e-04 - 3s/epoch - 3ms/step\n",
            "Score for fold 3: loss of 14.24107837677002\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 15.6238 - mse: 15.6238 - mae: 1.6304 - val_loss: 15.7837 - val_mse: 15.7837 - val_mae: 1.8431 - lr: 6.4242e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 3s - loss: 15.6388 - mse: 15.6388 - mae: 1.6299 - val_loss: 15.8464 - val_mse: 15.8464 - val_mae: 1.6361 - lr: 6.4242e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 3s - loss: 15.6163 - mse: 15.6163 - mae: 1.6312 - val_loss: 15.7561 - val_mse: 15.7561 - val_mae: 1.6765 - lr: 6.4242e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 3s - loss: 15.6443 - mse: 15.6443 - mae: 1.6307 - val_loss: 15.9153 - val_mse: 15.9153 - val_mae: 1.6156 - lr: 6.4242e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 3s - loss: 15.6508 - mse: 15.6508 - mae: 1.6335 - val_loss: 15.8142 - val_mse: 15.8142 - val_mae: 1.6430 - lr: 6.4242e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 3s - loss: 15.6166 - mse: 15.6166 - mae: 1.6331 - val_loss: 16.0022 - val_mse: 16.0022 - val_mae: 1.5963 - lr: 6.4242e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 3s - loss: 15.6522 - mse: 15.6522 - mae: 1.6315 - val_loss: 16.2769 - val_mse: 16.2769 - val_mae: 1.5678 - lr: 6.4242e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 3s - loss: 15.6530 - mse: 15.6530 - mae: 1.6306 - val_loss: 15.7299 - val_mse: 15.7299 - val_mae: 1.6613 - lr: 6.4242e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 3s - loss: 15.6803 - mse: 15.6803 - mae: 1.6277 - val_loss: 15.7507 - val_mse: 15.7507 - val_mae: 1.6620 - lr: 6.4242e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 3s - loss: 15.6689 - mse: 15.6689 - mae: 1.6304 - val_loss: 15.7899 - val_mse: 15.7899 - val_mae: 1.8466 - lr: 6.4242e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 3s - loss: 15.6867 - mse: 15.6867 - mae: 1.6306 - val_loss: 16.1174 - val_mse: 16.1174 - val_mae: 1.5818 - lr: 6.4242e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 3s - loss: 15.7473 - mse: 15.7473 - mae: 1.6300 - val_loss: 15.8363 - val_mse: 15.8363 - val_mae: 1.6400 - lr: 6.4242e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 13/100\n",
            "1000/1000 - 3s - loss: 15.6540 - mse: 15.6540 - mae: 1.6394 - val_loss: 15.7753 - val_mse: 15.7753 - val_mae: 1.8462 - lr: 6.4242e-04 - 3s/epoch - 3ms/step\n",
            "Score for fold 4: loss of 15.775259971618652\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 15.6898 - mse: 15.6898 - mae: 1.6512 - val_loss: 16.2149 - val_mse: 16.2149 - val_mae: 1.5812 - lr: 6.4242e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 3s - loss: 15.6284 - mse: 15.6284 - mae: 1.6423 - val_loss: 16.2422 - val_mse: 16.2422 - val_mae: 1.5734 - lr: 6.4242e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 3s - loss: 15.6211 - mse: 15.6211 - mae: 1.6472 - val_loss: 16.1869 - val_mse: 16.1869 - val_mae: 1.5852 - lr: 6.4242e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 3s - loss: 15.6130 - mse: 15.6130 - mae: 1.6495 - val_loss: 16.1282 - val_mse: 16.1282 - val_mae: 1.6924 - lr: 6.4242e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 3s - loss: 15.6131 - mse: 15.6131 - mae: 1.6473 - val_loss: 16.3555 - val_mse: 16.3555 - val_mae: 1.5500 - lr: 6.4242e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 3s - loss: 15.6327 - mse: 15.6327 - mae: 1.6434 - val_loss: 16.1048 - val_mse: 16.1048 - val_mae: 1.6646 - lr: 6.4242e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 3s - loss: 15.6084 - mse: 15.6084 - mae: 1.6535 - val_loss: 16.2600 - val_mse: 16.2600 - val_mae: 1.5570 - lr: 6.4242e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 3s - loss: 15.6486 - mse: 15.6486 - mae: 1.6506 - val_loss: 16.3974 - val_mse: 16.3974 - val_mae: 1.5447 - lr: 6.4242e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 3s - loss: 15.6266 - mse: 15.6266 - mae: 1.6468 - val_loss: 16.2937 - val_mse: 16.2937 - val_mae: 1.5618 - lr: 6.4242e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 3s - loss: 15.5752 - mse: 15.5752 - mae: 1.6425 - val_loss: 16.1314 - val_mse: 16.1314 - val_mae: 1.6919 - lr: 6.4242e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 3s - loss: 15.6528 - mse: 15.6528 - mae: 1.6500 - val_loss: 16.1071 - val_mse: 16.1071 - val_mae: 1.6362 - lr: 6.4242e-04 - 3s/epoch - 3ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-10-14 16:34:39,317]\u001b[0m Finished trial#47 resulted in value: 15.702000000000002. Current best value is 14.594 with parameters: {'activation': 'relu', 'num_hidden_layer': 4, 'i': 6, 'learning_rate': 0.000577682381030866}.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Score for fold 5: loss of 16.107099533081055\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 3s - loss: 15.9576 - mse: 15.9576 - mae: 1.6076 - val_loss: 13.8855 - val_mse: 13.8855 - val_mae: 1.6954 - lr: 0.0010 - 3s/epoch - 3ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 15.5370 - mse: 15.5370 - mae: 1.5900 - val_loss: 13.7742 - val_mse: 13.7742 - val_mae: 1.6663 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 15.4511 - mse: 15.4511 - mae: 1.5853 - val_loss: 13.7006 - val_mse: 13.7006 - val_mae: 1.6232 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 15.3958 - mse: 15.3958 - mae: 1.5809 - val_loss: 13.7239 - val_mse: 13.7239 - val_mae: 1.6140 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 15.3697 - mse: 15.3697 - mae: 1.5752 - val_loss: 13.6284 - val_mse: 13.6284 - val_mae: 1.6228 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 15.3574 - mse: 15.3574 - mae: 1.5769 - val_loss: 13.6492 - val_mse: 13.6492 - val_mae: 1.6404 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 15.3579 - mse: 15.3579 - mae: 1.5686 - val_loss: 13.7259 - val_mse: 13.7259 - val_mae: 1.6150 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 2s - loss: 15.2852 - mse: 15.2852 - mae: 1.5729 - val_loss: 13.5460 - val_mse: 13.5460 - val_mae: 1.6481 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 2s - loss: 15.3135 - mse: 15.3135 - mae: 1.5702 - val_loss: 13.7348 - val_mse: 13.7348 - val_mae: 1.5829 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 2s - loss: 15.2239 - mse: 15.2239 - mae: 1.5719 - val_loss: 13.6394 - val_mse: 13.6394 - val_mae: 1.6229 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 2s - loss: 15.3010 - mse: 15.3010 - mae: 1.5679 - val_loss: 13.7663 - val_mse: 13.7663 - val_mae: 1.5739 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 2s - loss: 15.2184 - mse: 15.2184 - mae: 1.5649 - val_loss: 13.6023 - val_mse: 13.6023 - val_mae: 1.6309 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 13/100\n",
            "1000/1000 - 2s - loss: 15.2498 - mse: 15.2498 - mae: 1.5656 - val_loss: 13.7307 - val_mse: 13.7307 - val_mae: 1.6218 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Score for fold 1: loss of 13.730724334716797\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 14.5870 - mse: 14.5870 - mae: 1.5787 - val_loss: 16.2458 - val_mse: 16.2458 - val_mae: 1.5849 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 14.5734 - mse: 14.5734 - mae: 1.5783 - val_loss: 16.0409 - val_mse: 16.0409 - val_mae: 1.5480 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 14.5186 - mse: 14.5186 - mae: 1.5762 - val_loss: 16.1733 - val_mse: 16.1733 - val_mae: 1.5753 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 14.5330 - mse: 14.5330 - mae: 1.5759 - val_loss: 16.2648 - val_mse: 16.2648 - val_mae: 1.5494 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 14.4953 - mse: 14.4953 - mae: 1.5728 - val_loss: 16.3951 - val_mse: 16.3951 - val_mae: 1.5450 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 14.4799 - mse: 14.4799 - mae: 1.5726 - val_loss: 16.2759 - val_mse: 16.2759 - val_mae: 1.5544 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 14.4482 - mse: 14.4482 - mae: 1.5731 - val_loss: 16.2272 - val_mse: 16.2272 - val_mae: 1.5681 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Score for fold 2: loss of 16.227231979370117\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 12.7528 - mse: 12.7528 - mae: 1.5670 - val_loss: 23.1259 - val_mse: 23.1259 - val_mae: 1.5845 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 12.7239 - mse: 12.7239 - mae: 1.5658 - val_loss: 23.3063 - val_mse: 23.3063 - val_mae: 1.5673 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 12.7130 - mse: 12.7130 - mae: 1.5632 - val_loss: 23.4478 - val_mse: 23.4478 - val_mae: 1.5904 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 12.6939 - mse: 12.6939 - mae: 1.5617 - val_loss: 23.6033 - val_mse: 23.6033 - val_mae: 1.5591 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 12.6521 - mse: 12.6521 - mae: 1.5600 - val_loss: 23.3340 - val_mse: 23.3340 - val_mae: 1.5549 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 12.6610 - mse: 12.6610 - mae: 1.5600 - val_loss: 23.3612 - val_mse: 23.3612 - val_mae: 1.5984 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Score for fold 3: loss of 23.361230850219727\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 16.2566 - mse: 16.2566 - mae: 1.5822 - val_loss: 9.3078 - val_mse: 9.3078 - val_mae: 1.4971 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 16.2057 - mse: 16.2057 - mae: 1.5790 - val_loss: 9.3565 - val_mse: 9.3565 - val_mae: 1.5678 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 16.1521 - mse: 16.1521 - mae: 1.5805 - val_loss: 9.0673 - val_mse: 9.0673 - val_mae: 1.5437 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 16.1834 - mse: 16.1834 - mae: 1.5808 - val_loss: 9.4445 - val_mse: 9.4445 - val_mae: 1.4591 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 16.0903 - mse: 16.0903 - mae: 1.5783 - val_loss: 9.1998 - val_mse: 9.1998 - val_mae: 1.5227 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 16.1098 - mse: 16.1098 - mae: 1.5774 - val_loss: 9.2598 - val_mse: 9.2598 - val_mae: 1.5608 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 16.0051 - mse: 16.0051 - mae: 1.5760 - val_loss: 9.4127 - val_mse: 9.4127 - val_mae: 1.5552 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 2s - loss: 16.1459 - mse: 16.1459 - mae: 1.5809 - val_loss: 9.2989 - val_mse: 9.2989 - val_mae: 1.5583 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Score for fold 4: loss of 9.298946380615234\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 15.5030 - mse: 15.5030 - mae: 1.5606 - val_loss: 11.6661 - val_mse: 11.6661 - val_mae: 1.5952 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 15.4365 - mse: 15.4365 - mae: 1.5561 - val_loss: 11.9239 - val_mse: 11.9239 - val_mae: 1.6356 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 15.4152 - mse: 15.4152 - mae: 1.5587 - val_loss: 11.9625 - val_mse: 11.9625 - val_mae: 1.5818 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 15.4559 - mse: 15.4559 - mae: 1.5548 - val_loss: 11.8330 - val_mse: 11.8330 - val_mae: 1.6450 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 15.4385 - mse: 15.4385 - mae: 1.5579 - val_loss: 12.2801 - val_mse: 12.2801 - val_mae: 1.6058 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 15.3577 - mse: 15.3577 - mae: 1.5559 - val_loss: 11.7588 - val_mse: 11.7588 - val_mae: 1.6508 - lr: 0.0010 - 2s/epoch - 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-10-14 16:36:02,205]\u001b[0m Finished trial#48 resulted in value: 14.876000000000001. Current best value is 14.594 with parameters: {'activation': 'relu', 'num_hidden_layer': 4, 'i': 6, 'learning_rate': 0.000577682381030866}.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Score for fold 5: loss of 11.7587890625\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 3s - loss: 14.9147 - mse: 14.9147 - mae: 1.6392 - val_loss: 17.7069 - val_mse: 17.7069 - val_mae: 1.5280 - lr: 0.0021 - 3s/epoch - 3ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 14.6441 - mse: 14.6441 - mae: 1.6208 - val_loss: 17.4358 - val_mse: 17.4358 - val_mae: 1.5864 - lr: 0.0021 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 3s - loss: 14.5709 - mse: 14.5709 - mae: 1.6149 - val_loss: 17.5000 - val_mse: 17.5000 - val_mae: 1.5801 - lr: 0.0021 - 3s/epoch - 3ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 3s - loss: 14.5100 - mse: 14.5100 - mae: 1.6039 - val_loss: 17.5669 - val_mse: 17.5669 - val_mae: 1.6655 - lr: 0.0021 - 3s/epoch - 3ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 3s - loss: 14.4332 - mse: 14.4332 - mae: 1.6066 - val_loss: 17.3003 - val_mse: 17.3003 - val_mae: 1.5059 - lr: 0.0021 - 3s/epoch - 3ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 3s - loss: 14.4249 - mse: 14.4249 - mae: 1.6012 - val_loss: 17.5734 - val_mse: 17.5734 - val_mae: 1.4822 - lr: 0.0021 - 3s/epoch - 3ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 3s - loss: 14.4289 - mse: 14.4289 - mae: 1.6054 - val_loss: 17.5678 - val_mse: 17.5678 - val_mae: 1.6229 - lr: 0.0021 - 3s/epoch - 3ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 3s - loss: 14.4094 - mse: 14.4094 - mae: 1.6096 - val_loss: 17.4006 - val_mse: 17.4006 - val_mae: 1.5847 - lr: 0.0021 - 3s/epoch - 3ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 3s - loss: 14.3479 - mse: 14.3479 - mae: 1.6039 - val_loss: 17.4808 - val_mse: 17.4808 - val_mae: 1.5165 - lr: 0.0021 - 3s/epoch - 3ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 3s - loss: 14.3352 - mse: 14.3352 - mae: 1.6022 - val_loss: 17.3411 - val_mse: 17.3411 - val_mae: 1.7024 - lr: 0.0021 - 3s/epoch - 3ms/step\n",
            "Score for fold 1: loss of 17.341083526611328\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 3s - loss: 15.6311 - mse: 15.6311 - mae: 1.5776 - val_loss: 11.3561 - val_mse: 11.3561 - val_mae: 1.5444 - lr: 0.0010 - 3s/epoch - 3ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 3s - loss: 15.6019 - mse: 15.6019 - mae: 1.5703 - val_loss: 11.2373 - val_mse: 11.2373 - val_mae: 1.5730 - lr: 0.0010 - 3s/epoch - 3ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 3s - loss: 15.5663 - mse: 15.5663 - mae: 1.5742 - val_loss: 11.2317 - val_mse: 11.2317 - val_mae: 1.5784 - lr: 0.0010 - 3s/epoch - 3ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 3s - loss: 15.6187 - mse: 15.6187 - mae: 1.5765 - val_loss: 11.3927 - val_mse: 11.3927 - val_mae: 1.7265 - lr: 0.0010 - 3s/epoch - 3ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 3s - loss: 15.6527 - mse: 15.6527 - mae: 1.5745 - val_loss: 11.6268 - val_mse: 11.6268 - val_mae: 1.6255 - lr: 0.0010 - 3s/epoch - 3ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 3s - loss: 15.5460 - mse: 15.5460 - mae: 1.5779 - val_loss: 11.6712 - val_mse: 11.6712 - val_mae: 1.6774 - lr: 0.0010 - 3s/epoch - 3ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 3s - loss: 15.5579 - mse: 15.5579 - mae: 1.5714 - val_loss: 11.4945 - val_mse: 11.4945 - val_mae: 1.5592 - lr: 0.0010 - 3s/epoch - 3ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 3s - loss: 15.5201 - mse: 15.5201 - mae: 1.5732 - val_loss: 11.4825 - val_mse: 11.4825 - val_mae: 1.5908 - lr: 0.0010 - 3s/epoch - 3ms/step\n",
            "Score for fold 2: loss of 11.482521057128906\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 3s - loss: 14.5289 - mse: 14.5289 - mae: 1.5743 - val_loss: 15.0454 - val_mse: 15.0454 - val_mae: 1.6133 - lr: 0.0010 - 3s/epoch - 3ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 3s - loss: 14.4080 - mse: 14.4080 - mae: 1.5738 - val_loss: 15.2087 - val_mse: 15.2087 - val_mae: 1.5948 - lr: 0.0010 - 3s/epoch - 3ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 3s - loss: 14.4760 - mse: 14.4760 - mae: 1.5726 - val_loss: 15.5485 - val_mse: 15.5485 - val_mae: 1.4709 - lr: 0.0010 - 3s/epoch - 3ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 3s - loss: 14.2828 - mse: 14.2828 - mae: 1.5748 - val_loss: 15.4143 - val_mse: 15.4143 - val_mae: 1.5201 - lr: 0.0010 - 3s/epoch - 3ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 3s - loss: 14.4345 - mse: 14.4345 - mae: 1.5735 - val_loss: 15.4226 - val_mse: 15.4226 - val_mae: 1.5164 - lr: 0.0010 - 3s/epoch - 3ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 3s - loss: 14.3634 - mse: 14.3634 - mae: 1.5771 - val_loss: 15.8686 - val_mse: 15.8686 - val_mae: 1.5604 - lr: 0.0010 - 3s/epoch - 3ms/step\n",
            "Score for fold 3: loss of 15.868586540222168\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 13.8792 - mse: 13.8792 - mae: 1.5685 - val_loss: 17.6724 - val_mse: 17.6724 - val_mae: 1.5586 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 3s - loss: 13.8563 - mse: 13.8563 - mae: 1.5674 - val_loss: 17.8242 - val_mse: 17.8242 - val_mae: 1.5141 - lr: 0.0010 - 3s/epoch - 3ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 3s - loss: 13.8712 - mse: 13.8712 - mae: 1.5695 - val_loss: 18.0695 - val_mse: 18.0695 - val_mae: 1.6169 - lr: 0.0010 - 3s/epoch - 3ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 3s - loss: 13.9519 - mse: 13.9519 - mae: 1.5683 - val_loss: 17.9514 - val_mse: 17.9514 - val_mae: 1.6536 - lr: 0.0010 - 3s/epoch - 3ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 3s - loss: 13.7894 - mse: 13.7894 - mae: 1.5646 - val_loss: 17.9688 - val_mse: 17.9688 - val_mae: 1.6403 - lr: 0.0010 - 3s/epoch - 3ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 3s - loss: 13.8823 - mse: 13.8823 - mae: 1.5650 - val_loss: 17.7387 - val_mse: 17.7387 - val_mae: 1.6172 - lr: 0.0010 - 3s/epoch - 3ms/step\n",
            "Score for fold 4: loss of 17.738739013671875\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 3s - loss: 15.2678 - mse: 15.2678 - mae: 1.5732 - val_loss: 12.1225 - val_mse: 12.1225 - val_mae: 1.5522 - lr: 0.0010 - 3s/epoch - 3ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 15.3110 - mse: 15.3110 - mae: 1.5724 - val_loss: 12.2965 - val_mse: 12.2965 - val_mae: 1.5625 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 3s - loss: 15.2778 - mse: 15.2778 - mae: 1.5757 - val_loss: 12.0021 - val_mse: 12.0021 - val_mae: 1.6181 - lr: 0.0010 - 3s/epoch - 3ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 15.2130 - mse: 15.2130 - mae: 1.5680 - val_loss: 11.6695 - val_mse: 11.6695 - val_mae: 1.5077 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 3s - loss: 15.1976 - mse: 15.1976 - mae: 1.5656 - val_loss: 12.0588 - val_mse: 12.0588 - val_mae: 1.5150 - lr: 0.0010 - 3s/epoch - 3ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 3s - loss: 15.1694 - mse: 15.1694 - mae: 1.5679 - val_loss: 12.0632 - val_mse: 12.0632 - val_mae: 1.6641 - lr: 0.0010 - 3s/epoch - 3ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 3s - loss: 15.2259 - mse: 15.2259 - mae: 1.5661 - val_loss: 11.7189 - val_mse: 11.7189 - val_mae: 1.6548 - lr: 0.0010 - 3s/epoch - 3ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 3s - loss: 15.1058 - mse: 15.1058 - mae: 1.5648 - val_loss: 11.9703 - val_mse: 11.9703 - val_mae: 1.5736 - lr: 0.0010 - 3s/epoch - 3ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 3s - loss: 15.1244 - mse: 15.1244 - mae: 1.5683 - val_loss: 12.8273 - val_mse: 12.8273 - val_mae: 1.7241 - lr: 0.0010 - 3s/epoch - 3ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-10-14 16:37:48,691]\u001b[0m Finished trial#49 resulted in value: 15.051999999999998. Current best value is 14.594 with parameters: {'activation': 'relu', 'num_hidden_layer': 4, 'i': 6, 'learning_rate': 0.000577682381030866}.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Score for fold 5: loss of 12.82724380493164\n"
          ]
        }
      ],
      "source": [
        "model_list=[]\n",
        "history_list=[]\n",
        "study_name = 'NN_study_2'\n",
        "study = optuna.create_study(study_name=study_name, load_if_exists=True)\n",
        "func = lambda trial: objective(trial,training_shuffled1,labelsForTrain_shuffled1)\n",
        "study.optimize(func, n_trials=50,)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_tgpDC4c5U2w",
        "outputId": "d20aee4f-cba9-4099-cb85-41ab69e2d4bd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "1250/1250 - 3s - loss: 15.6789 - mse: 15.6789 - mae: 1.6297 - val_loss: 13.6266 - val_mse: 13.6266 - val_mae: 1.5842 - lr: 5.7768e-04 - 3s/epoch - 2ms/step\n",
            "Epoch 2/20\n",
            "1250/1250 - 2s - loss: 15.1218 - mse: 15.1218 - mae: 1.5981 - val_loss: 13.7104 - val_mse: 13.7104 - val_mae: 1.5945 - lr: 5.7768e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 3/20\n",
            "1250/1250 - 2s - loss: 15.0900 - mse: 15.0900 - mae: 1.5918 - val_loss: 13.3743 - val_mse: 13.3743 - val_mae: 1.6734 - lr: 5.7768e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 4/20\n",
            "1250/1250 - 2s - loss: 15.0565 - mse: 15.0565 - mae: 1.5889 - val_loss: 13.4266 - val_mse: 13.4266 - val_mae: 1.5840 - lr: 5.7768e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 5/20\n",
            "1250/1250 - 2s - loss: 14.9781 - mse: 14.9781 - mae: 1.5840 - val_loss: 13.3230 - val_mse: 13.3230 - val_mae: 1.5776 - lr: 5.7768e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 6/20\n",
            "1250/1250 - 2s - loss: 14.9715 - mse: 14.9715 - mae: 1.5828 - val_loss: 13.4451 - val_mse: 13.4451 - val_mae: 1.5467 - lr: 5.7768e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 7/20\n",
            "1250/1250 - 2s - loss: 14.9251 - mse: 14.9251 - mae: 1.5800 - val_loss: 13.4425 - val_mse: 13.4425 - val_mae: 1.5806 - lr: 5.7768e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 8/20\n",
            "1250/1250 - 2s - loss: 14.9084 - mse: 14.9084 - mae: 1.5790 - val_loss: 13.3533 - val_mse: 13.3533 - val_mae: 1.5775 - lr: 5.7768e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 9/20\n",
            "1250/1250 - 2s - loss: 14.8745 - mse: 14.8745 - mae: 1.5791 - val_loss: 13.3195 - val_mse: 13.3195 - val_mae: 1.5632 - lr: 5.7768e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 10/20\n",
            "1250/1250 - 2s - loss: 14.9004 - mse: 14.9004 - mae: 1.5802 - val_loss: 13.2463 - val_mse: 13.2463 - val_mae: 1.7254 - lr: 5.7768e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 11/20\n",
            "1250/1250 - 2s - loss: 14.8436 - mse: 14.8436 - mae: 1.5802 - val_loss: 13.3047 - val_mse: 13.3047 - val_mae: 1.6033 - lr: 5.7768e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 12/20\n",
            "1250/1250 - 2s - loss: 14.8427 - mse: 14.8427 - mae: 1.5820 - val_loss: 13.2129 - val_mse: 13.2129 - val_mae: 1.5871 - lr: 5.7768e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 13/20\n",
            "1250/1250 - 2s - loss: 14.8727 - mse: 14.8727 - mae: 1.5819 - val_loss: 13.2078 - val_mse: 13.2078 - val_mae: 1.6085 - lr: 5.7768e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 14/20\n",
            "1250/1250 - 2s - loss: 14.7575 - mse: 14.7575 - mae: 1.5763 - val_loss: 13.3150 - val_mse: 13.3150 - val_mae: 1.6369 - lr: 5.7768e-04 - 2s/epoch - 1ms/step\n",
            "Epoch 15/20\n",
            "1250/1250 - 2s - loss: 14.7420 - mse: 14.7420 - mae: 1.5737 - val_loss: 13.3301 - val_mse: 13.3301 - val_mae: 1.6875 - lr: 5.7768e-04 - 2s/epoch - 1ms/step\n",
            "Epoch 16/20\n",
            "1250/1250 - 2s - loss: 14.7831 - mse: 14.7831 - mae: 1.5751 - val_loss: 13.5015 - val_mse: 13.5015 - val_mae: 1.6000 - lr: 5.7768e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 17/20\n",
            "1250/1250 - 2s - loss: 14.7314 - mse: 14.7314 - mae: 1.5750 - val_loss: 13.6896 - val_mse: 13.6896 - val_mae: 1.5908 - lr: 5.7768e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 18/20\n",
            "1250/1250 - 2s - loss: 14.7711 - mse: 14.7711 - mae: 1.5755 - val_loss: 13.3759 - val_mse: 13.3759 - val_mae: 1.6281 - lr: 5.7768e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 19/20\n",
            "1250/1250 - 2s - loss: 14.7124 - mse: 14.7124 - mae: 1.5736 - val_loss: 13.2517 - val_mse: 13.2517 - val_mae: 1.6212 - lr: 5.7768e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 20/20\n",
            "1250/1250 - 2s - loss: 14.7485 - mse: 14.7485 - mae: 1.5740 - val_loss: 13.2254 - val_mse: 13.2254 - val_mae: 1.5693 - lr: 5.7768e-04 - 2s/epoch - 2ms/step\n"
          ]
        }
      ],
      "source": [
        "#{'activation': 'relu', 'num_hidden_layer': 4, 'i': 6, 'learning_rate': 0.000577682381030866}\n",
        "optimizer = Adam(learning_rate=0.000577682381030866 ,clipnorm=1.0)\n",
        "model_2 = create_model(activation=\"relu\",num_hidden_layer=4,num_hidden_unit=64)\n",
        "\n",
        "es = EarlyStopping(monitor='mse', patience=5)\n",
        "reduce_lr = ReduceLROnPlateau(monitor='val_mse', factor=0.2,\n",
        "                              patience=20, min_lr=0.001)\n",
        "model_2.compile(loss='mse',optimizer=optimizer,metrics=['mse','mae'])\n",
        "history = model_2.fit(training,labelsForTrain,batch_size=64,epochs=20,verbose=2,validation_data=(valing,labelsForVal),validation_batch_size=64,callbacks=[es,reduce_lr])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FaO51aiBDYsw",
        "outputId": "12370c93-0891-41b4-ff2b-96653ae96119"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "313/313 [==============================] - 0s 1ms/step - loss: 11.8047 - mse: 11.8047 - mae: 1.5521\n"
          ]
        }
      ],
      "source": [
        "results_model2 = model_2.evaluate(testing, labelsForTest, batch_size=32)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NmfHslRiGobG"
      },
      "source": [
        "## Shuffle Repetation 3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JbXmpk3DGq1X"
      },
      "outputs": [],
      "source": [
        "# Shuffle the data after train\n",
        "shuffled2 = shuffle(train_df, random_state=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ujyXt7kyGvCA"
      },
      "outputs": [],
      "source": [
        "training_shuffled2,labelsForTrain_shuffled2=process_shuffle_dataset(shuffled2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B_z0MIxxG4ns",
        "outputId": "fdc23656-4e0b-407a-fdb0-b2acb436931d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 3s - loss: 16.2146 - mse: 16.2146 - mae: 1.6847 - val_loss: 15.7815 - val_mse: 15.7815 - val_mae: 1.6218 - lr: 3.4778e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 15.4812 - mse: 15.4812 - mae: 1.6125 - val_loss: 15.6420 - val_mse: 15.6420 - val_mae: 1.6365 - lr: 3.4778e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 15.3792 - mse: 15.3792 - mae: 1.6073 - val_loss: 15.4634 - val_mse: 15.4634 - val_mae: 1.5639 - lr: 3.4778e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 15.2884 - mse: 15.2884 - mae: 1.6018 - val_loss: 15.3333 - val_mse: 15.3333 - val_mae: 1.5757 - lr: 3.4778e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 15.2079 - mse: 15.2079 - mae: 1.5985 - val_loss: 15.2733 - val_mse: 15.2733 - val_mae: 1.6433 - lr: 3.4778e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 15.1718 - mse: 15.1718 - mae: 1.5928 - val_loss: 15.2233 - val_mse: 15.2233 - val_mae: 1.6019 - lr: 3.4778e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 15.1441 - mse: 15.1441 - mae: 1.5910 - val_loss: 15.2783 - val_mse: 15.2783 - val_mae: 1.5701 - lr: 3.4778e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 2s - loss: 15.1173 - mse: 15.1173 - mae: 1.5888 - val_loss: 15.1093 - val_mse: 15.1093 - val_mae: 1.5649 - lr: 3.4778e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 2s - loss: 15.0847 - mse: 15.0847 - mae: 1.5893 - val_loss: 15.2122 - val_mse: 15.2122 - val_mae: 1.5846 - lr: 3.4778e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 2s - loss: 15.0798 - mse: 15.0798 - mae: 1.5880 - val_loss: 15.1231 - val_mse: 15.1231 - val_mae: 1.5780 - lr: 3.4778e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 2s - loss: 15.0521 - mse: 15.0521 - mae: 1.5892 - val_loss: 15.1238 - val_mse: 15.1238 - val_mae: 1.5757 - lr: 3.4778e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 2s - loss: 15.0428 - mse: 15.0428 - mae: 1.5875 - val_loss: 15.0878 - val_mse: 15.0878 - val_mae: 1.5728 - lr: 3.4778e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 13/100\n",
            "1000/1000 - 2s - loss: 15.0516 - mse: 15.0516 - mae: 1.5871 - val_loss: 15.1311 - val_mse: 15.1311 - val_mae: 1.5847 - lr: 3.4778e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 14/100\n",
            "1000/1000 - 2s - loss: 15.0384 - mse: 15.0384 - mae: 1.5877 - val_loss: 15.0531 - val_mse: 15.0531 - val_mae: 1.5912 - lr: 3.4778e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 15/100\n",
            "1000/1000 - 2s - loss: 15.0110 - mse: 15.0110 - mae: 1.5873 - val_loss: 15.1296 - val_mse: 15.1296 - val_mae: 1.5592 - lr: 3.4778e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 16/100\n",
            "1000/1000 - 2s - loss: 15.0087 - mse: 15.0087 - mae: 1.5808 - val_loss: 15.0817 - val_mse: 15.0817 - val_mae: 1.5953 - lr: 3.4778e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 17/100\n",
            "1000/1000 - 2s - loss: 15.0039 - mse: 15.0039 - mae: 1.5844 - val_loss: 15.1148 - val_mse: 15.1148 - val_mae: 1.5735 - lr: 3.4778e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 18/100\n",
            "1000/1000 - 2s - loss: 14.9928 - mse: 14.9928 - mae: 1.5837 - val_loss: 15.0625 - val_mse: 15.0625 - val_mae: 1.6133 - lr: 3.4778e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 19/100\n",
            "1000/1000 - 2s - loss: 14.9917 - mse: 14.9917 - mae: 1.5841 - val_loss: 15.0210 - val_mse: 15.0210 - val_mae: 1.5724 - lr: 3.4778e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 20/100\n",
            "1000/1000 - 2s - loss: 14.9757 - mse: 14.9757 - mae: 1.5811 - val_loss: 15.0903 - val_mse: 15.0903 - val_mae: 1.5631 - lr: 3.4778e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 21/100\n",
            "1000/1000 - 2s - loss: 14.9907 - mse: 14.9907 - mae: 1.5805 - val_loss: 14.9944 - val_mse: 14.9944 - val_mae: 1.6281 - lr: 3.4778e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 22/100\n",
            "1000/1000 - 2s - loss: 14.9702 - mse: 14.9702 - mae: 1.5832 - val_loss: 15.0134 - val_mse: 15.0134 - val_mae: 1.5736 - lr: 3.4778e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 23/100\n",
            "1000/1000 - 2s - loss: 14.9351 - mse: 14.9351 - mae: 1.5842 - val_loss: 14.9940 - val_mse: 14.9940 - val_mae: 1.5906 - lr: 3.4778e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 24/100\n",
            "1000/1000 - 2s - loss: 14.9432 - mse: 14.9432 - mae: 1.5821 - val_loss: 14.9985 - val_mse: 14.9985 - val_mae: 1.5980 - lr: 3.4778e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 25/100\n",
            "1000/1000 - 2s - loss: 14.9150 - mse: 14.9150 - mae: 1.5850 - val_loss: 15.0861 - val_mse: 15.0861 - val_mae: 1.5556 - lr: 3.4778e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 26/100\n",
            "1000/1000 - 2s - loss: 14.9383 - mse: 14.9383 - mae: 1.5806 - val_loss: 15.0168 - val_mse: 15.0168 - val_mae: 1.5913 - lr: 3.4778e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 27/100\n",
            "1000/1000 - 2s - loss: 14.9379 - mse: 14.9379 - mae: 1.5814 - val_loss: 14.9567 - val_mse: 14.9567 - val_mae: 1.5827 - lr: 3.4778e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 28/100\n",
            "1000/1000 - 2s - loss: 14.9147 - mse: 14.9147 - mae: 1.5781 - val_loss: 14.9497 - val_mse: 14.9497 - val_mae: 1.5912 - lr: 3.4778e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 29/100\n",
            "1000/1000 - 2s - loss: 14.9195 - mse: 14.9195 - mae: 1.5808 - val_loss: 14.9471 - val_mse: 14.9471 - val_mae: 1.5866 - lr: 3.4778e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 30/100\n",
            "1000/1000 - 2s - loss: 14.9136 - mse: 14.9136 - mae: 1.5801 - val_loss: 15.0536 - val_mse: 15.0536 - val_mae: 1.5899 - lr: 3.4778e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 31/100\n",
            "1000/1000 - 2s - loss: 14.8967 - mse: 14.8967 - mae: 1.5803 - val_loss: 15.0698 - val_mse: 15.0698 - val_mae: 1.5582 - lr: 3.4778e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 32/100\n",
            "1000/1000 - 2s - loss: 14.9037 - mse: 14.9037 - mae: 1.5791 - val_loss: 14.9054 - val_mse: 14.9054 - val_mae: 1.5782 - lr: 3.4778e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 33/100\n",
            "1000/1000 - 2s - loss: 14.8761 - mse: 14.8761 - mae: 1.5770 - val_loss: 14.9383 - val_mse: 14.9383 - val_mae: 1.6191 - lr: 3.4778e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 34/100\n",
            "1000/1000 - 2s - loss: 14.9067 - mse: 14.9067 - mae: 1.5776 - val_loss: 14.9131 - val_mse: 14.9131 - val_mae: 1.5970 - lr: 3.4778e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 35/100\n",
            "1000/1000 - 2s - loss: 14.8841 - mse: 14.8841 - mae: 1.5760 - val_loss: 15.0452 - val_mse: 15.0452 - val_mae: 1.6133 - lr: 3.4778e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 36/100\n",
            "1000/1000 - 2s - loss: 14.8775 - mse: 14.8775 - mae: 1.5767 - val_loss: 14.9762 - val_mse: 14.9762 - val_mae: 1.6062 - lr: 3.4778e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 37/100\n",
            "1000/1000 - 2s - loss: 14.8812 - mse: 14.8812 - mae: 1.5791 - val_loss: 14.8456 - val_mse: 14.8456 - val_mae: 1.6288 - lr: 3.4778e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 38/100\n",
            "1000/1000 - 2s - loss: 14.8368 - mse: 14.8368 - mae: 1.5770 - val_loss: 14.8502 - val_mse: 14.8502 - val_mae: 1.5931 - lr: 3.4778e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 39/100\n",
            "1000/1000 - 2s - loss: 14.8328 - mse: 14.8328 - mae: 1.5782 - val_loss: 14.9172 - val_mse: 14.9172 - val_mae: 1.5878 - lr: 3.4778e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 40/100\n",
            "1000/1000 - 2s - loss: 14.8156 - mse: 14.8156 - mae: 1.5744 - val_loss: 14.9381 - val_mse: 14.9381 - val_mae: 1.5912 - lr: 3.4778e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 41/100\n",
            "1000/1000 - 2s - loss: 14.8120 - mse: 14.8120 - mae: 1.5768 - val_loss: 14.8743 - val_mse: 14.8743 - val_mae: 1.5820 - lr: 3.4778e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 42/100\n",
            "1000/1000 - 2s - loss: 14.8332 - mse: 14.8332 - mae: 1.5724 - val_loss: 14.8798 - val_mse: 14.8798 - val_mae: 1.6001 - lr: 3.4778e-04 - 2s/epoch - 2ms/step\n",
            "Score for fold 1: loss of 14.8798246383667\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 14.7838 - mse: 14.7838 - mae: 1.5786 - val_loss: 15.1503 - val_mse: 15.1503 - val_mae: 1.5656 - lr: 3.4778e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 14.7314 - mse: 14.7314 - mae: 1.5762 - val_loss: 15.1552 - val_mse: 15.1552 - val_mae: 1.5748 - lr: 3.4778e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 14.7465 - mse: 14.7465 - mae: 1.5727 - val_loss: 15.1612 - val_mse: 15.1612 - val_mae: 1.5941 - lr: 3.4778e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 14.7525 - mse: 14.7525 - mae: 1.5743 - val_loss: 15.1479 - val_mse: 15.1479 - val_mae: 1.5816 - lr: 3.4778e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 14.7236 - mse: 14.7236 - mae: 1.5759 - val_loss: 15.2800 - val_mse: 15.2800 - val_mae: 1.5563 - lr: 3.4778e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 14.7131 - mse: 14.7131 - mae: 1.5751 - val_loss: 15.1841 - val_mse: 15.1841 - val_mae: 1.5534 - lr: 3.4778e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 14.6588 - mse: 14.6588 - mae: 1.5729 - val_loss: 15.2773 - val_mse: 15.2773 - val_mae: 1.6041 - lr: 3.4778e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 2s - loss: 14.7179 - mse: 14.7179 - mae: 1.5745 - val_loss: 15.2968 - val_mse: 15.2968 - val_mae: 1.5697 - lr: 3.4778e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 2s - loss: 14.6964 - mse: 14.6964 - mae: 1.5747 - val_loss: 15.1492 - val_mse: 15.1492 - val_mae: 1.6019 - lr: 3.4778e-04 - 2s/epoch - 2ms/step\n",
            "Score for fold 2: loss of 15.149221420288086\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 14.9910 - mse: 14.9910 - mae: 1.5629 - val_loss: 14.4965 - val_mse: 14.4965 - val_mae: 1.5889 - lr: 3.4778e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 14.9829 - mse: 14.9829 - mae: 1.5572 - val_loss: 14.0942 - val_mse: 14.0942 - val_mae: 1.6277 - lr: 3.4778e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 14.9538 - mse: 14.9538 - mae: 1.5571 - val_loss: 14.1478 - val_mse: 14.1478 - val_mae: 1.6608 - lr: 3.4778e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 14.9359 - mse: 14.9359 - mae: 1.5582 - val_loss: 14.0445 - val_mse: 14.0445 - val_mae: 1.6026 - lr: 3.4778e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 14.9358 - mse: 14.9358 - mae: 1.5556 - val_loss: 14.1628 - val_mse: 14.1628 - val_mae: 1.6224 - lr: 3.4778e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 14.9331 - mse: 14.9331 - mae: 1.5577 - val_loss: 14.0976 - val_mse: 14.0976 - val_mae: 1.5857 - lr: 3.4778e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 14.9179 - mse: 14.9179 - mae: 1.5550 - val_loss: 14.1223 - val_mse: 14.1223 - val_mae: 1.5786 - lr: 3.4778e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 2s - loss: 14.9079 - mse: 14.9079 - mae: 1.5574 - val_loss: 14.0246 - val_mse: 14.0246 - val_mae: 1.6084 - lr: 3.4778e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 2s - loss: 14.9305 - mse: 14.9305 - mae: 1.5583 - val_loss: 13.9812 - val_mse: 13.9812 - val_mae: 1.6058 - lr: 3.4778e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 2s - loss: 14.9131 - mse: 14.9131 - mae: 1.5577 - val_loss: 14.0295 - val_mse: 14.0295 - val_mae: 1.6081 - lr: 3.4778e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 2s - loss: 14.8871 - mse: 14.8871 - mae: 1.5564 - val_loss: 14.0105 - val_mse: 14.0105 - val_mae: 1.6470 - lr: 3.4778e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 2s - loss: 14.9049 - mse: 14.9049 - mae: 1.5534 - val_loss: 14.0264 - val_mse: 14.0264 - val_mae: 1.6360 - lr: 3.4778e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 13/100\n",
            "1000/1000 - 2s - loss: 14.9239 - mse: 14.9239 - mae: 1.5531 - val_loss: 14.0419 - val_mse: 14.0419 - val_mae: 1.6223 - lr: 3.4778e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 14/100\n",
            "1000/1000 - 2s - loss: 14.9296 - mse: 14.9296 - mae: 1.5505 - val_loss: 14.1781 - val_mse: 14.1781 - val_mae: 1.6056 - lr: 3.4778e-04 - 2s/epoch - 2ms/step\n",
            "Score for fold 3: loss of 14.178133964538574\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 13.7593 - mse: 13.7593 - mae: 1.5721 - val_loss: 18.6302 - val_mse: 18.6302 - val_mae: 1.5453 - lr: 3.4778e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 13.7246 - mse: 13.7246 - mae: 1.5691 - val_loss: 18.7439 - val_mse: 18.7439 - val_mae: 1.5544 - lr: 3.4778e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 13.7311 - mse: 13.7311 - mae: 1.5669 - val_loss: 18.6939 - val_mse: 18.6939 - val_mae: 1.5528 - lr: 3.4778e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 13.7026 - mse: 13.7026 - mae: 1.5661 - val_loss: 18.8462 - val_mse: 18.8462 - val_mae: 1.5416 - lr: 3.4778e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 13.6815 - mse: 13.6815 - mae: 1.5656 - val_loss: 18.8102 - val_mse: 18.8102 - val_mae: 1.5525 - lr: 3.4778e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 13.6784 - mse: 13.6784 - mae: 1.5639 - val_loss: 18.9051 - val_mse: 18.9051 - val_mae: 1.5211 - lr: 3.4778e-04 - 2s/epoch - 2ms/step\n",
            "Score for fold 4: loss of 18.905136108398438\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 15.6498 - mse: 15.6498 - mae: 1.5743 - val_loss: 11.3653 - val_mse: 11.3653 - val_mae: 1.5133 - lr: 3.4778e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 15.6289 - mse: 15.6289 - mae: 1.5730 - val_loss: 11.1627 - val_mse: 11.1627 - val_mae: 1.5309 - lr: 3.4778e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 15.6235 - mse: 15.6235 - mae: 1.5749 - val_loss: 11.1389 - val_mse: 11.1389 - val_mae: 1.5735 - lr: 3.4778e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 15.6332 - mse: 15.6332 - mae: 1.5713 - val_loss: 11.1410 - val_mse: 11.1410 - val_mae: 1.5479 - lr: 3.4778e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 15.6025 - mse: 15.6025 - mae: 1.5737 - val_loss: 11.0821 - val_mse: 11.0821 - val_mae: 1.5745 - lr: 3.4778e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 15.5701 - mse: 15.5701 - mae: 1.5698 - val_loss: 11.2551 - val_mse: 11.2551 - val_mae: 1.5296 - lr: 3.4778e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 15.5955 - mse: 15.5955 - mae: 1.5719 - val_loss: 11.1866 - val_mse: 11.1866 - val_mae: 1.5140 - lr: 3.4778e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 2s - loss: 15.5928 - mse: 15.5928 - mae: 1.5677 - val_loss: 11.2825 - val_mse: 11.2825 - val_mae: 1.5402 - lr: 3.4778e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 2s - loss: 15.5461 - mse: 15.5461 - mae: 1.5714 - val_loss: 11.6058 - val_mse: 11.6058 - val_mae: 1.4814 - lr: 3.4778e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 2s - loss: 15.5599 - mse: 15.5599 - mae: 1.5679 - val_loss: 11.1954 - val_mse: 11.1954 - val_mae: 1.5520 - lr: 3.4778e-04 - 2s/epoch - 2ms/step\n",
            "Score for fold 5: loss of 11.195423126220703\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-10-14 20:32:34,883]\u001b[0m Finished trial#0 resulted in value: 14.864. Current best value is 14.864 with parameters: {'activation': 'tanh', 'num_hidden_layer': 4, 'i': 6, 'learning_rate': 0.0003477839020306978}.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 3s - loss: 16.5482 - mse: 16.5482 - mae: 1.6479 - val_loss: 12.1395 - val_mse: 12.1395 - val_mae: 1.5700 - lr: 4.9576e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 15.9349 - mse: 15.9349 - mae: 1.5993 - val_loss: 12.0229 - val_mse: 12.0229 - val_mae: 1.5408 - lr: 4.9576e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 15.8630 - mse: 15.8630 - mae: 1.5932 - val_loss: 11.9242 - val_mse: 11.9242 - val_mae: 1.6121 - lr: 4.9576e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 15.8001 - mse: 15.8001 - mae: 1.5948 - val_loss: 11.9091 - val_mse: 11.9091 - val_mae: 1.5533 - lr: 4.9576e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 15.7582 - mse: 15.7582 - mae: 1.5938 - val_loss: 11.8464 - val_mse: 11.8464 - val_mae: 1.5461 - lr: 4.9576e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 15.7378 - mse: 15.7378 - mae: 1.5886 - val_loss: 11.7748 - val_mse: 11.7748 - val_mae: 1.6137 - lr: 4.9576e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 15.6949 - mse: 15.6949 - mae: 1.5879 - val_loss: 11.7674 - val_mse: 11.7674 - val_mae: 1.6003 - lr: 4.9576e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 2s - loss: 15.6140 - mse: 15.6140 - mae: 1.5841 - val_loss: 11.8977 - val_mse: 11.8977 - val_mae: 1.5699 - lr: 4.9576e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 2s - loss: 15.6476 - mse: 15.6476 - mae: 1.5830 - val_loss: 11.7269 - val_mse: 11.7269 - val_mae: 1.5867 - lr: 4.9576e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 2s - loss: 15.6374 - mse: 15.6374 - mae: 1.5824 - val_loss: 11.9499 - val_mse: 11.9499 - val_mae: 1.4904 - lr: 4.9576e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 2s - loss: 15.6512 - mse: 15.6512 - mae: 1.5774 - val_loss: 11.7837 - val_mse: 11.7837 - val_mae: 1.5444 - lr: 4.9576e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 2s - loss: 15.6190 - mse: 15.6190 - mae: 1.5750 - val_loss: 11.8601 - val_mse: 11.8601 - val_mae: 1.6411 - lr: 4.9576e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 13/100\n",
            "1000/1000 - 2s - loss: 15.5553 - mse: 15.5553 - mae: 1.5782 - val_loss: 11.8665 - val_mse: 11.8665 - val_mae: 1.5805 - lr: 4.9576e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 14/100\n",
            "1000/1000 - 2s - loss: 15.6125 - mse: 15.6125 - mae: 1.5793 - val_loss: 11.8143 - val_mse: 11.8143 - val_mae: 1.6241 - lr: 4.9576e-04 - 2s/epoch - 2ms/step\n",
            "Score for fold 1: loss of 11.81429386138916\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 16.0393 - mse: 16.0393 - mae: 1.5834 - val_loss: 10.2506 - val_mse: 10.2506 - val_mae: 1.5587 - lr: 4.9576e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 15.9539 - mse: 15.9539 - mae: 1.5834 - val_loss: 10.3700 - val_mse: 10.3700 - val_mae: 1.5290 - lr: 4.9576e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 15.9944 - mse: 15.9944 - mae: 1.5815 - val_loss: 10.5208 - val_mse: 10.5208 - val_mae: 1.5079 - lr: 4.9576e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 15.9167 - mse: 15.9167 - mae: 1.5819 - val_loss: 10.5152 - val_mse: 10.5152 - val_mae: 1.5849 - lr: 4.9576e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 15.9371 - mse: 15.9371 - mae: 1.5826 - val_loss: 10.5449 - val_mse: 10.5449 - val_mae: 1.5309 - lr: 4.9576e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 15.8837 - mse: 15.8837 - mae: 1.5768 - val_loss: 10.8682 - val_mse: 10.8682 - val_mae: 1.4695 - lr: 4.9576e-04 - 2s/epoch - 2ms/step\n",
            "Score for fold 2: loss of 10.868194580078125\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 14.3647 - mse: 14.3647 - mae: 1.5669 - val_loss: 16.5995 - val_mse: 16.5995 - val_mae: 1.6732 - lr: 4.9576e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 14.3940 - mse: 14.3940 - mae: 1.5679 - val_loss: 16.0866 - val_mse: 16.0866 - val_mae: 1.6833 - lr: 4.9576e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 14.3152 - mse: 14.3152 - mae: 1.5625 - val_loss: 16.2543 - val_mse: 16.2543 - val_mae: 1.6359 - lr: 4.9576e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 14.3588 - mse: 14.3588 - mae: 1.5697 - val_loss: 16.5797 - val_mse: 16.5797 - val_mae: 1.5578 - lr: 4.9576e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 14.2864 - mse: 14.2864 - mae: 1.5671 - val_loss: 16.8744 - val_mse: 16.8744 - val_mae: 1.6635 - lr: 4.9576e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 14.2963 - mse: 14.2963 - mae: 1.5675 - val_loss: 16.6011 - val_mse: 16.6011 - val_mae: 1.5616 - lr: 4.9576e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 14.2971 - mse: 14.2971 - mae: 1.5648 - val_loss: 17.0976 - val_mse: 17.0976 - val_mae: 1.5948 - lr: 4.9576e-04 - 2s/epoch - 2ms/step\n",
            "Score for fold 3: loss of 17.097614288330078\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 12.9506 - mse: 12.9506 - mae: 1.5788 - val_loss: 22.3958 - val_mse: 22.3958 - val_mae: 1.6600 - lr: 4.9576e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 12.9628 - mse: 12.9628 - mae: 1.5687 - val_loss: 22.4323 - val_mse: 22.4323 - val_mae: 1.6262 - lr: 4.9576e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 12.8665 - mse: 12.8665 - mae: 1.5694 - val_loss: 22.4514 - val_mse: 22.4514 - val_mae: 1.5413 - lr: 4.9576e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 12.7550 - mse: 12.7550 - mae: 1.5692 - val_loss: 22.2935 - val_mse: 22.2935 - val_mae: 1.5664 - lr: 4.9576e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 12.6086 - mse: 12.6086 - mae: 1.5644 - val_loss: 22.2593 - val_mse: 22.2593 - val_mae: 1.5946 - lr: 4.9576e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 12.7419 - mse: 12.7419 - mae: 1.5691 - val_loss: 22.5115 - val_mse: 22.5115 - val_mae: 1.5241 - lr: 4.9576e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 12.5935 - mse: 12.5935 - mae: 1.5720 - val_loss: 22.4266 - val_mse: 22.4266 - val_mae: 1.5685 - lr: 4.9576e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 2s - loss: 12.6782 - mse: 12.6782 - mae: 1.5697 - val_loss: 22.5301 - val_mse: 22.5301 - val_mae: 1.7235 - lr: 4.9576e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 2s - loss: 12.6600 - mse: 12.6600 - mae: 1.5689 - val_loss: 22.4276 - val_mse: 22.4276 - val_mae: 1.7153 - lr: 4.9576e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 2s - loss: 12.5152 - mse: 12.5152 - mae: 1.5597 - val_loss: 22.2964 - val_mse: 22.2964 - val_mae: 1.6141 - lr: 4.9576e-04 - 2s/epoch - 2ms/step\n",
            "Score for fold 4: loss of 22.296369552612305\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 14.9098 - mse: 14.9098 - mae: 1.5769 - val_loss: 12.9829 - val_mse: 12.9829 - val_mae: 1.5090 - lr: 4.9576e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 14.9526 - mse: 14.9526 - mae: 1.5806 - val_loss: 12.9973 - val_mse: 12.9973 - val_mae: 1.5392 - lr: 4.9576e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 14.9356 - mse: 14.9356 - mae: 1.5782 - val_loss: 13.1801 - val_mse: 13.1801 - val_mae: 1.5449 - lr: 4.9576e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 14.7267 - mse: 14.7267 - mae: 1.5780 - val_loss: 13.1707 - val_mse: 13.1707 - val_mae: 1.5070 - lr: 4.9576e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 14.7777 - mse: 14.7777 - mae: 1.5753 - val_loss: 13.2653 - val_mse: 13.2653 - val_mae: 1.6109 - lr: 4.9576e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 14.7459 - mse: 14.7459 - mae: 1.5688 - val_loss: 13.4655 - val_mse: 13.4655 - val_mae: 1.5439 - lr: 4.9576e-04 - 2s/epoch - 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-10-14 20:33:58,639]\u001b[0m Finished trial#1 resulted in value: 15.11. Current best value is 14.864 with parameters: {'activation': 'tanh', 'num_hidden_layer': 4, 'i': 6, 'learning_rate': 0.0003477839020306978}.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Score for fold 5: loss of 13.465463638305664\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 11s - loss: 14.6271 - mse: 14.6271 - mae: 1.7276 - val_loss: 23.3357 - val_mse: 23.3357 - val_mae: 2.1713 - lr: 0.0013 - 11s/epoch - 11ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 10s - loss: 14.3222 - mse: 14.3222 - mae: 1.6913 - val_loss: 22.1833 - val_mse: 22.1833 - val_mae: 1.8103 - lr: 0.0013 - 10s/epoch - 10ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 9s - loss: 14.2439 - mse: 14.2439 - mae: 1.7082 - val_loss: 22.5607 - val_mse: 22.5607 - val_mae: 1.5195 - lr: 0.0013 - 9s/epoch - 9ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 9s - loss: 14.1643 - mse: 14.1643 - mae: 1.7178 - val_loss: 23.6399 - val_mse: 23.6399 - val_mae: 1.7459 - lr: 0.0013 - 9s/epoch - 9ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 9s - loss: 14.0643 - mse: 14.0643 - mae: 1.7008 - val_loss: 21.9044 - val_mse: 21.9044 - val_mae: 1.7154 - lr: 0.0013 - 9s/epoch - 9ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 9s - loss: 13.9937 - mse: 13.9937 - mae: 1.6784 - val_loss: 23.4275 - val_mse: 23.4275 - val_mae: 1.5157 - lr: 0.0013 - 9s/epoch - 9ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 9s - loss: 13.9166 - mse: 13.9166 - mae: 1.6897 - val_loss: 21.6904 - val_mse: 21.6904 - val_mae: 1.6379 - lr: 0.0013 - 9s/epoch - 9ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 10s - loss: 14.0367 - mse: 14.0367 - mae: 1.6955 - val_loss: 22.0171 - val_mse: 22.0171 - val_mae: 1.7406 - lr: 0.0013 - 10s/epoch - 10ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 9s - loss: 13.9818 - mse: 13.9818 - mae: 1.6812 - val_loss: 21.9900 - val_mse: 21.9900 - val_mae: 1.6671 - lr: 0.0013 - 9s/epoch - 9ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 9s - loss: 13.9291 - mse: 13.9291 - mae: 1.6696 - val_loss: 22.1923 - val_mse: 22.1923 - val_mae: 2.0136 - lr: 0.0013 - 9s/epoch - 9ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 9s - loss: 13.8813 - mse: 13.8813 - mae: 1.6779 - val_loss: 22.1842 - val_mse: 22.1842 - val_mae: 1.6645 - lr: 0.0013 - 9s/epoch - 9ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 9s - loss: 13.9295 - mse: 13.9295 - mae: 1.6889 - val_loss: 21.6530 - val_mse: 21.6530 - val_mae: 1.7497 - lr: 0.0013 - 9s/epoch - 9ms/step\n",
            "Epoch 13/100\n",
            "1000/1000 - 9s - loss: 13.8908 - mse: 13.8908 - mae: 1.6825 - val_loss: 21.8568 - val_mse: 21.8568 - val_mae: 1.5502 - lr: 0.0013 - 9s/epoch - 9ms/step\n",
            "Epoch 14/100\n",
            "1000/1000 - 9s - loss: 13.8199 - mse: 13.8199 - mae: 1.6790 - val_loss: 21.9286 - val_mse: 21.9286 - val_mae: 1.5160 - lr: 0.0013 - 9s/epoch - 9ms/step\n",
            "Epoch 15/100\n",
            "1000/1000 - 10s - loss: 13.7422 - mse: 13.7422 - mae: 1.6759 - val_loss: 21.9960 - val_mse: 21.9960 - val_mae: 1.8002 - lr: 0.0013 - 10s/epoch - 10ms/step\n",
            "Epoch 16/100\n",
            "1000/1000 - 10s - loss: 13.9283 - mse: 13.9283 - mae: 1.6903 - val_loss: 22.5711 - val_mse: 22.5711 - val_mae: 1.5379 - lr: 0.0013 - 10s/epoch - 10ms/step\n",
            "Epoch 17/100\n",
            "1000/1000 - 10s - loss: 13.8565 - mse: 13.8565 - mae: 1.6758 - val_loss: 22.1908 - val_mse: 22.1908 - val_mae: 1.5238 - lr: 0.0013 - 10s/epoch - 10ms/step\n",
            "Score for fold 1: loss of 22.190799713134766\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 9s - loss: 16.0584 - mse: 16.0584 - mae: 1.6814 - val_loss: 12.6872 - val_mse: 12.6872 - val_mae: 1.5330 - lr: 0.0010 - 9s/epoch - 9ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 10s - loss: 15.9598 - mse: 15.9598 - mae: 1.6773 - val_loss: 12.6548 - val_mse: 12.6548 - val_mae: 1.6191 - lr: 0.0010 - 10s/epoch - 10ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 10s - loss: 16.0195 - mse: 16.0195 - mae: 1.6904 - val_loss: 12.8260 - val_mse: 12.8260 - val_mae: 1.4907 - lr: 0.0010 - 10s/epoch - 10ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 10s - loss: 16.0636 - mse: 16.0636 - mae: 1.6916 - val_loss: 12.6694 - val_mse: 12.6694 - val_mae: 1.6580 - lr: 0.0010 - 10s/epoch - 10ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 10s - loss: 16.1437 - mse: 16.1437 - mae: 1.7003 - val_loss: 13.1711 - val_mse: 13.1711 - val_mae: 1.9153 - lr: 0.0010 - 10s/epoch - 10ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 10s - loss: 16.1314 - mse: 16.1314 - mae: 1.7039 - val_loss: 12.9991 - val_mse: 12.9991 - val_mae: 1.4592 - lr: 0.0010 - 10s/epoch - 10ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 10s - loss: 16.2078 - mse: 16.2078 - mae: 1.6884 - val_loss: 12.7414 - val_mse: 12.7414 - val_mae: 1.5152 - lr: 0.0010 - 10s/epoch - 10ms/step\n",
            "Score for fold 2: loss of 12.741448402404785\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 11s - loss: 16.3786 - mse: 16.3786 - mae: 1.6925 - val_loss: 11.7044 - val_mse: 11.7044 - val_mae: 1.7837 - lr: 0.0010 - 11s/epoch - 11ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 11s - loss: 16.4141 - mse: 16.4141 - mae: 1.6785 - val_loss: 11.4406 - val_mse: 11.4406 - val_mae: 1.5538 - lr: 0.0010 - 11s/epoch - 11ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 10s - loss: 16.4889 - mse: 16.4889 - mae: 1.6931 - val_loss: 11.8485 - val_mse: 11.8485 - val_mae: 1.7169 - lr: 0.0010 - 10s/epoch - 10ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 10s - loss: 16.3095 - mse: 16.3095 - mae: 1.6654 - val_loss: 11.6364 - val_mse: 11.6364 - val_mae: 1.6071 - lr: 0.0010 - 10s/epoch - 10ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 10s - loss: 16.3662 - mse: 16.3662 - mae: 1.6714 - val_loss: 11.4650 - val_mse: 11.4650 - val_mae: 1.5484 - lr: 0.0010 - 10s/epoch - 10ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 10s - loss: 16.3824 - mse: 16.3824 - mae: 1.6802 - val_loss: 11.9992 - val_mse: 11.9992 - val_mae: 1.8343 - lr: 0.0010 - 10s/epoch - 10ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 10s - loss: 16.3980 - mse: 16.3980 - mae: 1.6926 - val_loss: 11.9585 - val_mse: 11.9585 - val_mae: 1.8181 - lr: 0.0010 - 10s/epoch - 10ms/step\n",
            "Score for fold 3: loss of 11.958462715148926\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 11s - loss: 15.8258 - mse: 15.8258 - mae: 1.6479 - val_loss: 13.5163 - val_mse: 13.5163 - val_mae: 1.5362 - lr: 0.0010 - 11s/epoch - 11ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 10s - loss: 15.9293 - mse: 15.9293 - mae: 1.6679 - val_loss: 13.8538 - val_mse: 13.8538 - val_mae: 1.6210 - lr: 0.0010 - 10s/epoch - 10ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 10s - loss: 15.9304 - mse: 15.9304 - mae: 1.6702 - val_loss: 13.3841 - val_mse: 13.3841 - val_mae: 1.6680 - lr: 0.0010 - 10s/epoch - 10ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 10s - loss: 15.9152 - mse: 15.9152 - mae: 1.6666 - val_loss: 13.9906 - val_mse: 13.9906 - val_mae: 1.9664 - lr: 0.0010 - 10s/epoch - 10ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 10s - loss: 15.9722 - mse: 15.9722 - mae: 1.6791 - val_loss: 13.3736 - val_mse: 13.3736 - val_mae: 1.8065 - lr: 0.0010 - 10s/epoch - 10ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 10s - loss: 16.0206 - mse: 16.0206 - mae: 1.6863 - val_loss: 13.4295 - val_mse: 13.4295 - val_mae: 1.6178 - lr: 0.0010 - 10s/epoch - 10ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 10s - loss: 15.9050 - mse: 15.9050 - mae: 1.6584 - val_loss: 14.1281 - val_mse: 14.1281 - val_mae: 1.5016 - lr: 0.0010 - 10s/epoch - 10ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 11s - loss: 15.8104 - mse: 15.8104 - mae: 1.6567 - val_loss: 14.0739 - val_mse: 14.0739 - val_mae: 1.6825 - lr: 0.0010 - 11s/epoch - 11ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 11s - loss: 15.9766 - mse: 15.9766 - mae: 1.6924 - val_loss: 13.9805 - val_mse: 13.9805 - val_mae: 1.8537 - lr: 0.0010 - 11s/epoch - 11ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 11s - loss: 15.9078 - mse: 15.9078 - mae: 1.6813 - val_loss: 13.3916 - val_mse: 13.3916 - val_mae: 1.6151 - lr: 0.0010 - 11s/epoch - 11ms/step\n",
            "Score for fold 4: loss of 13.391632080078125\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 11s - loss: 15.2664 - mse: 15.2664 - mae: 1.6658 - val_loss: 16.8180 - val_mse: 16.8180 - val_mae: 1.5197 - lr: 0.0010 - 11s/epoch - 11ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 11s - loss: 15.1230 - mse: 15.1230 - mae: 1.6676 - val_loss: 16.5284 - val_mse: 16.5284 - val_mae: 1.6514 - lr: 0.0010 - 11s/epoch - 11ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 11s - loss: 15.1452 - mse: 15.1452 - mae: 1.6670 - val_loss: 16.7241 - val_mse: 16.7241 - val_mae: 1.5272 - lr: 0.0010 - 11s/epoch - 11ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 10s - loss: 15.2471 - mse: 15.2471 - mae: 1.6720 - val_loss: 16.7597 - val_mse: 16.7597 - val_mae: 1.6096 - lr: 0.0010 - 10s/epoch - 10ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 11s - loss: 15.0799 - mse: 15.0799 - mae: 1.6730 - val_loss: 17.2332 - val_mse: 17.2332 - val_mae: 1.5130 - lr: 0.0010 - 11s/epoch - 11ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 10s - loss: 15.0690 - mse: 15.0690 - mae: 1.6560 - val_loss: 16.3963 - val_mse: 16.3963 - val_mae: 1.6224 - lr: 0.0010 - 10s/epoch - 10ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 11s - loss: 15.1161 - mse: 15.1161 - mae: 1.6691 - val_loss: 16.8986 - val_mse: 16.8986 - val_mae: 1.8857 - lr: 0.0010 - 11s/epoch - 11ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 11s - loss: 15.1147 - mse: 15.1147 - mae: 1.6707 - val_loss: 16.3811 - val_mse: 16.3811 - val_mae: 1.7001 - lr: 0.0010 - 11s/epoch - 11ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 11s - loss: 15.0984 - mse: 15.0984 - mae: 1.6670 - val_loss: 16.5701 - val_mse: 16.5701 - val_mae: 1.5796 - lr: 0.0010 - 11s/epoch - 11ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 11s - loss: 15.1367 - mse: 15.1367 - mae: 1.6665 - val_loss: 16.9014 - val_mse: 16.9014 - val_mae: 1.5317 - lr: 0.0010 - 11s/epoch - 11ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 11s - loss: 15.0935 - mse: 15.0935 - mae: 1.6575 - val_loss: 16.7017 - val_mse: 16.7017 - val_mae: 1.8330 - lr: 0.0010 - 11s/epoch - 11ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 11s - loss: 15.2700 - mse: 15.2700 - mae: 1.6695 - val_loss: 16.3471 - val_mse: 16.3471 - val_mae: 1.7090 - lr: 0.0010 - 11s/epoch - 11ms/step\n",
            "Epoch 13/100\n",
            "1000/1000 - 12s - loss: 15.0476 - mse: 15.0476 - mae: 1.6586 - val_loss: 16.5099 - val_mse: 16.5099 - val_mae: 1.6236 - lr: 0.0010 - 12s/epoch - 12ms/step\n",
            "Epoch 14/100\n",
            "1000/1000 - 11s - loss: 15.2411 - mse: 15.2411 - mae: 1.6649 - val_loss: 16.8870 - val_mse: 16.8870 - val_mae: 1.5431 - lr: 0.0010 - 11s/epoch - 11ms/step\n",
            "Epoch 15/100\n",
            "1000/1000 - 11s - loss: 15.1393 - mse: 15.1393 - mae: 1.6940 - val_loss: 16.4329 - val_mse: 16.4329 - val_mae: 1.6800 - lr: 0.0010 - 11s/epoch - 11ms/step\n",
            "Epoch 16/100\n",
            "1000/1000 - 12s - loss: 15.0946 - mse: 15.0946 - mae: 1.6666 - val_loss: 16.5915 - val_mse: 16.5915 - val_mae: 1.6280 - lr: 0.0010 - 12s/epoch - 12ms/step\n",
            "Epoch 17/100\n",
            "1000/1000 - 12s - loss: 15.0229 - mse: 15.0229 - mae: 1.6641 - val_loss: 16.4523 - val_mse: 16.4523 - val_mae: 1.8156 - lr: 0.0010 - 12s/epoch - 12ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-10-14 20:43:58,496]\u001b[0m Finished trial#2 resulted in value: 15.346. Current best value is 14.864 with parameters: {'activation': 'tanh', 'num_hidden_layer': 4, 'i': 6, 'learning_rate': 0.0003477839020306978}.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Score for fold 5: loss of 16.452253341674805\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 3s - loss: 14.0242 - mse: 14.0242 - mae: 1.6660 - val_loss: 24.0750 - val_mse: 24.0750 - val_mae: 1.7569 - lr: 0.0025 - 3s/epoch - 3ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 13.6760 - mse: 13.6760 - mae: 1.6458 - val_loss: 24.3464 - val_mse: 24.3464 - val_mae: 1.7011 - lr: 0.0025 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 13.7035 - mse: 13.7035 - mae: 1.6462 - val_loss: 24.0837 - val_mse: 24.0837 - val_mae: 1.7642 - lr: 0.0025 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 13.6244 - mse: 13.6244 - mae: 1.6366 - val_loss: 24.1904 - val_mse: 24.1904 - val_mae: 1.7144 - lr: 0.0025 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 13.6320 - mse: 13.6320 - mae: 1.6337 - val_loss: 24.1728 - val_mse: 24.1728 - val_mae: 1.6922 - lr: 0.0025 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 13.5928 - mse: 13.5928 - mae: 1.6316 - val_loss: 24.2225 - val_mse: 24.2225 - val_mae: 1.6564 - lr: 0.0025 - 2s/epoch - 2ms/step\n",
            "Score for fold 1: loss of 24.222431182861328\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 16.3185 - mse: 16.3185 - mae: 1.6405 - val_loss: 13.1336 - val_mse: 13.1336 - val_mae: 1.6370 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 16.2577 - mse: 16.2577 - mae: 1.6425 - val_loss: 13.1027 - val_mse: 13.1027 - val_mae: 1.6367 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 16.2692 - mse: 16.2692 - mae: 1.6482 - val_loss: 13.0909 - val_mse: 13.0909 - val_mae: 1.6376 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 16.3059 - mse: 16.3059 - mae: 1.6442 - val_loss: 13.1447 - val_mse: 13.1447 - val_mae: 1.5871 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 16.3036 - mse: 16.3036 - mae: 1.6471 - val_loss: 13.2180 - val_mse: 13.2180 - val_mae: 1.5626 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 16.3160 - mse: 16.3160 - mae: 1.6441 - val_loss: 13.1825 - val_mse: 13.1825 - val_mae: 1.7076 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 16.3382 - mse: 16.3382 - mae: 1.6414 - val_loss: 13.1390 - val_mse: 13.1390 - val_mae: 1.6841 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 2s - loss: 16.3286 - mse: 16.3286 - mae: 1.6449 - val_loss: 13.1148 - val_mse: 13.1148 - val_mae: 1.6002 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Score for fold 2: loss of 13.114806175231934\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 3s - loss: 17.0574 - mse: 17.0574 - mae: 1.6659 - val_loss: 10.2914 - val_mse: 10.2914 - val_mae: 1.5542 - lr: 0.0010 - 3s/epoch - 3ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 3s - loss: 17.0888 - mse: 17.0888 - mae: 1.6595 - val_loss: 10.2752 - val_mse: 10.2752 - val_mae: 1.6109 - lr: 0.0010 - 3s/epoch - 3ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 3s - loss: 17.1054 - mse: 17.1054 - mae: 1.6616 - val_loss: 10.3013 - val_mse: 10.3013 - val_mae: 1.5375 - lr: 0.0010 - 3s/epoch - 3ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 17.1241 - mse: 17.1241 - mae: 1.6576 - val_loss: 10.3155 - val_mse: 10.3155 - val_mae: 1.6486 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 17.1005 - mse: 17.1005 - mae: 1.6690 - val_loss: 10.2542 - val_mse: 10.2542 - val_mae: 1.5907 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 17.1383 - mse: 17.1383 - mae: 1.6657 - val_loss: 10.3194 - val_mse: 10.3194 - val_mae: 1.6532 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 17.0753 - mse: 17.0753 - mae: 1.6673 - val_loss: 10.3104 - val_mse: 10.3104 - val_mae: 1.5279 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 2s - loss: 17.1286 - mse: 17.1286 - mae: 1.6713 - val_loss: 10.3545 - val_mse: 10.3545 - val_mae: 1.6680 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 2s - loss: 17.1091 - mse: 17.1091 - mae: 1.6626 - val_loss: 10.2592 - val_mse: 10.2592 - val_mae: 1.6051 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 2s - loss: 17.1538 - mse: 17.1538 - mae: 1.6679 - val_loss: 10.3752 - val_mse: 10.3752 - val_mae: 1.6590 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Score for fold 3: loss of 10.375238418579102\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 15.5615 - mse: 15.5615 - mae: 1.6399 - val_loss: 16.6718 - val_mse: 16.6718 - val_mae: 1.6899 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 15.4659 - mse: 15.4659 - mae: 1.6400 - val_loss: 16.8225 - val_mse: 16.8225 - val_mae: 1.6221 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 15.6189 - mse: 15.6189 - mae: 1.6502 - val_loss: 16.7646 - val_mse: 16.7646 - val_mae: 1.6489 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 15.4095 - mse: 15.4095 - mae: 1.6330 - val_loss: 16.6628 - val_mse: 16.6628 - val_mae: 1.6637 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 15.5297 - mse: 15.5297 - mae: 1.6408 - val_loss: 17.4473 - val_mse: 17.4473 - val_mae: 1.5966 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 15.4768 - mse: 15.4768 - mae: 1.6366 - val_loss: 16.8348 - val_mse: 16.8348 - val_mae: 1.6329 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 15.5521 - mse: 15.5521 - mae: 1.6377 - val_loss: 16.8474 - val_mse: 16.8474 - val_mae: 1.6337 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 2s - loss: 15.5327 - mse: 15.5327 - mae: 1.6357 - val_loss: 16.8337 - val_mse: 16.8337 - val_mae: 1.6291 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 2s - loss: 15.4647 - mse: 15.4647 - mae: 1.6437 - val_loss: 18.1777 - val_mse: 18.1777 - val_mae: 1.5914 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Score for fold 4: loss of 18.17764663696289\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 16.2510 - mse: 16.2510 - mae: 1.6504 - val_loss: 13.8690 - val_mse: 13.8690 - val_mae: 1.6672 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 16.1810 - mse: 16.1810 - mae: 1.6427 - val_loss: 14.5776 - val_mse: 14.5776 - val_mae: 1.8830 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 16.1581 - mse: 16.1581 - mae: 1.6431 - val_loss: 13.8291 - val_mse: 13.8291 - val_mae: 1.7442 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 16.3033 - mse: 16.3033 - mae: 1.6403 - val_loss: 13.9808 - val_mse: 13.9808 - val_mae: 1.6357 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 16.2095 - mse: 16.2095 - mae: 1.6434 - val_loss: 13.8038 - val_mse: 13.8038 - val_mae: 1.7290 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 16.1770 - mse: 16.1770 - mae: 1.6449 - val_loss: 13.7971 - val_mse: 13.7971 - val_mae: 1.6732 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 16.1645 - mse: 16.1645 - mae: 1.6405 - val_loss: 13.8270 - val_mse: 13.8270 - val_mae: 1.7340 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 2s - loss: 16.2100 - mse: 16.2100 - mae: 1.6437 - val_loss: 13.8016 - val_mse: 13.8016 - val_mae: 1.7191 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 2s - loss: 16.1856 - mse: 16.1856 - mae: 1.6429 - val_loss: 14.3173 - val_mse: 14.3173 - val_mae: 1.5862 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 2s - loss: 16.2335 - mse: 16.2335 - mae: 1.6482 - val_loss: 13.8544 - val_mse: 13.8544 - val_mae: 1.6336 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 2s - loss: 16.1747 - mse: 16.1747 - mae: 1.6431 - val_loss: 14.1737 - val_mse: 14.1737 - val_mae: 1.6077 - lr: 0.0010 - 2s/epoch - 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-10-14 20:45:47,440]\u001b[0m Finished trial#3 resulted in value: 16.012. Current best value is 14.864 with parameters: {'activation': 'tanh', 'num_hidden_layer': 4, 'i': 6, 'learning_rate': 0.0003477839020306978}.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Score for fold 5: loss of 14.173736572265625\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 4s - loss: 16.0245 - mse: 16.0245 - mae: 1.7212 - val_loss: 16.1511 - val_mse: 16.1511 - val_mae: 1.5005 - lr: 0.0063 - 4s/epoch - 4ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 3s - loss: 15.6172 - mse: 15.6172 - mae: 1.6890 - val_loss: 15.6925 - val_mse: 15.6925 - val_mae: 1.6956 - lr: 0.0063 - 3s/epoch - 3ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 3s - loss: 15.5995 - mse: 15.5995 - mae: 1.7042 - val_loss: 16.1508 - val_mse: 16.1508 - val_mae: 1.5727 - lr: 0.0063 - 3s/epoch - 3ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 3s - loss: 15.5633 - mse: 15.5633 - mae: 1.6962 - val_loss: 15.8878 - val_mse: 15.8878 - val_mae: 1.6861 - lr: 0.0063 - 3s/epoch - 3ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 3s - loss: 15.6201 - mse: 15.6201 - mae: 1.7068 - val_loss: 15.8972 - val_mse: 15.8972 - val_mae: 1.6802 - lr: 0.0063 - 3s/epoch - 3ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 3s - loss: 15.6638 - mse: 15.6638 - mae: 1.7263 - val_loss: 16.6450 - val_mse: 16.6450 - val_mae: 2.1164 - lr: 0.0063 - 3s/epoch - 3ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 3s - loss: 15.5209 - mse: 15.5209 - mae: 1.7011 - val_loss: 15.9150 - val_mse: 15.9150 - val_mae: 1.7115 - lr: 0.0063 - 3s/epoch - 3ms/step\n",
            "Score for fold 1: loss of 15.915003776550293\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 3s - loss: 16.5964 - mse: 16.5964 - mae: 1.6403 - val_loss: 9.8002 - val_mse: 9.8002 - val_mae: 1.6227 - lr: 0.0013 - 3s/epoch - 3ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 3s - loss: 16.4605 - mse: 16.4605 - mae: 1.6329 - val_loss: 9.8609 - val_mse: 9.8609 - val_mae: 1.4969 - lr: 0.0013 - 3s/epoch - 3ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 3s - loss: 16.3988 - mse: 16.3988 - mae: 1.6323 - val_loss: 9.8949 - val_mse: 9.8949 - val_mae: 1.5978 - lr: 0.0013 - 3s/epoch - 3ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 3s - loss: 16.3457 - mse: 16.3457 - mae: 1.6280 - val_loss: 9.9112 - val_mse: 9.9112 - val_mae: 1.5259 - lr: 0.0013 - 3s/epoch - 3ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 3s - loss: 16.3343 - mse: 16.3343 - mae: 1.6250 - val_loss: 9.8939 - val_mse: 9.8939 - val_mae: 1.5834 - lr: 0.0013 - 3s/epoch - 3ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 3s - loss: 16.2548 - mse: 16.2548 - mae: 1.6280 - val_loss: 9.8450 - val_mse: 9.8450 - val_mae: 1.5746 - lr: 0.0013 - 3s/epoch - 3ms/step\n",
            "Score for fold 2: loss of 9.844959259033203\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 3s - loss: 13.3294 - mse: 13.3294 - mae: 1.6000 - val_loss: 21.6900 - val_mse: 21.6900 - val_mae: 1.5777 - lr: 0.0010 - 3s/epoch - 3ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 3s - loss: 13.2360 - mse: 13.2360 - mae: 1.5951 - val_loss: 21.4887 - val_mse: 21.4887 - val_mae: 1.6066 - lr: 0.0010 - 3s/epoch - 3ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 3s - loss: 13.2551 - mse: 13.2551 - mae: 1.5949 - val_loss: 21.5253 - val_mse: 21.5253 - val_mae: 1.6926 - lr: 0.0010 - 3s/epoch - 3ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 13.2417 - mse: 13.2417 - mae: 1.5860 - val_loss: 21.4073 - val_mse: 21.4073 - val_mae: 1.6708 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 13.1900 - mse: 13.1900 - mae: 1.5940 - val_loss: 21.5031 - val_mse: 21.5031 - val_mae: 1.6307 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 13.2098 - mse: 13.2098 - mae: 1.5907 - val_loss: 21.6428 - val_mse: 21.6428 - val_mae: 1.7006 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 13.2045 - mse: 13.2045 - mae: 1.5989 - val_loss: 21.5822 - val_mse: 21.5822 - val_mae: 1.7145 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 2s - loss: 13.1564 - mse: 13.1564 - mae: 1.5952 - val_loss: 21.7064 - val_mse: 21.7064 - val_mae: 1.6601 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 2s - loss: 13.1643 - mse: 13.1643 - mae: 1.5949 - val_loss: 21.9713 - val_mse: 21.9713 - val_mae: 1.5627 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Score for fold 3: loss of 21.971281051635742\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 14.8013 - mse: 14.8013 - mae: 1.5938 - val_loss: 15.1823 - val_mse: 15.1823 - val_mae: 1.6404 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 14.7269 - mse: 14.7269 - mae: 1.5872 - val_loss: 15.3102 - val_mse: 15.3102 - val_mae: 1.6631 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 14.7006 - mse: 14.7006 - mae: 1.5843 - val_loss: 15.4684 - val_mse: 15.4684 - val_mae: 1.7812 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 14.6633 - mse: 14.6633 - mae: 1.5833 - val_loss: 15.3761 - val_mse: 15.3761 - val_mae: 1.6341 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 14.6310 - mse: 14.6310 - mae: 1.5851 - val_loss: 15.4030 - val_mse: 15.4030 - val_mae: 1.6385 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 14.6498 - mse: 14.6498 - mae: 1.5813 - val_loss: 15.4002 - val_mse: 15.4002 - val_mae: 1.6921 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Score for fold 4: loss of 15.40024471282959\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 15.4043 - mse: 15.4043 - mae: 1.6050 - val_loss: 12.4181 - val_mse: 12.4181 - val_mae: 1.5344 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 15.2933 - mse: 15.2933 - mae: 1.5976 - val_loss: 12.3694 - val_mse: 12.3694 - val_mae: 1.6198 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 15.2463 - mse: 15.2463 - mae: 1.6045 - val_loss: 12.4429 - val_mse: 12.4429 - val_mae: 1.6282 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 15.1972 - mse: 15.1972 - mae: 1.5948 - val_loss: 12.6570 - val_mse: 12.6570 - val_mae: 1.5275 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 15.2000 - mse: 15.2000 - mae: 1.5950 - val_loss: 12.2784 - val_mse: 12.2784 - val_mae: 1.6123 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 15.1380 - mse: 15.1380 - mae: 1.5945 - val_loss: 12.7730 - val_mse: 12.7730 - val_mae: 1.5750 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 15.0763 - mse: 15.0763 - mae: 1.5868 - val_loss: 12.4412 - val_mse: 12.4412 - val_mae: 1.5670 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 2s - loss: 15.1615 - mse: 15.1615 - mae: 1.5935 - val_loss: 12.4290 - val_mse: 12.4290 - val_mae: 1.6257 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 2s - loss: 15.0389 - mse: 15.0389 - mae: 1.5847 - val_loss: 12.4476 - val_mse: 12.4476 - val_mae: 1.5709 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 2s - loss: 15.1030 - mse: 15.1030 - mae: 1.5849 - val_loss: 12.3805 - val_mse: 12.3805 - val_mae: 1.6028 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Score for fold 5: loss of 12.380520820617676\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-10-14 20:47:28,968]\u001b[0m Finished trial#4 resulted in value: 15.101999999999999. Current best value is 14.864 with parameters: {'activation': 'tanh', 'num_hidden_layer': 4, 'i': 6, 'learning_rate': 0.0003477839020306978}.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 14.1491 - mse: 14.1491 - mae: 1.6609 - val_loss: 22.7627 - val_mse: 22.7627 - val_mae: 1.6785 - lr: 0.0014 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 13.9469 - mse: 13.9469 - mae: 1.6317 - val_loss: 22.6204 - val_mse: 22.6204 - val_mae: 1.6644 - lr: 0.0014 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 13.9106 - mse: 13.9106 - mae: 1.6315 - val_loss: 22.8004 - val_mse: 22.8004 - val_mae: 1.5991 - lr: 0.0014 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 13.9272 - mse: 13.9272 - mae: 1.6350 - val_loss: 22.9180 - val_mse: 22.9180 - val_mae: 1.6185 - lr: 0.0014 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 13.9074 - mse: 13.9074 - mae: 1.6405 - val_loss: 23.0945 - val_mse: 23.0945 - val_mae: 1.5375 - lr: 0.0014 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 13.9309 - mse: 13.9309 - mae: 1.6342 - val_loss: 22.7196 - val_mse: 22.7196 - val_mae: 1.6659 - lr: 0.0014 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 13.9120 - mse: 13.9120 - mae: 1.6337 - val_loss: 22.7037 - val_mse: 22.7037 - val_mae: 1.6349 - lr: 0.0014 - 2s/epoch - 2ms/step\n",
            "Score for fold 1: loss of 22.703659057617188\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 14.7527 - mse: 14.7527 - mae: 1.6314 - val_loss: 19.3084 - val_mse: 19.3084 - val_mae: 1.6975 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 14.7411 - mse: 14.7411 - mae: 1.6327 - val_loss: 19.4980 - val_mse: 19.4980 - val_mae: 1.5804 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 14.7097 - mse: 14.7097 - mae: 1.6367 - val_loss: 19.4985 - val_mse: 19.4985 - val_mae: 1.5839 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 14.7764 - mse: 14.7764 - mae: 1.6379 - val_loss: 19.2891 - val_mse: 19.2891 - val_mae: 1.7287 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 14.7436 - mse: 14.7436 - mae: 1.6330 - val_loss: 19.3094 - val_mse: 19.3094 - val_mae: 1.6648 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 14.7466 - mse: 14.7466 - mae: 1.6353 - val_loss: 19.2907 - val_mse: 19.2907 - val_mae: 1.6312 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 14.7578 - mse: 14.7578 - mae: 1.6295 - val_loss: 19.3313 - val_mse: 19.3313 - val_mae: 1.6675 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 2s - loss: 14.7294 - mse: 14.7294 - mae: 1.6388 - val_loss: 19.3281 - val_mse: 19.3281 - val_mae: 1.6540 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 2s - loss: 14.7639 - mse: 14.7639 - mae: 1.6405 - val_loss: 19.4480 - val_mse: 19.4480 - val_mae: 1.6162 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Score for fold 2: loss of 19.4479923248291\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 16.6986 - mse: 16.6986 - mae: 1.6492 - val_loss: 11.5582 - val_mse: 11.5582 - val_mae: 1.6326 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 16.7000 - mse: 16.7000 - mae: 1.6421 - val_loss: 11.8299 - val_mse: 11.8299 - val_mae: 1.8194 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 16.6912 - mse: 16.6912 - mae: 1.6428 - val_loss: 11.5588 - val_mse: 11.5588 - val_mae: 1.6683 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 16.6534 - mse: 16.6534 - mae: 1.6488 - val_loss: 11.7432 - val_mse: 11.7432 - val_mae: 1.5428 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 16.7080 - mse: 16.7080 - mae: 1.6427 - val_loss: 11.6257 - val_mse: 11.6257 - val_mae: 1.7171 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 16.6695 - mse: 16.6695 - mae: 1.6505 - val_loss: 11.5628 - val_mse: 11.5628 - val_mae: 1.6062 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Score for fold 3: loss of 11.562793731689453\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 16.8033 - mse: 16.8033 - mae: 1.6408 - val_loss: 11.4676 - val_mse: 11.4676 - val_mae: 1.5370 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 16.8424 - mse: 16.8424 - mae: 1.6457 - val_loss: 11.4648 - val_mse: 11.4648 - val_mae: 1.5237 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 16.7972 - mse: 16.7972 - mae: 1.6406 - val_loss: 11.4939 - val_mse: 11.4939 - val_mae: 1.5303 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 16.8275 - mse: 16.8275 - mae: 1.6450 - val_loss: 11.1534 - val_mse: 11.1534 - val_mae: 1.6076 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 16.7975 - mse: 16.7975 - mae: 1.6511 - val_loss: 11.1448 - val_mse: 11.1448 - val_mae: 1.6872 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 16.8704 - mse: 16.8704 - mae: 1.6495 - val_loss: 11.1284 - val_mse: 11.1284 - val_mae: 1.6447 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 16.8468 - mse: 16.8468 - mae: 1.6571 - val_loss: 11.2765 - val_mse: 11.2765 - val_mae: 1.5637 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 2s - loss: 16.9183 - mse: 16.9183 - mae: 1.6430 - val_loss: 11.2201 - val_mse: 11.2201 - val_mae: 1.7388 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 2s - loss: 16.8856 - mse: 16.8856 - mae: 1.6463 - val_loss: 11.2312 - val_mse: 11.2312 - val_mae: 1.5802 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 2s - loss: 16.8405 - mse: 16.8405 - mae: 1.6516 - val_loss: 11.4007 - val_mse: 11.4007 - val_mae: 1.7904 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 2s - loss: 16.8783 - mse: 16.8783 - mae: 1.6461 - val_loss: 11.2023 - val_mse: 11.2023 - val_mae: 1.7104 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Score for fold 4: loss of 11.202274322509766\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 16.2964 - mse: 16.2964 - mae: 1.6508 - val_loss: 13.4881 - val_mse: 13.4881 - val_mae: 1.6068 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 16.2362 - mse: 16.2362 - mae: 1.6429 - val_loss: 13.4035 - val_mse: 13.4035 - val_mae: 1.6337 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 16.3075 - mse: 16.3075 - mae: 1.6331 - val_loss: 13.4482 - val_mse: 13.4482 - val_mae: 1.7448 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 16.3430 - mse: 16.3430 - mae: 1.6415 - val_loss: 13.6667 - val_mse: 13.6667 - val_mae: 1.8325 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 16.3406 - mse: 16.3406 - mae: 1.6404 - val_loss: 13.3773 - val_mse: 13.3773 - val_mae: 1.6605 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 16.2822 - mse: 16.2822 - mae: 1.6390 - val_loss: 13.4106 - val_mse: 13.4106 - val_mae: 1.6395 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 16.3281 - mse: 16.3281 - mae: 1.6421 - val_loss: 13.3924 - val_mse: 13.3924 - val_mae: 1.6948 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 2s - loss: 16.3428 - mse: 16.3428 - mae: 1.6446 - val_loss: 13.3955 - val_mse: 13.3955 - val_mae: 1.6617 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 2s - loss: 16.2734 - mse: 16.2734 - mae: 1.6397 - val_loss: 13.3944 - val_mse: 13.3944 - val_mae: 1.6969 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 2s - loss: 16.3268 - mse: 16.3268 - mae: 1.6415 - val_loss: 13.4972 - val_mse: 13.4972 - val_mae: 1.7708 - lr: 0.0010 - 2s/epoch - 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-10-14 20:48:42,022]\u001b[0m Finished trial#5 resulted in value: 15.681999999999999. Current best value is 14.864 with parameters: {'activation': 'tanh', 'num_hidden_layer': 4, 'i': 6, 'learning_rate': 0.0003477839020306978}.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Score for fold 5: loss of 13.497208595275879\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 16.0239 - mse: 16.0239 - mae: 1.6736 - val_loss: 15.8263 - val_mse: 15.8263 - val_mae: 1.6434 - lr: 0.0063 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 15.8461 - mse: 15.8461 - mae: 1.6521 - val_loss: 15.7834 - val_mse: 15.7834 - val_mae: 1.6342 - lr: 0.0063 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 15.8688 - mse: 15.8688 - mae: 1.6557 - val_loss: 15.6442 - val_mse: 15.6442 - val_mae: 1.7212 - lr: 0.0063 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 15.8167 - mse: 15.8167 - mae: 1.6471 - val_loss: 15.7002 - val_mse: 15.7002 - val_mae: 1.6354 - lr: 0.0063 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 15.8403 - mse: 15.8403 - mae: 1.6517 - val_loss: 15.8052 - val_mse: 15.8052 - val_mae: 1.8059 - lr: 0.0063 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 15.8155 - mse: 15.8155 - mae: 1.6473 - val_loss: 15.9336 - val_mse: 15.9336 - val_mae: 1.6012 - lr: 0.0063 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 15.8321 - mse: 15.8321 - mae: 1.6511 - val_loss: 16.1453 - val_mse: 16.1453 - val_mae: 1.6448 - lr: 0.0063 - 2s/epoch - 2ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 2s - loss: 15.9119 - mse: 15.9119 - mae: 1.6474 - val_loss: 15.6923 - val_mse: 15.6923 - val_mae: 1.6447 - lr: 0.0063 - 2s/epoch - 2ms/step\n",
            "Score for fold 1: loss of 15.692283630371094\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 16.9248 - mse: 16.9248 - mae: 1.6472 - val_loss: 10.5886 - val_mse: 10.5886 - val_mae: 1.6114 - lr: 0.0013 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 16.8975 - mse: 16.8975 - mae: 1.6443 - val_loss: 10.7155 - val_mse: 10.7155 - val_mae: 1.5652 - lr: 0.0013 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 16.9227 - mse: 16.9227 - mae: 1.6478 - val_loss: 10.6727 - val_mse: 10.6727 - val_mae: 1.6259 - lr: 0.0013 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 16.9139 - mse: 16.9139 - mae: 1.6504 - val_loss: 10.6529 - val_mse: 10.6529 - val_mae: 1.6013 - lr: 0.0013 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 16.9324 - mse: 16.9324 - mae: 1.6475 - val_loss: 10.6758 - val_mse: 10.6758 - val_mae: 1.6031 - lr: 0.0013 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 16.9319 - mse: 16.9319 - mae: 1.6505 - val_loss: 10.5948 - val_mse: 10.5948 - val_mae: 1.6034 - lr: 0.0013 - 2s/epoch - 2ms/step\n",
            "Score for fold 2: loss of 10.594779014587402\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 14.0298 - mse: 14.0298 - mae: 1.6368 - val_loss: 22.0926 - val_mse: 22.0926 - val_mae: 1.6950 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 14.0749 - mse: 14.0749 - mae: 1.6339 - val_loss: 22.0515 - val_mse: 22.0515 - val_mae: 1.6554 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 14.0859 - mse: 14.0859 - mae: 1.6331 - val_loss: 22.0473 - val_mse: 22.0473 - val_mae: 1.6550 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 14.0771 - mse: 14.0771 - mae: 1.6355 - val_loss: 22.0936 - val_mse: 22.0936 - val_mae: 1.6475 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 14.0830 - mse: 14.0830 - mae: 1.6395 - val_loss: 22.0854 - val_mse: 22.0854 - val_mae: 1.7054 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 14.1699 - mse: 14.1699 - mae: 1.6403 - val_loss: 22.1007 - val_mse: 22.1007 - val_mae: 1.7408 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 14.1447 - mse: 14.1447 - mae: 1.6422 - val_loss: 22.4822 - val_mse: 22.4822 - val_mae: 1.8633 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 2s - loss: 14.0527 - mse: 14.0527 - mae: 1.6378 - val_loss: 22.5168 - val_mse: 22.5168 - val_mae: 1.6002 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Score for fold 3: loss of 22.516761779785156\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 15.2153 - mse: 15.2153 - mae: 1.6372 - val_loss: 17.5942 - val_mse: 17.5942 - val_mae: 1.6737 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 15.1888 - mse: 15.1888 - mae: 1.6289 - val_loss: 17.6652 - val_mse: 17.6652 - val_mae: 1.6846 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 15.2066 - mse: 15.2066 - mae: 1.6384 - val_loss: 17.5527 - val_mse: 17.5527 - val_mae: 1.7171 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 15.2080 - mse: 15.2080 - mae: 1.6312 - val_loss: 17.5528 - val_mse: 17.5528 - val_mae: 1.7569 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 15.1965 - mse: 15.1965 - mae: 1.6390 - val_loss: 17.5360 - val_mse: 17.5360 - val_mae: 1.7668 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 15.2043 - mse: 15.2043 - mae: 1.6333 - val_loss: 17.8532 - val_mse: 17.8532 - val_mae: 1.6383 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 15.2775 - mse: 15.2775 - mae: 1.6370 - val_loss: 17.6159 - val_mse: 17.6159 - val_mae: 1.6662 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 2s - loss: 15.2799 - mse: 15.2799 - mae: 1.6360 - val_loss: 17.5981 - val_mse: 17.5981 - val_mae: 1.7533 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 2s - loss: 15.2025 - mse: 15.2025 - mae: 1.6362 - val_loss: 17.7804 - val_mse: 17.7804 - val_mae: 1.6702 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 2s - loss: 15.2701 - mse: 15.2701 - mae: 1.6459 - val_loss: 17.6995 - val_mse: 17.6995 - val_mae: 1.6720 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Score for fold 4: loss of 17.699480056762695\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 16.6344 - mse: 16.6344 - mae: 1.6663 - val_loss: 12.1700 - val_mse: 12.1700 - val_mae: 1.6308 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 16.5908 - mse: 16.5908 - mae: 1.6614 - val_loss: 12.2173 - val_mse: 12.2173 - val_mae: 1.6708 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 16.6180 - mse: 16.6180 - mae: 1.6590 - val_loss: 12.1414 - val_mse: 12.1414 - val_mae: 1.6106 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 16.5475 - mse: 16.5475 - mae: 1.6634 - val_loss: 12.2152 - val_mse: 12.2152 - val_mae: 1.5442 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 16.6084 - mse: 16.6084 - mae: 1.6640 - val_loss: 12.1538 - val_mse: 12.1538 - val_mae: 1.5525 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 16.5494 - mse: 16.5494 - mae: 1.6604 - val_loss: 12.1535 - val_mse: 12.1535 - val_mae: 1.6337 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 16.5208 - mse: 16.5208 - mae: 1.6630 - val_loss: 12.1581 - val_mse: 12.1581 - val_mae: 1.5658 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 2s - loss: 16.6209 - mse: 16.6209 - mae: 1.6617 - val_loss: 12.1594 - val_mse: 12.1594 - val_mae: 1.6391 - lr: 0.0010 - 2s/epoch - 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-10-14 20:49:50,892]\u001b[0m Finished trial#6 resulted in value: 15.732. Current best value is 14.864 with parameters: {'activation': 'tanh', 'num_hidden_layer': 4, 'i': 6, 'learning_rate': 0.0003477839020306978}.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Score for fold 5: loss of 12.159394264221191\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 4s - loss: 16.8444 - mse: 16.8444 - mae: 1.7450 - val_loss: 13.2376 - val_mse: 13.2376 - val_mae: 1.7612 - lr: 0.0070 - 4s/epoch - 4ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 3s - loss: 16.7499 - mse: 16.7499 - mae: 1.7464 - val_loss: 13.9182 - val_mse: 13.9182 - val_mae: 1.5214 - lr: 0.0070 - 3s/epoch - 3ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 3s - loss: 16.7177 - mse: 16.7177 - mae: 1.7454 - val_loss: 13.9317 - val_mse: 13.9317 - val_mae: 1.5484 - lr: 0.0070 - 3s/epoch - 3ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 3s - loss: 16.8261 - mse: 16.8261 - mae: 1.7740 - val_loss: 13.9373 - val_mse: 13.9373 - val_mae: 1.9969 - lr: 0.0070 - 3s/epoch - 3ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 3s - loss: 16.7031 - mse: 16.7031 - mae: 1.7434 - val_loss: 13.6609 - val_mse: 13.6609 - val_mae: 1.6051 - lr: 0.0070 - 3s/epoch - 3ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 4s - loss: 16.8568 - mse: 16.8568 - mae: 1.7657 - val_loss: 14.5517 - val_mse: 14.5517 - val_mae: 1.5151 - lr: 0.0070 - 4s/epoch - 4ms/step\n",
            "Score for fold 1: loss of 14.551711082458496\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 3s - loss: 16.0117 - mse: 16.0117 - mae: 1.6816 - val_loss: 14.5067 - val_mse: 14.5067 - val_mae: 1.7465 - lr: 0.0014 - 3s/epoch - 3ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 3s - loss: 15.9329 - mse: 15.9329 - mae: 1.6811 - val_loss: 14.7500 - val_mse: 14.7500 - val_mae: 1.8436 - lr: 0.0014 - 3s/epoch - 3ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 3s - loss: 15.7880 - mse: 15.7880 - mae: 1.6576 - val_loss: 14.3588 - val_mse: 14.3588 - val_mae: 1.6289 - lr: 0.0014 - 3s/epoch - 3ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 3s - loss: 15.6468 - mse: 15.6468 - mae: 1.6462 - val_loss: 14.2531 - val_mse: 14.2531 - val_mae: 1.7488 - lr: 0.0014 - 3s/epoch - 3ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 3s - loss: 15.6347 - mse: 15.6347 - mae: 1.6346 - val_loss: 14.2721 - val_mse: 14.2721 - val_mae: 1.6816 - lr: 0.0014 - 3s/epoch - 3ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 3s - loss: 15.6331 - mse: 15.6331 - mae: 1.6448 - val_loss: 14.2121 - val_mse: 14.2121 - val_mae: 1.6467 - lr: 0.0014 - 3s/epoch - 3ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 3s - loss: 15.5388 - mse: 15.5388 - mae: 1.6268 - val_loss: 14.5176 - val_mse: 14.5176 - val_mae: 1.7992 - lr: 0.0014 - 3s/epoch - 3ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 3s - loss: 15.5322 - mse: 15.5322 - mae: 1.6353 - val_loss: 14.2636 - val_mse: 14.2636 - val_mae: 1.6229 - lr: 0.0014 - 3s/epoch - 3ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 3s - loss: 15.6370 - mse: 15.6370 - mae: 1.6355 - val_loss: 14.1742 - val_mse: 14.1742 - val_mae: 1.7190 - lr: 0.0014 - 3s/epoch - 3ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 3s - loss: 15.4882 - mse: 15.4882 - mae: 1.6259 - val_loss: 14.2553 - val_mse: 14.2553 - val_mae: 1.5855 - lr: 0.0014 - 3s/epoch - 3ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 3s - loss: 15.4778 - mse: 15.4778 - mae: 1.6147 - val_loss: 14.4464 - val_mse: 14.4464 - val_mae: 1.5426 - lr: 0.0014 - 3s/epoch - 3ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 3s - loss: 15.4590 - mse: 15.4590 - mae: 1.6222 - val_loss: 14.2435 - val_mse: 14.2435 - val_mae: 1.7669 - lr: 0.0014 - 3s/epoch - 3ms/step\n",
            "Epoch 13/100\n",
            "1000/1000 - 3s - loss: 15.4598 - mse: 15.4598 - mae: 1.6265 - val_loss: 14.2962 - val_mse: 14.2962 - val_mae: 1.5454 - lr: 0.0014 - 3s/epoch - 3ms/step\n",
            "Epoch 14/100\n",
            "1000/1000 - 3s - loss: 15.4430 - mse: 15.4430 - mae: 1.6302 - val_loss: 14.4177 - val_mse: 14.4177 - val_mae: 1.8436 - lr: 0.0014 - 3s/epoch - 3ms/step\n",
            "Score for fold 2: loss of 14.417671203613281\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 3s - loss: 14.2708 - mse: 14.2708 - mae: 1.6243 - val_loss: 18.7249 - val_mse: 18.7249 - val_mae: 1.6403 - lr: 0.0010 - 3s/epoch - 3ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 3s - loss: 14.1296 - mse: 14.1296 - mae: 1.6247 - val_loss: 18.8881 - val_mse: 18.8881 - val_mae: 1.6045 - lr: 0.0010 - 3s/epoch - 3ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 3s - loss: 14.0744 - mse: 14.0744 - mae: 1.6249 - val_loss: 18.7271 - val_mse: 18.7271 - val_mae: 1.6528 - lr: 0.0010 - 3s/epoch - 3ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 3s - loss: 14.0440 - mse: 14.0440 - mae: 1.6169 - val_loss: 19.0410 - val_mse: 19.0410 - val_mae: 1.4958 - lr: 0.0010 - 3s/epoch - 3ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 3s - loss: 14.0862 - mse: 14.0862 - mae: 1.6269 - val_loss: 18.7482 - val_mse: 18.7482 - val_mae: 1.5864 - lr: 0.0010 - 3s/epoch - 3ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 3s - loss: 13.9378 - mse: 13.9378 - mae: 1.6136 - val_loss: 18.6359 - val_mse: 18.6359 - val_mae: 1.6249 - lr: 0.0010 - 3s/epoch - 3ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 3s - loss: 14.0259 - mse: 14.0259 - mae: 1.6194 - val_loss: 18.5735 - val_mse: 18.5735 - val_mae: 1.6459 - lr: 0.0010 - 3s/epoch - 3ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 3s - loss: 13.9961 - mse: 13.9961 - mae: 1.6148 - val_loss: 18.9530 - val_mse: 18.9530 - val_mae: 1.5429 - lr: 0.0010 - 3s/epoch - 3ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 3s - loss: 13.9941 - mse: 13.9941 - mae: 1.6135 - val_loss: 18.8145 - val_mse: 18.8145 - val_mae: 1.5155 - lr: 0.0010 - 3s/epoch - 3ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 3s - loss: 14.0427 - mse: 14.0427 - mae: 1.6125 - val_loss: 18.8669 - val_mse: 18.8669 - val_mae: 1.7228 - lr: 0.0010 - 3s/epoch - 3ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 3s - loss: 14.0685 - mse: 14.0685 - mae: 1.6155 - val_loss: 18.8144 - val_mse: 18.8144 - val_mae: 1.6366 - lr: 0.0010 - 3s/epoch - 3ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 3s - loss: 13.9514 - mse: 13.9514 - mae: 1.6144 - val_loss: 18.7652 - val_mse: 18.7652 - val_mae: 1.5278 - lr: 0.0010 - 3s/epoch - 3ms/step\n",
            "Score for fold 3: loss of 18.765193939208984\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 3s - loss: 14.6404 - mse: 14.6404 - mae: 1.6007 - val_loss: 16.2389 - val_mse: 16.2389 - val_mae: 1.5645 - lr: 0.0010 - 3s/epoch - 3ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 3s - loss: 14.5907 - mse: 14.5907 - mae: 1.5958 - val_loss: 16.3827 - val_mse: 16.3827 - val_mae: 1.5741 - lr: 0.0010 - 3s/epoch - 3ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 3s - loss: 14.5526 - mse: 14.5526 - mae: 1.5989 - val_loss: 16.3123 - val_mse: 16.3123 - val_mae: 1.6827 - lr: 0.0010 - 3s/epoch - 3ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 3s - loss: 14.5686 - mse: 14.5686 - mae: 1.6100 - val_loss: 16.2203 - val_mse: 16.2203 - val_mae: 1.7653 - lr: 0.0010 - 3s/epoch - 3ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 3s - loss: 14.5823 - mse: 14.5823 - mae: 1.5960 - val_loss: 16.2458 - val_mse: 16.2458 - val_mae: 1.6327 - lr: 0.0010 - 3s/epoch - 3ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 3s - loss: 14.6006 - mse: 14.6006 - mae: 1.6012 - val_loss: 16.2133 - val_mse: 16.2133 - val_mae: 1.6866 - lr: 0.0010 - 3s/epoch - 3ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 3s - loss: 14.6122 - mse: 14.6122 - mae: 1.6061 - val_loss: 16.4906 - val_mse: 16.4906 - val_mae: 1.5764 - lr: 0.0010 - 3s/epoch - 3ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 3s - loss: 14.4996 - mse: 14.4996 - mae: 1.6018 - val_loss: 16.4207 - val_mse: 16.4207 - val_mae: 1.6542 - lr: 0.0010 - 3s/epoch - 3ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 3s - loss: 14.5400 - mse: 14.5400 - mae: 1.6041 - val_loss: 16.8798 - val_mse: 16.8798 - val_mae: 1.9275 - lr: 0.0010 - 3s/epoch - 3ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 3s - loss: 14.5232 - mse: 14.5232 - mae: 1.5927 - val_loss: 16.2936 - val_mse: 16.2936 - val_mae: 1.6759 - lr: 0.0010 - 3s/epoch - 3ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 3s - loss: 14.4731 - mse: 14.4731 - mae: 1.5933 - val_loss: 16.8252 - val_mse: 16.8252 - val_mae: 1.6097 - lr: 0.0010 - 3s/epoch - 3ms/step\n",
            "Score for fold 4: loss of 16.82524299621582\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 3s - loss: 15.4875 - mse: 15.4875 - mae: 1.6257 - val_loss: 13.3084 - val_mse: 13.3084 - val_mae: 1.6085 - lr: 0.0010 - 3s/epoch - 3ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 3s - loss: 15.3132 - mse: 15.3132 - mae: 1.6168 - val_loss: 13.4012 - val_mse: 13.4012 - val_mae: 1.5398 - lr: 0.0010 - 3s/epoch - 3ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 3s - loss: 15.4029 - mse: 15.4029 - mae: 1.6314 - val_loss: 13.3195 - val_mse: 13.3195 - val_mae: 1.5171 - lr: 0.0010 - 3s/epoch - 3ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 3s - loss: 15.2866 - mse: 15.2866 - mae: 1.6232 - val_loss: 13.9246 - val_mse: 13.9246 - val_mae: 1.5758 - lr: 0.0010 - 3s/epoch - 3ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 3s - loss: 15.2783 - mse: 15.2783 - mae: 1.6300 - val_loss: 13.4940 - val_mse: 13.4940 - val_mae: 1.4888 - lr: 0.0010 - 3s/epoch - 3ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 3s - loss: 15.2562 - mse: 15.2562 - mae: 1.6311 - val_loss: 13.5694 - val_mse: 13.5694 - val_mae: 1.6457 - lr: 0.0010 - 3s/epoch - 3ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-10-14 20:52:26,913]\u001b[0m Finished trial#7 resulted in value: 15.627999999999997. Current best value is 14.864 with parameters: {'activation': 'tanh', 'num_hidden_layer': 4, 'i': 6, 'learning_rate': 0.0003477839020306978}.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Score for fold 5: loss of 13.569367408752441\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 5s - loss: 17.9621 - mse: 17.9621 - mae: 1.7805 - val_loss: 10.6768 - val_mse: 10.6768 - val_mae: 1.6352 - lr: 0.0043 - 5s/epoch - 5ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 4s - loss: 17.4763 - mse: 17.4763 - mae: 1.7225 - val_loss: 11.0881 - val_mse: 11.0881 - val_mae: 1.8056 - lr: 0.0043 - 4s/epoch - 4ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 4s - loss: 17.3135 - mse: 17.3135 - mae: 1.7071 - val_loss: 12.0280 - val_mse: 12.0280 - val_mae: 1.8222 - lr: 0.0043 - 4s/epoch - 4ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 5s - loss: 17.4045 - mse: 17.4045 - mae: 1.7235 - val_loss: 11.1365 - val_mse: 11.1365 - val_mae: 1.5894 - lr: 0.0043 - 5s/epoch - 5ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 4s - loss: 17.4138 - mse: 17.4138 - mae: 1.7140 - val_loss: 10.8691 - val_mse: 10.8691 - val_mae: 1.5639 - lr: 0.0043 - 4s/epoch - 4ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 4s - loss: 17.1621 - mse: 17.1621 - mae: 1.7024 - val_loss: 11.2093 - val_mse: 11.2093 - val_mae: 1.8343 - lr: 0.0043 - 4s/epoch - 4ms/step\n",
            "Score for fold 1: loss of 11.209266662597656\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 5s - loss: 16.0709 - mse: 16.0709 - mae: 1.6429 - val_loss: 14.1235 - val_mse: 14.1235 - val_mae: 1.6545 - lr: 0.0010 - 5s/epoch - 5ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 5s - loss: 16.0875 - mse: 16.0875 - mae: 1.6394 - val_loss: 14.1508 - val_mse: 14.1508 - val_mae: 1.6378 - lr: 0.0010 - 5s/epoch - 5ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 5s - loss: 16.0859 - mse: 16.0859 - mae: 1.6385 - val_loss: 14.4075 - val_mse: 14.4075 - val_mae: 1.5789 - lr: 0.0010 - 5s/epoch - 5ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 5s - loss: 16.1430 - mse: 16.1430 - mae: 1.6425 - val_loss: 14.4295 - val_mse: 14.4295 - val_mae: 1.6001 - lr: 0.0010 - 5s/epoch - 5ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 4s - loss: 16.1266 - mse: 16.1266 - mae: 1.6475 - val_loss: 14.4583 - val_mse: 14.4583 - val_mae: 1.5813 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 5s - loss: 16.0812 - mse: 16.0812 - mae: 1.6414 - val_loss: 14.1350 - val_mse: 14.1350 - val_mae: 1.7041 - lr: 0.0010 - 5s/epoch - 5ms/step\n",
            "Score for fold 2: loss of 14.134949684143066\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 5s - loss: 16.0444 - mse: 16.0444 - mae: 1.6538 - val_loss: 14.6793 - val_mse: 14.6793 - val_mae: 1.6117 - lr: 0.0010 - 5s/epoch - 5ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 5s - loss: 16.0166 - mse: 16.0166 - mae: 1.6498 - val_loss: 14.7345 - val_mse: 14.7345 - val_mae: 1.5925 - lr: 0.0010 - 5s/epoch - 5ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 5s - loss: 16.0045 - mse: 16.0045 - mae: 1.6545 - val_loss: 14.5974 - val_mse: 14.5974 - val_mae: 1.6124 - lr: 0.0010 - 5s/epoch - 5ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 4s - loss: 15.9835 - mse: 15.9835 - mae: 1.6560 - val_loss: 14.6669 - val_mse: 14.6669 - val_mae: 1.6142 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 4s - loss: 15.9906 - mse: 15.9906 - mae: 1.6495 - val_loss: 14.9858 - val_mse: 14.9858 - val_mae: 1.5816 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 4s - loss: 16.0224 - mse: 16.0224 - mae: 1.6554 - val_loss: 14.8980 - val_mse: 14.8980 - val_mae: 1.5536 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 4s - loss: 16.0455 - mse: 16.0455 - mae: 1.6512 - val_loss: 14.9256 - val_mse: 14.9256 - val_mae: 1.5803 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 4s - loss: 16.0335 - mse: 16.0335 - mae: 1.6526 - val_loss: 14.6340 - val_mse: 14.6340 - val_mae: 1.6109 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Score for fold 3: loss of 14.6339750289917\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 4s - loss: 15.1395 - mse: 15.1395 - mae: 1.6482 - val_loss: 18.4164 - val_mse: 18.4164 - val_mae: 1.6240 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 4s - loss: 15.0671 - mse: 15.0671 - mae: 1.6465 - val_loss: 19.4884 - val_mse: 19.4884 - val_mae: 1.5751 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 4s - loss: 15.1107 - mse: 15.1107 - mae: 1.6503 - val_loss: 18.3484 - val_mse: 18.3484 - val_mae: 1.6599 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 4s - loss: 15.1287 - mse: 15.1287 - mae: 1.6434 - val_loss: 18.2896 - val_mse: 18.2896 - val_mae: 1.6470 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 4s - loss: 15.1176 - mse: 15.1176 - mae: 1.6501 - val_loss: 18.3825 - val_mse: 18.3825 - val_mae: 1.6499 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 4s - loss: 15.0805 - mse: 15.0805 - mae: 1.6469 - val_loss: 18.3555 - val_mse: 18.3555 - val_mae: 1.6676 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 4s - loss: 15.0893 - mse: 15.0893 - mae: 1.6436 - val_loss: 18.3104 - val_mse: 18.3104 - val_mae: 1.6387 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 4s - loss: 15.1315 - mse: 15.1315 - mae: 1.6458 - val_loss: 18.3818 - val_mse: 18.3818 - val_mae: 1.6144 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 4s - loss: 15.0761 - mse: 15.0761 - mae: 1.6534 - val_loss: 18.7393 - val_mse: 18.7393 - val_mae: 1.5825 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Score for fold 4: loss of 18.73928451538086\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 4s - loss: 14.5876 - mse: 14.5876 - mae: 1.6331 - val_loss: 20.4153 - val_mse: 20.4153 - val_mae: 1.6729 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 4s - loss: 14.5933 - mse: 14.5933 - mae: 1.6315 - val_loss: 20.3637 - val_mse: 20.3637 - val_mae: 1.6951 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 4s - loss: 14.5018 - mse: 14.5018 - mae: 1.6325 - val_loss: 20.3456 - val_mse: 20.3456 - val_mae: 1.6965 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 5s - loss: 14.6938 - mse: 14.6938 - mae: 1.6316 - val_loss: 20.3627 - val_mse: 20.3627 - val_mae: 1.6763 - lr: 0.0010 - 5s/epoch - 5ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 5s - loss: 14.5813 - mse: 14.5813 - mae: 1.6357 - val_loss: 20.4268 - val_mse: 20.4268 - val_mae: 1.6748 - lr: 0.0010 - 5s/epoch - 5ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 4s - loss: 14.5857 - mse: 14.5857 - mae: 1.6344 - val_loss: 20.5137 - val_mse: 20.5137 - val_mae: 1.6336 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 4s - loss: 14.6192 - mse: 14.6192 - mae: 1.6307 - val_loss: 20.4061 - val_mse: 20.4061 - val_mae: 1.7139 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 5s - loss: 14.5784 - mse: 14.5784 - mae: 1.6280 - val_loss: 21.8886 - val_mse: 21.8886 - val_mae: 1.7040 - lr: 0.0010 - 5s/epoch - 5ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-10-14 20:55:18,184]\u001b[0m Finished trial#8 resulted in value: 16.12. Current best value is 14.864 with parameters: {'activation': 'tanh', 'num_hidden_layer': 4, 'i': 6, 'learning_rate': 0.0003477839020306978}.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Score for fold 5: loss of 21.88858985900879\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 14.5611 - mse: 14.5611 - mae: 1.6026 - val_loss: 19.4826 - val_mse: 19.4826 - val_mae: 1.5895 - lr: 8.6563e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 14.1468 - mse: 14.1468 - mae: 1.5870 - val_loss: 19.4456 - val_mse: 19.4456 - val_mae: 1.6322 - lr: 8.6563e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 14.0286 - mse: 14.0286 - mae: 1.5860 - val_loss: 19.5014 - val_mse: 19.5014 - val_mae: 1.5878 - lr: 8.6563e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 14.0043 - mse: 14.0043 - mae: 1.5837 - val_loss: 19.5552 - val_mse: 19.5552 - val_mae: 1.6129 - lr: 8.6563e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 13.9469 - mse: 13.9469 - mae: 1.5784 - val_loss: 19.4593 - val_mse: 19.4593 - val_mae: 1.6093 - lr: 8.6563e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 13.9179 - mse: 13.9179 - mae: 1.5797 - val_loss: 19.7128 - val_mse: 19.7128 - val_mae: 1.5807 - lr: 8.6563e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 13.9246 - mse: 13.9246 - mae: 1.5755 - val_loss: 19.2921 - val_mse: 19.2921 - val_mae: 1.6273 - lr: 8.6563e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 2s - loss: 13.8954 - mse: 13.8954 - mae: 1.5752 - val_loss: 19.3611 - val_mse: 19.3611 - val_mae: 1.6247 - lr: 8.6563e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 2s - loss: 13.9330 - mse: 13.9330 - mae: 1.5749 - val_loss: 19.3862 - val_mse: 19.3862 - val_mae: 1.6099 - lr: 8.6563e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 2s - loss: 13.9204 - mse: 13.9204 - mae: 1.5742 - val_loss: 19.2945 - val_mse: 19.2945 - val_mae: 1.6455 - lr: 8.6563e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 2s - loss: 13.8636 - mse: 13.8636 - mae: 1.5719 - val_loss: 19.3440 - val_mse: 19.3440 - val_mae: 1.6399 - lr: 8.6563e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 2s - loss: 13.9194 - mse: 13.9194 - mae: 1.5722 - val_loss: 19.4853 - val_mse: 19.4853 - val_mae: 1.6051 - lr: 8.6563e-04 - 2s/epoch - 2ms/step\n",
            "Score for fold 1: loss of 19.485347747802734\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 15.1781 - mse: 15.1781 - mae: 1.5862 - val_loss: 14.3660 - val_mse: 14.3660 - val_mae: 1.5644 - lr: 8.6563e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 15.1436 - mse: 15.1436 - mae: 1.5809 - val_loss: 14.3750 - val_mse: 14.3750 - val_mae: 1.6044 - lr: 8.6563e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 15.1360 - mse: 15.1360 - mae: 1.5825 - val_loss: 14.2308 - val_mse: 14.2308 - val_mae: 1.6046 - lr: 8.6563e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 15.1234 - mse: 15.1234 - mae: 1.5838 - val_loss: 14.3662 - val_mse: 14.3662 - val_mae: 1.5695 - lr: 8.6563e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 15.0797 - mse: 15.0797 - mae: 1.5763 - val_loss: 14.3313 - val_mse: 14.3313 - val_mae: 1.5453 - lr: 8.6563e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 15.0824 - mse: 15.0824 - mae: 1.5804 - val_loss: 14.2315 - val_mse: 14.2315 - val_mae: 1.6053 - lr: 8.6563e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 15.0788 - mse: 15.0788 - mae: 1.5801 - val_loss: 14.3018 - val_mse: 14.3018 - val_mae: 1.6400 - lr: 8.6563e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 2s - loss: 15.0965 - mse: 15.0965 - mae: 1.5759 - val_loss: 14.2904 - val_mse: 14.2904 - val_mae: 1.5785 - lr: 8.6563e-04 - 2s/epoch - 2ms/step\n",
            "Score for fold 2: loss of 14.290444374084473\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 14.9560 - mse: 14.9560 - mae: 1.5785 - val_loss: 14.7673 - val_mse: 14.7673 - val_mae: 1.6093 - lr: 8.6563e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 14.9214 - mse: 14.9214 - mae: 1.5769 - val_loss: 14.9049 - val_mse: 14.9049 - val_mae: 1.5625 - lr: 8.6563e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 14.9431 - mse: 14.9431 - mae: 1.5728 - val_loss: 14.9929 - val_mse: 14.9929 - val_mae: 1.5556 - lr: 8.6563e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 14.9433 - mse: 14.9433 - mae: 1.5745 - val_loss: 14.8994 - val_mse: 14.8994 - val_mae: 1.5549 - lr: 8.6563e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 14.9280 - mse: 14.9280 - mae: 1.5772 - val_loss: 14.8987 - val_mse: 14.8987 - val_mae: 1.5822 - lr: 8.6563e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 14.9011 - mse: 14.9011 - mae: 1.5717 - val_loss: 14.7818 - val_mse: 14.7818 - val_mae: 1.6001 - lr: 8.6563e-04 - 2s/epoch - 2ms/step\n",
            "Score for fold 3: loss of 14.781753540039062\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 15.8967 - mse: 15.8967 - mae: 1.5807 - val_loss: 10.7163 - val_mse: 10.7163 - val_mae: 1.5702 - lr: 8.6563e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 15.9347 - mse: 15.9347 - mae: 1.5834 - val_loss: 10.6996 - val_mse: 10.6996 - val_mae: 1.5677 - lr: 8.6563e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 15.8988 - mse: 15.8988 - mae: 1.5838 - val_loss: 10.7150 - val_mse: 10.7150 - val_mae: 1.5596 - lr: 8.6563e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 15.8977 - mse: 15.8977 - mae: 1.5835 - val_loss: 10.7405 - val_mse: 10.7405 - val_mae: 1.5603 - lr: 8.6563e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 15.9182 - mse: 15.9182 - mae: 1.5814 - val_loss: 10.7483 - val_mse: 10.7483 - val_mae: 1.5608 - lr: 8.6563e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 15.8697 - mse: 15.8697 - mae: 1.5824 - val_loss: 10.7740 - val_mse: 10.7740 - val_mae: 1.5414 - lr: 8.6563e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 15.8946 - mse: 15.8946 - mae: 1.5813 - val_loss: 10.6994 - val_mse: 10.6994 - val_mae: 1.5542 - lr: 8.6563e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 2s - loss: 15.9073 - mse: 15.9073 - mae: 1.5862 - val_loss: 10.7335 - val_mse: 10.7335 - val_mae: 1.5661 - lr: 8.6563e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 2s - loss: 15.8938 - mse: 15.8938 - mae: 1.5836 - val_loss: 10.7800 - val_mse: 10.7800 - val_mae: 1.5162 - lr: 8.6563e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 2s - loss: 15.8947 - mse: 15.8947 - mae: 1.5833 - val_loss: 10.7534 - val_mse: 10.7534 - val_mae: 1.5600 - lr: 8.6563e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 2s - loss: 15.8508 - mse: 15.8508 - mae: 1.5828 - val_loss: 10.7497 - val_mse: 10.7497 - val_mae: 1.5338 - lr: 8.6563e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 2s - loss: 15.8731 - mse: 15.8731 - mae: 1.5815 - val_loss: 10.7538 - val_mse: 10.7538 - val_mae: 1.5632 - lr: 8.6563e-04 - 2s/epoch - 2ms/step\n",
            "Score for fold 4: loss of 10.75376033782959\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 14.7024 - mse: 14.7024 - mae: 1.5802 - val_loss: 15.4395 - val_mse: 15.4395 - val_mae: 1.5842 - lr: 8.6563e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 14.6810 - mse: 14.6810 - mae: 1.5781 - val_loss: 15.6199 - val_mse: 15.6199 - val_mae: 1.5586 - lr: 8.6563e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 14.6883 - mse: 14.6883 - mae: 1.5765 - val_loss: 15.5100 - val_mse: 15.5100 - val_mae: 1.5704 - lr: 8.6563e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 14.6820 - mse: 14.6820 - mae: 1.5788 - val_loss: 15.3899 - val_mse: 15.3899 - val_mae: 1.5766 - lr: 8.6563e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 14.6670 - mse: 14.6670 - mae: 1.5775 - val_loss: 15.6443 - val_mse: 15.6443 - val_mae: 1.6068 - lr: 8.6563e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 14.6711 - mse: 14.6711 - mae: 1.5770 - val_loss: 15.4336 - val_mse: 15.4336 - val_mae: 1.5655 - lr: 8.6563e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 14.6769 - mse: 14.6769 - mae: 1.5775 - val_loss: 15.4864 - val_mse: 15.4864 - val_mae: 1.6244 - lr: 8.6563e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 2s - loss: 14.6459 - mse: 14.6459 - mae: 1.5755 - val_loss: 15.5710 - val_mse: 15.5710 - val_mae: 1.5931 - lr: 8.6563e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 2s - loss: 14.6531 - mse: 14.6531 - mae: 1.5782 - val_loss: 15.5529 - val_mse: 15.5529 - val_mae: 1.5344 - lr: 8.6563e-04 - 2s/epoch - 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-10-14 20:56:44,163]\u001b[0m Finished trial#9 resulted in value: 14.972. Current best value is 14.864 with parameters: {'activation': 'tanh', 'num_hidden_layer': 4, 'i': 6, 'learning_rate': 0.0003477839020306978}.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Score for fold 5: loss of 15.552952766418457\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 19.4758 - mse: 19.4758 - mae: 1.8798 - val_loss: 12.3258 - val_mse: 12.3258 - val_mae: 1.5976 - lr: 1.1353e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 16.5311 - mse: 16.5311 - mae: 1.6242 - val_loss: 12.2096 - val_mse: 12.2096 - val_mae: 1.5866 - lr: 1.1353e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 16.4302 - mse: 16.4302 - mae: 1.6186 - val_loss: 12.1546 - val_mse: 12.1546 - val_mae: 1.5882 - lr: 1.1353e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 16.3642 - mse: 16.3642 - mae: 1.6145 - val_loss: 12.1169 - val_mse: 12.1169 - val_mae: 1.5852 - lr: 1.1353e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 16.3396 - mse: 16.3396 - mae: 1.6118 - val_loss: 12.0824 - val_mse: 12.0824 - val_mae: 1.5765 - lr: 1.1353e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 16.2974 - mse: 16.2974 - mae: 1.6118 - val_loss: 12.0554 - val_mse: 12.0554 - val_mae: 1.5809 - lr: 1.1353e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 16.2619 - mse: 16.2619 - mae: 1.6130 - val_loss: 12.0629 - val_mse: 12.0629 - val_mae: 1.5609 - lr: 1.1353e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 2s - loss: 16.2462 - mse: 16.2462 - mae: 1.6038 - val_loss: 12.0193 - val_mse: 12.0193 - val_mae: 1.5744 - lr: 1.1353e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 2s - loss: 16.2180 - mse: 16.2180 - mae: 1.6041 - val_loss: 12.0167 - val_mse: 12.0167 - val_mae: 1.5959 - lr: 1.1353e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 2s - loss: 16.2085 - mse: 16.2085 - mae: 1.6049 - val_loss: 11.9998 - val_mse: 11.9998 - val_mae: 1.5847 - lr: 1.1353e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 2s - loss: 16.1726 - mse: 16.1726 - mae: 1.6078 - val_loss: 12.0115 - val_mse: 12.0115 - val_mae: 1.5689 - lr: 1.1353e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 2s - loss: 16.1607 - mse: 16.1607 - mae: 1.6011 - val_loss: 11.9696 - val_mse: 11.9696 - val_mae: 1.5946 - lr: 1.1353e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 13/100\n",
            "1000/1000 - 2s - loss: 16.1321 - mse: 16.1321 - mae: 1.6029 - val_loss: 11.9607 - val_mse: 11.9607 - val_mae: 1.5662 - lr: 1.1353e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 14/100\n",
            "1000/1000 - 2s - loss: 16.1235 - mse: 16.1235 - mae: 1.5970 - val_loss: 11.9354 - val_mse: 11.9354 - val_mae: 1.5719 - lr: 1.1353e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 15/100\n",
            "1000/1000 - 2s - loss: 16.0935 - mse: 16.0935 - mae: 1.5988 - val_loss: 11.9538 - val_mse: 11.9538 - val_mae: 1.5641 - lr: 1.1353e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 16/100\n",
            "1000/1000 - 2s - loss: 16.0754 - mse: 16.0754 - mae: 1.5972 - val_loss: 11.9713 - val_mse: 11.9713 - val_mae: 1.5552 - lr: 1.1353e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 17/100\n",
            "1000/1000 - 2s - loss: 16.0513 - mse: 16.0513 - mae: 1.5951 - val_loss: 11.9380 - val_mse: 11.9380 - val_mae: 1.5561 - lr: 1.1353e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 18/100\n",
            "1000/1000 - 2s - loss: 16.0289 - mse: 16.0289 - mae: 1.5948 - val_loss: 11.8927 - val_mse: 11.8927 - val_mae: 1.5720 - lr: 1.1353e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 19/100\n",
            "1000/1000 - 2s - loss: 16.0095 - mse: 16.0095 - mae: 1.5932 - val_loss: 11.9077 - val_mse: 11.9077 - val_mae: 1.5735 - lr: 1.1353e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 20/100\n",
            "1000/1000 - 2s - loss: 16.0044 - mse: 16.0044 - mae: 1.5932 - val_loss: 11.8836 - val_mse: 11.8836 - val_mae: 1.5679 - lr: 1.1353e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 21/100\n",
            "1000/1000 - 2s - loss: 15.9904 - mse: 15.9904 - mae: 1.5899 - val_loss: 11.8658 - val_mse: 11.8658 - val_mae: 1.5836 - lr: 1.1353e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 22/100\n",
            "1000/1000 - 2s - loss: 15.9678 - mse: 15.9678 - mae: 1.5920 - val_loss: 11.8687 - val_mse: 11.8687 - val_mae: 1.5617 - lr: 1.1353e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 23/100\n",
            "1000/1000 - 2s - loss: 15.9481 - mse: 15.9481 - mae: 1.5911 - val_loss: 11.8979 - val_mse: 11.8979 - val_mae: 1.5465 - lr: 1.1353e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 24/100\n",
            "1000/1000 - 2s - loss: 15.9294 - mse: 15.9294 - mae: 1.5895 - val_loss: 11.8722 - val_mse: 11.8722 - val_mae: 1.5764 - lr: 1.1353e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 25/100\n",
            "1000/1000 - 2s - loss: 15.9263 - mse: 15.9263 - mae: 1.5903 - val_loss: 11.8552 - val_mse: 11.8552 - val_mae: 1.5667 - lr: 1.1353e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 26/100\n",
            "1000/1000 - 2s - loss: 15.9179 - mse: 15.9179 - mae: 1.5851 - val_loss: 11.8340 - val_mse: 11.8340 - val_mae: 1.5818 - lr: 1.1353e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 27/100\n",
            "1000/1000 - 2s - loss: 15.8971 - mse: 15.8971 - mae: 1.5892 - val_loss: 11.8700 - val_mse: 11.8700 - val_mae: 1.5584 - lr: 1.1353e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 28/100\n",
            "1000/1000 - 2s - loss: 15.8908 - mse: 15.8908 - mae: 1.5873 - val_loss: 11.8441 - val_mse: 11.8441 - val_mae: 1.5602 - lr: 1.1353e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 29/100\n",
            "1000/1000 - 2s - loss: 15.8845 - mse: 15.8845 - mae: 1.5843 - val_loss: 11.8633 - val_mse: 11.8633 - val_mae: 1.5487 - lr: 1.1353e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 30/100\n",
            "1000/1000 - 2s - loss: 15.8803 - mse: 15.8803 - mae: 1.5825 - val_loss: 11.7995 - val_mse: 11.7995 - val_mae: 1.5812 - lr: 1.1353e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 31/100\n",
            "1000/1000 - 2s - loss: 15.8617 - mse: 15.8617 - mae: 1.5892 - val_loss: 11.8480 - val_mse: 11.8480 - val_mae: 1.5490 - lr: 1.1353e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 32/100\n",
            "1000/1000 - 2s - loss: 15.8522 - mse: 15.8522 - mae: 1.5856 - val_loss: 11.8015 - val_mse: 11.8015 - val_mae: 1.5613 - lr: 1.1353e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 33/100\n",
            "1000/1000 - 2s - loss: 15.8545 - mse: 15.8545 - mae: 1.5837 - val_loss: 11.8054 - val_mse: 11.8054 - val_mae: 1.5589 - lr: 1.1353e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 34/100\n",
            "1000/1000 - 2s - loss: 15.8463 - mse: 15.8463 - mae: 1.5834 - val_loss: 11.8106 - val_mse: 11.8106 - val_mae: 1.5723 - lr: 1.1353e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 35/100\n",
            "1000/1000 - 2s - loss: 15.8347 - mse: 15.8347 - mae: 1.5829 - val_loss: 11.8140 - val_mse: 11.8140 - val_mae: 1.5625 - lr: 1.1353e-04 - 2s/epoch - 2ms/step\n",
            "Score for fold 1: loss of 11.814014434814453\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 14.9993 - mse: 14.9993 - mae: 1.5730 - val_loss: 15.1087 - val_mse: 15.1087 - val_mae: 1.6039 - lr: 1.1353e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 15.0011 - mse: 15.0011 - mae: 1.5670 - val_loss: 15.1223 - val_mse: 15.1223 - val_mae: 1.6152 - lr: 1.1353e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 15.0058 - mse: 15.0058 - mae: 1.5678 - val_loss: 15.1031 - val_mse: 15.1031 - val_mae: 1.6184 - lr: 1.1353e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 14.9957 - mse: 14.9957 - mae: 1.5673 - val_loss: 15.1378 - val_mse: 15.1378 - val_mae: 1.6092 - lr: 1.1353e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 14.9846 - mse: 14.9846 - mae: 1.5709 - val_loss: 15.1284 - val_mse: 15.1284 - val_mae: 1.6155 - lr: 1.1353e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 14.9923 - mse: 14.9923 - mae: 1.5662 - val_loss: 15.1327 - val_mse: 15.1327 - val_mae: 1.6196 - lr: 1.1353e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 14.9824 - mse: 14.9824 - mae: 1.5671 - val_loss: 15.0918 - val_mse: 15.0918 - val_mae: 1.6183 - lr: 1.1353e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 2s - loss: 14.9696 - mse: 14.9696 - mae: 1.5659 - val_loss: 15.1825 - val_mse: 15.1825 - val_mae: 1.5936 - lr: 1.1353e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 2s - loss: 14.9671 - mse: 14.9671 - mae: 1.5630 - val_loss: 15.1367 - val_mse: 15.1367 - val_mae: 1.6176 - lr: 1.1353e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 2s - loss: 14.9607 - mse: 14.9607 - mae: 1.5650 - val_loss: 15.1107 - val_mse: 15.1107 - val_mae: 1.6289 - lr: 1.1353e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 2s - loss: 14.9628 - mse: 14.9628 - mae: 1.5632 - val_loss: 15.0880 - val_mse: 15.0880 - val_mae: 1.6140 - lr: 1.1353e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 2s - loss: 14.9558 - mse: 14.9558 - mae: 1.5642 - val_loss: 15.1022 - val_mse: 15.1022 - val_mae: 1.6028 - lr: 1.1353e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 13/100\n",
            "1000/1000 - 2s - loss: 14.9473 - mse: 14.9473 - mae: 1.5638 - val_loss: 15.0894 - val_mse: 15.0894 - val_mae: 1.6140 - lr: 1.1353e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 14/100\n",
            "1000/1000 - 2s - loss: 14.9539 - mse: 14.9539 - mae: 1.5629 - val_loss: 15.1099 - val_mse: 15.1099 - val_mae: 1.6299 - lr: 1.1353e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 15/100\n",
            "1000/1000 - 2s - loss: 14.9459 - mse: 14.9459 - mae: 1.5648 - val_loss: 15.0862 - val_mse: 15.0862 - val_mae: 1.6324 - lr: 1.1353e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 16/100\n",
            "1000/1000 - 2s - loss: 14.9507 - mse: 14.9507 - mae: 1.5644 - val_loss: 15.0956 - val_mse: 15.0956 - val_mae: 1.6149 - lr: 1.1353e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 17/100\n",
            "1000/1000 - 2s - loss: 14.9400 - mse: 14.9400 - mae: 1.5643 - val_loss: 15.0876 - val_mse: 15.0876 - val_mae: 1.6213 - lr: 1.1353e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 18/100\n",
            "1000/1000 - 2s - loss: 14.9346 - mse: 14.9346 - mae: 1.5625 - val_loss: 15.0923 - val_mse: 15.0923 - val_mae: 1.6207 - lr: 1.1353e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 19/100\n",
            "1000/1000 - 2s - loss: 14.9352 - mse: 14.9352 - mae: 1.5617 - val_loss: 15.1042 - val_mse: 15.1042 - val_mae: 1.6153 - lr: 1.1353e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 20/100\n",
            "1000/1000 - 2s - loss: 14.9305 - mse: 14.9305 - mae: 1.5606 - val_loss: 15.0701 - val_mse: 15.0701 - val_mae: 1.6195 - lr: 1.1353e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 21/100\n",
            "1000/1000 - 2s - loss: 14.9259 - mse: 14.9259 - mae: 1.5593 - val_loss: 15.0690 - val_mse: 15.0690 - val_mae: 1.6156 - lr: 1.1353e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 22/100\n",
            "1000/1000 - 2s - loss: 14.9207 - mse: 14.9207 - mae: 1.5627 - val_loss: 15.0520 - val_mse: 15.0520 - val_mae: 1.6328 - lr: 1.1353e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 23/100\n",
            "1000/1000 - 2s - loss: 14.9091 - mse: 14.9091 - mae: 1.5619 - val_loss: 15.0819 - val_mse: 15.0819 - val_mae: 1.6184 - lr: 1.1353e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 24/100\n",
            "1000/1000 - 2s - loss: 14.9243 - mse: 14.9243 - mae: 1.5604 - val_loss: 15.0925 - val_mse: 15.0925 - val_mae: 1.6088 - lr: 1.1353e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 25/100\n",
            "1000/1000 - 2s - loss: 14.9108 - mse: 14.9108 - mae: 1.5607 - val_loss: 15.1065 - val_mse: 15.1065 - val_mae: 1.6120 - lr: 1.1353e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 26/100\n",
            "1000/1000 - 2s - loss: 14.9029 - mse: 14.9029 - mae: 1.5647 - val_loss: 15.0996 - val_mse: 15.0996 - val_mae: 1.5979 - lr: 1.1353e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 27/100\n",
            "1000/1000 - 2s - loss: 14.9054 - mse: 14.9054 - mae: 1.5607 - val_loss: 15.1151 - val_mse: 15.1151 - val_mae: 1.6098 - lr: 1.1353e-04 - 2s/epoch - 2ms/step\n",
            "Score for fold 2: loss of 15.11508560180664\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 14.0550 - mse: 14.0550 - mae: 1.5642 - val_loss: 18.4614 - val_mse: 18.4614 - val_mae: 1.5986 - lr: 1.1353e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 14.0488 - mse: 14.0488 - mae: 1.5608 - val_loss: 18.5125 - val_mse: 18.5125 - val_mae: 1.6015 - lr: 1.1353e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 14.0563 - mse: 14.0563 - mae: 1.5628 - val_loss: 18.4762 - val_mse: 18.4762 - val_mae: 1.5992 - lr: 1.1353e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 14.0348 - mse: 14.0348 - mae: 1.5621 - val_loss: 18.4751 - val_mse: 18.4751 - val_mae: 1.6090 - lr: 1.1353e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 14.0303 - mse: 14.0303 - mae: 1.5622 - val_loss: 18.5032 - val_mse: 18.5032 - val_mae: 1.6110 - lr: 1.1353e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 14.0312 - mse: 14.0312 - mae: 1.5618 - val_loss: 18.4867 - val_mse: 18.4867 - val_mae: 1.6003 - lr: 1.1353e-04 - 2s/epoch - 2ms/step\n",
            "Score for fold 3: loss of 18.486656188964844\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 16.1218 - mse: 16.1218 - mae: 1.5892 - val_loss: 10.0958 - val_mse: 10.0958 - val_mae: 1.5146 - lr: 1.1353e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 16.1088 - mse: 16.1088 - mae: 1.5922 - val_loss: 10.0915 - val_mse: 10.0915 - val_mae: 1.5113 - lr: 1.1353e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 16.1110 - mse: 16.1110 - mae: 1.5881 - val_loss: 10.1282 - val_mse: 10.1282 - val_mae: 1.5255 - lr: 1.1353e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 16.1087 - mse: 16.1087 - mae: 1.5919 - val_loss: 10.1041 - val_mse: 10.1041 - val_mae: 1.5290 - lr: 1.1353e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 16.1035 - mse: 16.1035 - mae: 1.5909 - val_loss: 10.1082 - val_mse: 10.1082 - val_mae: 1.5191 - lr: 1.1353e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 16.0941 - mse: 16.0941 - mae: 1.5882 - val_loss: 10.1581 - val_mse: 10.1581 - val_mae: 1.4950 - lr: 1.1353e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 16.1018 - mse: 16.1018 - mae: 1.5902 - val_loss: 10.1239 - val_mse: 10.1239 - val_mae: 1.5061 - lr: 1.1353e-04 - 2s/epoch - 2ms/step\n",
            "Score for fold 4: loss of 10.123946189880371\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 13.8036 - mse: 13.8036 - mae: 1.5752 - val_loss: 19.2808 - val_mse: 19.2808 - val_mae: 1.5576 - lr: 1.1353e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 13.8083 - mse: 13.8083 - mae: 1.5724 - val_loss: 19.2917 - val_mse: 19.2917 - val_mae: 1.5692 - lr: 1.1353e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 13.8080 - mse: 13.8080 - mae: 1.5759 - val_loss: 19.2607 - val_mse: 19.2607 - val_mae: 1.6000 - lr: 1.1353e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 13.7865 - mse: 13.7865 - mae: 1.5760 - val_loss: 19.3184 - val_mse: 19.3184 - val_mae: 1.5594 - lr: 1.1353e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 13.7942 - mse: 13.7942 - mae: 1.5737 - val_loss: 19.3001 - val_mse: 19.3001 - val_mae: 1.5936 - lr: 1.1353e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 13.7802 - mse: 13.7802 - mae: 1.5766 - val_loss: 19.2961 - val_mse: 19.2961 - val_mae: 1.5680 - lr: 1.1353e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 13.7827 - mse: 13.7827 - mae: 1.5728 - val_loss: 19.3056 - val_mse: 19.3056 - val_mae: 1.5928 - lr: 1.1353e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 2s - loss: 13.7888 - mse: 13.7888 - mae: 1.5729 - val_loss: 19.3017 - val_mse: 19.3017 - val_mae: 1.6036 - lr: 1.1353e-04 - 2s/epoch - 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-10-14 20:59:02,230]\u001b[0m Finished trial#10 resulted in value: 14.968. Current best value is 14.864 with parameters: {'activation': 'tanh', 'num_hidden_layer': 4, 'i': 6, 'learning_rate': 0.0003477839020306978}.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Score for fold 5: loss of 19.301645278930664\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 18.5868 - mse: 18.5868 - mae: 1.8929 - val_loss: 17.4430 - val_mse: 17.4430 - val_mae: 1.6536 - lr: 1.0601e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 15.3832 - mse: 15.3832 - mae: 1.6235 - val_loss: 17.3200 - val_mse: 17.3200 - val_mae: 1.6273 - lr: 1.0601e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 15.2798 - mse: 15.2798 - mae: 1.6150 - val_loss: 17.2107 - val_mse: 17.2107 - val_mae: 1.6429 - lr: 1.0601e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 15.2111 - mse: 15.2111 - mae: 1.6092 - val_loss: 17.1359 - val_mse: 17.1359 - val_mae: 1.6410 - lr: 1.0601e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 15.1554 - mse: 15.1554 - mae: 1.6097 - val_loss: 17.1040 - val_mse: 17.1040 - val_mae: 1.6366 - lr: 1.0601e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 15.1286 - mse: 15.1286 - mae: 1.6014 - val_loss: 17.0946 - val_mse: 17.0946 - val_mae: 1.6100 - lr: 1.0601e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 15.0912 - mse: 15.0912 - mae: 1.6002 - val_loss: 17.0335 - val_mse: 17.0335 - val_mae: 1.6347 - lr: 1.0601e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 2s - loss: 15.0623 - mse: 15.0623 - mae: 1.6033 - val_loss: 17.0050 - val_mse: 17.0050 - val_mae: 1.6223 - lr: 1.0601e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 2s - loss: 15.0449 - mse: 15.0449 - mae: 1.5987 - val_loss: 16.9891 - val_mse: 16.9891 - val_mae: 1.6178 - lr: 1.0601e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 2s - loss: 15.0094 - mse: 15.0094 - mae: 1.5986 - val_loss: 16.9484 - val_mse: 16.9484 - val_mae: 1.6242 - lr: 1.0601e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 2s - loss: 14.9849 - mse: 14.9849 - mae: 1.5949 - val_loss: 16.9415 - val_mse: 16.9415 - val_mae: 1.6198 - lr: 1.0601e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 2s - loss: 14.9678 - mse: 14.9678 - mae: 1.5941 - val_loss: 16.9266 - val_mse: 16.9266 - val_mae: 1.6196 - lr: 1.0601e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 13/100\n",
            "1000/1000 - 2s - loss: 14.9570 - mse: 14.9570 - mae: 1.5915 - val_loss: 16.9162 - val_mse: 16.9162 - val_mae: 1.6050 - lr: 1.0601e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 14/100\n",
            "1000/1000 - 2s - loss: 14.9386 - mse: 14.9386 - mae: 1.5893 - val_loss: 16.8712 - val_mse: 16.8712 - val_mae: 1.6113 - lr: 1.0601e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 15/100\n",
            "1000/1000 - 2s - loss: 14.9138 - mse: 14.9138 - mae: 1.5899 - val_loss: 16.8302 - val_mse: 16.8302 - val_mae: 1.6219 - lr: 1.0601e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 16/100\n",
            "1000/1000 - 2s - loss: 14.8914 - mse: 14.8914 - mae: 1.5933 - val_loss: 16.8435 - val_mse: 16.8435 - val_mae: 1.6018 - lr: 1.0601e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 17/100\n",
            "1000/1000 - 2s - loss: 14.8780 - mse: 14.8780 - mae: 1.5868 - val_loss: 16.7992 - val_mse: 16.7992 - val_mae: 1.6171 - lr: 1.0601e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 18/100\n",
            "1000/1000 - 2s - loss: 14.8623 - mse: 14.8623 - mae: 1.5871 - val_loss: 16.7916 - val_mse: 16.7916 - val_mae: 1.6107 - lr: 1.0601e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 19/100\n",
            "1000/1000 - 2s - loss: 14.8386 - mse: 14.8386 - mae: 1.5877 - val_loss: 16.8266 - val_mse: 16.8266 - val_mae: 1.5938 - lr: 1.0601e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 20/100\n",
            "1000/1000 - 2s - loss: 14.8297 - mse: 14.8297 - mae: 1.5861 - val_loss: 16.7903 - val_mse: 16.7903 - val_mae: 1.5939 - lr: 1.0601e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 21/100\n",
            "1000/1000 - 2s - loss: 14.8095 - mse: 14.8095 - mae: 1.5850 - val_loss: 16.7556 - val_mse: 16.7556 - val_mae: 1.6079 - lr: 1.0601e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 22/100\n",
            "1000/1000 - 2s - loss: 14.7942 - mse: 14.7942 - mae: 1.5839 - val_loss: 16.7580 - val_mse: 16.7580 - val_mae: 1.5898 - lr: 1.0601e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 23/100\n",
            "1000/1000 - 2s - loss: 14.7795 - mse: 14.7795 - mae: 1.5842 - val_loss: 16.7292 - val_mse: 16.7292 - val_mae: 1.6132 - lr: 1.0601e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 24/100\n",
            "1000/1000 - 2s - loss: 14.7577 - mse: 14.7577 - mae: 1.5846 - val_loss: 16.7424 - val_mse: 16.7424 - val_mae: 1.5928 - lr: 1.0601e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 25/100\n",
            "1000/1000 - 2s - loss: 14.7611 - mse: 14.7611 - mae: 1.5800 - val_loss: 16.6940 - val_mse: 16.6940 - val_mae: 1.6015 - lr: 1.0601e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 26/100\n",
            "1000/1000 - 2s - loss: 14.7416 - mse: 14.7416 - mae: 1.5834 - val_loss: 16.6984 - val_mse: 16.6984 - val_mae: 1.5915 - lr: 1.0601e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 27/100\n",
            "1000/1000 - 2s - loss: 14.7364 - mse: 14.7364 - mae: 1.5816 - val_loss: 16.6888 - val_mse: 16.6888 - val_mae: 1.5939 - lr: 1.0601e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 28/100\n",
            "1000/1000 - 2s - loss: 14.7184 - mse: 14.7184 - mae: 1.5801 - val_loss: 16.6975 - val_mse: 16.6975 - val_mae: 1.5901 - lr: 1.0601e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 29/100\n",
            "1000/1000 - 2s - loss: 14.7184 - mse: 14.7184 - mae: 1.5796 - val_loss: 16.6818 - val_mse: 16.6818 - val_mae: 1.6027 - lr: 1.0601e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 30/100\n",
            "1000/1000 - 2s - loss: 14.7046 - mse: 14.7046 - mae: 1.5814 - val_loss: 16.6553 - val_mse: 16.6553 - val_mae: 1.6010 - lr: 1.0601e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 31/100\n",
            "1000/1000 - 2s - loss: 14.7015 - mse: 14.7015 - mae: 1.5818 - val_loss: 16.6843 - val_mse: 16.6843 - val_mae: 1.5853 - lr: 1.0601e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 32/100\n",
            "1000/1000 - 2s - loss: 14.6986 - mse: 14.6986 - mae: 1.5774 - val_loss: 16.6415 - val_mse: 16.6415 - val_mae: 1.5989 - lr: 1.0601e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 33/100\n",
            "1000/1000 - 2s - loss: 14.6808 - mse: 14.6808 - mae: 1.5804 - val_loss: 16.6785 - val_mse: 16.6785 - val_mae: 1.5808 - lr: 1.0601e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 34/100\n",
            "1000/1000 - 2s - loss: 14.6768 - mse: 14.6768 - mae: 1.5780 - val_loss: 16.6705 - val_mse: 16.6705 - val_mae: 1.5944 - lr: 1.0601e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 35/100\n",
            "1000/1000 - 2s - loss: 14.6789 - mse: 14.6789 - mae: 1.5750 - val_loss: 16.6335 - val_mse: 16.6335 - val_mae: 1.5974 - lr: 1.0601e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 36/100\n",
            "1000/1000 - 2s - loss: 14.6664 - mse: 14.6664 - mae: 1.5771 - val_loss: 16.6377 - val_mse: 16.6377 - val_mae: 1.5926 - lr: 1.0601e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 37/100\n",
            "1000/1000 - 2s - loss: 14.6527 - mse: 14.6527 - mae: 1.5783 - val_loss: 16.6042 - val_mse: 16.6042 - val_mae: 1.6207 - lr: 1.0601e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 38/100\n",
            "1000/1000 - 2s - loss: 14.6487 - mse: 14.6487 - mae: 1.5786 - val_loss: 16.6287 - val_mse: 16.6287 - val_mae: 1.5915 - lr: 1.0601e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 39/100\n",
            "1000/1000 - 2s - loss: 14.6410 - mse: 14.6410 - mae: 1.5792 - val_loss: 16.6156 - val_mse: 16.6156 - val_mae: 1.5844 - lr: 1.0601e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 40/100\n",
            "1000/1000 - 2s - loss: 14.6453 - mse: 14.6453 - mae: 1.5729 - val_loss: 16.6053 - val_mse: 16.6053 - val_mae: 1.6051 - lr: 1.0601e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 41/100\n",
            "1000/1000 - 2s - loss: 14.6330 - mse: 14.6330 - mae: 1.5786 - val_loss: 16.5788 - val_mse: 16.5788 - val_mae: 1.5943 - lr: 1.0601e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 42/100\n",
            "1000/1000 - 2s - loss: 14.6274 - mse: 14.6274 - mae: 1.5759 - val_loss: 16.6058 - val_mse: 16.6058 - val_mae: 1.5878 - lr: 1.0601e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 43/100\n",
            "1000/1000 - 2s - loss: 14.6172 - mse: 14.6172 - mae: 1.5770 - val_loss: 16.5797 - val_mse: 16.5797 - val_mae: 1.6099 - lr: 1.0601e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 44/100\n",
            "1000/1000 - 2s - loss: 14.6224 - mse: 14.6224 - mae: 1.5749 - val_loss: 16.5906 - val_mse: 16.5906 - val_mae: 1.6009 - lr: 1.0601e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 45/100\n",
            "1000/1000 - 2s - loss: 14.6157 - mse: 14.6157 - mae: 1.5743 - val_loss: 16.5945 - val_mse: 16.5945 - val_mae: 1.6037 - lr: 1.0601e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 46/100\n",
            "1000/1000 - 2s - loss: 14.6161 - mse: 14.6161 - mae: 1.5749 - val_loss: 16.5680 - val_mse: 16.5680 - val_mae: 1.6070 - lr: 1.0601e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 47/100\n",
            "1000/1000 - 2s - loss: 14.6026 - mse: 14.6026 - mae: 1.5763 - val_loss: 16.5632 - val_mse: 16.5632 - val_mae: 1.5924 - lr: 1.0601e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 48/100\n",
            "1000/1000 - 2s - loss: 14.5927 - mse: 14.5927 - mae: 1.5732 - val_loss: 16.5794 - val_mse: 16.5794 - val_mae: 1.6028 - lr: 1.0601e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 49/100\n",
            "1000/1000 - 2s - loss: 14.5989 - mse: 14.5989 - mae: 1.5716 - val_loss: 16.5524 - val_mse: 16.5524 - val_mae: 1.6395 - lr: 1.0601e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 50/100\n",
            "1000/1000 - 2s - loss: 14.5800 - mse: 14.5800 - mae: 1.5769 - val_loss: 16.6320 - val_mse: 16.6320 - val_mae: 1.5789 - lr: 1.0601e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 51/100\n",
            "1000/1000 - 2s - loss: 14.5887 - mse: 14.5887 - mae: 1.5752 - val_loss: 16.6059 - val_mse: 16.6059 - val_mae: 1.5797 - lr: 1.0601e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 52/100\n",
            "1000/1000 - 2s - loss: 14.5827 - mse: 14.5827 - mae: 1.5720 - val_loss: 16.5619 - val_mse: 16.5619 - val_mae: 1.6052 - lr: 1.0601e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 53/100\n",
            "1000/1000 - 2s - loss: 14.5754 - mse: 14.5754 - mae: 1.5725 - val_loss: 16.5606 - val_mse: 16.5606 - val_mae: 1.6159 - lr: 1.0601e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 54/100\n",
            "1000/1000 - 2s - loss: 14.5695 - mse: 14.5695 - mae: 1.5739 - val_loss: 16.5700 - val_mse: 16.5700 - val_mae: 1.6010 - lr: 1.0601e-04 - 2s/epoch - 2ms/step\n",
            "Score for fold 1: loss of 16.570003509521484\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 15.6053 - mse: 15.6053 - mae: 1.5822 - val_loss: 12.4064 - val_mse: 12.4064 - val_mae: 1.5790 - lr: 1.0601e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 15.5926 - mse: 15.5926 - mae: 1.5803 - val_loss: 12.4352 - val_mse: 12.4352 - val_mae: 1.5547 - lr: 1.0601e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 15.6014 - mse: 15.6014 - mae: 1.5783 - val_loss: 12.4076 - val_mse: 12.4076 - val_mae: 1.5737 - lr: 1.0601e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 15.5887 - mse: 15.5887 - mae: 1.5801 - val_loss: 12.4365 - val_mse: 12.4365 - val_mae: 1.5521 - lr: 1.0601e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 15.5821 - mse: 15.5821 - mae: 1.5762 - val_loss: 12.4283 - val_mse: 12.4283 - val_mae: 1.5715 - lr: 1.0601e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 15.5858 - mse: 15.5858 - mae: 1.5802 - val_loss: 12.4314 - val_mse: 12.4314 - val_mae: 1.5621 - lr: 1.0601e-04 - 2s/epoch - 2ms/step\n",
            "Score for fold 2: loss of 12.431358337402344\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 13.2580 - mse: 13.2580 - mae: 1.5654 - val_loss: 21.7580 - val_mse: 21.7580 - val_mae: 1.6087 - lr: 1.0601e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 13.2494 - mse: 13.2494 - mae: 1.5625 - val_loss: 21.8052 - val_mse: 21.8052 - val_mae: 1.6033 - lr: 1.0601e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 13.2510 - mse: 13.2510 - mae: 1.5629 - val_loss: 21.7722 - val_mse: 21.7722 - val_mae: 1.6070 - lr: 1.0601e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 13.2556 - mse: 13.2556 - mae: 1.5613 - val_loss: 21.7984 - val_mse: 21.7984 - val_mae: 1.6004 - lr: 1.0601e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 13.2489 - mse: 13.2489 - mae: 1.5635 - val_loss: 21.8217 - val_mse: 21.8217 - val_mae: 1.5964 - lr: 1.0601e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 13.2628 - mse: 13.2628 - mae: 1.5574 - val_loss: 21.7895 - val_mse: 21.7895 - val_mae: 1.6090 - lr: 1.0601e-04 - 2s/epoch - 2ms/step\n",
            "Score for fold 3: loss of 21.7895565032959\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 16.1658 - mse: 16.1658 - mae: 1.5839 - val_loss: 10.0079 - val_mse: 10.0079 - val_mae: 1.5552 - lr: 1.0601e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 16.1620 - mse: 16.1620 - mae: 1.5875 - val_loss: 10.0401 - val_mse: 10.0401 - val_mae: 1.5260 - lr: 1.0601e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 16.1620 - mse: 16.1620 - mae: 1.5858 - val_loss: 10.0188 - val_mse: 10.0188 - val_mae: 1.5414 - lr: 1.0601e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 16.1486 - mse: 16.1486 - mae: 1.5836 - val_loss: 10.0108 - val_mse: 10.0108 - val_mae: 1.5574 - lr: 1.0601e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 16.1318 - mse: 16.1318 - mae: 1.5853 - val_loss: 10.0443 - val_mse: 10.0443 - val_mae: 1.5238 - lr: 1.0601e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 16.1432 - mse: 16.1432 - mae: 1.5822 - val_loss: 10.0225 - val_mse: 10.0225 - val_mae: 1.5576 - lr: 1.0601e-04 - 2s/epoch - 2ms/step\n",
            "Score for fold 4: loss of 10.022523880004883\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 15.1499 - mse: 15.1499 - mae: 1.5771 - val_loss: 13.9744 - val_mse: 13.9744 - val_mae: 1.5884 - lr: 1.0601e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 15.1461 - mse: 15.1461 - mae: 1.5749 - val_loss: 13.9904 - val_mse: 13.9904 - val_mae: 1.5644 - lr: 1.0601e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 15.1448 - mse: 15.1448 - mae: 1.5735 - val_loss: 13.9684 - val_mse: 13.9684 - val_mae: 1.5761 - lr: 1.0601e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 15.1514 - mse: 15.1514 - mae: 1.5757 - val_loss: 13.9955 - val_mse: 13.9955 - val_mae: 1.5656 - lr: 1.0601e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 15.1248 - mse: 15.1248 - mae: 1.5768 - val_loss: 14.0249 - val_mse: 14.0249 - val_mae: 1.5502 - lr: 1.0601e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 15.1378 - mse: 15.1378 - mae: 1.5721 - val_loss: 13.9836 - val_mse: 13.9836 - val_mae: 1.5636 - lr: 1.0601e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 15.1260 - mse: 15.1260 - mae: 1.5775 - val_loss: 13.9911 - val_mse: 13.9911 - val_mae: 1.5631 - lr: 1.0601e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 2s - loss: 15.1279 - mse: 15.1279 - mae: 1.5726 - val_loss: 14.0197 - val_mse: 14.0197 - val_mae: 1.5693 - lr: 1.0601e-04 - 2s/epoch - 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-10-14 21:01:13,950]\u001b[0m Finished trial#11 resulted in value: 14.966. Current best value is 14.864 with parameters: {'activation': 'tanh', 'num_hidden_layer': 4, 'i': 6, 'learning_rate': 0.0003477839020306978}.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Score for fold 5: loss of 14.019725799560547\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 17.7938 - mse: 17.7938 - mae: 1.8639 - val_loss: 18.2620 - val_mse: 18.2620 - val_mae: 1.6331 - lr: 1.1875e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 15.0994 - mse: 15.0994 - mae: 1.6080 - val_loss: 18.1232 - val_mse: 18.1232 - val_mae: 1.6325 - lr: 1.1875e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 14.9996 - mse: 14.9996 - mae: 1.6067 - val_loss: 18.0422 - val_mse: 18.0422 - val_mae: 1.6279 - lr: 1.1875e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 14.9380 - mse: 14.9380 - mae: 1.6021 - val_loss: 17.9626 - val_mse: 17.9626 - val_mae: 1.6322 - lr: 1.1875e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 14.8879 - mse: 14.8879 - mae: 1.5992 - val_loss: 17.9197 - val_mse: 17.9197 - val_mae: 1.6137 - lr: 1.1875e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 14.8336 - mse: 14.8336 - mae: 1.5956 - val_loss: 17.8661 - val_mse: 17.8661 - val_mae: 1.6179 - lr: 1.1875e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 14.7966 - mse: 14.7966 - mae: 1.5953 - val_loss: 17.8220 - val_mse: 17.8220 - val_mae: 1.6229 - lr: 1.1875e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 2s - loss: 14.7613 - mse: 14.7613 - mae: 1.5932 - val_loss: 17.7821 - val_mse: 17.7821 - val_mae: 1.6156 - lr: 1.1875e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 2s - loss: 14.7334 - mse: 14.7334 - mae: 1.5913 - val_loss: 17.7562 - val_mse: 17.7562 - val_mae: 1.6134 - lr: 1.1875e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 2s - loss: 14.7060 - mse: 14.7060 - mae: 1.5870 - val_loss: 17.7450 - val_mse: 17.7450 - val_mae: 1.6107 - lr: 1.1875e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 2s - loss: 14.6749 - mse: 14.6749 - mae: 1.5870 - val_loss: 17.6962 - val_mse: 17.6962 - val_mae: 1.6198 - lr: 1.1875e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 2s - loss: 14.6709 - mse: 14.6709 - mae: 1.5816 - val_loss: 17.6474 - val_mse: 17.6474 - val_mae: 1.6386 - lr: 1.1875e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 13/100\n",
            "1000/1000 - 2s - loss: 14.6449 - mse: 14.6449 - mae: 1.5850 - val_loss: 17.6564 - val_mse: 17.6564 - val_mae: 1.6052 - lr: 1.1875e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 14/100\n",
            "1000/1000 - 2s - loss: 14.6324 - mse: 14.6324 - mae: 1.5816 - val_loss: 17.6274 - val_mse: 17.6274 - val_mae: 1.6116 - lr: 1.1875e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 15/100\n",
            "1000/1000 - 2s - loss: 14.6152 - mse: 14.6152 - mae: 1.5802 - val_loss: 17.6361 - val_mse: 17.6361 - val_mae: 1.6176 - lr: 1.1875e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 16/100\n",
            "1000/1000 - 2s - loss: 14.5909 - mse: 14.5909 - mae: 1.5795 - val_loss: 17.5978 - val_mse: 17.5978 - val_mae: 1.6307 - lr: 1.1875e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 17/100\n",
            "1000/1000 - 2s - loss: 14.5909 - mse: 14.5909 - mae: 1.5799 - val_loss: 17.5842 - val_mse: 17.5842 - val_mae: 1.6248 - lr: 1.1875e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 18/100\n",
            "1000/1000 - 2s - loss: 14.5648 - mse: 14.5648 - mae: 1.5812 - val_loss: 17.5711 - val_mse: 17.5711 - val_mae: 1.6096 - lr: 1.1875e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 19/100\n",
            "1000/1000 - 2s - loss: 14.5605 - mse: 14.5605 - mae: 1.5772 - val_loss: 17.5644 - val_mse: 17.5644 - val_mae: 1.6231 - lr: 1.1875e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 20/100\n",
            "1000/1000 - 2s - loss: 14.5554 - mse: 14.5554 - mae: 1.5784 - val_loss: 17.5403 - val_mse: 17.5403 - val_mae: 1.6222 - lr: 1.1875e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 21/100\n",
            "1000/1000 - 2s - loss: 14.5425 - mse: 14.5425 - mae: 1.5762 - val_loss: 17.5768 - val_mse: 17.5768 - val_mae: 1.5988 - lr: 1.1875e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 22/100\n",
            "1000/1000 - 2s - loss: 14.5403 - mse: 14.5403 - mae: 1.5761 - val_loss: 17.5406 - val_mse: 17.5406 - val_mae: 1.6065 - lr: 1.1875e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 23/100\n",
            "1000/1000 - 2s - loss: 14.5253 - mse: 14.5253 - mae: 1.5743 - val_loss: 17.5622 - val_mse: 17.5622 - val_mae: 1.6090 - lr: 1.1875e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 24/100\n",
            "1000/1000 - 2s - loss: 14.5208 - mse: 14.5208 - mae: 1.5736 - val_loss: 17.5206 - val_mse: 17.5206 - val_mae: 1.6210 - lr: 1.1875e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 25/100\n",
            "1000/1000 - 2s - loss: 14.5093 - mse: 14.5093 - mae: 1.5768 - val_loss: 17.5486 - val_mse: 17.5486 - val_mae: 1.5983 - lr: 1.1875e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 26/100\n",
            "1000/1000 - 2s - loss: 14.5017 - mse: 14.5017 - mae: 1.5747 - val_loss: 17.5086 - val_mse: 17.5086 - val_mae: 1.6186 - lr: 1.1875e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 27/100\n",
            "1000/1000 - 2s - loss: 14.4924 - mse: 14.4924 - mae: 1.5773 - val_loss: 17.4934 - val_mse: 17.4934 - val_mae: 1.6167 - lr: 1.1875e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 28/100\n",
            "1000/1000 - 2s - loss: 14.4889 - mse: 14.4889 - mae: 1.5732 - val_loss: 17.5129 - val_mse: 17.5129 - val_mae: 1.6093 - lr: 1.1875e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 29/100\n",
            "1000/1000 - 2s - loss: 14.4872 - mse: 14.4872 - mae: 1.5714 - val_loss: 17.4909 - val_mse: 17.4909 - val_mae: 1.6112 - lr: 1.1875e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 30/100\n",
            "1000/1000 - 2s - loss: 14.4825 - mse: 14.4825 - mae: 1.5754 - val_loss: 17.5039 - val_mse: 17.5039 - val_mae: 1.6056 - lr: 1.1875e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 31/100\n",
            "1000/1000 - 2s - loss: 14.4776 - mse: 14.4776 - mae: 1.5712 - val_loss: 17.4881 - val_mse: 17.4881 - val_mae: 1.6105 - lr: 1.1875e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 32/100\n",
            "1000/1000 - 2s - loss: 14.4719 - mse: 14.4719 - mae: 1.5734 - val_loss: 17.5052 - val_mse: 17.5052 - val_mae: 1.6144 - lr: 1.1875e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 33/100\n",
            "1000/1000 - 2s - loss: 14.4699 - mse: 14.4699 - mae: 1.5723 - val_loss: 17.4775 - val_mse: 17.4775 - val_mae: 1.6225 - lr: 1.1875e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 34/100\n",
            "1000/1000 - 2s - loss: 14.4691 - mse: 14.4691 - mae: 1.5703 - val_loss: 17.4493 - val_mse: 17.4493 - val_mae: 1.6204 - lr: 1.1875e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 35/100\n",
            "1000/1000 - 2s - loss: 14.4497 - mse: 14.4497 - mae: 1.5719 - val_loss: 17.4543 - val_mse: 17.4543 - val_mae: 1.6168 - lr: 1.1875e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 36/100\n",
            "1000/1000 - 2s - loss: 14.4501 - mse: 14.4501 - mae: 1.5726 - val_loss: 17.4591 - val_mse: 17.4591 - val_mae: 1.6097 - lr: 1.1875e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 37/100\n",
            "1000/1000 - 2s - loss: 14.4453 - mse: 14.4453 - mae: 1.5708 - val_loss: 17.4560 - val_mse: 17.4560 - val_mae: 1.5978 - lr: 1.1875e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 38/100\n",
            "1000/1000 - 2s - loss: 14.4406 - mse: 14.4406 - mae: 1.5683 - val_loss: 17.4633 - val_mse: 17.4633 - val_mae: 1.6075 - lr: 1.1875e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 39/100\n",
            "1000/1000 - 2s - loss: 14.4444 - mse: 14.4444 - mae: 1.5702 - val_loss: 17.4481 - val_mse: 17.4481 - val_mae: 1.6041 - lr: 1.1875e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 40/100\n",
            "1000/1000 - 2s - loss: 14.4347 - mse: 14.4347 - mae: 1.5680 - val_loss: 17.4376 - val_mse: 17.4376 - val_mae: 1.6092 - lr: 1.1875e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 41/100\n",
            "1000/1000 - 2s - loss: 14.4409 - mse: 14.4409 - mae: 1.5680 - val_loss: 17.4632 - val_mse: 17.4632 - val_mae: 1.6067 - lr: 1.1875e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 42/100\n",
            "1000/1000 - 2s - loss: 14.4238 - mse: 14.4238 - mae: 1.5709 - val_loss: 17.4498 - val_mse: 17.4498 - val_mae: 1.5938 - lr: 1.1875e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 43/100\n",
            "1000/1000 - 2s - loss: 14.4225 - mse: 14.4225 - mae: 1.5700 - val_loss: 17.4936 - val_mse: 17.4936 - val_mae: 1.5793 - lr: 1.1875e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 44/100\n",
            "1000/1000 - 2s - loss: 14.4331 - mse: 14.4331 - mae: 1.5676 - val_loss: 17.4355 - val_mse: 17.4355 - val_mae: 1.6117 - lr: 1.1875e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 45/100\n",
            "1000/1000 - 2s - loss: 14.4184 - mse: 14.4184 - mae: 1.5709 - val_loss: 17.4121 - val_mse: 17.4121 - val_mae: 1.6014 - lr: 1.1875e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 46/100\n",
            "1000/1000 - 2s - loss: 14.4118 - mse: 14.4118 - mae: 1.5698 - val_loss: 17.4449 - val_mse: 17.4449 - val_mae: 1.5963 - lr: 1.1875e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 47/100\n",
            "1000/1000 - 2s - loss: 14.4198 - mse: 14.4198 - mae: 1.5658 - val_loss: 17.4512 - val_mse: 17.4512 - val_mae: 1.5865 - lr: 1.1875e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 48/100\n",
            "1000/1000 - 2s - loss: 14.4132 - mse: 14.4132 - mae: 1.5665 - val_loss: 17.4236 - val_mse: 17.4236 - val_mae: 1.6143 - lr: 1.1875e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 49/100\n",
            "1000/1000 - 2s - loss: 14.4024 - mse: 14.4024 - mae: 1.5674 - val_loss: 17.4124 - val_mse: 17.4124 - val_mae: 1.6020 - lr: 1.1875e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 50/100\n",
            "1000/1000 - 2s - loss: 14.4000 - mse: 14.4000 - mae: 1.5685 - val_loss: 17.4335 - val_mse: 17.4335 - val_mae: 1.6043 - lr: 1.1875e-04 - 2s/epoch - 2ms/step\n",
            "Score for fold 1: loss of 17.43347930908203\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 15.1726 - mse: 15.1726 - mae: 1.5723 - val_loss: 14.3855 - val_mse: 14.3855 - val_mae: 1.5922 - lr: 1.1875e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 15.1722 - mse: 15.1722 - mae: 1.5697 - val_loss: 14.3582 - val_mse: 14.3582 - val_mae: 1.6142 - lr: 1.1875e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 15.1652 - mse: 15.1652 - mae: 1.5715 - val_loss: 14.3746 - val_mse: 14.3746 - val_mae: 1.6128 - lr: 1.1875e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 15.1597 - mse: 15.1597 - mae: 1.5692 - val_loss: 14.4000 - val_mse: 14.4000 - val_mae: 1.5911 - lr: 1.1875e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 15.1618 - mse: 15.1618 - mae: 1.5700 - val_loss: 14.3578 - val_mse: 14.3578 - val_mae: 1.6025 - lr: 1.1875e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 15.1540 - mse: 15.1540 - mae: 1.5692 - val_loss: 14.4086 - val_mse: 14.4086 - val_mae: 1.5919 - lr: 1.1875e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 15.1530 - mse: 15.1530 - mae: 1.5714 - val_loss: 14.4240 - val_mse: 14.4240 - val_mae: 1.5785 - lr: 1.1875e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 2s - loss: 15.1562 - mse: 15.1562 - mae: 1.5683 - val_loss: 14.4073 - val_mse: 14.4073 - val_mae: 1.5858 - lr: 1.1875e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 2s - loss: 15.1425 - mse: 15.1425 - mae: 1.5695 - val_loss: 14.3484 - val_mse: 14.3484 - val_mae: 1.6196 - lr: 1.1875e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 2s - loss: 15.1409 - mse: 15.1409 - mae: 1.5670 - val_loss: 14.3765 - val_mse: 14.3765 - val_mae: 1.5878 - lr: 1.1875e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 2s - loss: 15.1420 - mse: 15.1420 - mae: 1.5689 - val_loss: 14.3841 - val_mse: 14.3841 - val_mae: 1.5884 - lr: 1.1875e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 2s - loss: 15.1392 - mse: 15.1392 - mae: 1.5687 - val_loss: 14.3853 - val_mse: 14.3853 - val_mae: 1.5945 - lr: 1.1875e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 13/100\n",
            "1000/1000 - 2s - loss: 15.1357 - mse: 15.1357 - mae: 1.5670 - val_loss: 14.4230 - val_mse: 14.4230 - val_mae: 1.5842 - lr: 1.1875e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 14/100\n",
            "1000/1000 - 2s - loss: 15.1303 - mse: 15.1303 - mae: 1.5673 - val_loss: 14.4389 - val_mse: 14.4389 - val_mae: 1.5855 - lr: 1.1875e-04 - 2s/epoch - 2ms/step\n",
            "Score for fold 2: loss of 14.438850402832031\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 15.7925 - mse: 15.7925 - mae: 1.5843 - val_loss: 11.6358 - val_mse: 11.6358 - val_mae: 1.5623 - lr: 1.1875e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 15.7829 - mse: 15.7829 - mae: 1.5863 - val_loss: 11.6580 - val_mse: 11.6580 - val_mae: 1.5278 - lr: 1.1875e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 15.7853 - mse: 15.7853 - mae: 1.5863 - val_loss: 11.6519 - val_mse: 11.6519 - val_mae: 1.5536 - lr: 1.1875e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 15.7753 - mse: 15.7753 - mae: 1.5872 - val_loss: 11.6680 - val_mse: 11.6680 - val_mae: 1.5330 - lr: 1.1875e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 15.7622 - mse: 15.7622 - mae: 1.5826 - val_loss: 11.6633 - val_mse: 11.6633 - val_mae: 1.5477 - lr: 1.1875e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 15.7539 - mse: 15.7539 - mae: 1.5842 - val_loss: 11.6613 - val_mse: 11.6613 - val_mae: 1.5587 - lr: 1.1875e-04 - 2s/epoch - 2ms/step\n",
            "Score for fold 3: loss of 11.66130542755127\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 13.7734 - mse: 13.7734 - mae: 1.5781 - val_loss: 19.6297 - val_mse: 19.6297 - val_mae: 1.5501 - lr: 1.1875e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 13.7649 - mse: 13.7649 - mae: 1.5780 - val_loss: 19.6235 - val_mse: 19.6235 - val_mae: 1.5533 - lr: 1.1875e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 13.7721 - mse: 13.7721 - mae: 1.5753 - val_loss: 19.6275 - val_mse: 19.6275 - val_mae: 1.5556 - lr: 1.1875e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 13.7663 - mse: 13.7663 - mae: 1.5782 - val_loss: 19.6306 - val_mse: 19.6306 - val_mae: 1.5790 - lr: 1.1875e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 13.7612 - mse: 13.7612 - mae: 1.5789 - val_loss: 19.6621 - val_mse: 19.6621 - val_mae: 1.5565 - lr: 1.1875e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 13.7613 - mse: 13.7613 - mae: 1.5757 - val_loss: 19.6423 - val_mse: 19.6423 - val_mae: 1.5754 - lr: 1.1875e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 13.7572 - mse: 13.7572 - mae: 1.5774 - val_loss: 19.6736 - val_mse: 19.6736 - val_mae: 1.5402 - lr: 1.1875e-04 - 2s/epoch - 2ms/step\n",
            "Score for fold 4: loss of 19.673593521118164\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 15.7215 - mse: 15.7215 - mae: 1.5707 - val_loss: 11.7738 - val_mse: 11.7738 - val_mae: 1.5925 - lr: 1.1875e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 15.7152 - mse: 15.7152 - mae: 1.5743 - val_loss: 11.7936 - val_mse: 11.7936 - val_mae: 1.5673 - lr: 1.1875e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 15.7116 - mse: 15.7116 - mae: 1.5702 - val_loss: 11.7912 - val_mse: 11.7912 - val_mae: 1.5913 - lr: 1.1875e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 15.7142 - mse: 15.7142 - mae: 1.5710 - val_loss: 11.8061 - val_mse: 11.8061 - val_mae: 1.5740 - lr: 1.1875e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 15.7021 - mse: 15.7021 - mae: 1.5712 - val_loss: 11.8253 - val_mse: 11.8253 - val_mae: 1.5753 - lr: 1.1875e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 15.7038 - mse: 15.7038 - mae: 1.5716 - val_loss: 11.8541 - val_mse: 11.8541 - val_mae: 1.5675 - lr: 1.1875e-04 - 2s/epoch - 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-10-14 21:03:31,364]\u001b[0m Finished trial#12 resulted in value: 15.01. Current best value is 14.864 with parameters: {'activation': 'tanh', 'num_hidden_layer': 4, 'i': 6, 'learning_rate': 0.0003477839020306978}.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Score for fold 5: loss of 11.854129791259766\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 17.6386 - mse: 17.6386 - mae: 1.7001 - val_loss: 10.3226 - val_mse: 10.3226 - val_mae: 1.5529 - lr: 2.9074e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 16.7725 - mse: 16.7725 - mae: 1.6181 - val_loss: 10.1483 - val_mse: 10.1483 - val_mae: 1.5840 - lr: 2.9074e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 16.6587 - mse: 16.6587 - mae: 1.6144 - val_loss: 10.1211 - val_mse: 10.1211 - val_mae: 1.5639 - lr: 2.9074e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 16.5928 - mse: 16.5928 - mae: 1.6040 - val_loss: 10.0281 - val_mse: 10.0281 - val_mae: 1.5685 - lr: 2.9074e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 16.5115 - mse: 16.5115 - mae: 1.6040 - val_loss: 10.0524 - val_mse: 10.0524 - val_mae: 1.5568 - lr: 2.9074e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 16.4967 - mse: 16.4967 - mae: 1.6010 - val_loss: 10.0275 - val_mse: 10.0275 - val_mae: 1.5304 - lr: 2.9074e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 16.4737 - mse: 16.4737 - mae: 1.5995 - val_loss: 9.9448 - val_mse: 9.9448 - val_mae: 1.5647 - lr: 2.9074e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 2s - loss: 16.4285 - mse: 16.4285 - mae: 1.5995 - val_loss: 9.9615 - val_mse: 9.9615 - val_mae: 1.5480 - lr: 2.9074e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 2s - loss: 16.4209 - mse: 16.4209 - mae: 1.5967 - val_loss: 9.8842 - val_mse: 9.8842 - val_mae: 1.5283 - lr: 2.9074e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 2s - loss: 16.3879 - mse: 16.3879 - mae: 1.5945 - val_loss: 9.9084 - val_mse: 9.9084 - val_mae: 1.5230 - lr: 2.9074e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 2s - loss: 16.3718 - mse: 16.3718 - mae: 1.5926 - val_loss: 9.9912 - val_mse: 9.9912 - val_mae: 1.6171 - lr: 2.9074e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 2s - loss: 16.3713 - mse: 16.3713 - mae: 1.5885 - val_loss: 9.8772 - val_mse: 9.8772 - val_mae: 1.5837 - lr: 2.9074e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 13/100\n",
            "1000/1000 - 2s - loss: 16.3541 - mse: 16.3541 - mae: 1.5927 - val_loss: 9.9822 - val_mse: 9.9822 - val_mae: 1.5145 - lr: 2.9074e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 14/100\n",
            "1000/1000 - 2s - loss: 16.3554 - mse: 16.3554 - mae: 1.5899 - val_loss: 9.9738 - val_mse: 9.9738 - val_mae: 1.5298 - lr: 2.9074e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 15/100\n",
            "1000/1000 - 2s - loss: 16.3290 - mse: 16.3290 - mae: 1.5901 - val_loss: 9.9091 - val_mse: 9.9091 - val_mae: 1.5411 - lr: 2.9074e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 16/100\n",
            "1000/1000 - 2s - loss: 16.3342 - mse: 16.3342 - mae: 1.5911 - val_loss: 9.9152 - val_mse: 9.9152 - val_mae: 1.5542 - lr: 2.9074e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 17/100\n",
            "1000/1000 - 2s - loss: 16.3201 - mse: 16.3201 - mae: 1.5910 - val_loss: 10.0029 - val_mse: 10.0029 - val_mae: 1.5592 - lr: 2.9074e-04 - 2s/epoch - 2ms/step\n",
            "Score for fold 1: loss of 10.002884864807129\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 15.8775 - mse: 15.8775 - mae: 1.5936 - val_loss: 11.5491 - val_mse: 11.5491 - val_mae: 1.5161 - lr: 2.9074e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 15.8616 - mse: 15.8616 - mae: 1.5915 - val_loss: 11.5321 - val_mse: 11.5321 - val_mae: 1.5694 - lr: 2.9074e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 15.8430 - mse: 15.8430 - mae: 1.5943 - val_loss: 11.5576 - val_mse: 11.5576 - val_mae: 1.5407 - lr: 2.9074e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 15.8489 - mse: 15.8489 - mae: 1.5920 - val_loss: 11.5721 - val_mse: 11.5721 - val_mae: 1.5289 - lr: 2.9074e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 15.8198 - mse: 15.8198 - mae: 1.5897 - val_loss: 11.5612 - val_mse: 11.5612 - val_mae: 1.5605 - lr: 2.9074e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 15.8368 - mse: 15.8368 - mae: 1.5918 - val_loss: 11.5867 - val_mse: 11.5867 - val_mae: 1.5427 - lr: 2.9074e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 15.8339 - mse: 15.8339 - mae: 1.5902 - val_loss: 11.5370 - val_mse: 11.5370 - val_mae: 1.5412 - lr: 2.9074e-04 - 2s/epoch - 2ms/step\n",
            "Score for fold 2: loss of 11.536962509155273\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 13.4587 - mse: 13.4587 - mae: 1.5606 - val_loss: 20.9500 - val_mse: 20.9500 - val_mae: 1.6346 - lr: 2.9074e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 13.4444 - mse: 13.4444 - mae: 1.5616 - val_loss: 20.9920 - val_mse: 20.9920 - val_mae: 1.6208 - lr: 2.9074e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 13.4376 - mse: 13.4376 - mae: 1.5607 - val_loss: 20.9701 - val_mse: 20.9701 - val_mae: 1.5907 - lr: 2.9074e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 13.4038 - mse: 13.4038 - mae: 1.5585 - val_loss: 20.9761 - val_mse: 20.9761 - val_mae: 1.6277 - lr: 2.9074e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 13.4106 - mse: 13.4106 - mae: 1.5613 - val_loss: 21.0279 - val_mse: 21.0279 - val_mae: 1.5793 - lr: 2.9074e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 13.3938 - mse: 13.3938 - mae: 1.5592 - val_loss: 21.0133 - val_mse: 21.0133 - val_mae: 1.5929 - lr: 2.9074e-04 - 2s/epoch - 2ms/step\n",
            "Score for fold 3: loss of 21.013320922851562\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 14.0304 - mse: 14.0304 - mae: 1.5650 - val_loss: 18.3872 - val_mse: 18.3872 - val_mae: 1.6008 - lr: 2.9074e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 14.0188 - mse: 14.0188 - mae: 1.5670 - val_loss: 18.4665 - val_mse: 18.4665 - val_mae: 1.6181 - lr: 2.9074e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 14.0155 - mse: 14.0155 - mae: 1.5648 - val_loss: 18.4853 - val_mse: 18.4853 - val_mae: 1.5976 - lr: 2.9074e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 13.9862 - mse: 13.9862 - mae: 1.5673 - val_loss: 18.4594 - val_mse: 18.4594 - val_mae: 1.5988 - lr: 2.9074e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 14.0099 - mse: 14.0099 - mae: 1.5612 - val_loss: 18.4455 - val_mse: 18.4455 - val_mae: 1.5852 - lr: 2.9074e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 13.9894 - mse: 13.9894 - mae: 1.5655 - val_loss: 18.4325 - val_mse: 18.4325 - val_mae: 1.6142 - lr: 2.9074e-04 - 2s/epoch - 2ms/step\n",
            "Score for fold 4: loss of 18.432443618774414\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 15.1341 - mse: 15.1341 - mae: 1.5737 - val_loss: 13.8202 - val_mse: 13.8202 - val_mae: 1.5611 - lr: 2.9074e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 15.1098 - mse: 15.1098 - mae: 1.5697 - val_loss: 13.9924 - val_mse: 13.9924 - val_mae: 1.5709 - lr: 2.9074e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 15.0940 - mse: 15.0940 - mae: 1.5700 - val_loss: 13.9286 - val_mse: 13.9286 - val_mae: 1.5427 - lr: 2.9074e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 15.0965 - mse: 15.0965 - mae: 1.5668 - val_loss: 13.9514 - val_mse: 13.9514 - val_mae: 1.5681 - lr: 2.9074e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 15.0872 - mse: 15.0872 - mae: 1.5706 - val_loss: 13.8408 - val_mse: 13.8408 - val_mae: 1.5855 - lr: 2.9074e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 15.0729 - mse: 15.0729 - mae: 1.5708 - val_loss: 13.9129 - val_mse: 13.9129 - val_mae: 1.5558 - lr: 2.9074e-04 - 2s/epoch - 2ms/step\n",
            "Score for fold 5: loss of 13.912858963012695\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-10-14 21:04:50,022]\u001b[0m Finished trial#13 resulted in value: 14.978. Current best value is 14.864 with parameters: {'activation': 'tanh', 'num_hidden_layer': 4, 'i': 6, 'learning_rate': 0.0003477839020306978}.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 16.7004 - mse: 16.7004 - mae: 1.7063 - val_loss: 14.7762 - val_mse: 14.7762 - val_mae: 1.5842 - lr: 2.2683e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 15.6779 - mse: 15.6779 - mae: 1.6140 - val_loss: 14.7637 - val_mse: 14.7637 - val_mae: 1.5741 - lr: 2.2683e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 15.6261 - mse: 15.6261 - mae: 1.6106 - val_loss: 14.7556 - val_mse: 14.7556 - val_mae: 1.5890 - lr: 2.2683e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 15.5845 - mse: 15.5845 - mae: 1.6092 - val_loss: 14.6751 - val_mse: 14.6751 - val_mae: 1.5823 - lr: 2.2683e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 15.5576 - mse: 15.5576 - mae: 1.6022 - val_loss: 14.5451 - val_mse: 14.5451 - val_mae: 1.6270 - lr: 2.2683e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 15.5073 - mse: 15.5073 - mae: 1.6034 - val_loss: 14.5194 - val_mse: 14.5194 - val_mae: 1.6077 - lr: 2.2683e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 15.4801 - mse: 15.4801 - mae: 1.5977 - val_loss: 14.4737 - val_mse: 14.4737 - val_mae: 1.5689 - lr: 2.2683e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 2s - loss: 15.4261 - mse: 15.4261 - mae: 1.5986 - val_loss: 14.5092 - val_mse: 14.5092 - val_mae: 1.5649 - lr: 2.2683e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 2s - loss: 15.3992 - mse: 15.3992 - mae: 1.5950 - val_loss: 14.3687 - val_mse: 14.3687 - val_mae: 1.5791 - lr: 2.2683e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 2s - loss: 15.3761 - mse: 15.3761 - mae: 1.5911 - val_loss: 14.3664 - val_mse: 14.3664 - val_mae: 1.5851 - lr: 2.2683e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 2s - loss: 15.3420 - mse: 15.3420 - mae: 1.5909 - val_loss: 14.4117 - val_mse: 14.4117 - val_mae: 1.5667 - lr: 2.2683e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 2s - loss: 15.3271 - mse: 15.3271 - mae: 1.5883 - val_loss: 14.2405 - val_mse: 14.2405 - val_mae: 1.6099 - lr: 2.2683e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 13/100\n",
            "1000/1000 - 2s - loss: 15.3048 - mse: 15.3048 - mae: 1.5895 - val_loss: 14.2574 - val_mse: 14.2574 - val_mae: 1.5758 - lr: 2.2683e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 14/100\n",
            "1000/1000 - 2s - loss: 15.2990 - mse: 15.2990 - mae: 1.5873 - val_loss: 14.1954 - val_mse: 14.1954 - val_mae: 1.5703 - lr: 2.2683e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 15/100\n",
            "1000/1000 - 2s - loss: 15.2651 - mse: 15.2651 - mae: 1.5861 - val_loss: 14.2525 - val_mse: 14.2525 - val_mae: 1.5754 - lr: 2.2683e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 16/100\n",
            "1000/1000 - 2s - loss: 15.2735 - mse: 15.2735 - mae: 1.5877 - val_loss: 14.2439 - val_mse: 14.2439 - val_mae: 1.5720 - lr: 2.2683e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 17/100\n",
            "1000/1000 - 2s - loss: 15.2355 - mse: 15.2355 - mae: 1.5886 - val_loss: 14.2245 - val_mse: 14.2245 - val_mae: 1.5817 - lr: 2.2683e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 18/100\n",
            "1000/1000 - 2s - loss: 15.2442 - mse: 15.2442 - mae: 1.5855 - val_loss: 14.1628 - val_mse: 14.1628 - val_mae: 1.5789 - lr: 2.2683e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 19/100\n",
            "1000/1000 - 2s - loss: 15.2352 - mse: 15.2352 - mae: 1.5846 - val_loss: 14.2373 - val_mse: 14.2373 - val_mae: 1.5476 - lr: 2.2683e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 20/100\n",
            "1000/1000 - 2s - loss: 15.2453 - mse: 15.2453 - mae: 1.5849 - val_loss: 14.2227 - val_mse: 14.2227 - val_mae: 1.5553 - lr: 2.2683e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 21/100\n",
            "1000/1000 - 2s - loss: 15.2293 - mse: 15.2293 - mae: 1.5839 - val_loss: 14.2484 - val_mse: 14.2484 - val_mae: 1.5597 - lr: 2.2683e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 22/100\n",
            "1000/1000 - 2s - loss: 15.2318 - mse: 15.2318 - mae: 1.5833 - val_loss: 14.1911 - val_mse: 14.1911 - val_mae: 1.5744 - lr: 2.2683e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 23/100\n",
            "1000/1000 - 2s - loss: 15.2237 - mse: 15.2237 - mae: 1.5808 - val_loss: 14.1979 - val_mse: 14.1979 - val_mae: 1.5636 - lr: 2.2683e-04 - 2s/epoch - 2ms/step\n",
            "Score for fold 1: loss of 14.197920799255371\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 16.2764 - mse: 16.2764 - mae: 1.5825 - val_loss: 9.9707 - val_mse: 9.9707 - val_mae: 1.5571 - lr: 2.2683e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 16.2528 - mse: 16.2528 - mae: 1.5810 - val_loss: 9.9032 - val_mse: 9.9032 - val_mae: 1.5668 - lr: 2.2683e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 16.2321 - mse: 16.2321 - mae: 1.5806 - val_loss: 10.0210 - val_mse: 10.0210 - val_mae: 1.5722 - lr: 2.2683e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 16.2343 - mse: 16.2343 - mae: 1.5844 - val_loss: 9.9439 - val_mse: 9.9439 - val_mae: 1.6310 - lr: 2.2683e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 16.2281 - mse: 16.2281 - mae: 1.5819 - val_loss: 9.9557 - val_mse: 9.9557 - val_mae: 1.5549 - lr: 2.2683e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 16.2209 - mse: 16.2209 - mae: 1.5818 - val_loss: 9.9718 - val_mse: 9.9718 - val_mae: 1.5761 - lr: 2.2683e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 16.2027 - mse: 16.2027 - mae: 1.5824 - val_loss: 10.0172 - val_mse: 10.0172 - val_mae: 1.5758 - lr: 2.2683e-04 - 2s/epoch - 2ms/step\n",
            "Score for fold 2: loss of 10.017160415649414\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 14.9424 - mse: 14.9424 - mae: 1.5708 - val_loss: 15.0984 - val_mse: 15.0984 - val_mae: 1.6073 - lr: 2.2683e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 14.9185 - mse: 14.9185 - mae: 1.5664 - val_loss: 15.1124 - val_mse: 15.1124 - val_mae: 1.6068 - lr: 2.2683e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 14.9178 - mse: 14.9178 - mae: 1.5689 - val_loss: 15.1072 - val_mse: 15.1072 - val_mae: 1.6468 - lr: 2.2683e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 14.9004 - mse: 14.9004 - mae: 1.5704 - val_loss: 15.1319 - val_mse: 15.1319 - val_mae: 1.6112 - lr: 2.2683e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 14.8989 - mse: 14.8989 - mae: 1.5681 - val_loss: 15.1157 - val_mse: 15.1157 - val_mae: 1.5936 - lr: 2.2683e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 14.9001 - mse: 14.9001 - mae: 1.5699 - val_loss: 15.0867 - val_mse: 15.0867 - val_mae: 1.6242 - lr: 2.2683e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 14.8902 - mse: 14.8902 - mae: 1.5707 - val_loss: 15.2587 - val_mse: 15.2587 - val_mae: 1.5899 - lr: 2.2683e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 2s - loss: 14.8821 - mse: 14.8821 - mae: 1.5674 - val_loss: 15.1693 - val_mse: 15.1693 - val_mae: 1.5994 - lr: 2.2683e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 2s - loss: 14.8700 - mse: 14.8700 - mae: 1.5671 - val_loss: 15.2079 - val_mse: 15.2079 - val_mae: 1.5946 - lr: 2.2683e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 2s - loss: 14.8788 - mse: 14.8788 - mae: 1.5696 - val_loss: 15.2342 - val_mse: 15.2342 - val_mae: 1.5917 - lr: 2.2683e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 2s - loss: 14.8783 - mse: 14.8783 - mae: 1.5672 - val_loss: 15.2227 - val_mse: 15.2227 - val_mae: 1.5782 - lr: 2.2683e-04 - 2s/epoch - 2ms/step\n",
            "Score for fold 3: loss of 15.22272777557373\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 15.9180 - mse: 15.9180 - mae: 1.5806 - val_loss: 11.0842 - val_mse: 11.0842 - val_mae: 1.6013 - lr: 2.2683e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 15.9299 - mse: 15.9299 - mae: 1.5770 - val_loss: 10.9763 - val_mse: 10.9763 - val_mae: 1.5728 - lr: 2.2683e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 15.9145 - mse: 15.9145 - mae: 1.5798 - val_loss: 10.9765 - val_mse: 10.9765 - val_mae: 1.5571 - lr: 2.2683e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 15.9127 - mse: 15.9127 - mae: 1.5794 - val_loss: 11.0851 - val_mse: 11.0851 - val_mae: 1.5063 - lr: 2.2683e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 15.9074 - mse: 15.9074 - mae: 1.5751 - val_loss: 11.0810 - val_mse: 11.0810 - val_mae: 1.5677 - lr: 2.2683e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 15.8962 - mse: 15.8962 - mae: 1.5782 - val_loss: 11.0989 - val_mse: 11.0989 - val_mae: 1.5519 - lr: 2.2683e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 15.8979 - mse: 15.8979 - mae: 1.5769 - val_loss: 10.9856 - val_mse: 10.9856 - val_mae: 1.5614 - lr: 2.2683e-04 - 2s/epoch - 2ms/step\n",
            "Score for fold 4: loss of 10.98559284210205\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 12.5060 - mse: 12.5060 - mae: 1.5698 - val_loss: 24.5389 - val_mse: 24.5389 - val_mae: 1.6179 - lr: 2.2683e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 12.4792 - mse: 12.4792 - mae: 1.5682 - val_loss: 24.5868 - val_mse: 24.5868 - val_mae: 1.5674 - lr: 2.2683e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 12.4739 - mse: 12.4739 - mae: 1.5678 - val_loss: 24.5414 - val_mse: 24.5414 - val_mae: 1.6040 - lr: 2.2683e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 12.4689 - mse: 12.4689 - mae: 1.5655 - val_loss: 24.5400 - val_mse: 24.5400 - val_mae: 1.5898 - lr: 2.2683e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 12.4549 - mse: 12.4549 - mae: 1.5699 - val_loss: 24.5634 - val_mse: 24.5634 - val_mae: 1.5827 - lr: 2.2683e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 12.4506 - mse: 12.4506 - mae: 1.5685 - val_loss: 24.5884 - val_mse: 24.5884 - val_mae: 1.6040 - lr: 2.2683e-04 - 2s/epoch - 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-10-14 21:06:27,915]\u001b[0m Finished trial#14 resulted in value: 15.004. Current best value is 14.864 with parameters: {'activation': 'tanh', 'num_hidden_layer': 4, 'i': 6, 'learning_rate': 0.0003477839020306978}.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Score for fold 5: loss of 24.588363647460938\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 15.6100 - mse: 15.6100 - mae: 1.7131 - val_loss: 20.2289 - val_mse: 20.2289 - val_mae: 1.6101 - lr: 2.0579e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 14.4619 - mse: 14.4619 - mae: 1.6236 - val_loss: 20.1609 - val_mse: 20.1609 - val_mae: 1.6024 - lr: 2.0579e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 14.3712 - mse: 14.3712 - mae: 1.6172 - val_loss: 20.0611 - val_mse: 20.0611 - val_mae: 1.5715 - lr: 2.0579e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 14.3004 - mse: 14.3004 - mae: 1.6108 - val_loss: 20.0678 - val_mse: 20.0678 - val_mae: 1.5549 - lr: 2.0579e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 14.2508 - mse: 14.2508 - mae: 1.6095 - val_loss: 19.9768 - val_mse: 19.9768 - val_mae: 1.5900 - lr: 2.0579e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 14.2014 - mse: 14.2014 - mae: 1.6092 - val_loss: 20.0010 - val_mse: 20.0010 - val_mae: 1.5937 - lr: 2.0579e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 14.1724 - mse: 14.1724 - mae: 1.6034 - val_loss: 19.9977 - val_mse: 19.9977 - val_mae: 1.6374 - lr: 2.0579e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 2s - loss: 14.1210 - mse: 14.1210 - mae: 1.6024 - val_loss: 19.9215 - val_mse: 19.9215 - val_mae: 1.5835 - lr: 2.0579e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 2s - loss: 14.0894 - mse: 14.0894 - mae: 1.6004 - val_loss: 19.8673 - val_mse: 19.8673 - val_mae: 1.5965 - lr: 2.0579e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 2s - loss: 14.0425 - mse: 14.0425 - mae: 1.5993 - val_loss: 19.9237 - val_mse: 19.9237 - val_mae: 1.5550 - lr: 2.0579e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 2s - loss: 14.0141 - mse: 14.0141 - mae: 1.5986 - val_loss: 19.8829 - val_mse: 19.8829 - val_mae: 1.5446 - lr: 2.0579e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 2s - loss: 13.9848 - mse: 13.9848 - mae: 1.5965 - val_loss: 19.8442 - val_mse: 19.8442 - val_mae: 1.5372 - lr: 2.0579e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 13/100\n",
            "1000/1000 - 2s - loss: 13.9578 - mse: 13.9578 - mae: 1.5930 - val_loss: 19.9215 - val_mse: 19.9215 - val_mae: 1.5588 - lr: 2.0579e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 14/100\n",
            "1000/1000 - 2s - loss: 13.9472 - mse: 13.9472 - mae: 1.5919 - val_loss: 19.8626 - val_mse: 19.8626 - val_mae: 1.6050 - lr: 2.0579e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 15/100\n",
            "1000/1000 - 2s - loss: 13.9261 - mse: 13.9261 - mae: 1.5902 - val_loss: 19.8389 - val_mse: 19.8389 - val_mae: 1.5892 - lr: 2.0579e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 16/100\n",
            "1000/1000 - 2s - loss: 13.9027 - mse: 13.9027 - mae: 1.5915 - val_loss: 19.7936 - val_mse: 19.7936 - val_mae: 1.6064 - lr: 2.0579e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 17/100\n",
            "1000/1000 - 2s - loss: 13.8820 - mse: 13.8820 - mae: 1.5913 - val_loss: 19.8036 - val_mse: 19.8036 - val_mae: 1.5907 - lr: 2.0579e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 18/100\n",
            "1000/1000 - 2s - loss: 13.8628 - mse: 13.8628 - mae: 1.5884 - val_loss: 19.8202 - val_mse: 19.8202 - val_mae: 1.5599 - lr: 2.0579e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 19/100\n",
            "1000/1000 - 2s - loss: 13.8640 - mse: 13.8640 - mae: 1.5850 - val_loss: 19.7663 - val_mse: 19.7663 - val_mae: 1.5778 - lr: 2.0579e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 20/100\n",
            "1000/1000 - 2s - loss: 13.8455 - mse: 13.8455 - mae: 1.5896 - val_loss: 19.8286 - val_mse: 19.8286 - val_mae: 1.5717 - lr: 2.0579e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 21/100\n",
            "1000/1000 - 2s - loss: 13.8285 - mse: 13.8285 - mae: 1.5913 - val_loss: 19.7987 - val_mse: 19.7987 - val_mae: 1.5762 - lr: 2.0579e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 22/100\n",
            "1000/1000 - 2s - loss: 13.8218 - mse: 13.8218 - mae: 1.5872 - val_loss: 19.8433 - val_mse: 19.8433 - val_mae: 1.5737 - lr: 2.0579e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 23/100\n",
            "1000/1000 - 2s - loss: 13.8177 - mse: 13.8177 - mae: 1.5870 - val_loss: 19.7800 - val_mse: 19.7800 - val_mae: 1.5711 - lr: 2.0579e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 24/100\n",
            "1000/1000 - 2s - loss: 13.8102 - mse: 13.8102 - mae: 1.5831 - val_loss: 19.8268 - val_mse: 19.8268 - val_mae: 1.5656 - lr: 2.0579e-04 - 2s/epoch - 2ms/step\n",
            "Score for fold 1: loss of 19.826797485351562\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 14.7458 - mse: 14.7458 - mae: 1.5714 - val_loss: 16.0947 - val_mse: 16.0947 - val_mae: 1.6266 - lr: 2.0579e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 14.7143 - mse: 14.7143 - mae: 1.5735 - val_loss: 16.1143 - val_mse: 16.1143 - val_mae: 1.6363 - lr: 2.0579e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 14.7195 - mse: 14.7195 - mae: 1.5706 - val_loss: 16.2149 - val_mse: 16.2149 - val_mae: 1.5735 - lr: 2.0579e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 14.7051 - mse: 14.7051 - mae: 1.5688 - val_loss: 16.0989 - val_mse: 16.0989 - val_mae: 1.5989 - lr: 2.0579e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 14.7015 - mse: 14.7015 - mae: 1.5672 - val_loss: 16.1133 - val_mse: 16.1133 - val_mae: 1.6532 - lr: 2.0579e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 14.6973 - mse: 14.6973 - mae: 1.5681 - val_loss: 16.1573 - val_mse: 16.1573 - val_mae: 1.6276 - lr: 2.0579e-04 - 2s/epoch - 2ms/step\n",
            "Score for fold 2: loss of 16.157245635986328\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 15.1342 - mse: 15.1342 - mae: 1.5807 - val_loss: 14.3467 - val_mse: 14.3467 - val_mae: 1.5594 - lr: 2.0579e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 15.1192 - mse: 15.1192 - mae: 1.5784 - val_loss: 14.3085 - val_mse: 14.3085 - val_mae: 1.5920 - lr: 2.0579e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 15.0816 - mse: 15.0816 - mae: 1.5758 - val_loss: 14.3567 - val_mse: 14.3567 - val_mae: 1.6046 - lr: 2.0579e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 15.0947 - mse: 15.0947 - mae: 1.5764 - val_loss: 14.3332 - val_mse: 14.3332 - val_mae: 1.5793 - lr: 2.0579e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 15.0872 - mse: 15.0872 - mae: 1.5767 - val_loss: 14.3901 - val_mse: 14.3901 - val_mae: 1.5744 - lr: 2.0579e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 15.0864 - mse: 15.0864 - mae: 1.5775 - val_loss: 14.3346 - val_mse: 14.3346 - val_mae: 1.5830 - lr: 2.0579e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 15.0637 - mse: 15.0637 - mae: 1.5808 - val_loss: 14.3962 - val_mse: 14.3962 - val_mae: 1.5807 - lr: 2.0579e-04 - 2s/epoch - 2ms/step\n",
            "Score for fold 3: loss of 14.396160125732422\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 15.5556 - mse: 15.5556 - mae: 1.5781 - val_loss: 12.4631 - val_mse: 12.4631 - val_mae: 1.6148 - lr: 2.0579e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 15.5239 - mse: 15.5239 - mae: 1.5782 - val_loss: 12.5502 - val_mse: 12.5502 - val_mae: 1.5757 - lr: 2.0579e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 15.5345 - mse: 15.5345 - mae: 1.5786 - val_loss: 12.5583 - val_mse: 12.5583 - val_mae: 1.5768 - lr: 2.0579e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 15.5183 - mse: 15.5183 - mae: 1.5782 - val_loss: 12.5654 - val_mse: 12.5654 - val_mae: 1.5829 - lr: 2.0579e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 15.5100 - mse: 15.5100 - mae: 1.5768 - val_loss: 12.5824 - val_mse: 12.5824 - val_mae: 1.5530 - lr: 2.0579e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 15.4895 - mse: 15.4895 - mae: 1.5767 - val_loss: 12.6059 - val_mse: 12.6059 - val_mae: 1.5905 - lr: 2.0579e-04 - 2s/epoch - 2ms/step\n",
            "Score for fold 4: loss of 12.605911254882812\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 15.6072 - mse: 15.6072 - mae: 1.5830 - val_loss: 11.9833 - val_mse: 11.9833 - val_mae: 1.5696 - lr: 2.0579e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 15.6072 - mse: 15.6072 - mae: 1.5820 - val_loss: 12.0430 - val_mse: 12.0430 - val_mae: 1.5589 - lr: 2.0579e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 15.6197 - mse: 15.6197 - mae: 1.5827 - val_loss: 12.0759 - val_mse: 12.0759 - val_mae: 1.5461 - lr: 2.0579e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 15.6106 - mse: 15.6106 - mae: 1.5797 - val_loss: 12.0687 - val_mse: 12.0687 - val_mae: 1.5509 - lr: 2.0579e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 15.5890 - mse: 15.5890 - mae: 1.5814 - val_loss: 12.0535 - val_mse: 12.0535 - val_mae: 1.5391 - lr: 2.0579e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 15.5694 - mse: 15.5694 - mae: 1.5776 - val_loss: 12.0651 - val_mse: 12.0651 - val_mae: 1.5416 - lr: 2.0579e-04 - 2s/epoch - 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-10-14 21:07:55,695]\u001b[0m Finished trial#15 resulted in value: 15.014. Current best value is 14.864 with parameters: {'activation': 'tanh', 'num_hidden_layer': 4, 'i': 6, 'learning_rate': 0.0003477839020306978}.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Score for fold 5: loss of 12.065079689025879\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 17.0439 - mse: 17.0439 - mae: 1.6482 - val_loss: 12.0449 - val_mse: 12.0449 - val_mae: 1.5848 - lr: 4.5942e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 16.1328 - mse: 16.1328 - mae: 1.6013 - val_loss: 11.9162 - val_mse: 11.9162 - val_mae: 1.5806 - lr: 4.5942e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 16.0180 - mse: 16.0180 - mae: 1.5979 - val_loss: 11.8764 - val_mse: 11.8764 - val_mae: 1.5552 - lr: 4.5942e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 15.9629 - mse: 15.9629 - mae: 1.5975 - val_loss: 11.8310 - val_mse: 11.8310 - val_mae: 1.5855 - lr: 4.5942e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 15.9302 - mse: 15.9302 - mae: 1.5939 - val_loss: 11.8534 - val_mse: 11.8534 - val_mae: 1.5594 - lr: 4.5942e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 1s - loss: 15.9181 - mse: 15.9181 - mae: 1.5930 - val_loss: 11.8423 - val_mse: 11.8423 - val_mae: 1.5846 - lr: 4.5942e-04 - 1s/epoch - 1ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 1s - loss: 15.9093 - mse: 15.9093 - mae: 1.5960 - val_loss: 11.8278 - val_mse: 11.8278 - val_mae: 1.5674 - lr: 4.5942e-04 - 1s/epoch - 1ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 2s - loss: 15.8896 - mse: 15.8896 - mae: 1.5932 - val_loss: 11.7901 - val_mse: 11.7901 - val_mae: 1.5823 - lr: 4.5942e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 2s - loss: 15.8644 - mse: 15.8644 - mae: 1.5916 - val_loss: 11.8150 - val_mse: 11.8150 - val_mae: 1.5486 - lr: 4.5942e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 1s - loss: 15.8595 - mse: 15.8595 - mae: 1.5924 - val_loss: 11.7714 - val_mse: 11.7714 - val_mae: 1.5737 - lr: 4.5942e-04 - 1s/epoch - 1ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 1s - loss: 15.8297 - mse: 15.8297 - mae: 1.5918 - val_loss: 11.7599 - val_mse: 11.7599 - val_mae: 1.5870 - lr: 4.5942e-04 - 1s/epoch - 1ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 1s - loss: 15.8097 - mse: 15.8097 - mae: 1.5897 - val_loss: 11.7833 - val_mse: 11.7833 - val_mae: 1.5896 - lr: 4.5942e-04 - 1s/epoch - 1ms/step\n",
            "Epoch 13/100\n",
            "1000/1000 - 1s - loss: 15.8160 - mse: 15.8160 - mae: 1.5898 - val_loss: 11.7656 - val_mse: 11.7656 - val_mae: 1.5692 - lr: 4.5942e-04 - 1s/epoch - 1ms/step\n",
            "Epoch 14/100\n",
            "1000/1000 - 2s - loss: 15.7962 - mse: 15.7962 - mae: 1.5866 - val_loss: 11.7723 - val_mse: 11.7723 - val_mae: 1.5641 - lr: 4.5942e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 15/100\n",
            "1000/1000 - 1s - loss: 15.7763 - mse: 15.7763 - mae: 1.5904 - val_loss: 11.7443 - val_mse: 11.7443 - val_mae: 1.5940 - lr: 4.5942e-04 - 1s/epoch - 1ms/step\n",
            "Epoch 16/100\n",
            "1000/1000 - 1s - loss: 15.7754 - mse: 15.7754 - mae: 1.5896 - val_loss: 11.7350 - val_mse: 11.7350 - val_mae: 1.5969 - lr: 4.5942e-04 - 1s/epoch - 1ms/step\n",
            "Epoch 17/100\n",
            "1000/1000 - 2s - loss: 15.7450 - mse: 15.7450 - mae: 1.5891 - val_loss: 11.7421 - val_mse: 11.7421 - val_mae: 1.5658 - lr: 4.5942e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 18/100\n",
            "1000/1000 - 1s - loss: 15.7635 - mse: 15.7635 - mae: 1.5871 - val_loss: 11.7944 - val_mse: 11.7944 - val_mae: 1.5363 - lr: 4.5942e-04 - 1s/epoch - 1ms/step\n",
            "Epoch 19/100\n",
            "1000/1000 - 1s - loss: 15.7603 - mse: 15.7603 - mae: 1.5881 - val_loss: 11.7653 - val_mse: 11.7653 - val_mae: 1.5667 - lr: 4.5942e-04 - 1s/epoch - 1ms/step\n",
            "Epoch 20/100\n",
            "1000/1000 - 1s - loss: 15.7499 - mse: 15.7499 - mae: 1.5877 - val_loss: 11.7809 - val_mse: 11.7809 - val_mae: 1.5359 - lr: 4.5942e-04 - 1s/epoch - 1ms/step\n",
            "Epoch 21/100\n",
            "1000/1000 - 2s - loss: 15.7536 - mse: 15.7536 - mae: 1.5846 - val_loss: 11.7286 - val_mse: 11.7286 - val_mae: 1.5521 - lr: 4.5942e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 22/100\n",
            "1000/1000 - 1s - loss: 15.7457 - mse: 15.7457 - mae: 1.5842 - val_loss: 11.7096 - val_mse: 11.7096 - val_mae: 1.5782 - lr: 4.5942e-04 - 1s/epoch - 1ms/step\n",
            "Epoch 23/100\n",
            "1000/1000 - 1s - loss: 15.7411 - mse: 15.7411 - mae: 1.5845 - val_loss: 11.6987 - val_mse: 11.6987 - val_mae: 1.5711 - lr: 4.5942e-04 - 1s/epoch - 1ms/step\n",
            "Epoch 24/100\n",
            "1000/1000 - 1s - loss: 15.7014 - mse: 15.7014 - mae: 1.5833 - val_loss: 11.7449 - val_mse: 11.7449 - val_mae: 1.5410 - lr: 4.5942e-04 - 1s/epoch - 1ms/step\n",
            "Epoch 25/100\n",
            "1000/1000 - 1s - loss: 15.7316 - mse: 15.7316 - mae: 1.5820 - val_loss: 11.6909 - val_mse: 11.6909 - val_mae: 1.5662 - lr: 4.5942e-04 - 1s/epoch - 1ms/step\n",
            "Epoch 26/100\n",
            "1000/1000 - 1s - loss: 15.7246 - mse: 15.7246 - mae: 1.5809 - val_loss: 11.6935 - val_mse: 11.6935 - val_mae: 1.5611 - lr: 4.5942e-04 - 1s/epoch - 1ms/step\n",
            "Epoch 27/100\n",
            "1000/1000 - 1s - loss: 15.7071 - mse: 15.7071 - mae: 1.5818 - val_loss: 11.6982 - val_mse: 11.6982 - val_mae: 1.5597 - lr: 4.5942e-04 - 1s/epoch - 1ms/step\n",
            "Epoch 28/100\n",
            "1000/1000 - 1s - loss: 15.7244 - mse: 15.7244 - mae: 1.5821 - val_loss: 11.7075 - val_mse: 11.7075 - val_mae: 1.5528 - lr: 4.5942e-04 - 1s/epoch - 1ms/step\n",
            "Epoch 29/100\n",
            "1000/1000 - 2s - loss: 15.7016 - mse: 15.7016 - mae: 1.5841 - val_loss: 11.7090 - val_mse: 11.7090 - val_mae: 1.5952 - lr: 4.5942e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 30/100\n",
            "1000/1000 - 1s - loss: 15.6915 - mse: 15.6915 - mae: 1.5820 - val_loss: 11.7101 - val_mse: 11.7101 - val_mae: 1.5983 - lr: 4.5942e-04 - 1s/epoch - 1ms/step\n",
            "Score for fold 1: loss of 11.710097312927246\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 15.8816 - mse: 15.8816 - mae: 1.5800 - val_loss: 11.1659 - val_mse: 11.1659 - val_mae: 1.5567 - lr: 4.5942e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 1s - loss: 15.9054 - mse: 15.9054 - mae: 1.5770 - val_loss: 11.1254 - val_mse: 11.1254 - val_mae: 1.5407 - lr: 4.5942e-04 - 1s/epoch - 1ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 1s - loss: 15.8673 - mse: 15.8673 - mae: 1.5757 - val_loss: 11.1540 - val_mse: 11.1540 - val_mae: 1.5794 - lr: 4.5942e-04 - 1s/epoch - 1ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 1s - loss: 15.8843 - mse: 15.8843 - mae: 1.5781 - val_loss: 11.2041 - val_mse: 11.2041 - val_mae: 1.5279 - lr: 4.5942e-04 - 1s/epoch - 1ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 1s - loss: 15.8863 - mse: 15.8863 - mae: 1.5761 - val_loss: 11.1170 - val_mse: 11.1170 - val_mae: 1.5346 - lr: 4.5942e-04 - 1s/epoch - 1ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 15.8759 - mse: 15.8759 - mae: 1.5779 - val_loss: 11.1163 - val_mse: 11.1163 - val_mae: 1.5942 - lr: 4.5942e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 15.8624 - mse: 15.8624 - mae: 1.5741 - val_loss: 11.1905 - val_mse: 11.1905 - val_mae: 1.5471 - lr: 4.5942e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 2s - loss: 15.8724 - mse: 15.8724 - mae: 1.5759 - val_loss: 11.1144 - val_mse: 11.1144 - val_mae: 1.5627 - lr: 4.5942e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 2s - loss: 15.8566 - mse: 15.8566 - mae: 1.5761 - val_loss: 11.1838 - val_mse: 11.1838 - val_mae: 1.5493 - lr: 4.5942e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 2s - loss: 15.8531 - mse: 15.8531 - mae: 1.5777 - val_loss: 11.1426 - val_mse: 11.1426 - val_mae: 1.5700 - lr: 4.5942e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 2s - loss: 15.8510 - mse: 15.8510 - mae: 1.5769 - val_loss: 11.1308 - val_mse: 11.1308 - val_mae: 1.5589 - lr: 4.5942e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 2s - loss: 15.8589 - mse: 15.8589 - mae: 1.5764 - val_loss: 11.1421 - val_mse: 11.1421 - val_mae: 1.5665 - lr: 4.5942e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 13/100\n",
            "1000/1000 - 2s - loss: 15.8396 - mse: 15.8396 - mae: 1.5751 - val_loss: 11.0902 - val_mse: 11.0902 - val_mae: 1.5844 - lr: 4.5942e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 14/100\n",
            "1000/1000 - 2s - loss: 15.8476 - mse: 15.8476 - mae: 1.5747 - val_loss: 11.0952 - val_mse: 11.0952 - val_mae: 1.5528 - lr: 4.5942e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 15/100\n",
            "1000/1000 - 2s - loss: 15.8524 - mse: 15.8524 - mae: 1.5738 - val_loss: 11.0567 - val_mse: 11.0567 - val_mae: 1.5809 - lr: 4.5942e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 16/100\n",
            "1000/1000 - 2s - loss: 15.8286 - mse: 15.8286 - mae: 1.5773 - val_loss: 11.0906 - val_mse: 11.0906 - val_mae: 1.5843 - lr: 4.5942e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 17/100\n",
            "1000/1000 - 2s - loss: 15.8504 - mse: 15.8504 - mae: 1.5754 - val_loss: 11.1576 - val_mse: 11.1576 - val_mae: 1.5360 - lr: 4.5942e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 18/100\n",
            "1000/1000 - 2s - loss: 15.8397 - mse: 15.8397 - mae: 1.5734 - val_loss: 11.1428 - val_mse: 11.1428 - val_mae: 1.5703 - lr: 4.5942e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 19/100\n",
            "1000/1000 - 2s - loss: 15.8284 - mse: 15.8284 - mae: 1.5780 - val_loss: 11.0513 - val_mse: 11.0513 - val_mae: 1.5669 - lr: 4.5942e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 20/100\n",
            "1000/1000 - 1s - loss: 15.8498 - mse: 15.8498 - mae: 1.5734 - val_loss: 11.0739 - val_mse: 11.0739 - val_mae: 1.5962 - lr: 4.5942e-04 - 1s/epoch - 1ms/step\n",
            "Epoch 21/100\n",
            "1000/1000 - 2s - loss: 15.8313 - mse: 15.8313 - mae: 1.5771 - val_loss: 11.0977 - val_mse: 11.0977 - val_mae: 1.5751 - lr: 4.5942e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 22/100\n",
            "1000/1000 - 2s - loss: 15.8241 - mse: 15.8241 - mae: 1.5743 - val_loss: 11.2447 - val_mse: 11.2447 - val_mae: 1.5711 - lr: 4.5942e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 23/100\n",
            "1000/1000 - 2s - loss: 15.8322 - mse: 15.8322 - mae: 1.5742 - val_loss: 11.1526 - val_mse: 11.1526 - val_mae: 1.5795 - lr: 4.5942e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 24/100\n",
            "1000/1000 - 1s - loss: 15.8249 - mse: 15.8249 - mae: 1.5734 - val_loss: 11.0643 - val_mse: 11.0643 - val_mae: 1.6135 - lr: 4.5942e-04 - 1s/epoch - 1ms/step\n",
            "Score for fold 2: loss of 11.06433391571045\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 10.7657 - mse: 10.7657 - mae: 1.5501 - val_loss: 31.2730 - val_mse: 31.2730 - val_mae: 1.6624 - lr: 4.5942e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 1s - loss: 10.7693 - mse: 10.7693 - mae: 1.5493 - val_loss: 31.2646 - val_mse: 31.2646 - val_mae: 1.6368 - lr: 4.5942e-04 - 1s/epoch - 1ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 10.7652 - mse: 10.7652 - mae: 1.5482 - val_loss: 31.2781 - val_mse: 31.2781 - val_mae: 1.6716 - lr: 4.5942e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 10.7579 - mse: 10.7579 - mae: 1.5501 - val_loss: 31.4609 - val_mse: 31.4609 - val_mae: 1.6397 - lr: 4.5942e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 10.7605 - mse: 10.7605 - mae: 1.5489 - val_loss: 31.3135 - val_mse: 31.3135 - val_mae: 1.6493 - lr: 4.5942e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 1s - loss: 10.7582 - mse: 10.7582 - mae: 1.5492 - val_loss: 31.3324 - val_mse: 31.3324 - val_mae: 1.6375 - lr: 4.5942e-04 - 1s/epoch - 1ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 10.7526 - mse: 10.7526 - mae: 1.5482 - val_loss: 31.4056 - val_mse: 31.4056 - val_mae: 1.6165 - lr: 4.5942e-04 - 2s/epoch - 2ms/step\n",
            "Score for fold 3: loss of 31.405603408813477\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 15.9285 - mse: 15.9285 - mae: 1.5790 - val_loss: 10.6185 - val_mse: 10.6185 - val_mae: 1.5257 - lr: 4.5942e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 15.9225 - mse: 15.9225 - mae: 1.5755 - val_loss: 10.6239 - val_mse: 10.6239 - val_mae: 1.5429 - lr: 4.5942e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 15.9227 - mse: 15.9227 - mae: 1.5790 - val_loss: 10.6168 - val_mse: 10.6168 - val_mae: 1.5798 - lr: 4.5942e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 15.9227 - mse: 15.9227 - mae: 1.5808 - val_loss: 10.5989 - val_mse: 10.5989 - val_mae: 1.5508 - lr: 4.5942e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 15.9036 - mse: 15.9036 - mae: 1.5811 - val_loss: 10.5965 - val_mse: 10.5965 - val_mae: 1.5470 - lr: 4.5942e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 1s - loss: 15.9032 - mse: 15.9032 - mae: 1.5787 - val_loss: 10.6062 - val_mse: 10.6062 - val_mae: 1.5425 - lr: 4.5942e-04 - 1s/epoch - 1ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 15.8993 - mse: 15.8993 - mae: 1.5777 - val_loss: 10.6356 - val_mse: 10.6356 - val_mae: 1.5604 - lr: 4.5942e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 2s - loss: 15.8892 - mse: 15.8892 - mae: 1.5786 - val_loss: 10.6197 - val_mse: 10.6197 - val_mae: 1.5496 - lr: 4.5942e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 2s - loss: 15.9102 - mse: 15.9102 - mae: 1.5764 - val_loss: 10.6534 - val_mse: 10.6534 - val_mae: 1.5604 - lr: 4.5942e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 2s - loss: 15.8901 - mse: 15.8901 - mae: 1.5776 - val_loss: 10.6135 - val_mse: 10.6135 - val_mae: 1.5765 - lr: 4.5942e-04 - 2s/epoch - 2ms/step\n",
            "Score for fold 4: loss of 10.613468170166016\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 16.0975 - mse: 16.0975 - mae: 1.5797 - val_loss: 9.7704 - val_mse: 9.7704 - val_mae: 1.5508 - lr: 4.5942e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 16.0719 - mse: 16.0719 - mae: 1.5799 - val_loss: 9.7645 - val_mse: 9.7645 - val_mae: 1.5497 - lr: 4.5942e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 16.0799 - mse: 16.0799 - mae: 1.5798 - val_loss: 9.7848 - val_mse: 9.7848 - val_mae: 1.5476 - lr: 4.5942e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 1s - loss: 16.0862 - mse: 16.0862 - mae: 1.5825 - val_loss: 9.8118 - val_mse: 9.8118 - val_mae: 1.5108 - lr: 4.5942e-04 - 1s/epoch - 1ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 16.0893 - mse: 16.0893 - mae: 1.5776 - val_loss: 9.7711 - val_mse: 9.7711 - val_mae: 1.5342 - lr: 4.5942e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 1s - loss: 16.0621 - mse: 16.0621 - mae: 1.5770 - val_loss: 9.8027 - val_mse: 9.8027 - val_mae: 1.5410 - lr: 4.5942e-04 - 1s/epoch - 1ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 16.0741 - mse: 16.0741 - mae: 1.5786 - val_loss: 9.7874 - val_mse: 9.7874 - val_mae: 1.5412 - lr: 4.5942e-04 - 2s/epoch - 2ms/step\n",
            "Score for fold 5: loss of 9.787407875061035\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-10-14 21:09:57,649]\u001b[0m Finished trial#16 resulted in value: 14.916000000000002. Current best value is 14.864 with parameters: {'activation': 'tanh', 'num_hidden_layer': 4, 'i': 6, 'learning_rate': 0.0003477839020306978}.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 15.1505 - mse: 15.1505 - mae: 1.6492 - val_loss: 19.9693 - val_mse: 19.9693 - val_mae: 1.5984 - lr: 5.0302e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 14.1917 - mse: 14.1917 - mae: 1.5936 - val_loss: 19.7421 - val_mse: 19.7421 - val_mae: 1.5746 - lr: 5.0302e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 14.0472 - mse: 14.0472 - mae: 1.5909 - val_loss: 19.7230 - val_mse: 19.7230 - val_mae: 1.5772 - lr: 5.0302e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 14.0184 - mse: 14.0184 - mae: 1.5933 - val_loss: 19.6185 - val_mse: 19.6185 - val_mae: 1.6189 - lr: 5.0302e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 1s - loss: 14.0067 - mse: 14.0067 - mae: 1.5888 - val_loss: 19.6659 - val_mse: 19.6659 - val_mae: 1.5603 - lr: 5.0302e-04 - 1s/epoch - 1ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 1s - loss: 13.9575 - mse: 13.9575 - mae: 1.5877 - val_loss: 19.6691 - val_mse: 19.6691 - val_mae: 1.5627 - lr: 5.0302e-04 - 1s/epoch - 1ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 13.9598 - mse: 13.9598 - mae: 1.5885 - val_loss: 19.5695 - val_mse: 19.5695 - val_mae: 1.6127 - lr: 5.0302e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 1s - loss: 13.9061 - mse: 13.9061 - mae: 1.5861 - val_loss: 19.6531 - val_mse: 19.6531 - val_mae: 1.5700 - lr: 5.0302e-04 - 1s/epoch - 1ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 1s - loss: 13.9123 - mse: 13.9123 - mae: 1.5856 - val_loss: 19.6073 - val_mse: 19.6073 - val_mae: 1.5721 - lr: 5.0302e-04 - 1s/epoch - 1ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 1s - loss: 13.8935 - mse: 13.8935 - mae: 1.5857 - val_loss: 19.5255 - val_mse: 19.5255 - val_mae: 1.6053 - lr: 5.0302e-04 - 1s/epoch - 1ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 2s - loss: 13.8784 - mse: 13.8784 - mae: 1.5834 - val_loss: 19.5550 - val_mse: 19.5550 - val_mae: 1.5717 - lr: 5.0302e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 2s - loss: 13.8712 - mse: 13.8712 - mae: 1.5826 - val_loss: 19.5880 - val_mse: 19.5880 - val_mae: 1.5616 - lr: 5.0302e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 13/100\n",
            "1000/1000 - 2s - loss: 13.8500 - mse: 13.8500 - mae: 1.5822 - val_loss: 19.5366 - val_mse: 19.5366 - val_mae: 1.5810 - lr: 5.0302e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 14/100\n",
            "1000/1000 - 2s - loss: 13.8138 - mse: 13.8138 - mae: 1.5814 - val_loss: 19.4804 - val_mse: 19.4804 - val_mae: 1.6373 - lr: 5.0302e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 15/100\n",
            "1000/1000 - 2s - loss: 13.8179 - mse: 13.8179 - mae: 1.5791 - val_loss: 19.5197 - val_mse: 19.5197 - val_mae: 1.6046 - lr: 5.0302e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 16/100\n",
            "1000/1000 - 2s - loss: 13.7894 - mse: 13.7894 - mae: 1.5811 - val_loss: 19.5571 - val_mse: 19.5571 - val_mae: 1.5685 - lr: 5.0302e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 17/100\n",
            "1000/1000 - 2s - loss: 13.7951 - mse: 13.7951 - mae: 1.5812 - val_loss: 19.5565 - val_mse: 19.5565 - val_mae: 1.5812 - lr: 5.0302e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 18/100\n",
            "1000/1000 - 2s - loss: 13.7827 - mse: 13.7827 - mae: 1.5787 - val_loss: 19.4945 - val_mse: 19.4945 - val_mae: 1.5722 - lr: 5.0302e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 19/100\n",
            "1000/1000 - 2s - loss: 13.7690 - mse: 13.7690 - mae: 1.5770 - val_loss: 19.4874 - val_mse: 19.4874 - val_mae: 1.5657 - lr: 5.0302e-04 - 2s/epoch - 2ms/step\n",
            "Score for fold 1: loss of 19.487442016601562\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 15.3599 - mse: 15.3599 - mae: 1.5742 - val_loss: 13.2580 - val_mse: 13.2580 - val_mae: 1.5713 - lr: 5.0302e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 15.3550 - mse: 15.3550 - mae: 1.5761 - val_loss: 13.2807 - val_mse: 13.2807 - val_mae: 1.5992 - lr: 5.0302e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 15.3552 - mse: 15.3552 - mae: 1.5755 - val_loss: 13.2018 - val_mse: 13.2018 - val_mae: 1.5884 - lr: 5.0302e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 15.3393 - mse: 15.3393 - mae: 1.5746 - val_loss: 13.2099 - val_mse: 13.2099 - val_mae: 1.5858 - lr: 5.0302e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 15.3485 - mse: 15.3485 - mae: 1.5743 - val_loss: 13.2598 - val_mse: 13.2598 - val_mae: 1.5741 - lr: 5.0302e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 15.3457 - mse: 15.3457 - mae: 1.5738 - val_loss: 13.1506 - val_mse: 13.1506 - val_mae: 1.5794 - lr: 5.0302e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 15.3297 - mse: 15.3297 - mae: 1.5750 - val_loss: 13.1750 - val_mse: 13.1750 - val_mae: 1.5887 - lr: 5.0302e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 2s - loss: 15.3232 - mse: 15.3232 - mae: 1.5738 - val_loss: 13.2079 - val_mse: 13.2079 - val_mae: 1.6035 - lr: 5.0302e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 2s - loss: 15.3209 - mse: 15.3209 - mae: 1.5739 - val_loss: 13.3242 - val_mse: 13.3242 - val_mae: 1.5523 - lr: 5.0302e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 2s - loss: 15.3241 - mse: 15.3241 - mae: 1.5737 - val_loss: 13.2537 - val_mse: 13.2537 - val_mae: 1.5941 - lr: 5.0302e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 2s - loss: 15.3161 - mse: 15.3161 - mae: 1.5740 - val_loss: 13.2127 - val_mse: 13.2127 - val_mae: 1.6098 - lr: 5.0302e-04 - 2s/epoch - 2ms/step\n",
            "Score for fold 2: loss of 13.212729454040527\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 14.4312 - mse: 14.4312 - mae: 1.5670 - val_loss: 16.7760 - val_mse: 16.7760 - val_mae: 1.5920 - lr: 5.0302e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 14.3863 - mse: 14.3863 - mae: 1.5657 - val_loss: 16.8496 - val_mse: 16.8496 - val_mae: 1.5808 - lr: 5.0302e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 14.3830 - mse: 14.3830 - mae: 1.5639 - val_loss: 16.7696 - val_mse: 16.7696 - val_mae: 1.6172 - lr: 5.0302e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 14.3786 - mse: 14.3786 - mae: 1.5681 - val_loss: 16.9310 - val_mse: 16.9310 - val_mae: 1.6134 - lr: 5.0302e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 14.4046 - mse: 14.4046 - mae: 1.5631 - val_loss: 16.7822 - val_mse: 16.7822 - val_mae: 1.6229 - lr: 5.0302e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 14.4011 - mse: 14.4011 - mae: 1.5662 - val_loss: 16.8452 - val_mse: 16.8452 - val_mae: 1.5995 - lr: 5.0302e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 14.4108 - mse: 14.4108 - mae: 1.5649 - val_loss: 16.8422 - val_mse: 16.8422 - val_mae: 1.5978 - lr: 5.0302e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 2s - loss: 14.3962 - mse: 14.3962 - mae: 1.5648 - val_loss: 16.8711 - val_mse: 16.8711 - val_mae: 1.6180 - lr: 5.0302e-04 - 2s/epoch - 2ms/step\n",
            "Score for fold 3: loss of 16.871091842651367\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 15.7714 - mse: 15.7714 - mae: 1.5920 - val_loss: 11.2640 - val_mse: 11.2640 - val_mae: 1.5024 - lr: 5.0302e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 15.7587 - mse: 15.7587 - mae: 1.5915 - val_loss: 11.2812 - val_mse: 11.2812 - val_mae: 1.5247 - lr: 5.0302e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 15.7832 - mse: 15.7832 - mae: 1.5920 - val_loss: 11.3102 - val_mse: 11.3102 - val_mae: 1.4893 - lr: 5.0302e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 15.7649 - mse: 15.7649 - mae: 1.5906 - val_loss: 11.2668 - val_mse: 11.2668 - val_mae: 1.5511 - lr: 5.0302e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 15.7494 - mse: 15.7494 - mae: 1.5923 - val_loss: 11.2790 - val_mse: 11.2790 - val_mae: 1.5273 - lr: 5.0302e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 15.7731 - mse: 15.7731 - mae: 1.5905 - val_loss: 11.2944 - val_mse: 11.2944 - val_mae: 1.5124 - lr: 5.0302e-04 - 2s/epoch - 2ms/step\n",
            "Score for fold 4: loss of 11.294442176818848\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 15.1631 - mse: 15.1631 - mae: 1.5698 - val_loss: 13.7534 - val_mse: 13.7534 - val_mae: 1.5738 - lr: 5.0302e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 15.1686 - mse: 15.1686 - mae: 1.5695 - val_loss: 13.6912 - val_mse: 13.6912 - val_mae: 1.5852 - lr: 5.0302e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 15.1526 - mse: 15.1526 - mae: 1.5699 - val_loss: 13.7942 - val_mse: 13.7942 - val_mae: 1.5771 - lr: 5.0302e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 15.1419 - mse: 15.1419 - mae: 1.5690 - val_loss: 13.7243 - val_mse: 13.7243 - val_mae: 1.5845 - lr: 5.0302e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 15.1364 - mse: 15.1364 - mae: 1.5684 - val_loss: 13.8039 - val_mse: 13.8039 - val_mae: 1.5539 - lr: 5.0302e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 15.1263 - mse: 15.1263 - mae: 1.5683 - val_loss: 13.7387 - val_mse: 13.7387 - val_mae: 1.5835 - lr: 5.0302e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 15.1253 - mse: 15.1253 - mae: 1.5699 - val_loss: 13.6945 - val_mse: 13.6945 - val_mae: 1.6082 - lr: 5.0302e-04 - 2s/epoch - 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-10-14 21:11:21,394]\u001b[0m Finished trial#17 resulted in value: 14.910000000000002. Current best value is 14.864 with parameters: {'activation': 'tanh', 'num_hidden_layer': 4, 'i': 6, 'learning_rate': 0.0003477839020306978}.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Score for fold 5: loss of 13.694479942321777\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 16.8633 - mse: 16.8633 - mae: 1.6425 - val_loss: 12.1609 - val_mse: 12.1609 - val_mae: 1.5855 - lr: 6.0171e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 16.0587 - mse: 16.0587 - mae: 1.5978 - val_loss: 11.9531 - val_mse: 11.9531 - val_mae: 1.5422 - lr: 6.0171e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 15.9855 - mse: 15.9855 - mae: 1.5985 - val_loss: 11.8676 - val_mse: 11.8676 - val_mae: 1.5614 - lr: 6.0171e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 15.9359 - mse: 15.9359 - mae: 1.5982 - val_loss: 11.8291 - val_mse: 11.8291 - val_mae: 1.5638 - lr: 6.0171e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 15.9127 - mse: 15.9127 - mae: 1.5970 - val_loss: 11.8062 - val_mse: 11.8062 - val_mae: 1.5584 - lr: 6.0171e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 15.9082 - mse: 15.9082 - mae: 1.5962 - val_loss: 11.8345 - val_mse: 11.8345 - val_mae: 1.5334 - lr: 6.0171e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 15.8671 - mse: 15.8671 - mae: 1.5974 - val_loss: 11.7970 - val_mse: 11.7970 - val_mae: 1.5410 - lr: 6.0171e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 2s - loss: 15.8490 - mse: 15.8490 - mae: 1.5976 - val_loss: 11.7673 - val_mse: 11.7673 - val_mae: 1.6057 - lr: 6.0171e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 2s - loss: 15.8608 - mse: 15.8608 - mae: 1.5937 - val_loss: 11.7343 - val_mse: 11.7343 - val_mae: 1.5867 - lr: 6.0171e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 2s - loss: 15.8395 - mse: 15.8395 - mae: 1.5959 - val_loss: 11.7304 - val_mse: 11.7304 - val_mae: 1.5527 - lr: 6.0171e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 2s - loss: 15.8490 - mse: 15.8490 - mae: 1.5916 - val_loss: 11.7429 - val_mse: 11.7429 - val_mae: 1.5402 - lr: 6.0171e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 2s - loss: 15.8149 - mse: 15.8149 - mae: 1.5918 - val_loss: 11.7721 - val_mse: 11.7721 - val_mae: 1.5551 - lr: 6.0171e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 13/100\n",
            "1000/1000 - 2s - loss: 15.8014 - mse: 15.8014 - mae: 1.5904 - val_loss: 11.7227 - val_mse: 11.7227 - val_mae: 1.5568 - lr: 6.0171e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 14/100\n",
            "1000/1000 - 2s - loss: 15.7987 - mse: 15.7987 - mae: 1.5902 - val_loss: 11.6956 - val_mse: 11.6956 - val_mae: 1.5683 - lr: 6.0171e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 15/100\n",
            "1000/1000 - 2s - loss: 15.7961 - mse: 15.7961 - mae: 1.5887 - val_loss: 11.6832 - val_mse: 11.6832 - val_mae: 1.5535 - lr: 6.0171e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 16/100\n",
            "1000/1000 - 2s - loss: 15.7953 - mse: 15.7953 - mae: 1.5884 - val_loss: 11.6813 - val_mse: 11.6813 - val_mae: 1.5470 - lr: 6.0171e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 17/100\n",
            "1000/1000 - 2s - loss: 15.7814 - mse: 15.7814 - mae: 1.5870 - val_loss: 11.6788 - val_mse: 11.6788 - val_mae: 1.5443 - lr: 6.0171e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 18/100\n",
            "1000/1000 - 2s - loss: 15.7645 - mse: 15.7645 - mae: 1.5882 - val_loss: 11.6526 - val_mse: 11.6526 - val_mae: 1.5668 - lr: 6.0171e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 19/100\n",
            "1000/1000 - 2s - loss: 15.7469 - mse: 15.7469 - mae: 1.5843 - val_loss: 11.6717 - val_mse: 11.6717 - val_mae: 1.5387 - lr: 6.0171e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 20/100\n",
            "1000/1000 - 2s - loss: 15.7268 - mse: 15.7268 - mae: 1.5864 - val_loss: 11.6599 - val_mse: 11.6599 - val_mae: 1.5381 - lr: 6.0171e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 21/100\n",
            "1000/1000 - 1s - loss: 15.7351 - mse: 15.7351 - mae: 1.5876 - val_loss: 11.6683 - val_mse: 11.6683 - val_mae: 1.5403 - lr: 6.0171e-04 - 1s/epoch - 1ms/step\n",
            "Epoch 22/100\n",
            "1000/1000 - 1s - loss: 15.7196 - mse: 15.7196 - mae: 1.5859 - val_loss: 11.6676 - val_mse: 11.6676 - val_mae: 1.5486 - lr: 6.0171e-04 - 1s/epoch - 1ms/step\n",
            "Epoch 23/100\n",
            "1000/1000 - 2s - loss: 15.7360 - mse: 15.7360 - mae: 1.5863 - val_loss: 11.6147 - val_mse: 11.6147 - val_mae: 1.5823 - lr: 6.0171e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 24/100\n",
            "1000/1000 - 2s - loss: 15.7065 - mse: 15.7065 - mae: 1.5837 - val_loss: 11.6606 - val_mse: 11.6606 - val_mae: 1.5337 - lr: 6.0171e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 25/100\n",
            "1000/1000 - 1s - loss: 15.7072 - mse: 15.7072 - mae: 1.5832 - val_loss: 11.6387 - val_mse: 11.6387 - val_mae: 1.5550 - lr: 6.0171e-04 - 1s/epoch - 1ms/step\n",
            "Epoch 26/100\n",
            "1000/1000 - 1s - loss: 15.7181 - mse: 15.7181 - mae: 1.5830 - val_loss: 11.6947 - val_mse: 11.6947 - val_mae: 1.5204 - lr: 6.0171e-04 - 1s/epoch - 1ms/step\n",
            "Epoch 27/100\n",
            "1000/1000 - 2s - loss: 15.7177 - mse: 15.7177 - mae: 1.5839 - val_loss: 11.6684 - val_mse: 11.6684 - val_mae: 1.5402 - lr: 6.0171e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 28/100\n",
            "1000/1000 - 2s - loss: 15.7051 - mse: 15.7051 - mae: 1.5845 - val_loss: 11.6428 - val_mse: 11.6428 - val_mae: 1.5345 - lr: 6.0171e-04 - 2s/epoch - 2ms/step\n",
            "Score for fold 1: loss of 11.642780303955078\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 14.9681 - mse: 14.9681 - mae: 1.5787 - val_loss: 14.6546 - val_mse: 14.6546 - val_mae: 1.5482 - lr: 6.0171e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 14.9643 - mse: 14.9643 - mae: 1.5777 - val_loss: 14.6247 - val_mse: 14.6247 - val_mae: 1.5688 - lr: 6.0171e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 14.9548 - mse: 14.9548 - mae: 1.5774 - val_loss: 14.6410 - val_mse: 14.6410 - val_mae: 1.5585 - lr: 6.0171e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 14.9489 - mse: 14.9489 - mae: 1.5756 - val_loss: 14.6593 - val_mse: 14.6593 - val_mae: 1.5677 - lr: 6.0171e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 14.9448 - mse: 14.9448 - mae: 1.5772 - val_loss: 14.7171 - val_mse: 14.7171 - val_mae: 1.5397 - lr: 6.0171e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 14.9551 - mse: 14.9551 - mae: 1.5759 - val_loss: 14.6016 - val_mse: 14.6016 - val_mae: 1.5750 - lr: 6.0171e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 14.9193 - mse: 14.9193 - mae: 1.5816 - val_loss: 14.7331 - val_mse: 14.7331 - val_mae: 1.5131 - lr: 6.0171e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 2s - loss: 14.9506 - mse: 14.9506 - mae: 1.5745 - val_loss: 14.6807 - val_mse: 14.6807 - val_mae: 1.5400 - lr: 6.0171e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 2s - loss: 14.9383 - mse: 14.9383 - mae: 1.5775 - val_loss: 14.6470 - val_mse: 14.6470 - val_mae: 1.5888 - lr: 6.0171e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 2s - loss: 14.9391 - mse: 14.9391 - mae: 1.5761 - val_loss: 14.6773 - val_mse: 14.6773 - val_mae: 1.5756 - lr: 6.0171e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 2s - loss: 14.9414 - mse: 14.9414 - mae: 1.5782 - val_loss: 14.7324 - val_mse: 14.7324 - val_mae: 1.5440 - lr: 6.0171e-04 - 2s/epoch - 2ms/step\n",
            "Score for fold 2: loss of 14.732426643371582\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 12.8855 - mse: 12.8855 - mae: 1.5607 - val_loss: 22.9169 - val_mse: 22.9169 - val_mae: 1.6060 - lr: 6.0171e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 12.8725 - mse: 12.8725 - mae: 1.5606 - val_loss: 22.8565 - val_mse: 22.8565 - val_mae: 1.5995 - lr: 6.0171e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 12.8732 - mse: 12.8732 - mae: 1.5602 - val_loss: 22.9170 - val_mse: 22.9170 - val_mae: 1.6172 - lr: 6.0171e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 12.8722 - mse: 12.8722 - mae: 1.5610 - val_loss: 22.9080 - val_mse: 22.9080 - val_mae: 1.5981 - lr: 6.0171e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 12.8502 - mse: 12.8502 - mae: 1.5604 - val_loss: 22.9342 - val_mse: 22.9342 - val_mae: 1.6085 - lr: 6.0171e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 12.8535 - mse: 12.8535 - mae: 1.5602 - val_loss: 22.8860 - val_mse: 22.8860 - val_mae: 1.6135 - lr: 6.0171e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 12.8428 - mse: 12.8428 - mae: 1.5608 - val_loss: 22.9025 - val_mse: 22.9025 - val_mae: 1.6116 - lr: 6.0171e-04 - 2s/epoch - 2ms/step\n",
            "Score for fold 3: loss of 22.902498245239258\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 14.7136 - mse: 14.7136 - mae: 1.5614 - val_loss: 15.3539 - val_mse: 15.3539 - val_mae: 1.6263 - lr: 6.0171e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 14.7115 - mse: 14.7115 - mae: 1.5660 - val_loss: 15.4968 - val_mse: 15.4968 - val_mae: 1.6560 - lr: 6.0171e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 14.7290 - mse: 14.7290 - mae: 1.5662 - val_loss: 15.4781 - val_mse: 15.4781 - val_mae: 1.5812 - lr: 6.0171e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 14.7294 - mse: 14.7294 - mae: 1.5653 - val_loss: 15.5150 - val_mse: 15.5150 - val_mae: 1.5740 - lr: 6.0171e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 14.7181 - mse: 14.7181 - mae: 1.5638 - val_loss: 15.5154 - val_mse: 15.5154 - val_mae: 1.5677 - lr: 6.0171e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 14.7097 - mse: 14.7097 - mae: 1.5622 - val_loss: 15.4063 - val_mse: 15.4063 - val_mae: 1.6391 - lr: 6.0171e-04 - 2s/epoch - 2ms/step\n",
            "Score for fold 4: loss of 15.406338691711426\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 16.0625 - mse: 16.0625 - mae: 1.5883 - val_loss: 9.8198 - val_mse: 9.8198 - val_mae: 1.5327 - lr: 6.0171e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 16.0656 - mse: 16.0656 - mae: 1.5870 - val_loss: 9.8599 - val_mse: 9.8599 - val_mae: 1.5346 - lr: 6.0171e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 16.0558 - mse: 16.0558 - mae: 1.5872 - val_loss: 9.8736 - val_mse: 9.8736 - val_mae: 1.5925 - lr: 6.0171e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 16.0609 - mse: 16.0609 - mae: 1.5873 - val_loss: 9.8695 - val_mse: 9.8695 - val_mae: 1.5503 - lr: 6.0171e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 16.0478 - mse: 16.0478 - mae: 1.5884 - val_loss: 9.8889 - val_mse: 9.8889 - val_mae: 1.5396 - lr: 6.0171e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 16.0620 - mse: 16.0620 - mae: 1.5876 - val_loss: 9.8886 - val_mse: 9.8886 - val_mae: 1.5247 - lr: 6.0171e-04 - 2s/epoch - 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-10-14 21:12:54,819]\u001b[0m Finished trial#18 resulted in value: 14.913999999999998. Current best value is 14.864 with parameters: {'activation': 'tanh', 'num_hidden_layer': 4, 'i': 6, 'learning_rate': 0.0003477839020306978}.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Score for fold 5: loss of 9.88858413696289\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 17.4520 - mse: 17.4520 - mae: 1.6625 - val_loss: 11.5956 - val_mse: 11.5956 - val_mae: 1.5617 - lr: 3.1119e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 1s - loss: 16.3970 - mse: 16.3970 - mae: 1.6073 - val_loss: 11.4188 - val_mse: 11.4188 - val_mae: 1.5625 - lr: 3.1119e-04 - 1s/epoch - 1ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 1s - loss: 16.2537 - mse: 16.2537 - mae: 1.5979 - val_loss: 11.3706 - val_mse: 11.3706 - val_mae: 1.5638 - lr: 3.1119e-04 - 1s/epoch - 1ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 1s - loss: 16.1870 - mse: 16.1870 - mae: 1.5985 - val_loss: 11.3166 - val_mse: 11.3166 - val_mae: 1.5527 - lr: 3.1119e-04 - 1s/epoch - 1ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 1s - loss: 16.1399 - mse: 16.1399 - mae: 1.5982 - val_loss: 11.2923 - val_mse: 11.2923 - val_mae: 1.5714 - lr: 3.1119e-04 - 1s/epoch - 1ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 16.1226 - mse: 16.1226 - mae: 1.5969 - val_loss: 11.3017 - val_mse: 11.3017 - val_mae: 1.5527 - lr: 3.1119e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 1s - loss: 16.0970 - mse: 16.0970 - mae: 1.6000 - val_loss: 11.2555 - val_mse: 11.2555 - val_mae: 1.5676 - lr: 3.1119e-04 - 1s/epoch - 1ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 1s - loss: 16.0699 - mse: 16.0699 - mae: 1.5982 - val_loss: 11.2707 - val_mse: 11.2707 - val_mae: 1.5536 - lr: 3.1119e-04 - 1s/epoch - 1ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 2s - loss: 16.0694 - mse: 16.0694 - mae: 1.5945 - val_loss: 11.2674 - val_mse: 11.2674 - val_mae: 1.5662 - lr: 3.1119e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 2s - loss: 16.0518 - mse: 16.0518 - mae: 1.5965 - val_loss: 11.2377 - val_mse: 11.2377 - val_mae: 1.5607 - lr: 3.1119e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 2s - loss: 16.0421 - mse: 16.0421 - mae: 1.5957 - val_loss: 11.2293 - val_mse: 11.2293 - val_mae: 1.5651 - lr: 3.1119e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 2s - loss: 16.0278 - mse: 16.0278 - mae: 1.5983 - val_loss: 11.2251 - val_mse: 11.2251 - val_mae: 1.5569 - lr: 3.1119e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 13/100\n",
            "1000/1000 - 2s - loss: 16.0068 - mse: 16.0068 - mae: 1.5936 - val_loss: 11.2795 - val_mse: 11.2795 - val_mae: 1.5386 - lr: 3.1119e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 14/100\n",
            "1000/1000 - 2s - loss: 16.0225 - mse: 16.0225 - mae: 1.5932 - val_loss: 11.1909 - val_mse: 11.1909 - val_mae: 1.5608 - lr: 3.1119e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 15/100\n",
            "1000/1000 - 2s - loss: 15.9805 - mse: 15.9805 - mae: 1.5945 - val_loss: 11.2273 - val_mse: 11.2273 - val_mae: 1.5600 - lr: 3.1119e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 16/100\n",
            "1000/1000 - 2s - loss: 15.9886 - mse: 15.9886 - mae: 1.5908 - val_loss: 11.1877 - val_mse: 11.1877 - val_mae: 1.5591 - lr: 3.1119e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 17/100\n",
            "1000/1000 - 1s - loss: 15.9503 - mse: 15.9503 - mae: 1.5943 - val_loss: 11.2084 - val_mse: 11.2084 - val_mae: 1.5515 - lr: 3.1119e-04 - 1s/epoch - 1ms/step\n",
            "Epoch 18/100\n",
            "1000/1000 - 1s - loss: 15.9597 - mse: 15.9597 - mae: 1.5917 - val_loss: 11.1949 - val_mse: 11.1949 - val_mae: 1.5655 - lr: 3.1119e-04 - 1s/epoch - 1ms/step\n",
            "Epoch 19/100\n",
            "1000/1000 - 1s - loss: 15.9492 - mse: 15.9492 - mae: 1.5923 - val_loss: 11.1786 - val_mse: 11.1786 - val_mae: 1.5597 - lr: 3.1119e-04 - 1s/epoch - 1ms/step\n",
            "Epoch 20/100\n",
            "1000/1000 - 2s - loss: 15.9524 - mse: 15.9524 - mae: 1.5901 - val_loss: 11.1668 - val_mse: 11.1668 - val_mae: 1.5607 - lr: 3.1119e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 21/100\n",
            "1000/1000 - 2s - loss: 15.9260 - mse: 15.9260 - mae: 1.5896 - val_loss: 11.2329 - val_mse: 11.2329 - val_mae: 1.5655 - lr: 3.1119e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 22/100\n",
            "1000/1000 - 2s - loss: 15.9445 - mse: 15.9445 - mae: 1.5890 - val_loss: 11.1701 - val_mse: 11.1701 - val_mae: 1.5348 - lr: 3.1119e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 23/100\n",
            "1000/1000 - 2s - loss: 15.9042 - mse: 15.9042 - mae: 1.5888 - val_loss: 11.1691 - val_mse: 11.1691 - val_mae: 1.5716 - lr: 3.1119e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 24/100\n",
            "1000/1000 - 2s - loss: 15.9289 - mse: 15.9289 - mae: 1.5880 - val_loss: 11.1352 - val_mse: 11.1352 - val_mae: 1.5651 - lr: 3.1119e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 25/100\n",
            "1000/1000 - 2s - loss: 15.9191 - mse: 15.9191 - mae: 1.5887 - val_loss: 11.1643 - val_mse: 11.1643 - val_mae: 1.5549 - lr: 3.1119e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 26/100\n",
            "1000/1000 - 1s - loss: 15.9091 - mse: 15.9091 - mae: 1.5883 - val_loss: 11.1696 - val_mse: 11.1696 - val_mae: 1.5457 - lr: 3.1119e-04 - 1s/epoch - 1ms/step\n",
            "Epoch 27/100\n",
            "1000/1000 - 1s - loss: 15.8968 - mse: 15.8968 - mae: 1.5865 - val_loss: 11.1566 - val_mse: 11.1566 - val_mae: 1.5731 - lr: 3.1119e-04 - 1s/epoch - 1ms/step\n",
            "Epoch 28/100\n",
            "1000/1000 - 2s - loss: 15.8844 - mse: 15.8844 - mae: 1.5890 - val_loss: 11.1842 - val_mse: 11.1842 - val_mae: 1.5507 - lr: 3.1119e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 29/100\n",
            "1000/1000 - 2s - loss: 15.8925 - mse: 15.8925 - mae: 1.5859 - val_loss: 11.1783 - val_mse: 11.1783 - val_mae: 1.5454 - lr: 3.1119e-04 - 2s/epoch - 2ms/step\n",
            "Score for fold 1: loss of 11.178277969360352\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 15.3845 - mse: 15.3845 - mae: 1.5814 - val_loss: 13.1480 - val_mse: 13.1480 - val_mae: 1.5928 - lr: 3.1119e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 15.3683 - mse: 15.3683 - mae: 1.5818 - val_loss: 13.2682 - val_mse: 13.2682 - val_mae: 1.5784 - lr: 3.1119e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 15.3907 - mse: 15.3907 - mae: 1.5788 - val_loss: 13.1748 - val_mse: 13.1748 - val_mae: 1.5754 - lr: 3.1119e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 15.3604 - mse: 15.3604 - mae: 1.5778 - val_loss: 13.3115 - val_mse: 13.3115 - val_mae: 1.5558 - lr: 3.1119e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 15.3571 - mse: 15.3571 - mae: 1.5782 - val_loss: 13.2298 - val_mse: 13.2298 - val_mae: 1.5674 - lr: 3.1119e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 15.3606 - mse: 15.3606 - mae: 1.5785 - val_loss: 13.2197 - val_mse: 13.2197 - val_mae: 1.5847 - lr: 3.1119e-04 - 2s/epoch - 2ms/step\n",
            "Score for fold 2: loss of 13.219710350036621\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 14.9422 - mse: 14.9422 - mae: 1.5795 - val_loss: 14.8086 - val_mse: 14.8086 - val_mae: 1.5802 - lr: 3.1119e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 14.9444 - mse: 14.9444 - mae: 1.5783 - val_loss: 14.8460 - val_mse: 14.8460 - val_mae: 1.5497 - lr: 3.1119e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 14.9322 - mse: 14.9322 - mae: 1.5755 - val_loss: 14.8209 - val_mse: 14.8209 - val_mae: 1.5760 - lr: 3.1119e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 14.9250 - mse: 14.9250 - mae: 1.5748 - val_loss: 14.8059 - val_mse: 14.8059 - val_mae: 1.5741 - lr: 3.1119e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 1s - loss: 14.9196 - mse: 14.9196 - mae: 1.5724 - val_loss: 14.8398 - val_mse: 14.8398 - val_mae: 1.5689 - lr: 3.1119e-04 - 1s/epoch - 1ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 14.8900 - mse: 14.8900 - mae: 1.5754 - val_loss: 14.8692 - val_mse: 14.8692 - val_mae: 1.5750 - lr: 3.1119e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 14.9222 - mse: 14.9222 - mae: 1.5752 - val_loss: 14.8420 - val_mse: 14.8420 - val_mae: 1.5554 - lr: 3.1119e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 2s - loss: 14.8903 - mse: 14.8903 - mae: 1.5728 - val_loss: 14.8379 - val_mse: 14.8379 - val_mae: 1.5826 - lr: 3.1119e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 2s - loss: 14.8972 - mse: 14.8972 - mae: 1.5711 - val_loss: 14.8203 - val_mse: 14.8203 - val_mae: 1.5836 - lr: 3.1119e-04 - 2s/epoch - 2ms/step\n",
            "Score for fold 3: loss of 14.820307731628418\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 12.7141 - mse: 12.7141 - mae: 1.5702 - val_loss: 23.5934 - val_mse: 23.5934 - val_mae: 1.6024 - lr: 3.1119e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 12.7045 - mse: 12.7045 - mae: 1.5655 - val_loss: 23.6275 - val_mse: 23.6275 - val_mae: 1.5857 - lr: 3.1119e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 12.6961 - mse: 12.6961 - mae: 1.5628 - val_loss: 23.6132 - val_mse: 23.6132 - val_mae: 1.6061 - lr: 3.1119e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 12.6873 - mse: 12.6873 - mae: 1.5650 - val_loss: 23.7093 - val_mse: 23.7093 - val_mae: 1.6117 - lr: 3.1119e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 12.6901 - mse: 12.6901 - mae: 1.5636 - val_loss: 23.5882 - val_mse: 23.5882 - val_mae: 1.6254 - lr: 3.1119e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 12.6623 - mse: 12.6623 - mae: 1.5653 - val_loss: 23.6512 - val_mse: 23.6512 - val_mae: 1.6131 - lr: 3.1119e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 12.6933 - mse: 12.6933 - mae: 1.5617 - val_loss: 23.6461 - val_mse: 23.6461 - val_mae: 1.5986 - lr: 3.1119e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 2s - loss: 12.6846 - mse: 12.6846 - mae: 1.5611 - val_loss: 23.5870 - val_mse: 23.5870 - val_mae: 1.6119 - lr: 3.1119e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 2s - loss: 12.6600 - mse: 12.6600 - mae: 1.5659 - val_loss: 23.6643 - val_mse: 23.6643 - val_mae: 1.6056 - lr: 3.1119e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 2s - loss: 12.6686 - mse: 12.6686 - mae: 1.5631 - val_loss: 23.6633 - val_mse: 23.6633 - val_mae: 1.6142 - lr: 3.1119e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 2s - loss: 12.6660 - mse: 12.6660 - mae: 1.5615 - val_loss: 23.7503 - val_mse: 23.7503 - val_mae: 1.5772 - lr: 3.1119e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 2s - loss: 12.6701 - mse: 12.6701 - mae: 1.5597 - val_loss: 23.6270 - val_mse: 23.6270 - val_mae: 1.6000 - lr: 3.1119e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 13/100\n",
            "1000/1000 - 2s - loss: 12.6582 - mse: 12.6582 - mae: 1.5611 - val_loss: 23.6817 - val_mse: 23.6817 - val_mae: 1.5894 - lr: 3.1119e-04 - 2s/epoch - 2ms/step\n",
            "Score for fold 4: loss of 23.681699752807617\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 15.6484 - mse: 15.6484 - mae: 1.5705 - val_loss: 11.7748 - val_mse: 11.7748 - val_mae: 1.5716 - lr: 3.1119e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 15.6548 - mse: 15.6548 - mae: 1.5700 - val_loss: 11.7714 - val_mse: 11.7714 - val_mae: 1.5598 - lr: 3.1119e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 15.6648 - mse: 15.6648 - mae: 1.5657 - val_loss: 11.7738 - val_mse: 11.7738 - val_mae: 1.5625 - lr: 3.1119e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 15.6259 - mse: 15.6259 - mae: 1.5700 - val_loss: 11.8626 - val_mse: 11.8626 - val_mae: 1.5657 - lr: 3.1119e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 15.6365 - mse: 15.6365 - mae: 1.5673 - val_loss: 11.7328 - val_mse: 11.7328 - val_mae: 1.5773 - lr: 3.1119e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 15.6156 - mse: 15.6156 - mae: 1.5687 - val_loss: 11.7854 - val_mse: 11.7854 - val_mae: 1.5908 - lr: 3.1119e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 15.6208 - mse: 15.6208 - mae: 1.5701 - val_loss: 11.7936 - val_mse: 11.7936 - val_mae: 1.5726 - lr: 3.1119e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 2s - loss: 15.6343 - mse: 15.6343 - mae: 1.5681 - val_loss: 11.7721 - val_mse: 11.7721 - val_mae: 1.5867 - lr: 3.1119e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 2s - loss: 15.6286 - mse: 15.6286 - mae: 1.5663 - val_loss: 11.7769 - val_mse: 11.7769 - val_mae: 1.5834 - lr: 3.1119e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 2s - loss: 15.6247 - mse: 15.6247 - mae: 1.5658 - val_loss: 11.8045 - val_mse: 11.8045 - val_mae: 1.5978 - lr: 3.1119e-04 - 2s/epoch - 2ms/step\n",
            "Score for fold 5: loss of 11.804450035095215\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-10-14 21:14:41,733]\u001b[0m Finished trial#19 resulted in value: 14.940000000000001. Current best value is 14.864 with parameters: {'activation': 'tanh', 'num_hidden_layer': 4, 'i': 6, 'learning_rate': 0.0003477839020306978}.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 16.6259 - mse: 16.6259 - mae: 1.6183 - val_loss: 11.2087 - val_mse: 11.2087 - val_mae: 1.6526 - lr: 8.1780e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 16.1017 - mse: 16.1017 - mae: 1.6048 - val_loss: 11.1531 - val_mse: 11.1531 - val_mae: 1.5752 - lr: 8.1780e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 16.0555 - mse: 16.0555 - mae: 1.5984 - val_loss: 11.1039 - val_mse: 11.1039 - val_mae: 1.5803 - lr: 8.1780e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 16.0110 - mse: 16.0110 - mae: 1.5969 - val_loss: 11.1282 - val_mse: 11.1282 - val_mae: 1.5180 - lr: 8.1780e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 16.0012 - mse: 16.0012 - mae: 1.5974 - val_loss: 11.1472 - val_mse: 11.1472 - val_mae: 1.5162 - lr: 8.1780e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 15.9498 - mse: 15.9498 - mae: 1.5942 - val_loss: 11.0098 - val_mse: 11.0098 - val_mae: 1.5224 - lr: 8.1780e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 15.9559 - mse: 15.9559 - mae: 1.5905 - val_loss: 10.9657 - val_mse: 10.9657 - val_mae: 1.5583 - lr: 8.1780e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 2s - loss: 15.9229 - mse: 15.9229 - mae: 1.5909 - val_loss: 11.0628 - val_mse: 11.0628 - val_mae: 1.5400 - lr: 8.1780e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 2s - loss: 15.9456 - mse: 15.9456 - mae: 1.5891 - val_loss: 11.0397 - val_mse: 11.0397 - val_mae: 1.5635 - lr: 8.1780e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 2s - loss: 15.9447 - mse: 15.9447 - mae: 1.5876 - val_loss: 11.0450 - val_mse: 11.0450 - val_mae: 1.6359 - lr: 8.1780e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 2s - loss: 15.9579 - mse: 15.9579 - mae: 1.5861 - val_loss: 10.9789 - val_mse: 10.9789 - val_mae: 1.5477 - lr: 8.1780e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 2s - loss: 15.8797 - mse: 15.8797 - mae: 1.5859 - val_loss: 10.9796 - val_mse: 10.9796 - val_mae: 1.5902 - lr: 8.1780e-04 - 2s/epoch - 2ms/step\n",
            "Score for fold 1: loss of 10.97964096069336\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 15.1161 - mse: 15.1161 - mae: 1.5621 - val_loss: 14.2821 - val_mse: 14.2821 - val_mae: 1.5811 - lr: 8.1780e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 15.1455 - mse: 15.1455 - mae: 1.5633 - val_loss: 14.3763 - val_mse: 14.3763 - val_mae: 1.6514 - lr: 8.1780e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 15.1077 - mse: 15.1077 - mae: 1.5579 - val_loss: 14.6041 - val_mse: 14.6041 - val_mae: 1.6233 - lr: 8.1780e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 15.1385 - mse: 15.1385 - mae: 1.5640 - val_loss: 14.2414 - val_mse: 14.2414 - val_mae: 1.6186 - lr: 8.1780e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 15.1095 - mse: 15.1095 - mae: 1.5609 - val_loss: 14.3909 - val_mse: 14.3909 - val_mae: 1.6370 - lr: 8.1780e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 15.0702 - mse: 15.0702 - mae: 1.5619 - val_loss: 14.1995 - val_mse: 14.1995 - val_mae: 1.6360 - lr: 8.1780e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 15.1413 - mse: 15.1413 - mae: 1.5613 - val_loss: 14.1893 - val_mse: 14.1893 - val_mae: 1.6197 - lr: 8.1780e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 2s - loss: 15.0968 - mse: 15.0968 - mae: 1.5603 - val_loss: 14.5498 - val_mse: 14.5498 - val_mae: 1.6121 - lr: 8.1780e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 2s - loss: 15.1097 - mse: 15.1097 - mae: 1.5633 - val_loss: 14.3133 - val_mse: 14.3133 - val_mae: 1.6416 - lr: 8.1780e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 2s - loss: 15.0613 - mse: 15.0613 - mae: 1.5632 - val_loss: 14.3613 - val_mse: 14.3613 - val_mae: 1.6031 - lr: 8.1780e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 2s - loss: 15.0998 - mse: 15.0998 - mae: 1.5585 - val_loss: 14.2288 - val_mse: 14.2288 - val_mae: 1.6471 - lr: 8.1780e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 2s - loss: 15.0205 - mse: 15.0205 - mae: 1.5614 - val_loss: 14.2413 - val_mse: 14.2413 - val_mae: 1.6509 - lr: 8.1780e-04 - 2s/epoch - 2ms/step\n",
            "Score for fold 2: loss of 14.24134635925293\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 13.4792 - mse: 13.4792 - mae: 1.5718 - val_loss: 20.6386 - val_mse: 20.6386 - val_mae: 1.5439 - lr: 8.1780e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 13.4055 - mse: 13.4055 - mae: 1.5678 - val_loss: 20.6030 - val_mse: 20.6030 - val_mae: 1.6106 - lr: 8.1780e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 13.3955 - mse: 13.3955 - mae: 1.5698 - val_loss: 20.7116 - val_mse: 20.7116 - val_mae: 1.5628 - lr: 8.1780e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 13.3312 - mse: 13.3312 - mae: 1.5662 - val_loss: 20.7728 - val_mse: 20.7728 - val_mae: 1.5738 - lr: 8.1780e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 13.3572 - mse: 13.3572 - mae: 1.5650 - val_loss: 20.6986 - val_mse: 20.6986 - val_mae: 1.5672 - lr: 8.1780e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 13.3111 - mse: 13.3111 - mae: 1.5634 - val_loss: 20.7124 - val_mse: 20.7124 - val_mae: 1.5573 - lr: 8.1780e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 13.2532 - mse: 13.2532 - mae: 1.5675 - val_loss: 20.7571 - val_mse: 20.7571 - val_mae: 1.5419 - lr: 8.1780e-04 - 2s/epoch - 2ms/step\n",
            "Score for fold 3: loss of 20.757068634033203\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 15.1549 - mse: 15.1549 - mae: 1.5730 - val_loss: 13.0468 - val_mse: 13.0468 - val_mae: 1.5504 - lr: 8.1780e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 15.0926 - mse: 15.0926 - mae: 1.5737 - val_loss: 13.2469 - val_mse: 13.2469 - val_mae: 1.5333 - lr: 8.1780e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 15.1640 - mse: 15.1640 - mae: 1.5723 - val_loss: 12.9850 - val_mse: 12.9850 - val_mae: 1.5674 - lr: 8.1780e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 15.0772 - mse: 15.0772 - mae: 1.5692 - val_loss: 13.4683 - val_mse: 13.4683 - val_mae: 1.5470 - lr: 8.1780e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 15.0808 - mse: 15.0808 - mae: 1.5700 - val_loss: 13.1274 - val_mse: 13.1274 - val_mae: 1.5750 - lr: 8.1780e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 15.0679 - mse: 15.0679 - mae: 1.5679 - val_loss: 13.4381 - val_mse: 13.4381 - val_mae: 1.5141 - lr: 8.1780e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 15.0670 - mse: 15.0670 - mae: 1.5649 - val_loss: 13.0970 - val_mse: 13.0970 - val_mae: 1.6285 - lr: 8.1780e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 2s - loss: 14.9977 - mse: 14.9977 - mae: 1.5650 - val_loss: 13.2969 - val_mse: 13.2969 - val_mae: 1.5610 - lr: 8.1780e-04 - 2s/epoch - 2ms/step\n",
            "Score for fold 4: loss of 13.296916961669922\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 14.5473 - mse: 14.5473 - mae: 1.5646 - val_loss: 15.1931 - val_mse: 15.1931 - val_mae: 1.5448 - lr: 8.1780e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 14.5184 - mse: 14.5184 - mae: 1.5625 - val_loss: 15.1952 - val_mse: 15.1952 - val_mae: 1.6110 - lr: 8.1780e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 14.4544 - mse: 14.4544 - mae: 1.5599 - val_loss: 15.3546 - val_mse: 15.3546 - val_mae: 1.5531 - lr: 8.1780e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 14.4540 - mse: 14.4540 - mae: 1.5620 - val_loss: 15.2175 - val_mse: 15.2175 - val_mae: 1.5886 - lr: 8.1780e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 14.4429 - mse: 14.4429 - mae: 1.5599 - val_loss: 15.2528 - val_mse: 15.2528 - val_mae: 1.6346 - lr: 8.1780e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 14.4257 - mse: 14.4257 - mae: 1.5602 - val_loss: 15.2567 - val_mse: 15.2567 - val_mae: 1.5923 - lr: 8.1780e-04 - 2s/epoch - 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-10-14 21:15:59,303]\u001b[0m Finished trial#20 resulted in value: 14.908000000000001. Current best value is 14.864 with parameters: {'activation': 'tanh', 'num_hidden_layer': 4, 'i': 6, 'learning_rate': 0.0003477839020306978}.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Score for fold 5: loss of 15.256699562072754\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 16.6977 - mse: 16.6977 - mae: 1.6206 - val_loss: 11.1372 - val_mse: 11.1372 - val_mae: 1.6207 - lr: 8.0638e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 16.1528 - mse: 16.1528 - mae: 1.5918 - val_loss: 11.0448 - val_mse: 11.0448 - val_mae: 1.6637 - lr: 8.0638e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 16.0944 - mse: 16.0944 - mae: 1.5947 - val_loss: 11.1306 - val_mse: 11.1306 - val_mae: 1.5970 - lr: 8.0638e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 16.0395 - mse: 16.0395 - mae: 1.5899 - val_loss: 11.1067 - val_mse: 11.1067 - val_mae: 1.5784 - lr: 8.0638e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 16.0048 - mse: 16.0048 - mae: 1.5909 - val_loss: 11.1432 - val_mse: 11.1432 - val_mae: 1.5309 - lr: 8.0638e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 15.9932 - mse: 15.9932 - mae: 1.5849 - val_loss: 11.2494 - val_mse: 11.2494 - val_mae: 1.5756 - lr: 8.0638e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 15.9803 - mse: 15.9803 - mae: 1.5857 - val_loss: 10.9392 - val_mse: 10.9392 - val_mae: 1.5585 - lr: 8.0638e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 2s - loss: 15.9914 - mse: 15.9914 - mae: 1.5815 - val_loss: 11.3693 - val_mse: 11.3693 - val_mae: 1.5467 - lr: 8.0638e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 2s - loss: 15.9334 - mse: 15.9334 - mae: 1.5800 - val_loss: 10.9731 - val_mse: 10.9731 - val_mae: 1.6260 - lr: 8.0638e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 2s - loss: 15.9573 - mse: 15.9573 - mae: 1.5816 - val_loss: 11.1245 - val_mse: 11.1245 - val_mae: 1.5650 - lr: 8.0638e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 2s - loss: 15.9419 - mse: 15.9419 - mae: 1.5801 - val_loss: 10.8270 - val_mse: 10.8270 - val_mae: 1.6044 - lr: 8.0638e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 2s - loss: 15.8772 - mse: 15.8772 - mae: 1.5780 - val_loss: 11.0711 - val_mse: 11.0711 - val_mae: 1.5455 - lr: 8.0638e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 13/100\n",
            "1000/1000 - 2s - loss: 15.8828 - mse: 15.8828 - mae: 1.5745 - val_loss: 10.8790 - val_mse: 10.8790 - val_mae: 1.5770 - lr: 8.0638e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 14/100\n",
            "1000/1000 - 2s - loss: 15.8963 - mse: 15.8963 - mae: 1.5760 - val_loss: 10.9609 - val_mse: 10.9609 - val_mae: 1.6231 - lr: 8.0638e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 15/100\n",
            "1000/1000 - 2s - loss: 15.9014 - mse: 15.9014 - mae: 1.5791 - val_loss: 10.9192 - val_mse: 10.9192 - val_mae: 1.6438 - lr: 8.0638e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 16/100\n",
            "1000/1000 - 2s - loss: 15.8884 - mse: 15.8884 - mae: 1.5748 - val_loss: 10.9029 - val_mse: 10.9029 - val_mae: 1.5657 - lr: 8.0638e-04 - 2s/epoch - 2ms/step\n",
            "Score for fold 1: loss of 10.902929306030273\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 15.7247 - mse: 15.7247 - mae: 1.5712 - val_loss: 11.3231 - val_mse: 11.3231 - val_mae: 1.6018 - lr: 8.0638e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 15.7198 - mse: 15.7198 - mae: 1.5696 - val_loss: 11.3819 - val_mse: 11.3819 - val_mae: 1.5929 - lr: 8.0638e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 15.6296 - mse: 15.6296 - mae: 1.5693 - val_loss: 11.4040 - val_mse: 11.4040 - val_mae: 1.5734 - lr: 8.0638e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 15.6899 - mse: 15.6899 - mae: 1.5703 - val_loss: 11.4634 - val_mse: 11.4634 - val_mae: 1.5323 - lr: 8.0638e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 15.6283 - mse: 15.6283 - mae: 1.5689 - val_loss: 11.4489 - val_mse: 11.4489 - val_mae: 1.5997 - lr: 8.0638e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 15.6762 - mse: 15.6762 - mae: 1.5673 - val_loss: 11.4490 - val_mse: 11.4490 - val_mae: 1.5548 - lr: 8.0638e-04 - 2s/epoch - 2ms/step\n",
            "Score for fold 2: loss of 11.44895076751709\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 12.3560 - mse: 12.3560 - mae: 1.5586 - val_loss: 24.2218 - val_mse: 24.2218 - val_mae: 1.6474 - lr: 8.0638e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 12.3812 - mse: 12.3812 - mae: 1.5629 - val_loss: 24.2212 - val_mse: 24.2212 - val_mae: 1.6262 - lr: 8.0638e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 12.3700 - mse: 12.3700 - mae: 1.5560 - val_loss: 24.3604 - val_mse: 24.3604 - val_mae: 1.5932 - lr: 8.0638e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 12.3022 - mse: 12.3022 - mae: 1.5584 - val_loss: 24.2972 - val_mse: 24.2972 - val_mae: 1.6374 - lr: 8.0638e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 12.3584 - mse: 12.3584 - mae: 1.5559 - val_loss: 24.3205 - val_mse: 24.3205 - val_mae: 1.6050 - lr: 8.0638e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 12.3361 - mse: 12.3361 - mae: 1.5577 - val_loss: 24.4200 - val_mse: 24.4200 - val_mae: 1.5840 - lr: 8.0638e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 12.3961 - mse: 12.3961 - mae: 1.5532 - val_loss: 24.5168 - val_mse: 24.5168 - val_mae: 1.5869 - lr: 8.0638e-04 - 2s/epoch - 2ms/step\n",
            "Score for fold 3: loss of 24.51679039001465\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 14.9986 - mse: 14.9986 - mae: 1.5714 - val_loss: 14.0576 - val_mse: 14.0576 - val_mae: 1.5304 - lr: 8.0638e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 14.9248 - mse: 14.9248 - mae: 1.5701 - val_loss: 14.0050 - val_mse: 14.0050 - val_mae: 1.5793 - lr: 8.0638e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 14.9169 - mse: 14.9169 - mae: 1.5660 - val_loss: 14.4188 - val_mse: 14.4188 - val_mae: 1.5289 - lr: 8.0638e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 14.8893 - mse: 14.8893 - mae: 1.5721 - val_loss: 14.0530 - val_mse: 14.0530 - val_mae: 1.5738 - lr: 8.0638e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 14.8705 - mse: 14.8705 - mae: 1.5694 - val_loss: 14.5525 - val_mse: 14.5525 - val_mae: 1.5665 - lr: 8.0638e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 14.8389 - mse: 14.8389 - mae: 1.5693 - val_loss: 14.3540 - val_mse: 14.3540 - val_mae: 1.5149 - lr: 8.0638e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 14.8072 - mse: 14.8072 - mae: 1.5684 - val_loss: 14.2962 - val_mse: 14.2962 - val_mae: 1.5218 - lr: 8.0638e-04 - 2s/epoch - 2ms/step\n",
            "Score for fold 4: loss of 14.296159744262695\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 15.1153 - mse: 15.1153 - mae: 1.5785 - val_loss: 13.4384 - val_mse: 13.4384 - val_mae: 1.5066 - lr: 8.0638e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 15.0864 - mse: 15.0864 - mae: 1.5786 - val_loss: 13.4166 - val_mse: 13.4166 - val_mae: 1.4883 - lr: 8.0638e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 15.0937 - mse: 15.0937 - mae: 1.5780 - val_loss: 13.2661 - val_mse: 13.2661 - val_mae: 1.6315 - lr: 8.0638e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 15.0556 - mse: 15.0556 - mae: 1.5758 - val_loss: 13.3212 - val_mse: 13.3212 - val_mae: 1.5486 - lr: 8.0638e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 15.0435 - mse: 15.0435 - mae: 1.5719 - val_loss: 13.5018 - val_mse: 13.5018 - val_mae: 1.5201 - lr: 8.0638e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 15.0576 - mse: 15.0576 - mae: 1.5721 - val_loss: 13.5032 - val_mse: 13.5032 - val_mae: 1.5264 - lr: 8.0638e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 14.9751 - mse: 14.9751 - mae: 1.5730 - val_loss: 13.4112 - val_mse: 13.4112 - val_mae: 1.5477 - lr: 8.0638e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 2s - loss: 14.9761 - mse: 14.9761 - mae: 1.5711 - val_loss: 13.3361 - val_mse: 13.3361 - val_mae: 1.5262 - lr: 8.0638e-04 - 2s/epoch - 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-10-14 21:17:15,761]\u001b[0m Finished trial#21 resulted in value: 14.902000000000001. Current best value is 14.864 with parameters: {'activation': 'tanh', 'num_hidden_layer': 4, 'i': 6, 'learning_rate': 0.0003477839020306978}.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Score for fold 5: loss of 13.33604907989502\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 13.2617 - mse: 13.2617 - mae: 1.6082 - val_loss: 23.8908 - val_mse: 23.8908 - val_mae: 1.6552 - lr: 0.0019 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 12.9583 - mse: 12.9583 - mae: 1.5846 - val_loss: 23.6242 - val_mse: 23.6242 - val_mae: 1.6039 - lr: 0.0019 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 12.9195 - mse: 12.9195 - mae: 1.5831 - val_loss: 23.9430 - val_mse: 23.9430 - val_mae: 1.6378 - lr: 0.0019 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 12.9128 - mse: 12.9128 - mae: 1.5791 - val_loss: 24.0189 - val_mse: 24.0189 - val_mae: 1.6123 - lr: 0.0019 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 12.9004 - mse: 12.9004 - mae: 1.5775 - val_loss: 23.8536 - val_mse: 23.8536 - val_mae: 1.5693 - lr: 0.0019 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 12.8344 - mse: 12.8344 - mae: 1.5719 - val_loss: 23.7924 - val_mse: 23.7924 - val_mae: 1.5779 - lr: 0.0019 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 12.8178 - mse: 12.8178 - mae: 1.5765 - val_loss: 23.5309 - val_mse: 23.5309 - val_mae: 1.6057 - lr: 0.0019 - 2s/epoch - 2ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 2s - loss: 12.8422 - mse: 12.8422 - mae: 1.5732 - val_loss: 23.5756 - val_mse: 23.5756 - val_mae: 1.6887 - lr: 0.0019 - 2s/epoch - 2ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 2s - loss: 12.8346 - mse: 12.8346 - mae: 1.5763 - val_loss: 23.4293 - val_mse: 23.4293 - val_mae: 1.6251 - lr: 0.0019 - 2s/epoch - 2ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 2s - loss: 12.7767 - mse: 12.7767 - mae: 1.5728 - val_loss: 23.5384 - val_mse: 23.5384 - val_mae: 1.6450 - lr: 0.0019 - 2s/epoch - 2ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 2s - loss: 12.8186 - mse: 12.8186 - mae: 1.5729 - val_loss: 23.4106 - val_mse: 23.4106 - val_mae: 1.6274 - lr: 0.0019 - 2s/epoch - 2ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 2s - loss: 12.7922 - mse: 12.7922 - mae: 1.5704 - val_loss: 23.5432 - val_mse: 23.5432 - val_mae: 1.6177 - lr: 0.0019 - 2s/epoch - 2ms/step\n",
            "Epoch 13/100\n",
            "1000/1000 - 2s - loss: 12.7692 - mse: 12.7692 - mae: 1.5715 - val_loss: 23.6766 - val_mse: 23.6766 - val_mae: 1.6405 - lr: 0.0019 - 2s/epoch - 2ms/step\n",
            "Epoch 14/100\n",
            "1000/1000 - 2s - loss: 12.7628 - mse: 12.7628 - mae: 1.5671 - val_loss: 23.8309 - val_mse: 23.8309 - val_mae: 1.5865 - lr: 0.0019 - 2s/epoch - 2ms/step\n",
            "Epoch 15/100\n",
            "1000/1000 - 2s - loss: 12.8060 - mse: 12.8060 - mae: 1.5654 - val_loss: 23.4688 - val_mse: 23.4688 - val_mae: 1.6181 - lr: 0.0019 - 2s/epoch - 2ms/step\n",
            "Epoch 16/100\n",
            "1000/1000 - 2s - loss: 12.7273 - mse: 12.7273 - mae: 1.5689 - val_loss: 23.5654 - val_mse: 23.5654 - val_mae: 1.6101 - lr: 0.0019 - 2s/epoch - 2ms/step\n",
            "Score for fold 1: loss of 23.565343856811523\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 15.4470 - mse: 15.4470 - mae: 1.5657 - val_loss: 12.2504 - val_mse: 12.2504 - val_mae: 1.5991 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 15.4607 - mse: 15.4607 - mae: 1.5616 - val_loss: 12.1849 - val_mse: 12.1849 - val_mae: 1.5777 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 15.4739 - mse: 15.4739 - mae: 1.5645 - val_loss: 12.1893 - val_mse: 12.1893 - val_mae: 1.6131 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 15.3807 - mse: 15.3807 - mae: 1.5640 - val_loss: 12.2944 - val_mse: 12.2944 - val_mae: 1.5727 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 15.4303 - mse: 15.4303 - mae: 1.5614 - val_loss: 12.4084 - val_mse: 12.4084 - val_mae: 1.5719 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 15.3587 - mse: 15.3587 - mae: 1.5631 - val_loss: 12.3702 - val_mse: 12.3702 - val_mae: 1.5862 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 15.3689 - mse: 15.3689 - mae: 1.5615 - val_loss: 12.4627 - val_mse: 12.4627 - val_mae: 1.5643 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Score for fold 2: loss of 12.462716102600098\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 14.5566 - mse: 14.5566 - mae: 1.5756 - val_loss: 15.5235 - val_mse: 15.5235 - val_mae: 1.5528 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 14.5724 - mse: 14.5724 - mae: 1.5731 - val_loss: 15.5544 - val_mse: 15.5544 - val_mae: 1.5685 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 14.5773 - mse: 14.5773 - mae: 1.5733 - val_loss: 15.6765 - val_mse: 15.6765 - val_mae: 1.5514 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 14.5381 - mse: 14.5381 - mae: 1.5722 - val_loss: 15.8981 - val_mse: 15.8981 - val_mae: 1.5640 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 14.5533 - mse: 14.5533 - mae: 1.5700 - val_loss: 16.1395 - val_mse: 16.1395 - val_mae: 1.5149 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 14.4717 - mse: 14.4717 - mae: 1.5686 - val_loss: 15.8914 - val_mse: 15.8914 - val_mae: 1.5319 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Score for fold 3: loss of 15.891372680664062\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 15.8145 - mse: 15.8145 - mae: 1.5784 - val_loss: 11.0041 - val_mse: 11.0041 - val_mae: 1.4915 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 15.7196 - mse: 15.7196 - mae: 1.5740 - val_loss: 10.8167 - val_mse: 10.8167 - val_mae: 1.5279 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 15.7162 - mse: 15.7162 - mae: 1.5740 - val_loss: 11.1809 - val_mse: 11.1809 - val_mae: 1.4993 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 15.6215 - mse: 15.6215 - mae: 1.5726 - val_loss: 10.8840 - val_mse: 10.8840 - val_mae: 1.5222 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 15.6601 - mse: 15.6601 - mae: 1.5742 - val_loss: 10.7943 - val_mse: 10.7943 - val_mae: 1.5211 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 15.5902 - mse: 15.5902 - mae: 1.5729 - val_loss: 10.8613 - val_mse: 10.8613 - val_mae: 1.5759 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 15.5165 - mse: 15.5165 - mae: 1.5745 - val_loss: 10.9813 - val_mse: 10.9813 - val_mae: 1.5801 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 2s - loss: 15.6401 - mse: 15.6401 - mae: 1.5764 - val_loss: 11.3141 - val_mse: 11.3141 - val_mae: 1.5183 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 2s - loss: 15.6690 - mse: 15.6690 - mae: 1.5745 - val_loss: 11.0095 - val_mse: 11.0095 - val_mae: 1.5753 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 2s - loss: 15.5908 - mse: 15.5908 - mae: 1.5750 - val_loss: 11.0212 - val_mse: 11.0212 - val_mae: 1.5502 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Score for fold 4: loss of 11.021200180053711\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 15.3763 - mse: 15.3763 - mae: 1.5670 - val_loss: 11.8656 - val_mse: 11.8656 - val_mae: 1.5494 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 15.4567 - mse: 15.4567 - mae: 1.5667 - val_loss: 11.6603 - val_mse: 11.6603 - val_mae: 1.5843 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 15.3599 - mse: 15.3599 - mae: 1.5684 - val_loss: 11.7668 - val_mse: 11.7668 - val_mae: 1.5488 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 15.3870 - mse: 15.3870 - mae: 1.5651 - val_loss: 11.5537 - val_mse: 11.5537 - val_mae: 1.6132 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 15.3474 - mse: 15.3474 - mae: 1.5645 - val_loss: 11.6238 - val_mse: 11.6238 - val_mae: 1.5841 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 15.2604 - mse: 15.2604 - mae: 1.5643 - val_loss: 11.6419 - val_mse: 11.6419 - val_mae: 1.5984 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 15.2746 - mse: 15.2746 - mae: 1.5641 - val_loss: 11.7440 - val_mse: 11.7440 - val_mae: 1.5379 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 2s - loss: 15.1455 - mse: 15.1455 - mae: 1.5618 - val_loss: 11.8505 - val_mse: 11.8505 - val_mae: 1.5278 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 2s - loss: 15.1875 - mse: 15.1875 - mae: 1.5583 - val_loss: 11.6767 - val_mse: 11.6767 - val_mae: 1.5593 - lr: 0.0010 - 2s/epoch - 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-10-14 21:18:37,945]\u001b[0m Finished trial#22 resulted in value: 14.924000000000001. Current best value is 14.864 with parameters: {'activation': 'tanh', 'num_hidden_layer': 4, 'i': 6, 'learning_rate': 0.0003477839020306978}.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Score for fold 5: loss of 11.67667293548584\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 15.7043 - mse: 15.7043 - mae: 1.6114 - val_loss: 15.1023 - val_mse: 15.1023 - val_mae: 1.6186 - lr: 7.7429e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 15.1933 - mse: 15.1933 - mae: 1.5855 - val_loss: 15.0885 - val_mse: 15.0885 - val_mae: 1.6543 - lr: 7.7429e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 15.1259 - mse: 15.1259 - mae: 1.5802 - val_loss: 15.3023 - val_mse: 15.3023 - val_mae: 1.5494 - lr: 7.7429e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 15.0784 - mse: 15.0784 - mae: 1.5781 - val_loss: 14.9318 - val_mse: 14.9318 - val_mae: 1.6001 - lr: 7.7429e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 15.0661 - mse: 15.0661 - mae: 1.5762 - val_loss: 14.8702 - val_mse: 14.8702 - val_mae: 1.6202 - lr: 7.7429e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 15.0169 - mse: 15.0169 - mae: 1.5715 - val_loss: 15.0942 - val_mse: 15.0942 - val_mae: 1.5920 - lr: 7.7429e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 15.0348 - mse: 15.0348 - mae: 1.5751 - val_loss: 14.9533 - val_mse: 14.9533 - val_mae: 1.6094 - lr: 7.7429e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 2s - loss: 14.9978 - mse: 14.9978 - mae: 1.5690 - val_loss: 14.9146 - val_mse: 14.9146 - val_mae: 1.6415 - lr: 7.7429e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 2s - loss: 14.9560 - mse: 14.9560 - mae: 1.5713 - val_loss: 15.0825 - val_mse: 15.0825 - val_mae: 1.5860 - lr: 7.7429e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 2s - loss: 14.9582 - mse: 14.9582 - mae: 1.5689 - val_loss: 14.9869 - val_mse: 14.9869 - val_mae: 1.6546 - lr: 7.7429e-04 - 2s/epoch - 2ms/step\n",
            "Score for fold 1: loss of 14.986915588378906\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 15.5346 - mse: 15.5346 - mae: 1.5912 - val_loss: 12.3472 - val_mse: 12.3472 - val_mae: 1.5285 - lr: 7.7429e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 15.5085 - mse: 15.5085 - mae: 1.5822 - val_loss: 12.3240 - val_mse: 12.3240 - val_mae: 1.6157 - lr: 7.7429e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 15.5068 - mse: 15.5068 - mae: 1.5834 - val_loss: 12.4514 - val_mse: 12.4514 - val_mae: 1.5044 - lr: 7.7429e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 15.4587 - mse: 15.4587 - mae: 1.5822 - val_loss: 12.3664 - val_mse: 12.3664 - val_mae: 1.6004 - lr: 7.7429e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 15.4613 - mse: 15.4613 - mae: 1.5815 - val_loss: 12.3550 - val_mse: 12.3550 - val_mae: 1.5768 - lr: 7.7429e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 15.4170 - mse: 15.4170 - mae: 1.5834 - val_loss: 12.4256 - val_mse: 12.4256 - val_mae: 1.5453 - lr: 7.7429e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 15.4117 - mse: 15.4117 - mae: 1.5839 - val_loss: 12.3834 - val_mse: 12.3834 - val_mae: 1.5635 - lr: 7.7429e-04 - 2s/epoch - 2ms/step\n",
            "Score for fold 2: loss of 12.383438110351562\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 15.7598 - mse: 15.7598 - mae: 1.5760 - val_loss: 11.2787 - val_mse: 11.2787 - val_mae: 1.5732 - lr: 7.7429e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 15.7832 - mse: 15.7832 - mae: 1.5696 - val_loss: 11.1917 - val_mse: 11.1917 - val_mae: 1.5601 - lr: 7.7429e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 15.7940 - mse: 15.7940 - mae: 1.5683 - val_loss: 11.2899 - val_mse: 11.2899 - val_mae: 1.5734 - lr: 7.7429e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 15.7763 - mse: 15.7763 - mae: 1.5717 - val_loss: 11.3607 - val_mse: 11.3607 - val_mae: 1.5889 - lr: 7.7429e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 15.7876 - mse: 15.7876 - mae: 1.5711 - val_loss: 11.4266 - val_mse: 11.4266 - val_mae: 1.5619 - lr: 7.7429e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 15.7329 - mse: 15.7329 - mae: 1.5673 - val_loss: 11.6947 - val_mse: 11.6947 - val_mae: 1.5644 - lr: 7.7429e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 15.7895 - mse: 15.7895 - mae: 1.5695 - val_loss: 11.5412 - val_mse: 11.5412 - val_mae: 1.5705 - lr: 7.7429e-04 - 2s/epoch - 2ms/step\n",
            "Score for fold 3: loss of 11.5412015914917\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 14.0344 - mse: 14.0344 - mae: 1.5669 - val_loss: 18.2464 - val_mse: 18.2464 - val_mae: 1.5725 - lr: 7.7429e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 13.9764 - mse: 13.9764 - mae: 1.5637 - val_loss: 17.8084 - val_mse: 17.8084 - val_mae: 1.6517 - lr: 7.7429e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 13.9293 - mse: 13.9293 - mae: 1.5609 - val_loss: 18.4608 - val_mse: 18.4608 - val_mae: 1.6223 - lr: 7.7429e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 13.9838 - mse: 13.9838 - mae: 1.5595 - val_loss: 17.7226 - val_mse: 17.7226 - val_mae: 1.6303 - lr: 7.7429e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 13.9471 - mse: 13.9471 - mae: 1.5592 - val_loss: 17.9182 - val_mse: 17.9182 - val_mae: 1.6080 - lr: 7.7429e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 13.9371 - mse: 13.9371 - mae: 1.5568 - val_loss: 18.3410 - val_mse: 18.3410 - val_mae: 1.6402 - lr: 7.7429e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 13.9051 - mse: 13.9051 - mae: 1.5556 - val_loss: 17.8184 - val_mse: 17.8184 - val_mae: 1.6123 - lr: 7.7429e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 2s - loss: 13.9233 - mse: 13.9233 - mae: 1.5529 - val_loss: 18.1051 - val_mse: 18.1051 - val_mae: 1.5986 - lr: 7.7429e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 2s - loss: 13.9119 - mse: 13.9119 - mae: 1.5551 - val_loss: 18.1228 - val_mse: 18.1228 - val_mae: 1.6100 - lr: 7.7429e-04 - 2s/epoch - 2ms/step\n",
            "Score for fold 4: loss of 18.122804641723633\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 13.9965 - mse: 13.9965 - mae: 1.5759 - val_loss: 17.7306 - val_mse: 17.7306 - val_mae: 1.5302 - lr: 7.7429e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 13.9864 - mse: 13.9864 - mae: 1.5732 - val_loss: 17.7955 - val_mse: 17.7955 - val_mae: 1.5130 - lr: 7.7429e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 13.9351 - mse: 13.9351 - mae: 1.5726 - val_loss: 17.7381 - val_mse: 17.7381 - val_mae: 1.5566 - lr: 7.7429e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 13.8832 - mse: 13.8832 - mae: 1.5718 - val_loss: 17.7441 - val_mse: 17.7441 - val_mae: 1.5445 - lr: 7.7429e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 13.9362 - mse: 13.9362 - mae: 1.5760 - val_loss: 17.7537 - val_mse: 17.7537 - val_mae: 1.5350 - lr: 7.7429e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 13.9592 - mse: 13.9592 - mae: 1.5748 - val_loss: 17.7466 - val_mse: 17.7466 - val_mae: 1.5084 - lr: 7.7429e-04 - 2s/epoch - 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-10-14 21:19:45,527]\u001b[0m Finished trial#23 resulted in value: 14.956. Current best value is 14.864 with parameters: {'activation': 'tanh', 'num_hidden_layer': 4, 'i': 6, 'learning_rate': 0.0003477839020306978}.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Score for fold 5: loss of 17.746641159057617\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 16.4985 - mse: 16.4985 - mae: 1.6241 - val_loss: 12.1004 - val_mse: 12.1004 - val_mae: 1.5086 - lr: 0.0011 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 15.9709 - mse: 15.9709 - mae: 1.6055 - val_loss: 11.9780 - val_mse: 11.9780 - val_mae: 1.5864 - lr: 0.0011 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 15.9657 - mse: 15.9657 - mae: 1.5984 - val_loss: 11.9457 - val_mse: 11.9457 - val_mae: 1.5713 - lr: 0.0011 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 15.8654 - mse: 15.8654 - mae: 1.5979 - val_loss: 11.7066 - val_mse: 11.7066 - val_mae: 1.5170 - lr: 0.0011 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 15.8143 - mse: 15.8143 - mae: 1.5898 - val_loss: 11.9620 - val_mse: 11.9620 - val_mae: 1.5094 - lr: 0.0011 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 15.8351 - mse: 15.8351 - mae: 1.5888 - val_loss: 11.6487 - val_mse: 11.6487 - val_mae: 1.6031 - lr: 0.0011 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 15.7997 - mse: 15.7997 - mae: 1.5862 - val_loss: 11.5434 - val_mse: 11.5434 - val_mae: 1.5906 - lr: 0.0011 - 2s/epoch - 2ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 2s - loss: 15.7399 - mse: 15.7399 - mae: 1.5857 - val_loss: 11.4557 - val_mse: 11.4557 - val_mae: 1.5368 - lr: 0.0011 - 2s/epoch - 2ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 2s - loss: 15.7544 - mse: 15.7544 - mae: 1.5830 - val_loss: 11.6333 - val_mse: 11.6333 - val_mae: 1.5646 - lr: 0.0011 - 2s/epoch - 2ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 2s - loss: 15.7555 - mse: 15.7555 - mae: 1.5822 - val_loss: 11.6588 - val_mse: 11.6588 - val_mae: 1.5792 - lr: 0.0011 - 2s/epoch - 2ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 2s - loss: 15.7449 - mse: 15.7449 - mae: 1.5808 - val_loss: 11.4474 - val_mse: 11.4474 - val_mae: 1.6160 - lr: 0.0011 - 2s/epoch - 2ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 2s - loss: 15.6772 - mse: 15.6772 - mae: 1.5821 - val_loss: 11.5563 - val_mse: 11.5563 - val_mae: 1.5758 - lr: 0.0011 - 2s/epoch - 2ms/step\n",
            "Epoch 13/100\n",
            "1000/1000 - 2s - loss: 15.7375 - mse: 15.7375 - mae: 1.5823 - val_loss: 11.6935 - val_mse: 11.6935 - val_mae: 1.5492 - lr: 0.0011 - 2s/epoch - 2ms/step\n",
            "Epoch 14/100\n",
            "1000/1000 - 2s - loss: 15.7079 - mse: 15.7079 - mae: 1.5817 - val_loss: 11.6053 - val_mse: 11.6053 - val_mae: 1.5687 - lr: 0.0011 - 2s/epoch - 2ms/step\n",
            "Epoch 15/100\n",
            "1000/1000 - 2s - loss: 15.6290 - mse: 15.6290 - mae: 1.5862 - val_loss: 11.4580 - val_mse: 11.4580 - val_mae: 1.5747 - lr: 0.0011 - 2s/epoch - 2ms/step\n",
            "Epoch 16/100\n",
            "1000/1000 - 2s - loss: 15.7124 - mse: 15.7124 - mae: 1.5816 - val_loss: 11.4774 - val_mse: 11.4774 - val_mae: 1.5576 - lr: 0.0011 - 2s/epoch - 2ms/step\n",
            "Score for fold 1: loss of 11.477413177490234\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 14.8147 - mse: 14.8147 - mae: 1.5708 - val_loss: 15.2575 - val_mse: 15.2575 - val_mae: 1.6296 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 14.8274 - mse: 14.8274 - mae: 1.5693 - val_loss: 15.2868 - val_mse: 15.2868 - val_mae: 1.5879 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 14.8016 - mse: 14.8016 - mae: 1.5653 - val_loss: 15.1182 - val_mse: 15.1182 - val_mae: 1.6004 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 14.7482 - mse: 14.7482 - mae: 1.5618 - val_loss: 15.2538 - val_mse: 15.2538 - val_mae: 1.5929 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 14.7664 - mse: 14.7664 - mae: 1.5644 - val_loss: 15.4615 - val_mse: 15.4615 - val_mae: 1.6019 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 14.7028 - mse: 14.7028 - mae: 1.5593 - val_loss: 15.2536 - val_mse: 15.2536 - val_mae: 1.6236 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 14.7274 - mse: 14.7274 - mae: 1.5601 - val_loss: 15.1827 - val_mse: 15.1827 - val_mae: 1.6261 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 2s - loss: 14.7241 - mse: 14.7241 - mae: 1.5609 - val_loss: 15.1850 - val_mse: 15.1850 - val_mae: 1.6634 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Score for fold 2: loss of 15.184962272644043\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 12.9966 - mse: 12.9966 - mae: 1.5742 - val_loss: 22.1756 - val_mse: 22.1756 - val_mae: 1.5673 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 12.9394 - mse: 12.9394 - mae: 1.5699 - val_loss: 22.1941 - val_mse: 22.1941 - val_mae: 1.5343 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 12.8879 - mse: 12.8879 - mae: 1.5670 - val_loss: 22.4608 - val_mse: 22.4608 - val_mae: 1.5414 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 12.7998 - mse: 12.7998 - mae: 1.5646 - val_loss: 22.2041 - val_mse: 22.2041 - val_mae: 1.5566 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 12.8588 - mse: 12.8588 - mae: 1.5656 - val_loss: 22.2696 - val_mse: 22.2696 - val_mae: 1.6052 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 12.8523 - mse: 12.8523 - mae: 1.5612 - val_loss: 22.3663 - val_mse: 22.3663 - val_mae: 1.5549 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Score for fold 3: loss of 22.366329193115234\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 14.8185 - mse: 14.8185 - mae: 1.5591 - val_loss: 13.9487 - val_mse: 13.9487 - val_mae: 1.5718 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 14.8369 - mse: 14.8369 - mae: 1.5584 - val_loss: 14.1216 - val_mse: 14.1216 - val_mae: 1.5804 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 14.7811 - mse: 14.7811 - mae: 1.5579 - val_loss: 14.1801 - val_mse: 14.1801 - val_mae: 1.6071 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 14.7730 - mse: 14.7730 - mae: 1.5584 - val_loss: 14.3338 - val_mse: 14.3338 - val_mae: 1.5925 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 14.7998 - mse: 14.7998 - mae: 1.5573 - val_loss: 13.9902 - val_mse: 13.9902 - val_mae: 1.5904 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 14.6893 - mse: 14.6893 - mae: 1.5591 - val_loss: 13.9076 - val_mse: 13.9076 - val_mae: 1.6131 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 14.6994 - mse: 14.6994 - mae: 1.5575 - val_loss: 14.5612 - val_mse: 14.5612 - val_mae: 1.5670 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 2s - loss: 14.7130 - mse: 14.7130 - mae: 1.5535 - val_loss: 14.1210 - val_mse: 14.1210 - val_mae: 1.5713 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 2s - loss: 14.7348 - mse: 14.7348 - mae: 1.5545 - val_loss: 14.1490 - val_mse: 14.1490 - val_mae: 1.5691 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 2s - loss: 14.6669 - mse: 14.6669 - mae: 1.5561 - val_loss: 14.1147 - val_mse: 14.1147 - val_mae: 1.5748 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 2s - loss: 14.7190 - mse: 14.7190 - mae: 1.5549 - val_loss: 14.2102 - val_mse: 14.2102 - val_mae: 1.5590 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Score for fold 4: loss of 14.21020793914795\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 15.5563 - mse: 15.5563 - mae: 1.5703 - val_loss: 10.9865 - val_mse: 10.9865 - val_mae: 1.5196 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 15.5578 - mse: 15.5578 - mae: 1.5736 - val_loss: 10.9075 - val_mse: 10.9075 - val_mae: 1.5326 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 15.5427 - mse: 15.5427 - mae: 1.5710 - val_loss: 10.9830 - val_mse: 10.9830 - val_mae: 1.5616 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 15.3966 - mse: 15.3966 - mae: 1.5728 - val_loss: 10.9300 - val_mse: 10.9300 - val_mae: 1.5756 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 15.3701 - mse: 15.3701 - mae: 1.5674 - val_loss: 10.9537 - val_mse: 10.9537 - val_mae: 1.5638 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 15.4130 - mse: 15.4130 - mae: 1.5658 - val_loss: 11.1532 - val_mse: 11.1532 - val_mae: 1.5063 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 15.4400 - mse: 15.4400 - mae: 1.5643 - val_loss: 11.0369 - val_mse: 11.0369 - val_mae: 1.6272 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Score for fold 5: loss of 11.036910057067871\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-10-14 21:21:07,089]\u001b[0m Finished trial#24 resulted in value: 14.856. Current best value is 14.856 with parameters: {'activation': 'relu', 'num_hidden_layer': 3, 'i': 6, 'learning_rate': 0.0011290700036520834}.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 16.1792 - mse: 16.1792 - mae: 1.6291 - val_loss: 12.8534 - val_mse: 12.8534 - val_mae: 1.5570 - lr: 0.0019 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 15.8574 - mse: 15.8574 - mae: 1.6110 - val_loss: 12.5221 - val_mse: 12.5221 - val_mae: 1.5595 - lr: 0.0019 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 15.7805 - mse: 15.7805 - mae: 1.5960 - val_loss: 12.5414 - val_mse: 12.5414 - val_mae: 1.5681 - lr: 0.0019 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 15.6920 - mse: 15.6920 - mae: 1.5948 - val_loss: 12.7483 - val_mse: 12.7483 - val_mae: 1.5643 - lr: 0.0019 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 15.7391 - mse: 15.7391 - mae: 1.5891 - val_loss: 12.2533 - val_mse: 12.2533 - val_mae: 1.6018 - lr: 0.0019 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 15.7296 - mse: 15.7296 - mae: 1.5897 - val_loss: 12.6491 - val_mse: 12.6491 - val_mae: 1.5569 - lr: 0.0019 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 15.7177 - mse: 15.7177 - mae: 1.5873 - val_loss: 12.4651 - val_mse: 12.4651 - val_mae: 1.5754 - lr: 0.0019 - 2s/epoch - 2ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 2s - loss: 15.7000 - mse: 15.7000 - mae: 1.5914 - val_loss: 12.7613 - val_mse: 12.7613 - val_mae: 1.5189 - lr: 0.0019 - 2s/epoch - 2ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 2s - loss: 15.6221 - mse: 15.6221 - mae: 1.5954 - val_loss: 12.6781 - val_mse: 12.6781 - val_mae: 1.6122 - lr: 0.0019 - 2s/epoch - 2ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 2s - loss: 15.6734 - mse: 15.6734 - mae: 1.5924 - val_loss: 12.5472 - val_mse: 12.5472 - val_mae: 1.6043 - lr: 0.0019 - 2s/epoch - 2ms/step\n",
            "Score for fold 1: loss of 12.547174453735352\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 14.9048 - mse: 14.9048 - mae: 1.5907 - val_loss: 15.3782 - val_mse: 15.3782 - val_mae: 1.6033 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 14.7577 - mse: 14.7577 - mae: 1.5819 - val_loss: 15.2398 - val_mse: 15.2398 - val_mae: 1.5843 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 14.7766 - mse: 14.7766 - mae: 1.5846 - val_loss: 15.3497 - val_mse: 15.3497 - val_mae: 1.6735 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 14.7272 - mse: 14.7272 - mae: 1.5898 - val_loss: 15.4953 - val_mse: 15.4953 - val_mae: 1.6021 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 14.7212 - mse: 14.7212 - mae: 1.5877 - val_loss: 15.4444 - val_mse: 15.4444 - val_mae: 1.5726 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 14.7973 - mse: 14.7973 - mae: 1.5920 - val_loss: 15.4302 - val_mse: 15.4302 - val_mae: 1.5696 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 14.6132 - mse: 14.6132 - mae: 1.5871 - val_loss: 15.4788 - val_mse: 15.4788 - val_mae: 1.6106 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Score for fold 2: loss of 15.478799819946289\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 13.4317 - mse: 13.4317 - mae: 1.5746 - val_loss: 20.0784 - val_mse: 20.0784 - val_mae: 1.5969 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 13.4114 - mse: 13.4114 - mae: 1.5774 - val_loss: 20.0540 - val_mse: 20.0540 - val_mae: 1.5989 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 13.4165 - mse: 13.4165 - mae: 1.5705 - val_loss: 20.1928 - val_mse: 20.1928 - val_mae: 1.6395 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 13.3547 - mse: 13.3547 - mae: 1.5669 - val_loss: 20.1529 - val_mse: 20.1529 - val_mae: 1.6823 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 13.3364 - mse: 13.3364 - mae: 1.5665 - val_loss: 20.7114 - val_mse: 20.7114 - val_mae: 1.5171 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 13.3165 - mse: 13.3165 - mae: 1.5654 - val_loss: 20.5286 - val_mse: 20.5286 - val_mae: 1.5242 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 13.2722 - mse: 13.2722 - mae: 1.5596 - val_loss: 20.2173 - val_mse: 20.2173 - val_mae: 1.6415 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Score for fold 3: loss of 20.217247009277344\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 15.2589 - mse: 15.2589 - mae: 1.5681 - val_loss: 11.5805 - val_mse: 11.5805 - val_mae: 1.6504 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 15.3298 - mse: 15.3298 - mae: 1.5564 - val_loss: 11.9893 - val_mse: 11.9893 - val_mae: 1.5876 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 15.3093 - mse: 15.3093 - mae: 1.5638 - val_loss: 11.8131 - val_mse: 11.8131 - val_mae: 1.6087 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 15.1966 - mse: 15.1966 - mae: 1.5629 - val_loss: 11.6998 - val_mse: 11.6998 - val_mae: 1.5357 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 15.1826 - mse: 15.1826 - mae: 1.5635 - val_loss: 11.7571 - val_mse: 11.7571 - val_mae: 1.5974 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 15.1220 - mse: 15.1220 - mae: 1.5589 - val_loss: 11.8011 - val_mse: 11.8011 - val_mae: 1.5735 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Score for fold 4: loss of 11.801079750061035\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 14.4998 - mse: 14.4998 - mae: 1.5849 - val_loss: 14.1293 - val_mse: 14.1293 - val_mae: 1.5111 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 14.4274 - mse: 14.4274 - mae: 1.5770 - val_loss: 14.5331 - val_mse: 14.5331 - val_mae: 1.4823 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 14.4218 - mse: 14.4218 - mae: 1.5729 - val_loss: 14.4079 - val_mse: 14.4079 - val_mae: 1.4470 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 14.4256 - mse: 14.4256 - mae: 1.5779 - val_loss: 14.3649 - val_mse: 14.3649 - val_mae: 1.5837 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 14.3310 - mse: 14.3310 - mae: 1.5739 - val_loss: 14.3923 - val_mse: 14.3923 - val_mae: 1.5178 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 14.2935 - mse: 14.2935 - mae: 1.5697 - val_loss: 14.6408 - val_mse: 14.6408 - val_mae: 1.5180 - lr: 0.0010 - 2s/epoch - 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-10-14 21:22:13,438]\u001b[0m Finished trial#25 resulted in value: 14.937999999999999. Current best value is 14.856 with parameters: {'activation': 'relu', 'num_hidden_layer': 3, 'i': 6, 'learning_rate': 0.0011290700036520834}.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Score for fold 5: loss of 14.640796661376953\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 15.5108 - mse: 15.5108 - mae: 1.6053 - val_loss: 15.4132 - val_mse: 15.4132 - val_mae: 1.5810 - lr: 0.0031 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 15.2413 - mse: 15.2413 - mae: 1.5931 - val_loss: 15.2296 - val_mse: 15.2296 - val_mae: 1.6463 - lr: 0.0031 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 15.2354 - mse: 15.2354 - mae: 1.5845 - val_loss: 14.6499 - val_mse: 14.6499 - val_mae: 1.7044 - lr: 0.0031 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 15.1654 - mse: 15.1654 - mae: 1.5787 - val_loss: 15.1677 - val_mse: 15.1677 - val_mae: 1.6393 - lr: 0.0031 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 15.1751 - mse: 15.1751 - mae: 1.5827 - val_loss: 14.8031 - val_mse: 14.8031 - val_mae: 1.5980 - lr: 0.0031 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 15.1729 - mse: 15.1729 - mae: 1.5807 - val_loss: 14.5828 - val_mse: 14.5828 - val_mae: 1.6658 - lr: 0.0031 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 15.0963 - mse: 15.0963 - mae: 1.5804 - val_loss: 14.3969 - val_mse: 14.3969 - val_mae: 1.6739 - lr: 0.0031 - 2s/epoch - 2ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 2s - loss: 15.0250 - mse: 15.0250 - mae: 1.5795 - val_loss: 14.6719 - val_mse: 14.6719 - val_mae: 1.6283 - lr: 0.0031 - 2s/epoch - 2ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 2s - loss: 15.1643 - mse: 15.1643 - mae: 1.5762 - val_loss: 14.7380 - val_mse: 14.7380 - val_mae: 1.5972 - lr: 0.0031 - 2s/epoch - 2ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 2s - loss: 15.1864 - mse: 15.1864 - mae: 1.5745 - val_loss: 14.4568 - val_mse: 14.4568 - val_mae: 1.6356 - lr: 0.0031 - 2s/epoch - 2ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 2s - loss: 15.1327 - mse: 15.1327 - mae: 1.5727 - val_loss: 14.6384 - val_mse: 14.6384 - val_mae: 1.6435 - lr: 0.0031 - 2s/epoch - 2ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 2s - loss: 15.1178 - mse: 15.1178 - mae: 1.5687 - val_loss: 14.4214 - val_mse: 14.4214 - val_mae: 1.6090 - lr: 0.0031 - 2s/epoch - 2ms/step\n",
            "Score for fold 1: loss of 14.42141342163086\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 15.0919 - mse: 15.0919 - mae: 1.5793 - val_loss: 13.6065 - val_mse: 13.6065 - val_mae: 1.5839 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 15.0876 - mse: 15.0876 - mae: 1.5801 - val_loss: 13.9341 - val_mse: 13.9341 - val_mae: 1.5315 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 15.0734 - mse: 15.0734 - mae: 1.5788 - val_loss: 13.9577 - val_mse: 13.9577 - val_mae: 1.5496 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 15.0727 - mse: 15.0727 - mae: 1.5802 - val_loss: 13.9572 - val_mse: 13.9572 - val_mae: 1.5235 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 15.0370 - mse: 15.0370 - mae: 1.5791 - val_loss: 13.9483 - val_mse: 13.9483 - val_mae: 1.5673 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 15.0456 - mse: 15.0456 - mae: 1.5773 - val_loss: 13.9588 - val_mse: 13.9588 - val_mae: 1.5408 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Score for fold 2: loss of 13.958759307861328\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 15.8050 - mse: 15.8050 - mae: 1.5740 - val_loss: 10.9747 - val_mse: 10.9747 - val_mae: 1.5310 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 15.7540 - mse: 15.7540 - mae: 1.5692 - val_loss: 10.9301 - val_mse: 10.9301 - val_mae: 1.5959 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 15.7511 - mse: 15.7511 - mae: 1.5725 - val_loss: 10.8282 - val_mse: 10.8282 - val_mae: 1.6025 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 15.7299 - mse: 15.7299 - mae: 1.5713 - val_loss: 10.9053 - val_mse: 10.9053 - val_mae: 1.6467 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 15.7337 - mse: 15.7337 - mae: 1.5725 - val_loss: 10.9563 - val_mse: 10.9563 - val_mae: 1.5748 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 15.7327 - mse: 15.7327 - mae: 1.5773 - val_loss: 10.9333 - val_mse: 10.9333 - val_mae: 1.5648 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 15.6058 - mse: 15.6058 - mae: 1.5726 - val_loss: 10.9609 - val_mse: 10.9609 - val_mae: 1.5320 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 2s - loss: 15.6932 - mse: 15.6932 - mae: 1.5701 - val_loss: 10.9073 - val_mse: 10.9073 - val_mae: 1.5732 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Score for fold 3: loss of 10.907336235046387\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 14.3373 - mse: 14.3373 - mae: 1.5701 - val_loss: 16.4191 - val_mse: 16.4191 - val_mae: 1.5585 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 14.3266 - mse: 14.3266 - mae: 1.5680 - val_loss: 16.4382 - val_mse: 16.4382 - val_mae: 1.5777 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 14.3654 - mse: 14.3654 - mae: 1.5689 - val_loss: 16.4352 - val_mse: 16.4352 - val_mae: 1.6067 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 14.2330 - mse: 14.2330 - mae: 1.5704 - val_loss: 16.4083 - val_mse: 16.4083 - val_mae: 1.5951 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 14.2717 - mse: 14.2717 - mae: 1.5686 - val_loss: 16.4965 - val_mse: 16.4965 - val_mae: 1.5389 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 14.2488 - mse: 14.2488 - mae: 1.5681 - val_loss: 16.3742 - val_mse: 16.3742 - val_mae: 1.5683 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 14.2636 - mse: 14.2636 - mae: 1.5705 - val_loss: 16.3992 - val_mse: 16.3992 - val_mae: 1.5758 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 2s - loss: 14.2295 - mse: 14.2295 - mae: 1.5690 - val_loss: 16.4165 - val_mse: 16.4165 - val_mae: 1.5779 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 2s - loss: 14.2128 - mse: 14.2128 - mae: 1.5658 - val_loss: 16.5383 - val_mse: 16.5383 - val_mae: 1.5706 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 2s - loss: 14.2266 - mse: 14.2266 - mae: 1.5694 - val_loss: 16.4657 - val_mse: 16.4657 - val_mae: 1.6035 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 2s - loss: 14.2028 - mse: 14.2028 - mae: 1.5687 - val_loss: 16.3994 - val_mse: 16.3994 - val_mae: 1.5618 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Score for fold 4: loss of 16.399452209472656\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 13.6947 - mse: 13.6947 - mae: 1.5718 - val_loss: 18.7451 - val_mse: 18.7451 - val_mae: 1.5554 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 13.6917 - mse: 13.6917 - mae: 1.5755 - val_loss: 18.5335 - val_mse: 18.5335 - val_mae: 1.5793 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 13.6939 - mse: 13.6939 - mae: 1.5738 - val_loss: 18.6723 - val_mse: 18.6723 - val_mae: 1.5246 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 13.6793 - mse: 13.6793 - mae: 1.5690 - val_loss: 18.6096 - val_mse: 18.6096 - val_mae: 1.5426 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 13.7195 - mse: 13.7195 - mae: 1.5711 - val_loss: 18.5611 - val_mse: 18.5611 - val_mae: 1.5843 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 13.6092 - mse: 13.6092 - mae: 1.5743 - val_loss: 18.6445 - val_mse: 18.6445 - val_mae: 1.5358 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 13.5832 - mse: 13.5832 - mae: 1.5669 - val_loss: 18.7255 - val_mse: 18.7255 - val_mae: 1.5809 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Score for fold 5: loss of 18.725536346435547\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-10-14 21:23:29,138]\u001b[0m Finished trial#26 resulted in value: 14.884. Current best value is 14.856 with parameters: {'activation': 'relu', 'num_hidden_layer': 3, 'i': 6, 'learning_rate': 0.0011290700036520834}.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 3s - loss: 15.0814 - mse: 15.0814 - mae: 1.6306 - val_loss: 16.3167 - val_mse: 16.3167 - val_mae: 1.6380 - lr: 0.0034 - 3s/epoch - 3ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 15.0162 - mse: 15.0162 - mae: 1.5994 - val_loss: 16.3817 - val_mse: 16.3817 - val_mae: 1.6830 - lr: 0.0034 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 14.9482 - mse: 14.9482 - mae: 1.5950 - val_loss: 16.5539 - val_mse: 16.5539 - val_mae: 1.7708 - lr: 0.0034 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 14.9145 - mse: 14.9145 - mae: 1.5932 - val_loss: 16.1755 - val_mse: 16.1755 - val_mae: 1.6520 - lr: 0.0034 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 14.8299 - mse: 14.8299 - mae: 1.5829 - val_loss: 16.5157 - val_mse: 16.5157 - val_mae: 1.6130 - lr: 0.0034 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 14.7581 - mse: 14.7581 - mae: 1.5945 - val_loss: 16.4272 - val_mse: 16.4272 - val_mae: 1.7092 - lr: 0.0034 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 14.8071 - mse: 14.8071 - mae: 1.5989 - val_loss: 16.0957 - val_mse: 16.0957 - val_mae: 1.6196 - lr: 0.0034 - 2s/epoch - 2ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 2s - loss: 14.6553 - mse: 14.6553 - mae: 1.5863 - val_loss: 16.3682 - val_mse: 16.3682 - val_mae: 1.6484 - lr: 0.0034 - 2s/epoch - 2ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 2s - loss: 14.6811 - mse: 14.6811 - mae: 1.5808 - val_loss: 16.1430 - val_mse: 16.1430 - val_mae: 1.5879 - lr: 0.0034 - 2s/epoch - 2ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 2s - loss: 14.7361 - mse: 14.7361 - mae: 1.5760 - val_loss: 16.2636 - val_mse: 16.2636 - val_mae: 1.6423 - lr: 0.0034 - 2s/epoch - 2ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 2s - loss: 14.7065 - mse: 14.7065 - mae: 1.5827 - val_loss: 18.1734 - val_mse: 18.1734 - val_mae: 1.6545 - lr: 0.0034 - 2s/epoch - 2ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 2s - loss: 14.6836 - mse: 14.6836 - mae: 1.5823 - val_loss: 16.5637 - val_mse: 16.5637 - val_mae: 1.6189 - lr: 0.0034 - 2s/epoch - 2ms/step\n",
            "Score for fold 1: loss of 16.56365203857422\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 15.8991 - mse: 15.8991 - mae: 1.5835 - val_loss: 10.9292 - val_mse: 10.9292 - val_mae: 1.5621 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 15.7504 - mse: 15.7504 - mae: 1.5789 - val_loss: 10.9353 - val_mse: 10.9353 - val_mae: 1.5449 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 15.7769 - mse: 15.7769 - mae: 1.5777 - val_loss: 10.9901 - val_mse: 10.9901 - val_mae: 1.5445 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 15.8426 - mse: 15.8426 - mae: 1.5824 - val_loss: 11.0231 - val_mse: 11.0231 - val_mae: 1.5202 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 15.8126 - mse: 15.8126 - mae: 1.5761 - val_loss: 10.8507 - val_mse: 10.8507 - val_mae: 1.5973 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 15.7546 - mse: 15.7546 - mae: 1.5755 - val_loss: 10.8395 - val_mse: 10.8395 - val_mae: 1.5516 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 15.8067 - mse: 15.8067 - mae: 1.5773 - val_loss: 11.1361 - val_mse: 11.1361 - val_mae: 1.5442 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 2s - loss: 15.8118 - mse: 15.8118 - mae: 1.5753 - val_loss: 11.1742 - val_mse: 11.1742 - val_mae: 1.5300 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 2s - loss: 15.6900 - mse: 15.6900 - mae: 1.5773 - val_loss: 11.0391 - val_mse: 11.0391 - val_mae: 1.5753 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 2s - loss: 15.7152 - mse: 15.7152 - mae: 1.5777 - val_loss: 10.9752 - val_mse: 10.9752 - val_mae: 1.5691 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 2s - loss: 15.8060 - mse: 15.8060 - mae: 1.5784 - val_loss: 11.1129 - val_mse: 11.1129 - val_mae: 1.5710 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Score for fold 2: loss of 11.11286735534668\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 15.6463 - mse: 15.6463 - mae: 1.5907 - val_loss: 11.7637 - val_mse: 11.7637 - val_mae: 1.5156 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 15.5985 - mse: 15.5985 - mae: 1.5914 - val_loss: 11.5996 - val_mse: 11.5996 - val_mae: 1.5703 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 15.5064 - mse: 15.5064 - mae: 1.5915 - val_loss: 11.7382 - val_mse: 11.7382 - val_mae: 1.5038 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 15.4678 - mse: 15.4678 - mae: 1.5914 - val_loss: 11.6830 - val_mse: 11.6830 - val_mae: 1.5429 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 15.4373 - mse: 15.4373 - mae: 1.5881 - val_loss: 11.7332 - val_mse: 11.7332 - val_mae: 1.5244 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 15.4916 - mse: 15.4916 - mae: 1.5893 - val_loss: 11.8925 - val_mse: 11.8925 - val_mae: 1.5239 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 15.5566 - mse: 15.5566 - mae: 1.5932 - val_loss: 11.9152 - val_mse: 11.9152 - val_mae: 1.5511 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Score for fold 3: loss of 11.9151611328125\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 13.0577 - mse: 13.0577 - mae: 1.5695 - val_loss: 21.4511 - val_mse: 21.4511 - val_mae: 1.5773 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 13.0225 - mse: 13.0225 - mae: 1.5702 - val_loss: 21.3323 - val_mse: 21.3323 - val_mae: 1.5774 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 12.9824 - mse: 12.9824 - mae: 1.5651 - val_loss: 21.2746 - val_mse: 21.2746 - val_mae: 1.5635 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 13.0369 - mse: 13.0369 - mae: 1.5606 - val_loss: 21.4569 - val_mse: 21.4569 - val_mae: 1.6488 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 12.9293 - mse: 12.9293 - mae: 1.5650 - val_loss: 22.0051 - val_mse: 22.0051 - val_mae: 1.6937 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 12.9444 - mse: 12.9444 - mae: 1.5671 - val_loss: 21.6057 - val_mse: 21.6057 - val_mae: 1.6986 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 12.9488 - mse: 12.9488 - mae: 1.5694 - val_loss: 21.4049 - val_mse: 21.4049 - val_mae: 1.6460 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 2s - loss: 12.9482 - mse: 12.9482 - mae: 1.5609 - val_loss: 21.6427 - val_mse: 21.6427 - val_mae: 1.5883 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Score for fold 4: loss of 21.642709732055664\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 15.0352 - mse: 15.0352 - mae: 1.5772 - val_loss: 13.2836 - val_mse: 13.2836 - val_mae: 1.5263 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 15.0499 - mse: 15.0499 - mae: 1.5814 - val_loss: 13.3322 - val_mse: 13.3322 - val_mae: 1.5837 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 15.0402 - mse: 15.0402 - mae: 1.5802 - val_loss: 13.4224 - val_mse: 13.4224 - val_mae: 1.5598 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 14.9952 - mse: 14.9952 - mae: 1.5799 - val_loss: 13.7530 - val_mse: 13.7530 - val_mae: 1.5377 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 14.9369 - mse: 14.9369 - mae: 1.5804 - val_loss: 13.5845 - val_mse: 13.5845 - val_mae: 1.5768 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 14.9521 - mse: 14.9521 - mae: 1.5790 - val_loss: 13.7551 - val_mse: 13.7551 - val_mae: 1.5694 - lr: 0.0010 - 2s/epoch - 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-10-14 21:25:04,875]\u001b[0m Finished trial#27 resulted in value: 14.998. Current best value is 14.856 with parameters: {'activation': 'relu', 'num_hidden_layer': 3, 'i': 6, 'learning_rate': 0.0011290700036520834}.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Score for fold 5: loss of 13.755070686340332\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 16.3955 - mse: 16.3955 - mae: 1.6346 - val_loss: 11.4165 - val_mse: 11.4165 - val_mae: 1.5743 - lr: 0.0032 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 16.1230 - mse: 16.1230 - mae: 1.6125 - val_loss: 11.3011 - val_mse: 11.3011 - val_mae: 1.6633 - lr: 0.0032 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 16.0452 - mse: 16.0452 - mae: 1.6071 - val_loss: 11.4078 - val_mse: 11.4078 - val_mae: 1.5584 - lr: 0.0032 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 16.0489 - mse: 16.0489 - mae: 1.6040 - val_loss: 11.2100 - val_mse: 11.2100 - val_mae: 1.5562 - lr: 0.0032 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 15.9887 - mse: 15.9887 - mae: 1.6029 - val_loss: 11.2314 - val_mse: 11.2314 - val_mae: 1.6000 - lr: 0.0032 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 16.0412 - mse: 16.0412 - mae: 1.6023 - val_loss: 11.3172 - val_mse: 11.3172 - val_mae: 1.5817 - lr: 0.0032 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 15.9588 - mse: 15.9588 - mae: 1.5955 - val_loss: 11.4618 - val_mse: 11.4618 - val_mae: 1.5581 - lr: 0.0032 - 2s/epoch - 2ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 2s - loss: 15.8768 - mse: 15.8768 - mae: 1.5950 - val_loss: 11.2790 - val_mse: 11.2790 - val_mae: 1.5671 - lr: 0.0032 - 2s/epoch - 2ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 2s - loss: 15.8902 - mse: 15.8902 - mae: 1.5913 - val_loss: 11.5541 - val_mse: 11.5541 - val_mae: 1.5095 - lr: 0.0032 - 2s/epoch - 2ms/step\n",
            "Score for fold 1: loss of 11.554084777832031\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 14.7253 - mse: 14.7253 - mae: 1.5567 - val_loss: 15.5828 - val_mse: 15.5828 - val_mae: 1.6209 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 14.6800 - mse: 14.6800 - mae: 1.5588 - val_loss: 15.8071 - val_mse: 15.8071 - val_mae: 1.5943 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 14.6337 - mse: 14.6337 - mae: 1.5589 - val_loss: 15.4701 - val_mse: 15.4701 - val_mae: 1.5886 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 14.6517 - mse: 14.6517 - mae: 1.5587 - val_loss: 15.6377 - val_mse: 15.6377 - val_mae: 1.5743 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 14.6463 - mse: 14.6463 - mae: 1.5580 - val_loss: 15.4585 - val_mse: 15.4585 - val_mae: 1.5892 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 14.6395 - mse: 14.6395 - mae: 1.5535 - val_loss: 15.4221 - val_mse: 15.4221 - val_mae: 1.6087 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 14.5908 - mse: 14.5908 - mae: 1.5593 - val_loss: 15.5219 - val_mse: 15.5219 - val_mae: 1.5857 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 2s - loss: 14.5963 - mse: 14.5963 - mae: 1.5580 - val_loss: 15.4445 - val_mse: 15.4445 - val_mae: 1.5582 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 2s - loss: 14.5868 - mse: 14.5868 - mae: 1.5512 - val_loss: 15.3895 - val_mse: 15.3895 - val_mae: 1.6066 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 2s - loss: 14.5807 - mse: 14.5807 - mae: 1.5557 - val_loss: 15.6324 - val_mse: 15.6324 - val_mae: 1.6410 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 2s - loss: 14.6028 - mse: 14.6028 - mae: 1.5537 - val_loss: 15.5275 - val_mse: 15.5275 - val_mae: 1.5970 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 2s - loss: 14.6074 - mse: 14.6074 - mae: 1.5544 - val_loss: 15.5679 - val_mse: 15.5679 - val_mae: 1.6012 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 13/100\n",
            "1000/1000 - 2s - loss: 14.5498 - mse: 14.5498 - mae: 1.5526 - val_loss: 15.7611 - val_mse: 15.7611 - val_mae: 1.6364 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 14/100\n",
            "1000/1000 - 2s - loss: 14.5778 - mse: 14.5778 - mae: 1.5533 - val_loss: 15.7569 - val_mse: 15.7569 - val_mae: 1.5895 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Score for fold 2: loss of 15.75694465637207\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 15.3286 - mse: 15.3286 - mae: 1.5594 - val_loss: 12.9649 - val_mse: 12.9649 - val_mae: 1.5710 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 15.3072 - mse: 15.3072 - mae: 1.5552 - val_loss: 12.8957 - val_mse: 12.8957 - val_mae: 1.6122 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 15.2521 - mse: 15.2521 - mae: 1.5542 - val_loss: 12.8978 - val_mse: 12.8978 - val_mae: 1.6053 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 15.1904 - mse: 15.1904 - mae: 1.5563 - val_loss: 13.1502 - val_mse: 13.1502 - val_mae: 1.6113 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 15.2753 - mse: 15.2753 - mae: 1.5575 - val_loss: 12.8251 - val_mse: 12.8251 - val_mae: 1.5977 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 15.1897 - mse: 15.1897 - mae: 1.5542 - val_loss: 12.9115 - val_mse: 12.9115 - val_mae: 1.6465 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 15.2079 - mse: 15.2079 - mae: 1.5569 - val_loss: 12.9418 - val_mse: 12.9418 - val_mae: 1.5731 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 2s - loss: 15.1713 - mse: 15.1713 - mae: 1.5561 - val_loss: 12.9229 - val_mse: 12.9229 - val_mae: 1.6598 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 2s - loss: 15.2503 - mse: 15.2503 - mae: 1.5569 - val_loss: 12.8543 - val_mse: 12.8543 - val_mae: 1.6070 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 2s - loss: 15.2174 - mse: 15.2174 - mae: 1.5555 - val_loss: 12.8538 - val_mse: 12.8538 - val_mae: 1.6435 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Score for fold 3: loss of 12.853801727294922\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 14.3882 - mse: 14.3882 - mae: 1.5763 - val_loss: 16.0810 - val_mse: 16.0810 - val_mae: 1.5222 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 14.3899 - mse: 14.3899 - mae: 1.5727 - val_loss: 16.1460 - val_mse: 16.1460 - val_mae: 1.6219 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 14.3725 - mse: 14.3725 - mae: 1.5752 - val_loss: 16.1862 - val_mse: 16.1862 - val_mae: 1.5023 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 14.3908 - mse: 14.3908 - mae: 1.5748 - val_loss: 16.2204 - val_mse: 16.2204 - val_mae: 1.5622 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 14.3777 - mse: 14.3777 - mae: 1.5766 - val_loss: 16.5364 - val_mse: 16.5364 - val_mae: 1.5046 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 14.3822 - mse: 14.3822 - mae: 1.5739 - val_loss: 16.1093 - val_mse: 16.1093 - val_mae: 1.5678 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Score for fold 4: loss of 16.10927391052246\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 13.7611 - mse: 13.7611 - mae: 1.5756 - val_loss: 18.4462 - val_mse: 18.4462 - val_mae: 1.5421 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 13.8155 - mse: 13.8155 - mae: 1.5807 - val_loss: 18.4399 - val_mse: 18.4399 - val_mae: 1.5616 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 13.7569 - mse: 13.7569 - mae: 1.5756 - val_loss: 18.4875 - val_mse: 18.4875 - val_mae: 1.6408 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 13.7158 - mse: 13.7158 - mae: 1.5747 - val_loss: 18.6603 - val_mse: 18.6603 - val_mae: 1.5823 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 13.7202 - mse: 13.7202 - mae: 1.5743 - val_loss: 18.4990 - val_mse: 18.4990 - val_mae: 1.6117 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 13.7553 - mse: 13.7553 - mae: 1.5761 - val_loss: 18.6344 - val_mse: 18.6344 - val_mae: 1.5204 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 13.7385 - mse: 13.7385 - mae: 1.5719 - val_loss: 18.4920 - val_mse: 18.4920 - val_mae: 1.5386 - lr: 0.0010 - 2s/epoch - 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-10-14 21:26:22,187]\u001b[0m Finished trial#28 resulted in value: 14.952000000000002. Current best value is 14.856 with parameters: {'activation': 'relu', 'num_hidden_layer': 3, 'i': 6, 'learning_rate': 0.0011290700036520834}.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Score for fold 5: loss of 18.491979598999023\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 13.5620 - mse: 13.5620 - mae: 1.6166 - val_loss: 23.0406 - val_mse: 23.0406 - val_mae: 1.6166 - lr: 0.0014 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 13.1451 - mse: 13.1451 - mae: 1.5943 - val_loss: 22.8805 - val_mse: 22.8805 - val_mae: 1.6757 - lr: 0.0014 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 13.1074 - mse: 13.1074 - mae: 1.5882 - val_loss: 22.8688 - val_mse: 22.8688 - val_mae: 1.6589 - lr: 0.0014 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 13.0543 - mse: 13.0543 - mae: 1.5844 - val_loss: 23.0969 - val_mse: 23.0969 - val_mae: 1.5678 - lr: 0.0014 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 12.9855 - mse: 12.9855 - mae: 1.5804 - val_loss: 23.0065 - val_mse: 23.0065 - val_mae: 1.6967 - lr: 0.0014 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 12.9171 - mse: 12.9171 - mae: 1.5785 - val_loss: 23.0782 - val_mse: 23.0782 - val_mae: 1.6125 - lr: 0.0014 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 12.9746 - mse: 12.9746 - mae: 1.5746 - val_loss: 23.0113 - val_mse: 23.0113 - val_mae: 1.6055 - lr: 0.0014 - 2s/epoch - 2ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 2s - loss: 12.9797 - mse: 12.9797 - mae: 1.5789 - val_loss: 22.9950 - val_mse: 22.9950 - val_mae: 1.6123 - lr: 0.0014 - 2s/epoch - 2ms/step\n",
            "Score for fold 1: loss of 22.994985580444336\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 14.8100 - mse: 14.8100 - mae: 1.5743 - val_loss: 16.1379 - val_mse: 16.1379 - val_mae: 1.5805 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 14.8270 - mse: 14.8270 - mae: 1.5757 - val_loss: 15.4460 - val_mse: 15.4460 - val_mae: 1.5737 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 14.7235 - mse: 14.7235 - mae: 1.5634 - val_loss: 15.5507 - val_mse: 15.5507 - val_mae: 1.7058 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 14.6770 - mse: 14.6770 - mae: 1.5694 - val_loss: 15.5862 - val_mse: 15.5862 - val_mae: 1.6188 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 14.6785 - mse: 14.6785 - mae: 1.5667 - val_loss: 15.7206 - val_mse: 15.7206 - val_mae: 1.6084 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 14.6328 - mse: 14.6328 - mae: 1.5684 - val_loss: 15.6289 - val_mse: 15.6289 - val_mae: 1.7037 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 14.6948 - mse: 14.6948 - mae: 1.5728 - val_loss: 15.7725 - val_mse: 15.7725 - val_mae: 1.6527 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Score for fold 2: loss of 15.77246379852295\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 15.1029 - mse: 15.1029 - mae: 1.5911 - val_loss: 13.3722 - val_mse: 13.3722 - val_mae: 1.5739 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 15.1306 - mse: 15.1306 - mae: 1.5862 - val_loss: 13.8319 - val_mse: 13.8319 - val_mae: 1.5616 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 15.0357 - mse: 15.0357 - mae: 1.5912 - val_loss: 13.6839 - val_mse: 13.6839 - val_mae: 1.5760 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 14.9561 - mse: 14.9561 - mae: 1.5860 - val_loss: 13.8571 - val_mse: 13.8571 - val_mae: 1.6722 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 14.9792 - mse: 14.9792 - mae: 1.5877 - val_loss: 14.2208 - val_mse: 14.2208 - val_mae: 1.5622 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 15.0461 - mse: 15.0461 - mae: 1.5832 - val_loss: 13.6810 - val_mse: 13.6810 - val_mae: 1.5586 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Score for fold 3: loss of 13.681035041809082\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 15.3827 - mse: 15.3827 - mae: 1.5905 - val_loss: 12.2249 - val_mse: 12.2249 - val_mae: 1.6244 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 15.3686 - mse: 15.3686 - mae: 1.5942 - val_loss: 12.2776 - val_mse: 12.2776 - val_mae: 1.4637 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 15.3668 - mse: 15.3668 - mae: 1.5922 - val_loss: 12.7571 - val_mse: 12.7571 - val_mae: 1.6546 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 15.3483 - mse: 15.3483 - mae: 1.5927 - val_loss: 12.3543 - val_mse: 12.3543 - val_mae: 1.5782 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 15.2402 - mse: 15.2402 - mae: 1.5910 - val_loss: 12.2797 - val_mse: 12.2797 - val_mae: 1.5426 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 15.3152 - mse: 15.3152 - mae: 1.5877 - val_loss: 12.4453 - val_mse: 12.4453 - val_mae: 1.5955 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Score for fold 4: loss of 12.445333480834961\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 15.8597 - mse: 15.8597 - mae: 1.5825 - val_loss: 9.7127 - val_mse: 9.7127 - val_mae: 1.7045 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 16.0095 - mse: 16.0095 - mae: 1.5876 - val_loss: 9.6266 - val_mse: 9.6266 - val_mae: 1.5432 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 15.8391 - mse: 15.8391 - mae: 1.5813 - val_loss: 9.8100 - val_mse: 9.8100 - val_mae: 1.4965 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 15.8980 - mse: 15.8980 - mae: 1.5825 - val_loss: 9.8815 - val_mse: 9.8815 - val_mae: 1.6788 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 15.8235 - mse: 15.8235 - mae: 1.5785 - val_loss: 9.9058 - val_mse: 9.9058 - val_mae: 1.5333 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 15.7467 - mse: 15.7467 - mae: 1.5812 - val_loss: 10.0076 - val_mse: 10.0076 - val_mae: 1.6156 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 15.6730 - mse: 15.6730 - mae: 1.5852 - val_loss: 9.9980 - val_mse: 9.9980 - val_mae: 1.6868 - lr: 0.0010 - 2s/epoch - 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-10-14 21:27:24,439]\u001b[0m Finished trial#29 resulted in value: 14.978. Current best value is 14.856 with parameters: {'activation': 'relu', 'num_hidden_layer': 3, 'i': 6, 'learning_rate': 0.0011290700036520834}.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Score for fold 5: loss of 9.998004913330078\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 6s - loss: 13.3879 - mse: 13.3879 - mae: 1.6058 - val_loss: 23.1497 - val_mse: 23.1497 - val_mae: 1.6063 - lr: 4.1035e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 5s - loss: 13.1351 - mse: 13.1351 - mae: 1.5890 - val_loss: 23.1560 - val_mse: 23.1560 - val_mae: 1.6552 - lr: 4.1035e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 5s - loss: 13.1284 - mse: 13.1284 - mae: 1.5845 - val_loss: 23.3338 - val_mse: 23.3338 - val_mae: 1.5697 - lr: 4.1035e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 5s - loss: 12.9886 - mse: 12.9886 - mae: 1.5768 - val_loss: 23.3243 - val_mse: 23.3243 - val_mae: 1.5671 - lr: 4.1035e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 6s - loss: 13.0113 - mse: 13.0113 - mae: 1.5746 - val_loss: 23.1115 - val_mse: 23.1115 - val_mae: 1.6574 - lr: 4.1035e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 5s - loss: 12.9110 - mse: 12.9110 - mae: 1.5743 - val_loss: 22.9219 - val_mse: 22.9219 - val_mae: 1.7235 - lr: 4.1035e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 6s - loss: 12.9716 - mse: 12.9716 - mae: 1.5691 - val_loss: 23.0600 - val_mse: 23.0600 - val_mae: 1.6606 - lr: 4.1035e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 6s - loss: 12.8899 - mse: 12.8899 - mae: 1.5690 - val_loss: 23.2748 - val_mse: 23.2748 - val_mae: 1.5860 - lr: 4.1035e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 6s - loss: 12.8029 - mse: 12.8029 - mae: 1.5650 - val_loss: 23.0465 - val_mse: 23.0465 - val_mae: 1.6189 - lr: 4.1035e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 5s - loss: 12.7771 - mse: 12.7771 - mae: 1.5660 - val_loss: 23.0055 - val_mse: 23.0055 - val_mae: 1.6628 - lr: 4.1035e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 5s - loss: 12.8332 - mse: 12.8332 - mae: 1.5611 - val_loss: 23.0164 - val_mse: 23.0164 - val_mae: 1.5594 - lr: 4.1035e-04 - 5s/epoch - 5ms/step\n",
            "Score for fold 1: loss of 23.016437530517578\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 6s - loss: 15.4500 - mse: 15.4500 - mae: 1.5744 - val_loss: 12.4437 - val_mse: 12.4437 - val_mae: 1.5352 - lr: 4.1035e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 5s - loss: 15.4556 - mse: 15.4556 - mae: 1.5754 - val_loss: 12.6285 - val_mse: 12.6285 - val_mae: 1.5437 - lr: 4.1035e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 6s - loss: 15.3257 - mse: 15.3257 - mae: 1.5687 - val_loss: 12.5560 - val_mse: 12.5560 - val_mae: 1.5402 - lr: 4.1035e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 6s - loss: 15.4324 - mse: 15.4324 - mae: 1.5677 - val_loss: 12.6490 - val_mse: 12.6490 - val_mae: 1.5613 - lr: 4.1035e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 6s - loss: 15.2761 - mse: 15.2761 - mae: 1.5671 - val_loss: 12.4975 - val_mse: 12.4975 - val_mae: 1.5944 - lr: 4.1035e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 6s - loss: 15.3482 - mse: 15.3482 - mae: 1.5664 - val_loss: 12.7085 - val_mse: 12.7085 - val_mae: 1.5254 - lr: 4.1035e-04 - 6s/epoch - 6ms/step\n",
            "Score for fold 2: loss of 12.708471298217773\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 6s - loss: 14.7975 - mse: 14.7975 - mae: 1.5622 - val_loss: 14.3419 - val_mse: 14.3419 - val_mae: 1.5390 - lr: 4.1035e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 6s - loss: 14.7179 - mse: 14.7179 - mae: 1.5603 - val_loss: 14.3380 - val_mse: 14.3380 - val_mae: 1.5680 - lr: 4.1035e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 6s - loss: 14.7609 - mse: 14.7609 - mae: 1.5576 - val_loss: 14.7778 - val_mse: 14.7778 - val_mae: 1.5728 - lr: 4.1035e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 6s - loss: 14.7245 - mse: 14.7245 - mae: 1.5575 - val_loss: 14.7879 - val_mse: 14.7879 - val_mae: 1.5449 - lr: 4.1035e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 6s - loss: 14.6688 - mse: 14.6688 - mae: 1.5534 - val_loss: 14.7812 - val_mse: 14.7812 - val_mae: 1.5503 - lr: 4.1035e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 6s - loss: 14.5997 - mse: 14.5997 - mae: 1.5552 - val_loss: 14.8241 - val_mse: 14.8241 - val_mae: 1.5918 - lr: 4.1035e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 6s - loss: 14.6356 - mse: 14.6356 - mae: 1.5539 - val_loss: 14.7881 - val_mse: 14.7881 - val_mae: 1.6077 - lr: 4.1035e-04 - 6s/epoch - 6ms/step\n",
            "Score for fold 3: loss of 14.788067817687988\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 6s - loss: 15.5043 - mse: 15.5043 - mae: 1.5606 - val_loss: 11.3500 - val_mse: 11.3500 - val_mae: 1.4962 - lr: 4.1035e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 6s - loss: 15.2897 - mse: 15.2897 - mae: 1.5576 - val_loss: 11.1728 - val_mse: 11.1728 - val_mae: 1.5497 - lr: 4.1035e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 6s - loss: 15.3864 - mse: 15.3864 - mae: 1.5619 - val_loss: 11.2381 - val_mse: 11.2381 - val_mae: 1.5622 - lr: 4.1035e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 6s - loss: 15.4199 - mse: 15.4199 - mae: 1.5618 - val_loss: 11.3637 - val_mse: 11.3637 - val_mae: 1.5240 - lr: 4.1035e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 6s - loss: 15.2980 - mse: 15.2980 - mae: 1.5582 - val_loss: 11.3253 - val_mse: 11.3253 - val_mae: 1.5518 - lr: 4.1035e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 6s - loss: 15.2841 - mse: 15.2841 - mae: 1.5551 - val_loss: 11.3949 - val_mse: 11.3949 - val_mae: 1.5840 - lr: 4.1035e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 5s - loss: 15.2621 - mse: 15.2621 - mae: 1.5541 - val_loss: 11.4287 - val_mse: 11.4287 - val_mae: 1.5299 - lr: 4.1035e-04 - 5s/epoch - 5ms/step\n",
            "Score for fold 4: loss of 11.428715705871582\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 5s - loss: 15.0590 - mse: 15.0590 - mae: 1.5509 - val_loss: 12.5325 - val_mse: 12.5325 - val_mae: 1.5015 - lr: 4.1035e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 6s - loss: 15.0336 - mse: 15.0336 - mae: 1.5527 - val_loss: 12.2783 - val_mse: 12.2783 - val_mae: 1.5618 - lr: 4.1035e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 5s - loss: 14.9586 - mse: 14.9586 - mae: 1.5496 - val_loss: 12.1076 - val_mse: 12.1076 - val_mae: 1.5902 - lr: 4.1035e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 5s - loss: 14.8851 - mse: 14.8851 - mae: 1.5458 - val_loss: 12.0703 - val_mse: 12.0703 - val_mae: 1.5344 - lr: 4.1035e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 5s - loss: 14.9209 - mse: 14.9209 - mae: 1.5440 - val_loss: 12.6042 - val_mse: 12.6042 - val_mae: 1.5200 - lr: 4.1035e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 5s - loss: 14.8108 - mse: 14.8108 - mae: 1.5398 - val_loss: 12.3715 - val_mse: 12.3715 - val_mae: 1.5396 - lr: 4.1035e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 5s - loss: 14.7137 - mse: 14.7137 - mae: 1.5353 - val_loss: 11.9496 - val_mse: 11.9496 - val_mae: 1.5393 - lr: 4.1035e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 5s - loss: 14.7589 - mse: 14.7589 - mae: 1.5326 - val_loss: 12.7369 - val_mse: 12.7369 - val_mae: 1.5432 - lr: 4.1035e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 5s - loss: 14.6564 - mse: 14.6564 - mae: 1.5303 - val_loss: 12.8585 - val_mse: 12.8585 - val_mae: 1.5590 - lr: 4.1035e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 5s - loss: 14.6324 - mse: 14.6324 - mae: 1.5266 - val_loss: 12.6814 - val_mse: 12.6814 - val_mae: 1.4939 - lr: 4.1035e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 5s - loss: 14.5698 - mse: 14.5698 - mae: 1.5204 - val_loss: 12.6141 - val_mse: 12.6141 - val_mae: 1.6143 - lr: 4.1035e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 5s - loss: 14.4998 - mse: 14.4998 - mae: 1.5174 - val_loss: 12.6815 - val_mse: 12.6815 - val_mae: 1.5502 - lr: 4.1035e-04 - 5s/epoch - 5ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-10-14 21:31:29,734]\u001b[0m Finished trial#30 resulted in value: 14.925999999999998. Current best value is 14.856 with parameters: {'activation': 'relu', 'num_hidden_layer': 3, 'i': 6, 'learning_rate': 0.0011290700036520834}.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Score for fold 5: loss of 12.681550979614258\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 16.5706 - mse: 16.5706 - mae: 1.6401 - val_loss: 12.0160 - val_mse: 12.0160 - val_mae: 1.5620 - lr: 6.4185e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 15.9664 - mse: 15.9664 - mae: 1.6036 - val_loss: 11.9825 - val_mse: 11.9825 - val_mae: 1.5727 - lr: 6.4185e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 15.8819 - mse: 15.8819 - mae: 1.6043 - val_loss: 11.8647 - val_mse: 11.8647 - val_mae: 1.5269 - lr: 6.4185e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 15.8576 - mse: 15.8576 - mae: 1.5967 - val_loss: 11.8436 - val_mse: 11.8436 - val_mae: 1.6226 - lr: 6.4185e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 15.8278 - mse: 15.8278 - mae: 1.6018 - val_loss: 11.9914 - val_mse: 11.9914 - val_mae: 1.4732 - lr: 6.4185e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 15.8112 - mse: 15.8112 - mae: 1.5975 - val_loss: 11.8412 - val_mse: 11.8412 - val_mae: 1.4945 - lr: 6.4185e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 15.7527 - mse: 15.7527 - mae: 1.5917 - val_loss: 11.9684 - val_mse: 11.9684 - val_mae: 1.5180 - lr: 6.4185e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 2s - loss: 15.7591 - mse: 15.7591 - mae: 1.5929 - val_loss: 11.7881 - val_mse: 11.7881 - val_mae: 1.5322 - lr: 6.4185e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 2s - loss: 15.7623 - mse: 15.7623 - mae: 1.5882 - val_loss: 11.7605 - val_mse: 11.7605 - val_mae: 1.5288 - lr: 6.4185e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 2s - loss: 15.6959 - mse: 15.6959 - mae: 1.5910 - val_loss: 11.7975 - val_mse: 11.7975 - val_mae: 1.5172 - lr: 6.4185e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 2s - loss: 15.6848 - mse: 15.6848 - mae: 1.5874 - val_loss: 11.8564 - val_mse: 11.8564 - val_mae: 1.5308 - lr: 6.4185e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 2s - loss: 15.6710 - mse: 15.6710 - mae: 1.5845 - val_loss: 11.7110 - val_mse: 11.7110 - val_mae: 1.5861 - lr: 6.4185e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 13/100\n",
            "1000/1000 - 2s - loss: 15.6489 - mse: 15.6489 - mae: 1.5862 - val_loss: 11.7688 - val_mse: 11.7688 - val_mae: 1.4884 - lr: 6.4185e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 14/100\n",
            "1000/1000 - 2s - loss: 15.7131 - mse: 15.7131 - mae: 1.5839 - val_loss: 11.6879 - val_mse: 11.6879 - val_mae: 1.5635 - lr: 6.4185e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 15/100\n",
            "1000/1000 - 2s - loss: 15.6185 - mse: 15.6185 - mae: 1.5828 - val_loss: 11.7116 - val_mse: 11.7116 - val_mae: 1.5286 - lr: 6.4185e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 16/100\n",
            "1000/1000 - 2s - loss: 15.6263 - mse: 15.6263 - mae: 1.5819 - val_loss: 11.7522 - val_mse: 11.7522 - val_mae: 1.4978 - lr: 6.4185e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 17/100\n",
            "1000/1000 - 2s - loss: 15.6100 - mse: 15.6100 - mae: 1.5827 - val_loss: 11.7623 - val_mse: 11.7623 - val_mae: 1.5225 - lr: 6.4185e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 18/100\n",
            "1000/1000 - 2s - loss: 15.6067 - mse: 15.6067 - mae: 1.5802 - val_loss: 11.7276 - val_mse: 11.7276 - val_mae: 1.4972 - lr: 6.4185e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 19/100\n",
            "1000/1000 - 2s - loss: 15.5931 - mse: 15.5931 - mae: 1.5790 - val_loss: 11.6442 - val_mse: 11.6442 - val_mae: 1.5276 - lr: 6.4185e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 20/100\n",
            "1000/1000 - 2s - loss: 15.5639 - mse: 15.5639 - mae: 1.5777 - val_loss: 11.6354 - val_mse: 11.6354 - val_mae: 1.5282 - lr: 6.4185e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 21/100\n",
            "1000/1000 - 2s - loss: 15.5642 - mse: 15.5642 - mae: 1.5808 - val_loss: 11.6070 - val_mse: 11.6070 - val_mae: 1.5404 - lr: 6.4185e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 22/100\n",
            "1000/1000 - 2s - loss: 15.5239 - mse: 15.5239 - mae: 1.5799 - val_loss: 11.6163 - val_mse: 11.6163 - val_mae: 1.5451 - lr: 6.4185e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 23/100\n",
            "1000/1000 - 2s - loss: 15.5838 - mse: 15.5838 - mae: 1.5772 - val_loss: 11.5959 - val_mse: 11.5959 - val_mae: 1.5926 - lr: 6.4185e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 24/100\n",
            "1000/1000 - 2s - loss: 15.4904 - mse: 15.4904 - mae: 1.5807 - val_loss: 11.6860 - val_mse: 11.6860 - val_mae: 1.5059 - lr: 6.4185e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 25/100\n",
            "1000/1000 - 2s - loss: 15.5738 - mse: 15.5738 - mae: 1.5786 - val_loss: 11.6314 - val_mse: 11.6314 - val_mae: 1.5368 - lr: 6.4185e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 26/100\n",
            "1000/1000 - 2s - loss: 15.4992 - mse: 15.4992 - mae: 1.5777 - val_loss: 11.6113 - val_mse: 11.6113 - val_mae: 1.5624 - lr: 6.4185e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 27/100\n",
            "1000/1000 - 2s - loss: 15.4962 - mse: 15.4962 - mae: 1.5769 - val_loss: 11.5615 - val_mse: 11.5615 - val_mae: 1.5556 - lr: 6.4185e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 28/100\n",
            "1000/1000 - 2s - loss: 15.5122 - mse: 15.5122 - mae: 1.5759 - val_loss: 11.5966 - val_mse: 11.5966 - val_mae: 1.6012 - lr: 6.4185e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 29/100\n",
            "1000/1000 - 2s - loss: 15.5192 - mse: 15.5192 - mae: 1.5763 - val_loss: 11.7049 - val_mse: 11.7049 - val_mae: 1.4953 - lr: 6.4185e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 30/100\n",
            "1000/1000 - 2s - loss: 15.4897 - mse: 15.4897 - mae: 1.5796 - val_loss: 11.7571 - val_mse: 11.7571 - val_mae: 1.5447 - lr: 6.4185e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 31/100\n",
            "1000/1000 - 2s - loss: 15.4451 - mse: 15.4451 - mae: 1.5734 - val_loss: 11.7462 - val_mse: 11.7462 - val_mae: 1.5322 - lr: 6.4185e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 32/100\n",
            "1000/1000 - 2s - loss: 15.4602 - mse: 15.4602 - mae: 1.5753 - val_loss: 11.7749 - val_mse: 11.7749 - val_mae: 1.5137 - lr: 6.4185e-04 - 2s/epoch - 2ms/step\n",
            "Score for fold 1: loss of 11.774919509887695\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 14.8986 - mse: 14.8986 - mae: 1.5678 - val_loss: 13.9276 - val_mse: 13.9276 - val_mae: 1.5474 - lr: 6.4185e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 14.9774 - mse: 14.9774 - mae: 1.5710 - val_loss: 13.7857 - val_mse: 13.7857 - val_mae: 1.5449 - lr: 6.4185e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 14.9629 - mse: 14.9629 - mae: 1.5709 - val_loss: 13.7972 - val_mse: 13.7972 - val_mae: 1.5642 - lr: 6.4185e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 14.9292 - mse: 14.9292 - mae: 1.5684 - val_loss: 13.7892 - val_mse: 13.7892 - val_mae: 1.5739 - lr: 6.4185e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 14.8854 - mse: 14.8854 - mae: 1.5650 - val_loss: 13.8946 - val_mse: 13.8946 - val_mae: 1.5701 - lr: 6.4185e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 14.8199 - mse: 14.8199 - mae: 1.5640 - val_loss: 13.8234 - val_mse: 13.8234 - val_mae: 1.5807 - lr: 6.4185e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 14.8385 - mse: 14.8385 - mae: 1.5657 - val_loss: 13.8634 - val_mse: 13.8634 - val_mae: 1.5559 - lr: 6.4185e-04 - 2s/epoch - 2ms/step\n",
            "Score for fold 2: loss of 13.86337661743164\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 15.5164 - mse: 15.5164 - mae: 1.5665 - val_loss: 11.2387 - val_mse: 11.2387 - val_mae: 1.5197 - lr: 6.4185e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 15.4862 - mse: 15.4862 - mae: 1.5648 - val_loss: 11.3183 - val_mse: 11.3183 - val_mae: 1.5465 - lr: 6.4185e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 15.4955 - mse: 15.4955 - mae: 1.5648 - val_loss: 11.2818 - val_mse: 11.2818 - val_mae: 1.5505 - lr: 6.4185e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 15.5108 - mse: 15.5108 - mae: 1.5610 - val_loss: 11.2790 - val_mse: 11.2790 - val_mae: 1.5346 - lr: 6.4185e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 15.3601 - mse: 15.3601 - mae: 1.5606 - val_loss: 11.1557 - val_mse: 11.1557 - val_mae: 1.6009 - lr: 6.4185e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 15.5194 - mse: 15.5194 - mae: 1.5625 - val_loss: 11.1620 - val_mse: 11.1620 - val_mae: 1.5538 - lr: 6.4185e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 15.4917 - mse: 15.4917 - mae: 1.5621 - val_loss: 11.2223 - val_mse: 11.2223 - val_mae: 1.5756 - lr: 6.4185e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 2s - loss: 15.4279 - mse: 15.4279 - mae: 1.5613 - val_loss: 11.2810 - val_mse: 11.2810 - val_mae: 1.5564 - lr: 6.4185e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 2s - loss: 15.4509 - mse: 15.4509 - mae: 1.5613 - val_loss: 11.4879 - val_mse: 11.4879 - val_mae: 1.5754 - lr: 6.4185e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 2s - loss: 15.4386 - mse: 15.4386 - mae: 1.5614 - val_loss: 11.4815 - val_mse: 11.4815 - val_mae: 1.5808 - lr: 6.4185e-04 - 2s/epoch - 2ms/step\n",
            "Score for fold 3: loss of 11.48147964477539\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 12.4804 - mse: 12.4804 - mae: 1.5434 - val_loss: 23.2401 - val_mse: 23.2401 - val_mae: 1.5791 - lr: 6.4185e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 12.4473 - mse: 12.4473 - mae: 1.5421 - val_loss: 23.1849 - val_mse: 23.1849 - val_mae: 1.6401 - lr: 6.4185e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 12.4676 - mse: 12.4676 - mae: 1.5352 - val_loss: 23.3360 - val_mse: 23.3360 - val_mae: 1.6215 - lr: 6.4185e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 12.4411 - mse: 12.4411 - mae: 1.5365 - val_loss: 23.5927 - val_mse: 23.5927 - val_mae: 1.6343 - lr: 6.4185e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 12.3807 - mse: 12.3807 - mae: 1.5377 - val_loss: 23.3595 - val_mse: 23.3595 - val_mae: 1.6337 - lr: 6.4185e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 12.4045 - mse: 12.4045 - mae: 1.5374 - val_loss: 23.4312 - val_mse: 23.4312 - val_mae: 1.6241 - lr: 6.4185e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 12.4071 - mse: 12.4071 - mae: 1.5346 - val_loss: 23.4744 - val_mse: 23.4744 - val_mae: 1.6200 - lr: 6.4185e-04 - 2s/epoch - 2ms/step\n",
            "Score for fold 4: loss of 23.474409103393555\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 14.8617 - mse: 14.8617 - mae: 1.5589 - val_loss: 13.5275 - val_mse: 13.5275 - val_mae: 1.5339 - lr: 6.4185e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 14.8530 - mse: 14.8530 - mae: 1.5607 - val_loss: 13.2768 - val_mse: 13.2768 - val_mae: 1.5642 - lr: 6.4185e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 14.8182 - mse: 14.8182 - mae: 1.5554 - val_loss: 13.6696 - val_mse: 13.6696 - val_mae: 1.5482 - lr: 6.4185e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 14.8135 - mse: 14.8135 - mae: 1.5576 - val_loss: 13.6590 - val_mse: 13.6590 - val_mae: 1.5388 - lr: 6.4185e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 14.7744 - mse: 14.7744 - mae: 1.5537 - val_loss: 13.5564 - val_mse: 13.5564 - val_mae: 1.5507 - lr: 6.4185e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 14.7503 - mse: 14.7503 - mae: 1.5550 - val_loss: 13.7920 - val_mse: 13.7920 - val_mae: 1.5468 - lr: 6.4185e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 14.7550 - mse: 14.7550 - mae: 1.5540 - val_loss: 13.5522 - val_mse: 13.5522 - val_mae: 1.5485 - lr: 6.4185e-04 - 2s/epoch - 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-10-14 21:33:13,187]\u001b[0m Finished trial#31 resulted in value: 14.825999999999999. Current best value is 14.825999999999999 with parameters: {'activation': 'relu', 'num_hidden_layer': 3, 'i': 6, 'learning_rate': 0.000641854957781097}.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Score for fold 5: loss of 13.552206993103027\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 15.5027 - mse: 15.5027 - mae: 1.6144 - val_loss: 15.6931 - val_mse: 15.6931 - val_mae: 1.6555 - lr: 0.0011 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 15.0135 - mse: 15.0135 - mae: 1.5910 - val_loss: 15.6367 - val_mse: 15.6367 - val_mae: 1.5974 - lr: 0.0011 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 14.9554 - mse: 14.9554 - mae: 1.5872 - val_loss: 15.6590 - val_mse: 15.6590 - val_mae: 1.6620 - lr: 0.0011 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 14.9221 - mse: 14.9221 - mae: 1.5834 - val_loss: 15.7155 - val_mse: 15.7155 - val_mae: 1.5444 - lr: 0.0011 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 14.8574 - mse: 14.8574 - mae: 1.5806 - val_loss: 15.6467 - val_mse: 15.6467 - val_mae: 1.5872 - lr: 0.0011 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 14.8429 - mse: 14.8429 - mae: 1.5759 - val_loss: 15.4630 - val_mse: 15.4630 - val_mae: 1.6369 - lr: 0.0011 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 14.7857 - mse: 14.7857 - mae: 1.5736 - val_loss: 15.7793 - val_mse: 15.7793 - val_mae: 1.5630 - lr: 0.0011 - 2s/epoch - 2ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 2s - loss: 14.7793 - mse: 14.7793 - mae: 1.5700 - val_loss: 15.6170 - val_mse: 15.6170 - val_mae: 1.5740 - lr: 0.0011 - 2s/epoch - 2ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 2s - loss: 14.7683 - mse: 14.7683 - mae: 1.5716 - val_loss: 15.7706 - val_mse: 15.7706 - val_mae: 1.6004 - lr: 0.0011 - 2s/epoch - 2ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 2s - loss: 14.7332 - mse: 14.7332 - mae: 1.5704 - val_loss: 15.6055 - val_mse: 15.6055 - val_mae: 1.5912 - lr: 0.0011 - 2s/epoch - 2ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 2s - loss: 14.7464 - mse: 14.7464 - mae: 1.5703 - val_loss: 15.7253 - val_mse: 15.7253 - val_mae: 1.5384 - lr: 0.0011 - 2s/epoch - 2ms/step\n",
            "Score for fold 1: loss of 15.725255966186523\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 15.8723 - mse: 15.8723 - mae: 1.5807 - val_loss: 10.8691 - val_mse: 10.8691 - val_mae: 1.6070 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 15.8753 - mse: 15.8753 - mae: 1.5821 - val_loss: 10.8153 - val_mse: 10.8153 - val_mae: 1.5684 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 15.8157 - mse: 15.8157 - mae: 1.5826 - val_loss: 10.8689 - val_mse: 10.8689 - val_mae: 1.5466 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 15.8389 - mse: 15.8389 - mae: 1.5835 - val_loss: 10.9049 - val_mse: 10.9049 - val_mae: 1.5489 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 15.8168 - mse: 15.8168 - mae: 1.5807 - val_loss: 10.9659 - val_mse: 10.9659 - val_mae: 1.5497 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 15.8234 - mse: 15.8234 - mae: 1.5833 - val_loss: 10.8481 - val_mse: 10.8481 - val_mae: 1.5792 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 15.7527 - mse: 15.7527 - mae: 1.5770 - val_loss: 10.9818 - val_mse: 10.9818 - val_mae: 1.5417 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Score for fold 2: loss of 10.981758117675781\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 15.4913 - mse: 15.4913 - mae: 1.5794 - val_loss: 12.3179 - val_mse: 12.3179 - val_mae: 1.5938 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 15.4666 - mse: 15.4666 - mae: 1.5784 - val_loss: 12.3581 - val_mse: 12.3581 - val_mae: 1.5659 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 15.4654 - mse: 15.4654 - mae: 1.5755 - val_loss: 12.2718 - val_mse: 12.2718 - val_mae: 1.5531 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 15.4762 - mse: 15.4762 - mae: 1.5733 - val_loss: 12.3999 - val_mse: 12.3999 - val_mae: 1.5507 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 15.4313 - mse: 15.4313 - mae: 1.5770 - val_loss: 12.3134 - val_mse: 12.3134 - val_mae: 1.5557 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 15.3856 - mse: 15.3856 - mae: 1.5710 - val_loss: 12.1618 - val_mse: 12.1618 - val_mae: 1.5804 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 15.4097 - mse: 15.4097 - mae: 1.5701 - val_loss: 12.5217 - val_mse: 12.5217 - val_mae: 1.5765 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 2s - loss: 15.3980 - mse: 15.3980 - mae: 1.5726 - val_loss: 12.3686 - val_mse: 12.3686 - val_mae: 1.6299 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 2s - loss: 15.3626 - mse: 15.3626 - mae: 1.5727 - val_loss: 12.3051 - val_mse: 12.3051 - val_mae: 1.5562 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 2s - loss: 15.3521 - mse: 15.3521 - mae: 1.5686 - val_loss: 12.2469 - val_mse: 12.2469 - val_mae: 1.5681 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 2s - loss: 15.3685 - mse: 15.3685 - mae: 1.5661 - val_loss: 12.5880 - val_mse: 12.5880 - val_mae: 1.5245 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Score for fold 3: loss of 12.587986946105957\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 12.3858 - mse: 12.3858 - mae: 1.5540 - val_loss: 24.2714 - val_mse: 24.2714 - val_mae: 1.6341 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 12.2368 - mse: 12.2368 - mae: 1.5554 - val_loss: 24.2754 - val_mse: 24.2754 - val_mae: 1.5918 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 12.3923 - mse: 12.3923 - mae: 1.5567 - val_loss: 24.2417 - val_mse: 24.2417 - val_mae: 1.6258 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 12.3367 - mse: 12.3367 - mae: 1.5571 - val_loss: 24.3362 - val_mse: 24.3362 - val_mae: 1.6338 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 12.2894 - mse: 12.2894 - mae: 1.5514 - val_loss: 24.3160 - val_mse: 24.3160 - val_mae: 1.5839 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 12.2869 - mse: 12.2869 - mae: 1.5544 - val_loss: 24.2255 - val_mse: 24.2255 - val_mae: 1.6138 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 12.2790 - mse: 12.2790 - mae: 1.5529 - val_loss: 24.3656 - val_mse: 24.3656 - val_mae: 1.5986 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 2s - loss: 12.2466 - mse: 12.2466 - mae: 1.5518 - val_loss: 24.2430 - val_mse: 24.2430 - val_mae: 1.6258 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 2s - loss: 12.2056 - mse: 12.2056 - mae: 1.5529 - val_loss: 24.3364 - val_mse: 24.3364 - val_mae: 1.6238 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 2s - loss: 12.1463 - mse: 12.1463 - mae: 1.5505 - val_loss: 24.2639 - val_mse: 24.2639 - val_mae: 1.6596 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 2s - loss: 12.2037 - mse: 12.2037 - mae: 1.5494 - val_loss: 24.5294 - val_mse: 24.5294 - val_mae: 1.5940 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Score for fold 4: loss of 24.529390335083008\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 15.5645 - mse: 15.5645 - mae: 1.5716 - val_loss: 11.0196 - val_mse: 11.0196 - val_mae: 1.5501 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 15.5552 - mse: 15.5552 - mae: 1.5698 - val_loss: 10.9851 - val_mse: 10.9851 - val_mae: 1.5398 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 15.5002 - mse: 15.5002 - mae: 1.5710 - val_loss: 11.1261 - val_mse: 11.1261 - val_mae: 1.5097 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 15.3445 - mse: 15.3445 - mae: 1.5682 - val_loss: 11.3620 - val_mse: 11.3620 - val_mae: 1.5483 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 15.5303 - mse: 15.5303 - mae: 1.5691 - val_loss: 10.8920 - val_mse: 10.8920 - val_mae: 1.6128 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 15.5517 - mse: 15.5517 - mae: 1.5670 - val_loss: 11.2412 - val_mse: 11.2412 - val_mae: 1.5540 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 15.4620 - mse: 15.4620 - mae: 1.5654 - val_loss: 11.0124 - val_mse: 11.0124 - val_mae: 1.5531 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 2s - loss: 15.4839 - mse: 15.4839 - mae: 1.5638 - val_loss: 11.0233 - val_mse: 11.0233 - val_mae: 1.6023 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 2s - loss: 15.4442 - mse: 15.4442 - mae: 1.5647 - val_loss: 11.1574 - val_mse: 11.1574 - val_mae: 1.5643 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 2s - loss: 15.3999 - mse: 15.3999 - mae: 1.5651 - val_loss: 11.0813 - val_mse: 11.0813 - val_mae: 1.5400 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Score for fold 5: loss of 11.08126163482666\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-10-14 21:34:35,706]\u001b[0m Finished trial#32 resulted in value: 14.982. Current best value is 14.825999999999999 with parameters: {'activation': 'relu', 'num_hidden_layer': 3, 'i': 6, 'learning_rate': 0.000641854957781097}.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 14.4819 - mse: 14.4819 - mae: 1.6171 - val_loss: 19.6652 - val_mse: 19.6652 - val_mae: 1.6078 - lr: 6.4106e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 14.0116 - mse: 14.0116 - mae: 1.6008 - val_loss: 19.6508 - val_mse: 19.6508 - val_mae: 1.6381 - lr: 6.4106e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 13.9799 - mse: 13.9799 - mae: 1.5947 - val_loss: 19.4346 - val_mse: 19.4346 - val_mae: 1.6342 - lr: 6.4106e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 13.9309 - mse: 13.9309 - mae: 1.5881 - val_loss: 19.4245 - val_mse: 19.4245 - val_mae: 1.5596 - lr: 6.4106e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 13.9137 - mse: 13.9137 - mae: 1.5879 - val_loss: 19.4571 - val_mse: 19.4571 - val_mae: 1.5397 - lr: 6.4106e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 13.7869 - mse: 13.7869 - mae: 1.5867 - val_loss: 19.3941 - val_mse: 19.3941 - val_mae: 1.6687 - lr: 6.4106e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 13.8183 - mse: 13.8183 - mae: 1.5834 - val_loss: 19.4218 - val_mse: 19.4218 - val_mae: 1.5714 - lr: 6.4106e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 2s - loss: 13.8042 - mse: 13.8042 - mae: 1.5819 - val_loss: 19.3757 - val_mse: 19.3757 - val_mae: 1.6132 - lr: 6.4106e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 2s - loss: 13.7580 - mse: 13.7580 - mae: 1.5813 - val_loss: 19.3558 - val_mse: 19.3558 - val_mae: 1.5524 - lr: 6.4106e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 2s - loss: 13.6827 - mse: 13.6827 - mae: 1.5749 - val_loss: 19.3097 - val_mse: 19.3097 - val_mae: 1.5460 - lr: 6.4106e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 2s - loss: 13.7590 - mse: 13.7590 - mae: 1.5796 - val_loss: 19.4433 - val_mse: 19.4433 - val_mae: 1.5036 - lr: 6.4106e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 2s - loss: 13.7285 - mse: 13.7285 - mae: 1.5741 - val_loss: 19.3009 - val_mse: 19.3009 - val_mae: 1.6068 - lr: 6.4106e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 13/100\n",
            "1000/1000 - 2s - loss: 13.7067 - mse: 13.7067 - mae: 1.5788 - val_loss: 19.2698 - val_mse: 19.2698 - val_mae: 1.5890 - lr: 6.4106e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 14/100\n",
            "1000/1000 - 2s - loss: 13.6504 - mse: 13.6504 - mae: 1.5766 - val_loss: 19.2674 - val_mse: 19.2674 - val_mae: 1.6207 - lr: 6.4106e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 15/100\n",
            "1000/1000 - 2s - loss: 13.6621 - mse: 13.6621 - mae: 1.5758 - val_loss: 19.3565 - val_mse: 19.3565 - val_mae: 1.6232 - lr: 6.4106e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 16/100\n",
            "1000/1000 - 2s - loss: 13.6941 - mse: 13.6941 - mae: 1.5751 - val_loss: 19.2648 - val_mse: 19.2648 - val_mae: 1.5750 - lr: 6.4106e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 17/100\n",
            "1000/1000 - 2s - loss: 13.6663 - mse: 13.6663 - mae: 1.5777 - val_loss: 19.3133 - val_mse: 19.3133 - val_mae: 1.5986 - lr: 6.4106e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 18/100\n",
            "1000/1000 - 2s - loss: 13.6402 - mse: 13.6402 - mae: 1.5743 - val_loss: 19.4017 - val_mse: 19.4017 - val_mae: 1.5496 - lr: 6.4106e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 19/100\n",
            "1000/1000 - 2s - loss: 13.6362 - mse: 13.6362 - mae: 1.5666 - val_loss: 19.3110 - val_mse: 19.3110 - val_mae: 1.5614 - lr: 6.4106e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 20/100\n",
            "1000/1000 - 2s - loss: 13.6664 - mse: 13.6664 - mae: 1.5727 - val_loss: 19.4427 - val_mse: 19.4427 - val_mae: 1.5802 - lr: 6.4106e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 21/100\n",
            "1000/1000 - 2s - loss: 13.6020 - mse: 13.6020 - mae: 1.5730 - val_loss: 19.5436 - val_mse: 19.5436 - val_mae: 1.5220 - lr: 6.4106e-04 - 2s/epoch - 2ms/step\n",
            "Score for fold 1: loss of 19.54358673095703\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 15.8671 - mse: 15.8671 - mae: 1.5644 - val_loss: 10.7877 - val_mse: 10.7877 - val_mae: 1.5644 - lr: 6.4106e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 15.7276 - mse: 15.7276 - mae: 1.5677 - val_loss: 10.7820 - val_mse: 10.7820 - val_mae: 1.6269 - lr: 6.4106e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 15.7144 - mse: 15.7144 - mae: 1.5648 - val_loss: 10.8681 - val_mse: 10.8681 - val_mae: 1.5791 - lr: 6.4106e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 15.7479 - mse: 15.7479 - mae: 1.5593 - val_loss: 10.8573 - val_mse: 10.8573 - val_mae: 1.5546 - lr: 6.4106e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 15.6528 - mse: 15.6528 - mae: 1.5592 - val_loss: 10.9758 - val_mse: 10.9758 - val_mae: 1.5733 - lr: 6.4106e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 15.5801 - mse: 15.5801 - mae: 1.5585 - val_loss: 11.0497 - val_mse: 11.0497 - val_mae: 1.5316 - lr: 6.4106e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 15.6318 - mse: 15.6318 - mae: 1.5569 - val_loss: 10.8460 - val_mse: 10.8460 - val_mae: 1.5500 - lr: 6.4106e-04 - 2s/epoch - 2ms/step\n",
            "Score for fold 2: loss of 10.846037864685059\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 14.2040 - mse: 14.2040 - mae: 1.5594 - val_loss: 16.3730 - val_mse: 16.3730 - val_mae: 1.5355 - lr: 6.4106e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 14.1098 - mse: 14.1098 - mae: 1.5535 - val_loss: 16.6724 - val_mse: 16.6724 - val_mae: 1.5348 - lr: 6.4106e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 14.1288 - mse: 14.1288 - mae: 1.5546 - val_loss: 16.4324 - val_mse: 16.4324 - val_mae: 1.6097 - lr: 6.4106e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 14.1900 - mse: 14.1900 - mae: 1.5531 - val_loss: 16.5973 - val_mse: 16.5973 - val_mae: 1.5863 - lr: 6.4106e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 14.0953 - mse: 14.0953 - mae: 1.5505 - val_loss: 16.9463 - val_mse: 16.9463 - val_mae: 1.5491 - lr: 6.4106e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 14.0858 - mse: 14.0858 - mae: 1.5525 - val_loss: 16.7442 - val_mse: 16.7442 - val_mae: 1.6004 - lr: 6.4106e-04 - 2s/epoch - 2ms/step\n",
            "Score for fold 3: loss of 16.744186401367188\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 15.6228 - mse: 15.6228 - mae: 1.5625 - val_loss: 11.0456 - val_mse: 11.0456 - val_mae: 1.5464 - lr: 6.4106e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 15.5446 - mse: 15.5446 - mae: 1.5677 - val_loss: 10.7941 - val_mse: 10.7941 - val_mae: 1.5416 - lr: 6.4106e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 15.4954 - mse: 15.4954 - mae: 1.5640 - val_loss: 10.8904 - val_mse: 10.8904 - val_mae: 1.4769 - lr: 6.4106e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 15.4967 - mse: 15.4967 - mae: 1.5641 - val_loss: 10.9540 - val_mse: 10.9540 - val_mae: 1.5139 - lr: 6.4106e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 15.5052 - mse: 15.5052 - mae: 1.5622 - val_loss: 11.0243 - val_mse: 11.0243 - val_mae: 1.5471 - lr: 6.4106e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 15.4302 - mse: 15.4302 - mae: 1.5614 - val_loss: 11.0073 - val_mse: 11.0073 - val_mae: 1.6043 - lr: 6.4106e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 15.4293 - mse: 15.4293 - mae: 1.5566 - val_loss: 11.0215 - val_mse: 11.0215 - val_mae: 1.6062 - lr: 6.4106e-04 - 2s/epoch - 2ms/step\n",
            "Score for fold 4: loss of 11.021512985229492\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 14.2358 - mse: 14.2358 - mae: 1.5435 - val_loss: 15.9923 - val_mse: 15.9923 - val_mae: 1.6054 - lr: 6.4106e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 14.1831 - mse: 14.1831 - mae: 1.5362 - val_loss: 15.6358 - val_mse: 15.6358 - val_mae: 1.6014 - lr: 6.4106e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 14.1203 - mse: 14.1203 - mae: 1.5351 - val_loss: 16.3386 - val_mse: 16.3386 - val_mae: 1.5541 - lr: 6.4106e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 14.1217 - mse: 14.1217 - mae: 1.5330 - val_loss: 16.2947 - val_mse: 16.2947 - val_mae: 1.5712 - lr: 6.4106e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 14.0687 - mse: 14.0687 - mae: 1.5319 - val_loss: 16.1116 - val_mse: 16.1116 - val_mae: 1.6374 - lr: 6.4106e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 14.0974 - mse: 14.0974 - mae: 1.5331 - val_loss: 16.0684 - val_mse: 16.0684 - val_mae: 1.5985 - lr: 6.4106e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 13.9971 - mse: 13.9971 - mae: 1.5310 - val_loss: 16.4011 - val_mse: 16.4011 - val_mae: 1.6219 - lr: 6.4106e-04 - 2s/epoch - 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-10-14 21:36:04,434]\u001b[0m Finished trial#33 resulted in value: 14.909999999999997. Current best value is 14.825999999999999 with parameters: {'activation': 'relu', 'num_hidden_layer': 3, 'i': 6, 'learning_rate': 0.000641854957781097}.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Score for fold 5: loss of 16.401086807250977\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 16.9755 - mse: 16.9755 - mae: 1.6647 - val_loss: 12.6304 - val_mse: 12.6304 - val_mae: 1.5778 - lr: 1.8536e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 16.0388 - mse: 16.0388 - mae: 1.6000 - val_loss: 12.3148 - val_mse: 12.3148 - val_mae: 1.6227 - lr: 1.8536e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 15.8675 - mse: 15.8675 - mae: 1.5931 - val_loss: 12.2696 - val_mse: 12.2696 - val_mae: 1.5656 - lr: 1.8536e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 15.7989 - mse: 15.7989 - mae: 1.5939 - val_loss: 12.1839 - val_mse: 12.1839 - val_mae: 1.6155 - lr: 1.8536e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 15.7809 - mse: 15.7809 - mae: 1.5908 - val_loss: 12.1819 - val_mse: 12.1819 - val_mae: 1.6002 - lr: 1.8536e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 15.7654 - mse: 15.7654 - mae: 1.5913 - val_loss: 12.1586 - val_mse: 12.1586 - val_mae: 1.5751 - lr: 1.8536e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 15.7548 - mse: 15.7548 - mae: 1.5899 - val_loss: 12.1105 - val_mse: 12.1105 - val_mae: 1.6372 - lr: 1.8536e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 2s - loss: 15.6629 - mse: 15.6629 - mae: 1.5861 - val_loss: 12.1371 - val_mse: 12.1371 - val_mae: 1.6745 - lr: 1.8536e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 2s - loss: 15.6650 - mse: 15.6650 - mae: 1.5883 - val_loss: 12.1285 - val_mse: 12.1285 - val_mae: 1.5616 - lr: 1.8536e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 2s - loss: 15.6721 - mse: 15.6721 - mae: 1.5859 - val_loss: 12.1095 - val_mse: 12.1095 - val_mae: 1.5559 - lr: 1.8536e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 2s - loss: 15.6642 - mse: 15.6642 - mae: 1.5836 - val_loss: 12.0811 - val_mse: 12.0811 - val_mae: 1.6331 - lr: 1.8536e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 2s - loss: 15.6405 - mse: 15.6405 - mae: 1.5864 - val_loss: 12.1189 - val_mse: 12.1189 - val_mae: 1.5340 - lr: 1.8536e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 13/100\n",
            "1000/1000 - 2s - loss: 15.6078 - mse: 15.6078 - mae: 1.5820 - val_loss: 12.0621 - val_mse: 12.0621 - val_mae: 1.5869 - lr: 1.8536e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 14/100\n",
            "1000/1000 - 2s - loss: 15.6399 - mse: 15.6399 - mae: 1.5789 - val_loss: 12.1167 - val_mse: 12.1167 - val_mae: 1.5520 - lr: 1.8536e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 15/100\n",
            "1000/1000 - 2s - loss: 15.6155 - mse: 15.6155 - mae: 1.5838 - val_loss: 12.1233 - val_mse: 12.1233 - val_mae: 1.5406 - lr: 1.8536e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 16/100\n",
            "1000/1000 - 2s - loss: 15.5657 - mse: 15.5657 - mae: 1.5819 - val_loss: 12.1718 - val_mse: 12.1718 - val_mae: 1.5281 - lr: 1.8536e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 17/100\n",
            "1000/1000 - 2s - loss: 15.5940 - mse: 15.5940 - mae: 1.5813 - val_loss: 12.0492 - val_mse: 12.0492 - val_mae: 1.6224 - lr: 1.8536e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 18/100\n",
            "1000/1000 - 2s - loss: 15.5427 - mse: 15.5427 - mae: 1.5795 - val_loss: 12.0437 - val_mse: 12.0437 - val_mae: 1.5662 - lr: 1.8536e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 19/100\n",
            "1000/1000 - 2s - loss: 15.5977 - mse: 15.5977 - mae: 1.5800 - val_loss: 12.0545 - val_mse: 12.0545 - val_mae: 1.5622 - lr: 1.8536e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 20/100\n",
            "1000/1000 - 2s - loss: 15.5254 - mse: 15.5254 - mae: 1.5793 - val_loss: 12.0351 - val_mse: 12.0351 - val_mae: 1.5510 - lr: 1.8536e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 21/100\n",
            "1000/1000 - 2s - loss: 15.5317 - mse: 15.5317 - mae: 1.5769 - val_loss: 12.0989 - val_mse: 12.0989 - val_mae: 1.5308 - lr: 1.8536e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 22/100\n",
            "1000/1000 - 2s - loss: 15.5330 - mse: 15.5330 - mae: 1.5751 - val_loss: 12.0389 - val_mse: 12.0389 - val_mae: 1.6405 - lr: 1.8536e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 23/100\n",
            "1000/1000 - 2s - loss: 15.4993 - mse: 15.4993 - mae: 1.5783 - val_loss: 12.0834 - val_mse: 12.0834 - val_mae: 1.5907 - lr: 1.8536e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 24/100\n",
            "1000/1000 - 2s - loss: 15.4983 - mse: 15.4983 - mae: 1.5782 - val_loss: 12.0992 - val_mse: 12.0992 - val_mae: 1.5138 - lr: 1.8536e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 25/100\n",
            "1000/1000 - 2s - loss: 15.4944 - mse: 15.4944 - mae: 1.5767 - val_loss: 12.0606 - val_mse: 12.0606 - val_mae: 1.5438 - lr: 1.8536e-04 - 2s/epoch - 2ms/step\n",
            "Score for fold 1: loss of 12.06054973602295\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 14.8432 - mse: 14.8432 - mae: 1.5636 - val_loss: 14.6905 - val_mse: 14.6905 - val_mae: 1.6104 - lr: 1.8536e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 14.7920 - mse: 14.7920 - mae: 1.5681 - val_loss: 14.8447 - val_mse: 14.8447 - val_mae: 1.6087 - lr: 1.8536e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 14.7850 - mse: 14.7850 - mae: 1.5620 - val_loss: 14.8533 - val_mse: 14.8533 - val_mae: 1.5866 - lr: 1.8536e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 14.8114 - mse: 14.8114 - mae: 1.5656 - val_loss: 14.8314 - val_mse: 14.8314 - val_mae: 1.6234 - lr: 1.8536e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 14.7643 - mse: 14.7643 - mae: 1.5627 - val_loss: 15.0595 - val_mse: 15.0595 - val_mae: 1.5604 - lr: 1.8536e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 14.7614 - mse: 14.7614 - mae: 1.5599 - val_loss: 14.8247 - val_mse: 14.8247 - val_mae: 1.6442 - lr: 1.8536e-04 - 2s/epoch - 2ms/step\n",
            "Score for fold 2: loss of 14.824708938598633\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 13.7015 - mse: 13.7015 - mae: 1.5740 - val_loss: 19.0412 - val_mse: 19.0412 - val_mae: 1.5903 - lr: 1.8536e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 13.7038 - mse: 13.7038 - mae: 1.5713 - val_loss: 19.0628 - val_mse: 19.0628 - val_mae: 1.5395 - lr: 1.8536e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 13.7004 - mse: 13.7004 - mae: 1.5710 - val_loss: 19.0162 - val_mse: 19.0162 - val_mae: 1.6469 - lr: 1.8536e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 13.6286 - mse: 13.6286 - mae: 1.5710 - val_loss: 19.2010 - val_mse: 19.2010 - val_mae: 1.5182 - lr: 1.8536e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 13.6539 - mse: 13.6539 - mae: 1.5730 - val_loss: 19.0816 - val_mse: 19.0816 - val_mae: 1.5681 - lr: 1.8536e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 13.6564 - mse: 13.6564 - mae: 1.5695 - val_loss: 19.0965 - val_mse: 19.0965 - val_mae: 1.5208 - lr: 1.8536e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 13.6250 - mse: 13.6250 - mae: 1.5658 - val_loss: 19.1583 - val_mse: 19.1583 - val_mae: 1.5637 - lr: 1.8536e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 2s - loss: 13.6333 - mse: 13.6333 - mae: 1.5677 - val_loss: 19.1748 - val_mse: 19.1748 - val_mae: 1.5622 - lr: 1.8536e-04 - 2s/epoch - 2ms/step\n",
            "Score for fold 3: loss of 19.174842834472656\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 14.1540 - mse: 14.1540 - mae: 1.5647 - val_loss: 16.7511 - val_mse: 16.7511 - val_mae: 1.5753 - lr: 1.8536e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 14.1312 - mse: 14.1312 - mae: 1.5601 - val_loss: 17.1213 - val_mse: 17.1213 - val_mae: 1.5699 - lr: 1.8536e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 14.1114 - mse: 14.1114 - mae: 1.5563 - val_loss: 16.8884 - val_mse: 16.8884 - val_mae: 1.6345 - lr: 1.8536e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 14.1197 - mse: 14.1197 - mae: 1.5637 - val_loss: 17.2691 - val_mse: 17.2691 - val_mae: 1.5560 - lr: 1.8536e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 14.0996 - mse: 14.0996 - mae: 1.5585 - val_loss: 17.2206 - val_mse: 17.2206 - val_mae: 1.5715 - lr: 1.8536e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 14.0590 - mse: 14.0590 - mae: 1.5587 - val_loss: 17.4048 - val_mse: 17.4048 - val_mae: 1.5888 - lr: 1.8536e-04 - 2s/epoch - 2ms/step\n",
            "Score for fold 4: loss of 17.404817581176758\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 15.6430 - mse: 15.6430 - mae: 1.5712 - val_loss: 11.0566 - val_mse: 11.0566 - val_mae: 1.5228 - lr: 1.8536e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 15.7024 - mse: 15.7024 - mae: 1.5734 - val_loss: 10.9184 - val_mse: 10.9184 - val_mae: 1.5272 - lr: 1.8536e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 15.6297 - mse: 15.6297 - mae: 1.5718 - val_loss: 10.9738 - val_mse: 10.9738 - val_mae: 1.5919 - lr: 1.8536e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 15.5993 - mse: 15.5993 - mae: 1.5674 - val_loss: 11.2794 - val_mse: 11.2794 - val_mae: 1.5142 - lr: 1.8536e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 15.5390 - mse: 15.5390 - mae: 1.5664 - val_loss: 11.2366 - val_mse: 11.2366 - val_mae: 1.5154 - lr: 1.8536e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 15.5855 - mse: 15.5855 - mae: 1.5686 - val_loss: 11.1177 - val_mse: 11.1177 - val_mae: 1.5402 - lr: 1.8536e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 15.5686 - mse: 15.5686 - mae: 1.5665 - val_loss: 11.0201 - val_mse: 11.0201 - val_mae: 1.5624 - lr: 1.8536e-04 - 2s/epoch - 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-10-14 21:37:37,096]\u001b[0m Finished trial#34 resulted in value: 14.894. Current best value is 14.825999999999999 with parameters: {'activation': 'relu', 'num_hidden_layer': 3, 'i': 6, 'learning_rate': 0.000641854957781097}.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Score for fold 5: loss of 11.02009391784668\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 16.5340 - mse: 16.5340 - mae: 1.6823 - val_loss: 13.7751 - val_mse: 13.7751 - val_mae: 1.7884 - lr: 0.0097 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 16.3104 - mse: 16.3104 - mae: 1.6605 - val_loss: 13.8064 - val_mse: 13.8064 - val_mae: 1.6049 - lr: 0.0097 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 16.3525 - mse: 16.3525 - mae: 1.6670 - val_loss: 13.7420 - val_mse: 13.7420 - val_mae: 1.5445 - lr: 0.0097 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 16.2935 - mse: 16.2935 - mae: 1.6657 - val_loss: 13.5022 - val_mse: 13.5022 - val_mae: 1.6522 - lr: 0.0097 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 16.2843 - mse: 16.2843 - mae: 1.6635 - val_loss: 14.0548 - val_mse: 14.0548 - val_mae: 1.7405 - lr: 0.0097 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 16.3655 - mse: 16.3655 - mae: 1.6675 - val_loss: 13.7890 - val_mse: 13.7890 - val_mae: 1.5876 - lr: 0.0097 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 16.3771 - mse: 16.3771 - mae: 1.6669 - val_loss: 14.8877 - val_mse: 14.8877 - val_mae: 1.5622 - lr: 0.0097 - 2s/epoch - 2ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 2s - loss: 16.3584 - mse: 16.3584 - mae: 1.6612 - val_loss: 13.5033 - val_mse: 13.5033 - val_mae: 1.6712 - lr: 0.0097 - 2s/epoch - 2ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 2s - loss: 16.3223 - mse: 16.3223 - mae: 1.6642 - val_loss: 13.5166 - val_mse: 13.5166 - val_mae: 1.6182 - lr: 0.0097 - 2s/epoch - 2ms/step\n",
            "Score for fold 1: loss of 13.516630172729492\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 16.1733 - mse: 16.1733 - mae: 1.6399 - val_loss: 13.7524 - val_mse: 13.7524 - val_mae: 1.6670 - lr: 0.0019 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 16.1198 - mse: 16.1198 - mae: 1.6404 - val_loss: 13.7580 - val_mse: 13.7580 - val_mae: 1.6434 - lr: 0.0019 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 16.1306 - mse: 16.1306 - mae: 1.6458 - val_loss: 13.7877 - val_mse: 13.7877 - val_mae: 1.6978 - lr: 0.0019 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 16.1196 - mse: 16.1196 - mae: 1.6448 - val_loss: 13.9103 - val_mse: 13.9103 - val_mae: 1.5974 - lr: 0.0019 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 16.1304 - mse: 16.1304 - mae: 1.6448 - val_loss: 13.8445 - val_mse: 13.8445 - val_mae: 1.5984 - lr: 0.0019 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 16.1670 - mse: 16.1670 - mae: 1.6387 - val_loss: 13.9589 - val_mse: 13.9589 - val_mae: 1.7773 - lr: 0.0019 - 2s/epoch - 2ms/step\n",
            "Score for fold 2: loss of 13.95893383026123\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 16.3017 - mse: 16.3017 - mae: 1.6353 - val_loss: 13.0367 - val_mse: 13.0367 - val_mae: 1.6419 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 16.3712 - mse: 16.3712 - mae: 1.6327 - val_loss: 13.0050 - val_mse: 13.0050 - val_mae: 1.6825 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 16.3347 - mse: 16.3347 - mae: 1.6318 - val_loss: 13.1397 - val_mse: 13.1397 - val_mae: 1.7356 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 16.3699 - mse: 16.3699 - mae: 1.6335 - val_loss: 13.1811 - val_mse: 13.1811 - val_mae: 1.6097 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 16.4290 - mse: 16.4290 - mae: 1.6298 - val_loss: 13.1254 - val_mse: 13.1254 - val_mae: 1.6265 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 16.3746 - mse: 16.3746 - mae: 1.6382 - val_loss: 13.0788 - val_mse: 13.0788 - val_mae: 1.7110 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 16.4244 - mse: 16.4244 - mae: 1.6388 - val_loss: 13.1989 - val_mse: 13.1989 - val_mae: 1.6099 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Score for fold 3: loss of 13.19887638092041\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 14.2760 - mse: 14.2760 - mae: 1.6388 - val_loss: 22.1833 - val_mse: 22.1833 - val_mae: 1.6058 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 14.1958 - mse: 14.1958 - mae: 1.6442 - val_loss: 22.7637 - val_mse: 22.7637 - val_mae: 1.9843 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 14.3080 - mse: 14.3080 - mae: 1.6434 - val_loss: 21.9766 - val_mse: 21.9766 - val_mae: 1.6183 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 14.2808 - mse: 14.2808 - mae: 1.6442 - val_loss: 21.5587 - val_mse: 21.5587 - val_mae: 1.7413 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 14.2127 - mse: 14.2127 - mae: 1.6436 - val_loss: 21.5804 - val_mse: 21.5804 - val_mae: 1.6666 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 14.3369 - mse: 14.3369 - mae: 1.6447 - val_loss: 21.5435 - val_mse: 21.5435 - val_mae: 1.6974 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 14.1807 - mse: 14.1807 - mae: 1.6439 - val_loss: 21.6186 - val_mse: 21.6186 - val_mae: 1.6583 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 2s - loss: 14.2703 - mse: 14.2703 - mae: 1.6440 - val_loss: 21.5670 - val_mse: 21.5670 - val_mae: 1.6851 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 2s - loss: 14.2784 - mse: 14.2784 - mae: 1.6418 - val_loss: 21.6195 - val_mse: 21.6195 - val_mae: 1.6791 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 2s - loss: 14.2884 - mse: 14.2884 - mae: 1.6436 - val_loss: 21.5596 - val_mse: 21.5596 - val_mae: 1.6850 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 2s - loss: 14.2445 - mse: 14.2445 - mae: 1.6433 - val_loss: 21.5203 - val_mse: 21.5203 - val_mae: 1.6882 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 2s - loss: 14.2029 - mse: 14.2029 - mae: 1.6423 - val_loss: 21.6077 - val_mse: 21.6077 - val_mae: 1.6534 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 13/100\n",
            "1000/1000 - 2s - loss: 14.1619 - mse: 14.1619 - mae: 1.6404 - val_loss: 22.4949 - val_mse: 22.4949 - val_mae: 1.5940 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 14/100\n",
            "1000/1000 - 2s - loss: 14.3122 - mse: 14.3122 - mae: 1.6456 - val_loss: 21.5425 - val_mse: 21.5425 - val_mae: 1.7270 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 15/100\n",
            "1000/1000 - 2s - loss: 14.2684 - mse: 14.2684 - mae: 1.6404 - val_loss: 21.5241 - val_mse: 21.5241 - val_mae: 1.7206 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 16/100\n",
            "1000/1000 - 2s - loss: 14.2655 - mse: 14.2655 - mae: 1.6426 - val_loss: 22.0067 - val_mse: 22.0067 - val_mae: 1.6183 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Score for fold 4: loss of 22.00668716430664\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 15.6441 - mse: 15.6441 - mae: 1.6522 - val_loss: 16.0604 - val_mse: 16.0604 - val_mae: 1.6690 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 15.5717 - mse: 15.5717 - mae: 1.6514 - val_loss: 16.5357 - val_mse: 16.5357 - val_mae: 1.5701 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 15.6424 - mse: 15.6424 - mae: 1.6562 - val_loss: 16.0499 - val_mse: 16.0499 - val_mae: 1.6543 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 15.6630 - mse: 15.6630 - mae: 1.6541 - val_loss: 16.0628 - val_mse: 16.0628 - val_mae: 1.6689 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 15.6765 - mse: 15.6765 - mae: 1.6515 - val_loss: 17.0206 - val_mse: 17.0206 - val_mae: 1.5592 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 15.5438 - mse: 15.5438 - mae: 1.6551 - val_loss: 16.3807 - val_mse: 16.3807 - val_mae: 1.5862 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 15.6389 - mse: 15.6389 - mae: 1.6579 - val_loss: 16.0782 - val_mse: 16.0782 - val_mae: 1.6174 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 2s - loss: 15.6448 - mse: 15.6448 - mae: 1.6491 - val_loss: 16.5617 - val_mse: 16.5617 - val_mae: 1.5735 - lr: 0.0010 - 2s/epoch - 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-10-14 21:38:54,987]\u001b[0m Finished trial#35 resulted in value: 15.85. Current best value is 14.825999999999999 with parameters: {'activation': 'relu', 'num_hidden_layer': 3, 'i': 6, 'learning_rate': 0.000641854957781097}.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Score for fold 5: loss of 16.561683654785156\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 3s - loss: 16.8519 - mse: 16.8519 - mae: 1.6768 - val_loss: 12.2484 - val_mse: 12.2484 - val_mae: 1.5734 - lr: 3.5811e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 16.2315 - mse: 16.2315 - mae: 1.6249 - val_loss: 12.0477 - val_mse: 12.0477 - val_mae: 1.5719 - lr: 3.5811e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 16.1239 - mse: 16.1239 - mae: 1.6134 - val_loss: 12.0649 - val_mse: 12.0649 - val_mae: 1.5642 - lr: 3.5811e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 16.0482 - mse: 16.0482 - mae: 1.6089 - val_loss: 12.0255 - val_mse: 12.0255 - val_mae: 1.6293 - lr: 3.5811e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 15.9862 - mse: 15.9862 - mae: 1.6097 - val_loss: 12.1809 - val_mse: 12.1809 - val_mae: 1.5069 - lr: 3.5811e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 15.9445 - mse: 15.9445 - mae: 1.6001 - val_loss: 11.8775 - val_mse: 11.8775 - val_mae: 1.6019 - lr: 3.5811e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 15.9042 - mse: 15.9042 - mae: 1.5977 - val_loss: 11.8375 - val_mse: 11.8375 - val_mae: 1.5840 - lr: 3.5811e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 2s - loss: 15.8455 - mse: 15.8455 - mae: 1.5979 - val_loss: 11.9190 - val_mse: 11.9190 - val_mae: 1.5884 - lr: 3.5811e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 2s - loss: 15.8652 - mse: 15.8652 - mae: 1.5951 - val_loss: 11.8600 - val_mse: 11.8600 - val_mae: 1.5729 - lr: 3.5811e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 2s - loss: 15.8332 - mse: 15.8332 - mae: 1.5944 - val_loss: 11.9619 - val_mse: 11.9619 - val_mae: 1.5223 - lr: 3.5811e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 2s - loss: 15.8187 - mse: 15.8187 - mae: 1.5968 - val_loss: 11.8447 - val_mse: 11.8447 - val_mae: 1.5654 - lr: 3.5811e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 2s - loss: 15.8061 - mse: 15.8061 - mae: 1.5939 - val_loss: 11.8779 - val_mse: 11.8779 - val_mae: 1.5318 - lr: 3.5811e-04 - 2s/epoch - 2ms/step\n",
            "Score for fold 1: loss of 11.8778657913208\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 15.2880 - mse: 15.2880 - mae: 1.5816 - val_loss: 13.8907 - val_mse: 13.8907 - val_mae: 1.6082 - lr: 3.5811e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 15.2748 - mse: 15.2748 - mae: 1.5750 - val_loss: 13.9399 - val_mse: 13.9399 - val_mae: 1.6411 - lr: 3.5811e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 15.2573 - mse: 15.2573 - mae: 1.5783 - val_loss: 13.9373 - val_mse: 13.9373 - val_mae: 1.5946 - lr: 3.5811e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 15.2443 - mse: 15.2443 - mae: 1.5740 - val_loss: 13.9870 - val_mse: 13.9870 - val_mae: 1.5774 - lr: 3.5811e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 15.2516 - mse: 15.2516 - mae: 1.5772 - val_loss: 13.9799 - val_mse: 13.9799 - val_mae: 1.5805 - lr: 3.5811e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 15.2202 - mse: 15.2202 - mae: 1.5764 - val_loss: 13.9562 - val_mse: 13.9562 - val_mae: 1.6226 - lr: 3.5811e-04 - 2s/epoch - 2ms/step\n",
            "Score for fold 2: loss of 13.956185340881348\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 15.6192 - mse: 15.6192 - mae: 1.5874 - val_loss: 12.3236 - val_mse: 12.3236 - val_mae: 1.6228 - lr: 3.5811e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 15.6261 - mse: 15.6261 - mae: 1.5850 - val_loss: 12.2270 - val_mse: 12.2270 - val_mae: 1.5902 - lr: 3.5811e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 15.5660 - mse: 15.5660 - mae: 1.5848 - val_loss: 12.4802 - val_mse: 12.4802 - val_mae: 1.5547 - lr: 3.5811e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 15.6028 - mse: 15.6028 - mae: 1.5857 - val_loss: 12.3061 - val_mse: 12.3061 - val_mae: 1.5604 - lr: 3.5811e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 15.5544 - mse: 15.5544 - mae: 1.5845 - val_loss: 12.4040 - val_mse: 12.4040 - val_mae: 1.5647 - lr: 3.5811e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 15.5557 - mse: 15.5557 - mae: 1.5839 - val_loss: 12.2904 - val_mse: 12.2904 - val_mae: 1.5873 - lr: 3.5811e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 15.5187 - mse: 15.5187 - mae: 1.5841 - val_loss: 12.5630 - val_mse: 12.5630 - val_mae: 1.5435 - lr: 3.5811e-04 - 2s/epoch - 2ms/step\n",
            "Score for fold 3: loss of 12.56302261352539\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 14.3538 - mse: 14.3538 - mae: 1.5833 - val_loss: 17.2327 - val_mse: 17.2327 - val_mae: 1.5406 - lr: 3.5811e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 14.2727 - mse: 14.2727 - mae: 1.5804 - val_loss: 17.4557 - val_mse: 17.4557 - val_mae: 1.5205 - lr: 3.5811e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 14.2143 - mse: 14.2143 - mae: 1.5786 - val_loss: 17.4392 - val_mse: 17.4392 - val_mae: 1.6129 - lr: 3.5811e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 14.2272 - mse: 14.2272 - mae: 1.5791 - val_loss: 17.3772 - val_mse: 17.3772 - val_mae: 1.5076 - lr: 3.5811e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 14.2298 - mse: 14.2298 - mae: 1.5772 - val_loss: 17.2829 - val_mse: 17.2829 - val_mae: 1.5809 - lr: 3.5811e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 14.1976 - mse: 14.1976 - mae: 1.5751 - val_loss: 17.3035 - val_mse: 17.3035 - val_mae: 1.5388 - lr: 3.5811e-04 - 2s/epoch - 2ms/step\n",
            "Score for fold 4: loss of 17.303470611572266\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 13.6789 - mse: 13.6789 - mae: 1.5665 - val_loss: 19.1647 - val_mse: 19.1647 - val_mae: 1.6044 - lr: 3.5811e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 13.6539 - mse: 13.6539 - mae: 1.5695 - val_loss: 19.3294 - val_mse: 19.3294 - val_mae: 1.6515 - lr: 3.5811e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 13.6321 - mse: 13.6321 - mae: 1.5637 - val_loss: 19.2275 - val_mse: 19.2275 - val_mae: 1.6107 - lr: 3.5811e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 13.6284 - mse: 13.6284 - mae: 1.5594 - val_loss: 19.1078 - val_mse: 19.1078 - val_mae: 1.6155 - lr: 3.5811e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 13.5935 - mse: 13.5935 - mae: 1.5627 - val_loss: 19.3309 - val_mse: 19.3309 - val_mae: 1.6065 - lr: 3.5811e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 13.5701 - mse: 13.5701 - mae: 1.5656 - val_loss: 19.3039 - val_mse: 19.3039 - val_mae: 1.5735 - lr: 3.5811e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 13.6019 - mse: 13.6019 - mae: 1.5590 - val_loss: 19.4175 - val_mse: 19.4175 - val_mae: 1.5840 - lr: 3.5811e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 2s - loss: 13.5876 - mse: 13.5876 - mae: 1.5548 - val_loss: 19.3813 - val_mse: 19.3813 - val_mae: 1.6013 - lr: 3.5811e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 2s - loss: 13.5651 - mse: 13.5651 - mae: 1.5565 - val_loss: 19.5143 - val_mse: 19.5143 - val_mae: 1.6251 - lr: 3.5811e-04 - 2s/epoch - 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-10-14 21:40:22,686]\u001b[0m Finished trial#36 resulted in value: 15.042000000000002. Current best value is 14.825999999999999 with parameters: {'activation': 'relu', 'num_hidden_layer': 3, 'i': 6, 'learning_rate': 0.000641854957781097}.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Score for fold 5: loss of 19.514266967773438\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 14.5884 - mse: 14.5884 - mae: 1.6178 - val_loss: 19.0288 - val_mse: 19.0288 - val_mae: 1.6198 - lr: 0.0018 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 14.3108 - mse: 14.3108 - mae: 1.5942 - val_loss: 18.7899 - val_mse: 18.7899 - val_mae: 1.6404 - lr: 0.0018 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 14.1497 - mse: 14.1497 - mae: 1.5918 - val_loss: 19.1679 - val_mse: 19.1679 - val_mae: 1.5723 - lr: 0.0018 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 14.1444 - mse: 14.1444 - mae: 1.5879 - val_loss: 18.8753 - val_mse: 18.8753 - val_mae: 1.6071 - lr: 0.0018 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 14.0717 - mse: 14.0717 - mae: 1.5815 - val_loss: 18.8806 - val_mse: 18.8806 - val_mae: 1.5935 - lr: 0.0018 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 14.1066 - mse: 14.1066 - mae: 1.5820 - val_loss: 18.7492 - val_mse: 18.7492 - val_mae: 1.5785 - lr: 0.0018 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 14.0301 - mse: 14.0301 - mae: 1.5868 - val_loss: 18.6770 - val_mse: 18.6770 - val_mae: 1.6156 - lr: 0.0018 - 2s/epoch - 2ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 2s - loss: 14.0116 - mse: 14.0116 - mae: 1.5850 - val_loss: 18.7154 - val_mse: 18.7154 - val_mae: 1.5790 - lr: 0.0018 - 2s/epoch - 2ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 2s - loss: 14.0288 - mse: 14.0288 - mae: 1.5799 - val_loss: 18.7766 - val_mse: 18.7766 - val_mae: 1.6094 - lr: 0.0018 - 2s/epoch - 2ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 2s - loss: 13.9567 - mse: 13.9567 - mae: 1.5740 - val_loss: 18.9366 - val_mse: 18.9366 - val_mae: 1.5480 - lr: 0.0018 - 2s/epoch - 2ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 2s - loss: 13.9265 - mse: 13.9265 - mae: 1.5731 - val_loss: 18.6708 - val_mse: 18.6708 - val_mae: 1.6472 - lr: 0.0018 - 2s/epoch - 2ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 2s - loss: 13.8878 - mse: 13.8878 - mae: 1.5739 - val_loss: 18.6388 - val_mse: 18.6388 - val_mae: 1.6172 - lr: 0.0018 - 2s/epoch - 2ms/step\n",
            "Epoch 13/100\n",
            "1000/1000 - 2s - loss: 13.9523 - mse: 13.9523 - mae: 1.5707 - val_loss: 18.9131 - val_mse: 18.9131 - val_mae: 1.5788 - lr: 0.0018 - 2s/epoch - 2ms/step\n",
            "Epoch 14/100\n",
            "1000/1000 - 2s - loss: 13.9522 - mse: 13.9522 - mae: 1.5717 - val_loss: 18.6844 - val_mse: 18.6844 - val_mae: 1.5927 - lr: 0.0018 - 2s/epoch - 2ms/step\n",
            "Epoch 15/100\n",
            "1000/1000 - 2s - loss: 13.8819 - mse: 13.8819 - mae: 1.5673 - val_loss: 18.8736 - val_mse: 18.8736 - val_mae: 1.5701 - lr: 0.0018 - 2s/epoch - 2ms/step\n",
            "Epoch 16/100\n",
            "1000/1000 - 2s - loss: 13.8919 - mse: 13.8919 - mae: 1.5680 - val_loss: 18.8064 - val_mse: 18.8064 - val_mae: 1.5823 - lr: 0.0018 - 2s/epoch - 2ms/step\n",
            "Epoch 17/100\n",
            "1000/1000 - 2s - loss: 13.8483 - mse: 13.8483 - mae: 1.5695 - val_loss: 18.6803 - val_mse: 18.6803 - val_mae: 1.5947 - lr: 0.0018 - 2s/epoch - 2ms/step\n",
            "Score for fold 1: loss of 18.680356979370117\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 16.0185 - mse: 16.0185 - mae: 1.5730 - val_loss: 9.5068 - val_mse: 9.5068 - val_mae: 1.5337 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 15.9833 - mse: 15.9833 - mae: 1.5707 - val_loss: 9.4248 - val_mse: 9.4248 - val_mae: 1.5216 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 15.9537 - mse: 15.9537 - mae: 1.5692 - val_loss: 9.4611 - val_mse: 9.4611 - val_mae: 1.5575 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 15.9066 - mse: 15.9066 - mae: 1.5717 - val_loss: 9.4870 - val_mse: 9.4870 - val_mae: 1.5483 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 15.9101 - mse: 15.9101 - mae: 1.5708 - val_loss: 9.4041 - val_mse: 9.4041 - val_mae: 1.5263 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 15.8540 - mse: 15.8540 - mae: 1.5681 - val_loss: 9.5194 - val_mse: 9.5194 - val_mae: 1.5200 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 15.8048 - mse: 15.8048 - mae: 1.5638 - val_loss: 9.5029 - val_mse: 9.5029 - val_mae: 1.5266 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 2s - loss: 15.7992 - mse: 15.7992 - mae: 1.5676 - val_loss: 9.5185 - val_mse: 9.5185 - val_mae: 1.5242 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 2s - loss: 15.8727 - mse: 15.8727 - mae: 1.5697 - val_loss: 9.6588 - val_mse: 9.6588 - val_mae: 1.5109 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 2s - loss: 15.8331 - mse: 15.8331 - mae: 1.5670 - val_loss: 9.6689 - val_mse: 9.6689 - val_mae: 1.5271 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Score for fold 2: loss of 9.668869972229004\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 15.8271 - mse: 15.8271 - mae: 1.5786 - val_loss: 9.4663 - val_mse: 9.4663 - val_mae: 1.5160 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 15.8119 - mse: 15.8119 - mae: 1.5796 - val_loss: 9.5803 - val_mse: 9.5803 - val_mae: 1.5306 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 15.7772 - mse: 15.7772 - mae: 1.5776 - val_loss: 9.6652 - val_mse: 9.6652 - val_mae: 1.5281 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 15.7863 - mse: 15.7863 - mae: 1.5775 - val_loss: 9.7094 - val_mse: 9.7094 - val_mae: 1.5076 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 15.7818 - mse: 15.7818 - mae: 1.5775 - val_loss: 9.6822 - val_mse: 9.6822 - val_mae: 1.5534 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 15.8198 - mse: 15.8198 - mae: 1.5787 - val_loss: 9.6618 - val_mse: 9.6618 - val_mae: 1.5277 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Score for fold 3: loss of 9.661816596984863\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 12.3189 - mse: 12.3189 - mae: 1.5496 - val_loss: 23.6067 - val_mse: 23.6067 - val_mae: 1.5998 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 12.2836 - mse: 12.2836 - mae: 1.5465 - val_loss: 23.9134 - val_mse: 23.9134 - val_mae: 1.6327 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 12.2657 - mse: 12.2657 - mae: 1.5479 - val_loss: 23.9311 - val_mse: 23.9311 - val_mae: 1.6153 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 12.2728 - mse: 12.2728 - mae: 1.5450 - val_loss: 23.6288 - val_mse: 23.6288 - val_mae: 1.6237 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 12.2414 - mse: 12.2414 - mae: 1.5417 - val_loss: 23.4505 - val_mse: 23.4505 - val_mae: 1.6222 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 12.1573 - mse: 12.1573 - mae: 1.5424 - val_loss: 23.9843 - val_mse: 23.9843 - val_mae: 1.6159 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 12.2064 - mse: 12.2064 - mae: 1.5439 - val_loss: 23.7986 - val_mse: 23.7986 - val_mae: 1.6111 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 2s - loss: 12.1677 - mse: 12.1677 - mae: 1.5462 - val_loss: 23.8011 - val_mse: 23.8011 - val_mae: 1.6137 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 2s - loss: 12.1990 - mse: 12.1990 - mae: 1.5417 - val_loss: 23.5709 - val_mse: 23.5709 - val_mae: 1.6428 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 2s - loss: 12.1198 - mse: 12.1198 - mae: 1.5393 - val_loss: 24.0145 - val_mse: 24.0145 - val_mae: 1.6540 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Score for fold 4: loss of 24.014463424682617\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 15.1681 - mse: 15.1681 - mae: 1.5567 - val_loss: 11.8293 - val_mse: 11.8293 - val_mae: 1.5532 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 15.1793 - mse: 15.1793 - mae: 1.5590 - val_loss: 11.7566 - val_mse: 11.7566 - val_mae: 1.5794 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 15.1270 - mse: 15.1270 - mae: 1.5568 - val_loss: 11.7284 - val_mse: 11.7284 - val_mae: 1.6080 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 15.0711 - mse: 15.0711 - mae: 1.5602 - val_loss: 11.8716 - val_mse: 11.8716 - val_mae: 1.5368 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 15.0352 - mse: 15.0352 - mae: 1.5548 - val_loss: 11.7699 - val_mse: 11.7699 - val_mae: 1.5805 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 15.0815 - mse: 15.0815 - mae: 1.5515 - val_loss: 11.8507 - val_mse: 11.8507 - val_mae: 1.5564 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 15.0907 - mse: 15.0907 - mae: 1.5554 - val_loss: 11.8727 - val_mse: 11.8727 - val_mae: 1.5296 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 2s - loss: 15.0545 - mse: 15.0545 - mae: 1.5548 - val_loss: 11.8343 - val_mse: 11.8343 - val_mae: 1.5748 - lr: 0.0010 - 2s/epoch - 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-10-14 21:41:53,522]\u001b[0m Finished trial#37 resulted in value: 14.770000000000001. Current best value is 14.770000000000001 with parameters: {'activation': 'relu', 'num_hidden_layer': 3, 'i': 6, 'learning_rate': 0.0018384095418089272}.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Score for fold 5: loss of 11.83426570892334\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 3s - loss: 14.7972 - mse: 14.7972 - mae: 1.6856 - val_loss: 20.6036 - val_mse: 20.6036 - val_mae: 1.5794 - lr: 0.0017 - 3s/epoch - 3ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 14.5605 - mse: 14.5605 - mae: 1.6510 - val_loss: 20.4677 - val_mse: 20.4677 - val_mae: 1.5967 - lr: 0.0017 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 14.5230 - mse: 14.5230 - mae: 1.6459 - val_loss: 20.5371 - val_mse: 20.5371 - val_mae: 1.5845 - lr: 0.0017 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 14.5114 - mse: 14.5114 - mae: 1.6468 - val_loss: 20.5338 - val_mse: 20.5338 - val_mae: 1.6838 - lr: 0.0017 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 14.5296 - mse: 14.5296 - mae: 1.6555 - val_loss: 20.5794 - val_mse: 20.5794 - val_mae: 1.5279 - lr: 0.0017 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 14.5081 - mse: 14.5081 - mae: 1.6569 - val_loss: 20.8784 - val_mse: 20.8784 - val_mae: 1.5192 - lr: 0.0017 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 14.5416 - mse: 14.5416 - mae: 1.6551 - val_loss: 20.5179 - val_mse: 20.5179 - val_mae: 1.6089 - lr: 0.0017 - 2s/epoch - 2ms/step\n",
            "Score for fold 1: loss of 20.517864227294922\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 15.8154 - mse: 15.8154 - mae: 1.6442 - val_loss: 15.1145 - val_mse: 15.1145 - val_mae: 1.5474 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 15.8268 - mse: 15.8268 - mae: 1.6464 - val_loss: 15.1327 - val_mse: 15.1327 - val_mae: 1.7123 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 15.8213 - mse: 15.8213 - mae: 1.6501 - val_loss: 15.0205 - val_mse: 15.0205 - val_mae: 1.6135 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 15.8154 - mse: 15.8154 - mae: 1.6514 - val_loss: 15.1173 - val_mse: 15.1173 - val_mae: 1.5510 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 15.8177 - mse: 15.8177 - mae: 1.6530 - val_loss: 15.0264 - val_mse: 15.0264 - val_mae: 1.6378 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 15.8389 - mse: 15.8389 - mae: 1.6569 - val_loss: 15.1153 - val_mse: 15.1153 - val_mae: 1.5544 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 15.8093 - mse: 15.8093 - mae: 1.6559 - val_loss: 15.0261 - val_mse: 15.0261 - val_mae: 1.5853 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 3s - loss: 15.7982 - mse: 15.7982 - mae: 1.6490 - val_loss: 15.4052 - val_mse: 15.4052 - val_mae: 1.8546 - lr: 0.0010 - 3s/epoch - 3ms/step\n",
            "Score for fold 2: loss of 15.405220031738281\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 16.4640 - mse: 16.4640 - mae: 1.6360 - val_loss: 12.5431 - val_mse: 12.5431 - val_mae: 1.7115 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 16.4903 - mse: 16.4903 - mae: 1.6387 - val_loss: 12.5795 - val_mse: 12.5795 - val_mae: 1.7168 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 16.5363 - mse: 16.5363 - mae: 1.6356 - val_loss: 12.5960 - val_mse: 12.5960 - val_mae: 1.7608 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 16.4533 - mse: 16.4533 - mae: 1.6413 - val_loss: 13.2657 - val_mse: 13.2657 - val_mae: 1.5057 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 16.5222 - mse: 16.5222 - mae: 1.6433 - val_loss: 12.6051 - val_mse: 12.6051 - val_mae: 1.7711 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 16.5098 - mse: 16.5098 - mae: 1.6440 - val_loss: 12.5387 - val_mse: 12.5387 - val_mae: 1.6985 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 16.4508 - mse: 16.4508 - mae: 1.6452 - val_loss: 13.3658 - val_mse: 13.3658 - val_mae: 1.5006 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 3s - loss: 16.5480 - mse: 16.5480 - mae: 1.6417 - val_loss: 12.5425 - val_mse: 12.5425 - val_mae: 1.6530 - lr: 0.0010 - 3s/epoch - 3ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 2s - loss: 16.5572 - mse: 16.5572 - mae: 1.6464 - val_loss: 12.5245 - val_mse: 12.5245 - val_mae: 1.6636 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 2s - loss: 16.5133 - mse: 16.5133 - mae: 1.6407 - val_loss: 12.5503 - val_mse: 12.5503 - val_mae: 1.6758 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 3s - loss: 16.5381 - mse: 16.5381 - mae: 1.6465 - val_loss: 12.5441 - val_mse: 12.5441 - val_mae: 1.6968 - lr: 0.0010 - 3s/epoch - 3ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 3s - loss: 16.5211 - mse: 16.5211 - mae: 1.6437 - val_loss: 12.8097 - val_mse: 12.8097 - val_mae: 1.5558 - lr: 0.0010 - 3s/epoch - 3ms/step\n",
            "Epoch 13/100\n",
            "1000/1000 - 2s - loss: 16.4521 - mse: 16.4521 - mae: 1.6378 - val_loss: 12.6855 - val_mse: 12.6855 - val_mae: 1.5832 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 14/100\n",
            "1000/1000 - 3s - loss: 16.5115 - mse: 16.5115 - mae: 1.6349 - val_loss: 12.5643 - val_mse: 12.5643 - val_mae: 1.6240 - lr: 0.0010 - 3s/epoch - 3ms/step\n",
            "Score for fold 3: loss of 12.564327239990234\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 16.2323 - mse: 16.2323 - mae: 1.6509 - val_loss: 14.0492 - val_mse: 14.0492 - val_mae: 1.8899 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 16.2822 - mse: 16.2822 - mae: 1.6535 - val_loss: 13.5953 - val_mse: 13.5953 - val_mae: 1.6109 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 16.3127 - mse: 16.3127 - mae: 1.6481 - val_loss: 14.3620 - val_mse: 14.3620 - val_mae: 1.9651 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 16.2970 - mse: 16.2970 - mae: 1.6530 - val_loss: 13.6218 - val_mse: 13.6218 - val_mae: 1.7177 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 16.2447 - mse: 16.2447 - mae: 1.6498 - val_loss: 13.5604 - val_mse: 13.5604 - val_mae: 1.6557 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 16.3240 - mse: 16.3240 - mae: 1.6536 - val_loss: 13.5717 - val_mse: 13.5717 - val_mae: 1.5884 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 16.3256 - mse: 16.3256 - mae: 1.6485 - val_loss: 13.8631 - val_mse: 13.8631 - val_mae: 1.8085 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 2s - loss: 16.2799 - mse: 16.2799 - mae: 1.6539 - val_loss: 13.6401 - val_mse: 13.6401 - val_mae: 1.5778 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 2s - loss: 16.2201 - mse: 16.2201 - mae: 1.6537 - val_loss: 13.7205 - val_mse: 13.7205 - val_mae: 1.5633 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 2s - loss: 16.2606 - mse: 16.2606 - mae: 1.6458 - val_loss: 13.9440 - val_mse: 13.9440 - val_mae: 1.5490 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Score for fold 4: loss of 13.94404411315918\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 15.5818 - mse: 15.5818 - mae: 1.6368 - val_loss: 16.6232 - val_mse: 16.6232 - val_mae: 1.6476 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 15.5982 - mse: 15.5982 - mae: 1.6357 - val_loss: 16.4075 - val_mse: 16.4075 - val_mae: 1.8423 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 15.5921 - mse: 15.5921 - mae: 1.6325 - val_loss: 16.5497 - val_mse: 16.5497 - val_mae: 1.6680 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 15.5680 - mse: 15.5680 - mae: 1.6353 - val_loss: 17.9155 - val_mse: 17.9155 - val_mae: 1.5969 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 15.5255 - mse: 15.5255 - mae: 1.6301 - val_loss: 16.6201 - val_mse: 16.6201 - val_mae: 1.6631 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 15.5502 - mse: 15.5502 - mae: 1.6340 - val_loss: 16.3863 - val_mse: 16.3863 - val_mae: 1.6764 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 15.4802 - mse: 15.4802 - mae: 1.6313 - val_loss: 16.3056 - val_mse: 16.3056 - val_mae: 1.7033 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 2s - loss: 15.5488 - mse: 15.5488 - mae: 1.6340 - val_loss: 16.4803 - val_mse: 16.4803 - val_mae: 1.6778 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 2s - loss: 15.5820 - mse: 15.5820 - mae: 1.6335 - val_loss: 16.2681 - val_mse: 16.2681 - val_mae: 1.7549 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 2s - loss: 15.5753 - mse: 15.5753 - mae: 1.6355 - val_loss: 16.3205 - val_mse: 16.3205 - val_mae: 1.7706 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 2s - loss: 15.5121 - mse: 15.5121 - mae: 1.6332 - val_loss: 16.4684 - val_mse: 16.4684 - val_mae: 1.6875 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 2s - loss: 15.5565 - mse: 15.5565 - mae: 1.6348 - val_loss: 16.4834 - val_mse: 16.4834 - val_mae: 1.6396 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 13/100\n",
            "1000/1000 - 2s - loss: 15.5429 - mse: 15.5429 - mae: 1.6366 - val_loss: 16.5121 - val_mse: 16.5121 - val_mae: 1.6381 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 14/100\n",
            "1000/1000 - 3s - loss: 15.5433 - mse: 15.5433 - mae: 1.6281 - val_loss: 16.6741 - val_mse: 16.6741 - val_mae: 1.6582 - lr: 0.0010 - 3s/epoch - 3ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-10-14 21:44:08,737]\u001b[0m Finished trial#38 resulted in value: 15.819999999999999. Current best value is 14.770000000000001 with parameters: {'activation': 'relu', 'num_hidden_layer': 3, 'i': 6, 'learning_rate': 0.0018384095418089272}.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Score for fold 5: loss of 16.674089431762695\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 3s - loss: 16.5383 - mse: 16.5383 - mae: 1.6578 - val_loss: 13.0236 - val_mse: 13.0236 - val_mae: 1.5637 - lr: 0.0011 - 3s/epoch - 3ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 16.1411 - mse: 16.1411 - mae: 1.6138 - val_loss: 12.4725 - val_mse: 12.4725 - val_mae: 1.6842 - lr: 0.0011 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 16.0305 - mse: 16.0305 - mae: 1.6099 - val_loss: 12.6832 - val_mse: 12.6832 - val_mae: 1.6298 - lr: 0.0011 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 15.9698 - mse: 15.9698 - mae: 1.6068 - val_loss: 12.5919 - val_mse: 12.5919 - val_mae: 1.6462 - lr: 0.0011 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 15.9063 - mse: 15.9063 - mae: 1.6057 - val_loss: 12.4727 - val_mse: 12.4727 - val_mae: 1.5993 - lr: 0.0011 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 15.8990 - mse: 15.8990 - mae: 1.5992 - val_loss: 12.3086 - val_mse: 12.3086 - val_mae: 1.6159 - lr: 0.0011 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 15.8076 - mse: 15.8076 - mae: 1.5958 - val_loss: 12.4734 - val_mse: 12.4734 - val_mae: 1.6425 - lr: 0.0011 - 2s/epoch - 2ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 2s - loss: 15.8512 - mse: 15.8512 - mae: 1.5922 - val_loss: 12.4306 - val_mse: 12.4306 - val_mae: 1.5963 - lr: 0.0011 - 2s/epoch - 2ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 2s - loss: 15.7810 - mse: 15.7810 - mae: 1.5967 - val_loss: 12.3760 - val_mse: 12.3760 - val_mae: 1.6391 - lr: 0.0011 - 2s/epoch - 2ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 2s - loss: 15.7657 - mse: 15.7657 - mae: 1.5918 - val_loss: 12.5316 - val_mse: 12.5316 - val_mae: 1.5546 - lr: 0.0011 - 2s/epoch - 2ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 2s - loss: 15.8186 - mse: 15.8186 - mae: 1.5963 - val_loss: 12.4653 - val_mse: 12.4653 - val_mae: 1.5545 - lr: 0.0011 - 2s/epoch - 2ms/step\n",
            "Score for fold 1: loss of 12.465330123901367\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 14.9648 - mse: 14.9648 - mae: 1.5996 - val_loss: 15.6299 - val_mse: 15.6299 - val_mae: 1.5573 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 14.9386 - mse: 14.9386 - mae: 1.5978 - val_loss: 15.5692 - val_mse: 15.5692 - val_mae: 1.5665 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 14.9519 - mse: 14.9519 - mae: 1.5992 - val_loss: 15.4934 - val_mse: 15.4934 - val_mae: 1.6947 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 14.9131 - mse: 14.9131 - mae: 1.5956 - val_loss: 15.5061 - val_mse: 15.5061 - val_mae: 1.6295 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 14.9134 - mse: 14.9134 - mae: 1.5898 - val_loss: 15.8971 - val_mse: 15.8971 - val_mae: 1.8502 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 14.8919 - mse: 14.8919 - mae: 1.5971 - val_loss: 15.5750 - val_mse: 15.5750 - val_mae: 1.5647 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 14.8787 - mse: 14.8787 - mae: 1.5906 - val_loss: 15.7241 - val_mse: 15.7241 - val_mae: 1.6403 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 2s - loss: 14.8148 - mse: 14.8148 - mae: 1.5911 - val_loss: 15.8569 - val_mse: 15.8569 - val_mae: 1.6784 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Score for fold 2: loss of 15.856877326965332\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 13.5709 - mse: 13.5709 - mae: 1.5940 - val_loss: 20.4751 - val_mse: 20.4751 - val_mae: 1.6556 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 13.5550 - mse: 13.5550 - mae: 1.5951 - val_loss: 20.6677 - val_mse: 20.6677 - val_mae: 1.6329 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 13.5398 - mse: 13.5398 - mae: 1.5962 - val_loss: 20.5605 - val_mse: 20.5605 - val_mae: 1.5204 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 13.4725 - mse: 13.4725 - mae: 1.5949 - val_loss: 20.7799 - val_mse: 20.7799 - val_mae: 1.5071 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 13.4883 - mse: 13.4883 - mae: 1.5956 - val_loss: 20.5232 - val_mse: 20.5232 - val_mae: 1.6114 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 13.5441 - mse: 13.5441 - mae: 1.5919 - val_loss: 20.5290 - val_mse: 20.5290 - val_mae: 1.5717 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Score for fold 3: loss of 20.528966903686523\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 15.8935 - mse: 15.8935 - mae: 1.5891 - val_loss: 11.1628 - val_mse: 11.1628 - val_mae: 1.6093 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 15.8915 - mse: 15.8915 - mae: 1.5838 - val_loss: 11.3078 - val_mse: 11.3078 - val_mae: 1.5737 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 15.8498 - mse: 15.8498 - mae: 1.5848 - val_loss: 11.3271 - val_mse: 11.3271 - val_mae: 1.5425 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 15.8899 - mse: 15.8899 - mae: 1.5826 - val_loss: 11.2942 - val_mse: 11.2942 - val_mae: 1.5484 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 15.8449 - mse: 15.8449 - mae: 1.5805 - val_loss: 11.2755 - val_mse: 11.2755 - val_mae: 1.5818 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 15.8330 - mse: 15.8330 - mae: 1.5846 - val_loss: 11.3993 - val_mse: 11.3993 - val_mae: 1.5575 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Score for fold 4: loss of 11.399333000183105\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 14.8179 - mse: 14.8179 - mae: 1.5926 - val_loss: 15.3209 - val_mse: 15.3209 - val_mae: 1.5494 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 14.8749 - mse: 14.8749 - mae: 1.5977 - val_loss: 15.4715 - val_mse: 15.4715 - val_mae: 1.5775 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 14.8982 - mse: 14.8982 - mae: 1.6043 - val_loss: 15.5129 - val_mse: 15.5129 - val_mae: 1.6091 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 14.8386 - mse: 14.8386 - mae: 1.6031 - val_loss: 15.4848 - val_mse: 15.4848 - val_mae: 1.5614 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 14.8721 - mse: 14.8721 - mae: 1.6010 - val_loss: 15.5376 - val_mse: 15.5376 - val_mae: 1.6053 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 14.8337 - mse: 14.8337 - mae: 1.5986 - val_loss: 15.4466 - val_mse: 15.4466 - val_mae: 1.5968 - lr: 0.0010 - 2s/epoch - 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-10-14 21:45:30,354]\u001b[0m Finished trial#39 resulted in value: 15.142. Current best value is 14.770000000000001 with parameters: {'activation': 'relu', 'num_hidden_layer': 3, 'i': 6, 'learning_rate': 0.0018384095418089272}.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Score for fold 5: loss of 15.446636199951172\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 16.0589 - mse: 16.0589 - mae: 1.6304 - val_loss: 14.4606 - val_mse: 14.4606 - val_mae: 1.5904 - lr: 0.0015 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 1s - loss: 15.3921 - mse: 15.3921 - mae: 1.6052 - val_loss: 14.3962 - val_mse: 14.3962 - val_mae: 1.6233 - lr: 0.0015 - 1s/epoch - 1ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 1s - loss: 15.3339 - mse: 15.3339 - mae: 1.6003 - val_loss: 14.3857 - val_mse: 14.3857 - val_mae: 1.5634 - lr: 0.0015 - 1s/epoch - 1ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 1s - loss: 15.3116 - mse: 15.3116 - mae: 1.5995 - val_loss: 14.3768 - val_mse: 14.3768 - val_mae: 1.5779 - lr: 0.0015 - 1s/epoch - 1ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 1s - loss: 15.2821 - mse: 15.2821 - mae: 1.5987 - val_loss: 14.3281 - val_mse: 14.3281 - val_mae: 1.5826 - lr: 0.0015 - 1s/epoch - 1ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 1s - loss: 15.2560 - mse: 15.2560 - mae: 1.5942 - val_loss: 14.3738 - val_mse: 14.3738 - val_mae: 1.5707 - lr: 0.0015 - 1s/epoch - 1ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 1s - loss: 15.2210 - mse: 15.2210 - mae: 1.5979 - val_loss: 14.4386 - val_mse: 14.4386 - val_mae: 1.5427 - lr: 0.0015 - 1s/epoch - 1ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 1s - loss: 15.2387 - mse: 15.2387 - mae: 1.5941 - val_loss: 14.3946 - val_mse: 14.3946 - val_mae: 1.5845 - lr: 0.0015 - 1s/epoch - 1ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 1s - loss: 15.1614 - mse: 15.1614 - mae: 1.5949 - val_loss: 14.3690 - val_mse: 14.3690 - val_mae: 1.5639 - lr: 0.0015 - 1s/epoch - 1ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 1s - loss: 15.1790 - mse: 15.1790 - mae: 1.5937 - val_loss: 14.3111 - val_mse: 14.3111 - val_mae: 1.6068 - lr: 0.0015 - 1s/epoch - 1ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 1s - loss: 15.1543 - mse: 15.1543 - mae: 1.5948 - val_loss: 14.3860 - val_mse: 14.3860 - val_mae: 1.5468 - lr: 0.0015 - 1s/epoch - 1ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 1s - loss: 15.1604 - mse: 15.1604 - mae: 1.5904 - val_loss: 14.3967 - val_mse: 14.3967 - val_mae: 1.5844 - lr: 0.0015 - 1s/epoch - 1ms/step\n",
            "Epoch 13/100\n",
            "1000/1000 - 1s - loss: 15.1509 - mse: 15.1509 - mae: 1.5905 - val_loss: 14.2770 - val_mse: 14.2770 - val_mae: 1.6048 - lr: 0.0015 - 1s/epoch - 1ms/step\n",
            "Epoch 14/100\n",
            "1000/1000 - 1s - loss: 15.1328 - mse: 15.1328 - mae: 1.5941 - val_loss: 14.3020 - val_mse: 14.3020 - val_mae: 1.5852 - lr: 0.0015 - 1s/epoch - 1ms/step\n",
            "Epoch 15/100\n",
            "1000/1000 - 1s - loss: 15.1358 - mse: 15.1358 - mae: 1.5916 - val_loss: 14.2894 - val_mse: 14.2894 - val_mae: 1.5645 - lr: 0.0015 - 1s/epoch - 1ms/step\n",
            "Epoch 16/100\n",
            "1000/1000 - 1s - loss: 15.1507 - mse: 15.1507 - mae: 1.5911 - val_loss: 14.3135 - val_mse: 14.3135 - val_mae: 1.5784 - lr: 0.0015 - 1s/epoch - 1ms/step\n",
            "Epoch 17/100\n",
            "1000/1000 - 1s - loss: 15.1541 - mse: 15.1541 - mae: 1.5919 - val_loss: 14.2635 - val_mse: 14.2635 - val_mae: 1.6039 - lr: 0.0015 - 1s/epoch - 1ms/step\n",
            "Epoch 18/100\n",
            "1000/1000 - 1s - loss: 15.1163 - mse: 15.1163 - mae: 1.5927 - val_loss: 14.2890 - val_mse: 14.2890 - val_mae: 1.5905 - lr: 0.0015 - 1s/epoch - 1ms/step\n",
            "Epoch 19/100\n",
            "1000/1000 - 1s - loss: 15.0878 - mse: 15.0878 - mae: 1.5916 - val_loss: 14.3260 - val_mse: 14.3260 - val_mae: 1.5857 - lr: 0.0015 - 1s/epoch - 1ms/step\n",
            "Epoch 20/100\n",
            "1000/1000 - 1s - loss: 15.0927 - mse: 15.0927 - mae: 1.5912 - val_loss: 14.3621 - val_mse: 14.3621 - val_mae: 1.5964 - lr: 0.0015 - 1s/epoch - 1ms/step\n",
            "Epoch 21/100\n",
            "1000/1000 - 1s - loss: 15.1125 - mse: 15.1125 - mae: 1.5888 - val_loss: 14.2475 - val_mse: 14.2475 - val_mae: 1.5990 - lr: 0.0015 - 1s/epoch - 1ms/step\n",
            "Epoch 22/100\n",
            "1000/1000 - 1s - loss: 15.1113 - mse: 15.1113 - mae: 1.5909 - val_loss: 14.3073 - val_mse: 14.3073 - val_mae: 1.5842 - lr: 0.0015 - 1s/epoch - 1ms/step\n",
            "Epoch 23/100\n",
            "1000/1000 - 1s - loss: 15.0912 - mse: 15.0912 - mae: 1.5891 - val_loss: 14.3032 - val_mse: 14.3032 - val_mae: 1.5873 - lr: 0.0015 - 1s/epoch - 1ms/step\n",
            "Epoch 24/100\n",
            "1000/1000 - 1s - loss: 15.0989 - mse: 15.0989 - mae: 1.5911 - val_loss: 14.3353 - val_mse: 14.3353 - val_mae: 1.5617 - lr: 0.0015 - 1s/epoch - 1ms/step\n",
            "Epoch 25/100\n",
            "1000/1000 - 1s - loss: 15.1361 - mse: 15.1361 - mae: 1.5920 - val_loss: 14.3506 - val_mse: 14.3506 - val_mae: 1.5394 - lr: 0.0015 - 1s/epoch - 1ms/step\n",
            "Epoch 26/100\n",
            "1000/1000 - 1s - loss: 15.0760 - mse: 15.0760 - mae: 1.5931 - val_loss: 14.2790 - val_mse: 14.2790 - val_mae: 1.6233 - lr: 0.0015 - 1s/epoch - 1ms/step\n",
            "Score for fold 1: loss of 14.278977394104004\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 1s - loss: 13.4118 - mse: 13.4118 - mae: 1.5787 - val_loss: 20.9686 - val_mse: 20.9686 - val_mae: 1.5863 - lr: 0.0010 - 1s/epoch - 1ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 1s - loss: 13.4106 - mse: 13.4106 - mae: 1.5768 - val_loss: 20.9247 - val_mse: 20.9247 - val_mae: 1.6217 - lr: 0.0010 - 1s/epoch - 1ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 1s - loss: 13.4017 - mse: 13.4017 - mae: 1.5765 - val_loss: 20.9952 - val_mse: 20.9952 - val_mae: 1.5920 - lr: 0.0010 - 1s/epoch - 1ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 1s - loss: 13.4285 - mse: 13.4285 - mae: 1.5758 - val_loss: 20.9703 - val_mse: 20.9703 - val_mae: 1.6177 - lr: 0.0010 - 1s/epoch - 1ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 1s - loss: 13.4072 - mse: 13.4072 - mae: 1.5751 - val_loss: 20.9628 - val_mse: 20.9628 - val_mae: 1.6091 - lr: 0.0010 - 1s/epoch - 1ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 1s - loss: 13.4176 - mse: 13.4176 - mae: 1.5719 - val_loss: 21.0330 - val_mse: 21.0330 - val_mae: 1.6230 - lr: 0.0010 - 1s/epoch - 1ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 1s - loss: 13.3869 - mse: 13.3869 - mae: 1.5736 - val_loss: 21.1330 - val_mse: 21.1330 - val_mae: 1.5693 - lr: 0.0010 - 1s/epoch - 1ms/step\n",
            "Score for fold 2: loss of 21.1330509185791\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 14.8160 - mse: 14.8160 - mae: 1.5827 - val_loss: 15.4462 - val_mse: 15.4462 - val_mae: 1.5623 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 1s - loss: 14.8109 - mse: 14.8109 - mae: 1.5812 - val_loss: 15.3600 - val_mse: 15.3600 - val_mae: 1.5799 - lr: 0.0010 - 1s/epoch - 1ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 1s - loss: 14.8057 - mse: 14.8057 - mae: 1.5840 - val_loss: 15.3941 - val_mse: 15.3941 - val_mae: 1.5833 - lr: 0.0010 - 1s/epoch - 1ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 1s - loss: 14.8137 - mse: 14.8137 - mae: 1.5822 - val_loss: 15.3866 - val_mse: 15.3866 - val_mae: 1.6072 - lr: 0.0010 - 1s/epoch - 1ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 1s - loss: 14.8056 - mse: 14.8056 - mae: 1.5857 - val_loss: 15.4907 - val_mse: 15.4907 - val_mae: 1.5722 - lr: 0.0010 - 1s/epoch - 1ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 1s - loss: 14.8008 - mse: 14.8008 - mae: 1.5864 - val_loss: 15.5091 - val_mse: 15.5091 - val_mae: 1.5977 - lr: 0.0010 - 1s/epoch - 1ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 1s - loss: 14.8034 - mse: 14.8034 - mae: 1.5851 - val_loss: 15.3790 - val_mse: 15.3790 - val_mae: 1.5805 - lr: 0.0010 - 1s/epoch - 1ms/step\n",
            "Score for fold 3: loss of 15.379027366638184\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 15.5999 - mse: 15.5999 - mae: 1.5793 - val_loss: 12.1786 - val_mse: 12.1786 - val_mae: 1.6197 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 1s - loss: 15.6141 - mse: 15.6141 - mae: 1.5814 - val_loss: 12.1622 - val_mse: 12.1622 - val_mae: 1.5887 - lr: 0.0010 - 1s/epoch - 1ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 1s - loss: 15.5772 - mse: 15.5772 - mae: 1.5839 - val_loss: 12.1620 - val_mse: 12.1620 - val_mae: 1.5723 - lr: 0.0010 - 1s/epoch - 1ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 1s - loss: 15.5801 - mse: 15.5801 - mae: 1.5773 - val_loss: 12.1857 - val_mse: 12.1857 - val_mae: 1.5640 - lr: 0.0010 - 1s/epoch - 1ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 1s - loss: 15.5876 - mse: 15.5876 - mae: 1.5770 - val_loss: 12.1706 - val_mse: 12.1706 - val_mae: 1.6110 - lr: 0.0010 - 1s/epoch - 1ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 1s - loss: 15.5910 - mse: 15.5910 - mae: 1.5798 - val_loss: 12.2323 - val_mse: 12.2323 - val_mae: 1.5449 - lr: 0.0010 - 1s/epoch - 1ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 1s - loss: 15.5734 - mse: 15.5734 - mae: 1.5798 - val_loss: 12.2031 - val_mse: 12.2031 - val_mae: 1.5537 - lr: 0.0010 - 1s/epoch - 1ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 1s - loss: 15.5842 - mse: 15.5842 - mae: 1.5782 - val_loss: 12.1837 - val_mse: 12.1837 - val_mae: 1.5678 - lr: 0.0010 - 1s/epoch - 1ms/step\n",
            "Score for fold 4: loss of 12.183660507202148\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 1s - loss: 15.6706 - mse: 15.6706 - mae: 1.5815 - val_loss: 11.8250 - val_mse: 11.8250 - val_mae: 1.5673 - lr: 0.0010 - 1s/epoch - 1ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 1s - loss: 15.6762 - mse: 15.6762 - mae: 1.5799 - val_loss: 11.7918 - val_mse: 11.7918 - val_mae: 1.5657 - lr: 0.0010 - 1s/epoch - 1ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 1s - loss: 15.6596 - mse: 15.6596 - mae: 1.5780 - val_loss: 11.8142 - val_mse: 11.8142 - val_mae: 1.6022 - lr: 0.0010 - 1s/epoch - 1ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 1s - loss: 15.6873 - mse: 15.6873 - mae: 1.5791 - val_loss: 11.8122 - val_mse: 11.8122 - val_mae: 1.6126 - lr: 0.0010 - 1s/epoch - 1ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 1s - loss: 15.6613 - mse: 15.6613 - mae: 1.5784 - val_loss: 11.8315 - val_mse: 11.8315 - val_mae: 1.5813 - lr: 0.0010 - 1s/epoch - 1ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 1s - loss: 15.6497 - mse: 15.6497 - mae: 1.5836 - val_loss: 11.8606 - val_mse: 11.8606 - val_mae: 1.5514 - lr: 0.0010 - 1s/epoch - 1ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 1s - loss: 15.6365 - mse: 15.6365 - mae: 1.5792 - val_loss: 11.9169 - val_mse: 11.9169 - val_mae: 1.5429 - lr: 0.0010 - 1s/epoch - 1ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-10-14 21:46:53,584]\u001b[0m Finished trial#40 resulted in value: 14.978. Current best value is 14.770000000000001 with parameters: {'activation': 'relu', 'num_hidden_layer': 3, 'i': 6, 'learning_rate': 0.0018384095418089272}.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Score for fold 5: loss of 11.91694450378418\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 16.2421 - mse: 16.2421 - mae: 1.6356 - val_loss: 12.3718 - val_mse: 12.3718 - val_mae: 1.5632 - lr: 0.0024 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 15.8580 - mse: 15.8580 - mae: 1.6120 - val_loss: 12.4960 - val_mse: 12.4960 - val_mae: 1.5112 - lr: 0.0024 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 15.7937 - mse: 15.7937 - mae: 1.6134 - val_loss: 12.2357 - val_mse: 12.2357 - val_mae: 1.5573 - lr: 0.0024 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 15.7718 - mse: 15.7718 - mae: 1.6039 - val_loss: 12.2747 - val_mse: 12.2747 - val_mae: 1.4988 - lr: 0.0024 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 15.6565 - mse: 15.6565 - mae: 1.6023 - val_loss: 12.1885 - val_mse: 12.1885 - val_mae: 1.5866 - lr: 0.0024 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 15.6908 - mse: 15.6908 - mae: 1.5969 - val_loss: 12.2012 - val_mse: 12.2012 - val_mae: 1.5706 - lr: 0.0024 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 15.6025 - mse: 15.6025 - mae: 1.6030 - val_loss: 12.0844 - val_mse: 12.0844 - val_mae: 1.5349 - lr: 0.0024 - 2s/epoch - 2ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 2s - loss: 15.5625 - mse: 15.5625 - mae: 1.5981 - val_loss: 12.2390 - val_mse: 12.2390 - val_mae: 1.5134 - lr: 0.0024 - 2s/epoch - 2ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 2s - loss: 15.6492 - mse: 15.6492 - mae: 1.5992 - val_loss: 12.0992 - val_mse: 12.0992 - val_mae: 1.5199 - lr: 0.0024 - 2s/epoch - 2ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 2s - loss: 15.6301 - mse: 15.6301 - mae: 1.5934 - val_loss: 12.0163 - val_mse: 12.0163 - val_mae: 1.5513 - lr: 0.0024 - 2s/epoch - 2ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 2s - loss: 15.6532 - mse: 15.6532 - mae: 1.5901 - val_loss: 12.0752 - val_mse: 12.0752 - val_mae: 1.5412 - lr: 0.0024 - 2s/epoch - 2ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 2s - loss: 15.5202 - mse: 15.5202 - mae: 1.5865 - val_loss: 12.3987 - val_mse: 12.3987 - val_mae: 1.5398 - lr: 0.0024 - 2s/epoch - 2ms/step\n",
            "Epoch 13/100\n",
            "1000/1000 - 2s - loss: 15.5312 - mse: 15.5312 - mae: 1.5880 - val_loss: 12.2311 - val_mse: 12.2311 - val_mae: 1.5240 - lr: 0.0024 - 2s/epoch - 2ms/step\n",
            "Epoch 14/100\n",
            "1000/1000 - 2s - loss: 15.5718 - mse: 15.5718 - mae: 1.5858 - val_loss: 12.2005 - val_mse: 12.2005 - val_mae: 1.5412 - lr: 0.0024 - 2s/epoch - 2ms/step\n",
            "Epoch 15/100\n",
            "1000/1000 - 2s - loss: 15.6178 - mse: 15.6178 - mae: 1.5915 - val_loss: 12.0925 - val_mse: 12.0925 - val_mae: 1.5182 - lr: 0.0024 - 2s/epoch - 2ms/step\n",
            "Score for fold 1: loss of 12.092485427856445\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 14.1287 - mse: 14.1287 - mae: 1.5524 - val_loss: 17.6468 - val_mse: 17.6468 - val_mae: 1.5998 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 14.0866 - mse: 14.0866 - mae: 1.5532 - val_loss: 17.4640 - val_mse: 17.4640 - val_mae: 1.6335 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 14.0983 - mse: 14.0983 - mae: 1.5515 - val_loss: 17.5151 - val_mse: 17.5151 - val_mae: 1.6057 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 14.1061 - mse: 14.1061 - mae: 1.5496 - val_loss: 17.4698 - val_mse: 17.4698 - val_mae: 1.6214 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 14.0431 - mse: 14.0431 - mae: 1.5504 - val_loss: 17.5542 - val_mse: 17.5542 - val_mae: 1.6077 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 14.0041 - mse: 14.0041 - mae: 1.5509 - val_loss: 17.4159 - val_mse: 17.4159 - val_mae: 1.6803 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 14.0154 - mse: 14.0154 - mae: 1.5510 - val_loss: 17.4843 - val_mse: 17.4843 - val_mae: 1.6318 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 2s - loss: 13.9974 - mse: 13.9974 - mae: 1.5477 - val_loss: 17.4749 - val_mse: 17.4749 - val_mae: 1.6177 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 2s - loss: 13.8835 - mse: 13.8835 - mae: 1.5471 - val_loss: 17.5358 - val_mse: 17.5358 - val_mae: 1.5948 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 2s - loss: 13.8945 - mse: 13.8945 - mae: 1.5455 - val_loss: 17.5772 - val_mse: 17.5772 - val_mae: 1.6251 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 2s - loss: 13.9224 - mse: 13.9224 - mae: 1.5459 - val_loss: 17.2000 - val_mse: 17.2000 - val_mae: 1.6375 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 2s - loss: 13.9960 - mse: 13.9960 - mae: 1.5461 - val_loss: 17.4088 - val_mse: 17.4088 - val_mae: 1.5906 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 13/100\n",
            "1000/1000 - 2s - loss: 13.8711 - mse: 13.8711 - mae: 1.5451 - val_loss: 17.2958 - val_mse: 17.2958 - val_mae: 1.6550 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 14/100\n",
            "1000/1000 - 2s - loss: 13.8546 - mse: 13.8546 - mae: 1.5468 - val_loss: 17.4413 - val_mse: 17.4413 - val_mae: 1.6538 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 15/100\n",
            "1000/1000 - 2s - loss: 13.8351 - mse: 13.8351 - mae: 1.5455 - val_loss: 17.5837 - val_mse: 17.5837 - val_mae: 1.6195 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 16/100\n",
            "1000/1000 - 2s - loss: 13.7678 - mse: 13.7678 - mae: 1.5462 - val_loss: 17.4698 - val_mse: 17.4698 - val_mae: 1.6201 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Score for fold 2: loss of 17.469757080078125\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 14.4019 - mse: 14.4019 - mae: 1.5617 - val_loss: 15.2238 - val_mse: 15.2238 - val_mae: 1.5895 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 14.4138 - mse: 14.4138 - mae: 1.5637 - val_loss: 15.7279 - val_mse: 15.7279 - val_mae: 1.5357 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 14.3712 - mse: 14.3712 - mae: 1.5665 - val_loss: 15.5364 - val_mse: 15.5364 - val_mae: 1.5781 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 14.3652 - mse: 14.3652 - mae: 1.5604 - val_loss: 15.6766 - val_mse: 15.6766 - val_mae: 1.5562 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 14.3442 - mse: 14.3442 - mae: 1.5666 - val_loss: 15.3796 - val_mse: 15.3796 - val_mae: 1.5666 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 14.3356 - mse: 14.3356 - mae: 1.5619 - val_loss: 15.5758 - val_mse: 15.5758 - val_mae: 1.5586 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Score for fold 3: loss of 15.5757417678833\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 15.7989 - mse: 15.7989 - mae: 1.5668 - val_loss: 9.8752 - val_mse: 9.8752 - val_mae: 1.5227 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 15.7044 - mse: 15.7044 - mae: 1.5658 - val_loss: 9.8314 - val_mse: 9.8314 - val_mae: 1.5222 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 15.6527 - mse: 15.6527 - mae: 1.5658 - val_loss: 9.8078 - val_mse: 9.8078 - val_mae: 1.6035 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 15.7528 - mse: 15.7528 - mae: 1.5635 - val_loss: 9.9432 - val_mse: 9.9432 - val_mae: 1.5750 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 15.6405 - mse: 15.6405 - mae: 1.5669 - val_loss: 9.8716 - val_mse: 9.8716 - val_mae: 1.5442 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 15.5655 - mse: 15.5655 - mae: 1.5670 - val_loss: 9.9140 - val_mse: 9.9140 - val_mae: 1.5676 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 15.6177 - mse: 15.6177 - mae: 1.5648 - val_loss: 9.8740 - val_mse: 9.8740 - val_mae: 1.5577 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 2s - loss: 15.5644 - mse: 15.5644 - mae: 1.5685 - val_loss: 9.9705 - val_mse: 9.9705 - val_mae: 1.5526 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Score for fold 4: loss of 9.970476150512695\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 13.4613 - mse: 13.4613 - mae: 1.5684 - val_loss: 18.4112 - val_mse: 18.4112 - val_mae: 1.5974 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 13.3758 - mse: 13.3758 - mae: 1.5645 - val_loss: 18.5167 - val_mse: 18.5167 - val_mae: 1.5388 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 13.4127 - mse: 13.4127 - mae: 1.5646 - val_loss: 18.2890 - val_mse: 18.2890 - val_mae: 1.5750 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 13.3190 - mse: 13.3190 - mae: 1.5607 - val_loss: 18.2115 - val_mse: 18.2115 - val_mae: 1.5906 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 13.3517 - mse: 13.3517 - mae: 1.5607 - val_loss: 18.5410 - val_mse: 18.5410 - val_mae: 1.5196 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 13.3266 - mse: 13.3266 - mae: 1.5567 - val_loss: 18.4821 - val_mse: 18.4821 - val_mae: 1.5936 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 13.2551 - mse: 13.2551 - mae: 1.5597 - val_loss: 18.5392 - val_mse: 18.5392 - val_mae: 1.5592 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 2s - loss: 13.3704 - mse: 13.3704 - mae: 1.5588 - val_loss: 18.5498 - val_mse: 18.5498 - val_mae: 1.6085 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 2s - loss: 13.3100 - mse: 13.3100 - mae: 1.5499 - val_loss: 18.5077 - val_mse: 18.5077 - val_mae: 1.5871 - lr: 0.0010 - 2s/epoch - 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-10-14 21:48:26,636]\u001b[0m Finished trial#41 resulted in value: 14.724. Current best value is 14.724 with parameters: {'activation': 'relu', 'num_hidden_layer': 3, 'i': 6, 'learning_rate': 0.0023989159773794054}.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Score for fold 5: loss of 18.507694244384766\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 16.0001 - mse: 16.0001 - mae: 1.6175 - val_loss: 13.2874 - val_mse: 13.2874 - val_mae: 1.5143 - lr: 0.0022 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 15.7618 - mse: 15.7618 - mae: 1.5928 - val_loss: 13.2716 - val_mse: 13.2716 - val_mae: 1.5306 - lr: 0.0022 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 15.6590 - mse: 15.6590 - mae: 1.5882 - val_loss: 12.9888 - val_mse: 12.9888 - val_mae: 1.6159 - lr: 0.0022 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 15.5760 - mse: 15.5760 - mae: 1.5822 - val_loss: 12.9505 - val_mse: 12.9505 - val_mae: 1.6254 - lr: 0.0022 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 15.5728 - mse: 15.5728 - mae: 1.5846 - val_loss: 12.9501 - val_mse: 12.9501 - val_mae: 1.5761 - lr: 0.0022 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 15.5244 - mse: 15.5244 - mae: 1.5819 - val_loss: 12.8890 - val_mse: 12.8890 - val_mae: 1.6754 - lr: 0.0022 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 15.5203 - mse: 15.5203 - mae: 1.5811 - val_loss: 13.0097 - val_mse: 13.0097 - val_mae: 1.5729 - lr: 0.0022 - 2s/epoch - 2ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 2s - loss: 15.4152 - mse: 15.4152 - mae: 1.5804 - val_loss: 12.9899 - val_mse: 12.9899 - val_mae: 1.5728 - lr: 0.0022 - 2s/epoch - 2ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 2s - loss: 15.5315 - mse: 15.5315 - mae: 1.5750 - val_loss: 13.0277 - val_mse: 13.0277 - val_mae: 1.5740 - lr: 0.0022 - 2s/epoch - 2ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 2s - loss: 15.5079 - mse: 15.5079 - mae: 1.5754 - val_loss: 13.0929 - val_mse: 13.0929 - val_mae: 1.6143 - lr: 0.0022 - 2s/epoch - 2ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 2s - loss: 15.3793 - mse: 15.3793 - mae: 1.5736 - val_loss: 12.8817 - val_mse: 12.8817 - val_mae: 1.6118 - lr: 0.0022 - 2s/epoch - 2ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 2s - loss: 15.4385 - mse: 15.4385 - mae: 1.5752 - val_loss: 12.8646 - val_mse: 12.8646 - val_mae: 1.6327 - lr: 0.0022 - 2s/epoch - 2ms/step\n",
            "Epoch 13/100\n",
            "1000/1000 - 2s - loss: 15.3865 - mse: 15.3865 - mae: 1.5675 - val_loss: 12.8216 - val_mse: 12.8216 - val_mae: 1.6296 - lr: 0.0022 - 2s/epoch - 2ms/step\n",
            "Epoch 14/100\n",
            "1000/1000 - 2s - loss: 15.4560 - mse: 15.4560 - mae: 1.5747 - val_loss: 12.9020 - val_mse: 12.9020 - val_mae: 1.6066 - lr: 0.0022 - 2s/epoch - 2ms/step\n",
            "Epoch 15/100\n",
            "1000/1000 - 2s - loss: 15.4131 - mse: 15.4131 - mae: 1.5664 - val_loss: 12.9861 - val_mse: 12.9861 - val_mae: 1.6109 - lr: 0.0022 - 2s/epoch - 2ms/step\n",
            "Epoch 16/100\n",
            "1000/1000 - 2s - loss: 15.3544 - mse: 15.3544 - mae: 1.5737 - val_loss: 13.0546 - val_mse: 13.0546 - val_mae: 1.6109 - lr: 0.0022 - 2s/epoch - 2ms/step\n",
            "Epoch 17/100\n",
            "1000/1000 - 2s - loss: 15.3871 - mse: 15.3871 - mae: 1.5727 - val_loss: 12.8970 - val_mse: 12.8970 - val_mae: 1.6817 - lr: 0.0022 - 2s/epoch - 2ms/step\n",
            "Epoch 18/100\n",
            "1000/1000 - 2s - loss: 15.3843 - mse: 15.3843 - mae: 1.5708 - val_loss: 13.0923 - val_mse: 13.0923 - val_mae: 1.6193 - lr: 0.0022 - 2s/epoch - 2ms/step\n",
            "Score for fold 1: loss of 13.092339515686035\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 14.1789 - mse: 14.1789 - mae: 1.5748 - val_loss: 16.9330 - val_mse: 16.9330 - val_mae: 1.5713 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 14.1491 - mse: 14.1491 - mae: 1.5690 - val_loss: 17.1042 - val_mse: 17.1042 - val_mae: 1.5630 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 14.1220 - mse: 14.1220 - mae: 1.5653 - val_loss: 17.1001 - val_mse: 17.1001 - val_mae: 1.5747 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 14.1609 - mse: 14.1609 - mae: 1.5691 - val_loss: 17.2733 - val_mse: 17.2733 - val_mae: 1.5706 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 14.2088 - mse: 14.2088 - mae: 1.5702 - val_loss: 17.4151 - val_mse: 17.4151 - val_mae: 1.5612 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 14.1816 - mse: 14.1816 - mae: 1.5722 - val_loss: 17.2938 - val_mse: 17.2938 - val_mae: 1.5755 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Score for fold 2: loss of 17.293800354003906\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 14.2319 - mse: 14.2319 - mae: 1.5812 - val_loss: 16.8333 - val_mse: 16.8333 - val_mae: 1.5128 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 14.2518 - mse: 14.2518 - mae: 1.5809 - val_loss: 17.0927 - val_mse: 17.0927 - val_mae: 1.5629 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 14.2197 - mse: 14.2197 - mae: 1.5799 - val_loss: 16.9642 - val_mse: 16.9642 - val_mae: 1.5226 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 14.2520 - mse: 14.2520 - mae: 1.5799 - val_loss: 17.1308 - val_mse: 17.1308 - val_mae: 1.5336 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 14.2526 - mse: 14.2526 - mae: 1.5831 - val_loss: 17.0103 - val_mse: 17.0103 - val_mae: 1.5266 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 14.2185 - mse: 14.2185 - mae: 1.5844 - val_loss: 17.0798 - val_mse: 17.0798 - val_mae: 1.5805 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Score for fold 3: loss of 17.07978630065918\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 14.8061 - mse: 14.8061 - mae: 1.5676 - val_loss: 14.7408 - val_mse: 14.7408 - val_mae: 1.6137 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 14.7056 - mse: 14.7056 - mae: 1.5643 - val_loss: 14.7896 - val_mse: 14.7896 - val_mae: 1.6011 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 14.7539 - mse: 14.7539 - mae: 1.5675 - val_loss: 14.8538 - val_mse: 14.8538 - val_mae: 1.6221 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 14.7052 - mse: 14.7052 - mae: 1.5614 - val_loss: 14.8832 - val_mse: 14.8832 - val_mae: 1.5968 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 14.7243 - mse: 14.7243 - mae: 1.5605 - val_loss: 14.8569 - val_mse: 14.8569 - val_mae: 1.6125 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 14.7137 - mse: 14.7137 - mae: 1.5627 - val_loss: 14.8452 - val_mse: 14.8452 - val_mae: 1.6650 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Score for fold 4: loss of 14.845227241516113\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 15.2866 - mse: 15.2866 - mae: 1.5799 - val_loss: 12.1194 - val_mse: 12.1194 - val_mae: 1.5471 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 15.2466 - mse: 15.2466 - mae: 1.5756 - val_loss: 12.3336 - val_mse: 12.3336 - val_mae: 1.5700 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 15.2140 - mse: 15.2140 - mae: 1.5728 - val_loss: 12.3022 - val_mse: 12.3022 - val_mae: 1.5671 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 15.2524 - mse: 15.2524 - mae: 1.5728 - val_loss: 12.2537 - val_mse: 12.2537 - val_mae: 1.6724 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 15.2597 - mse: 15.2597 - mae: 1.5735 - val_loss: 12.4051 - val_mse: 12.4051 - val_mae: 1.5691 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 15.1576 - mse: 15.1576 - mae: 1.5713 - val_loss: 12.4645 - val_mse: 12.4645 - val_mae: 1.5217 - lr: 0.0010 - 2s/epoch - 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-10-14 21:49:37,976]\u001b[0m Finished trial#42 resulted in value: 14.953999999999999. Current best value is 14.724 with parameters: {'activation': 'relu', 'num_hidden_layer': 3, 'i': 6, 'learning_rate': 0.0023989159773794054}.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Score for fold 5: loss of 12.464463233947754\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 16.8296 - mse: 16.8296 - mae: 1.6302 - val_loss: 9.9201 - val_mse: 9.9201 - val_mae: 1.7070 - lr: 0.0023 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 16.5563 - mse: 16.5563 - mae: 1.6051 - val_loss: 10.1040 - val_mse: 10.1040 - val_mae: 1.5365 - lr: 0.0023 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 16.5005 - mse: 16.5005 - mae: 1.6050 - val_loss: 9.7323 - val_mse: 9.7323 - val_mae: 1.5891 - lr: 0.0023 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 16.4013 - mse: 16.4013 - mae: 1.6022 - val_loss: 9.9203 - val_mse: 9.9203 - val_mae: 1.5252 - lr: 0.0023 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 16.3777 - mse: 16.3777 - mae: 1.5989 - val_loss: 9.8628 - val_mse: 9.8628 - val_mae: 1.5237 - lr: 0.0023 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 16.4194 - mse: 16.4194 - mae: 1.5977 - val_loss: 9.8322 - val_mse: 9.8322 - val_mae: 1.5653 - lr: 0.0023 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 16.3947 - mse: 16.3947 - mae: 1.5947 - val_loss: 9.8035 - val_mse: 9.8035 - val_mae: 1.5796 - lr: 0.0023 - 2s/epoch - 2ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 2s - loss: 16.3619 - mse: 16.3619 - mae: 1.5964 - val_loss: 9.7048 - val_mse: 9.7048 - val_mae: 1.5461 - lr: 0.0023 - 2s/epoch - 2ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 2s - loss: 16.3134 - mse: 16.3134 - mae: 1.5932 - val_loss: 9.5546 - val_mse: 9.5546 - val_mae: 1.5271 - lr: 0.0023 - 2s/epoch - 2ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 2s - loss: 16.3376 - mse: 16.3376 - mae: 1.5905 - val_loss: 9.6183 - val_mse: 9.6183 - val_mae: 1.5568 - lr: 0.0023 - 2s/epoch - 2ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 2s - loss: 16.2535 - mse: 16.2535 - mae: 1.5850 - val_loss: 9.7052 - val_mse: 9.7052 - val_mae: 1.5409 - lr: 0.0023 - 2s/epoch - 2ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 2s - loss: 16.2568 - mse: 16.2568 - mae: 1.5862 - val_loss: 9.7551 - val_mse: 9.7551 - val_mae: 1.5652 - lr: 0.0023 - 2s/epoch - 2ms/step\n",
            "Epoch 13/100\n",
            "1000/1000 - 2s - loss: 16.1474 - mse: 16.1474 - mae: 1.5835 - val_loss: 9.9680 - val_mse: 9.9680 - val_mae: 1.5342 - lr: 0.0023 - 2s/epoch - 2ms/step\n",
            "Epoch 14/100\n",
            "1000/1000 - 2s - loss: 16.2222 - mse: 16.2222 - mae: 1.5858 - val_loss: 9.6826 - val_mse: 9.6826 - val_mae: 1.4966 - lr: 0.0023 - 2s/epoch - 2ms/step\n",
            "Score for fold 1: loss of 9.682609558105469\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 14.6157 - mse: 14.6157 - mae: 1.5606 - val_loss: 15.5212 - val_mse: 15.5212 - val_mae: 1.6297 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 14.5884 - mse: 14.5884 - mae: 1.5625 - val_loss: 15.7393 - val_mse: 15.7393 - val_mae: 1.5775 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 14.6047 - mse: 14.6047 - mae: 1.5576 - val_loss: 15.7110 - val_mse: 15.7110 - val_mae: 1.6473 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 14.5799 - mse: 14.5799 - mae: 1.5609 - val_loss: 15.6976 - val_mse: 15.6976 - val_mae: 1.6054 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 14.5954 - mse: 14.5954 - mae: 1.5600 - val_loss: 15.7427 - val_mse: 15.7427 - val_mae: 1.6248 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 14.5376 - mse: 14.5376 - mae: 1.5637 - val_loss: 15.8267 - val_mse: 15.8267 - val_mae: 1.5965 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Score for fold 2: loss of 15.826689720153809\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 13.2915 - mse: 13.2915 - mae: 1.5610 - val_loss: 20.9594 - val_mse: 20.9594 - val_mae: 1.6150 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 13.2871 - mse: 13.2871 - mae: 1.5656 - val_loss: 20.7294 - val_mse: 20.7294 - val_mae: 1.5971 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 13.2701 - mse: 13.2701 - mae: 1.5646 - val_loss: 20.7619 - val_mse: 20.7619 - val_mae: 1.6237 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 13.2466 - mse: 13.2466 - mae: 1.5636 - val_loss: 20.9471 - val_mse: 20.9471 - val_mae: 1.6073 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 13.2202 - mse: 13.2202 - mae: 1.5637 - val_loss: 20.9524 - val_mse: 20.9524 - val_mae: 1.6058 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 13.1863 - mse: 13.1863 - mae: 1.5647 - val_loss: 20.9275 - val_mse: 20.9275 - val_mae: 1.6032 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 13.1647 - mse: 13.1647 - mae: 1.5627 - val_loss: 20.9754 - val_mse: 20.9754 - val_mae: 1.5872 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Score for fold 3: loss of 20.975372314453125\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 15.5882 - mse: 15.5882 - mae: 1.5842 - val_loss: 11.5598 - val_mse: 11.5598 - val_mae: 1.5495 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 15.5038 - mse: 15.5038 - mae: 1.5832 - val_loss: 11.7981 - val_mse: 11.7981 - val_mae: 1.4998 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 15.5270 - mse: 15.5270 - mae: 1.5840 - val_loss: 11.7399 - val_mse: 11.7399 - val_mae: 1.5350 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 15.5011 - mse: 15.5011 - mae: 1.5856 - val_loss: 11.6376 - val_mse: 11.6376 - val_mae: 1.5723 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 15.4687 - mse: 15.4687 - mae: 1.5853 - val_loss: 11.5103 - val_mse: 11.5103 - val_mae: 1.5140 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 15.4802 - mse: 15.4802 - mae: 1.5812 - val_loss: 11.4253 - val_mse: 11.4253 - val_mae: 1.5444 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 15.4690 - mse: 15.4690 - mae: 1.5810 - val_loss: 11.6526 - val_mse: 11.6526 - val_mae: 1.5664 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 2s - loss: 15.4482 - mse: 15.4482 - mae: 1.5801 - val_loss: 11.4618 - val_mse: 11.4618 - val_mae: 1.5582 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 2s - loss: 15.4419 - mse: 15.4419 - mae: 1.5838 - val_loss: 11.6110 - val_mse: 11.6110 - val_mae: 1.5099 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 2s - loss: 15.4319 - mse: 15.4319 - mae: 1.5821 - val_loss: 11.5660 - val_mse: 11.5660 - val_mae: 1.5630 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 2s - loss: 15.4581 - mse: 15.4581 - mae: 1.5809 - val_loss: 11.9260 - val_mse: 11.9260 - val_mae: 1.5920 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Score for fold 4: loss of 11.926021575927734\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 14.3281 - mse: 14.3281 - mae: 1.5719 - val_loss: 16.2298 - val_mse: 16.2298 - val_mae: 1.5989 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 14.2535 - mse: 14.2535 - mae: 1.5627 - val_loss: 16.3113 - val_mse: 16.3113 - val_mae: 1.6167 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 14.2237 - mse: 14.2237 - mae: 1.5691 - val_loss: 16.4090 - val_mse: 16.4090 - val_mae: 1.5407 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 14.2462 - mse: 14.2462 - mae: 1.5650 - val_loss: 16.3205 - val_mse: 16.3205 - val_mae: 1.5838 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 14.2143 - mse: 14.2143 - mae: 1.5649 - val_loss: 16.2181 - val_mse: 16.2181 - val_mae: 1.5960 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 14.1424 - mse: 14.1424 - mae: 1.5622 - val_loss: 16.4954 - val_mse: 16.4954 - val_mae: 1.5618 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 14.1243 - mse: 14.1243 - mae: 1.5603 - val_loss: 16.1226 - val_mse: 16.1226 - val_mae: 1.5944 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 2s - loss: 14.1446 - mse: 14.1446 - mae: 1.5617 - val_loss: 16.3932 - val_mse: 16.3932 - val_mae: 1.5910 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 2s - loss: 14.1799 - mse: 14.1799 - mae: 1.5583 - val_loss: 16.0139 - val_mse: 16.0139 - val_mae: 1.6057 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 2s - loss: 14.1795 - mse: 14.1795 - mae: 1.5619 - val_loss: 16.4112 - val_mse: 16.4112 - val_mae: 1.5623 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 2s - loss: 14.1355 - mse: 14.1355 - mae: 1.5568 - val_loss: 16.1921 - val_mse: 16.1921 - val_mae: 1.6463 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 2s - loss: 14.0845 - mse: 14.0845 - mae: 1.5596 - val_loss: 16.1671 - val_mse: 16.1671 - val_mae: 1.5885 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 13/100\n",
            "1000/1000 - 2s - loss: 14.0995 - mse: 14.0995 - mae: 1.5591 - val_loss: 16.2637 - val_mse: 16.2637 - val_mae: 1.5814 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 14/100\n",
            "1000/1000 - 2s - loss: 14.0784 - mse: 14.0784 - mae: 1.5551 - val_loss: 16.3266 - val_mse: 16.3266 - val_mae: 1.6338 - lr: 0.0010 - 2s/epoch - 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-10-14 21:51:06,394]\u001b[0m Finished trial#43 resulted in value: 14.95. Current best value is 14.724 with parameters: {'activation': 'relu', 'num_hidden_layer': 3, 'i': 6, 'learning_rate': 0.0023989159773794054}.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Score for fold 5: loss of 16.326597213745117\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 16.7787 - mse: 16.7787 - mae: 1.6391 - val_loss: 10.4442 - val_mse: 10.4442 - val_mae: 1.6160 - lr: 0.0050 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 16.3813 - mse: 16.3813 - mae: 1.6151 - val_loss: 10.2708 - val_mse: 10.2708 - val_mae: 1.6112 - lr: 0.0050 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 16.4233 - mse: 16.4233 - mae: 1.6100 - val_loss: 10.1870 - val_mse: 10.1870 - val_mae: 1.5959 - lr: 0.0050 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 16.3441 - mse: 16.3441 - mae: 1.6181 - val_loss: 10.1756 - val_mse: 10.1756 - val_mae: 1.5722 - lr: 0.0050 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 16.3330 - mse: 16.3330 - mae: 1.6152 - val_loss: 10.2468 - val_mse: 10.2468 - val_mae: 1.5013 - lr: 0.0050 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 16.2947 - mse: 16.2947 - mae: 1.6033 - val_loss: 10.3407 - val_mse: 10.3407 - val_mae: 1.5115 - lr: 0.0050 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 16.2322 - mse: 16.2322 - mae: 1.6024 - val_loss: 10.1837 - val_mse: 10.1837 - val_mae: 1.5561 - lr: 0.0050 - 2s/epoch - 2ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 2s - loss: 16.3082 - mse: 16.3082 - mae: 1.6042 - val_loss: 10.1155 - val_mse: 10.1155 - val_mae: 1.5510 - lr: 0.0050 - 2s/epoch - 2ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 2s - loss: 16.2480 - mse: 16.2480 - mae: 1.6017 - val_loss: 10.1204 - val_mse: 10.1204 - val_mae: 1.5427 - lr: 0.0050 - 2s/epoch - 2ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 2s - loss: 16.1988 - mse: 16.1988 - mae: 1.6020 - val_loss: 10.1843 - val_mse: 10.1843 - val_mae: 1.6009 - lr: 0.0050 - 2s/epoch - 2ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 2s - loss: 16.2376 - mse: 16.2376 - mae: 1.6001 - val_loss: 10.1474 - val_mse: 10.1474 - val_mae: 1.5629 - lr: 0.0050 - 2s/epoch - 2ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 2s - loss: 16.3422 - mse: 16.3422 - mae: 1.6089 - val_loss: 10.3296 - val_mse: 10.3296 - val_mae: 1.4565 - lr: 0.0050 - 2s/epoch - 2ms/step\n",
            "Epoch 13/100\n",
            "1000/1000 - 2s - loss: 16.2020 - mse: 16.2020 - mae: 1.6065 - val_loss: 10.1039 - val_mse: 10.1039 - val_mae: 1.5649 - lr: 0.0050 - 2s/epoch - 2ms/step\n",
            "Epoch 14/100\n",
            "1000/1000 - 2s - loss: 16.2322 - mse: 16.2322 - mae: 1.6064 - val_loss: 10.0933 - val_mse: 10.0933 - val_mae: 1.4801 - lr: 0.0050 - 2s/epoch - 2ms/step\n",
            "Epoch 15/100\n",
            "1000/1000 - 2s - loss: 16.1549 - mse: 16.1549 - mae: 1.5990 - val_loss: 10.1444 - val_mse: 10.1444 - val_mae: 1.5597 - lr: 0.0050 - 2s/epoch - 2ms/step\n",
            "Epoch 16/100\n",
            "1000/1000 - 2s - loss: 16.1625 - mse: 16.1625 - mae: 1.6018 - val_loss: 10.0753 - val_mse: 10.0753 - val_mae: 1.5893 - lr: 0.0050 - 2s/epoch - 2ms/step\n",
            "Epoch 17/100\n",
            "1000/1000 - 2s - loss: 16.1616 - mse: 16.1616 - mae: 1.6098 - val_loss: 10.1008 - val_mse: 10.1008 - val_mae: 1.4956 - lr: 0.0050 - 2s/epoch - 2ms/step\n",
            "Epoch 18/100\n",
            "1000/1000 - 2s - loss: 16.2957 - mse: 16.2957 - mae: 1.6178 - val_loss: 10.0302 - val_mse: 10.0302 - val_mae: 1.5711 - lr: 0.0050 - 2s/epoch - 2ms/step\n",
            "Epoch 19/100\n",
            "1000/1000 - 2s - loss: 16.3024 - mse: 16.3024 - mae: 1.6148 - val_loss: 10.0978 - val_mse: 10.0978 - val_mae: 1.5439 - lr: 0.0050 - 2s/epoch - 2ms/step\n",
            "Epoch 20/100\n",
            "1000/1000 - 2s - loss: 16.2561 - mse: 16.2561 - mae: 1.6127 - val_loss: 10.0274 - val_mse: 10.0274 - val_mae: 1.5451 - lr: 0.0050 - 2s/epoch - 2ms/step\n",
            "Epoch 21/100\n",
            "1000/1000 - 2s - loss: 16.1945 - mse: 16.1945 - mae: 1.6174 - val_loss: 9.8918 - val_mse: 9.8918 - val_mae: 1.5462 - lr: 0.0050 - 2s/epoch - 2ms/step\n",
            "Epoch 22/100\n",
            "1000/1000 - 2s - loss: 16.2283 - mse: 16.2283 - mae: 1.6110 - val_loss: 10.1992 - val_mse: 10.1992 - val_mae: 1.5528 - lr: 0.0050 - 2s/epoch - 2ms/step\n",
            "Epoch 23/100\n",
            "1000/1000 - 2s - loss: 16.2530 - mse: 16.2530 - mae: 1.6228 - val_loss: 10.0278 - val_mse: 10.0278 - val_mae: 1.5981 - lr: 0.0050 - 2s/epoch - 2ms/step\n",
            "Epoch 24/100\n",
            "1000/1000 - 2s - loss: 16.2292 - mse: 16.2292 - mae: 1.6178 - val_loss: 10.2869 - val_mse: 10.2869 - val_mae: 1.5716 - lr: 0.0050 - 2s/epoch - 2ms/step\n",
            "Epoch 25/100\n",
            "1000/1000 - 2s - loss: 16.3253 - mse: 16.3253 - mae: 1.6275 - val_loss: 10.1128 - val_mse: 10.1128 - val_mae: 1.5850 - lr: 0.0050 - 2s/epoch - 2ms/step\n",
            "Epoch 26/100\n",
            "1000/1000 - 2s - loss: 16.2457 - mse: 16.2457 - mae: 1.6265 - val_loss: 10.5355 - val_mse: 10.5355 - val_mae: 1.4569 - lr: 0.0050 - 2s/epoch - 2ms/step\n",
            "Score for fold 1: loss of 10.535531997680664\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 15.7959 - mse: 15.7959 - mae: 1.5871 - val_loss: 11.5027 - val_mse: 11.5027 - val_mae: 1.5232 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 15.7251 - mse: 15.7251 - mae: 1.5819 - val_loss: 11.3991 - val_mse: 11.3991 - val_mae: 1.5162 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 15.6808 - mse: 15.6808 - mae: 1.5796 - val_loss: 11.4467 - val_mse: 11.4467 - val_mae: 1.6319 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 15.6303 - mse: 15.6303 - mae: 1.5838 - val_loss: 11.4105 - val_mse: 11.4105 - val_mae: 1.5645 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 15.6254 - mse: 15.6254 - mae: 1.5822 - val_loss: 11.4491 - val_mse: 11.4491 - val_mae: 1.5803 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 15.6532 - mse: 15.6532 - mae: 1.5841 - val_loss: 11.5074 - val_mse: 11.5074 - val_mae: 1.5218 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 15.6071 - mse: 15.6071 - mae: 1.5812 - val_loss: 11.6456 - val_mse: 11.6456 - val_mae: 1.4844 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Score for fold 2: loss of 11.645621299743652\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 12.8374 - mse: 12.8374 - mae: 1.5718 - val_loss: 22.7878 - val_mse: 22.7878 - val_mae: 1.5552 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 12.7790 - mse: 12.7790 - mae: 1.5696 - val_loss: 22.6889 - val_mse: 22.6889 - val_mae: 1.5797 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 12.7797 - mse: 12.7797 - mae: 1.5657 - val_loss: 22.6897 - val_mse: 22.6897 - val_mae: 1.5565 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 12.7640 - mse: 12.7640 - mae: 1.5705 - val_loss: 22.5521 - val_mse: 22.5521 - val_mae: 1.6203 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 12.7848 - mse: 12.7848 - mae: 1.5684 - val_loss: 22.6806 - val_mse: 22.6806 - val_mae: 1.5497 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 12.7630 - mse: 12.7630 - mae: 1.5690 - val_loss: 22.6022 - val_mse: 22.6022 - val_mae: 1.5709 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 12.7167 - mse: 12.7167 - mae: 1.5691 - val_loss: 22.8164 - val_mse: 22.8164 - val_mae: 1.5446 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 2s - loss: 12.7475 - mse: 12.7475 - mae: 1.5714 - val_loss: 22.6191 - val_mse: 22.6191 - val_mae: 1.6171 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 2s - loss: 12.7464 - mse: 12.7464 - mae: 1.5706 - val_loss: 22.6264 - val_mse: 22.6264 - val_mae: 1.6586 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Score for fold 3: loss of 22.626394271850586\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 14.5846 - mse: 14.5846 - mae: 1.5758 - val_loss: 15.2533 - val_mse: 15.2533 - val_mae: 1.5801 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 14.5700 - mse: 14.5700 - mae: 1.5743 - val_loss: 15.3766 - val_mse: 15.3766 - val_mae: 1.5139 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 14.5277 - mse: 14.5277 - mae: 1.5746 - val_loss: 15.3975 - val_mse: 15.3975 - val_mae: 1.5950 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 14.4988 - mse: 14.4988 - mae: 1.5754 - val_loss: 15.3834 - val_mse: 15.3834 - val_mae: 1.6498 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 14.4529 - mse: 14.4529 - mae: 1.5725 - val_loss: 15.3851 - val_mse: 15.3851 - val_mae: 1.6359 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 14.4868 - mse: 14.4868 - mae: 1.5762 - val_loss: 15.4062 - val_mse: 15.4062 - val_mae: 1.5687 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Score for fold 4: loss of 15.40615463256836\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 14.7394 - mse: 14.7394 - mae: 1.5665 - val_loss: 14.2858 - val_mse: 14.2858 - val_mae: 1.5717 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 14.6880 - mse: 14.6880 - mae: 1.5705 - val_loss: 14.5984 - val_mse: 14.5984 - val_mae: 1.5040 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 14.6701 - mse: 14.6701 - mae: 1.5688 - val_loss: 14.5014 - val_mse: 14.5014 - val_mae: 1.5536 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 14.6494 - mse: 14.6494 - mae: 1.5702 - val_loss: 14.4666 - val_mse: 14.4666 - val_mae: 1.6465 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 14.6582 - mse: 14.6582 - mae: 1.5631 - val_loss: 14.2787 - val_mse: 14.2787 - val_mae: 1.6306 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 14.6494 - mse: 14.6494 - mae: 1.5712 - val_loss: 14.3543 - val_mse: 14.3543 - val_mae: 1.5908 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 14.6059 - mse: 14.6059 - mae: 1.5626 - val_loss: 14.5673 - val_mse: 14.5673 - val_mae: 1.5556 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 2s - loss: 14.6132 - mse: 14.6132 - mae: 1.5645 - val_loss: 14.2900 - val_mse: 14.2900 - val_mae: 1.6508 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 2s - loss: 14.6173 - mse: 14.6173 - mae: 1.5719 - val_loss: 14.3002 - val_mse: 14.3002 - val_mae: 1.5789 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 2s - loss: 14.6438 - mse: 14.6438 - mae: 1.5658 - val_loss: 14.2325 - val_mse: 14.2325 - val_mae: 1.6160 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 2s - loss: 14.5967 - mse: 14.5967 - mae: 1.5704 - val_loss: 14.4176 - val_mse: 14.4176 - val_mae: 1.5662 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 2s - loss: 14.6162 - mse: 14.6162 - mae: 1.5656 - val_loss: 14.3311 - val_mse: 14.3311 - val_mae: 1.6132 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 13/100\n",
            "1000/1000 - 2s - loss: 14.5945 - mse: 14.5945 - mae: 1.5673 - val_loss: 14.3743 - val_mse: 14.3743 - val_mae: 1.6134 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 14/100\n",
            "1000/1000 - 2s - loss: 14.5784 - mse: 14.5784 - mae: 1.5697 - val_loss: 14.3867 - val_mse: 14.3867 - val_mae: 1.6369 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 15/100\n",
            "1000/1000 - 2s - loss: 14.6274 - mse: 14.6274 - mae: 1.5671 - val_loss: 14.3449 - val_mse: 14.3449 - val_mae: 1.6604 - lr: 0.0010 - 2s/epoch - 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-10-14 21:52:57,496]\u001b[0m Finished trial#44 resulted in value: 14.913999999999998. Current best value is 14.724 with parameters: {'activation': 'relu', 'num_hidden_layer': 3, 'i': 6, 'learning_rate': 0.0023989159773794054}.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Score for fold 5: loss of 14.344902038574219\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 16.6297 - mse: 16.6297 - mae: 1.6869 - val_loss: 13.9429 - val_mse: 13.9429 - val_mae: 1.5851 - lr: 6.2713e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 16.1621 - mse: 16.1621 - mae: 1.6379 - val_loss: 13.8316 - val_mse: 13.8316 - val_mae: 1.6497 - lr: 6.2713e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 16.1528 - mse: 16.1528 - mae: 1.6362 - val_loss: 13.8876 - val_mse: 13.8876 - val_mae: 1.6079 - lr: 6.2713e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 16.1146 - mse: 16.1146 - mae: 1.6390 - val_loss: 13.8147 - val_mse: 13.8147 - val_mae: 1.6296 - lr: 6.2713e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 16.1290 - mse: 16.1290 - mae: 1.6400 - val_loss: 13.8195 - val_mse: 13.8195 - val_mae: 1.6517 - lr: 6.2713e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 16.1570 - mse: 16.1570 - mae: 1.6436 - val_loss: 13.8393 - val_mse: 13.8393 - val_mae: 1.6043 - lr: 6.2713e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 16.0986 - mse: 16.0986 - mae: 1.6399 - val_loss: 13.8602 - val_mse: 13.8602 - val_mae: 1.5924 - lr: 6.2713e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 2s - loss: 16.1070 - mse: 16.1070 - mae: 1.6434 - val_loss: 13.8121 - val_mse: 13.8121 - val_mae: 1.6580 - lr: 6.2713e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 2s - loss: 16.1086 - mse: 16.1086 - mae: 1.6399 - val_loss: 13.8747 - val_mse: 13.8747 - val_mae: 1.6221 - lr: 6.2713e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 2s - loss: 16.1195 - mse: 16.1195 - mae: 1.6449 - val_loss: 13.8504 - val_mse: 13.8504 - val_mae: 1.6908 - lr: 6.2713e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 2s - loss: 16.1194 - mse: 16.1194 - mae: 1.6421 - val_loss: 13.8676 - val_mse: 13.8676 - val_mae: 1.6132 - lr: 6.2713e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 2s - loss: 16.1383 - mse: 16.1383 - mae: 1.6405 - val_loss: 13.8373 - val_mse: 13.8373 - val_mae: 1.6427 - lr: 6.2713e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 13/100\n",
            "1000/1000 - 2s - loss: 16.0994 - mse: 16.0994 - mae: 1.6466 - val_loss: 13.8140 - val_mse: 13.8140 - val_mae: 1.6636 - lr: 6.2713e-04 - 2s/epoch - 2ms/step\n",
            "Score for fold 1: loss of 13.814007759094238\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 13.8825 - mse: 13.8825 - mae: 1.6335 - val_loss: 22.6377 - val_mse: 22.6377 - val_mae: 1.6945 - lr: 6.2713e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 13.9215 - mse: 13.9215 - mae: 1.6305 - val_loss: 22.7208 - val_mse: 22.7208 - val_mae: 1.6372 - lr: 6.2713e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 13.9011 - mse: 13.9011 - mae: 1.6287 - val_loss: 22.6298 - val_mse: 22.6298 - val_mae: 1.7015 - lr: 6.2713e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 13.9079 - mse: 13.9079 - mae: 1.6295 - val_loss: 22.6798 - val_mse: 22.6798 - val_mae: 1.6499 - lr: 6.2713e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 13.9304 - mse: 13.9304 - mae: 1.6287 - val_loss: 22.6542 - val_mse: 22.6542 - val_mae: 1.6869 - lr: 6.2713e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 13.9221 - mse: 13.9221 - mae: 1.6387 - val_loss: 22.6559 - val_mse: 22.6559 - val_mae: 1.6588 - lr: 6.2713e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 13.8982 - mse: 13.8982 - mae: 1.6285 - val_loss: 22.7387 - val_mse: 22.7387 - val_mae: 1.6371 - lr: 6.2713e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 2s - loss: 13.8808 - mse: 13.8808 - mae: 1.6303 - val_loss: 22.6722 - val_mse: 22.6722 - val_mae: 1.6617 - lr: 6.2713e-04 - 2s/epoch - 2ms/step\n",
            "Score for fold 2: loss of 22.67218780517578\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 16.8245 - mse: 16.8245 - mae: 1.6424 - val_loss: 10.9230 - val_mse: 10.9230 - val_mae: 1.6831 - lr: 6.2713e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 16.8513 - mse: 16.8513 - mae: 1.6399 - val_loss: 10.9407 - val_mse: 10.9407 - val_mae: 1.6182 - lr: 6.2713e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 16.8215 - mse: 16.8215 - mae: 1.6400 - val_loss: 10.9619 - val_mse: 10.9619 - val_mae: 1.5942 - lr: 6.2713e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 16.8421 - mse: 16.8421 - mae: 1.6411 - val_loss: 10.9238 - val_mse: 10.9238 - val_mae: 1.6328 - lr: 6.2713e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 16.8578 - mse: 16.8578 - mae: 1.6406 - val_loss: 10.9165 - val_mse: 10.9165 - val_mae: 1.6459 - lr: 6.2713e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 16.8866 - mse: 16.8866 - mae: 1.6372 - val_loss: 11.0446 - val_mse: 11.0446 - val_mae: 1.5866 - lr: 6.2713e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 16.8413 - mse: 16.8413 - mae: 1.6386 - val_loss: 10.9434 - val_mse: 10.9434 - val_mae: 1.6248 - lr: 6.2713e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 2s - loss: 16.8508 - mse: 16.8508 - mae: 1.6418 - val_loss: 10.9544 - val_mse: 10.9544 - val_mae: 1.7186 - lr: 6.2713e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 2s - loss: 16.8550 - mse: 16.8550 - mae: 1.6431 - val_loss: 11.1661 - val_mse: 11.1661 - val_mae: 1.5350 - lr: 6.2713e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 2s - loss: 16.8541 - mse: 16.8541 - mae: 1.6383 - val_loss: 11.1385 - val_mse: 11.1385 - val_mae: 1.5599 - lr: 6.2713e-04 - 2s/epoch - 2ms/step\n",
            "Score for fold 3: loss of 11.138466835021973\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 15.4736 - mse: 15.4736 - mae: 1.6334 - val_loss: 16.4102 - val_mse: 16.4102 - val_mae: 1.6581 - lr: 6.2713e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 15.4626 - mse: 15.4626 - mae: 1.6372 - val_loss: 16.3923 - val_mse: 16.3923 - val_mae: 1.6582 - lr: 6.2713e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 15.4694 - mse: 15.4694 - mae: 1.6339 - val_loss: 16.6923 - val_mse: 16.6923 - val_mae: 1.5780 - lr: 6.2713e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 15.4586 - mse: 15.4586 - mae: 1.6377 - val_loss: 16.3700 - val_mse: 16.3700 - val_mae: 1.6770 - lr: 6.2713e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 15.4741 - mse: 15.4741 - mae: 1.6313 - val_loss: 16.5879 - val_mse: 16.5879 - val_mae: 1.6140 - lr: 6.2713e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 15.4570 - mse: 15.4570 - mae: 1.6352 - val_loss: 16.4579 - val_mse: 16.4579 - val_mae: 1.6326 - lr: 6.2713e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 15.4628 - mse: 15.4628 - mae: 1.6366 - val_loss: 17.1219 - val_mse: 17.1219 - val_mae: 1.5375 - lr: 6.2713e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 2s - loss: 15.4447 - mse: 15.4447 - mae: 1.6345 - val_loss: 16.4969 - val_mse: 16.4969 - val_mae: 1.6157 - lr: 6.2713e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 2s - loss: 15.4701 - mse: 15.4701 - mae: 1.6339 - val_loss: 16.3585 - val_mse: 16.3585 - val_mae: 1.6741 - lr: 6.2713e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 2s - loss: 15.4714 - mse: 15.4714 - mae: 1.6367 - val_loss: 16.3943 - val_mse: 16.3943 - val_mae: 1.6537 - lr: 6.2713e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 2s - loss: 15.4639 - mse: 15.4639 - mae: 1.6333 - val_loss: 16.3624 - val_mse: 16.3624 - val_mae: 1.6807 - lr: 6.2713e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 2s - loss: 15.4985 - mse: 15.4985 - mae: 1.6317 - val_loss: 16.3957 - val_mse: 16.3957 - val_mae: 1.7510 - lr: 6.2713e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 13/100\n",
            "1000/1000 - 2s - loss: 15.4609 - mse: 15.4609 - mae: 1.6320 - val_loss: 16.6349 - val_mse: 16.6349 - val_mae: 1.5880 - lr: 6.2713e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 14/100\n",
            "1000/1000 - 2s - loss: 15.4723 - mse: 15.4723 - mae: 1.6325 - val_loss: 16.5182 - val_mse: 16.5182 - val_mae: 1.6221 - lr: 6.2713e-04 - 2s/epoch - 2ms/step\n",
            "Score for fold 4: loss of 16.51822853088379\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 16.0239 - mse: 16.0239 - mae: 1.6328 - val_loss: 14.2794 - val_mse: 14.2794 - val_mae: 1.6892 - lr: 6.2713e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 15.9997 - mse: 15.9997 - mae: 1.6381 - val_loss: 14.2883 - val_mse: 14.2883 - val_mae: 1.6424 - lr: 6.2713e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 16.0284 - mse: 16.0284 - mae: 1.6309 - val_loss: 14.3397 - val_mse: 14.3397 - val_mae: 1.6256 - lr: 6.2713e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 16.0130 - mse: 16.0130 - mae: 1.6325 - val_loss: 14.3174 - val_mse: 14.3174 - val_mae: 1.6269 - lr: 6.2713e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 16.0241 - mse: 16.0241 - mae: 1.6319 - val_loss: 14.2537 - val_mse: 14.2537 - val_mae: 1.6536 - lr: 6.2713e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 16.0245 - mse: 16.0245 - mae: 1.6322 - val_loss: 14.3235 - val_mse: 14.3235 - val_mae: 1.6527 - lr: 6.2713e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 16.0070 - mse: 16.0070 - mae: 1.6389 - val_loss: 14.2578 - val_mse: 14.2578 - val_mae: 1.6838 - lr: 6.2713e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 2s - loss: 16.0174 - mse: 16.0174 - mae: 1.6392 - val_loss: 14.3727 - val_mse: 14.3727 - val_mae: 1.5908 - lr: 6.2713e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 2s - loss: 15.9969 - mse: 15.9969 - mae: 1.6364 - val_loss: 14.3417 - val_mse: 14.3417 - val_mae: 1.6164 - lr: 6.2713e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 2s - loss: 16.0107 - mse: 16.0107 - mae: 1.6362 - val_loss: 14.3584 - val_mse: 14.3584 - val_mae: 1.6190 - lr: 6.2713e-04 - 2s/epoch - 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-10-14 21:54:34,612]\u001b[0m Finished trial#45 resulted in value: 15.7. Current best value is 14.724 with parameters: {'activation': 'relu', 'num_hidden_layer': 3, 'i': 6, 'learning_rate': 0.0023989159773794054}.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Score for fold 5: loss of 14.358413696289062\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 14.2174 - mse: 14.2174 - mae: 1.6451 - val_loss: 21.8525 - val_mse: 21.8525 - val_mae: 1.6179 - lr: 0.0013 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 13.7177 - mse: 13.7177 - mae: 1.6005 - val_loss: 21.7456 - val_mse: 21.7456 - val_mae: 1.6254 - lr: 0.0013 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 13.6537 - mse: 13.6537 - mae: 1.5913 - val_loss: 21.7034 - val_mse: 21.7034 - val_mae: 1.6826 - lr: 0.0013 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 13.5979 - mse: 13.5979 - mae: 1.5929 - val_loss: 21.8000 - val_mse: 21.8000 - val_mae: 1.6138 - lr: 0.0013 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 13.5750 - mse: 13.5750 - mae: 1.5865 - val_loss: 21.5859 - val_mse: 21.5859 - val_mae: 1.6407 - lr: 0.0013 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 13.5042 - mse: 13.5042 - mae: 1.5887 - val_loss: 21.6176 - val_mse: 21.6176 - val_mae: 1.5920 - lr: 0.0013 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 13.5130 - mse: 13.5130 - mae: 1.5839 - val_loss: 21.6291 - val_mse: 21.6291 - val_mae: 1.6217 - lr: 0.0013 - 2s/epoch - 2ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 2s - loss: 13.4817 - mse: 13.4817 - mae: 1.5818 - val_loss: 21.5333 - val_mse: 21.5333 - val_mae: 1.6174 - lr: 0.0013 - 2s/epoch - 2ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 2s - loss: 13.4323 - mse: 13.4323 - mae: 1.5856 - val_loss: 21.6339 - val_mse: 21.6339 - val_mae: 1.5709 - lr: 0.0013 - 2s/epoch - 2ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 2s - loss: 13.4338 - mse: 13.4338 - mae: 1.5826 - val_loss: 21.5655 - val_mse: 21.5655 - val_mae: 1.5889 - lr: 0.0013 - 2s/epoch - 2ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 2s - loss: 13.4187 - mse: 13.4187 - mae: 1.5806 - val_loss: 21.5417 - val_mse: 21.5417 - val_mae: 1.5749 - lr: 0.0013 - 2s/epoch - 2ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 2s - loss: 13.4036 - mse: 13.4036 - mae: 1.5806 - val_loss: 21.4887 - val_mse: 21.4887 - val_mae: 1.6201 - lr: 0.0013 - 2s/epoch - 2ms/step\n",
            "Epoch 13/100\n",
            "1000/1000 - 2s - loss: 13.3986 - mse: 13.3986 - mae: 1.5784 - val_loss: 21.5144 - val_mse: 21.5144 - val_mae: 1.6762 - lr: 0.0013 - 2s/epoch - 2ms/step\n",
            "Epoch 14/100\n",
            "1000/1000 - 2s - loss: 13.3527 - mse: 13.3527 - mae: 1.5747 - val_loss: 21.4264 - val_mse: 21.4264 - val_mae: 1.6223 - lr: 0.0013 - 2s/epoch - 2ms/step\n",
            "Epoch 15/100\n",
            "1000/1000 - 2s - loss: 13.3542 - mse: 13.3542 - mae: 1.5754 - val_loss: 21.4125 - val_mse: 21.4125 - val_mae: 1.6464 - lr: 0.0013 - 2s/epoch - 2ms/step\n",
            "Epoch 16/100\n",
            "1000/1000 - 2s - loss: 13.3480 - mse: 13.3480 - mae: 1.5765 - val_loss: 21.4170 - val_mse: 21.4170 - val_mae: 1.6238 - lr: 0.0013 - 2s/epoch - 2ms/step\n",
            "Epoch 17/100\n",
            "1000/1000 - 2s - loss: 13.3332 - mse: 13.3332 - mae: 1.5775 - val_loss: 21.3937 - val_mse: 21.3937 - val_mae: 1.6796 - lr: 0.0013 - 2s/epoch - 2ms/step\n",
            "Epoch 18/100\n",
            "1000/1000 - 2s - loss: 13.2975 - mse: 13.2975 - mae: 1.5792 - val_loss: 21.5790 - val_mse: 21.5790 - val_mae: 1.6955 - lr: 0.0013 - 2s/epoch - 2ms/step\n",
            "Epoch 19/100\n",
            "1000/1000 - 2s - loss: 13.3137 - mse: 13.3137 - mae: 1.5772 - val_loss: 21.4231 - val_mse: 21.4231 - val_mae: 1.5753 - lr: 0.0013 - 2s/epoch - 2ms/step\n",
            "Epoch 20/100\n",
            "1000/1000 - 2s - loss: 13.3019 - mse: 13.3019 - mae: 1.5743 - val_loss: 21.3770 - val_mse: 21.3770 - val_mae: 1.5981 - lr: 0.0013 - 2s/epoch - 2ms/step\n",
            "Epoch 21/100\n",
            "1000/1000 - 2s - loss: 13.3105 - mse: 13.3105 - mae: 1.5781 - val_loss: 21.3527 - val_mse: 21.3527 - val_mae: 1.6257 - lr: 0.0013 - 2s/epoch - 2ms/step\n",
            "Epoch 22/100\n",
            "1000/1000 - 2s - loss: 13.2906 - mse: 13.2906 - mae: 1.5779 - val_loss: 21.5646 - val_mse: 21.5646 - val_mae: 1.6383 - lr: 0.0013 - 2s/epoch - 2ms/step\n",
            "Epoch 23/100\n",
            "1000/1000 - 2s - loss: 13.2960 - mse: 13.2960 - mae: 1.5732 - val_loss: 21.3888 - val_mse: 21.3888 - val_mae: 1.6304 - lr: 0.0013 - 2s/epoch - 2ms/step\n",
            "Epoch 24/100\n",
            "1000/1000 - 2s - loss: 13.2616 - mse: 13.2616 - mae: 1.5753 - val_loss: 21.4034 - val_mse: 21.4034 - val_mae: 1.7014 - lr: 0.0013 - 2s/epoch - 2ms/step\n",
            "Epoch 25/100\n",
            "1000/1000 - 2s - loss: 13.2666 - mse: 13.2666 - mae: 1.5734 - val_loss: 21.3594 - val_mse: 21.3594 - val_mae: 1.6133 - lr: 0.0013 - 2s/epoch - 2ms/step\n",
            "Epoch 26/100\n",
            "1000/1000 - 2s - loss: 13.2721 - mse: 13.2721 - mae: 1.5761 - val_loss: 21.3976 - val_mse: 21.3976 - val_mae: 1.6138 - lr: 0.0013 - 2s/epoch - 2ms/step\n",
            "Score for fold 1: loss of 21.397615432739258\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 14.9559 - mse: 14.9559 - mae: 1.5842 - val_loss: 14.1130 - val_mse: 14.1130 - val_mae: 1.6175 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 14.9458 - mse: 14.9458 - mae: 1.5786 - val_loss: 14.2722 - val_mse: 14.2722 - val_mae: 1.5633 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 14.9621 - mse: 14.9621 - mae: 1.5800 - val_loss: 14.1869 - val_mse: 14.1869 - val_mae: 1.5415 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 14.9443 - mse: 14.9443 - mae: 1.5766 - val_loss: 14.2517 - val_mse: 14.2517 - val_mae: 1.5455 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 14.9282 - mse: 14.9282 - mae: 1.5768 - val_loss: 14.3437 - val_mse: 14.3437 - val_mae: 1.5704 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 14.8592 - mse: 14.8592 - mae: 1.5777 - val_loss: 14.2133 - val_mse: 14.2133 - val_mae: 1.5334 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Score for fold 2: loss of 14.213329315185547\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 14.4511 - mse: 14.4511 - mae: 1.5652 - val_loss: 16.2735 - val_mse: 16.2735 - val_mae: 1.6177 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 14.4226 - mse: 14.4226 - mae: 1.5599 - val_loss: 16.1707 - val_mse: 16.1707 - val_mae: 1.6674 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 14.4062 - mse: 14.4062 - mae: 1.5669 - val_loss: 16.2177 - val_mse: 16.2177 - val_mae: 1.6150 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 14.3888 - mse: 14.3888 - mae: 1.5627 - val_loss: 16.3321 - val_mse: 16.3321 - val_mae: 1.6209 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 14.3902 - mse: 14.3902 - mae: 1.5599 - val_loss: 16.2573 - val_mse: 16.2573 - val_mae: 1.6306 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 14.4198 - mse: 14.4198 - mae: 1.5603 - val_loss: 16.4309 - val_mse: 16.4309 - val_mae: 1.6697 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 14.3973 - mse: 14.3973 - mae: 1.5577 - val_loss: 16.6114 - val_mse: 16.6114 - val_mae: 1.6134 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Score for fold 3: loss of 16.611360549926758\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 15.5583 - mse: 15.5583 - mae: 1.5845 - val_loss: 11.4229 - val_mse: 11.4229 - val_mae: 1.5520 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 15.4942 - mse: 15.4942 - mae: 1.5836 - val_loss: 11.5678 - val_mse: 11.5678 - val_mae: 1.5320 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 15.5060 - mse: 15.5060 - mae: 1.5791 - val_loss: 11.6208 - val_mse: 11.6208 - val_mae: 1.5573 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 15.5002 - mse: 15.5002 - mae: 1.5806 - val_loss: 11.4141 - val_mse: 11.4141 - val_mae: 1.5369 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 15.5248 - mse: 15.5248 - mae: 1.5795 - val_loss: 11.4921 - val_mse: 11.4921 - val_mae: 1.5083 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 15.5307 - mse: 15.5307 - mae: 1.5793 - val_loss: 11.5096 - val_mse: 11.5096 - val_mae: 1.5716 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 15.4993 - mse: 15.4993 - mae: 1.5827 - val_loss: 11.6164 - val_mse: 11.6164 - val_mae: 1.5080 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 2s - loss: 15.5323 - mse: 15.5323 - mae: 1.5763 - val_loss: 11.5207 - val_mse: 11.5207 - val_mae: 1.5374 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 2s - loss: 15.4515 - mse: 15.4515 - mae: 1.5780 - val_loss: 11.6373 - val_mse: 11.6373 - val_mae: 1.6174 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Score for fold 4: loss of 11.637288093566895\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 15.6921 - mse: 15.6921 - mae: 1.5837 - val_loss: 10.6325 - val_mse: 10.6325 - val_mae: 1.5505 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 15.6552 - mse: 15.6552 - mae: 1.5802 - val_loss: 10.7947 - val_mse: 10.7947 - val_mae: 1.4807 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 15.7128 - mse: 15.7128 - mae: 1.5830 - val_loss: 11.1813 - val_mse: 11.1813 - val_mae: 1.4847 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 15.6655 - mse: 15.6655 - mae: 1.5785 - val_loss: 10.9458 - val_mse: 10.9458 - val_mae: 1.5741 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 15.6320 - mse: 15.6320 - mae: 1.5802 - val_loss: 10.7200 - val_mse: 10.7200 - val_mae: 1.5472 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 15.5937 - mse: 15.5937 - mae: 1.5771 - val_loss: 10.8726 - val_mse: 10.8726 - val_mae: 1.5542 - lr: 0.0010 - 2s/epoch - 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-10-14 21:56:09,031]\u001b[0m Finished trial#46 resulted in value: 14.946000000000002. Current best value is 14.724 with parameters: {'activation': 'relu', 'num_hidden_layer': 3, 'i': 6, 'learning_rate': 0.0023989159773794054}.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Score for fold 5: loss of 10.872580528259277\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 3s - loss: 17.0120 - mse: 17.0120 - mae: 1.6578 - val_loss: 13.0129 - val_mse: 13.0129 - val_mae: 1.5752 - lr: 1.4775e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 15.9940 - mse: 15.9940 - mae: 1.5919 - val_loss: 12.6756 - val_mse: 12.6756 - val_mae: 1.5690 - lr: 1.4775e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 15.8010 - mse: 15.8010 - mae: 1.5887 - val_loss: 12.5476 - val_mse: 12.5476 - val_mae: 1.6027 - lr: 1.4775e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 15.7509 - mse: 15.7509 - mae: 1.5878 - val_loss: 12.5104 - val_mse: 12.5104 - val_mae: 1.5904 - lr: 1.4775e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 15.7245 - mse: 15.7245 - mae: 1.5848 - val_loss: 12.5415 - val_mse: 12.5415 - val_mae: 1.5722 - lr: 1.4775e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 15.6920 - mse: 15.6920 - mae: 1.5838 - val_loss: 12.4831 - val_mse: 12.4831 - val_mae: 1.5623 - lr: 1.4775e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 15.6553 - mse: 15.6553 - mae: 1.5858 - val_loss: 12.5429 - val_mse: 12.5429 - val_mae: 1.5365 - lr: 1.4775e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 2s - loss: 15.6612 - mse: 15.6612 - mae: 1.5795 - val_loss: 12.4266 - val_mse: 12.4266 - val_mae: 1.5783 - lr: 1.4775e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 2s - loss: 15.6131 - mse: 15.6131 - mae: 1.5838 - val_loss: 12.3968 - val_mse: 12.3968 - val_mae: 1.5816 - lr: 1.4775e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 2s - loss: 15.6066 - mse: 15.6066 - mae: 1.5825 - val_loss: 12.3726 - val_mse: 12.3726 - val_mae: 1.5668 - lr: 1.4775e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 2s - loss: 15.6045 - mse: 15.6045 - mae: 1.5773 - val_loss: 12.3645 - val_mse: 12.3645 - val_mae: 1.6134 - lr: 1.4775e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 2s - loss: 15.5773 - mse: 15.5773 - mae: 1.5810 - val_loss: 12.4123 - val_mse: 12.4123 - val_mae: 1.5548 - lr: 1.4775e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 13/100\n",
            "1000/1000 - 2s - loss: 15.5641 - mse: 15.5641 - mae: 1.5783 - val_loss: 12.3994 - val_mse: 12.3994 - val_mae: 1.5685 - lr: 1.4775e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 14/100\n",
            "1000/1000 - 2s - loss: 15.5556 - mse: 15.5556 - mae: 1.5786 - val_loss: 12.3812 - val_mse: 12.3812 - val_mae: 1.5707 - lr: 1.4775e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 15/100\n",
            "1000/1000 - 2s - loss: 15.5557 - mse: 15.5557 - mae: 1.5741 - val_loss: 12.3619 - val_mse: 12.3619 - val_mae: 1.5967 - lr: 1.4775e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 16/100\n",
            "1000/1000 - 2s - loss: 15.5265 - mse: 15.5265 - mae: 1.5781 - val_loss: 12.4001 - val_mse: 12.4001 - val_mae: 1.5725 - lr: 1.4775e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 17/100\n",
            "1000/1000 - 2s - loss: 15.5341 - mse: 15.5341 - mae: 1.5751 - val_loss: 12.3525 - val_mse: 12.3525 - val_mae: 1.5495 - lr: 1.4775e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 18/100\n",
            "1000/1000 - 2s - loss: 15.5281 - mse: 15.5281 - mae: 1.5718 - val_loss: 12.4063 - val_mse: 12.4063 - val_mae: 1.5593 - lr: 1.4775e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 19/100\n",
            "1000/1000 - 2s - loss: 15.5009 - mse: 15.5009 - mae: 1.5738 - val_loss: 12.4182 - val_mse: 12.4182 - val_mae: 1.5740 - lr: 1.4775e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 20/100\n",
            "1000/1000 - 2s - loss: 15.5096 - mse: 15.5096 - mae: 1.5757 - val_loss: 12.3374 - val_mse: 12.3374 - val_mae: 1.6216 - lr: 1.4775e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 21/100\n",
            "1000/1000 - 2s - loss: 15.5063 - mse: 15.5063 - mae: 1.5729 - val_loss: 12.3327 - val_mse: 12.3327 - val_mae: 1.5722 - lr: 1.4775e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 22/100\n",
            "1000/1000 - 2s - loss: 15.4739 - mse: 15.4739 - mae: 1.5738 - val_loss: 12.4894 - val_mse: 12.4894 - val_mae: 1.5164 - lr: 1.4775e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 23/100\n",
            "1000/1000 - 2s - loss: 15.4672 - mse: 15.4672 - mae: 1.5692 - val_loss: 12.4586 - val_mse: 12.4586 - val_mae: 1.5527 - lr: 1.4775e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 24/100\n",
            "1000/1000 - 2s - loss: 15.4668 - mse: 15.4668 - mae: 1.5736 - val_loss: 12.4220 - val_mse: 12.4220 - val_mae: 1.5550 - lr: 1.4775e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 25/100\n",
            "1000/1000 - 2s - loss: 15.4607 - mse: 15.4607 - mae: 1.5698 - val_loss: 12.3151 - val_mse: 12.3151 - val_mae: 1.5598 - lr: 1.4775e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 26/100\n",
            "1000/1000 - 2s - loss: 15.4678 - mse: 15.4678 - mae: 1.5677 - val_loss: 12.3368 - val_mse: 12.3368 - val_mae: 1.6309 - lr: 1.4775e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 27/100\n",
            "1000/1000 - 2s - loss: 15.4480 - mse: 15.4480 - mae: 1.5720 - val_loss: 12.3414 - val_mse: 12.3414 - val_mae: 1.5626 - lr: 1.4775e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 28/100\n",
            "1000/1000 - 2s - loss: 15.4269 - mse: 15.4269 - mae: 1.5690 - val_loss: 12.3194 - val_mse: 12.3194 - val_mae: 1.6165 - lr: 1.4775e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 29/100\n",
            "1000/1000 - 2s - loss: 15.4454 - mse: 15.4454 - mae: 1.5672 - val_loss: 12.3662 - val_mse: 12.3662 - val_mae: 1.5885 - lr: 1.4775e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 30/100\n",
            "1000/1000 - 2s - loss: 15.4047 - mse: 15.4047 - mae: 1.5674 - val_loss: 12.3145 - val_mse: 12.3145 - val_mae: 1.6217 - lr: 1.4775e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 31/100\n",
            "1000/1000 - 2s - loss: 15.3992 - mse: 15.3992 - mae: 1.5674 - val_loss: 12.3843 - val_mse: 12.3843 - val_mae: 1.5607 - lr: 1.4775e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 32/100\n",
            "1000/1000 - 2s - loss: 15.4155 - mse: 15.4155 - mae: 1.5693 - val_loss: 12.3597 - val_mse: 12.3597 - val_mae: 1.5551 - lr: 1.4775e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 33/100\n",
            "1000/1000 - 2s - loss: 15.4126 - mse: 15.4126 - mae: 1.5649 - val_loss: 12.4187 - val_mse: 12.4187 - val_mae: 1.5298 - lr: 1.4775e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 34/100\n",
            "1000/1000 - 2s - loss: 15.3912 - mse: 15.3912 - mae: 1.5651 - val_loss: 12.3473 - val_mse: 12.3473 - val_mae: 1.5644 - lr: 1.4775e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 35/100\n",
            "1000/1000 - 2s - loss: 15.3889 - mse: 15.3889 - mae: 1.5646 - val_loss: 12.3265 - val_mse: 12.3265 - val_mae: 1.5652 - lr: 1.4775e-04 - 2s/epoch - 2ms/step\n",
            "Score for fold 1: loss of 12.326502799987793\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 13.5133 - mse: 13.5133 - mae: 1.5639 - val_loss: 19.9157 - val_mse: 19.9157 - val_mae: 1.5755 - lr: 1.4775e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 13.5304 - mse: 13.5304 - mae: 1.5639 - val_loss: 19.7857 - val_mse: 19.7857 - val_mae: 1.5928 - lr: 1.4775e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 13.5107 - mse: 13.5107 - mae: 1.5639 - val_loss: 19.7577 - val_mse: 19.7577 - val_mae: 1.6041 - lr: 1.4775e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 13.5001 - mse: 13.5001 - mae: 1.5636 - val_loss: 19.7998 - val_mse: 19.7998 - val_mae: 1.5613 - lr: 1.4775e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 13.4846 - mse: 13.4846 - mae: 1.5608 - val_loss: 19.8276 - val_mse: 19.8276 - val_mae: 1.5593 - lr: 1.4775e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 13.4933 - mse: 13.4933 - mae: 1.5611 - val_loss: 19.7718 - val_mse: 19.7718 - val_mae: 1.6010 - lr: 1.4775e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 13.4641 - mse: 13.4641 - mae: 1.5633 - val_loss: 19.8566 - val_mse: 19.8566 - val_mae: 1.5554 - lr: 1.4775e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 2s - loss: 13.4903 - mse: 13.4903 - mae: 1.5589 - val_loss: 19.7595 - val_mse: 19.7595 - val_mae: 1.6126 - lr: 1.4775e-04 - 2s/epoch - 2ms/step\n",
            "Score for fold 2: loss of 19.759490966796875\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 15.4802 - mse: 15.4802 - mae: 1.5708 - val_loss: 11.8257 - val_mse: 11.8257 - val_mae: 1.5428 - lr: 1.4775e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 15.4577 - mse: 15.4577 - mae: 1.5699 - val_loss: 11.8898 - val_mse: 11.8898 - val_mae: 1.5034 - lr: 1.4775e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 15.4387 - mse: 15.4387 - mae: 1.5691 - val_loss: 11.8033 - val_mse: 11.8033 - val_mae: 1.5489 - lr: 1.4775e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 15.4239 - mse: 15.4239 - mae: 1.5684 - val_loss: 11.8686 - val_mse: 11.8686 - val_mae: 1.5567 - lr: 1.4775e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 15.4119 - mse: 15.4119 - mae: 1.5638 - val_loss: 11.8955 - val_mse: 11.8955 - val_mae: 1.5658 - lr: 1.4775e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 15.4178 - mse: 15.4178 - mae: 1.5677 - val_loss: 11.7789 - val_mse: 11.7789 - val_mae: 1.5468 - lr: 1.4775e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 15.4270 - mse: 15.4270 - mae: 1.5630 - val_loss: 11.8315 - val_mse: 11.8315 - val_mae: 1.5803 - lr: 1.4775e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 2s - loss: 15.3915 - mse: 15.3915 - mae: 1.5661 - val_loss: 11.8885 - val_mse: 11.8885 - val_mae: 1.5757 - lr: 1.4775e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 2s - loss: 15.4388 - mse: 15.4388 - mae: 1.5639 - val_loss: 11.8142 - val_mse: 11.8142 - val_mae: 1.5784 - lr: 1.4775e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 2s - loss: 15.4022 - mse: 15.4022 - mae: 1.5630 - val_loss: 11.8838 - val_mse: 11.8838 - val_mae: 1.5503 - lr: 1.4775e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 2s - loss: 15.4113 - mse: 15.4113 - mae: 1.5637 - val_loss: 11.9736 - val_mse: 11.9736 - val_mae: 1.5018 - lr: 1.4775e-04 - 2s/epoch - 2ms/step\n",
            "Score for fold 3: loss of 11.973583221435547\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 14.1744 - mse: 14.1744 - mae: 1.5565 - val_loss: 16.8841 - val_mse: 16.8841 - val_mae: 1.5858 - lr: 1.4775e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 14.1850 - mse: 14.1850 - mae: 1.5599 - val_loss: 16.8543 - val_mse: 16.8543 - val_mae: 1.5880 - lr: 1.4775e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 14.1357 - mse: 14.1357 - mae: 1.5564 - val_loss: 16.8766 - val_mse: 16.8766 - val_mae: 1.5833 - lr: 1.4775e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 14.1590 - mse: 14.1590 - mae: 1.5577 - val_loss: 16.9382 - val_mse: 16.9382 - val_mae: 1.5653 - lr: 1.4775e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 14.1402 - mse: 14.1402 - mae: 1.5565 - val_loss: 16.8839 - val_mse: 16.8839 - val_mae: 1.5729 - lr: 1.4775e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 14.1177 - mse: 14.1177 - mae: 1.5558 - val_loss: 16.9288 - val_mse: 16.9288 - val_mae: 1.5984 - lr: 1.4775e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 14.1326 - mse: 14.1326 - mae: 1.5565 - val_loss: 17.0891 - val_mse: 17.0891 - val_mae: 1.5234 - lr: 1.4775e-04 - 2s/epoch - 2ms/step\n",
            "Score for fold 4: loss of 17.089048385620117\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 15.1281 - mse: 15.1281 - mae: 1.5627 - val_loss: 12.9800 - val_mse: 12.9800 - val_mae: 1.5359 - lr: 1.4775e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 15.1416 - mse: 15.1416 - mae: 1.5582 - val_loss: 12.8395 - val_mse: 12.8395 - val_mae: 1.5593 - lr: 1.4775e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 15.1500 - mse: 15.1500 - mae: 1.5583 - val_loss: 13.0524 - val_mse: 13.0524 - val_mae: 1.5809 - lr: 1.4775e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 15.1439 - mse: 15.1439 - mae: 1.5581 - val_loss: 12.8773 - val_mse: 12.8773 - val_mae: 1.5498 - lr: 1.4775e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 15.1092 - mse: 15.1092 - mae: 1.5547 - val_loss: 12.8730 - val_mse: 12.8730 - val_mae: 1.6089 - lr: 1.4775e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 15.0969 - mse: 15.0969 - mae: 1.5583 - val_loss: 12.8942 - val_mse: 12.8942 - val_mae: 1.5711 - lr: 1.4775e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 15.1024 - mse: 15.1024 - mae: 1.5567 - val_loss: 13.1049 - val_mse: 13.1049 - val_mae: 1.5907 - lr: 1.4775e-04 - 2s/epoch - 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-10-14 21:58:19,337]\u001b[0m Finished trial#47 resulted in value: 14.85. Current best value is 14.724 with parameters: {'activation': 'relu', 'num_hidden_layer': 3, 'i': 6, 'learning_rate': 0.0023989159773794054}.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Score for fold 5: loss of 13.104900360107422\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 3s - loss: 16.5320 - mse: 16.5320 - mae: 1.6129 - val_loss: 12.9108 - val_mse: 12.9108 - val_mae: 1.6737 - lr: 1.4991e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 15.7824 - mse: 15.7824 - mae: 1.5708 - val_loss: 12.8251 - val_mse: 12.8251 - val_mae: 1.6030 - lr: 1.4991e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 15.7011 - mse: 15.7011 - mae: 1.5676 - val_loss: 12.9226 - val_mse: 12.9226 - val_mae: 1.5806 - lr: 1.4991e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 15.6451 - mse: 15.6451 - mae: 1.5698 - val_loss: 12.7380 - val_mse: 12.7380 - val_mae: 1.6407 - lr: 1.4991e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 15.5741 - mse: 15.5741 - mae: 1.5661 - val_loss: 12.7180 - val_mse: 12.7180 - val_mae: 1.6405 - lr: 1.4991e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 15.5372 - mse: 15.5372 - mae: 1.5694 - val_loss: 12.8286 - val_mse: 12.8286 - val_mae: 1.6318 - lr: 1.4991e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 15.5624 - mse: 15.5624 - mae: 1.5650 - val_loss: 12.8351 - val_mse: 12.8351 - val_mae: 1.5750 - lr: 1.4991e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 2s - loss: 15.5457 - mse: 15.5457 - mae: 1.5605 - val_loss: 12.7041 - val_mse: 12.7041 - val_mae: 1.6062 - lr: 1.4991e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 2s - loss: 15.4753 - mse: 15.4753 - mae: 1.5641 - val_loss: 12.6697 - val_mse: 12.6697 - val_mae: 1.6376 - lr: 1.4991e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 2s - loss: 15.4642 - mse: 15.4642 - mae: 1.5636 - val_loss: 12.7059 - val_mse: 12.7059 - val_mae: 1.5988 - lr: 1.4991e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 2s - loss: 15.4769 - mse: 15.4769 - mae: 1.5587 - val_loss: 12.6780 - val_mse: 12.6780 - val_mae: 1.6588 - lr: 1.4991e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 2s - loss: 15.4545 - mse: 15.4545 - mae: 1.5601 - val_loss: 12.6523 - val_mse: 12.6523 - val_mae: 1.6331 - lr: 1.4991e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 13/100\n",
            "1000/1000 - 2s - loss: 15.4038 - mse: 15.4038 - mae: 1.5572 - val_loss: 12.6502 - val_mse: 12.6502 - val_mae: 1.6754 - lr: 1.4991e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 14/100\n",
            "1000/1000 - 2s - loss: 15.4071 - mse: 15.4071 - mae: 1.5574 - val_loss: 12.7709 - val_mse: 12.7709 - val_mae: 1.5873 - lr: 1.4991e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 15/100\n",
            "1000/1000 - 2s - loss: 15.3597 - mse: 15.3597 - mae: 1.5587 - val_loss: 12.6048 - val_mse: 12.6048 - val_mae: 1.6267 - lr: 1.4991e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 16/100\n",
            "1000/1000 - 2s - loss: 15.3733 - mse: 15.3733 - mae: 1.5558 - val_loss: 12.7245 - val_mse: 12.7245 - val_mae: 1.5891 - lr: 1.4991e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 17/100\n",
            "1000/1000 - 2s - loss: 15.3943 - mse: 15.3943 - mae: 1.5528 - val_loss: 12.6436 - val_mse: 12.6436 - val_mae: 1.6328 - lr: 1.4991e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 18/100\n",
            "1000/1000 - 2s - loss: 15.3333 - mse: 15.3333 - mae: 1.5520 - val_loss: 12.6441 - val_mse: 12.6441 - val_mae: 1.6112 - lr: 1.4991e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 19/100\n",
            "1000/1000 - 2s - loss: 15.3248 - mse: 15.3248 - mae: 1.5504 - val_loss: 12.7403 - val_mse: 12.7403 - val_mae: 1.6101 - lr: 1.4991e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 20/100\n",
            "1000/1000 - 2s - loss: 15.3470 - mse: 15.3470 - mae: 1.5521 - val_loss: 12.7452 - val_mse: 12.7452 - val_mae: 1.5842 - lr: 1.4991e-04 - 2s/epoch - 2ms/step\n",
            "Score for fold 1: loss of 12.745149612426758\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 15.7747 - mse: 15.7747 - mae: 1.5772 - val_loss: 10.8975 - val_mse: 10.8975 - val_mae: 1.5108 - lr: 1.4991e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 15.7535 - mse: 15.7535 - mae: 1.5754 - val_loss: 10.9390 - val_mse: 10.9390 - val_mae: 1.5398 - lr: 1.4991e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 15.7556 - mse: 15.7556 - mae: 1.5756 - val_loss: 10.8728 - val_mse: 10.8728 - val_mae: 1.5436 - lr: 1.4991e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 15.7261 - mse: 15.7261 - mae: 1.5774 - val_loss: 10.9098 - val_mse: 10.9098 - val_mae: 1.5263 - lr: 1.4991e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 15.7049 - mse: 15.7049 - mae: 1.5723 - val_loss: 10.9493 - val_mse: 10.9493 - val_mae: 1.5876 - lr: 1.4991e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 15.7051 - mse: 15.7051 - mae: 1.5777 - val_loss: 10.9884 - val_mse: 10.9884 - val_mae: 1.5335 - lr: 1.4991e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 15.7106 - mse: 15.7106 - mae: 1.5742 - val_loss: 10.9804 - val_mse: 10.9804 - val_mae: 1.5024 - lr: 1.4991e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 2s - loss: 15.7196 - mse: 15.7196 - mae: 1.5728 - val_loss: 11.1804 - val_mse: 11.1804 - val_mae: 1.5136 - lr: 1.4991e-04 - 2s/epoch - 2ms/step\n",
            "Score for fold 2: loss of 11.180364608764648\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 11.6172 - mse: 11.6172 - mae: 1.5462 - val_loss: 27.2212 - val_mse: 27.2212 - val_mae: 1.6129 - lr: 1.4991e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 11.6070 - mse: 11.6070 - mae: 1.5415 - val_loss: 27.3251 - val_mse: 27.3251 - val_mae: 1.6097 - lr: 1.4991e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 11.5725 - mse: 11.5725 - mae: 1.5428 - val_loss: 27.3768 - val_mse: 27.3768 - val_mae: 1.6301 - lr: 1.4991e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 11.5890 - mse: 11.5890 - mae: 1.5411 - val_loss: 27.2133 - val_mse: 27.2133 - val_mae: 1.6451 - lr: 1.4991e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 11.5221 - mse: 11.5221 - mae: 1.5440 - val_loss: 27.5336 - val_mse: 27.5336 - val_mae: 1.5747 - lr: 1.4991e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 11.5550 - mse: 11.5550 - mae: 1.5428 - val_loss: 27.3310 - val_mse: 27.3310 - val_mae: 1.6072 - lr: 1.4991e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 11.5354 - mse: 11.5354 - mae: 1.5400 - val_loss: 27.2175 - val_mse: 27.2175 - val_mae: 1.6243 - lr: 1.4991e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 2s - loss: 11.5135 - mse: 11.5135 - mae: 1.5396 - val_loss: 27.4630 - val_mse: 27.4630 - val_mae: 1.6301 - lr: 1.4991e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 2s - loss: 11.5122 - mse: 11.5122 - mae: 1.5395 - val_loss: 27.2318 - val_mse: 27.2318 - val_mae: 1.6421 - lr: 1.4991e-04 - 2s/epoch - 2ms/step\n",
            "Score for fold 3: loss of 27.23178482055664\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 15.0734 - mse: 15.0734 - mae: 1.5599 - val_loss: 13.2388 - val_mse: 13.2388 - val_mae: 1.5571 - lr: 1.4991e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 15.0061 - mse: 15.0061 - mae: 1.5564 - val_loss: 13.4011 - val_mse: 13.4011 - val_mae: 1.5443 - lr: 1.4991e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 15.0406 - mse: 15.0406 - mae: 1.5549 - val_loss: 13.3316 - val_mse: 13.3316 - val_mae: 1.5429 - lr: 1.4991e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 15.0256 - mse: 15.0256 - mae: 1.5541 - val_loss: 13.3989 - val_mse: 13.3989 - val_mae: 1.5312 - lr: 1.4991e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 15.0294 - mse: 15.0294 - mae: 1.5559 - val_loss: 13.4127 - val_mse: 13.4127 - val_mae: 1.5969 - lr: 1.4991e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 14.9825 - mse: 14.9825 - mae: 1.5509 - val_loss: 13.4806 - val_mse: 13.4806 - val_mae: 1.5480 - lr: 1.4991e-04 - 2s/epoch - 2ms/step\n",
            "Score for fold 4: loss of 13.480589866638184\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 15.9653 - mse: 15.9653 - mae: 1.5792 - val_loss: 9.4651 - val_mse: 9.4651 - val_mae: 1.5235 - lr: 1.4991e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 15.8791 - mse: 15.8791 - mae: 1.5797 - val_loss: 9.6591 - val_mse: 9.6591 - val_mae: 1.4272 - lr: 1.4991e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 15.8741 - mse: 15.8741 - mae: 1.5763 - val_loss: 9.6392 - val_mse: 9.6392 - val_mae: 1.4353 - lr: 1.4991e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 15.8962 - mse: 15.8962 - mae: 1.5737 - val_loss: 9.5666 - val_mse: 9.5666 - val_mae: 1.4600 - lr: 1.4991e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 15.8920 - mse: 15.8920 - mae: 1.5729 - val_loss: 9.5756 - val_mse: 9.5756 - val_mae: 1.5098 - lr: 1.4991e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 15.8734 - mse: 15.8734 - mae: 1.5701 - val_loss: 9.6834 - val_mse: 9.6834 - val_mae: 1.5608 - lr: 1.4991e-04 - 2s/epoch - 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-10-14 22:00:22,727]\u001b[0m Finished trial#48 resulted in value: 14.863999999999999. Current best value is 14.724 with parameters: {'activation': 'relu', 'num_hidden_layer': 3, 'i': 6, 'learning_rate': 0.0023989159773794054}.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Score for fold 5: loss of 9.683448791503906\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 16.3926 - mse: 16.3926 - mae: 1.6314 - val_loss: 11.2192 - val_mse: 11.2192 - val_mae: 1.5592 - lr: 0.0027 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 16.1855 - mse: 16.1855 - mae: 1.6141 - val_loss: 11.3460 - val_mse: 11.3460 - val_mae: 1.5422 - lr: 0.0027 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 16.0401 - mse: 16.0401 - mae: 1.6087 - val_loss: 11.2456 - val_mse: 11.2456 - val_mae: 1.5657 - lr: 0.0027 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 16.0770 - mse: 16.0770 - mae: 1.6046 - val_loss: 11.1208 - val_mse: 11.1208 - val_mae: 1.5693 - lr: 0.0027 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 16.0197 - mse: 16.0197 - mae: 1.6002 - val_loss: 11.1580 - val_mse: 11.1580 - val_mae: 1.6108 - lr: 0.0027 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 16.0012 - mse: 16.0012 - mae: 1.6050 - val_loss: 11.2611 - val_mse: 11.2611 - val_mae: 1.5520 - lr: 0.0027 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 15.9966 - mse: 15.9966 - mae: 1.6082 - val_loss: 11.1393 - val_mse: 11.1393 - val_mae: 1.5223 - lr: 0.0027 - 2s/epoch - 2ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 2s - loss: 16.0542 - mse: 16.0542 - mae: 1.5991 - val_loss: 11.2400 - val_mse: 11.2400 - val_mae: 1.5982 - lr: 0.0027 - 2s/epoch - 2ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 2s - loss: 16.0198 - mse: 16.0198 - mae: 1.5994 - val_loss: 11.3079 - val_mse: 11.3079 - val_mae: 1.4847 - lr: 0.0027 - 2s/epoch - 2ms/step\n",
            "Score for fold 1: loss of 11.30789566040039\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 15.7590 - mse: 15.7590 - mae: 1.5821 - val_loss: 11.3901 - val_mse: 11.3901 - val_mae: 1.5621 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 15.7245 - mse: 15.7245 - mae: 1.5760 - val_loss: 11.4551 - val_mse: 11.4551 - val_mae: 1.5490 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 15.6381 - mse: 15.6381 - mae: 1.5765 - val_loss: 11.4571 - val_mse: 11.4571 - val_mae: 1.5719 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 15.6714 - mse: 15.6714 - mae: 1.5745 - val_loss: 11.5466 - val_mse: 11.5466 - val_mae: 1.5364 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 15.5765 - mse: 15.5765 - mae: 1.5739 - val_loss: 11.3811 - val_mse: 11.3811 - val_mae: 1.6032 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 15.6015 - mse: 15.6015 - mae: 1.5708 - val_loss: 11.5370 - val_mse: 11.5370 - val_mae: 1.5402 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 15.6326 - mse: 15.6326 - mae: 1.5708 - val_loss: 11.3312 - val_mse: 11.3312 - val_mae: 1.5566 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 2s - loss: 15.5640 - mse: 15.5640 - mae: 1.5728 - val_loss: 11.4523 - val_mse: 11.4523 - val_mae: 1.5228 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 2s - loss: 15.4999 - mse: 15.4999 - mae: 1.5687 - val_loss: 11.4138 - val_mse: 11.4138 - val_mae: 1.5371 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 2s - loss: 15.4547 - mse: 15.4547 - mae: 1.5716 - val_loss: 11.4357 - val_mse: 11.4357 - val_mae: 1.5816 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 2s - loss: 15.4572 - mse: 15.4572 - mae: 1.5709 - val_loss: 11.4949 - val_mse: 11.4949 - val_mae: 1.5516 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 2s - loss: 15.4767 - mse: 15.4767 - mae: 1.5707 - val_loss: 11.5087 - val_mse: 11.5087 - val_mae: 1.5414 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Score for fold 2: loss of 11.508662223815918\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 12.8108 - mse: 12.8108 - mae: 1.5623 - val_loss: 21.9146 - val_mse: 21.9146 - val_mae: 1.5597 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 12.9068 - mse: 12.9068 - mae: 1.5596 - val_loss: 21.8461 - val_mse: 21.8461 - val_mae: 1.5610 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 12.8417 - mse: 12.8417 - mae: 1.5597 - val_loss: 21.8213 - val_mse: 21.8213 - val_mae: 1.6080 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 12.8000 - mse: 12.8000 - mae: 1.5601 - val_loss: 21.9653 - val_mse: 21.9653 - val_mae: 1.5566 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 12.8764 - mse: 12.8764 - mae: 1.5595 - val_loss: 21.8743 - val_mse: 21.8743 - val_mae: 1.6220 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 12.8686 - mse: 12.8686 - mae: 1.5602 - val_loss: 22.1040 - val_mse: 22.1040 - val_mae: 1.6143 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 12.8779 - mse: 12.8779 - mae: 1.5659 - val_loss: 21.9802 - val_mse: 21.9802 - val_mae: 1.5836 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 2s - loss: 12.7990 - mse: 12.7990 - mae: 1.5632 - val_loss: 22.2796 - val_mse: 22.2796 - val_mae: 1.5610 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Score for fold 3: loss of 22.27960968017578\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 15.2038 - mse: 15.2038 - mae: 1.5676 - val_loss: 12.7466 - val_mse: 12.7466 - val_mae: 1.5682 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 15.1825 - mse: 15.1825 - mae: 1.5661 - val_loss: 12.7329 - val_mse: 12.7329 - val_mae: 1.5924 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 15.0965 - mse: 15.0965 - mae: 1.5722 - val_loss: 12.7860 - val_mse: 12.7860 - val_mae: 1.6176 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 15.1456 - mse: 15.1456 - mae: 1.5658 - val_loss: 12.7540 - val_mse: 12.7540 - val_mae: 1.5975 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 15.1394 - mse: 15.1394 - mae: 1.5610 - val_loss: 12.9668 - val_mse: 12.9668 - val_mae: 1.5697 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 15.0095 - mse: 15.0095 - mae: 1.5599 - val_loss: 12.9268 - val_mse: 12.9268 - val_mae: 1.5481 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 15.0371 - mse: 15.0371 - mae: 1.5578 - val_loss: 13.0648 - val_mse: 13.0648 - val_mae: 1.5553 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Score for fold 4: loss of 13.064821243286133\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 14.1533 - mse: 14.1533 - mae: 1.5538 - val_loss: 16.1147 - val_mse: 16.1147 - val_mae: 1.6154 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 14.1213 - mse: 14.1213 - mae: 1.5550 - val_loss: 17.1383 - val_mse: 17.1383 - val_mae: 1.5907 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 14.1376 - mse: 14.1376 - mae: 1.5527 - val_loss: 16.3343 - val_mse: 16.3343 - val_mae: 1.6015 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 14.0937 - mse: 14.0937 - mae: 1.5545 - val_loss: 16.4263 - val_mse: 16.4263 - val_mae: 1.5481 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 14.0680 - mse: 14.0680 - mae: 1.5498 - val_loss: 16.4759 - val_mse: 16.4759 - val_mae: 1.6259 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 14.0544 - mse: 14.0544 - mae: 1.5483 - val_loss: 16.3998 - val_mse: 16.3998 - val_mae: 1.5953 - lr: 0.0010 - 2s/epoch - 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-10-14 22:01:44,761]\u001b[0m Finished trial#49 resulted in value: 14.912. Current best value is 14.724 with parameters: {'activation': 'relu', 'num_hidden_layer': 3, 'i': 6, 'learning_rate': 0.0023989159773794054}.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Score for fold 5: loss of 16.399837493896484\n"
          ]
        }
      ],
      "source": [
        "model_list=[]\n",
        "history_list=[]\n",
        "study_name = 'NN_study_3'\n",
        "study = optuna.create_study(study_name=study_name, load_if_exists=True)\n",
        "func = lambda trial: objective(trial,training_shuffled2,labelsForTrain_shuffled2) \n",
        "study.optimize(func, n_trials=50,)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UqBj5WCO_2P6",
        "outputId": "becfca02-3bbe-4e32-ab50-a3316018e729"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "1250/1250 - 2s - loss: 15.3970 - mse: 15.3970 - mae: 1.6125 - val_loss: 13.7548 - val_mse: 13.7548 - val_mae: 1.5799 - lr: 0.0024 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1250/1250 - 2s - loss: 15.1364 - mse: 15.1364 - mae: 1.5975 - val_loss: 13.3447 - val_mse: 13.3447 - val_mae: 1.6615 - lr: 0.0024 - 2s/epoch - 1ms/step\n",
            "Epoch 3/100\n",
            "1250/1250 - 2s - loss: 15.1007 - mse: 15.1007 - mae: 1.5920 - val_loss: 13.3369 - val_mse: 13.3369 - val_mae: 1.5922 - lr: 0.0024 - 2s/epoch - 1ms/step\n",
            "Epoch 4/100\n",
            "1250/1250 - 2s - loss: 15.0337 - mse: 15.0337 - mae: 1.5879 - val_loss: 13.3207 - val_mse: 13.3207 - val_mae: 1.6769 - lr: 0.0024 - 2s/epoch - 1ms/step\n",
            "Epoch 5/100\n",
            "1250/1250 - 2s - loss: 15.0871 - mse: 15.0871 - mae: 1.5891 - val_loss: 13.2484 - val_mse: 13.2484 - val_mae: 1.6134 - lr: 0.0024 - 2s/epoch - 1ms/step\n",
            "Epoch 6/100\n",
            "1250/1250 - 2s - loss: 15.0044 - mse: 15.0044 - mae: 1.5944 - val_loss: 13.5017 - val_mse: 13.5017 - val_mae: 1.5716 - lr: 0.0024 - 2s/epoch - 1ms/step\n",
            "Epoch 7/100\n",
            "1250/1250 - 2s - loss: 14.9788 - mse: 14.9788 - mae: 1.5879 - val_loss: 13.6388 - val_mse: 13.6388 - val_mae: 1.5879 - lr: 0.0024 - 2s/epoch - 1ms/step\n",
            "Epoch 8/100\n",
            "1250/1250 - 2s - loss: 14.9595 - mse: 14.9595 - mae: 1.5823 - val_loss: 13.1516 - val_mse: 13.1516 - val_mae: 1.5883 - lr: 0.0024 - 2s/epoch - 1ms/step\n",
            "Epoch 9/100\n",
            "1250/1250 - 2s - loss: 14.9431 - mse: 14.9431 - mae: 1.5842 - val_loss: 13.1738 - val_mse: 13.1738 - val_mae: 1.6421 - lr: 0.0024 - 2s/epoch - 1ms/step\n",
            "Epoch 10/100\n",
            "1250/1250 - 2s - loss: 14.9387 - mse: 14.9387 - mae: 1.5794 - val_loss: 13.3896 - val_mse: 13.3896 - val_mae: 1.5723 - lr: 0.0024 - 2s/epoch - 1ms/step\n",
            "Epoch 11/100\n",
            "1250/1250 - 2s - loss: 14.8864 - mse: 14.8864 - mae: 1.5789 - val_loss: 13.5289 - val_mse: 13.5289 - val_mae: 1.6142 - lr: 0.0024 - 2s/epoch - 1ms/step\n",
            "Epoch 12/100\n",
            "1250/1250 - 2s - loss: 14.9290 - mse: 14.9290 - mae: 1.5765 - val_loss: 13.5495 - val_mse: 13.5495 - val_mae: 1.5505 - lr: 0.0024 - 2s/epoch - 1ms/step\n",
            "Epoch 13/100\n",
            "1250/1250 - 2s - loss: 14.9664 - mse: 14.9664 - mae: 1.5797 - val_loss: 13.2750 - val_mse: 13.2750 - val_mae: 1.5749 - lr: 0.0024 - 2s/epoch - 1ms/step\n",
            "Epoch 14/100\n",
            "1250/1250 - 2s - loss: 14.9193 - mse: 14.9193 - mae: 1.5814 - val_loss: 13.4647 - val_mse: 13.4647 - val_mae: 1.6348 - lr: 0.0024 - 2s/epoch - 1ms/step\n",
            "Epoch 15/100\n",
            "1250/1250 - 2s - loss: 14.9491 - mse: 14.9491 - mae: 1.5805 - val_loss: 13.4859 - val_mse: 13.4859 - val_mae: 1.5821 - lr: 0.0024 - 2s/epoch - 1ms/step\n",
            "Epoch 16/100\n",
            "1250/1250 - 2s - loss: 14.9684 - mse: 14.9684 - mae: 1.5815 - val_loss: 13.5169 - val_mse: 13.5169 - val_mae: 1.5909 - lr: 0.0024 - 2s/epoch - 1ms/step\n",
            "Epoch 17/100\n",
            "1250/1250 - 2s - loss: 14.9467 - mse: 14.9467 - mae: 1.5818 - val_loss: 13.4388 - val_mse: 13.4388 - val_mae: 1.5953 - lr: 0.0024 - 2s/epoch - 1ms/step\n",
            "Epoch 18/100\n",
            "1250/1250 - 2s - loss: 14.9118 - mse: 14.9118 - mae: 1.5808 - val_loss: 13.4380 - val_mse: 13.4380 - val_mae: 1.6398 - lr: 0.0024 - 2s/epoch - 1ms/step\n",
            "Epoch 19/100\n",
            "1250/1250 - 2s - loss: 14.8999 - mse: 14.8999 - mae: 1.5795 - val_loss: 13.3827 - val_mse: 13.3827 - val_mae: 1.5878 - lr: 0.0024 - 2s/epoch - 1ms/step\n",
            "Epoch 20/100\n",
            "1250/1250 - 2s - loss: 14.8345 - mse: 14.8345 - mae: 1.5814 - val_loss: 13.4821 - val_mse: 13.4821 - val_mae: 1.6236 - lr: 0.0024 - 2s/epoch - 1ms/step\n",
            "Epoch 21/100\n",
            "1250/1250 - 2s - loss: 14.9274 - mse: 14.9274 - mae: 1.5833 - val_loss: 13.3769 - val_mse: 13.3769 - val_mae: 1.6496 - lr: 0.0024 - 2s/epoch - 1ms/step\n",
            "Epoch 22/100\n",
            "1250/1250 - 2s - loss: 14.9144 - mse: 14.9144 - mae: 1.5827 - val_loss: 13.2740 - val_mse: 13.2740 - val_mae: 1.6076 - lr: 0.0024 - 2s/epoch - 1ms/step\n",
            "Epoch 23/100\n",
            "1250/1250 - 2s - loss: 14.9195 - mse: 14.9195 - mae: 1.5849 - val_loss: 13.5676 - val_mse: 13.5676 - val_mae: 1.6306 - lr: 0.0024 - 2s/epoch - 1ms/step\n",
            "Epoch 24/100\n",
            "1250/1250 - 2s - loss: 14.8339 - mse: 14.8339 - mae: 1.5836 - val_loss: 13.5518 - val_mse: 13.5518 - val_mae: 1.6108 - lr: 0.0024 - 2s/epoch - 1ms/step\n",
            "Epoch 25/100\n",
            "1250/1250 - 2s - loss: 14.8853 - mse: 14.8853 - mae: 1.5876 - val_loss: 13.3075 - val_mse: 13.3075 - val_mae: 1.6439 - lr: 0.0024 - 2s/epoch - 1ms/step\n",
            "Epoch 26/100\n",
            "1250/1250 - 2s - loss: 14.8191 - mse: 14.8191 - mae: 1.5848 - val_loss: 13.3995 - val_mse: 13.3995 - val_mae: 1.5670 - lr: 0.0024 - 2s/epoch - 1ms/step\n",
            "Epoch 27/100\n",
            "1250/1250 - 2s - loss: 14.8722 - mse: 14.8722 - mae: 1.5846 - val_loss: 13.3864 - val_mse: 13.3864 - val_mae: 1.6201 - lr: 0.0024 - 2s/epoch - 1ms/step\n",
            "Epoch 28/100\n",
            "1250/1250 - 2s - loss: 14.9210 - mse: 14.9210 - mae: 1.5865 - val_loss: 13.2454 - val_mse: 13.2454 - val_mae: 1.6678 - lr: 0.0024 - 2s/epoch - 1ms/step\n",
            "Epoch 29/100\n",
            "1250/1250 - 2s - loss: 14.6741 - mse: 14.6741 - mae: 1.5737 - val_loss: 13.3821 - val_mse: 13.3821 - val_mae: 1.6131 - lr: 0.0010 - 2s/epoch - 1ms/step\n",
            "Epoch 30/100\n",
            "1250/1250 - 2s - loss: 14.6958 - mse: 14.6958 - mae: 1.5690 - val_loss: 13.2221 - val_mse: 13.2221 - val_mae: 1.6181 - lr: 0.0010 - 2s/epoch - 1ms/step\n",
            "Epoch 31/100\n",
            "1250/1250 - 2s - loss: 14.6440 - mse: 14.6440 - mae: 1.5701 - val_loss: 13.3734 - val_mse: 13.3734 - val_mae: 1.5966 - lr: 0.0010 - 2s/epoch - 1ms/step\n",
            "Epoch 32/100\n",
            "1250/1250 - 2s - loss: 14.6329 - mse: 14.6329 - mae: 1.5722 - val_loss: 13.3604 - val_mse: 13.3604 - val_mae: 1.6083 - lr: 0.0010 - 2s/epoch - 1ms/step\n",
            "Epoch 33/100\n",
            "1250/1250 - 2s - loss: 14.6538 - mse: 14.6538 - mae: 1.5701 - val_loss: 13.3160 - val_mse: 13.3160 - val_mae: 1.5933 - lr: 0.0010 - 2s/epoch - 1ms/step\n",
            "Epoch 34/100\n",
            "1250/1250 - 2s - loss: 14.6377 - mse: 14.6377 - mae: 1.5698 - val_loss: 13.3046 - val_mse: 13.3046 - val_mae: 1.5779 - lr: 0.0010 - 2s/epoch - 1ms/step\n",
            "Epoch 35/100\n",
            "1250/1250 - 2s - loss: 14.6191 - mse: 14.6191 - mae: 1.5658 - val_loss: 13.3574 - val_mse: 13.3574 - val_mae: 1.5930 - lr: 0.0010 - 2s/epoch - 1ms/step\n",
            "Epoch 36/100\n",
            "1250/1250 - 2s - loss: 14.6521 - mse: 14.6521 - mae: 1.5697 - val_loss: 13.3965 - val_mse: 13.3965 - val_mae: 1.5862 - lr: 0.0010 - 2s/epoch - 1ms/step\n",
            "Epoch 37/100\n",
            "1250/1250 - 2s - loss: 14.6185 - mse: 14.6185 - mae: 1.5691 - val_loss: 13.2767 - val_mse: 13.2767 - val_mae: 1.5979 - lr: 0.0010 - 2s/epoch - 1ms/step\n",
            "Epoch 38/100\n",
            "1250/1250 - 2s - loss: 14.6172 - mse: 14.6172 - mae: 1.5708 - val_loss: 13.3153 - val_mse: 13.3153 - val_mae: 1.5823 - lr: 0.0010 - 2s/epoch - 1ms/step\n",
            "Epoch 39/100\n",
            "1250/1250 - 2s - loss: 14.5986 - mse: 14.5986 - mae: 1.5737 - val_loss: 13.2969 - val_mse: 13.2969 - val_mae: 1.6220 - lr: 0.0010 - 2s/epoch - 1ms/step\n",
            "Epoch 40/100\n",
            "1250/1250 - 2s - loss: 14.6248 - mse: 14.6248 - mae: 1.5722 - val_loss: 13.3603 - val_mse: 13.3603 - val_mae: 1.5507 - lr: 0.0010 - 2s/epoch - 1ms/step\n",
            "Epoch 41/100\n",
            "1250/1250 - 2s - loss: 14.5783 - mse: 14.5783 - mae: 1.5663 - val_loss: 13.2816 - val_mse: 13.2816 - val_mae: 1.6384 - lr: 0.0010 - 2s/epoch - 1ms/step\n",
            "Epoch 42/100\n",
            "1250/1250 - 2s - loss: 14.5831 - mse: 14.5831 - mae: 1.5682 - val_loss: 13.2468 - val_mse: 13.2468 - val_mae: 1.6359 - lr: 0.0010 - 2s/epoch - 1ms/step\n",
            "Epoch 43/100\n",
            "1250/1250 - 2s - loss: 14.5967 - mse: 14.5967 - mae: 1.5698 - val_loss: 13.3442 - val_mse: 13.3442 - val_mae: 1.5646 - lr: 0.0010 - 2s/epoch - 1ms/step\n",
            "Epoch 44/100\n",
            "1250/1250 - 2s - loss: 14.6353 - mse: 14.6353 - mae: 1.5685 - val_loss: 13.2519 - val_mse: 13.2519 - val_mae: 1.6146 - lr: 0.0010 - 2s/epoch - 1ms/step\n",
            "Epoch 45/100\n",
            "1250/1250 - 2s - loss: 14.6164 - mse: 14.6164 - mae: 1.5683 - val_loss: 13.3796 - val_mse: 13.3796 - val_mae: 1.6225 - lr: 0.0010 - 2s/epoch - 1ms/step\n",
            "Epoch 46/100\n",
            "1250/1250 - 2s - loss: 14.5423 - mse: 14.5423 - mae: 1.5735 - val_loss: 13.3052 - val_mse: 13.3052 - val_mae: 1.6294 - lr: 0.0010 - 2s/epoch - 1ms/step\n",
            "Epoch 47/100\n",
            "1250/1250 - 2s - loss: 14.5100 - mse: 14.5100 - mae: 1.5674 - val_loss: 13.2740 - val_mse: 13.2740 - val_mae: 1.6444 - lr: 0.0010 - 2s/epoch - 1ms/step\n",
            "Epoch 48/100\n",
            "1250/1250 - 2s - loss: 14.5313 - mse: 14.5313 - mae: 1.5704 - val_loss: 13.3785 - val_mse: 13.3785 - val_mae: 1.5847 - lr: 0.0010 - 2s/epoch - 1ms/step\n",
            "Epoch 49/100\n",
            "1250/1250 - 2s - loss: 14.4920 - mse: 14.4920 - mae: 1.5695 - val_loss: 13.4818 - val_mse: 13.4818 - val_mae: 1.5756 - lr: 0.0010 - 2s/epoch - 1ms/step\n",
            "Epoch 50/100\n",
            "1250/1250 - 2s - loss: 14.5622 - mse: 14.5622 - mae: 1.5660 - val_loss: 13.3717 - val_mse: 13.3717 - val_mae: 1.5809 - lr: 0.0010 - 2s/epoch - 1ms/step\n",
            "Epoch 51/100\n",
            "1250/1250 - 2s - loss: 14.5391 - mse: 14.5391 - mae: 1.5742 - val_loss: 13.3858 - val_mse: 13.3858 - val_mae: 1.6265 - lr: 0.0010 - 2s/epoch - 1ms/step\n",
            "Epoch 52/100\n",
            "1250/1250 - 2s - loss: 14.5066 - mse: 14.5066 - mae: 1.5710 - val_loss: 13.5809 - val_mse: 13.5809 - val_mae: 1.5834 - lr: 0.0010 - 2s/epoch - 1ms/step\n",
            "Epoch 53/100\n",
            "1250/1250 - 2s - loss: 14.5149 - mse: 14.5149 - mae: 1.5714 - val_loss: 13.3352 - val_mse: 13.3352 - val_mae: 1.6076 - lr: 0.0010 - 2s/epoch - 1ms/step\n",
            "Epoch 54/100\n",
            "1250/1250 - 2s - loss: 14.5272 - mse: 14.5272 - mae: 1.5691 - val_loss: 13.3875 - val_mse: 13.3875 - val_mae: 1.6028 - lr: 0.0010 - 2s/epoch - 1ms/step\n",
            "Epoch 55/100\n",
            "1250/1250 - 2s - loss: 14.4865 - mse: 14.4865 - mae: 1.5708 - val_loss: 13.3591 - val_mse: 13.3591 - val_mae: 1.5937 - lr: 0.0010 - 2s/epoch - 1ms/step\n",
            "Epoch 56/100\n",
            "1250/1250 - 2s - loss: 14.5334 - mse: 14.5334 - mae: 1.5693 - val_loss: 13.6592 - val_mse: 13.6592 - val_mae: 1.6601 - lr: 0.0010 - 2s/epoch - 1ms/step\n",
            "Epoch 57/100\n",
            "1250/1250 - 2s - loss: 14.4194 - mse: 14.4194 - mae: 1.5671 - val_loss: 13.3409 - val_mse: 13.3409 - val_mae: 1.6047 - lr: 0.0010 - 2s/epoch - 1ms/step\n",
            "Epoch 58/100\n",
            "1250/1250 - 2s - loss: 14.4976 - mse: 14.4976 - mae: 1.5675 - val_loss: 13.8483 - val_mse: 13.8483 - val_mae: 1.6034 - lr: 0.0010 - 2s/epoch - 1ms/step\n",
            "Epoch 59/100\n",
            "1250/1250 - 2s - loss: 14.4336 - mse: 14.4336 - mae: 1.5681 - val_loss: 13.4636 - val_mse: 13.4636 - val_mae: 1.5562 - lr: 0.0010 - 2s/epoch - 1ms/step\n",
            "Epoch 60/100\n",
            "1250/1250 - 2s - loss: 14.4414 - mse: 14.4414 - mae: 1.5640 - val_loss: 13.3057 - val_mse: 13.3057 - val_mae: 1.6155 - lr: 0.0010 - 2s/epoch - 1ms/step\n",
            "Epoch 61/100\n",
            "1250/1250 - 2s - loss: 14.4570 - mse: 14.4570 - mae: 1.5633 - val_loss: 13.4172 - val_mse: 13.4172 - val_mae: 1.5892 - lr: 0.0010 - 2s/epoch - 1ms/step\n",
            "Epoch 62/100\n",
            "1250/1250 - 2s - loss: 14.3871 - mse: 14.3871 - mae: 1.5672 - val_loss: 13.3203 - val_mse: 13.3203 - val_mae: 1.6529 - lr: 0.0010 - 2s/epoch - 1ms/step\n",
            "Epoch 63/100\n",
            "1250/1250 - 2s - loss: 14.3985 - mse: 14.3985 - mae: 1.5642 - val_loss: 13.4825 - val_mse: 13.4825 - val_mae: 1.5742 - lr: 0.0010 - 2s/epoch - 1ms/step\n",
            "Epoch 64/100\n",
            "1250/1250 - 2s - loss: 14.4502 - mse: 14.4502 - mae: 1.5668 - val_loss: 13.5788 - val_mse: 13.5788 - val_mae: 1.5655 - lr: 0.0010 - 2s/epoch - 1ms/step\n",
            "Epoch 65/100\n",
            "1250/1250 - 2s - loss: 14.4643 - mse: 14.4643 - mae: 1.5666 - val_loss: 13.5791 - val_mse: 13.5791 - val_mae: 1.6075 - lr: 0.0010 - 2s/epoch - 1ms/step\n",
            "Epoch 66/100\n",
            "1250/1250 - 2s - loss: 14.4189 - mse: 14.4189 - mae: 1.5619 - val_loss: 13.4656 - val_mse: 13.4656 - val_mae: 1.6669 - lr: 0.0010 - 2s/epoch - 1ms/step\n",
            "Epoch 67/100\n",
            "1250/1250 - 2s - loss: 14.3839 - mse: 14.3839 - mae: 1.5679 - val_loss: 13.9095 - val_mse: 13.9095 - val_mae: 1.6595 - lr: 0.0010 - 2s/epoch - 1ms/step\n",
            "Epoch 68/100\n",
            "1250/1250 - 2s - loss: 14.4278 - mse: 14.4278 - mae: 1.5662 - val_loss: 13.7272 - val_mse: 13.7272 - val_mae: 1.6625 - lr: 0.0010 - 2s/epoch - 1ms/step\n",
            "Epoch 69/100\n",
            "1250/1250 - 2s - loss: 14.3125 - mse: 14.3125 - mae: 1.5607 - val_loss: 13.9169 - val_mse: 13.9169 - val_mae: 1.6220 - lr: 0.0010 - 2s/epoch - 1ms/step\n",
            "Epoch 70/100\n",
            "1250/1250 - 2s - loss: 14.3857 - mse: 14.3857 - mae: 1.5680 - val_loss: 13.5358 - val_mse: 13.5358 - val_mae: 1.6403 - lr: 0.0010 - 2s/epoch - 1ms/step\n",
            "Epoch 71/100\n",
            "1250/1250 - 2s - loss: 14.3251 - mse: 14.3251 - mae: 1.5631 - val_loss: 13.7246 - val_mse: 13.7246 - val_mae: 1.7148 - lr: 0.0010 - 2s/epoch - 1ms/step\n",
            "Epoch 72/100\n",
            "1250/1250 - 2s - loss: 14.2803 - mse: 14.2803 - mae: 1.5690 - val_loss: 13.6355 - val_mse: 13.6355 - val_mae: 1.6353 - lr: 0.0010 - 2s/epoch - 1ms/step\n",
            "Epoch 73/100\n",
            "1250/1250 - 2s - loss: 14.2913 - mse: 14.2913 - mae: 1.5670 - val_loss: 13.4042 - val_mse: 13.4042 - val_mae: 1.6360 - lr: 0.0010 - 2s/epoch - 1ms/step\n",
            "Epoch 74/100\n",
            "1250/1250 - 2s - loss: 14.3888 - mse: 14.3888 - mae: 1.5668 - val_loss: 13.8159 - val_mse: 13.8159 - val_mae: 1.5414 - lr: 0.0010 - 2s/epoch - 1ms/step\n",
            "Epoch 75/100\n",
            "1250/1250 - 2s - loss: 14.3513 - mse: 14.3513 - mae: 1.5683 - val_loss: 13.9638 - val_mse: 13.9638 - val_mae: 1.5178 - lr: 0.0010 - 2s/epoch - 1ms/step\n",
            "Epoch 76/100\n",
            "1250/1250 - 2s - loss: 14.2259 - mse: 14.2259 - mae: 1.5649 - val_loss: 13.8640 - val_mse: 13.8640 - val_mae: 1.6163 - lr: 0.0010 - 2s/epoch - 1ms/step\n",
            "Epoch 77/100\n",
            "1250/1250 - 2s - loss: 14.2852 - mse: 14.2852 - mae: 1.5629 - val_loss: 13.9563 - val_mse: 13.9563 - val_mae: 1.6413 - lr: 0.0010 - 2s/epoch - 1ms/step\n",
            "Epoch 78/100\n",
            "1250/1250 - 2s - loss: 14.2092 - mse: 14.2092 - mae: 1.5588 - val_loss: 13.7001 - val_mse: 13.7001 - val_mae: 1.5798 - lr: 0.0010 - 2s/epoch - 1ms/step\n",
            "Epoch 79/100\n",
            "1250/1250 - 2s - loss: 14.3103 - mse: 14.3103 - mae: 1.5681 - val_loss: 14.0182 - val_mse: 14.0182 - val_mae: 1.6512 - lr: 0.0010 - 2s/epoch - 1ms/step\n",
            "Epoch 80/100\n",
            "1250/1250 - 2s - loss: 14.2936 - mse: 14.2936 - mae: 1.5604 - val_loss: 13.8317 - val_mse: 13.8317 - val_mae: 1.5951 - lr: 0.0010 - 2s/epoch - 1ms/step\n",
            "Epoch 81/100\n",
            "1250/1250 - 2s - loss: 14.2537 - mse: 14.2537 - mae: 1.5614 - val_loss: 13.7896 - val_mse: 13.7896 - val_mae: 1.5460 - lr: 0.0010 - 2s/epoch - 1ms/step\n",
            "Epoch 82/100\n",
            "1250/1250 - 2s - loss: 14.3073 - mse: 14.3073 - mae: 1.5647 - val_loss: 13.8426 - val_mse: 13.8426 - val_mae: 1.6067 - lr: 0.0010 - 2s/epoch - 1ms/step\n",
            "Epoch 83/100\n",
            "1250/1250 - 2s - loss: 14.2601 - mse: 14.2601 - mae: 1.5661 - val_loss: 13.7492 - val_mse: 13.7492 - val_mae: 1.6866 - lr: 0.0010 - 2s/epoch - 1ms/step\n",
            "Epoch 84/100\n",
            "1250/1250 - 2s - loss: 14.3254 - mse: 14.3254 - mae: 1.5626 - val_loss: 13.7661 - val_mse: 13.7661 - val_mae: 1.6323 - lr: 0.0010 - 2s/epoch - 1ms/step\n",
            "Epoch 85/100\n",
            "1250/1250 - 2s - loss: 14.2810 - mse: 14.2810 - mae: 1.5644 - val_loss: 13.8558 - val_mse: 13.8558 - val_mae: 1.7245 - lr: 0.0010 - 2s/epoch - 1ms/step\n",
            "Epoch 86/100\n",
            "1250/1250 - 2s - loss: 14.3020 - mse: 14.3020 - mae: 1.5620 - val_loss: 13.6801 - val_mse: 13.6801 - val_mae: 1.6807 - lr: 0.0010 - 2s/epoch - 1ms/step\n",
            "Epoch 87/100\n",
            "1250/1250 - 2s - loss: 14.2041 - mse: 14.2041 - mae: 1.5592 - val_loss: 13.7936 - val_mse: 13.7936 - val_mae: 1.6749 - lr: 0.0010 - 2s/epoch - 1ms/step\n",
            "Epoch 88/100\n",
            "1250/1250 - 2s - loss: 14.2202 - mse: 14.2202 - mae: 1.5679 - val_loss: 13.8183 - val_mse: 13.8183 - val_mae: 1.6469 - lr: 0.0010 - 2s/epoch - 1ms/step\n",
            "Epoch 89/100\n",
            "1250/1250 - 2s - loss: 14.2168 - mse: 14.2168 - mae: 1.5600 - val_loss: 13.6906 - val_mse: 13.6906 - val_mae: 1.6874 - lr: 0.0010 - 2s/epoch - 1ms/step\n",
            "Epoch 90/100\n",
            "1250/1250 - 2s - loss: 14.3308 - mse: 14.3308 - mae: 1.5608 - val_loss: 13.8573 - val_mse: 13.8573 - val_mae: 1.5869 - lr: 0.0010 - 2s/epoch - 1ms/step\n",
            "Epoch 91/100\n",
            "1250/1250 - 2s - loss: 14.3979 - mse: 14.3979 - mae: 1.5690 - val_loss: 13.8756 - val_mse: 13.8756 - val_mae: 1.6148 - lr: 0.0010 - 2s/epoch - 1ms/step\n",
            "Epoch 92/100\n",
            "1250/1250 - 2s - loss: 14.3134 - mse: 14.3134 - mae: 1.5663 - val_loss: 13.7566 - val_mse: 13.7566 - val_mae: 1.6101 - lr: 0.0010 - 2s/epoch - 1ms/step\n",
            "Epoch 93/100\n",
            "1250/1250 - 2s - loss: 14.2749 - mse: 14.2749 - mae: 1.5636 - val_loss: 14.0775 - val_mse: 14.0775 - val_mae: 1.5811 - lr: 0.0010 - 2s/epoch - 1ms/step\n",
            "Epoch 94/100\n",
            "1250/1250 - 2s - loss: 14.3073 - mse: 14.3073 - mae: 1.5654 - val_loss: 13.7674 - val_mse: 13.7674 - val_mae: 1.6442 - lr: 0.0010 - 2s/epoch - 1ms/step\n",
            "Epoch 95/100\n",
            "1250/1250 - 2s - loss: 14.3345 - mse: 14.3345 - mae: 1.5687 - val_loss: 13.9425 - val_mse: 13.9425 - val_mae: 1.6513 - lr: 0.0010 - 2s/epoch - 1ms/step\n",
            "Epoch 96/100\n",
            "1250/1250 - 2s - loss: 14.2369 - mse: 14.2369 - mae: 1.5628 - val_loss: 13.8755 - val_mse: 13.8755 - val_mae: 1.5325 - lr: 0.0010 - 2s/epoch - 1ms/step\n",
            "Epoch 97/100\n",
            "1250/1250 - 2s - loss: 14.2808 - mse: 14.2808 - mae: 1.5702 - val_loss: 13.9607 - val_mse: 13.9607 - val_mae: 1.5880 - lr: 0.0010 - 2s/epoch - 1ms/step\n",
            "Epoch 98/100\n",
            "1250/1250 - 2s - loss: 14.2681 - mse: 14.2681 - mae: 1.5669 - val_loss: 13.7493 - val_mse: 13.7493 - val_mae: 1.6629 - lr: 0.0010 - 2s/epoch - 1ms/step\n",
            "Epoch 99/100\n",
            "1250/1250 - 2s - loss: 14.2304 - mse: 14.2304 - mae: 1.5653 - val_loss: 13.9308 - val_mse: 13.9308 - val_mae: 1.6396 - lr: 0.0010 - 2s/epoch - 1ms/step\n",
            "Epoch 100/100\n",
            "1250/1250 - 2s - loss: 14.3030 - mse: 14.3030 - mae: 1.5703 - val_loss: 13.7585 - val_mse: 13.7585 - val_mae: 1.6095 - lr: 0.0010 - 2s/epoch - 1ms/step\n"
          ]
        }
      ],
      "source": [
        "#{'activation': 'relu', 'num_hidden_layer': 3, 'i': 6, 'learning_rate': 0.0023989159773794054}.\n",
        "optimizer = Adam(learning_rate=0.0023989159773794054 ,clipnorm=1.0)\n",
        "model_3 = create_model(activation=\"relu\",num_hidden_layer=3,num_hidden_unit=64)\n",
        "\n",
        "es = EarlyStopping(monitor='mse', patience=20)\n",
        "reduce_lr = ReduceLROnPlateau(monitor='val_mse', factor=0.2,\n",
        "                              patience=20, min_lr=0.001)\n",
        "model_3.compile(loss='mse',optimizer=optimizer,metrics=['mse','mae'])\n",
        "history = model_3.fit(training,labelsForTrain,\n",
        "                batch_size=64,\n",
        "                epochs=100,\n",
        "                verbose=2,\n",
        "                validation_data=(valing,labelsForVal),\n",
        "                validation_batch_size=64,\n",
        "                callbacks=[es,reduce_lr])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P873ZE1UCcDx",
        "outputId": "a13d6d21-00eb-431d-a5e0-9af365f18082"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "313/313 [==============================] - 0s 1ms/step - loss: 11.7022 - mse: 11.7022 - mae: 1.5718\n"
          ]
        }
      ],
      "source": [
        "results_model3 = model_3.evaluate(testing, labelsForTest, batch_size=32)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x5qEUSduCg37"
      },
      "source": [
        "## Shuffle Repetation 4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NdMGB-QGCk7r"
      },
      "outputs": [],
      "source": [
        "# Shuffle the data after train\n",
        "shuffled3 = shuffle(train_df, random_state=3)\n",
        "training_shuffled3,labelsForTrain_shuffled3=process_shuffle_dataset(shuffled3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pOGCiNHdCpn9",
        "outputId": "3b3c113f-07b5-400c-c8e8-fd3d1cb426da"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 4s - loss: 15.2986 - mse: 15.2986 - mae: 1.6165 - val_loss: 16.2284 - val_mse: 16.2284 - val_mae: 1.5737 - lr: 0.0017 - 4s/epoch - 4ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 3s - loss: 14.9629 - mse: 14.9629 - mae: 1.5914 - val_loss: 15.9311 - val_mse: 15.9311 - val_mae: 1.5997 - lr: 0.0017 - 3s/epoch - 3ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 3s - loss: 14.9289 - mse: 14.9289 - mae: 1.5874 - val_loss: 15.8437 - val_mse: 15.8437 - val_mae: 1.6022 - lr: 0.0017 - 3s/epoch - 3ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 3s - loss: 14.8404 - mse: 14.8404 - mae: 1.5810 - val_loss: 15.9394 - val_mse: 15.9394 - val_mae: 1.6320 - lr: 0.0017 - 3s/epoch - 3ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 4s - loss: 14.9030 - mse: 14.9030 - mae: 1.5782 - val_loss: 15.8178 - val_mse: 15.8178 - val_mae: 1.5926 - lr: 0.0017 - 4s/epoch - 4ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 3s - loss: 14.8280 - mse: 14.8280 - mae: 1.5772 - val_loss: 15.7772 - val_mse: 15.7772 - val_mae: 1.6778 - lr: 0.0017 - 3s/epoch - 3ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 3s - loss: 14.8315 - mse: 14.8315 - mae: 1.5725 - val_loss: 15.7925 - val_mse: 15.7925 - val_mae: 1.6545 - lr: 0.0017 - 3s/epoch - 3ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 3s - loss: 14.7669 - mse: 14.7669 - mae: 1.5736 - val_loss: 15.6749 - val_mse: 15.6749 - val_mae: 1.6256 - lr: 0.0017 - 3s/epoch - 3ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 3s - loss: 14.7895 - mse: 14.7895 - mae: 1.5815 - val_loss: 15.7366 - val_mse: 15.7366 - val_mae: 1.6427 - lr: 0.0017 - 3s/epoch - 3ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 3s - loss: 14.8004 - mse: 14.8004 - mae: 1.5760 - val_loss: 15.7029 - val_mse: 15.7029 - val_mae: 1.6099 - lr: 0.0017 - 3s/epoch - 3ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 3s - loss: 14.7377 - mse: 14.7377 - mae: 1.5688 - val_loss: 15.6700 - val_mse: 15.6700 - val_mae: 1.5912 - lr: 0.0017 - 3s/epoch - 3ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 3s - loss: 14.6004 - mse: 14.6004 - mae: 1.5693 - val_loss: 15.8857 - val_mse: 15.8857 - val_mae: 1.5613 - lr: 0.0017 - 3s/epoch - 3ms/step\n",
            "Epoch 13/100\n",
            "1000/1000 - 3s - loss: 14.6892 - mse: 14.6892 - mae: 1.5685 - val_loss: 15.7384 - val_mse: 15.7384 - val_mae: 1.6256 - lr: 0.0017 - 3s/epoch - 3ms/step\n",
            "Epoch 14/100\n",
            "1000/1000 - 3s - loss: 14.6801 - mse: 14.6801 - mae: 1.5680 - val_loss: 15.7715 - val_mse: 15.7715 - val_mae: 1.5745 - lr: 0.0017 - 3s/epoch - 3ms/step\n",
            "Epoch 15/100\n",
            "1000/1000 - 4s - loss: 14.6377 - mse: 14.6377 - mae: 1.5647 - val_loss: 15.7855 - val_mse: 15.7855 - val_mae: 1.5640 - lr: 0.0017 - 4s/epoch - 4ms/step\n",
            "Epoch 16/100\n",
            "1000/1000 - 5s - loss: 14.6369 - mse: 14.6369 - mae: 1.5605 - val_loss: 15.8557 - val_mse: 15.8557 - val_mae: 1.5583 - lr: 0.0017 - 5s/epoch - 5ms/step\n",
            "Score for fold 1: loss of 15.855707168579102\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 3s - loss: 15.2517 - mse: 15.2517 - mae: 1.5692 - val_loss: 13.1386 - val_mse: 13.1386 - val_mae: 1.5640 - lr: 0.0010 - 3s/epoch - 3ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 3s - loss: 15.2077 - mse: 15.2077 - mae: 1.5670 - val_loss: 12.9775 - val_mse: 12.9775 - val_mae: 1.5766 - lr: 0.0010 - 3s/epoch - 3ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 3s - loss: 15.1621 - mse: 15.1621 - mae: 1.5619 - val_loss: 13.1550 - val_mse: 13.1550 - val_mae: 1.5431 - lr: 0.0010 - 3s/epoch - 3ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 3s - loss: 15.1693 - mse: 15.1693 - mae: 1.5663 - val_loss: 12.7951 - val_mse: 12.7951 - val_mae: 1.6595 - lr: 0.0010 - 3s/epoch - 3ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 3s - loss: 15.1235 - mse: 15.1235 - mae: 1.5656 - val_loss: 13.2245 - val_mse: 13.2245 - val_mae: 1.6005 - lr: 0.0010 - 3s/epoch - 3ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 3s - loss: 15.1229 - mse: 15.1229 - mae: 1.5671 - val_loss: 13.3830 - val_mse: 13.3830 - val_mae: 1.6099 - lr: 0.0010 - 3s/epoch - 3ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 3s - loss: 15.1800 - mse: 15.1800 - mae: 1.5687 - val_loss: 13.2381 - val_mse: 13.2381 - val_mae: 1.6035 - lr: 0.0010 - 3s/epoch - 3ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 3s - loss: 15.1692 - mse: 15.1692 - mae: 1.5684 - val_loss: 13.2583 - val_mse: 13.2583 - val_mae: 1.5660 - lr: 0.0010 - 3s/epoch - 3ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 3s - loss: 15.1303 - mse: 15.1303 - mae: 1.5670 - val_loss: 13.5296 - val_mse: 13.5296 - val_mae: 1.6349 - lr: 0.0010 - 3s/epoch - 3ms/step\n",
            "Score for fold 2: loss of 13.529609680175781\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 3s - loss: 15.7518 - mse: 15.7518 - mae: 1.5825 - val_loss: 10.6489 - val_mse: 10.6489 - val_mae: 1.5413 - lr: 0.0010 - 3s/epoch - 3ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 3s - loss: 15.6713 - mse: 15.6713 - mae: 1.5775 - val_loss: 10.6966 - val_mse: 10.6966 - val_mae: 1.5388 - lr: 0.0010 - 3s/epoch - 3ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 4s - loss: 15.6942 - mse: 15.6942 - mae: 1.5807 - val_loss: 10.8556 - val_mse: 10.8556 - val_mae: 1.5131 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 3s - loss: 15.6840 - mse: 15.6840 - mae: 1.5777 - val_loss: 10.6304 - val_mse: 10.6304 - val_mae: 1.5260 - lr: 0.0010 - 3s/epoch - 3ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 3s - loss: 15.6976 - mse: 15.6976 - mae: 1.5780 - val_loss: 10.7821 - val_mse: 10.7821 - val_mae: 1.5155 - lr: 0.0010 - 3s/epoch - 3ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 3s - loss: 15.6628 - mse: 15.6628 - mae: 1.5775 - val_loss: 10.9005 - val_mse: 10.9005 - val_mae: 1.5418 - lr: 0.0010 - 3s/epoch - 3ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 3s - loss: 15.5566 - mse: 15.5566 - mae: 1.5734 - val_loss: 10.6830 - val_mse: 10.6830 - val_mae: 1.6025 - lr: 0.0010 - 3s/epoch - 3ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 3s - loss: 15.4529 - mse: 15.4529 - mae: 1.5746 - val_loss: 10.7295 - val_mse: 10.7295 - val_mae: 1.5175 - lr: 0.0010 - 3s/epoch - 3ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 4s - loss: 15.5480 - mse: 15.5480 - mae: 1.5708 - val_loss: 10.7552 - val_mse: 10.7552 - val_mae: 1.6276 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Score for fold 3: loss of 10.755226135253906\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 3s - loss: 14.9566 - mse: 14.9566 - mae: 1.5724 - val_loss: 12.8735 - val_mse: 12.8735 - val_mae: 1.5805 - lr: 0.0010 - 3s/epoch - 3ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 3s - loss: 15.0704 - mse: 15.0704 - mae: 1.5673 - val_loss: 13.2438 - val_mse: 13.2438 - val_mae: 1.5250 - lr: 0.0010 - 3s/epoch - 3ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 3s - loss: 15.0761 - mse: 15.0761 - mae: 1.5702 - val_loss: 13.3866 - val_mse: 13.3866 - val_mae: 1.5172 - lr: 0.0010 - 3s/epoch - 3ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 3s - loss: 14.9930 - mse: 14.9930 - mae: 1.5627 - val_loss: 13.1245 - val_mse: 13.1245 - val_mae: 1.5458 - lr: 0.0010 - 3s/epoch - 3ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 3s - loss: 15.0328 - mse: 15.0328 - mae: 1.5634 - val_loss: 13.1303 - val_mse: 13.1303 - val_mae: 1.5638 - lr: 0.0010 - 3s/epoch - 3ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 3s - loss: 14.9827 - mse: 14.9827 - mae: 1.5604 - val_loss: 13.3124 - val_mse: 13.3124 - val_mae: 1.5464 - lr: 0.0010 - 3s/epoch - 3ms/step\n",
            "Score for fold 4: loss of 13.312437057495117\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 4s - loss: 13.0241 - mse: 13.0241 - mae: 1.5663 - val_loss: 21.4645 - val_mse: 21.4645 - val_mae: 1.5245 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 3s - loss: 13.0821 - mse: 13.0821 - mae: 1.5596 - val_loss: 21.2779 - val_mse: 21.2779 - val_mae: 1.6157 - lr: 0.0010 - 3s/epoch - 3ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 3s - loss: 12.9300 - mse: 12.9300 - mae: 1.5631 - val_loss: 21.1614 - val_mse: 21.1614 - val_mae: 1.5812 - lr: 0.0010 - 3s/epoch - 3ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 5s - loss: 12.9925 - mse: 12.9925 - mae: 1.5616 - val_loss: 21.3648 - val_mse: 21.3648 - val_mae: 1.5293 - lr: 0.0010 - 5s/epoch - 5ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 4s - loss: 12.9436 - mse: 12.9436 - mae: 1.5583 - val_loss: 21.4827 - val_mse: 21.4827 - val_mae: 1.5697 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 3s - loss: 12.9307 - mse: 12.9307 - mae: 1.5546 - val_loss: 21.7253 - val_mse: 21.7253 - val_mae: 1.5874 - lr: 0.0010 - 3s/epoch - 3ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 3s - loss: 12.9230 - mse: 12.9230 - mae: 1.5534 - val_loss: 21.1473 - val_mse: 21.1473 - val_mae: 1.5560 - lr: 0.0010 - 3s/epoch - 3ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 3s - loss: 12.8175 - mse: 12.8175 - mae: 1.5530 - val_loss: 21.6370 - val_mse: 21.6370 - val_mae: 1.5784 - lr: 0.0010 - 3s/epoch - 3ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 3s - loss: 12.8786 - mse: 12.8786 - mae: 1.5533 - val_loss: 21.4335 - val_mse: 21.4335 - val_mae: 1.5980 - lr: 0.0010 - 3s/epoch - 3ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 3s - loss: 12.7656 - mse: 12.7656 - mae: 1.5470 - val_loss: 21.1118 - val_mse: 21.1118 - val_mae: 1.6384 - lr: 0.0010 - 3s/epoch - 3ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 3s - loss: 12.7150 - mse: 12.7150 - mae: 1.5477 - val_loss: 21.3908 - val_mse: 21.3908 - val_mae: 1.5680 - lr: 0.0010 - 3s/epoch - 3ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 3s - loss: 12.7282 - mse: 12.7282 - mae: 1.5457 - val_loss: 21.1910 - val_mse: 21.1910 - val_mae: 1.6080 - lr: 0.0010 - 3s/epoch - 3ms/step\n",
            "Epoch 13/100\n",
            "1000/1000 - 3s - loss: 12.7048 - mse: 12.7048 - mae: 1.5431 - val_loss: 21.3607 - val_mse: 21.3607 - val_mae: 1.5930 - lr: 0.0010 - 3s/epoch - 3ms/step\n",
            "Epoch 14/100\n",
            "1000/1000 - 3s - loss: 12.6593 - mse: 12.6593 - mae: 1.5427 - val_loss: 21.3254 - val_mse: 21.3254 - val_mae: 1.5860 - lr: 0.0010 - 3s/epoch - 3ms/step\n",
            "Epoch 15/100\n",
            "1000/1000 - 3s - loss: 12.6346 - mse: 12.6346 - mae: 1.5445 - val_loss: 21.0068 - val_mse: 21.0068 - val_mae: 1.5860 - lr: 0.0010 - 3s/epoch - 3ms/step\n",
            "Epoch 16/100\n",
            "1000/1000 - 3s - loss: 12.5650 - mse: 12.5650 - mae: 1.5421 - val_loss: 21.2698 - val_mse: 21.2698 - val_mae: 1.6347 - lr: 0.0010 - 3s/epoch - 3ms/step\n",
            "Epoch 17/100\n",
            "1000/1000 - 3s - loss: 12.5152 - mse: 12.5152 - mae: 1.5418 - val_loss: 21.3427 - val_mse: 21.3427 - val_mae: 1.6586 - lr: 0.0010 - 3s/epoch - 3ms/step\n",
            "Epoch 18/100\n",
            "1000/1000 - 4s - loss: 12.4825 - mse: 12.4825 - mae: 1.5451 - val_loss: 21.1716 - val_mse: 21.1716 - val_mae: 1.5501 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 19/100\n",
            "1000/1000 - 3s - loss: 12.5326 - mse: 12.5326 - mae: 1.5367 - val_loss: 21.4763 - val_mse: 21.4763 - val_mae: 1.5423 - lr: 0.0010 - 3s/epoch - 3ms/step\n",
            "Epoch 20/100\n",
            "1000/1000 - 3s - loss: 12.5050 - mse: 12.5050 - mae: 1.5378 - val_loss: 21.3988 - val_mse: 21.3988 - val_mae: 1.6194 - lr: 0.0010 - 3s/epoch - 3ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-10-15 05:09:44,739]\u001b[0m Finished trial#0 resulted in value: 14.972. Current best value is 14.972 with parameters: {'activation': 'relu', 'num_hidden_layer': 3, 'i': 8, 'learning_rate': 0.0017060481633639777}.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Score for fold 5: loss of 21.398807525634766\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 3s - loss: 15.8214 - mse: 15.8214 - mae: 1.6932 - val_loss: 18.8939 - val_mse: 18.8939 - val_mae: 1.6622 - lr: 3.6064e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 14.7938 - mse: 14.7938 - mae: 1.5995 - val_loss: 18.7106 - val_mse: 18.7106 - val_mae: 1.6247 - lr: 3.6064e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 14.6917 - mse: 14.6917 - mae: 1.5935 - val_loss: 18.5636 - val_mse: 18.5636 - val_mae: 1.6342 - lr: 3.6064e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 14.6237 - mse: 14.6237 - mae: 1.5893 - val_loss: 18.3851 - val_mse: 18.3851 - val_mae: 1.6750 - lr: 3.6064e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 14.5631 - mse: 14.5631 - mae: 1.5856 - val_loss: 18.3360 - val_mse: 18.3360 - val_mae: 1.6488 - lr: 3.6064e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 14.5185 - mse: 14.5185 - mae: 1.5819 - val_loss: 18.3078 - val_mse: 18.3078 - val_mae: 1.6384 - lr: 3.6064e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 14.4727 - mse: 14.4727 - mae: 1.5789 - val_loss: 18.2267 - val_mse: 18.2267 - val_mae: 1.6235 - lr: 3.6064e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 2s - loss: 14.4424 - mse: 14.4424 - mae: 1.5749 - val_loss: 18.1343 - val_mse: 18.1343 - val_mae: 1.6288 - lr: 3.6064e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 2s - loss: 14.4159 - mse: 14.4159 - mae: 1.5743 - val_loss: 18.2056 - val_mse: 18.2056 - val_mae: 1.6286 - lr: 3.6064e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 2s - loss: 14.3843 - mse: 14.3843 - mae: 1.5728 - val_loss: 18.1125 - val_mse: 18.1125 - val_mae: 1.6734 - lr: 3.6064e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 2s - loss: 14.3815 - mse: 14.3815 - mae: 1.5738 - val_loss: 18.0388 - val_mse: 18.0388 - val_mae: 1.6640 - lr: 3.6064e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 2s - loss: 14.3537 - mse: 14.3537 - mae: 1.5753 - val_loss: 18.0063 - val_mse: 18.0063 - val_mae: 1.6211 - lr: 3.6064e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 13/100\n",
            "1000/1000 - 2s - loss: 14.3474 - mse: 14.3474 - mae: 1.5714 - val_loss: 17.9934 - val_mse: 17.9934 - val_mae: 1.6364 - lr: 3.6064e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 14/100\n",
            "1000/1000 - 2s - loss: 14.3401 - mse: 14.3401 - mae: 1.5707 - val_loss: 18.0536 - val_mse: 18.0536 - val_mae: 1.6315 - lr: 3.6064e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 15/100\n",
            "1000/1000 - 2s - loss: 14.3360 - mse: 14.3360 - mae: 1.5690 - val_loss: 17.9973 - val_mse: 17.9973 - val_mae: 1.6351 - lr: 3.6064e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 16/100\n",
            "1000/1000 - 2s - loss: 14.3150 - mse: 14.3150 - mae: 1.5696 - val_loss: 17.9320 - val_mse: 17.9320 - val_mae: 1.6528 - lr: 3.6064e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 17/100\n",
            "1000/1000 - 2s - loss: 14.2975 - mse: 14.2975 - mae: 1.5703 - val_loss: 17.9159 - val_mse: 17.9159 - val_mae: 1.6296 - lr: 3.6064e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 18/100\n",
            "1000/1000 - 2s - loss: 14.2944 - mse: 14.2944 - mae: 1.5686 - val_loss: 17.9993 - val_mse: 17.9993 - val_mae: 1.5902 - lr: 3.6064e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 19/100\n",
            "1000/1000 - 2s - loss: 14.3028 - mse: 14.3028 - mae: 1.5687 - val_loss: 17.9050 - val_mse: 17.9050 - val_mae: 1.6348 - lr: 3.6064e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 20/100\n",
            "1000/1000 - 2s - loss: 14.3019 - mse: 14.3019 - mae: 1.5685 - val_loss: 18.0003 - val_mse: 18.0003 - val_mae: 1.6337 - lr: 3.6064e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 21/100\n",
            "1000/1000 - 2s - loss: 14.2807 - mse: 14.2807 - mae: 1.5683 - val_loss: 18.0015 - val_mse: 18.0015 - val_mae: 1.6140 - lr: 3.6064e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 22/100\n",
            "1000/1000 - 2s - loss: 14.2878 - mse: 14.2878 - mae: 1.5688 - val_loss: 17.9344 - val_mse: 17.9344 - val_mae: 1.6241 - lr: 3.6064e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 23/100\n",
            "1000/1000 - 2s - loss: 14.2698 - mse: 14.2698 - mae: 1.5669 - val_loss: 17.8860 - val_mse: 17.8860 - val_mae: 1.6392 - lr: 3.6064e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 24/100\n",
            "1000/1000 - 2s - loss: 14.2782 - mse: 14.2782 - mae: 1.5685 - val_loss: 17.9058 - val_mse: 17.9058 - val_mae: 1.6218 - lr: 3.6064e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 25/100\n",
            "1000/1000 - 2s - loss: 14.2640 - mse: 14.2640 - mae: 1.5670 - val_loss: 17.9583 - val_mse: 17.9583 - val_mae: 1.6433 - lr: 3.6064e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 26/100\n",
            "1000/1000 - 2s - loss: 14.2564 - mse: 14.2564 - mae: 1.5653 - val_loss: 17.8794 - val_mse: 17.8794 - val_mae: 1.6423 - lr: 3.6064e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 27/100\n",
            "1000/1000 - 2s - loss: 14.2529 - mse: 14.2529 - mae: 1.5666 - val_loss: 18.0116 - val_mse: 18.0116 - val_mae: 1.5957 - lr: 3.6064e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 28/100\n",
            "1000/1000 - 2s - loss: 14.2574 - mse: 14.2574 - mae: 1.5643 - val_loss: 17.8774 - val_mse: 17.8774 - val_mae: 1.6480 - lr: 3.6064e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 29/100\n",
            "1000/1000 - 2s - loss: 14.2552 - mse: 14.2552 - mae: 1.5681 - val_loss: 17.8055 - val_mse: 17.8055 - val_mae: 1.6468 - lr: 3.6064e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 30/100\n",
            "1000/1000 - 2s - loss: 14.2372 - mse: 14.2372 - mae: 1.5629 - val_loss: 17.8181 - val_mse: 17.8181 - val_mae: 1.6257 - lr: 3.6064e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 31/100\n",
            "1000/1000 - 2s - loss: 14.2253 - mse: 14.2253 - mae: 1.5673 - val_loss: 17.8806 - val_mse: 17.8806 - val_mae: 1.6042 - lr: 3.6064e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 32/100\n",
            "1000/1000 - 2s - loss: 14.2235 - mse: 14.2235 - mae: 1.5659 - val_loss: 17.7820 - val_mse: 17.7820 - val_mae: 1.6507 - lr: 3.6064e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 33/100\n",
            "1000/1000 - 2s - loss: 14.2272 - mse: 14.2272 - mae: 1.5648 - val_loss: 17.8626 - val_mse: 17.8626 - val_mae: 1.6215 - lr: 3.6064e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 34/100\n",
            "1000/1000 - 2s - loss: 14.2251 - mse: 14.2251 - mae: 1.5668 - val_loss: 17.7897 - val_mse: 17.7897 - val_mae: 1.6515 - lr: 3.6064e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 35/100\n",
            "1000/1000 - 3s - loss: 14.2025 - mse: 14.2025 - mae: 1.5658 - val_loss: 17.8120 - val_mse: 17.8120 - val_mae: 1.6081 - lr: 3.6064e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 36/100\n",
            "1000/1000 - 3s - loss: 14.2051 - mse: 14.2051 - mae: 1.5651 - val_loss: 17.8403 - val_mse: 17.8403 - val_mae: 1.6271 - lr: 3.6064e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 37/100\n",
            "1000/1000 - 2s - loss: 14.2102 - mse: 14.2102 - mae: 1.5620 - val_loss: 17.7986 - val_mse: 17.7986 - val_mae: 1.6242 - lr: 3.6064e-04 - 2s/epoch - 2ms/step\n",
            "Score for fold 1: loss of 17.798580169677734\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 13.6121 - mse: 13.6121 - mae: 1.5802 - val_loss: 20.2342 - val_mse: 20.2342 - val_mae: 1.5509 - lr: 3.6064e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 13.5789 - mse: 13.5789 - mae: 1.5773 - val_loss: 20.2909 - val_mse: 20.2909 - val_mae: 1.5495 - lr: 3.6064e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 13.5798 - mse: 13.5798 - mae: 1.5751 - val_loss: 20.2885 - val_mse: 20.2885 - val_mae: 1.5375 - lr: 3.6064e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 13.5816 - mse: 13.5816 - mae: 1.5749 - val_loss: 20.2579 - val_mse: 20.2579 - val_mae: 1.5712 - lr: 3.6064e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 13.5736 - mse: 13.5736 - mae: 1.5735 - val_loss: 20.2266 - val_mse: 20.2266 - val_mae: 1.5748 - lr: 3.6064e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 13.5556 - mse: 13.5556 - mae: 1.5738 - val_loss: 20.1899 - val_mse: 20.1899 - val_mae: 1.5923 - lr: 3.6064e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 13.5534 - mse: 13.5534 - mae: 1.5753 - val_loss: 20.2601 - val_mse: 20.2601 - val_mae: 1.5911 - lr: 3.6064e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 2s - loss: 13.5438 - mse: 13.5438 - mae: 1.5771 - val_loss: 20.2084 - val_mse: 20.2084 - val_mae: 1.5743 - lr: 3.6064e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 2s - loss: 13.5412 - mse: 13.5412 - mae: 1.5723 - val_loss: 20.2133 - val_mse: 20.2133 - val_mae: 1.5937 - lr: 3.6064e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 2s - loss: 13.5345 - mse: 13.5345 - mae: 1.5744 - val_loss: 20.2953 - val_mse: 20.2953 - val_mae: 1.5455 - lr: 3.6064e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 2s - loss: 13.5353 - mse: 13.5353 - mae: 1.5722 - val_loss: 20.2261 - val_mse: 20.2261 - val_mae: 1.5661 - lr: 3.6064e-04 - 2s/epoch - 2ms/step\n",
            "Score for fold 2: loss of 20.226070404052734\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 15.9471 - mse: 15.9471 - mae: 1.5798 - val_loss: 10.4745 - val_mse: 10.4745 - val_mae: 1.5628 - lr: 3.6064e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 15.9249 - mse: 15.9249 - mae: 1.5786 - val_loss: 10.5787 - val_mse: 10.5787 - val_mae: 1.5267 - lr: 3.6064e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 15.9664 - mse: 15.9664 - mae: 1.5788 - val_loss: 10.5924 - val_mse: 10.5924 - val_mae: 1.5386 - lr: 3.6064e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 15.9292 - mse: 15.9292 - mae: 1.5759 - val_loss: 10.6418 - val_mse: 10.6418 - val_mae: 1.5802 - lr: 3.6064e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 15.9379 - mse: 15.9379 - mae: 1.5772 - val_loss: 10.5826 - val_mse: 10.5826 - val_mae: 1.5269 - lr: 3.6064e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 15.9233 - mse: 15.9233 - mae: 1.5753 - val_loss: 10.5706 - val_mse: 10.5706 - val_mae: 1.5544 - lr: 3.6064e-04 - 2s/epoch - 2ms/step\n",
            "Score for fold 3: loss of 10.570587158203125\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 16.0590 - mse: 16.0590 - mae: 1.5836 - val_loss: 9.8855 - val_mse: 9.8855 - val_mae: 1.5402 - lr: 3.6064e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 16.0416 - mse: 16.0416 - mae: 1.5828 - val_loss: 9.9153 - val_mse: 9.9153 - val_mae: 1.5381 - lr: 3.6064e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 16.0316 - mse: 16.0316 - mae: 1.5801 - val_loss: 9.9826 - val_mse: 9.9826 - val_mae: 1.5570 - lr: 3.6064e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 16.0392 - mse: 16.0392 - mae: 1.5791 - val_loss: 9.9218 - val_mse: 9.9218 - val_mae: 1.5471 - lr: 3.6064e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 16.0149 - mse: 16.0149 - mae: 1.5826 - val_loss: 9.9731 - val_mse: 9.9731 - val_mae: 1.5487 - lr: 3.6064e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 16.0148 - mse: 16.0148 - mae: 1.5835 - val_loss: 9.9541 - val_mse: 9.9541 - val_mae: 1.5633 - lr: 3.6064e-04 - 2s/epoch - 2ms/step\n",
            "Score for fold 4: loss of 9.954148292541504\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 14.5322 - mse: 14.5322 - mae: 1.5712 - val_loss: 15.9358 - val_mse: 15.9358 - val_mae: 1.5767 - lr: 3.6064e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 14.5243 - mse: 14.5243 - mae: 1.5690 - val_loss: 15.9469 - val_mse: 15.9469 - val_mae: 1.5646 - lr: 3.6064e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 14.4878 - mse: 14.4878 - mae: 1.5699 - val_loss: 15.9602 - val_mse: 15.9602 - val_mae: 1.5492 - lr: 3.6064e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 14.4964 - mse: 14.4964 - mae: 1.5714 - val_loss: 16.0759 - val_mse: 16.0759 - val_mae: 1.5559 - lr: 3.6064e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 14.4915 - mse: 14.4915 - mae: 1.5737 - val_loss: 15.9976 - val_mse: 15.9976 - val_mae: 1.5623 - lr: 3.6064e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 14.4786 - mse: 14.4786 - mae: 1.5715 - val_loss: 15.9332 - val_mse: 15.9332 - val_mae: 1.5793 - lr: 3.6064e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 14.4668 - mse: 14.4668 - mae: 1.5711 - val_loss: 15.9483 - val_mse: 15.9483 - val_mae: 1.5759 - lr: 3.6064e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 2s - loss: 14.4689 - mse: 14.4689 - mae: 1.5697 - val_loss: 16.1141 - val_mse: 16.1141 - val_mae: 1.5683 - lr: 3.6064e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 2s - loss: 14.4682 - mse: 14.4682 - mae: 1.5701 - val_loss: 15.9839 - val_mse: 15.9839 - val_mae: 1.5671 - lr: 3.6064e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 2s - loss: 14.4581 - mse: 14.4581 - mae: 1.5680 - val_loss: 16.0548 - val_mse: 16.0548 - val_mae: 1.5664 - lr: 3.6064e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 2s - loss: 14.4340 - mse: 14.4340 - mae: 1.5689 - val_loss: 15.9473 - val_mse: 15.9473 - val_mae: 1.5914 - lr: 3.6064e-04 - 2s/epoch - 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-10-15 05:12:31,874]\u001b[0m Finished trial#1 resulted in value: 14.9. Current best value is 14.9 with parameters: {'activation': 'tanh', 'num_hidden_layer': 3, 'i': 6, 'learning_rate': 0.0003606405459716391}.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Score for fold 5: loss of 15.947320938110352\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 3s - loss: 16.9103 - mse: 16.9103 - mae: 1.6817 - val_loss: 11.7166 - val_mse: 11.7166 - val_mae: 1.5915 - lr: 0.0082 - 3s/epoch - 3ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 16.7630 - mse: 16.7630 - mae: 1.6669 - val_loss: 11.9591 - val_mse: 11.9591 - val_mae: 1.7922 - lr: 0.0082 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 16.6542 - mse: 16.6542 - mae: 1.6570 - val_loss: 11.6039 - val_mse: 11.6039 - val_mae: 1.5640 - lr: 0.0082 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 16.4803 - mse: 16.4803 - mae: 1.6386 - val_loss: 11.8657 - val_mse: 11.8657 - val_mae: 1.5701 - lr: 0.0082 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 16.3960 - mse: 16.3960 - mae: 1.6367 - val_loss: 11.9266 - val_mse: 11.9266 - val_mae: 1.5632 - lr: 0.0082 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 16.3600 - mse: 16.3600 - mae: 1.6333 - val_loss: 11.6614 - val_mse: 11.6614 - val_mae: 1.6409 - lr: 0.0082 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 16.3568 - mse: 16.3568 - mae: 1.6376 - val_loss: 11.5198 - val_mse: 11.5198 - val_mae: 1.5760 - lr: 0.0082 - 2s/epoch - 2ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 2s - loss: 16.3224 - mse: 16.3224 - mae: 1.6315 - val_loss: 11.8079 - val_mse: 11.8079 - val_mae: 1.7062 - lr: 0.0082 - 2s/epoch - 2ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 2s - loss: 16.2356 - mse: 16.2356 - mae: 1.6353 - val_loss: 11.5045 - val_mse: 11.5045 - val_mae: 1.7026 - lr: 0.0082 - 2s/epoch - 2ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 2s - loss: 16.2367 - mse: 16.2367 - mae: 1.6406 - val_loss: 11.2482 - val_mse: 11.2482 - val_mae: 1.6370 - lr: 0.0082 - 2s/epoch - 2ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 2s - loss: 16.2780 - mse: 16.2780 - mae: 1.6360 - val_loss: 11.4878 - val_mse: 11.4878 - val_mae: 1.5627 - lr: 0.0082 - 2s/epoch - 2ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 2s - loss: 16.3007 - mse: 16.3007 - mae: 1.6423 - val_loss: 11.4270 - val_mse: 11.4270 - val_mae: 1.5346 - lr: 0.0082 - 2s/epoch - 2ms/step\n",
            "Epoch 13/100\n",
            "1000/1000 - 2s - loss: 16.3203 - mse: 16.3203 - mae: 1.6452 - val_loss: 11.4739 - val_mse: 11.4739 - val_mae: 1.5577 - lr: 0.0082 - 2s/epoch - 2ms/step\n",
            "Epoch 14/100\n",
            "1000/1000 - 3s - loss: 16.3057 - mse: 16.3057 - mae: 1.6433 - val_loss: 11.5322 - val_mse: 11.5322 - val_mae: 1.5835 - lr: 0.0082 - 3s/epoch - 3ms/step\n",
            "Epoch 15/100\n",
            "1000/1000 - 2s - loss: 16.2823 - mse: 16.2823 - mae: 1.6447 - val_loss: 11.2724 - val_mse: 11.2724 - val_mae: 1.6330 - lr: 0.0082 - 2s/epoch - 2ms/step\n",
            "Score for fold 1: loss of 11.272420883178711\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 15.4866 - mse: 15.4866 - mae: 1.5836 - val_loss: 13.1861 - val_mse: 13.1861 - val_mae: 1.6243 - lr: 0.0016 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 15.4549 - mse: 15.4549 - mae: 1.5749 - val_loss: 13.2723 - val_mse: 13.2723 - val_mae: 1.5757 - lr: 0.0016 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 15.4347 - mse: 15.4347 - mae: 1.5783 - val_loss: 13.3756 - val_mse: 13.3756 - val_mae: 1.5612 - lr: 0.0016 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 15.4291 - mse: 15.4291 - mae: 1.5789 - val_loss: 13.2603 - val_mse: 13.2603 - val_mae: 1.5712 - lr: 0.0016 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 15.4123 - mse: 15.4123 - mae: 1.5763 - val_loss: 13.2234 - val_mse: 13.2234 - val_mae: 1.6129 - lr: 0.0016 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 15.4072 - mse: 15.4072 - mae: 1.5803 - val_loss: 13.2370 - val_mse: 13.2370 - val_mae: 1.6039 - lr: 0.0016 - 2s/epoch - 2ms/step\n",
            "Score for fold 2: loss of 13.237025260925293\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 15.3520 - mse: 15.3520 - mae: 1.5782 - val_loss: 13.5704 - val_mse: 13.5704 - val_mae: 1.5239 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 15.3627 - mse: 15.3627 - mae: 1.5746 - val_loss: 13.5445 - val_mse: 13.5445 - val_mae: 1.5186 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 15.3640 - mse: 15.3640 - mae: 1.5777 - val_loss: 13.4384 - val_mse: 13.4384 - val_mae: 1.5929 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 15.3337 - mse: 15.3337 - mae: 1.5771 - val_loss: 13.4342 - val_mse: 13.4342 - val_mae: 1.6029 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 15.3454 - mse: 15.3454 - mae: 1.5746 - val_loss: 13.4051 - val_mse: 13.4051 - val_mae: 1.5969 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 15.3284 - mse: 15.3284 - mae: 1.5763 - val_loss: 13.4697 - val_mse: 13.4697 - val_mae: 1.6063 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 15.3402 - mse: 15.3402 - mae: 1.5770 - val_loss: 13.3867 - val_mse: 13.3867 - val_mae: 1.6053 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 2s - loss: 15.3353 - mse: 15.3353 - mae: 1.5754 - val_loss: 13.4770 - val_mse: 13.4770 - val_mae: 1.5638 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 2s - loss: 15.3192 - mse: 15.3192 - mae: 1.5749 - val_loss: 13.4016 - val_mse: 13.4016 - val_mae: 1.6104 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 2s - loss: 15.3335 - mse: 15.3335 - mae: 1.5782 - val_loss: 13.5055 - val_mse: 13.5055 - val_mae: 1.5666 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 2s - loss: 15.3206 - mse: 15.3206 - mae: 1.5784 - val_loss: 13.3558 - val_mse: 13.3558 - val_mae: 1.5992 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 2s - loss: 15.3234 - mse: 15.3234 - mae: 1.5772 - val_loss: 13.3704 - val_mse: 13.3704 - val_mae: 1.5925 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 13/100\n",
            "1000/1000 - 2s - loss: 15.3073 - mse: 15.3073 - mae: 1.5779 - val_loss: 13.4567 - val_mse: 13.4567 - val_mae: 1.5607 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 14/100\n",
            "1000/1000 - 2s - loss: 15.3301 - mse: 15.3301 - mae: 1.5762 - val_loss: 13.4910 - val_mse: 13.4910 - val_mae: 1.5733 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 15/100\n",
            "1000/1000 - 2s - loss: 15.3409 - mse: 15.3409 - mae: 1.5754 - val_loss: 13.4727 - val_mse: 13.4727 - val_mae: 1.6125 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 16/100\n",
            "1000/1000 - 2s - loss: 15.3304 - mse: 15.3304 - mae: 1.5791 - val_loss: 13.4859 - val_mse: 13.4859 - val_mae: 1.5629 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Score for fold 3: loss of 13.485940933227539\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 13.5230 - mse: 13.5230 - mae: 1.5712 - val_loss: 20.6775 - val_mse: 20.6775 - val_mae: 1.6017 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 13.4955 - mse: 13.4955 - mae: 1.5697 - val_loss: 20.7413 - val_mse: 20.7413 - val_mae: 1.5484 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 13.5104 - mse: 13.5104 - mae: 1.5698 - val_loss: 20.6513 - val_mse: 20.6513 - val_mae: 1.6017 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 13.4896 - mse: 13.4896 - mae: 1.5698 - val_loss: 20.7340 - val_mse: 20.7340 - val_mae: 1.5709 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 13.4980 - mse: 13.4980 - mae: 1.5725 - val_loss: 20.7824 - val_mse: 20.7824 - val_mae: 1.5770 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 13.5036 - mse: 13.5036 - mae: 1.5715 - val_loss: 20.6982 - val_mse: 20.6982 - val_mae: 1.5942 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 13.4715 - mse: 13.4715 - mae: 1.5710 - val_loss: 20.7026 - val_mse: 20.7026 - val_mae: 1.5984 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 2s - loss: 13.4761 - mse: 13.4761 - mae: 1.5696 - val_loss: 20.6761 - val_mse: 20.6761 - val_mae: 1.6243 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Score for fold 4: loss of 20.67608642578125\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 14.5712 - mse: 14.5712 - mae: 1.5845 - val_loss: 16.4583 - val_mse: 16.4583 - val_mae: 1.5383 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 14.5755 - mse: 14.5755 - mae: 1.5804 - val_loss: 16.4711 - val_mse: 16.4711 - val_mae: 1.5599 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 14.5726 - mse: 14.5726 - mae: 1.5823 - val_loss: 16.4676 - val_mse: 16.4676 - val_mae: 1.6050 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 14.5674 - mse: 14.5674 - mae: 1.5811 - val_loss: 16.4346 - val_mse: 16.4346 - val_mae: 1.5799 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 14.5657 - mse: 14.5657 - mae: 1.5862 - val_loss: 16.4618 - val_mse: 16.4618 - val_mae: 1.5567 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 14.5617 - mse: 14.5617 - mae: 1.5826 - val_loss: 16.4912 - val_mse: 16.4912 - val_mae: 1.5727 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 14.5554 - mse: 14.5554 - mae: 1.5821 - val_loss: 16.4722 - val_mse: 16.4722 - val_mae: 1.5875 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 2s - loss: 14.5392 - mse: 14.5392 - mae: 1.5799 - val_loss: 16.4178 - val_mse: 16.4178 - val_mae: 1.5741 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 2s - loss: 14.5348 - mse: 14.5348 - mae: 1.5817 - val_loss: 16.4586 - val_mse: 16.4586 - val_mae: 1.5764 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 2s - loss: 14.5346 - mse: 14.5346 - mae: 1.5840 - val_loss: 16.4254 - val_mse: 16.4254 - val_mae: 1.6193 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 2s - loss: 14.5522 - mse: 14.5522 - mae: 1.5815 - val_loss: 16.3797 - val_mse: 16.3797 - val_mae: 1.5746 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 2s - loss: 14.5515 - mse: 14.5515 - mae: 1.5797 - val_loss: 16.3805 - val_mse: 16.3805 - val_mae: 1.5696 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 13/100\n",
            "1000/1000 - 2s - loss: 14.5516 - mse: 14.5516 - mae: 1.5812 - val_loss: 16.4876 - val_mse: 16.4876 - val_mae: 1.5728 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 14/100\n",
            "1000/1000 - 2s - loss: 14.5712 - mse: 14.5712 - mae: 1.5782 - val_loss: 16.5022 - val_mse: 16.5022 - val_mae: 1.5875 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 15/100\n",
            "1000/1000 - 2s - loss: 14.5525 - mse: 14.5525 - mae: 1.5788 - val_loss: 16.5785 - val_mse: 16.5785 - val_mae: 1.5517 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 16/100\n",
            "1000/1000 - 2s - loss: 14.5605 - mse: 14.5605 - mae: 1.5837 - val_loss: 16.4588 - val_mse: 16.4588 - val_mae: 1.5605 - lr: 0.0010 - 2s/epoch - 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-10-15 05:15:03,049]\u001b[0m Finished trial#2 resulted in value: 15.028. Current best value is 14.9 with parameters: {'activation': 'tanh', 'num_hidden_layer': 3, 'i': 6, 'learning_rate': 0.0003606405459716391}.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Score for fold 5: loss of 16.45880126953125\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 3s - loss: 15.2963 - mse: 15.2963 - mae: 1.6817 - val_loss: 17.9408 - val_mse: 17.9408 - val_mae: 1.6983 - lr: 0.0054 - 3s/epoch - 3ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 15.2113 - mse: 15.2113 - mae: 1.6589 - val_loss: 17.8036 - val_mse: 17.8036 - val_mae: 1.6645 - lr: 0.0054 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 3s - loss: 15.2012 - mse: 15.2012 - mae: 1.6513 - val_loss: 18.1150 - val_mse: 18.1150 - val_mae: 1.5242 - lr: 0.0054 - 3s/epoch - 3ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 15.0699 - mse: 15.0699 - mae: 1.6455 - val_loss: 17.7824 - val_mse: 17.7824 - val_mae: 1.6349 - lr: 0.0054 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 14.9492 - mse: 14.9492 - mae: 1.6430 - val_loss: 17.7779 - val_mse: 17.7779 - val_mae: 1.5541 - lr: 0.0054 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 14.8657 - mse: 14.8657 - mae: 1.6355 - val_loss: 17.6657 - val_mse: 17.6657 - val_mae: 1.6517 - lr: 0.0054 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 14.7629 - mse: 14.7629 - mae: 1.6223 - val_loss: 17.5864 - val_mse: 17.5864 - val_mae: 1.6032 - lr: 0.0054 - 2s/epoch - 2ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 2s - loss: 14.6962 - mse: 14.6962 - mae: 1.6148 - val_loss: 17.6287 - val_mse: 17.6287 - val_mae: 1.5179 - lr: 0.0054 - 2s/epoch - 2ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 2s - loss: 14.6880 - mse: 14.6880 - mae: 1.6195 - val_loss: 17.7814 - val_mse: 17.7814 - val_mae: 1.5997 - lr: 0.0054 - 2s/epoch - 2ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 2s - loss: 14.6645 - mse: 14.6645 - mae: 1.6183 - val_loss: 17.6453 - val_mse: 17.6453 - val_mae: 1.5038 - lr: 0.0054 - 2s/epoch - 2ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 2s - loss: 14.6400 - mse: 14.6400 - mae: 1.6215 - val_loss: 17.4757 - val_mse: 17.4757 - val_mae: 1.5431 - lr: 0.0054 - 2s/epoch - 2ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 2s - loss: 14.6299 - mse: 14.6299 - mae: 1.6231 - val_loss: 17.5066 - val_mse: 17.5066 - val_mae: 1.5580 - lr: 0.0054 - 2s/epoch - 2ms/step\n",
            "Epoch 13/100\n",
            "1000/1000 - 2s - loss: 14.5983 - mse: 14.5983 - mae: 1.6272 - val_loss: 17.6772 - val_mse: 17.6772 - val_mae: 1.5483 - lr: 0.0054 - 2s/epoch - 2ms/step\n",
            "Epoch 14/100\n",
            "1000/1000 - 2s - loss: 14.6134 - mse: 14.6134 - mae: 1.6239 - val_loss: 17.5595 - val_mse: 17.5595 - val_mae: 1.6906 - lr: 0.0054 - 2s/epoch - 2ms/step\n",
            "Epoch 15/100\n",
            "1000/1000 - 2s - loss: 14.6221 - mse: 14.6221 - mae: 1.6307 - val_loss: 17.5655 - val_mse: 17.5655 - val_mae: 1.7083 - lr: 0.0054 - 2s/epoch - 2ms/step\n",
            "Epoch 16/100\n",
            "1000/1000 - 2s - loss: 14.6104 - mse: 14.6104 - mae: 1.6297 - val_loss: 17.5915 - val_mse: 17.5915 - val_mae: 1.5454 - lr: 0.0054 - 2s/epoch - 2ms/step\n",
            "Score for fold 1: loss of 17.59149742126465\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 15.6258 - mse: 15.6258 - mae: 1.5871 - val_loss: 12.4798 - val_mse: 12.4798 - val_mae: 1.5760 - lr: 0.0011 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 15.6152 - mse: 15.6152 - mae: 1.5848 - val_loss: 12.4702 - val_mse: 12.4702 - val_mae: 1.5613 - lr: 0.0011 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 15.5977 - mse: 15.5977 - mae: 1.5852 - val_loss: 12.5403 - val_mse: 12.5403 - val_mae: 1.5829 - lr: 0.0011 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 15.5987 - mse: 15.5987 - mae: 1.5849 - val_loss: 12.4402 - val_mse: 12.4402 - val_mae: 1.5565 - lr: 0.0011 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 15.5779 - mse: 15.5779 - mae: 1.5830 - val_loss: 12.5829 - val_mse: 12.5829 - val_mae: 1.5483 - lr: 0.0011 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 15.5886 - mse: 15.5886 - mae: 1.5839 - val_loss: 12.5212 - val_mse: 12.5212 - val_mae: 1.6055 - lr: 0.0011 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 15.5638 - mse: 15.5638 - mae: 1.5866 - val_loss: 12.4587 - val_mse: 12.4587 - val_mae: 1.5692 - lr: 0.0011 - 2s/epoch - 2ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 2s - loss: 15.5508 - mse: 15.5508 - mae: 1.5903 - val_loss: 12.4734 - val_mse: 12.4734 - val_mae: 1.6221 - lr: 0.0011 - 2s/epoch - 2ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 2s - loss: 15.5507 - mse: 15.5507 - mae: 1.5847 - val_loss: 12.4919 - val_mse: 12.4919 - val_mae: 1.5913 - lr: 0.0011 - 2s/epoch - 2ms/step\n",
            "Score for fold 2: loss of 12.491935729980469\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 14.6778 - mse: 14.6778 - mae: 1.5901 - val_loss: 16.0231 - val_mse: 16.0231 - val_mae: 1.5625 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 14.6789 - mse: 14.6789 - mae: 1.5885 - val_loss: 16.0316 - val_mse: 16.0316 - val_mae: 1.6035 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 14.6838 - mse: 14.6838 - mae: 1.5869 - val_loss: 16.0390 - val_mse: 16.0390 - val_mae: 1.5959 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 14.6832 - mse: 14.6832 - mae: 1.5839 - val_loss: 16.0916 - val_mse: 16.0916 - val_mae: 1.5791 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 14.6704 - mse: 14.6704 - mae: 1.5858 - val_loss: 16.1300 - val_mse: 16.1300 - val_mae: 1.5569 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 14.6637 - mse: 14.6637 - mae: 1.5892 - val_loss: 16.2052 - val_mse: 16.2052 - val_mae: 1.5654 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Score for fold 3: loss of 16.205223083496094\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 14.7700 - mse: 14.7700 - mae: 1.5756 - val_loss: 15.8312 - val_mse: 15.8312 - val_mae: 1.5837 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 14.7488 - mse: 14.7488 - mae: 1.5773 - val_loss: 15.8697 - val_mse: 15.8697 - val_mae: 1.5913 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 14.7328 - mse: 14.7328 - mae: 1.5778 - val_loss: 15.8416 - val_mse: 15.8416 - val_mae: 1.6106 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 14.7582 - mse: 14.7582 - mae: 1.5758 - val_loss: 15.7754 - val_mse: 15.7754 - val_mae: 1.5842 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 14.7169 - mse: 14.7169 - mae: 1.5752 - val_loss: 15.8485 - val_mse: 15.8485 - val_mae: 1.5801 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 14.7228 - mse: 14.7228 - mae: 1.5734 - val_loss: 15.7805 - val_mse: 15.7805 - val_mae: 1.6065 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 14.7211 - mse: 14.7211 - mae: 1.5731 - val_loss: 15.7900 - val_mse: 15.7900 - val_mae: 1.6191 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 2s - loss: 14.7046 - mse: 14.7046 - mae: 1.5750 - val_loss: 15.8601 - val_mse: 15.8601 - val_mae: 1.5985 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 2s - loss: 14.7080 - mse: 14.7080 - mae: 1.5754 - val_loss: 15.8275 - val_mse: 15.8275 - val_mae: 1.5969 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Score for fold 4: loss of 15.827469825744629\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 15.4285 - mse: 15.4285 - mae: 1.5746 - val_loss: 13.1239 - val_mse: 13.1239 - val_mae: 1.5910 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 15.3948 - mse: 15.3948 - mae: 1.5771 - val_loss: 13.0980 - val_mse: 13.0980 - val_mae: 1.6089 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 15.4090 - mse: 15.4090 - mae: 1.5727 - val_loss: 13.2993 - val_mse: 13.2993 - val_mae: 1.5481 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 15.4151 - mse: 15.4151 - mae: 1.5754 - val_loss: 13.0289 - val_mse: 13.0289 - val_mae: 1.6068 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 15.4282 - mse: 15.4282 - mae: 1.5774 - val_loss: 13.0771 - val_mse: 13.0771 - val_mae: 1.5878 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 15.4063 - mse: 15.4063 - mae: 1.5721 - val_loss: 13.0399 - val_mse: 13.0399 - val_mae: 1.6056 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 15.4053 - mse: 15.4053 - mae: 1.5763 - val_loss: 13.1917 - val_mse: 13.1917 - val_mae: 1.5732 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 2s - loss: 15.4087 - mse: 15.4087 - mae: 1.5724 - val_loss: 13.0920 - val_mse: 13.0920 - val_mae: 1.6223 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 2s - loss: 15.3906 - mse: 15.3906 - mae: 1.5742 - val_loss: 13.1363 - val_mse: 13.1363 - val_mae: 1.6558 - lr: 0.0010 - 2s/epoch - 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-10-15 05:17:10,370]\u001b[0m Finished trial#3 resulted in value: 15.051999999999998. Current best value is 14.9 with parameters: {'activation': 'tanh', 'num_hidden_layer': 3, 'i': 6, 'learning_rate': 0.0003606405459716391}.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Score for fold 5: loss of 13.136285781860352\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 3s - loss: 17.8721 - mse: 17.8721 - mae: 1.7027 - val_loss: 10.6842 - val_mse: 10.6842 - val_mae: 1.5904 - lr: 2.6516e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 3s - loss: 16.8248 - mse: 16.8248 - mae: 1.6190 - val_loss: 10.6072 - val_mse: 10.6072 - val_mae: 1.5484 - lr: 2.6516e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 4s - loss: 16.7109 - mse: 16.7109 - mae: 1.6109 - val_loss: 10.4763 - val_mse: 10.4763 - val_mae: 1.5420 - lr: 2.6516e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 16.5999 - mse: 16.5999 - mae: 1.6044 - val_loss: 10.3439 - val_mse: 10.3439 - val_mae: 1.5593 - lr: 2.6516e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 16.5262 - mse: 16.5262 - mae: 1.6003 - val_loss: 10.2872 - val_mse: 10.2872 - val_mae: 1.5555 - lr: 2.6516e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 16.4833 - mse: 16.4833 - mae: 1.5989 - val_loss: 10.2456 - val_mse: 10.2456 - val_mae: 1.5961 - lr: 2.6516e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 16.4426 - mse: 16.4426 - mae: 1.5985 - val_loss: 10.2530 - val_mse: 10.2530 - val_mae: 1.5382 - lr: 2.6516e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 2s - loss: 16.4172 - mse: 16.4172 - mae: 1.5941 - val_loss: 10.2827 - val_mse: 10.2827 - val_mae: 1.5230 - lr: 2.6516e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 2s - loss: 16.3967 - mse: 16.3967 - mae: 1.5937 - val_loss: 10.2084 - val_mse: 10.2084 - val_mae: 1.6300 - lr: 2.6516e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 2s - loss: 16.3683 - mse: 16.3683 - mae: 1.5952 - val_loss: 10.1658 - val_mse: 10.1658 - val_mae: 1.5518 - lr: 2.6516e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 2s - loss: 16.3521 - mse: 16.3521 - mae: 1.5952 - val_loss: 10.3049 - val_mse: 10.3049 - val_mae: 1.5421 - lr: 2.6516e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 2s - loss: 16.3489 - mse: 16.3489 - mae: 1.5931 - val_loss: 10.1952 - val_mse: 10.1952 - val_mae: 1.5485 - lr: 2.6516e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 13/100\n",
            "1000/1000 - 2s - loss: 16.3102 - mse: 16.3102 - mae: 1.5952 - val_loss: 10.1084 - val_mse: 10.1084 - val_mae: 1.5879 - lr: 2.6516e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 14/100\n",
            "1000/1000 - 2s - loss: 16.3086 - mse: 16.3086 - mae: 1.5943 - val_loss: 10.1408 - val_mse: 10.1408 - val_mae: 1.5715 - lr: 2.6516e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 15/100\n",
            "1000/1000 - 2s - loss: 16.3004 - mse: 16.3004 - mae: 1.5917 - val_loss: 10.1610 - val_mse: 10.1610 - val_mae: 1.5635 - lr: 2.6516e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 16/100\n",
            "1000/1000 - 2s - loss: 16.2825 - mse: 16.2825 - mae: 1.5955 - val_loss: 10.1325 - val_mse: 10.1325 - val_mae: 1.5606 - lr: 2.6516e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 17/100\n",
            "1000/1000 - 2s - loss: 16.2676 - mse: 16.2676 - mae: 1.5948 - val_loss: 10.1352 - val_mse: 10.1352 - val_mae: 1.5768 - lr: 2.6516e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 18/100\n",
            "1000/1000 - 2s - loss: 16.2765 - mse: 16.2765 - mae: 1.5918 - val_loss: 10.1557 - val_mse: 10.1557 - val_mae: 1.5583 - lr: 2.6516e-04 - 2s/epoch - 2ms/step\n",
            "Score for fold 1: loss of 10.155733108520508\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 14.9225 - mse: 14.9225 - mae: 1.5865 - val_loss: 15.4454 - val_mse: 15.4454 - val_mae: 1.5956 - lr: 2.6516e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 14.8996 - mse: 14.8996 - mae: 1.5915 - val_loss: 15.4950 - val_mse: 15.4950 - val_mae: 1.5842 - lr: 2.6516e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 14.8748 - mse: 14.8748 - mae: 1.5904 - val_loss: 15.5502 - val_mse: 15.5502 - val_mae: 1.5816 - lr: 2.6516e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 14.8692 - mse: 14.8692 - mae: 1.5856 - val_loss: 15.4952 - val_mse: 15.4952 - val_mae: 1.5736 - lr: 2.6516e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 14.8492 - mse: 14.8492 - mae: 1.5829 - val_loss: 15.5382 - val_mse: 15.5382 - val_mae: 1.6281 - lr: 2.6516e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 14.8431 - mse: 14.8431 - mae: 1.5849 - val_loss: 15.5411 - val_mse: 15.5411 - val_mae: 1.5898 - lr: 2.6516e-04 - 2s/epoch - 2ms/step\n",
            "Score for fold 2: loss of 15.541107177734375\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 15.9099 - mse: 15.9099 - mae: 1.5923 - val_loss: 11.2432 - val_mse: 11.2432 - val_mae: 1.5621 - lr: 2.6516e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 15.8716 - mse: 15.8716 - mae: 1.5874 - val_loss: 11.2978 - val_mse: 11.2978 - val_mae: 1.5641 - lr: 2.6516e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 15.8678 - mse: 15.8678 - mae: 1.5874 - val_loss: 11.2094 - val_mse: 11.2094 - val_mae: 1.5721 - lr: 2.6516e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 15.8500 - mse: 15.8500 - mae: 1.5870 - val_loss: 11.2967 - val_mse: 11.2967 - val_mae: 1.5506 - lr: 2.6516e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 15.8288 - mse: 15.8288 - mae: 1.5850 - val_loss: 11.3186 - val_mse: 11.3186 - val_mae: 1.5695 - lr: 2.6516e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 15.8470 - mse: 15.8470 - mae: 1.5875 - val_loss: 11.3388 - val_mse: 11.3388 - val_mae: 1.5752 - lr: 2.6516e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 15.8140 - mse: 15.8140 - mae: 1.5847 - val_loss: 11.3395 - val_mse: 11.3395 - val_mae: 1.5605 - lr: 2.6516e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 2s - loss: 15.8066 - mse: 15.8066 - mae: 1.5885 - val_loss: 11.2615 - val_mse: 11.2615 - val_mae: 1.5696 - lr: 2.6516e-04 - 2s/epoch - 2ms/step\n",
            "Score for fold 3: loss of 11.261544227600098\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 13.6268 - mse: 13.6268 - mae: 1.5778 - val_loss: 19.9790 - val_mse: 19.9790 - val_mae: 1.5788 - lr: 2.6516e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 13.6124 - mse: 13.6124 - mae: 1.5783 - val_loss: 19.9569 - val_mse: 19.9569 - val_mae: 1.5842 - lr: 2.6516e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 13.6084 - mse: 13.6084 - mae: 1.5764 - val_loss: 19.9998 - val_mse: 19.9998 - val_mae: 1.6267 - lr: 2.6516e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 13.6117 - mse: 13.6117 - mae: 1.5745 - val_loss: 19.9746 - val_mse: 19.9746 - val_mae: 1.6129 - lr: 2.6516e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 13.5975 - mse: 13.5975 - mae: 1.5758 - val_loss: 20.2657 - val_mse: 20.2657 - val_mae: 1.5453 - lr: 2.6516e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 13.5880 - mse: 13.5880 - mae: 1.5738 - val_loss: 20.0796 - val_mse: 20.0796 - val_mae: 1.5810 - lr: 2.6516e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 13.5871 - mse: 13.5871 - mae: 1.5736 - val_loss: 20.0272 - val_mse: 20.0272 - val_mae: 1.6074 - lr: 2.6516e-04 - 2s/epoch - 2ms/step\n",
            "Score for fold 4: loss of 20.027158737182617\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 14.1170 - mse: 14.1170 - mae: 1.5661 - val_loss: 17.7867 - val_mse: 17.7867 - val_mae: 1.5880 - lr: 2.6516e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 14.0996 - mse: 14.0996 - mae: 1.5668 - val_loss: 17.7845 - val_mse: 17.7845 - val_mae: 1.5990 - lr: 2.6516e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 14.1248 - mse: 14.1248 - mae: 1.5647 - val_loss: 17.7672 - val_mse: 17.7672 - val_mae: 1.6161 - lr: 2.6516e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 14.1008 - mse: 14.1008 - mae: 1.5646 - val_loss: 17.7248 - val_mse: 17.7248 - val_mae: 1.6015 - lr: 2.6516e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 14.0752 - mse: 14.0752 - mae: 1.5659 - val_loss: 17.7664 - val_mse: 17.7664 - val_mae: 1.6981 - lr: 2.6516e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 14.0661 - mse: 14.0661 - mae: 1.5658 - val_loss: 17.9406 - val_mse: 17.9406 - val_mae: 1.5788 - lr: 2.6516e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 14.0925 - mse: 14.0925 - mae: 1.5610 - val_loss: 17.7461 - val_mse: 17.7461 - val_mae: 1.6012 - lr: 2.6516e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 2s - loss: 14.0713 - mse: 14.0713 - mae: 1.5609 - val_loss: 17.7241 - val_mse: 17.7241 - val_mae: 1.6119 - lr: 2.6516e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 2s - loss: 14.0565 - mse: 14.0565 - mae: 1.5634 - val_loss: 17.8709 - val_mse: 17.8709 - val_mae: 1.5732 - lr: 2.6516e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 3s - loss: 14.0478 - mse: 14.0478 - mae: 1.5611 - val_loss: 17.7366 - val_mse: 17.7366 - val_mae: 1.6108 - lr: 2.6516e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 2s - loss: 14.0600 - mse: 14.0600 - mae: 1.5605 - val_loss: 17.7148 - val_mse: 17.7148 - val_mae: 1.6173 - lr: 2.6516e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 2s - loss: 14.0471 - mse: 14.0471 - mae: 1.5606 - val_loss: 17.8206 - val_mse: 17.8206 - val_mae: 1.6150 - lr: 2.6516e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 13/100\n",
            "1000/1000 - 3s - loss: 14.0616 - mse: 14.0616 - mae: 1.5638 - val_loss: 17.7886 - val_mse: 17.7886 - val_mae: 1.6405 - lr: 2.6516e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 14/100\n",
            "1000/1000 - 4s - loss: 14.0286 - mse: 14.0286 - mae: 1.5603 - val_loss: 17.7336 - val_mse: 17.7336 - val_mae: 1.6239 - lr: 2.6516e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 15/100\n",
            "1000/1000 - 2s - loss: 14.0424 - mse: 14.0424 - mae: 1.5621 - val_loss: 17.8920 - val_mse: 17.8920 - val_mae: 1.6635 - lr: 2.6516e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 16/100\n",
            "1000/1000 - 2s - loss: 14.0440 - mse: 14.0440 - mae: 1.5611 - val_loss: 17.7584 - val_mse: 17.7584 - val_mae: 1.6062 - lr: 2.6516e-04 - 2s/epoch - 2ms/step\n",
            "Score for fold 5: loss of 17.75837516784668\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-10-15 05:19:35,815]\u001b[0m Finished trial#4 resulted in value: 14.95. Current best value is 14.9 with parameters: {'activation': 'tanh', 'num_hidden_layer': 3, 'i': 6, 'learning_rate': 0.0003606405459716391}.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 11s - loss: 18.9533 - mse: 18.9533 - mae: 1.9053 - val_loss: 14.1971 - val_mse: 14.1971 - val_mae: 1.8474 - lr: 0.0072 - 11s/epoch - 11ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 10s - loss: 17.2355 - mse: 17.2355 - mae: 1.7660 - val_loss: 13.5981 - val_mse: 13.5981 - val_mae: 1.6652 - lr: 0.0072 - 10s/epoch - 10ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 10s - loss: 17.3490 - mse: 17.3490 - mae: 1.7972 - val_loss: 14.9391 - val_mse: 14.9391 - val_mae: 1.6641 - lr: 0.0072 - 10s/epoch - 10ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 10s - loss: 17.2312 - mse: 17.2312 - mae: 1.7795 - val_loss: 13.4377 - val_mse: 13.4377 - val_mae: 1.5702 - lr: 0.0072 - 10s/epoch - 10ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 10s - loss: 17.2470 - mse: 17.2470 - mae: 1.7812 - val_loss: 13.4222 - val_mse: 13.4222 - val_mae: 1.6490 - lr: 0.0072 - 10s/epoch - 10ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 9s - loss: 17.0599 - mse: 17.0599 - mae: 1.7709 - val_loss: 13.6482 - val_mse: 13.6482 - val_mae: 1.7068 - lr: 0.0072 - 9s/epoch - 9ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 10s - loss: 17.4665 - mse: 17.4665 - mae: 1.8080 - val_loss: 13.7187 - val_mse: 13.7187 - val_mae: 1.7920 - lr: 0.0072 - 10s/epoch - 10ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 10s - loss: 17.1527 - mse: 17.1527 - mae: 1.7705 - val_loss: 13.8925 - val_mse: 13.8925 - val_mae: 1.7230 - lr: 0.0072 - 10s/epoch - 10ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 9s - loss: 17.2870 - mse: 17.2870 - mae: 1.7900 - val_loss: 13.4491 - val_mse: 13.4491 - val_mae: 1.6386 - lr: 0.0072 - 9s/epoch - 9ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 10s - loss: 17.2774 - mse: 17.2774 - mae: 1.7858 - val_loss: 13.8921 - val_mse: 13.8921 - val_mae: 1.5321 - lr: 0.0072 - 10s/epoch - 10ms/step\n",
            "Score for fold 1: loss of 13.892088890075684\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 10s - loss: 16.1265 - mse: 16.1265 - mae: 1.6463 - val_loss: 13.9819 - val_mse: 13.9819 - val_mae: 1.6032 - lr: 0.0014 - 10s/epoch - 10ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 10s - loss: 16.1154 - mse: 16.1154 - mae: 1.6463 - val_loss: 14.0319 - val_mse: 14.0319 - val_mae: 1.6696 - lr: 0.0014 - 10s/epoch - 10ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 10s - loss: 16.1046 - mse: 16.1046 - mae: 1.6504 - val_loss: 14.0500 - val_mse: 14.0500 - val_mae: 1.6115 - lr: 0.0014 - 10s/epoch - 10ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 10s - loss: 16.1304 - mse: 16.1304 - mae: 1.6521 - val_loss: 14.1483 - val_mse: 14.1483 - val_mae: 1.5776 - lr: 0.0014 - 10s/epoch - 10ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 10s - loss: 16.0965 - mse: 16.0965 - mae: 1.6496 - val_loss: 14.0443 - val_mse: 14.0443 - val_mae: 1.5957 - lr: 0.0014 - 10s/epoch - 10ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 10s - loss: 16.0983 - mse: 16.0983 - mae: 1.6519 - val_loss: 14.2290 - val_mse: 14.2290 - val_mae: 1.7047 - lr: 0.0014 - 10s/epoch - 10ms/step\n",
            "Score for fold 2: loss of 14.22899055480957\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 10s - loss: 15.8065 - mse: 15.8065 - mae: 1.6404 - val_loss: 15.3398 - val_mse: 15.3398 - val_mae: 1.6451 - lr: 0.0010 - 10s/epoch - 10ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 10s - loss: 15.7703 - mse: 15.7703 - mae: 1.6363 - val_loss: 15.4469 - val_mse: 15.4469 - val_mae: 1.6319 - lr: 0.0010 - 10s/epoch - 10ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 9s - loss: 15.7512 - mse: 15.7512 - mae: 1.6413 - val_loss: 15.3200 - val_mse: 15.3200 - val_mae: 1.6364 - lr: 0.0010 - 9s/epoch - 9ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 12s - loss: 15.7978 - mse: 15.7978 - mae: 1.6448 - val_loss: 15.5083 - val_mse: 15.5083 - val_mae: 1.5825 - lr: 0.0010 - 12s/epoch - 12ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 10s - loss: 15.7702 - mse: 15.7702 - mae: 1.6407 - val_loss: 15.4034 - val_mse: 15.4034 - val_mae: 1.6259 - lr: 0.0010 - 10s/epoch - 10ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 10s - loss: 15.8124 - mse: 15.8124 - mae: 1.6401 - val_loss: 15.2659 - val_mse: 15.2659 - val_mae: 1.6634 - lr: 0.0010 - 10s/epoch - 10ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 10s - loss: 15.8117 - mse: 15.8117 - mae: 1.6432 - val_loss: 15.2810 - val_mse: 15.2810 - val_mae: 1.6626 - lr: 0.0010 - 10s/epoch - 10ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 9s - loss: 15.7892 - mse: 15.7892 - mae: 1.6393 - val_loss: 15.4616 - val_mse: 15.4616 - val_mae: 1.6340 - lr: 0.0010 - 9s/epoch - 9ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 10s - loss: 15.7918 - mse: 15.7918 - mae: 1.6410 - val_loss: 15.3358 - val_mse: 15.3358 - val_mae: 1.6517 - lr: 0.0010 - 10s/epoch - 10ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 10s - loss: 15.7900 - mse: 15.7900 - mae: 1.6421 - val_loss: 15.3443 - val_mse: 15.3443 - val_mae: 1.6334 - lr: 0.0010 - 10s/epoch - 10ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 10s - loss: 15.7282 - mse: 15.7282 - mae: 1.6421 - val_loss: 15.4964 - val_mse: 15.4964 - val_mae: 1.6200 - lr: 0.0010 - 10s/epoch - 10ms/step\n",
            "Score for fold 3: loss of 15.496394157409668\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 10s - loss: 15.4393 - mse: 15.4393 - mae: 1.6383 - val_loss: 16.5511 - val_mse: 16.5511 - val_mae: 1.6464 - lr: 0.0010 - 10s/epoch - 10ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 10s - loss: 15.4897 - mse: 15.4897 - mae: 1.6382 - val_loss: 16.5132 - val_mse: 16.5132 - val_mae: 1.7175 - lr: 0.0010 - 10s/epoch - 10ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 12s - loss: 15.4482 - mse: 15.4482 - mae: 1.6419 - val_loss: 16.6336 - val_mse: 16.6336 - val_mae: 1.6531 - lr: 0.0010 - 12s/epoch - 12ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 10s - loss: 15.4765 - mse: 15.4765 - mae: 1.6377 - val_loss: 16.6130 - val_mse: 16.6130 - val_mae: 1.6589 - lr: 0.0010 - 10s/epoch - 10ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 10s - loss: 15.4808 - mse: 15.4808 - mae: 1.6449 - val_loss: 16.6751 - val_mse: 16.6751 - val_mae: 1.6830 - lr: 0.0010 - 10s/epoch - 10ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 10s - loss: 15.4295 - mse: 15.4295 - mae: 1.6383 - val_loss: 16.6359 - val_mse: 16.6359 - val_mae: 1.6355 - lr: 0.0010 - 10s/epoch - 10ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 10s - loss: 15.4527 - mse: 15.4527 - mae: 1.6372 - val_loss: 16.4940 - val_mse: 16.4940 - val_mae: 1.6794 - lr: 0.0010 - 10s/epoch - 10ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 10s - loss: 15.4769 - mse: 15.4769 - mae: 1.6414 - val_loss: 16.6861 - val_mse: 16.6861 - val_mae: 1.6508 - lr: 0.0010 - 10s/epoch - 10ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 9s - loss: 15.4920 - mse: 15.4920 - mae: 1.6370 - val_loss: 16.6530 - val_mse: 16.6530 - val_mae: 1.6667 - lr: 0.0010 - 9s/epoch - 9ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 10s - loss: 15.4852 - mse: 15.4852 - mae: 1.6468 - val_loss: 16.5666 - val_mse: 16.5666 - val_mae: 1.6381 - lr: 0.0010 - 10s/epoch - 10ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 10s - loss: 15.5229 - mse: 15.5229 - mae: 1.6400 - val_loss: 16.8468 - val_mse: 16.8468 - val_mae: 1.7740 - lr: 0.0010 - 10s/epoch - 10ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 10s - loss: 15.4300 - mse: 15.4300 - mae: 1.6454 - val_loss: 16.5923 - val_mse: 16.5923 - val_mae: 1.6250 - lr: 0.0010 - 10s/epoch - 10ms/step\n",
            "Score for fold 4: loss of 16.592254638671875\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 12s - loss: 14.8683 - mse: 14.8683 - mae: 1.6447 - val_loss: 18.9650 - val_mse: 18.9650 - val_mae: 1.6290 - lr: 0.0010 - 12s/epoch - 12ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 10s - loss: 14.9157 - mse: 14.9157 - mae: 1.6542 - val_loss: 19.0752 - val_mse: 19.0752 - val_mae: 1.6152 - lr: 0.0010 - 10s/epoch - 10ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 10s - loss: 14.8765 - mse: 14.8765 - mae: 1.6554 - val_loss: 18.8993 - val_mse: 18.8993 - val_mae: 1.6381 - lr: 0.0010 - 10s/epoch - 10ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 10s - loss: 14.9459 - mse: 14.9459 - mae: 1.6500 - val_loss: 19.0721 - val_mse: 19.0721 - val_mae: 1.6059 - lr: 0.0010 - 10s/epoch - 10ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 10s - loss: 14.9588 - mse: 14.9588 - mae: 1.6514 - val_loss: 18.9995 - val_mse: 18.9995 - val_mae: 1.6107 - lr: 0.0010 - 10s/epoch - 10ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 10s - loss: 14.9361 - mse: 14.9361 - mae: 1.6508 - val_loss: 19.2524 - val_mse: 19.2524 - val_mae: 1.6066 - lr: 0.0010 - 10s/epoch - 10ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 10s - loss: 15.0158 - mse: 15.0158 - mae: 1.6564 - val_loss: 19.3803 - val_mse: 19.3803 - val_mae: 1.5972 - lr: 0.0010 - 10s/epoch - 10ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 10s - loss: 14.8954 - mse: 14.8954 - mae: 1.6463 - val_loss: 18.9795 - val_mse: 18.9795 - val_mae: 1.6224 - lr: 0.0010 - 10s/epoch - 10ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-10-15 05:29:13,478]\u001b[0m Finished trial#5 resulted in value: 15.838000000000003. Current best value is 14.9 with parameters: {'activation': 'tanh', 'num_hidden_layer': 3, 'i': 6, 'learning_rate': 0.0003606405459716391}.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Score for fold 5: loss of 18.979503631591797\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 3s - loss: 16.1021 - mse: 16.1021 - mae: 1.6473 - val_loss: 13.7826 - val_mse: 13.7826 - val_mae: 1.5756 - lr: 0.0058 - 3s/epoch - 3ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 15.8211 - mse: 15.8211 - mae: 1.6349 - val_loss: 13.0850 - val_mse: 13.0850 - val_mae: 1.6340 - lr: 0.0058 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 15.7655 - mse: 15.7655 - mae: 1.6227 - val_loss: 13.3262 - val_mse: 13.3262 - val_mae: 1.5248 - lr: 0.0058 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 15.7285 - mse: 15.7285 - mae: 1.6210 - val_loss: 13.3724 - val_mse: 13.3724 - val_mae: 1.6200 - lr: 0.0058 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 15.7431 - mse: 15.7431 - mae: 1.6219 - val_loss: 13.0823 - val_mse: 13.0823 - val_mae: 1.5957 - lr: 0.0058 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 15.7432 - mse: 15.7432 - mae: 1.6222 - val_loss: 13.1371 - val_mse: 13.1371 - val_mae: 1.5341 - lr: 0.0058 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 15.6608 - mse: 15.6608 - mae: 1.6192 - val_loss: 13.4538 - val_mse: 13.4538 - val_mae: 1.5562 - lr: 0.0058 - 2s/epoch - 2ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 2s - loss: 15.7348 - mse: 15.7348 - mae: 1.6196 - val_loss: 13.0343 - val_mse: 13.0343 - val_mae: 1.5048 - lr: 0.0058 - 2s/epoch - 2ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 2s - loss: 15.7033 - mse: 15.7033 - mae: 1.6181 - val_loss: 13.1365 - val_mse: 13.1365 - val_mae: 1.5484 - lr: 0.0058 - 2s/epoch - 2ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 2s - loss: 15.6475 - mse: 15.6475 - mae: 1.6185 - val_loss: 13.1500 - val_mse: 13.1500 - val_mae: 1.5384 - lr: 0.0058 - 2s/epoch - 2ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 2s - loss: 15.7013 - mse: 15.7013 - mae: 1.6144 - val_loss: 12.9377 - val_mse: 12.9377 - val_mae: 1.6670 - lr: 0.0058 - 2s/epoch - 2ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 2s - loss: 15.5995 - mse: 15.5995 - mae: 1.6113 - val_loss: 13.1528 - val_mse: 13.1528 - val_mae: 1.5441 - lr: 0.0058 - 2s/epoch - 2ms/step\n",
            "Epoch 13/100\n",
            "1000/1000 - 2s - loss: 15.6699 - mse: 15.6699 - mae: 1.6108 - val_loss: 12.9043 - val_mse: 12.9043 - val_mae: 1.5949 - lr: 0.0058 - 2s/epoch - 2ms/step\n",
            "Epoch 14/100\n",
            "1000/1000 - 2s - loss: 15.6330 - mse: 15.6330 - mae: 1.6054 - val_loss: 13.0500 - val_mse: 13.0500 - val_mae: 1.5638 - lr: 0.0058 - 2s/epoch - 2ms/step\n",
            "Epoch 15/100\n",
            "1000/1000 - 2s - loss: 15.6398 - mse: 15.6398 - mae: 1.6074 - val_loss: 12.7870 - val_mse: 12.7870 - val_mae: 1.5945 - lr: 0.0058 - 2s/epoch - 2ms/step\n",
            "Epoch 16/100\n",
            "1000/1000 - 2s - loss: 15.6486 - mse: 15.6486 - mae: 1.6090 - val_loss: 13.0679 - val_mse: 13.0679 - val_mae: 1.5503 - lr: 0.0058 - 2s/epoch - 2ms/step\n",
            "Epoch 17/100\n",
            "1000/1000 - 2s - loss: 15.6217 - mse: 15.6217 - mae: 1.6090 - val_loss: 12.9394 - val_mse: 12.9394 - val_mae: 1.5667 - lr: 0.0058 - 2s/epoch - 2ms/step\n",
            "Epoch 18/100\n",
            "1000/1000 - 2s - loss: 15.5891 - mse: 15.5891 - mae: 1.6092 - val_loss: 12.9669 - val_mse: 12.9669 - val_mae: 1.5694 - lr: 0.0058 - 2s/epoch - 2ms/step\n",
            "Epoch 19/100\n",
            "1000/1000 - 2s - loss: 15.5688 - mse: 15.5688 - mae: 1.6096 - val_loss: 12.8161 - val_mse: 12.8161 - val_mae: 1.6376 - lr: 0.0058 - 2s/epoch - 2ms/step\n",
            "Epoch 20/100\n",
            "1000/1000 - 2s - loss: 15.6280 - mse: 15.6280 - mae: 1.6124 - val_loss: 12.8641 - val_mse: 12.8641 - val_mae: 1.5967 - lr: 0.0058 - 2s/epoch - 2ms/step\n",
            "Score for fold 1: loss of 12.864100456237793\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 13.0615 - mse: 13.0615 - mae: 1.5813 - val_loss: 22.4570 - val_mse: 22.4570 - val_mae: 1.5880 - lr: 0.0012 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 13.0466 - mse: 13.0466 - mae: 1.5747 - val_loss: 22.4726 - val_mse: 22.4726 - val_mae: 1.6007 - lr: 0.0012 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 13.0348 - mse: 13.0348 - mae: 1.5808 - val_loss: 22.3440 - val_mse: 22.3440 - val_mae: 1.6072 - lr: 0.0012 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 13.0105 - mse: 13.0105 - mae: 1.5767 - val_loss: 22.3434 - val_mse: 22.3434 - val_mae: 1.5819 - lr: 0.0012 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 3s - loss: 12.9890 - mse: 12.9890 - mae: 1.5776 - val_loss: 22.4014 - val_mse: 22.4014 - val_mae: 1.5942 - lr: 0.0012 - 3s/epoch - 3ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 12.9963 - mse: 12.9963 - mae: 1.5802 - val_loss: 22.3935 - val_mse: 22.3935 - val_mae: 1.6052 - lr: 0.0012 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 13.0219 - mse: 13.0219 - mae: 1.5777 - val_loss: 22.4266 - val_mse: 22.4266 - val_mae: 1.5800 - lr: 0.0012 - 2s/epoch - 2ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 2s - loss: 12.9994 - mse: 12.9994 - mae: 1.5813 - val_loss: 22.3916 - val_mse: 22.3916 - val_mae: 1.5984 - lr: 0.0012 - 2s/epoch - 2ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 2s - loss: 12.9609 - mse: 12.9609 - mae: 1.5779 - val_loss: 22.4898 - val_mse: 22.4898 - val_mae: 1.5710 - lr: 0.0012 - 2s/epoch - 2ms/step\n",
            "Score for fold 2: loss of 22.489835739135742\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 13.7850 - mse: 13.7850 - mae: 1.5620 - val_loss: 19.2187 - val_mse: 19.2187 - val_mae: 1.6172 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 13.7376 - mse: 13.7376 - mae: 1.5607 - val_loss: 19.1314 - val_mse: 19.1314 - val_mae: 1.7087 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 13.7418 - mse: 13.7418 - mae: 1.5584 - val_loss: 19.3583 - val_mse: 19.3583 - val_mae: 1.6429 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 13.7428 - mse: 13.7428 - mae: 1.5597 - val_loss: 19.2732 - val_mse: 19.2732 - val_mae: 1.6385 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 13.7692 - mse: 13.7692 - mae: 1.5625 - val_loss: 19.1821 - val_mse: 19.1821 - val_mae: 1.6527 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 13.7532 - mse: 13.7532 - mae: 1.5589 - val_loss: 19.2916 - val_mse: 19.2916 - val_mae: 1.6531 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 13.7318 - mse: 13.7318 - mae: 1.5613 - val_loss: 19.3443 - val_mse: 19.3443 - val_mae: 1.6497 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Score for fold 3: loss of 19.34424591064453\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 16.1553 - mse: 16.1553 - mae: 1.5870 - val_loss: 9.7621 - val_mse: 9.7621 - val_mae: 1.5226 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 16.1324 - mse: 16.1324 - mae: 1.5883 - val_loss: 9.7762 - val_mse: 9.7762 - val_mae: 1.5371 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 16.1322 - mse: 16.1322 - mae: 1.5834 - val_loss: 9.9103 - val_mse: 9.9103 - val_mae: 1.5456 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 16.1646 - mse: 16.1646 - mae: 1.5876 - val_loss: 9.8168 - val_mse: 9.8168 - val_mae: 1.5252 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 16.1113 - mse: 16.1113 - mae: 1.5867 - val_loss: 9.7966 - val_mse: 9.7966 - val_mae: 1.5565 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 16.0888 - mse: 16.0888 - mae: 1.5840 - val_loss: 9.9397 - val_mse: 9.9397 - val_mae: 1.5405 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Score for fold 4: loss of 9.939663887023926\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 16.0050 - mse: 16.0050 - mae: 1.5867 - val_loss: 10.2109 - val_mse: 10.2109 - val_mae: 1.5436 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 15.9498 - mse: 15.9498 - mae: 1.5875 - val_loss: 10.2676 - val_mse: 10.2676 - val_mae: 1.5228 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 15.9712 - mse: 15.9712 - mae: 1.5864 - val_loss: 10.2087 - val_mse: 10.2087 - val_mae: 1.5267 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 15.9506 - mse: 15.9506 - mae: 1.5809 - val_loss: 10.1614 - val_mse: 10.1614 - val_mae: 1.5589 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 15.9518 - mse: 15.9518 - mae: 1.5821 - val_loss: 10.2642 - val_mse: 10.2642 - val_mae: 1.5683 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 15.9426 - mse: 15.9426 - mae: 1.5844 - val_loss: 10.2244 - val_mse: 10.2244 - val_mae: 1.5356 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 15.9191 - mse: 15.9191 - mae: 1.5816 - val_loss: 10.2824 - val_mse: 10.2824 - val_mae: 1.5304 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 2s - loss: 15.9156 - mse: 15.9156 - mae: 1.5823 - val_loss: 10.4507 - val_mse: 10.4507 - val_mae: 1.4965 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 2s - loss: 15.9340 - mse: 15.9340 - mae: 1.5838 - val_loss: 10.3690 - val_mse: 10.3690 - val_mae: 1.5465 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Score for fold 5: loss of 10.369013786315918\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-10-15 05:31:21,582]\u001b[0m Finished trial#6 resulted in value: 15.0. Current best value is 14.9 with parameters: {'activation': 'tanh', 'num_hidden_layer': 3, 'i': 6, 'learning_rate': 0.0003606405459716391}.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 3s - loss: 16.6217 - mse: 16.6217 - mae: 1.6761 - val_loss: 12.3863 - val_mse: 12.3863 - val_mae: 1.5910 - lr: 0.0043 - 3s/epoch - 3ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 3s - loss: 16.5121 - mse: 16.5121 - mae: 1.6518 - val_loss: 12.4592 - val_mse: 12.4592 - val_mae: 1.6354 - lr: 0.0043 - 3s/epoch - 3ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 16.5503 - mse: 16.5503 - mae: 1.6479 - val_loss: 12.2917 - val_mse: 12.2917 - val_mae: 1.5682 - lr: 0.0043 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 3s - loss: 16.5724 - mse: 16.5724 - mae: 1.6494 - val_loss: 12.3252 - val_mse: 12.3252 - val_mae: 1.6403 - lr: 0.0043 - 3s/epoch - 3ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 3s - loss: 16.4943 - mse: 16.4943 - mae: 1.6410 - val_loss: 12.1234 - val_mse: 12.1234 - val_mae: 1.6305 - lr: 0.0043 - 3s/epoch - 3ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 3s - loss: 16.4977 - mse: 16.4977 - mae: 1.6366 - val_loss: 12.3621 - val_mse: 12.3621 - val_mae: 1.7269 - lr: 0.0043 - 3s/epoch - 3ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 16.4068 - mse: 16.4068 - mae: 1.6288 - val_loss: 12.3080 - val_mse: 12.3080 - val_mae: 1.5809 - lr: 0.0043 - 2s/epoch - 2ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 3s - loss: 16.3187 - mse: 16.3187 - mae: 1.6182 - val_loss: 12.0090 - val_mse: 12.0090 - val_mae: 1.6098 - lr: 0.0043 - 3s/epoch - 3ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 3s - loss: 16.2010 - mse: 16.2010 - mae: 1.6113 - val_loss: 12.0097 - val_mse: 12.0097 - val_mae: 1.6375 - lr: 0.0043 - 3s/epoch - 3ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 2s - loss: 16.0813 - mse: 16.0813 - mae: 1.6115 - val_loss: 11.9900 - val_mse: 11.9900 - val_mae: 1.5487 - lr: 0.0043 - 2s/epoch - 2ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 3s - loss: 16.0434 - mse: 16.0434 - mae: 1.6107 - val_loss: 11.9805 - val_mse: 11.9805 - val_mae: 1.6322 - lr: 0.0043 - 3s/epoch - 3ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 3s - loss: 16.0322 - mse: 16.0322 - mae: 1.6098 - val_loss: 11.8678 - val_mse: 11.8678 - val_mae: 1.6416 - lr: 0.0043 - 3s/epoch - 3ms/step\n",
            "Epoch 13/100\n",
            "1000/1000 - 3s - loss: 16.0000 - mse: 16.0000 - mae: 1.6128 - val_loss: 11.8650 - val_mse: 11.8650 - val_mae: 1.6912 - lr: 0.0043 - 3s/epoch - 3ms/step\n",
            "Epoch 14/100\n",
            "1000/1000 - 3s - loss: 15.9975 - mse: 15.9975 - mae: 1.6123 - val_loss: 11.9753 - val_mse: 11.9753 - val_mae: 1.6122 - lr: 0.0043 - 3s/epoch - 3ms/step\n",
            "Epoch 15/100\n",
            "1000/1000 - 3s - loss: 15.9623 - mse: 15.9623 - mae: 1.6073 - val_loss: 11.8572 - val_mse: 11.8572 - val_mae: 1.6296 - lr: 0.0043 - 3s/epoch - 3ms/step\n",
            "Epoch 16/100\n",
            "1000/1000 - 2s - loss: 15.9868 - mse: 15.9868 - mae: 1.6066 - val_loss: 12.0909 - val_mse: 12.0909 - val_mae: 1.6691 - lr: 0.0043 - 2s/epoch - 2ms/step\n",
            "Epoch 17/100\n",
            "1000/1000 - 2s - loss: 15.9635 - mse: 15.9635 - mae: 1.6136 - val_loss: 11.8452 - val_mse: 11.8452 - val_mae: 1.5514 - lr: 0.0043 - 2s/epoch - 2ms/step\n",
            "Epoch 18/100\n",
            "1000/1000 - 3s - loss: 15.9217 - mse: 15.9217 - mae: 1.6113 - val_loss: 11.8814 - val_mse: 11.8814 - val_mae: 1.5849 - lr: 0.0043 - 3s/epoch - 3ms/step\n",
            "Epoch 19/100\n",
            "1000/1000 - 4s - loss: 15.9125 - mse: 15.9125 - mae: 1.6140 - val_loss: 11.8889 - val_mse: 11.8889 - val_mae: 1.6405 - lr: 0.0043 - 4s/epoch - 4ms/step\n",
            "Epoch 20/100\n",
            "1000/1000 - 4s - loss: 15.9298 - mse: 15.9298 - mae: 1.6096 - val_loss: 11.8804 - val_mse: 11.8804 - val_mae: 1.6576 - lr: 0.0043 - 4s/epoch - 4ms/step\n",
            "Epoch 21/100\n",
            "1000/1000 - 3s - loss: 15.9080 - mse: 15.9080 - mae: 1.6140 - val_loss: 11.8052 - val_mse: 11.8052 - val_mae: 1.6376 - lr: 0.0043 - 3s/epoch - 3ms/step\n",
            "Epoch 22/100\n",
            "1000/1000 - 3s - loss: 15.9145 - mse: 15.9145 - mae: 1.6083 - val_loss: 11.7454 - val_mse: 11.7454 - val_mae: 1.5747 - lr: 0.0043 - 3s/epoch - 3ms/step\n",
            "Epoch 23/100\n",
            "1000/1000 - 3s - loss: 15.9243 - mse: 15.9243 - mae: 1.6093 - val_loss: 11.9506 - val_mse: 11.9506 - val_mae: 1.5795 - lr: 0.0043 - 3s/epoch - 3ms/step\n",
            "Epoch 24/100\n",
            "1000/1000 - 3s - loss: 15.9307 - mse: 15.9307 - mae: 1.6101 - val_loss: 11.8058 - val_mse: 11.8058 - val_mae: 1.5951 - lr: 0.0043 - 3s/epoch - 3ms/step\n",
            "Epoch 25/100\n",
            "1000/1000 - 2s - loss: 15.9481 - mse: 15.9481 - mae: 1.6142 - val_loss: 11.8213 - val_mse: 11.8213 - val_mae: 1.6571 - lr: 0.0043 - 2s/epoch - 2ms/step\n",
            "Epoch 26/100\n",
            "1000/1000 - 3s - loss: 15.9019 - mse: 15.9019 - mae: 1.6081 - val_loss: 11.9505 - val_mse: 11.9505 - val_mae: 1.6424 - lr: 0.0043 - 3s/epoch - 3ms/step\n",
            "Epoch 27/100\n",
            "1000/1000 - 3s - loss: 15.9662 - mse: 15.9662 - mae: 1.6159 - val_loss: 12.0100 - val_mse: 12.0100 - val_mae: 1.6299 - lr: 0.0043 - 3s/epoch - 3ms/step\n",
            "Score for fold 1: loss of 12.010010719299316\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 3s - loss: 14.4515 - mse: 14.4515 - mae: 1.5795 - val_loss: 16.9539 - val_mse: 16.9539 - val_mae: 1.5659 - lr: 0.0010 - 3s/epoch - 3ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 3s - loss: 14.4627 - mse: 14.4627 - mae: 1.5775 - val_loss: 16.9237 - val_mse: 16.9237 - val_mae: 1.5489 - lr: 0.0010 - 3s/epoch - 3ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 3s - loss: 14.4452 - mse: 14.4452 - mae: 1.5720 - val_loss: 16.8761 - val_mse: 16.8761 - val_mae: 1.6133 - lr: 0.0010 - 3s/epoch - 3ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 3s - loss: 14.4294 - mse: 14.4294 - mae: 1.5789 - val_loss: 16.9762 - val_mse: 16.9762 - val_mae: 1.5674 - lr: 0.0010 - 3s/epoch - 3ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 3s - loss: 14.4158 - mse: 14.4158 - mae: 1.5757 - val_loss: 16.9299 - val_mse: 16.9299 - val_mae: 1.5731 - lr: 0.0010 - 3s/epoch - 3ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 14.4229 - mse: 14.4229 - mae: 1.5777 - val_loss: 16.9442 - val_mse: 16.9442 - val_mae: 1.6043 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 3s - loss: 14.4282 - mse: 14.4282 - mae: 1.5737 - val_loss: 16.9924 - val_mse: 16.9924 - val_mae: 1.6258 - lr: 0.0010 - 3s/epoch - 3ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 3s - loss: 14.4144 - mse: 14.4144 - mae: 1.5753 - val_loss: 16.9299 - val_mse: 16.9299 - val_mae: 1.6059 - lr: 0.0010 - 3s/epoch - 3ms/step\n",
            "Score for fold 2: loss of 16.929908752441406\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 3s - loss: 15.1853 - mse: 15.1853 - mae: 1.5746 - val_loss: 13.8222 - val_mse: 13.8222 - val_mae: 1.5775 - lr: 0.0010 - 3s/epoch - 3ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 15.1637 - mse: 15.1637 - mae: 1.5724 - val_loss: 13.8815 - val_mse: 13.8815 - val_mae: 1.5721 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 3s - loss: 15.1728 - mse: 15.1728 - mae: 1.5717 - val_loss: 13.8239 - val_mse: 13.8239 - val_mae: 1.6119 - lr: 0.0010 - 3s/epoch - 3ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 3s - loss: 15.1559 - mse: 15.1559 - mae: 1.5767 - val_loss: 13.9653 - val_mse: 13.9653 - val_mae: 1.5753 - lr: 0.0010 - 3s/epoch - 3ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 3s - loss: 15.1706 - mse: 15.1706 - mae: 1.5711 - val_loss: 13.8415 - val_mse: 13.8415 - val_mae: 1.5807 - lr: 0.0010 - 3s/epoch - 3ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 3s - loss: 15.1593 - mse: 15.1593 - mae: 1.5742 - val_loss: 13.7947 - val_mse: 13.7947 - val_mae: 1.6136 - lr: 0.0010 - 3s/epoch - 3ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 3s - loss: 15.1641 - mse: 15.1641 - mae: 1.5758 - val_loss: 13.9534 - val_mse: 13.9534 - val_mae: 1.5696 - lr: 0.0010 - 3s/epoch - 3ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 2s - loss: 15.1508 - mse: 15.1508 - mae: 1.5732 - val_loss: 13.8785 - val_mse: 13.8785 - val_mae: 1.5728 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 2s - loss: 15.1539 - mse: 15.1539 - mae: 1.5737 - val_loss: 13.8581 - val_mse: 13.8581 - val_mae: 1.5780 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 3s - loss: 15.1534 - mse: 15.1534 - mae: 1.5699 - val_loss: 13.8081 - val_mse: 13.8081 - val_mae: 1.6131 - lr: 0.0010 - 3s/epoch - 3ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 2s - loss: 15.1237 - mse: 15.1237 - mae: 1.5725 - val_loss: 13.9266 - val_mse: 13.9266 - val_mae: 1.5772 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Score for fold 3: loss of 13.92662525177002\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 3s - loss: 13.9639 - mse: 13.9639 - mae: 1.5738 - val_loss: 18.5875 - val_mse: 18.5875 - val_mae: 1.6071 - lr: 0.0010 - 3s/epoch - 3ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 13.9424 - mse: 13.9424 - mae: 1.5766 - val_loss: 18.5833 - val_mse: 18.5833 - val_mae: 1.5832 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 3s - loss: 13.9589 - mse: 13.9589 - mae: 1.5786 - val_loss: 18.6207 - val_mse: 18.6207 - val_mae: 1.5907 - lr: 0.0010 - 3s/epoch - 3ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 3s - loss: 13.9466 - mse: 13.9466 - mae: 1.5759 - val_loss: 18.6422 - val_mse: 18.6422 - val_mae: 1.5614 - lr: 0.0010 - 3s/epoch - 3ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 13.9469 - mse: 13.9469 - mae: 1.5782 - val_loss: 18.7656 - val_mse: 18.7656 - val_mae: 1.5221 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 3s - loss: 13.9287 - mse: 13.9287 - mae: 1.5775 - val_loss: 18.6608 - val_mse: 18.6608 - val_mae: 1.5706 - lr: 0.0010 - 3s/epoch - 3ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 3s - loss: 13.9712 - mse: 13.9712 - mae: 1.5742 - val_loss: 18.6333 - val_mse: 18.6333 - val_mae: 1.6087 - lr: 0.0010 - 3s/epoch - 3ms/step\n",
            "Score for fold 4: loss of 18.633342742919922\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 3s - loss: 15.2351 - mse: 15.2351 - mae: 1.5842 - val_loss: 13.3870 - val_mse: 13.3870 - val_mae: 1.5407 - lr: 0.0010 - 3s/epoch - 3ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 15.2599 - mse: 15.2599 - mae: 1.5838 - val_loss: 13.3926 - val_mse: 13.3926 - val_mae: 1.5480 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 3s - loss: 15.2468 - mse: 15.2468 - mae: 1.5828 - val_loss: 13.4497 - val_mse: 13.4497 - val_mae: 1.5584 - lr: 0.0010 - 3s/epoch - 3ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 3s - loss: 15.2622 - mse: 15.2622 - mae: 1.5826 - val_loss: 13.4453 - val_mse: 13.4453 - val_mae: 1.5603 - lr: 0.0010 - 3s/epoch - 3ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 3s - loss: 15.2274 - mse: 15.2274 - mae: 1.5855 - val_loss: 13.4447 - val_mse: 13.4447 - val_mae: 1.5729 - lr: 0.0010 - 3s/epoch - 3ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 15.2468 - mse: 15.2468 - mae: 1.5827 - val_loss: 13.4555 - val_mse: 13.4555 - val_mae: 1.5272 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Score for fold 5: loss of 13.455547332763672\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-10-15 05:34:13,329]\u001b[0m Finished trial#7 resulted in value: 14.992. Current best value is 14.9 with parameters: {'activation': 'tanh', 'num_hidden_layer': 3, 'i': 6, 'learning_rate': 0.0003606405459716391}.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 3s - loss: 18.0018 - mse: 18.0018 - mae: 1.7835 - val_loss: 15.1317 - val_mse: 15.1317 - val_mae: 1.6128 - lr: 2.1874e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 15.6809 - mse: 15.6809 - mae: 1.6246 - val_loss: 15.0694 - val_mse: 15.0694 - val_mae: 1.5882 - lr: 2.1874e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 15.6154 - mse: 15.6154 - mae: 1.6193 - val_loss: 14.9652 - val_mse: 14.9652 - val_mae: 1.6189 - lr: 2.1874e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 15.5722 - mse: 15.5722 - mae: 1.6183 - val_loss: 14.9563 - val_mse: 14.9563 - val_mae: 1.5828 - lr: 2.1874e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 3s - loss: 15.5427 - mse: 15.5427 - mae: 1.6144 - val_loss: 14.9069 - val_mse: 14.9069 - val_mae: 1.5834 - lr: 2.1874e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 15.5202 - mse: 15.5202 - mae: 1.6099 - val_loss: 14.8821 - val_mse: 14.8821 - val_mae: 1.5906 - lr: 2.1874e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 15.4962 - mse: 15.4962 - mae: 1.6102 - val_loss: 14.8470 - val_mse: 14.8470 - val_mae: 1.5944 - lr: 2.1874e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 2s - loss: 15.4683 - mse: 15.4683 - mae: 1.6131 - val_loss: 14.7984 - val_mse: 14.7984 - val_mae: 1.6076 - lr: 2.1874e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 2s - loss: 15.4359 - mse: 15.4359 - mae: 1.6090 - val_loss: 14.8321 - val_mse: 14.8321 - val_mae: 1.5783 - lr: 2.1874e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 2s - loss: 15.4374 - mse: 15.4374 - mae: 1.6106 - val_loss: 14.7888 - val_mse: 14.7888 - val_mae: 1.5920 - lr: 2.1874e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 2s - loss: 15.4129 - mse: 15.4129 - mae: 1.6061 - val_loss: 14.7667 - val_mse: 14.7667 - val_mae: 1.5900 - lr: 2.1874e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 2s - loss: 15.3952 - mse: 15.3952 - mae: 1.6035 - val_loss: 14.7806 - val_mse: 14.7806 - val_mae: 1.5764 - lr: 2.1874e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 13/100\n",
            "1000/1000 - 2s - loss: 15.3834 - mse: 15.3834 - mae: 1.6023 - val_loss: 14.7288 - val_mse: 14.7288 - val_mae: 1.5848 - lr: 2.1874e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 14/100\n",
            "1000/1000 - 2s - loss: 15.3637 - mse: 15.3637 - mae: 1.6021 - val_loss: 14.7150 - val_mse: 14.7150 - val_mae: 1.5852 - lr: 2.1874e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 15/100\n",
            "1000/1000 - 2s - loss: 15.3538 - mse: 15.3538 - mae: 1.5994 - val_loss: 14.6879 - val_mse: 14.6879 - val_mae: 1.6195 - lr: 2.1874e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 16/100\n",
            "1000/1000 - 2s - loss: 15.3331 - mse: 15.3331 - mae: 1.6036 - val_loss: 14.7069 - val_mse: 14.7069 - val_mae: 1.5775 - lr: 2.1874e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 17/100\n",
            "1000/1000 - 2s - loss: 15.3327 - mse: 15.3327 - mae: 1.5979 - val_loss: 14.6899 - val_mse: 14.6899 - val_mae: 1.5705 - lr: 2.1874e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 18/100\n",
            "1000/1000 - 2s - loss: 15.3156 - mse: 15.3156 - mae: 1.5971 - val_loss: 14.6760 - val_mse: 14.6760 - val_mae: 1.5884 - lr: 2.1874e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 19/100\n",
            "1000/1000 - 2s - loss: 15.3078 - mse: 15.3078 - mae: 1.5996 - val_loss: 14.6766 - val_mse: 14.6766 - val_mae: 1.5766 - lr: 2.1874e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 20/100\n",
            "1000/1000 - 2s - loss: 15.3027 - mse: 15.3027 - mae: 1.5947 - val_loss: 14.6474 - val_mse: 14.6474 - val_mae: 1.5898 - lr: 2.1874e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 21/100\n",
            "1000/1000 - 2s - loss: 15.2899 - mse: 15.2899 - mae: 1.5975 - val_loss: 14.6864 - val_mse: 14.6864 - val_mae: 1.5545 - lr: 2.1874e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 22/100\n",
            "1000/1000 - 2s - loss: 15.2870 - mse: 15.2870 - mae: 1.5916 - val_loss: 14.6287 - val_mse: 14.6287 - val_mae: 1.5822 - lr: 2.1874e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 23/100\n",
            "1000/1000 - 2s - loss: 15.2691 - mse: 15.2691 - mae: 1.5954 - val_loss: 14.6326 - val_mse: 14.6326 - val_mae: 1.5824 - lr: 2.1874e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 24/100\n",
            "1000/1000 - 2s - loss: 15.2661 - mse: 15.2661 - mae: 1.5964 - val_loss: 14.6240 - val_mse: 14.6240 - val_mae: 1.5748 - lr: 2.1874e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 25/100\n",
            "1000/1000 - 2s - loss: 15.2542 - mse: 15.2542 - mae: 1.5934 - val_loss: 14.6272 - val_mse: 14.6272 - val_mae: 1.5743 - lr: 2.1874e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 26/100\n",
            "1000/1000 - 2s - loss: 15.2533 - mse: 15.2533 - mae: 1.5936 - val_loss: 14.6162 - val_mse: 14.6162 - val_mae: 1.5648 - lr: 2.1874e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 27/100\n",
            "1000/1000 - 2s - loss: 15.2531 - mse: 15.2531 - mae: 1.5905 - val_loss: 14.6076 - val_mse: 14.6076 - val_mae: 1.5759 - lr: 2.1874e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 28/100\n",
            "1000/1000 - 2s - loss: 15.2388 - mse: 15.2388 - mae: 1.5902 - val_loss: 14.6062 - val_mse: 14.6062 - val_mae: 1.5757 - lr: 2.1874e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 29/100\n",
            "1000/1000 - 2s - loss: 15.2407 - mse: 15.2407 - mae: 1.5908 - val_loss: 14.5708 - val_mse: 14.5708 - val_mae: 1.5813 - lr: 2.1874e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 30/100\n",
            "1000/1000 - 2s - loss: 15.2183 - mse: 15.2183 - mae: 1.5942 - val_loss: 14.5930 - val_mse: 14.5930 - val_mae: 1.5712 - lr: 2.1874e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 31/100\n",
            "1000/1000 - 2s - loss: 15.2260 - mse: 15.2260 - mae: 1.5879 - val_loss: 14.5670 - val_mse: 14.5670 - val_mae: 1.5949 - lr: 2.1874e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 32/100\n",
            "1000/1000 - 2s - loss: 15.2085 - mse: 15.2085 - mae: 1.5879 - val_loss: 14.6199 - val_mse: 14.6199 - val_mae: 1.5569 - lr: 2.1874e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 33/100\n",
            "1000/1000 - 2s - loss: 15.2233 - mse: 15.2233 - mae: 1.5883 - val_loss: 14.5739 - val_mse: 14.5739 - val_mae: 1.5783 - lr: 2.1874e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 34/100\n",
            "1000/1000 - 2s - loss: 15.2150 - mse: 15.2150 - mae: 1.5886 - val_loss: 14.5738 - val_mse: 14.5738 - val_mae: 1.5777 - lr: 2.1874e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 35/100\n",
            "1000/1000 - 2s - loss: 15.2172 - mse: 15.2172 - mae: 1.5891 - val_loss: 14.5961 - val_mse: 14.5961 - val_mae: 1.5575 - lr: 2.1874e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 36/100\n",
            "1000/1000 - 2s - loss: 15.2104 - mse: 15.2104 - mae: 1.5886 - val_loss: 14.5590 - val_mse: 14.5590 - val_mae: 1.5872 - lr: 2.1874e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 37/100\n",
            "1000/1000 - 2s - loss: 15.2031 - mse: 15.2031 - mae: 1.5891 - val_loss: 14.5550 - val_mse: 14.5550 - val_mae: 1.5830 - lr: 2.1874e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 38/100\n",
            "1000/1000 - 2s - loss: 15.1944 - mse: 15.1944 - mae: 1.5878 - val_loss: 14.5893 - val_mse: 14.5893 - val_mae: 1.5608 - lr: 2.1874e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 39/100\n",
            "1000/1000 - 2s - loss: 15.1996 - mse: 15.1996 - mae: 1.5862 - val_loss: 14.5391 - val_mse: 14.5391 - val_mae: 1.5942 - lr: 2.1874e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 40/100\n",
            "1000/1000 - 2s - loss: 15.1865 - mse: 15.1865 - mae: 1.5896 - val_loss: 14.5681 - val_mse: 14.5681 - val_mae: 1.5648 - lr: 2.1874e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 41/100\n",
            "1000/1000 - 2s - loss: 15.1887 - mse: 15.1887 - mae: 1.5877 - val_loss: 14.5721 - val_mse: 14.5721 - val_mae: 1.5555 - lr: 2.1874e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 42/100\n",
            "1000/1000 - 2s - loss: 15.1921 - mse: 15.1921 - mae: 1.5853 - val_loss: 14.5539 - val_mse: 14.5539 - val_mae: 1.5736 - lr: 2.1874e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 43/100\n",
            "1000/1000 - 2s - loss: 15.1879 - mse: 15.1879 - mae: 1.5847 - val_loss: 14.5627 - val_mse: 14.5627 - val_mae: 1.5755 - lr: 2.1874e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 44/100\n",
            "1000/1000 - 2s - loss: 15.2009 - mse: 15.2009 - mae: 1.5858 - val_loss: 14.5535 - val_mse: 14.5535 - val_mae: 1.5818 - lr: 2.1874e-04 - 2s/epoch - 2ms/step\n",
            "Score for fold 1: loss of 14.553470611572266\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 13.9965 - mse: 13.9965 - mae: 1.5781 - val_loss: 19.3081 - val_mse: 19.3081 - val_mae: 1.6078 - lr: 2.1874e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 13.9902 - mse: 13.9902 - mae: 1.5737 - val_loss: 19.3316 - val_mse: 19.3316 - val_mae: 1.5960 - lr: 2.1874e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 13.9935 - mse: 13.9935 - mae: 1.5736 - val_loss: 19.3455 - val_mse: 19.3455 - val_mae: 1.5827 - lr: 2.1874e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 13.9868 - mse: 13.9868 - mae: 1.5734 - val_loss: 19.3472 - val_mse: 19.3472 - val_mae: 1.5929 - lr: 2.1874e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 13.9970 - mse: 13.9970 - mae: 1.5748 - val_loss: 19.3408 - val_mse: 19.3408 - val_mae: 1.5957 - lr: 2.1874e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 13.9835 - mse: 13.9835 - mae: 1.5738 - val_loss: 19.3229 - val_mse: 19.3229 - val_mae: 1.6051 - lr: 2.1874e-04 - 2s/epoch - 2ms/step\n",
            "Score for fold 2: loss of 19.322946548461914\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 15.0304 - mse: 15.0304 - mae: 1.5725 - val_loss: 15.1774 - val_mse: 15.1774 - val_mae: 1.5949 - lr: 2.1874e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 15.0275 - mse: 15.0275 - mae: 1.5712 - val_loss: 15.1476 - val_mse: 15.1476 - val_mae: 1.6056 - lr: 2.1874e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 15.0226 - mse: 15.0226 - mae: 1.5714 - val_loss: 15.1637 - val_mse: 15.1637 - val_mae: 1.6193 - lr: 2.1874e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 15.0162 - mse: 15.0162 - mae: 1.5710 - val_loss: 15.1651 - val_mse: 15.1651 - val_mae: 1.6120 - lr: 2.1874e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 15.0098 - mse: 15.0098 - mae: 1.5723 - val_loss: 15.1705 - val_mse: 15.1705 - val_mae: 1.6044 - lr: 2.1874e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 15.0131 - mse: 15.0131 - mae: 1.5693 - val_loss: 15.1730 - val_mse: 15.1730 - val_mae: 1.6146 - lr: 2.1874e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 15.0113 - mse: 15.0113 - mae: 1.5672 - val_loss: 15.1730 - val_mse: 15.1730 - val_mae: 1.6172 - lr: 2.1874e-04 - 2s/epoch - 2ms/step\n",
            "Score for fold 3: loss of 15.17297077178955\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 15.5552 - mse: 15.5552 - mae: 1.5810 - val_loss: 12.9831 - val_mse: 12.9831 - val_mae: 1.5576 - lr: 2.1874e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 15.5689 - mse: 15.5689 - mae: 1.5770 - val_loss: 12.9464 - val_mse: 12.9464 - val_mae: 1.5824 - lr: 2.1874e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 15.5491 - mse: 15.5491 - mae: 1.5800 - val_loss: 12.9660 - val_mse: 12.9660 - val_mae: 1.5689 - lr: 2.1874e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 15.5491 - mse: 15.5491 - mae: 1.5802 - val_loss: 12.9356 - val_mse: 12.9356 - val_mae: 1.5863 - lr: 2.1874e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 15.5370 - mse: 15.5370 - mae: 1.5805 - val_loss: 12.9555 - val_mse: 12.9555 - val_mae: 1.5751 - lr: 2.1874e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 15.5508 - mse: 15.5508 - mae: 1.5782 - val_loss: 12.9462 - val_mse: 12.9462 - val_mae: 1.5858 - lr: 2.1874e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 15.5354 - mse: 15.5354 - mae: 1.5801 - val_loss: 12.9792 - val_mse: 12.9792 - val_mae: 1.5657 - lr: 2.1874e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 2s - loss: 15.5369 - mse: 15.5369 - mae: 1.5824 - val_loss: 12.9595 - val_mse: 12.9595 - val_mae: 1.5576 - lr: 2.1874e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 2s - loss: 15.5389 - mse: 15.5389 - mae: 1.5780 - val_loss: 12.9525 - val_mse: 12.9525 - val_mae: 1.5749 - lr: 2.1874e-04 - 2s/epoch - 2ms/step\n",
            "Score for fold 4: loss of 12.952544212341309\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 15.4681 - mse: 15.4681 - mae: 1.5838 - val_loss: 13.2031 - val_mse: 13.2031 - val_mae: 1.5612 - lr: 2.1874e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 15.4601 - mse: 15.4601 - mae: 1.5884 - val_loss: 13.1937 - val_mse: 13.1937 - val_mae: 1.5509 - lr: 2.1874e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 3s - loss: 15.4531 - mse: 15.4531 - mae: 1.5826 - val_loss: 13.1923 - val_mse: 13.1923 - val_mae: 1.5723 - lr: 2.1874e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 15.4523 - mse: 15.4523 - mae: 1.5842 - val_loss: 13.1888 - val_mse: 13.1888 - val_mae: 1.5532 - lr: 2.1874e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 15.4443 - mse: 15.4443 - mae: 1.5839 - val_loss: 13.2081 - val_mse: 13.2081 - val_mae: 1.5538 - lr: 2.1874e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 15.4427 - mse: 15.4427 - mae: 1.5868 - val_loss: 13.2184 - val_mse: 13.2184 - val_mae: 1.5728 - lr: 2.1874e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 15.4452 - mse: 15.4452 - mae: 1.5844 - val_loss: 13.2174 - val_mse: 13.2174 - val_mae: 1.5475 - lr: 2.1874e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 2s - loss: 15.4482 - mse: 15.4482 - mae: 1.5831 - val_loss: 13.2048 - val_mse: 13.2048 - val_mae: 1.5513 - lr: 2.1874e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 2s - loss: 15.4409 - mse: 15.4409 - mae: 1.5849 - val_loss: 13.1984 - val_mse: 13.1984 - val_mae: 1.5669 - lr: 2.1874e-04 - 2s/epoch - 2ms/step\n",
            "Score for fold 5: loss of 13.198356628417969\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-10-15 05:36:48,846]\u001b[0m Finished trial#8 resulted in value: 15.038000000000002. Current best value is 14.9 with parameters: {'activation': 'tanh', 'num_hidden_layer': 3, 'i': 6, 'learning_rate': 0.0003606405459716391}.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 3s - loss: 18.5895 - mse: 18.5895 - mae: 1.7713 - val_loss: 11.5678 - val_mse: 11.5678 - val_mae: 1.6136 - lr: 2.0518e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 16.7201 - mse: 16.7201 - mae: 1.6198 - val_loss: 11.4976 - val_mse: 11.4976 - val_mae: 1.5984 - lr: 2.0518e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 16.6288 - mse: 16.6288 - mae: 1.6109 - val_loss: 11.3881 - val_mse: 11.3881 - val_mae: 1.5837 - lr: 2.0518e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 16.5467 - mse: 16.5467 - mae: 1.6077 - val_loss: 11.2758 - val_mse: 11.2758 - val_mae: 1.6047 - lr: 2.0518e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 16.4853 - mse: 16.4853 - mae: 1.6048 - val_loss: 11.2478 - val_mse: 11.2478 - val_mae: 1.5691 - lr: 2.0518e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 16.4510 - mse: 16.4510 - mae: 1.5999 - val_loss: 11.1780 - val_mse: 11.1780 - val_mae: 1.5798 - lr: 2.0518e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 16.4040 - mse: 16.4040 - mae: 1.5961 - val_loss: 11.1571 - val_mse: 11.1571 - val_mae: 1.5842 - lr: 2.0518e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 2s - loss: 16.3669 - mse: 16.3669 - mae: 1.5965 - val_loss: 11.1384 - val_mse: 11.1384 - val_mae: 1.5748 - lr: 2.0518e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 2s - loss: 16.3302 - mse: 16.3302 - mae: 1.5923 - val_loss: 11.1490 - val_mse: 11.1490 - val_mae: 1.5669 - lr: 2.0518e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 2s - loss: 16.3009 - mse: 16.3009 - mae: 1.5880 - val_loss: 11.0719 - val_mse: 11.0719 - val_mae: 1.5825 - lr: 2.0518e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 2s - loss: 16.2828 - mse: 16.2828 - mae: 1.5877 - val_loss: 11.1161 - val_mse: 11.1161 - val_mae: 1.5893 - lr: 2.0518e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 2s - loss: 16.2396 - mse: 16.2396 - mae: 1.5890 - val_loss: 11.0345 - val_mse: 11.0345 - val_mae: 1.5663 - lr: 2.0518e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 13/100\n",
            "1000/1000 - 2s - loss: 16.2384 - mse: 16.2384 - mae: 1.5889 - val_loss: 11.0040 - val_mse: 11.0040 - val_mae: 1.5906 - lr: 2.0518e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 14/100\n",
            "1000/1000 - 2s - loss: 16.2104 - mse: 16.2104 - mae: 1.5873 - val_loss: 10.9795 - val_mse: 10.9795 - val_mae: 1.5839 - lr: 2.0518e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 15/100\n",
            "1000/1000 - 2s - loss: 16.1937 - mse: 16.1937 - mae: 1.5840 - val_loss: 10.9876 - val_mse: 10.9876 - val_mae: 1.5787 - lr: 2.0518e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 16/100\n",
            "1000/1000 - 2s - loss: 16.1758 - mse: 16.1758 - mae: 1.5875 - val_loss: 10.9657 - val_mse: 10.9657 - val_mae: 1.5776 - lr: 2.0518e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 17/100\n",
            "1000/1000 - 2s - loss: 16.1508 - mse: 16.1508 - mae: 1.5870 - val_loss: 10.9869 - val_mse: 10.9869 - val_mae: 1.5569 - lr: 2.0518e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 18/100\n",
            "1000/1000 - 2s - loss: 16.1529 - mse: 16.1529 - mae: 1.5838 - val_loss: 11.0146 - val_mse: 11.0146 - val_mae: 1.5472 - lr: 2.0518e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 19/100\n",
            "1000/1000 - 2s - loss: 16.1467 - mse: 16.1467 - mae: 1.5834 - val_loss: 10.9662 - val_mse: 10.9662 - val_mae: 1.5596 - lr: 2.0518e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 20/100\n",
            "1000/1000 - 2s - loss: 16.1279 - mse: 16.1279 - mae: 1.5837 - val_loss: 10.9623 - val_mse: 10.9623 - val_mae: 1.5697 - lr: 2.0518e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 21/100\n",
            "1000/1000 - 2s - loss: 16.1083 - mse: 16.1083 - mae: 1.5825 - val_loss: 10.9298 - val_mse: 10.9298 - val_mae: 1.5794 - lr: 2.0518e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 22/100\n",
            "1000/1000 - 2s - loss: 16.1055 - mse: 16.1055 - mae: 1.5826 - val_loss: 10.9020 - val_mse: 10.9020 - val_mae: 1.6068 - lr: 2.0518e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 23/100\n",
            "1000/1000 - 2s - loss: 16.1053 - mse: 16.1053 - mae: 1.5826 - val_loss: 10.9745 - val_mse: 10.9745 - val_mae: 1.5475 - lr: 2.0518e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 24/100\n",
            "1000/1000 - 2s - loss: 16.0870 - mse: 16.0870 - mae: 1.5837 - val_loss: 10.9015 - val_mse: 10.9015 - val_mae: 1.5735 - lr: 2.0518e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 25/100\n",
            "1000/1000 - 2s - loss: 16.0856 - mse: 16.0856 - mae: 1.5810 - val_loss: 10.8824 - val_mse: 10.8824 - val_mae: 1.5840 - lr: 2.0518e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 26/100\n",
            "1000/1000 - 2s - loss: 16.0641 - mse: 16.0641 - mae: 1.5835 - val_loss: 10.8989 - val_mse: 10.8989 - val_mae: 1.5725 - lr: 2.0518e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 27/100\n",
            "1000/1000 - 2s - loss: 16.0707 - mse: 16.0707 - mae: 1.5813 - val_loss: 10.9424 - val_mse: 10.9424 - val_mae: 1.5703 - lr: 2.0518e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 28/100\n",
            "1000/1000 - 2s - loss: 16.0618 - mse: 16.0618 - mae: 1.5812 - val_loss: 10.8981 - val_mse: 10.8981 - val_mae: 1.5506 - lr: 2.0518e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 29/100\n",
            "1000/1000 - 2s - loss: 16.0546 - mse: 16.0546 - mae: 1.5821 - val_loss: 10.9287 - val_mse: 10.9287 - val_mae: 1.5447 - lr: 2.0518e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 30/100\n",
            "1000/1000 - 2s - loss: 16.0463 - mse: 16.0463 - mae: 1.5801 - val_loss: 10.8605 - val_mse: 10.8605 - val_mae: 1.5871 - lr: 2.0518e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 31/100\n",
            "1000/1000 - 2s - loss: 16.0428 - mse: 16.0428 - mae: 1.5791 - val_loss: 10.9217 - val_mse: 10.9217 - val_mae: 1.5644 - lr: 2.0518e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 32/100\n",
            "1000/1000 - 2s - loss: 16.0360 - mse: 16.0360 - mae: 1.5811 - val_loss: 10.8618 - val_mse: 10.8618 - val_mae: 1.5729 - lr: 2.0518e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 33/100\n",
            "1000/1000 - 2s - loss: 16.0218 - mse: 16.0218 - mae: 1.5805 - val_loss: 10.8500 - val_mse: 10.8500 - val_mae: 1.5839 - lr: 2.0518e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 34/100\n",
            "1000/1000 - 2s - loss: 16.0183 - mse: 16.0183 - mae: 1.5794 - val_loss: 10.8854 - val_mse: 10.8854 - val_mae: 1.5829 - lr: 2.0518e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 35/100\n",
            "1000/1000 - 2s - loss: 16.0160 - mse: 16.0160 - mae: 1.5773 - val_loss: 10.8662 - val_mse: 10.8662 - val_mae: 1.5712 - lr: 2.0518e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 36/100\n",
            "1000/1000 - 2s - loss: 16.0072 - mse: 16.0072 - mae: 1.5813 - val_loss: 10.8411 - val_mse: 10.8411 - val_mae: 1.5739 - lr: 2.0518e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 37/100\n",
            "1000/1000 - 2s - loss: 16.0070 - mse: 16.0070 - mae: 1.5800 - val_loss: 10.8599 - val_mse: 10.8599 - val_mae: 1.5806 - lr: 2.0518e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 38/100\n",
            "1000/1000 - 2s - loss: 16.0063 - mse: 16.0063 - mae: 1.5792 - val_loss: 10.8451 - val_mse: 10.8451 - val_mae: 1.5860 - lr: 2.0518e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 39/100\n",
            "1000/1000 - 2s - loss: 15.9839 - mse: 15.9839 - mae: 1.5818 - val_loss: 10.9075 - val_mse: 10.9075 - val_mae: 1.5670 - lr: 2.0518e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 40/100\n",
            "1000/1000 - 2s - loss: 15.9922 - mse: 15.9922 - mae: 1.5781 - val_loss: 10.8727 - val_mse: 10.8727 - val_mae: 1.5717 - lr: 2.0518e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 41/100\n",
            "1000/1000 - 2s - loss: 15.9856 - mse: 15.9856 - mae: 1.5771 - val_loss: 10.8419 - val_mse: 10.8419 - val_mae: 1.5596 - lr: 2.0518e-04 - 2s/epoch - 2ms/step\n",
            "Score for fold 1: loss of 10.841939926147461\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 13.6661 - mse: 13.6661 - mae: 1.5679 - val_loss: 20.1289 - val_mse: 20.1289 - val_mae: 1.5843 - lr: 2.0518e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 13.6639 - mse: 13.6639 - mae: 1.5672 - val_loss: 20.0963 - val_mse: 20.0963 - val_mae: 1.5981 - lr: 2.0518e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 13.6496 - mse: 13.6496 - mae: 1.5717 - val_loss: 20.1303 - val_mse: 20.1303 - val_mae: 1.5876 - lr: 2.0518e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 13.6291 - mse: 13.6291 - mae: 1.5687 - val_loss: 20.0828 - val_mse: 20.0828 - val_mae: 1.6006 - lr: 2.0518e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 13.6491 - mse: 13.6491 - mae: 1.5700 - val_loss: 20.1010 - val_mse: 20.1010 - val_mae: 1.5781 - lr: 2.0518e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 13.6336 - mse: 13.6336 - mae: 1.5659 - val_loss: 20.1300 - val_mse: 20.1300 - val_mae: 1.5939 - lr: 2.0518e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 13.6362 - mse: 13.6362 - mae: 1.5659 - val_loss: 20.0766 - val_mse: 20.0766 - val_mae: 1.6233 - lr: 2.0518e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 2s - loss: 13.6203 - mse: 13.6203 - mae: 1.5686 - val_loss: 20.1051 - val_mse: 20.1051 - val_mae: 1.6006 - lr: 2.0518e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 2s - loss: 13.6102 - mse: 13.6102 - mae: 1.5677 - val_loss: 20.0819 - val_mse: 20.0819 - val_mae: 1.6056 - lr: 2.0518e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 2s - loss: 13.6084 - mse: 13.6084 - mae: 1.5661 - val_loss: 20.1091 - val_mse: 20.1091 - val_mae: 1.5996 - lr: 2.0518e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 2s - loss: 13.6009 - mse: 13.6009 - mae: 1.5666 - val_loss: 20.1609 - val_mse: 20.1609 - val_mae: 1.5704 - lr: 2.0518e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 2s - loss: 13.6006 - mse: 13.6006 - mae: 1.5648 - val_loss: 20.1653 - val_mse: 20.1653 - val_mae: 1.5878 - lr: 2.0518e-04 - 2s/epoch - 2ms/step\n",
            "Score for fold 2: loss of 20.165260314941406\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 15.9627 - mse: 15.9627 - mae: 1.5850 - val_loss: 10.6001 - val_mse: 10.6001 - val_mae: 1.5554 - lr: 2.0518e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 15.9545 - mse: 15.9545 - mae: 1.5870 - val_loss: 10.6193 - val_mse: 10.6193 - val_mae: 1.5402 - lr: 2.0518e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 3s - loss: 15.9429 - mse: 15.9429 - mae: 1.5847 - val_loss: 10.6347 - val_mse: 10.6347 - val_mae: 1.5412 - lr: 2.0518e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 15.9201 - mse: 15.9201 - mae: 1.5870 - val_loss: 10.6884 - val_mse: 10.6884 - val_mae: 1.5039 - lr: 2.0518e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 15.9160 - mse: 15.9160 - mae: 1.5849 - val_loss: 10.6345 - val_mse: 10.6345 - val_mae: 1.5358 - lr: 2.0518e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 15.9066 - mse: 15.9066 - mae: 1.5821 - val_loss: 10.6654 - val_mse: 10.6654 - val_mae: 1.5728 - lr: 2.0518e-04 - 2s/epoch - 2ms/step\n",
            "Score for fold 3: loss of 10.665430068969727\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 15.2014 - mse: 15.2014 - mae: 1.5667 - val_loss: 13.5266 - val_mse: 13.5266 - val_mae: 1.6005 - lr: 2.0518e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 15.2087 - mse: 15.2087 - mae: 1.5678 - val_loss: 13.5578 - val_mse: 13.5578 - val_mae: 1.5630 - lr: 2.0518e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 15.2020 - mse: 15.2020 - mae: 1.5654 - val_loss: 13.4446 - val_mse: 13.4446 - val_mae: 1.6170 - lr: 2.0518e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 15.1822 - mse: 15.1822 - mae: 1.5672 - val_loss: 13.5139 - val_mse: 13.5139 - val_mae: 1.5915 - lr: 2.0518e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 15.1933 - mse: 15.1933 - mae: 1.5650 - val_loss: 13.4841 - val_mse: 13.4841 - val_mae: 1.6131 - lr: 2.0518e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 15.1766 - mse: 15.1766 - mae: 1.5631 - val_loss: 13.5014 - val_mse: 13.5014 - val_mae: 1.6407 - lr: 2.0518e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 15.1810 - mse: 15.1810 - mae: 1.5633 - val_loss: 13.5510 - val_mse: 13.5510 - val_mae: 1.6106 - lr: 2.0518e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 2s - loss: 15.1797 - mse: 15.1797 - mae: 1.5618 - val_loss: 13.5085 - val_mse: 13.5085 - val_mae: 1.5988 - lr: 2.0518e-04 - 2s/epoch - 2ms/step\n",
            "Score for fold 4: loss of 13.508484840393066\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 13.7024 - mse: 13.7024 - mae: 1.5751 - val_loss: 19.3982 - val_mse: 19.3982 - val_mae: 1.5869 - lr: 2.0518e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 13.6987 - mse: 13.6987 - mae: 1.5729 - val_loss: 19.3888 - val_mse: 19.3888 - val_mae: 1.6138 - lr: 2.0518e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 13.6777 - mse: 13.6777 - mae: 1.5787 - val_loss: 19.5266 - val_mse: 19.5266 - val_mae: 1.5542 - lr: 2.0518e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 13.6870 - mse: 13.6870 - mae: 1.5758 - val_loss: 19.4355 - val_mse: 19.4355 - val_mae: 1.5649 - lr: 2.0518e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 13.6845 - mse: 13.6845 - mae: 1.5762 - val_loss: 19.4889 - val_mse: 19.4889 - val_mae: 1.5647 - lr: 2.0518e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 13.6829 - mse: 13.6829 - mae: 1.5705 - val_loss: 19.3970 - val_mse: 19.3970 - val_mae: 1.5642 - lr: 2.0518e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 13.6744 - mse: 13.6744 - mae: 1.5726 - val_loss: 19.4293 - val_mse: 19.4293 - val_mae: 1.5969 - lr: 2.0518e-04 - 2s/epoch - 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-10-15 05:39:33,887]\u001b[0m Finished trial#9 resulted in value: 14.924000000000001. Current best value is 14.9 with parameters: {'activation': 'tanh', 'num_hidden_layer': 3, 'i': 6, 'learning_rate': 0.0003606405459716391}.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Score for fold 5: loss of 19.429256439208984\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 3s - loss: 16.7333 - mse: 16.7333 - mae: 1.6746 - val_loss: 13.1931 - val_mse: 13.1931 - val_mae: 1.6258 - lr: 6.5129e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 16.3676 - mse: 16.3676 - mae: 1.6338 - val_loss: 13.4178 - val_mse: 13.4178 - val_mae: 1.5834 - lr: 6.5129e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 16.3335 - mse: 16.3335 - mae: 1.6283 - val_loss: 13.3996 - val_mse: 13.3996 - val_mae: 1.6061 - lr: 6.5129e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 16.3314 - mse: 16.3314 - mae: 1.6309 - val_loss: 13.1496 - val_mse: 13.1496 - val_mae: 1.6536 - lr: 6.5129e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 16.3340 - mse: 16.3340 - mae: 1.6336 - val_loss: 13.1763 - val_mse: 13.1763 - val_mae: 1.6383 - lr: 6.5129e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 16.2866 - mse: 16.2866 - mae: 1.6340 - val_loss: 13.1394 - val_mse: 13.1394 - val_mae: 1.6695 - lr: 6.5129e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 16.3181 - mse: 16.3181 - mae: 1.6291 - val_loss: 13.1484 - val_mse: 13.1484 - val_mae: 1.6445 - lr: 6.5129e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 2s - loss: 16.2655 - mse: 16.2655 - mae: 1.6316 - val_loss: 13.2434 - val_mse: 13.2434 - val_mae: 1.7874 - lr: 6.5129e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 2s - loss: 16.2968 - mse: 16.2968 - mae: 1.6343 - val_loss: 13.1622 - val_mse: 13.1622 - val_mae: 1.6657 - lr: 6.5129e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 2s - loss: 16.2696 - mse: 16.2696 - mae: 1.6336 - val_loss: 13.1282 - val_mse: 13.1282 - val_mae: 1.6665 - lr: 6.5129e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 2s - loss: 16.3039 - mse: 16.3039 - mae: 1.6305 - val_loss: 13.1382 - val_mse: 13.1382 - val_mae: 1.6815 - lr: 6.5129e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 2s - loss: 16.2513 - mse: 16.2513 - mae: 1.6323 - val_loss: 13.1460 - val_mse: 13.1460 - val_mae: 1.6326 - lr: 6.5129e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 13/100\n",
            "1000/1000 - 2s - loss: 16.2674 - mse: 16.2674 - mae: 1.6310 - val_loss: 13.1273 - val_mse: 13.1273 - val_mae: 1.6936 - lr: 6.5129e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 14/100\n",
            "1000/1000 - 2s - loss: 16.2846 - mse: 16.2846 - mae: 1.6289 - val_loss: 13.1343 - val_mse: 13.1343 - val_mae: 1.6470 - lr: 6.5129e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 15/100\n",
            "1000/1000 - 2s - loss: 16.2633 - mse: 16.2633 - mae: 1.6310 - val_loss: 13.1707 - val_mse: 13.1707 - val_mae: 1.6538 - lr: 6.5129e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 16/100\n",
            "1000/1000 - 2s - loss: 16.2767 - mse: 16.2767 - mae: 1.6331 - val_loss: 13.1780 - val_mse: 13.1780 - val_mae: 1.6159 - lr: 6.5129e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 17/100\n",
            "1000/1000 - 2s - loss: 16.2769 - mse: 16.2769 - mae: 1.6325 - val_loss: 13.1478 - val_mse: 13.1478 - val_mae: 1.6374 - lr: 6.5129e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 18/100\n",
            "1000/1000 - 2s - loss: 16.2495 - mse: 16.2495 - mae: 1.6341 - val_loss: 13.1327 - val_mse: 13.1327 - val_mae: 1.6647 - lr: 6.5129e-04 - 2s/epoch - 2ms/step\n",
            "Score for fold 1: loss of 13.13272762298584\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 16.7064 - mse: 16.7064 - mae: 1.6437 - val_loss: 11.3037 - val_mse: 11.3037 - val_mae: 1.6554 - lr: 6.5129e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 16.7161 - mse: 16.7161 - mae: 1.6407 - val_loss: 11.3015 - val_mse: 11.3015 - val_mae: 1.6257 - lr: 6.5129e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 16.7220 - mse: 16.7220 - mae: 1.6431 - val_loss: 11.2995 - val_mse: 11.2995 - val_mae: 1.6715 - lr: 6.5129e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 16.7205 - mse: 16.7205 - mae: 1.6395 - val_loss: 11.3345 - val_mse: 11.3345 - val_mae: 1.5737 - lr: 6.5129e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 16.7178 - mse: 16.7178 - mae: 1.6458 - val_loss: 11.3231 - val_mse: 11.3231 - val_mae: 1.5743 - lr: 6.5129e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 16.7577 - mse: 16.7577 - mae: 1.6394 - val_loss: 11.3224 - val_mse: 11.3224 - val_mae: 1.5908 - lr: 6.5129e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 16.7226 - mse: 16.7226 - mae: 1.6433 - val_loss: 11.5126 - val_mse: 11.5126 - val_mae: 1.5102 - lr: 6.5129e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 2s - loss: 16.7591 - mse: 16.7591 - mae: 1.6397 - val_loss: 11.2966 - val_mse: 11.2966 - val_mae: 1.6094 - lr: 6.5129e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 2s - loss: 16.7119 - mse: 16.7119 - mae: 1.6435 - val_loss: 11.3002 - val_mse: 11.3002 - val_mae: 1.6104 - lr: 6.5129e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 2s - loss: 16.7175 - mse: 16.7175 - mae: 1.6483 - val_loss: 11.3759 - val_mse: 11.3759 - val_mae: 1.5534 - lr: 6.5129e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 2s - loss: 16.7623 - mse: 16.7623 - mae: 1.6440 - val_loss: 11.2898 - val_mse: 11.2898 - val_mae: 1.6245 - lr: 6.5129e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 2s - loss: 16.7208 - mse: 16.7208 - mae: 1.6402 - val_loss: 11.2872 - val_mse: 11.2872 - val_mae: 1.6199 - lr: 6.5129e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 13/100\n",
            "1000/1000 - 2s - loss: 16.7441 - mse: 16.7441 - mae: 1.6483 - val_loss: 11.3318 - val_mse: 11.3318 - val_mae: 1.5736 - lr: 6.5129e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 14/100\n",
            "1000/1000 - 2s - loss: 16.7671 - mse: 16.7671 - mae: 1.6396 - val_loss: 11.3064 - val_mse: 11.3064 - val_mae: 1.6612 - lr: 6.5129e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 15/100\n",
            "1000/1000 - 2s - loss: 16.8297 - mse: 16.8297 - mae: 1.6554 - val_loss: 11.3703 - val_mse: 11.3703 - val_mae: 1.5494 - lr: 6.5129e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 16/100\n",
            "1000/1000 - 2s - loss: 16.7961 - mse: 16.7961 - mae: 1.6508 - val_loss: 11.4101 - val_mse: 11.4101 - val_mae: 1.5446 - lr: 6.5129e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 17/100\n",
            "1000/1000 - 3s - loss: 16.8003 - mse: 16.8003 - mae: 1.6460 - val_loss: 11.3121 - val_mse: 11.3121 - val_mae: 1.6642 - lr: 6.5129e-04 - 3s/epoch - 3ms/step\n",
            "Score for fold 2: loss of 11.312094688415527\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 3s - loss: 16.1246 - mse: 16.1246 - mae: 1.6421 - val_loss: 13.8579 - val_mse: 13.8579 - val_mae: 1.7718 - lr: 6.5129e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 16.1728 - mse: 16.1728 - mae: 1.6352 - val_loss: 13.9595 - val_mse: 13.9595 - val_mae: 1.5890 - lr: 6.5129e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 16.1903 - mse: 16.1903 - mae: 1.6425 - val_loss: 14.0506 - val_mse: 14.0506 - val_mae: 1.5779 - lr: 6.5129e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 3s - loss: 16.1618 - mse: 16.1618 - mae: 1.6392 - val_loss: 13.8977 - val_mse: 13.8977 - val_mae: 1.6144 - lr: 6.5129e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 16.1692 - mse: 16.1692 - mae: 1.6398 - val_loss: 13.9190 - val_mse: 13.9190 - val_mae: 1.6183 - lr: 6.5129e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 16.1447 - mse: 16.1447 - mae: 1.6339 - val_loss: 13.8811 - val_mse: 13.8811 - val_mae: 1.6218 - lr: 6.5129e-04 - 2s/epoch - 2ms/step\n",
            "Score for fold 3: loss of 13.881062507629395\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 15.9211 - mse: 15.9211 - mae: 1.6475 - val_loss: 14.9007 - val_mse: 14.9007 - val_mae: 1.6784 - lr: 6.5129e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 15.9306 - mse: 15.9306 - mae: 1.6477 - val_loss: 15.0021 - val_mse: 15.0021 - val_mae: 1.5945 - lr: 6.5129e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 15.9099 - mse: 15.9099 - mae: 1.6540 - val_loss: 15.0494 - val_mse: 15.0494 - val_mae: 1.5639 - lr: 6.5129e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 15.8738 - mse: 15.8738 - mae: 1.6630 - val_loss: 14.9923 - val_mse: 14.9923 - val_mae: 1.7490 - lr: 6.5129e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 15.9254 - mse: 15.9254 - mae: 1.6432 - val_loss: 14.9050 - val_mse: 14.9050 - val_mae: 1.6847 - lr: 6.5129e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 15.9434 - mse: 15.9434 - mae: 1.6570 - val_loss: 15.1714 - val_mse: 15.1714 - val_mae: 1.5535 - lr: 6.5129e-04 - 2s/epoch - 2ms/step\n",
            "Score for fold 4: loss of 15.171369552612305\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 13.4028 - mse: 13.4028 - mae: 1.6400 - val_loss: 24.8532 - val_mse: 24.8532 - val_mae: 1.6886 - lr: 6.5129e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 13.3826 - mse: 13.3826 - mae: 1.6373 - val_loss: 25.0921 - val_mse: 25.0921 - val_mae: 1.6193 - lr: 6.5129e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 13.3783 - mse: 13.3783 - mae: 1.6354 - val_loss: 25.0751 - val_mse: 25.0751 - val_mae: 1.6222 - lr: 6.5129e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 13.4481 - mse: 13.4481 - mae: 1.6354 - val_loss: 24.9470 - val_mse: 24.9470 - val_mae: 1.6669 - lr: 6.5129e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 13.4816 - mse: 13.4816 - mae: 1.6417 - val_loss: 24.8609 - val_mse: 24.8609 - val_mae: 1.7424 - lr: 6.5129e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 13.4582 - mse: 13.4582 - mae: 1.6465 - val_loss: 24.8164 - val_mse: 24.8164 - val_mae: 1.7273 - lr: 6.5129e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 13.4216 - mse: 13.4216 - mae: 1.6378 - val_loss: 24.8749 - val_mse: 24.8749 - val_mae: 1.6854 - lr: 6.5129e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 2s - loss: 13.4288 - mse: 13.4288 - mae: 1.6445 - val_loss: 25.3121 - val_mse: 25.3121 - val_mae: 1.5940 - lr: 6.5129e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 2s - loss: 13.3958 - mse: 13.3958 - mae: 1.6479 - val_loss: 24.9257 - val_mse: 24.9257 - val_mae: 1.6509 - lr: 6.5129e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 2s - loss: 13.4033 - mse: 13.4033 - mae: 1.6376 - val_loss: 25.6003 - val_mse: 25.6003 - val_mae: 1.5630 - lr: 6.5129e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 2s - loss: 13.4067 - mse: 13.4067 - mae: 1.6370 - val_loss: 25.2590 - val_mse: 25.2590 - val_mae: 1.5866 - lr: 6.5129e-04 - 2s/epoch - 2ms/step\n",
            "Score for fold 5: loss of 25.25894546508789\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-10-15 05:42:00,176]\u001b[0m Finished trial#10 resulted in value: 15.75. Current best value is 14.9 with parameters: {'activation': 'tanh', 'num_hidden_layer': 3, 'i': 6, 'learning_rate': 0.0003606405459716391}.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 3s - loss: 18.8311 - mse: 18.8311 - mae: 1.8276 - val_loss: 12.3948 - val_mse: 12.3948 - val_mae: 1.6158 - lr: 1.2253e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 16.4926 - mse: 16.4926 - mae: 1.6209 - val_loss: 12.2889 - val_mse: 12.2889 - val_mae: 1.5984 - lr: 1.2253e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 16.4034 - mse: 16.4034 - mae: 1.6146 - val_loss: 12.2275 - val_mse: 12.2275 - val_mae: 1.5874 - lr: 1.2253e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 16.3421 - mse: 16.3421 - mae: 1.6120 - val_loss: 12.1859 - val_mse: 12.1859 - val_mae: 1.5956 - lr: 1.2253e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 16.3071 - mse: 16.3071 - mae: 1.6081 - val_loss: 12.1335 - val_mse: 12.1335 - val_mae: 1.5985 - lr: 1.2253e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 16.2681 - mse: 16.2681 - mae: 1.6042 - val_loss: 12.0951 - val_mse: 12.0951 - val_mae: 1.5912 - lr: 1.2253e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 16.2241 - mse: 16.2241 - mae: 1.6046 - val_loss: 12.1011 - val_mse: 12.1011 - val_mae: 1.5972 - lr: 1.2253e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 2s - loss: 16.1961 - mse: 16.1961 - mae: 1.5981 - val_loss: 12.0465 - val_mse: 12.0465 - val_mae: 1.5935 - lr: 1.2253e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 2s - loss: 16.1624 - mse: 16.1624 - mae: 1.5981 - val_loss: 12.0114 - val_mse: 12.0114 - val_mae: 1.5803 - lr: 1.2253e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 3s - loss: 16.1394 - mse: 16.1394 - mae: 1.5949 - val_loss: 11.9957 - val_mse: 11.9957 - val_mae: 1.5869 - lr: 1.2253e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 2s - loss: 16.1129 - mse: 16.1129 - mae: 1.5932 - val_loss: 11.9756 - val_mse: 11.9756 - val_mae: 1.5887 - lr: 1.2253e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 2s - loss: 16.0848 - mse: 16.0848 - mae: 1.5961 - val_loss: 11.9587 - val_mse: 11.9587 - val_mae: 1.5840 - lr: 1.2253e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 13/100\n",
            "1000/1000 - 2s - loss: 16.0599 - mse: 16.0599 - mae: 1.5956 - val_loss: 11.9533 - val_mse: 11.9533 - val_mae: 1.5762 - lr: 1.2253e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 14/100\n",
            "1000/1000 - 2s - loss: 16.0479 - mse: 16.0479 - mae: 1.5875 - val_loss: 11.9011 - val_mse: 11.9011 - val_mae: 1.5860 - lr: 1.2253e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 15/100\n",
            "1000/1000 - 2s - loss: 16.0186 - mse: 16.0186 - mae: 1.5889 - val_loss: 11.8944 - val_mse: 11.8944 - val_mae: 1.5997 - lr: 1.2253e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 16/100\n",
            "1000/1000 - 2s - loss: 16.0036 - mse: 16.0036 - mae: 1.5911 - val_loss: 11.9138 - val_mse: 11.9138 - val_mae: 1.5657 - lr: 1.2253e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 17/100\n",
            "1000/1000 - 2s - loss: 15.9913 - mse: 15.9913 - mae: 1.5877 - val_loss: 11.8798 - val_mse: 11.8798 - val_mae: 1.5736 - lr: 1.2253e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 18/100\n",
            "1000/1000 - 2s - loss: 15.9792 - mse: 15.9792 - mae: 1.5863 - val_loss: 11.8552 - val_mse: 11.8552 - val_mae: 1.5847 - lr: 1.2253e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 19/100\n",
            "1000/1000 - 2s - loss: 15.9617 - mse: 15.9617 - mae: 1.5888 - val_loss: 11.8545 - val_mse: 11.8545 - val_mae: 1.5691 - lr: 1.2253e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 20/100\n",
            "1000/1000 - 2s - loss: 15.9443 - mse: 15.9443 - mae: 1.5852 - val_loss: 11.8643 - val_mse: 11.8643 - val_mae: 1.5696 - lr: 1.2253e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 21/100\n",
            "1000/1000 - 2s - loss: 15.9510 - mse: 15.9510 - mae: 1.5837 - val_loss: 11.8164 - val_mse: 11.8164 - val_mae: 1.5854 - lr: 1.2253e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 22/100\n",
            "1000/1000 - 2s - loss: 15.9319 - mse: 15.9319 - mae: 1.5869 - val_loss: 11.8488 - val_mse: 11.8488 - val_mae: 1.5677 - lr: 1.2253e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 23/100\n",
            "1000/1000 - 2s - loss: 15.9290 - mse: 15.9290 - mae: 1.5853 - val_loss: 11.8539 - val_mse: 11.8539 - val_mae: 1.5538 - lr: 1.2253e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 24/100\n",
            "1000/1000 - 2s - loss: 15.9139 - mse: 15.9139 - mae: 1.5861 - val_loss: 11.8525 - val_mse: 11.8525 - val_mae: 1.5657 - lr: 1.2253e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 25/100\n",
            "1000/1000 - 2s - loss: 15.9166 - mse: 15.9166 - mae: 1.5838 - val_loss: 11.8470 - val_mse: 11.8470 - val_mae: 1.5743 - lr: 1.2253e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 26/100\n",
            "1000/1000 - 2s - loss: 15.9005 - mse: 15.9005 - mae: 1.5831 - val_loss: 11.7982 - val_mse: 11.7982 - val_mae: 1.5827 - lr: 1.2253e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 27/100\n",
            "1000/1000 - 2s - loss: 15.8917 - mse: 15.8917 - mae: 1.5864 - val_loss: 11.8484 - val_mse: 11.8484 - val_mae: 1.5617 - lr: 1.2253e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 28/100\n",
            "1000/1000 - 2s - loss: 15.8916 - mse: 15.8916 - mae: 1.5817 - val_loss: 11.7936 - val_mse: 11.7936 - val_mae: 1.5770 - lr: 1.2253e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 29/100\n",
            "1000/1000 - 2s - loss: 15.8780 - mse: 15.8780 - mae: 1.5850 - val_loss: 11.7791 - val_mse: 11.7791 - val_mae: 1.5659 - lr: 1.2253e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 30/100\n",
            "1000/1000 - 3s - loss: 15.8872 - mse: 15.8872 - mae: 1.5848 - val_loss: 11.7707 - val_mse: 11.7707 - val_mae: 1.5701 - lr: 1.2253e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 31/100\n",
            "1000/1000 - 2s - loss: 15.8532 - mse: 15.8532 - mae: 1.5885 - val_loss: 11.7950 - val_mse: 11.7950 - val_mae: 1.5713 - lr: 1.2253e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 32/100\n",
            "1000/1000 - 2s - loss: 15.8677 - mse: 15.8677 - mae: 1.5822 - val_loss: 11.7808 - val_mse: 11.7808 - val_mae: 1.5726 - lr: 1.2253e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 33/100\n",
            "1000/1000 - 2s - loss: 15.8622 - mse: 15.8622 - mae: 1.5804 - val_loss: 11.7756 - val_mse: 11.7756 - val_mae: 1.5769 - lr: 1.2253e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 34/100\n",
            "1000/1000 - 2s - loss: 15.8622 - mse: 15.8622 - mae: 1.5833 - val_loss: 11.7891 - val_mse: 11.7891 - val_mae: 1.5527 - lr: 1.2253e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 35/100\n",
            "1000/1000 - 2s - loss: 15.8502 - mse: 15.8502 - mae: 1.5812 - val_loss: 11.7677 - val_mse: 11.7677 - val_mae: 1.5636 - lr: 1.2253e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 36/100\n",
            "1000/1000 - 2s - loss: 15.8386 - mse: 15.8386 - mae: 1.5816 - val_loss: 11.8032 - val_mse: 11.8032 - val_mae: 1.5504 - lr: 1.2253e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 37/100\n",
            "1000/1000 - 2s - loss: 15.8535 - mse: 15.8535 - mae: 1.5774 - val_loss: 11.7748 - val_mse: 11.7748 - val_mae: 1.5536 - lr: 1.2253e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 38/100\n",
            "1000/1000 - 2s - loss: 15.8371 - mse: 15.8371 - mae: 1.5771 - val_loss: 11.7796 - val_mse: 11.7796 - val_mae: 1.5724 - lr: 1.2253e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 39/100\n",
            "1000/1000 - 2s - loss: 15.8365 - mse: 15.8365 - mae: 1.5810 - val_loss: 11.7723 - val_mse: 11.7723 - val_mae: 1.5816 - lr: 1.2253e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 40/100\n",
            "1000/1000 - 2s - loss: 15.8264 - mse: 15.8264 - mae: 1.5806 - val_loss: 11.7894 - val_mse: 11.7894 - val_mae: 1.5675 - lr: 1.2253e-04 - 2s/epoch - 2ms/step\n",
            "Score for fold 1: loss of 11.789363861083984\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 15.2021 - mse: 15.2021 - mae: 1.5829 - val_loss: 14.2319 - val_mse: 14.2319 - val_mae: 1.5768 - lr: 1.2253e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 15.1847 - mse: 15.1847 - mae: 1.5875 - val_loss: 14.2682 - val_mse: 14.2682 - val_mae: 1.5690 - lr: 1.2253e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 15.1737 - mse: 15.1737 - mae: 1.5844 - val_loss: 14.2802 - val_mse: 14.2802 - val_mae: 1.5520 - lr: 1.2253e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 15.1763 - mse: 15.1763 - mae: 1.5788 - val_loss: 14.2502 - val_mse: 14.2502 - val_mae: 1.5659 - lr: 1.2253e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 15.1675 - mse: 15.1675 - mae: 1.5817 - val_loss: 14.2483 - val_mse: 14.2483 - val_mae: 1.5661 - lr: 1.2253e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 15.1517 - mse: 15.1517 - mae: 1.5822 - val_loss: 14.2660 - val_mse: 14.2660 - val_mae: 1.5503 - lr: 1.2253e-04 - 2s/epoch - 2ms/step\n",
            "Score for fold 2: loss of 14.265969276428223\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 14.6500 - mse: 14.6500 - mae: 1.5704 - val_loss: 16.2853 - val_mse: 16.2853 - val_mae: 1.6004 - lr: 1.2253e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 14.6307 - mse: 14.6307 - mae: 1.5761 - val_loss: 16.3510 - val_mse: 16.3510 - val_mae: 1.5931 - lr: 1.2253e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 14.6419 - mse: 14.6419 - mae: 1.5686 - val_loss: 16.2992 - val_mse: 16.2992 - val_mae: 1.6125 - lr: 1.2253e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 14.6283 - mse: 14.6283 - mae: 1.5750 - val_loss: 16.3457 - val_mse: 16.3457 - val_mae: 1.5712 - lr: 1.2253e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 14.6256 - mse: 14.6256 - mae: 1.5743 - val_loss: 16.3832 - val_mse: 16.3832 - val_mae: 1.5524 - lr: 1.2253e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 14.6263 - mse: 14.6263 - mae: 1.5690 - val_loss: 16.2941 - val_mse: 16.2941 - val_mae: 1.5948 - lr: 1.2253e-04 - 2s/epoch - 2ms/step\n",
            "Score for fold 3: loss of 16.294118881225586\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 13.8712 - mse: 13.8712 - mae: 1.5734 - val_loss: 19.3646 - val_mse: 19.3646 - val_mae: 1.5820 - lr: 1.2253e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 13.8825 - mse: 13.8825 - mae: 1.5692 - val_loss: 19.3455 - val_mse: 19.3455 - val_mae: 1.6013 - lr: 1.2253e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 13.8631 - mse: 13.8631 - mae: 1.5722 - val_loss: 19.3975 - val_mse: 19.3975 - val_mae: 1.5767 - lr: 1.2253e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 13.8761 - mse: 13.8761 - mae: 1.5696 - val_loss: 19.3631 - val_mse: 19.3631 - val_mae: 1.5818 - lr: 1.2253e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 13.8643 - mse: 13.8643 - mae: 1.5713 - val_loss: 19.3737 - val_mse: 19.3737 - val_mae: 1.5782 - lr: 1.2253e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 13.8566 - mse: 13.8566 - mae: 1.5696 - val_loss: 19.3791 - val_mse: 19.3791 - val_mae: 1.5834 - lr: 1.2253e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 13.8621 - mse: 13.8621 - mae: 1.5713 - val_loss: 19.3542 - val_mse: 19.3542 - val_mae: 1.5788 - lr: 1.2253e-04 - 2s/epoch - 2ms/step\n",
            "Score for fold 4: loss of 19.35421371459961\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 15.3904 - mse: 15.3904 - mae: 1.5757 - val_loss: 13.1947 - val_mse: 13.1947 - val_mae: 1.5760 - lr: 1.2253e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 15.3863 - mse: 15.3863 - mae: 1.5768 - val_loss: 13.2361 - val_mse: 13.2361 - val_mae: 1.5751 - lr: 1.2253e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 15.3757 - mse: 15.3757 - mae: 1.5786 - val_loss: 13.2594 - val_mse: 13.2594 - val_mae: 1.5835 - lr: 1.2253e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 15.3876 - mse: 15.3876 - mae: 1.5770 - val_loss: 13.2493 - val_mse: 13.2493 - val_mae: 1.5669 - lr: 1.2253e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 15.3862 - mse: 15.3862 - mae: 1.5763 - val_loss: 13.2221 - val_mse: 13.2221 - val_mae: 1.5798 - lr: 1.2253e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 15.3766 - mse: 15.3766 - mae: 1.5761 - val_loss: 13.2297 - val_mse: 13.2297 - val_mae: 1.5675 - lr: 1.2253e-04 - 2s/epoch - 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-10-15 05:44:34,102]\u001b[0m Finished trial#11 resulted in value: 14.985999999999999. Current best value is 14.9 with parameters: {'activation': 'tanh', 'num_hidden_layer': 3, 'i': 6, 'learning_rate': 0.0003606405459716391}.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Score for fold 5: loss of 13.229694366455078\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 3s - loss: 14.8502 - mse: 14.8502 - mae: 1.6789 - val_loss: 21.7312 - val_mse: 21.7312 - val_mae: 1.6191 - lr: 4.8617e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 13.8865 - mse: 13.8865 - mae: 1.6050 - val_loss: 21.5593 - val_mse: 21.5593 - val_mae: 1.6071 - lr: 4.8617e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 13.7843 - mse: 13.7843 - mae: 1.5963 - val_loss: 21.5024 - val_mse: 21.5024 - val_mae: 1.6133 - lr: 4.8617e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 13.7042 - mse: 13.7042 - mae: 1.5931 - val_loss: 21.4532 - val_mse: 21.4532 - val_mae: 1.6064 - lr: 4.8617e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 13.6493 - mse: 13.6493 - mae: 1.5894 - val_loss: 21.5228 - val_mse: 21.5228 - val_mae: 1.6032 - lr: 4.8617e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 13.6249 - mse: 13.6249 - mae: 1.5855 - val_loss: 21.4359 - val_mse: 21.4359 - val_mae: 1.5973 - lr: 4.8617e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 13.5870 - mse: 13.5870 - mae: 1.5843 - val_loss: 21.3535 - val_mse: 21.3535 - val_mae: 1.6219 - lr: 4.8617e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 2s - loss: 13.5504 - mse: 13.5504 - mae: 1.5797 - val_loss: 21.3842 - val_mse: 21.3842 - val_mae: 1.6047 - lr: 4.8617e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 2s - loss: 13.5377 - mse: 13.5377 - mae: 1.5811 - val_loss: 21.3772 - val_mse: 21.3772 - val_mae: 1.6053 - lr: 4.8617e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 2s - loss: 13.5107 - mse: 13.5107 - mae: 1.5814 - val_loss: 21.3115 - val_mse: 21.3115 - val_mae: 1.6200 - lr: 4.8617e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 2s - loss: 13.4862 - mse: 13.4862 - mae: 1.5806 - val_loss: 21.3203 - val_mse: 21.3203 - val_mae: 1.5955 - lr: 4.8617e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 2s - loss: 13.4901 - mse: 13.4901 - mae: 1.5776 - val_loss: 21.2806 - val_mse: 21.2806 - val_mae: 1.5947 - lr: 4.8617e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 13/100\n",
            "1000/1000 - 2s - loss: 13.4767 - mse: 13.4767 - mae: 1.5728 - val_loss: 21.4007 - val_mse: 21.4007 - val_mae: 1.6120 - lr: 4.8617e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 14/100\n",
            "1000/1000 - 2s - loss: 13.4626 - mse: 13.4626 - mae: 1.5746 - val_loss: 21.2653 - val_mse: 21.2653 - val_mae: 1.6179 - lr: 4.8617e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 15/100\n",
            "1000/1000 - 2s - loss: 13.4441 - mse: 13.4441 - mae: 1.5759 - val_loss: 21.3272 - val_mse: 21.3272 - val_mae: 1.6003 - lr: 4.8617e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 16/100\n",
            "1000/1000 - 2s - loss: 13.4378 - mse: 13.4378 - mae: 1.5778 - val_loss: 21.2999 - val_mse: 21.2999 - val_mae: 1.5861 - lr: 4.8617e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 17/100\n",
            "1000/1000 - 2s - loss: 13.4297 - mse: 13.4297 - mae: 1.5767 - val_loss: 21.2160 - val_mse: 21.2160 - val_mae: 1.6333 - lr: 4.8617e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 18/100\n",
            "1000/1000 - 3s - loss: 13.4274 - mse: 13.4274 - mae: 1.5772 - val_loss: 21.2696 - val_mse: 21.2696 - val_mae: 1.5727 - lr: 4.8617e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 19/100\n",
            "1000/1000 - 3s - loss: 13.4263 - mse: 13.4263 - mae: 1.5715 - val_loss: 21.2451 - val_mse: 21.2451 - val_mae: 1.6339 - lr: 4.8617e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 20/100\n",
            "1000/1000 - 2s - loss: 13.4059 - mse: 13.4059 - mae: 1.5761 - val_loss: 21.2090 - val_mse: 21.2090 - val_mae: 1.6121 - lr: 4.8617e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 21/100\n",
            "1000/1000 - 2s - loss: 13.4125 - mse: 13.4125 - mae: 1.5737 - val_loss: 21.2089 - val_mse: 21.2089 - val_mae: 1.6373 - lr: 4.8617e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 22/100\n",
            "1000/1000 - 2s - loss: 13.3784 - mse: 13.3784 - mae: 1.5772 - val_loss: 21.2747 - val_mse: 21.2747 - val_mae: 1.5510 - lr: 4.8617e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 23/100\n",
            "1000/1000 - 2s - loss: 13.3793 - mse: 13.3793 - mae: 1.5718 - val_loss: 21.1844 - val_mse: 21.1844 - val_mae: 1.6384 - lr: 4.8617e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 24/100\n",
            "1000/1000 - 2s - loss: 13.3661 - mse: 13.3661 - mae: 1.5743 - val_loss: 21.2734 - val_mse: 21.2734 - val_mae: 1.6051 - lr: 4.8617e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 25/100\n",
            "1000/1000 - 2s - loss: 13.3717 - mse: 13.3717 - mae: 1.5748 - val_loss: 21.1909 - val_mse: 21.1909 - val_mae: 1.6106 - lr: 4.8617e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 26/100\n",
            "1000/1000 - 2s - loss: 13.3471 - mse: 13.3471 - mae: 1.5752 - val_loss: 21.2516 - val_mse: 21.2516 - val_mae: 1.6071 - lr: 4.8617e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 27/100\n",
            "1000/1000 - 2s - loss: 13.3525 - mse: 13.3525 - mae: 1.5726 - val_loss: 21.2842 - val_mse: 21.2842 - val_mae: 1.5698 - lr: 4.8617e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 28/100\n",
            "1000/1000 - 2s - loss: 13.3450 - mse: 13.3450 - mae: 1.5714 - val_loss: 21.1826 - val_mse: 21.1826 - val_mae: 1.5761 - lr: 4.8617e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 29/100\n",
            "1000/1000 - 2s - loss: 13.3382 - mse: 13.3382 - mae: 1.5731 - val_loss: 21.2648 - val_mse: 21.2648 - val_mae: 1.6039 - lr: 4.8617e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 30/100\n",
            "1000/1000 - 2s - loss: 13.3309 - mse: 13.3309 - mae: 1.5734 - val_loss: 21.1425 - val_mse: 21.1425 - val_mae: 1.5979 - lr: 4.8617e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 31/100\n",
            "1000/1000 - 2s - loss: 13.3262 - mse: 13.3262 - mae: 1.5724 - val_loss: 21.1717 - val_mse: 21.1717 - val_mae: 1.6462 - lr: 4.8617e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 32/100\n",
            "1000/1000 - 2s - loss: 13.2978 - mse: 13.2978 - mae: 1.5714 - val_loss: 21.1810 - val_mse: 21.1810 - val_mae: 1.5935 - lr: 4.8617e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 33/100\n",
            "1000/1000 - 2s - loss: 13.3148 - mse: 13.3148 - mae: 1.5689 - val_loss: 21.1443 - val_mse: 21.1443 - val_mae: 1.6082 - lr: 4.8617e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 34/100\n",
            "1000/1000 - 2s - loss: 13.3010 - mse: 13.3010 - mae: 1.5723 - val_loss: 21.1396 - val_mse: 21.1396 - val_mae: 1.6182 - lr: 4.8617e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 35/100\n",
            "1000/1000 - 2s - loss: 13.2882 - mse: 13.2882 - mae: 1.5704 - val_loss: 21.1274 - val_mse: 21.1274 - val_mae: 1.5888 - lr: 4.8617e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 36/100\n",
            "1000/1000 - 2s - loss: 13.2802 - mse: 13.2802 - mae: 1.5666 - val_loss: 21.1345 - val_mse: 21.1345 - val_mae: 1.6099 - lr: 4.8617e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 37/100\n",
            "1000/1000 - 2s - loss: 13.2643 - mse: 13.2643 - mae: 1.5697 - val_loss: 21.1644 - val_mse: 21.1644 - val_mae: 1.6175 - lr: 4.8617e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 38/100\n",
            "1000/1000 - 2s - loss: 13.2521 - mse: 13.2521 - mae: 1.5711 - val_loss: 21.2117 - val_mse: 21.2117 - val_mae: 1.5620 - lr: 4.8617e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 39/100\n",
            "1000/1000 - 2s - loss: 13.2518 - mse: 13.2518 - mae: 1.5704 - val_loss: 21.1751 - val_mse: 21.1751 - val_mae: 1.5897 - lr: 4.8617e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 40/100\n",
            "1000/1000 - 2s - loss: 13.2446 - mse: 13.2446 - mae: 1.5697 - val_loss: 21.1442 - val_mse: 21.1442 - val_mae: 1.6400 - lr: 4.8617e-04 - 2s/epoch - 2ms/step\n",
            "Score for fold 1: loss of 21.14421272277832\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 15.2270 - mse: 15.2270 - mae: 1.5715 - val_loss: 13.3138 - val_mse: 13.3138 - val_mae: 1.5946 - lr: 4.8617e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 15.2311 - mse: 15.2311 - mae: 1.5679 - val_loss: 13.1759 - val_mse: 13.1759 - val_mae: 1.6044 - lr: 4.8617e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 15.2202 - mse: 15.2202 - mae: 1.5702 - val_loss: 13.2333 - val_mse: 13.2333 - val_mae: 1.6077 - lr: 4.8617e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 15.2167 - mse: 15.2167 - mae: 1.5696 - val_loss: 13.2010 - val_mse: 13.2010 - val_mae: 1.5992 - lr: 4.8617e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 15.1928 - mse: 15.1928 - mae: 1.5707 - val_loss: 13.2120 - val_mse: 13.2120 - val_mae: 1.6468 - lr: 4.8617e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 15.1943 - mse: 15.1943 - mae: 1.5700 - val_loss: 13.2694 - val_mse: 13.2694 - val_mae: 1.5858 - lr: 4.8617e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 15.1972 - mse: 15.1972 - mae: 1.5691 - val_loss: 13.2440 - val_mse: 13.2440 - val_mae: 1.6150 - lr: 4.8617e-04 - 2s/epoch - 2ms/step\n",
            "Score for fold 2: loss of 13.243952751159668\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 15.0986 - mse: 15.0986 - mae: 1.5847 - val_loss: 13.6659 - val_mse: 13.6659 - val_mae: 1.5088 - lr: 4.8617e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 15.0505 - mse: 15.0505 - mae: 1.5771 - val_loss: 13.6924 - val_mse: 13.6924 - val_mae: 1.5825 - lr: 4.8617e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 15.0708 - mse: 15.0708 - mae: 1.5800 - val_loss: 13.7286 - val_mse: 13.7286 - val_mae: 1.6173 - lr: 4.8617e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 15.0528 - mse: 15.0528 - mae: 1.5829 - val_loss: 13.7340 - val_mse: 13.7340 - val_mae: 1.5185 - lr: 4.8617e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 15.0210 - mse: 15.0210 - mae: 1.5785 - val_loss: 13.6984 - val_mse: 13.6984 - val_mae: 1.5750 - lr: 4.8617e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 15.0008 - mse: 15.0008 - mae: 1.5822 - val_loss: 13.7917 - val_mse: 13.7917 - val_mae: 1.5254 - lr: 4.8617e-04 - 2s/epoch - 2ms/step\n",
            "Score for fold 3: loss of 13.791735649108887\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 15.1779 - mse: 15.1779 - mae: 1.5776 - val_loss: 13.0810 - val_mse: 13.0810 - val_mae: 1.5533 - lr: 4.8617e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 15.1590 - mse: 15.1590 - mae: 1.5756 - val_loss: 13.4399 - val_mse: 13.4399 - val_mae: 1.5598 - lr: 4.8617e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 15.1667 - mse: 15.1667 - mae: 1.5738 - val_loss: 13.1626 - val_mse: 13.1626 - val_mae: 1.5542 - lr: 4.8617e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 15.1602 - mse: 15.1602 - mae: 1.5740 - val_loss: 13.1417 - val_mse: 13.1417 - val_mae: 1.5878 - lr: 4.8617e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 15.1536 - mse: 15.1536 - mae: 1.5731 - val_loss: 13.1652 - val_mse: 13.1652 - val_mae: 1.5663 - lr: 4.8617e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 15.1362 - mse: 15.1362 - mae: 1.5739 - val_loss: 13.2156 - val_mse: 13.2156 - val_mae: 1.5643 - lr: 4.8617e-04 - 2s/epoch - 2ms/step\n",
            "Score for fold 4: loss of 13.215591430664062\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 15.2426 - mse: 15.2426 - mae: 1.5754 - val_loss: 12.7070 - val_mse: 12.7070 - val_mae: 1.5849 - lr: 4.8617e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 15.2323 - mse: 15.2323 - mae: 1.5750 - val_loss: 12.9814 - val_mse: 12.9814 - val_mae: 1.5488 - lr: 4.8617e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 15.2146 - mse: 15.2146 - mae: 1.5681 - val_loss: 12.8309 - val_mse: 12.8309 - val_mae: 1.6005 - lr: 4.8617e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 15.2177 - mse: 15.2177 - mae: 1.5701 - val_loss: 12.9112 - val_mse: 12.9112 - val_mae: 1.5776 - lr: 4.8617e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 15.1881 - mse: 15.1881 - mae: 1.5712 - val_loss: 13.1010 - val_mse: 13.1010 - val_mae: 1.5273 - lr: 4.8617e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 15.2236 - mse: 15.2236 - mae: 1.5705 - val_loss: 12.9320 - val_mse: 12.9320 - val_mae: 1.5774 - lr: 4.8617e-04 - 2s/epoch - 2ms/step\n",
            "Score for fold 5: loss of 12.932015419006348\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-10-15 05:47:06,836]\u001b[0m Finished trial#12 resulted in value: 14.863999999999999. Current best value is 14.863999999999999 with parameters: {'activation': 'tanh', 'num_hidden_layer': 3, 'i': 6, 'learning_rate': 0.00048617401659243237}.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 3s - loss: 16.1717 - mse: 16.1717 - mae: 1.6664 - val_loss: 14.5826 - val_mse: 14.5826 - val_mae: 1.6720 - lr: 6.2704e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 15.7391 - mse: 15.7391 - mae: 1.6236 - val_loss: 14.3864 - val_mse: 14.3864 - val_mae: 1.6453 - lr: 6.2704e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 15.5969 - mse: 15.5969 - mae: 1.6155 - val_loss: 14.2825 - val_mse: 14.2825 - val_mae: 1.6205 - lr: 6.2704e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 15.5130 - mse: 15.5130 - mae: 1.6080 - val_loss: 14.3047 - val_mse: 14.3047 - val_mae: 1.6646 - lr: 6.2704e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 15.4472 - mse: 15.4472 - mae: 1.6030 - val_loss: 14.2751 - val_mse: 14.2751 - val_mae: 1.6726 - lr: 6.2704e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 4s - loss: 15.3965 - mse: 15.3965 - mae: 1.6020 - val_loss: 14.1900 - val_mse: 14.1900 - val_mae: 1.5890 - lr: 6.2704e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 3s - loss: 15.3834 - mse: 15.3834 - mae: 1.6012 - val_loss: 14.1852 - val_mse: 14.1852 - val_mae: 1.5567 - lr: 6.2704e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 2s - loss: 15.3523 - mse: 15.3523 - mae: 1.5926 - val_loss: 14.1969 - val_mse: 14.1969 - val_mae: 1.6422 - lr: 6.2704e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 2s - loss: 15.3178 - mse: 15.3178 - mae: 1.5955 - val_loss: 14.1781 - val_mse: 14.1781 - val_mae: 1.5762 - lr: 6.2704e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 2s - loss: 15.3114 - mse: 15.3114 - mae: 1.5933 - val_loss: 14.2116 - val_mse: 14.2116 - val_mae: 1.6170 - lr: 6.2704e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 3s - loss: 15.2873 - mse: 15.2873 - mae: 1.5947 - val_loss: 14.0822 - val_mse: 14.0822 - val_mae: 1.5544 - lr: 6.2704e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 3s - loss: 15.2576 - mse: 15.2576 - mae: 1.5898 - val_loss: 14.1941 - val_mse: 14.1941 - val_mae: 1.6019 - lr: 6.2704e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 13/100\n",
            "1000/1000 - 2s - loss: 15.2601 - mse: 15.2601 - mae: 1.5849 - val_loss: 14.1122 - val_mse: 14.1122 - val_mae: 1.5927 - lr: 6.2704e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 14/100\n",
            "1000/1000 - 3s - loss: 15.2260 - mse: 15.2260 - mae: 1.5859 - val_loss: 14.0433 - val_mse: 14.0433 - val_mae: 1.6149 - lr: 6.2704e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 15/100\n",
            "1000/1000 - 3s - loss: 15.1841 - mse: 15.1841 - mae: 1.5862 - val_loss: 14.0529 - val_mse: 14.0529 - val_mae: 1.6370 - lr: 6.2704e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 16/100\n",
            "1000/1000 - 3s - loss: 15.1667 - mse: 15.1667 - mae: 1.5827 - val_loss: 14.1555 - val_mse: 14.1555 - val_mae: 1.5734 - lr: 6.2704e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 17/100\n",
            "1000/1000 - 2s - loss: 15.1798 - mse: 15.1798 - mae: 1.5833 - val_loss: 13.9135 - val_mse: 13.9135 - val_mae: 1.6129 - lr: 6.2704e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 18/100\n",
            "1000/1000 - 3s - loss: 15.1711 - mse: 15.1711 - mae: 1.5839 - val_loss: 14.0115 - val_mse: 14.0115 - val_mae: 1.6271 - lr: 6.2704e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 19/100\n",
            "1000/1000 - 3s - loss: 15.1335 - mse: 15.1335 - mae: 1.5830 - val_loss: 14.1616 - val_mse: 14.1616 - val_mae: 1.5640 - lr: 6.2704e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 20/100\n",
            "1000/1000 - 3s - loss: 15.1373 - mse: 15.1373 - mae: 1.5786 - val_loss: 14.1545 - val_mse: 14.1545 - val_mae: 1.5729 - lr: 6.2704e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 21/100\n",
            "1000/1000 - 3s - loss: 15.0993 - mse: 15.0993 - mae: 1.5768 - val_loss: 14.1785 - val_mse: 14.1785 - val_mae: 1.5886 - lr: 6.2704e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 22/100\n",
            "1000/1000 - 2s - loss: 15.0734 - mse: 15.0734 - mae: 1.5773 - val_loss: 13.9692 - val_mse: 13.9692 - val_mae: 1.5795 - lr: 6.2704e-04 - 2s/epoch - 2ms/step\n",
            "Score for fold 1: loss of 13.969204902648926\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 3s - loss: 15.0738 - mse: 15.0738 - mae: 1.5682 - val_loss: 13.9535 - val_mse: 13.9535 - val_mae: 1.5785 - lr: 6.2704e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 15.0414 - mse: 15.0414 - mae: 1.5659 - val_loss: 13.9404 - val_mse: 13.9404 - val_mae: 1.5713 - lr: 6.2704e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 3s - loss: 15.0325 - mse: 15.0325 - mae: 1.5612 - val_loss: 13.8981 - val_mse: 13.8981 - val_mae: 1.6003 - lr: 6.2704e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 14.9804 - mse: 14.9804 - mae: 1.5632 - val_loss: 13.9431 - val_mse: 13.9431 - val_mae: 1.6135 - lr: 6.2704e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 3s - loss: 14.9802 - mse: 14.9802 - mae: 1.5612 - val_loss: 13.9808 - val_mse: 13.9808 - val_mae: 1.6424 - lr: 6.2704e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 14.9105 - mse: 14.9105 - mae: 1.5614 - val_loss: 13.9583 - val_mse: 13.9583 - val_mae: 1.6431 - lr: 6.2704e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 3s - loss: 14.9279 - mse: 14.9279 - mae: 1.5595 - val_loss: 14.4587 - val_mse: 14.4587 - val_mae: 1.5563 - lr: 6.2704e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 2s - loss: 14.9019 - mse: 14.9019 - mae: 1.5556 - val_loss: 14.2192 - val_mse: 14.2192 - val_mae: 1.6268 - lr: 6.2704e-04 - 2s/epoch - 2ms/step\n",
            "Score for fold 2: loss of 14.219197273254395\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 3s - loss: 12.9833 - mse: 12.9833 - mae: 1.5696 - val_loss: 21.6106 - val_mse: 21.6106 - val_mae: 1.5629 - lr: 6.2704e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 3s - loss: 12.9678 - mse: 12.9678 - mae: 1.5682 - val_loss: 21.7932 - val_mse: 21.7932 - val_mae: 1.6372 - lr: 6.2704e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 12.9026 - mse: 12.9026 - mae: 1.5683 - val_loss: 21.7942 - val_mse: 21.7942 - val_mae: 1.5669 - lr: 6.2704e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 12.9265 - mse: 12.9265 - mae: 1.5626 - val_loss: 21.7861 - val_mse: 21.7861 - val_mae: 1.5909 - lr: 6.2704e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 3s - loss: 12.8881 - mse: 12.8881 - mae: 1.5630 - val_loss: 21.8399 - val_mse: 21.8399 - val_mae: 1.5744 - lr: 6.2704e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 3s - loss: 12.9365 - mse: 12.9365 - mae: 1.5606 - val_loss: 21.9001 - val_mse: 21.9001 - val_mae: 1.6096 - lr: 6.2704e-04 - 3s/epoch - 3ms/step\n",
            "Score for fold 3: loss of 21.900135040283203\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 15.0149 - mse: 15.0149 - mae: 1.5751 - val_loss: 13.3770 - val_mse: 13.3770 - val_mae: 1.5341 - lr: 6.2704e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 14.9231 - mse: 14.9231 - mae: 1.5697 - val_loss: 13.5515 - val_mse: 13.5515 - val_mae: 1.5353 - lr: 6.2704e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 14.9528 - mse: 14.9528 - mae: 1.5685 - val_loss: 13.5689 - val_mse: 13.5689 - val_mae: 1.5618 - lr: 6.2704e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 3s - loss: 14.8572 - mse: 14.8572 - mae: 1.5678 - val_loss: 13.4744 - val_mse: 13.4744 - val_mae: 1.5828 - lr: 6.2704e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 3s - loss: 14.8529 - mse: 14.8529 - mae: 1.5680 - val_loss: 13.4451 - val_mse: 13.4451 - val_mae: 1.5553 - lr: 6.2704e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 3s - loss: 14.8446 - mse: 14.8446 - mae: 1.5660 - val_loss: 13.5247 - val_mse: 13.5247 - val_mae: 1.5404 - lr: 6.2704e-04 - 3s/epoch - 3ms/step\n",
            "Score for fold 4: loss of 13.524677276611328\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 15.5559 - mse: 15.5559 - mae: 1.5704 - val_loss: 10.5591 - val_mse: 10.5591 - val_mae: 1.5595 - lr: 6.2704e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 15.4284 - mse: 15.4284 - mae: 1.5626 - val_loss: 10.6364 - val_mse: 10.6364 - val_mae: 1.5750 - lr: 6.2704e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 3s - loss: 15.4399 - mse: 15.4399 - mae: 1.5608 - val_loss: 10.7727 - val_mse: 10.7727 - val_mae: 1.5227 - lr: 6.2704e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 3s - loss: 15.5278 - mse: 15.5278 - mae: 1.5658 - val_loss: 10.7602 - val_mse: 10.7602 - val_mae: 1.5304 - lr: 6.2704e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 15.3792 - mse: 15.3792 - mae: 1.5599 - val_loss: 10.8876 - val_mse: 10.8876 - val_mae: 1.6038 - lr: 6.2704e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 3s - loss: 15.4399 - mse: 15.4399 - mae: 1.5596 - val_loss: 10.8093 - val_mse: 10.8093 - val_mae: 1.5196 - lr: 6.2704e-04 - 3s/epoch - 3ms/step\n",
            "Score for fold 5: loss of 10.809273719787598\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-10-15 05:49:43,996]\u001b[0m Finished trial#13 resulted in value: 14.884. Current best value is 14.863999999999999 with parameters: {'activation': 'tanh', 'num_hidden_layer': 3, 'i': 6, 'learning_rate': 0.00048617401659243237}.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 3s - loss: 16.8556 - mse: 16.8556 - mae: 1.6684 - val_loss: 11.6502 - val_mse: 11.6502 - val_mae: 1.5661 - lr: 0.0013 - 3s/epoch - 3ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 16.3623 - mse: 16.3623 - mae: 1.6288 - val_loss: 11.5438 - val_mse: 11.5438 - val_mae: 1.6802 - lr: 0.0013 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 3s - loss: 16.2670 - mse: 16.2670 - mae: 1.6180 - val_loss: 11.5801 - val_mse: 11.5801 - val_mae: 1.5682 - lr: 0.0013 - 3s/epoch - 3ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 3s - loss: 16.2175 - mse: 16.2175 - mae: 1.6115 - val_loss: 11.3458 - val_mse: 11.3458 - val_mae: 1.5845 - lr: 0.0013 - 3s/epoch - 3ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 3s - loss: 16.2092 - mse: 16.2092 - mae: 1.6097 - val_loss: 11.1793 - val_mse: 11.1793 - val_mae: 1.6687 - lr: 0.0013 - 3s/epoch - 3ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 3s - loss: 16.1681 - mse: 16.1681 - mae: 1.6103 - val_loss: 11.8296 - val_mse: 11.8296 - val_mae: 1.7102 - lr: 0.0013 - 3s/epoch - 3ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 16.1527 - mse: 16.1527 - mae: 1.6048 - val_loss: 11.4008 - val_mse: 11.4008 - val_mae: 1.6017 - lr: 0.0013 - 2s/epoch - 2ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 3s - loss: 16.1005 - mse: 16.1005 - mae: 1.6061 - val_loss: 11.4329 - val_mse: 11.4329 - val_mae: 1.7524 - lr: 0.0013 - 3s/epoch - 3ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 2s - loss: 16.1060 - mse: 16.1060 - mae: 1.6060 - val_loss: 11.0170 - val_mse: 11.0170 - val_mae: 1.5423 - lr: 0.0013 - 2s/epoch - 2ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 2s - loss: 16.1215 - mse: 16.1215 - mae: 1.6087 - val_loss: 11.4030 - val_mse: 11.4030 - val_mae: 1.5857 - lr: 0.0013 - 2s/epoch - 2ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 3s - loss: 16.0651 - mse: 16.0651 - mae: 1.5986 - val_loss: 11.4325 - val_mse: 11.4325 - val_mae: 1.4904 - lr: 0.0013 - 3s/epoch - 3ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 3s - loss: 16.0888 - mse: 16.0888 - mae: 1.6007 - val_loss: 11.4607 - val_mse: 11.4607 - val_mae: 1.5827 - lr: 0.0013 - 3s/epoch - 3ms/step\n",
            "Epoch 13/100\n",
            "1000/1000 - 3s - loss: 16.0827 - mse: 16.0827 - mae: 1.6050 - val_loss: 11.1492 - val_mse: 11.1492 - val_mae: 1.5335 - lr: 0.0013 - 3s/epoch - 3ms/step\n",
            "Epoch 14/100\n",
            "1000/1000 - 3s - loss: 16.0443 - mse: 16.0443 - mae: 1.6001 - val_loss: 11.5158 - val_mse: 11.5158 - val_mae: 1.5104 - lr: 0.0013 - 3s/epoch - 3ms/step\n",
            "Score for fold 1: loss of 11.515833854675293\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 3s - loss: 15.7230 - mse: 15.7230 - mae: 1.6053 - val_loss: 12.3093 - val_mse: 12.3093 - val_mae: 1.5366 - lr: 0.0010 - 3s/epoch - 3ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 15.6922 - mse: 15.6922 - mae: 1.6044 - val_loss: 12.1422 - val_mse: 12.1422 - val_mae: 1.5661 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 15.6710 - mse: 15.6710 - mae: 1.6091 - val_loss: 12.9838 - val_mse: 12.9838 - val_mae: 1.4679 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 3s - loss: 15.6753 - mse: 15.6753 - mae: 1.6112 - val_loss: 12.5286 - val_mse: 12.5286 - val_mae: 1.6033 - lr: 0.0010 - 3s/epoch - 3ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 3s - loss: 15.7113 - mse: 15.7113 - mae: 1.6100 - val_loss: 12.4908 - val_mse: 12.4908 - val_mae: 1.5327 - lr: 0.0010 - 3s/epoch - 3ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 15.6724 - mse: 15.6724 - mae: 1.6109 - val_loss: 12.4811 - val_mse: 12.4811 - val_mae: 1.7261 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 15.6537 - mse: 15.6537 - mae: 1.6079 - val_loss: 12.2439 - val_mse: 12.2439 - val_mae: 1.6092 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Score for fold 2: loss of 12.243858337402344\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 14.5368 - mse: 14.5368 - mae: 1.5849 - val_loss: 16.6521 - val_mse: 16.6521 - val_mae: 1.5561 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 14.5611 - mse: 14.5611 - mae: 1.5828 - val_loss: 16.4852 - val_mse: 16.4852 - val_mae: 1.7134 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 14.4942 - mse: 14.4942 - mae: 1.5841 - val_loss: 16.4076 - val_mse: 16.4076 - val_mae: 1.6191 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 14.5109 - mse: 14.5109 - mae: 1.5867 - val_loss: 16.4748 - val_mse: 16.4748 - val_mae: 1.6695 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 3s - loss: 14.5601 - mse: 14.5601 - mae: 1.5836 - val_loss: 16.5577 - val_mse: 16.5577 - val_mae: 1.6282 - lr: 0.0010 - 3s/epoch - 3ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 14.4882 - mse: 14.4882 - mae: 1.5880 - val_loss: 16.5314 - val_mse: 16.5314 - val_mae: 1.6449 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 3s - loss: 14.5183 - mse: 14.5183 - mae: 1.5899 - val_loss: 16.6098 - val_mse: 16.6098 - val_mae: 1.6016 - lr: 0.0010 - 3s/epoch - 3ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 3s - loss: 14.5417 - mse: 14.5417 - mae: 1.5832 - val_loss: 16.8940 - val_mse: 16.8940 - val_mae: 1.5347 - lr: 0.0010 - 3s/epoch - 3ms/step\n",
            "Score for fold 3: loss of 16.89400863647461\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 14.3748 - mse: 14.3748 - mae: 1.5916 - val_loss: 17.3023 - val_mse: 17.3023 - val_mae: 1.6132 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 14.3299 - mse: 14.3299 - mae: 1.5857 - val_loss: 17.3523 - val_mse: 17.3523 - val_mae: 1.6707 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 3s - loss: 14.3607 - mse: 14.3607 - mae: 1.5875 - val_loss: 17.4414 - val_mse: 17.4414 - val_mae: 1.5315 - lr: 0.0010 - 3s/epoch - 3ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 14.2870 - mse: 14.2870 - mae: 1.5878 - val_loss: 17.0608 - val_mse: 17.0608 - val_mae: 1.6949 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 3s - loss: 14.3277 - mse: 14.3277 - mae: 1.5912 - val_loss: 17.3165 - val_mse: 17.3165 - val_mae: 1.6258 - lr: 0.0010 - 3s/epoch - 3ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 3s - loss: 14.3153 - mse: 14.3153 - mae: 1.5923 - val_loss: 17.9887 - val_mse: 17.9887 - val_mae: 1.5658 - lr: 0.0010 - 3s/epoch - 3ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 3s - loss: 14.3489 - mse: 14.3489 - mae: 1.5945 - val_loss: 17.2923 - val_mse: 17.2923 - val_mae: 1.5791 - lr: 0.0010 - 3s/epoch - 3ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 3s - loss: 14.2492 - mse: 14.2492 - mae: 1.5851 - val_loss: 17.4776 - val_mse: 17.4776 - val_mae: 1.5522 - lr: 0.0010 - 3s/epoch - 3ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 2s - loss: 14.3511 - mse: 14.3511 - mae: 1.5894 - val_loss: 17.0759 - val_mse: 17.0759 - val_mae: 1.7797 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Score for fold 4: loss of 17.07590103149414\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 3s - loss: 14.2459 - mse: 14.2459 - mae: 1.5862 - val_loss: 17.5179 - val_mse: 17.5179 - val_mae: 1.5801 - lr: 0.0010 - 3s/epoch - 3ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 3s - loss: 14.2633 - mse: 14.2633 - mae: 1.5999 - val_loss: 17.3663 - val_mse: 17.3663 - val_mae: 1.6584 - lr: 0.0010 - 3s/epoch - 3ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 3s - loss: 14.2975 - mse: 14.2975 - mae: 1.6005 - val_loss: 17.3997 - val_mse: 17.3997 - val_mae: 1.6976 - lr: 0.0010 - 3s/epoch - 3ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 3s - loss: 14.3239 - mse: 14.3239 - mae: 1.5908 - val_loss: 17.4410 - val_mse: 17.4410 - val_mae: 1.6027 - lr: 0.0010 - 3s/epoch - 3ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 14.2624 - mse: 14.2624 - mae: 1.5948 - val_loss: 17.5906 - val_mse: 17.5906 - val_mae: 1.7623 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 3s - loss: 14.3016 - mse: 14.3016 - mae: 1.6049 - val_loss: 17.4360 - val_mse: 17.4360 - val_mae: 1.6814 - lr: 0.0010 - 3s/epoch - 3ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 14.1676 - mse: 14.1676 - mae: 1.5925 - val_loss: 17.5420 - val_mse: 17.5420 - val_mae: 1.6095 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Score for fold 5: loss of 17.54204559326172\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-10-15 05:52:09,497]\u001b[0m Finished trial#14 resulted in value: 15.053999999999998. Current best value is 14.863999999999999 with parameters: {'activation': 'tanh', 'num_hidden_layer': 3, 'i': 6, 'learning_rate': 0.00048617401659243237}.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 3s - loss: 14.2260 - mse: 14.2260 - mae: 1.6177 - val_loss: 20.6532 - val_mse: 20.6532 - val_mae: 1.6442 - lr: 6.7177e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 3s - loss: 13.7143 - mse: 13.7143 - mae: 1.5960 - val_loss: 20.7286 - val_mse: 20.7286 - val_mae: 1.6888 - lr: 6.7177e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 13.6379 - mse: 13.6379 - mae: 1.5929 - val_loss: 20.8433 - val_mse: 20.8433 - val_mae: 1.5680 - lr: 6.7177e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 13.5583 - mse: 13.5583 - mae: 1.5869 - val_loss: 20.7619 - val_mse: 20.7619 - val_mae: 1.5722 - lr: 6.7177e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 3s - loss: 13.4792 - mse: 13.4792 - mae: 1.5848 - val_loss: 20.7607 - val_mse: 20.7607 - val_mae: 1.5413 - lr: 6.7177e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 13.4990 - mse: 13.4990 - mae: 1.5809 - val_loss: 21.0498 - val_mse: 21.0498 - val_mae: 1.6080 - lr: 6.7177e-04 - 2s/epoch - 2ms/step\n",
            "Score for fold 1: loss of 21.049837112426758\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 3s - loss: 15.6133 - mse: 15.6133 - mae: 1.5806 - val_loss: 12.2485 - val_mse: 12.2485 - val_mae: 1.6081 - lr: 6.7177e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 3s - loss: 15.5475 - mse: 15.5475 - mae: 1.5792 - val_loss: 12.2730 - val_mse: 12.2730 - val_mae: 1.6172 - lr: 6.7177e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 15.4848 - mse: 15.4848 - mae: 1.5795 - val_loss: 12.2338 - val_mse: 12.2338 - val_mae: 1.6330 - lr: 6.7177e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 3s - loss: 15.4690 - mse: 15.4690 - mae: 1.5798 - val_loss: 12.5727 - val_mse: 12.5727 - val_mae: 1.5992 - lr: 6.7177e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 15.4236 - mse: 15.4236 - mae: 1.5775 - val_loss: 12.3998 - val_mse: 12.3998 - val_mae: 1.5576 - lr: 6.7177e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 15.4193 - mse: 15.4193 - mae: 1.5733 - val_loss: 12.5443 - val_mse: 12.5443 - val_mae: 1.6179 - lr: 6.7177e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 3s - loss: 15.4281 - mse: 15.4281 - mae: 1.5748 - val_loss: 12.4930 - val_mse: 12.4930 - val_mae: 1.5055 - lr: 6.7177e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 2s - loss: 15.3157 - mse: 15.3157 - mae: 1.5700 - val_loss: 12.2597 - val_mse: 12.2597 - val_mae: 1.5932 - lr: 6.7177e-04 - 2s/epoch - 2ms/step\n",
            "Score for fold 2: loss of 12.259722709655762\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 15.9321 - mse: 15.9321 - mae: 1.6093 - val_loss: 9.7790 - val_mse: 9.7790 - val_mae: 1.4314 - lr: 6.7177e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 15.9708 - mse: 15.9708 - mae: 1.5990 - val_loss: 9.7056 - val_mse: 9.7056 - val_mae: 1.5584 - lr: 6.7177e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 3s - loss: 15.8691 - mse: 15.8691 - mae: 1.5931 - val_loss: 9.7999 - val_mse: 9.7999 - val_mae: 1.4888 - lr: 6.7177e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 15.9292 - mse: 15.9292 - mae: 1.6007 - val_loss: 9.9630 - val_mse: 9.9630 - val_mae: 1.5449 - lr: 6.7177e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 3s - loss: 15.6217 - mse: 15.6217 - mae: 1.5928 - val_loss: 9.8406 - val_mse: 9.8406 - val_mae: 1.5561 - lr: 6.7177e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 15.6395 - mse: 15.6395 - mae: 1.5900 - val_loss: 10.0617 - val_mse: 10.0617 - val_mae: 1.5370 - lr: 6.7177e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 15.6349 - mse: 15.6349 - mae: 1.5850 - val_loss: 10.0100 - val_mse: 10.0100 - val_mae: 1.5930 - lr: 6.7177e-04 - 2s/epoch - 2ms/step\n",
            "Score for fold 3: loss of 10.010009765625\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 13.6314 - mse: 13.6314 - mae: 1.5617 - val_loss: 18.9724 - val_mse: 18.9724 - val_mae: 1.5475 - lr: 6.7177e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 3s - loss: 13.5742 - mse: 13.5742 - mae: 1.5627 - val_loss: 18.5281 - val_mse: 18.5281 - val_mae: 1.6601 - lr: 6.7177e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 13.5481 - mse: 13.5481 - mae: 1.5592 - val_loss: 18.0955 - val_mse: 18.0955 - val_mae: 1.7211 - lr: 6.7177e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 3s - loss: 13.5736 - mse: 13.5736 - mae: 1.5672 - val_loss: 18.6933 - val_mse: 18.6933 - val_mae: 1.5964 - lr: 6.7177e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 13.5032 - mse: 13.5032 - mae: 1.5626 - val_loss: 18.3012 - val_mse: 18.3012 - val_mae: 1.6582 - lr: 6.7177e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 13.5233 - mse: 13.5233 - mae: 1.5678 - val_loss: 19.0274 - val_mse: 19.0274 - val_mae: 1.6024 - lr: 6.7177e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 13.5534 - mse: 13.5534 - mae: 1.5665 - val_loss: 18.0940 - val_mse: 18.0940 - val_mae: 1.5698 - lr: 6.7177e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 2s - loss: 13.4017 - mse: 13.4017 - mae: 1.5660 - val_loss: 18.3440 - val_mse: 18.3440 - val_mae: 1.8393 - lr: 6.7177e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 3s - loss: 13.3789 - mse: 13.3789 - mae: 1.5663 - val_loss: 18.7553 - val_mse: 18.7553 - val_mae: 1.5697 - lr: 6.7177e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 3s - loss: 13.3647 - mse: 13.3647 - mae: 1.5647 - val_loss: 18.3911 - val_mse: 18.3911 - val_mae: 1.6068 - lr: 6.7177e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 2s - loss: 13.3642 - mse: 13.3642 - mae: 1.5623 - val_loss: 18.5274 - val_mse: 18.5274 - val_mae: 1.7021 - lr: 6.7177e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 3s - loss: 13.2989 - mse: 13.2989 - mae: 1.5584 - val_loss: 18.8540 - val_mse: 18.8540 - val_mae: 1.6521 - lr: 6.7177e-04 - 3s/epoch - 3ms/step\n",
            "Score for fold 4: loss of 18.853979110717773\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 3s - loss: 14.9383 - mse: 14.9383 - mae: 1.5708 - val_loss: 12.0173 - val_mse: 12.0173 - val_mae: 1.5157 - lr: 6.7177e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 14.9452 - mse: 14.9452 - mae: 1.5602 - val_loss: 12.0437 - val_mse: 12.0437 - val_mae: 1.7615 - lr: 6.7177e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 3s - loss: 14.8225 - mse: 14.8225 - mae: 1.5615 - val_loss: 12.2581 - val_mse: 12.2581 - val_mae: 1.5548 - lr: 6.7177e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 4s - loss: 14.5841 - mse: 14.5841 - mae: 1.5542 - val_loss: 12.4426 - val_mse: 12.4426 - val_mae: 1.7818 - lr: 6.7177e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 3s - loss: 14.6682 - mse: 14.6682 - mae: 1.5560 - val_loss: 12.3676 - val_mse: 12.3676 - val_mae: 1.6875 - lr: 6.7177e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 14.6844 - mse: 14.6844 - mae: 1.5579 - val_loss: 12.4040 - val_mse: 12.4040 - val_mae: 1.7406 - lr: 6.7177e-04 - 2s/epoch - 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-10-15 05:54:11,048]\u001b[0m Finished trial#15 resulted in value: 14.914000000000001. Current best value is 14.863999999999999 with parameters: {'activation': 'tanh', 'num_hidden_layer': 3, 'i': 6, 'learning_rate': 0.00048617401659243237}.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Score for fold 5: loss of 12.404019355773926\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 5s - loss: 17.2034 - mse: 17.2034 - mae: 1.6838 - val_loss: 11.3928 - val_mse: 11.3928 - val_mae: 1.6171 - lr: 0.0024 - 5s/epoch - 5ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 4s - loss: 16.8919 - mse: 16.8919 - mae: 1.6526 - val_loss: 11.2634 - val_mse: 11.2634 - val_mae: 1.6395 - lr: 0.0024 - 4s/epoch - 4ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 4s - loss: 16.9275 - mse: 16.9275 - mae: 1.6568 - val_loss: 11.2433 - val_mse: 11.2433 - val_mae: 1.6300 - lr: 0.0024 - 4s/epoch - 4ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 4s - loss: 16.9536 - mse: 16.9536 - mae: 1.6513 - val_loss: 11.2106 - val_mse: 11.2106 - val_mae: 1.7710 - lr: 0.0024 - 4s/epoch - 4ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 4s - loss: 16.9669 - mse: 16.9669 - mae: 1.6454 - val_loss: 11.3275 - val_mse: 11.3275 - val_mae: 1.5950 - lr: 0.0024 - 4s/epoch - 4ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 4s - loss: 16.9094 - mse: 16.9094 - mae: 1.6461 - val_loss: 11.2635 - val_mse: 11.2635 - val_mae: 1.7662 - lr: 0.0024 - 4s/epoch - 4ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 4s - loss: 16.9045 - mse: 16.9045 - mae: 1.6475 - val_loss: 11.1678 - val_mse: 11.1678 - val_mae: 1.7278 - lr: 0.0024 - 4s/epoch - 4ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 4s - loss: 16.9512 - mse: 16.9512 - mae: 1.6566 - val_loss: 11.1520 - val_mse: 11.1520 - val_mae: 1.6240 - lr: 0.0024 - 4s/epoch - 4ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 4s - loss: 16.9397 - mse: 16.9397 - mae: 1.6515 - val_loss: 11.2011 - val_mse: 11.2011 - val_mae: 1.7204 - lr: 0.0024 - 4s/epoch - 4ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 4s - loss: 16.9329 - mse: 16.9329 - mae: 1.6423 - val_loss: 11.1523 - val_mse: 11.1523 - val_mae: 1.6569 - lr: 0.0024 - 4s/epoch - 4ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 4s - loss: 16.9505 - mse: 16.9505 - mae: 1.6509 - val_loss: 11.2233 - val_mse: 11.2233 - val_mae: 1.6561 - lr: 0.0024 - 4s/epoch - 4ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 4s - loss: 16.9143 - mse: 16.9143 - mae: 1.6460 - val_loss: 11.1383 - val_mse: 11.1383 - val_mae: 1.6846 - lr: 0.0024 - 4s/epoch - 4ms/step\n",
            "Epoch 13/100\n",
            "1000/1000 - 4s - loss: 16.9237 - mse: 16.9237 - mae: 1.6581 - val_loss: 11.2549 - val_mse: 11.2549 - val_mae: 1.5862 - lr: 0.0024 - 4s/epoch - 4ms/step\n",
            "Epoch 14/100\n",
            "1000/1000 - 4s - loss: 16.8679 - mse: 16.8679 - mae: 1.6546 - val_loss: 11.1462 - val_mse: 11.1462 - val_mae: 1.6723 - lr: 0.0024 - 4s/epoch - 4ms/step\n",
            "Epoch 15/100\n",
            "1000/1000 - 4s - loss: 16.9152 - mse: 16.9152 - mae: 1.6617 - val_loss: 11.2022 - val_mse: 11.2022 - val_mae: 1.6018 - lr: 0.0024 - 4s/epoch - 4ms/step\n",
            "Epoch 16/100\n",
            "1000/1000 - 4s - loss: 16.8778 - mse: 16.8778 - mae: 1.6456 - val_loss: 11.1652 - val_mse: 11.1652 - val_mae: 1.6148 - lr: 0.0024 - 4s/epoch - 4ms/step\n",
            "Epoch 17/100\n",
            "1000/1000 - 4s - loss: 16.8980 - mse: 16.8980 - mae: 1.6507 - val_loss: 11.1540 - val_mse: 11.1540 - val_mae: 1.6636 - lr: 0.0024 - 4s/epoch - 4ms/step\n",
            "Score for fold 1: loss of 11.153997421264648\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 4s - loss: 16.2335 - mse: 16.2335 - mae: 1.6422 - val_loss: 13.3905 - val_mse: 13.3905 - val_mae: 1.7541 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 4s - loss: 16.2929 - mse: 16.2929 - mae: 1.6422 - val_loss: 13.3093 - val_mse: 13.3093 - val_mae: 1.6197 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 4s - loss: 16.3155 - mse: 16.3155 - mae: 1.6492 - val_loss: 13.2982 - val_mse: 13.2982 - val_mae: 1.6075 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 4s - loss: 16.2959 - mse: 16.2959 - mae: 1.6395 - val_loss: 13.3814 - val_mse: 13.3814 - val_mae: 1.6031 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 4s - loss: 16.3249 - mse: 16.3249 - mae: 1.6457 - val_loss: 13.5392 - val_mse: 13.5392 - val_mae: 1.5622 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 4s - loss: 16.3587 - mse: 16.3587 - mae: 1.6463 - val_loss: 13.2899 - val_mse: 13.2899 - val_mae: 1.6216 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 4s - loss: 16.3362 - mse: 16.3362 - mae: 1.6541 - val_loss: 13.4533 - val_mse: 13.4533 - val_mae: 1.5756 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 4s - loss: 16.3040 - mse: 16.3040 - mae: 1.6484 - val_loss: 13.3669 - val_mse: 13.3669 - val_mae: 1.5824 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 5s - loss: 16.3409 - mse: 16.3409 - mae: 1.6506 - val_loss: 13.2587 - val_mse: 13.2587 - val_mae: 1.6639 - lr: 0.0010 - 5s/epoch - 5ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 4s - loss: 16.2989 - mse: 16.2989 - mae: 1.6525 - val_loss: 13.5677 - val_mse: 13.5677 - val_mae: 1.5870 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 6s - loss: 16.3452 - mse: 16.3452 - mae: 1.6546 - val_loss: 13.3465 - val_mse: 13.3465 - val_mae: 1.7107 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 4s - loss: 16.3332 - mse: 16.3332 - mae: 1.6485 - val_loss: 13.5645 - val_mse: 13.5645 - val_mae: 1.5571 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 13/100\n",
            "1000/1000 - 4s - loss: 16.3799 - mse: 16.3799 - mae: 1.6502 - val_loss: 13.6417 - val_mse: 13.6417 - val_mae: 1.5724 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 14/100\n",
            "1000/1000 - 4s - loss: 16.3677 - mse: 16.3677 - mae: 1.6500 - val_loss: 13.3980 - val_mse: 13.3980 - val_mae: 1.5864 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Score for fold 2: loss of 13.397965431213379\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 4s - loss: 13.1111 - mse: 13.1111 - mae: 1.6362 - val_loss: 26.5216 - val_mse: 26.5216 - val_mae: 1.7909 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 5s - loss: 13.0460 - mse: 13.0460 - mae: 1.6357 - val_loss: 26.4109 - val_mse: 26.4109 - val_mae: 1.6696 - lr: 0.0010 - 5s/epoch - 5ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 4s - loss: 13.0580 - mse: 13.0580 - mae: 1.6361 - val_loss: 26.4508 - val_mse: 26.4508 - val_mae: 1.6622 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 4s - loss: 13.0273 - mse: 13.0273 - mae: 1.6380 - val_loss: 27.3399 - val_mse: 27.3399 - val_mae: 1.6246 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 4s - loss: 13.0835 - mse: 13.0835 - mae: 1.6397 - val_loss: 26.3855 - val_mse: 26.3855 - val_mae: 1.6693 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 4s - loss: 13.1017 - mse: 13.1017 - mae: 1.6418 - val_loss: 26.3470 - val_mse: 26.3470 - val_mae: 1.7152 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 4s - loss: 13.1316 - mse: 13.1316 - mae: 1.6377 - val_loss: 26.3937 - val_mse: 26.3937 - val_mae: 1.7295 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 4s - loss: 13.1283 - mse: 13.1283 - mae: 1.6421 - val_loss: 26.3741 - val_mse: 26.3741 - val_mae: 1.6924 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 4s - loss: 13.1379 - mse: 13.1379 - mae: 1.6414 - val_loss: 26.4388 - val_mse: 26.4388 - val_mae: 1.6683 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 4s - loss: 13.0121 - mse: 13.0121 - mae: 1.6331 - val_loss: 26.5537 - val_mse: 26.5537 - val_mae: 1.7799 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 4s - loss: 13.1600 - mse: 13.1600 - mae: 1.6394 - val_loss: 26.6440 - val_mse: 26.6440 - val_mae: 1.6456 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Score for fold 3: loss of 26.64404296875\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 4s - loss: 16.8415 - mse: 16.8415 - mae: 1.6690 - val_loss: 11.5156 - val_mse: 11.5156 - val_mae: 1.6105 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 4s - loss: 16.8244 - mse: 16.8244 - mae: 1.6665 - val_loss: 16.8127 - val_mse: 16.8127 - val_mae: 1.7764 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 4s - loss: 16.8111 - mse: 16.8111 - mae: 1.6700 - val_loss: 11.5524 - val_mse: 11.5524 - val_mae: 1.6277 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 5s - loss: 16.8987 - mse: 16.8987 - mae: 1.6770 - val_loss: 11.5288 - val_mse: 11.5288 - val_mae: 1.5927 - lr: 0.0010 - 5s/epoch - 5ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 4s - loss: 16.8314 - mse: 16.8314 - mae: 1.6677 - val_loss: 12.3731 - val_mse: 12.3731 - val_mae: 1.7482 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 4s - loss: 16.7995 - mse: 16.7995 - mae: 1.6607 - val_loss: 11.6517 - val_mse: 11.6517 - val_mae: 1.5575 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Score for fold 4: loss of 11.65172004699707\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 4s - loss: 15.7996 - mse: 15.7996 - mae: 1.6386 - val_loss: 15.7648 - val_mse: 15.7648 - val_mae: 1.6361 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 4s - loss: 15.8066 - mse: 15.8066 - mae: 1.6387 - val_loss: 16.9357 - val_mse: 16.9357 - val_mae: 1.6505 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 4s - loss: 15.7522 - mse: 15.7522 - mae: 1.6344 - val_loss: 15.6153 - val_mse: 15.6153 - val_mae: 1.7121 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 5s - loss: 15.7416 - mse: 15.7416 - mae: 1.6345 - val_loss: 15.6289 - val_mse: 15.6289 - val_mae: 1.6976 - lr: 0.0010 - 5s/epoch - 5ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 4s - loss: 15.8286 - mse: 15.8286 - mae: 1.6405 - val_loss: 15.7723 - val_mse: 15.7723 - val_mae: 1.6413 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 4s - loss: 15.7427 - mse: 15.7427 - mae: 1.6443 - val_loss: 15.6051 - val_mse: 15.6051 - val_mae: 1.6780 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 4s - loss: 15.7228 - mse: 15.7228 - mae: 1.6300 - val_loss: 15.6254 - val_mse: 15.6254 - val_mae: 1.6696 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 4s - loss: 15.7278 - mse: 15.7278 - mae: 1.6398 - val_loss: 15.7239 - val_mse: 15.7239 - val_mae: 1.6471 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 4s - loss: 15.7359 - mse: 15.7359 - mae: 1.6386 - val_loss: 15.6362 - val_mse: 15.6362 - val_mae: 1.6854 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 4s - loss: 15.7091 - mse: 15.7091 - mae: 1.6425 - val_loss: 15.9854 - val_mse: 15.9854 - val_mae: 1.6120 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 4s - loss: 15.8537 - mse: 15.8537 - mae: 1.6411 - val_loss: 15.7764 - val_mse: 15.7764 - val_mae: 1.7176 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Score for fold 5: loss of 15.776410102844238\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-10-15 05:59:57,533]\u001b[0m Finished trial#16 resulted in value: 15.723999999999998. Current best value is 14.863999999999999 with parameters: {'activation': 'tanh', 'num_hidden_layer': 3, 'i': 6, 'learning_rate': 0.00048617401659243237}.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 3s - loss: 15.9976 - mse: 15.9976 - mae: 1.6670 - val_loss: 15.2563 - val_mse: 15.2563 - val_mae: 1.5495 - lr: 5.7477e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 15.5849 - mse: 15.5849 - mae: 1.6154 - val_loss: 15.1458 - val_mse: 15.1458 - val_mae: 1.6418 - lr: 5.7477e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 15.4186 - mse: 15.4186 - mae: 1.6120 - val_loss: 14.8130 - val_mse: 14.8130 - val_mae: 1.5989 - lr: 5.7477e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 15.3352 - mse: 15.3352 - mae: 1.6042 - val_loss: 14.8572 - val_mse: 14.8572 - val_mae: 1.5593 - lr: 5.7477e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 3s - loss: 15.2836 - mse: 15.2836 - mae: 1.6012 - val_loss: 14.8347 - val_mse: 14.8347 - val_mae: 1.5616 - lr: 5.7477e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 15.2340 - mse: 15.2340 - mae: 1.5950 - val_loss: 14.9188 - val_mse: 14.9188 - val_mae: 1.5997 - lr: 5.7477e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 15.2165 - mse: 15.2165 - mae: 1.5945 - val_loss: 14.7246 - val_mse: 14.7246 - val_mae: 1.5809 - lr: 5.7477e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 2s - loss: 15.1906 - mse: 15.1906 - mae: 1.5933 - val_loss: 14.7732 - val_mse: 14.7732 - val_mae: 1.6270 - lr: 5.7477e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 3s - loss: 15.1808 - mse: 15.1808 - mae: 1.5977 - val_loss: 15.1142 - val_mse: 15.1142 - val_mae: 1.5744 - lr: 5.7477e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 3s - loss: 15.1332 - mse: 15.1332 - mae: 1.5889 - val_loss: 15.0866 - val_mse: 15.0866 - val_mae: 1.6114 - lr: 5.7477e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 2s - loss: 15.1120 - mse: 15.1120 - mae: 1.5898 - val_loss: 14.7714 - val_mse: 14.7714 - val_mae: 1.5837 - lr: 5.7477e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 2s - loss: 15.0840 - mse: 15.0840 - mae: 1.5903 - val_loss: 14.8445 - val_mse: 14.8445 - val_mae: 1.5377 - lr: 5.7477e-04 - 2s/epoch - 2ms/step\n",
            "Score for fold 1: loss of 14.844517707824707\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 3s - loss: 13.2027 - mse: 13.2027 - mae: 1.5801 - val_loss: 22.3836 - val_mse: 22.3836 - val_mae: 1.6059 - lr: 5.7477e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 13.1754 - mse: 13.1754 - mae: 1.5788 - val_loss: 22.4141 - val_mse: 22.4141 - val_mae: 1.6075 - lr: 5.7477e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 3s - loss: 13.1724 - mse: 13.1724 - mae: 1.5803 - val_loss: 22.2614 - val_mse: 22.2614 - val_mae: 1.6721 - lr: 5.7477e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 13.1561 - mse: 13.1561 - mae: 1.5773 - val_loss: 22.3556 - val_mse: 22.3556 - val_mae: 1.6243 - lr: 5.7477e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 13.1392 - mse: 13.1392 - mae: 1.5740 - val_loss: 22.1947 - val_mse: 22.1947 - val_mae: 1.6125 - lr: 5.7477e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 3s - loss: 13.0867 - mse: 13.0867 - mae: 1.5700 - val_loss: 22.3934 - val_mse: 22.3934 - val_mae: 1.5958 - lr: 5.7477e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 13.0655 - mse: 13.0655 - mae: 1.5681 - val_loss: 22.2147 - val_mse: 22.2147 - val_mae: 1.6456 - lr: 5.7477e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 3s - loss: 13.0560 - mse: 13.0560 - mae: 1.5668 - val_loss: 22.1515 - val_mse: 22.1515 - val_mae: 1.6524 - lr: 5.7477e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 3s - loss: 13.0367 - mse: 13.0367 - mae: 1.5684 - val_loss: 22.7054 - val_mse: 22.7054 - val_mae: 1.5838 - lr: 5.7477e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 2s - loss: 12.9894 - mse: 12.9894 - mae: 1.5654 - val_loss: 22.2053 - val_mse: 22.2053 - val_mae: 1.6646 - lr: 5.7477e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 3s - loss: 12.9735 - mse: 12.9735 - mae: 1.5652 - val_loss: 22.5924 - val_mse: 22.5924 - val_mae: 1.6439 - lr: 5.7477e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 3s - loss: 12.9176 - mse: 12.9176 - mae: 1.5622 - val_loss: 22.3314 - val_mse: 22.3314 - val_mae: 1.6171 - lr: 5.7477e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 13/100\n",
            "1000/1000 - 2s - loss: 12.9123 - mse: 12.9123 - mae: 1.5615 - val_loss: 22.2479 - val_mse: 22.2479 - val_mae: 1.6057 - lr: 5.7477e-04 - 2s/epoch - 2ms/step\n",
            "Score for fold 2: loss of 22.24786949157715\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 3s - loss: 15.6037 - mse: 15.6037 - mae: 1.5812 - val_loss: 11.6379 - val_mse: 11.6379 - val_mae: 1.6139 - lr: 5.7477e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 3s - loss: 15.5676 - mse: 15.5676 - mae: 1.5777 - val_loss: 11.6609 - val_mse: 11.6609 - val_mae: 1.6370 - lr: 5.7477e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 3s - loss: 15.5185 - mse: 15.5185 - mae: 1.5768 - val_loss: 11.5710 - val_mse: 11.5710 - val_mae: 1.5413 - lr: 5.7477e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 15.4985 - mse: 15.4985 - mae: 1.5757 - val_loss: 11.5761 - val_mse: 11.5761 - val_mae: 1.6267 - lr: 5.7477e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 15.5243 - mse: 15.5243 - mae: 1.5749 - val_loss: 11.7168 - val_mse: 11.7168 - val_mae: 1.5454 - lr: 5.7477e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 3s - loss: 15.4754 - mse: 15.4754 - mae: 1.5747 - val_loss: 11.6833 - val_mse: 11.6833 - val_mae: 1.5854 - lr: 5.7477e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 15.4464 - mse: 15.4464 - mae: 1.5727 - val_loss: 11.6607 - val_mse: 11.6607 - val_mae: 1.5419 - lr: 5.7477e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 2s - loss: 15.3636 - mse: 15.3636 - mae: 1.5691 - val_loss: 11.8139 - val_mse: 11.8139 - val_mae: 1.5364 - lr: 5.7477e-04 - 2s/epoch - 2ms/step\n",
            "Score for fold 3: loss of 11.813899993896484\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 3s - loss: 14.5242 - mse: 14.5242 - mae: 1.5614 - val_loss: 15.4252 - val_mse: 15.4252 - val_mae: 1.6111 - lr: 5.7477e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 14.5324 - mse: 14.5324 - mae: 1.5596 - val_loss: 15.5111 - val_mse: 15.5111 - val_mae: 1.5944 - lr: 5.7477e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 3s - loss: 14.5009 - mse: 14.5009 - mae: 1.5540 - val_loss: 15.4946 - val_mse: 15.4946 - val_mae: 1.6122 - lr: 5.7477e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 14.4563 - mse: 14.4563 - mae: 1.5516 - val_loss: 15.2172 - val_mse: 15.2172 - val_mae: 1.6163 - lr: 5.7477e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 3s - loss: 14.4073 - mse: 14.4073 - mae: 1.5489 - val_loss: 15.3159 - val_mse: 15.3159 - val_mae: 1.6886 - lr: 5.7477e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 14.3328 - mse: 14.3328 - mae: 1.5504 - val_loss: 15.7152 - val_mse: 15.7152 - val_mae: 1.6136 - lr: 5.7477e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 14.3946 - mse: 14.3946 - mae: 1.5486 - val_loss: 15.8012 - val_mse: 15.8012 - val_mae: 1.6151 - lr: 5.7477e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 3s - loss: 14.3563 - mse: 14.3563 - mae: 1.5501 - val_loss: 15.5217 - val_mse: 15.5217 - val_mae: 1.5973 - lr: 5.7477e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 2s - loss: 14.2848 - mse: 14.2848 - mae: 1.5445 - val_loss: 15.5764 - val_mse: 15.5764 - val_mae: 1.5943 - lr: 5.7477e-04 - 2s/epoch - 2ms/step\n",
            "Score for fold 4: loss of 15.576432228088379\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 3s - loss: 15.6683 - mse: 15.6683 - mae: 1.5747 - val_loss: 10.0064 - val_mse: 10.0064 - val_mae: 1.5561 - lr: 5.7477e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 15.5675 - mse: 15.5675 - mae: 1.5723 - val_loss: 10.1486 - val_mse: 10.1486 - val_mae: 1.5474 - lr: 5.7477e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 3s - loss: 15.6150 - mse: 15.6150 - mae: 1.5704 - val_loss: 10.1273 - val_mse: 10.1273 - val_mae: 1.5638 - lr: 5.7477e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 15.5486 - mse: 15.5486 - mae: 1.5737 - val_loss: 10.5711 - val_mse: 10.5711 - val_mae: 1.5061 - lr: 5.7477e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 3s - loss: 15.5173 - mse: 15.5173 - mae: 1.5679 - val_loss: 10.2194 - val_mse: 10.2194 - val_mae: 1.6157 - lr: 5.7477e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 3s - loss: 15.5364 - mse: 15.5364 - mae: 1.5692 - val_loss: 10.3505 - val_mse: 10.3505 - val_mae: 1.5575 - lr: 5.7477e-04 - 3s/epoch - 3ms/step\n",
            "Score for fold 5: loss of 10.350475311279297\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-10-15 06:02:34,838]\u001b[0m Finished trial#17 resulted in value: 14.966. Current best value is 14.863999999999999 with parameters: {'activation': 'tanh', 'num_hidden_layer': 3, 'i': 6, 'learning_rate': 0.00048617401659243237}.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 3s - loss: 13.3289 - mse: 13.3289 - mae: 1.6926 - val_loss: 27.8641 - val_mse: 27.8641 - val_mae: 1.6451 - lr: 4.1597e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 12.4404 - mse: 12.4404 - mae: 1.6044 - val_loss: 27.7295 - val_mse: 27.7295 - val_mae: 1.6477 - lr: 4.1597e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 12.3512 - mse: 12.3512 - mae: 1.5979 - val_loss: 27.5969 - val_mse: 27.5969 - val_mae: 1.6197 - lr: 4.1597e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 12.2576 - mse: 12.2576 - mae: 1.5948 - val_loss: 27.6338 - val_mse: 27.6338 - val_mae: 1.6155 - lr: 4.1597e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 12.2123 - mse: 12.2123 - mae: 1.5858 - val_loss: 27.4730 - val_mse: 27.4730 - val_mae: 1.6330 - lr: 4.1597e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 12.1552 - mse: 12.1552 - mae: 1.5879 - val_loss: 27.3444 - val_mse: 27.3444 - val_mae: 1.6300 - lr: 4.1597e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 3s - loss: 12.1205 - mse: 12.1205 - mae: 1.5834 - val_loss: 27.4069 - val_mse: 27.4069 - val_mae: 1.6418 - lr: 4.1597e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 2s - loss: 12.1069 - mse: 12.1069 - mae: 1.5798 - val_loss: 27.3669 - val_mse: 27.3669 - val_mae: 1.6197 - lr: 4.1597e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 2s - loss: 12.0829 - mse: 12.0829 - mae: 1.5806 - val_loss: 27.2503 - val_mse: 27.2503 - val_mae: 1.6242 - lr: 4.1597e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 2s - loss: 12.0693 - mse: 12.0693 - mae: 1.5797 - val_loss: 27.2712 - val_mse: 27.2712 - val_mae: 1.6381 - lr: 4.1597e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 2s - loss: 12.0426 - mse: 12.0426 - mae: 1.5809 - val_loss: 27.2621 - val_mse: 27.2621 - val_mae: 1.5989 - lr: 4.1597e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 2s - loss: 12.0288 - mse: 12.0288 - mae: 1.5766 - val_loss: 27.2570 - val_mse: 27.2570 - val_mae: 1.6076 - lr: 4.1597e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 13/100\n",
            "1000/1000 - 2s - loss: 12.0196 - mse: 12.0196 - mae: 1.5739 - val_loss: 27.1397 - val_mse: 27.1397 - val_mae: 1.6412 - lr: 4.1597e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 14/100\n",
            "1000/1000 - 2s - loss: 12.0006 - mse: 12.0006 - mae: 1.5770 - val_loss: 27.1906 - val_mse: 27.1906 - val_mae: 1.5995 - lr: 4.1597e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 15/100\n",
            "1000/1000 - 2s - loss: 11.9866 - mse: 11.9866 - mae: 1.5717 - val_loss: 27.1504 - val_mse: 27.1504 - val_mae: 1.6270 - lr: 4.1597e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 16/100\n",
            "1000/1000 - 2s - loss: 11.9758 - mse: 11.9758 - mae: 1.5751 - val_loss: 27.1563 - val_mse: 27.1563 - val_mae: 1.6382 - lr: 4.1597e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 17/100\n",
            "1000/1000 - 2s - loss: 11.9870 - mse: 11.9870 - mae: 1.5760 - val_loss: 27.1137 - val_mse: 27.1137 - val_mae: 1.6165 - lr: 4.1597e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 18/100\n",
            "1000/1000 - 2s - loss: 11.9723 - mse: 11.9723 - mae: 1.5720 - val_loss: 27.1205 - val_mse: 27.1205 - val_mae: 1.6061 - lr: 4.1597e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 19/100\n",
            "1000/1000 - 2s - loss: 11.9557 - mse: 11.9557 - mae: 1.5727 - val_loss: 27.1190 - val_mse: 27.1190 - val_mae: 1.6203 - lr: 4.1597e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 20/100\n",
            "1000/1000 - 2s - loss: 11.9582 - mse: 11.9582 - mae: 1.5721 - val_loss: 27.1707 - val_mse: 27.1707 - val_mae: 1.6488 - lr: 4.1597e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 21/100\n",
            "1000/1000 - 2s - loss: 11.9632 - mse: 11.9632 - mae: 1.5717 - val_loss: 27.1520 - val_mse: 27.1520 - val_mae: 1.6235 - lr: 4.1597e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 22/100\n",
            "1000/1000 - 2s - loss: 11.9564 - mse: 11.9564 - mae: 1.5724 - val_loss: 27.0639 - val_mse: 27.0639 - val_mae: 1.6172 - lr: 4.1597e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 23/100\n",
            "1000/1000 - 2s - loss: 11.9429 - mse: 11.9429 - mae: 1.5709 - val_loss: 27.1285 - val_mse: 27.1285 - val_mae: 1.5747 - lr: 4.1597e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 24/100\n",
            "1000/1000 - 2s - loss: 11.9313 - mse: 11.9313 - mae: 1.5739 - val_loss: 27.0902 - val_mse: 27.0902 - val_mae: 1.6500 - lr: 4.1597e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 25/100\n",
            "1000/1000 - 2s - loss: 11.9240 - mse: 11.9240 - mae: 1.5692 - val_loss: 27.0703 - val_mse: 27.0703 - val_mae: 1.6265 - lr: 4.1597e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 26/100\n",
            "1000/1000 - 2s - loss: 11.9328 - mse: 11.9328 - mae: 1.5680 - val_loss: 27.0584 - val_mse: 27.0584 - val_mae: 1.6093 - lr: 4.1597e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 27/100\n",
            "1000/1000 - 2s - loss: 11.9192 - mse: 11.9192 - mae: 1.5715 - val_loss: 27.0532 - val_mse: 27.0532 - val_mae: 1.6183 - lr: 4.1597e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 28/100\n",
            "1000/1000 - 2s - loss: 11.9154 - mse: 11.9154 - mae: 1.5708 - val_loss: 27.0321 - val_mse: 27.0321 - val_mae: 1.6209 - lr: 4.1597e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 29/100\n",
            "1000/1000 - 2s - loss: 11.8882 - mse: 11.8882 - mae: 1.5711 - val_loss: 27.1338 - val_mse: 27.1338 - val_mae: 1.5800 - lr: 4.1597e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 30/100\n",
            "1000/1000 - 2s - loss: 11.8855 - mse: 11.8855 - mae: 1.5691 - val_loss: 27.1150 - val_mse: 27.1150 - val_mae: 1.5907 - lr: 4.1597e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 31/100\n",
            "1000/1000 - 2s - loss: 11.8889 - mse: 11.8889 - mae: 1.5655 - val_loss: 27.1002 - val_mse: 27.1002 - val_mae: 1.6608 - lr: 4.1597e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 32/100\n",
            "1000/1000 - 2s - loss: 11.8802 - mse: 11.8802 - mae: 1.5680 - val_loss: 27.0251 - val_mse: 27.0251 - val_mae: 1.5824 - lr: 4.1597e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 33/100\n",
            "1000/1000 - 2s - loss: 11.8718 - mse: 11.8718 - mae: 1.5700 - val_loss: 27.0635 - val_mse: 27.0635 - val_mae: 1.6012 - lr: 4.1597e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 34/100\n",
            "1000/1000 - 2s - loss: 11.8734 - mse: 11.8734 - mae: 1.5705 - val_loss: 27.0584 - val_mse: 27.0584 - val_mae: 1.5935 - lr: 4.1597e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 35/100\n",
            "1000/1000 - 2s - loss: 11.8738 - mse: 11.8738 - mae: 1.5671 - val_loss: 26.9345 - val_mse: 26.9345 - val_mae: 1.6125 - lr: 4.1597e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 36/100\n",
            "1000/1000 - 2s - loss: 11.8400 - mse: 11.8400 - mae: 1.5659 - val_loss: 27.0666 - val_mse: 27.0666 - val_mae: 1.6546 - lr: 4.1597e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 37/100\n",
            "1000/1000 - 2s - loss: 11.8600 - mse: 11.8600 - mae: 1.5679 - val_loss: 26.9129 - val_mse: 26.9129 - val_mae: 1.5871 - lr: 4.1597e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 38/100\n",
            "1000/1000 - 2s - loss: 11.8394 - mse: 11.8394 - mae: 1.5674 - val_loss: 26.9386 - val_mse: 26.9386 - val_mae: 1.6190 - lr: 4.1597e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 39/100\n",
            "1000/1000 - 2s - loss: 11.8241 - mse: 11.8241 - mae: 1.5673 - val_loss: 26.9265 - val_mse: 26.9265 - val_mae: 1.6079 - lr: 4.1597e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 40/100\n",
            "1000/1000 - 2s - loss: 11.8248 - mse: 11.8248 - mae: 1.5637 - val_loss: 26.9289 - val_mse: 26.9289 - val_mae: 1.6212 - lr: 4.1597e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 41/100\n",
            "1000/1000 - 2s - loss: 11.8256 - mse: 11.8256 - mae: 1.5671 - val_loss: 26.9426 - val_mse: 26.9426 - val_mae: 1.6002 - lr: 4.1597e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 42/100\n",
            "1000/1000 - 2s - loss: 11.8071 - mse: 11.8071 - mae: 1.5672 - val_loss: 27.0039 - val_mse: 27.0039 - val_mae: 1.5813 - lr: 4.1597e-04 - 2s/epoch - 2ms/step\n",
            "Score for fold 1: loss of 27.00393295288086\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 15.9721 - mse: 15.9721 - mae: 1.5740 - val_loss: 10.2819 - val_mse: 10.2819 - val_mae: 1.5672 - lr: 4.1597e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 15.9857 - mse: 15.9857 - mae: 1.5704 - val_loss: 10.3078 - val_mse: 10.3078 - val_mae: 1.5409 - lr: 4.1597e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 15.9403 - mse: 15.9403 - mae: 1.5719 - val_loss: 10.2825 - val_mse: 10.2825 - val_mae: 1.5608 - lr: 4.1597e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 15.9245 - mse: 15.9245 - mae: 1.5725 - val_loss: 10.2928 - val_mse: 10.2928 - val_mae: 1.5664 - lr: 4.1597e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 15.9299 - mse: 15.9299 - mae: 1.5703 - val_loss: 10.3749 - val_mse: 10.3749 - val_mae: 1.5834 - lr: 4.1597e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 15.9009 - mse: 15.9009 - mae: 1.5711 - val_loss: 10.3930 - val_mse: 10.3930 - val_mae: 1.5962 - lr: 4.1597e-04 - 2s/epoch - 2ms/step\n",
            "Score for fold 2: loss of 10.39305591583252\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 15.4775 - mse: 15.4775 - mae: 1.5780 - val_loss: 12.2975 - val_mse: 12.2975 - val_mae: 1.5104 - lr: 4.1597e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 15.4493 - mse: 15.4493 - mae: 1.5733 - val_loss: 12.0831 - val_mse: 12.0831 - val_mae: 1.5793 - lr: 4.1597e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 15.4495 - mse: 15.4495 - mae: 1.5707 - val_loss: 12.0747 - val_mse: 12.0747 - val_mae: 1.5831 - lr: 4.1597e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 15.4338 - mse: 15.4338 - mae: 1.5758 - val_loss: 12.0897 - val_mse: 12.0897 - val_mae: 1.5637 - lr: 4.1597e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 15.4423 - mse: 15.4423 - mae: 1.5704 - val_loss: 12.1047 - val_mse: 12.1047 - val_mae: 1.5850 - lr: 4.1597e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 15.4153 - mse: 15.4153 - mae: 1.5720 - val_loss: 12.1357 - val_mse: 12.1357 - val_mae: 1.5783 - lr: 4.1597e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 15.3980 - mse: 15.3980 - mae: 1.5733 - val_loss: 12.2052 - val_mse: 12.2052 - val_mae: 1.5657 - lr: 4.1597e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 2s - loss: 15.4152 - mse: 15.4152 - mae: 1.5746 - val_loss: 12.1795 - val_mse: 12.1795 - val_mae: 1.5225 - lr: 4.1597e-04 - 2s/epoch - 2ms/step\n",
            "Score for fold 3: loss of 12.179513931274414\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 14.8287 - mse: 14.8287 - mae: 1.5711 - val_loss: 14.3857 - val_mse: 14.3857 - val_mae: 1.5323 - lr: 4.1597e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 14.8147 - mse: 14.8147 - mae: 1.5702 - val_loss: 14.4097 - val_mse: 14.4097 - val_mae: 1.5552 - lr: 4.1597e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 14.8177 - mse: 14.8177 - mae: 1.5703 - val_loss: 14.4883 - val_mse: 14.4883 - val_mae: 1.5441 - lr: 4.1597e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 14.7996 - mse: 14.7996 - mae: 1.5686 - val_loss: 14.5212 - val_mse: 14.5212 - val_mae: 1.5349 - lr: 4.1597e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 14.8091 - mse: 14.8091 - mae: 1.5683 - val_loss: 14.4346 - val_mse: 14.4346 - val_mae: 1.6069 - lr: 4.1597e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 14.7661 - mse: 14.7661 - mae: 1.5704 - val_loss: 14.4771 - val_mse: 14.4771 - val_mae: 1.5615 - lr: 4.1597e-04 - 2s/epoch - 2ms/step\n",
            "Score for fold 4: loss of 14.477060317993164\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 15.8081 - mse: 15.8081 - mae: 1.5731 - val_loss: 10.3059 - val_mse: 10.3059 - val_mae: 1.5608 - lr: 4.1597e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 15.7984 - mse: 15.7984 - mae: 1.5742 - val_loss: 10.2636 - val_mse: 10.2636 - val_mae: 1.5534 - lr: 4.1597e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 15.8000 - mse: 15.8000 - mae: 1.5716 - val_loss: 10.3442 - val_mse: 10.3442 - val_mae: 1.5721 - lr: 4.1597e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 15.7849 - mse: 15.7849 - mae: 1.5701 - val_loss: 10.2890 - val_mse: 10.2890 - val_mae: 1.5586 - lr: 4.1597e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 15.7777 - mse: 15.7777 - mae: 1.5690 - val_loss: 10.3393 - val_mse: 10.3393 - val_mae: 1.5580 - lr: 4.1597e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 15.7706 - mse: 15.7706 - mae: 1.5699 - val_loss: 10.3431 - val_mse: 10.3431 - val_mae: 1.5672 - lr: 4.1597e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 15.7700 - mse: 15.7700 - mae: 1.5725 - val_loss: 10.4589 - val_mse: 10.4589 - val_mae: 1.5514 - lr: 4.1597e-04 - 2s/epoch - 2ms/step\n",
            "Score for fold 5: loss of 10.458876609802246\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-10-15 06:05:17,959]\u001b[0m Finished trial#18 resulted in value: 14.901999999999997. Current best value is 14.863999999999999 with parameters: {'activation': 'tanh', 'num_hidden_layer': 3, 'i': 6, 'learning_rate': 0.00048617401659243237}.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 5s - loss: 17.3733 - mse: 17.3733 - mae: 1.7027 - val_loss: 10.7146 - val_mse: 10.7146 - val_mae: 1.5889 - lr: 1.2310e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 4s - loss: 16.6193 - mse: 16.6193 - mae: 1.6219 - val_loss: 10.7069 - val_mse: 10.7069 - val_mae: 1.6373 - lr: 1.2310e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 4s - loss: 16.4917 - mse: 16.4917 - mae: 1.6142 - val_loss: 10.6061 - val_mse: 10.6061 - val_mae: 1.5405 - lr: 1.2310e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 4s - loss: 16.4081 - mse: 16.4081 - mae: 1.6072 - val_loss: 10.6351 - val_mse: 10.6351 - val_mae: 1.5391 - lr: 1.2310e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 4s - loss: 16.3846 - mse: 16.3846 - mae: 1.6022 - val_loss: 10.5625 - val_mse: 10.5625 - val_mae: 1.5914 - lr: 1.2310e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 4s - loss: 16.3644 - mse: 16.3644 - mae: 1.6008 - val_loss: 10.4923 - val_mse: 10.4923 - val_mae: 1.5909 - lr: 1.2310e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 4s - loss: 16.3235 - mse: 16.3235 - mae: 1.6013 - val_loss: 10.5083 - val_mse: 10.5083 - val_mae: 1.5468 - lr: 1.2310e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 4s - loss: 16.3272 - mse: 16.3272 - mae: 1.5982 - val_loss: 10.5264 - val_mse: 10.5264 - val_mae: 1.5736 - lr: 1.2310e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 5s - loss: 16.3134 - mse: 16.3134 - mae: 1.5963 - val_loss: 10.5842 - val_mse: 10.5842 - val_mae: 1.5707 - lr: 1.2310e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 4s - loss: 16.3039 - mse: 16.3039 - mae: 1.5989 - val_loss: 10.5023 - val_mse: 10.5023 - val_mae: 1.6034 - lr: 1.2310e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 5s - loss: 16.2827 - mse: 16.2827 - mae: 1.5982 - val_loss: 10.4491 - val_mse: 10.4491 - val_mae: 1.5877 - lr: 1.2310e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 4s - loss: 16.2747 - mse: 16.2747 - mae: 1.5944 - val_loss: 10.4678 - val_mse: 10.4678 - val_mae: 1.5385 - lr: 1.2310e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 13/100\n",
            "1000/1000 - 5s - loss: 16.2687 - mse: 16.2687 - mae: 1.5926 - val_loss: 10.5264 - val_mse: 10.5264 - val_mae: 1.5708 - lr: 1.2310e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 14/100\n",
            "1000/1000 - 4s - loss: 16.2518 - mse: 16.2518 - mae: 1.5960 - val_loss: 10.5573 - val_mse: 10.5573 - val_mae: 1.5087 - lr: 1.2310e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 15/100\n",
            "1000/1000 - 4s - loss: 16.2629 - mse: 16.2629 - mae: 1.5932 - val_loss: 10.5242 - val_mse: 10.5242 - val_mae: 1.5334 - lr: 1.2310e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 16/100\n",
            "1000/1000 - 4s - loss: 16.2709 - mse: 16.2709 - mae: 1.5929 - val_loss: 10.5147 - val_mse: 10.5147 - val_mae: 1.5250 - lr: 1.2310e-04 - 4s/epoch - 4ms/step\n",
            "Score for fold 1: loss of 10.514678955078125\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 5s - loss: 15.3142 - mse: 15.3142 - mae: 1.5796 - val_loss: 14.2958 - val_mse: 14.2958 - val_mae: 1.5784 - lr: 1.2310e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 4s - loss: 15.3123 - mse: 15.3123 - mae: 1.5770 - val_loss: 14.2787 - val_mse: 14.2787 - val_mae: 1.5681 - lr: 1.2310e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 4s - loss: 15.2783 - mse: 15.2783 - mae: 1.5782 - val_loss: 14.3435 - val_mse: 14.3435 - val_mae: 1.5835 - lr: 1.2310e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 5s - loss: 15.2755 - mse: 15.2755 - mae: 1.5777 - val_loss: 14.3013 - val_mse: 14.3013 - val_mae: 1.5826 - lr: 1.2310e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 4s - loss: 15.2700 - mse: 15.2700 - mae: 1.5761 - val_loss: 14.2193 - val_mse: 14.2193 - val_mae: 1.5978 - lr: 1.2310e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 4s - loss: 15.2485 - mse: 15.2485 - mae: 1.5757 - val_loss: 14.5451 - val_mse: 14.5451 - val_mae: 1.6233 - lr: 1.2310e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 4s - loss: 15.2429 - mse: 15.2429 - mae: 1.5737 - val_loss: 14.2007 - val_mse: 14.2007 - val_mae: 1.6292 - lr: 1.2310e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 5s - loss: 15.2650 - mse: 15.2650 - mae: 1.5786 - val_loss: 14.3976 - val_mse: 14.3976 - val_mae: 1.6191 - lr: 1.2310e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 4s - loss: 15.2596 - mse: 15.2596 - mae: 1.5742 - val_loss: 14.2746 - val_mse: 14.2746 - val_mae: 1.6006 - lr: 1.2310e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 7s - loss: 15.2665 - mse: 15.2665 - mae: 1.5738 - val_loss: 14.2252 - val_mse: 14.2252 - val_mae: 1.6511 - lr: 1.2310e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 4s - loss: 15.2281 - mse: 15.2281 - mae: 1.5754 - val_loss: 14.4821 - val_mse: 14.4821 - val_mae: 1.5553 - lr: 1.2310e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 4s - loss: 15.2353 - mse: 15.2353 - mae: 1.5716 - val_loss: 14.2094 - val_mse: 14.2094 - val_mae: 1.6097 - lr: 1.2310e-04 - 4s/epoch - 4ms/step\n",
            "Score for fold 2: loss of 14.209376335144043\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 5s - loss: 15.5352 - mse: 15.5352 - mae: 1.5836 - val_loss: 12.7125 - val_mse: 12.7125 - val_mae: 1.5897 - lr: 1.2310e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 5s - loss: 15.5427 - mse: 15.5427 - mae: 1.5817 - val_loss: 12.7452 - val_mse: 12.7452 - val_mae: 1.5949 - lr: 1.2310e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 4s - loss: 15.5127 - mse: 15.5127 - mae: 1.5830 - val_loss: 12.7364 - val_mse: 12.7364 - val_mae: 1.5497 - lr: 1.2310e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 5s - loss: 15.4855 - mse: 15.4855 - mae: 1.5809 - val_loss: 12.7434 - val_mse: 12.7434 - val_mae: 1.5783 - lr: 1.2310e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 5s - loss: 15.4804 - mse: 15.4804 - mae: 1.5778 - val_loss: 12.8368 - val_mse: 12.8368 - val_mae: 1.5369 - lr: 1.2310e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 5s - loss: 15.4860 - mse: 15.4860 - mae: 1.5793 - val_loss: 12.8227 - val_mse: 12.8227 - val_mae: 1.6105 - lr: 1.2310e-04 - 5s/epoch - 5ms/step\n",
            "Score for fold 3: loss of 12.822735786437988\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 5s - loss: 11.6050 - mse: 11.6050 - mae: 1.5583 - val_loss: 28.1698 - val_mse: 28.1698 - val_mae: 1.6424 - lr: 1.2310e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 5s - loss: 11.5646 - mse: 11.5646 - mae: 1.5576 - val_loss: 28.3251 - val_mse: 28.3251 - val_mae: 1.6471 - lr: 1.2310e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 5s - loss: 11.5598 - mse: 11.5598 - mae: 1.5551 - val_loss: 28.2191 - val_mse: 28.2191 - val_mae: 1.6769 - lr: 1.2310e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 4s - loss: 11.5351 - mse: 11.5351 - mae: 1.5525 - val_loss: 28.2815 - val_mse: 28.2815 - val_mae: 1.6425 - lr: 1.2310e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 5s - loss: 11.5230 - mse: 11.5230 - mae: 1.5540 - val_loss: 28.3668 - val_mse: 28.3668 - val_mae: 1.6583 - lr: 1.2310e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 5s - loss: 11.5028 - mse: 11.5028 - mae: 1.5547 - val_loss: 28.2453 - val_mse: 28.2453 - val_mae: 1.6612 - lr: 1.2310e-04 - 5s/epoch - 5ms/step\n",
            "Score for fold 4: loss of 28.24527359008789\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 5s - loss: 16.3041 - mse: 16.3041 - mae: 1.5890 - val_loss: 9.1846 - val_mse: 9.1846 - val_mae: 1.5433 - lr: 1.2310e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 4s - loss: 16.2816 - mse: 16.2816 - mae: 1.5909 - val_loss: 9.1316 - val_mse: 9.1316 - val_mae: 1.5188 - lr: 1.2310e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 4s - loss: 16.2571 - mse: 16.2571 - mae: 1.5855 - val_loss: 9.2007 - val_mse: 9.2007 - val_mae: 1.5253 - lr: 1.2310e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 4s - loss: 16.2301 - mse: 16.2301 - mae: 1.5864 - val_loss: 9.2219 - val_mse: 9.2219 - val_mae: 1.5131 - lr: 1.2310e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 4s - loss: 16.2079 - mse: 16.2079 - mae: 1.5851 - val_loss: 9.1335 - val_mse: 9.1335 - val_mae: 1.5612 - lr: 1.2310e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 5s - loss: 16.2140 - mse: 16.2140 - mae: 1.5853 - val_loss: 9.0869 - val_mse: 9.0869 - val_mae: 1.5412 - lr: 1.2310e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 5s - loss: 16.1668 - mse: 16.1668 - mae: 1.5863 - val_loss: 9.2850 - val_mse: 9.2850 - val_mae: 1.4963 - lr: 1.2310e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 6s - loss: 16.1658 - mse: 16.1658 - mae: 1.5846 - val_loss: 9.1837 - val_mse: 9.1837 - val_mae: 1.5546 - lr: 1.2310e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 5s - loss: 16.1799 - mse: 16.1799 - mae: 1.5834 - val_loss: 9.3848 - val_mse: 9.3848 - val_mae: 1.4943 - lr: 1.2310e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 4s - loss: 16.1619 - mse: 16.1619 - mae: 1.5882 - val_loss: 9.1790 - val_mse: 9.1790 - val_mae: 1.5162 - lr: 1.2310e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 5s - loss: 16.1601 - mse: 16.1601 - mae: 1.5828 - val_loss: 9.3420 - val_mse: 9.3420 - val_mae: 1.5228 - lr: 1.2310e-04 - 5s/epoch - 5ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-10-15 06:09:44,872]\u001b[0m Finished trial#19 resulted in value: 15.026. Current best value is 14.863999999999999 with parameters: {'activation': 'tanh', 'num_hidden_layer': 3, 'i': 6, 'learning_rate': 0.00048617401659243237}.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Score for fold 5: loss of 9.342013359069824\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 3s - loss: 16.1243 - mse: 16.1243 - mae: 1.6290 - val_loss: 12.7455 - val_mse: 12.7455 - val_mae: 1.6498 - lr: 9.3034e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 15.7095 - mse: 15.7095 - mae: 1.6108 - val_loss: 12.6166 - val_mse: 12.6166 - val_mae: 1.4923 - lr: 9.3034e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 15.6579 - mse: 15.6579 - mae: 1.6072 - val_loss: 12.5609 - val_mse: 12.5609 - val_mae: 1.5770 - lr: 9.3034e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 15.5714 - mse: 15.5714 - mae: 1.6014 - val_loss: 12.5778 - val_mse: 12.5778 - val_mae: 1.5185 - lr: 9.3034e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 15.6049 - mse: 15.6049 - mae: 1.5963 - val_loss: 12.5579 - val_mse: 12.5579 - val_mae: 1.5452 - lr: 9.3034e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 15.5337 - mse: 15.5337 - mae: 1.5993 - val_loss: 12.6233 - val_mse: 12.6233 - val_mae: 1.5283 - lr: 9.3034e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 15.5281 - mse: 15.5281 - mae: 1.5907 - val_loss: 12.6252 - val_mse: 12.6252 - val_mae: 1.5859 - lr: 9.3034e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 2s - loss: 15.4395 - mse: 15.4395 - mae: 1.5942 - val_loss: 12.5773 - val_mse: 12.5773 - val_mae: 1.5606 - lr: 9.3034e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 2s - loss: 15.4998 - mse: 15.4998 - mae: 1.5912 - val_loss: 12.5382 - val_mse: 12.5382 - val_mae: 1.5157 - lr: 9.3034e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 2s - loss: 15.4926 - mse: 15.4926 - mae: 1.5868 - val_loss: 12.4896 - val_mse: 12.4896 - val_mae: 1.5283 - lr: 9.3034e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 2s - loss: 15.4248 - mse: 15.4248 - mae: 1.5875 - val_loss: 12.5032 - val_mse: 12.5032 - val_mae: 1.5328 - lr: 9.3034e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 2s - loss: 15.4502 - mse: 15.4502 - mae: 1.5895 - val_loss: 12.6501 - val_mse: 12.6501 - val_mae: 1.5401 - lr: 9.3034e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 13/100\n",
            "1000/1000 - 2s - loss: 15.4607 - mse: 15.4607 - mae: 1.5817 - val_loss: 12.5452 - val_mse: 12.5452 - val_mae: 1.5343 - lr: 9.3034e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 14/100\n",
            "1000/1000 - 2s - loss: 15.4136 - mse: 15.4136 - mae: 1.5888 - val_loss: 12.4897 - val_mse: 12.4897 - val_mae: 1.5637 - lr: 9.3034e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 15/100\n",
            "1000/1000 - 2s - loss: 15.4255 - mse: 15.4255 - mae: 1.5890 - val_loss: 12.5388 - val_mse: 12.5388 - val_mae: 1.5015 - lr: 9.3034e-04 - 2s/epoch - 2ms/step\n",
            "Score for fold 1: loss of 12.538800239562988\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 13.9292 - mse: 13.9292 - mae: 1.5671 - val_loss: 18.7529 - val_mse: 18.7529 - val_mae: 1.6331 - lr: 9.3034e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 13.8085 - mse: 13.8085 - mae: 1.5678 - val_loss: 19.1856 - val_mse: 19.1856 - val_mae: 1.6269 - lr: 9.3034e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 13.8866 - mse: 13.8866 - mae: 1.5641 - val_loss: 18.9081 - val_mse: 18.9081 - val_mae: 1.5802 - lr: 9.3034e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 13.7875 - mse: 13.7875 - mae: 1.5595 - val_loss: 19.3100 - val_mse: 19.3100 - val_mae: 1.6057 - lr: 9.3034e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 13.8492 - mse: 13.8492 - mae: 1.5590 - val_loss: 19.0729 - val_mse: 19.0729 - val_mae: 1.6291 - lr: 9.3034e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 13.7697 - mse: 13.7697 - mae: 1.5578 - val_loss: 19.0279 - val_mse: 19.0279 - val_mae: 1.5865 - lr: 9.3034e-04 - 2s/epoch - 2ms/step\n",
            "Score for fold 2: loss of 19.027929306030273\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 15.1749 - mse: 15.1749 - mae: 1.5655 - val_loss: 13.3491 - val_mse: 13.3491 - val_mae: 1.5761 - lr: 9.3034e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 15.1542 - mse: 15.1542 - mae: 1.5660 - val_loss: 13.3841 - val_mse: 13.3841 - val_mae: 1.5486 - lr: 9.3034e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 15.0791 - mse: 15.0791 - mae: 1.5602 - val_loss: 13.4904 - val_mse: 13.4904 - val_mae: 1.5793 - lr: 9.3034e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 15.0293 - mse: 15.0293 - mae: 1.5596 - val_loss: 13.5666 - val_mse: 13.5666 - val_mae: 1.5370 - lr: 9.3034e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 15.0296 - mse: 15.0296 - mae: 1.5575 - val_loss: 13.5741 - val_mse: 13.5741 - val_mae: 1.5939 - lr: 9.3034e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 14.9900 - mse: 14.9900 - mae: 1.5584 - val_loss: 13.5952 - val_mse: 13.5952 - val_mae: 1.5534 - lr: 9.3034e-04 - 2s/epoch - 2ms/step\n",
            "Score for fold 3: loss of 13.595193862915039\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 14.3997 - mse: 14.3997 - mae: 1.5619 - val_loss: 16.0163 - val_mse: 16.0163 - val_mae: 1.6027 - lr: 9.3034e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 14.3583 - mse: 14.3583 - mae: 1.5590 - val_loss: 15.7773 - val_mse: 15.7773 - val_mae: 1.5848 - lr: 9.3034e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 14.3478 - mse: 14.3478 - mae: 1.5551 - val_loss: 16.0354 - val_mse: 16.0354 - val_mae: 1.5962 - lr: 9.3034e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 14.2972 - mse: 14.2972 - mae: 1.5563 - val_loss: 15.7920 - val_mse: 15.7920 - val_mae: 1.5505 - lr: 9.3034e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 14.2524 - mse: 14.2524 - mae: 1.5517 - val_loss: 16.0166 - val_mse: 16.0166 - val_mae: 1.5663 - lr: 9.3034e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 14.1889 - mse: 14.1889 - mae: 1.5523 - val_loss: 15.8779 - val_mse: 15.8779 - val_mae: 1.5846 - lr: 9.3034e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 14.2112 - mse: 14.2112 - mae: 1.5512 - val_loss: 16.0503 - val_mse: 16.0503 - val_mae: 1.5367 - lr: 9.3034e-04 - 2s/epoch - 2ms/step\n",
            "Score for fold 4: loss of 16.050315856933594\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 15.0813 - mse: 15.0813 - mae: 1.5531 - val_loss: 12.8181 - val_mse: 12.8181 - val_mae: 1.5432 - lr: 9.3034e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 14.7453 - mse: 14.7453 - mae: 1.5475 - val_loss: 12.8639 - val_mse: 12.8639 - val_mae: 1.5569 - lr: 9.3034e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 14.9084 - mse: 14.9084 - mae: 1.5468 - val_loss: 12.8031 - val_mse: 12.8031 - val_mae: 1.5501 - lr: 9.3034e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 14.9445 - mse: 14.9445 - mae: 1.5455 - val_loss: 12.9449 - val_mse: 12.9449 - val_mae: 1.5747 - lr: 9.3034e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 14.8691 - mse: 14.8691 - mae: 1.5423 - val_loss: 12.9225 - val_mse: 12.9225 - val_mae: 1.6046 - lr: 9.3034e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 14.9653 - mse: 14.9653 - mae: 1.5468 - val_loss: 12.7864 - val_mse: 12.7864 - val_mae: 1.6005 - lr: 9.3034e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 14.8690 - mse: 14.8690 - mae: 1.5450 - val_loss: 12.9331 - val_mse: 12.9331 - val_mae: 1.5868 - lr: 9.3034e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 2s - loss: 14.8365 - mse: 14.8365 - mae: 1.5422 - val_loss: 13.1166 - val_mse: 13.1166 - val_mae: 1.6153 - lr: 9.3034e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 3s - loss: 14.7836 - mse: 14.7836 - mae: 1.5394 - val_loss: 12.9113 - val_mse: 12.9113 - val_mae: 1.5718 - lr: 9.3034e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 3s - loss: 14.7287 - mse: 14.7287 - mae: 1.5403 - val_loss: 12.9536 - val_mse: 12.9536 - val_mae: 1.5998 - lr: 9.3034e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 2s - loss: 14.7347 - mse: 14.7347 - mae: 1.5423 - val_loss: 12.9077 - val_mse: 12.9077 - val_mae: 1.5710 - lr: 9.3034e-04 - 2s/epoch - 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-10-15 06:11:48,950]\u001b[0m Finished trial#20 resulted in value: 14.825999999999999. Current best value is 14.825999999999999 with parameters: {'activation': 'relu', 'num_hidden_layer': 3, 'i': 7, 'learning_rate': 0.0009303434687518455}.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Score for fold 5: loss of 12.90773868560791\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 3s - loss: 14.9075 - mse: 14.9075 - mae: 1.6070 - val_loss: 17.0982 - val_mse: 17.0982 - val_mae: 1.6546 - lr: 0.0010 - 3s/epoch - 3ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 14.6461 - mse: 14.6461 - mae: 1.5923 - val_loss: 17.4015 - val_mse: 17.4015 - val_mae: 1.5684 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 14.5534 - mse: 14.5534 - mae: 1.5846 - val_loss: 17.2610 - val_mse: 17.2610 - val_mae: 1.5896 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 14.5404 - mse: 14.5404 - mae: 1.5797 - val_loss: 17.0118 - val_mse: 17.0118 - val_mae: 1.7136 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 14.5050 - mse: 14.5050 - mae: 1.5798 - val_loss: 17.5567 - val_mse: 17.5567 - val_mae: 1.5451 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 14.4843 - mse: 14.4843 - mae: 1.5739 - val_loss: 17.0103 - val_mse: 17.0103 - val_mae: 1.6328 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 14.3976 - mse: 14.3976 - mae: 1.5725 - val_loss: 16.9597 - val_mse: 16.9597 - val_mae: 1.6653 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 2s - loss: 14.3853 - mse: 14.3853 - mae: 1.5716 - val_loss: 17.1708 - val_mse: 17.1708 - val_mae: 1.5791 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 2s - loss: 14.3754 - mse: 14.3754 - mae: 1.5734 - val_loss: 17.1056 - val_mse: 17.1056 - val_mae: 1.6385 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 2s - loss: 14.3387 - mse: 14.3387 - mae: 1.5692 - val_loss: 17.1929 - val_mse: 17.1929 - val_mae: 1.6125 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 2s - loss: 14.3352 - mse: 14.3352 - mae: 1.5665 - val_loss: 17.1706 - val_mse: 17.1706 - val_mae: 1.5920 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 2s - loss: 14.3086 - mse: 14.3086 - mae: 1.5673 - val_loss: 16.9984 - val_mse: 16.9984 - val_mae: 1.6055 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Score for fold 1: loss of 16.998384475708008\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 15.5636 - mse: 15.5636 - mae: 1.5847 - val_loss: 12.0831 - val_mse: 12.0831 - val_mae: 1.5908 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 15.4882 - mse: 15.4882 - mae: 1.5823 - val_loss: 12.2944 - val_mse: 12.2944 - val_mae: 1.5171 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 15.4605 - mse: 15.4605 - mae: 1.5845 - val_loss: 12.2501 - val_mse: 12.2501 - val_mae: 1.6049 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 15.4496 - mse: 15.4496 - mae: 1.5829 - val_loss: 12.2788 - val_mse: 12.2788 - val_mae: 1.5262 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 15.4544 - mse: 15.4544 - mae: 1.5793 - val_loss: 12.2139 - val_mse: 12.2139 - val_mae: 1.5205 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 15.4377 - mse: 15.4377 - mae: 1.5791 - val_loss: 12.3021 - val_mse: 12.3021 - val_mae: 1.5642 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Score for fold 2: loss of 12.302055358886719\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 15.7637 - mse: 15.7637 - mae: 1.5648 - val_loss: 11.2147 - val_mse: 11.2147 - val_mae: 1.6207 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 15.6511 - mse: 15.6511 - mae: 1.5646 - val_loss: 11.3314 - val_mse: 11.3314 - val_mae: 1.5865 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 15.6054 - mse: 15.6054 - mae: 1.5612 - val_loss: 11.2530 - val_mse: 11.2530 - val_mae: 1.6359 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 15.5362 - mse: 15.5362 - mae: 1.5612 - val_loss: 11.3541 - val_mse: 11.3541 - val_mae: 1.5909 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 15.4812 - mse: 15.4812 - mae: 1.5562 - val_loss: 11.6996 - val_mse: 11.6996 - val_mae: 1.5529 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 15.5064 - mse: 15.5064 - mae: 1.5561 - val_loss: 11.6598 - val_mse: 11.6598 - val_mae: 1.6259 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Score for fold 3: loss of 11.659794807434082\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 13.5149 - mse: 13.5149 - mae: 1.5703 - val_loss: 19.1346 - val_mse: 19.1346 - val_mae: 1.5921 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 13.4506 - mse: 13.4506 - mae: 1.5660 - val_loss: 19.6121 - val_mse: 19.6121 - val_mae: 1.5312 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 13.3589 - mse: 13.3589 - mae: 1.5616 - val_loss: 19.5783 - val_mse: 19.5783 - val_mae: 1.6431 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 13.3575 - mse: 13.3575 - mae: 1.5655 - val_loss: 19.3654 - val_mse: 19.3654 - val_mae: 1.5625 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 13.3461 - mse: 13.3461 - mae: 1.5630 - val_loss: 19.5362 - val_mse: 19.5362 - val_mae: 1.5749 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 13.3091 - mse: 13.3091 - mae: 1.5616 - val_loss: 19.4557 - val_mse: 19.4557 - val_mae: 1.6130 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Score for fold 4: loss of 19.455671310424805\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 14.8284 - mse: 14.8284 - mae: 1.5599 - val_loss: 13.6308 - val_mse: 13.6308 - val_mae: 1.6190 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 14.8226 - mse: 14.8226 - mae: 1.5588 - val_loss: 13.6829 - val_mse: 13.6829 - val_mae: 1.6496 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 14.7706 - mse: 14.7706 - mae: 1.5569 - val_loss: 13.8773 - val_mse: 13.8773 - val_mae: 1.5563 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 14.7179 - mse: 14.7179 - mae: 1.5555 - val_loss: 13.7807 - val_mse: 13.7807 - val_mae: 1.5655 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 14.6683 - mse: 14.6683 - mae: 1.5554 - val_loss: 13.9382 - val_mse: 13.9382 - val_mae: 1.5401 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 14.6663 - mse: 14.6663 - mae: 1.5480 - val_loss: 14.0062 - val_mse: 14.0062 - val_mae: 1.5912 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Score for fold 5: loss of 14.006221771240234\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-10-15 06:13:36,484]\u001b[0m Finished trial#21 resulted in value: 14.886000000000001. Current best value is 14.825999999999999 with parameters: {'activation': 'relu', 'num_hidden_layer': 3, 'i': 7, 'learning_rate': 0.0009303434687518455}.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 3s - loss: 16.3436 - mse: 16.3436 - mae: 1.6161 - val_loss: 12.3676 - val_mse: 12.3676 - val_mae: 1.5016 - lr: 8.8757e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 15.9651 - mse: 15.9651 - mae: 1.5968 - val_loss: 12.1038 - val_mse: 12.1038 - val_mae: 1.6999 - lr: 8.8757e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 15.8865 - mse: 15.8865 - mae: 1.5917 - val_loss: 12.2182 - val_mse: 12.2182 - val_mae: 1.5387 - lr: 8.8757e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 15.8065 - mse: 15.8065 - mae: 1.5858 - val_loss: 11.8884 - val_mse: 11.8884 - val_mae: 1.6062 - lr: 8.8757e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 15.7901 - mse: 15.7901 - mae: 1.5839 - val_loss: 11.9500 - val_mse: 11.9500 - val_mae: 1.5762 - lr: 8.8757e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 15.7506 - mse: 15.7506 - mae: 1.5811 - val_loss: 12.1650 - val_mse: 12.1650 - val_mae: 1.5391 - lr: 8.8757e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 15.7334 - mse: 15.7334 - mae: 1.5833 - val_loss: 12.0190 - val_mse: 12.0190 - val_mae: 1.6120 - lr: 8.8757e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 2s - loss: 15.6716 - mse: 15.6716 - mae: 1.5767 - val_loss: 11.9812 - val_mse: 11.9812 - val_mae: 1.5233 - lr: 8.8757e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 2s - loss: 15.6699 - mse: 15.6699 - mae: 1.5768 - val_loss: 11.9556 - val_mse: 11.9556 - val_mae: 1.5427 - lr: 8.8757e-04 - 2s/epoch - 2ms/step\n",
            "Score for fold 1: loss of 11.955592155456543\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 4s - loss: 15.6113 - mse: 15.6113 - mae: 1.5742 - val_loss: 11.9638 - val_mse: 11.9638 - val_mae: 1.5447 - lr: 8.8757e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 15.5902 - mse: 15.5902 - mae: 1.5713 - val_loss: 12.0543 - val_mse: 12.0543 - val_mae: 1.6068 - lr: 8.8757e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 15.5252 - mse: 15.5252 - mae: 1.5733 - val_loss: 12.0248 - val_mse: 12.0248 - val_mae: 1.6003 - lr: 8.8757e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 15.6116 - mse: 15.6116 - mae: 1.5738 - val_loss: 12.1040 - val_mse: 12.1040 - val_mae: 1.5286 - lr: 8.8757e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 15.5746 - mse: 15.5746 - mae: 1.5647 - val_loss: 12.0153 - val_mse: 12.0153 - val_mae: 1.5830 - lr: 8.8757e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 15.4849 - mse: 15.4849 - mae: 1.5710 - val_loss: 11.9356 - val_mse: 11.9356 - val_mae: 1.6405 - lr: 8.8757e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 15.5183 - mse: 15.5183 - mae: 1.5705 - val_loss: 12.0213 - val_mse: 12.0213 - val_mae: 1.5649 - lr: 8.8757e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 2s - loss: 15.5629 - mse: 15.5629 - mae: 1.5661 - val_loss: 12.1526 - val_mse: 12.1526 - val_mae: 1.5450 - lr: 8.8757e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 2s - loss: 15.4332 - mse: 15.4332 - mae: 1.5663 - val_loss: 12.0836 - val_mse: 12.0836 - val_mae: 1.5289 - lr: 8.8757e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 2s - loss: 15.4559 - mse: 15.4559 - mae: 1.5695 - val_loss: 12.0350 - val_mse: 12.0350 - val_mae: 1.5684 - lr: 8.8757e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 2s - loss: 15.4949 - mse: 15.4949 - mae: 1.5649 - val_loss: 11.9490 - val_mse: 11.9490 - val_mae: 1.5984 - lr: 8.8757e-04 - 2s/epoch - 2ms/step\n",
            "Score for fold 2: loss of 11.948973655700684\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 12.5247 - mse: 12.5247 - mae: 1.5651 - val_loss: 24.0416 - val_mse: 24.0416 - val_mae: 1.6152 - lr: 8.8757e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 12.4255 - mse: 12.4255 - mae: 1.5653 - val_loss: 24.0744 - val_mse: 24.0744 - val_mae: 1.5727 - lr: 8.8757e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 12.4245 - mse: 12.4245 - mae: 1.5607 - val_loss: 24.1152 - val_mse: 24.1152 - val_mae: 1.5681 - lr: 8.8757e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 12.4423 - mse: 12.4423 - mae: 1.5595 - val_loss: 24.0888 - val_mse: 24.0888 - val_mae: 1.5654 - lr: 8.8757e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 12.3599 - mse: 12.3599 - mae: 1.5585 - val_loss: 24.0373 - val_mse: 24.0373 - val_mae: 1.5931 - lr: 8.8757e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 12.2660 - mse: 12.2660 - mae: 1.5553 - val_loss: 24.0440 - val_mse: 24.0440 - val_mae: 1.6107 - lr: 8.8757e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 12.3172 - mse: 12.3172 - mae: 1.5540 - val_loss: 24.1391 - val_mse: 24.1391 - val_mae: 1.5966 - lr: 8.8757e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 2s - loss: 12.3047 - mse: 12.3047 - mae: 1.5559 - val_loss: 24.1663 - val_mse: 24.1663 - val_mae: 1.5566 - lr: 8.8757e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 2s - loss: 12.2935 - mse: 12.2935 - mae: 1.5550 - val_loss: 24.2135 - val_mse: 24.2135 - val_mae: 1.5750 - lr: 8.8757e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 2s - loss: 12.2729 - mse: 12.2729 - mae: 1.5526 - val_loss: 24.1151 - val_mse: 24.1151 - val_mae: 1.5999 - lr: 8.8757e-04 - 2s/epoch - 2ms/step\n",
            "Score for fold 3: loss of 24.11507797241211\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 14.7057 - mse: 14.7057 - mae: 1.5565 - val_loss: 14.6669 - val_mse: 14.6669 - val_mae: 1.6172 - lr: 8.8757e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 14.6525 - mse: 14.6525 - mae: 1.5592 - val_loss: 14.6617 - val_mse: 14.6617 - val_mae: 1.5645 - lr: 8.8757e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 14.6433 - mse: 14.6433 - mae: 1.5514 - val_loss: 15.0801 - val_mse: 15.0801 - val_mae: 1.5485 - lr: 8.8757e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 14.6843 - mse: 14.6843 - mae: 1.5542 - val_loss: 14.4190 - val_mse: 14.4190 - val_mae: 1.5632 - lr: 8.8757e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 14.6502 - mse: 14.6502 - mae: 1.5501 - val_loss: 14.3291 - val_mse: 14.3291 - val_mae: 1.5796 - lr: 8.8757e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 14.5678 - mse: 14.5678 - mae: 1.5527 - val_loss: 14.6779 - val_mse: 14.6779 - val_mae: 1.6321 - lr: 8.8757e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 14.5664 - mse: 14.5664 - mae: 1.5515 - val_loss: 14.5228 - val_mse: 14.5228 - val_mae: 1.5809 - lr: 8.8757e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 2s - loss: 14.5326 - mse: 14.5326 - mae: 1.5479 - val_loss: 14.6156 - val_mse: 14.6156 - val_mae: 1.6026 - lr: 8.8757e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 2s - loss: 14.5058 - mse: 14.5058 - mae: 1.5467 - val_loss: 14.8189 - val_mse: 14.8189 - val_mae: 1.5921 - lr: 8.8757e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 2s - loss: 14.5124 - mse: 14.5124 - mae: 1.5515 - val_loss: 14.5472 - val_mse: 14.5472 - val_mae: 1.5802 - lr: 8.8757e-04 - 2s/epoch - 2ms/step\n",
            "Score for fold 4: loss of 14.547160148620605\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 15.3368 - mse: 15.3368 - mae: 1.5630 - val_loss: 11.5146 - val_mse: 11.5146 - val_mae: 1.4657 - lr: 8.8757e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 15.2056 - mse: 15.2056 - mae: 1.5609 - val_loss: 11.7669 - val_mse: 11.7669 - val_mae: 1.5253 - lr: 8.8757e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 15.2950 - mse: 15.2950 - mae: 1.5611 - val_loss: 11.3993 - val_mse: 11.3993 - val_mae: 1.6602 - lr: 8.8757e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 3s - loss: 15.2207 - mse: 15.2207 - mae: 1.5638 - val_loss: 11.5825 - val_mse: 11.5825 - val_mae: 1.5778 - lr: 8.8757e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 3s - loss: 15.2346 - mse: 15.2346 - mae: 1.5576 - val_loss: 11.6294 - val_mse: 11.6294 - val_mae: 1.5410 - lr: 8.8757e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 15.1813 - mse: 15.1813 - mae: 1.5548 - val_loss: 11.5201 - val_mse: 11.5201 - val_mae: 1.5579 - lr: 8.8757e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 15.1336 - mse: 15.1336 - mae: 1.5540 - val_loss: 11.7552 - val_mse: 11.7552 - val_mae: 1.5191 - lr: 8.8757e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 2s - loss: 15.1916 - mse: 15.1916 - mae: 1.5549 - val_loss: 11.7706 - val_mse: 11.7706 - val_mae: 1.5690 - lr: 8.8757e-04 - 2s/epoch - 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-10-15 06:16:25,413]\u001b[0m Finished trial#22 resulted in value: 14.87. Current best value is 14.825999999999999 with parameters: {'activation': 'relu', 'num_hidden_layer': 3, 'i': 7, 'learning_rate': 0.0009303434687518455}.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Score for fold 5: loss of 11.770613670349121\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 3s - loss: 15.9564 - mse: 15.9564 - mae: 1.6130 - val_loss: 13.2542 - val_mse: 13.2542 - val_mae: 1.7062 - lr: 0.0023 - 3s/epoch - 3ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 15.6570 - mse: 15.6570 - mae: 1.5909 - val_loss: 13.1401 - val_mse: 13.1401 - val_mae: 1.6394 - lr: 0.0023 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 15.5459 - mse: 15.5459 - mae: 1.5895 - val_loss: 13.7087 - val_mse: 13.7087 - val_mae: 1.5868 - lr: 0.0023 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 15.5981 - mse: 15.5981 - mae: 1.5872 - val_loss: 13.2259 - val_mse: 13.2259 - val_mae: 1.6484 - lr: 0.0023 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 15.4918 - mse: 15.4918 - mae: 1.5831 - val_loss: 13.1974 - val_mse: 13.1974 - val_mae: 1.7195 - lr: 0.0023 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 15.4414 - mse: 15.4414 - mae: 1.5765 - val_loss: 13.3995 - val_mse: 13.3995 - val_mae: 1.6361 - lr: 0.0023 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 15.4667 - mse: 15.4667 - mae: 1.5796 - val_loss: 13.4069 - val_mse: 13.4069 - val_mae: 1.6183 - lr: 0.0023 - 2s/epoch - 2ms/step\n",
            "Score for fold 1: loss of 13.406877517700195\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 15.9329 - mse: 15.9329 - mae: 1.5898 - val_loss: 10.5694 - val_mse: 10.5694 - val_mae: 1.5452 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 15.9563 - mse: 15.9563 - mae: 1.5861 - val_loss: 10.6290 - val_mse: 10.6290 - val_mae: 1.5900 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 15.9556 - mse: 15.9556 - mae: 1.5854 - val_loss: 10.5936 - val_mse: 10.5936 - val_mae: 1.5621 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 15.9352 - mse: 15.9352 - mae: 1.5836 - val_loss: 10.5077 - val_mse: 10.5077 - val_mae: 1.5873 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 15.9075 - mse: 15.9075 - mae: 1.5829 - val_loss: 10.5154 - val_mse: 10.5154 - val_mae: 1.5847 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 15.9264 - mse: 15.9264 - mae: 1.5814 - val_loss: 10.6330 - val_mse: 10.6330 - val_mae: 1.5616 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 15.9157 - mse: 15.9157 - mae: 1.5793 - val_loss: 10.6319 - val_mse: 10.6319 - val_mae: 1.5600 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 2s - loss: 15.8863 - mse: 15.8863 - mae: 1.5803 - val_loss: 10.6147 - val_mse: 10.6147 - val_mae: 1.5755 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 2s - loss: 15.8954 - mse: 15.8954 - mae: 1.5801 - val_loss: 10.6148 - val_mse: 10.6148 - val_mae: 1.6027 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Score for fold 2: loss of 10.614773750305176\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 14.7690 - mse: 14.7690 - mae: 1.5687 - val_loss: 15.1300 - val_mse: 15.1300 - val_mae: 1.5395 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 14.8073 - mse: 14.8073 - mae: 1.5654 - val_loss: 14.9604 - val_mse: 14.9604 - val_mae: 1.6204 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 14.7864 - mse: 14.7864 - mae: 1.5668 - val_loss: 14.9517 - val_mse: 14.9517 - val_mae: 1.6116 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 14.7170 - mse: 14.7170 - mae: 1.5641 - val_loss: 15.1896 - val_mse: 15.1896 - val_mae: 1.5257 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 14.7426 - mse: 14.7426 - mae: 1.5606 - val_loss: 14.9371 - val_mse: 14.9371 - val_mae: 1.5607 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 14.7533 - mse: 14.7533 - mae: 1.5626 - val_loss: 14.9658 - val_mse: 14.9658 - val_mae: 1.5779 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 14.7545 - mse: 14.7545 - mae: 1.5601 - val_loss: 15.0375 - val_mse: 15.0375 - val_mae: 1.5989 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 2s - loss: 14.7117 - mse: 14.7117 - mae: 1.5608 - val_loss: 14.9443 - val_mse: 14.9443 - val_mae: 1.5527 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 2s - loss: 14.6718 - mse: 14.6718 - mae: 1.5568 - val_loss: 15.1227 - val_mse: 15.1227 - val_mae: 1.5903 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 2s - loss: 14.6809 - mse: 14.6809 - mae: 1.5595 - val_loss: 15.0537 - val_mse: 15.0537 - val_mae: 1.5615 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Score for fold 3: loss of 15.053733825683594\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 14.7626 - mse: 14.7626 - mae: 1.5729 - val_loss: 14.3731 - val_mse: 14.3731 - val_mae: 1.5228 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 14.8322 - mse: 14.8322 - mae: 1.5759 - val_loss: 14.3339 - val_mse: 14.3339 - val_mae: 1.5662 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 14.7533 - mse: 14.7533 - mae: 1.5763 - val_loss: 14.3830 - val_mse: 14.3830 - val_mae: 1.5032 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 14.6766 - mse: 14.6766 - mae: 1.5745 - val_loss: 14.5625 - val_mse: 14.5625 - val_mae: 1.5527 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 14.7280 - mse: 14.7280 - mae: 1.5803 - val_loss: 14.4370 - val_mse: 14.4370 - val_mae: 1.6041 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 14.7201 - mse: 14.7201 - mae: 1.5735 - val_loss: 14.5119 - val_mse: 14.5119 - val_mae: 1.5405 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 14.6893 - mse: 14.6893 - mae: 1.5730 - val_loss: 14.2183 - val_mse: 14.2183 - val_mae: 1.5651 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 2s - loss: 14.6311 - mse: 14.6311 - mae: 1.5737 - val_loss: 14.2201 - val_mse: 14.2201 - val_mae: 1.5110 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 2s - loss: 14.6912 - mse: 14.6912 - mae: 1.5711 - val_loss: 14.4607 - val_mse: 14.4607 - val_mae: 1.5872 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 2s - loss: 14.6481 - mse: 14.6481 - mae: 1.5727 - val_loss: 14.4461 - val_mse: 14.4461 - val_mae: 1.5554 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 2s - loss: 14.6062 - mse: 14.6062 - mae: 1.5697 - val_loss: 14.5484 - val_mse: 14.5484 - val_mae: 1.5592 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 2s - loss: 14.7068 - mse: 14.7068 - mae: 1.5755 - val_loss: 14.3315 - val_mse: 14.3315 - val_mae: 1.5827 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Score for fold 4: loss of 14.331480979919434\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 12.9552 - mse: 12.9552 - mae: 1.5657 - val_loss: 21.3711 - val_mse: 21.3711 - val_mae: 1.5884 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 12.9819 - mse: 12.9819 - mae: 1.5615 - val_loss: 21.2376 - val_mse: 21.2376 - val_mae: 1.5380 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 13.0389 - mse: 13.0389 - mae: 1.5640 - val_loss: 21.3802 - val_mse: 21.3802 - val_mae: 1.5572 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 12.9714 - mse: 12.9714 - mae: 1.5591 - val_loss: 21.0882 - val_mse: 21.0882 - val_mae: 1.6237 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 13.0063 - mse: 13.0063 - mae: 1.5614 - val_loss: 21.1595 - val_mse: 21.1595 - val_mae: 1.6318 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 12.9667 - mse: 12.9667 - mae: 1.5632 - val_loss: 21.4929 - val_mse: 21.4929 - val_mae: 1.5448 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 12.9415 - mse: 12.9415 - mae: 1.5587 - val_loss: 21.2619 - val_mse: 21.2619 - val_mae: 1.5667 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 2s - loss: 12.9073 - mse: 12.9073 - mae: 1.5588 - val_loss: 21.2976 - val_mse: 21.2976 - val_mae: 1.6183 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 2s - loss: 12.8324 - mse: 12.8324 - mae: 1.5573 - val_loss: 21.8923 - val_mse: 21.8923 - val_mae: 1.5837 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Score for fold 5: loss of 21.892295837402344\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-10-15 06:18:08,702]\u001b[0m Finished trial#23 resulted in value: 15.057999999999998. Current best value is 14.825999999999999 with parameters: {'activation': 'relu', 'num_hidden_layer': 3, 'i': 7, 'learning_rate': 0.0009303434687518455}.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 4s - loss: 16.5089 - mse: 16.5089 - mae: 1.6159 - val_loss: 10.9416 - val_mse: 10.9416 - val_mae: 1.6454 - lr: 8.8550e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 3s - loss: 16.1878 - mse: 16.1878 - mae: 1.5956 - val_loss: 10.8472 - val_mse: 10.8472 - val_mae: 1.6490 - lr: 8.8550e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 3s - loss: 16.1055 - mse: 16.1055 - mae: 1.5941 - val_loss: 10.9855 - val_mse: 10.9855 - val_mae: 1.5370 - lr: 8.8550e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 3s - loss: 16.0761 - mse: 16.0761 - mae: 1.5828 - val_loss: 10.7946 - val_mse: 10.7946 - val_mae: 1.6077 - lr: 8.8550e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 3s - loss: 16.0298 - mse: 16.0298 - mae: 1.5829 - val_loss: 10.7668 - val_mse: 10.7668 - val_mae: 1.6102 - lr: 8.8550e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 4s - loss: 15.9662 - mse: 15.9662 - mae: 1.5836 - val_loss: 10.7590 - val_mse: 10.7590 - val_mae: 1.5378 - lr: 8.8550e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 5s - loss: 15.9684 - mse: 15.9684 - mae: 1.5747 - val_loss: 10.6871 - val_mse: 10.6871 - val_mae: 1.5896 - lr: 8.8550e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 3s - loss: 15.9932 - mse: 15.9932 - mae: 1.5750 - val_loss: 10.7554 - val_mse: 10.7554 - val_mae: 1.5613 - lr: 8.8550e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 3s - loss: 15.9329 - mse: 15.9329 - mae: 1.5722 - val_loss: 10.7076 - val_mse: 10.7076 - val_mae: 1.6073 - lr: 8.8550e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 3s - loss: 15.8977 - mse: 15.8977 - mae: 1.5722 - val_loss: 10.6534 - val_mse: 10.6534 - val_mae: 1.5720 - lr: 8.8550e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 3s - loss: 15.8520 - mse: 15.8520 - mae: 1.5693 - val_loss: 10.5841 - val_mse: 10.5841 - val_mae: 1.5840 - lr: 8.8550e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 3s - loss: 15.8854 - mse: 15.8854 - mae: 1.5683 - val_loss: 10.6365 - val_mse: 10.6365 - val_mae: 1.5957 - lr: 8.8550e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 13/100\n",
            "1000/1000 - 3s - loss: 15.8478 - mse: 15.8478 - mae: 1.5679 - val_loss: 10.6398 - val_mse: 10.6398 - val_mae: 1.6091 - lr: 8.8550e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 14/100\n",
            "1000/1000 - 3s - loss: 15.8767 - mse: 15.8767 - mae: 1.5698 - val_loss: 10.8951 - val_mse: 10.8951 - val_mae: 1.5127 - lr: 8.8550e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 15/100\n",
            "1000/1000 - 3s - loss: 15.8368 - mse: 15.8368 - mae: 1.5672 - val_loss: 10.7400 - val_mse: 10.7400 - val_mae: 1.5461 - lr: 8.8550e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 16/100\n",
            "1000/1000 - 3s - loss: 15.9078 - mse: 15.9078 - mae: 1.5673 - val_loss: 10.6239 - val_mse: 10.6239 - val_mae: 1.5994 - lr: 8.8550e-04 - 3s/epoch - 3ms/step\n",
            "Score for fold 1: loss of 10.623919486999512\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 3s - loss: 15.3331 - mse: 15.3331 - mae: 1.5784 - val_loss: 13.3705 - val_mse: 13.3705 - val_mae: 1.5183 - lr: 8.8550e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 3s - loss: 15.2848 - mse: 15.2848 - mae: 1.5773 - val_loss: 12.8687 - val_mse: 12.8687 - val_mae: 1.5268 - lr: 8.8550e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 3s - loss: 15.1866 - mse: 15.1866 - mae: 1.5774 - val_loss: 12.8508 - val_mse: 12.8508 - val_mae: 1.5509 - lr: 8.8550e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 3s - loss: 15.2888 - mse: 15.2888 - mae: 1.5767 - val_loss: 12.8025 - val_mse: 12.8025 - val_mae: 1.5659 - lr: 8.8550e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 3s - loss: 15.2040 - mse: 15.2040 - mae: 1.5734 - val_loss: 13.4526 - val_mse: 13.4526 - val_mae: 1.5225 - lr: 8.8550e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 3s - loss: 15.1059 - mse: 15.1059 - mae: 1.5704 - val_loss: 13.5076 - val_mse: 13.5076 - val_mae: 1.5312 - lr: 8.8550e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 3s - loss: 15.0258 - mse: 15.0258 - mae: 1.5672 - val_loss: 13.2278 - val_mse: 13.2278 - val_mae: 1.5519 - lr: 8.8550e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 3s - loss: 15.0366 - mse: 15.0366 - mae: 1.5672 - val_loss: 13.0077 - val_mse: 13.0077 - val_mae: 1.5857 - lr: 8.8550e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 3s - loss: 15.0640 - mse: 15.0640 - mae: 1.5657 - val_loss: 13.2272 - val_mse: 13.2272 - val_mae: 1.5321 - lr: 8.8550e-04 - 3s/epoch - 3ms/step\n",
            "Score for fold 2: loss of 13.227210998535156\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 3s - loss: 14.6163 - mse: 14.6163 - mae: 1.5538 - val_loss: 14.8808 - val_mse: 14.8808 - val_mae: 1.5712 - lr: 8.8550e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 3s - loss: 14.5391 - mse: 14.5391 - mae: 1.5504 - val_loss: 14.9545 - val_mse: 14.9545 - val_mae: 1.6340 - lr: 8.8550e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 3s - loss: 14.5043 - mse: 14.5043 - mae: 1.5506 - val_loss: 15.1147 - val_mse: 15.1147 - val_mae: 1.5571 - lr: 8.8550e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 3s - loss: 14.4360 - mse: 14.4360 - mae: 1.5514 - val_loss: 14.7851 - val_mse: 14.7851 - val_mae: 1.6038 - lr: 8.8550e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 3s - loss: 14.2775 - mse: 14.2775 - mae: 1.5463 - val_loss: 15.0771 - val_mse: 15.0771 - val_mae: 1.6359 - lr: 8.8550e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 3s - loss: 14.3903 - mse: 14.3903 - mae: 1.5453 - val_loss: 15.1342 - val_mse: 15.1342 - val_mae: 1.5687 - lr: 8.8550e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 3s - loss: 14.4691 - mse: 14.4691 - mae: 1.5452 - val_loss: 15.3850 - val_mse: 15.3850 - val_mae: 1.5789 - lr: 8.8550e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 3s - loss: 14.3306 - mse: 14.3306 - mae: 1.5460 - val_loss: 15.4351 - val_mse: 15.4351 - val_mae: 1.5749 - lr: 8.8550e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 3s - loss: 14.3536 - mse: 14.3536 - mae: 1.5414 - val_loss: 14.9144 - val_mse: 14.9144 - val_mae: 1.6817 - lr: 8.8550e-04 - 3s/epoch - 3ms/step\n",
            "Score for fold 3: loss of 14.914362907409668\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 3s - loss: 14.5095 - mse: 14.5095 - mae: 1.5472 - val_loss: 14.1985 - val_mse: 14.1985 - val_mae: 1.5412 - lr: 8.8550e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 3s - loss: 14.6043 - mse: 14.6043 - mae: 1.5461 - val_loss: 14.2839 - val_mse: 14.2839 - val_mae: 1.5408 - lr: 8.8550e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 3s - loss: 14.4289 - mse: 14.4289 - mae: 1.5394 - val_loss: 13.9933 - val_mse: 13.9933 - val_mae: 1.5868 - lr: 8.8550e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 4s - loss: 14.3579 - mse: 14.3579 - mae: 1.5392 - val_loss: 14.5029 - val_mse: 14.5029 - val_mae: 1.5827 - lr: 8.8550e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 4s - loss: 14.3689 - mse: 14.3689 - mae: 1.5381 - val_loss: 14.2503 - val_mse: 14.2503 - val_mae: 1.5691 - lr: 8.8550e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 3s - loss: 14.2698 - mse: 14.2698 - mae: 1.5369 - val_loss: 14.6815 - val_mse: 14.6815 - val_mae: 1.5634 - lr: 8.8550e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 3s - loss: 14.3343 - mse: 14.3343 - mae: 1.5352 - val_loss: 14.3108 - val_mse: 14.3108 - val_mae: 1.5802 - lr: 8.8550e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 3s - loss: 14.1789 - mse: 14.1789 - mae: 1.5333 - val_loss: 14.2001 - val_mse: 14.2001 - val_mae: 1.6283 - lr: 8.8550e-04 - 3s/epoch - 3ms/step\n",
            "Score for fold 4: loss of 14.200080871582031\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 3s - loss: 12.6922 - mse: 12.6922 - mae: 1.5493 - val_loss: 19.9684 - val_mse: 19.9684 - val_mae: 1.5691 - lr: 8.8550e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 3s - loss: 12.6949 - mse: 12.6949 - mae: 1.5452 - val_loss: 20.4304 - val_mse: 20.4304 - val_mae: 1.5056 - lr: 8.8550e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 3s - loss: 12.5945 - mse: 12.5945 - mae: 1.5449 - val_loss: 20.2781 - val_mse: 20.2781 - val_mae: 1.5738 - lr: 8.8550e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 3s - loss: 12.4490 - mse: 12.4490 - mae: 1.5368 - val_loss: 20.3395 - val_mse: 20.3395 - val_mae: 1.5413 - lr: 8.8550e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 3s - loss: 12.4930 - mse: 12.4930 - mae: 1.5348 - val_loss: 20.4252 - val_mse: 20.4252 - val_mae: 1.5495 - lr: 8.8550e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 3s - loss: 12.3310 - mse: 12.3310 - mae: 1.5319 - val_loss: 20.5155 - val_mse: 20.5155 - val_mae: 1.5154 - lr: 8.8550e-04 - 3s/epoch - 3ms/step\n",
            "Score for fold 5: loss of 20.51546287536621\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-10-15 06:21:38,553]\u001b[0m Finished trial#24 resulted in value: 14.696000000000002. Current best value is 14.696000000000002 with parameters: {'activation': 'relu', 'num_hidden_layer': 3, 'i': 8, 'learning_rate': 0.0008854984178603543}.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 4s - loss: 13.8893 - mse: 13.8893 - mae: 1.6062 - val_loss: 21.2429 - val_mse: 21.2429 - val_mae: 1.6802 - lr: 0.0012 - 4s/epoch - 4ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 3s - loss: 13.6868 - mse: 13.6868 - mae: 1.5872 - val_loss: 21.0010 - val_mse: 21.0010 - val_mae: 1.6476 - lr: 0.0012 - 3s/epoch - 3ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 3s - loss: 13.5610 - mse: 13.5610 - mae: 1.5841 - val_loss: 21.0499 - val_mse: 21.0499 - val_mae: 1.5925 - lr: 0.0012 - 3s/epoch - 3ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 3s - loss: 13.5011 - mse: 13.5011 - mae: 1.5791 - val_loss: 21.1849 - val_mse: 21.1849 - val_mae: 1.5815 - lr: 0.0012 - 3s/epoch - 3ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 3s - loss: 13.4167 - mse: 13.4167 - mae: 1.5734 - val_loss: 21.0526 - val_mse: 21.0526 - val_mae: 1.7055 - lr: 0.0012 - 3s/epoch - 3ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 3s - loss: 13.4622 - mse: 13.4622 - mae: 1.5727 - val_loss: 20.9472 - val_mse: 20.9472 - val_mae: 1.6035 - lr: 0.0012 - 3s/epoch - 3ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 3s - loss: 13.4039 - mse: 13.4039 - mae: 1.5720 - val_loss: 21.1683 - val_mse: 21.1683 - val_mae: 1.6533 - lr: 0.0012 - 3s/epoch - 3ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 3s - loss: 13.4098 - mse: 13.4098 - mae: 1.5706 - val_loss: 20.9264 - val_mse: 20.9264 - val_mae: 1.5662 - lr: 0.0012 - 3s/epoch - 3ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 3s - loss: 13.4359 - mse: 13.4359 - mae: 1.5657 - val_loss: 20.8602 - val_mse: 20.8602 - val_mae: 1.6332 - lr: 0.0012 - 3s/epoch - 3ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 3s - loss: 13.3849 - mse: 13.3849 - mae: 1.5659 - val_loss: 20.8809 - val_mse: 20.8809 - val_mae: 1.6331 - lr: 0.0012 - 3s/epoch - 3ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 3s - loss: 13.3276 - mse: 13.3276 - mae: 1.5705 - val_loss: 21.2327 - val_mse: 21.2327 - val_mae: 1.5763 - lr: 0.0012 - 3s/epoch - 3ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 3s - loss: 13.3196 - mse: 13.3196 - mae: 1.5686 - val_loss: 21.0006 - val_mse: 21.0006 - val_mae: 1.6155 - lr: 0.0012 - 3s/epoch - 3ms/step\n",
            "Epoch 13/100\n",
            "1000/1000 - 3s - loss: 13.3858 - mse: 13.3858 - mae: 1.5665 - val_loss: 20.9169 - val_mse: 20.9169 - val_mae: 1.6034 - lr: 0.0012 - 3s/epoch - 3ms/step\n",
            "Epoch 14/100\n",
            "1000/1000 - 3s - loss: 13.2003 - mse: 13.2003 - mae: 1.5641 - val_loss: 21.0033 - val_mse: 21.0033 - val_mae: 1.5546 - lr: 0.0012 - 3s/epoch - 3ms/step\n",
            "Score for fold 1: loss of 21.003263473510742\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 3s - loss: 15.5388 - mse: 15.5388 - mae: 1.5849 - val_loss: 11.8312 - val_mse: 11.8312 - val_mae: 1.5079 - lr: 0.0010 - 3s/epoch - 3ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 3s - loss: 15.4540 - mse: 15.4540 - mae: 1.5829 - val_loss: 11.9658 - val_mse: 11.9658 - val_mae: 1.5177 - lr: 0.0010 - 3s/epoch - 3ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 3s - loss: 15.4600 - mse: 15.4600 - mae: 1.5822 - val_loss: 12.0056 - val_mse: 12.0056 - val_mae: 1.5103 - lr: 0.0010 - 3s/epoch - 3ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 3s - loss: 15.4744 - mse: 15.4744 - mae: 1.5790 - val_loss: 11.8345 - val_mse: 11.8345 - val_mae: 1.5442 - lr: 0.0010 - 3s/epoch - 3ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 3s - loss: 15.4632 - mse: 15.4632 - mae: 1.5801 - val_loss: 11.9846 - val_mse: 11.9846 - val_mae: 1.5204 - lr: 0.0010 - 3s/epoch - 3ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 3s - loss: 15.4402 - mse: 15.4402 - mae: 1.5769 - val_loss: 12.0128 - val_mse: 12.0128 - val_mae: 1.5027 - lr: 0.0010 - 3s/epoch - 3ms/step\n",
            "Score for fold 2: loss of 12.0127534866333\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 3s - loss: 15.2011 - mse: 15.2011 - mae: 1.5644 - val_loss: 12.8045 - val_mse: 12.8045 - val_mae: 1.5983 - lr: 0.0010 - 3s/epoch - 3ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 3s - loss: 15.0956 - mse: 15.0956 - mae: 1.5643 - val_loss: 13.1054 - val_mse: 13.1054 - val_mae: 1.5837 - lr: 0.0010 - 3s/epoch - 3ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 3s - loss: 15.0796 - mse: 15.0796 - mae: 1.5616 - val_loss: 13.6181 - val_mse: 13.6181 - val_mae: 1.5289 - lr: 0.0010 - 3s/epoch - 3ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 3s - loss: 15.1163 - mse: 15.1163 - mae: 1.5606 - val_loss: 12.9906 - val_mse: 12.9906 - val_mae: 1.5599 - lr: 0.0010 - 3s/epoch - 3ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 5s - loss: 15.0309 - mse: 15.0309 - mae: 1.5547 - val_loss: 13.6334 - val_mse: 13.6334 - val_mae: 1.5233 - lr: 0.0010 - 5s/epoch - 5ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 4s - loss: 15.0356 - mse: 15.0356 - mae: 1.5581 - val_loss: 13.5182 - val_mse: 13.5182 - val_mae: 1.5899 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Score for fold 3: loss of 13.518206596374512\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 3s - loss: 14.5531 - mse: 14.5531 - mae: 1.5573 - val_loss: 15.1313 - val_mse: 15.1313 - val_mae: 1.5988 - lr: 0.0010 - 3s/epoch - 3ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 3s - loss: 14.4493 - mse: 14.4493 - mae: 1.5590 - val_loss: 15.2001 - val_mse: 15.2001 - val_mae: 1.5763 - lr: 0.0010 - 3s/epoch - 3ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 3s - loss: 14.4297 - mse: 14.4297 - mae: 1.5524 - val_loss: 15.3043 - val_mse: 15.3043 - val_mae: 1.5900 - lr: 0.0010 - 3s/epoch - 3ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 3s - loss: 14.4320 - mse: 14.4320 - mae: 1.5562 - val_loss: 15.2737 - val_mse: 15.2737 - val_mae: 1.5818 - lr: 0.0010 - 3s/epoch - 3ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 3s - loss: 14.2989 - mse: 14.2989 - mae: 1.5526 - val_loss: 15.4491 - val_mse: 15.4491 - val_mae: 1.5799 - lr: 0.0010 - 3s/epoch - 3ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 3s - loss: 14.3280 - mse: 14.3280 - mae: 1.5519 - val_loss: 15.2401 - val_mse: 15.2401 - val_mae: 1.5807 - lr: 0.0010 - 3s/epoch - 3ms/step\n",
            "Score for fold 4: loss of 15.240105628967285\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 3s - loss: 15.0481 - mse: 15.0481 - mae: 1.5609 - val_loss: 12.3654 - val_mse: 12.3654 - val_mae: 1.6077 - lr: 0.0010 - 3s/epoch - 3ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 3s - loss: 14.8976 - mse: 14.8976 - mae: 1.5583 - val_loss: 12.5068 - val_mse: 12.5068 - val_mae: 1.5154 - lr: 0.0010 - 3s/epoch - 3ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 3s - loss: 14.8905 - mse: 14.8905 - mae: 1.5565 - val_loss: 12.9226 - val_mse: 12.9226 - val_mae: 1.5419 - lr: 0.0010 - 3s/epoch - 3ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 3s - loss: 14.8803 - mse: 14.8803 - mae: 1.5564 - val_loss: 13.0352 - val_mse: 13.0352 - val_mae: 1.6003 - lr: 0.0010 - 3s/epoch - 3ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 3s - loss: 14.9096 - mse: 14.9096 - mae: 1.5515 - val_loss: 12.3363 - val_mse: 12.3363 - val_mae: 1.5775 - lr: 0.0010 - 3s/epoch - 3ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 3s - loss: 14.8016 - mse: 14.8016 - mae: 1.5534 - val_loss: 12.8515 - val_mse: 12.8515 - val_mae: 1.5280 - lr: 0.0010 - 3s/epoch - 3ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 3s - loss: 14.8547 - mse: 14.8547 - mae: 1.5492 - val_loss: 12.8046 - val_mse: 12.8046 - val_mae: 1.5742 - lr: 0.0010 - 3s/epoch - 3ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 3s - loss: 14.7317 - mse: 14.7317 - mae: 1.5509 - val_loss: 12.8617 - val_mse: 12.8617 - val_mae: 1.5722 - lr: 0.0010 - 3s/epoch - 3ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 3s - loss: 14.7481 - mse: 14.7481 - mae: 1.5487 - val_loss: 13.0406 - val_mse: 13.0406 - val_mae: 1.6023 - lr: 0.0010 - 3s/epoch - 3ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 3s - loss: 14.7796 - mse: 14.7796 - mae: 1.5508 - val_loss: 13.0583 - val_mse: 13.0583 - val_mae: 1.5535 - lr: 0.0010 - 3s/epoch - 3ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-10-15 06:24:05,751]\u001b[0m Finished trial#25 resulted in value: 14.966. Current best value is 14.696000000000002 with parameters: {'activation': 'relu', 'num_hidden_layer': 3, 'i': 8, 'learning_rate': 0.0008854984178603543}.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Score for fold 5: loss of 13.058348655700684\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 4s - loss: 15.5704 - mse: 15.5704 - mae: 1.6241 - val_loss: 15.2709 - val_mse: 15.2709 - val_mae: 1.6417 - lr: 3.4523e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 3s - loss: 15.1004 - mse: 15.1004 - mae: 1.5915 - val_loss: 15.1984 - val_mse: 15.1984 - val_mae: 1.6155 - lr: 3.4523e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 3s - loss: 15.0009 - mse: 15.0009 - mae: 1.5890 - val_loss: 15.1588 - val_mse: 15.1588 - val_mae: 1.5778 - lr: 3.4523e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 3s - loss: 14.9475 - mse: 14.9475 - mae: 1.5870 - val_loss: 15.2068 - val_mse: 15.2068 - val_mae: 1.5247 - lr: 3.4523e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 3s - loss: 14.9288 - mse: 14.9288 - mae: 1.5856 - val_loss: 15.1324 - val_mse: 15.1324 - val_mae: 1.5904 - lr: 3.4523e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 3s - loss: 14.8874 - mse: 14.8874 - mae: 1.5831 - val_loss: 15.1639 - val_mse: 15.1639 - val_mae: 1.5964 - lr: 3.4523e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 3s - loss: 14.8630 - mse: 14.8630 - mae: 1.5817 - val_loss: 15.1923 - val_mse: 15.1923 - val_mae: 1.5880 - lr: 3.4523e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 3s - loss: 14.8299 - mse: 14.8299 - mae: 1.5819 - val_loss: 15.1472 - val_mse: 15.1472 - val_mae: 1.5330 - lr: 3.4523e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 3s - loss: 14.8245 - mse: 14.8245 - mae: 1.5761 - val_loss: 15.0899 - val_mse: 15.0899 - val_mae: 1.5779 - lr: 3.4523e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 3s - loss: 14.7765 - mse: 14.7765 - mae: 1.5785 - val_loss: 15.0650 - val_mse: 15.0650 - val_mae: 1.5724 - lr: 3.4523e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 3s - loss: 14.7607 - mse: 14.7607 - mae: 1.5755 - val_loss: 15.1133 - val_mse: 15.1133 - val_mae: 1.5579 - lr: 3.4523e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 3s - loss: 14.7657 - mse: 14.7657 - mae: 1.5721 - val_loss: 15.1518 - val_mse: 15.1518 - val_mae: 1.5122 - lr: 3.4523e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 13/100\n",
            "1000/1000 - 3s - loss: 14.7377 - mse: 14.7377 - mae: 1.5743 - val_loss: 15.0925 - val_mse: 15.0925 - val_mae: 1.6286 - lr: 3.4523e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 14/100\n",
            "1000/1000 - 3s - loss: 14.7514 - mse: 14.7514 - mae: 1.5724 - val_loss: 15.0090 - val_mse: 15.0090 - val_mae: 1.5844 - lr: 3.4523e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 15/100\n",
            "1000/1000 - 3s - loss: 14.6719 - mse: 14.6719 - mae: 1.5692 - val_loss: 15.0585 - val_mse: 15.0585 - val_mae: 1.5556 - lr: 3.4523e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 16/100\n",
            "1000/1000 - 3s - loss: 14.6644 - mse: 14.6644 - mae: 1.5706 - val_loss: 14.9965 - val_mse: 14.9965 - val_mae: 1.5486 - lr: 3.4523e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 17/100\n",
            "1000/1000 - 3s - loss: 14.7188 - mse: 14.7188 - mae: 1.5661 - val_loss: 15.0520 - val_mse: 15.0520 - val_mae: 1.5387 - lr: 3.4523e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 18/100\n",
            "1000/1000 - 3s - loss: 14.6393 - mse: 14.6393 - mae: 1.5669 - val_loss: 15.0870 - val_mse: 15.0870 - val_mae: 1.5690 - lr: 3.4523e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 19/100\n",
            "1000/1000 - 3s - loss: 14.7204 - mse: 14.7204 - mae: 1.5662 - val_loss: 14.9719 - val_mse: 14.9719 - val_mae: 1.6154 - lr: 3.4523e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 20/100\n",
            "1000/1000 - 3s - loss: 14.6658 - mse: 14.6658 - mae: 1.5637 - val_loss: 15.1356 - val_mse: 15.1356 - val_mae: 1.6499 - lr: 3.4523e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 21/100\n",
            "1000/1000 - 3s - loss: 14.6145 - mse: 14.6145 - mae: 1.5668 - val_loss: 15.0916 - val_mse: 15.0916 - val_mae: 1.5593 - lr: 3.4523e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 22/100\n",
            "1000/1000 - 3s - loss: 14.5807 - mse: 14.5807 - mae: 1.5646 - val_loss: 15.0112 - val_mse: 15.0112 - val_mae: 1.5380 - lr: 3.4523e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 23/100\n",
            "1000/1000 - 4s - loss: 14.5976 - mse: 14.5976 - mae: 1.5609 - val_loss: 15.0638 - val_mse: 15.0638 - val_mae: 1.5341 - lr: 3.4523e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 24/100\n",
            "1000/1000 - 4s - loss: 14.5170 - mse: 14.5170 - mae: 1.5600 - val_loss: 15.0135 - val_mse: 15.0135 - val_mae: 1.5553 - lr: 3.4523e-04 - 4s/epoch - 4ms/step\n",
            "Score for fold 1: loss of 15.013518333435059\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 3s - loss: 14.8324 - mse: 14.8324 - mae: 1.5706 - val_loss: 14.0845 - val_mse: 14.0845 - val_mae: 1.5764 - lr: 3.4523e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 3s - loss: 14.9263 - mse: 14.9263 - mae: 1.5643 - val_loss: 13.9456 - val_mse: 13.9456 - val_mae: 1.5449 - lr: 3.4523e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 3s - loss: 14.7596 - mse: 14.7596 - mae: 1.5610 - val_loss: 13.9694 - val_mse: 13.9694 - val_mae: 1.5775 - lr: 3.4523e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 3s - loss: 14.8344 - mse: 14.8344 - mae: 1.5611 - val_loss: 13.9783 - val_mse: 13.9783 - val_mae: 1.5368 - lr: 3.4523e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 3s - loss: 14.7991 - mse: 14.7991 - mae: 1.5622 - val_loss: 14.1286 - val_mse: 14.1286 - val_mae: 1.5099 - lr: 3.4523e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 3s - loss: 14.7258 - mse: 14.7258 - mae: 1.5578 - val_loss: 14.0191 - val_mse: 14.0191 - val_mae: 1.5769 - lr: 3.4523e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 3s - loss: 14.7599 - mse: 14.7599 - mae: 1.5582 - val_loss: 14.0942 - val_mse: 14.0942 - val_mae: 1.5048 - lr: 3.4523e-04 - 3s/epoch - 3ms/step\n",
            "Score for fold 2: loss of 14.094182014465332\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 3s - loss: 14.8279 - mse: 14.8279 - mae: 1.5634 - val_loss: 14.0855 - val_mse: 14.0855 - val_mae: 1.5188 - lr: 3.4523e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 3s - loss: 14.7909 - mse: 14.7909 - mae: 1.5568 - val_loss: 13.6870 - val_mse: 13.6870 - val_mae: 1.5945 - lr: 3.4523e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 4s - loss: 14.7362 - mse: 14.7362 - mae: 1.5571 - val_loss: 13.8087 - val_mse: 13.8087 - val_mae: 1.5887 - lr: 3.4523e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 3s - loss: 14.7234 - mse: 14.7234 - mae: 1.5523 - val_loss: 13.9508 - val_mse: 13.9508 - val_mae: 1.5530 - lr: 3.4523e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 3s - loss: 14.7086 - mse: 14.7086 - mae: 1.5547 - val_loss: 14.4800 - val_mse: 14.4800 - val_mae: 1.5420 - lr: 3.4523e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 3s - loss: 14.6933 - mse: 14.6933 - mae: 1.5490 - val_loss: 14.8528 - val_mse: 14.8528 - val_mae: 1.6079 - lr: 3.4523e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 3s - loss: 14.6814 - mse: 14.6814 - mae: 1.5529 - val_loss: 14.1012 - val_mse: 14.1012 - val_mae: 1.5531 - lr: 3.4523e-04 - 3s/epoch - 3ms/step\n",
            "Score for fold 3: loss of 14.101228713989258\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 3s - loss: 15.0800 - mse: 15.0800 - mae: 1.5457 - val_loss: 12.3371 - val_mse: 12.3371 - val_mae: 1.5861 - lr: 3.4523e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 3s - loss: 15.0463 - mse: 15.0463 - mae: 1.5424 - val_loss: 12.3404 - val_mse: 12.3404 - val_mae: 1.5995 - lr: 3.4523e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 3s - loss: 14.9114 - mse: 14.9114 - mae: 1.5395 - val_loss: 12.4559 - val_mse: 12.4559 - val_mae: 1.5695 - lr: 3.4523e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 3s - loss: 14.8986 - mse: 14.8986 - mae: 1.5407 - val_loss: 12.4471 - val_mse: 12.4471 - val_mae: 1.5918 - lr: 3.4523e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 3s - loss: 14.9692 - mse: 14.9692 - mae: 1.5445 - val_loss: 12.7283 - val_mse: 12.7283 - val_mae: 1.5375 - lr: 3.4523e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 3s - loss: 14.8692 - mse: 14.8692 - mae: 1.5369 - val_loss: 12.5345 - val_mse: 12.5345 - val_mae: 1.5959 - lr: 3.4523e-04 - 3s/epoch - 3ms/step\n",
            "Score for fold 4: loss of 12.534463882446289\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 3s - loss: 13.5978 - mse: 13.5978 - mae: 1.5496 - val_loss: 17.7675 - val_mse: 17.7675 - val_mae: 1.5519 - lr: 3.4523e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 3s - loss: 13.6385 - mse: 13.6385 - mae: 1.5512 - val_loss: 17.8356 - val_mse: 17.8356 - val_mae: 1.5196 - lr: 3.4523e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 3s - loss: 13.5640 - mse: 13.5640 - mae: 1.5474 - val_loss: 17.9796 - val_mse: 17.9796 - val_mae: 1.5182 - lr: 3.4523e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 3s - loss: 13.6148 - mse: 13.6148 - mae: 1.5430 - val_loss: 17.9643 - val_mse: 17.9643 - val_mae: 1.5186 - lr: 3.4523e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 4s - loss: 13.4971 - mse: 13.4971 - mae: 1.5389 - val_loss: 18.1961 - val_mse: 18.1961 - val_mae: 1.5257 - lr: 3.4523e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 3s - loss: 13.5117 - mse: 13.5117 - mae: 1.5449 - val_loss: 17.8443 - val_mse: 17.8443 - val_mae: 1.5672 - lr: 3.4523e-04 - 3s/epoch - 3ms/step\n",
            "Score for fold 5: loss of 17.84425926208496\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-10-15 06:27:18,801]\u001b[0m Finished trial#26 resulted in value: 14.714000000000002. Current best value is 14.696000000000002 with parameters: {'activation': 'relu', 'num_hidden_layer': 3, 'i': 8, 'learning_rate': 0.0008854984178603543}.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 4s - loss: 16.3291 - mse: 16.3291 - mae: 1.6199 - val_loss: 12.0162 - val_mse: 12.0162 - val_mae: 1.5950 - lr: 2.7869e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 3s - loss: 15.9323 - mse: 15.9323 - mae: 1.6015 - val_loss: 11.9647 - val_mse: 11.9647 - val_mae: 1.5676 - lr: 2.7869e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 3s - loss: 15.8579 - mse: 15.8579 - mae: 1.5989 - val_loss: 11.9180 - val_mse: 11.9180 - val_mae: 1.5715 - lr: 2.7869e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 3s - loss: 15.8082 - mse: 15.8082 - mae: 1.5934 - val_loss: 11.9072 - val_mse: 11.9072 - val_mae: 1.5848 - lr: 2.7869e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 3s - loss: 15.7681 - mse: 15.7681 - mae: 1.5895 - val_loss: 11.9465 - val_mse: 11.9465 - val_mae: 1.5256 - lr: 2.7869e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 4s - loss: 15.7607 - mse: 15.7607 - mae: 1.5909 - val_loss: 11.9048 - val_mse: 11.9048 - val_mae: 1.5430 - lr: 2.7869e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 4s - loss: 15.6983 - mse: 15.6983 - mae: 1.5911 - val_loss: 11.9039 - val_mse: 11.9039 - val_mae: 1.6168 - lr: 2.7869e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 3s - loss: 15.6573 - mse: 15.6573 - mae: 1.5896 - val_loss: 11.8971 - val_mse: 11.8971 - val_mae: 1.5649 - lr: 2.7869e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 3s - loss: 15.6954 - mse: 15.6954 - mae: 1.5826 - val_loss: 11.9319 - val_mse: 11.9319 - val_mae: 1.5939 - lr: 2.7869e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 3s - loss: 15.5874 - mse: 15.5874 - mae: 1.5815 - val_loss: 11.8921 - val_mse: 11.8921 - val_mae: 1.5843 - lr: 2.7869e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 3s - loss: 15.5986 - mse: 15.5986 - mae: 1.5812 - val_loss: 11.8853 - val_mse: 11.8853 - val_mae: 1.5574 - lr: 2.7869e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 3s - loss: 15.6166 - mse: 15.6166 - mae: 1.5837 - val_loss: 11.8255 - val_mse: 11.8255 - val_mae: 1.5484 - lr: 2.7869e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 13/100\n",
            "1000/1000 - 3s - loss: 15.5523 - mse: 15.5523 - mae: 1.5809 - val_loss: 11.9136 - val_mse: 11.9136 - val_mae: 1.5439 - lr: 2.7869e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 14/100\n",
            "1000/1000 - 3s - loss: 15.6184 - mse: 15.6184 - mae: 1.5802 - val_loss: 11.8746 - val_mse: 11.8746 - val_mae: 1.5512 - lr: 2.7869e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 15/100\n",
            "1000/1000 - 3s - loss: 15.5279 - mse: 15.5279 - mae: 1.5744 - val_loss: 11.8793 - val_mse: 11.8793 - val_mae: 1.5677 - lr: 2.7869e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 16/100\n",
            "1000/1000 - 3s - loss: 15.5700 - mse: 15.5700 - mae: 1.5707 - val_loss: 11.8305 - val_mse: 11.8305 - val_mae: 1.5217 - lr: 2.7869e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 17/100\n",
            "1000/1000 - 3s - loss: 15.5693 - mse: 15.5693 - mae: 1.5763 - val_loss: 11.9634 - val_mse: 11.9634 - val_mae: 1.5080 - lr: 2.7869e-04 - 3s/epoch - 3ms/step\n",
            "Score for fold 1: loss of 11.963406562805176\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 3s - loss: 16.0950 - mse: 16.0950 - mae: 1.5745 - val_loss: 9.4980 - val_mse: 9.4980 - val_mae: 1.5537 - lr: 2.7869e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 3s - loss: 16.0586 - mse: 16.0586 - mae: 1.5744 - val_loss: 9.5949 - val_mse: 9.5949 - val_mae: 1.5270 - lr: 2.7869e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 3s - loss: 16.0652 - mse: 16.0652 - mae: 1.5750 - val_loss: 9.7896 - val_mse: 9.7896 - val_mae: 1.5091 - lr: 2.7869e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 3s - loss: 16.0306 - mse: 16.0306 - mae: 1.5715 - val_loss: 9.6875 - val_mse: 9.6875 - val_mae: 1.5769 - lr: 2.7869e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 3s - loss: 16.0261 - mse: 16.0261 - mae: 1.5707 - val_loss: 9.6597 - val_mse: 9.6597 - val_mae: 1.5404 - lr: 2.7869e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 3s - loss: 16.0027 - mse: 16.0027 - mae: 1.5724 - val_loss: 9.9362 - val_mse: 9.9362 - val_mae: 1.4911 - lr: 2.7869e-04 - 3s/epoch - 3ms/step\n",
            "Score for fold 2: loss of 9.936249732971191\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 3s - loss: 14.9412 - mse: 14.9412 - mae: 1.5608 - val_loss: 14.3998 - val_mse: 14.3998 - val_mae: 1.5408 - lr: 2.7869e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 3s - loss: 14.8716 - mse: 14.8716 - mae: 1.5616 - val_loss: 13.9713 - val_mse: 13.9713 - val_mae: 1.5460 - lr: 2.7869e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 3s - loss: 14.8707 - mse: 14.8707 - mae: 1.5568 - val_loss: 13.8781 - val_mse: 13.8781 - val_mae: 1.5369 - lr: 2.7869e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 3s - loss: 14.8604 - mse: 14.8604 - mae: 1.5552 - val_loss: 13.9666 - val_mse: 13.9666 - val_mae: 1.5607 - lr: 2.7869e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 3s - loss: 14.8315 - mse: 14.8315 - mae: 1.5561 - val_loss: 13.9927 - val_mse: 13.9927 - val_mae: 1.6045 - lr: 2.7869e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 3s - loss: 14.8416 - mse: 14.8416 - mae: 1.5556 - val_loss: 13.9839 - val_mse: 13.9839 - val_mae: 1.5416 - lr: 2.7869e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 3s - loss: 14.8170 - mse: 14.8170 - mae: 1.5553 - val_loss: 14.0025 - val_mse: 14.0025 - val_mae: 1.5558 - lr: 2.7869e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 3s - loss: 14.7626 - mse: 14.7626 - mae: 1.5505 - val_loss: 13.9838 - val_mse: 13.9838 - val_mae: 1.6232 - lr: 2.7869e-04 - 3s/epoch - 3ms/step\n",
            "Score for fold 3: loss of 13.983847618103027\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 4s - loss: 12.8486 - mse: 12.8486 - mae: 1.5545 - val_loss: 21.9602 - val_mse: 21.9602 - val_mae: 1.5586 - lr: 2.7869e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 3s - loss: 12.7972 - mse: 12.7972 - mae: 1.5527 - val_loss: 21.9234 - val_mse: 21.9234 - val_mae: 1.5814 - lr: 2.7869e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 4s - loss: 12.7675 - mse: 12.7675 - mae: 1.5508 - val_loss: 22.1678 - val_mse: 22.1678 - val_mae: 1.5564 - lr: 2.7869e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 4s - loss: 12.7827 - mse: 12.7827 - mae: 1.5487 - val_loss: 22.0576 - val_mse: 22.0576 - val_mae: 1.5772 - lr: 2.7869e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 3s - loss: 12.7844 - mse: 12.7844 - mae: 1.5502 - val_loss: 22.1530 - val_mse: 22.1530 - val_mae: 1.5595 - lr: 2.7869e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 3s - loss: 12.7624 - mse: 12.7624 - mae: 1.5480 - val_loss: 22.1622 - val_mse: 22.1622 - val_mae: 1.5646 - lr: 2.7869e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 3s - loss: 12.7584 - mse: 12.7584 - mae: 1.5493 - val_loss: 22.0814 - val_mse: 22.0814 - val_mae: 1.6609 - lr: 2.7869e-04 - 3s/epoch - 3ms/step\n",
            "Score for fold 4: loss of 22.081377029418945\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 3s - loss: 14.2686 - mse: 14.2686 - mae: 1.5499 - val_loss: 16.2368 - val_mse: 16.2368 - val_mae: 1.5701 - lr: 2.7869e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 3s - loss: 14.1294 - mse: 14.1294 - mae: 1.5434 - val_loss: 16.3679 - val_mse: 16.3679 - val_mae: 1.5894 - lr: 2.7869e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 3s - loss: 14.1628 - mse: 14.1628 - mae: 1.5447 - val_loss: 16.3014 - val_mse: 16.3014 - val_mae: 1.5949 - lr: 2.7869e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 3s - loss: 14.1352 - mse: 14.1352 - mae: 1.5416 - val_loss: 16.2760 - val_mse: 16.2760 - val_mae: 1.5947 - lr: 2.7869e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 3s - loss: 14.1453 - mse: 14.1453 - mae: 1.5421 - val_loss: 16.2073 - val_mse: 16.2073 - val_mae: 1.5568 - lr: 2.7869e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 3s - loss: 14.1125 - mse: 14.1125 - mae: 1.5404 - val_loss: 16.2996 - val_mse: 16.2996 - val_mae: 1.5789 - lr: 2.7869e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 3s - loss: 14.0835 - mse: 14.0835 - mae: 1.5373 - val_loss: 16.5678 - val_mse: 16.5678 - val_mae: 1.5882 - lr: 2.7869e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 3s - loss: 14.0708 - mse: 14.0708 - mae: 1.5383 - val_loss: 16.3253 - val_mse: 16.3253 - val_mae: 1.6176 - lr: 2.7869e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 3s - loss: 14.0950 - mse: 14.0950 - mae: 1.5391 - val_loss: 16.5150 - val_mse: 16.5150 - val_mae: 1.6136 - lr: 2.7869e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 3s - loss: 14.0831 - mse: 14.0831 - mae: 1.5368 - val_loss: 16.2634 - val_mse: 16.2634 - val_mae: 1.6493 - lr: 2.7869e-04 - 3s/epoch - 3ms/step\n",
            "Score for fold 5: loss of 16.263397216796875\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-10-15 06:30:45,712]\u001b[0m Finished trial#27 resulted in value: 14.844. Current best value is 14.696000000000002 with parameters: {'activation': 'relu', 'num_hidden_layer': 3, 'i': 8, 'learning_rate': 0.0008854984178603543}.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 21s - loss: 14.3261 - mse: 14.3261 - mae: 1.6365 - val_loss: 19.9694 - val_mse: 19.9694 - val_mae: 1.5963 - lr: 0.0018 - 21s/epoch - 21ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 21s - loss: 14.1346 - mse: 14.1346 - mae: 1.6112 - val_loss: 19.9484 - val_mse: 19.9484 - val_mae: 1.6025 - lr: 0.0018 - 21s/epoch - 21ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 20s - loss: 13.9851 - mse: 13.9851 - mae: 1.5994 - val_loss: 19.6694 - val_mse: 19.6694 - val_mae: 1.6747 - lr: 0.0018 - 20s/epoch - 20ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 20s - loss: 13.9494 - mse: 13.9494 - mae: 1.6004 - val_loss: 19.8527 - val_mse: 19.8527 - val_mae: 1.5320 - lr: 0.0018 - 20s/epoch - 20ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 22s - loss: 13.9992 - mse: 13.9992 - mae: 1.5958 - val_loss: 19.8978 - val_mse: 19.8978 - val_mae: 1.5983 - lr: 0.0018 - 22s/epoch - 22ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 20s - loss: 13.9307 - mse: 13.9307 - mae: 1.5892 - val_loss: 19.8855 - val_mse: 19.8855 - val_mae: 1.5686 - lr: 0.0018 - 20s/epoch - 20ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 20s - loss: 13.8690 - mse: 13.8690 - mae: 1.5851 - val_loss: 19.7674 - val_mse: 19.7674 - val_mae: 1.5239 - lr: 0.0018 - 20s/epoch - 20ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 20s - loss: 13.8598 - mse: 13.8598 - mae: 1.5856 - val_loss: 19.8838 - val_mse: 19.8838 - val_mae: 1.5980 - lr: 0.0018 - 20s/epoch - 20ms/step\n",
            "Score for fold 1: loss of 19.883821487426758\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 20s - loss: 15.1341 - mse: 15.1341 - mae: 1.5798 - val_loss: 13.9800 - val_mse: 13.9800 - val_mae: 1.6085 - lr: 0.0010 - 20s/epoch - 20ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 21s - loss: 15.1221 - mse: 15.1221 - mae: 1.5734 - val_loss: 14.2419 - val_mse: 14.2419 - val_mae: 1.5659 - lr: 0.0010 - 21s/epoch - 21ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 20s - loss: 15.0620 - mse: 15.0620 - mae: 1.5726 - val_loss: 14.2545 - val_mse: 14.2545 - val_mae: 1.5888 - lr: 0.0010 - 20s/epoch - 20ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 20s - loss: 15.0536 - mse: 15.0536 - mae: 1.5691 - val_loss: 14.3013 - val_mse: 14.3013 - val_mae: 1.5615 - lr: 0.0010 - 20s/epoch - 20ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 20s - loss: 14.9826 - mse: 14.9826 - mae: 1.5697 - val_loss: 14.3422 - val_mse: 14.3422 - val_mae: 1.5877 - lr: 0.0010 - 20s/epoch - 20ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 20s - loss: 14.9237 - mse: 14.9237 - mae: 1.5663 - val_loss: 14.4364 - val_mse: 14.4364 - val_mae: 1.5610 - lr: 0.0010 - 20s/epoch - 20ms/step\n",
            "Score for fold 2: loss of 14.436422348022461\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 20s - loss: 14.9881 - mse: 14.9881 - mae: 1.5676 - val_loss: 13.7798 - val_mse: 13.7798 - val_mae: 1.5682 - lr: 0.0010 - 20s/epoch - 20ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 22s - loss: 15.0132 - mse: 15.0132 - mae: 1.5699 - val_loss: 13.7345 - val_mse: 13.7345 - val_mae: 1.6229 - lr: 0.0010 - 22s/epoch - 22ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 20s - loss: 14.9553 - mse: 14.9553 - mae: 1.5640 - val_loss: 13.8684 - val_mse: 13.8684 - val_mae: 1.5288 - lr: 0.0010 - 20s/epoch - 20ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 20s - loss: 14.9370 - mse: 14.9370 - mae: 1.5626 - val_loss: 13.8877 - val_mse: 13.8877 - val_mae: 1.5392 - lr: 0.0010 - 20s/epoch - 20ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 20s - loss: 14.8689 - mse: 14.8689 - mae: 1.5607 - val_loss: 13.7623 - val_mse: 13.7623 - val_mae: 1.6292 - lr: 0.0010 - 20s/epoch - 20ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 20s - loss: 14.8640 - mse: 14.8640 - mae: 1.5566 - val_loss: 14.0192 - val_mse: 14.0192 - val_mae: 1.5372 - lr: 0.0010 - 20s/epoch - 20ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 20s - loss: 14.8274 - mse: 14.8274 - mae: 1.5550 - val_loss: 13.8920 - val_mse: 13.8920 - val_mae: 1.5692 - lr: 0.0010 - 20s/epoch - 20ms/step\n",
            "Score for fold 3: loss of 13.89198112487793\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 20s - loss: 14.6907 - mse: 14.6907 - mae: 1.5670 - val_loss: 13.8703 - val_mse: 13.8703 - val_mae: 1.5138 - lr: 0.0010 - 20s/epoch - 20ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 21s - loss: 14.7414 - mse: 14.7414 - mae: 1.5683 - val_loss: 13.8947 - val_mse: 13.8947 - val_mae: 1.5761 - lr: 0.0010 - 21s/epoch - 21ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 20s - loss: 14.6605 - mse: 14.6605 - mae: 1.5659 - val_loss: 14.0444 - val_mse: 14.0444 - val_mae: 1.5239 - lr: 0.0010 - 20s/epoch - 20ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 20s - loss: 14.6872 - mse: 14.6872 - mae: 1.5636 - val_loss: 14.0248 - val_mse: 14.0248 - val_mae: 1.5508 - lr: 0.0010 - 20s/epoch - 20ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 20s - loss: 14.5856 - mse: 14.5856 - mae: 1.5633 - val_loss: 14.0946 - val_mse: 14.0946 - val_mae: 1.5036 - lr: 0.0010 - 20s/epoch - 20ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 20s - loss: 14.5570 - mse: 14.5570 - mae: 1.5612 - val_loss: 14.4757 - val_mse: 14.4757 - val_mae: 1.4895 - lr: 0.0010 - 20s/epoch - 20ms/step\n",
            "Score for fold 4: loss of 14.47573471069336\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 20s - loss: 15.0790 - mse: 15.0790 - mae: 1.5513 - val_loss: 11.8719 - val_mse: 11.8719 - val_mae: 1.5313 - lr: 0.0010 - 20s/epoch - 20ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 22s - loss: 14.9078 - mse: 14.9078 - mae: 1.5431 - val_loss: 11.9440 - val_mse: 11.9440 - val_mae: 1.5490 - lr: 0.0010 - 22s/epoch - 22ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 20s - loss: 14.9655 - mse: 14.9655 - mae: 1.5400 - val_loss: 12.1202 - val_mse: 12.1202 - val_mae: 1.5577 - lr: 0.0010 - 20s/epoch - 20ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 20s - loss: 14.8889 - mse: 14.8889 - mae: 1.5388 - val_loss: 12.1744 - val_mse: 12.1744 - val_mae: 1.6391 - lr: 0.0010 - 20s/epoch - 20ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 21s - loss: 14.8700 - mse: 14.8700 - mae: 1.5362 - val_loss: 12.0059 - val_mse: 12.0059 - val_mae: 1.6325 - lr: 0.0010 - 21s/epoch - 21ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 21s - loss: 14.8246 - mse: 14.8246 - mae: 1.5362 - val_loss: 12.0501 - val_mse: 12.0501 - val_mae: 1.5701 - lr: 0.0010 - 21s/epoch - 21ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-10-15 06:43:25,007]\u001b[0m Finished trial#28 resulted in value: 14.947999999999999. Current best value is 14.696000000000002 with parameters: {'activation': 'relu', 'num_hidden_layer': 3, 'i': 8, 'learning_rate': 0.0008854984178603543}.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Score for fold 5: loss of 12.050094604492188\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 4s - loss: 13.8524 - mse: 13.8524 - mae: 1.6190 - val_loss: 22.1286 - val_mse: 22.1286 - val_mae: 1.5791 - lr: 0.0029 - 4s/epoch - 4ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 3s - loss: 13.5905 - mse: 13.5905 - mae: 1.5981 - val_loss: 21.5793 - val_mse: 21.5793 - val_mae: 1.6161 - lr: 0.0029 - 3s/epoch - 3ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 4s - loss: 13.4885 - mse: 13.4885 - mae: 1.5877 - val_loss: 21.8956 - val_mse: 21.8956 - val_mae: 1.6193 - lr: 0.0029 - 4s/epoch - 4ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 4s - loss: 13.4787 - mse: 13.4787 - mae: 1.5832 - val_loss: 22.1248 - val_mse: 22.1248 - val_mae: 1.6566 - lr: 0.0029 - 4s/epoch - 4ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 5s - loss: 13.4883 - mse: 13.4883 - mae: 1.5828 - val_loss: 21.9901 - val_mse: 21.9901 - val_mae: 1.5563 - lr: 0.0029 - 5s/epoch - 5ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 4s - loss: 13.4427 - mse: 13.4427 - mae: 1.5907 - val_loss: 21.7976 - val_mse: 21.7976 - val_mae: 1.6561 - lr: 0.0029 - 4s/epoch - 4ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 3s - loss: 13.4540 - mse: 13.4540 - mae: 1.5910 - val_loss: 21.6290 - val_mse: 21.6290 - val_mae: 1.6191 - lr: 0.0029 - 3s/epoch - 3ms/step\n",
            "Score for fold 1: loss of 21.628982543945312\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 3s - loss: 16.0011 - mse: 16.0011 - mae: 1.5938 - val_loss: 10.4304 - val_mse: 10.4304 - val_mae: 1.5401 - lr: 0.0010 - 3s/epoch - 3ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 3s - loss: 15.9428 - mse: 15.9428 - mae: 1.5886 - val_loss: 10.4578 - val_mse: 10.4578 - val_mae: 1.5404 - lr: 0.0010 - 3s/epoch - 3ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 3s - loss: 15.9967 - mse: 15.9967 - mae: 1.5839 - val_loss: 10.6726 - val_mse: 10.6726 - val_mae: 1.5398 - lr: 0.0010 - 3s/epoch - 3ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 4s - loss: 16.0396 - mse: 16.0396 - mae: 1.5861 - val_loss: 10.4464 - val_mse: 10.4464 - val_mae: 1.5888 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 4s - loss: 15.9467 - mse: 15.9467 - mae: 1.5816 - val_loss: 10.4927 - val_mse: 10.4927 - val_mae: 1.6064 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 3s - loss: 15.9080 - mse: 15.9080 - mae: 1.5889 - val_loss: 10.6006 - val_mse: 10.6006 - val_mae: 1.5509 - lr: 0.0010 - 3s/epoch - 3ms/step\n",
            "Score for fold 2: loss of 10.600607872009277\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 3s - loss: 15.3575 - mse: 15.3575 - mae: 1.5747 - val_loss: 12.9154 - val_mse: 12.9154 - val_mae: 1.5865 - lr: 0.0010 - 3s/epoch - 3ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 3s - loss: 15.3479 - mse: 15.3479 - mae: 1.5723 - val_loss: 12.9610 - val_mse: 12.9610 - val_mae: 1.5518 - lr: 0.0010 - 3s/epoch - 3ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 4s - loss: 15.2900 - mse: 15.2900 - mae: 1.5723 - val_loss: 13.0496 - val_mse: 13.0496 - val_mae: 1.5904 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 3s - loss: 15.2721 - mse: 15.2721 - mae: 1.5710 - val_loss: 12.9724 - val_mse: 12.9724 - val_mae: 1.5489 - lr: 0.0010 - 3s/epoch - 3ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 3s - loss: 15.1934 - mse: 15.1934 - mae: 1.5670 - val_loss: 13.0355 - val_mse: 13.0355 - val_mae: 1.5448 - lr: 0.0010 - 3s/epoch - 3ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 3s - loss: 15.2910 - mse: 15.2910 - mae: 1.5695 - val_loss: 13.0212 - val_mse: 13.0212 - val_mae: 1.5646 - lr: 0.0010 - 3s/epoch - 3ms/step\n",
            "Score for fold 3: loss of 13.021159172058105\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 4s - loss: 14.3173 - mse: 14.3173 - mae: 1.5620 - val_loss: 16.7386 - val_mse: 16.7386 - val_mae: 1.5991 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 3s - loss: 14.2988 - mse: 14.2988 - mae: 1.5545 - val_loss: 16.5574 - val_mse: 16.5574 - val_mae: 1.6239 - lr: 0.0010 - 3s/epoch - 3ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 3s - loss: 14.2705 - mse: 14.2705 - mae: 1.5568 - val_loss: 16.5861 - val_mse: 16.5861 - val_mae: 1.6366 - lr: 0.0010 - 3s/epoch - 3ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 3s - loss: 14.2672 - mse: 14.2672 - mae: 1.5519 - val_loss: 16.8548 - val_mse: 16.8548 - val_mae: 1.6479 - lr: 0.0010 - 3s/epoch - 3ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 3s - loss: 14.2365 - mse: 14.2365 - mae: 1.5527 - val_loss: 16.9261 - val_mse: 16.9261 - val_mae: 1.6188 - lr: 0.0010 - 3s/epoch - 3ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 3s - loss: 14.2271 - mse: 14.2271 - mae: 1.5484 - val_loss: 16.8152 - val_mse: 16.8152 - val_mae: 1.5737 - lr: 0.0010 - 3s/epoch - 3ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 3s - loss: 14.1994 - mse: 14.1994 - mae: 1.5462 - val_loss: 16.6381 - val_mse: 16.6381 - val_mae: 1.6358 - lr: 0.0010 - 3s/epoch - 3ms/step\n",
            "Score for fold 4: loss of 16.638086318969727\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 3s - loss: 15.3248 - mse: 15.3248 - mae: 1.5736 - val_loss: 12.4247 - val_mse: 12.4247 - val_mae: 1.4786 - lr: 0.0010 - 3s/epoch - 3ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 3s - loss: 15.2565 - mse: 15.2565 - mae: 1.5699 - val_loss: 12.3375 - val_mse: 12.3375 - val_mae: 1.5221 - lr: 0.0010 - 3s/epoch - 3ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 3s - loss: 15.2534 - mse: 15.2534 - mae: 1.5683 - val_loss: 12.2963 - val_mse: 12.2963 - val_mae: 1.5639 - lr: 0.0010 - 3s/epoch - 3ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 3s - loss: 15.1923 - mse: 15.1923 - mae: 1.5692 - val_loss: 12.4685 - val_mse: 12.4685 - val_mae: 1.5198 - lr: 0.0010 - 3s/epoch - 3ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 4s - loss: 15.1716 - mse: 15.1716 - mae: 1.5686 - val_loss: 12.5230 - val_mse: 12.5230 - val_mae: 1.4985 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 3s - loss: 15.1907 - mse: 15.1907 - mae: 1.5653 - val_loss: 12.5104 - val_mse: 12.5104 - val_mae: 1.6023 - lr: 0.0010 - 3s/epoch - 3ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 3s - loss: 15.1398 - mse: 15.1398 - mae: 1.5638 - val_loss: 12.6001 - val_mse: 12.6001 - val_mae: 1.5481 - lr: 0.0010 - 3s/epoch - 3ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 3s - loss: 15.0922 - mse: 15.0922 - mae: 1.5649 - val_loss: 12.4855 - val_mse: 12.4855 - val_mae: 1.5560 - lr: 0.0010 - 3s/epoch - 3ms/step\n",
            "Score for fold 5: loss of 12.485498428344727\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-10-15 06:46:02,009]\u001b[0m Finished trial#29 resulted in value: 14.876. Current best value is 14.696000000000002 with parameters: {'activation': 'relu', 'num_hidden_layer': 3, 'i': 8, 'learning_rate': 0.0008854984178603543}.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 13.4582 - mse: 13.4582 - mae: 1.6150 - val_loss: 23.8916 - val_mse: 23.8916 - val_mae: 1.5434 - lr: 0.0014 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 13.0787 - mse: 13.0787 - mae: 1.5953 - val_loss: 23.8758 - val_mse: 23.8758 - val_mae: 1.6142 - lr: 0.0014 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 13.0096 - mse: 13.0096 - mae: 1.5984 - val_loss: 23.5382 - val_mse: 23.5382 - val_mae: 1.5577 - lr: 0.0014 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 12.9775 - mse: 12.9775 - mae: 1.5908 - val_loss: 23.6442 - val_mse: 23.6442 - val_mae: 1.5625 - lr: 0.0014 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 12.9521 - mse: 12.9521 - mae: 1.5922 - val_loss: 23.5264 - val_mse: 23.5264 - val_mae: 1.6322 - lr: 0.0014 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 12.9206 - mse: 12.9206 - mae: 1.5903 - val_loss: 23.3925 - val_mse: 23.3925 - val_mae: 1.6161 - lr: 0.0014 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 12.9136 - mse: 12.9136 - mae: 1.5855 - val_loss: 23.4560 - val_mse: 23.4560 - val_mae: 1.6218 - lr: 0.0014 - 2s/epoch - 2ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 2s - loss: 12.9091 - mse: 12.9091 - mae: 1.5881 - val_loss: 23.5964 - val_mse: 23.5964 - val_mae: 1.5695 - lr: 0.0014 - 2s/epoch - 2ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 2s - loss: 12.8972 - mse: 12.8972 - mae: 1.5846 - val_loss: 23.4712 - val_mse: 23.4712 - val_mae: 1.5567 - lr: 0.0014 - 2s/epoch - 2ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 2s - loss: 12.8766 - mse: 12.8766 - mae: 1.5856 - val_loss: 23.5707 - val_mse: 23.5707 - val_mae: 1.5614 - lr: 0.0014 - 2s/epoch - 2ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 2s - loss: 12.8914 - mse: 12.8914 - mae: 1.5834 - val_loss: 23.5103 - val_mse: 23.5103 - val_mae: 1.5778 - lr: 0.0014 - 2s/epoch - 2ms/step\n",
            "Score for fold 1: loss of 23.510290145874023\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 15.5974 - mse: 15.5974 - mae: 1.5846 - val_loss: 12.4584 - val_mse: 12.4584 - val_mae: 1.5429 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 15.5882 - mse: 15.5882 - mae: 1.5825 - val_loss: 12.3914 - val_mse: 12.3914 - val_mae: 1.5766 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 15.5707 - mse: 15.5707 - mae: 1.5811 - val_loss: 12.4417 - val_mse: 12.4417 - val_mae: 1.5683 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 15.5505 - mse: 15.5505 - mae: 1.5818 - val_loss: 12.3614 - val_mse: 12.3614 - val_mae: 1.5896 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 15.5469 - mse: 15.5469 - mae: 1.5818 - val_loss: 12.3118 - val_mse: 12.3118 - val_mae: 1.6181 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 15.5134 - mse: 15.5134 - mae: 1.5820 - val_loss: 12.2996 - val_mse: 12.2996 - val_mae: 1.5679 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 15.5547 - mse: 15.5547 - mae: 1.5789 - val_loss: 12.4888 - val_mse: 12.4888 - val_mae: 1.5311 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 2s - loss: 15.5405 - mse: 15.5405 - mae: 1.5761 - val_loss: 12.3547 - val_mse: 12.3547 - val_mae: 1.5647 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 2s - loss: 15.5248 - mse: 15.5248 - mae: 1.5771 - val_loss: 12.3168 - val_mse: 12.3168 - val_mae: 1.6281 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 2s - loss: 15.5082 - mse: 15.5082 - mae: 1.5822 - val_loss: 12.5315 - val_mse: 12.5315 - val_mae: 1.5440 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 2s - loss: 15.5197 - mse: 15.5197 - mae: 1.5802 - val_loss: 12.3053 - val_mse: 12.3053 - val_mae: 1.5695 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Score for fold 2: loss of 12.30528736114502\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 14.9687 - mse: 14.9687 - mae: 1.5758 - val_loss: 14.5847 - val_mse: 14.5847 - val_mae: 1.5703 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 14.9589 - mse: 14.9589 - mae: 1.5757 - val_loss: 14.5784 - val_mse: 14.5784 - val_mae: 1.5635 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 14.9470 - mse: 14.9470 - mae: 1.5720 - val_loss: 14.7318 - val_mse: 14.7318 - val_mae: 1.5607 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 14.9521 - mse: 14.9521 - mae: 1.5738 - val_loss: 14.6649 - val_mse: 14.6649 - val_mae: 1.5450 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 14.9383 - mse: 14.9383 - mae: 1.5746 - val_loss: 14.5523 - val_mse: 14.5523 - val_mae: 1.5568 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 14.9248 - mse: 14.9248 - mae: 1.5716 - val_loss: 14.6050 - val_mse: 14.6050 - val_mae: 1.5538 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 14.9398 - mse: 14.9398 - mae: 1.5724 - val_loss: 14.5905 - val_mse: 14.5905 - val_mae: 1.6211 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 2s - loss: 14.9404 - mse: 14.9404 - mae: 1.5752 - val_loss: 14.6534 - val_mse: 14.6534 - val_mae: 1.5636 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 2s - loss: 14.9202 - mse: 14.9202 - mae: 1.5709 - val_loss: 14.5726 - val_mse: 14.5726 - val_mae: 1.6239 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 2s - loss: 14.9393 - mse: 14.9393 - mae: 1.5735 - val_loss: 14.5749 - val_mse: 14.5749 - val_mae: 1.6069 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Score for fold 3: loss of 14.574917793273926\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 16.1807 - mse: 16.1807 - mae: 1.5850 - val_loss: 9.7179 - val_mse: 9.7179 - val_mae: 1.5230 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 16.1845 - mse: 16.1845 - mae: 1.5814 - val_loss: 9.7120 - val_mse: 9.7120 - val_mae: 1.5342 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 16.1648 - mse: 16.1648 - mae: 1.5815 - val_loss: 9.7755 - val_mse: 9.7755 - val_mae: 1.5365 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 16.1618 - mse: 16.1618 - mae: 1.5813 - val_loss: 9.7396 - val_mse: 9.7396 - val_mae: 1.5706 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 16.1602 - mse: 16.1602 - mae: 1.5822 - val_loss: 9.7137 - val_mse: 9.7137 - val_mae: 1.5775 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 16.1661 - mse: 16.1661 - mae: 1.5824 - val_loss: 9.7216 - val_mse: 9.7216 - val_mae: 1.5439 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 16.1487 - mse: 16.1487 - mae: 1.5817 - val_loss: 9.6827 - val_mse: 9.6827 - val_mae: 1.5531 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 2s - loss: 16.1677 - mse: 16.1677 - mae: 1.5832 - val_loss: 9.7422 - val_mse: 9.7422 - val_mae: 1.5848 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 2s - loss: 16.1352 - mse: 16.1352 - mae: 1.5845 - val_loss: 9.6866 - val_mse: 9.6866 - val_mae: 1.5642 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 2s - loss: 16.1547 - mse: 16.1547 - mae: 1.5831 - val_loss: 9.7447 - val_mse: 9.7447 - val_mae: 1.5556 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 2s - loss: 16.1429 - mse: 16.1429 - mae: 1.5836 - val_loss: 9.8450 - val_mse: 9.8450 - val_mae: 1.5530 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 2s - loss: 16.1146 - mse: 16.1146 - mae: 1.5863 - val_loss: 9.7471 - val_mse: 9.7471 - val_mae: 1.5797 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Score for fold 4: loss of 9.747079849243164\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 14.9463 - mse: 14.9463 - mae: 1.5823 - val_loss: 14.4932 - val_mse: 14.4932 - val_mae: 1.5689 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 14.9306 - mse: 14.9306 - mae: 1.5760 - val_loss: 14.5210 - val_mse: 14.5210 - val_mae: 1.5534 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 14.9421 - mse: 14.9421 - mae: 1.5720 - val_loss: 14.4573 - val_mse: 14.4573 - val_mae: 1.5705 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 14.8783 - mse: 14.8783 - mae: 1.5746 - val_loss: 14.6058 - val_mse: 14.6058 - val_mae: 1.5414 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 14.9216 - mse: 14.9216 - mae: 1.5781 - val_loss: 14.5396 - val_mse: 14.5396 - val_mae: 1.5601 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 14.9148 - mse: 14.9148 - mae: 1.5765 - val_loss: 14.6564 - val_mse: 14.6564 - val_mae: 1.5381 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 14.9237 - mse: 14.9237 - mae: 1.5729 - val_loss: 14.5403 - val_mse: 14.5403 - val_mae: 1.5585 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 2s - loss: 14.8834 - mse: 14.8834 - mae: 1.5755 - val_loss: 14.5279 - val_mse: 14.5279 - val_mae: 1.5697 - lr: 0.0010 - 2s/epoch - 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-10-15 06:47:44,585]\u001b[0m Finished trial#30 resulted in value: 14.934000000000001. Current best value is 14.696000000000002 with parameters: {'activation': 'relu', 'num_hidden_layer': 3, 'i': 8, 'learning_rate': 0.0008854984178603543}.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Score for fold 5: loss of 14.527932167053223\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 4s - loss: 15.9407 - mse: 15.9407 - mae: 1.6213 - val_loss: 14.1372 - val_mse: 14.1372 - val_mae: 1.5674 - lr: 3.1053e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 3s - loss: 15.4271 - mse: 15.4271 - mae: 1.5972 - val_loss: 13.9935 - val_mse: 13.9935 - val_mae: 1.5582 - lr: 3.1053e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 3s - loss: 15.3234 - mse: 15.3234 - mae: 1.6001 - val_loss: 13.9739 - val_mse: 13.9739 - val_mae: 1.5914 - lr: 3.1053e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 3s - loss: 15.2842 - mse: 15.2842 - mae: 1.5938 - val_loss: 14.1027 - val_mse: 14.1027 - val_mae: 1.6165 - lr: 3.1053e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 3s - loss: 15.2235 - mse: 15.2235 - mae: 1.5922 - val_loss: 13.9932 - val_mse: 13.9932 - val_mae: 1.5299 - lr: 3.1053e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 3s - loss: 15.2144 - mse: 15.2144 - mae: 1.5875 - val_loss: 14.0307 - val_mse: 14.0307 - val_mae: 1.5434 - lr: 3.1053e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 3s - loss: 15.1406 - mse: 15.1406 - mae: 1.5860 - val_loss: 13.9983 - val_mse: 13.9983 - val_mae: 1.4958 - lr: 3.1053e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 3s - loss: 15.1632 - mse: 15.1632 - mae: 1.5839 - val_loss: 14.0053 - val_mse: 14.0053 - val_mae: 1.5122 - lr: 3.1053e-04 - 3s/epoch - 3ms/step\n",
            "Score for fold 1: loss of 14.005277633666992\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 3s - loss: 15.1361 - mse: 15.1361 - mae: 1.5704 - val_loss: 13.9701 - val_mse: 13.9701 - val_mae: 1.5755 - lr: 3.1053e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 3s - loss: 15.0794 - mse: 15.0794 - mae: 1.5696 - val_loss: 14.0812 - val_mse: 14.0812 - val_mae: 1.5759 - lr: 3.1053e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 3s - loss: 15.0762 - mse: 15.0762 - mae: 1.5699 - val_loss: 14.1383 - val_mse: 14.1383 - val_mae: 1.5785 - lr: 3.1053e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 3s - loss: 15.0589 - mse: 15.0589 - mae: 1.5629 - val_loss: 14.0803 - val_mse: 14.0803 - val_mae: 1.5684 - lr: 3.1053e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 3s - loss: 15.0442 - mse: 15.0442 - mae: 1.5635 - val_loss: 14.2090 - val_mse: 14.2090 - val_mae: 1.5810 - lr: 3.1053e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 3s - loss: 15.0135 - mse: 15.0135 - mae: 1.5636 - val_loss: 14.1989 - val_mse: 14.1989 - val_mae: 1.5411 - lr: 3.1053e-04 - 3s/epoch - 3ms/step\n",
            "Score for fold 2: loss of 14.19892406463623\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 3s - loss: 14.7193 - mse: 14.7193 - mae: 1.5658 - val_loss: 15.2908 - val_mse: 15.2908 - val_mae: 1.5600 - lr: 3.1053e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 3s - loss: 14.6981 - mse: 14.6981 - mae: 1.5669 - val_loss: 15.5015 - val_mse: 15.5015 - val_mae: 1.5205 - lr: 3.1053e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 3s - loss: 14.7070 - mse: 14.7070 - mae: 1.5665 - val_loss: 15.2578 - val_mse: 15.2578 - val_mae: 1.5937 - lr: 3.1053e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 3s - loss: 14.6722 - mse: 14.6722 - mae: 1.5623 - val_loss: 15.3665 - val_mse: 15.3665 - val_mae: 1.5682 - lr: 3.1053e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 3s - loss: 14.7347 - mse: 14.7347 - mae: 1.5618 - val_loss: 15.4348 - val_mse: 15.4348 - val_mae: 1.5616 - lr: 3.1053e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 3s - loss: 14.7061 - mse: 14.7061 - mae: 1.5617 - val_loss: 15.3255 - val_mse: 15.3255 - val_mae: 1.5945 - lr: 3.1053e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 3s - loss: 14.6235 - mse: 14.6235 - mae: 1.5646 - val_loss: 15.4020 - val_mse: 15.4020 - val_mae: 1.6488 - lr: 3.1053e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 3s - loss: 14.6387 - mse: 14.6387 - mae: 1.5640 - val_loss: 15.4187 - val_mse: 15.4187 - val_mae: 1.5895 - lr: 3.1053e-04 - 3s/epoch - 3ms/step\n",
            "Score for fold 3: loss of 15.418669700622559\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 3s - loss: 15.8466 - mse: 15.8466 - mae: 1.5713 - val_loss: 10.5450 - val_mse: 10.5450 - val_mae: 1.5396 - lr: 3.1053e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 3s - loss: 15.7793 - mse: 15.7793 - mae: 1.5677 - val_loss: 10.6662 - val_mse: 10.6662 - val_mae: 1.5650 - lr: 3.1053e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 3s - loss: 15.7054 - mse: 15.7054 - mae: 1.5696 - val_loss: 10.7452 - val_mse: 10.7452 - val_mae: 1.5078 - lr: 3.1053e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 3s - loss: 15.7758 - mse: 15.7758 - mae: 1.5651 - val_loss: 10.5987 - val_mse: 10.5987 - val_mae: 1.5175 - lr: 3.1053e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 3s - loss: 15.6814 - mse: 15.6814 - mae: 1.5610 - val_loss: 10.6409 - val_mse: 10.6409 - val_mae: 1.5673 - lr: 3.1053e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 3s - loss: 15.7690 - mse: 15.7690 - mae: 1.5653 - val_loss: 10.7749 - val_mse: 10.7749 - val_mae: 1.5605 - lr: 3.1053e-04 - 3s/epoch - 3ms/step\n",
            "Score for fold 4: loss of 10.77486515045166\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 3s - loss: 13.3734 - mse: 13.3734 - mae: 1.5539 - val_loss: 20.3440 - val_mse: 20.3440 - val_mae: 1.5835 - lr: 3.1053e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 3s - loss: 13.2898 - mse: 13.2898 - mae: 1.5506 - val_loss: 20.1400 - val_mse: 20.1400 - val_mae: 1.5643 - lr: 3.1053e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 3s - loss: 13.3353 - mse: 13.3353 - mae: 1.5451 - val_loss: 20.3506 - val_mse: 20.3506 - val_mae: 1.5959 - lr: 3.1053e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 3s - loss: 13.3163 - mse: 13.3163 - mae: 1.5484 - val_loss: 20.1739 - val_mse: 20.1739 - val_mae: 1.5794 - lr: 3.1053e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 3s - loss: 13.3030 - mse: 13.3030 - mae: 1.5449 - val_loss: 20.1258 - val_mse: 20.1258 - val_mae: 1.6369 - lr: 3.1053e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 3s - loss: 13.2359 - mse: 13.2359 - mae: 1.5475 - val_loss: 20.2432 - val_mse: 20.2432 - val_mae: 1.5867 - lr: 3.1053e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 3s - loss: 13.2563 - mse: 13.2563 - mae: 1.5479 - val_loss: 20.2367 - val_mse: 20.2367 - val_mae: 1.6095 - lr: 3.1053e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 3s - loss: 13.2512 - mse: 13.2512 - mae: 1.5469 - val_loss: 20.2695 - val_mse: 20.2695 - val_mae: 1.6002 - lr: 3.1053e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 3s - loss: 13.1951 - mse: 13.1951 - mae: 1.5415 - val_loss: 20.1493 - val_mse: 20.1493 - val_mae: 1.6788 - lr: 3.1053e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 3s - loss: 13.2452 - mse: 13.2452 - mae: 1.5457 - val_loss: 20.2885 - val_mse: 20.2885 - val_mae: 1.6106 - lr: 3.1053e-04 - 3s/epoch - 3ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-10-15 06:50:20,923]\u001b[0m Finished trial#31 resulted in value: 14.937999999999999. Current best value is 14.696000000000002 with parameters: {'activation': 'relu', 'num_hidden_layer': 3, 'i': 8, 'learning_rate': 0.0008854984178603543}.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Score for fold 5: loss of 20.288515090942383\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 4s - loss: 16.3876 - mse: 16.3876 - mae: 1.6219 - val_loss: 13.2818 - val_mse: 13.2818 - val_mae: 1.5868 - lr: 1.6639e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 3s - loss: 15.6743 - mse: 15.6743 - mae: 1.5820 - val_loss: 13.1832 - val_mse: 13.1832 - val_mae: 1.6270 - lr: 1.6639e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 3s - loss: 15.5671 - mse: 15.5671 - mae: 1.5796 - val_loss: 13.1879 - val_mse: 13.1879 - val_mae: 1.6142 - lr: 1.6639e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 3s - loss: 15.4955 - mse: 15.4955 - mae: 1.5834 - val_loss: 13.1033 - val_mse: 13.1033 - val_mae: 1.6347 - lr: 1.6639e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 4s - loss: 15.4238 - mse: 15.4238 - mae: 1.5811 - val_loss: 13.1165 - val_mse: 13.1165 - val_mae: 1.5803 - lr: 1.6639e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 4s - loss: 15.4106 - mse: 15.4106 - mae: 1.5785 - val_loss: 13.1041 - val_mse: 13.1041 - val_mae: 1.6080 - lr: 1.6639e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 3s - loss: 15.3911 - mse: 15.3911 - mae: 1.5765 - val_loss: 13.2205 - val_mse: 13.2205 - val_mae: 1.5826 - lr: 1.6639e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 3s - loss: 15.3743 - mse: 15.3743 - mae: 1.5756 - val_loss: 13.1289 - val_mse: 13.1289 - val_mae: 1.5662 - lr: 1.6639e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 3s - loss: 15.3473 - mse: 15.3473 - mae: 1.5752 - val_loss: 13.2403 - val_mse: 13.2403 - val_mae: 1.5745 - lr: 1.6639e-04 - 3s/epoch - 3ms/step\n",
            "Score for fold 1: loss of 13.240325927734375\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 3s - loss: 15.5283 - mse: 15.5283 - mae: 1.5796 - val_loss: 12.3695 - val_mse: 12.3695 - val_mae: 1.5344 - lr: 1.6639e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 3s - loss: 15.4637 - mse: 15.4637 - mae: 1.5802 - val_loss: 12.3546 - val_mse: 12.3546 - val_mae: 1.5899 - lr: 1.6639e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 3s - loss: 15.4908 - mse: 15.4908 - mae: 1.5762 - val_loss: 12.3994 - val_mse: 12.3994 - val_mae: 1.5529 - lr: 1.6639e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 3s - loss: 15.4938 - mse: 15.4938 - mae: 1.5756 - val_loss: 12.3595 - val_mse: 12.3595 - val_mae: 1.5625 - lr: 1.6639e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 3s - loss: 15.4314 - mse: 15.4314 - mae: 1.5732 - val_loss: 12.4433 - val_mse: 12.4433 - val_mae: 1.5354 - lr: 1.6639e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 3s - loss: 15.4237 - mse: 15.4237 - mae: 1.5732 - val_loss: 12.5447 - val_mse: 12.5447 - val_mae: 1.5933 - lr: 1.6639e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 3s - loss: 15.4270 - mse: 15.4270 - mae: 1.5693 - val_loss: 12.3878 - val_mse: 12.3878 - val_mae: 1.5676 - lr: 1.6639e-04 - 3s/epoch - 3ms/step\n",
            "Score for fold 2: loss of 12.387805938720703\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 3s - loss: 13.3702 - mse: 13.3702 - mae: 1.5693 - val_loss: 20.6982 - val_mse: 20.6982 - val_mae: 1.5885 - lr: 1.6639e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 4s - loss: 13.3465 - mse: 13.3465 - mae: 1.5684 - val_loss: 20.4646 - val_mse: 20.4646 - val_mae: 1.6136 - lr: 1.6639e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 3s - loss: 13.3547 - mse: 13.3547 - mae: 1.5678 - val_loss: 20.5577 - val_mse: 20.5577 - val_mae: 1.5625 - lr: 1.6639e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 3s - loss: 13.3388 - mse: 13.3388 - mae: 1.5673 - val_loss: 20.6090 - val_mse: 20.6090 - val_mae: 1.6036 - lr: 1.6639e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 3s - loss: 13.3533 - mse: 13.3533 - mae: 1.5595 - val_loss: 20.5014 - val_mse: 20.5014 - val_mae: 1.6036 - lr: 1.6639e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 3s - loss: 13.2949 - mse: 13.2949 - mae: 1.5642 - val_loss: 20.5800 - val_mse: 20.5800 - val_mae: 1.5786 - lr: 1.6639e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 3s - loss: 13.2985 - mse: 13.2985 - mae: 1.5619 - val_loss: 20.5583 - val_mse: 20.5583 - val_mae: 1.6193 - lr: 1.6639e-04 - 3s/epoch - 3ms/step\n",
            "Score for fold 3: loss of 20.558330535888672\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 3s - loss: 14.5712 - mse: 14.5712 - mae: 1.5682 - val_loss: 15.4994 - val_mse: 15.4994 - val_mae: 1.5403 - lr: 1.6639e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 3s - loss: 14.5547 - mse: 14.5547 - mae: 1.5657 - val_loss: 15.4939 - val_mse: 15.4939 - val_mae: 1.5861 - lr: 1.6639e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 3s - loss: 14.5498 - mse: 14.5498 - mae: 1.5678 - val_loss: 15.5382 - val_mse: 15.5382 - val_mae: 1.5822 - lr: 1.6639e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 3s - loss: 14.5089 - mse: 14.5089 - mae: 1.5638 - val_loss: 15.5805 - val_mse: 15.5805 - val_mae: 1.5678 - lr: 1.6639e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 3s - loss: 14.5020 - mse: 14.5020 - mae: 1.5614 - val_loss: 15.6263 - val_mse: 15.6263 - val_mae: 1.5585 - lr: 1.6639e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 3s - loss: 14.4945 - mse: 14.4945 - mae: 1.5624 - val_loss: 15.6335 - val_mse: 15.6335 - val_mae: 1.5739 - lr: 1.6639e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 3s - loss: 14.4733 - mse: 14.4733 - mae: 1.5602 - val_loss: 15.5742 - val_mse: 15.5742 - val_mae: 1.5678 - lr: 1.6639e-04 - 3s/epoch - 3ms/step\n",
            "Score for fold 4: loss of 15.574246406555176\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 3s - loss: 15.2879 - mse: 15.2879 - mae: 1.5613 - val_loss: 12.7439 - val_mse: 12.7439 - val_mae: 1.5041 - lr: 1.6639e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 3s - loss: 15.2921 - mse: 15.2921 - mae: 1.5545 - val_loss: 12.4608 - val_mse: 12.4608 - val_mae: 1.5624 - lr: 1.6639e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 3s - loss: 15.2577 - mse: 15.2577 - mae: 1.5556 - val_loss: 12.3865 - val_mse: 12.3865 - val_mae: 1.6010 - lr: 1.6639e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 3s - loss: 15.2380 - mse: 15.2380 - mae: 1.5555 - val_loss: 12.5146 - val_mse: 12.5146 - val_mae: 1.5726 - lr: 1.6639e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 3s - loss: 15.2304 - mse: 15.2304 - mae: 1.5540 - val_loss: 12.5959 - val_mse: 12.5959 - val_mae: 1.5381 - lr: 1.6639e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 3s - loss: 15.2168 - mse: 15.2168 - mae: 1.5544 - val_loss: 12.4075 - val_mse: 12.4075 - val_mae: 1.6065 - lr: 1.6639e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 3s - loss: 15.1984 - mse: 15.1984 - mae: 1.5548 - val_loss: 12.6937 - val_mse: 12.6937 - val_mae: 1.5895 - lr: 1.6639e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 3s - loss: 15.1904 - mse: 15.1904 - mae: 1.5537 - val_loss: 12.7409 - val_mse: 12.7409 - val_mae: 1.5461 - lr: 1.6639e-04 - 3s/epoch - 3ms/step\n",
            "Score for fold 5: loss of 12.740933418273926\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-10-15 06:52:59,188]\u001b[0m Finished trial#32 resulted in value: 14.9. Current best value is 14.696000000000002 with parameters: {'activation': 'relu', 'num_hidden_layer': 3, 'i': 8, 'learning_rate': 0.0008854984178603543}.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 7s - loss: 13.0718 - mse: 13.0718 - mae: 1.6009 - val_loss: 24.5522 - val_mse: 24.5522 - val_mae: 1.7254 - lr: 8.5986e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 6s - loss: 12.7976 - mse: 12.7976 - mae: 1.5795 - val_loss: 24.7507 - val_mse: 24.7507 - val_mae: 1.5851 - lr: 8.5986e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 6s - loss: 12.6746 - mse: 12.6746 - mae: 1.5759 - val_loss: 24.6294 - val_mse: 24.6294 - val_mae: 1.6145 - lr: 8.5986e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 6s - loss: 12.6323 - mse: 12.6323 - mae: 1.5659 - val_loss: 24.3931 - val_mse: 24.3931 - val_mae: 1.7328 - lr: 8.5986e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 6s - loss: 12.6717 - mse: 12.6717 - mae: 1.5668 - val_loss: 24.3533 - val_mse: 24.3533 - val_mae: 1.6634 - lr: 8.5986e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 6s - loss: 12.6124 - mse: 12.6124 - mae: 1.5640 - val_loss: 24.6683 - val_mse: 24.6683 - val_mae: 1.6276 - lr: 8.5986e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 6s - loss: 12.6194 - mse: 12.6194 - mae: 1.5610 - val_loss: 24.4845 - val_mse: 24.4845 - val_mae: 1.6503 - lr: 8.5986e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 6s - loss: 12.6076 - mse: 12.6076 - mae: 1.5605 - val_loss: 24.4006 - val_mse: 24.4006 - val_mae: 1.6770 - lr: 8.5986e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 6s - loss: 12.5611 - mse: 12.5611 - mae: 1.5586 - val_loss: 24.4708 - val_mse: 24.4708 - val_mae: 1.6603 - lr: 8.5986e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 6s - loss: 12.5679 - mse: 12.5679 - mae: 1.5592 - val_loss: 24.5230 - val_mse: 24.5230 - val_mae: 1.6536 - lr: 8.5986e-04 - 6s/epoch - 6ms/step\n",
            "Score for fold 1: loss of 24.523000717163086\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 6s - loss: 15.7671 - mse: 15.7671 - mae: 1.5912 - val_loss: 11.5870 - val_mse: 11.5870 - val_mae: 1.6242 - lr: 8.5986e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 6s - loss: 15.6708 - mse: 15.6708 - mae: 1.5885 - val_loss: 11.5730 - val_mse: 11.5730 - val_mae: 1.5049 - lr: 8.5986e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 6s - loss: 15.6301 - mse: 15.6301 - mae: 1.5836 - val_loss: 11.6626 - val_mse: 11.6626 - val_mae: 1.5929 - lr: 8.5986e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 6s - loss: 15.6751 - mse: 15.6751 - mae: 1.5887 - val_loss: 11.5645 - val_mse: 11.5645 - val_mae: 1.5259 - lr: 8.5986e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 6s - loss: 15.6303 - mse: 15.6303 - mae: 1.5907 - val_loss: 12.1956 - val_mse: 12.1956 - val_mae: 1.5014 - lr: 8.5986e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 6s - loss: 15.6114 - mse: 15.6114 - mae: 1.5888 - val_loss: 11.5833 - val_mse: 11.5833 - val_mae: 1.5869 - lr: 8.5986e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 6s - loss: 15.5885 - mse: 15.5885 - mae: 1.5848 - val_loss: 11.5475 - val_mse: 11.5475 - val_mae: 1.5685 - lr: 8.5986e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 6s - loss: 15.5547 - mse: 15.5547 - mae: 1.5801 - val_loss: 11.5481 - val_mse: 11.5481 - val_mae: 1.5594 - lr: 8.5986e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 8s - loss: 15.5826 - mse: 15.5826 - mae: 1.5837 - val_loss: 11.7163 - val_mse: 11.7163 - val_mae: 1.5217 - lr: 8.5986e-04 - 8s/epoch - 8ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 6s - loss: 15.5768 - mse: 15.5768 - mae: 1.5804 - val_loss: 11.6299 - val_mse: 11.6299 - val_mae: 1.5347 - lr: 8.5986e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 6s - loss: 15.4923 - mse: 15.4923 - mae: 1.5796 - val_loss: 12.0088 - val_mse: 12.0088 - val_mae: 1.5269 - lr: 8.5986e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 6s - loss: 15.5676 - mse: 15.5676 - mae: 1.5791 - val_loss: 11.7035 - val_mse: 11.7035 - val_mae: 1.5657 - lr: 8.5986e-04 - 6s/epoch - 6ms/step\n",
            "Score for fold 2: loss of 11.703523635864258\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 6s - loss: 15.6224 - mse: 15.6224 - mae: 1.5766 - val_loss: 11.2944 - val_mse: 11.2944 - val_mae: 1.5036 - lr: 8.5986e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 6s - loss: 15.5788 - mse: 15.5788 - mae: 1.5768 - val_loss: 11.4877 - val_mse: 11.4877 - val_mae: 1.5583 - lr: 8.5986e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 6s - loss: 15.4884 - mse: 15.4884 - mae: 1.5743 - val_loss: 11.3078 - val_mse: 11.3078 - val_mae: 1.5594 - lr: 8.5986e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 6s - loss: 15.4640 - mse: 15.4640 - mae: 1.5713 - val_loss: 11.2992 - val_mse: 11.2992 - val_mae: 1.5359 - lr: 8.5986e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 6s - loss: 15.4947 - mse: 15.4947 - mae: 1.5714 - val_loss: 11.6627 - val_mse: 11.6627 - val_mae: 1.5535 - lr: 8.5986e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 6s - loss: 15.5620 - mse: 15.5620 - mae: 1.5737 - val_loss: 11.3181 - val_mse: 11.3181 - val_mae: 1.5476 - lr: 8.5986e-04 - 6s/epoch - 6ms/step\n",
            "Score for fold 3: loss of 11.318061828613281\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 6s - loss: 15.0930 - mse: 15.0930 - mae: 1.5643 - val_loss: 13.2088 - val_mse: 13.2088 - val_mae: 1.5225 - lr: 8.5986e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 6s - loss: 15.0357 - mse: 15.0357 - mae: 1.5626 - val_loss: 13.5439 - val_mse: 13.5439 - val_mae: 1.5395 - lr: 8.5986e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 7s - loss: 14.9616 - mse: 14.9616 - mae: 1.5617 - val_loss: 13.5177 - val_mse: 13.5177 - val_mae: 1.5556 - lr: 8.5986e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 6s - loss: 14.8956 - mse: 14.8956 - mae: 1.5565 - val_loss: 13.4560 - val_mse: 13.4560 - val_mae: 1.5574 - lr: 8.5986e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 6s - loss: 14.9900 - mse: 14.9900 - mae: 1.5585 - val_loss: 13.2530 - val_mse: 13.2530 - val_mae: 1.5392 - lr: 8.5986e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 6s - loss: 14.9474 - mse: 14.9474 - mae: 1.5601 - val_loss: 13.3326 - val_mse: 13.3326 - val_mae: 1.5857 - lr: 8.5986e-04 - 6s/epoch - 6ms/step\n",
            "Score for fold 4: loss of 13.33262825012207\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 6s - loss: 14.9269 - mse: 14.9269 - mae: 1.5531 - val_loss: 13.1992 - val_mse: 13.1992 - val_mae: 1.5795 - lr: 8.5986e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 6s - loss: 14.8533 - mse: 14.8533 - mae: 1.5521 - val_loss: 13.0336 - val_mse: 13.0336 - val_mae: 1.5639 - lr: 8.5986e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 6s - loss: 14.7650 - mse: 14.7650 - mae: 1.5454 - val_loss: 13.0921 - val_mse: 13.0921 - val_mae: 1.6086 - lr: 8.5986e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 6s - loss: 14.8556 - mse: 14.8556 - mae: 1.5400 - val_loss: 13.3326 - val_mse: 13.3326 - val_mae: 1.6315 - lr: 8.5986e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 6s - loss: 14.6588 - mse: 14.6588 - mae: 1.5386 - val_loss: 13.4221 - val_mse: 13.4221 - val_mae: 1.5922 - lr: 8.5986e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 8s - loss: 14.6865 - mse: 14.6865 - mae: 1.5398 - val_loss: 13.3455 - val_mse: 13.3455 - val_mae: 1.6228 - lr: 8.5986e-04 - 8s/epoch - 8ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 6s - loss: 14.5635 - mse: 14.5635 - mae: 1.5343 - val_loss: 13.2630 - val_mse: 13.2630 - val_mae: 1.6089 - lr: 8.5986e-04 - 6s/epoch - 6ms/step\n",
            "Score for fold 5: loss of 13.263039588928223\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-10-15 06:58:21,373]\u001b[0m Finished trial#33 resulted in value: 14.825999999999999. Current best value is 14.696000000000002 with parameters: {'activation': 'relu', 'num_hidden_layer': 3, 'i': 8, 'learning_rate': 0.0008854984178603543}.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 7s - loss: 16.3525 - mse: 16.3525 - mae: 1.6209 - val_loss: 11.6029 - val_mse: 11.6029 - val_mae: 1.5352 - lr: 8.2766e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 6s - loss: 16.0442 - mse: 16.0442 - mae: 1.6078 - val_loss: 11.4114 - val_mse: 11.4114 - val_mae: 1.5636 - lr: 8.2766e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 6s - loss: 16.0138 - mse: 16.0138 - mae: 1.5943 - val_loss: 11.5031 - val_mse: 11.5031 - val_mae: 1.5398 - lr: 8.2766e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 6s - loss: 15.9801 - mse: 15.9801 - mae: 1.5916 - val_loss: 11.2665 - val_mse: 11.2665 - val_mae: 1.5648 - lr: 8.2766e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 6s - loss: 15.9237 - mse: 15.9237 - mae: 1.5922 - val_loss: 11.3880 - val_mse: 11.3880 - val_mae: 1.5635 - lr: 8.2766e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 6s - loss: 15.9661 - mse: 15.9661 - mae: 1.5877 - val_loss: 11.5325 - val_mse: 11.5325 - val_mae: 1.5048 - lr: 8.2766e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 6s - loss: 15.9204 - mse: 15.9204 - mae: 1.5854 - val_loss: 11.4601 - val_mse: 11.4601 - val_mae: 1.5477 - lr: 8.2766e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 6s - loss: 15.9433 - mse: 15.9433 - mae: 1.5823 - val_loss: 11.3297 - val_mse: 11.3297 - val_mae: 1.6215 - lr: 8.2766e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 6s - loss: 15.8431 - mse: 15.8431 - mae: 1.5796 - val_loss: 11.2107 - val_mse: 11.2107 - val_mae: 1.5988 - lr: 8.2766e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 6s - loss: 15.9067 - mse: 15.9067 - mae: 1.5772 - val_loss: 11.6587 - val_mse: 11.6587 - val_mae: 1.5303 - lr: 8.2766e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 6s - loss: 15.8729 - mse: 15.8729 - mae: 1.5792 - val_loss: 11.2539 - val_mse: 11.2539 - val_mae: 1.5655 - lr: 8.2766e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 6s - loss: 15.7872 - mse: 15.7872 - mae: 1.5786 - val_loss: 11.2891 - val_mse: 11.2891 - val_mae: 1.5800 - lr: 8.2766e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 13/100\n",
            "1000/1000 - 6s - loss: 15.6597 - mse: 15.6597 - mae: 1.5762 - val_loss: 11.3968 - val_mse: 11.3968 - val_mae: 1.5813 - lr: 8.2766e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 14/100\n",
            "1000/1000 - 6s - loss: 15.6670 - mse: 15.6670 - mae: 1.5735 - val_loss: 11.4794 - val_mse: 11.4794 - val_mae: 1.5707 - lr: 8.2766e-04 - 6s/epoch - 6ms/step\n",
            "Score for fold 1: loss of 11.479384422302246\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 6s - loss: 15.9894 - mse: 15.9894 - mae: 1.5779 - val_loss: 10.2135 - val_mse: 10.2135 - val_mae: 1.5430 - lr: 8.2766e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 6s - loss: 15.9539 - mse: 15.9539 - mae: 1.5785 - val_loss: 10.1570 - val_mse: 10.1570 - val_mae: 1.6101 - lr: 8.2766e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 6s - loss: 15.9152 - mse: 15.9152 - mae: 1.5778 - val_loss: 10.2953 - val_mse: 10.2953 - val_mae: 1.5565 - lr: 8.2766e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 6s - loss: 15.9912 - mse: 15.9912 - mae: 1.5730 - val_loss: 10.3134 - val_mse: 10.3134 - val_mae: 1.5635 - lr: 8.2766e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 6s - loss: 15.9654 - mse: 15.9654 - mae: 1.5695 - val_loss: 10.6810 - val_mse: 10.6810 - val_mae: 1.5971 - lr: 8.2766e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 6s - loss: 15.8626 - mse: 15.8626 - mae: 1.5740 - val_loss: 10.4289 - val_mse: 10.4289 - val_mae: 1.5189 - lr: 8.2766e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 6s - loss: 15.8142 - mse: 15.8142 - mae: 1.5684 - val_loss: 10.4100 - val_mse: 10.4100 - val_mae: 1.5413 - lr: 8.2766e-04 - 6s/epoch - 6ms/step\n",
            "Score for fold 2: loss of 10.410005569458008\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 6s - loss: 14.1417 - mse: 14.1417 - mae: 1.5686 - val_loss: 16.9726 - val_mse: 16.9726 - val_mae: 1.5923 - lr: 8.2766e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 6s - loss: 14.1486 - mse: 14.1486 - mae: 1.5618 - val_loss: 17.2721 - val_mse: 17.2721 - val_mae: 1.6184 - lr: 8.2766e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 6s - loss: 14.0921 - mse: 14.0921 - mae: 1.5620 - val_loss: 17.4304 - val_mse: 17.4304 - val_mae: 1.5489 - lr: 8.2766e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 6s - loss: 13.9956 - mse: 13.9956 - mae: 1.5562 - val_loss: 16.9757 - val_mse: 16.9757 - val_mae: 1.5807 - lr: 8.2766e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 6s - loss: 13.9914 - mse: 13.9914 - mae: 1.5528 - val_loss: 16.7589 - val_mse: 16.7589 - val_mae: 1.5836 - lr: 8.2766e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 6s - loss: 13.8935 - mse: 13.8935 - mae: 1.5514 - val_loss: 17.3992 - val_mse: 17.3992 - val_mae: 1.6079 - lr: 8.2766e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 6s - loss: 13.8837 - mse: 13.8837 - mae: 1.5510 - val_loss: 17.6034 - val_mse: 17.6034 - val_mae: 1.5695 - lr: 8.2766e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 8s - loss: 13.9379 - mse: 13.9379 - mae: 1.5473 - val_loss: 17.4471 - val_mse: 17.4471 - val_mae: 1.5620 - lr: 8.2766e-04 - 8s/epoch - 8ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 6s - loss: 13.8846 - mse: 13.8846 - mae: 1.5485 - val_loss: 17.0008 - val_mse: 17.0008 - val_mae: 1.5418 - lr: 8.2766e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 6s - loss: 13.8362 - mse: 13.8362 - mae: 1.5493 - val_loss: 17.2174 - val_mse: 17.2174 - val_mae: 1.6110 - lr: 8.2766e-04 - 6s/epoch - 6ms/step\n",
            "Score for fold 3: loss of 17.217430114746094\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 6s - loss: 15.2625 - mse: 15.2625 - mae: 1.5616 - val_loss: 11.6054 - val_mse: 11.6054 - val_mae: 1.5284 - lr: 8.2766e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 6s - loss: 15.2785 - mse: 15.2785 - mae: 1.5605 - val_loss: 12.0486 - val_mse: 12.0486 - val_mae: 1.5390 - lr: 8.2766e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 6s - loss: 15.2951 - mse: 15.2951 - mae: 1.5566 - val_loss: 11.4295 - val_mse: 11.4295 - val_mae: 1.5779 - lr: 8.2766e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 6s - loss: 15.1801 - mse: 15.1801 - mae: 1.5558 - val_loss: 11.7997 - val_mse: 11.7997 - val_mae: 1.5426 - lr: 8.2766e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 6s - loss: 15.1371 - mse: 15.1371 - mae: 1.5547 - val_loss: 11.5383 - val_mse: 11.5383 - val_mae: 1.5856 - lr: 8.2766e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 6s - loss: 15.2064 - mse: 15.2064 - mae: 1.5533 - val_loss: 11.7642 - val_mse: 11.7642 - val_mae: 1.5317 - lr: 8.2766e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 6s - loss: 15.1010 - mse: 15.1010 - mae: 1.5495 - val_loss: 11.6989 - val_mse: 11.6989 - val_mae: 1.5719 - lr: 8.2766e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 6s - loss: 15.0391 - mse: 15.0391 - mae: 1.5509 - val_loss: 11.7668 - val_mse: 11.7668 - val_mae: 1.5431 - lr: 8.2766e-04 - 6s/epoch - 6ms/step\n",
            "Score for fold 4: loss of 11.766775131225586\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 6s - loss: 12.1561 - mse: 12.1561 - mae: 1.5447 - val_loss: 23.1537 - val_mse: 23.1537 - val_mae: 1.5683 - lr: 8.2766e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 6s - loss: 12.2214 - mse: 12.2214 - mae: 1.5412 - val_loss: 23.1348 - val_mse: 23.1348 - val_mae: 1.5988 - lr: 8.2766e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 6s - loss: 12.1477 - mse: 12.1477 - mae: 1.5420 - val_loss: 23.3897 - val_mse: 23.3897 - val_mae: 1.5595 - lr: 8.2766e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 6s - loss: 12.1338 - mse: 12.1338 - mae: 1.5368 - val_loss: 23.2823 - val_mse: 23.2823 - val_mae: 1.5835 - lr: 8.2766e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 6s - loss: 11.9968 - mse: 11.9968 - mae: 1.5344 - val_loss: 23.2364 - val_mse: 23.2364 - val_mae: 1.6306 - lr: 8.2766e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 7s - loss: 11.9831 - mse: 11.9831 - mae: 1.5328 - val_loss: 23.3699 - val_mse: 23.3699 - val_mae: 1.6056 - lr: 8.2766e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 7s - loss: 11.8566 - mse: 11.8566 - mae: 1.5287 - val_loss: 23.3105 - val_mse: 23.3105 - val_mae: 1.5896 - lr: 8.2766e-04 - 7s/epoch - 7ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-10-15 07:05:16,889]\u001b[0m Finished trial#34 resulted in value: 14.838. Current best value is 14.696000000000002 with parameters: {'activation': 'relu', 'num_hidden_layer': 3, 'i': 8, 'learning_rate': 0.0008854984178603543}.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Score for fold 5: loss of 23.310508728027344\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 7s - loss: 15.9691 - mse: 15.9691 - mae: 1.6130 - val_loss: 13.4020 - val_mse: 13.4020 - val_mae: 1.5613 - lr: 4.0971e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 6s - loss: 15.6013 - mse: 15.6013 - mae: 1.5942 - val_loss: 13.3839 - val_mse: 13.3839 - val_mae: 1.5416 - lr: 4.0971e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 6s - loss: 15.5146 - mse: 15.5146 - mae: 1.5905 - val_loss: 13.1765 - val_mse: 13.1765 - val_mae: 1.5895 - lr: 4.0971e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 6s - loss: 15.4442 - mse: 15.4442 - mae: 1.5876 - val_loss: 13.4257 - val_mse: 13.4257 - val_mae: 1.5784 - lr: 4.0971e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 6s - loss: 15.4614 - mse: 15.4614 - mae: 1.5876 - val_loss: 13.2414 - val_mse: 13.2414 - val_mae: 1.5029 - lr: 4.0971e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 6s - loss: 15.4106 - mse: 15.4106 - mae: 1.5795 - val_loss: 13.0496 - val_mse: 13.0496 - val_mae: 1.6059 - lr: 4.0971e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 6s - loss: 15.3581 - mse: 15.3581 - mae: 1.5763 - val_loss: 12.9613 - val_mse: 12.9613 - val_mae: 1.6281 - lr: 4.0971e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 6s - loss: 15.3918 - mse: 15.3918 - mae: 1.5740 - val_loss: 13.2553 - val_mse: 13.2553 - val_mae: 1.5685 - lr: 4.0971e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 6s - loss: 15.3295 - mse: 15.3295 - mae: 1.5740 - val_loss: 13.4316 - val_mse: 13.4316 - val_mae: 1.5122 - lr: 4.0971e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 6s - loss: 15.3247 - mse: 15.3247 - mae: 1.5715 - val_loss: 13.2779 - val_mse: 13.2779 - val_mae: 1.5373 - lr: 4.0971e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 6s - loss: 15.2415 - mse: 15.2415 - mae: 1.5722 - val_loss: 13.1303 - val_mse: 13.1303 - val_mae: 1.5317 - lr: 4.0971e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 6s - loss: 15.2708 - mse: 15.2708 - mae: 1.5704 - val_loss: 13.2320 - val_mse: 13.2320 - val_mae: 1.5612 - lr: 4.0971e-04 - 6s/epoch - 6ms/step\n",
            "Score for fold 1: loss of 13.232032775878906\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 6s - loss: 15.1270 - mse: 15.1270 - mae: 1.5733 - val_loss: 13.4348 - val_mse: 13.4348 - val_mae: 1.5606 - lr: 4.0971e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 8s - loss: 15.1120 - mse: 15.1120 - mae: 1.5722 - val_loss: 13.6612 - val_mse: 13.6612 - val_mae: 1.4883 - lr: 4.0971e-04 - 8s/epoch - 8ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 6s - loss: 15.0490 - mse: 15.0490 - mae: 1.5708 - val_loss: 13.5070 - val_mse: 13.5070 - val_mae: 1.5593 - lr: 4.0971e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 6s - loss: 14.9975 - mse: 14.9975 - mae: 1.5728 - val_loss: 13.7314 - val_mse: 13.7314 - val_mae: 1.4711 - lr: 4.0971e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 6s - loss: 15.0225 - mse: 15.0225 - mae: 1.5687 - val_loss: 13.4778 - val_mse: 13.4778 - val_mae: 1.5709 - lr: 4.0971e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 6s - loss: 14.9939 - mse: 14.9939 - mae: 1.5689 - val_loss: 13.5657 - val_mse: 13.5657 - val_mae: 1.5486 - lr: 4.0971e-04 - 6s/epoch - 6ms/step\n",
            "Score for fold 2: loss of 13.565648078918457\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 6s - loss: 12.8240 - mse: 12.8240 - mae: 1.5685 - val_loss: 22.3192 - val_mse: 22.3192 - val_mae: 1.5729 - lr: 4.0971e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 6s - loss: 12.8696 - mse: 12.8696 - mae: 1.5639 - val_loss: 22.2432 - val_mse: 22.2432 - val_mae: 1.6239 - lr: 4.0971e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 6s - loss: 12.7768 - mse: 12.7768 - mae: 1.5645 - val_loss: 22.7089 - val_mse: 22.7089 - val_mae: 1.5639 - lr: 4.0971e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 6s - loss: 12.7442 - mse: 12.7442 - mae: 1.5610 - val_loss: 22.2566 - val_mse: 22.2566 - val_mae: 1.6187 - lr: 4.0971e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 6s - loss: 12.6947 - mse: 12.6947 - mae: 1.5580 - val_loss: 22.7504 - val_mse: 22.7504 - val_mae: 1.6782 - lr: 4.0971e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 6s - loss: 12.7278 - mse: 12.7278 - mae: 1.5608 - val_loss: 22.8746 - val_mse: 22.8746 - val_mae: 1.5450 - lr: 4.0971e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 6s - loss: 12.7134 - mse: 12.7134 - mae: 1.5523 - val_loss: 22.7512 - val_mse: 22.7512 - val_mae: 1.5920 - lr: 4.0971e-04 - 6s/epoch - 6ms/step\n",
            "Score for fold 3: loss of 22.751203536987305\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 6s - loss: 15.1816 - mse: 15.1816 - mae: 1.5609 - val_loss: 12.6054 - val_mse: 12.6054 - val_mae: 1.5724 - lr: 4.0971e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 6s - loss: 15.0508 - mse: 15.0508 - mae: 1.5546 - val_loss: 12.6395 - val_mse: 12.6395 - val_mae: 1.5664 - lr: 4.0971e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 6s - loss: 15.0352 - mse: 15.0352 - mae: 1.5544 - val_loss: 12.6897 - val_mse: 12.6897 - val_mae: 1.5442 - lr: 4.0971e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 6s - loss: 15.0612 - mse: 15.0612 - mae: 1.5539 - val_loss: 12.8162 - val_mse: 12.8162 - val_mae: 1.5965 - lr: 4.0971e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 6s - loss: 14.9809 - mse: 14.9809 - mae: 1.5511 - val_loss: 12.9311 - val_mse: 12.9311 - val_mae: 1.6833 - lr: 4.0971e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 6s - loss: 14.8300 - mse: 14.8300 - mae: 1.5482 - val_loss: 12.7472 - val_mse: 12.7472 - val_mae: 1.5872 - lr: 4.0971e-04 - 6s/epoch - 6ms/step\n",
            "Score for fold 4: loss of 12.74721622467041\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 6s - loss: 15.2101 - mse: 15.2101 - mae: 1.5500 - val_loss: 12.0122 - val_mse: 12.0122 - val_mae: 1.6135 - lr: 4.0971e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 6s - loss: 15.0959 - mse: 15.0959 - mae: 1.5466 - val_loss: 12.2543 - val_mse: 12.2543 - val_mae: 1.5441 - lr: 4.0971e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 6s - loss: 15.0765 - mse: 15.0765 - mae: 1.5431 - val_loss: 12.1220 - val_mse: 12.1220 - val_mae: 1.5901 - lr: 4.0971e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 6s - loss: 15.1674 - mse: 15.1674 - mae: 1.5413 - val_loss: 12.0957 - val_mse: 12.0957 - val_mae: 1.5789 - lr: 4.0971e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 8s - loss: 15.1549 - mse: 15.1549 - mae: 1.5383 - val_loss: 11.9958 - val_mse: 11.9958 - val_mae: 1.6102 - lr: 4.0971e-04 - 8s/epoch - 8ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 6s - loss: 15.0417 - mse: 15.0417 - mae: 1.5363 - val_loss: 12.0559 - val_mse: 12.0559 - val_mae: 1.5689 - lr: 4.0971e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 6s - loss: 14.9852 - mse: 14.9852 - mae: 1.5349 - val_loss: 11.8658 - val_mse: 11.8658 - val_mae: 1.5966 - lr: 4.0971e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 6s - loss: 14.9329 - mse: 14.9329 - mae: 1.5314 - val_loss: 12.0949 - val_mse: 12.0949 - val_mae: 1.6026 - lr: 4.0971e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 6s - loss: 14.8975 - mse: 14.8975 - mae: 1.5286 - val_loss: 12.1180 - val_mse: 12.1180 - val_mae: 1.5761 - lr: 4.0971e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 6s - loss: 14.9008 - mse: 14.9008 - mae: 1.5277 - val_loss: 12.1778 - val_mse: 12.1778 - val_mae: 1.5446 - lr: 4.0971e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 6s - loss: 14.8331 - mse: 14.8331 - mae: 1.5258 - val_loss: 12.1773 - val_mse: 12.1773 - val_mae: 1.6140 - lr: 4.0971e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 6s - loss: 14.7492 - mse: 14.7492 - mae: 1.5221 - val_loss: 12.2312 - val_mse: 12.2312 - val_mae: 1.5950 - lr: 4.0971e-04 - 6s/epoch - 6ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-10-15 07:10:06,021]\u001b[0m Finished trial#35 resulted in value: 14.906. Current best value is 14.696000000000002 with parameters: {'activation': 'relu', 'num_hidden_layer': 3, 'i': 8, 'learning_rate': 0.0008854984178603543}.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Score for fold 5: loss of 12.23116397857666\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 7s - loss: 16.1530 - mse: 16.1530 - mae: 1.6232 - val_loss: 12.3868 - val_mse: 12.3868 - val_mae: 1.5659 - lr: 0.0016 - 7s/epoch - 7ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 6s - loss: 15.9275 - mse: 15.9275 - mae: 1.6028 - val_loss: 12.1820 - val_mse: 12.1820 - val_mae: 1.5225 - lr: 0.0016 - 6s/epoch - 6ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 6s - loss: 15.8771 - mse: 15.8771 - mae: 1.5919 - val_loss: 12.0782 - val_mse: 12.0782 - val_mae: 1.5964 - lr: 0.0016 - 6s/epoch - 6ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 6s - loss: 15.7552 - mse: 15.7552 - mae: 1.5846 - val_loss: 12.0151 - val_mse: 12.0151 - val_mae: 1.6096 - lr: 0.0016 - 6s/epoch - 6ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 6s - loss: 15.7927 - mse: 15.7927 - mae: 1.5886 - val_loss: 12.0539 - val_mse: 12.0539 - val_mae: 1.6761 - lr: 0.0016 - 6s/epoch - 6ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 6s - loss: 15.8006 - mse: 15.8006 - mae: 1.5854 - val_loss: 12.0426 - val_mse: 12.0426 - val_mae: 1.5423 - lr: 0.0016 - 6s/epoch - 6ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 6s - loss: 15.7888 - mse: 15.7888 - mae: 1.5785 - val_loss: 11.8999 - val_mse: 11.8999 - val_mae: 1.6289 - lr: 0.0016 - 6s/epoch - 6ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 6s - loss: 15.8319 - mse: 15.8319 - mae: 1.5850 - val_loss: 12.2294 - val_mse: 12.2294 - val_mae: 1.5425 - lr: 0.0016 - 6s/epoch - 6ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 6s - loss: 15.6145 - mse: 15.6145 - mae: 1.5780 - val_loss: 12.3103 - val_mse: 12.3103 - val_mae: 1.5799 - lr: 0.0016 - 6s/epoch - 6ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 6s - loss: 15.7206 - mse: 15.7206 - mae: 1.5870 - val_loss: 12.0333 - val_mse: 12.0333 - val_mae: 1.5506 - lr: 0.0016 - 6s/epoch - 6ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 6s - loss: 15.6481 - mse: 15.6481 - mae: 1.5758 - val_loss: 11.8800 - val_mse: 11.8800 - val_mae: 1.5535 - lr: 0.0016 - 6s/epoch - 6ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 6s - loss: 15.6481 - mse: 15.6481 - mae: 1.5745 - val_loss: 11.9185 - val_mse: 11.9185 - val_mae: 1.5647 - lr: 0.0016 - 6s/epoch - 6ms/step\n",
            "Epoch 13/100\n",
            "1000/1000 - 6s - loss: 15.6303 - mse: 15.6303 - mae: 1.5715 - val_loss: 11.9166 - val_mse: 11.9166 - val_mae: 1.6081 - lr: 0.0016 - 6s/epoch - 6ms/step\n",
            "Epoch 14/100\n",
            "1000/1000 - 8s - loss: 15.5405 - mse: 15.5405 - mae: 1.5659 - val_loss: 11.9294 - val_mse: 11.9294 - val_mae: 1.5738 - lr: 0.0016 - 8s/epoch - 8ms/step\n",
            "Epoch 15/100\n",
            "1000/1000 - 6s - loss: 15.4765 - mse: 15.4765 - mae: 1.5642 - val_loss: 11.7421 - val_mse: 11.7421 - val_mae: 1.6195 - lr: 0.0016 - 6s/epoch - 6ms/step\n",
            "Epoch 16/100\n",
            "1000/1000 - 6s - loss: 15.5152 - mse: 15.5152 - mae: 1.5643 - val_loss: 11.8690 - val_mse: 11.8690 - val_mae: 1.5880 - lr: 0.0016 - 6s/epoch - 6ms/step\n",
            "Epoch 17/100\n",
            "1000/1000 - 6s - loss: 15.4447 - mse: 15.4447 - mae: 1.5594 - val_loss: 11.7803 - val_mse: 11.7803 - val_mae: 1.6197 - lr: 0.0016 - 6s/epoch - 6ms/step\n",
            "Epoch 18/100\n",
            "1000/1000 - 6s - loss: 15.5918 - mse: 15.5918 - mae: 1.5639 - val_loss: 11.8867 - val_mse: 11.8867 - val_mae: 1.5790 - lr: 0.0016 - 6s/epoch - 6ms/step\n",
            "Epoch 19/100\n",
            "1000/1000 - 6s - loss: 15.5221 - mse: 15.5221 - mae: 1.5602 - val_loss: 11.7919 - val_mse: 11.7919 - val_mae: 1.5678 - lr: 0.0016 - 6s/epoch - 6ms/step\n",
            "Epoch 20/100\n",
            "1000/1000 - 6s - loss: 15.3630 - mse: 15.3630 - mae: 1.5615 - val_loss: 11.7605 - val_mse: 11.7605 - val_mae: 1.6062 - lr: 0.0016 - 6s/epoch - 6ms/step\n",
            "Score for fold 1: loss of 11.760490417480469\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 6s - loss: 12.4781 - mse: 12.4781 - mae: 1.5527 - val_loss: 23.2330 - val_mse: 23.2330 - val_mae: 1.5940 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 6s - loss: 12.3914 - mse: 12.3914 - mae: 1.5507 - val_loss: 23.6498 - val_mse: 23.6498 - val_mae: 1.6046 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 6s - loss: 12.3467 - mse: 12.3467 - mae: 1.5470 - val_loss: 23.4790 - val_mse: 23.4790 - val_mae: 1.5858 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 6s - loss: 12.3572 - mse: 12.3572 - mae: 1.5462 - val_loss: 23.2251 - val_mse: 23.2251 - val_mae: 1.6452 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 6s - loss: 12.2999 - mse: 12.2999 - mae: 1.5456 - val_loss: 23.4335 - val_mse: 23.4335 - val_mae: 1.6118 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 6s - loss: 12.1494 - mse: 12.1494 - mae: 1.5428 - val_loss: 23.7225 - val_mse: 23.7225 - val_mae: 1.6050 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 6s - loss: 12.1829 - mse: 12.1829 - mae: 1.5410 - val_loss: 23.6721 - val_mse: 23.6721 - val_mae: 1.6358 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 6s - loss: 12.0598 - mse: 12.0598 - mae: 1.5378 - val_loss: 24.0345 - val_mse: 24.0345 - val_mae: 1.6667 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 6s - loss: 12.1065 - mse: 12.1065 - mae: 1.5381 - val_loss: 23.6659 - val_mse: 23.6659 - val_mae: 1.6074 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
            "Score for fold 2: loss of 23.6658992767334\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 6s - loss: 15.9349 - mse: 15.9349 - mae: 1.5622 - val_loss: 8.5822 - val_mse: 8.5822 - val_mae: 1.5176 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 6s - loss: 15.8014 - mse: 15.8014 - mae: 1.5631 - val_loss: 8.7286 - val_mse: 8.7286 - val_mae: 1.5576 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 8s - loss: 15.7333 - mse: 15.7333 - mae: 1.5625 - val_loss: 8.9840 - val_mse: 8.9840 - val_mae: 1.4753 - lr: 0.0010 - 8s/epoch - 8ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 6s - loss: 15.5975 - mse: 15.5975 - mae: 1.5573 - val_loss: 9.1069 - val_mse: 9.1069 - val_mae: 1.5238 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 6s - loss: 15.5887 - mse: 15.5887 - mae: 1.5509 - val_loss: 9.1357 - val_mse: 9.1357 - val_mae: 1.4848 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 6s - loss: 15.5393 - mse: 15.5393 - mae: 1.5520 - val_loss: 8.8415 - val_mse: 8.8415 - val_mae: 1.5229 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
            "Score for fold 3: loss of 8.841540336608887\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 6s - loss: 13.7572 - mse: 13.7572 - mae: 1.5419 - val_loss: 16.4302 - val_mse: 16.4302 - val_mae: 1.5067 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 6s - loss: 13.6450 - mse: 13.6450 - mae: 1.5331 - val_loss: 16.5440 - val_mse: 16.5440 - val_mae: 1.5388 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 6s - loss: 13.6641 - mse: 13.6641 - mae: 1.5364 - val_loss: 16.7523 - val_mse: 16.7523 - val_mae: 1.5626 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 6s - loss: 13.5025 - mse: 13.5025 - mae: 1.5327 - val_loss: 16.5376 - val_mse: 16.5376 - val_mae: 1.5935 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 6s - loss: 13.5586 - mse: 13.5586 - mae: 1.5332 - val_loss: 16.7680 - val_mse: 16.7680 - val_mae: 1.5759 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 6s - loss: 13.4502 - mse: 13.4502 - mae: 1.5300 - val_loss: 16.5137 - val_mse: 16.5137 - val_mae: 1.5641 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
            "Score for fold 4: loss of 16.513689041137695\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 6s - loss: 14.7395 - mse: 14.7395 - mae: 1.5424 - val_loss: 12.0810 - val_mse: 12.0810 - val_mae: 1.4668 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 6s - loss: 14.7488 - mse: 14.7488 - mae: 1.5359 - val_loss: 12.2704 - val_mse: 12.2704 - val_mae: 1.4970 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 6s - loss: 14.5918 - mse: 14.5918 - mae: 1.5311 - val_loss: 11.2453 - val_mse: 11.2453 - val_mae: 1.5578 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 6s - loss: 14.6049 - mse: 14.6049 - mae: 1.5328 - val_loss: 11.8435 - val_mse: 11.8435 - val_mae: 1.4917 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 6s - loss: 14.5554 - mse: 14.5554 - mae: 1.5292 - val_loss: 11.4468 - val_mse: 11.4468 - val_mae: 1.5158 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 6s - loss: 14.3486 - mse: 14.3486 - mae: 1.5277 - val_loss: 11.5366 - val_mse: 11.5366 - val_mae: 1.5658 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 6s - loss: 14.4393 - mse: 14.4393 - mae: 1.5235 - val_loss: 11.8355 - val_mse: 11.8355 - val_mae: 1.5192 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 6s - loss: 14.3210 - mse: 14.3210 - mae: 1.5208 - val_loss: 11.9798 - val_mse: 11.9798 - val_mae: 1.5817 - lr: 0.0010 - 6s/epoch - 6ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-10-15 07:16:11,177]\u001b[0m Finished trial#36 resulted in value: 14.552000000000001. Current best value is 14.552000000000001 with parameters: {'activation': 'relu', 'num_hidden_layer': 3, 'i': 9, 'learning_rate': 0.001648783292938752}.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Score for fold 5: loss of 11.97977066040039\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 3s - loss: 12.9586 - mse: 12.9586 - mae: 1.6068 - val_loss: 25.2716 - val_mse: 25.2716 - val_mae: 1.6128 - lr: 0.0017 - 3s/epoch - 3ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 12.6767 - mse: 12.6767 - mae: 1.6033 - val_loss: 25.2780 - val_mse: 25.2780 - val_mae: 1.6480 - lr: 0.0017 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 12.6362 - mse: 12.6362 - mae: 1.5938 - val_loss: 25.2133 - val_mse: 25.2133 - val_mae: 1.6958 - lr: 0.0017 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 12.6012 - mse: 12.6012 - mae: 1.5977 - val_loss: 25.2742 - val_mse: 25.2742 - val_mae: 1.5877 - lr: 0.0017 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 12.5902 - mse: 12.5902 - mae: 1.5938 - val_loss: 25.1269 - val_mse: 25.1269 - val_mae: 1.6554 - lr: 0.0017 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 12.5501 - mse: 12.5501 - mae: 1.5930 - val_loss: 25.2985 - val_mse: 25.2985 - val_mae: 1.6442 - lr: 0.0017 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 12.5556 - mse: 12.5556 - mae: 1.5873 - val_loss: 25.1218 - val_mse: 25.1218 - val_mae: 1.6769 - lr: 0.0017 - 2s/epoch - 2ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 2s - loss: 12.5494 - mse: 12.5494 - mae: 1.5883 - val_loss: 25.1155 - val_mse: 25.1155 - val_mae: 1.6624 - lr: 0.0017 - 2s/epoch - 2ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 2s - loss: 12.5404 - mse: 12.5404 - mae: 1.5852 - val_loss: 25.1809 - val_mse: 25.1809 - val_mae: 1.5903 - lr: 0.0017 - 2s/epoch - 2ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 2s - loss: 12.5295 - mse: 12.5295 - mae: 1.5851 - val_loss: 25.2254 - val_mse: 25.2254 - val_mae: 1.5542 - lr: 0.0017 - 2s/epoch - 2ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 2s - loss: 12.4509 - mse: 12.4509 - mae: 1.5853 - val_loss: 25.1209 - val_mse: 25.1209 - val_mae: 1.5800 - lr: 0.0017 - 2s/epoch - 2ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 2s - loss: 12.4737 - mse: 12.4737 - mae: 1.5815 - val_loss: 25.0901 - val_mse: 25.0901 - val_mae: 1.5993 - lr: 0.0017 - 2s/epoch - 2ms/step\n",
            "Epoch 13/100\n",
            "1000/1000 - 2s - loss: 12.4785 - mse: 12.4785 - mae: 1.5819 - val_loss: 25.0655 - val_mse: 25.0655 - val_mae: 1.6510 - lr: 0.0017 - 2s/epoch - 2ms/step\n",
            "Epoch 14/100\n",
            "1000/1000 - 2s - loss: 12.4469 - mse: 12.4469 - mae: 1.5831 - val_loss: 25.0901 - val_mse: 25.0901 - val_mae: 1.6081 - lr: 0.0017 - 2s/epoch - 2ms/step\n",
            "Epoch 15/100\n",
            "1000/1000 - 2s - loss: 12.4265 - mse: 12.4265 - mae: 1.5845 - val_loss: 25.0461 - val_mse: 25.0461 - val_mae: 1.6428 - lr: 0.0017 - 2s/epoch - 2ms/step\n",
            "Epoch 16/100\n",
            "1000/1000 - 2s - loss: 12.4681 - mse: 12.4681 - mae: 1.5814 - val_loss: 25.1798 - val_mse: 25.1798 - val_mae: 1.6490 - lr: 0.0017 - 2s/epoch - 2ms/step\n",
            "Epoch 17/100\n",
            "1000/1000 - 2s - loss: 12.4342 - mse: 12.4342 - mae: 1.5801 - val_loss: 25.0700 - val_mse: 25.0700 - val_mae: 1.6702 - lr: 0.0017 - 2s/epoch - 2ms/step\n",
            "Epoch 18/100\n",
            "1000/1000 - 2s - loss: 12.4561 - mse: 12.4561 - mae: 1.5859 - val_loss: 25.1991 - val_mse: 25.1991 - val_mae: 1.5833 - lr: 0.0017 - 2s/epoch - 2ms/step\n",
            "Epoch 19/100\n",
            "1000/1000 - 2s - loss: 12.4773 - mse: 12.4773 - mae: 1.5824 - val_loss: 25.0826 - val_mse: 25.0826 - val_mae: 1.6004 - lr: 0.0017 - 2s/epoch - 2ms/step\n",
            "Epoch 20/100\n",
            "1000/1000 - 2s - loss: 12.4510 - mse: 12.4510 - mae: 1.5801 - val_loss: 25.1153 - val_mse: 25.1153 - val_mae: 1.6202 - lr: 0.0017 - 2s/epoch - 2ms/step\n",
            "Score for fold 1: loss of 25.115283966064453\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 16.0658 - mse: 16.0658 - mae: 1.5815 - val_loss: 10.3500 - val_mse: 10.3500 - val_mae: 1.5840 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 16.0366 - mse: 16.0366 - mae: 1.5774 - val_loss: 10.3526 - val_mse: 10.3526 - val_mae: 1.5711 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 16.0183 - mse: 16.0183 - mae: 1.5785 - val_loss: 10.4600 - val_mse: 10.4600 - val_mae: 1.5798 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 15.9943 - mse: 15.9943 - mae: 1.5736 - val_loss: 10.3556 - val_mse: 10.3556 - val_mae: 1.6068 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 15.9733 - mse: 15.9733 - mae: 1.5744 - val_loss: 10.4030 - val_mse: 10.4030 - val_mae: 1.5504 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 15.9641 - mse: 15.9641 - mae: 1.5726 - val_loss: 10.5492 - val_mse: 10.5492 - val_mae: 1.5515 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Score for fold 2: loss of 10.549248695373535\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 15.4490 - mse: 15.4490 - mae: 1.5794 - val_loss: 12.4086 - val_mse: 12.4086 - val_mae: 1.6237 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 15.4060 - mse: 15.4060 - mae: 1.5802 - val_loss: 12.4270 - val_mse: 12.4270 - val_mae: 1.6496 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 15.3948 - mse: 15.3948 - mae: 1.5829 - val_loss: 12.4987 - val_mse: 12.4987 - val_mae: 1.5764 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 15.4244 - mse: 15.4244 - mae: 1.5806 - val_loss: 12.3875 - val_mse: 12.3875 - val_mae: 1.5499 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 15.3799 - mse: 15.3799 - mae: 1.5788 - val_loss: 12.5298 - val_mse: 12.5298 - val_mae: 1.5761 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 15.4051 - mse: 15.4051 - mae: 1.5781 - val_loss: 12.3948 - val_mse: 12.3948 - val_mae: 1.5916 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 15.3952 - mse: 15.3952 - mae: 1.5779 - val_loss: 12.4976 - val_mse: 12.4976 - val_mae: 1.5951 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 2s - loss: 15.3997 - mse: 15.3997 - mae: 1.5777 - val_loss: 12.4752 - val_mse: 12.4752 - val_mae: 1.5810 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 2s - loss: 15.3800 - mse: 15.3800 - mae: 1.5780 - val_loss: 12.5071 - val_mse: 12.5071 - val_mae: 1.5340 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Score for fold 3: loss of 12.507070541381836\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 14.8149 - mse: 14.8149 - mae: 1.5753 - val_loss: 14.8710 - val_mse: 14.8710 - val_mae: 1.5720 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 14.8105 - mse: 14.8105 - mae: 1.5740 - val_loss: 14.7272 - val_mse: 14.7272 - val_mae: 1.6179 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 14.7786 - mse: 14.7786 - mae: 1.5721 - val_loss: 14.8240 - val_mse: 14.8240 - val_mae: 1.6179 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 14.8023 - mse: 14.8023 - mae: 1.5733 - val_loss: 14.8617 - val_mse: 14.8617 - val_mae: 1.5613 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 14.7650 - mse: 14.7650 - mae: 1.5719 - val_loss: 14.9799 - val_mse: 14.9799 - val_mae: 1.5658 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 14.7775 - mse: 14.7775 - mae: 1.5686 - val_loss: 14.9945 - val_mse: 14.9945 - val_mae: 1.5677 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 14.7953 - mse: 14.7953 - mae: 1.5712 - val_loss: 14.8477 - val_mse: 14.8477 - val_mae: 1.5946 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Score for fold 4: loss of 14.84765625\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 15.6259 - mse: 15.6259 - mae: 1.5768 - val_loss: 11.6159 - val_mse: 11.6159 - val_mae: 1.5538 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 15.6349 - mse: 15.6349 - mae: 1.5819 - val_loss: 11.5266 - val_mse: 11.5266 - val_mae: 1.5471 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 15.6187 - mse: 15.6187 - mae: 1.5779 - val_loss: 11.4859 - val_mse: 11.4859 - val_mae: 1.5829 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 15.6173 - mse: 15.6173 - mae: 1.5790 - val_loss: 11.5623 - val_mse: 11.5623 - val_mae: 1.5515 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 15.5850 - mse: 15.5850 - mae: 1.5753 - val_loss: 11.6103 - val_mse: 11.6103 - val_mae: 1.5733 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 15.6245 - mse: 15.6245 - mae: 1.5783 - val_loss: 11.6858 - val_mse: 11.6858 - val_mae: 1.5303 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 15.6345 - mse: 15.6345 - mae: 1.5785 - val_loss: 11.5568 - val_mse: 11.5568 - val_mae: 1.5308 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 2s - loss: 15.6159 - mse: 15.6159 - mae: 1.5747 - val_loss: 11.6921 - val_mse: 11.6921 - val_mae: 1.5226 - lr: 0.0010 - 2s/epoch - 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-10-15 07:18:13,601]\u001b[0m Finished trial#37 resulted in value: 14.943999999999999. Current best value is 14.552000000000001 with parameters: {'activation': 'relu', 'num_hidden_layer': 3, 'i': 9, 'learning_rate': 0.001648783292938752}.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Score for fold 5: loss of 11.69214916229248\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 23s - loss: 15.9072 - mse: 15.9072 - mae: 1.6577 - val_loss: 14.9281 - val_mse: 14.9281 - val_mae: 1.6310 - lr: 0.0036 - 23s/epoch - 23ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 21s - loss: 15.4247 - mse: 15.4247 - mae: 1.6185 - val_loss: 14.9182 - val_mse: 14.9182 - val_mae: 1.6142 - lr: 0.0036 - 21s/epoch - 21ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 21s - loss: 15.3470 - mse: 15.3470 - mae: 1.6130 - val_loss: 14.6962 - val_mse: 14.6962 - val_mae: 1.6326 - lr: 0.0036 - 21s/epoch - 21ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 21s - loss: 15.2703 - mse: 15.2703 - mae: 1.6041 - val_loss: 14.6799 - val_mse: 14.6799 - val_mae: 1.6131 - lr: 0.0036 - 21s/epoch - 21ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 21s - loss: 15.3209 - mse: 15.3209 - mae: 1.6068 - val_loss: 14.7765 - val_mse: 14.7765 - val_mae: 1.5842 - lr: 0.0036 - 21s/epoch - 21ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 20s - loss: 15.2002 - mse: 15.2002 - mae: 1.6047 - val_loss: 14.6487 - val_mse: 14.6487 - val_mae: 1.6728 - lr: 0.0036 - 20s/epoch - 20ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 20s - loss: 15.2236 - mse: 15.2236 - mae: 1.6136 - val_loss: 15.1094 - val_mse: 15.1094 - val_mae: 1.5606 - lr: 0.0036 - 20s/epoch - 20ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 21s - loss: 15.2572 - mse: 15.2572 - mae: 1.6130 - val_loss: 14.7880 - val_mse: 14.7880 - val_mae: 1.5843 - lr: 0.0036 - 21s/epoch - 21ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 20s - loss: 15.2000 - mse: 15.2000 - mae: 1.6149 - val_loss: 15.2964 - val_mse: 15.2964 - val_mae: 1.5270 - lr: 0.0036 - 20s/epoch - 20ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 20s - loss: 15.0860 - mse: 15.0860 - mae: 1.6161 - val_loss: 14.6275 - val_mse: 14.6275 - val_mae: 1.6965 - lr: 0.0036 - 20s/epoch - 20ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 21s - loss: 15.1006 - mse: 15.1006 - mae: 1.6133 - val_loss: 14.6706 - val_mse: 14.6706 - val_mae: 1.5592 - lr: 0.0036 - 21s/epoch - 21ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 20s - loss: 15.1718 - mse: 15.1718 - mae: 1.6144 - val_loss: 14.6566 - val_mse: 14.6566 - val_mae: 1.6159 - lr: 0.0036 - 20s/epoch - 20ms/step\n",
            "Epoch 13/100\n",
            "1000/1000 - 20s - loss: 15.1898 - mse: 15.1898 - mae: 1.6156 - val_loss: 14.9047 - val_mse: 14.9047 - val_mae: 1.5431 - lr: 0.0036 - 20s/epoch - 20ms/step\n",
            "Epoch 14/100\n",
            "1000/1000 - 21s - loss: 15.0557 - mse: 15.0557 - mae: 1.6105 - val_loss: 14.6458 - val_mse: 14.6458 - val_mae: 1.6097 - lr: 0.0036 - 21s/epoch - 21ms/step\n",
            "Epoch 15/100\n",
            "1000/1000 - 22s - loss: 15.1433 - mse: 15.1433 - mae: 1.6116 - val_loss: 14.7155 - val_mse: 14.7155 - val_mae: 1.5505 - lr: 0.0036 - 22s/epoch - 22ms/step\n",
            "Score for fold 1: loss of 14.715459823608398\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 21s - loss: 15.2350 - mse: 15.2350 - mae: 1.6116 - val_loss: 13.4308 - val_mse: 13.4308 - val_mae: 1.5892 - lr: 0.0010 - 21s/epoch - 21ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 21s - loss: 15.1940 - mse: 15.1940 - mae: 1.6008 - val_loss: 13.4899 - val_mse: 13.4899 - val_mae: 1.6091 - lr: 0.0010 - 21s/epoch - 21ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 21s - loss: 15.2142 - mse: 15.2142 - mae: 1.5984 - val_loss: 13.5377 - val_mse: 13.5377 - val_mae: 1.6059 - lr: 0.0010 - 21s/epoch - 21ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 21s - loss: 15.1897 - mse: 15.1897 - mae: 1.5981 - val_loss: 13.5867 - val_mse: 13.5867 - val_mae: 1.5661 - lr: 0.0010 - 21s/epoch - 21ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 21s - loss: 15.1600 - mse: 15.1600 - mae: 1.5956 - val_loss: 13.4907 - val_mse: 13.4907 - val_mae: 1.6025 - lr: 0.0010 - 21s/epoch - 21ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 23s - loss: 15.1512 - mse: 15.1512 - mae: 1.5939 - val_loss: 13.5126 - val_mse: 13.5126 - val_mae: 1.6099 - lr: 0.0010 - 23s/epoch - 23ms/step\n",
            "Score for fold 2: loss of 13.512627601623535\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 21s - loss: 13.0138 - mse: 13.0138 - mae: 1.5770 - val_loss: 22.1052 - val_mse: 22.1052 - val_mae: 1.6120 - lr: 0.0010 - 21s/epoch - 21ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 21s - loss: 12.9411 - mse: 12.9411 - mae: 1.5753 - val_loss: 22.3776 - val_mse: 22.3776 - val_mae: 1.5716 - lr: 0.0010 - 21s/epoch - 21ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 21s - loss: 12.9114 - mse: 12.9114 - mae: 1.5783 - val_loss: 22.1405 - val_mse: 22.1405 - val_mae: 1.5754 - lr: 0.0010 - 21s/epoch - 21ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 21s - loss: 12.9376 - mse: 12.9376 - mae: 1.5757 - val_loss: 22.2861 - val_mse: 22.2861 - val_mae: 1.5843 - lr: 0.0010 - 21s/epoch - 21ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 20s - loss: 12.8720 - mse: 12.8720 - mae: 1.5709 - val_loss: 22.1591 - val_mse: 22.1591 - val_mae: 1.6041 - lr: 0.0010 - 20s/epoch - 20ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 22s - loss: 12.8558 - mse: 12.8558 - mae: 1.5708 - val_loss: 22.1227 - val_mse: 22.1227 - val_mae: 1.6003 - lr: 0.0010 - 22s/epoch - 22ms/step\n",
            "Score for fold 3: loss of 22.122718811035156\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 20s - loss: 15.5533 - mse: 15.5533 - mae: 1.5707 - val_loss: 11.4629 - val_mse: 11.4629 - val_mae: 1.5634 - lr: 0.0010 - 20s/epoch - 20ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 20s - loss: 15.5173 - mse: 15.5173 - mae: 1.5720 - val_loss: 11.3731 - val_mse: 11.3731 - val_mae: 1.5682 - lr: 0.0010 - 20s/epoch - 20ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 20s - loss: 15.4350 - mse: 15.4350 - mae: 1.5642 - val_loss: 11.3821 - val_mse: 11.3821 - val_mae: 1.5278 - lr: 0.0010 - 20s/epoch - 20ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 20s - loss: 15.4401 - mse: 15.4401 - mae: 1.5650 - val_loss: 11.4512 - val_mse: 11.4512 - val_mae: 1.5936 - lr: 0.0010 - 20s/epoch - 20ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 20s - loss: 15.4138 - mse: 15.4138 - mae: 1.5614 - val_loss: 11.3016 - val_mse: 11.3016 - val_mae: 1.5696 - lr: 0.0010 - 20s/epoch - 20ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 20s - loss: 15.3304 - mse: 15.3304 - mae: 1.5625 - val_loss: 11.3713 - val_mse: 11.3713 - val_mae: 1.5610 - lr: 0.0010 - 20s/epoch - 20ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 22s - loss: 15.3394 - mse: 15.3394 - mae: 1.5559 - val_loss: 11.3580 - val_mse: 11.3580 - val_mae: 1.5696 - lr: 0.0010 - 22s/epoch - 22ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 20s - loss: 15.3092 - mse: 15.3092 - mae: 1.5583 - val_loss: 11.3746 - val_mse: 11.3746 - val_mae: 1.5986 - lr: 0.0010 - 20s/epoch - 20ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 19s - loss: 15.2937 - mse: 15.2937 - mae: 1.5578 - val_loss: 11.2640 - val_mse: 11.2640 - val_mae: 1.5954 - lr: 0.0010 - 19s/epoch - 19ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 20s - loss: 15.2496 - mse: 15.2496 - mae: 1.5571 - val_loss: 11.2912 - val_mse: 11.2912 - val_mae: 1.5976 - lr: 0.0010 - 20s/epoch - 20ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 20s - loss: 15.1968 - mse: 15.1968 - mae: 1.5536 - val_loss: 11.2710 - val_mse: 11.2710 - val_mae: 1.6051 - lr: 0.0010 - 20s/epoch - 20ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 20s - loss: 15.2245 - mse: 15.2245 - mae: 1.5524 - val_loss: 11.4065 - val_mse: 11.4065 - val_mae: 1.5937 - lr: 0.0010 - 20s/epoch - 20ms/step\n",
            "Epoch 13/100\n",
            "1000/1000 - 19s - loss: 15.1627 - mse: 15.1627 - mae: 1.5528 - val_loss: 11.3530 - val_mse: 11.3530 - val_mae: 1.5482 - lr: 0.0010 - 19s/epoch - 19ms/step\n",
            "Epoch 14/100\n",
            "1000/1000 - 21s - loss: 15.1665 - mse: 15.1665 - mae: 1.5511 - val_loss: 11.4426 - val_mse: 11.4426 - val_mae: 1.5544 - lr: 0.0010 - 21s/epoch - 21ms/step\n",
            "Score for fold 4: loss of 11.442582130432129\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 19s - loss: 15.1309 - mse: 15.1309 - mae: 1.5613 - val_loss: 11.6437 - val_mse: 11.6437 - val_mae: 1.5358 - lr: 0.0010 - 19s/epoch - 19ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 20s - loss: 15.0314 - mse: 15.0314 - mae: 1.5599 - val_loss: 11.7057 - val_mse: 11.7057 - val_mae: 1.5157 - lr: 0.0010 - 20s/epoch - 20ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 20s - loss: 15.0000 - mse: 15.0000 - mae: 1.5562 - val_loss: 12.0496 - val_mse: 12.0496 - val_mae: 1.5497 - lr: 0.0010 - 20s/epoch - 20ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 20s - loss: 15.0064 - mse: 15.0064 - mae: 1.5546 - val_loss: 11.8972 - val_mse: 11.8972 - val_mae: 1.5824 - lr: 0.0010 - 20s/epoch - 20ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 22s - loss: 14.9324 - mse: 14.9324 - mae: 1.5557 - val_loss: 11.8883 - val_mse: 11.8883 - val_mae: 1.5681 - lr: 0.0010 - 22s/epoch - 22ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 20s - loss: 14.9378 - mse: 14.9378 - mae: 1.5543 - val_loss: 11.9779 - val_mse: 11.9779 - val_mae: 1.5822 - lr: 0.0010 - 20s/epoch - 20ms/step\n",
            "Score for fold 5: loss of 11.977899551391602\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-10-15 07:35:25,521]\u001b[0m Finished trial#38 resulted in value: 14.754. Current best value is 14.552000000000001 with parameters: {'activation': 'relu', 'num_hidden_layer': 3, 'i': 9, 'learning_rate': 0.001648783292938752}.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 3s - loss: 15.5411 - mse: 15.5411 - mae: 1.6326 - val_loss: 15.3364 - val_mse: 15.3364 - val_mae: 1.5819 - lr: 0.0036 - 3s/epoch - 3ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 15.3161 - mse: 15.3161 - mae: 1.6134 - val_loss: 15.1935 - val_mse: 15.1935 - val_mae: 1.5712 - lr: 0.0036 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 15.2693 - mse: 15.2693 - mae: 1.6097 - val_loss: 14.9762 - val_mse: 14.9762 - val_mae: 1.6814 - lr: 0.0036 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 3s - loss: 15.1687 - mse: 15.1687 - mae: 1.6079 - val_loss: 15.1431 - val_mse: 15.1431 - val_mae: 1.5609 - lr: 0.0036 - 3s/epoch - 3ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 15.2281 - mse: 15.2281 - mae: 1.6000 - val_loss: 15.0129 - val_mse: 15.0129 - val_mae: 1.6985 - lr: 0.0036 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 15.1174 - mse: 15.1174 - mae: 1.5988 - val_loss: 15.0759 - val_mse: 15.0759 - val_mae: 1.6401 - lr: 0.0036 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 15.1597 - mse: 15.1597 - mae: 1.5966 - val_loss: 15.0072 - val_mse: 15.0072 - val_mae: 1.6377 - lr: 0.0036 - 2s/epoch - 2ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 2s - loss: 15.1503 - mse: 15.1503 - mae: 1.5978 - val_loss: 14.9737 - val_mse: 14.9737 - val_mae: 1.6010 - lr: 0.0036 - 2s/epoch - 2ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 2s - loss: 15.0944 - mse: 15.0944 - mae: 1.5977 - val_loss: 14.9260 - val_mse: 14.9260 - val_mae: 1.6618 - lr: 0.0036 - 2s/epoch - 2ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 2s - loss: 15.1229 - mse: 15.1229 - mae: 1.5985 - val_loss: 15.0089 - val_mse: 15.0089 - val_mae: 1.6978 - lr: 0.0036 - 2s/epoch - 2ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 2s - loss: 15.0870 - mse: 15.0870 - mae: 1.5951 - val_loss: 14.9966 - val_mse: 14.9966 - val_mae: 1.6564 - lr: 0.0036 - 2s/epoch - 2ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 2s - loss: 15.0205 - mse: 15.0205 - mae: 1.5983 - val_loss: 14.9913 - val_mse: 14.9913 - val_mae: 1.6719 - lr: 0.0036 - 2s/epoch - 2ms/step\n",
            "Epoch 13/100\n",
            "1000/1000 - 2s - loss: 15.0711 - mse: 15.0711 - mae: 1.5946 - val_loss: 14.9867 - val_mse: 14.9867 - val_mae: 1.5844 - lr: 0.0036 - 2s/epoch - 2ms/step\n",
            "Epoch 14/100\n",
            "1000/1000 - 2s - loss: 15.0263 - mse: 15.0263 - mae: 1.5941 - val_loss: 15.1647 - val_mse: 15.1647 - val_mae: 1.5559 - lr: 0.0036 - 2s/epoch - 2ms/step\n",
            "Score for fold 1: loss of 15.164743423461914\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 12.7860 - mse: 12.7860 - mae: 1.5747 - val_loss: 23.5299 - val_mse: 23.5299 - val_mae: 1.6236 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 12.7689 - mse: 12.7689 - mae: 1.5704 - val_loss: 23.4855 - val_mse: 23.4855 - val_mae: 1.6152 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 12.7406 - mse: 12.7406 - mae: 1.5719 - val_loss: 23.6014 - val_mse: 23.6014 - val_mae: 1.6047 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 3s - loss: 12.7198 - mse: 12.7198 - mae: 1.5699 - val_loss: 23.5755 - val_mse: 23.5755 - val_mae: 1.6131 - lr: 0.0010 - 3s/epoch - 3ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 3s - loss: 12.7153 - mse: 12.7153 - mae: 1.5727 - val_loss: 23.6161 - val_mse: 23.6161 - val_mae: 1.5998 - lr: 0.0010 - 3s/epoch - 3ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 12.7005 - mse: 12.7005 - mae: 1.5704 - val_loss: 23.4570 - val_mse: 23.4570 - val_mae: 1.6360 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 12.7017 - mse: 12.7017 - mae: 1.5717 - val_loss: 23.4411 - val_mse: 23.4411 - val_mae: 1.6462 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 2s - loss: 12.6941 - mse: 12.6941 - mae: 1.5672 - val_loss: 23.5840 - val_mse: 23.5840 - val_mae: 1.5911 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 2s - loss: 12.6718 - mse: 12.6718 - mae: 1.5644 - val_loss: 23.5553 - val_mse: 23.5553 - val_mae: 1.6382 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 2s - loss: 12.6708 - mse: 12.6708 - mae: 1.5675 - val_loss: 23.7746 - val_mse: 23.7746 - val_mae: 1.5976 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 3s - loss: 12.6684 - mse: 12.6684 - mae: 1.5656 - val_loss: 23.5960 - val_mse: 23.5960 - val_mae: 1.6293 - lr: 0.0010 - 3s/epoch - 3ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 2s - loss: 12.6624 - mse: 12.6624 - mae: 1.5655 - val_loss: 23.5930 - val_mse: 23.5930 - val_mae: 1.6446 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Score for fold 2: loss of 23.593008041381836\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 14.8613 - mse: 14.8613 - mae: 1.5667 - val_loss: 14.8016 - val_mse: 14.8016 - val_mae: 1.6094 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 14.8306 - mse: 14.8306 - mae: 1.5684 - val_loss: 14.9487 - val_mse: 14.9487 - val_mae: 1.5864 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 14.8374 - mse: 14.8374 - mae: 1.5698 - val_loss: 14.9237 - val_mse: 14.9237 - val_mae: 1.6137 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 14.8527 - mse: 14.8527 - mae: 1.5628 - val_loss: 14.9285 - val_mse: 14.9285 - val_mae: 1.6027 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 14.8124 - mse: 14.8124 - mae: 1.5708 - val_loss: 14.9079 - val_mse: 14.9079 - val_mae: 1.6084 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 14.8241 - mse: 14.8241 - mae: 1.5657 - val_loss: 14.9239 - val_mse: 14.9239 - val_mae: 1.6184 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Score for fold 3: loss of 14.923864364624023\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 15.6789 - mse: 15.6789 - mae: 1.5838 - val_loss: 11.5110 - val_mse: 11.5110 - val_mae: 1.5217 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 15.6347 - mse: 15.6347 - mae: 1.5801 - val_loss: 11.5367 - val_mse: 11.5367 - val_mae: 1.5707 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 15.5931 - mse: 15.5931 - mae: 1.5831 - val_loss: 11.5690 - val_mse: 11.5690 - val_mae: 1.5431 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 3s - loss: 15.6610 - mse: 15.6610 - mae: 1.5822 - val_loss: 11.4571 - val_mse: 11.4571 - val_mae: 1.5889 - lr: 0.0010 - 3s/epoch - 3ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 15.6169 - mse: 15.6169 - mae: 1.5809 - val_loss: 11.4605 - val_mse: 11.4605 - val_mae: 1.5850 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 15.6307 - mse: 15.6307 - mae: 1.5788 - val_loss: 11.4922 - val_mse: 11.4922 - val_mae: 1.5747 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 15.5688 - mse: 15.5688 - mae: 1.5795 - val_loss: 11.4546 - val_mse: 11.4546 - val_mae: 1.5471 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 3s - loss: 15.5841 - mse: 15.5841 - mae: 1.5799 - val_loss: 11.7842 - val_mse: 11.7842 - val_mae: 1.6146 - lr: 0.0010 - 3s/epoch - 3ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 4s - loss: 15.6026 - mse: 15.6026 - mae: 1.5801 - val_loss: 11.4706 - val_mse: 11.4706 - val_mae: 1.5765 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 3s - loss: 15.5896 - mse: 15.5896 - mae: 1.5794 - val_loss: 11.5832 - val_mse: 11.5832 - val_mae: 1.5727 - lr: 0.0010 - 3s/epoch - 3ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 2s - loss: 15.5870 - mse: 15.5870 - mae: 1.5797 - val_loss: 11.5838 - val_mse: 11.5838 - val_mae: 1.5471 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 2s - loss: 15.5547 - mse: 15.5547 - mae: 1.5835 - val_loss: 11.7498 - val_mse: 11.7498 - val_mae: 1.5542 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Score for fold 4: loss of 11.749797821044922\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 16.0837 - mse: 16.0837 - mae: 1.5997 - val_loss: 9.5737 - val_mse: 9.5737 - val_mae: 1.5394 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 16.0665 - mse: 16.0665 - mae: 1.5971 - val_loss: 9.6517 - val_mse: 9.6517 - val_mae: 1.5468 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 3s - loss: 16.0215 - mse: 16.0215 - mae: 1.5941 - val_loss: 9.5663 - val_mse: 9.5663 - val_mae: 1.5068 - lr: 0.0010 - 3s/epoch - 3ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 16.0255 - mse: 16.0255 - mae: 1.5922 - val_loss: 9.5405 - val_mse: 9.5405 - val_mae: 1.5345 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 16.0358 - mse: 16.0358 - mae: 1.5966 - val_loss: 9.5719 - val_mse: 9.5719 - val_mae: 1.5492 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 3s - loss: 16.0137 - mse: 16.0137 - mae: 1.5906 - val_loss: 9.6396 - val_mse: 9.6396 - val_mae: 1.4791 - lr: 0.0010 - 3s/epoch - 3ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 15.9708 - mse: 15.9708 - mae: 1.5935 - val_loss: 9.6142 - val_mse: 9.6142 - val_mae: 1.4833 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 2s - loss: 16.0266 - mse: 16.0266 - mae: 1.5933 - val_loss: 9.6396 - val_mse: 9.6396 - val_mae: 1.5314 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 2s - loss: 16.0080 - mse: 16.0080 - mae: 1.5937 - val_loss: 9.5775 - val_mse: 9.5775 - val_mae: 1.4956 - lr: 0.0010 - 2s/epoch - 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-10-15 07:37:56,621]\u001b[0m Finished trial#39 resulted in value: 15.0. Current best value is 14.552000000000001 with parameters: {'activation': 'relu', 'num_hidden_layer': 3, 'i': 9, 'learning_rate': 0.001648783292938752}.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Score for fold 5: loss of 9.57753849029541\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 20s - loss: 17.7230 - mse: 17.7230 - mae: 1.7846 - val_loss: 11.7400 - val_mse: 11.7400 - val_mae: 1.6781 - lr: 0.0047 - 20s/epoch - 20ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 19s - loss: 17.1750 - mse: 17.1750 - mae: 1.7255 - val_loss: 11.7733 - val_mse: 11.7733 - val_mae: 1.6508 - lr: 0.0047 - 19s/epoch - 19ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 20s - loss: 17.3284 - mse: 17.3284 - mae: 1.7334 - val_loss: 11.7694 - val_mse: 11.7694 - val_mae: 1.5370 - lr: 0.0047 - 20s/epoch - 20ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 20s - loss: 17.2493 - mse: 17.2493 - mae: 1.7271 - val_loss: 11.6622 - val_mse: 11.6622 - val_mae: 1.5334 - lr: 0.0047 - 20s/epoch - 20ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 22s - loss: 17.0992 - mse: 17.0992 - mae: 1.7266 - val_loss: 12.3245 - val_mse: 12.3245 - val_mae: 1.5705 - lr: 0.0047 - 22s/epoch - 22ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 20s - loss: 17.2192 - mse: 17.2192 - mae: 1.7148 - val_loss: 11.7810 - val_mse: 11.7810 - val_mae: 1.5898 - lr: 0.0047 - 20s/epoch - 20ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 20s - loss: 17.1716 - mse: 17.1716 - mae: 1.7170 - val_loss: 12.5584 - val_mse: 12.5584 - val_mae: 1.9597 - lr: 0.0047 - 20s/epoch - 20ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 20s - loss: 17.2804 - mse: 17.2804 - mae: 1.7142 - val_loss: 12.0924 - val_mse: 12.0924 - val_mae: 1.8831 - lr: 0.0047 - 20s/epoch - 20ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 20s - loss: 17.2088 - mse: 17.2088 - mae: 1.7165 - val_loss: 11.6432 - val_mse: 11.6432 - val_mae: 1.5468 - lr: 0.0047 - 20s/epoch - 20ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 20s - loss: 17.2027 - mse: 17.2027 - mae: 1.7123 - val_loss: 12.2542 - val_mse: 12.2542 - val_mae: 1.7359 - lr: 0.0047 - 20s/epoch - 20ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 20s - loss: 17.1119 - mse: 17.1119 - mae: 1.7156 - val_loss: 11.7832 - val_mse: 11.7832 - val_mae: 1.6249 - lr: 0.0047 - 20s/epoch - 20ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 22s - loss: 17.1121 - mse: 17.1121 - mae: 1.7130 - val_loss: 11.9918 - val_mse: 11.9918 - val_mae: 1.7598 - lr: 0.0047 - 22s/epoch - 22ms/step\n",
            "Epoch 13/100\n",
            "1000/1000 - 21s - loss: 17.2852 - mse: 17.2852 - mae: 1.7286 - val_loss: 11.9665 - val_mse: 11.9665 - val_mae: 1.7689 - lr: 0.0047 - 21s/epoch - 21ms/step\n",
            "Epoch 14/100\n",
            "1000/1000 - 19s - loss: 17.1396 - mse: 17.1396 - mae: 1.7090 - val_loss: 11.7192 - val_mse: 11.7192 - val_mae: 1.6603 - lr: 0.0047 - 19s/epoch - 19ms/step\n",
            "Score for fold 1: loss of 11.719189643859863\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 20s - loss: 15.6316 - mse: 15.6316 - mae: 1.6391 - val_loss: 15.8279 - val_mse: 15.8279 - val_mae: 1.6696 - lr: 0.0010 - 20s/epoch - 20ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 20s - loss: 15.5822 - mse: 15.5822 - mae: 1.6333 - val_loss: 15.9191 - val_mse: 15.9191 - val_mae: 1.6707 - lr: 0.0010 - 20s/epoch - 20ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 22s - loss: 15.6099 - mse: 15.6099 - mae: 1.6373 - val_loss: 15.9203 - val_mse: 15.9203 - val_mae: 1.6716 - lr: 0.0010 - 22s/epoch - 22ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 20s - loss: 15.6566 - mse: 15.6566 - mae: 1.6371 - val_loss: 15.7836 - val_mse: 15.7836 - val_mae: 1.6694 - lr: 0.0010 - 20s/epoch - 20ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 20s - loss: 15.6651 - mse: 15.6651 - mae: 1.6375 - val_loss: 15.8513 - val_mse: 15.8513 - val_mae: 1.6692 - lr: 0.0010 - 20s/epoch - 20ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 20s - loss: 15.6241 - mse: 15.6241 - mae: 1.6386 - val_loss: 15.8344 - val_mse: 15.8344 - val_mae: 1.6594 - lr: 0.0010 - 20s/epoch - 20ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 19s - loss: 15.6732 - mse: 15.6732 - mae: 1.6430 - val_loss: 16.0652 - val_mse: 16.0652 - val_mae: 1.6351 - lr: 0.0010 - 19s/epoch - 19ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 20s - loss: 15.7127 - mse: 15.7127 - mae: 1.6385 - val_loss: 16.0328 - val_mse: 16.0328 - val_mae: 1.6448 - lr: 0.0010 - 20s/epoch - 20ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 20s - loss: 15.6748 - mse: 15.6748 - mae: 1.6410 - val_loss: 16.2389 - val_mse: 16.2389 - val_mae: 1.6656 - lr: 0.0010 - 20s/epoch - 20ms/step\n",
            "Score for fold 2: loss of 16.23890495300293\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 21s - loss: 16.4057 - mse: 16.4057 - mae: 1.6526 - val_loss: 13.0282 - val_mse: 13.0282 - val_mae: 1.6588 - lr: 0.0010 - 21s/epoch - 21ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 20s - loss: 16.3706 - mse: 16.3706 - mae: 1.6559 - val_loss: 13.0591 - val_mse: 13.0591 - val_mae: 1.6545 - lr: 0.0010 - 20s/epoch - 20ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 19s - loss: 16.3637 - mse: 16.3637 - mae: 1.6592 - val_loss: 13.0742 - val_mse: 13.0742 - val_mae: 1.6110 - lr: 0.0010 - 19s/epoch - 19ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 19s - loss: 16.4202 - mse: 16.4202 - mae: 1.6580 - val_loss: 13.0911 - val_mse: 13.0911 - val_mae: 1.6040 - lr: 0.0010 - 19s/epoch - 19ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 20s - loss: 16.3730 - mse: 16.3730 - mae: 1.6520 - val_loss: 13.0756 - val_mse: 13.0756 - val_mae: 1.6565 - lr: 0.0010 - 20s/epoch - 20ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 19s - loss: 16.3586 - mse: 16.3586 - mae: 1.6540 - val_loss: 13.4643 - val_mse: 13.4643 - val_mae: 1.6098 - lr: 0.0010 - 19s/epoch - 19ms/step\n",
            "Score for fold 3: loss of 13.464312553405762\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 19s - loss: 13.9584 - mse: 13.9584 - mae: 1.6511 - val_loss: 23.2551 - val_mse: 23.2551 - val_mae: 1.6085 - lr: 0.0010 - 19s/epoch - 19ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 21s - loss: 13.9367 - mse: 13.9367 - mae: 1.6474 - val_loss: 22.9657 - val_mse: 22.9657 - val_mae: 1.6979 - lr: 0.0010 - 21s/epoch - 21ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 20s - loss: 13.9227 - mse: 13.9227 - mae: 1.6496 - val_loss: 23.1490 - val_mse: 23.1490 - val_mae: 1.7015 - lr: 0.0010 - 20s/epoch - 20ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 19s - loss: 13.9048 - mse: 13.9048 - mae: 1.6468 - val_loss: 22.9383 - val_mse: 22.9383 - val_mae: 1.6578 - lr: 0.0010 - 19s/epoch - 19ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 19s - loss: 13.9518 - mse: 13.9518 - mae: 1.6530 - val_loss: 23.2796 - val_mse: 23.2796 - val_mae: 1.8008 - lr: 0.0010 - 19s/epoch - 19ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 20s - loss: 13.8589 - mse: 13.8589 - mae: 1.6470 - val_loss: 23.4678 - val_mse: 23.4678 - val_mae: 1.6168 - lr: 0.0010 - 20s/epoch - 20ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 20s - loss: 13.9108 - mse: 13.9108 - mae: 1.6547 - val_loss: 22.9459 - val_mse: 22.9459 - val_mae: 1.6578 - lr: 0.0010 - 20s/epoch - 20ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 20s - loss: 13.9116 - mse: 13.9116 - mae: 1.6463 - val_loss: 22.9825 - val_mse: 22.9825 - val_mae: 1.7040 - lr: 0.0010 - 20s/epoch - 20ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 21s - loss: 13.8610 - mse: 13.8610 - mae: 1.6498 - val_loss: 23.0219 - val_mse: 23.0219 - val_mae: 1.6735 - lr: 0.0010 - 21s/epoch - 21ms/step\n",
            "Score for fold 4: loss of 23.02194595336914\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 19s - loss: 15.9944 - mse: 15.9944 - mae: 1.6475 - val_loss: 14.5760 - val_mse: 14.5760 - val_mae: 1.6550 - lr: 0.0010 - 19s/epoch - 19ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 19s - loss: 15.9521 - mse: 15.9521 - mae: 1.6493 - val_loss: 14.6568 - val_mse: 14.6568 - val_mae: 1.6361 - lr: 0.0010 - 19s/epoch - 19ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 19s - loss: 15.9848 - mse: 15.9848 - mae: 1.6495 - val_loss: 14.5957 - val_mse: 14.5957 - val_mae: 1.6681 - lr: 0.0010 - 19s/epoch - 19ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 19s - loss: 15.9961 - mse: 15.9961 - mae: 1.6502 - val_loss: 14.5457 - val_mse: 14.5457 - val_mae: 1.6755 - lr: 0.0010 - 19s/epoch - 19ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 19s - loss: 16.0145 - mse: 16.0145 - mae: 1.6469 - val_loss: 14.6936 - val_mse: 14.6936 - val_mae: 1.6439 - lr: 0.0010 - 19s/epoch - 19ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 21s - loss: 16.0121 - mse: 16.0121 - mae: 1.6513 - val_loss: 14.6660 - val_mse: 14.6660 - val_mae: 1.6403 - lr: 0.0010 - 21s/epoch - 21ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 19s - loss: 15.9732 - mse: 15.9732 - mae: 1.6447 - val_loss: 14.9768 - val_mse: 14.9768 - val_mae: 1.6242 - lr: 0.0010 - 19s/epoch - 19ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 20s - loss: 15.9911 - mse: 15.9911 - mae: 1.6528 - val_loss: 14.6051 - val_mse: 14.6051 - val_mae: 1.6580 - lr: 0.0010 - 20s/epoch - 20ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 19s - loss: 15.9616 - mse: 15.9616 - mae: 1.6501 - val_loss: 14.7119 - val_mse: 14.7119 - val_mae: 1.6560 - lr: 0.0010 - 19s/epoch - 19ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-10-15 07:54:46,046]\u001b[0m Finished trial#40 resulted in value: 15.830000000000002. Current best value is 14.552000000000001 with parameters: {'activation': 'relu', 'num_hidden_layer': 3, 'i': 9, 'learning_rate': 0.001648783292938752}.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Score for fold 5: loss of 14.711946487426758\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 7s - loss: 16.5855 - mse: 16.5855 - mae: 1.6448 - val_loss: 11.3282 - val_mse: 11.3282 - val_mae: 1.4491 - lr: 0.0021 - 7s/epoch - 7ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 6s - loss: 16.2948 - mse: 16.2948 - mae: 1.6213 - val_loss: 10.9434 - val_mse: 10.9434 - val_mae: 1.6076 - lr: 0.0021 - 6s/epoch - 6ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 6s - loss: 16.1123 - mse: 16.1123 - mae: 1.6093 - val_loss: 11.0184 - val_mse: 11.0184 - val_mae: 1.6344 - lr: 0.0021 - 6s/epoch - 6ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 6s - loss: 16.1666 - mse: 16.1666 - mae: 1.6083 - val_loss: 10.9186 - val_mse: 10.9186 - val_mae: 1.5755 - lr: 0.0021 - 6s/epoch - 6ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 6s - loss: 16.1232 - mse: 16.1232 - mae: 1.6020 - val_loss: 11.2223 - val_mse: 11.2223 - val_mae: 1.5242 - lr: 0.0021 - 6s/epoch - 6ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 6s - loss: 16.0617 - mse: 16.0617 - mae: 1.6044 - val_loss: 10.9695 - val_mse: 10.9695 - val_mae: 1.6246 - lr: 0.0021 - 6s/epoch - 6ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 6s - loss: 16.0683 - mse: 16.0683 - mae: 1.6063 - val_loss: 10.8195 - val_mse: 10.8195 - val_mae: 1.5981 - lr: 0.0021 - 6s/epoch - 6ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 6s - loss: 16.1299 - mse: 16.1299 - mae: 1.6047 - val_loss: 10.8220 - val_mse: 10.8220 - val_mae: 1.5261 - lr: 0.0021 - 6s/epoch - 6ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 6s - loss: 15.9535 - mse: 15.9535 - mae: 1.6019 - val_loss: 10.8721 - val_mse: 10.8721 - val_mae: 1.6059 - lr: 0.0021 - 6s/epoch - 6ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 6s - loss: 16.0211 - mse: 16.0211 - mae: 1.6010 - val_loss: 10.8996 - val_mse: 10.8996 - val_mae: 1.5395 - lr: 0.0021 - 6s/epoch - 6ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 6s - loss: 16.0655 - mse: 16.0655 - mae: 1.5959 - val_loss: 10.9151 - val_mse: 10.9151 - val_mae: 1.6651 - lr: 0.0021 - 6s/epoch - 6ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 7s - loss: 16.0201 - mse: 16.0201 - mae: 1.5947 - val_loss: 10.9828 - val_mse: 10.9828 - val_mae: 1.5264 - lr: 0.0021 - 7s/epoch - 7ms/step\n",
            "Score for fold 1: loss of 10.98276138305664\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 6s - loss: 15.1492 - mse: 15.1492 - mae: 1.5677 - val_loss: 13.7722 - val_mse: 13.7722 - val_mae: 1.5635 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 6s - loss: 15.1168 - mse: 15.1168 - mae: 1.5640 - val_loss: 13.7205 - val_mse: 13.7205 - val_mae: 1.5728 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 6s - loss: 15.1217 - mse: 15.1217 - mae: 1.5664 - val_loss: 13.8448 - val_mse: 13.8448 - val_mae: 1.5635 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 6s - loss: 15.0059 - mse: 15.0059 - mae: 1.5637 - val_loss: 14.2928 - val_mse: 14.2928 - val_mae: 1.5862 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 6s - loss: 15.0255 - mse: 15.0255 - mae: 1.5655 - val_loss: 13.9179 - val_mse: 13.9179 - val_mae: 1.5700 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 6s - loss: 15.0462 - mse: 15.0462 - mae: 1.5620 - val_loss: 14.0032 - val_mse: 14.0032 - val_mae: 1.5517 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 6s - loss: 15.0113 - mse: 15.0113 - mae: 1.5588 - val_loss: 13.8438 - val_mse: 13.8438 - val_mae: 1.5684 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
            "Score for fold 2: loss of 13.843789100646973\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 6s - loss: 12.7567 - mse: 12.7567 - mae: 1.5505 - val_loss: 23.0855 - val_mse: 23.0855 - val_mae: 1.6151 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 6s - loss: 12.7175 - mse: 12.7175 - mae: 1.5498 - val_loss: 23.0341 - val_mse: 23.0341 - val_mae: 1.6215 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 6s - loss: 12.6297 - mse: 12.6297 - mae: 1.5555 - val_loss: 23.0165 - val_mse: 23.0165 - val_mae: 1.5830 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 6s - loss: 12.6213 - mse: 12.6213 - mae: 1.5483 - val_loss: 22.8951 - val_mse: 22.8951 - val_mae: 1.6726 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 6s - loss: 12.6018 - mse: 12.6018 - mae: 1.5485 - val_loss: 23.2472 - val_mse: 23.2472 - val_mae: 1.5596 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 6s - loss: 12.5174 - mse: 12.5174 - mae: 1.5503 - val_loss: 23.2195 - val_mse: 23.2195 - val_mae: 1.5728 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 6s - loss: 12.5786 - mse: 12.5786 - mae: 1.5502 - val_loss: 23.0886 - val_mse: 23.0886 - val_mae: 1.6163 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 6s - loss: 12.5334 - mse: 12.5334 - mae: 1.5491 - val_loss: 23.0527 - val_mse: 23.0527 - val_mae: 1.5823 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 6s - loss: 12.4939 - mse: 12.4939 - mae: 1.5487 - val_loss: 23.1418 - val_mse: 23.1418 - val_mae: 1.6152 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
            "Score for fold 3: loss of 23.141769409179688\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 7s - loss: 15.1711 - mse: 15.1711 - mae: 1.5551 - val_loss: 12.3361 - val_mse: 12.3361 - val_mae: 1.5473 - lr: 0.0010 - 7s/epoch - 7ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 6s - loss: 15.1233 - mse: 15.1233 - mae: 1.5598 - val_loss: 12.7380 - val_mse: 12.7380 - val_mae: 1.5492 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 6s - loss: 15.0629 - mse: 15.0629 - mae: 1.5554 - val_loss: 12.6379 - val_mse: 12.6379 - val_mae: 1.5876 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 6s - loss: 15.0461 - mse: 15.0461 - mae: 1.5521 - val_loss: 12.9415 - val_mse: 12.9415 - val_mae: 1.5431 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 6s - loss: 14.9776 - mse: 14.9776 - mae: 1.5518 - val_loss: 12.8138 - val_mse: 12.8138 - val_mae: 1.5265 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 6s - loss: 14.9953 - mse: 14.9953 - mae: 1.5526 - val_loss: 12.6485 - val_mse: 12.6485 - val_mae: 1.5980 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
            "Score for fold 4: loss of 12.648482322692871\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 6s - loss: 14.8350 - mse: 14.8350 - mae: 1.5579 - val_loss: 12.9975 - val_mse: 12.9975 - val_mae: 1.5195 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 6s - loss: 14.8032 - mse: 14.8032 - mae: 1.5550 - val_loss: 13.3748 - val_mse: 13.3748 - val_mae: 1.5615 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 6s - loss: 14.7432 - mse: 14.7432 - mae: 1.5508 - val_loss: 13.7010 - val_mse: 13.7010 - val_mae: 1.5341 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 6s - loss: 14.7257 - mse: 14.7257 - mae: 1.5496 - val_loss: 13.7315 - val_mse: 13.7315 - val_mae: 1.6099 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 6s - loss: 14.6984 - mse: 14.6984 - mae: 1.5498 - val_loss: 13.4413 - val_mse: 13.4413 - val_mae: 1.5478 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 6s - loss: 14.6456 - mse: 14.6456 - mae: 1.5479 - val_loss: 13.5095 - val_mse: 13.5095 - val_mae: 1.5979 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
            "Score for fold 5: loss of 13.509551048278809\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-10-15 07:59:31,987]\u001b[0m Finished trial#41 resulted in value: 14.824000000000002. Current best value is 14.552000000000001 with parameters: {'activation': 'relu', 'num_hidden_layer': 3, 'i': 9, 'learning_rate': 0.001648783292938752}.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 6s - loss: 15.9493 - mse: 15.9493 - mae: 1.6278 - val_loss: 13.2491 - val_mse: 13.2491 - val_mae: 1.7254 - lr: 0.0021 - 6s/epoch - 6ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 6s - loss: 15.7251 - mse: 15.7251 - mae: 1.5974 - val_loss: 13.2588 - val_mse: 13.2588 - val_mae: 1.5655 - lr: 0.0021 - 6s/epoch - 6ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 6s - loss: 15.6382 - mse: 15.6382 - mae: 1.5910 - val_loss: 13.6788 - val_mse: 13.6788 - val_mae: 1.6744 - lr: 0.0021 - 6s/epoch - 6ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 6s - loss: 15.5736 - mse: 15.5736 - mae: 1.5841 - val_loss: 13.1908 - val_mse: 13.1908 - val_mae: 1.6308 - lr: 0.0021 - 6s/epoch - 6ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 6s - loss: 15.6111 - mse: 15.6111 - mae: 1.5888 - val_loss: 13.1712 - val_mse: 13.1712 - val_mae: 1.7567 - lr: 0.0021 - 6s/epoch - 6ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 6s - loss: 15.5587 - mse: 15.5587 - mae: 1.5831 - val_loss: 13.1149 - val_mse: 13.1149 - val_mae: 1.7029 - lr: 0.0021 - 6s/epoch - 6ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 6s - loss: 15.5804 - mse: 15.5804 - mae: 1.5818 - val_loss: 13.1365 - val_mse: 13.1365 - val_mae: 1.6290 - lr: 0.0021 - 6s/epoch - 6ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 6s - loss: 15.5207 - mse: 15.5207 - mae: 1.5801 - val_loss: 13.6345 - val_mse: 13.6345 - val_mae: 1.5914 - lr: 0.0021 - 6s/epoch - 6ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 6s - loss: 15.4402 - mse: 15.4402 - mae: 1.5810 - val_loss: 13.3783 - val_mse: 13.3783 - val_mae: 1.6557 - lr: 0.0021 - 6s/epoch - 6ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 6s - loss: 15.4726 - mse: 15.4726 - mae: 1.5806 - val_loss: 13.3726 - val_mse: 13.3726 - val_mae: 1.6038 - lr: 0.0021 - 6s/epoch - 6ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 6s - loss: 15.4979 - mse: 15.4979 - mae: 1.5733 - val_loss: 13.3333 - val_mse: 13.3333 - val_mae: 1.6391 - lr: 0.0021 - 6s/epoch - 6ms/step\n",
            "Score for fold 1: loss of 13.333283424377441\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 6s - loss: 13.0743 - mse: 13.0743 - mae: 1.5704 - val_loss: 21.9288 - val_mse: 21.9288 - val_mae: 1.6461 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 6s - loss: 13.0770 - mse: 13.0770 - mae: 1.5668 - val_loss: 21.8886 - val_mse: 21.8886 - val_mae: 1.6246 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 6s - loss: 13.0685 - mse: 13.0685 - mae: 1.5673 - val_loss: 22.0347 - val_mse: 22.0347 - val_mae: 1.5510 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 6s - loss: 13.0123 - mse: 13.0123 - mae: 1.5611 - val_loss: 21.8897 - val_mse: 21.8897 - val_mae: 1.6219 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 6s - loss: 12.9750 - mse: 12.9750 - mae: 1.5610 - val_loss: 22.0827 - val_mse: 22.0827 - val_mae: 1.5989 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 6s - loss: 12.9805 - mse: 12.9805 - mae: 1.5615 - val_loss: 21.9368 - val_mse: 21.9368 - val_mae: 1.6184 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 6s - loss: 12.9059 - mse: 12.9059 - mae: 1.5576 - val_loss: 22.0798 - val_mse: 22.0798 - val_mae: 1.5769 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
            "Score for fold 2: loss of 22.079744338989258\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 6s - loss: 14.4934 - mse: 14.4934 - mae: 1.5621 - val_loss: 15.7312 - val_mse: 15.7312 - val_mae: 1.5574 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 6s - loss: 14.5124 - mse: 14.5124 - mae: 1.5565 - val_loss: 15.8085 - val_mse: 15.8085 - val_mae: 1.5719 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 6s - loss: 14.4841 - mse: 14.4841 - mae: 1.5562 - val_loss: 15.8623 - val_mse: 15.8623 - val_mae: 1.5914 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 6s - loss: 14.3238 - mse: 14.3238 - mae: 1.5559 - val_loss: 15.9537 - val_mse: 15.9537 - val_mae: 1.5989 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 6s - loss: 14.3736 - mse: 14.3736 - mae: 1.5492 - val_loss: 16.0049 - val_mse: 16.0049 - val_mae: 1.5896 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 7s - loss: 14.3356 - mse: 14.3356 - mae: 1.5496 - val_loss: 15.7201 - val_mse: 15.7201 - val_mae: 1.6492 - lr: 0.0010 - 7s/epoch - 7ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 6s - loss: 14.3213 - mse: 14.3213 - mae: 1.5493 - val_loss: 16.0698 - val_mse: 16.0698 - val_mae: 1.6160 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 6s - loss: 14.2722 - mse: 14.2722 - mae: 1.5437 - val_loss: 15.8922 - val_mse: 15.8922 - val_mae: 1.5872 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 6s - loss: 14.2314 - mse: 14.2314 - mae: 1.5456 - val_loss: 16.1467 - val_mse: 16.1467 - val_mae: 1.5665 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 6s - loss: 14.1650 - mse: 14.1650 - mae: 1.5434 - val_loss: 16.0035 - val_mse: 16.0035 - val_mae: 1.6003 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 6s - loss: 14.1073 - mse: 14.1073 - mae: 1.5432 - val_loss: 16.0090 - val_mse: 16.0090 - val_mae: 1.6205 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
            "Score for fold 3: loss of 16.009029388427734\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 6s - loss: 15.4459 - mse: 15.4459 - mae: 1.5723 - val_loss: 10.5316 - val_mse: 10.5316 - val_mae: 1.4725 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 6s - loss: 15.4163 - mse: 15.4163 - mae: 1.5726 - val_loss: 10.5670 - val_mse: 10.5670 - val_mae: 1.5107 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 6s - loss: 15.4130 - mse: 15.4130 - mae: 1.5689 - val_loss: 10.6144 - val_mse: 10.6144 - val_mae: 1.5185 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 6s - loss: 15.3284 - mse: 15.3284 - mae: 1.5649 - val_loss: 10.6024 - val_mse: 10.6024 - val_mae: 1.4682 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 6s - loss: 15.3901 - mse: 15.3901 - mae: 1.5654 - val_loss: 10.5509 - val_mse: 10.5509 - val_mae: 1.5059 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 6s - loss: 15.2849 - mse: 15.2849 - mae: 1.5645 - val_loss: 10.7580 - val_mse: 10.7580 - val_mae: 1.5124 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
            "Score for fold 4: loss of 10.757986068725586\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 6s - loss: 14.9969 - mse: 14.9969 - mae: 1.5587 - val_loss: 12.2249 - val_mse: 12.2249 - val_mae: 1.4967 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 6s - loss: 14.9797 - mse: 14.9797 - mae: 1.5527 - val_loss: 12.1925 - val_mse: 12.1925 - val_mae: 1.5675 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 6s - loss: 14.8985 - mse: 14.8985 - mae: 1.5494 - val_loss: 12.3813 - val_mse: 12.3813 - val_mae: 1.5246 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 6s - loss: 14.8124 - mse: 14.8124 - mae: 1.5445 - val_loss: 12.1889 - val_mse: 12.1889 - val_mae: 1.5831 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 6s - loss: 14.7701 - mse: 14.7701 - mae: 1.5465 - val_loss: 12.2281 - val_mse: 12.2281 - val_mae: 1.5229 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 6s - loss: 14.7420 - mse: 14.7420 - mae: 1.5421 - val_loss: 12.1939 - val_mse: 12.1939 - val_mae: 1.5747 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 6s - loss: 14.6332 - mse: 14.6332 - mae: 1.5397 - val_loss: 12.2073 - val_mse: 12.2073 - val_mae: 1.5592 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 6s - loss: 14.6563 - mse: 14.6563 - mae: 1.5426 - val_loss: 12.2466 - val_mse: 12.2466 - val_mae: 1.5755 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 6s - loss: 14.5295 - mse: 14.5295 - mae: 1.5403 - val_loss: 12.1459 - val_mse: 12.1459 - val_mae: 1.5533 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 6s - loss: 14.4931 - mse: 14.4931 - mae: 1.5348 - val_loss: 12.3090 - val_mse: 12.3090 - val_mae: 1.5862 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 6s - loss: 14.5306 - mse: 14.5306 - mae: 1.5357 - val_loss: 12.3487 - val_mse: 12.3487 - val_mae: 1.5351 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 6s - loss: 14.5340 - mse: 14.5340 - mae: 1.5324 - val_loss: 12.5052 - val_mse: 12.5052 - val_mae: 1.5544 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
            "Epoch 13/100\n",
            "1000/1000 - 6s - loss: 14.3944 - mse: 14.3944 - mae: 1.5283 - val_loss: 12.6321 - val_mse: 12.6321 - val_mae: 1.5409 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
            "Epoch 14/100\n",
            "1000/1000 - 6s - loss: 14.4610 - mse: 14.4610 - mae: 1.5294 - val_loss: 12.2640 - val_mse: 12.2640 - val_mae: 1.5846 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
            "Score for fold 5: loss of 12.26401138305664\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-10-15 08:06:03,315]\u001b[0m Finished trial#42 resulted in value: 14.888. Current best value is 14.552000000000001 with parameters: {'activation': 'relu', 'num_hidden_layer': 3, 'i': 9, 'learning_rate': 0.001648783292938752}.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 20s - loss: 16.8867 - mse: 16.8867 - mae: 1.6696 - val_loss: 11.3678 - val_mse: 11.3678 - val_mae: 1.7233 - lr: 0.0098 - 20s/epoch - 20ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 20s - loss: 16.3830 - mse: 16.3830 - mae: 1.6258 - val_loss: 11.3162 - val_mse: 11.3162 - val_mae: 1.7168 - lr: 0.0098 - 20s/epoch - 20ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 20s - loss: 16.3962 - mse: 16.3962 - mae: 1.6377 - val_loss: 11.3282 - val_mse: 11.3282 - val_mae: 1.5865 - lr: 0.0098 - 20s/epoch - 20ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 20s - loss: 16.2437 - mse: 16.2437 - mae: 1.6330 - val_loss: 11.1184 - val_mse: 11.1184 - val_mae: 1.6201 - lr: 0.0098 - 20s/epoch - 20ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 19s - loss: 16.2189 - mse: 16.2189 - mae: 1.6308 - val_loss: 11.1110 - val_mse: 11.1110 - val_mae: 1.6354 - lr: 0.0098 - 19s/epoch - 19ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 21s - loss: 16.2120 - mse: 16.2120 - mae: 1.6338 - val_loss: 11.2773 - val_mse: 11.2773 - val_mae: 1.6914 - lr: 0.0098 - 21s/epoch - 21ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 20s - loss: 16.2467 - mse: 16.2467 - mae: 1.6341 - val_loss: 11.2255 - val_mse: 11.2255 - val_mae: 1.7321 - lr: 0.0098 - 20s/epoch - 20ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 19s - loss: 16.2831 - mse: 16.2831 - mae: 1.6335 - val_loss: 11.0902 - val_mse: 11.0902 - val_mae: 1.6644 - lr: 0.0098 - 19s/epoch - 19ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 19s - loss: 16.1857 - mse: 16.1857 - mae: 1.6326 - val_loss: 11.2968 - val_mse: 11.2968 - val_mae: 1.5987 - lr: 0.0098 - 19s/epoch - 19ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 19s - loss: 16.3413 - mse: 16.3413 - mae: 1.6414 - val_loss: 11.6235 - val_mse: 11.6235 - val_mae: 1.5390 - lr: 0.0098 - 19s/epoch - 19ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 19s - loss: 16.1502 - mse: 16.1502 - mae: 1.6287 - val_loss: 11.2381 - val_mse: 11.2381 - val_mae: 1.6176 - lr: 0.0098 - 19s/epoch - 19ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 19s - loss: 16.1340 - mse: 16.1340 - mae: 1.6319 - val_loss: 11.9971 - val_mse: 11.9971 - val_mae: 1.5280 - lr: 0.0098 - 19s/epoch - 19ms/step\n",
            "Epoch 13/100\n",
            "1000/1000 - 21s - loss: 16.2047 - mse: 16.2047 - mae: 1.6335 - val_loss: 11.6268 - val_mse: 11.6268 - val_mae: 1.5460 - lr: 0.0098 - 21s/epoch - 21ms/step\n",
            "Score for fold 1: loss of 11.62679672241211\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 19s - loss: 15.7567 - mse: 15.7567 - mae: 1.6235 - val_loss: 11.5320 - val_mse: 11.5320 - val_mae: 1.6831 - lr: 0.0020 - 19s/epoch - 19ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 19s - loss: 15.7332 - mse: 15.7332 - mae: 1.6198 - val_loss: 11.5458 - val_mse: 11.5458 - val_mae: 1.6311 - lr: 0.0020 - 19s/epoch - 19ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 20s - loss: 15.8132 - mse: 15.8132 - mae: 1.6237 - val_loss: 11.5761 - val_mse: 11.5761 - val_mae: 1.6445 - lr: 0.0020 - 20s/epoch - 20ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 20s - loss: 15.8136 - mse: 15.8136 - mae: 1.6208 - val_loss: 11.8294 - val_mse: 11.8294 - val_mae: 1.5131 - lr: 0.0020 - 20s/epoch - 20ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 20s - loss: 15.7326 - mse: 15.7326 - mae: 1.6204 - val_loss: 11.7628 - val_mse: 11.7628 - val_mae: 1.5221 - lr: 0.0020 - 20s/epoch - 20ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 20s - loss: 15.7169 - mse: 15.7169 - mae: 1.6160 - val_loss: 11.6621 - val_mse: 11.6621 - val_mae: 1.7171 - lr: 0.0020 - 20s/epoch - 20ms/step\n",
            "Score for fold 2: loss of 11.662055015563965\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 20s - loss: 14.4041 - mse: 14.4041 - mae: 1.6054 - val_loss: 17.6810 - val_mse: 17.6810 - val_mae: 1.5776 - lr: 0.0010 - 20s/epoch - 20ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 19s - loss: 14.2528 - mse: 14.2528 - mae: 1.6043 - val_loss: 17.5218 - val_mse: 17.5218 - val_mae: 1.6560 - lr: 0.0010 - 19s/epoch - 19ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 19s - loss: 14.3198 - mse: 14.3198 - mae: 1.6054 - val_loss: 17.6809 - val_mse: 17.6809 - val_mae: 1.5785 - lr: 0.0010 - 19s/epoch - 19ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 20s - loss: 14.3826 - mse: 14.3826 - mae: 1.6105 - val_loss: 17.6531 - val_mse: 17.6531 - val_mae: 1.5816 - lr: 0.0010 - 20s/epoch - 20ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 20s - loss: 14.3162 - mse: 14.3162 - mae: 1.6071 - val_loss: 17.6312 - val_mse: 17.6312 - val_mae: 1.7153 - lr: 0.0010 - 20s/epoch - 20ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 20s - loss: 14.3058 - mse: 14.3058 - mae: 1.6075 - val_loss: 17.5560 - val_mse: 17.5560 - val_mae: 1.6028 - lr: 0.0010 - 20s/epoch - 20ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 21s - loss: 14.3693 - mse: 14.3693 - mae: 1.6039 - val_loss: 17.5519 - val_mse: 17.5519 - val_mae: 1.6098 - lr: 0.0010 - 21s/epoch - 21ms/step\n",
            "Score for fold 3: loss of 17.55188751220703\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 19s - loss: 14.2862 - mse: 14.2862 - mae: 1.6085 - val_loss: 17.6429 - val_mse: 17.6429 - val_mae: 1.6365 - lr: 0.0010 - 19s/epoch - 19ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 19s - loss: 14.3019 - mse: 14.3019 - mae: 1.6103 - val_loss: 17.6556 - val_mse: 17.6556 - val_mae: 1.6272 - lr: 0.0010 - 19s/epoch - 19ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 20s - loss: 14.3524 - mse: 14.3524 - mae: 1.6151 - val_loss: 17.8066 - val_mse: 17.8066 - val_mae: 1.7458 - lr: 0.0010 - 20s/epoch - 20ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 20s - loss: 14.2658 - mse: 14.2658 - mae: 1.6099 - val_loss: 18.7865 - val_mse: 18.7865 - val_mae: 1.5095 - lr: 0.0010 - 20s/epoch - 20ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 19s - loss: 14.3557 - mse: 14.3557 - mae: 1.6124 - val_loss: 17.7822 - val_mse: 17.7822 - val_mae: 1.5786 - lr: 0.0010 - 19s/epoch - 19ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 19s - loss: 14.2908 - mse: 14.2908 - mae: 1.6078 - val_loss: 17.7553 - val_mse: 17.7553 - val_mae: 1.7272 - lr: 0.0010 - 19s/epoch - 19ms/step\n",
            "Score for fold 4: loss of 17.755321502685547\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 19s - loss: 14.6377 - mse: 14.6377 - mae: 1.5928 - val_loss: 16.6776 - val_mse: 16.6776 - val_mae: 1.6272 - lr: 0.0010 - 19s/epoch - 19ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 19s - loss: 14.5849 - mse: 14.5849 - mae: 1.5932 - val_loss: 16.4568 - val_mse: 16.4568 - val_mae: 1.6767 - lr: 0.0010 - 19s/epoch - 19ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 19s - loss: 14.6179 - mse: 14.6179 - mae: 1.5927 - val_loss: 16.5019 - val_mse: 16.5019 - val_mae: 1.6620 - lr: 0.0010 - 19s/epoch - 19ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 19s - loss: 14.5947 - mse: 14.5947 - mae: 1.5898 - val_loss: 16.5323 - val_mse: 16.5323 - val_mae: 1.6723 - lr: 0.0010 - 19s/epoch - 19ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 19s - loss: 14.6953 - mse: 14.6953 - mae: 1.5970 - val_loss: 16.5135 - val_mse: 16.5135 - val_mae: 1.7651 - lr: 0.0010 - 19s/epoch - 19ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 19s - loss: 14.6946 - mse: 14.6946 - mae: 1.5989 - val_loss: 16.4400 - val_mse: 16.4400 - val_mae: 1.6894 - lr: 0.0010 - 19s/epoch - 19ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 21s - loss: 14.4937 - mse: 14.4937 - mae: 1.5933 - val_loss: 17.1970 - val_mse: 17.1970 - val_mae: 1.5825 - lr: 0.0010 - 21s/epoch - 21ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 20s - loss: 14.6005 - mse: 14.6005 - mae: 1.5921 - val_loss: 16.7609 - val_mse: 16.7609 - val_mae: 1.6149 - lr: 0.0010 - 20s/epoch - 20ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 20s - loss: 14.5541 - mse: 14.5541 - mae: 1.5869 - val_loss: 17.1780 - val_mse: 17.1780 - val_mae: 1.5999 - lr: 0.0010 - 20s/epoch - 20ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 19s - loss: 14.6717 - mse: 14.6717 - mae: 1.5979 - val_loss: 16.7900 - val_mse: 16.7900 - val_mae: 1.6177 - lr: 0.0010 - 19s/epoch - 19ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 20s - loss: 14.6354 - mse: 14.6354 - mae: 1.5958 - val_loss: 16.5218 - val_mse: 16.5218 - val_mae: 1.7563 - lr: 0.0010 - 20s/epoch - 20ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-10-15 08:21:12,039]\u001b[0m Finished trial#43 resulted in value: 15.024000000000001. Current best value is 14.552000000000001 with parameters: {'activation': 'relu', 'num_hidden_layer': 3, 'i': 9, 'learning_rate': 0.001648783292938752}.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Score for fold 5: loss of 16.52179527282715\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 6s - loss: 13.1422 - mse: 13.1422 - mae: 1.6224 - val_loss: 24.5126 - val_mse: 24.5126 - val_mae: 1.6534 - lr: 0.0033 - 6s/epoch - 6ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 6s - loss: 12.9265 - mse: 12.9265 - mae: 1.5936 - val_loss: 24.5688 - val_mse: 24.5688 - val_mae: 1.6858 - lr: 0.0033 - 6s/epoch - 6ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 6s - loss: 12.8840 - mse: 12.8840 - mae: 1.5927 - val_loss: 24.7662 - val_mse: 24.7662 - val_mae: 1.6111 - lr: 0.0033 - 6s/epoch - 6ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 6s - loss: 12.7887 - mse: 12.7887 - mae: 1.5884 - val_loss: 24.8798 - val_mse: 24.8798 - val_mae: 1.6235 - lr: 0.0033 - 6s/epoch - 6ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 6s - loss: 12.7343 - mse: 12.7343 - mae: 1.5854 - val_loss: 24.4189 - val_mse: 24.4189 - val_mae: 1.6187 - lr: 0.0033 - 6s/epoch - 6ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 6s - loss: 12.7293 - mse: 12.7293 - mae: 1.5889 - val_loss: 24.4092 - val_mse: 24.4092 - val_mae: 1.6900 - lr: 0.0033 - 6s/epoch - 6ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 6s - loss: 12.6816 - mse: 12.6816 - mae: 1.5858 - val_loss: 24.4429 - val_mse: 24.4429 - val_mae: 1.6241 - lr: 0.0033 - 6s/epoch - 6ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 6s - loss: 12.6771 - mse: 12.6771 - mae: 1.5777 - val_loss: 24.4900 - val_mse: 24.4900 - val_mae: 1.6019 - lr: 0.0033 - 6s/epoch - 6ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 8s - loss: 12.7320 - mse: 12.7320 - mae: 1.5774 - val_loss: 24.4735 - val_mse: 24.4735 - val_mae: 1.6041 - lr: 0.0033 - 8s/epoch - 8ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 6s - loss: 12.6105 - mse: 12.6105 - mae: 1.5732 - val_loss: 24.4889 - val_mse: 24.4889 - val_mae: 1.6366 - lr: 0.0033 - 6s/epoch - 6ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 6s - loss: 12.6291 - mse: 12.6291 - mae: 1.5684 - val_loss: 24.7749 - val_mse: 24.7749 - val_mae: 1.5932 - lr: 0.0033 - 6s/epoch - 6ms/step\n",
            "Score for fold 1: loss of 24.774871826171875\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 6s - loss: 14.8263 - mse: 14.8263 - mae: 1.5693 - val_loss: 15.0569 - val_mse: 15.0569 - val_mae: 1.6160 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 6s - loss: 14.7531 - mse: 14.7531 - mae: 1.5702 - val_loss: 15.0413 - val_mse: 15.0413 - val_mae: 1.5601 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 6s - loss: 14.7489 - mse: 14.7489 - mae: 1.5633 - val_loss: 14.9330 - val_mse: 14.9330 - val_mae: 1.5937 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 6s - loss: 14.6668 - mse: 14.6668 - mae: 1.5615 - val_loss: 15.0411 - val_mse: 15.0411 - val_mae: 1.5670 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 6s - loss: 14.6302 - mse: 14.6302 - mae: 1.5594 - val_loss: 15.0088 - val_mse: 15.0088 - val_mae: 1.5866 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 6s - loss: 14.6307 - mse: 14.6307 - mae: 1.5617 - val_loss: 14.9612 - val_mse: 14.9612 - val_mae: 1.5585 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 6s - loss: 14.5622 - mse: 14.5622 - mae: 1.5592 - val_loss: 15.0777 - val_mse: 15.0777 - val_mae: 1.5826 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 6s - loss: 14.5192 - mse: 14.5192 - mae: 1.5590 - val_loss: 15.0167 - val_mse: 15.0167 - val_mae: 1.6339 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
            "Score for fold 2: loss of 15.016650199890137\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 6s - loss: 15.9881 - mse: 15.9881 - mae: 1.5779 - val_loss: 9.2329 - val_mse: 9.2329 - val_mae: 1.5222 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 6s - loss: 15.9464 - mse: 15.9464 - mae: 1.5764 - val_loss: 9.3255 - val_mse: 9.3255 - val_mae: 1.5411 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 6s - loss: 16.0009 - mse: 16.0009 - mae: 1.5784 - val_loss: 9.2390 - val_mse: 9.2390 - val_mae: 1.5475 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 6s - loss: 15.9592 - mse: 15.9592 - mae: 1.5740 - val_loss: 9.2074 - val_mse: 9.2074 - val_mae: 1.4930 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 6s - loss: 15.8565 - mse: 15.8565 - mae: 1.5737 - val_loss: 9.3095 - val_mse: 9.3095 - val_mae: 1.5137 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 6s - loss: 15.8236 - mse: 15.8236 - mae: 1.5705 - val_loss: 9.3878 - val_mse: 9.3878 - val_mae: 1.4953 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 6s - loss: 15.7929 - mse: 15.7929 - mae: 1.5709 - val_loss: 9.2575 - val_mse: 9.2575 - val_mae: 1.5287 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 6s - loss: 15.8142 - mse: 15.8142 - mae: 1.5686 - val_loss: 9.3507 - val_mse: 9.3507 - val_mae: 1.5384 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 6s - loss: 15.8078 - mse: 15.8078 - mae: 1.5744 - val_loss: 9.3690 - val_mse: 9.3690 - val_mae: 1.5461 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
            "Score for fold 3: loss of 9.369023323059082\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 6s - loss: 14.9185 - mse: 14.9185 - mae: 1.5679 - val_loss: 12.9966 - val_mse: 12.9966 - val_mae: 1.5492 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 6s - loss: 14.8563 - mse: 14.8563 - mae: 1.5666 - val_loss: 13.0721 - val_mse: 13.0721 - val_mae: 1.5653 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 6s - loss: 14.8016 - mse: 14.8016 - mae: 1.5646 - val_loss: 12.9186 - val_mse: 12.9186 - val_mae: 1.5485 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 8s - loss: 14.7780 - mse: 14.7780 - mae: 1.5625 - val_loss: 13.1535 - val_mse: 13.1535 - val_mae: 1.5865 - lr: 0.0010 - 8s/epoch - 8ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 6s - loss: 14.7191 - mse: 14.7191 - mae: 1.5626 - val_loss: 13.4956 - val_mse: 13.4956 - val_mae: 1.5668 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 6s - loss: 14.7056 - mse: 14.7056 - mae: 1.5614 - val_loss: 13.4429 - val_mse: 13.4429 - val_mae: 1.5297 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 6s - loss: 14.7285 - mse: 14.7285 - mae: 1.5639 - val_loss: 13.4335 - val_mse: 13.4335 - val_mae: 1.5095 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 6s - loss: 14.6976 - mse: 14.6976 - mae: 1.5606 - val_loss: 13.6172 - val_mse: 13.6172 - val_mae: 1.5337 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
            "Score for fold 4: loss of 13.617177963256836\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 6s - loss: 15.2152 - mse: 15.2152 - mae: 1.5639 - val_loss: 11.5125 - val_mse: 11.5125 - val_mae: 1.5436 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 6s - loss: 15.1754 - mse: 15.1754 - mae: 1.5647 - val_loss: 11.4795 - val_mse: 11.4795 - val_mae: 1.6240 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 6s - loss: 15.1398 - mse: 15.1398 - mae: 1.5625 - val_loss: 11.4171 - val_mse: 11.4171 - val_mae: 1.5614 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 6s - loss: 15.1180 - mse: 15.1180 - mae: 1.5632 - val_loss: 11.4745 - val_mse: 11.4745 - val_mae: 1.5753 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 6s - loss: 15.0573 - mse: 15.0573 - mae: 1.5551 - val_loss: 11.5743 - val_mse: 11.5743 - val_mae: 1.5842 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 6s - loss: 15.0170 - mse: 15.0170 - mae: 1.5570 - val_loss: 11.6740 - val_mse: 11.6740 - val_mae: 1.5424 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 6s - loss: 14.9231 - mse: 14.9231 - mae: 1.5546 - val_loss: 11.6375 - val_mse: 11.6375 - val_mae: 1.6130 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 6s - loss: 15.0066 - mse: 15.0066 - mae: 1.5534 - val_loss: 11.7903 - val_mse: 11.7903 - val_mae: 1.5513 - lr: 0.0010 - 6s/epoch - 6ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-10-15 08:26:12,540]\u001b[0m Finished trial#44 resulted in value: 14.913999999999998. Current best value is 14.552000000000001 with parameters: {'activation': 'relu', 'num_hidden_layer': 3, 'i': 9, 'learning_rate': 0.001648783292938752}.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Score for fold 5: loss of 11.790289878845215\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 6s - loss: 16.8699 - mse: 16.8699 - mae: 1.6581 - val_loss: 10.1092 - val_mse: 10.1092 - val_mae: 1.5716 - lr: 0.0067 - 6s/epoch - 6ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 6s - loss: 16.5281 - mse: 16.5281 - mae: 1.6254 - val_loss: 10.2282 - val_mse: 10.2282 - val_mae: 1.5150 - lr: 0.0067 - 6s/epoch - 6ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 6s - loss: 16.4877 - mse: 16.4877 - mae: 1.6200 - val_loss: 10.1689 - val_mse: 10.1689 - val_mae: 1.5319 - lr: 0.0067 - 6s/epoch - 6ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 6s - loss: 16.3459 - mse: 16.3459 - mae: 1.6250 - val_loss: 10.1556 - val_mse: 10.1556 - val_mae: 1.6971 - lr: 0.0067 - 6s/epoch - 6ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 6s - loss: 16.4559 - mse: 16.4559 - mae: 1.6276 - val_loss: 10.4402 - val_mse: 10.4402 - val_mae: 1.5437 - lr: 0.0067 - 6s/epoch - 6ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 8s - loss: 16.3092 - mse: 16.3092 - mae: 1.6274 - val_loss: 10.0037 - val_mse: 10.0037 - val_mae: 1.6156 - lr: 0.0067 - 8s/epoch - 8ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 6s - loss: 16.5568 - mse: 16.5568 - mae: 1.6274 - val_loss: 10.4353 - val_mse: 10.4353 - val_mae: 1.5236 - lr: 0.0067 - 6s/epoch - 6ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 6s - loss: 16.3281 - mse: 16.3281 - mae: 1.6337 - val_loss: 10.1703 - val_mse: 10.1703 - val_mae: 1.5766 - lr: 0.0067 - 6s/epoch - 6ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 6s - loss: 16.4029 - mse: 16.4029 - mae: 1.6302 - val_loss: 10.0581 - val_mse: 10.0581 - val_mae: 1.5593 - lr: 0.0067 - 6s/epoch - 6ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 6s - loss: 16.4360 - mse: 16.4360 - mae: 1.6304 - val_loss: 10.4593 - val_mse: 10.4593 - val_mae: 1.5088 - lr: 0.0067 - 6s/epoch - 6ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 6s - loss: 16.4920 - mse: 16.4920 - mae: 1.6276 - val_loss: 10.1032 - val_mse: 10.1032 - val_mae: 1.5461 - lr: 0.0067 - 6s/epoch - 6ms/step\n",
            "Score for fold 1: loss of 10.103212356567383\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 6s - loss: 15.8309 - mse: 15.8309 - mae: 1.6180 - val_loss: 11.8232 - val_mse: 11.8232 - val_mae: 1.6291 - lr: 0.0013 - 6s/epoch - 6ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 6s - loss: 15.8220 - mse: 15.8220 - mae: 1.6206 - val_loss: 11.8334 - val_mse: 11.8334 - val_mae: 1.5817 - lr: 0.0013 - 6s/epoch - 6ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 6s - loss: 15.8751 - mse: 15.8751 - mae: 1.6151 - val_loss: 11.8664 - val_mse: 11.8664 - val_mae: 1.6419 - lr: 0.0013 - 6s/epoch - 6ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 6s - loss: 15.8788 - mse: 15.8788 - mae: 1.6208 - val_loss: 11.8695 - val_mse: 11.8695 - val_mae: 1.5697 - lr: 0.0013 - 6s/epoch - 6ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 6s - loss: 15.8335 - mse: 15.8335 - mae: 1.6211 - val_loss: 11.8438 - val_mse: 11.8438 - val_mae: 1.5722 - lr: 0.0013 - 6s/epoch - 6ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 6s - loss: 15.8925 - mse: 15.8925 - mae: 1.6154 - val_loss: 11.9061 - val_mse: 11.9061 - val_mae: 1.6654 - lr: 0.0013 - 6s/epoch - 6ms/step\n",
            "Score for fold 2: loss of 11.906147003173828\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 6s - loss: 14.7792 - mse: 14.7792 - mae: 1.6024 - val_loss: 16.5465 - val_mse: 16.5465 - val_mae: 1.6038 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 6s - loss: 14.7325 - mse: 14.7325 - mae: 1.5980 - val_loss: 16.4103 - val_mse: 16.4103 - val_mae: 1.7035 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 6s - loss: 14.6456 - mse: 14.6456 - mae: 1.5967 - val_loss: 16.4042 - val_mse: 16.4042 - val_mae: 1.7108 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 6s - loss: 14.7904 - mse: 14.7904 - mae: 1.6006 - val_loss: 16.5158 - val_mse: 16.5158 - val_mae: 1.6126 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 6s - loss: 14.7288 - mse: 14.7288 - mae: 1.6004 - val_loss: 16.5296 - val_mse: 16.5296 - val_mae: 1.6076 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 6s - loss: 14.8199 - mse: 14.8199 - mae: 1.5962 - val_loss: 16.4712 - val_mse: 16.4712 - val_mae: 1.6436 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 6s - loss: 14.8101 - mse: 14.8101 - mae: 1.5976 - val_loss: 16.5343 - val_mse: 16.5343 - val_mae: 1.6249 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 6s - loss: 14.7953 - mse: 14.7953 - mae: 1.6006 - val_loss: 16.4453 - val_mse: 16.4453 - val_mae: 1.6915 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
            "Score for fold 3: loss of 16.445283889770508\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 6s - loss: 15.3293 - mse: 15.3293 - mae: 1.6103 - val_loss: 14.2688 - val_mse: 14.2688 - val_mae: 1.6418 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 6s - loss: 15.3359 - mse: 15.3359 - mae: 1.6202 - val_loss: 14.1997 - val_mse: 14.1997 - val_mae: 1.6450 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 6s - loss: 15.3447 - mse: 15.3447 - mae: 1.6140 - val_loss: 14.2302 - val_mse: 14.2302 - val_mae: 1.6390 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 6s - loss: 15.2560 - mse: 15.2560 - mae: 1.6148 - val_loss: 14.3445 - val_mse: 14.3445 - val_mae: 1.5877 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 6s - loss: 15.2898 - mse: 15.2898 - mae: 1.6170 - val_loss: 14.6464 - val_mse: 14.6464 - val_mae: 1.5394 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 6s - loss: 15.3518 - mse: 15.3518 - mae: 1.6170 - val_loss: 14.3542 - val_mse: 14.3542 - val_mae: 1.5852 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 6s - loss: 15.3212 - mse: 15.3212 - mae: 1.6142 - val_loss: 14.3618 - val_mse: 14.3618 - val_mae: 1.7417 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
            "Score for fold 4: loss of 14.361834526062012\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 6s - loss: 13.2861 - mse: 13.2861 - mae: 1.6005 - val_loss: 23.0310 - val_mse: 23.0310 - val_mae: 1.5829 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 6s - loss: 13.2532 - mse: 13.2532 - mae: 1.5963 - val_loss: 22.7571 - val_mse: 22.7571 - val_mae: 1.6025 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 6s - loss: 13.2801 - mse: 13.2801 - mae: 1.6021 - val_loss: 22.3856 - val_mse: 22.3856 - val_mae: 1.6480 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 6s - loss: 13.2407 - mse: 13.2407 - mae: 1.5933 - val_loss: 22.8558 - val_mse: 22.8558 - val_mae: 1.6006 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 6s - loss: 13.3038 - mse: 13.3038 - mae: 1.6021 - val_loss: 22.9879 - val_mse: 22.9879 - val_mae: 1.5969 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 6s - loss: 13.2918 - mse: 13.2918 - mae: 1.6018 - val_loss: 22.7588 - val_mse: 22.7588 - val_mae: 1.6104 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 6s - loss: 13.2130 - mse: 13.2130 - mae: 1.5990 - val_loss: 22.2788 - val_mse: 22.2788 - val_mae: 1.6995 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 6s - loss: 13.2154 - mse: 13.2154 - mae: 1.5978 - val_loss: 22.7076 - val_mse: 22.7076 - val_mae: 1.6104 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 6s - loss: 13.1767 - mse: 13.1767 - mae: 1.5980 - val_loss: 22.2505 - val_mse: 22.2505 - val_mae: 1.7314 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 6s - loss: 13.1749 - mse: 13.1749 - mae: 1.6006 - val_loss: 22.4807 - val_mse: 22.4807 - val_mae: 1.6458 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 6s - loss: 13.2785 - mse: 13.2785 - mae: 1.6055 - val_loss: 22.4583 - val_mse: 22.4583 - val_mae: 1.6264 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 6s - loss: 13.3286 - mse: 13.3286 - mae: 1.5976 - val_loss: 22.2453 - val_mse: 22.2453 - val_mae: 1.6897 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
            "Epoch 13/100\n",
            "1000/1000 - 8s - loss: 13.2982 - mse: 13.2982 - mae: 1.5994 - val_loss: 22.5854 - val_mse: 22.5854 - val_mae: 1.6200 - lr: 0.0010 - 8s/epoch - 8ms/step\n",
            "Epoch 14/100\n",
            "1000/1000 - 6s - loss: 13.2855 - mse: 13.2855 - mae: 1.6028 - val_loss: 22.2483 - val_mse: 22.2483 - val_mae: 1.6961 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
            "Epoch 15/100\n",
            "1000/1000 - 6s - loss: 13.2743 - mse: 13.2743 - mae: 1.5955 - val_loss: 22.6846 - val_mse: 22.6846 - val_mae: 1.6213 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
            "Epoch 16/100\n",
            "1000/1000 - 6s - loss: 13.2741 - mse: 13.2741 - mae: 1.6059 - val_loss: 22.2694 - val_mse: 22.2694 - val_mae: 1.6967 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
            "Epoch 17/100\n",
            "1000/1000 - 6s - loss: 13.2883 - mse: 13.2883 - mae: 1.6043 - val_loss: 22.8710 - val_mse: 22.8710 - val_mae: 1.5970 - lr: 0.0010 - 6s/epoch - 6ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-10-15 08:31:58,928]\u001b[0m Finished trial#45 resulted in value: 15.138. Current best value is 14.552000000000001 with parameters: {'activation': 'relu', 'num_hidden_layer': 3, 'i': 9, 'learning_rate': 0.001648783292938752}.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Score for fold 5: loss of 22.87099266052246\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 20s - loss: 15.5650 - mse: 15.5650 - mae: 1.6322 - val_loss: 15.6827 - val_mse: 15.6827 - val_mae: 1.5937 - lr: 0.0027 - 20s/epoch - 20ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 19s - loss: 15.2799 - mse: 15.2799 - mae: 1.6114 - val_loss: 15.6036 - val_mse: 15.6036 - val_mae: 1.6741 - lr: 0.0027 - 19s/epoch - 19ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 19s - loss: 15.1195 - mse: 15.1195 - mae: 1.6008 - val_loss: 15.8480 - val_mse: 15.8480 - val_mae: 1.7181 - lr: 0.0027 - 19s/epoch - 19ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 19s - loss: 15.1227 - mse: 15.1227 - mae: 1.6033 - val_loss: 15.1476 - val_mse: 15.1476 - val_mae: 1.6481 - lr: 0.0027 - 19s/epoch - 19ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 18s - loss: 15.0720 - mse: 15.0720 - mae: 1.5983 - val_loss: 14.9258 - val_mse: 14.9258 - val_mae: 1.5713 - lr: 0.0027 - 18s/epoch - 18ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 19s - loss: 15.0245 - mse: 15.0245 - mae: 1.5876 - val_loss: 14.8753 - val_mse: 14.8753 - val_mae: 1.5462 - lr: 0.0027 - 19s/epoch - 19ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 20s - loss: 15.0424 - mse: 15.0424 - mae: 1.5864 - val_loss: 14.9351 - val_mse: 14.9351 - val_mae: 1.6359 - lr: 0.0027 - 20s/epoch - 20ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 19s - loss: 15.0113 - mse: 15.0113 - mae: 1.5938 - val_loss: 15.1325 - val_mse: 15.1325 - val_mae: 1.5971 - lr: 0.0027 - 19s/epoch - 19ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 19s - loss: 15.0317 - mse: 15.0317 - mae: 1.5946 - val_loss: 15.0878 - val_mse: 15.0878 - val_mae: 1.6085 - lr: 0.0027 - 19s/epoch - 19ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 19s - loss: 14.9677 - mse: 14.9677 - mae: 1.5852 - val_loss: 15.2410 - val_mse: 15.2410 - val_mae: 1.6196 - lr: 0.0027 - 19s/epoch - 19ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 19s - loss: 14.9484 - mse: 14.9484 - mae: 1.5806 - val_loss: 15.1975 - val_mse: 15.1975 - val_mae: 1.6579 - lr: 0.0027 - 19s/epoch - 19ms/step\n",
            "Score for fold 1: loss of 15.197537422180176\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 19s - loss: 15.9153 - mse: 15.9153 - mae: 1.5891 - val_loss: 10.5388 - val_mse: 10.5388 - val_mae: 1.5044 - lr: 0.0010 - 19s/epoch - 19ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 19s - loss: 15.8274 - mse: 15.8274 - mae: 1.5866 - val_loss: 10.6040 - val_mse: 10.6040 - val_mae: 1.5105 - lr: 0.0010 - 19s/epoch - 19ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 19s - loss: 15.8018 - mse: 15.8018 - mae: 1.5784 - val_loss: 10.5980 - val_mse: 10.5980 - val_mae: 1.5431 - lr: 0.0010 - 19s/epoch - 19ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 19s - loss: 15.7279 - mse: 15.7279 - mae: 1.5784 - val_loss: 10.5896 - val_mse: 10.5896 - val_mae: 1.5248 - lr: 0.0010 - 19s/epoch - 19ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 19s - loss: 15.7240 - mse: 15.7240 - mae: 1.5770 - val_loss: 10.6047 - val_mse: 10.6047 - val_mae: 1.5050 - lr: 0.0010 - 19s/epoch - 19ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 19s - loss: 15.6555 - mse: 15.6555 - mae: 1.5758 - val_loss: 10.6196 - val_mse: 10.6196 - val_mae: 1.4946 - lr: 0.0010 - 19s/epoch - 19ms/step\n",
            "Score for fold 2: loss of 10.619641304016113\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 19s - loss: 14.3687 - mse: 14.3687 - mae: 1.5532 - val_loss: 16.0969 - val_mse: 16.0969 - val_mae: 1.5286 - lr: 0.0010 - 19s/epoch - 19ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 19s - loss: 14.3414 - mse: 14.3414 - mae: 1.5530 - val_loss: 16.1935 - val_mse: 16.1935 - val_mae: 1.5518 - lr: 0.0010 - 19s/epoch - 19ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 19s - loss: 14.2624 - mse: 14.2624 - mae: 1.5508 - val_loss: 16.0319 - val_mse: 16.0319 - val_mae: 1.5709 - lr: 0.0010 - 19s/epoch - 19ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 19s - loss: 14.2420 - mse: 14.2420 - mae: 1.5487 - val_loss: 15.9178 - val_mse: 15.9178 - val_mae: 1.5854 - lr: 0.0010 - 19s/epoch - 19ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 20s - loss: 14.1801 - mse: 14.1801 - mae: 1.5459 - val_loss: 15.9273 - val_mse: 15.9273 - val_mae: 1.5919 - lr: 0.0010 - 20s/epoch - 20ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 19s - loss: 14.1453 - mse: 14.1453 - mae: 1.5457 - val_loss: 16.0425 - val_mse: 16.0425 - val_mae: 1.5658 - lr: 0.0010 - 19s/epoch - 19ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 19s - loss: 14.1625 - mse: 14.1625 - mae: 1.5444 - val_loss: 15.7370 - val_mse: 15.7370 - val_mae: 1.5861 - lr: 0.0010 - 19s/epoch - 19ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 21s - loss: 14.0617 - mse: 14.0617 - mae: 1.5423 - val_loss: 16.1767 - val_mse: 16.1767 - val_mae: 1.5782 - lr: 0.0010 - 21s/epoch - 21ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 19s - loss: 14.0018 - mse: 14.0018 - mae: 1.5405 - val_loss: 16.0923 - val_mse: 16.0923 - val_mae: 1.6182 - lr: 0.0010 - 19s/epoch - 19ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 19s - loss: 13.9069 - mse: 13.9069 - mae: 1.5378 - val_loss: 15.8257 - val_mse: 15.8257 - val_mae: 1.5696 - lr: 0.0010 - 19s/epoch - 19ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 19s - loss: 13.9267 - mse: 13.9267 - mae: 1.5356 - val_loss: 15.9814 - val_mse: 15.9814 - val_mae: 1.5738 - lr: 0.0010 - 19s/epoch - 19ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 20s - loss: 13.8279 - mse: 13.8279 - mae: 1.5355 - val_loss: 16.2646 - val_mse: 16.2646 - val_mae: 1.5714 - lr: 0.0010 - 20s/epoch - 20ms/step\n",
            "Score for fold 3: loss of 16.26457977294922\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 21s - loss: 14.5428 - mse: 14.5428 - mae: 1.5491 - val_loss: 12.7469 - val_mse: 12.7469 - val_mae: 1.5513 - lr: 0.0010 - 21s/epoch - 21ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 19s - loss: 14.5352 - mse: 14.5352 - mae: 1.5505 - val_loss: 13.0213 - val_mse: 13.0213 - val_mae: 1.5555 - lr: 0.0010 - 19s/epoch - 19ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 19s - loss: 14.4097 - mse: 14.4097 - mae: 1.5439 - val_loss: 13.0756 - val_mse: 13.0756 - val_mae: 1.5276 - lr: 0.0010 - 19s/epoch - 19ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 19s - loss: 14.3292 - mse: 14.3292 - mae: 1.5409 - val_loss: 13.1048 - val_mse: 13.1048 - val_mae: 1.5146 - lr: 0.0010 - 19s/epoch - 19ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 19s - loss: 14.3123 - mse: 14.3123 - mae: 1.5409 - val_loss: 13.0399 - val_mse: 13.0399 - val_mae: 1.5177 - lr: 0.0010 - 19s/epoch - 19ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 19s - loss: 14.1993 - mse: 14.1993 - mae: 1.5357 - val_loss: 13.0883 - val_mse: 13.0883 - val_mae: 1.5541 - lr: 0.0010 - 19s/epoch - 19ms/step\n",
            "Score for fold 4: loss of 13.088274002075195\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 21s - loss: 12.9106 - mse: 12.9106 - mae: 1.5319 - val_loss: 18.3763 - val_mse: 18.3763 - val_mae: 1.5592 - lr: 0.0010 - 21s/epoch - 21ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 20s - loss: 12.8324 - mse: 12.8324 - mae: 1.5294 - val_loss: 18.4498 - val_mse: 18.4498 - val_mae: 1.5765 - lr: 0.0010 - 20s/epoch - 20ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 19s - loss: 12.7860 - mse: 12.7860 - mae: 1.5268 - val_loss: 18.5739 - val_mse: 18.5739 - val_mae: 1.5489 - lr: 0.0010 - 19s/epoch - 19ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 19s - loss: 12.7943 - mse: 12.7943 - mae: 1.5193 - val_loss: 18.8128 - val_mse: 18.8128 - val_mae: 1.5721 - lr: 0.0010 - 19s/epoch - 19ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 19s - loss: 12.7686 - mse: 12.7686 - mae: 1.5151 - val_loss: 18.5184 - val_mse: 18.5184 - val_mae: 1.5943 - lr: 0.0010 - 19s/epoch - 19ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 20s - loss: 12.6435 - mse: 12.6435 - mae: 1.5169 - val_loss: 18.5850 - val_mse: 18.5850 - val_mae: 1.5453 - lr: 0.0010 - 20s/epoch - 20ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-10-15 08:47:57,991]\u001b[0m Finished trial#46 resulted in value: 14.75. Current best value is 14.552000000000001 with parameters: {'activation': 'relu', 'num_hidden_layer': 3, 'i': 9, 'learning_rate': 0.001648783292938752}.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Score for fold 5: loss of 18.584997177124023\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 21s - loss: 16.2755 - mse: 16.2755 - mae: 1.6469 - val_loss: 12.2009 - val_mse: 12.2009 - val_mae: 1.6563 - lr: 0.0028 - 21s/epoch - 21ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 20s - loss: 16.0629 - mse: 16.0629 - mae: 1.6185 - val_loss: 11.9752 - val_mse: 11.9752 - val_mae: 1.5700 - lr: 0.0028 - 20s/epoch - 20ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 19s - loss: 15.9147 - mse: 15.9147 - mae: 1.6076 - val_loss: 12.1107 - val_mse: 12.1107 - val_mae: 1.5662 - lr: 0.0028 - 19s/epoch - 19ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 19s - loss: 15.8803 - mse: 15.8803 - mae: 1.6094 - val_loss: 12.1383 - val_mse: 12.1383 - val_mae: 1.6054 - lr: 0.0028 - 19s/epoch - 19ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 19s - loss: 15.8164 - mse: 15.8164 - mae: 1.6073 - val_loss: 12.1946 - val_mse: 12.1946 - val_mae: 1.6002 - lr: 0.0028 - 19s/epoch - 19ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 19s - loss: 15.8940 - mse: 15.8940 - mae: 1.6074 - val_loss: 12.2525 - val_mse: 12.2525 - val_mae: 1.6189 - lr: 0.0028 - 19s/epoch - 19ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 19s - loss: 15.8018 - mse: 15.8018 - mae: 1.6090 - val_loss: 11.9536 - val_mse: 11.9536 - val_mae: 1.5693 - lr: 0.0028 - 19s/epoch - 19ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 19s - loss: 15.8327 - mse: 15.8327 - mae: 1.6111 - val_loss: 12.0072 - val_mse: 12.0072 - val_mae: 1.6014 - lr: 0.0028 - 19s/epoch - 19ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 20s - loss: 15.7827 - mse: 15.7827 - mae: 1.6148 - val_loss: 12.0812 - val_mse: 12.0812 - val_mae: 1.6597 - lr: 0.0028 - 20s/epoch - 20ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 19s - loss: 15.7711 - mse: 15.7711 - mae: 1.6049 - val_loss: 11.9651 - val_mse: 11.9651 - val_mae: 1.6794 - lr: 0.0028 - 19s/epoch - 19ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 19s - loss: 15.7142 - mse: 15.7142 - mae: 1.6013 - val_loss: 12.1987 - val_mse: 12.1987 - val_mae: 1.5156 - lr: 0.0028 - 19s/epoch - 19ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 19s - loss: 15.6706 - mse: 15.6706 - mae: 1.5939 - val_loss: 12.2330 - val_mse: 12.2330 - val_mae: 1.5808 - lr: 0.0028 - 19s/epoch - 19ms/step\n",
            "Score for fold 1: loss of 12.232965469360352\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 19s - loss: 15.8545 - mse: 15.8545 - mae: 1.5869 - val_loss: 10.8983 - val_mse: 10.8983 - val_mae: 1.5914 - lr: 0.0010 - 19s/epoch - 19ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 20s - loss: 15.7822 - mse: 15.7822 - mae: 1.5806 - val_loss: 10.8398 - val_mse: 10.8398 - val_mae: 1.5539 - lr: 0.0010 - 20s/epoch - 20ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 21s - loss: 15.7020 - mse: 15.7020 - mae: 1.5736 - val_loss: 10.8037 - val_mse: 10.8037 - val_mae: 1.5878 - lr: 0.0010 - 21s/epoch - 21ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 19s - loss: 15.6816 - mse: 15.6816 - mae: 1.5744 - val_loss: 10.7580 - val_mse: 10.7580 - val_mae: 1.5374 - lr: 0.0010 - 19s/epoch - 19ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 19s - loss: 15.6711 - mse: 15.6711 - mae: 1.5668 - val_loss: 10.8389 - val_mse: 10.8389 - val_mae: 1.5432 - lr: 0.0010 - 19s/epoch - 19ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 20s - loss: 15.5673 - mse: 15.5673 - mae: 1.5655 - val_loss: 10.9064 - val_mse: 10.9064 - val_mae: 1.5282 - lr: 0.0010 - 20s/epoch - 20ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 20s - loss: 15.5656 - mse: 15.5656 - mae: 1.5663 - val_loss: 10.8885 - val_mse: 10.8885 - val_mae: 1.5703 - lr: 0.0010 - 20s/epoch - 20ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 20s - loss: 15.5764 - mse: 15.5764 - mae: 1.5616 - val_loss: 10.8315 - val_mse: 10.8315 - val_mae: 1.5260 - lr: 0.0010 - 20s/epoch - 20ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 20s - loss: 15.5769 - mse: 15.5769 - mae: 1.5636 - val_loss: 10.9606 - val_mse: 10.9606 - val_mae: 1.5242 - lr: 0.0010 - 20s/epoch - 20ms/step\n",
            "Score for fold 2: loss of 10.9605712890625\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 20s - loss: 14.5835 - mse: 14.5835 - mae: 1.5550 - val_loss: 15.0003 - val_mse: 15.0003 - val_mae: 1.6031 - lr: 0.0010 - 20s/epoch - 20ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 20s - loss: 14.5309 - mse: 14.5309 - mae: 1.5513 - val_loss: 14.9377 - val_mse: 14.9377 - val_mae: 1.5937 - lr: 0.0010 - 20s/epoch - 20ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 20s - loss: 14.4973 - mse: 14.4973 - mae: 1.5540 - val_loss: 14.8110 - val_mse: 14.8110 - val_mae: 1.5714 - lr: 0.0010 - 20s/epoch - 20ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 20s - loss: 14.4886 - mse: 14.4886 - mae: 1.5474 - val_loss: 14.9298 - val_mse: 14.9298 - val_mae: 1.5378 - lr: 0.0010 - 20s/epoch - 20ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 19s - loss: 14.4402 - mse: 14.4402 - mae: 1.5496 - val_loss: 14.6241 - val_mse: 14.6241 - val_mae: 1.5533 - lr: 0.0010 - 19s/epoch - 19ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 20s - loss: 14.3796 - mse: 14.3796 - mae: 1.5460 - val_loss: 14.7543 - val_mse: 14.7543 - val_mae: 1.6001 - lr: 0.0010 - 20s/epoch - 20ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 22s - loss: 14.3450 - mse: 14.3450 - mae: 1.5423 - val_loss: 14.5771 - val_mse: 14.5771 - val_mae: 1.5746 - lr: 0.0010 - 22s/epoch - 22ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 20s - loss: 14.2934 - mse: 14.2934 - mae: 1.5411 - val_loss: 14.8475 - val_mse: 14.8475 - val_mae: 1.6105 - lr: 0.0010 - 20s/epoch - 20ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 19s - loss: 14.2552 - mse: 14.2552 - mae: 1.5436 - val_loss: 14.8323 - val_mse: 14.8323 - val_mae: 1.5289 - lr: 0.0010 - 19s/epoch - 19ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 20s - loss: 14.2058 - mse: 14.2058 - mae: 1.5379 - val_loss: 14.8174 - val_mse: 14.8174 - val_mae: 1.6391 - lr: 0.0010 - 20s/epoch - 20ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 19s - loss: 14.1958 - mse: 14.1958 - mae: 1.5390 - val_loss: 15.0546 - val_mse: 15.0546 - val_mae: 1.5580 - lr: 0.0010 - 19s/epoch - 19ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 19s - loss: 14.1929 - mse: 14.1929 - mae: 1.5360 - val_loss: 14.7949 - val_mse: 14.7949 - val_mae: 1.5890 - lr: 0.0010 - 19s/epoch - 19ms/step\n",
            "Score for fold 3: loss of 14.794898986816406\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 22s - loss: 12.6808 - mse: 12.6808 - mae: 1.5536 - val_loss: 20.7960 - val_mse: 20.7960 - val_mae: 1.5540 - lr: 0.0010 - 22s/epoch - 22ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 20s - loss: 12.4986 - mse: 12.4986 - mae: 1.5433 - val_loss: 20.8093 - val_mse: 20.8093 - val_mae: 1.5343 - lr: 0.0010 - 20s/epoch - 20ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 20s - loss: 12.5521 - mse: 12.5521 - mae: 1.5433 - val_loss: 21.2036 - val_mse: 21.2036 - val_mae: 1.5086 - lr: 0.0010 - 20s/epoch - 20ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 19s - loss: 12.4011 - mse: 12.4011 - mae: 1.5399 - val_loss: 21.0037 - val_mse: 21.0037 - val_mae: 1.5553 - lr: 0.0010 - 19s/epoch - 19ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 19s - loss: 12.4619 - mse: 12.4619 - mae: 1.5357 - val_loss: 20.9351 - val_mse: 20.9351 - val_mae: 1.5780 - lr: 0.0010 - 19s/epoch - 19ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 19s - loss: 12.3338 - mse: 12.3338 - mae: 1.5345 - val_loss: 21.0082 - val_mse: 21.0082 - val_mae: 1.5961 - lr: 0.0010 - 19s/epoch - 19ms/step\n",
            "Score for fold 4: loss of 21.008207321166992\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 22s - loss: 14.1084 - mse: 14.1084 - mae: 1.5376 - val_loss: 14.1949 - val_mse: 14.1949 - val_mae: 1.5019 - lr: 0.0010 - 22s/epoch - 22ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 20s - loss: 14.1230 - mse: 14.1230 - mae: 1.5355 - val_loss: 14.0736 - val_mse: 14.0736 - val_mae: 1.5659 - lr: 0.0010 - 20s/epoch - 20ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 20s - loss: 13.8664 - mse: 13.8664 - mae: 1.5302 - val_loss: 14.4168 - val_mse: 14.4168 - val_mae: 1.5166 - lr: 0.0010 - 20s/epoch - 20ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 20s - loss: 13.9918 - mse: 13.9918 - mae: 1.5292 - val_loss: 14.2428 - val_mse: 14.2428 - val_mae: 1.5360 - lr: 0.0010 - 20s/epoch - 20ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 20s - loss: 13.9351 - mse: 13.9351 - mae: 1.5281 - val_loss: 14.4492 - val_mse: 14.4492 - val_mae: 1.5848 - lr: 0.0010 - 20s/epoch - 20ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 20s - loss: 13.8215 - mse: 13.8215 - mae: 1.5251 - val_loss: 14.4806 - val_mse: 14.4806 - val_mae: 1.5981 - lr: 0.0010 - 20s/epoch - 20ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 20s - loss: 13.8842 - mse: 13.8842 - mae: 1.5247 - val_loss: 14.4890 - val_mse: 14.4890 - val_mae: 1.5434 - lr: 0.0010 - 20s/epoch - 20ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-10-15 09:04:57,267]\u001b[0m Finished trial#47 resulted in value: 14.696000000000002. Current best value is 14.552000000000001 with parameters: {'activation': 'relu', 'num_hidden_layer': 3, 'i': 9, 'learning_rate': 0.001648783292938752}.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Score for fold 5: loss of 14.488956451416016\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 20s - loss: 16.9003 - mse: 16.9003 - mae: 1.6519 - val_loss: 10.7619 - val_mse: 10.7619 - val_mae: 1.4665 - lr: 0.0026 - 20s/epoch - 20ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 20s - loss: 16.5096 - mse: 16.5096 - mae: 1.6240 - val_loss: 10.6518 - val_mse: 10.6518 - val_mae: 1.6230 - lr: 0.0026 - 20s/epoch - 20ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 19s - loss: 16.5830 - mse: 16.5830 - mae: 1.6206 - val_loss: 10.0838 - val_mse: 10.0838 - val_mae: 1.6448 - lr: 0.0026 - 19s/epoch - 19ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 19s - loss: 16.5102 - mse: 16.5102 - mae: 1.6187 - val_loss: 10.1582 - val_mse: 10.1582 - val_mae: 1.6388 - lr: 0.0026 - 19s/epoch - 19ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 19s - loss: 16.4208 - mse: 16.4208 - mae: 1.6173 - val_loss: 10.1047 - val_mse: 10.1047 - val_mae: 1.6698 - lr: 0.0026 - 19s/epoch - 19ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 19s - loss: 16.3836 - mse: 16.3836 - mae: 1.6147 - val_loss: 10.3847 - val_mse: 10.3847 - val_mae: 1.4793 - lr: 0.0026 - 19s/epoch - 19ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 19s - loss: 16.3633 - mse: 16.3633 - mae: 1.6064 - val_loss: 10.4062 - val_mse: 10.4062 - val_mae: 1.5290 - lr: 0.0026 - 19s/epoch - 19ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 19s - loss: 16.2883 - mse: 16.2883 - mae: 1.6121 - val_loss: 9.9118 - val_mse: 9.9118 - val_mae: 1.6159 - lr: 0.0026 - 19s/epoch - 19ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 21s - loss: 16.2731 - mse: 16.2731 - mae: 1.6154 - val_loss: 9.9426 - val_mse: 9.9426 - val_mae: 1.5704 - lr: 0.0026 - 21s/epoch - 21ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 19s - loss: 16.2491 - mse: 16.2491 - mae: 1.6129 - val_loss: 10.0872 - val_mse: 10.0872 - val_mae: 1.5786 - lr: 0.0026 - 19s/epoch - 19ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 19s - loss: 16.2338 - mse: 16.2338 - mae: 1.6181 - val_loss: 9.8598 - val_mse: 9.8598 - val_mae: 1.5641 - lr: 0.0026 - 19s/epoch - 19ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 19s - loss: 16.2931 - mse: 16.2931 - mae: 1.6149 - val_loss: 10.0986 - val_mse: 10.0986 - val_mae: 1.5102 - lr: 0.0026 - 19s/epoch - 19ms/step\n",
            "Epoch 13/100\n",
            "1000/1000 - 19s - loss: 16.1407 - mse: 16.1407 - mae: 1.5974 - val_loss: 9.8383 - val_mse: 9.8383 - val_mae: 1.5651 - lr: 0.0026 - 19s/epoch - 19ms/step\n",
            "Epoch 14/100\n",
            "1000/1000 - 19s - loss: 16.1843 - mse: 16.1843 - mae: 1.5979 - val_loss: 9.9936 - val_mse: 9.9936 - val_mae: 1.6354 - lr: 0.0026 - 19s/epoch - 19ms/step\n",
            "Epoch 15/100\n",
            "1000/1000 - 19s - loss: 16.1254 - mse: 16.1254 - mae: 1.5902 - val_loss: 10.0668 - val_mse: 10.0668 - val_mae: 1.5735 - lr: 0.0026 - 19s/epoch - 19ms/step\n",
            "Epoch 16/100\n",
            "1000/1000 - 19s - loss: 16.0788 - mse: 16.0788 - mae: 1.5855 - val_loss: 10.0028 - val_mse: 10.0028 - val_mae: 1.5317 - lr: 0.0026 - 19s/epoch - 19ms/step\n",
            "Epoch 17/100\n",
            "1000/1000 - 21s - loss: 15.9989 - mse: 15.9989 - mae: 1.5817 - val_loss: 10.0446 - val_mse: 10.0446 - val_mae: 1.5939 - lr: 0.0026 - 21s/epoch - 21ms/step\n",
            "Epoch 18/100\n",
            "1000/1000 - 19s - loss: 16.0302 - mse: 16.0302 - mae: 1.5821 - val_loss: 10.1272 - val_mse: 10.1272 - val_mae: 1.5387 - lr: 0.0026 - 19s/epoch - 19ms/step\n",
            "Score for fold 1: loss of 10.127218246459961\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 19s - loss: 14.0133 - mse: 14.0133 - mae: 1.5562 - val_loss: 17.2546 - val_mse: 17.2546 - val_mae: 1.6050 - lr: 0.0010 - 19s/epoch - 19ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 19s - loss: 13.9410 - mse: 13.9410 - mae: 1.5535 - val_loss: 17.2271 - val_mse: 17.2271 - val_mae: 1.6375 - lr: 0.0010 - 19s/epoch - 19ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 19s - loss: 13.9328 - mse: 13.9328 - mae: 1.5471 - val_loss: 17.4736 - val_mse: 17.4736 - val_mae: 1.6067 - lr: 0.0010 - 19s/epoch - 19ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 19s - loss: 13.8739 - mse: 13.8739 - mae: 1.5459 - val_loss: 17.4020 - val_mse: 17.4020 - val_mae: 1.6279 - lr: 0.0010 - 19s/epoch - 19ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 20s - loss: 13.8176 - mse: 13.8176 - mae: 1.5443 - val_loss: 16.8870 - val_mse: 16.8870 - val_mae: 1.6392 - lr: 0.0010 - 20s/epoch - 20ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 21s - loss: 13.7701 - mse: 13.7701 - mae: 1.5432 - val_loss: 17.3552 - val_mse: 17.3552 - val_mae: 1.5922 - lr: 0.0010 - 21s/epoch - 21ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 20s - loss: 13.7087 - mse: 13.7087 - mae: 1.5409 - val_loss: 17.3081 - val_mse: 17.3081 - val_mae: 1.5553 - lr: 0.0010 - 20s/epoch - 20ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 19s - loss: 13.7056 - mse: 13.7056 - mae: 1.5371 - val_loss: 17.4326 - val_mse: 17.4326 - val_mae: 1.6003 - lr: 0.0010 - 19s/epoch - 19ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 19s - loss: 13.6827 - mse: 13.6827 - mae: 1.5392 - val_loss: 16.9483 - val_mse: 16.9483 - val_mae: 1.5678 - lr: 0.0010 - 19s/epoch - 19ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 20s - loss: 13.5888 - mse: 13.5888 - mae: 1.5360 - val_loss: 17.4954 - val_mse: 17.4954 - val_mae: 1.6205 - lr: 0.0010 - 20s/epoch - 20ms/step\n",
            "Score for fold 2: loss of 17.495380401611328\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 20s - loss: 15.3133 - mse: 15.3133 - mae: 1.5470 - val_loss: 10.5895 - val_mse: 10.5895 - val_mae: 1.5473 - lr: 0.0010 - 20s/epoch - 20ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 19s - loss: 15.1621 - mse: 15.1621 - mae: 1.5465 - val_loss: 10.7493 - val_mse: 10.7493 - val_mae: 1.6132 - lr: 0.0010 - 19s/epoch - 19ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 19s - loss: 14.9974 - mse: 14.9974 - mae: 1.5435 - val_loss: 10.9767 - val_mse: 10.9767 - val_mae: 1.5432 - lr: 0.0010 - 19s/epoch - 19ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 21s - loss: 14.9989 - mse: 14.9989 - mae: 1.5399 - val_loss: 10.8697 - val_mse: 10.8697 - val_mae: 1.5381 - lr: 0.0010 - 21s/epoch - 21ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 19s - loss: 14.9851 - mse: 14.9851 - mae: 1.5372 - val_loss: 10.8651 - val_mse: 10.8651 - val_mae: 1.5285 - lr: 0.0010 - 19s/epoch - 19ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 19s - loss: 14.9235 - mse: 14.9235 - mae: 1.5377 - val_loss: 10.9696 - val_mse: 10.9696 - val_mae: 1.6103 - lr: 0.0010 - 19s/epoch - 19ms/step\n",
            "Score for fold 3: loss of 10.969555854797363\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 19s - loss: 12.9873 - mse: 12.9873 - mae: 1.5493 - val_loss: 18.3742 - val_mse: 18.3742 - val_mae: 1.5273 - lr: 0.0010 - 19s/epoch - 19ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 19s - loss: 12.9115 - mse: 12.9115 - mae: 1.5462 - val_loss: 18.4140 - val_mse: 18.4140 - val_mae: 1.5667 - lr: 0.0010 - 19s/epoch - 19ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 19s - loss: 12.9249 - mse: 12.9249 - mae: 1.5420 - val_loss: 18.6650 - val_mse: 18.6650 - val_mae: 1.5383 - lr: 0.0010 - 19s/epoch - 19ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 21s - loss: 12.9219 - mse: 12.9219 - mae: 1.5409 - val_loss: 18.5792 - val_mse: 18.5792 - val_mae: 1.5146 - lr: 0.0010 - 21s/epoch - 21ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 20s - loss: 12.8706 - mse: 12.8706 - mae: 1.5410 - val_loss: 18.6089 - val_mse: 18.6089 - val_mae: 1.5752 - lr: 0.0010 - 20s/epoch - 20ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 19s - loss: 12.7038 - mse: 12.7038 - mae: 1.5369 - val_loss: 18.5716 - val_mse: 18.5716 - val_mae: 1.5627 - lr: 0.0010 - 19s/epoch - 19ms/step\n",
            "Score for fold 4: loss of 18.571603775024414\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 19s - loss: 13.5954 - mse: 13.5954 - mae: 1.5391 - val_loss: 15.7292 - val_mse: 15.7292 - val_mae: 1.5200 - lr: 0.0010 - 19s/epoch - 19ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 19s - loss: 13.5704 - mse: 13.5704 - mae: 1.5377 - val_loss: 15.7015 - val_mse: 15.7015 - val_mae: 1.5066 - lr: 0.0010 - 19s/epoch - 19ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 19s - loss: 13.4006 - mse: 13.4006 - mae: 1.5395 - val_loss: 15.8484 - val_mse: 15.8484 - val_mae: 1.5853 - lr: 0.0010 - 19s/epoch - 19ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 21s - loss: 13.4622 - mse: 13.4622 - mae: 1.5344 - val_loss: 15.8008 - val_mse: 15.8008 - val_mae: 1.6063 - lr: 0.0010 - 21s/epoch - 21ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 20s - loss: 13.4064 - mse: 13.4064 - mae: 1.5309 - val_loss: 15.9361 - val_mse: 15.9361 - val_mae: 1.6077 - lr: 0.0010 - 20s/epoch - 20ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 20s - loss: 13.3927 - mse: 13.3927 - mae: 1.5290 - val_loss: 15.9413 - val_mse: 15.9413 - val_mae: 1.5109 - lr: 0.0010 - 20s/epoch - 20ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 20s - loss: 13.3947 - mse: 13.3947 - mae: 1.5267 - val_loss: 15.9618 - val_mse: 15.9618 - val_mae: 1.5246 - lr: 0.0010 - 20s/epoch - 20ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-10-15 09:21:16,502]\u001b[0m Finished trial#48 resulted in value: 14.626. Current best value is 14.552000000000001 with parameters: {'activation': 'relu', 'num_hidden_layer': 3, 'i': 9, 'learning_rate': 0.001648783292938752}.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Score for fold 5: loss of 15.96181583404541\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 3s - loss: 16.1275 - mse: 16.1275 - mae: 1.6505 - val_loss: 14.8619 - val_mse: 14.8619 - val_mae: 1.6000 - lr: 0.0015 - 3s/epoch - 3ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 16.0216 - mse: 16.0216 - mae: 1.6289 - val_loss: 14.7681 - val_mse: 14.7681 - val_mae: 1.6193 - lr: 0.0015 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 15.9716 - mse: 15.9716 - mae: 1.6314 - val_loss: 14.7108 - val_mse: 14.7108 - val_mae: 1.6682 - lr: 0.0015 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 15.9531 - mse: 15.9531 - mae: 1.6339 - val_loss: 14.6141 - val_mse: 14.6141 - val_mae: 1.7305 - lr: 0.0015 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 15.9678 - mse: 15.9678 - mae: 1.6316 - val_loss: 14.7638 - val_mse: 14.7638 - val_mae: 1.6477 - lr: 0.0015 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 15.9539 - mse: 15.9539 - mae: 1.6308 - val_loss: 14.6193 - val_mse: 14.6193 - val_mae: 1.6787 - lr: 0.0015 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 15.9664 - mse: 15.9664 - mae: 1.6338 - val_loss: 14.6390 - val_mse: 14.6390 - val_mae: 1.6627 - lr: 0.0015 - 2s/epoch - 2ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 2s - loss: 15.9965 - mse: 15.9965 - mae: 1.6338 - val_loss: 14.6938 - val_mse: 14.6938 - val_mae: 1.6269 - lr: 0.0015 - 2s/epoch - 2ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 2s - loss: 15.9663 - mse: 15.9663 - mae: 1.6328 - val_loss: 14.6359 - val_mse: 14.6359 - val_mae: 1.6733 - lr: 0.0015 - 2s/epoch - 2ms/step\n",
            "Score for fold 1: loss of 14.635940551757812\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 13.7299 - mse: 13.7299 - mae: 1.6315 - val_loss: 23.3268 - val_mse: 23.3268 - val_mae: 1.6752 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 13.7649 - mse: 13.7649 - mae: 1.6352 - val_loss: 23.3289 - val_mse: 23.3289 - val_mae: 1.7222 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 13.8091 - mse: 13.8091 - mae: 1.6324 - val_loss: 23.4623 - val_mse: 23.4623 - val_mae: 1.5725 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 13.7640 - mse: 13.7640 - mae: 1.6369 - val_loss: 23.3232 - val_mse: 23.3232 - val_mae: 1.7072 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 13.7503 - mse: 13.7503 - mae: 1.6416 - val_loss: 23.3301 - val_mse: 23.3301 - val_mae: 1.6834 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 13.7285 - mse: 13.7285 - mae: 1.6416 - val_loss: 23.4765 - val_mse: 23.4765 - val_mae: 1.8203 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 13.7406 - mse: 13.7406 - mae: 1.6479 - val_loss: 23.3056 - val_mse: 23.3056 - val_mae: 1.6586 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 2s - loss: 13.7554 - mse: 13.7554 - mae: 1.6358 - val_loss: 23.3178 - val_mse: 23.3178 - val_mae: 1.6925 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 2s - loss: 13.7351 - mse: 13.7351 - mae: 1.6358 - val_loss: 23.3938 - val_mse: 23.3938 - val_mae: 1.7647 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 2s - loss: 13.7970 - mse: 13.7970 - mae: 1.6483 - val_loss: 23.3470 - val_mse: 23.3470 - val_mae: 1.6210 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 2s - loss: 13.7887 - mse: 13.7887 - mae: 1.6400 - val_loss: 23.4883 - val_mse: 23.4883 - val_mae: 1.8078 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 2s - loss: 13.7355 - mse: 13.7355 - mae: 1.6426 - val_loss: 23.4810 - val_mse: 23.4810 - val_mae: 1.7681 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Score for fold 2: loss of 23.480947494506836\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 16.7048 - mse: 16.7048 - mae: 1.6519 - val_loss: 11.7882 - val_mse: 11.7882 - val_mae: 1.5616 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 16.6646 - mse: 16.6646 - mae: 1.6545 - val_loss: 11.7120 - val_mse: 11.7120 - val_mae: 1.5923 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 16.6589 - mse: 16.6589 - mae: 1.6496 - val_loss: 11.6997 - val_mse: 11.6997 - val_mae: 1.6012 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 16.6995 - mse: 16.6995 - mae: 1.6479 - val_loss: 11.7327 - val_mse: 11.7327 - val_mae: 1.6644 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 16.6878 - mse: 16.6878 - mae: 1.6441 - val_loss: 11.9953 - val_mse: 11.9953 - val_mae: 1.5387 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 16.7678 - mse: 16.7678 - mae: 1.6565 - val_loss: 11.7215 - val_mse: 11.7215 - val_mae: 1.7104 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 16.7034 - mse: 16.7034 - mae: 1.6564 - val_loss: 11.7916 - val_mse: 11.7916 - val_mae: 1.7471 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 2s - loss: 16.7444 - mse: 16.7444 - mae: 1.6437 - val_loss: 11.7093 - val_mse: 11.7093 - val_mae: 1.6492 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Score for fold 3: loss of 11.709334373474121\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 16.1054 - mse: 16.1054 - mae: 1.6481 - val_loss: 14.3811 - val_mse: 14.3811 - val_mae: 1.7181 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 16.0434 - mse: 16.0434 - mae: 1.6485 - val_loss: 14.5757 - val_mse: 14.5757 - val_mae: 1.5802 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 4s - loss: 16.0727 - mse: 16.0727 - mae: 1.6489 - val_loss: 14.7158 - val_mse: 14.7158 - val_mae: 1.5651 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 16.0946 - mse: 16.0946 - mae: 1.6536 - val_loss: 14.3044 - val_mse: 14.3044 - val_mae: 1.6578 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 16.0171 - mse: 16.0171 - mae: 1.6482 - val_loss: 14.2994 - val_mse: 14.2994 - val_mae: 1.7286 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 16.0836 - mse: 16.0836 - mae: 1.6545 - val_loss: 14.3322 - val_mse: 14.3322 - val_mae: 1.6691 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 16.0337 - mse: 16.0337 - mae: 1.6435 - val_loss: 14.3671 - val_mse: 14.3671 - val_mae: 1.6389 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 2s - loss: 16.0123 - mse: 16.0123 - mae: 1.6444 - val_loss: 14.4866 - val_mse: 14.4866 - val_mae: 1.6016 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 2s - loss: 15.9999 - mse: 15.9999 - mae: 1.6550 - val_loss: 14.3497 - val_mse: 14.3497 - val_mae: 1.6642 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 2s - loss: 16.0912 - mse: 16.0912 - mae: 1.6381 - val_loss: 14.4538 - val_mse: 14.4538 - val_mae: 1.6365 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Score for fold 4: loss of 14.453753471374512\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 16.1906 - mse: 16.1906 - mae: 1.6597 - val_loss: 14.1075 - val_mse: 14.1075 - val_mae: 1.5889 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 16.1950 - mse: 16.1950 - mae: 1.6582 - val_loss: 14.0999 - val_mse: 14.0999 - val_mae: 1.6130 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 16.1259 - mse: 16.1259 - mae: 1.6568 - val_loss: 14.1154 - val_mse: 14.1154 - val_mae: 1.6431 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 16.1720 - mse: 16.1720 - mae: 1.6491 - val_loss: 14.0853 - val_mse: 14.0853 - val_mae: 1.6528 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 16.1465 - mse: 16.1465 - mae: 1.6631 - val_loss: 14.0795 - val_mse: 14.0795 - val_mae: 1.6482 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 16.1606 - mse: 16.1606 - mae: 1.6523 - val_loss: 14.2792 - val_mse: 14.2792 - val_mae: 1.7407 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 16.1132 - mse: 16.1132 - mae: 1.6492 - val_loss: 14.1613 - val_mse: 14.1613 - val_mae: 1.7356 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 2s - loss: 16.1134 - mse: 16.1134 - mae: 1.6622 - val_loss: 14.0948 - val_mse: 14.0948 - val_mae: 1.6285 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 2s - loss: 16.0760 - mse: 16.0760 - mae: 1.6510 - val_loss: 14.1626 - val_mse: 14.1626 - val_mae: 1.5822 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 2s - loss: 16.1186 - mse: 16.1186 - mae: 1.6437 - val_loss: 14.8407 - val_mse: 14.8407 - val_mae: 1.5207 - lr: 0.0010 - 2s/epoch - 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-10-15 09:23:30,124]\u001b[0m Finished trial#49 resulted in value: 15.824000000000002. Current best value is 14.552000000000001 with parameters: {'activation': 'relu', 'num_hidden_layer': 3, 'i': 9, 'learning_rate': 0.001648783292938752}.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Score for fold 5: loss of 14.840733528137207\n"
          ]
        }
      ],
      "source": [
        "model_list=[]\n",
        "history_list=[]\n",
        "study_name = 'NN_study_4'\n",
        "study = optuna.create_study(study_name=study_name, load_if_exists=True)\n",
        "func = lambda trial: objective(trial,training_shuffled3,labelsForTrain_shuffled3)\n",
        "study.optimize(func, n_trials=50,)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RxAKqk4RyJ39",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f78e84b3-6db2-4659-8cec-50fdd18877a2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "1250/1250 - 4s - loss: 15.3966 - mse: 15.3966 - mae: 1.6204 - val_loss: 13.5829 - val_mse: 13.5829 - val_mae: 1.5739 - lr: 0.0016 - 4s/epoch - 3ms/step\n",
            "Epoch 2/20\n",
            "1250/1250 - 4s - loss: 15.2353 - mse: 15.2353 - mae: 1.6009 - val_loss: 13.7331 - val_mse: 13.7331 - val_mae: 1.7328 - lr: 0.0016 - 4s/epoch - 3ms/step\n",
            "Epoch 3/20\n",
            "1250/1250 - 4s - loss: 15.1292 - mse: 15.1292 - mae: 1.6030 - val_loss: 13.5205 - val_mse: 13.5205 - val_mae: 1.5912 - lr: 0.0016 - 4s/epoch - 3ms/step\n",
            "Epoch 4/20\n",
            "1250/1250 - 3s - loss: 15.1487 - mse: 15.1487 - mae: 1.6013 - val_loss: 13.6057 - val_mse: 13.6057 - val_mae: 1.5702 - lr: 0.0016 - 3s/epoch - 3ms/step\n",
            "Epoch 5/20\n",
            "1250/1250 - 3s - loss: 15.0921 - mse: 15.0921 - mae: 1.5955 - val_loss: 13.3360 - val_mse: 13.3360 - val_mae: 1.6124 - lr: 0.0016 - 3s/epoch - 3ms/step\n",
            "Epoch 6/20\n",
            "1250/1250 - 3s - loss: 15.0889 - mse: 15.0889 - mae: 1.5906 - val_loss: 13.6683 - val_mse: 13.6683 - val_mae: 1.6144 - lr: 0.0016 - 3s/epoch - 3ms/step\n",
            "Epoch 7/20\n",
            "1250/1250 - 4s - loss: 15.0983 - mse: 15.0983 - mae: 1.5869 - val_loss: 13.2312 - val_mse: 13.2312 - val_mae: 1.6695 - lr: 0.0016 - 4s/epoch - 3ms/step\n",
            "Epoch 8/20\n",
            "1250/1250 - 3s - loss: 15.0369 - mse: 15.0369 - mae: 1.5895 - val_loss: 13.4372 - val_mse: 13.4372 - val_mae: 1.5718 - lr: 0.0016 - 3s/epoch - 3ms/step\n",
            "Epoch 9/20\n",
            "1250/1250 - 4s - loss: 15.0156 - mse: 15.0156 - mae: 1.5863 - val_loss: 13.2734 - val_mse: 13.2734 - val_mae: 1.6379 - lr: 0.0016 - 4s/epoch - 3ms/step\n",
            "Epoch 10/20\n",
            "1250/1250 - 3s - loss: 14.9504 - mse: 14.9504 - mae: 1.5793 - val_loss: 13.1951 - val_mse: 13.1951 - val_mae: 1.6070 - lr: 0.0016 - 3s/epoch - 3ms/step\n",
            "Epoch 11/20\n",
            "1250/1250 - 3s - loss: 14.9310 - mse: 14.9310 - mae: 1.5805 - val_loss: 13.6083 - val_mse: 13.6083 - val_mae: 1.5741 - lr: 0.0016 - 3s/epoch - 3ms/step\n",
            "Epoch 12/20\n",
            "1250/1250 - 3s - loss: 14.8696 - mse: 14.8696 - mae: 1.5714 - val_loss: 13.1636 - val_mse: 13.1636 - val_mae: 1.5869 - lr: 0.0016 - 3s/epoch - 3ms/step\n",
            "Epoch 13/20\n",
            "1250/1250 - 4s - loss: 14.9265 - mse: 14.9265 - mae: 1.5724 - val_loss: 13.2122 - val_mse: 13.2122 - val_mae: 1.6001 - lr: 0.0016 - 4s/epoch - 3ms/step\n",
            "Epoch 14/20\n",
            "1250/1250 - 3s - loss: 14.8590 - mse: 14.8590 - mae: 1.5677 - val_loss: 13.4533 - val_mse: 13.4533 - val_mae: 1.5772 - lr: 0.0016 - 3s/epoch - 3ms/step\n",
            "Epoch 15/20\n",
            "1250/1250 - 3s - loss: 14.8491 - mse: 14.8491 - mae: 1.5684 - val_loss: 13.2273 - val_mse: 13.2273 - val_mae: 1.6484 - lr: 0.0016 - 3s/epoch - 3ms/step\n",
            "Epoch 16/20\n",
            "1250/1250 - 4s - loss: 14.8135 - mse: 14.8135 - mae: 1.5653 - val_loss: 13.1280 - val_mse: 13.1280 - val_mae: 1.5797 - lr: 0.0016 - 4s/epoch - 3ms/step\n",
            "Epoch 17/20\n",
            "1250/1250 - 4s - loss: 14.7823 - mse: 14.7823 - mae: 1.5653 - val_loss: 13.2080 - val_mse: 13.2080 - val_mae: 1.6379 - lr: 0.0016 - 4s/epoch - 3ms/step\n",
            "Epoch 18/20\n",
            "1250/1250 - 3s - loss: 14.6959 - mse: 14.6959 - mae: 1.5696 - val_loss: 13.2296 - val_mse: 13.2296 - val_mae: 1.6330 - lr: 0.0016 - 3s/epoch - 3ms/step\n",
            "Epoch 19/20\n",
            "1250/1250 - 4s - loss: 14.6703 - mse: 14.6703 - mae: 1.5654 - val_loss: 13.2192 - val_mse: 13.2192 - val_mae: 1.5949 - lr: 0.0016 - 4s/epoch - 3ms/step\n",
            "Epoch 20/20\n",
            "1250/1250 - 4s - loss: 14.7345 - mse: 14.7345 - mae: 1.5650 - val_loss: 13.0573 - val_mse: 13.0573 - val_mae: 1.5547 - lr: 0.0016 - 4s/epoch - 3ms/step\n"
          ]
        }
      ],
      "source": [
        "#{'activation': 'relu', 'num_hidden_layer': 3, 'i': 9, 'learning_rate': 0.001648783292938752}.\n",
        "optimizer = Adam(learning_rate= 0.001648783292938752 ,clipnorm=1.0)\n",
        "model_4 = create_model(activation=\"relu\",num_hidden_layer=3,num_hidden_unit=512)\n",
        "\n",
        "es = EarlyStopping(monitor='mse', patience=5)\n",
        "reduce_lr = ReduceLROnPlateau(monitor='val_mse', factor=0.2,\n",
        "                              patience=20, min_lr=0.001)\n",
        "model_4.compile(loss='mse',optimizer=optimizer,metrics=['mse','mae'])\n",
        "history = model_4.fit(training,labelsForTrain,\n",
        "                batch_size=64,\n",
        "                epochs=20,\n",
        "                verbose=2,\n",
        "                validation_data=(valing,labelsForVal),\n",
        "                validation_batch_size=64,\n",
        "                callbacks=[es,reduce_lr])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6h-GPMVRyPru",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c0d457e2-bf2e-472b-a7da-8fdf136523d1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "313/313 [==============================] - 0s 1ms/step - loss: 11.7363 - mse: 11.7363 - mae: 1.5377\n"
          ]
        }
      ],
      "source": [
        "results_model4 = model_4.evaluate(testing, labelsForTest, batch_size=32)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dV7ly-XIyTWR"
      },
      "source": [
        "## Shuffle Repetation 5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MJ-DVmmlyYDk"
      },
      "outputs": [],
      "source": [
        "# Shuffle the data after train\n",
        "shuffled4 = shuffle(train_df, random_state=4)\n",
        "training_shuffled4,labelsForTrain_shuffled4=process_shuffle_dataset(shuffled4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "54_G8WmeyesV",
        "outputId": "f47feb0a-c6fb-4b45-9c36-3d11c3937832"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 3s - loss: 18.1175 - mse: 18.1175 - mae: 1.7368 - val_loss: 15.2867 - val_mse: 15.2867 - val_mae: 1.6144 - lr: 1.9163e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 4s - loss: 15.9689 - mse: 15.9689 - mae: 1.6161 - val_loss: 15.0112 - val_mse: 15.0112 - val_mae: 1.6072 - lr: 1.9163e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 3s - loss: 15.7510 - mse: 15.7510 - mae: 1.6028 - val_loss: 14.8815 - val_mse: 14.8815 - val_mae: 1.6053 - lr: 1.9163e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 3s - loss: 15.6120 - mse: 15.6120 - mae: 1.6000 - val_loss: 14.8071 - val_mse: 14.8071 - val_mae: 1.5896 - lr: 1.9163e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 3s - loss: 15.5397 - mse: 15.5397 - mae: 1.5938 - val_loss: 14.7412 - val_mse: 14.7412 - val_mae: 1.5965 - lr: 1.9163e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 15.4823 - mse: 15.4823 - mae: 1.5919 - val_loss: 14.7149 - val_mse: 14.7149 - val_mae: 1.5815 - lr: 1.9163e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 15.4499 - mse: 15.4499 - mae: 1.5885 - val_loss: 14.6665 - val_mse: 14.6665 - val_mae: 1.5883 - lr: 1.9163e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 2s - loss: 15.3962 - mse: 15.3962 - mae: 1.5871 - val_loss: 14.6696 - val_mse: 14.6696 - val_mae: 1.5869 - lr: 1.9163e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 2s - loss: 15.3906 - mse: 15.3906 - mae: 1.5890 - val_loss: 14.6188 - val_mse: 14.6188 - val_mae: 1.5876 - lr: 1.9163e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 2s - loss: 15.3562 - mse: 15.3562 - mae: 1.5860 - val_loss: 14.5973 - val_mse: 14.5973 - val_mae: 1.6001 - lr: 1.9163e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 2s - loss: 15.3513 - mse: 15.3513 - mae: 1.5893 - val_loss: 14.5830 - val_mse: 14.5830 - val_mae: 1.5972 - lr: 1.9163e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 2s - loss: 15.3369 - mse: 15.3369 - mae: 1.5882 - val_loss: 14.5696 - val_mse: 14.5696 - val_mae: 1.6001 - lr: 1.9163e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 13/100\n",
            "1000/1000 - 2s - loss: 15.3080 - mse: 15.3080 - mae: 1.5880 - val_loss: 14.5507 - val_mse: 14.5507 - val_mae: 1.5981 - lr: 1.9163e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 14/100\n",
            "1000/1000 - 2s - loss: 15.3101 - mse: 15.3101 - mae: 1.5873 - val_loss: 14.5500 - val_mse: 14.5500 - val_mae: 1.5986 - lr: 1.9163e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 15/100\n",
            "1000/1000 - 2s - loss: 15.2977 - mse: 15.2977 - mae: 1.5871 - val_loss: 14.5233 - val_mse: 14.5233 - val_mae: 1.6035 - lr: 1.9163e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 16/100\n",
            "1000/1000 - 2s - loss: 15.2789 - mse: 15.2789 - mae: 1.5880 - val_loss: 14.5072 - val_mse: 14.5072 - val_mae: 1.6010 - lr: 1.9163e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 17/100\n",
            "1000/1000 - 2s - loss: 15.2620 - mse: 15.2620 - mae: 1.5908 - val_loss: 14.5150 - val_mse: 14.5150 - val_mae: 1.5938 - lr: 1.9163e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 18/100\n",
            "1000/1000 - 2s - loss: 15.2712 - mse: 15.2712 - mae: 1.5878 - val_loss: 14.5134 - val_mse: 14.5134 - val_mae: 1.5934 - lr: 1.9163e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 19/100\n",
            "1000/1000 - 2s - loss: 15.2600 - mse: 15.2600 - mae: 1.5845 - val_loss: 14.4842 - val_mse: 14.4842 - val_mae: 1.6102 - lr: 1.9163e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 20/100\n",
            "1000/1000 - 2s - loss: 15.2490 - mse: 15.2490 - mae: 1.5847 - val_loss: 14.4837 - val_mse: 14.4837 - val_mae: 1.6086 - lr: 1.9163e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 21/100\n",
            "1000/1000 - 2s - loss: 15.2404 - mse: 15.2404 - mae: 1.5850 - val_loss: 14.4906 - val_mse: 14.4906 - val_mae: 1.5942 - lr: 1.9163e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 22/100\n",
            "1000/1000 - 2s - loss: 15.2395 - mse: 15.2395 - mae: 1.5851 - val_loss: 14.4836 - val_mse: 14.4836 - val_mae: 1.5899 - lr: 1.9163e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 23/100\n",
            "1000/1000 - 2s - loss: 15.2386 - mse: 15.2386 - mae: 1.5846 - val_loss: 14.4736 - val_mse: 14.4736 - val_mae: 1.5879 - lr: 1.9163e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 24/100\n",
            "1000/1000 - 2s - loss: 15.2470 - mse: 15.2470 - mae: 1.5827 - val_loss: 14.4581 - val_mse: 14.4581 - val_mae: 1.6084 - lr: 1.9163e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 25/100\n",
            "1000/1000 - 2s - loss: 15.2266 - mse: 15.2266 - mae: 1.5858 - val_loss: 14.4384 - val_mse: 14.4384 - val_mae: 1.5974 - lr: 1.9163e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 26/100\n",
            "1000/1000 - 2s - loss: 15.2091 - mse: 15.2091 - mae: 1.5863 - val_loss: 14.4241 - val_mse: 14.4241 - val_mae: 1.6025 - lr: 1.9163e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 27/100\n",
            "1000/1000 - 2s - loss: 15.1968 - mse: 15.1968 - mae: 1.5864 - val_loss: 14.4434 - val_mse: 14.4434 - val_mae: 1.5980 - lr: 1.9163e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 28/100\n",
            "1000/1000 - 2s - loss: 15.2074 - mse: 15.2074 - mae: 1.5818 - val_loss: 14.4564 - val_mse: 14.4564 - val_mae: 1.5887 - lr: 1.9163e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 29/100\n",
            "1000/1000 - 2s - loss: 15.2207 - mse: 15.2207 - mae: 1.5857 - val_loss: 14.4305 - val_mse: 14.4305 - val_mae: 1.5897 - lr: 1.9163e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 30/100\n",
            "1000/1000 - 2s - loss: 15.1995 - mse: 15.1995 - mae: 1.5836 - val_loss: 14.4355 - val_mse: 14.4355 - val_mae: 1.5978 - lr: 1.9163e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 31/100\n",
            "1000/1000 - 2s - loss: 15.1908 - mse: 15.1908 - mae: 1.5853 - val_loss: 14.4179 - val_mse: 14.4179 - val_mae: 1.5977 - lr: 1.9163e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 32/100\n",
            "1000/1000 - 2s - loss: 15.1949 - mse: 15.1949 - mae: 1.5821 - val_loss: 14.4202 - val_mse: 14.4202 - val_mae: 1.5933 - lr: 1.9163e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 33/100\n",
            "1000/1000 - 2s - loss: 15.1880 - mse: 15.1880 - mae: 1.5853 - val_loss: 14.4250 - val_mse: 14.4250 - val_mae: 1.5938 - lr: 1.9163e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 34/100\n",
            "1000/1000 - 2s - loss: 15.1901 - mse: 15.1901 - mae: 1.5839 - val_loss: 14.4202 - val_mse: 14.4202 - val_mae: 1.5960 - lr: 1.9163e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 35/100\n",
            "1000/1000 - 2s - loss: 15.1861 - mse: 15.1861 - mae: 1.5810 - val_loss: 14.4235 - val_mse: 14.4235 - val_mae: 1.5911 - lr: 1.9163e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 36/100\n",
            "1000/1000 - 2s - loss: 15.1799 - mse: 15.1799 - mae: 1.5817 - val_loss: 14.4049 - val_mse: 14.4049 - val_mae: 1.5922 - lr: 1.9163e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 37/100\n",
            "1000/1000 - 2s - loss: 15.1710 - mse: 15.1710 - mae: 1.5821 - val_loss: 14.4009 - val_mse: 14.4009 - val_mae: 1.6019 - lr: 1.9163e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 38/100\n",
            "1000/1000 - 2s - loss: 15.1696 - mse: 15.1696 - mae: 1.5853 - val_loss: 14.4071 - val_mse: 14.4071 - val_mae: 1.5844 - lr: 1.9163e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 39/100\n",
            "1000/1000 - 2s - loss: 15.1655 - mse: 15.1655 - mae: 1.5813 - val_loss: 14.4044 - val_mse: 14.4044 - val_mae: 1.5893 - lr: 1.9163e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 40/100\n",
            "1000/1000 - 2s - loss: 15.1637 - mse: 15.1637 - mae: 1.5817 - val_loss: 14.3948 - val_mse: 14.3948 - val_mae: 1.6011 - lr: 1.9163e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 41/100\n",
            "1000/1000 - 2s - loss: 15.1599 - mse: 15.1599 - mae: 1.5844 - val_loss: 14.4284 - val_mse: 14.4284 - val_mae: 1.5840 - lr: 1.9163e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 42/100\n",
            "1000/1000 - 2s - loss: 15.1696 - mse: 15.1696 - mae: 1.5786 - val_loss: 14.3700 - val_mse: 14.3700 - val_mae: 1.6070 - lr: 1.9163e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 43/100\n",
            "1000/1000 - 2s - loss: 15.1508 - mse: 15.1508 - mae: 1.5819 - val_loss: 14.3872 - val_mse: 14.3872 - val_mae: 1.5899 - lr: 1.9163e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 44/100\n",
            "1000/1000 - 2s - loss: 15.1435 - mse: 15.1435 - mae: 1.5817 - val_loss: 14.3818 - val_mse: 14.3818 - val_mae: 1.5961 - lr: 1.9163e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 45/100\n",
            "1000/1000 - 2s - loss: 15.1491 - mse: 15.1491 - mae: 1.5816 - val_loss: 14.3693 - val_mse: 14.3693 - val_mae: 1.5960 - lr: 1.9163e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 46/100\n",
            "1000/1000 - 2s - loss: 15.1486 - mse: 15.1486 - mae: 1.5803 - val_loss: 14.3630 - val_mse: 14.3630 - val_mae: 1.5955 - lr: 1.9163e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 47/100\n",
            "1000/1000 - 2s - loss: 15.1428 - mse: 15.1428 - mae: 1.5824 - val_loss: 14.3599 - val_mse: 14.3599 - val_mae: 1.5967 - lr: 1.9163e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 48/100\n",
            "1000/1000 - 3s - loss: 15.1324 - mse: 15.1324 - mae: 1.5811 - val_loss: 14.3741 - val_mse: 14.3741 - val_mae: 1.5919 - lr: 1.9163e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 49/100\n",
            "1000/1000 - 2s - loss: 15.1237 - mse: 15.1237 - mae: 1.5807 - val_loss: 14.3809 - val_mse: 14.3809 - val_mae: 1.5814 - lr: 1.9163e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 50/100\n",
            "1000/1000 - 2s - loss: 15.1408 - mse: 15.1408 - mae: 1.5780 - val_loss: 14.3649 - val_mse: 14.3649 - val_mae: 1.5896 - lr: 1.9163e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 51/100\n",
            "1000/1000 - 2s - loss: 15.1244 - mse: 15.1244 - mae: 1.5772 - val_loss: 14.3530 - val_mse: 14.3530 - val_mae: 1.6049 - lr: 1.9163e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 52/100\n",
            "1000/1000 - 2s - loss: 15.1337 - mse: 15.1337 - mae: 1.5791 - val_loss: 14.3520 - val_mse: 14.3520 - val_mae: 1.5888 - lr: 1.9163e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 53/100\n",
            "1000/1000 - 2s - loss: 15.1174 - mse: 15.1174 - mae: 1.5802 - val_loss: 14.3599 - val_mse: 14.3599 - val_mae: 1.5978 - lr: 1.9163e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 54/100\n",
            "1000/1000 - 2s - loss: 15.1263 - mse: 15.1263 - mae: 1.5785 - val_loss: 14.3195 - val_mse: 14.3195 - val_mae: 1.6077 - lr: 1.9163e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 55/100\n",
            "1000/1000 - 2s - loss: 15.1031 - mse: 15.1031 - mae: 1.5799 - val_loss: 14.3777 - val_mse: 14.3777 - val_mae: 1.5848 - lr: 1.9163e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 56/100\n",
            "1000/1000 - 2s - loss: 15.1232 - mse: 15.1232 - mae: 1.5775 - val_loss: 14.3404 - val_mse: 14.3404 - val_mae: 1.6059 - lr: 1.9163e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 57/100\n",
            "1000/1000 - 2s - loss: 15.1142 - mse: 15.1142 - mae: 1.5807 - val_loss: 14.3611 - val_mse: 14.3611 - val_mae: 1.5850 - lr: 1.9163e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 58/100\n",
            "1000/1000 - 2s - loss: 15.1177 - mse: 15.1177 - mae: 1.5791 - val_loss: 14.3399 - val_mse: 14.3399 - val_mae: 1.5916 - lr: 1.9163e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 59/100\n",
            "1000/1000 - 2s - loss: 15.1060 - mse: 15.1060 - mae: 1.5798 - val_loss: 14.3856 - val_mse: 14.3856 - val_mae: 1.5662 - lr: 1.9163e-04 - 2s/epoch - 2ms/step\n",
            "Score for fold 1: loss of 14.385565757751465\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 14.3225 - mse: 14.3225 - mae: 1.5880 - val_loss: 17.4782 - val_mse: 17.4782 - val_mae: 1.5647 - lr: 1.9163e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 14.3167 - mse: 14.3167 - mae: 1.5894 - val_loss: 17.4848 - val_mse: 17.4848 - val_mae: 1.5427 - lr: 1.9163e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 14.3091 - mse: 14.3091 - mae: 1.5888 - val_loss: 17.4761 - val_mse: 17.4761 - val_mae: 1.5678 - lr: 1.9163e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 14.3035 - mse: 14.3035 - mae: 1.5879 - val_loss: 17.4883 - val_mse: 17.4883 - val_mae: 1.5615 - lr: 1.9163e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 14.3026 - mse: 14.3026 - mae: 1.5943 - val_loss: 17.4909 - val_mse: 17.4909 - val_mae: 1.5339 - lr: 1.9163e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 14.2975 - mse: 14.2975 - mae: 1.5888 - val_loss: 17.4852 - val_mse: 17.4852 - val_mae: 1.5497 - lr: 1.9163e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 14.2992 - mse: 14.2992 - mae: 1.5883 - val_loss: 17.4838 - val_mse: 17.4838 - val_mae: 1.5465 - lr: 1.9163e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 2s - loss: 14.2956 - mse: 14.2956 - mae: 1.5909 - val_loss: 17.4731 - val_mse: 17.4731 - val_mae: 1.5640 - lr: 1.9163e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 2s - loss: 14.2760 - mse: 14.2760 - mae: 1.5931 - val_loss: 17.4763 - val_mse: 17.4763 - val_mae: 1.5566 - lr: 1.9163e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 2s - loss: 14.2998 - mse: 14.2998 - mae: 1.5849 - val_loss: 17.4760 - val_mse: 17.4760 - val_mae: 1.5872 - lr: 1.9163e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 2s - loss: 14.2806 - mse: 14.2806 - mae: 1.5912 - val_loss: 17.4946 - val_mse: 17.4946 - val_mae: 1.5450 - lr: 1.9163e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 2s - loss: 14.2980 - mse: 14.2980 - mae: 1.5866 - val_loss: 17.4740 - val_mse: 17.4740 - val_mae: 1.5649 - lr: 1.9163e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 13/100\n",
            "1000/1000 - 2s - loss: 14.2861 - mse: 14.2861 - mae: 1.5869 - val_loss: 17.4911 - val_mse: 17.4911 - val_mae: 1.5547 - lr: 1.9163e-04 - 2s/epoch - 2ms/step\n",
            "Score for fold 2: loss of 17.49106788635254\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 15.1819 - mse: 15.1819 - mae: 1.5757 - val_loss: 13.9781 - val_mse: 13.9781 - val_mae: 1.5932 - lr: 1.9163e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 15.1907 - mse: 15.1907 - mae: 1.5775 - val_loss: 13.9685 - val_mse: 13.9685 - val_mae: 1.5853 - lr: 1.9163e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 15.1764 - mse: 15.1764 - mae: 1.5765 - val_loss: 13.9411 - val_mse: 13.9411 - val_mae: 1.5906 - lr: 1.9163e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 15.1630 - mse: 15.1630 - mae: 1.5789 - val_loss: 13.9540 - val_mse: 13.9540 - val_mae: 1.6038 - lr: 1.9163e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 15.1739 - mse: 15.1739 - mae: 1.5760 - val_loss: 13.9645 - val_mse: 13.9645 - val_mae: 1.5959 - lr: 1.9163e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 15.1641 - mse: 15.1641 - mae: 1.5767 - val_loss: 13.9663 - val_mse: 13.9663 - val_mae: 1.5866 - lr: 1.9163e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 15.1454 - mse: 15.1454 - mae: 1.5786 - val_loss: 14.0102 - val_mse: 14.0102 - val_mae: 1.5864 - lr: 1.9163e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 2s - loss: 15.1785 - mse: 15.1785 - mae: 1.5735 - val_loss: 13.9707 - val_mse: 13.9707 - val_mae: 1.5964 - lr: 1.9163e-04 - 2s/epoch - 2ms/step\n",
            "Score for fold 3: loss of 13.970687866210938\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 14.3735 - mse: 14.3735 - mae: 1.5711 - val_loss: 17.1501 - val_mse: 17.1501 - val_mae: 1.6030 - lr: 1.9163e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 14.3637 - mse: 14.3637 - mae: 1.5686 - val_loss: 17.1545 - val_mse: 17.1545 - val_mae: 1.6000 - lr: 1.9163e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 14.3602 - mse: 14.3602 - mae: 1.5688 - val_loss: 17.1683 - val_mse: 17.1683 - val_mae: 1.5936 - lr: 1.9163e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 14.3651 - mse: 14.3651 - mae: 1.5684 - val_loss: 17.1320 - val_mse: 17.1320 - val_mae: 1.6092 - lr: 1.9163e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 14.3543 - mse: 14.3543 - mae: 1.5695 - val_loss: 17.1990 - val_mse: 17.1990 - val_mae: 1.5946 - lr: 1.9163e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 14.3643 - mse: 14.3643 - mae: 1.5661 - val_loss: 17.2190 - val_mse: 17.2190 - val_mae: 1.6100 - lr: 1.9163e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 14.3633 - mse: 14.3633 - mae: 1.5689 - val_loss: 17.1961 - val_mse: 17.1961 - val_mae: 1.5854 - lr: 1.9163e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 2s - loss: 14.3694 - mse: 14.3694 - mae: 1.5636 - val_loss: 17.2148 - val_mse: 17.2148 - val_mae: 1.5921 - lr: 1.9163e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 2s - loss: 14.3645 - mse: 14.3645 - mae: 1.5639 - val_loss: 17.1629 - val_mse: 17.1629 - val_mae: 1.6164 - lr: 1.9163e-04 - 2s/epoch - 2ms/step\n",
            "Score for fold 4: loss of 17.162921905517578\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 15.7182 - mse: 15.7182 - mae: 1.5829 - val_loss: 11.7600 - val_mse: 11.7600 - val_mae: 1.5606 - lr: 1.9163e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 15.7181 - mse: 15.7181 - mae: 1.5823 - val_loss: 11.7420 - val_mse: 11.7420 - val_mae: 1.5555 - lr: 1.9163e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 15.7129 - mse: 15.7129 - mae: 1.5796 - val_loss: 11.7397 - val_mse: 11.7397 - val_mae: 1.5660 - lr: 1.9163e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 15.6950 - mse: 15.6950 - mae: 1.5833 - val_loss: 11.7582 - val_mse: 11.7582 - val_mae: 1.5696 - lr: 1.9163e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 15.7151 - mse: 15.7151 - mae: 1.5811 - val_loss: 11.7560 - val_mse: 11.7560 - val_mae: 1.5583 - lr: 1.9163e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 15.7119 - mse: 15.7119 - mae: 1.5818 - val_loss: 11.7504 - val_mse: 11.7504 - val_mae: 1.5633 - lr: 1.9163e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 15.7100 - mse: 15.7100 - mae: 1.5818 - val_loss: 11.7512 - val_mse: 11.7512 - val_mae: 1.5607 - lr: 1.9163e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 2s - loss: 15.7085 - mse: 15.7085 - mae: 1.5816 - val_loss: 11.7396 - val_mse: 11.7396 - val_mae: 1.5746 - lr: 1.9163e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 2s - loss: 15.7083 - mse: 15.7083 - mae: 1.5821 - val_loss: 11.7793 - val_mse: 11.7793 - val_mae: 1.5430 - lr: 1.9163e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 2s - loss: 15.7141 - mse: 15.7141 - mae: 1.5778 - val_loss: 11.7466 - val_mse: 11.7466 - val_mae: 1.5642 - lr: 1.9163e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 2s - loss: 15.6939 - mse: 15.6939 - mae: 1.5814 - val_loss: 11.7639 - val_mse: 11.7639 - val_mae: 1.5591 - lr: 1.9163e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 2s - loss: 15.7049 - mse: 15.7049 - mae: 1.5812 - val_loss: 11.7910 - val_mse: 11.7910 - val_mae: 1.5423 - lr: 1.9163e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 13/100\n",
            "1000/1000 - 2s - loss: 15.7024 - mse: 15.7024 - mae: 1.5828 - val_loss: 11.7837 - val_mse: 11.7837 - val_mae: 1.5462 - lr: 1.9163e-04 - 2s/epoch - 2ms/step\n",
            "Score for fold 5: loss of 11.78368854522705\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-10-15 09:28:37,791]\u001b[0m Finished trial#0 resulted in value: 14.958000000000002. Current best value is 14.958000000000002 with parameters: {'activation': 'relu', 'num_hidden_layer': 2, 'i': 6, 'learning_rate': 0.00019163230472226588}.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 3s - loss: 15.9022 - mse: 15.9022 - mae: 1.6207 - val_loss: 13.8443 - val_mse: 13.8443 - val_mae: 1.5760 - lr: 6.3354e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 15.4485 - mse: 15.4485 - mae: 1.5953 - val_loss: 14.1652 - val_mse: 14.1652 - val_mae: 1.5305 - lr: 6.3354e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 15.4032 - mse: 15.4032 - mae: 1.5935 - val_loss: 13.8331 - val_mse: 13.8331 - val_mae: 1.6129 - lr: 6.3354e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 15.3698 - mse: 15.3698 - mae: 1.5903 - val_loss: 13.7752 - val_mse: 13.7752 - val_mae: 1.5240 - lr: 6.3354e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 15.3593 - mse: 15.3593 - mae: 1.5879 - val_loss: 13.6717 - val_mse: 13.6717 - val_mae: 1.5926 - lr: 6.3354e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 15.3101 - mse: 15.3101 - mae: 1.5855 - val_loss: 13.5930 - val_mse: 13.5930 - val_mae: 1.6014 - lr: 6.3354e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 15.2779 - mse: 15.2779 - mae: 1.5856 - val_loss: 14.1127 - val_mse: 14.1127 - val_mae: 1.5510 - lr: 6.3354e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 2s - loss: 15.2877 - mse: 15.2877 - mae: 1.5867 - val_loss: 13.6197 - val_mse: 13.6197 - val_mae: 1.6116 - lr: 6.3354e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 2s - loss: 15.2589 - mse: 15.2589 - mae: 1.5781 - val_loss: 13.2388 - val_mse: 13.2388 - val_mae: 1.6048 - lr: 6.3354e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 2s - loss: 15.2658 - mse: 15.2658 - mae: 1.5831 - val_loss: 13.5739 - val_mse: 13.5739 - val_mae: 1.5120 - lr: 6.3354e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 2s - loss: 15.2379 - mse: 15.2379 - mae: 1.5797 - val_loss: 13.3947 - val_mse: 13.3947 - val_mae: 1.5448 - lr: 6.3354e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 2s - loss: 15.2425 - mse: 15.2425 - mae: 1.5765 - val_loss: 13.6014 - val_mse: 13.6014 - val_mae: 1.5245 - lr: 6.3354e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 13/100\n",
            "1000/1000 - 2s - loss: 15.2227 - mse: 15.2227 - mae: 1.5768 - val_loss: 13.4748 - val_mse: 13.4748 - val_mae: 1.6270 - lr: 6.3354e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 14/100\n",
            "1000/1000 - 2s - loss: 15.2277 - mse: 15.2277 - mae: 1.5786 - val_loss: 13.8938 - val_mse: 13.8938 - val_mae: 1.5882 - lr: 6.3354e-04 - 2s/epoch - 2ms/step\n",
            "Score for fold 1: loss of 13.893783569335938\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 13.4316 - mse: 13.4316 - mae: 1.5556 - val_loss: 20.4005 - val_mse: 20.4005 - val_mae: 1.6233 - lr: 6.3354e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 13.4166 - mse: 13.4166 - mae: 1.5556 - val_loss: 20.4609 - val_mse: 20.4609 - val_mae: 1.6477 - lr: 6.3354e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 13.3475 - mse: 13.3475 - mae: 1.5523 - val_loss: 20.4354 - val_mse: 20.4354 - val_mae: 1.6418 - lr: 6.3354e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 13.3663 - mse: 13.3663 - mae: 1.5557 - val_loss: 20.4149 - val_mse: 20.4149 - val_mae: 1.6349 - lr: 6.3354e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 13.3360 - mse: 13.3360 - mae: 1.5525 - val_loss: 20.4972 - val_mse: 20.4972 - val_mae: 1.6367 - lr: 6.3354e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 13.3418 - mse: 13.3418 - mae: 1.5511 - val_loss: 20.4111 - val_mse: 20.4111 - val_mae: 1.6275 - lr: 6.3354e-04 - 2s/epoch - 2ms/step\n",
            "Score for fold 2: loss of 20.41107940673828\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 16.0725 - mse: 16.0725 - mae: 1.5880 - val_loss: 9.4236 - val_mse: 9.4236 - val_mae: 1.5138 - lr: 6.3354e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 16.0873 - mse: 16.0873 - mae: 1.5858 - val_loss: 9.5068 - val_mse: 9.5068 - val_mae: 1.5065 - lr: 6.3354e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 16.0714 - mse: 16.0714 - mae: 1.5831 - val_loss: 9.5174 - val_mse: 9.5174 - val_mae: 1.5411 - lr: 6.3354e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 16.0851 - mse: 16.0851 - mae: 1.5810 - val_loss: 9.5182 - val_mse: 9.5182 - val_mae: 1.5890 - lr: 6.3354e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 16.0315 - mse: 16.0315 - mae: 1.5817 - val_loss: 9.5034 - val_mse: 9.5034 - val_mae: 1.5379 - lr: 6.3354e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 16.0153 - mse: 16.0153 - mae: 1.5841 - val_loss: 9.5439 - val_mse: 9.5439 - val_mae: 1.4998 - lr: 6.3354e-04 - 2s/epoch - 2ms/step\n",
            "Score for fold 3: loss of 9.543856620788574\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 14.4234 - mse: 14.4234 - mae: 1.5559 - val_loss: 16.3041 - val_mse: 16.3041 - val_mae: 1.5750 - lr: 6.3354e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 3s - loss: 14.3753 - mse: 14.3753 - mae: 1.5584 - val_loss: 16.1813 - val_mse: 16.1813 - val_mae: 1.5818 - lr: 6.3354e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 3s - loss: 14.3909 - mse: 14.3909 - mae: 1.5544 - val_loss: 16.1051 - val_mse: 16.1051 - val_mae: 1.6197 - lr: 6.3354e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 14.3481 - mse: 14.3481 - mae: 1.5525 - val_loss: 16.2517 - val_mse: 16.2517 - val_mae: 1.5642 - lr: 6.3354e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 14.3809 - mse: 14.3809 - mae: 1.5561 - val_loss: 16.2233 - val_mse: 16.2233 - val_mae: 1.5751 - lr: 6.3354e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 14.2867 - mse: 14.2867 - mae: 1.5556 - val_loss: 16.2847 - val_mse: 16.2847 - val_mae: 1.5753 - lr: 6.3354e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 14.3515 - mse: 14.3515 - mae: 1.5524 - val_loss: 16.3955 - val_mse: 16.3955 - val_mae: 1.5798 - lr: 6.3354e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 2s - loss: 14.3545 - mse: 14.3545 - mae: 1.5497 - val_loss: 16.2448 - val_mse: 16.2448 - val_mae: 1.5892 - lr: 6.3354e-04 - 2s/epoch - 2ms/step\n",
            "Score for fold 4: loss of 16.244848251342773\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 14.8273 - mse: 14.8273 - mae: 1.5651 - val_loss: 14.3319 - val_mse: 14.3319 - val_mae: 1.5220 - lr: 6.3354e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 14.8386 - mse: 14.8386 - mae: 1.5584 - val_loss: 14.2767 - val_mse: 14.2767 - val_mae: 1.5759 - lr: 6.3354e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 14.7911 - mse: 14.7911 - mae: 1.5591 - val_loss: 14.2363 - val_mse: 14.2363 - val_mae: 1.5466 - lr: 6.3354e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 14.7263 - mse: 14.7263 - mae: 1.5616 - val_loss: 14.3880 - val_mse: 14.3880 - val_mae: 1.5539 - lr: 6.3354e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 14.7181 - mse: 14.7181 - mae: 1.5565 - val_loss: 14.6424 - val_mse: 14.6424 - val_mae: 1.5307 - lr: 6.3354e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 14.6741 - mse: 14.6741 - mae: 1.5561 - val_loss: 14.4765 - val_mse: 14.4765 - val_mae: 1.5766 - lr: 6.3354e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 14.6805 - mse: 14.6805 - mae: 1.5536 - val_loss: 14.4227 - val_mse: 14.4227 - val_mae: 1.5942 - lr: 6.3354e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 2s - loss: 14.7104 - mse: 14.7104 - mae: 1.5568 - val_loss: 14.5333 - val_mse: 14.5333 - val_mae: 1.5546 - lr: 6.3354e-04 - 2s/epoch - 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-10-15 09:30:32,418]\u001b[0m Finished trial#1 resulted in value: 14.922. Current best value is 14.922 with parameters: {'activation': 'relu', 'num_hidden_layer': 3, 'i': 7, 'learning_rate': 0.0006335354564972164}.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Score for fold 5: loss of 14.533296585083008\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 37s - loss: 29.3071 - mse: 29.3071 - mae: 2.1981 - val_loss: 17.0357 - val_mse: 17.0357 - val_mae: 1.6009 - lr: 0.0060 - 37s/epoch - 37ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 36s - loss: 17.9744 - mse: 17.9744 - mae: 1.9549 - val_loss: 16.2100 - val_mse: 16.2100 - val_mae: 1.6271 - lr: 0.0060 - 36s/epoch - 36ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 37s - loss: 17.6447 - mse: 17.6447 - mae: 1.9134 - val_loss: 19.1415 - val_mse: 19.1415 - val_mae: 1.8652 - lr: 0.0060 - 37s/epoch - 37ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 38s - loss: 17.6546 - mse: 17.6546 - mae: 1.9241 - val_loss: 20.1910 - val_mse: 20.1910 - val_mae: 2.1977 - lr: 0.0060 - 38s/epoch - 38ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 39s - loss: 17.2029 - mse: 17.2029 - mae: 1.8998 - val_loss: 16.0543 - val_mse: 16.0543 - val_mae: 1.6408 - lr: 0.0060 - 39s/epoch - 39ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 37s - loss: 17.6675 - mse: 17.6675 - mae: 1.8971 - val_loss: 20.5515 - val_mse: 20.5515 - val_mae: 2.4861 - lr: 0.0060 - 37s/epoch - 37ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 39s - loss: 17.2344 - mse: 17.2344 - mae: 1.8974 - val_loss: 16.3631 - val_mse: 16.3631 - val_mae: 1.6130 - lr: 0.0060 - 39s/epoch - 39ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 37s - loss: 17.2151 - mse: 17.2151 - mae: 1.8697 - val_loss: 18.0985 - val_mse: 18.0985 - val_mae: 2.0605 - lr: 0.0060 - 37s/epoch - 37ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 37s - loss: 17.2445 - mse: 17.2445 - mae: 1.8901 - val_loss: 19.1860 - val_mse: 19.1860 - val_mae: 2.1088 - lr: 0.0060 - 37s/epoch - 37ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 37s - loss: 17.4260 - mse: 17.4260 - mae: 1.8963 - val_loss: 16.1513 - val_mse: 16.1513 - val_mae: 1.7661 - lr: 0.0060 - 37s/epoch - 37ms/step\n",
            "Score for fold 1: loss of 16.15131950378418\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 39s - loss: 13.4793 - mse: 13.4793 - mae: 1.6196 - val_loss: 24.6716 - val_mse: 24.6716 - val_mae: 1.6955 - lr: 0.0012 - 39s/epoch - 39ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 38s - loss: 13.4990 - mse: 13.4990 - mae: 1.6169 - val_loss: 24.6934 - val_mse: 24.6934 - val_mae: 1.8445 - lr: 0.0012 - 38s/epoch - 38ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 37s - loss: 13.4876 - mse: 13.4876 - mae: 1.6182 - val_loss: 24.6942 - val_mse: 24.6942 - val_mae: 1.6668 - lr: 0.0012 - 37s/epoch - 37ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 37s - loss: 13.4602 - mse: 13.4602 - mae: 1.6167 - val_loss: 24.6831 - val_mse: 24.6831 - val_mae: 1.7232 - lr: 0.0012 - 37s/epoch - 37ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 39s - loss: 13.4756 - mse: 13.4756 - mae: 1.6203 - val_loss: 24.6439 - val_mse: 24.6439 - val_mae: 1.7070 - lr: 0.0012 - 39s/epoch - 39ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 37s - loss: 13.5474 - mse: 13.5474 - mae: 1.6245 - val_loss: 24.7509 - val_mse: 24.7509 - val_mae: 1.7067 - lr: 0.0012 - 37s/epoch - 37ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 37s - loss: 13.4774 - mse: 13.4774 - mae: 1.6164 - val_loss: 24.5605 - val_mse: 24.5605 - val_mae: 1.7148 - lr: 0.0012 - 37s/epoch - 37ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 37s - loss: 13.4895 - mse: 13.4895 - mae: 1.6250 - val_loss: 24.5796 - val_mse: 24.5796 - val_mae: 1.7163 - lr: 0.0012 - 37s/epoch - 37ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 39s - loss: 13.4674 - mse: 13.4674 - mae: 1.6207 - val_loss: 24.8980 - val_mse: 24.8980 - val_mae: 1.6945 - lr: 0.0012 - 39s/epoch - 39ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 37s - loss: 13.4995 - mse: 13.4995 - mae: 1.6205 - val_loss: 24.6862 - val_mse: 24.6862 - val_mae: 1.7576 - lr: 0.0012 - 37s/epoch - 37ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 37s - loss: 13.5185 - mse: 13.5185 - mae: 1.6302 - val_loss: 24.7185 - val_mse: 24.7185 - val_mae: 1.8164 - lr: 0.0012 - 37s/epoch - 37ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 40s - loss: 13.4537 - mse: 13.4537 - mae: 1.6209 - val_loss: 24.5036 - val_mse: 24.5036 - val_mae: 1.7633 - lr: 0.0012 - 40s/epoch - 40ms/step\n",
            "Epoch 13/100\n",
            "1000/1000 - 40s - loss: 13.5487 - mse: 13.5487 - mae: 1.6268 - val_loss: 25.4845 - val_mse: 25.4845 - val_mae: 1.6707 - lr: 0.0012 - 40s/epoch - 40ms/step\n",
            "Epoch 14/100\n",
            "1000/1000 - 38s - loss: 13.4913 - mse: 13.4913 - mae: 1.6204 - val_loss: 24.8518 - val_mse: 24.8518 - val_mae: 1.7249 - lr: 0.0012 - 38s/epoch - 38ms/step\n",
            "Epoch 15/100\n",
            "1000/1000 - 39s - loss: 13.5183 - mse: 13.5183 - mae: 1.6204 - val_loss: 24.7663 - val_mse: 24.7663 - val_mae: 1.6916 - lr: 0.0012 - 39s/epoch - 39ms/step\n",
            "Epoch 16/100\n",
            "1000/1000 - 38s - loss: 13.4916 - mse: 13.4916 - mae: 1.6224 - val_loss: 24.8692 - val_mse: 24.8692 - val_mae: 1.7303 - lr: 0.0012 - 38s/epoch - 38ms/step\n",
            "Epoch 17/100\n",
            "1000/1000 - 40s - loss: 13.5090 - mse: 13.5090 - mae: 1.6218 - val_loss: 24.6738 - val_mse: 24.6738 - val_mae: 1.7114 - lr: 0.0012 - 40s/epoch - 40ms/step\n",
            "Score for fold 2: loss of 24.673816680908203\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 38s - loss: 15.5221 - mse: 15.5221 - mae: 1.6458 - val_loss: 16.2793 - val_mse: 16.2793 - val_mae: 1.6697 - lr: 0.0010 - 38s/epoch - 38ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 38s - loss: 15.6058 - mse: 15.6058 - mae: 1.6551 - val_loss: 16.3394 - val_mse: 16.3394 - val_mae: 1.6354 - lr: 0.0010 - 38s/epoch - 38ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 40s - loss: 15.5172 - mse: 15.5172 - mae: 1.6501 - val_loss: 16.3335 - val_mse: 16.3335 - val_mae: 1.6513 - lr: 0.0010 - 40s/epoch - 40ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 38s - loss: 15.5210 - mse: 15.5210 - mae: 1.6511 - val_loss: 16.3251 - val_mse: 16.3251 - val_mae: 1.6522 - lr: 0.0010 - 38s/epoch - 38ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 38s - loss: 15.5254 - mse: 15.5254 - mae: 1.6533 - val_loss: 16.2477 - val_mse: 16.2477 - val_mae: 1.6508 - lr: 0.0010 - 38s/epoch - 38ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 38s - loss: 15.5473 - mse: 15.5473 - mae: 1.6503 - val_loss: 16.3146 - val_mse: 16.3146 - val_mae: 1.6153 - lr: 0.0010 - 38s/epoch - 38ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 40s - loss: 15.6039 - mse: 15.6039 - mae: 1.6579 - val_loss: 16.2906 - val_mse: 16.2906 - val_mae: 1.6078 - lr: 0.0010 - 40s/epoch - 40ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 39s - loss: 15.5777 - mse: 15.5777 - mae: 1.6508 - val_loss: 16.3412 - val_mse: 16.3412 - val_mae: 1.6589 - lr: 0.0010 - 39s/epoch - 39ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 38s - loss: 15.5533 - mse: 15.5533 - mae: 1.6494 - val_loss: 16.8272 - val_mse: 16.8272 - val_mae: 1.6759 - lr: 0.0010 - 38s/epoch - 38ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 38s - loss: 15.5337 - mse: 15.5337 - mae: 1.6545 - val_loss: 16.5580 - val_mse: 16.5580 - val_mae: 1.6341 - lr: 0.0010 - 38s/epoch - 38ms/step\n",
            "Score for fold 3: loss of 16.557971954345703\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 39s - loss: 16.9423 - mse: 16.9423 - mae: 1.6545 - val_loss: 10.3003 - val_mse: 10.3003 - val_mae: 1.6202 - lr: 0.0010 - 39s/epoch - 39ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 38s - loss: 16.9618 - mse: 16.9618 - mae: 1.6607 - val_loss: 10.4220 - val_mse: 10.4220 - val_mae: 1.6657 - lr: 0.0010 - 38s/epoch - 38ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 40s - loss: 17.0349 - mse: 17.0349 - mae: 1.6601 - val_loss: 10.3242 - val_mse: 10.3242 - val_mae: 1.6010 - lr: 0.0010 - 40s/epoch - 40ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 39s - loss: 16.9842 - mse: 16.9842 - mae: 1.6587 - val_loss: 10.7818 - val_mse: 10.7818 - val_mae: 1.6311 - lr: 0.0010 - 39s/epoch - 39ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 37s - loss: 17.0391 - mse: 17.0391 - mae: 1.6623 - val_loss: 10.4556 - val_mse: 10.4556 - val_mae: 1.5654 - lr: 0.0010 - 37s/epoch - 37ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 38s - loss: 17.0352 - mse: 17.0352 - mae: 1.6597 - val_loss: 10.3913 - val_mse: 10.3913 - val_mae: 1.6311 - lr: 0.0010 - 38s/epoch - 38ms/step\n",
            "Score for fold 4: loss of 10.391261100769043\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 39s - loss: 16.9283 - mse: 16.9283 - mae: 1.6625 - val_loss: 10.9814 - val_mse: 10.9814 - val_mae: 1.6060 - lr: 0.0010 - 39s/epoch - 39ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 38s - loss: 16.8542 - mse: 16.8542 - mae: 1.6576 - val_loss: 10.9814 - val_mse: 10.9814 - val_mae: 1.6372 - lr: 0.0010 - 38s/epoch - 38ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 38s - loss: 16.8642 - mse: 16.8642 - mae: 1.6662 - val_loss: 10.9895 - val_mse: 10.9895 - val_mae: 1.6400 - lr: 0.0010 - 38s/epoch - 38ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 41s - loss: 16.8942 - mse: 16.8942 - mae: 1.6581 - val_loss: 10.9710 - val_mse: 10.9710 - val_mae: 1.5944 - lr: 0.0010 - 41s/epoch - 41ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 40s - loss: 16.8416 - mse: 16.8416 - mae: 1.6577 - val_loss: 10.9978 - val_mse: 10.9978 - val_mae: 1.6443 - lr: 0.0010 - 40s/epoch - 40ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 38s - loss: 16.9232 - mse: 16.9232 - mae: 1.6572 - val_loss: 11.0916 - val_mse: 11.0916 - val_mae: 1.6557 - lr: 0.0010 - 38s/epoch - 38ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 38s - loss: 16.8454 - mse: 16.8454 - mae: 1.6603 - val_loss: 10.9844 - val_mse: 10.9844 - val_mae: 1.5989 - lr: 0.0010 - 38s/epoch - 38ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 40s - loss: 16.8369 - mse: 16.8369 - mae: 1.6585 - val_loss: 11.2447 - val_mse: 11.2447 - val_mae: 1.6388 - lr: 0.0010 - 40s/epoch - 40ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 38s - loss: 16.8612 - mse: 16.8612 - mae: 1.6605 - val_loss: 11.0359 - val_mse: 11.0359 - val_mae: 1.6235 - lr: 0.0010 - 38s/epoch - 38ms/step\n",
            "Score for fold 5: loss of 11.035886764526367\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-10-15 10:06:03,733]\u001b[0m Finished trial#2 resulted in value: 15.762. Current best value is 14.922 with parameters: {'activation': 'relu', 'num_hidden_layer': 3, 'i': 7, 'learning_rate': 0.0006335354564972164}.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 3s - loss: 15.4546 - mse: 15.4546 - mae: 1.6703 - val_loss: 18.0772 - val_mse: 18.0772 - val_mae: 1.5575 - lr: 0.0095 - 3s/epoch - 3ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 15.2926 - mse: 15.2926 - mae: 1.6494 - val_loss: 18.0341 - val_mse: 18.0341 - val_mae: 1.5789 - lr: 0.0095 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 15.2862 - mse: 15.2862 - mae: 1.6510 - val_loss: 17.8509 - val_mse: 17.8509 - val_mae: 1.6864 - lr: 0.0095 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 15.2387 - mse: 15.2387 - mae: 1.6517 - val_loss: 18.5926 - val_mse: 18.5926 - val_mae: 1.5582 - lr: 0.0095 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 15.2273 - mse: 15.2273 - mae: 1.6556 - val_loss: 18.1383 - val_mse: 18.1383 - val_mae: 1.5882 - lr: 0.0095 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 15.2083 - mse: 15.2083 - mae: 1.6515 - val_loss: 18.0260 - val_mse: 18.0260 - val_mae: 1.6769 - lr: 0.0095 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 15.2408 - mse: 15.2408 - mae: 1.6527 - val_loss: 17.9672 - val_mse: 17.9672 - val_mae: 1.6940 - lr: 0.0095 - 2s/epoch - 2ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 2s - loss: 15.3043 - mse: 15.3043 - mae: 1.6537 - val_loss: 17.9167 - val_mse: 17.9167 - val_mae: 1.7164 - lr: 0.0095 - 2s/epoch - 2ms/step\n",
            "Score for fold 1: loss of 17.91669464111328\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 16.3201 - mse: 16.3201 - mae: 1.6361 - val_loss: 12.8639 - val_mse: 12.8639 - val_mae: 1.6749 - lr: 0.0019 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 16.2910 - mse: 16.2910 - mae: 1.6357 - val_loss: 13.0085 - val_mse: 13.0085 - val_mae: 1.6425 - lr: 0.0019 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 16.3889 - mse: 16.3889 - mae: 1.6363 - val_loss: 12.9486 - val_mse: 12.9486 - val_mae: 1.6454 - lr: 0.0019 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 16.3869 - mse: 16.3869 - mae: 1.6421 - val_loss: 12.8976 - val_mse: 12.8976 - val_mae: 1.6319 - lr: 0.0019 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 16.4520 - mse: 16.4520 - mae: 1.6342 - val_loss: 12.8969 - val_mse: 12.8969 - val_mae: 1.7132 - lr: 0.0019 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 16.3373 - mse: 16.3373 - mae: 1.6410 - val_loss: 12.8722 - val_mse: 12.8722 - val_mae: 1.6893 - lr: 0.0019 - 2s/epoch - 2ms/step\n",
            "Score for fold 2: loss of 12.872225761413574\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 13.6411 - mse: 13.6411 - mae: 1.6403 - val_loss: 23.8254 - val_mse: 23.8254 - val_mae: 1.6440 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 13.6150 - mse: 13.6150 - mae: 1.6477 - val_loss: 23.9487 - val_mse: 23.9487 - val_mae: 1.6030 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 13.5935 - mse: 13.5935 - mae: 1.6502 - val_loss: 24.1232 - val_mse: 24.1232 - val_mae: 1.5694 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 13.6521 - mse: 13.6521 - mae: 1.6479 - val_loss: 23.8606 - val_mse: 23.8606 - val_mae: 1.6211 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 13.6630 - mse: 13.6630 - mae: 1.6431 - val_loss: 23.9817 - val_mse: 23.9817 - val_mae: 1.6001 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 13.6743 - mse: 13.6743 - mae: 1.6472 - val_loss: 23.7934 - val_mse: 23.7934 - val_mae: 1.6636 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 13.7368 - mse: 13.7368 - mae: 1.6459 - val_loss: 23.8146 - val_mse: 23.8146 - val_mae: 1.6451 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 2s - loss: 13.7398 - mse: 13.7398 - mae: 1.6557 - val_loss: 24.5711 - val_mse: 24.5711 - val_mae: 1.5720 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 2s - loss: 13.7030 - mse: 13.7030 - mae: 1.6547 - val_loss: 24.0477 - val_mse: 24.0477 - val_mae: 1.5839 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 2s - loss: 13.6862 - mse: 13.6862 - mae: 1.6486 - val_loss: 24.3495 - val_mse: 24.3495 - val_mae: 1.5809 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 2s - loss: 13.8319 - mse: 13.8319 - mae: 1.6543 - val_loss: 23.8813 - val_mse: 23.8813 - val_mae: 1.6145 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Score for fold 3: loss of 23.88125991821289\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 3s - loss: 16.8236 - mse: 16.8236 - mae: 1.6530 - val_loss: 12.1551 - val_mse: 12.1551 - val_mae: 1.7523 - lr: 0.0010 - 3s/epoch - 3ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 16.5949 - mse: 16.5949 - mae: 1.6529 - val_loss: 11.9071 - val_mse: 11.9071 - val_mae: 1.5800 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 16.7368 - mse: 16.7368 - mae: 1.6477 - val_loss: 11.7071 - val_mse: 11.7071 - val_mae: 1.5936 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 16.7056 - mse: 16.7056 - mae: 1.6503 - val_loss: 11.9863 - val_mse: 11.9863 - val_mae: 1.7084 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 16.8320 - mse: 16.8320 - mae: 1.6514 - val_loss: 11.8913 - val_mse: 11.8913 - val_mae: 1.5920 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 16.7158 - mse: 16.7158 - mae: 1.6540 - val_loss: 11.7641 - val_mse: 11.7641 - val_mae: 1.6573 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 16.7194 - mse: 16.7194 - mae: 1.6523 - val_loss: 11.7905 - val_mse: 11.7905 - val_mae: 1.5920 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 2s - loss: 16.6964 - mse: 16.6964 - mae: 1.6497 - val_loss: 12.0671 - val_mse: 12.0671 - val_mae: 1.7448 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Score for fold 4: loss of 12.067073822021484\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 16.6429 - mse: 16.6429 - mae: 1.6466 - val_loss: 11.8273 - val_mse: 11.8273 - val_mae: 1.6660 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 16.6857 - mse: 16.6857 - mae: 1.6495 - val_loss: 11.7798 - val_mse: 11.7798 - val_mae: 1.7019 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 16.7391 - mse: 16.7391 - mae: 1.6471 - val_loss: 11.8435 - val_mse: 11.8435 - val_mae: 1.7210 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 16.6505 - mse: 16.6505 - mae: 1.6475 - val_loss: 12.1044 - val_mse: 12.1044 - val_mae: 1.5950 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 16.7003 - mse: 16.7003 - mae: 1.6437 - val_loss: 11.7613 - val_mse: 11.7613 - val_mae: 1.6730 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 16.6644 - mse: 16.6644 - mae: 1.6452 - val_loss: 11.9132 - val_mse: 11.9132 - val_mae: 1.6124 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 16.6853 - mse: 16.6853 - mae: 1.6459 - val_loss: 11.8185 - val_mse: 11.8185 - val_mae: 1.6426 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 2s - loss: 16.6587 - mse: 16.6587 - mae: 1.6435 - val_loss: 11.8443 - val_mse: 11.8443 - val_mae: 1.6293 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 2s - loss: 16.6492 - mse: 16.6492 - mae: 1.6457 - val_loss: 11.8143 - val_mse: 11.8143 - val_mae: 1.6314 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 2s - loss: 16.6087 - mse: 16.6087 - mae: 1.6456 - val_loss: 11.7541 - val_mse: 11.7541 - val_mae: 1.6558 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 2s - loss: 16.6071 - mse: 16.6071 - mae: 1.6404 - val_loss: 11.8617 - val_mse: 11.8617 - val_mae: 1.6247 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 2s - loss: 16.6645 - mse: 16.6645 - mae: 1.6456 - val_loss: 11.7673 - val_mse: 11.7673 - val_mae: 1.6995 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 13/100\n",
            "1000/1000 - 2s - loss: 16.6484 - mse: 16.6484 - mae: 1.6480 - val_loss: 11.7730 - val_mse: 11.7730 - val_mae: 1.6716 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 14/100\n",
            "1000/1000 - 2s - loss: 16.6540 - mse: 16.6540 - mae: 1.6438 - val_loss: 12.1461 - val_mse: 12.1461 - val_mae: 1.5828 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 15/100\n",
            "1000/1000 - 2s - loss: 16.6239 - mse: 16.6239 - mae: 1.6480 - val_loss: 11.9565 - val_mse: 11.9565 - val_mae: 1.6228 - lr: 0.0010 - 2s/epoch - 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-10-15 10:08:23,324]\u001b[0m Finished trial#3 resulted in value: 15.740000000000004. Current best value is 14.922 with parameters: {'activation': 'relu', 'num_hidden_layer': 3, 'i': 7, 'learning_rate': 0.0006335354564972164}.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Score for fold 5: loss of 11.956452369689941\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 3s - loss: 14.3902 - mse: 14.3902 - mae: 1.6189 - val_loss: 19.2163 - val_mse: 19.2163 - val_mae: 1.5732 - lr: 0.0013 - 3s/epoch - 3ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 3s - loss: 14.0390 - mse: 14.0390 - mae: 1.6045 - val_loss: 19.2405 - val_mse: 19.2405 - val_mae: 1.7045 - lr: 0.0013 - 3s/epoch - 3ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 3s - loss: 14.0575 - mse: 14.0575 - mae: 1.5966 - val_loss: 19.1510 - val_mse: 19.1510 - val_mae: 1.6462 - lr: 0.0013 - 3s/epoch - 3ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 3s - loss: 14.0259 - mse: 14.0259 - mae: 1.5903 - val_loss: 19.3574 - val_mse: 19.3574 - val_mae: 1.5088 - lr: 0.0013 - 3s/epoch - 3ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 13.9938 - mse: 13.9938 - mae: 1.5876 - val_loss: 19.3809 - val_mse: 19.3809 - val_mae: 1.6267 - lr: 0.0013 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 13.9685 - mse: 13.9685 - mae: 1.5870 - val_loss: 19.1313 - val_mse: 19.1313 - val_mae: 1.6503 - lr: 0.0013 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 14.0110 - mse: 14.0110 - mae: 1.5840 - val_loss: 19.1886 - val_mse: 19.1886 - val_mae: 1.5413 - lr: 0.0013 - 2s/epoch - 2ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 2s - loss: 13.9322 - mse: 13.9322 - mae: 1.5859 - val_loss: 19.2678 - val_mse: 19.2678 - val_mae: 1.5534 - lr: 0.0013 - 2s/epoch - 2ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 2s - loss: 14.0355 - mse: 14.0355 - mae: 1.5812 - val_loss: 19.1559 - val_mse: 19.1559 - val_mae: 1.6240 - lr: 0.0013 - 2s/epoch - 2ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 3s - loss: 13.9066 - mse: 13.9066 - mae: 1.5795 - val_loss: 19.2049 - val_mse: 19.2049 - val_mae: 1.7391 - lr: 0.0013 - 3s/epoch - 3ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 3s - loss: 13.8427 - mse: 13.8427 - mae: 1.5804 - val_loss: 19.1850 - val_mse: 19.1850 - val_mae: 1.5600 - lr: 0.0013 - 3s/epoch - 3ms/step\n",
            "Score for fold 1: loss of 19.18499755859375\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 3s - loss: 15.0174 - mse: 15.0174 - mae: 1.5907 - val_loss: 14.4111 - val_mse: 14.4111 - val_mae: 1.5356 - lr: 0.0010 - 3s/epoch - 3ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 3s - loss: 14.9428 - mse: 14.9428 - mae: 1.5833 - val_loss: 14.4937 - val_mse: 14.4937 - val_mae: 1.6667 - lr: 0.0010 - 3s/epoch - 3ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 3s - loss: 14.9670 - mse: 14.9670 - mae: 1.5876 - val_loss: 14.2773 - val_mse: 14.2773 - val_mae: 1.6213 - lr: 0.0010 - 3s/epoch - 3ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 3s - loss: 14.9063 - mse: 14.9063 - mae: 1.5846 - val_loss: 14.4130 - val_mse: 14.4130 - val_mae: 1.5927 - lr: 0.0010 - 3s/epoch - 3ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 14.9059 - mse: 14.9059 - mae: 1.5848 - val_loss: 14.5529 - val_mse: 14.5529 - val_mae: 1.5685 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 14.7549 - mse: 14.7549 - mae: 1.5828 - val_loss: 14.2804 - val_mse: 14.2804 - val_mae: 1.5934 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 3s - loss: 14.8984 - mse: 14.8984 - mae: 1.5842 - val_loss: 14.4896 - val_mse: 14.4896 - val_mae: 1.5253 - lr: 0.0010 - 3s/epoch - 3ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 3s - loss: 14.9067 - mse: 14.9067 - mae: 1.5843 - val_loss: 14.2863 - val_mse: 14.2863 - val_mae: 1.5736 - lr: 0.0010 - 3s/epoch - 3ms/step\n",
            "Score for fold 2: loss of 14.28626537322998\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 3s - loss: 15.9239 - mse: 15.9239 - mae: 1.5908 - val_loss: 9.8833 - val_mse: 9.8833 - val_mae: 1.5027 - lr: 0.0010 - 3s/epoch - 3ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 3s - loss: 15.8431 - mse: 15.8431 - mae: 1.5861 - val_loss: 9.6573 - val_mse: 9.6573 - val_mae: 1.5501 - lr: 0.0010 - 3s/epoch - 3ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 3s - loss: 15.6341 - mse: 15.6341 - mae: 1.5843 - val_loss: 9.7929 - val_mse: 9.7929 - val_mae: 1.5925 - lr: 0.0010 - 3s/epoch - 3ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 15.8794 - mse: 15.8794 - mae: 1.5836 - val_loss: 9.9337 - val_mse: 9.9337 - val_mae: 1.5550 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 3s - loss: 15.6692 - mse: 15.6692 - mae: 1.5784 - val_loss: 9.8317 - val_mse: 9.8317 - val_mae: 1.5372 - lr: 0.0010 - 3s/epoch - 3ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 3s - loss: 15.7809 - mse: 15.7809 - mae: 1.5749 - val_loss: 9.7843 - val_mse: 9.7843 - val_mae: 1.5455 - lr: 0.0010 - 3s/epoch - 3ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 3s - loss: 15.7398 - mse: 15.7398 - mae: 1.5764 - val_loss: 9.7458 - val_mse: 9.7458 - val_mae: 1.5476 - lr: 0.0010 - 3s/epoch - 3ms/step\n",
            "Score for fold 3: loss of 9.74575138092041\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 3s - loss: 15.1186 - mse: 15.1186 - mae: 1.5820 - val_loss: 12.3169 - val_mse: 12.3169 - val_mae: 1.5225 - lr: 0.0010 - 3s/epoch - 3ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 15.1264 - mse: 15.1264 - mae: 1.5807 - val_loss: 12.7648 - val_mse: 12.7648 - val_mae: 1.5114 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 3s - loss: 15.0526 - mse: 15.0526 - mae: 1.5749 - val_loss: 12.2256 - val_mse: 12.2256 - val_mae: 1.5925 - lr: 0.0010 - 3s/epoch - 3ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 4s - loss: 15.0614 - mse: 15.0614 - mae: 1.5761 - val_loss: 12.4843 - val_mse: 12.4843 - val_mae: 1.5130 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 3s - loss: 14.9371 - mse: 14.9371 - mae: 1.5794 - val_loss: 13.0977 - val_mse: 13.0977 - val_mae: 1.5492 - lr: 0.0010 - 3s/epoch - 3ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 3s - loss: 15.0036 - mse: 15.0036 - mae: 1.5769 - val_loss: 12.3415 - val_mse: 12.3415 - val_mae: 1.5265 - lr: 0.0010 - 3s/epoch - 3ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 3s - loss: 14.9142 - mse: 14.9142 - mae: 1.5735 - val_loss: 12.8840 - val_mse: 12.8840 - val_mae: 1.5301 - lr: 0.0010 - 3s/epoch - 3ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 3s - loss: 14.9031 - mse: 14.9031 - mae: 1.5774 - val_loss: 12.6755 - val_mse: 12.6755 - val_mae: 1.4833 - lr: 0.0010 - 3s/epoch - 3ms/step\n",
            "Score for fold 4: loss of 12.675464630126953\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 3s - loss: 13.5168 - mse: 13.5168 - mae: 1.5583 - val_loss: 18.4863 - val_mse: 18.4863 - val_mae: 1.9214 - lr: 0.0010 - 3s/epoch - 3ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 13.5179 - mse: 13.5179 - mae: 1.5652 - val_loss: 18.0052 - val_mse: 18.0052 - val_mae: 1.7515 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 3s - loss: 13.5285 - mse: 13.5285 - mae: 1.5634 - val_loss: 18.1768 - val_mse: 18.1768 - val_mae: 1.5947 - lr: 0.0010 - 3s/epoch - 3ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 3s - loss: 13.4210 - mse: 13.4210 - mae: 1.5597 - val_loss: 18.2034 - val_mse: 18.2034 - val_mae: 1.5752 - lr: 0.0010 - 3s/epoch - 3ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 3s - loss: 13.4938 - mse: 13.4938 - mae: 1.5596 - val_loss: 18.3077 - val_mse: 18.3077 - val_mae: 1.6309 - lr: 0.0010 - 3s/epoch - 3ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 3s - loss: 13.4590 - mse: 13.4590 - mae: 1.5641 - val_loss: 18.4209 - val_mse: 18.4209 - val_mae: 1.5563 - lr: 0.0010 - 3s/epoch - 3ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 13.3759 - mse: 13.3759 - mae: 1.5636 - val_loss: 18.2690 - val_mse: 18.2690 - val_mae: 1.6572 - lr: 0.0010 - 2s/epoch - 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-10-15 10:10:52,067]\u001b[0m Finished trial#4 resulted in value: 14.834. Current best value is 14.834 with parameters: {'activation': 'relu', 'num_hidden_layer': 4, 'i': 7, 'learning_rate': 0.0013260910885080563}.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Score for fold 5: loss of 18.268970489501953\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 5s - loss: 16.3565 - mse: 16.3565 - mae: 1.6382 - val_loss: 11.3589 - val_mse: 11.3589 - val_mae: 1.5593 - lr: 0.0014 - 5s/epoch - 5ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 4s - loss: 16.1845 - mse: 16.1845 - mae: 1.6159 - val_loss: 11.3687 - val_mse: 11.3687 - val_mae: 1.5313 - lr: 0.0014 - 4s/epoch - 4ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 5s - loss: 16.0890 - mse: 16.0890 - mae: 1.6114 - val_loss: 11.2308 - val_mse: 11.2308 - val_mae: 1.5146 - lr: 0.0014 - 5s/epoch - 5ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 5s - loss: 16.0722 - mse: 16.0722 - mae: 1.6103 - val_loss: 11.1663 - val_mse: 11.1663 - val_mae: 1.5324 - lr: 0.0014 - 5s/epoch - 5ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 4s - loss: 16.0639 - mse: 16.0639 - mae: 1.5990 - val_loss: 11.2699 - val_mse: 11.2699 - val_mae: 1.5207 - lr: 0.0014 - 4s/epoch - 4ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 4s - loss: 16.0108 - mse: 16.0108 - mae: 1.6028 - val_loss: 11.5997 - val_mse: 11.5997 - val_mae: 1.5614 - lr: 0.0014 - 4s/epoch - 4ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 4s - loss: 16.0395 - mse: 16.0395 - mae: 1.6061 - val_loss: 11.0642 - val_mse: 11.0642 - val_mae: 1.5864 - lr: 0.0014 - 4s/epoch - 4ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 5s - loss: 16.0637 - mse: 16.0637 - mae: 1.6102 - val_loss: 11.1526 - val_mse: 11.1526 - val_mae: 1.4885 - lr: 0.0014 - 5s/epoch - 5ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 5s - loss: 16.0076 - mse: 16.0076 - mae: 1.6149 - val_loss: 11.2527 - val_mse: 11.2527 - val_mae: 1.6023 - lr: 0.0014 - 5s/epoch - 5ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 4s - loss: 16.0926 - mse: 16.0926 - mae: 1.6186 - val_loss: 11.3357 - val_mse: 11.3357 - val_mae: 1.5911 - lr: 0.0014 - 4s/epoch - 4ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 4s - loss: 16.0581 - mse: 16.0581 - mae: 1.6193 - val_loss: 11.1216 - val_mse: 11.1216 - val_mae: 1.6294 - lr: 0.0014 - 4s/epoch - 4ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 4s - loss: 15.9951 - mse: 15.9951 - mae: 1.6194 - val_loss: 11.5449 - val_mse: 11.5449 - val_mae: 1.4694 - lr: 0.0014 - 4s/epoch - 4ms/step\n",
            "Score for fold 1: loss of 11.544872283935547\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 5s - loss: 13.8430 - mse: 13.8430 - mae: 1.5854 - val_loss: 19.0983 - val_mse: 19.0983 - val_mae: 1.6202 - lr: 0.0010 - 5s/epoch - 5ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 4s - loss: 13.8605 - mse: 13.8605 - mae: 1.5821 - val_loss: 19.5247 - val_mse: 19.5247 - val_mae: 1.5970 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 5s - loss: 13.8789 - mse: 13.8789 - mae: 1.5890 - val_loss: 19.4773 - val_mse: 19.4773 - val_mae: 1.5834 - lr: 0.0010 - 5s/epoch - 5ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 7s - loss: 13.8622 - mse: 13.8622 - mae: 1.5794 - val_loss: 19.8675 - val_mse: 19.8675 - val_mae: 1.6014 - lr: 0.0010 - 7s/epoch - 7ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 5s - loss: 13.8452 - mse: 13.8452 - mae: 1.5795 - val_loss: 19.4709 - val_mse: 19.4709 - val_mae: 1.6380 - lr: 0.0010 - 5s/epoch - 5ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 4s - loss: 13.7598 - mse: 13.7598 - mae: 1.5863 - val_loss: 19.6294 - val_mse: 19.6294 - val_mae: 1.5848 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Score for fold 2: loss of 19.629425048828125\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 4s - loss: 14.6556 - mse: 14.6556 - mae: 1.5792 - val_loss: 16.0874 - val_mse: 16.0874 - val_mae: 1.6716 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 5s - loss: 14.5154 - mse: 14.5154 - mae: 1.5801 - val_loss: 15.8736 - val_mse: 15.8736 - val_mae: 1.6648 - lr: 0.0010 - 5s/epoch - 5ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 4s - loss: 14.5860 - mse: 14.5860 - mae: 1.5786 - val_loss: 15.9522 - val_mse: 15.9522 - val_mae: 1.6929 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 4s - loss: 14.2944 - mse: 14.2944 - mae: 1.5727 - val_loss: 15.9323 - val_mse: 15.9323 - val_mae: 1.6444 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 4s - loss: 14.4209 - mse: 14.4209 - mae: 1.5781 - val_loss: 15.9916 - val_mse: 15.9916 - val_mae: 1.6571 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 5s - loss: 14.5046 - mse: 14.5046 - mae: 1.5743 - val_loss: 16.0435 - val_mse: 16.0435 - val_mae: 1.6135 - lr: 0.0010 - 5s/epoch - 5ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 4s - loss: 14.3514 - mse: 14.3514 - mae: 1.5753 - val_loss: 16.0387 - val_mse: 16.0387 - val_mae: 1.6576 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Score for fold 3: loss of 16.038660049438477\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 4s - loss: 14.0322 - mse: 14.0322 - mae: 1.5963 - val_loss: 17.6127 - val_mse: 17.6127 - val_mae: 1.5592 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 5s - loss: 13.9404 - mse: 13.9404 - mae: 1.5974 - val_loss: 17.6391 - val_mse: 17.6391 - val_mae: 1.5971 - lr: 0.0010 - 5s/epoch - 5ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 5s - loss: 13.9220 - mse: 13.9220 - mae: 1.6005 - val_loss: 17.8678 - val_mse: 17.8678 - val_mae: 1.6701 - lr: 0.0010 - 5s/epoch - 5ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 5s - loss: 13.8945 - mse: 13.8945 - mae: 1.6025 - val_loss: 17.8757 - val_mse: 17.8757 - val_mae: 1.4775 - lr: 0.0010 - 5s/epoch - 5ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 5s - loss: 13.8819 - mse: 13.8819 - mae: 1.6050 - val_loss: 18.0052 - val_mse: 18.0052 - val_mae: 1.4954 - lr: 0.0010 - 5s/epoch - 5ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 5s - loss: 14.0152 - mse: 14.0152 - mae: 1.6157 - val_loss: 17.7301 - val_mse: 17.7301 - val_mae: 1.5389 - lr: 0.0010 - 5s/epoch - 5ms/step\n",
            "Score for fold 4: loss of 17.73013687133789\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 5s - loss: 15.8793 - mse: 15.8793 - mae: 1.5932 - val_loss: 9.9441 - val_mse: 9.9441 - val_mae: 1.6456 - lr: 0.0010 - 5s/epoch - 5ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 5s - loss: 15.7562 - mse: 15.7562 - mae: 1.6036 - val_loss: 10.1664 - val_mse: 10.1664 - val_mae: 1.6310 - lr: 0.0010 - 5s/epoch - 5ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 5s - loss: 15.6794 - mse: 15.6794 - mae: 1.5994 - val_loss: 10.5497 - val_mse: 10.5497 - val_mae: 1.5121 - lr: 0.0010 - 5s/epoch - 5ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 4s - loss: 15.8604 - mse: 15.8604 - mae: 1.6153 - val_loss: 10.8824 - val_mse: 10.8824 - val_mae: 1.6681 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 4s - loss: 15.6870 - mse: 15.6870 - mae: 1.5991 - val_loss: 10.4115 - val_mse: 10.4115 - val_mae: 1.8064 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 4s - loss: 15.6672 - mse: 15.6672 - mae: 1.5924 - val_loss: 10.3448 - val_mse: 10.3448 - val_mae: 1.5171 - lr: 0.0010 - 4s/epoch - 4ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-10-15 10:14:53,434]\u001b[0m Finished trial#5 resulted in value: 15.056000000000001. Current best value is 14.834 with parameters: {'activation': 'relu', 'num_hidden_layer': 4, 'i': 7, 'learning_rate': 0.0013260910885080563}.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Score for fold 5: loss of 10.344826698303223\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 40s - loss: 15.6172 - mse: 15.6172 - mae: 1.6225 - val_loss: 14.2931 - val_mse: 14.2931 - val_mae: 1.5874 - lr: 1.9959e-04 - 40s/epoch - 40ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 38s - loss: 15.3537 - mse: 15.3537 - mae: 1.5995 - val_loss: 14.1648 - val_mse: 14.1648 - val_mae: 1.6298 - lr: 1.9959e-04 - 38s/epoch - 38ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 38s - loss: 15.2714 - mse: 15.2714 - mae: 1.6020 - val_loss: 14.2500 - val_mse: 14.2500 - val_mae: 1.5423 - lr: 1.9959e-04 - 38s/epoch - 38ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 38s - loss: 15.2462 - mse: 15.2462 - mae: 1.5952 - val_loss: 14.1285 - val_mse: 14.1285 - val_mae: 1.5831 - lr: 1.9959e-04 - 38s/epoch - 38ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 40s - loss: 15.1727 - mse: 15.1727 - mae: 1.5908 - val_loss: 14.3754 - val_mse: 14.3754 - val_mae: 1.5551 - lr: 1.9959e-04 - 40s/epoch - 40ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 39s - loss: 15.1184 - mse: 15.1184 - mae: 1.5852 - val_loss: 14.4444 - val_mse: 14.4444 - val_mae: 1.5922 - lr: 1.9959e-04 - 39s/epoch - 39ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 38s - loss: 15.1448 - mse: 15.1448 - mae: 1.5825 - val_loss: 14.1769 - val_mse: 14.1769 - val_mae: 1.6544 - lr: 1.9959e-04 - 38s/epoch - 38ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 37s - loss: 15.0326 - mse: 15.0326 - mae: 1.5894 - val_loss: 14.1016 - val_mse: 14.1016 - val_mae: 1.6102 - lr: 1.9959e-04 - 37s/epoch - 37ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 39s - loss: 14.9855 - mse: 14.9855 - mae: 1.5861 - val_loss: 14.2246 - val_mse: 14.2246 - val_mae: 1.5875 - lr: 1.9959e-04 - 39s/epoch - 39ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 38s - loss: 14.9427 - mse: 14.9427 - mae: 1.5825 - val_loss: 14.4740 - val_mse: 14.4740 - val_mae: 1.4571 - lr: 1.9959e-04 - 38s/epoch - 38ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 38s - loss: 14.8186 - mse: 14.8186 - mae: 1.5773 - val_loss: 14.5291 - val_mse: 14.5291 - val_mae: 1.6993 - lr: 1.9959e-04 - 38s/epoch - 38ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 38s - loss: 14.8244 - mse: 14.8244 - mae: 1.5692 - val_loss: 14.1000 - val_mse: 14.1000 - val_mae: 1.6328 - lr: 1.9959e-04 - 38s/epoch - 38ms/step\n",
            "Epoch 13/100\n",
            "1000/1000 - 40s - loss: 14.5679 - mse: 14.5679 - mae: 1.5749 - val_loss: 14.4480 - val_mse: 14.4480 - val_mae: 1.5865 - lr: 1.9959e-04 - 40s/epoch - 40ms/step\n",
            "Epoch 14/100\n",
            "1000/1000 - 38s - loss: 14.6980 - mse: 14.6980 - mae: 1.5793 - val_loss: 14.3346 - val_mse: 14.3346 - val_mae: 1.7214 - lr: 1.9959e-04 - 38s/epoch - 38ms/step\n",
            "Epoch 15/100\n",
            "1000/1000 - 37s - loss: 14.6623 - mse: 14.6623 - mae: 1.5752 - val_loss: 14.3168 - val_mse: 14.3168 - val_mae: 1.5280 - lr: 1.9959e-04 - 37s/epoch - 37ms/step\n",
            "Epoch 16/100\n",
            "1000/1000 - 37s - loss: 14.4598 - mse: 14.4598 - mae: 1.5696 - val_loss: 14.3166 - val_mse: 14.3166 - val_mae: 1.5368 - lr: 1.9959e-04 - 37s/epoch - 37ms/step\n",
            "Epoch 17/100\n",
            "1000/1000 - 39s - loss: 14.4566 - mse: 14.4566 - mae: 1.5712 - val_loss: 14.2323 - val_mse: 14.2323 - val_mae: 1.6576 - lr: 1.9959e-04 - 39s/epoch - 39ms/step\n",
            "Score for fold 1: loss of 14.232344627380371\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 38s - loss: 14.7979 - mse: 14.7979 - mae: 1.5780 - val_loss: 12.8514 - val_mse: 12.8514 - val_mae: 1.5328 - lr: 1.9959e-04 - 38s/epoch - 38ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 37s - loss: 14.5769 - mse: 14.5769 - mae: 1.5814 - val_loss: 12.9203 - val_mse: 12.9203 - val_mae: 1.5747 - lr: 1.9959e-04 - 37s/epoch - 37ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 37s - loss: 14.5144 - mse: 14.5144 - mae: 1.5751 - val_loss: 13.2393 - val_mse: 13.2393 - val_mae: 1.6185 - lr: 1.9959e-04 - 37s/epoch - 37ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 39s - loss: 14.4816 - mse: 14.4816 - mae: 1.5765 - val_loss: 13.4067 - val_mse: 13.4067 - val_mae: 1.7074 - lr: 1.9959e-04 - 39s/epoch - 39ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 37s - loss: 14.4131 - mse: 14.4131 - mae: 1.5741 - val_loss: 13.4408 - val_mse: 13.4408 - val_mae: 1.4955 - lr: 1.9959e-04 - 37s/epoch - 37ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 37s - loss: 14.2124 - mse: 14.2124 - mae: 1.5767 - val_loss: 13.4328 - val_mse: 13.4328 - val_mae: 1.4939 - lr: 1.9959e-04 - 37s/epoch - 37ms/step\n",
            "Score for fold 2: loss of 13.432808876037598\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 38s - loss: 13.7687 - mse: 13.7687 - mae: 1.5808 - val_loss: 15.3293 - val_mse: 15.3293 - val_mae: 1.5109 - lr: 1.9959e-04 - 38s/epoch - 38ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 37s - loss: 13.7467 - mse: 13.7467 - mae: 1.5688 - val_loss: 15.8407 - val_mse: 15.8407 - val_mae: 1.4432 - lr: 1.9959e-04 - 37s/epoch - 37ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 37s - loss: 13.6516 - mse: 13.6516 - mae: 1.5759 - val_loss: 15.5942 - val_mse: 15.5942 - val_mae: 1.5794 - lr: 1.9959e-04 - 37s/epoch - 37ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 37s - loss: 13.3818 - mse: 13.3818 - mae: 1.5801 - val_loss: 15.9424 - val_mse: 15.9424 - val_mae: 1.4710 - lr: 1.9959e-04 - 37s/epoch - 37ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 39s - loss: 13.4762 - mse: 13.4762 - mae: 1.5792 - val_loss: 15.8818 - val_mse: 15.8818 - val_mae: 1.5424 - lr: 1.9959e-04 - 39s/epoch - 39ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 37s - loss: 13.4094 - mse: 13.4094 - mae: 1.5913 - val_loss: 16.4426 - val_mse: 16.4426 - val_mae: 1.5162 - lr: 1.9959e-04 - 37s/epoch - 37ms/step\n",
            "Score for fold 3: loss of 16.442567825317383\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 37s - loss: 12.9054 - mse: 12.9054 - mae: 1.6075 - val_loss: 18.9135 - val_mse: 18.9135 - val_mae: 1.5052 - lr: 1.9959e-04 - 37s/epoch - 37ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 37s - loss: 12.6850 - mse: 12.6850 - mae: 1.6148 - val_loss: 19.5703 - val_mse: 19.5703 - val_mae: 1.5070 - lr: 1.9959e-04 - 37s/epoch - 37ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 39s - loss: 12.6663 - mse: 12.6663 - mae: 1.6180 - val_loss: 19.2979 - val_mse: 19.2979 - val_mae: 1.6441 - lr: 1.9959e-04 - 39s/epoch - 39ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 37s - loss: 12.5132 - mse: 12.5132 - mae: 1.5944 - val_loss: 19.0115 - val_mse: 19.0115 - val_mae: 1.6820 - lr: 1.9959e-04 - 37s/epoch - 37ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 37s - loss: 12.4658 - mse: 12.4658 - mae: 1.6113 - val_loss: 19.0197 - val_mse: 19.0197 - val_mae: 1.6819 - lr: 1.9959e-04 - 37s/epoch - 37ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 37s - loss: 12.4554 - mse: 12.4554 - mae: 1.6125 - val_loss: 19.4268 - val_mse: 19.4268 - val_mae: 1.5058 - lr: 1.9959e-04 - 37s/epoch - 37ms/step\n",
            "Score for fold 4: loss of 19.42682456970215\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 39s - loss: 15.1315 - mse: 15.1315 - mae: 1.6248 - val_loss: 8.6753 - val_mse: 8.6753 - val_mae: 1.4462 - lr: 1.9959e-04 - 39s/epoch - 39ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 37s - loss: 15.1528 - mse: 15.1528 - mae: 1.6163 - val_loss: 9.0318 - val_mse: 9.0318 - val_mae: 1.8037 - lr: 1.9959e-04 - 37s/epoch - 37ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 36s - loss: 14.8236 - mse: 14.8236 - mae: 1.6077 - val_loss: 8.8319 - val_mse: 8.8319 - val_mae: 1.4614 - lr: 1.9959e-04 - 36s/epoch - 36ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 37s - loss: 14.8880 - mse: 14.8880 - mae: 1.6253 - val_loss: 9.0657 - val_mse: 9.0657 - val_mae: 1.7980 - lr: 1.9959e-04 - 37s/epoch - 37ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 39s - loss: 14.8117 - mse: 14.8117 - mae: 1.6176 - val_loss: 10.1302 - val_mse: 10.1302 - val_mae: 2.1471 - lr: 1.9959e-04 - 39s/epoch - 39ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 37s - loss: 14.7305 - mse: 14.7305 - mae: 1.6226 - val_loss: 9.5550 - val_mse: 9.5550 - val_mae: 1.4384 - lr: 1.9959e-04 - 37s/epoch - 37ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-10-15 10:41:35,395]\u001b[0m Finished trial#6 resulted in value: 14.618. Current best value is 14.618 with parameters: {'activation': 'relu', 'num_hidden_layer': 4, 'i': 10, 'learning_rate': 0.0001995902281877497}.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Score for fold 5: loss of 9.555013656616211\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 3s - loss: 19.4144 - mse: 19.4144 - mae: 1.8318 - val_loss: 10.0822 - val_mse: 10.0822 - val_mae: 1.6173 - lr: 1.7110e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 17.0807 - mse: 17.0807 - mae: 1.6323 - val_loss: 10.0519 - val_mse: 10.0519 - val_mae: 1.6094 - lr: 1.7110e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 17.0686 - mse: 17.0686 - mae: 1.6323 - val_loss: 10.0190 - val_mse: 10.0190 - val_mae: 1.6154 - lr: 1.7110e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 17.0415 - mse: 17.0415 - mae: 1.6338 - val_loss: 10.0427 - val_mse: 10.0427 - val_mae: 1.6122 - lr: 1.7110e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 17.0457 - mse: 17.0457 - mae: 1.6338 - val_loss: 10.0437 - val_mse: 10.0437 - val_mae: 1.6135 - lr: 1.7110e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 17.0329 - mse: 17.0329 - mae: 1.6260 - val_loss: 10.0086 - val_mse: 10.0086 - val_mae: 1.6314 - lr: 1.7110e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 17.0377 - mse: 17.0377 - mae: 1.6324 - val_loss: 10.0032 - val_mse: 10.0032 - val_mae: 1.6280 - lr: 1.7110e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 2s - loss: 17.0343 - mse: 17.0343 - mae: 1.6313 - val_loss: 10.0217 - val_mse: 10.0217 - val_mae: 1.6061 - lr: 1.7110e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 2s - loss: 17.0345 - mse: 17.0345 - mae: 1.6287 - val_loss: 10.0242 - val_mse: 10.0242 - val_mae: 1.6215 - lr: 1.7110e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 2s - loss: 17.0331 - mse: 17.0331 - mae: 1.6320 - val_loss: 10.0155 - val_mse: 10.0155 - val_mae: 1.6202 - lr: 1.7110e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 2s - loss: 17.0373 - mse: 17.0373 - mae: 1.6294 - val_loss: 10.0200 - val_mse: 10.0200 - val_mae: 1.6238 - lr: 1.7110e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 2s - loss: 17.0407 - mse: 17.0407 - mae: 1.6305 - val_loss: 10.0122 - val_mse: 10.0122 - val_mae: 1.6118 - lr: 1.7110e-04 - 2s/epoch - 2ms/step\n",
            "Score for fold 1: loss of 10.012222290039062\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 15.1705 - mse: 15.1705 - mae: 1.6210 - val_loss: 17.4160 - val_mse: 17.4160 - val_mae: 1.6403 - lr: 1.7110e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 15.1888 - mse: 15.1888 - mae: 1.6189 - val_loss: 17.4472 - val_mse: 17.4472 - val_mae: 1.6351 - lr: 1.7110e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 15.1735 - mse: 15.1735 - mae: 1.6150 - val_loss: 17.4207 - val_mse: 17.4207 - val_mae: 1.6494 - lr: 1.7110e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 15.1729 - mse: 15.1729 - mae: 1.6217 - val_loss: 17.4546 - val_mse: 17.4546 - val_mae: 1.6389 - lr: 1.7110e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 15.1814 - mse: 15.1814 - mae: 1.6217 - val_loss: 17.4615 - val_mse: 17.4615 - val_mae: 1.6316 - lr: 1.7110e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 15.1705 - mse: 15.1705 - mae: 1.6203 - val_loss: 17.4723 - val_mse: 17.4723 - val_mae: 1.6372 - lr: 1.7110e-04 - 2s/epoch - 2ms/step\n",
            "Score for fold 2: loss of 17.472326278686523\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 16.7496 - mse: 16.7496 - mae: 1.6239 - val_loss: 11.1835 - val_mse: 11.1835 - val_mae: 1.6198 - lr: 1.7110e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 16.7208 - mse: 16.7208 - mae: 1.6254 - val_loss: 11.2005 - val_mse: 11.2005 - val_mae: 1.6114 - lr: 1.7110e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 16.7365 - mse: 16.7365 - mae: 1.6212 - val_loss: 11.1935 - val_mse: 11.1935 - val_mae: 1.6209 - lr: 1.7110e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 16.7244 - mse: 16.7244 - mae: 1.6246 - val_loss: 11.2108 - val_mse: 11.2108 - val_mae: 1.6133 - lr: 1.7110e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 16.7239 - mse: 16.7239 - mae: 1.6214 - val_loss: 11.1848 - val_mse: 11.1848 - val_mae: 1.6243 - lr: 1.7110e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 16.7303 - mse: 16.7303 - mae: 1.6253 - val_loss: 11.1881 - val_mse: 11.1881 - val_mae: 1.6289 - lr: 1.7110e-04 - 2s/epoch - 2ms/step\n",
            "Score for fold 3: loss of 11.188124656677246\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 16.8720 - mse: 16.8720 - mae: 1.6362 - val_loss: 10.5780 - val_mse: 10.5780 - val_mae: 1.5996 - lr: 1.7110e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 16.8812 - mse: 16.8812 - mae: 1.6364 - val_loss: 10.5707 - val_mse: 10.5707 - val_mae: 1.5953 - lr: 1.7110e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 16.8733 - mse: 16.8733 - mae: 1.6357 - val_loss: 10.6063 - val_mse: 10.6063 - val_mae: 1.5832 - lr: 1.7110e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 16.8771 - mse: 16.8771 - mae: 1.6327 - val_loss: 10.5739 - val_mse: 10.5739 - val_mae: 1.5980 - lr: 1.7110e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 3s - loss: 16.8676 - mse: 16.8676 - mae: 1.6372 - val_loss: 10.5835 - val_mse: 10.5835 - val_mae: 1.5965 - lr: 1.7110e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 3s - loss: 16.8812 - mse: 16.8812 - mae: 1.6350 - val_loss: 10.5887 - val_mse: 10.5887 - val_mae: 1.5885 - lr: 1.7110e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 16.8715 - mse: 16.8715 - mae: 1.6367 - val_loss: 10.5845 - val_mse: 10.5845 - val_mae: 1.5883 - lr: 1.7110e-04 - 2s/epoch - 2ms/step\n",
            "Score for fold 4: loss of 10.584498405456543\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 12.3085 - mse: 12.3085 - mae: 1.6198 - val_loss: 28.8766 - val_mse: 28.8766 - val_mae: 1.6666 - lr: 1.7110e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 12.3220 - mse: 12.3220 - mae: 1.6189 - val_loss: 28.8803 - val_mse: 28.8803 - val_mae: 1.6740 - lr: 1.7110e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 12.3175 - mse: 12.3175 - mae: 1.6212 - val_loss: 28.9206 - val_mse: 28.9206 - val_mae: 1.6609 - lr: 1.7110e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 12.3179 - mse: 12.3179 - mae: 1.6213 - val_loss: 28.8618 - val_mse: 28.8618 - val_mae: 1.6922 - lr: 1.7110e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 12.3194 - mse: 12.3194 - mae: 1.6222 - val_loss: 28.8959 - val_mse: 28.8959 - val_mae: 1.6690 - lr: 1.7110e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 12.3167 - mse: 12.3167 - mae: 1.6203 - val_loss: 28.8656 - val_mse: 28.8656 - val_mae: 1.6701 - lr: 1.7110e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 12.3189 - mse: 12.3189 - mae: 1.6193 - val_loss: 28.9211 - val_mse: 28.9211 - val_mae: 1.6512 - lr: 1.7110e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 2s - loss: 12.3193 - mse: 12.3193 - mae: 1.6221 - val_loss: 28.8798 - val_mse: 28.8798 - val_mae: 1.6623 - lr: 1.7110e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 2s - loss: 12.3219 - mse: 12.3219 - mae: 1.6218 - val_loss: 28.8776 - val_mse: 28.8776 - val_mae: 1.6684 - lr: 1.7110e-04 - 2s/epoch - 2ms/step\n",
            "Score for fold 5: loss of 28.877586364746094\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-10-15 10:43:24,645]\u001b[0m Finished trial#7 resulted in value: 15.626. Current best value is 14.618 with parameters: {'activation': 'relu', 'num_hidden_layer': 4, 'i': 10, 'learning_rate': 0.0001995902281877497}.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 5s - loss: 17.8881 - mse: 17.8881 - mae: 1.7158 - val_loss: 9.0638 - val_mse: 9.0638 - val_mae: 1.6322 - lr: 0.0012 - 5s/epoch - 5ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 4s - loss: 17.5510 - mse: 17.5510 - mae: 1.6842 - val_loss: 9.1413 - val_mse: 9.1413 - val_mae: 1.5585 - lr: 0.0012 - 4s/epoch - 4ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 4s - loss: 17.5033 - mse: 17.5033 - mae: 1.6765 - val_loss: 9.7998 - val_mse: 9.7998 - val_mae: 1.8626 - lr: 0.0012 - 4s/epoch - 4ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 5s - loss: 17.4754 - mse: 17.4754 - mae: 1.6677 - val_loss: 8.8329 - val_mse: 8.8329 - val_mae: 1.5896 - lr: 0.0012 - 5s/epoch - 5ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 4s - loss: 17.4305 - mse: 17.4305 - mae: 1.6718 - val_loss: 8.9517 - val_mse: 8.9517 - val_mae: 1.4958 - lr: 0.0012 - 4s/epoch - 4ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 4s - loss: 17.4335 - mse: 17.4335 - mae: 1.6599 - val_loss: 8.8638 - val_mse: 8.8638 - val_mae: 1.5159 - lr: 0.0012 - 4s/epoch - 4ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 4s - loss: 17.4049 - mse: 17.4049 - mae: 1.6650 - val_loss: 8.8080 - val_mse: 8.8080 - val_mae: 1.5771 - lr: 0.0012 - 4s/epoch - 4ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 4s - loss: 17.4579 - mse: 17.4579 - mae: 1.6660 - val_loss: 8.8381 - val_mse: 8.8381 - val_mae: 1.5648 - lr: 0.0012 - 4s/epoch - 4ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 4s - loss: 17.4072 - mse: 17.4072 - mae: 1.6672 - val_loss: 8.8237 - val_mse: 8.8237 - val_mae: 1.5758 - lr: 0.0012 - 4s/epoch - 4ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 4s - loss: 17.4534 - mse: 17.4534 - mae: 1.6637 - val_loss: 8.8379 - val_mse: 8.8379 - val_mae: 1.5865 - lr: 0.0012 - 4s/epoch - 4ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 4s - loss: 17.4028 - mse: 17.4028 - mae: 1.6745 - val_loss: 8.9019 - val_mse: 8.9019 - val_mae: 1.4699 - lr: 0.0012 - 4s/epoch - 4ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 4s - loss: 17.4271 - mse: 17.4271 - mae: 1.6621 - val_loss: 8.8245 - val_mse: 8.8245 - val_mae: 1.5300 - lr: 0.0012 - 4s/epoch - 4ms/step\n",
            "Score for fold 1: loss of 8.824490547180176\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 4s - loss: 15.1746 - mse: 15.1746 - mae: 1.6297 - val_loss: 17.7575 - val_mse: 17.7575 - val_mae: 1.6320 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 4s - loss: 15.1936 - mse: 15.1936 - mae: 1.6240 - val_loss: 18.1610 - val_mse: 18.1610 - val_mae: 1.5621 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 4s - loss: 15.1903 - mse: 15.1903 - mae: 1.6401 - val_loss: 17.7037 - val_mse: 17.7037 - val_mae: 1.6500 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 4s - loss: 15.2350 - mse: 15.2350 - mae: 1.6388 - val_loss: 17.7181 - val_mse: 17.7181 - val_mae: 1.6291 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 4s - loss: 15.1986 - mse: 15.1986 - mae: 1.6265 - val_loss: 18.3567 - val_mse: 18.3567 - val_mae: 1.5403 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 4s - loss: 15.2208 - mse: 15.2208 - mae: 1.6331 - val_loss: 17.8422 - val_mse: 17.8422 - val_mae: 1.6057 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 4s - loss: 15.2535 - mse: 15.2535 - mae: 1.6428 - val_loss: 17.9674 - val_mse: 17.9674 - val_mae: 1.5734 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 4s - loss: 15.2173 - mse: 15.2173 - mae: 1.6339 - val_loss: 17.7905 - val_mse: 17.7905 - val_mae: 1.6391 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Score for fold 2: loss of 17.790475845336914\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 4s - loss: 13.7914 - mse: 13.7914 - mae: 1.6301 - val_loss: 23.4410 - val_mse: 23.4410 - val_mae: 1.6490 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 4s - loss: 13.7681 - mse: 13.7681 - mae: 1.6393 - val_loss: 23.2317 - val_mse: 23.2317 - val_mae: 1.7156 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 4s - loss: 13.8041 - mse: 13.8041 - mae: 1.6341 - val_loss: 23.6524 - val_mse: 23.6524 - val_mae: 1.5799 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 4s - loss: 13.7618 - mse: 13.7618 - mae: 1.6295 - val_loss: 23.2242 - val_mse: 23.2242 - val_mae: 1.7146 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 4s - loss: 13.7869 - mse: 13.7869 - mae: 1.6326 - val_loss: 23.7012 - val_mse: 23.7012 - val_mae: 1.5766 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 5s - loss: 13.8028 - mse: 13.8028 - mae: 1.6380 - val_loss: 23.4214 - val_mse: 23.4214 - val_mae: 1.9090 - lr: 0.0010 - 5s/epoch - 5ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 6s - loss: 13.7852 - mse: 13.7852 - mae: 1.6364 - val_loss: 23.7390 - val_mse: 23.7390 - val_mae: 1.5902 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 4s - loss: 13.8339 - mse: 13.8339 - mae: 1.6349 - val_loss: 23.5341 - val_mse: 23.5341 - val_mae: 1.5976 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 4s - loss: 13.8045 - mse: 13.8045 - mae: 1.6291 - val_loss: 23.2199 - val_mse: 23.2199 - val_mae: 1.7343 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 4s - loss: 13.8561 - mse: 13.8561 - mae: 1.6413 - val_loss: 24.1536 - val_mse: 24.1536 - val_mae: 1.5583 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 4s - loss: 13.8484 - mse: 13.8484 - mae: 1.6324 - val_loss: 23.5543 - val_mse: 23.5543 - val_mae: 1.6033 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 4s - loss: 13.8131 - mse: 13.8131 - mae: 1.6375 - val_loss: 23.4266 - val_mse: 23.4266 - val_mae: 1.6376 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 13/100\n",
            "1000/1000 - 4s - loss: 13.7705 - mse: 13.7705 - mae: 1.6314 - val_loss: 23.2091 - val_mse: 23.2091 - val_mae: 1.7458 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 14/100\n",
            "1000/1000 - 4s - loss: 13.8175 - mse: 13.8175 - mae: 1.6272 - val_loss: 23.2222 - val_mse: 23.2222 - val_mae: 1.7037 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 15/100\n",
            "1000/1000 - 4s - loss: 13.8681 - mse: 13.8681 - mae: 1.6282 - val_loss: 23.4485 - val_mse: 23.4485 - val_mae: 1.6146 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 16/100\n",
            "1000/1000 - 4s - loss: 13.8977 - mse: 13.8977 - mae: 1.6376 - val_loss: 23.2908 - val_mse: 23.2908 - val_mae: 1.7742 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 17/100\n",
            "1000/1000 - 4s - loss: 13.8734 - mse: 13.8734 - mae: 1.6301 - val_loss: 23.3523 - val_mse: 23.3523 - val_mae: 1.6546 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 18/100\n",
            "1000/1000 - 4s - loss: 13.8897 - mse: 13.8897 - mae: 1.6331 - val_loss: 23.2933 - val_mse: 23.2933 - val_mae: 1.6575 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Score for fold 3: loss of 23.293277740478516\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 4s - loss: 16.3864 - mse: 16.3864 - mae: 1.6475 - val_loss: 12.9994 - val_mse: 12.9994 - val_mae: 1.6381 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 4s - loss: 16.4811 - mse: 16.4811 - mae: 1.6551 - val_loss: 13.8443 - val_mse: 13.8443 - val_mae: 1.5389 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 4s - loss: 16.3694 - mse: 16.3694 - mae: 1.6456 - val_loss: 12.9889 - val_mse: 12.9889 - val_mae: 1.6427 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 4s - loss: 16.3723 - mse: 16.3723 - mae: 1.6375 - val_loss: 13.1601 - val_mse: 13.1601 - val_mae: 1.6005 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 4s - loss: 16.3518 - mse: 16.3518 - mae: 1.6453 - val_loss: 13.0391 - val_mse: 13.0391 - val_mae: 1.6362 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 4s - loss: 16.4902 - mse: 16.4902 - mae: 1.6432 - val_loss: 13.9518 - val_mse: 13.9518 - val_mae: 1.5991 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 4s - loss: 16.4835 - mse: 16.4835 - mae: 1.6427 - val_loss: 13.1063 - val_mse: 13.1063 - val_mae: 1.6257 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 4s - loss: 16.3567 - mse: 16.3567 - mae: 1.6477 - val_loss: 13.1692 - val_mse: 13.1692 - val_mae: 1.7620 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Score for fold 4: loss of 13.169190406799316\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 4s - loss: 15.8668 - mse: 15.8668 - mae: 1.6467 - val_loss: 15.3040 - val_mse: 15.3040 - val_mae: 1.6434 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 4s - loss: 15.9201 - mse: 15.9201 - mae: 1.6472 - val_loss: 15.3038 - val_mse: 15.3038 - val_mae: 1.6218 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 4s - loss: 15.9035 - mse: 15.9035 - mae: 1.6510 - val_loss: 15.2452 - val_mse: 15.2452 - val_mae: 1.6479 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 4s - loss: 15.7424 - mse: 15.7424 - mae: 1.6494 - val_loss: 15.2244 - val_mse: 15.2244 - val_mae: 1.6697 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 4s - loss: 15.8832 - mse: 15.8832 - mae: 1.6536 - val_loss: 15.5527 - val_mse: 15.5527 - val_mae: 1.7975 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 4s - loss: 15.8464 - mse: 15.8464 - mae: 1.6453 - val_loss: 15.8842 - val_mse: 15.8842 - val_mae: 1.6310 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 4s - loss: 15.8868 - mse: 15.8868 - mae: 1.6504 - val_loss: 15.2720 - val_mse: 15.2720 - val_mae: 1.6329 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 4s - loss: 15.8641 - mse: 15.8641 - mae: 1.6462 - val_loss: 15.2249 - val_mse: 15.2249 - val_mae: 1.6594 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 4s - loss: 15.7792 - mse: 15.7792 - mae: 1.6483 - val_loss: 17.5912 - val_mse: 17.5912 - val_mae: 2.0271 - lr: 0.0010 - 4s/epoch - 4ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-10-15 10:47:40,586]\u001b[0m Finished trial#8 resulted in value: 16.131999999999998. Current best value is 14.618 with parameters: {'activation': 'relu', 'num_hidden_layer': 4, 'i': 10, 'learning_rate': 0.0001995902281877497}.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Score for fold 5: loss of 17.591217041015625\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 4s - loss: 16.7126 - mse: 16.7126 - mae: 1.7063 - val_loss: 12.8582 - val_mse: 12.8582 - val_mae: 1.4509 - lr: 0.0033 - 4s/epoch - 4ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 4s - loss: 16.4016 - mse: 16.4016 - mae: 1.6658 - val_loss: 12.6393 - val_mse: 12.6393 - val_mae: 1.5177 - lr: 0.0033 - 4s/epoch - 4ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 4s - loss: 16.2274 - mse: 16.2274 - mae: 1.6507 - val_loss: 12.5920 - val_mse: 12.5920 - val_mae: 1.6866 - lr: 0.0033 - 4s/epoch - 4ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 5s - loss: 16.1482 - mse: 16.1482 - mae: 1.6441 - val_loss: 12.5254 - val_mse: 12.5254 - val_mae: 1.5582 - lr: 0.0033 - 5s/epoch - 5ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 3s - loss: 16.1379 - mse: 16.1379 - mae: 1.6526 - val_loss: 12.5115 - val_mse: 12.5115 - val_mae: 1.4653 - lr: 0.0033 - 3s/epoch - 3ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 3s - loss: 16.1365 - mse: 16.1365 - mae: 1.6536 - val_loss: 12.8377 - val_mse: 12.8377 - val_mae: 1.6824 - lr: 0.0033 - 3s/epoch - 3ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 3s - loss: 16.1069 - mse: 16.1069 - mae: 1.6576 - val_loss: 12.5394 - val_mse: 12.5394 - val_mae: 1.7247 - lr: 0.0033 - 3s/epoch - 3ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 3s - loss: 16.0905 - mse: 16.0905 - mae: 1.6641 - val_loss: 12.5595 - val_mse: 12.5595 - val_mae: 1.4530 - lr: 0.0033 - 3s/epoch - 3ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 3s - loss: 16.1277 - mse: 16.1277 - mae: 1.6651 - val_loss: 12.6451 - val_mse: 12.6451 - val_mae: 1.5430 - lr: 0.0033 - 3s/epoch - 3ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 3s - loss: 16.1196 - mse: 16.1196 - mae: 1.6668 - val_loss: 12.4391 - val_mse: 12.4391 - val_mae: 1.6397 - lr: 0.0033 - 3s/epoch - 3ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 3s - loss: 16.1894 - mse: 16.1894 - mae: 1.6578 - val_loss: 12.4195 - val_mse: 12.4195 - val_mae: 1.6751 - lr: 0.0033 - 3s/epoch - 3ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 3s - loss: 16.0809 - mse: 16.0809 - mae: 1.6544 - val_loss: 12.4680 - val_mse: 12.4680 - val_mae: 1.4885 - lr: 0.0033 - 3s/epoch - 3ms/step\n",
            "Epoch 13/100\n",
            "1000/1000 - 3s - loss: 16.0326 - mse: 16.0326 - mae: 1.6595 - val_loss: 12.5070 - val_mse: 12.5070 - val_mae: 1.5524 - lr: 0.0033 - 3s/epoch - 3ms/step\n",
            "Epoch 14/100\n",
            "1000/1000 - 3s - loss: 16.0124 - mse: 16.0124 - mae: 1.6681 - val_loss: 12.3923 - val_mse: 12.3923 - val_mae: 1.6279 - lr: 0.0033 - 3s/epoch - 3ms/step\n",
            "Epoch 15/100\n",
            "1000/1000 - 3s - loss: 16.1458 - mse: 16.1458 - mae: 1.6703 - val_loss: 12.4608 - val_mse: 12.4608 - val_mae: 1.7286 - lr: 0.0033 - 3s/epoch - 3ms/step\n",
            "Epoch 16/100\n",
            "1000/1000 - 3s - loss: 16.0804 - mse: 16.0804 - mae: 1.6599 - val_loss: 12.5975 - val_mse: 12.5975 - val_mae: 1.4569 - lr: 0.0033 - 3s/epoch - 3ms/step\n",
            "Epoch 17/100\n",
            "1000/1000 - 3s - loss: 16.0749 - mse: 16.0749 - mae: 1.6654 - val_loss: 12.6010 - val_mse: 12.6010 - val_mae: 1.7170 - lr: 0.0033 - 3s/epoch - 3ms/step\n",
            "Epoch 18/100\n",
            "1000/1000 - 3s - loss: 16.1035 - mse: 16.1035 - mae: 1.6678 - val_loss: 12.4822 - val_mse: 12.4822 - val_mae: 1.4573 - lr: 0.0033 - 3s/epoch - 3ms/step\n",
            "Epoch 19/100\n",
            "1000/1000 - 3s - loss: 16.1343 - mse: 16.1343 - mae: 1.6756 - val_loss: 12.3814 - val_mse: 12.3814 - val_mae: 1.5952 - lr: 0.0033 - 3s/epoch - 3ms/step\n",
            "Epoch 20/100\n",
            "1000/1000 - 3s - loss: 16.1100 - mse: 16.1100 - mae: 1.6737 - val_loss: 12.5480 - val_mse: 12.5480 - val_mae: 1.5043 - lr: 0.0033 - 3s/epoch - 3ms/step\n",
            "Epoch 21/100\n",
            "1000/1000 - 4s - loss: 16.0976 - mse: 16.0976 - mae: 1.6704 - val_loss: 12.5152 - val_mse: 12.5152 - val_mae: 1.7312 - lr: 0.0033 - 4s/epoch - 4ms/step\n",
            "Epoch 22/100\n",
            "1000/1000 - 3s - loss: 16.1451 - mse: 16.1451 - mae: 1.6630 - val_loss: 12.4558 - val_mse: 12.4558 - val_mae: 1.5353 - lr: 0.0033 - 3s/epoch - 3ms/step\n",
            "Epoch 23/100\n",
            "1000/1000 - 3s - loss: 16.0591 - mse: 16.0591 - mae: 1.6551 - val_loss: 12.5070 - val_mse: 12.5070 - val_mae: 1.4945 - lr: 0.0033 - 3s/epoch - 3ms/step\n",
            "Epoch 24/100\n",
            "1000/1000 - 3s - loss: 16.0786 - mse: 16.0786 - mae: 1.6620 - val_loss: 12.4631 - val_mse: 12.4631 - val_mae: 1.6660 - lr: 0.0033 - 3s/epoch - 3ms/step\n",
            "Score for fold 1: loss of 12.463117599487305\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 4s - loss: 14.3897 - mse: 14.3897 - mae: 1.6091 - val_loss: 18.2905 - val_mse: 18.2905 - val_mae: 1.5321 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 3s - loss: 14.2840 - mse: 14.2840 - mae: 1.6009 - val_loss: 18.1064 - val_mse: 18.1064 - val_mae: 1.5794 - lr: 0.0010 - 3s/epoch - 3ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 3s - loss: 14.2458 - mse: 14.2458 - mae: 1.5987 - val_loss: 18.1731 - val_mse: 18.1731 - val_mae: 1.5272 - lr: 0.0010 - 3s/epoch - 3ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 3s - loss: 14.1644 - mse: 14.1644 - mae: 1.5927 - val_loss: 18.1012 - val_mse: 18.1012 - val_mae: 1.6658 - lr: 0.0010 - 3s/epoch - 3ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 3s - loss: 14.2058 - mse: 14.2058 - mae: 1.5956 - val_loss: 18.0122 - val_mse: 18.0122 - val_mae: 1.6762 - lr: 0.0010 - 3s/epoch - 3ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 3s - loss: 14.1140 - mse: 14.1140 - mae: 1.5936 - val_loss: 18.0278 - val_mse: 18.0278 - val_mae: 1.5720 - lr: 0.0010 - 3s/epoch - 3ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 3s - loss: 14.1224 - mse: 14.1224 - mae: 1.5929 - val_loss: 18.3323 - val_mse: 18.3323 - val_mae: 1.7435 - lr: 0.0010 - 3s/epoch - 3ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 3s - loss: 14.1509 - mse: 14.1509 - mae: 1.6002 - val_loss: 18.1683 - val_mse: 18.1683 - val_mae: 1.5625 - lr: 0.0010 - 3s/epoch - 3ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 3s - loss: 14.1065 - mse: 14.1065 - mae: 1.5989 - val_loss: 18.1637 - val_mse: 18.1637 - val_mae: 1.5436 - lr: 0.0010 - 3s/epoch - 3ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 3s - loss: 14.0462 - mse: 14.0462 - mae: 1.5946 - val_loss: 18.1638 - val_mse: 18.1638 - val_mae: 1.6380 - lr: 0.0010 - 3s/epoch - 3ms/step\n",
            "Score for fold 2: loss of 18.163761138916016\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 4s - loss: 15.8467 - mse: 15.8467 - mae: 1.5984 - val_loss: 10.9832 - val_mse: 10.9832 - val_mae: 1.5171 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 3s - loss: 15.7665 - mse: 15.7665 - mae: 1.5974 - val_loss: 11.1468 - val_mse: 11.1468 - val_mae: 1.6294 - lr: 0.0010 - 3s/epoch - 3ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 3s - loss: 15.7244 - mse: 15.7244 - mae: 1.5862 - val_loss: 11.0958 - val_mse: 11.0958 - val_mae: 1.5333 - lr: 0.0010 - 3s/epoch - 3ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 3s - loss: 15.7525 - mse: 15.7525 - mae: 1.5870 - val_loss: 10.9930 - val_mse: 10.9930 - val_mae: 1.6659 - lr: 0.0010 - 3s/epoch - 3ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 3s - loss: 15.7307 - mse: 15.7307 - mae: 1.5890 - val_loss: 11.2905 - val_mse: 11.2905 - val_mae: 1.5221 - lr: 0.0010 - 3s/epoch - 3ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 3s - loss: 15.7435 - mse: 15.7435 - mae: 1.5911 - val_loss: 11.1190 - val_mse: 11.1190 - val_mae: 1.5658 - lr: 0.0010 - 3s/epoch - 3ms/step\n",
            "Score for fold 3: loss of 11.118987083435059\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 3s - loss: 13.5733 - mse: 13.5733 - mae: 1.5857 - val_loss: 19.9519 - val_mse: 19.9519 - val_mae: 1.6159 - lr: 0.0010 - 3s/epoch - 3ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 4s - loss: 13.5072 - mse: 13.5072 - mae: 1.5825 - val_loss: 19.8202 - val_mse: 19.8202 - val_mae: 1.5999 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 4s - loss: 13.4782 - mse: 13.4782 - mae: 1.5857 - val_loss: 19.8777 - val_mse: 19.8777 - val_mae: 1.6366 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 4s - loss: 13.4578 - mse: 13.4578 - mae: 1.5884 - val_loss: 19.9181 - val_mse: 19.9181 - val_mae: 1.5709 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 5s - loss: 13.4363 - mse: 13.4363 - mae: 1.5869 - val_loss: 19.9673 - val_mse: 19.9673 - val_mae: 1.7368 - lr: 0.0010 - 5s/epoch - 5ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 3s - loss: 13.4543 - mse: 13.4543 - mae: 1.5935 - val_loss: 19.9691 - val_mse: 19.9691 - val_mae: 1.7156 - lr: 0.0010 - 3s/epoch - 3ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 3s - loss: 13.4536 - mse: 13.4536 - mae: 1.5830 - val_loss: 19.9801 - val_mse: 19.9801 - val_mae: 1.7774 - lr: 0.0010 - 3s/epoch - 3ms/step\n",
            "Score for fold 4: loss of 19.980093002319336\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 4s - loss: 15.1860 - mse: 15.1860 - mae: 1.5880 - val_loss: 13.2861 - val_mse: 13.2861 - val_mae: 1.6860 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 3s - loss: 15.0883 - mse: 15.0883 - mae: 1.5802 - val_loss: 13.2963 - val_mse: 13.2963 - val_mae: 1.5751 - lr: 0.0010 - 3s/epoch - 3ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 3s - loss: 15.1353 - mse: 15.1353 - mae: 1.5713 - val_loss: 13.3036 - val_mse: 13.3036 - val_mae: 1.6482 - lr: 0.0010 - 3s/epoch - 3ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 3s - loss: 15.1140 - mse: 15.1140 - mae: 1.5757 - val_loss: 13.3206 - val_mse: 13.3206 - val_mae: 1.5449 - lr: 0.0010 - 3s/epoch - 3ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 3s - loss: 14.9930 - mse: 14.9930 - mae: 1.5739 - val_loss: 13.4599 - val_mse: 13.4599 - val_mae: 1.7957 - lr: 0.0010 - 3s/epoch - 3ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 3s - loss: 15.0176 - mse: 15.0176 - mae: 1.5811 - val_loss: 13.1404 - val_mse: 13.1404 - val_mae: 1.7168 - lr: 0.0010 - 3s/epoch - 3ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 3s - loss: 15.0891 - mse: 15.0891 - mae: 1.5765 - val_loss: 13.2960 - val_mse: 13.2960 - val_mae: 1.6507 - lr: 0.0010 - 3s/epoch - 3ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 3s - loss: 14.9836 - mse: 14.9836 - mae: 1.5758 - val_loss: 13.3192 - val_mse: 13.3192 - val_mae: 1.6041 - lr: 0.0010 - 3s/epoch - 3ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 4s - loss: 14.9864 - mse: 14.9864 - mae: 1.5739 - val_loss: 13.6012 - val_mse: 13.6012 - val_mae: 1.5669 - lr: 0.0010 - 4s/epoch - 4ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 3s - loss: 15.0084 - mse: 15.0084 - mae: 1.5740 - val_loss: 13.3230 - val_mse: 13.3230 - val_mae: 1.6289 - lr: 0.0010 - 3s/epoch - 3ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 3s - loss: 15.0210 - mse: 15.0210 - mae: 1.5788 - val_loss: 13.2616 - val_mse: 13.2616 - val_mae: 1.5962 - lr: 0.0010 - 3s/epoch - 3ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-10-15 10:51:18,082]\u001b[0m Finished trial#9 resulted in value: 14.996. Current best value is 14.618 with parameters: {'activation': 'relu', 'num_hidden_layer': 4, 'i': 10, 'learning_rate': 0.0001995902281877497}.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Score for fold 5: loss of 13.261651039123535\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 38s - loss: 14.5601 - mse: 14.5601 - mae: 1.6813 - val_loss: 21.4267 - val_mse: 21.4267 - val_mae: 1.6405 - lr: 1.0807e-04 - 38s/epoch - 38ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 37s - loss: 14.0404 - mse: 14.0404 - mae: 1.6269 - val_loss: 21.3677 - val_mse: 21.3677 - val_mae: 1.5944 - lr: 1.0807e-04 - 37s/epoch - 37ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 39s - loss: 13.9483 - mse: 13.9483 - mae: 1.6114 - val_loss: 21.1579 - val_mse: 21.1579 - val_mae: 1.6341 - lr: 1.0807e-04 - 39s/epoch - 39ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 37s - loss: 13.9202 - mse: 13.9202 - mae: 1.6116 - val_loss: 21.1101 - val_mse: 21.1101 - val_mae: 1.6718 - lr: 1.0807e-04 - 37s/epoch - 37ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 36s - loss: 13.8879 - mse: 13.8879 - mae: 1.6042 - val_loss: 21.1245 - val_mse: 21.1245 - val_mae: 1.6247 - lr: 1.0807e-04 - 36s/epoch - 36ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 36s - loss: 13.8661 - mse: 13.8661 - mae: 1.5998 - val_loss: 21.1931 - val_mse: 21.1931 - val_mae: 1.6062 - lr: 1.0807e-04 - 36s/epoch - 36ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 38s - loss: 13.8560 - mse: 13.8560 - mae: 1.6053 - val_loss: 21.4158 - val_mse: 21.4158 - val_mae: 1.6264 - lr: 1.0807e-04 - 38s/epoch - 38ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 36s - loss: 13.8666 - mse: 13.8666 - mae: 1.6023 - val_loss: 21.0860 - val_mse: 21.0860 - val_mae: 1.6390 - lr: 1.0807e-04 - 36s/epoch - 36ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 36s - loss: 13.8154 - mse: 13.8154 - mae: 1.5973 - val_loss: 21.3969 - val_mse: 21.3969 - val_mae: 1.5715 - lr: 1.0807e-04 - 36s/epoch - 36ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 36s - loss: 13.7589 - mse: 13.7589 - mae: 1.5966 - val_loss: 21.2715 - val_mse: 21.2715 - val_mae: 1.5941 - lr: 1.0807e-04 - 36s/epoch - 36ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 38s - loss: 13.7773 - mse: 13.7773 - mae: 1.5918 - val_loss: 21.2885 - val_mse: 21.2885 - val_mae: 1.5777 - lr: 1.0807e-04 - 38s/epoch - 38ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 36s - loss: 13.7828 - mse: 13.7828 - mae: 1.5947 - val_loss: 21.0749 - val_mse: 21.0749 - val_mae: 1.6613 - lr: 1.0807e-04 - 36s/epoch - 36ms/step\n",
            "Epoch 13/100\n",
            "1000/1000 - 39s - loss: 13.7706 - mse: 13.7706 - mae: 1.5948 - val_loss: 21.1072 - val_mse: 21.1072 - val_mae: 1.6120 - lr: 1.0807e-04 - 39s/epoch - 39ms/step\n",
            "Epoch 14/100\n",
            "1000/1000 - 36s - loss: 13.7612 - mse: 13.7612 - mae: 1.5909 - val_loss: 21.2655 - val_mse: 21.2655 - val_mae: 1.5603 - lr: 1.0807e-04 - 36s/epoch - 36ms/step\n",
            "Epoch 15/100\n",
            "1000/1000 - 39s - loss: 13.7503 - mse: 13.7503 - mae: 1.5949 - val_loss: 21.0234 - val_mse: 21.0234 - val_mae: 1.6008 - lr: 1.0807e-04 - 39s/epoch - 39ms/step\n",
            "Epoch 16/100\n",
            "1000/1000 - 36s - loss: 13.7173 - mse: 13.7173 - mae: 1.5934 - val_loss: 21.1243 - val_mse: 21.1243 - val_mae: 1.6239 - lr: 1.0807e-04 - 36s/epoch - 36ms/step\n",
            "Epoch 17/100\n",
            "1000/1000 - 36s - loss: 13.7244 - mse: 13.7244 - mae: 1.5866 - val_loss: 20.9440 - val_mse: 20.9440 - val_mae: 1.6133 - lr: 1.0807e-04 - 36s/epoch - 36ms/step\n",
            "Epoch 18/100\n",
            "1000/1000 - 36s - loss: 13.6916 - mse: 13.6916 - mae: 1.5913 - val_loss: 21.1667 - val_mse: 21.1667 - val_mae: 1.6200 - lr: 1.0807e-04 - 36s/epoch - 36ms/step\n",
            "Epoch 19/100\n",
            "1000/1000 - 38s - loss: 13.6700 - mse: 13.6700 - mae: 1.5927 - val_loss: 21.4237 - val_mse: 21.4237 - val_mae: 1.5959 - lr: 1.0807e-04 - 38s/epoch - 38ms/step\n",
            "Epoch 20/100\n",
            "1000/1000 - 37s - loss: 13.6851 - mse: 13.6851 - mae: 1.5905 - val_loss: 20.9532 - val_mse: 20.9532 - val_mae: 1.6664 - lr: 1.0807e-04 - 37s/epoch - 37ms/step\n",
            "Epoch 21/100\n",
            "1000/1000 - 37s - loss: 13.6444 - mse: 13.6444 - mae: 1.5851 - val_loss: 20.8921 - val_mse: 20.8921 - val_mae: 1.6085 - lr: 1.0807e-04 - 37s/epoch - 37ms/step\n",
            "Epoch 22/100\n",
            "1000/1000 - 37s - loss: 13.6184 - mse: 13.6184 - mae: 1.5895 - val_loss: 21.0730 - val_mse: 21.0730 - val_mae: 1.5542 - lr: 1.0807e-04 - 37s/epoch - 37ms/step\n",
            "Epoch 23/100\n",
            "1000/1000 - 38s - loss: 13.6364 - mse: 13.6364 - mae: 1.5875 - val_loss: 21.0256 - val_mse: 21.0256 - val_mae: 1.6058 - lr: 1.0807e-04 - 38s/epoch - 38ms/step\n",
            "Epoch 24/100\n",
            "1000/1000 - 36s - loss: 13.6034 - mse: 13.6034 - mae: 1.5854 - val_loss: 21.1125 - val_mse: 21.1125 - val_mae: 1.6024 - lr: 1.0807e-04 - 36s/epoch - 36ms/step\n",
            "Epoch 25/100\n",
            "1000/1000 - 36s - loss: 13.5815 - mse: 13.5815 - mae: 1.5840 - val_loss: 20.8644 - val_mse: 20.8644 - val_mae: 1.6542 - lr: 1.0807e-04 - 36s/epoch - 36ms/step\n",
            "Epoch 26/100\n",
            "1000/1000 - 36s - loss: 13.5541 - mse: 13.5541 - mae: 1.5810 - val_loss: 20.9535 - val_mse: 20.9535 - val_mae: 1.7047 - lr: 1.0807e-04 - 36s/epoch - 36ms/step\n",
            "Epoch 27/100\n",
            "1000/1000 - 37s - loss: 13.5674 - mse: 13.5674 - mae: 1.5869 - val_loss: 20.9262 - val_mse: 20.9262 - val_mae: 1.6652 - lr: 1.0807e-04 - 37s/epoch - 37ms/step\n",
            "Epoch 28/100\n",
            "1000/1000 - 38s - loss: 13.5436 - mse: 13.5436 - mae: 1.5834 - val_loss: 21.0384 - val_mse: 21.0384 - val_mae: 1.5417 - lr: 1.0807e-04 - 38s/epoch - 38ms/step\n",
            "Epoch 29/100\n",
            "1000/1000 - 38s - loss: 13.5238 - mse: 13.5238 - mae: 1.5867 - val_loss: 21.1748 - val_mse: 21.1748 - val_mae: 1.5510 - lr: 1.0807e-04 - 38s/epoch - 38ms/step\n",
            "Epoch 30/100\n",
            "1000/1000 - 37s - loss: 13.5466 - mse: 13.5466 - mae: 1.5798 - val_loss: 21.1654 - val_mse: 21.1654 - val_mae: 1.6532 - lr: 1.0807e-04 - 37s/epoch - 37ms/step\n",
            "Score for fold 1: loss of 21.165395736694336\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 40s - loss: 16.1222 - mse: 16.1222 - mae: 1.5966 - val_loss: 10.3460 - val_mse: 10.3460 - val_mae: 1.5820 - lr: 1.0807e-04 - 40s/epoch - 40ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 37s - loss: 16.1724 - mse: 16.1724 - mae: 1.5945 - val_loss: 10.5646 - val_mse: 10.5646 - val_mae: 1.5534 - lr: 1.0807e-04 - 37s/epoch - 37ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 37s - loss: 16.1564 - mse: 16.1564 - mae: 1.5916 - val_loss: 10.4838 - val_mse: 10.4838 - val_mae: 1.5894 - lr: 1.0807e-04 - 37s/epoch - 37ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 37s - loss: 16.1123 - mse: 16.1123 - mae: 1.5941 - val_loss: 10.6594 - val_mse: 10.6594 - val_mae: 1.6229 - lr: 1.0807e-04 - 37s/epoch - 37ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 39s - loss: 16.1325 - mse: 16.1325 - mae: 1.5946 - val_loss: 10.5707 - val_mse: 10.5707 - val_mae: 1.5360 - lr: 1.0807e-04 - 39s/epoch - 39ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 37s - loss: 16.1239 - mse: 16.1239 - mae: 1.5911 - val_loss: 10.7525 - val_mse: 10.7525 - val_mae: 1.5932 - lr: 1.0807e-04 - 37s/epoch - 37ms/step\n",
            "Score for fold 2: loss of 10.752457618713379\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 38s - loss: 13.8241 - mse: 13.8241 - mae: 1.5842 - val_loss: 19.7089 - val_mse: 19.7089 - val_mae: 1.5390 - lr: 1.0807e-04 - 38s/epoch - 38ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 39s - loss: 13.7826 - mse: 13.7826 - mae: 1.5810 - val_loss: 19.6242 - val_mse: 19.6242 - val_mae: 1.6253 - lr: 1.0807e-04 - 39s/epoch - 39ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 38s - loss: 13.7620 - mse: 13.7620 - mae: 1.5765 - val_loss: 19.6093 - val_mse: 19.6093 - val_mae: 1.6787 - lr: 1.0807e-04 - 38s/epoch - 38ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 37s - loss: 13.7853 - mse: 13.7853 - mae: 1.5791 - val_loss: 19.5976 - val_mse: 19.5976 - val_mae: 1.7386 - lr: 1.0807e-04 - 37s/epoch - 37ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 38s - loss: 13.7935 - mse: 13.7935 - mae: 1.5811 - val_loss: 19.6309 - val_mse: 19.6309 - val_mae: 1.5912 - lr: 1.0807e-04 - 38s/epoch - 38ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 39s - loss: 13.7381 - mse: 13.7381 - mae: 1.5756 - val_loss: 19.7149 - val_mse: 19.7149 - val_mae: 1.6261 - lr: 1.0807e-04 - 39s/epoch - 39ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 37s - loss: 13.7760 - mse: 13.7760 - mae: 1.5777 - val_loss: 19.7526 - val_mse: 19.7526 - val_mae: 1.6165 - lr: 1.0807e-04 - 37s/epoch - 37ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 38s - loss: 13.8029 - mse: 13.8029 - mae: 1.5769 - val_loss: 19.5999 - val_mse: 19.5999 - val_mae: 1.6569 - lr: 1.0807e-04 - 38s/epoch - 38ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 37s - loss: 13.7394 - mse: 13.7394 - mae: 1.5796 - val_loss: 19.6943 - val_mse: 19.6943 - val_mae: 1.6517 - lr: 1.0807e-04 - 37s/epoch - 37ms/step\n",
            "Score for fold 3: loss of 19.69432258605957\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 36s - loss: 16.1200 - mse: 16.1200 - mae: 1.6019 - val_loss: 10.4301 - val_mse: 10.4301 - val_mae: 1.5594 - lr: 1.0807e-04 - 36s/epoch - 36ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 36s - loss: 16.0313 - mse: 16.0313 - mae: 1.5947 - val_loss: 10.4207 - val_mse: 10.4207 - val_mae: 1.6255 - lr: 1.0807e-04 - 36s/epoch - 36ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 36s - loss: 16.0061 - mse: 16.0061 - mae: 1.5944 - val_loss: 10.5618 - val_mse: 10.5618 - val_mae: 1.4821 - lr: 1.0807e-04 - 36s/epoch - 36ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 37s - loss: 15.9970 - mse: 15.9970 - mae: 1.5940 - val_loss: 10.4148 - val_mse: 10.4148 - val_mae: 1.6197 - lr: 1.0807e-04 - 37s/epoch - 37ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 35s - loss: 15.9428 - mse: 15.9428 - mae: 1.5940 - val_loss: 10.4497 - val_mse: 10.4497 - val_mae: 1.5502 - lr: 1.0807e-04 - 35s/epoch - 35ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 37s - loss: 15.9502 - mse: 15.9502 - mae: 1.5938 - val_loss: 10.4700 - val_mse: 10.4700 - val_mae: 1.5220 - lr: 1.0807e-04 - 37s/epoch - 37ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 35s - loss: 15.9593 - mse: 15.9593 - mae: 1.5944 - val_loss: 10.9587 - val_mse: 10.9587 - val_mae: 1.7395 - lr: 1.0807e-04 - 35s/epoch - 35ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 37s - loss: 15.8843 - mse: 15.8843 - mae: 1.5916 - val_loss: 10.4323 - val_mse: 10.4323 - val_mae: 1.4642 - lr: 1.0807e-04 - 37s/epoch - 37ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 35s - loss: 15.9179 - mse: 15.9179 - mae: 1.5924 - val_loss: 10.4711 - val_mse: 10.4711 - val_mae: 1.5618 - lr: 1.0807e-04 - 35s/epoch - 35ms/step\n",
            "Score for fold 4: loss of 10.471107482910156\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 34s - loss: 15.2630 - mse: 15.2630 - mae: 1.5875 - val_loss: 13.0520 - val_mse: 13.0520 - val_mae: 1.5872 - lr: 1.0807e-04 - 34s/epoch - 34ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 37s - loss: 15.1950 - mse: 15.1950 - mae: 1.5861 - val_loss: 13.2253 - val_mse: 13.2253 - val_mae: 1.5755 - lr: 1.0807e-04 - 37s/epoch - 37ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 35s - loss: 15.2226 - mse: 15.2226 - mae: 1.5814 - val_loss: 13.1076 - val_mse: 13.1076 - val_mae: 1.5772 - lr: 1.0807e-04 - 35s/epoch - 35ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 35s - loss: 15.1606 - mse: 15.1606 - mae: 1.5818 - val_loss: 13.4533 - val_mse: 13.4533 - val_mae: 1.5544 - lr: 1.0807e-04 - 35s/epoch - 35ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 35s - loss: 15.1955 - mse: 15.1955 - mae: 1.5792 - val_loss: 13.3301 - val_mse: 13.3301 - val_mae: 1.5346 - lr: 1.0807e-04 - 35s/epoch - 35ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 37s - loss: 15.1353 - mse: 15.1353 - mae: 1.5755 - val_loss: 13.3257 - val_mse: 13.3257 - val_mae: 1.6208 - lr: 1.0807e-04 - 37s/epoch - 37ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-10-15 11:31:31,153]\u001b[0m Finished trial#10 resulted in value: 15.081999999999999. Current best value is 14.618 with parameters: {'activation': 'relu', 'num_hidden_layer': 4, 'i': 10, 'learning_rate': 0.0001995902281877497}.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Score for fold 5: loss of 13.325726509094238\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 11s - loss: 15.4876 - mse: 15.4876 - mae: 1.6118 - val_loss: 14.7074 - val_mse: 14.7074 - val_mae: 1.7010 - lr: 5.1839e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 10s - loss: 15.1982 - mse: 15.1982 - mae: 1.6024 - val_loss: 14.6784 - val_mse: 14.6784 - val_mae: 1.5864 - lr: 5.1839e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 10s - loss: 15.1785 - mse: 15.1785 - mae: 1.5971 - val_loss: 14.6599 - val_mse: 14.6599 - val_mae: 1.6093 - lr: 5.1839e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 10s - loss: 15.1482 - mse: 15.1482 - mae: 1.5889 - val_loss: 14.8920 - val_mse: 14.8920 - val_mae: 1.5541 - lr: 5.1839e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 10s - loss: 15.0555 - mse: 15.0555 - mae: 1.5835 - val_loss: 14.7365 - val_mse: 14.7365 - val_mae: 1.6007 - lr: 5.1839e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 10s - loss: 15.0580 - mse: 15.0580 - mae: 1.5777 - val_loss: 14.4995 - val_mse: 14.4995 - val_mae: 1.6900 - lr: 5.1839e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 10s - loss: 15.0484 - mse: 15.0484 - mae: 1.5776 - val_loss: 14.7584 - val_mse: 14.7584 - val_mae: 1.6478 - lr: 5.1839e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 11s - loss: 14.9575 - mse: 14.9575 - mae: 1.5738 - val_loss: 14.6577 - val_mse: 14.6577 - val_mae: 1.6277 - lr: 5.1839e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 10s - loss: 14.9442 - mse: 14.9442 - mae: 1.5745 - val_loss: 14.5909 - val_mse: 14.5909 - val_mae: 1.6046 - lr: 5.1839e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 10s - loss: 14.9987 - mse: 14.9987 - mae: 1.5736 - val_loss: 14.4765 - val_mse: 14.4765 - val_mae: 1.5996 - lr: 5.1839e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 10s - loss: 14.8666 - mse: 14.8666 - mae: 1.5766 - val_loss: 14.7228 - val_mse: 14.7228 - val_mae: 1.6397 - lr: 5.1839e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 10s - loss: 14.9301 - mse: 14.9301 - mae: 1.5791 - val_loss: 14.6532 - val_mse: 14.6532 - val_mae: 1.6675 - lr: 5.1839e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 13/100\n",
            "1000/1000 - 10s - loss: 14.7980 - mse: 14.7980 - mae: 1.5746 - val_loss: 14.8611 - val_mse: 14.8611 - val_mae: 1.6479 - lr: 5.1839e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 14/100\n",
            "1000/1000 - 10s - loss: 14.7739 - mse: 14.7739 - mae: 1.5792 - val_loss: 14.7404 - val_mse: 14.7404 - val_mae: 1.6800 - lr: 5.1839e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 15/100\n",
            "1000/1000 - 10s - loss: 14.7369 - mse: 14.7369 - mae: 1.5787 - val_loss: 14.8030 - val_mse: 14.8030 - val_mae: 1.6504 - lr: 5.1839e-04 - 10s/epoch - 10ms/step\n",
            "Score for fold 1: loss of 14.802990913391113\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 9s - loss: 15.8774 - mse: 15.8774 - mae: 1.6160 - val_loss: 10.4080 - val_mse: 10.4080 - val_mae: 1.6346 - lr: 5.1839e-04 - 9s/epoch - 9ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 9s - loss: 15.8331 - mse: 15.8331 - mae: 1.6113 - val_loss: 10.7398 - val_mse: 10.7398 - val_mae: 1.5881 - lr: 5.1839e-04 - 9s/epoch - 9ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 12s - loss: 15.6064 - mse: 15.6064 - mae: 1.6058 - val_loss: 10.4783 - val_mse: 10.4783 - val_mae: 1.5728 - lr: 5.1839e-04 - 12s/epoch - 12ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 10s - loss: 15.8204 - mse: 15.8204 - mae: 1.6094 - val_loss: 10.7512 - val_mse: 10.7512 - val_mae: 1.5287 - lr: 5.1839e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 10s - loss: 15.6079 - mse: 15.6079 - mae: 1.6019 - val_loss: 10.6188 - val_mse: 10.6188 - val_mae: 1.5468 - lr: 5.1839e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 10s - loss: 15.6488 - mse: 15.6488 - mae: 1.6125 - val_loss: 10.2571 - val_mse: 10.2571 - val_mae: 1.4548 - lr: 5.1839e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 10s - loss: 15.6425 - mse: 15.6425 - mae: 1.6061 - val_loss: 10.7379 - val_mse: 10.7379 - val_mae: 1.5638 - lr: 5.1839e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 10s - loss: 15.5591 - mse: 15.5591 - mae: 1.6077 - val_loss: 10.9189 - val_mse: 10.9189 - val_mae: 1.7957 - lr: 5.1839e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 10s - loss: 15.6274 - mse: 15.6274 - mae: 1.6144 - val_loss: 11.0920 - val_mse: 11.0920 - val_mae: 1.5157 - lr: 5.1839e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 9s - loss: 15.6057 - mse: 15.6057 - mae: 1.6049 - val_loss: 11.0124 - val_mse: 11.0124 - val_mae: 1.4960 - lr: 5.1839e-04 - 9s/epoch - 9ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 9s - loss: 15.3389 - mse: 15.3389 - mae: 1.5971 - val_loss: 10.9988 - val_mse: 10.9988 - val_mae: 1.5692 - lr: 5.1839e-04 - 9s/epoch - 9ms/step\n",
            "Score for fold 2: loss of 10.998758316040039\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 9s - loss: 13.7428 - mse: 13.7428 - mae: 1.5835 - val_loss: 18.0149 - val_mse: 18.0149 - val_mae: 1.7559 - lr: 5.1839e-04 - 9s/epoch - 9ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 9s - loss: 13.5701 - mse: 13.5701 - mae: 1.5766 - val_loss: 18.0483 - val_mse: 18.0483 - val_mae: 1.7095 - lr: 5.1839e-04 - 9s/epoch - 9ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 9s - loss: 13.5261 - mse: 13.5261 - mae: 1.5731 - val_loss: 17.8493 - val_mse: 17.8493 - val_mae: 1.6325 - lr: 5.1839e-04 - 9s/epoch - 9ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 12s - loss: 13.4159 - mse: 13.4159 - mae: 1.5743 - val_loss: 18.1308 - val_mse: 18.1308 - val_mae: 1.6428 - lr: 5.1839e-04 - 12s/epoch - 12ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 9s - loss: 13.7180 - mse: 13.7180 - mae: 1.5827 - val_loss: 18.2052 - val_mse: 18.2052 - val_mae: 1.5177 - lr: 5.1839e-04 - 9s/epoch - 9ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 10s - loss: 13.3979 - mse: 13.3979 - mae: 1.5783 - val_loss: 18.1715 - val_mse: 18.1715 - val_mae: 1.6236 - lr: 5.1839e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 10s - loss: 13.3964 - mse: 13.3964 - mae: 1.5790 - val_loss: 18.1636 - val_mse: 18.1636 - val_mae: 1.6057 - lr: 5.1839e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 10s - loss: 13.4565 - mse: 13.4565 - mae: 1.5863 - val_loss: 18.1860 - val_mse: 18.1860 - val_mae: 1.6229 - lr: 5.1839e-04 - 10s/epoch - 10ms/step\n",
            "Score for fold 3: loss of 18.185997009277344\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 10s - loss: 12.9305 - mse: 12.9305 - mae: 1.5716 - val_loss: 20.2458 - val_mse: 20.2458 - val_mae: 1.5727 - lr: 5.1839e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 9s - loss: 13.0320 - mse: 13.0320 - mae: 1.5771 - val_loss: 19.8126 - val_mse: 19.8126 - val_mae: 1.5428 - lr: 5.1839e-04 - 9s/epoch - 9ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 9s - loss: 12.7828 - mse: 12.7828 - mae: 1.5721 - val_loss: 19.9671 - val_mse: 19.9671 - val_mae: 1.5867 - lr: 5.1839e-04 - 9s/epoch - 9ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 10s - loss: 12.7847 - mse: 12.7847 - mae: 1.5629 - val_loss: 20.6938 - val_mse: 20.6938 - val_mae: 1.6386 - lr: 5.1839e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 9s - loss: 12.8061 - mse: 12.8061 - mae: 1.5670 - val_loss: 19.9583 - val_mse: 19.9583 - val_mae: 1.5839 - lr: 5.1839e-04 - 9s/epoch - 9ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 9s - loss: 12.8303 - mse: 12.8303 - mae: 1.5642 - val_loss: 20.6628 - val_mse: 20.6628 - val_mae: 1.6708 - lr: 5.1839e-04 - 9s/epoch - 9ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 9s - loss: 12.7227 - mse: 12.7227 - mae: 1.5519 - val_loss: 20.5724 - val_mse: 20.5724 - val_mae: 1.5143 - lr: 5.1839e-04 - 9s/epoch - 9ms/step\n",
            "Score for fold 4: loss of 20.572439193725586\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 10s - loss: 15.5650 - mse: 15.5650 - mae: 1.5816 - val_loss: 9.3396 - val_mse: 9.3396 - val_mae: 1.6052 - lr: 5.1839e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 10s - loss: 15.3979 - mse: 15.3979 - mae: 1.5821 - val_loss: 9.1993 - val_mse: 9.1993 - val_mae: 1.5993 - lr: 5.1839e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 11s - loss: 15.6098 - mse: 15.6098 - mae: 1.5825 - val_loss: 9.5457 - val_mse: 9.5457 - val_mae: 1.4548 - lr: 5.1839e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 11s - loss: 15.4032 - mse: 15.4032 - mae: 1.5775 - val_loss: 9.4139 - val_mse: 9.4139 - val_mae: 1.5491 - lr: 5.1839e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 10s - loss: 15.4215 - mse: 15.4215 - mae: 1.5818 - val_loss: 10.3744 - val_mse: 10.3744 - val_mae: 1.4356 - lr: 5.1839e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 10s - loss: 15.3227 - mse: 15.3227 - mae: 1.5816 - val_loss: 9.8330 - val_mse: 9.8330 - val_mae: 1.5239 - lr: 5.1839e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 10s - loss: 15.3091 - mse: 15.3091 - mae: 1.5841 - val_loss: 9.6263 - val_mse: 9.6263 - val_mae: 1.5292 - lr: 5.1839e-04 - 10s/epoch - 10ms/step\n",
            "Score for fold 5: loss of 9.626299858093262\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-10-15 11:41:04,279]\u001b[0m Finished trial#11 resulted in value: 14.838. Current best value is 14.618 with parameters: {'activation': 'relu', 'num_hidden_layer': 4, 'i': 10, 'learning_rate': 0.0001995902281877497}.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 4s - loss: 15.5747 - mse: 15.5747 - mae: 1.6190 - val_loss: 15.5832 - val_mse: 15.5832 - val_mae: 1.5581 - lr: 3.6030e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 3s - loss: 14.9954 - mse: 14.9954 - mae: 1.5913 - val_loss: 15.4302 - val_mse: 15.4302 - val_mae: 1.5826 - lr: 3.6030e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 14.9032 - mse: 14.9032 - mae: 1.5838 - val_loss: 15.3240 - val_mse: 15.3240 - val_mae: 1.6266 - lr: 3.6030e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 14.8849 - mse: 14.8849 - mae: 1.5871 - val_loss: 15.2980 - val_mse: 15.2980 - val_mae: 1.6293 - lr: 3.6030e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 3s - loss: 14.8302 - mse: 14.8302 - mae: 1.5885 - val_loss: 15.4359 - val_mse: 15.4359 - val_mae: 1.5966 - lr: 3.6030e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 14.7913 - mse: 14.7913 - mae: 1.5845 - val_loss: 15.3402 - val_mse: 15.3402 - val_mae: 1.5844 - lr: 3.6030e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 14.7763 - mse: 14.7763 - mae: 1.5804 - val_loss: 15.4062 - val_mse: 15.4062 - val_mae: 1.5794 - lr: 3.6030e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 2s - loss: 14.7131 - mse: 14.7131 - mae: 1.5757 - val_loss: 15.2977 - val_mse: 15.2977 - val_mae: 1.5851 - lr: 3.6030e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 2s - loss: 14.7613 - mse: 14.7613 - mae: 1.5707 - val_loss: 15.2677 - val_mse: 15.2677 - val_mae: 1.6118 - lr: 3.6030e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 2s - loss: 14.7195 - mse: 14.7195 - mae: 1.5727 - val_loss: 15.2229 - val_mse: 15.2229 - val_mae: 1.6253 - lr: 3.6030e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 3s - loss: 14.7007 - mse: 14.7007 - mae: 1.5697 - val_loss: 15.3136 - val_mse: 15.3136 - val_mae: 1.5517 - lr: 3.6030e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 3s - loss: 14.6644 - mse: 14.6644 - mae: 1.5662 - val_loss: 15.2769 - val_mse: 15.2769 - val_mae: 1.5465 - lr: 3.6030e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 13/100\n",
            "1000/1000 - 3s - loss: 14.6079 - mse: 14.6079 - mae: 1.5661 - val_loss: 15.2025 - val_mse: 15.2025 - val_mae: 1.5547 - lr: 3.6030e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 14/100\n",
            "1000/1000 - 2s - loss: 14.6459 - mse: 14.6459 - mae: 1.5665 - val_loss: 15.2951 - val_mse: 15.2951 - val_mae: 1.6148 - lr: 3.6030e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 15/100\n",
            "1000/1000 - 2s - loss: 14.6411 - mse: 14.6411 - mae: 1.5714 - val_loss: 15.1795 - val_mse: 15.1795 - val_mae: 1.6297 - lr: 3.6030e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 16/100\n",
            "1000/1000 - 3s - loss: 14.6159 - mse: 14.6159 - mae: 1.5659 - val_loss: 15.2939 - val_mse: 15.2939 - val_mae: 1.6286 - lr: 3.6030e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 17/100\n",
            "1000/1000 - 2s - loss: 14.5355 - mse: 14.5355 - mae: 1.5631 - val_loss: 15.3399 - val_mse: 15.3399 - val_mae: 1.5709 - lr: 3.6030e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 18/100\n",
            "1000/1000 - 3s - loss: 14.5070 - mse: 14.5070 - mae: 1.5636 - val_loss: 15.2091 - val_mse: 15.2091 - val_mae: 1.6286 - lr: 3.6030e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 19/100\n",
            "1000/1000 - 2s - loss: 14.5145 - mse: 14.5145 - mae: 1.5622 - val_loss: 15.2305 - val_mse: 15.2305 - val_mae: 1.5844 - lr: 3.6030e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 20/100\n",
            "1000/1000 - 2s - loss: 14.4690 - mse: 14.4690 - mae: 1.5601 - val_loss: 15.2533 - val_mse: 15.2533 - val_mae: 1.5857 - lr: 3.6030e-04 - 2s/epoch - 2ms/step\n",
            "Score for fold 1: loss of 15.253280639648438\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 3s - loss: 14.9106 - mse: 14.9106 - mae: 1.5739 - val_loss: 13.5944 - val_mse: 13.5944 - val_mae: 1.6093 - lr: 3.6030e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 3s - loss: 14.8360 - mse: 14.8360 - mae: 1.5693 - val_loss: 13.6596 - val_mse: 13.6596 - val_mae: 1.5557 - lr: 3.6030e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 14.7976 - mse: 14.7976 - mae: 1.5698 - val_loss: 13.3801 - val_mse: 13.3801 - val_mae: 1.5516 - lr: 3.6030e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 3s - loss: 14.7663 - mse: 14.7663 - mae: 1.5706 - val_loss: 13.5917 - val_mse: 13.5917 - val_mae: 1.6735 - lr: 3.6030e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 14.7217 - mse: 14.7217 - mae: 1.5633 - val_loss: 13.6891 - val_mse: 13.6891 - val_mae: 1.6675 - lr: 3.6030e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 14.6600 - mse: 14.6600 - mae: 1.5619 - val_loss: 13.8000 - val_mse: 13.8000 - val_mae: 1.5106 - lr: 3.6030e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 14.6872 - mse: 14.6872 - mae: 1.5589 - val_loss: 13.7560 - val_mse: 13.7560 - val_mae: 1.5456 - lr: 3.6030e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 3s - loss: 14.6139 - mse: 14.6139 - mae: 1.5628 - val_loss: 13.9178 - val_mse: 13.9178 - val_mae: 1.6983 - lr: 3.6030e-04 - 3s/epoch - 3ms/step\n",
            "Score for fold 2: loss of 13.91775894165039\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 3s - loss: 14.5445 - mse: 14.5445 - mae: 1.5639 - val_loss: 14.4067 - val_mse: 14.4067 - val_mae: 1.5400 - lr: 3.6030e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 14.4649 - mse: 14.4649 - mae: 1.5619 - val_loss: 14.3278 - val_mse: 14.3278 - val_mae: 1.5479 - lr: 3.6030e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 14.3285 - mse: 14.3285 - mae: 1.5601 - val_loss: 14.6060 - val_mse: 14.6060 - val_mae: 1.5798 - lr: 3.6030e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 14.2899 - mse: 14.2899 - mae: 1.5597 - val_loss: 14.8386 - val_mse: 14.8386 - val_mae: 1.5678 - lr: 3.6030e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 4s - loss: 14.2508 - mse: 14.2508 - mae: 1.5515 - val_loss: 14.6963 - val_mse: 14.6963 - val_mae: 1.6875 - lr: 3.6030e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 3s - loss: 14.1153 - mse: 14.1153 - mae: 1.5563 - val_loss: 14.8350 - val_mse: 14.8350 - val_mae: 1.5584 - lr: 3.6030e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 14.0833 - mse: 14.0833 - mae: 1.5511 - val_loss: 15.1263 - val_mse: 15.1263 - val_mae: 1.4961 - lr: 3.6030e-04 - 2s/epoch - 2ms/step\n",
            "Score for fold 3: loss of 15.126311302185059\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 3s - loss: 15.4091 - mse: 15.4091 - mae: 1.5769 - val_loss: 9.8730 - val_mse: 9.8730 - val_mae: 1.5626 - lr: 3.6030e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 3s - loss: 15.2972 - mse: 15.2972 - mae: 1.5780 - val_loss: 10.1030 - val_mse: 10.1030 - val_mae: 1.6168 - lr: 3.6030e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 3s - loss: 15.1998 - mse: 15.1998 - mae: 1.5745 - val_loss: 9.9826 - val_mse: 9.9826 - val_mae: 1.6007 - lr: 3.6030e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 15.1114 - mse: 15.1114 - mae: 1.5716 - val_loss: 10.2408 - val_mse: 10.2408 - val_mae: 1.6135 - lr: 3.6030e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 3s - loss: 15.1094 - mse: 15.1094 - mae: 1.5719 - val_loss: 10.1784 - val_mse: 10.1784 - val_mae: 1.5399 - lr: 3.6030e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 15.0017 - mse: 15.0017 - mae: 1.5641 - val_loss: 10.0635 - val_mse: 10.0635 - val_mae: 1.5702 - lr: 3.6030e-04 - 2s/epoch - 2ms/step\n",
            "Score for fold 4: loss of 10.063516616821289\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 12.9324 - mse: 12.9324 - mae: 1.5542 - val_loss: 19.2104 - val_mse: 19.2104 - val_mae: 1.9598 - lr: 3.6030e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 3s - loss: 12.8089 - mse: 12.8089 - mae: 1.5589 - val_loss: 18.7673 - val_mse: 18.7673 - val_mae: 1.6432 - lr: 3.6030e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 3s - loss: 12.6761 - mse: 12.6761 - mae: 1.5528 - val_loss: 18.8886 - val_mse: 18.8886 - val_mae: 1.5333 - lr: 3.6030e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 3s - loss: 12.5754 - mse: 12.5754 - mae: 1.5555 - val_loss: 18.9821 - val_mse: 18.9821 - val_mae: 1.5644 - lr: 3.6030e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 3s - loss: 12.5711 - mse: 12.5711 - mae: 1.5465 - val_loss: 18.9495 - val_mse: 18.9495 - val_mae: 1.5083 - lr: 3.6030e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 12.7019 - mse: 12.7019 - mae: 1.5449 - val_loss: 19.2708 - val_mse: 19.2708 - val_mae: 1.5244 - lr: 3.6030e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 12.5571 - mse: 12.5571 - mae: 1.5512 - val_loss: 19.2150 - val_mse: 19.2150 - val_mae: 1.6161 - lr: 3.6030e-04 - 2s/epoch - 2ms/step\n",
            "Score for fold 5: loss of 19.214962005615234\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-10-15 11:43:43,736]\u001b[0m Finished trial#12 resulted in value: 14.714000000000002. Current best value is 14.618 with parameters: {'activation': 'relu', 'num_hidden_layer': 4, 'i': 10, 'learning_rate': 0.0001995902281877497}.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 7s - loss: 14.8757 - mse: 14.8757 - mae: 1.6067 - val_loss: 17.6354 - val_mse: 17.6354 - val_mae: 1.5512 - lr: 3.2331e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 6s - loss: 14.5521 - mse: 14.5521 - mae: 1.5903 - val_loss: 17.7809 - val_mse: 17.7809 - val_mae: 1.5994 - lr: 3.2331e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 6s - loss: 14.4819 - mse: 14.4819 - mae: 1.5851 - val_loss: 17.6437 - val_mse: 17.6437 - val_mae: 1.5915 - lr: 3.2331e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 6s - loss: 14.4294 - mse: 14.4294 - mae: 1.5829 - val_loss: 17.6850 - val_mse: 17.6850 - val_mae: 1.5271 - lr: 3.2331e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 7s - loss: 14.3971 - mse: 14.3971 - mae: 1.5770 - val_loss: 17.2451 - val_mse: 17.2451 - val_mae: 1.5825 - lr: 3.2331e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 6s - loss: 14.3534 - mse: 14.3534 - mae: 1.5771 - val_loss: 17.4298 - val_mse: 17.4298 - val_mae: 1.5802 - lr: 3.2331e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 6s - loss: 14.2682 - mse: 14.2682 - mae: 1.5728 - val_loss: 17.5363 - val_mse: 17.5363 - val_mae: 1.5847 - lr: 3.2331e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 6s - loss: 14.2830 - mse: 14.2830 - mae: 1.5707 - val_loss: 17.3797 - val_mse: 17.3797 - val_mae: 1.5584 - lr: 3.2331e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 6s - loss: 14.2697 - mse: 14.2697 - mae: 1.5682 - val_loss: 17.1659 - val_mse: 17.1659 - val_mae: 1.6692 - lr: 3.2331e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 6s - loss: 14.1993 - mse: 14.1993 - mae: 1.5637 - val_loss: 17.4037 - val_mse: 17.4037 - val_mae: 1.6130 - lr: 3.2331e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 6s - loss: 14.2280 - mse: 14.2280 - mae: 1.5666 - val_loss: 17.3741 - val_mse: 17.3741 - val_mae: 1.6519 - lr: 3.2331e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 6s - loss: 14.1645 - mse: 14.1645 - mae: 1.5655 - val_loss: 17.4338 - val_mse: 17.4338 - val_mae: 1.6107 - lr: 3.2331e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 13/100\n",
            "1000/1000 - 6s - loss: 14.1549 - mse: 14.1549 - mae: 1.5604 - val_loss: 17.5642 - val_mse: 17.5642 - val_mae: 1.5636 - lr: 3.2331e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 14/100\n",
            "1000/1000 - 6s - loss: 14.1368 - mse: 14.1368 - mae: 1.5626 - val_loss: 17.4663 - val_mse: 17.4663 - val_mae: 1.5947 - lr: 3.2331e-04 - 6s/epoch - 6ms/step\n",
            "Score for fold 1: loss of 17.466291427612305\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 6s - loss: 14.8341 - mse: 14.8341 - mae: 1.5569 - val_loss: 14.5051 - val_mse: 14.5051 - val_mae: 1.5870 - lr: 3.2331e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 8s - loss: 14.7948 - mse: 14.7948 - mae: 1.5534 - val_loss: 14.5272 - val_mse: 14.5272 - val_mae: 1.5568 - lr: 3.2331e-04 - 8s/epoch - 8ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 6s - loss: 14.7902 - mse: 14.7902 - mae: 1.5541 - val_loss: 14.7970 - val_mse: 14.7970 - val_mae: 1.5284 - lr: 3.2331e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 6s - loss: 14.7886 - mse: 14.7886 - mae: 1.5511 - val_loss: 14.5081 - val_mse: 14.5081 - val_mae: 1.6265 - lr: 3.2331e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 6s - loss: 14.7511 - mse: 14.7511 - mae: 1.5538 - val_loss: 14.5762 - val_mse: 14.5762 - val_mae: 1.5803 - lr: 3.2331e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 6s - loss: 14.7893 - mse: 14.7893 - mae: 1.5519 - val_loss: 14.4669 - val_mse: 14.4669 - val_mae: 1.6063 - lr: 3.2331e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 6s - loss: 14.7842 - mse: 14.7842 - mae: 1.5507 - val_loss: 14.5930 - val_mse: 14.5930 - val_mae: 1.6256 - lr: 3.2331e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 6s - loss: 14.6178 - mse: 14.6178 - mae: 1.5481 - val_loss: 14.5333 - val_mse: 14.5333 - val_mae: 1.6181 - lr: 3.2331e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 6s - loss: 14.6484 - mse: 14.6484 - mae: 1.5459 - val_loss: 14.7198 - val_mse: 14.7198 - val_mae: 1.6047 - lr: 3.2331e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 6s - loss: 14.5337 - mse: 14.5337 - mae: 1.5492 - val_loss: 14.8311 - val_mse: 14.8311 - val_mae: 1.5316 - lr: 3.2331e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 6s - loss: 14.5966 - mse: 14.5966 - mae: 1.5448 - val_loss: 14.6212 - val_mse: 14.6212 - val_mae: 1.5799 - lr: 3.2331e-04 - 6s/epoch - 6ms/step\n",
            "Score for fold 2: loss of 14.62123966217041\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 6s - loss: 13.5283 - mse: 13.5283 - mae: 1.5652 - val_loss: 19.0765 - val_mse: 19.0765 - val_mae: 1.5086 - lr: 3.2331e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 6s - loss: 13.4664 - mse: 13.4664 - mae: 1.5535 - val_loss: 19.2294 - val_mse: 19.2294 - val_mae: 1.5560 - lr: 3.2331e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 6s - loss: 13.4353 - mse: 13.4353 - mae: 1.5522 - val_loss: 19.2959 - val_mse: 19.2959 - val_mae: 1.5270 - lr: 3.2331e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 6s - loss: 13.4676 - mse: 13.4676 - mae: 1.5564 - val_loss: 19.1224 - val_mse: 19.1224 - val_mae: 1.5618 - lr: 3.2331e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 6s - loss: 13.3608 - mse: 13.3608 - mae: 1.5535 - val_loss: 19.2489 - val_mse: 19.2489 - val_mae: 1.5647 - lr: 3.2331e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 6s - loss: 13.3444 - mse: 13.3444 - mae: 1.5503 - val_loss: 19.3076 - val_mse: 19.3076 - val_mae: 1.5424 - lr: 3.2331e-04 - 6s/epoch - 6ms/step\n",
            "Score for fold 3: loss of 19.307573318481445\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 6s - loss: 15.5728 - mse: 15.5728 - mae: 1.5580 - val_loss: 10.6087 - val_mse: 10.6087 - val_mae: 1.4846 - lr: 3.2331e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 6s - loss: 15.5182 - mse: 15.5182 - mae: 1.5563 - val_loss: 10.7783 - val_mse: 10.7783 - val_mae: 1.5129 - lr: 3.2331e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 6s - loss: 15.4229 - mse: 15.4229 - mae: 1.5528 - val_loss: 10.9771 - val_mse: 10.9771 - val_mae: 1.5527 - lr: 3.2331e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 6s - loss: 15.3684 - mse: 15.3684 - mae: 1.5489 - val_loss: 10.6723 - val_mse: 10.6723 - val_mae: 1.5090 - lr: 3.2331e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 6s - loss: 15.4013 - mse: 15.4013 - mae: 1.5487 - val_loss: 10.5427 - val_mse: 10.5427 - val_mae: 1.5619 - lr: 3.2331e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 6s - loss: 15.4360 - mse: 15.4360 - mae: 1.5496 - val_loss: 10.8784 - val_mse: 10.8784 - val_mae: 1.5667 - lr: 3.2331e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 6s - loss: 15.3904 - mse: 15.3904 - mae: 1.5413 - val_loss: 10.6881 - val_mse: 10.6881 - val_mae: 1.5344 - lr: 3.2331e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 7s - loss: 15.2640 - mse: 15.2640 - mae: 1.5427 - val_loss: 10.7015 - val_mse: 10.7015 - val_mae: 1.5219 - lr: 3.2331e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 6s - loss: 15.3496 - mse: 15.3496 - mae: 1.5395 - val_loss: 11.0111 - val_mse: 11.0111 - val_mae: 1.5745 - lr: 3.2331e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 6s - loss: 15.1340 - mse: 15.1340 - mae: 1.5416 - val_loss: 10.7073 - val_mse: 10.7073 - val_mae: 1.5240 - lr: 3.2331e-04 - 6s/epoch - 6ms/step\n",
            "Score for fold 4: loss of 10.707305908203125\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 6s - loss: 15.0850 - mse: 15.0850 - mae: 1.5458 - val_loss: 11.5213 - val_mse: 11.5213 - val_mae: 1.5190 - lr: 3.2331e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 6s - loss: 15.0365 - mse: 15.0365 - mae: 1.5406 - val_loss: 11.3586 - val_mse: 11.3586 - val_mae: 1.5341 - lr: 3.2331e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 6s - loss: 14.9703 - mse: 14.9703 - mae: 1.5379 - val_loss: 11.5493 - val_mse: 11.5493 - val_mae: 1.5152 - lr: 3.2331e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 6s - loss: 14.9028 - mse: 14.9028 - mae: 1.5327 - val_loss: 11.3856 - val_mse: 11.3856 - val_mae: 1.5315 - lr: 3.2331e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 6s - loss: 14.9359 - mse: 14.9359 - mae: 1.5342 - val_loss: 11.6654 - val_mse: 11.6654 - val_mae: 1.5225 - lr: 3.2331e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 6s - loss: 14.8706 - mse: 14.8706 - mae: 1.5325 - val_loss: 11.4706 - val_mse: 11.4706 - val_mae: 1.5565 - lr: 3.2331e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 6s - loss: 14.8014 - mse: 14.8014 - mae: 1.5235 - val_loss: 11.5913 - val_mse: 11.5913 - val_mae: 1.4966 - lr: 3.2331e-04 - 6s/epoch - 6ms/step\n",
            "Score for fold 5: loss of 11.591313362121582\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-10-15 11:49:30,059]\u001b[0m Finished trial#13 resulted in value: 14.739999999999998. Current best value is 14.618 with parameters: {'activation': 'relu', 'num_hidden_layer': 4, 'i': 10, 'learning_rate': 0.0001995902281877497}.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 11s - loss: 16.4025 - mse: 16.4025 - mae: 1.6190 - val_loss: 11.6410 - val_mse: 11.6410 - val_mae: 1.5835 - lr: 2.8510e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 10s - loss: 15.9247 - mse: 15.9247 - mae: 1.6033 - val_loss: 11.6380 - val_mse: 11.6380 - val_mae: 1.5020 - lr: 2.8510e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 10s - loss: 15.8977 - mse: 15.8977 - mae: 1.6011 - val_loss: 11.5779 - val_mse: 11.5779 - val_mae: 1.6184 - lr: 2.8510e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 10s - loss: 15.8315 - mse: 15.8315 - mae: 1.5969 - val_loss: 11.5825 - val_mse: 11.5825 - val_mae: 1.5519 - lr: 2.8510e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 10s - loss: 15.7597 - mse: 15.7597 - mae: 1.5974 - val_loss: 11.4404 - val_mse: 11.4404 - val_mae: 1.5724 - lr: 2.8510e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 12s - loss: 15.7711 - mse: 15.7711 - mae: 1.5894 - val_loss: 11.4180 - val_mse: 11.4180 - val_mae: 1.6009 - lr: 2.8510e-04 - 12s/epoch - 12ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 10s - loss: 15.6195 - mse: 15.6195 - mae: 1.5871 - val_loss: 11.5894 - val_mse: 11.5894 - val_mae: 1.4879 - lr: 2.8510e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 10s - loss: 15.7240 - mse: 15.7240 - mae: 1.5819 - val_loss: 11.5111 - val_mse: 11.5111 - val_mae: 1.5180 - lr: 2.8510e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 10s - loss: 15.6595 - mse: 15.6595 - mae: 1.5825 - val_loss: 11.4783 - val_mse: 11.4783 - val_mae: 1.6098 - lr: 2.8510e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 10s - loss: 15.6419 - mse: 15.6419 - mae: 1.5866 - val_loss: 11.4608 - val_mse: 11.4608 - val_mae: 1.6018 - lr: 2.8510e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 10s - loss: 15.4947 - mse: 15.4947 - mae: 1.5833 - val_loss: 11.5303 - val_mse: 11.5303 - val_mae: 1.6501 - lr: 2.8510e-04 - 10s/epoch - 10ms/step\n",
            "Score for fold 1: loss of 11.53031063079834\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 9s - loss: 14.8787 - mse: 14.8787 - mae: 1.5816 - val_loss: 14.2137 - val_mse: 14.2137 - val_mae: 1.5801 - lr: 2.8510e-04 - 9s/epoch - 9ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 10s - loss: 14.8136 - mse: 14.8136 - mae: 1.5761 - val_loss: 14.1739 - val_mse: 14.1739 - val_mae: 1.5758 - lr: 2.8510e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 10s - loss: 14.6152 - mse: 14.6152 - mae: 1.5720 - val_loss: 14.4396 - val_mse: 14.4396 - val_mae: 1.5667 - lr: 2.8510e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 10s - loss: 14.5422 - mse: 14.5422 - mae: 1.5659 - val_loss: 14.9622 - val_mse: 14.9622 - val_mae: 1.7857 - lr: 2.8510e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 10s - loss: 14.4993 - mse: 14.4993 - mae: 1.5759 - val_loss: 14.3818 - val_mse: 14.3818 - val_mae: 1.5728 - lr: 2.8510e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 10s - loss: 14.5195 - mse: 14.5195 - mae: 1.5787 - val_loss: 14.7564 - val_mse: 14.7564 - val_mae: 1.5240 - lr: 2.8510e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 10s - loss: 14.4083 - mse: 14.4083 - mae: 1.5775 - val_loss: 14.8203 - val_mse: 14.8203 - val_mae: 1.7912 - lr: 2.8510e-04 - 10s/epoch - 10ms/step\n",
            "Score for fold 2: loss of 14.820274353027344\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 11s - loss: 15.0977 - mse: 15.0977 - mae: 1.5854 - val_loss: 12.3039 - val_mse: 12.3039 - val_mae: 1.4330 - lr: 2.8510e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 10s - loss: 15.0184 - mse: 15.0184 - mae: 1.5862 - val_loss: 12.2715 - val_mse: 12.2715 - val_mae: 1.6030 - lr: 2.8510e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 10s - loss: 14.8682 - mse: 14.8682 - mae: 1.5796 - val_loss: 12.5438 - val_mse: 12.5438 - val_mae: 1.9275 - lr: 2.8510e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 9s - loss: 14.8180 - mse: 14.8180 - mae: 1.5999 - val_loss: 12.4702 - val_mse: 12.4702 - val_mae: 1.7198 - lr: 2.8510e-04 - 9s/epoch - 9ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 10s - loss: 14.7950 - mse: 14.7950 - mae: 1.5992 - val_loss: 13.1882 - val_mse: 13.1882 - val_mae: 1.5455 - lr: 2.8510e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 9s - loss: 14.7238 - mse: 14.7238 - mae: 1.5952 - val_loss: 12.8559 - val_mse: 12.8559 - val_mae: 1.5015 - lr: 2.8510e-04 - 9s/epoch - 9ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 9s - loss: 14.6305 - mse: 14.6305 - mae: 1.5945 - val_loss: 12.5554 - val_mse: 12.5554 - val_mae: 1.5082 - lr: 2.8510e-04 - 9s/epoch - 9ms/step\n",
            "Score for fold 3: loss of 12.555424690246582\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 10s - loss: 14.3239 - mse: 14.3239 - mae: 1.6122 - val_loss: 13.8092 - val_mse: 13.8092 - val_mae: 1.6157 - lr: 2.8510e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 10s - loss: 14.2742 - mse: 14.2742 - mae: 1.6098 - val_loss: 14.5389 - val_mse: 14.5389 - val_mae: 1.5311 - lr: 2.8510e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 9s - loss: 14.1757 - mse: 14.1757 - mae: 1.6033 - val_loss: 14.1149 - val_mse: 14.1149 - val_mae: 1.5422 - lr: 2.8510e-04 - 9s/epoch - 9ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 10s - loss: 14.2045 - mse: 14.2045 - mae: 1.6268 - val_loss: 14.7986 - val_mse: 14.7986 - val_mae: 1.4687 - lr: 2.8510e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 10s - loss: 14.0067 - mse: 14.0067 - mae: 1.6277 - val_loss: 14.4769 - val_mse: 14.4769 - val_mae: 1.7744 - lr: 2.8510e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 10s - loss: 14.0061 - mse: 14.0061 - mae: 1.6111 - val_loss: 16.2099 - val_mse: 16.2099 - val_mae: 2.3641 - lr: 2.8510e-04 - 10s/epoch - 10ms/step\n",
            "Score for fold 4: loss of 16.209932327270508\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 10s - loss: 12.8235 - mse: 12.8235 - mae: 1.6500 - val_loss: 19.2511 - val_mse: 19.2511 - val_mae: 1.4884 - lr: 2.8510e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 9s - loss: 12.7449 - mse: 12.7449 - mae: 1.6412 - val_loss: 19.4084 - val_mse: 19.4084 - val_mae: 1.4835 - lr: 2.8510e-04 - 9s/epoch - 9ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 10s - loss: 12.6646 - mse: 12.6646 - mae: 1.6407 - val_loss: 19.5784 - val_mse: 19.5784 - val_mae: 1.8931 - lr: 2.8510e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 10s - loss: 12.5131 - mse: 12.5131 - mae: 1.6395 - val_loss: 19.2842 - val_mse: 19.2842 - val_mae: 1.5153 - lr: 2.8510e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 10s - loss: 12.5619 - mse: 12.5619 - mae: 1.6292 - val_loss: 19.7030 - val_mse: 19.7030 - val_mae: 1.5157 - lr: 2.8510e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 10s - loss: 12.5645 - mse: 12.5645 - mae: 1.6377 - val_loss: 20.0511 - val_mse: 20.0511 - val_mae: 1.4968 - lr: 2.8510e-04 - 10s/epoch - 10ms/step\n",
            "Score for fold 5: loss of 20.051074981689453\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-10-15 11:56:56,724]\u001b[0m Finished trial#14 resulted in value: 15.034. Current best value is 14.618 with parameters: {'activation': 'relu', 'num_hidden_layer': 4, 'i': 10, 'learning_rate': 0.0001995902281877497}.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 3s - loss: 17.3819 - mse: 17.3819 - mae: 1.6758 - val_loss: 13.1790 - val_mse: 13.1790 - val_mae: 1.5659 - lr: 1.0740e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 16.1574 - mse: 16.1574 - mae: 1.6050 - val_loss: 12.8236 - val_mse: 12.8236 - val_mae: 1.5684 - lr: 1.0740e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 15.9241 - mse: 15.9241 - mae: 1.6006 - val_loss: 12.6860 - val_mse: 12.6860 - val_mae: 1.5698 - lr: 1.0740e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 15.7954 - mse: 15.7954 - mae: 1.6054 - val_loss: 12.6432 - val_mse: 12.6432 - val_mae: 1.5351 - lr: 1.0740e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 15.7680 - mse: 15.7680 - mae: 1.5976 - val_loss: 12.5966 - val_mse: 12.5966 - val_mae: 1.5401 - lr: 1.0740e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 15.7168 - mse: 15.7168 - mae: 1.5934 - val_loss: 12.5794 - val_mse: 12.5794 - val_mae: 1.5482 - lr: 1.0740e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 15.6898 - mse: 15.6898 - mae: 1.6003 - val_loss: 12.5599 - val_mse: 12.5599 - val_mae: 1.5531 - lr: 1.0740e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 2s - loss: 15.6507 - mse: 15.6507 - mae: 1.5969 - val_loss: 12.5619 - val_mse: 12.5619 - val_mae: 1.5513 - lr: 1.0740e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 2s - loss: 15.6375 - mse: 15.6375 - mae: 1.5962 - val_loss: 12.5242 - val_mse: 12.5242 - val_mae: 1.5399 - lr: 1.0740e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 2s - loss: 15.6340 - mse: 15.6340 - mae: 1.5901 - val_loss: 12.5287 - val_mse: 12.5287 - val_mae: 1.5450 - lr: 1.0740e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 2s - loss: 15.6047 - mse: 15.6047 - mae: 1.5917 - val_loss: 12.5186 - val_mse: 12.5186 - val_mae: 1.5285 - lr: 1.0740e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 2s - loss: 15.5913 - mse: 15.5913 - mae: 1.5942 - val_loss: 12.4896 - val_mse: 12.4896 - val_mae: 1.5521 - lr: 1.0740e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 13/100\n",
            "1000/1000 - 2s - loss: 15.5859 - mse: 15.5859 - mae: 1.5931 - val_loss: 12.4980 - val_mse: 12.4980 - val_mae: 1.5348 - lr: 1.0740e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 14/100\n",
            "1000/1000 - 2s - loss: 15.5767 - mse: 15.5767 - mae: 1.5876 - val_loss: 12.4883 - val_mse: 12.4883 - val_mae: 1.5659 - lr: 1.0740e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 15/100\n",
            "1000/1000 - 2s - loss: 15.5689 - mse: 15.5689 - mae: 1.5889 - val_loss: 12.4891 - val_mse: 12.4891 - val_mae: 1.5387 - lr: 1.0740e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 16/100\n",
            "1000/1000 - 2s - loss: 15.5365 - mse: 15.5365 - mae: 1.5902 - val_loss: 12.4725 - val_mse: 12.4725 - val_mae: 1.5588 - lr: 1.0740e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 17/100\n",
            "1000/1000 - 2s - loss: 15.5261 - mse: 15.5261 - mae: 1.5885 - val_loss: 12.4826 - val_mse: 12.4826 - val_mae: 1.5576 - lr: 1.0740e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 18/100\n",
            "1000/1000 - 2s - loss: 15.5257 - mse: 15.5257 - mae: 1.5880 - val_loss: 12.4735 - val_mse: 12.4735 - val_mae: 1.5350 - lr: 1.0740e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 19/100\n",
            "1000/1000 - 2s - loss: 15.5063 - mse: 15.5063 - mae: 1.5889 - val_loss: 12.4739 - val_mse: 12.4739 - val_mae: 1.5306 - lr: 1.0740e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 20/100\n",
            "1000/1000 - 2s - loss: 15.5067 - mse: 15.5067 - mae: 1.5850 - val_loss: 12.4210 - val_mse: 12.4210 - val_mae: 1.5598 - lr: 1.0740e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 21/100\n",
            "1000/1000 - 2s - loss: 15.4876 - mse: 15.4876 - mae: 1.5848 - val_loss: 12.4220 - val_mse: 12.4220 - val_mae: 1.5563 - lr: 1.0740e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 22/100\n",
            "1000/1000 - 2s - loss: 15.4832 - mse: 15.4832 - mae: 1.5830 - val_loss: 12.4253 - val_mse: 12.4253 - val_mae: 1.5364 - lr: 1.0740e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 23/100\n",
            "1000/1000 - 2s - loss: 15.4702 - mse: 15.4702 - mae: 1.5848 - val_loss: 12.4069 - val_mse: 12.4069 - val_mae: 1.5548 - lr: 1.0740e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 24/100\n",
            "1000/1000 - 3s - loss: 15.4797 - mse: 15.4797 - mae: 1.5805 - val_loss: 12.4625 - val_mse: 12.4625 - val_mae: 1.5000 - lr: 1.0740e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 25/100\n",
            "1000/1000 - 3s - loss: 15.4867 - mse: 15.4867 - mae: 1.5791 - val_loss: 12.4134 - val_mse: 12.4134 - val_mae: 1.5351 - lr: 1.0740e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 26/100\n",
            "1000/1000 - 2s - loss: 15.4660 - mse: 15.4660 - mae: 1.5819 - val_loss: 12.4014 - val_mse: 12.4014 - val_mae: 1.5627 - lr: 1.0740e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 27/100\n",
            "1000/1000 - 2s - loss: 15.4343 - mse: 15.4343 - mae: 1.5810 - val_loss: 12.4966 - val_mse: 12.4966 - val_mae: 1.5199 - lr: 1.0740e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 28/100\n",
            "1000/1000 - 2s - loss: 15.4749 - mse: 15.4749 - mae: 1.5808 - val_loss: 12.4176 - val_mse: 12.4176 - val_mae: 1.5438 - lr: 1.0740e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 29/100\n",
            "1000/1000 - 2s - loss: 15.4463 - mse: 15.4463 - mae: 1.5816 - val_loss: 12.4168 - val_mse: 12.4168 - val_mae: 1.5464 - lr: 1.0740e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 30/100\n",
            "1000/1000 - 2s - loss: 15.4376 - mse: 15.4376 - mae: 1.5783 - val_loss: 12.4022 - val_mse: 12.4022 - val_mae: 1.5472 - lr: 1.0740e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 31/100\n",
            "1000/1000 - 2s - loss: 15.4369 - mse: 15.4369 - mae: 1.5759 - val_loss: 12.4261 - val_mse: 12.4261 - val_mae: 1.5246 - lr: 1.0740e-04 - 2s/epoch - 2ms/step\n",
            "Score for fold 1: loss of 12.426092147827148\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 15.5149 - mse: 15.5149 - mae: 1.5717 - val_loss: 12.0948 - val_mse: 12.0948 - val_mae: 1.5583 - lr: 1.0740e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 15.4912 - mse: 15.4912 - mae: 1.5725 - val_loss: 12.1228 - val_mse: 12.1228 - val_mae: 1.5532 - lr: 1.0740e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 15.4834 - mse: 15.4834 - mae: 1.5699 - val_loss: 12.1784 - val_mse: 12.1784 - val_mae: 1.5605 - lr: 1.0740e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 15.4856 - mse: 15.4856 - mae: 1.5706 - val_loss: 12.1098 - val_mse: 12.1098 - val_mae: 1.5760 - lr: 1.0740e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 15.4454 - mse: 15.4454 - mae: 1.5680 - val_loss: 12.1041 - val_mse: 12.1041 - val_mae: 1.5487 - lr: 1.0740e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 15.4460 - mse: 15.4460 - mae: 1.5691 - val_loss: 12.1505 - val_mse: 12.1505 - val_mae: 1.6196 - lr: 1.0740e-04 - 2s/epoch - 2ms/step\n",
            "Score for fold 2: loss of 12.150535583496094\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 13.4464 - mse: 13.4464 - mae: 1.5544 - val_loss: 20.2077 - val_mse: 20.2077 - val_mae: 1.5769 - lr: 1.0740e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 13.4372 - mse: 13.4372 - mae: 1.5516 - val_loss: 20.1658 - val_mse: 20.1658 - val_mae: 1.5842 - lr: 1.0740e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 13.4233 - mse: 13.4233 - mae: 1.5530 - val_loss: 20.1976 - val_mse: 20.1976 - val_mae: 1.5972 - lr: 1.0740e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 13.4500 - mse: 13.4500 - mae: 1.5511 - val_loss: 20.1322 - val_mse: 20.1322 - val_mae: 1.6162 - lr: 1.0740e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 13.4205 - mse: 13.4205 - mae: 1.5489 - val_loss: 20.1680 - val_mse: 20.1680 - val_mae: 1.6021 - lr: 1.0740e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 13.3837 - mse: 13.3837 - mae: 1.5486 - val_loss: 20.1783 - val_mse: 20.1783 - val_mae: 1.5905 - lr: 1.0740e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 13.3922 - mse: 13.3922 - mae: 1.5505 - val_loss: 20.1846 - val_mse: 20.1846 - val_mae: 1.6018 - lr: 1.0740e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 2s - loss: 13.4007 - mse: 13.4007 - mae: 1.5510 - val_loss: 20.1740 - val_mse: 20.1740 - val_mae: 1.5950 - lr: 1.0740e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 2s - loss: 13.3921 - mse: 13.3921 - mae: 1.5485 - val_loss: 20.1559 - val_mse: 20.1559 - val_mae: 1.6180 - lr: 1.0740e-04 - 2s/epoch - 2ms/step\n",
            "Score for fold 3: loss of 20.155887603759766\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 14.5375 - mse: 14.5375 - mae: 1.5526 - val_loss: 15.7582 - val_mse: 15.7582 - val_mae: 1.5666 - lr: 1.0740e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 14.5396 - mse: 14.5396 - mae: 1.5528 - val_loss: 15.7087 - val_mse: 15.7087 - val_mae: 1.5676 - lr: 1.0740e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 14.5413 - mse: 14.5413 - mae: 1.5499 - val_loss: 15.6382 - val_mse: 15.6382 - val_mae: 1.5927 - lr: 1.0740e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 14.5243 - mse: 14.5243 - mae: 1.5490 - val_loss: 15.6990 - val_mse: 15.6990 - val_mae: 1.5645 - lr: 1.0740e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 14.4970 - mse: 14.4970 - mae: 1.5524 - val_loss: 15.8916 - val_mse: 15.8916 - val_mae: 1.5811 - lr: 1.0740e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 14.5425 - mse: 14.5425 - mae: 1.5505 - val_loss: 15.7205 - val_mse: 15.7205 - val_mae: 1.6081 - lr: 1.0740e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 14.5511 - mse: 14.5511 - mae: 1.5517 - val_loss: 15.6682 - val_mse: 15.6682 - val_mae: 1.5926 - lr: 1.0740e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 2s - loss: 14.5009 - mse: 14.5009 - mae: 1.5505 - val_loss: 15.9701 - val_mse: 15.9701 - val_mae: 1.6184 - lr: 1.0740e-04 - 2s/epoch - 2ms/step\n",
            "Score for fold 4: loss of 15.970105171203613\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 15.1072 - mse: 15.1072 - mae: 1.5713 - val_loss: 13.5155 - val_mse: 13.5155 - val_mae: 1.5683 - lr: 1.0740e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 15.0578 - mse: 15.0578 - mae: 1.5713 - val_loss: 13.5685 - val_mse: 13.5685 - val_mae: 1.5337 - lr: 1.0740e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 15.0577 - mse: 15.0577 - mae: 1.5693 - val_loss: 13.5576 - val_mse: 13.5576 - val_mae: 1.5420 - lr: 1.0740e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 15.0508 - mse: 15.0508 - mae: 1.5708 - val_loss: 13.6138 - val_mse: 13.6138 - val_mae: 1.5308 - lr: 1.0740e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 15.0684 - mse: 15.0684 - mae: 1.5688 - val_loss: 13.6149 - val_mse: 13.6149 - val_mae: 1.5126 - lr: 1.0740e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 15.0349 - mse: 15.0349 - mae: 1.5688 - val_loss: 13.5478 - val_mse: 13.5478 - val_mae: 1.5555 - lr: 1.0740e-04 - 2s/epoch - 2ms/step\n",
            "Score for fold 5: loss of 13.547826766967773\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-10-15 11:59:28,019]\u001b[0m Finished trial#15 resulted in value: 14.851999999999999. Current best value is 14.618 with parameters: {'activation': 'relu', 'num_hidden_layer': 4, 'i': 10, 'learning_rate': 0.0001995902281877497}.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 3s - loss: 16.3634 - mse: 16.3634 - mae: 1.6672 - val_loss: 14.8082 - val_mse: 14.8082 - val_mae: 1.6310 - lr: 5.1161e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 15.6767 - mse: 15.6767 - mae: 1.6125 - val_loss: 14.6149 - val_mse: 14.6149 - val_mae: 1.6815 - lr: 5.1161e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 15.5566 - mse: 15.5566 - mae: 1.6029 - val_loss: 14.4004 - val_mse: 14.4004 - val_mae: 1.5955 - lr: 5.1161e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 15.4799 - mse: 15.4799 - mae: 1.5977 - val_loss: 14.4333 - val_mse: 14.4333 - val_mae: 1.5825 - lr: 5.1161e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 15.4095 - mse: 15.4095 - mae: 1.5949 - val_loss: 14.5225 - val_mse: 14.5225 - val_mae: 1.5882 - lr: 5.1161e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 15.3854 - mse: 15.3854 - mae: 1.5927 - val_loss: 14.3650 - val_mse: 14.3650 - val_mae: 1.6149 - lr: 5.1161e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 15.3440 - mse: 15.3440 - mae: 1.5961 - val_loss: 14.4536 - val_mse: 14.4536 - val_mae: 1.5604 - lr: 5.1161e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 2s - loss: 15.3145 - mse: 15.3145 - mae: 1.5890 - val_loss: 14.3875 - val_mse: 14.3875 - val_mae: 1.5692 - lr: 5.1161e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 2s - loss: 15.2872 - mse: 15.2872 - mae: 1.5875 - val_loss: 14.3237 - val_mse: 14.3237 - val_mae: 1.5982 - lr: 5.1161e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 2s - loss: 15.2600 - mse: 15.2600 - mae: 1.5875 - val_loss: 14.3250 - val_mse: 14.3250 - val_mae: 1.6286 - lr: 5.1161e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 2s - loss: 15.2373 - mse: 15.2373 - mae: 1.5899 - val_loss: 14.3737 - val_mse: 14.3737 - val_mae: 1.5895 - lr: 5.1161e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 2s - loss: 15.2158 - mse: 15.2158 - mae: 1.5874 - val_loss: 14.2734 - val_mse: 14.2734 - val_mae: 1.5854 - lr: 5.1161e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 13/100\n",
            "1000/1000 - 2s - loss: 15.1970 - mse: 15.1970 - mae: 1.5914 - val_loss: 14.3384 - val_mse: 14.3384 - val_mae: 1.5578 - lr: 5.1161e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 14/100\n",
            "1000/1000 - 2s - loss: 15.1656 - mse: 15.1656 - mae: 1.5835 - val_loss: 14.2526 - val_mse: 14.2526 - val_mae: 1.6279 - lr: 5.1161e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 15/100\n",
            "1000/1000 - 2s - loss: 15.1766 - mse: 15.1766 - mae: 1.5877 - val_loss: 14.4597 - val_mse: 14.4597 - val_mae: 1.5579 - lr: 5.1161e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 16/100\n",
            "1000/1000 - 2s - loss: 15.1548 - mse: 15.1548 - mae: 1.5885 - val_loss: 14.3438 - val_mse: 14.3438 - val_mae: 1.6154 - lr: 5.1161e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 17/100\n",
            "1000/1000 - 2s - loss: 15.1477 - mse: 15.1477 - mae: 1.5856 - val_loss: 14.3050 - val_mse: 14.3050 - val_mae: 1.5838 - lr: 5.1161e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 18/100\n",
            "1000/1000 - 2s - loss: 15.1382 - mse: 15.1382 - mae: 1.5848 - val_loss: 14.3244 - val_mse: 14.3244 - val_mae: 1.6207 - lr: 5.1161e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 19/100\n",
            "1000/1000 - 2s - loss: 15.1223 - mse: 15.1223 - mae: 1.5855 - val_loss: 14.4411 - val_mse: 14.4411 - val_mae: 1.5977 - lr: 5.1161e-04 - 2s/epoch - 2ms/step\n",
            "Score for fold 1: loss of 14.4411039352417\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 3s - loss: 15.9596 - mse: 15.9596 - mae: 1.5855 - val_loss: 10.9427 - val_mse: 10.9427 - val_mae: 1.6056 - lr: 5.1161e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 15.9288 - mse: 15.9288 - mae: 1.5842 - val_loss: 11.0395 - val_mse: 11.0395 - val_mae: 1.5570 - lr: 5.1161e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 15.9019 - mse: 15.9019 - mae: 1.5887 - val_loss: 11.1284 - val_mse: 11.1284 - val_mae: 1.5325 - lr: 5.1161e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 15.8956 - mse: 15.8956 - mae: 1.5842 - val_loss: 10.9392 - val_mse: 10.9392 - val_mae: 1.5563 - lr: 5.1161e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 15.8704 - mse: 15.8704 - mae: 1.5815 - val_loss: 10.8818 - val_mse: 10.8818 - val_mae: 1.6046 - lr: 5.1161e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 15.8938 - mse: 15.8938 - mae: 1.5876 - val_loss: 10.9798 - val_mse: 10.9798 - val_mae: 1.6175 - lr: 5.1161e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 15.8616 - mse: 15.8616 - mae: 1.5823 - val_loss: 10.9423 - val_mse: 10.9423 - val_mae: 1.6249 - lr: 5.1161e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 2s - loss: 15.8471 - mse: 15.8471 - mae: 1.5840 - val_loss: 11.1389 - val_mse: 11.1389 - val_mae: 1.7036 - lr: 5.1161e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 2s - loss: 15.8443 - mse: 15.8443 - mae: 1.5838 - val_loss: 10.9283 - val_mse: 10.9283 - val_mae: 1.6249 - lr: 5.1161e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 2s - loss: 15.8239 - mse: 15.8239 - mae: 1.5823 - val_loss: 11.0317 - val_mse: 11.0317 - val_mae: 1.5865 - lr: 5.1161e-04 - 2s/epoch - 2ms/step\n",
            "Score for fold 2: loss of 11.031744003295898\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 3s - loss: 13.6053 - mse: 13.6053 - mae: 1.5875 - val_loss: 20.0548 - val_mse: 20.0548 - val_mae: 1.6595 - lr: 5.1161e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 13.6145 - mse: 13.6145 - mae: 1.5842 - val_loss: 19.7039 - val_mse: 19.7039 - val_mae: 1.6041 - lr: 5.1161e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 13.5699 - mse: 13.5699 - mae: 1.5858 - val_loss: 19.9167 - val_mse: 19.9167 - val_mae: 1.5311 - lr: 5.1161e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 13.5756 - mse: 13.5756 - mae: 1.5849 - val_loss: 19.8746 - val_mse: 19.8746 - val_mae: 1.5591 - lr: 5.1161e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 13.5256 - mse: 13.5256 - mae: 1.5825 - val_loss: 19.9272 - val_mse: 19.9272 - val_mae: 1.6106 - lr: 5.1161e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 13.5456 - mse: 13.5456 - mae: 1.5812 - val_loss: 19.8009 - val_mse: 19.8009 - val_mae: 1.5693 - lr: 5.1161e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 3s - loss: 13.5357 - mse: 13.5357 - mae: 1.5827 - val_loss: 19.9941 - val_mse: 19.9941 - val_mae: 1.5608 - lr: 5.1161e-04 - 3s/epoch - 3ms/step\n",
            "Score for fold 3: loss of 19.994060516357422\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 3s - loss: 15.1051 - mse: 15.1051 - mae: 1.5806 - val_loss: 13.4570 - val_mse: 13.4570 - val_mae: 1.5480 - lr: 5.1161e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 15.0598 - mse: 15.0598 - mae: 1.5745 - val_loss: 13.7916 - val_mse: 13.7916 - val_mae: 1.5780 - lr: 5.1161e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 3s - loss: 15.0477 - mse: 15.0477 - mae: 1.5718 - val_loss: 13.5424 - val_mse: 13.5424 - val_mae: 1.5774 - lr: 5.1161e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 15.0853 - mse: 15.0853 - mae: 1.5758 - val_loss: 13.8131 - val_mse: 13.8131 - val_mae: 1.5208 - lr: 5.1161e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 3s - loss: 15.0673 - mse: 15.0673 - mae: 1.5723 - val_loss: 13.5183 - val_mse: 13.5183 - val_mae: 1.5893 - lr: 5.1161e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 3s - loss: 15.0368 - mse: 15.0368 - mae: 1.5762 - val_loss: 13.5288 - val_mse: 13.5288 - val_mae: 1.6169 - lr: 5.1161e-04 - 3s/epoch - 3ms/step\n",
            "Score for fold 4: loss of 13.528814315795898\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 3s - loss: 14.5539 - mse: 14.5539 - mae: 1.5651 - val_loss: 15.5148 - val_mse: 15.5148 - val_mae: 1.5934 - lr: 5.1161e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 14.5231 - mse: 14.5231 - mae: 1.5666 - val_loss: 15.6762 - val_mse: 15.6762 - val_mae: 1.5523 - lr: 5.1161e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 14.5255 - mse: 14.5255 - mae: 1.5599 - val_loss: 15.6779 - val_mse: 15.6779 - val_mae: 1.6168 - lr: 5.1161e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 14.5285 - mse: 14.5285 - mae: 1.5615 - val_loss: 15.6809 - val_mse: 15.6809 - val_mae: 1.6027 - lr: 5.1161e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 14.4768 - mse: 14.4768 - mae: 1.5613 - val_loss: 15.7706 - val_mse: 15.7706 - val_mae: 1.6209 - lr: 5.1161e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 3s - loss: 14.4207 - mse: 14.4207 - mae: 1.5594 - val_loss: 15.7933 - val_mse: 15.7933 - val_mae: 1.6062 - lr: 5.1161e-04 - 3s/epoch - 3ms/step\n",
            "Score for fold 5: loss of 15.793277740478516\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-10-15 12:02:39,023]\u001b[0m Finished trial#16 resulted in value: 14.956. Current best value is 14.618 with parameters: {'activation': 'relu', 'num_hidden_layer': 4, 'i': 10, 'learning_rate': 0.0001995902281877497}.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 42s - loss: 13.9606 - mse: 13.9606 - mae: 1.6156 - val_loss: 21.5658 - val_mse: 21.5658 - val_mae: 1.5850 - lr: 1.8909e-04 - 42s/epoch - 42ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 39s - loss: 13.5992 - mse: 13.5992 - mae: 1.6010 - val_loss: 21.2919 - val_mse: 21.2919 - val_mae: 1.5967 - lr: 1.8909e-04 - 39s/epoch - 39ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 40s - loss: 13.4026 - mse: 13.4026 - mae: 1.6020 - val_loss: 21.4767 - val_mse: 21.4767 - val_mae: 1.5459 - lr: 1.8909e-04 - 40s/epoch - 40ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 41s - loss: 13.4023 - mse: 13.4023 - mae: 1.5965 - val_loss: 21.2846 - val_mse: 21.2846 - val_mae: 1.5779 - lr: 1.8909e-04 - 41s/epoch - 41ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 40s - loss: 13.3886 - mse: 13.3886 - mae: 1.5914 - val_loss: 21.2352 - val_mse: 21.2352 - val_mae: 1.5754 - lr: 1.8909e-04 - 40s/epoch - 40ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 39s - loss: 13.2409 - mse: 13.2409 - mae: 1.5852 - val_loss: 21.3512 - val_mse: 21.3512 - val_mae: 1.6780 - lr: 1.8909e-04 - 39s/epoch - 39ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 40s - loss: 13.2669 - mse: 13.2669 - mae: 1.5845 - val_loss: 21.4171 - val_mse: 21.4171 - val_mae: 1.5581 - lr: 1.8909e-04 - 40s/epoch - 40ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 41s - loss: 13.1851 - mse: 13.1851 - mae: 1.5802 - val_loss: 21.3901 - val_mse: 21.3901 - val_mae: 1.5196 - lr: 1.8909e-04 - 41s/epoch - 41ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 39s - loss: 13.1402 - mse: 13.1402 - mae: 1.5803 - val_loss: 21.4585 - val_mse: 21.4585 - val_mae: 1.6647 - lr: 1.8909e-04 - 39s/epoch - 39ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 39s - loss: 13.0900 - mse: 13.0900 - mae: 1.5770 - val_loss: 21.4734 - val_mse: 21.4734 - val_mae: 1.5474 - lr: 1.8909e-04 - 39s/epoch - 39ms/step\n",
            "Score for fold 1: loss of 21.473453521728516\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 41s - loss: 14.4759 - mse: 14.4759 - mae: 1.5731 - val_loss: 15.7950 - val_mse: 15.7950 - val_mae: 1.5785 - lr: 1.8909e-04 - 41s/epoch - 41ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 39s - loss: 14.3475 - mse: 14.3475 - mae: 1.5708 - val_loss: 16.0793 - val_mse: 16.0793 - val_mae: 1.5133 - lr: 1.8909e-04 - 39s/epoch - 39ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 38s - loss: 14.3268 - mse: 14.3268 - mae: 1.5642 - val_loss: 15.5615 - val_mse: 15.5615 - val_mae: 1.6452 - lr: 1.8909e-04 - 38s/epoch - 38ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 39s - loss: 14.2022 - mse: 14.2022 - mae: 1.5635 - val_loss: 16.0483 - val_mse: 16.0483 - val_mae: 1.5897 - lr: 1.8909e-04 - 39s/epoch - 39ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 41s - loss: 14.1748 - mse: 14.1748 - mae: 1.5570 - val_loss: 15.8950 - val_mse: 15.8950 - val_mae: 1.5907 - lr: 1.8909e-04 - 41s/epoch - 41ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 38s - loss: 14.0049 - mse: 14.0049 - mae: 1.5553 - val_loss: 16.2935 - val_mse: 16.2935 - val_mae: 1.5693 - lr: 1.8909e-04 - 38s/epoch - 38ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 39s - loss: 13.9347 - mse: 13.9347 - mae: 1.5533 - val_loss: 16.1783 - val_mse: 16.1783 - val_mae: 1.6249 - lr: 1.8909e-04 - 39s/epoch - 39ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 39s - loss: 13.9101 - mse: 13.9101 - mae: 1.5581 - val_loss: 16.1141 - val_mse: 16.1141 - val_mae: 1.5845 - lr: 1.8909e-04 - 39s/epoch - 39ms/step\n",
            "Score for fold 2: loss of 16.114078521728516\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 39s - loss: 15.1194 - mse: 15.1194 - mae: 1.5723 - val_loss: 11.3581 - val_mse: 11.3581 - val_mae: 1.4523 - lr: 1.8909e-04 - 39s/epoch - 39ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 39s - loss: 14.8760 - mse: 14.8760 - mae: 1.5672 - val_loss: 11.3749 - val_mse: 11.3749 - val_mae: 1.7599 - lr: 1.8909e-04 - 39s/epoch - 39ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 39s - loss: 14.8790 - mse: 14.8790 - mae: 1.5652 - val_loss: 11.8965 - val_mse: 11.8965 - val_mae: 1.6571 - lr: 1.8909e-04 - 39s/epoch - 39ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 41s - loss: 14.6781 - mse: 14.6781 - mae: 1.5588 - val_loss: 11.9651 - val_mse: 11.9651 - val_mae: 1.8623 - lr: 1.8909e-04 - 41s/epoch - 41ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 39s - loss: 14.5695 - mse: 14.5695 - mae: 1.5605 - val_loss: 11.7743 - val_mse: 11.7743 - val_mae: 1.4898 - lr: 1.8909e-04 - 39s/epoch - 39ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 39s - loss: 14.5652 - mse: 14.5652 - mae: 1.5650 - val_loss: 11.6992 - val_mse: 11.6992 - val_mae: 1.4800 - lr: 1.8909e-04 - 39s/epoch - 39ms/step\n",
            "Score for fold 3: loss of 11.69919204711914\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 41s - loss: 14.9227 - mse: 14.9227 - mae: 1.5737 - val_loss: 10.1676 - val_mse: 10.1676 - val_mae: 1.5913 - lr: 1.8909e-04 - 41s/epoch - 41ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 39s - loss: 14.8832 - mse: 14.8832 - mae: 1.5892 - val_loss: 10.5083 - val_mse: 10.5083 - val_mae: 1.5046 - lr: 1.8909e-04 - 39s/epoch - 39ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 39s - loss: 14.6851 - mse: 14.6851 - mae: 1.5762 - val_loss: 10.3651 - val_mse: 10.3651 - val_mae: 1.5836 - lr: 1.8909e-04 - 39s/epoch - 39ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 39s - loss: 14.6804 - mse: 14.6804 - mae: 1.5746 - val_loss: 10.8551 - val_mse: 10.8551 - val_mae: 1.6813 - lr: 1.8909e-04 - 39s/epoch - 39ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 41s - loss: 14.5958 - mse: 14.5958 - mae: 1.5713 - val_loss: 11.2413 - val_mse: 11.2413 - val_mae: 1.4571 - lr: 1.8909e-04 - 41s/epoch - 41ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 38s - loss: 14.2836 - mse: 14.2836 - mae: 1.5730 - val_loss: 11.0215 - val_mse: 11.0215 - val_mae: 1.5266 - lr: 1.8909e-04 - 38s/epoch - 38ms/step\n",
            "Score for fold 4: loss of 11.021478652954102\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 40s - loss: 14.3164 - mse: 14.3164 - mae: 1.6103 - val_loss: 12.2953 - val_mse: 12.2953 - val_mae: 1.4801 - lr: 1.8909e-04 - 40s/epoch - 40ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 41s - loss: 14.2849 - mse: 14.2849 - mae: 1.6058 - val_loss: 12.5233 - val_mse: 12.5233 - val_mae: 1.7363 - lr: 1.8909e-04 - 41s/epoch - 41ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 39s - loss: 14.2156 - mse: 14.2156 - mae: 1.6269 - val_loss: 12.4812 - val_mse: 12.4812 - val_mae: 1.6833 - lr: 1.8909e-04 - 39s/epoch - 39ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 39s - loss: 14.0455 - mse: 14.0455 - mae: 1.6140 - val_loss: 12.6289 - val_mse: 12.6289 - val_mae: 1.8024 - lr: 1.8909e-04 - 39s/epoch - 39ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 39s - loss: 13.9456 - mse: 13.9456 - mae: 1.6258 - val_loss: 12.6768 - val_mse: 12.6768 - val_mae: 1.4581 - lr: 1.8909e-04 - 39s/epoch - 39ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 42s - loss: 13.9595 - mse: 13.9595 - mae: 1.6238 - val_loss: 14.3072 - val_mse: 14.3072 - val_mae: 1.7475 - lr: 1.8909e-04 - 42s/epoch - 42ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-10-15 12:28:43,597]\u001b[0m Finished trial#17 resulted in value: 14.922. Current best value is 14.618 with parameters: {'activation': 'relu', 'num_hidden_layer': 4, 'i': 10, 'learning_rate': 0.0001995902281877497}.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Score for fold 5: loss of 14.307209014892578\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 7s - loss: 16.3814 - mse: 16.3814 - mae: 1.6276 - val_loss: 11.2809 - val_mse: 11.2809 - val_mae: 1.5547 - lr: 3.1006e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 6s - loss: 16.0259 - mse: 16.0259 - mae: 1.6092 - val_loss: 11.4603 - val_mse: 11.4603 - val_mae: 1.4700 - lr: 3.1006e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 6s - loss: 15.9717 - mse: 15.9717 - mae: 1.6048 - val_loss: 11.3206 - val_mse: 11.3206 - val_mae: 1.5246 - lr: 3.1006e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 6s - loss: 15.9617 - mse: 15.9617 - mae: 1.6020 - val_loss: 11.2174 - val_mse: 11.2174 - val_mae: 1.5661 - lr: 3.1006e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 6s - loss: 15.9050 - mse: 15.9050 - mae: 1.6012 - val_loss: 11.2457 - val_mse: 11.2457 - val_mae: 1.6231 - lr: 3.1006e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 6s - loss: 15.8387 - mse: 15.8387 - mae: 1.5946 - val_loss: 11.3688 - val_mse: 11.3688 - val_mae: 1.5404 - lr: 3.1006e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 6s - loss: 15.8702 - mse: 15.8702 - mae: 1.5935 - val_loss: 11.0842 - val_mse: 11.0842 - val_mae: 1.5816 - lr: 3.1006e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 6s - loss: 15.8527 - mse: 15.8527 - mae: 1.5927 - val_loss: 11.3983 - val_mse: 11.3983 - val_mae: 1.4865 - lr: 3.1006e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 6s - loss: 15.8666 - mse: 15.8666 - mae: 1.5855 - val_loss: 11.0539 - val_mse: 11.0539 - val_mae: 1.5417 - lr: 3.1006e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 6s - loss: 15.7663 - mse: 15.7663 - mae: 1.5873 - val_loss: 11.3036 - val_mse: 11.3036 - val_mae: 1.4725 - lr: 3.1006e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 6s - loss: 15.7379 - mse: 15.7379 - mae: 1.5839 - val_loss: 11.2176 - val_mse: 11.2176 - val_mae: 1.5004 - lr: 3.1006e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 6s - loss: 15.7309 - mse: 15.7309 - mae: 1.5803 - val_loss: 11.3709 - val_mse: 11.3709 - val_mae: 1.5101 - lr: 3.1006e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 13/100\n",
            "1000/1000 - 6s - loss: 15.7286 - mse: 15.7286 - mae: 1.5794 - val_loss: 11.2124 - val_mse: 11.2124 - val_mae: 1.5082 - lr: 3.1006e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 14/100\n",
            "1000/1000 - 6s - loss: 15.6933 - mse: 15.6933 - mae: 1.5835 - val_loss: 11.1297 - val_mse: 11.1297 - val_mae: 1.5562 - lr: 3.1006e-04 - 6s/epoch - 6ms/step\n",
            "Score for fold 1: loss of 11.129691123962402\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 6s - loss: 15.7719 - mse: 15.7719 - mae: 1.5765 - val_loss: 10.9439 - val_mse: 10.9439 - val_mae: 1.5243 - lr: 3.1006e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 6s - loss: 15.7664 - mse: 15.7664 - mae: 1.5738 - val_loss: 10.9527 - val_mse: 10.9527 - val_mae: 1.5671 - lr: 3.1006e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 6s - loss: 15.6490 - mse: 15.6490 - mae: 1.5746 - val_loss: 11.1933 - val_mse: 11.1933 - val_mae: 1.5220 - lr: 3.1006e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 6s - loss: 15.5991 - mse: 15.5991 - mae: 1.5735 - val_loss: 10.9869 - val_mse: 10.9869 - val_mae: 1.5626 - lr: 3.1006e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 7s - loss: 15.6481 - mse: 15.6481 - mae: 1.5702 - val_loss: 11.0539 - val_mse: 11.0539 - val_mae: 1.5947 - lr: 3.1006e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 7s - loss: 15.5676 - mse: 15.5676 - mae: 1.5709 - val_loss: 11.0441 - val_mse: 11.0441 - val_mae: 1.5412 - lr: 3.1006e-04 - 7s/epoch - 7ms/step\n",
            "Score for fold 2: loss of 11.044069290161133\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 6s - loss: 14.7970 - mse: 14.7970 - mae: 1.5461 - val_loss: 14.4182 - val_mse: 14.4182 - val_mae: 1.5537 - lr: 3.1006e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 6s - loss: 14.7784 - mse: 14.7784 - mae: 1.5439 - val_loss: 14.5076 - val_mse: 14.5076 - val_mae: 1.6322 - lr: 3.1006e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 6s - loss: 14.8061 - mse: 14.8061 - mae: 1.5407 - val_loss: 14.1490 - val_mse: 14.1490 - val_mae: 1.6430 - lr: 3.1006e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 6s - loss: 14.7530 - mse: 14.7530 - mae: 1.5444 - val_loss: 14.3013 - val_mse: 14.3013 - val_mae: 1.5926 - lr: 3.1006e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 6s - loss: 14.6906 - mse: 14.6906 - mae: 1.5413 - val_loss: 14.5156 - val_mse: 14.5156 - val_mae: 1.6861 - lr: 3.1006e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 6s - loss: 14.7006 - mse: 14.7006 - mae: 1.5418 - val_loss: 14.8015 - val_mse: 14.8015 - val_mae: 1.5854 - lr: 3.1006e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 6s - loss: 14.7149 - mse: 14.7149 - mae: 1.5360 - val_loss: 14.4205 - val_mse: 14.4205 - val_mae: 1.5884 - lr: 3.1006e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 6s - loss: 14.7065 - mse: 14.7065 - mae: 1.5347 - val_loss: 14.2929 - val_mse: 14.2929 - val_mae: 1.6179 - lr: 3.1006e-04 - 6s/epoch - 6ms/step\n",
            "Score for fold 3: loss of 14.292882919311523\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 6s - loss: 12.7819 - mse: 12.7819 - mae: 1.5546 - val_loss: 21.9263 - val_mse: 21.9263 - val_mae: 1.5446 - lr: 3.1006e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 6s - loss: 12.7689 - mse: 12.7689 - mae: 1.5466 - val_loss: 21.9094 - val_mse: 21.9094 - val_mae: 1.5805 - lr: 3.1006e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 6s - loss: 12.7076 - mse: 12.7076 - mae: 1.5512 - val_loss: 21.8697 - val_mse: 21.8697 - val_mae: 1.6309 - lr: 3.1006e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 6s - loss: 12.7096 - mse: 12.7096 - mae: 1.5471 - val_loss: 21.9144 - val_mse: 21.9144 - val_mae: 1.5986 - lr: 3.1006e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 6s - loss: 12.6516 - mse: 12.6516 - mae: 1.5452 - val_loss: 22.0917 - val_mse: 22.0917 - val_mae: 1.5879 - lr: 3.1006e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 6s - loss: 12.6379 - mse: 12.6379 - mae: 1.5448 - val_loss: 22.1067 - val_mse: 22.1067 - val_mae: 1.5716 - lr: 3.1006e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 6s - loss: 12.6288 - mse: 12.6288 - mae: 1.5395 - val_loss: 21.9885 - val_mse: 21.9885 - val_mae: 1.5780 - lr: 3.1006e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 6s - loss: 12.5925 - mse: 12.5925 - mae: 1.5440 - val_loss: 22.0724 - val_mse: 22.0724 - val_mae: 1.5997 - lr: 3.1006e-04 - 6s/epoch - 6ms/step\n",
            "Score for fold 4: loss of 22.072425842285156\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 6s - loss: 14.4525 - mse: 14.4525 - mae: 1.5486 - val_loss: 14.7156 - val_mse: 14.7156 - val_mae: 1.5944 - lr: 3.1006e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 7s - loss: 14.3597 - mse: 14.3597 - mae: 1.5453 - val_loss: 15.2007 - val_mse: 15.2007 - val_mae: 1.6049 - lr: 3.1006e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 7s - loss: 14.3394 - mse: 14.3394 - mae: 1.5470 - val_loss: 15.1584 - val_mse: 15.1584 - val_mae: 1.5613 - lr: 3.1006e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 6s - loss: 14.3115 - mse: 14.3115 - mae: 1.5386 - val_loss: 14.9387 - val_mse: 14.9387 - val_mae: 1.5740 - lr: 3.1006e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 6s - loss: 14.2417 - mse: 14.2417 - mae: 1.5397 - val_loss: 15.0641 - val_mse: 15.0641 - val_mae: 1.5738 - lr: 3.1006e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 6s - loss: 14.1550 - mse: 14.1550 - mae: 1.5375 - val_loss: 15.3728 - val_mse: 15.3728 - val_mae: 1.5696 - lr: 3.1006e-04 - 6s/epoch - 6ms/step\n",
            "Score for fold 5: loss of 15.372793197631836\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-10-15 12:33:42,812]\u001b[0m Finished trial#18 resulted in value: 14.780000000000001. Current best value is 14.618 with parameters: {'activation': 'relu', 'num_hidden_layer': 4, 'i': 10, 'learning_rate': 0.0001995902281877497}.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 16.6404 - mse: 16.6404 - mae: 1.6259 - val_loss: 12.3700 - val_mse: 12.3700 - val_mae: 1.5570 - lr: 7.6586e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 16.0160 - mse: 16.0160 - mae: 1.5948 - val_loss: 12.1233 - val_mse: 12.1233 - val_mae: 1.6154 - lr: 7.6586e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 15.9007 - mse: 15.9007 - mae: 1.5949 - val_loss: 12.1194 - val_mse: 12.1194 - val_mae: 1.6211 - lr: 7.6586e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 15.8800 - mse: 15.8800 - mae: 1.5923 - val_loss: 12.0821 - val_mse: 12.0821 - val_mae: 1.5902 - lr: 7.6586e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 15.8174 - mse: 15.8174 - mae: 1.5914 - val_loss: 12.0553 - val_mse: 12.0553 - val_mae: 1.5776 - lr: 7.6586e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 15.8156 - mse: 15.8156 - mae: 1.5921 - val_loss: 12.0514 - val_mse: 12.0514 - val_mae: 1.5881 - lr: 7.6586e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 15.7856 - mse: 15.7856 - mae: 1.5902 - val_loss: 12.0617 - val_mse: 12.0617 - val_mae: 1.5928 - lr: 7.6586e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 2s - loss: 15.7736 - mse: 15.7736 - mae: 1.5890 - val_loss: 12.0783 - val_mse: 12.0783 - val_mae: 1.5635 - lr: 7.6586e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 2s - loss: 15.7493 - mse: 15.7493 - mae: 1.5873 - val_loss: 12.0272 - val_mse: 12.0272 - val_mae: 1.5979 - lr: 7.6586e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 2s - loss: 15.7211 - mse: 15.7211 - mae: 1.5865 - val_loss: 12.0020 - val_mse: 12.0020 - val_mae: 1.5890 - lr: 7.6586e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 2s - loss: 15.7453 - mse: 15.7453 - mae: 1.5845 - val_loss: 12.0047 - val_mse: 12.0047 - val_mae: 1.5879 - lr: 7.6586e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 2s - loss: 15.7269 - mse: 15.7269 - mae: 1.5842 - val_loss: 11.9917 - val_mse: 11.9917 - val_mae: 1.6112 - lr: 7.6586e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 13/100\n",
            "1000/1000 - 2s - loss: 15.6780 - mse: 15.6780 - mae: 1.5843 - val_loss: 12.0500 - val_mse: 12.0500 - val_mae: 1.5706 - lr: 7.6586e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 14/100\n",
            "1000/1000 - 2s - loss: 15.7198 - mse: 15.7198 - mae: 1.5813 - val_loss: 12.0012 - val_mse: 12.0012 - val_mae: 1.5814 - lr: 7.6586e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 15/100\n",
            "1000/1000 - 2s - loss: 15.6939 - mse: 15.6939 - mae: 1.5834 - val_loss: 12.0464 - val_mse: 12.0464 - val_mae: 1.5404 - lr: 7.6586e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 16/100\n",
            "1000/1000 - 2s - loss: 15.6921 - mse: 15.6921 - mae: 1.5810 - val_loss: 11.9611 - val_mse: 11.9611 - val_mae: 1.6176 - lr: 7.6586e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 17/100\n",
            "1000/1000 - 2s - loss: 15.6564 - mse: 15.6564 - mae: 1.5849 - val_loss: 12.0189 - val_mse: 12.0189 - val_mae: 1.5771 - lr: 7.6586e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 18/100\n",
            "1000/1000 - 2s - loss: 15.6594 - mse: 15.6594 - mae: 1.5798 - val_loss: 11.9999 - val_mse: 11.9999 - val_mae: 1.5799 - lr: 7.6586e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 19/100\n",
            "1000/1000 - 2s - loss: 15.6584 - mse: 15.6584 - mae: 1.5831 - val_loss: 12.0219 - val_mse: 12.0219 - val_mae: 1.5489 - lr: 7.6586e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 20/100\n",
            "1000/1000 - 2s - loss: 15.6611 - mse: 15.6611 - mae: 1.5817 - val_loss: 12.0327 - val_mse: 12.0327 - val_mae: 1.5478 - lr: 7.6586e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 21/100\n",
            "1000/1000 - 2s - loss: 15.6975 - mse: 15.6975 - mae: 1.5759 - val_loss: 11.9673 - val_mse: 11.9673 - val_mae: 1.5868 - lr: 7.6586e-04 - 2s/epoch - 2ms/step\n",
            "Score for fold 1: loss of 11.967299461364746\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 15.0532 - mse: 15.0532 - mae: 1.5876 - val_loss: 14.4336 - val_mse: 14.4336 - val_mae: 1.5633 - lr: 7.6586e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 15.0535 - mse: 15.0535 - mae: 1.5873 - val_loss: 14.2744 - val_mse: 14.2744 - val_mae: 1.5817 - lr: 7.6586e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 15.0305 - mse: 15.0305 - mae: 1.5831 - val_loss: 14.3489 - val_mse: 14.3489 - val_mae: 1.5334 - lr: 7.6586e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 15.0543 - mse: 15.0543 - mae: 1.5801 - val_loss: 14.4919 - val_mse: 14.4919 - val_mae: 1.5677 - lr: 7.6586e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 15.0495 - mse: 15.0495 - mae: 1.5810 - val_loss: 14.4028 - val_mse: 14.4028 - val_mae: 1.5379 - lr: 7.6586e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 15.0322 - mse: 15.0322 - mae: 1.5835 - val_loss: 14.4579 - val_mse: 14.4579 - val_mae: 1.5359 - lr: 7.6586e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 15.0356 - mse: 15.0356 - mae: 1.5847 - val_loss: 14.3268 - val_mse: 14.3268 - val_mae: 1.5624 - lr: 7.6586e-04 - 2s/epoch - 2ms/step\n",
            "Score for fold 2: loss of 14.326797485351562\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 15.4853 - mse: 15.4853 - mae: 1.5814 - val_loss: 12.6472 - val_mse: 12.6472 - val_mae: 1.5884 - lr: 7.6586e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 15.4656 - mse: 15.4656 - mae: 1.5826 - val_loss: 12.6191 - val_mse: 12.6191 - val_mae: 1.5670 - lr: 7.6586e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 15.4777 - mse: 15.4777 - mae: 1.5767 - val_loss: 12.5757 - val_mse: 12.5757 - val_mae: 1.6307 - lr: 7.6586e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 15.4623 - mse: 15.4623 - mae: 1.5791 - val_loss: 12.5749 - val_mse: 12.5749 - val_mae: 1.6059 - lr: 7.6586e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 15.4696 - mse: 15.4696 - mae: 1.5787 - val_loss: 12.6103 - val_mse: 12.6103 - val_mae: 1.5809 - lr: 7.6586e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 15.4678 - mse: 15.4678 - mae: 1.5795 - val_loss: 12.5803 - val_mse: 12.5803 - val_mae: 1.5797 - lr: 7.6586e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 15.4493 - mse: 15.4493 - mae: 1.5789 - val_loss: 12.6500 - val_mse: 12.6500 - val_mae: 1.5953 - lr: 7.6586e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 2s - loss: 15.4542 - mse: 15.4542 - mae: 1.5782 - val_loss: 12.6011 - val_mse: 12.6011 - val_mae: 1.5959 - lr: 7.6586e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 2s - loss: 15.4727 - mse: 15.4727 - mae: 1.5772 - val_loss: 12.5808 - val_mse: 12.5808 - val_mae: 1.5760 - lr: 7.6586e-04 - 2s/epoch - 2ms/step\n",
            "Score for fold 3: loss of 12.580841064453125\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 13.1961 - mse: 13.1961 - mae: 1.5725 - val_loss: 21.6445 - val_mse: 21.6445 - val_mae: 1.5828 - lr: 7.6586e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 13.1823 - mse: 13.1823 - mae: 1.5726 - val_loss: 21.5953 - val_mse: 21.5953 - val_mae: 1.5730 - lr: 7.6586e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 13.1742 - mse: 13.1742 - mae: 1.5696 - val_loss: 21.7451 - val_mse: 21.7451 - val_mae: 1.6112 - lr: 7.6586e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 13.1837 - mse: 13.1837 - mae: 1.5727 - val_loss: 21.7085 - val_mse: 21.7085 - val_mae: 1.6111 - lr: 7.6586e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 13.1729 - mse: 13.1729 - mae: 1.5707 - val_loss: 21.6370 - val_mse: 21.6370 - val_mae: 1.6066 - lr: 7.6586e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 13.1661 - mse: 13.1661 - mae: 1.5718 - val_loss: 21.6672 - val_mse: 21.6672 - val_mae: 1.5562 - lr: 7.6586e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 13.1596 - mse: 13.1596 - mae: 1.5694 - val_loss: 21.7262 - val_mse: 21.7262 - val_mae: 1.5948 - lr: 7.6586e-04 - 2s/epoch - 2ms/step\n",
            "Score for fold 4: loss of 21.726226806640625\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 15.1202 - mse: 15.1202 - mae: 1.5732 - val_loss: 13.8121 - val_mse: 13.8121 - val_mae: 1.5632 - lr: 7.6586e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 15.1178 - mse: 15.1178 - mae: 1.5725 - val_loss: 13.8122 - val_mse: 13.8122 - val_mae: 1.5968 - lr: 7.6586e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 15.1061 - mse: 15.1061 - mae: 1.5721 - val_loss: 13.8465 - val_mse: 13.8465 - val_mae: 1.5792 - lr: 7.6586e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 15.1078 - mse: 15.1078 - mae: 1.5733 - val_loss: 13.9318 - val_mse: 13.9318 - val_mae: 1.5831 - lr: 7.6586e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 15.0992 - mse: 15.0992 - mae: 1.5722 - val_loss: 13.8346 - val_mse: 13.8346 - val_mae: 1.6134 - lr: 7.6586e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 15.1260 - mse: 15.1260 - mae: 1.5741 - val_loss: 13.9446 - val_mse: 13.9446 - val_mae: 1.5888 - lr: 7.6586e-04 - 2s/epoch - 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-10-15 12:35:34,493]\u001b[0m Finished trial#19 resulted in value: 14.91. Current best value is 14.618 with parameters: {'activation': 'relu', 'num_hidden_layer': 4, 'i': 10, 'learning_rate': 0.0001995902281877497}.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Score for fold 5: loss of 13.94462776184082\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 5s - loss: 16.8778 - mse: 16.8778 - mae: 1.6726 - val_loss: 11.5448 - val_mse: 11.5448 - val_mae: 1.6061 - lr: 3.8713e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 4s - loss: 16.4827 - mse: 16.4827 - mae: 1.6285 - val_loss: 11.3022 - val_mse: 11.3022 - val_mae: 1.5811 - lr: 3.8713e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 7s - loss: 16.4128 - mse: 16.4128 - mae: 1.6188 - val_loss: 11.1542 - val_mse: 11.1542 - val_mae: 1.6421 - lr: 3.8713e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 5s - loss: 16.3382 - mse: 16.3382 - mae: 1.6095 - val_loss: 11.0961 - val_mse: 11.0961 - val_mae: 1.5758 - lr: 3.8713e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 5s - loss: 16.3084 - mse: 16.3084 - mae: 1.6126 - val_loss: 11.1888 - val_mse: 11.1888 - val_mae: 1.5810 - lr: 3.8713e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 5s - loss: 16.2798 - mse: 16.2798 - mae: 1.6067 - val_loss: 11.1224 - val_mse: 11.1224 - val_mae: 1.5765 - lr: 3.8713e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 4s - loss: 16.2395 - mse: 16.2395 - mae: 1.6022 - val_loss: 11.1730 - val_mse: 11.1730 - val_mae: 1.6210 - lr: 3.8713e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 5s - loss: 16.2145 - mse: 16.2145 - mae: 1.6050 - val_loss: 11.2477 - val_mse: 11.2477 - val_mae: 1.5205 - lr: 3.8713e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 4s - loss: 16.1570 - mse: 16.1570 - mae: 1.5979 - val_loss: 11.2595 - val_mse: 11.2595 - val_mae: 1.6274 - lr: 3.8713e-04 - 4s/epoch - 4ms/step\n",
            "Score for fold 1: loss of 11.259533882141113\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 5s - loss: 15.1596 - mse: 15.1596 - mae: 1.6013 - val_loss: 15.0227 - val_mse: 15.0227 - val_mae: 1.6360 - lr: 3.8713e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 5s - loss: 15.0906 - mse: 15.0906 - mae: 1.5964 - val_loss: 15.1836 - val_mse: 15.1836 - val_mae: 1.6447 - lr: 3.8713e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 5s - loss: 15.0919 - mse: 15.0919 - mae: 1.5993 - val_loss: 15.0480 - val_mse: 15.0480 - val_mae: 1.6493 - lr: 3.8713e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 5s - loss: 15.0557 - mse: 15.0557 - mae: 1.5998 - val_loss: 15.3729 - val_mse: 15.3729 - val_mae: 1.5928 - lr: 3.8713e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 5s - loss: 15.0448 - mse: 15.0448 - mae: 1.5943 - val_loss: 15.0046 - val_mse: 15.0046 - val_mae: 1.5693 - lr: 3.8713e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 5s - loss: 14.9756 - mse: 14.9756 - mae: 1.5905 - val_loss: 15.0300 - val_mse: 15.0300 - val_mae: 1.6019 - lr: 3.8713e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 5s - loss: 14.9570 - mse: 14.9570 - mae: 1.5941 - val_loss: 14.9017 - val_mse: 14.9017 - val_mae: 1.5750 - lr: 3.8713e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 5s - loss: 14.9271 - mse: 14.9271 - mae: 1.5867 - val_loss: 14.9288 - val_mse: 14.9288 - val_mae: 1.5539 - lr: 3.8713e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 5s - loss: 14.8963 - mse: 14.8963 - mae: 1.5864 - val_loss: 14.9042 - val_mse: 14.9042 - val_mae: 1.6304 - lr: 3.8713e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 5s - loss: 14.9227 - mse: 14.9227 - mae: 1.5878 - val_loss: 14.8878 - val_mse: 14.8878 - val_mae: 1.5816 - lr: 3.8713e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 4s - loss: 14.8350 - mse: 14.8350 - mae: 1.5852 - val_loss: 15.0880 - val_mse: 15.0880 - val_mae: 1.5604 - lr: 3.8713e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 4s - loss: 14.8214 - mse: 14.8214 - mae: 1.5804 - val_loss: 14.8950 - val_mse: 14.8950 - val_mae: 1.5521 - lr: 3.8713e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 13/100\n",
            "1000/1000 - 5s - loss: 14.8012 - mse: 14.8012 - mae: 1.5837 - val_loss: 14.9731 - val_mse: 14.9731 - val_mae: 1.5867 - lr: 3.8713e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 14/100\n",
            "1000/1000 - 4s - loss: 14.7672 - mse: 14.7672 - mae: 1.5822 - val_loss: 14.9740 - val_mse: 14.9740 - val_mae: 1.6568 - lr: 3.8713e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 15/100\n",
            "1000/1000 - 4s - loss: 14.7442 - mse: 14.7442 - mae: 1.5847 - val_loss: 14.8748 - val_mse: 14.8748 - val_mae: 1.5643 - lr: 3.8713e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 16/100\n",
            "1000/1000 - 4s - loss: 14.7347 - mse: 14.7347 - mae: 1.5767 - val_loss: 15.0757 - val_mse: 15.0757 - val_mae: 1.6107 - lr: 3.8713e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 17/100\n",
            "1000/1000 - 5s - loss: 14.6850 - mse: 14.6850 - mae: 1.5775 - val_loss: 15.0761 - val_mse: 15.0761 - val_mae: 1.5280 - lr: 3.8713e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 18/100\n",
            "1000/1000 - 5s - loss: 14.6970 - mse: 14.6970 - mae: 1.5753 - val_loss: 15.2026 - val_mse: 15.2026 - val_mae: 1.6810 - lr: 3.8713e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 19/100\n",
            "1000/1000 - 5s - loss: 14.5904 - mse: 14.5904 - mae: 1.5777 - val_loss: 14.9495 - val_mse: 14.9495 - val_mae: 1.5903 - lr: 3.8713e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 20/100\n",
            "1000/1000 - 5s - loss: 14.5707 - mse: 14.5707 - mae: 1.5720 - val_loss: 14.9278 - val_mse: 14.9278 - val_mae: 1.5956 - lr: 3.8713e-04 - 5s/epoch - 5ms/step\n",
            "Score for fold 2: loss of 14.927823066711426\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 4s - loss: 14.8403 - mse: 14.8403 - mae: 1.5679 - val_loss: 13.9811 - val_mse: 13.9811 - val_mae: 1.6233 - lr: 3.8713e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 4s - loss: 14.7615 - mse: 14.7615 - mae: 1.5668 - val_loss: 14.0597 - val_mse: 14.0597 - val_mae: 1.6249 - lr: 3.8713e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 5s - loss: 14.6949 - mse: 14.6949 - mae: 1.5607 - val_loss: 14.1271 - val_mse: 14.1271 - val_mae: 1.6262 - lr: 3.8713e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 5s - loss: 14.6433 - mse: 14.6433 - mae: 1.5561 - val_loss: 14.1635 - val_mse: 14.1635 - val_mae: 1.6031 - lr: 3.8713e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 5s - loss: 14.6099 - mse: 14.6099 - mae: 1.5524 - val_loss: 14.1783 - val_mse: 14.1783 - val_mae: 1.6017 - lr: 3.8713e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 4s - loss: 14.5895 - mse: 14.5895 - mae: 1.5553 - val_loss: 14.2189 - val_mse: 14.2189 - val_mae: 1.6127 - lr: 3.8713e-04 - 4s/epoch - 4ms/step\n",
            "Score for fold 3: loss of 14.218877792358398\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 5s - loss: 15.2092 - mse: 15.2092 - mae: 1.5809 - val_loss: 11.7250 - val_mse: 11.7250 - val_mae: 1.4868 - lr: 3.8713e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 5s - loss: 15.1316 - mse: 15.1316 - mae: 1.5784 - val_loss: 11.6829 - val_mse: 11.6829 - val_mae: 1.5174 - lr: 3.8713e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 5s - loss: 15.0747 - mse: 15.0747 - mae: 1.5741 - val_loss: 11.8102 - val_mse: 11.8102 - val_mae: 1.5629 - lr: 3.8713e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 4s - loss: 15.0064 - mse: 15.0064 - mae: 1.5723 - val_loss: 11.9270 - val_mse: 11.9270 - val_mae: 1.4601 - lr: 3.8713e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 5s - loss: 14.9777 - mse: 14.9777 - mae: 1.5677 - val_loss: 11.8534 - val_mse: 11.8534 - val_mae: 1.5105 - lr: 3.8713e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 5s - loss: 14.9781 - mse: 14.9781 - mae: 1.5717 - val_loss: 11.8794 - val_mse: 11.8794 - val_mae: 1.5246 - lr: 3.8713e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 5s - loss: 14.9432 - mse: 14.9432 - mae: 1.5614 - val_loss: 12.0487 - val_mse: 12.0487 - val_mae: 1.5995 - lr: 3.8713e-04 - 5s/epoch - 5ms/step\n",
            "Score for fold 4: loss of 12.04874324798584\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 5s - loss: 12.5606 - mse: 12.5606 - mae: 1.5403 - val_loss: 21.4085 - val_mse: 21.4085 - val_mae: 1.5921 - lr: 3.8713e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 5s - loss: 12.4583 - mse: 12.4583 - mae: 1.5354 - val_loss: 21.6357 - val_mse: 21.6357 - val_mae: 1.6078 - lr: 3.8713e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 5s - loss: 12.4171 - mse: 12.4171 - mae: 1.5326 - val_loss: 21.8297 - val_mse: 21.8297 - val_mae: 1.6053 - lr: 3.8713e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 4s - loss: 12.3877 - mse: 12.3877 - mae: 1.5296 - val_loss: 21.6839 - val_mse: 21.6839 - val_mae: 1.6326 - lr: 3.8713e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 5s - loss: 12.3267 - mse: 12.3267 - mae: 1.5273 - val_loss: 21.9434 - val_mse: 21.9434 - val_mae: 1.5850 - lr: 3.8713e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 4s - loss: 12.3058 - mse: 12.3058 - mae: 1.5202 - val_loss: 21.9489 - val_mse: 21.9489 - val_mae: 1.6635 - lr: 3.8713e-04 - 4s/epoch - 4ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-10-15 12:40:35,733]\u001b[0m Finished trial#20 resulted in value: 14.882. Current best value is 14.618 with parameters: {'activation': 'relu', 'num_hidden_layer': 4, 'i': 10, 'learning_rate': 0.0001995902281877497}.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Score for fold 5: loss of 21.948936462402344\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 7s - loss: 16.3583 - mse: 16.3583 - mae: 1.6190 - val_loss: 11.9144 - val_mse: 11.9144 - val_mae: 1.5825 - lr: 2.3470e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 6s - loss: 15.8765 - mse: 15.8765 - mae: 1.5935 - val_loss: 11.8940 - val_mse: 11.8940 - val_mae: 1.5458 - lr: 2.3470e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 6s - loss: 15.8677 - mse: 15.8677 - mae: 1.5878 - val_loss: 11.9511 - val_mse: 11.9511 - val_mae: 1.5132 - lr: 2.3470e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 8s - loss: 15.8041 - mse: 15.8041 - mae: 1.5884 - val_loss: 11.8625 - val_mse: 11.8625 - val_mae: 1.5524 - lr: 2.3470e-04 - 8s/epoch - 8ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 6s - loss: 15.7447 - mse: 15.7447 - mae: 1.5835 - val_loss: 11.9140 - val_mse: 11.9140 - val_mae: 1.5582 - lr: 2.3470e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 6s - loss: 15.7042 - mse: 15.7042 - mae: 1.5830 - val_loss: 11.8818 - val_mse: 11.8818 - val_mae: 1.6159 - lr: 2.3470e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 6s - loss: 15.7231 - mse: 15.7231 - mae: 1.5788 - val_loss: 11.7687 - val_mse: 11.7687 - val_mae: 1.5700 - lr: 2.3470e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 6s - loss: 15.6589 - mse: 15.6589 - mae: 1.5792 - val_loss: 11.9112 - val_mse: 11.9112 - val_mae: 1.5293 - lr: 2.3470e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 6s - loss: 15.6801 - mse: 15.6801 - mae: 1.5762 - val_loss: 11.7814 - val_mse: 11.7814 - val_mae: 1.5820 - lr: 2.3470e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 6s - loss: 15.6274 - mse: 15.6274 - mae: 1.5792 - val_loss: 11.8025 - val_mse: 11.8025 - val_mae: 1.6239 - lr: 2.3470e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 6s - loss: 15.6326 - mse: 15.6326 - mae: 1.5765 - val_loss: 11.7826 - val_mse: 11.7826 - val_mae: 1.5564 - lr: 2.3470e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 6s - loss: 15.6058 - mse: 15.6058 - mae: 1.5758 - val_loss: 11.7851 - val_mse: 11.7851 - val_mae: 1.5386 - lr: 2.3470e-04 - 6s/epoch - 6ms/step\n",
            "Score for fold 1: loss of 11.785140991210938\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 6s - loss: 15.4281 - mse: 15.4281 - mae: 1.5626 - val_loss: 12.3879 - val_mse: 12.3879 - val_mae: 1.5606 - lr: 2.3470e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 6s - loss: 15.4241 - mse: 15.4241 - mae: 1.5630 - val_loss: 12.3201 - val_mse: 12.3201 - val_mae: 1.6253 - lr: 2.3470e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 6s - loss: 15.3608 - mse: 15.3608 - mae: 1.5617 - val_loss: 12.4106 - val_mse: 12.4106 - val_mae: 1.5872 - lr: 2.3470e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 6s - loss: 15.3735 - mse: 15.3735 - mae: 1.5586 - val_loss: 12.3481 - val_mse: 12.3481 - val_mae: 1.5591 - lr: 2.3470e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 6s - loss: 15.3360 - mse: 15.3360 - mae: 1.5592 - val_loss: 12.5049 - val_mse: 12.5049 - val_mae: 1.5461 - lr: 2.3470e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 6s - loss: 15.3008 - mse: 15.3008 - mae: 1.5568 - val_loss: 12.4935 - val_mse: 12.4935 - val_mae: 1.5664 - lr: 2.3470e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 6s - loss: 15.2957 - mse: 15.2957 - mae: 1.5576 - val_loss: 12.4077 - val_mse: 12.4077 - val_mae: 1.6312 - lr: 2.3470e-04 - 6s/epoch - 6ms/step\n",
            "Score for fold 2: loss of 12.40774154663086\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 6s - loss: 12.7329 - mse: 12.7329 - mae: 1.5602 - val_loss: 22.8392 - val_mse: 22.8392 - val_mae: 1.5150 - lr: 2.3470e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 6s - loss: 12.7324 - mse: 12.7324 - mae: 1.5561 - val_loss: 22.5613 - val_mse: 22.5613 - val_mae: 1.5738 - lr: 2.3470e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 6s - loss: 12.7206 - mse: 12.7206 - mae: 1.5526 - val_loss: 22.5367 - val_mse: 22.5367 - val_mae: 1.5722 - lr: 2.3470e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 6s - loss: 12.6543 - mse: 12.6543 - mae: 1.5533 - val_loss: 22.8546 - val_mse: 22.8546 - val_mae: 1.5582 - lr: 2.3470e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 6s - loss: 12.6990 - mse: 12.6990 - mae: 1.5533 - val_loss: 22.6997 - val_mse: 22.6997 - val_mae: 1.5967 - lr: 2.3470e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 6s - loss: 12.6407 - mse: 12.6407 - mae: 1.5496 - val_loss: 23.0520 - val_mse: 23.0520 - val_mae: 1.5427 - lr: 2.3470e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 6s - loss: 12.6688 - mse: 12.6688 - mae: 1.5510 - val_loss: 22.8312 - val_mse: 22.8312 - val_mae: 1.6084 - lr: 2.3470e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 6s - loss: 12.5850 - mse: 12.5850 - mae: 1.5466 - val_loss: 22.7481 - val_mse: 22.7481 - val_mae: 1.6160 - lr: 2.3470e-04 - 6s/epoch - 6ms/step\n",
            "Score for fold 3: loss of 22.748140335083008\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 8s - loss: 13.9137 - mse: 13.9137 - mae: 1.5434 - val_loss: 18.0113 - val_mse: 18.0113 - val_mae: 1.5702 - lr: 2.3470e-04 - 8s/epoch - 8ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 6s - loss: 13.8183 - mse: 13.8183 - mae: 1.5440 - val_loss: 18.0475 - val_mse: 18.0475 - val_mae: 1.5351 - lr: 2.3470e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 6s - loss: 13.8168 - mse: 13.8168 - mae: 1.5417 - val_loss: 17.8372 - val_mse: 17.8372 - val_mae: 1.5798 - lr: 2.3470e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 6s - loss: 13.8240 - mse: 13.8240 - mae: 1.5397 - val_loss: 17.8384 - val_mse: 17.8384 - val_mae: 1.6077 - lr: 2.3470e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 6s - loss: 13.8134 - mse: 13.8134 - mae: 1.5371 - val_loss: 18.2797 - val_mse: 18.2797 - val_mae: 1.6335 - lr: 2.3470e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 6s - loss: 13.7698 - mse: 13.7698 - mae: 1.5400 - val_loss: 18.2068 - val_mse: 18.2068 - val_mae: 1.6456 - lr: 2.3470e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 6s - loss: 13.8088 - mse: 13.8088 - mae: 1.5386 - val_loss: 17.9730 - val_mse: 17.9730 - val_mae: 1.6605 - lr: 2.3470e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 6s - loss: 13.7760 - mse: 13.7760 - mae: 1.5366 - val_loss: 17.9458 - val_mse: 17.9458 - val_mae: 1.6026 - lr: 2.3470e-04 - 6s/epoch - 6ms/step\n",
            "Score for fold 4: loss of 17.945816040039062\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 6s - loss: 15.9951 - mse: 15.9951 - mae: 1.5716 - val_loss: 9.0713 - val_mse: 9.0713 - val_mae: 1.4644 - lr: 2.3470e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 6s - loss: 16.0439 - mse: 16.0439 - mae: 1.5703 - val_loss: 9.0091 - val_mse: 9.0091 - val_mae: 1.5542 - lr: 2.3470e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 6s - loss: 15.9520 - mse: 15.9520 - mae: 1.5680 - val_loss: 9.0168 - val_mse: 9.0168 - val_mae: 1.5041 - lr: 2.3470e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 6s - loss: 15.9079 - mse: 15.9079 - mae: 1.5684 - val_loss: 9.1819 - val_mse: 9.1819 - val_mae: 1.5044 - lr: 2.3470e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 6s - loss: 15.8879 - mse: 15.8879 - mae: 1.5632 - val_loss: 9.0742 - val_mse: 9.0742 - val_mae: 1.5046 - lr: 2.3470e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 6s - loss: 15.8459 - mse: 15.8459 - mae: 1.5617 - val_loss: 9.1318 - val_mse: 9.1318 - val_mae: 1.5436 - lr: 2.3470e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 6s - loss: 15.7222 - mse: 15.7222 - mae: 1.5634 - val_loss: 9.1870 - val_mse: 9.1870 - val_mae: 1.4998 - lr: 2.3470e-04 - 6s/epoch - 6ms/step\n",
            "Score for fold 5: loss of 9.18703556060791\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-10-15 12:46:16,454]\u001b[0m Finished trial#21 resulted in value: 14.818000000000001. Current best value is 14.618 with parameters: {'activation': 'relu', 'num_hidden_layer': 4, 'i': 10, 'learning_rate': 0.0001995902281877497}.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 21s - loss: 15.7454 - mse: 15.7454 - mae: 1.6116 - val_loss: 14.3121 - val_mse: 14.3121 - val_mae: 1.6505 - lr: 1.4829e-04 - 21s/epoch - 21ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 20s - loss: 15.3341 - mse: 15.3341 - mae: 1.5897 - val_loss: 14.3661 - val_mse: 14.3661 - val_mae: 1.5880 - lr: 1.4829e-04 - 20s/epoch - 20ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 20s - loss: 15.2463 - mse: 15.2463 - mae: 1.5884 - val_loss: 14.3782 - val_mse: 14.3782 - val_mae: 1.5583 - lr: 1.4829e-04 - 20s/epoch - 20ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 20s - loss: 15.2313 - mse: 15.2313 - mae: 1.5853 - val_loss: 14.1952 - val_mse: 14.1952 - val_mae: 1.5705 - lr: 1.4829e-04 - 20s/epoch - 20ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 21s - loss: 15.1873 - mse: 15.1873 - mae: 1.5791 - val_loss: 14.0310 - val_mse: 14.0310 - val_mae: 1.6043 - lr: 1.4829e-04 - 21s/epoch - 21ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 21s - loss: 15.1394 - mse: 15.1394 - mae: 1.5750 - val_loss: 14.3684 - val_mse: 14.3684 - val_mae: 1.5401 - lr: 1.4829e-04 - 21s/epoch - 21ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 21s - loss: 15.1389 - mse: 15.1389 - mae: 1.5763 - val_loss: 14.1326 - val_mse: 14.1326 - val_mae: 1.5748 - lr: 1.4829e-04 - 21s/epoch - 21ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 22s - loss: 15.1099 - mse: 15.1099 - mae: 1.5750 - val_loss: 13.9171 - val_mse: 13.9171 - val_mae: 1.6094 - lr: 1.4829e-04 - 22s/epoch - 22ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 20s - loss: 15.0887 - mse: 15.0887 - mae: 1.5700 - val_loss: 14.0960 - val_mse: 14.0960 - val_mae: 1.5328 - lr: 1.4829e-04 - 20s/epoch - 20ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 20s - loss: 15.0371 - mse: 15.0371 - mae: 1.5693 - val_loss: 14.2585 - val_mse: 14.2585 - val_mae: 1.6262 - lr: 1.4829e-04 - 20s/epoch - 20ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 21s - loss: 15.0233 - mse: 15.0233 - mae: 1.5717 - val_loss: 14.1984 - val_mse: 14.1984 - val_mae: 1.6163 - lr: 1.4829e-04 - 21s/epoch - 21ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 21s - loss: 15.0154 - mse: 15.0154 - mae: 1.5647 - val_loss: 13.9408 - val_mse: 13.9408 - val_mae: 1.6131 - lr: 1.4829e-04 - 21s/epoch - 21ms/step\n",
            "Epoch 13/100\n",
            "1000/1000 - 20s - loss: 14.9507 - mse: 14.9507 - mae: 1.5676 - val_loss: 14.1252 - val_mse: 14.1252 - val_mae: 1.5639 - lr: 1.4829e-04 - 20s/epoch - 20ms/step\n",
            "Score for fold 1: loss of 14.125237464904785\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 21s - loss: 15.0355 - mse: 15.0355 - mae: 1.5727 - val_loss: 13.9327 - val_mse: 13.9327 - val_mae: 1.5022 - lr: 1.4829e-04 - 21s/epoch - 21ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 22s - loss: 14.9480 - mse: 14.9480 - mae: 1.5674 - val_loss: 13.8087 - val_mse: 13.8087 - val_mae: 1.6020 - lr: 1.4829e-04 - 22s/epoch - 22ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 21s - loss: 14.9493 - mse: 14.9493 - mae: 1.5666 - val_loss: 13.8508 - val_mse: 13.8508 - val_mae: 1.5860 - lr: 1.4829e-04 - 21s/epoch - 21ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 21s - loss: 14.9408 - mse: 14.9408 - mae: 1.5639 - val_loss: 13.8112 - val_mse: 13.8112 - val_mae: 1.5896 - lr: 1.4829e-04 - 21s/epoch - 21ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 21s - loss: 14.9589 - mse: 14.9589 - mae: 1.5613 - val_loss: 13.8665 - val_mse: 13.8665 - val_mae: 1.5901 - lr: 1.4829e-04 - 21s/epoch - 21ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 20s - loss: 14.9142 - mse: 14.9142 - mae: 1.5628 - val_loss: 13.9048 - val_mse: 13.9048 - val_mae: 1.5393 - lr: 1.4829e-04 - 20s/epoch - 20ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 20s - loss: 14.9044 - mse: 14.9044 - mae: 1.5589 - val_loss: 13.9578 - val_mse: 13.9578 - val_mae: 1.5374 - lr: 1.4829e-04 - 20s/epoch - 20ms/step\n",
            "Score for fold 2: loss of 13.957826614379883\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 21s - loss: 14.5204 - mse: 14.5204 - mae: 1.5572 - val_loss: 15.3071 - val_mse: 15.3071 - val_mae: 1.5846 - lr: 1.4829e-04 - 21s/epoch - 21ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 21s - loss: 14.5491 - mse: 14.5491 - mae: 1.5567 - val_loss: 15.5176 - val_mse: 15.5176 - val_mae: 1.5710 - lr: 1.4829e-04 - 21s/epoch - 21ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 20s - loss: 14.5005 - mse: 14.5005 - mae: 1.5551 - val_loss: 15.3768 - val_mse: 15.3768 - val_mae: 1.5849 - lr: 1.4829e-04 - 20s/epoch - 20ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 20s - loss: 14.4855 - mse: 14.4855 - mae: 1.5548 - val_loss: 15.4835 - val_mse: 15.4835 - val_mae: 1.6539 - lr: 1.4829e-04 - 20s/epoch - 20ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 21s - loss: 14.5027 - mse: 14.5027 - mae: 1.5515 - val_loss: 15.2671 - val_mse: 15.2671 - val_mae: 1.6201 - lr: 1.4829e-04 - 21s/epoch - 21ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 21s - loss: 14.4175 - mse: 14.4175 - mae: 1.5534 - val_loss: 15.4032 - val_mse: 15.4032 - val_mae: 1.5969 - lr: 1.4829e-04 - 21s/epoch - 21ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 20s - loss: 14.3776 - mse: 14.3776 - mae: 1.5483 - val_loss: 15.3532 - val_mse: 15.3532 - val_mae: 1.5912 - lr: 1.4829e-04 - 20s/epoch - 20ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 22s - loss: 14.4392 - mse: 14.4392 - mae: 1.5509 - val_loss: 15.4152 - val_mse: 15.4152 - val_mae: 1.5667 - lr: 1.4829e-04 - 22s/epoch - 22ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 20s - loss: 14.3545 - mse: 14.3545 - mae: 1.5468 - val_loss: 15.4152 - val_mse: 15.4152 - val_mae: 1.6017 - lr: 1.4829e-04 - 20s/epoch - 20ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 21s - loss: 14.3275 - mse: 14.3275 - mae: 1.5420 - val_loss: 15.3909 - val_mse: 15.3909 - val_mae: 1.6127 - lr: 1.4829e-04 - 21s/epoch - 21ms/step\n",
            "Score for fold 3: loss of 15.390900611877441\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 20s - loss: 15.2923 - mse: 15.2923 - mae: 1.5624 - val_loss: 11.8744 - val_mse: 11.8744 - val_mae: 1.5196 - lr: 1.4829e-04 - 20s/epoch - 20ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 20s - loss: 15.2116 - mse: 15.2116 - mae: 1.5562 - val_loss: 12.0216 - val_mse: 12.0216 - val_mae: 1.5208 - lr: 1.4829e-04 - 20s/epoch - 20ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 20s - loss: 15.2274 - mse: 15.2274 - mae: 1.5560 - val_loss: 11.8988 - val_mse: 11.8988 - val_mae: 1.5281 - lr: 1.4829e-04 - 20s/epoch - 20ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 20s - loss: 15.2237 - mse: 15.2237 - mae: 1.5558 - val_loss: 11.9306 - val_mse: 11.9306 - val_mae: 1.5300 - lr: 1.4829e-04 - 20s/epoch - 20ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 20s - loss: 15.1635 - mse: 15.1635 - mae: 1.5526 - val_loss: 11.9852 - val_mse: 11.9852 - val_mae: 1.5245 - lr: 1.4829e-04 - 20s/epoch - 20ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 22s - loss: 15.0609 - mse: 15.0609 - mae: 1.5512 - val_loss: 11.8050 - val_mse: 11.8050 - val_mae: 1.5128 - lr: 1.4829e-04 - 22s/epoch - 22ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 21s - loss: 15.1461 - mse: 15.1461 - mae: 1.5464 - val_loss: 12.0027 - val_mse: 12.0027 - val_mae: 1.5845 - lr: 1.4829e-04 - 21s/epoch - 21ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 20s - loss: 15.0934 - mse: 15.0934 - mae: 1.5466 - val_loss: 12.1509 - val_mse: 12.1509 - val_mae: 1.5171 - lr: 1.4829e-04 - 20s/epoch - 20ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 20s - loss: 15.0653 - mse: 15.0653 - mae: 1.5452 - val_loss: 12.0014 - val_mse: 12.0014 - val_mae: 1.5073 - lr: 1.4829e-04 - 20s/epoch - 20ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 20s - loss: 15.0277 - mse: 15.0277 - mae: 1.5450 - val_loss: 12.1336 - val_mse: 12.1336 - val_mae: 1.5314 - lr: 1.4829e-04 - 20s/epoch - 20ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 20s - loss: 14.9868 - mse: 14.9868 - mae: 1.5419 - val_loss: 11.9924 - val_mse: 11.9924 - val_mae: 1.6187 - lr: 1.4829e-04 - 20s/epoch - 20ms/step\n",
            "Score for fold 4: loss of 11.992398262023926\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 21s - loss: 13.5382 - mse: 13.5382 - mae: 1.5431 - val_loss: 18.0448 - val_mse: 18.0448 - val_mae: 1.5494 - lr: 1.4829e-04 - 21s/epoch - 21ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 22s - loss: 13.4296 - mse: 13.4296 - mae: 1.5363 - val_loss: 18.2228 - val_mse: 18.2228 - val_mae: 1.5576 - lr: 1.4829e-04 - 22s/epoch - 22ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 20s - loss: 13.3216 - mse: 13.3216 - mae: 1.5367 - val_loss: 18.2841 - val_mse: 18.2841 - val_mae: 1.6359 - lr: 1.4829e-04 - 20s/epoch - 20ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 21s - loss: 13.4306 - mse: 13.4306 - mae: 1.5345 - val_loss: 18.1985 - val_mse: 18.1985 - val_mae: 1.6027 - lr: 1.4829e-04 - 21s/epoch - 21ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 20s - loss: 13.2972 - mse: 13.2972 - mae: 1.5343 - val_loss: 18.3127 - val_mse: 18.3127 - val_mae: 1.5533 - lr: 1.4829e-04 - 20s/epoch - 20ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 20s - loss: 13.3016 - mse: 13.3016 - mae: 1.5309 - val_loss: 18.3110 - val_mse: 18.3110 - val_mae: 1.5455 - lr: 1.4829e-04 - 20s/epoch - 20ms/step\n",
            "Score for fold 5: loss of 18.311046600341797\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-10-15 13:03:31,273]\u001b[0m Finished trial#22 resulted in value: 14.756. Current best value is 14.618 with parameters: {'activation': 'relu', 'num_hidden_layer': 4, 'i': 10, 'learning_rate': 0.0001995902281877497}.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 3s - loss: 17.2170 - mse: 17.2170 - mae: 1.6315 - val_loss: 10.3030 - val_mse: 10.3030 - val_mae: 1.5649 - lr: 3.8702e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 16.4732 - mse: 16.4732 - mae: 1.6013 - val_loss: 10.1727 - val_mse: 10.1727 - val_mae: 1.5670 - lr: 3.8702e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 16.3949 - mse: 16.3949 - mae: 1.6022 - val_loss: 10.1086 - val_mse: 10.1086 - val_mae: 1.6003 - lr: 3.8702e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 16.3443 - mse: 16.3443 - mae: 1.6010 - val_loss: 10.1933 - val_mse: 10.1933 - val_mae: 1.5174 - lr: 3.8702e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 16.3336 - mse: 16.3336 - mae: 1.5992 - val_loss: 10.2018 - val_mse: 10.2018 - val_mae: 1.5524 - lr: 3.8702e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 16.2691 - mse: 16.2691 - mae: 1.5983 - val_loss: 10.0860 - val_mse: 10.0860 - val_mae: 1.5675 - lr: 3.8702e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 16.2736 - mse: 16.2736 - mae: 1.5949 - val_loss: 10.0289 - val_mse: 10.0289 - val_mae: 1.5509 - lr: 3.8702e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 2s - loss: 16.2475 - mse: 16.2475 - mae: 1.5958 - val_loss: 10.1357 - val_mse: 10.1357 - val_mae: 1.5591 - lr: 3.8702e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 2s - loss: 16.2332 - mse: 16.2332 - mae: 1.5928 - val_loss: 10.0438 - val_mse: 10.0438 - val_mae: 1.5735 - lr: 3.8702e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 2s - loss: 16.2376 - mse: 16.2376 - mae: 1.5906 - val_loss: 10.0521 - val_mse: 10.0521 - val_mae: 1.5271 - lr: 3.8702e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 2s - loss: 16.2294 - mse: 16.2294 - mae: 1.5874 - val_loss: 9.9839 - val_mse: 9.9839 - val_mae: 1.5606 - lr: 3.8702e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 2s - loss: 16.1996 - mse: 16.1996 - mae: 1.5902 - val_loss: 10.1158 - val_mse: 10.1158 - val_mae: 1.5013 - lr: 3.8702e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 13/100\n",
            "1000/1000 - 2s - loss: 16.1807 - mse: 16.1807 - mae: 1.5859 - val_loss: 10.0458 - val_mse: 10.0458 - val_mae: 1.5549 - lr: 3.8702e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 14/100\n",
            "1000/1000 - 2s - loss: 16.1723 - mse: 16.1723 - mae: 1.5860 - val_loss: 10.1022 - val_mse: 10.1022 - val_mae: 1.5418 - lr: 3.8702e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 15/100\n",
            "1000/1000 - 2s - loss: 16.1815 - mse: 16.1815 - mae: 1.5860 - val_loss: 10.0425 - val_mse: 10.0425 - val_mae: 1.5627 - lr: 3.8702e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 16/100\n",
            "1000/1000 - 2s - loss: 16.1819 - mse: 16.1819 - mae: 1.5855 - val_loss: 9.9825 - val_mse: 9.9825 - val_mae: 1.5466 - lr: 3.8702e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 17/100\n",
            "1000/1000 - 2s - loss: 16.1349 - mse: 16.1349 - mae: 1.5857 - val_loss: 10.0699 - val_mse: 10.0699 - val_mae: 1.5652 - lr: 3.8702e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 18/100\n",
            "1000/1000 - 2s - loss: 16.1268 - mse: 16.1268 - mae: 1.5869 - val_loss: 10.0046 - val_mse: 10.0046 - val_mae: 1.5648 - lr: 3.8702e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 19/100\n",
            "1000/1000 - 2s - loss: 16.1277 - mse: 16.1277 - mae: 1.5832 - val_loss: 9.9844 - val_mse: 9.9844 - val_mae: 1.5554 - lr: 3.8702e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 20/100\n",
            "1000/1000 - 2s - loss: 16.1355 - mse: 16.1355 - mae: 1.5824 - val_loss: 9.9786 - val_mse: 9.9786 - val_mae: 1.5496 - lr: 3.8702e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 21/100\n",
            "1000/1000 - 2s - loss: 16.1011 - mse: 16.1011 - mae: 1.5825 - val_loss: 10.0319 - val_mse: 10.0319 - val_mae: 1.5220 - lr: 3.8702e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 22/100\n",
            "1000/1000 - 2s - loss: 16.1249 - mse: 16.1249 - mae: 1.5836 - val_loss: 9.9815 - val_mse: 9.9815 - val_mae: 1.5500 - lr: 3.8702e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 23/100\n",
            "1000/1000 - 2s - loss: 16.1012 - mse: 16.1012 - mae: 1.5810 - val_loss: 9.9871 - val_mse: 9.9871 - val_mae: 1.5415 - lr: 3.8702e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 24/100\n",
            "1000/1000 - 2s - loss: 16.0780 - mse: 16.0780 - mae: 1.5827 - val_loss: 10.0524 - val_mse: 10.0524 - val_mae: 1.5085 - lr: 3.8702e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 25/100\n",
            "1000/1000 - 2s - loss: 16.0898 - mse: 16.0898 - mae: 1.5796 - val_loss: 9.9945 - val_mse: 9.9945 - val_mae: 1.5488 - lr: 3.8702e-04 - 2s/epoch - 2ms/step\n",
            "Score for fold 1: loss of 9.994453430175781\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 15.0804 - mse: 15.0804 - mae: 1.5733 - val_loss: 14.1654 - val_mse: 14.1654 - val_mae: 1.5598 - lr: 3.8702e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 15.0671 - mse: 15.0671 - mae: 1.5738 - val_loss: 14.1689 - val_mse: 14.1689 - val_mae: 1.5701 - lr: 3.8702e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 15.0746 - mse: 15.0746 - mae: 1.5716 - val_loss: 14.1101 - val_mse: 14.1101 - val_mae: 1.5637 - lr: 3.8702e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 15.0653 - mse: 15.0653 - mae: 1.5723 - val_loss: 14.0927 - val_mse: 14.0927 - val_mae: 1.5701 - lr: 3.8702e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 15.0383 - mse: 15.0383 - mae: 1.5714 - val_loss: 14.2708 - val_mse: 14.2708 - val_mae: 1.5410 - lr: 3.8702e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 15.0482 - mse: 15.0482 - mae: 1.5707 - val_loss: 14.1835 - val_mse: 14.1835 - val_mae: 1.5589 - lr: 3.8702e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 15.0552 - mse: 15.0552 - mae: 1.5691 - val_loss: 14.0833 - val_mse: 14.0833 - val_mae: 1.5773 - lr: 3.8702e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 2s - loss: 15.0206 - mse: 15.0206 - mae: 1.5689 - val_loss: 14.1733 - val_mse: 14.1733 - val_mae: 1.5811 - lr: 3.8702e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 2s - loss: 15.0232 - mse: 15.0232 - mae: 1.5645 - val_loss: 14.0978 - val_mse: 14.0978 - val_mae: 1.5980 - lr: 3.8702e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 2s - loss: 15.0224 - mse: 15.0224 - mae: 1.5700 - val_loss: 14.2180 - val_mse: 14.2180 - val_mae: 1.5300 - lr: 3.8702e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 2s - loss: 15.0280 - mse: 15.0280 - mae: 1.5662 - val_loss: 14.1154 - val_mse: 14.1154 - val_mae: 1.5788 - lr: 3.8702e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 2s - loss: 15.0199 - mse: 15.0199 - mae: 1.5695 - val_loss: 14.1759 - val_mse: 14.1759 - val_mae: 1.5318 - lr: 3.8702e-04 - 2s/epoch - 2ms/step\n",
            "Score for fold 2: loss of 14.175944328308105\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 12.3715 - mse: 12.3715 - mae: 1.5589 - val_loss: 24.6693 - val_mse: 24.6693 - val_mae: 1.6466 - lr: 3.8702e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 12.3672 - mse: 12.3672 - mae: 1.5617 - val_loss: 24.7509 - val_mse: 24.7509 - val_mae: 1.6104 - lr: 3.8702e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 12.3281 - mse: 12.3281 - mae: 1.5573 - val_loss: 24.9000 - val_mse: 24.9000 - val_mae: 1.5930 - lr: 3.8702e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 12.3454 - mse: 12.3454 - mae: 1.5583 - val_loss: 24.7537 - val_mse: 24.7537 - val_mae: 1.6210 - lr: 3.8702e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 12.3334 - mse: 12.3334 - mae: 1.5588 - val_loss: 24.8092 - val_mse: 24.8092 - val_mae: 1.6281 - lr: 3.8702e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 12.3201 - mse: 12.3201 - mae: 1.5577 - val_loss: 24.7898 - val_mse: 24.7898 - val_mae: 1.6404 - lr: 3.8702e-04 - 2s/epoch - 2ms/step\n",
            "Score for fold 3: loss of 24.7897891998291\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 15.4383 - mse: 15.4383 - mae: 1.5747 - val_loss: 12.4771 - val_mse: 12.4771 - val_mae: 1.5138 - lr: 3.8702e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 15.4222 - mse: 15.4222 - mae: 1.5690 - val_loss: 12.4255 - val_mse: 12.4255 - val_mae: 1.5893 - lr: 3.8702e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 15.3896 - mse: 15.3896 - mae: 1.5706 - val_loss: 12.5152 - val_mse: 12.5152 - val_mae: 1.5347 - lr: 3.8702e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 15.3971 - mse: 15.3971 - mae: 1.5718 - val_loss: 12.4945 - val_mse: 12.4945 - val_mae: 1.5499 - lr: 3.8702e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 15.3918 - mse: 15.3918 - mae: 1.5700 - val_loss: 12.4780 - val_mse: 12.4780 - val_mae: 1.5945 - lr: 3.8702e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 15.3818 - mse: 15.3818 - mae: 1.5682 - val_loss: 12.4770 - val_mse: 12.4770 - val_mae: 1.5691 - lr: 3.8702e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 15.3772 - mse: 15.3772 - mae: 1.5713 - val_loss: 12.5453 - val_mse: 12.5453 - val_mae: 1.5595 - lr: 3.8702e-04 - 2s/epoch - 2ms/step\n",
            "Score for fold 4: loss of 12.54526424407959\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 15.3035 - mse: 15.3035 - mae: 1.5710 - val_loss: 12.9212 - val_mse: 12.9212 - val_mae: 1.5957 - lr: 3.8702e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 15.2909 - mse: 15.2909 - mae: 1.5696 - val_loss: 12.8811 - val_mse: 12.8811 - val_mae: 1.5838 - lr: 3.8702e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 15.2840 - mse: 15.2840 - mae: 1.5658 - val_loss: 12.9478 - val_mse: 12.9478 - val_mae: 1.5674 - lr: 3.8702e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 15.2962 - mse: 15.2962 - mae: 1.5663 - val_loss: 12.8825 - val_mse: 12.8825 - val_mae: 1.5477 - lr: 3.8702e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 15.2714 - mse: 15.2714 - mae: 1.5676 - val_loss: 12.8513 - val_mse: 12.8513 - val_mae: 1.5985 - lr: 3.8702e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 15.2770 - mse: 15.2770 - mae: 1.5649 - val_loss: 12.8365 - val_mse: 12.8365 - val_mae: 1.6325 - lr: 3.8702e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 15.2483 - mse: 15.2483 - mae: 1.5675 - val_loss: 12.8890 - val_mse: 12.8890 - val_mae: 1.5800 - lr: 3.8702e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 2s - loss: 15.2605 - mse: 15.2605 - mae: 1.5656 - val_loss: 12.9330 - val_mse: 12.9330 - val_mae: 1.5701 - lr: 3.8702e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 2s - loss: 15.2707 - mse: 15.2707 - mae: 1.5650 - val_loss: 12.9152 - val_mse: 12.9152 - val_mae: 1.5687 - lr: 3.8702e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 2s - loss: 15.2659 - mse: 15.2659 - mae: 1.5633 - val_loss: 13.0998 - val_mse: 13.0998 - val_mae: 1.5473 - lr: 3.8702e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 2s - loss: 15.2611 - mse: 15.2611 - mae: 1.5647 - val_loss: 12.9630 - val_mse: 12.9630 - val_mae: 1.5713 - lr: 3.8702e-04 - 2s/epoch - 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-10-15 13:07:02,551]\u001b[0m Finished trial#23 resulted in value: 14.894. Current best value is 14.618 with parameters: {'activation': 'relu', 'num_hidden_layer': 4, 'i': 10, 'learning_rate': 0.0001995902281877497}.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Score for fold 5: loss of 12.962990760803223\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 41s - loss: 15.6952 - mse: 15.6952 - mae: 1.6309 - val_loss: 14.7707 - val_mse: 14.7707 - val_mae: 1.5946 - lr: 8.1999e-04 - 41s/epoch - 41ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 39s - loss: 15.3488 - mse: 15.3488 - mae: 1.6047 - val_loss: 14.5348 - val_mse: 14.5348 - val_mae: 1.6640 - lr: 8.1999e-04 - 39s/epoch - 39ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 39s - loss: 15.3505 - mse: 15.3505 - mae: 1.5974 - val_loss: 14.2390 - val_mse: 14.2390 - val_mae: 1.6830 - lr: 8.1999e-04 - 39s/epoch - 39ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 39s - loss: 15.2269 - mse: 15.2269 - mae: 1.5875 - val_loss: 15.1853 - val_mse: 15.1853 - val_mae: 1.5669 - lr: 8.1999e-04 - 39s/epoch - 39ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 41s - loss: 15.2224 - mse: 15.2224 - mae: 1.5884 - val_loss: 14.9728 - val_mse: 14.9728 - val_mae: 1.5002 - lr: 8.1999e-04 - 41s/epoch - 41ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 39s - loss: 15.2117 - mse: 15.2117 - mae: 1.5957 - val_loss: 14.4756 - val_mse: 14.4756 - val_mae: 1.6335 - lr: 8.1999e-04 - 39s/epoch - 39ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 39s - loss: 15.3140 - mse: 15.3140 - mae: 1.5970 - val_loss: 14.5849 - val_mse: 14.5849 - val_mae: 1.7249 - lr: 8.1999e-04 - 39s/epoch - 39ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 39s - loss: 15.2819 - mse: 15.2819 - mae: 1.5979 - val_loss: 14.4600 - val_mse: 14.4600 - val_mae: 1.5340 - lr: 8.1999e-04 - 39s/epoch - 39ms/step\n",
            "Score for fold 1: loss of 14.459986686706543\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 42s - loss: 15.3410 - mse: 15.3410 - mae: 1.6001 - val_loss: 14.0485 - val_mse: 14.0485 - val_mae: 1.5007 - lr: 8.1999e-04 - 42s/epoch - 42ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 41s - loss: 15.3286 - mse: 15.3286 - mae: 1.5977 - val_loss: 14.3404 - val_mse: 14.3404 - val_mae: 1.5826 - lr: 8.1999e-04 - 41s/epoch - 41ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 41s - loss: 15.2587 - mse: 15.2587 - mae: 1.5931 - val_loss: 14.4109 - val_mse: 14.4109 - val_mae: 1.6850 - lr: 8.1999e-04 - 41s/epoch - 41ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 42s - loss: 15.3092 - mse: 15.3092 - mae: 1.5898 - val_loss: 14.3641 - val_mse: 14.3641 - val_mae: 1.5081 - lr: 8.1999e-04 - 42s/epoch - 42ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 42s - loss: 15.2580 - mse: 15.2580 - mae: 1.5882 - val_loss: 14.0172 - val_mse: 14.0172 - val_mae: 1.5904 - lr: 8.1999e-04 - 42s/epoch - 42ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 41s - loss: 15.1944 - mse: 15.1944 - mae: 1.5872 - val_loss: 14.3706 - val_mse: 14.3706 - val_mae: 1.5518 - lr: 8.1999e-04 - 41s/epoch - 41ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 41s - loss: 14.9953 - mse: 14.9953 - mae: 1.5847 - val_loss: 14.2702 - val_mse: 14.2702 - val_mae: 1.5488 - lr: 8.1999e-04 - 41s/epoch - 41ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 41s - loss: 15.2641 - mse: 15.2641 - mae: 1.5824 - val_loss: 14.2360 - val_mse: 14.2360 - val_mae: 1.5401 - lr: 8.1999e-04 - 41s/epoch - 41ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 43s - loss: 15.1527 - mse: 15.1527 - mae: 1.5905 - val_loss: 14.1486 - val_mse: 14.1486 - val_mae: 1.5153 - lr: 8.1999e-04 - 43s/epoch - 43ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 40s - loss: 15.0105 - mse: 15.0105 - mae: 1.5828 - val_loss: 13.9375 - val_mse: 13.9375 - val_mae: 1.6233 - lr: 8.1999e-04 - 40s/epoch - 40ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 40s - loss: 15.0165 - mse: 15.0165 - mae: 1.5816 - val_loss: 14.2889 - val_mse: 14.2889 - val_mae: 1.5879 - lr: 8.1999e-04 - 40s/epoch - 40ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 41s - loss: 15.1418 - mse: 15.1418 - mae: 1.5899 - val_loss: 14.1122 - val_mse: 14.1122 - val_mae: 1.6615 - lr: 8.1999e-04 - 41s/epoch - 41ms/step\n",
            "Epoch 13/100\n",
            "1000/1000 - 42s - loss: 15.0229 - mse: 15.0229 - mae: 1.5797 - val_loss: 14.2510 - val_mse: 14.2510 - val_mae: 1.6090 - lr: 8.1999e-04 - 42s/epoch - 42ms/step\n",
            "Epoch 14/100\n",
            "1000/1000 - 41s - loss: 15.0966 - mse: 15.0966 - mae: 1.5773 - val_loss: 14.0212 - val_mse: 14.0212 - val_mae: 1.5939 - lr: 8.1999e-04 - 41s/epoch - 41ms/step\n",
            "Epoch 15/100\n",
            "1000/1000 - 41s - loss: 15.1083 - mse: 15.1083 - mae: 1.5785 - val_loss: 14.1195 - val_mse: 14.1195 - val_mae: 1.6521 - lr: 8.1999e-04 - 41s/epoch - 41ms/step\n",
            "Score for fold 2: loss of 14.119531631469727\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 42s - loss: 15.0690 - mse: 15.0690 - mae: 1.5829 - val_loss: 13.4296 - val_mse: 13.4296 - val_mae: 1.5863 - lr: 8.1999e-04 - 42s/epoch - 42ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 41s - loss: 15.1198 - mse: 15.1198 - mae: 1.5802 - val_loss: 13.6993 - val_mse: 13.6993 - val_mae: 1.5973 - lr: 8.1999e-04 - 41s/epoch - 41ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 41s - loss: 15.0653 - mse: 15.0653 - mae: 1.5844 - val_loss: 13.9979 - val_mse: 13.9979 - val_mae: 1.5967 - lr: 8.1999e-04 - 41s/epoch - 41ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 41s - loss: 15.0719 - mse: 15.0719 - mae: 1.5848 - val_loss: 14.5910 - val_mse: 14.5910 - val_mae: 1.7610 - lr: 8.1999e-04 - 41s/epoch - 41ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 42s - loss: 15.1324 - mse: 15.1324 - mae: 1.5933 - val_loss: 14.1810 - val_mse: 14.1810 - val_mae: 1.6479 - lr: 8.1999e-04 - 42s/epoch - 42ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 42s - loss: 15.2257 - mse: 15.2257 - mae: 1.6038 - val_loss: 14.3966 - val_mse: 14.3966 - val_mae: 1.8867 - lr: 8.1999e-04 - 42s/epoch - 42ms/step\n",
            "Score for fold 3: loss of 14.396602630615234\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 41s - loss: 16.1349 - mse: 16.1349 - mae: 1.6122 - val_loss: 10.4707 - val_mse: 10.4707 - val_mae: 1.5739 - lr: 8.1999e-04 - 41s/epoch - 41ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 40s - loss: 16.0880 - mse: 16.0880 - mae: 1.6144 - val_loss: 10.5450 - val_mse: 10.5450 - val_mae: 1.6819 - lr: 8.1999e-04 - 40s/epoch - 40ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 42s - loss: 16.0551 - mse: 16.0551 - mae: 1.6272 - val_loss: 10.6748 - val_mse: 10.6748 - val_mae: 1.7150 - lr: 8.1999e-04 - 42s/epoch - 42ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 40s - loss: 16.0265 - mse: 16.0265 - mae: 1.6226 - val_loss: 10.7834 - val_mse: 10.7834 - val_mae: 1.6162 - lr: 8.1999e-04 - 40s/epoch - 40ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 41s - loss: 15.8692 - mse: 15.8692 - mae: 1.6203 - val_loss: 10.6636 - val_mse: 10.6636 - val_mae: 1.5496 - lr: 8.1999e-04 - 41s/epoch - 41ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 40s - loss: 15.9743 - mse: 15.9743 - mae: 1.6485 - val_loss: 11.0414 - val_mse: 11.0414 - val_mae: 1.7978 - lr: 8.1999e-04 - 40s/epoch - 40ms/step\n",
            "Score for fold 4: loss of 11.041394233703613\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 41s - loss: 13.3198 - mse: 13.3198 - mae: 1.6429 - val_loss: 22.0460 - val_mse: 22.0460 - val_mae: 1.6981 - lr: 8.1999e-04 - 41s/epoch - 41ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 41s - loss: 13.3239 - mse: 13.3239 - mae: 1.6503 - val_loss: 21.7309 - val_mse: 21.7309 - val_mae: 1.5988 - lr: 8.1999e-04 - 41s/epoch - 41ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 40s - loss: 13.4530 - mse: 13.4530 - mae: 1.6497 - val_loss: 21.8753 - val_mse: 21.8753 - val_mae: 1.4969 - lr: 8.1999e-04 - 40s/epoch - 40ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 43s - loss: 13.3206 - mse: 13.3206 - mae: 1.6425 - val_loss: 21.5621 - val_mse: 21.5621 - val_mae: 1.5914 - lr: 8.1999e-04 - 43s/epoch - 43ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 41s - loss: 13.0414 - mse: 13.0414 - mae: 1.6431 - val_loss: 22.1262 - val_mse: 22.1262 - val_mae: 1.5267 - lr: 8.1999e-04 - 41s/epoch - 41ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 41s - loss: 13.4439 - mse: 13.4439 - mae: 1.6577 - val_loss: 21.8062 - val_mse: 21.8062 - val_mae: 1.6387 - lr: 8.1999e-04 - 41s/epoch - 41ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 41s - loss: 13.3390 - mse: 13.3390 - mae: 1.6639 - val_loss: 21.9345 - val_mse: 21.9345 - val_mae: 1.5773 - lr: 8.1999e-04 - 41s/epoch - 41ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 42s - loss: 13.4145 - mse: 13.4145 - mae: 1.6574 - val_loss: 22.0575 - val_mse: 22.0575 - val_mae: 1.8841 - lr: 8.1999e-04 - 42s/epoch - 42ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 41s - loss: 13.2051 - mse: 13.2051 - mae: 1.6895 - val_loss: 21.9382 - val_mse: 21.9382 - val_mae: 1.7223 - lr: 8.1999e-04 - 41s/epoch - 41ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-10-15 13:38:03,336]\u001b[0m Finished trial#24 resulted in value: 15.191999999999998. Current best value is 14.618 with parameters: {'activation': 'relu', 'num_hidden_layer': 4, 'i': 10, 'learning_rate': 0.0001995902281877497}.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Score for fold 5: loss of 21.93817138671875\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 7s - loss: 16.5969 - mse: 16.5969 - mae: 1.6233 - val_loss: 11.5084 - val_mse: 11.5084 - val_mae: 1.5462 - lr: 1.3013e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 6s - loss: 16.0282 - mse: 16.0282 - mae: 1.5969 - val_loss: 11.4932 - val_mse: 11.4932 - val_mae: 1.6013 - lr: 1.3013e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 6s - loss: 15.9391 - mse: 15.9391 - mae: 1.5935 - val_loss: 11.4441 - val_mse: 11.4441 - val_mae: 1.6253 - lr: 1.3013e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 6s - loss: 15.9472 - mse: 15.9472 - mae: 1.5938 - val_loss: 11.3584 - val_mse: 11.3584 - val_mae: 1.5553 - lr: 1.3013e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 6s - loss: 15.9209 - mse: 15.9209 - mae: 1.5876 - val_loss: 11.4527 - val_mse: 11.4527 - val_mae: 1.5811 - lr: 1.3013e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 6s - loss: 15.8968 - mse: 15.8968 - mae: 1.5869 - val_loss: 11.4041 - val_mse: 11.4041 - val_mae: 1.5408 - lr: 1.3013e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 6s - loss: 15.8611 - mse: 15.8611 - mae: 1.5823 - val_loss: 11.2747 - val_mse: 11.2747 - val_mae: 1.6040 - lr: 1.3013e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 7s - loss: 15.8369 - mse: 15.8369 - mae: 1.5786 - val_loss: 11.3041 - val_mse: 11.3041 - val_mae: 1.5830 - lr: 1.3013e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 8s - loss: 15.7542 - mse: 15.7542 - mae: 1.5806 - val_loss: 11.3605 - val_mse: 11.3605 - val_mae: 1.5285 - lr: 1.3013e-04 - 8s/epoch - 8ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 7s - loss: 15.7742 - mse: 15.7742 - mae: 1.5785 - val_loss: 11.2801 - val_mse: 11.2801 - val_mae: 1.5529 - lr: 1.3013e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 7s - loss: 15.7091 - mse: 15.7091 - mae: 1.5784 - val_loss: 11.2999 - val_mse: 11.2999 - val_mae: 1.5394 - lr: 1.3013e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 7s - loss: 15.7599 - mse: 15.7599 - mae: 1.5798 - val_loss: 11.2301 - val_mse: 11.2301 - val_mae: 1.5797 - lr: 1.3013e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 13/100\n",
            "1000/1000 - 7s - loss: 15.7368 - mse: 15.7368 - mae: 1.5757 - val_loss: 11.3680 - val_mse: 11.3680 - val_mae: 1.5318 - lr: 1.3013e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 14/100\n",
            "1000/1000 - 7s - loss: 15.7190 - mse: 15.7190 - mae: 1.5738 - val_loss: 11.2013 - val_mse: 11.2013 - val_mae: 1.5670 - lr: 1.3013e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 15/100\n",
            "1000/1000 - 9s - loss: 15.6908 - mse: 15.6908 - mae: 1.5738 - val_loss: 11.2164 - val_mse: 11.2164 - val_mae: 1.5472 - lr: 1.3013e-04 - 9s/epoch - 9ms/step\n",
            "Epoch 16/100\n",
            "1000/1000 - 7s - loss: 15.7122 - mse: 15.7122 - mae: 1.5699 - val_loss: 11.2230 - val_mse: 11.2230 - val_mae: 1.5982 - lr: 1.3013e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 17/100\n",
            "1000/1000 - 6s - loss: 15.6537 - mse: 15.6537 - mae: 1.5712 - val_loss: 11.2380 - val_mse: 11.2380 - val_mae: 1.5664 - lr: 1.3013e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 18/100\n",
            "1000/1000 - 7s - loss: 15.6612 - mse: 15.6612 - mae: 1.5719 - val_loss: 11.2982 - val_mse: 11.2982 - val_mae: 1.5306 - lr: 1.3013e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 19/100\n",
            "1000/1000 - 6s - loss: 15.6517 - mse: 15.6517 - mae: 1.5681 - val_loss: 11.2466 - val_mse: 11.2466 - val_mae: 1.5306 - lr: 1.3013e-04 - 6s/epoch - 6ms/step\n",
            "Score for fold 1: loss of 11.246630668640137\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 7s - loss: 15.5302 - mse: 15.5302 - mae: 1.5727 - val_loss: 11.8541 - val_mse: 11.8541 - val_mae: 1.5453 - lr: 1.3013e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 6s - loss: 15.4998 - mse: 15.4998 - mae: 1.5731 - val_loss: 11.7708 - val_mse: 11.7708 - val_mae: 1.5325 - lr: 1.3013e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 6s - loss: 15.4596 - mse: 15.4596 - mae: 1.5710 - val_loss: 11.9533 - val_mse: 11.9533 - val_mae: 1.5054 - lr: 1.3013e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 6s - loss: 15.4556 - mse: 15.4556 - mae: 1.5693 - val_loss: 11.8909 - val_mse: 11.8909 - val_mae: 1.5712 - lr: 1.3013e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 6s - loss: 15.4871 - mse: 15.4871 - mae: 1.5676 - val_loss: 11.9049 - val_mse: 11.9049 - val_mae: 1.6462 - lr: 1.3013e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 6s - loss: 15.4461 - mse: 15.4461 - mae: 1.5674 - val_loss: 11.9098 - val_mse: 11.9098 - val_mae: 1.5658 - lr: 1.3013e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 7s - loss: 15.4229 - mse: 15.4229 - mae: 1.5662 - val_loss: 12.0146 - val_mse: 12.0146 - val_mae: 1.5385 - lr: 1.3013e-04 - 7s/epoch - 7ms/step\n",
            "Score for fold 2: loss of 12.014656066894531\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 7s - loss: 13.3711 - mse: 13.3711 - mae: 1.5472 - val_loss: 19.9183 - val_mse: 19.9183 - val_mae: 1.5790 - lr: 1.3013e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 6s - loss: 13.4249 - mse: 13.4249 - mae: 1.5412 - val_loss: 19.8953 - val_mse: 19.8953 - val_mae: 1.6174 - lr: 1.3013e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 6s - loss: 13.3705 - mse: 13.3705 - mae: 1.5424 - val_loss: 19.9142 - val_mse: 19.9142 - val_mae: 1.6252 - lr: 1.3013e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 6s - loss: 13.3371 - mse: 13.3371 - mae: 1.5382 - val_loss: 19.9978 - val_mse: 19.9978 - val_mae: 1.6269 - lr: 1.3013e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 6s - loss: 13.3523 - mse: 13.3523 - mae: 1.5396 - val_loss: 20.0234 - val_mse: 20.0234 - val_mae: 1.6266 - lr: 1.3013e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 6s - loss: 13.2942 - mse: 13.2942 - mae: 1.5398 - val_loss: 19.9747 - val_mse: 19.9747 - val_mae: 1.6270 - lr: 1.3013e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 6s - loss: 13.3113 - mse: 13.3113 - mae: 1.5374 - val_loss: 20.0371 - val_mse: 20.0371 - val_mae: 1.6166 - lr: 1.3013e-04 - 6s/epoch - 6ms/step\n",
            "Score for fold 3: loss of 20.03709602355957\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 7s - loss: 13.0587 - mse: 13.0587 - mae: 1.5468 - val_loss: 21.1179 - val_mse: 21.1179 - val_mae: 1.5655 - lr: 1.3013e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 6s - loss: 13.0219 - mse: 13.0219 - mae: 1.5464 - val_loss: 21.1368 - val_mse: 21.1368 - val_mae: 1.6345 - lr: 1.3013e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 6s - loss: 13.0216 - mse: 13.0216 - mae: 1.5440 - val_loss: 21.3259 - val_mse: 21.3259 - val_mae: 1.5676 - lr: 1.3013e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 6s - loss: 12.9947 - mse: 12.9947 - mae: 1.5431 - val_loss: 21.2824 - val_mse: 21.2824 - val_mae: 1.5703 - lr: 1.3013e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 7s - loss: 12.9496 - mse: 12.9496 - mae: 1.5451 - val_loss: 21.1826 - val_mse: 21.1826 - val_mae: 1.6090 - lr: 1.3013e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 6s - loss: 12.9361 - mse: 12.9361 - mae: 1.5435 - val_loss: 21.3690 - val_mse: 21.3690 - val_mae: 1.5872 - lr: 1.3013e-04 - 6s/epoch - 6ms/step\n",
            "Score for fold 4: loss of 21.368982315063477\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 6s - loss: 15.9347 - mse: 15.9347 - mae: 1.5670 - val_loss: 9.3150 - val_mse: 9.3150 - val_mae: 1.5367 - lr: 1.3013e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 6s - loss: 15.8698 - mse: 15.8698 - mae: 1.5685 - val_loss: 9.3456 - val_mse: 9.3456 - val_mae: 1.4545 - lr: 1.3013e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 7s - loss: 15.8588 - mse: 15.8588 - mae: 1.5622 - val_loss: 9.4233 - val_mse: 9.4233 - val_mae: 1.5194 - lr: 1.3013e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 6s - loss: 15.9012 - mse: 15.9012 - mae: 1.5639 - val_loss: 9.4883 - val_mse: 9.4883 - val_mae: 1.4899 - lr: 1.3013e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 6s - loss: 15.8707 - mse: 15.8707 - mae: 1.5651 - val_loss: 9.3212 - val_mse: 9.3212 - val_mae: 1.5178 - lr: 1.3013e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 6s - loss: 15.8411 - mse: 15.8411 - mae: 1.5640 - val_loss: 9.5815 - val_mse: 9.5815 - val_mae: 1.4495 - lr: 1.3013e-04 - 6s/epoch - 6ms/step\n",
            "Score for fold 5: loss of 9.581506729125977\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-10-15 13:43:10,992]\u001b[0m Finished trial#25 resulted in value: 14.85. Current best value is 14.618 with parameters: {'activation': 'relu', 'num_hidden_layer': 4, 'i': 10, 'learning_rate': 0.0001995902281877497}.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 41s - loss: 16.4986 - mse: 16.4986 - mae: 1.6267 - val_loss: 11.1608 - val_mse: 11.1608 - val_mae: 1.5515 - lr: 3.5948e-04 - 41s/epoch - 41ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 40s - loss: 16.1273 - mse: 16.1273 - mae: 1.6062 - val_loss: 11.2098 - val_mse: 11.2098 - val_mae: 1.5325 - lr: 3.5948e-04 - 40s/epoch - 40ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 41s - loss: 16.0128 - mse: 16.0128 - mae: 1.6056 - val_loss: 11.2374 - val_mse: 11.2374 - val_mae: 1.5299 - lr: 3.5948e-04 - 41s/epoch - 41ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 42s - loss: 16.0341 - mse: 16.0341 - mae: 1.5932 - val_loss: 11.1428 - val_mse: 11.1428 - val_mae: 1.5064 - lr: 3.5948e-04 - 42s/epoch - 42ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 40s - loss: 16.0151 - mse: 16.0151 - mae: 1.5882 - val_loss: 11.2412 - val_mse: 11.2412 - val_mae: 1.4998 - lr: 3.5948e-04 - 40s/epoch - 40ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 40s - loss: 15.9422 - mse: 15.9422 - mae: 1.5881 - val_loss: 11.0744 - val_mse: 11.0744 - val_mae: 1.5784 - lr: 3.5948e-04 - 40s/epoch - 40ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 40s - loss: 15.8051 - mse: 15.8051 - mae: 1.5824 - val_loss: 11.0596 - val_mse: 11.0596 - val_mae: 1.5098 - lr: 3.5948e-04 - 40s/epoch - 40ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 42s - loss: 15.6997 - mse: 15.6997 - mae: 1.5770 - val_loss: 11.1039 - val_mse: 11.1039 - val_mae: 1.5494 - lr: 3.5948e-04 - 42s/epoch - 42ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 40s - loss: 15.7201 - mse: 15.7201 - mae: 1.5846 - val_loss: 11.2515 - val_mse: 11.2515 - val_mae: 1.5750 - lr: 3.5948e-04 - 40s/epoch - 40ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 41s - loss: 15.7644 - mse: 15.7644 - mae: 1.5899 - val_loss: 11.1280 - val_mse: 11.1280 - val_mae: 1.6574 - lr: 3.5948e-04 - 41s/epoch - 41ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 40s - loss: 15.6373 - mse: 15.6373 - mae: 1.5865 - val_loss: 11.0827 - val_mse: 11.0827 - val_mae: 1.6373 - lr: 3.5948e-04 - 40s/epoch - 40ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 41s - loss: 15.6184 - mse: 15.6184 - mae: 1.5843 - val_loss: 11.2078 - val_mse: 11.2078 - val_mae: 1.5508 - lr: 3.5948e-04 - 41s/epoch - 41ms/step\n",
            "Score for fold 1: loss of 11.207762718200684\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 39s - loss: 15.1815 - mse: 15.1815 - mae: 1.5918 - val_loss: 12.8356 - val_mse: 12.8356 - val_mae: 1.5365 - lr: 3.5948e-04 - 39s/epoch - 39ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 40s - loss: 15.0926 - mse: 15.0926 - mae: 1.5833 - val_loss: 13.3027 - val_mse: 13.3027 - val_mae: 1.6132 - lr: 3.5948e-04 - 40s/epoch - 40ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 40s - loss: 15.0696 - mse: 15.0696 - mae: 1.5842 - val_loss: 12.7571 - val_mse: 12.7571 - val_mae: 1.5688 - lr: 3.5948e-04 - 40s/epoch - 40ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 41s - loss: 15.1255 - mse: 15.1255 - mae: 1.5859 - val_loss: 13.1568 - val_mse: 13.1568 - val_mae: 1.7420 - lr: 3.5948e-04 - 41s/epoch - 41ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 40s - loss: 14.9766 - mse: 14.9766 - mae: 1.5841 - val_loss: 12.9991 - val_mse: 12.9991 - val_mae: 1.5075 - lr: 3.5948e-04 - 40s/epoch - 40ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 40s - loss: 15.0570 - mse: 15.0570 - mae: 1.5954 - val_loss: 13.5324 - val_mse: 13.5324 - val_mae: 1.9036 - lr: 3.5948e-04 - 40s/epoch - 40ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 40s - loss: 15.0607 - mse: 15.0607 - mae: 1.5803 - val_loss: 13.2566 - val_mse: 13.2566 - val_mae: 1.6995 - lr: 3.5948e-04 - 40s/epoch - 40ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 41s - loss: 14.9454 - mse: 14.9454 - mae: 1.5779 - val_loss: 12.9918 - val_mse: 12.9918 - val_mae: 1.5891 - lr: 3.5948e-04 - 41s/epoch - 41ms/step\n",
            "Score for fold 2: loss of 12.99180793762207\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 40s - loss: 14.4212 - mse: 14.4212 - mae: 1.5842 - val_loss: 15.3196 - val_mse: 15.3196 - val_mae: 1.5384 - lr: 3.5948e-04 - 40s/epoch - 40ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 40s - loss: 14.3618 - mse: 14.3618 - mae: 1.5834 - val_loss: 15.1025 - val_mse: 15.1025 - val_mae: 1.4819 - lr: 3.5948e-04 - 40s/epoch - 40ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 40s - loss: 14.2789 - mse: 14.2789 - mae: 1.5898 - val_loss: 14.8863 - val_mse: 14.8863 - val_mae: 1.5080 - lr: 3.5948e-04 - 40s/epoch - 40ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 42s - loss: 14.3512 - mse: 14.3512 - mae: 1.5834 - val_loss: 14.8054 - val_mse: 14.8054 - val_mae: 1.5546 - lr: 3.5948e-04 - 42s/epoch - 42ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 40s - loss: 14.0463 - mse: 14.0463 - mae: 1.5821 - val_loss: 15.0214 - val_mse: 15.0214 - val_mae: 1.7245 - lr: 3.5948e-04 - 40s/epoch - 40ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 40s - loss: 14.1001 - mse: 14.1001 - mae: 1.5803 - val_loss: 15.1561 - val_mse: 15.1561 - val_mae: 1.4954 - lr: 3.5948e-04 - 40s/epoch - 40ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 40s - loss: 14.0268 - mse: 14.0268 - mae: 1.5743 - val_loss: 15.5861 - val_mse: 15.5861 - val_mae: 1.7953 - lr: 3.5948e-04 - 40s/epoch - 40ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 42s - loss: 14.0705 - mse: 14.0705 - mae: 1.5907 - val_loss: 15.6519 - val_mse: 15.6519 - val_mae: 1.5161 - lr: 3.5948e-04 - 42s/epoch - 42ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 40s - loss: 14.0485 - mse: 14.0485 - mae: 1.5840 - val_loss: 15.3458 - val_mse: 15.3458 - val_mae: 1.6529 - lr: 3.5948e-04 - 40s/epoch - 40ms/step\n",
            "Score for fold 3: loss of 15.345820426940918\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 39s - loss: 12.5883 - mse: 12.5883 - mae: 1.5900 - val_loss: 21.2026 - val_mse: 21.2026 - val_mae: 1.5878 - lr: 3.5948e-04 - 39s/epoch - 39ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 39s - loss: 12.6185 - mse: 12.6185 - mae: 1.5854 - val_loss: 21.2077 - val_mse: 21.2077 - val_mae: 1.6581 - lr: 3.5948e-04 - 39s/epoch - 39ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 40s - loss: 12.6421 - mse: 12.6421 - mae: 1.5912 - val_loss: 21.8680 - val_mse: 21.8680 - val_mae: 1.4586 - lr: 3.5948e-04 - 40s/epoch - 40ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 40s - loss: 12.4856 - mse: 12.4856 - mae: 1.5936 - val_loss: 21.4858 - val_mse: 21.4858 - val_mae: 1.5437 - lr: 3.5948e-04 - 40s/epoch - 40ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 40s - loss: 12.4857 - mse: 12.4857 - mae: 1.5829 - val_loss: 21.2371 - val_mse: 21.2371 - val_mae: 1.6529 - lr: 3.5948e-04 - 40s/epoch - 40ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 39s - loss: 12.4579 - mse: 12.4579 - mae: 1.5912 - val_loss: 21.9697 - val_mse: 21.9697 - val_mae: 1.6511 - lr: 3.5948e-04 - 39s/epoch - 39ms/step\n",
            "Score for fold 4: loss of 21.969655990600586\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 41s - loss: 14.8568 - mse: 14.8568 - mae: 1.6105 - val_loss: 12.5636 - val_mse: 12.5636 - val_mae: 1.5706 - lr: 3.5948e-04 - 41s/epoch - 41ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 40s - loss: 14.8853 - mse: 14.8853 - mae: 1.6140 - val_loss: 12.4232 - val_mse: 12.4232 - val_mae: 1.5261 - lr: 3.5948e-04 - 40s/epoch - 40ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 40s - loss: 14.7381 - mse: 14.7381 - mae: 1.6256 - val_loss: 12.4970 - val_mse: 12.4970 - val_mae: 1.5304 - lr: 3.5948e-04 - 40s/epoch - 40ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 40s - loss: 14.6600 - mse: 14.6600 - mae: 1.6120 - val_loss: 13.1125 - val_mse: 13.1125 - val_mae: 1.4947 - lr: 3.5948e-04 - 40s/epoch - 40ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 42s - loss: 14.8116 - mse: 14.8116 - mae: 1.6120 - val_loss: 12.8585 - val_mse: 12.8585 - val_mae: 1.5413 - lr: 3.5948e-04 - 42s/epoch - 42ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 39s - loss: 14.7035 - mse: 14.7035 - mae: 1.6340 - val_loss: 13.2548 - val_mse: 13.2548 - val_mae: 1.4537 - lr: 3.5948e-04 - 39s/epoch - 39ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 39s - loss: 14.7390 - mse: 14.7390 - mae: 1.6292 - val_loss: 13.8407 - val_mse: 13.8407 - val_mae: 2.1468 - lr: 3.5948e-04 - 39s/epoch - 39ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-10-15 14:12:40,791]\u001b[0m Finished trial#26 resulted in value: 15.072. Current best value is 14.618 with parameters: {'activation': 'relu', 'num_hidden_layer': 4, 'i': 10, 'learning_rate': 0.0001995902281877497}.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Score for fold 5: loss of 13.840697288513184\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 7s - loss: 13.1178 - mse: 13.1178 - mae: 1.6046 - val_loss: 25.3629 - val_mse: 25.3629 - val_mae: 1.6165 - lr: 2.3682e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 6s - loss: 12.6386 - mse: 12.6386 - mae: 1.5770 - val_loss: 25.2093 - val_mse: 25.2093 - val_mae: 1.6465 - lr: 2.3682e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 6s - loss: 12.5600 - mse: 12.5600 - mae: 1.5785 - val_loss: 25.0260 - val_mse: 25.0260 - val_mae: 1.6726 - lr: 2.3682e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 6s - loss: 12.5005 - mse: 12.5005 - mae: 1.5738 - val_loss: 25.3198 - val_mse: 25.3198 - val_mae: 1.6510 - lr: 2.3682e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 6s - loss: 12.4735 - mse: 12.4735 - mae: 1.5754 - val_loss: 25.0892 - val_mse: 25.0892 - val_mae: 1.5949 - lr: 2.3682e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 8s - loss: 12.4320 - mse: 12.4320 - mae: 1.5702 - val_loss: 25.0904 - val_mse: 25.0904 - val_mae: 1.6055 - lr: 2.3682e-04 - 8s/epoch - 8ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 6s - loss: 12.4260 - mse: 12.4260 - mae: 1.5682 - val_loss: 25.1471 - val_mse: 25.1471 - val_mae: 1.6422 - lr: 2.3682e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 6s - loss: 12.3549 - mse: 12.3549 - mae: 1.5688 - val_loss: 25.1574 - val_mse: 25.1574 - val_mae: 1.5838 - lr: 2.3682e-04 - 6s/epoch - 6ms/step\n",
            "Score for fold 1: loss of 25.15735626220703\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 6s - loss: 14.9212 - mse: 14.9212 - mae: 1.5787 - val_loss: 14.8710 - val_mse: 14.8710 - val_mae: 1.5947 - lr: 2.3682e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 6s - loss: 14.9067 - mse: 14.9067 - mae: 1.5789 - val_loss: 14.8904 - val_mse: 14.8904 - val_mae: 1.5602 - lr: 2.3682e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 6s - loss: 14.8139 - mse: 14.8139 - mae: 1.5753 - val_loss: 14.7850 - val_mse: 14.7850 - val_mae: 1.5736 - lr: 2.3682e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 6s - loss: 14.8064 - mse: 14.8064 - mae: 1.5722 - val_loss: 15.1322 - val_mse: 15.1322 - val_mae: 1.5017 - lr: 2.3682e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 6s - loss: 14.7699 - mse: 14.7699 - mae: 1.5724 - val_loss: 14.9215 - val_mse: 14.9215 - val_mae: 1.5958 - lr: 2.3682e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 6s - loss: 14.7257 - mse: 14.7257 - mae: 1.5690 - val_loss: 15.2231 - val_mse: 15.2231 - val_mae: 1.5752 - lr: 2.3682e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 6s - loss: 14.7346 - mse: 14.7346 - mae: 1.5669 - val_loss: 14.9533 - val_mse: 14.9533 - val_mae: 1.5349 - lr: 2.3682e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 6s - loss: 14.7193 - mse: 14.7193 - mae: 1.5677 - val_loss: 14.8041 - val_mse: 14.8041 - val_mae: 1.5868 - lr: 2.3682e-04 - 6s/epoch - 6ms/step\n",
            "Score for fold 2: loss of 14.804107666015625\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 6s - loss: 15.4957 - mse: 15.4957 - mae: 1.5704 - val_loss: 11.8372 - val_mse: 11.8372 - val_mae: 1.5168 - lr: 2.3682e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 6s - loss: 15.4794 - mse: 15.4794 - mae: 1.5679 - val_loss: 11.8347 - val_mse: 11.8347 - val_mae: 1.5343 - lr: 2.3682e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 6s - loss: 15.4216 - mse: 15.4216 - mae: 1.5644 - val_loss: 11.8503 - val_mse: 11.8503 - val_mae: 1.5889 - lr: 2.3682e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 6s - loss: 15.3899 - mse: 15.3899 - mae: 1.5644 - val_loss: 11.8628 - val_mse: 11.8628 - val_mae: 1.5810 - lr: 2.3682e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 6s - loss: 15.3925 - mse: 15.3925 - mae: 1.5659 - val_loss: 12.1517 - val_mse: 12.1517 - val_mae: 1.4993 - lr: 2.3682e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 6s - loss: 15.3625 - mse: 15.3625 - mae: 1.5616 - val_loss: 11.9989 - val_mse: 11.9989 - val_mae: 1.5281 - lr: 2.3682e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 6s - loss: 15.3857 - mse: 15.3857 - mae: 1.5615 - val_loss: 11.9465 - val_mse: 11.9465 - val_mae: 1.5671 - lr: 2.3682e-04 - 6s/epoch - 6ms/step\n",
            "Score for fold 3: loss of 11.946488380432129\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 6s - loss: 15.6947 - mse: 15.6947 - mae: 1.5560 - val_loss: 10.7481 - val_mse: 10.7481 - val_mae: 1.5767 - lr: 2.3682e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 6s - loss: 15.5477 - mse: 15.5477 - mae: 1.5528 - val_loss: 10.9032 - val_mse: 10.9032 - val_mae: 1.6785 - lr: 2.3682e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 6s - loss: 15.5972 - mse: 15.5972 - mae: 1.5549 - val_loss: 10.9745 - val_mse: 10.9745 - val_mae: 1.5777 - lr: 2.3682e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 6s - loss: 15.5842 - mse: 15.5842 - mae: 1.5550 - val_loss: 11.1459 - val_mse: 11.1459 - val_mae: 1.5365 - lr: 2.3682e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 7s - loss: 15.5590 - mse: 15.5590 - mae: 1.5521 - val_loss: 10.8737 - val_mse: 10.8737 - val_mae: 1.5934 - lr: 2.3682e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 6s - loss: 15.4742 - mse: 15.4742 - mae: 1.5497 - val_loss: 11.0445 - val_mse: 11.0445 - val_mae: 1.6140 - lr: 2.3682e-04 - 6s/epoch - 6ms/step\n",
            "Score for fold 4: loss of 11.044492721557617\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 6s - loss: 15.4228 - mse: 15.4228 - mae: 1.5608 - val_loss: 11.2745 - val_mse: 11.2745 - val_mae: 1.5377 - lr: 2.3682e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 6s - loss: 15.4258 - mse: 15.4258 - mae: 1.5575 - val_loss: 11.2572 - val_mse: 11.2572 - val_mae: 1.5396 - lr: 2.3682e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 8s - loss: 15.3134 - mse: 15.3134 - mae: 1.5575 - val_loss: 11.2451 - val_mse: 11.2451 - val_mae: 1.4967 - lr: 2.3682e-04 - 8s/epoch - 8ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 6s - loss: 15.4108 - mse: 15.4108 - mae: 1.5554 - val_loss: 11.4806 - val_mse: 11.4806 - val_mae: 1.4852 - lr: 2.3682e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 6s - loss: 15.3810 - mse: 15.3810 - mae: 1.5523 - val_loss: 11.3204 - val_mse: 11.3204 - val_mae: 1.5094 - lr: 2.3682e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 6s - loss: 15.2866 - mse: 15.2866 - mae: 1.5513 - val_loss: 11.2517 - val_mse: 11.2517 - val_mae: 1.5368 - lr: 2.3682e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 6s - loss: 15.2479 - mse: 15.2479 - mae: 1.5504 - val_loss: 11.1973 - val_mse: 11.1973 - val_mae: 1.5433 - lr: 2.3682e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 6s - loss: 15.3013 - mse: 15.3013 - mae: 1.5502 - val_loss: 11.3996 - val_mse: 11.3996 - val_mae: 1.5194 - lr: 2.3682e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 6s - loss: 15.2102 - mse: 15.2102 - mae: 1.5483 - val_loss: 11.2316 - val_mse: 11.2316 - val_mae: 1.5521 - lr: 2.3682e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 6s - loss: 15.2522 - mse: 15.2522 - mae: 1.5497 - val_loss: 11.3443 - val_mse: 11.3443 - val_mae: 1.5768 - lr: 2.3682e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 6s - loss: 15.1380 - mse: 15.1380 - mae: 1.5453 - val_loss: 11.2923 - val_mse: 11.2923 - val_mae: 1.5673 - lr: 2.3682e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 6s - loss: 15.1002 - mse: 15.1002 - mae: 1.5446 - val_loss: 11.1921 - val_mse: 11.1921 - val_mae: 1.5116 - lr: 2.3682e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 13/100\n",
            "1000/1000 - 6s - loss: 15.1707 - mse: 15.1707 - mae: 1.5464 - val_loss: 11.4425 - val_mse: 11.4425 - val_mae: 1.5327 - lr: 2.3682e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 14/100\n",
            "1000/1000 - 6s - loss: 15.1117 - mse: 15.1117 - mae: 1.5453 - val_loss: 11.4818 - val_mse: 11.4818 - val_mae: 1.5609 - lr: 2.3682e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 15/100\n",
            "1000/1000 - 6s - loss: 15.0991 - mse: 15.0991 - mae: 1.5434 - val_loss: 11.4277 - val_mse: 11.4277 - val_mae: 1.5572 - lr: 2.3682e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 16/100\n",
            "1000/1000 - 6s - loss: 15.0455 - mse: 15.0455 - mae: 1.5413 - val_loss: 11.4470 - val_mse: 11.4470 - val_mae: 1.5436 - lr: 2.3682e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 17/100\n",
            "1000/1000 - 6s - loss: 15.0324 - mse: 15.0324 - mae: 1.5389 - val_loss: 11.5375 - val_mse: 11.5375 - val_mae: 1.5552 - lr: 2.3682e-04 - 6s/epoch - 6ms/step\n",
            "Score for fold 5: loss of 11.537456512451172\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-10-15 14:18:14,627]\u001b[0m Finished trial#27 resulted in value: 14.898. Current best value is 14.618 with parameters: {'activation': 'relu', 'num_hidden_layer': 4, 'i': 10, 'learning_rate': 0.0001995902281877497}.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 6s - loss: 14.0177 - mse: 14.0177 - mae: 1.6120 - val_loss: 21.3818 - val_mse: 21.3818 - val_mae: 1.5173 - lr: 5.1034e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 5s - loss: 13.7073 - mse: 13.7073 - mae: 1.5940 - val_loss: 20.9050 - val_mse: 20.9050 - val_mae: 1.5897 - lr: 5.1034e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 5s - loss: 13.6676 - mse: 13.6676 - mae: 1.5929 - val_loss: 20.8600 - val_mse: 20.8600 - val_mae: 1.5674 - lr: 5.1034e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 4s - loss: 13.5134 - mse: 13.5134 - mae: 1.5837 - val_loss: 20.8242 - val_mse: 20.8242 - val_mae: 1.6001 - lr: 5.1034e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 5s - loss: 13.4807 - mse: 13.4807 - mae: 1.5865 - val_loss: 20.6196 - val_mse: 20.6196 - val_mae: 1.6192 - lr: 5.1034e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 5s - loss: 13.5568 - mse: 13.5568 - mae: 1.5810 - val_loss: 20.6560 - val_mse: 20.6560 - val_mae: 1.5842 - lr: 5.1034e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 5s - loss: 13.4756 - mse: 13.4756 - mae: 1.5766 - val_loss: 20.7604 - val_mse: 20.7604 - val_mae: 1.5786 - lr: 5.1034e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 4s - loss: 13.4742 - mse: 13.4742 - mae: 1.5751 - val_loss: 20.7950 - val_mse: 20.7950 - val_mae: 1.5462 - lr: 5.1034e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 5s - loss: 13.4047 - mse: 13.4047 - mae: 1.5765 - val_loss: 20.5172 - val_mse: 20.5172 - val_mae: 1.6663 - lr: 5.1034e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 7s - loss: 13.3980 - mse: 13.3980 - mae: 1.5758 - val_loss: 20.6021 - val_mse: 20.6021 - val_mae: 1.5775 - lr: 5.1034e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 4s - loss: 13.2919 - mse: 13.2919 - mae: 1.5744 - val_loss: 20.7448 - val_mse: 20.7448 - val_mae: 1.5526 - lr: 5.1034e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 5s - loss: 13.2290 - mse: 13.2290 - mae: 1.5687 - val_loss: 20.6567 - val_mse: 20.6567 - val_mae: 1.5730 - lr: 5.1034e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 13/100\n",
            "1000/1000 - 5s - loss: 13.1985 - mse: 13.1985 - mae: 1.5691 - val_loss: 20.7251 - val_mse: 20.7251 - val_mae: 1.5492 - lr: 5.1034e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 14/100\n",
            "1000/1000 - 5s - loss: 13.0860 - mse: 13.0860 - mae: 1.5714 - val_loss: 20.6206 - val_mse: 20.6206 - val_mae: 1.5246 - lr: 5.1034e-04 - 5s/epoch - 5ms/step\n",
            "Score for fold 1: loss of 20.62061309814453\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 5s - loss: 15.6221 - mse: 15.6221 - mae: 1.5909 - val_loss: 10.7362 - val_mse: 10.7362 - val_mae: 1.5594 - lr: 5.1034e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 4s - loss: 15.4707 - mse: 15.4707 - mae: 1.5903 - val_loss: 10.6734 - val_mse: 10.6734 - val_mae: 1.5027 - lr: 5.1034e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 5s - loss: 15.5141 - mse: 15.5141 - mae: 1.5918 - val_loss: 10.9934 - val_mse: 10.9934 - val_mae: 1.5561 - lr: 5.1034e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 5s - loss: 15.4924 - mse: 15.4924 - mae: 1.5877 - val_loss: 11.1978 - val_mse: 11.1978 - val_mae: 1.5703 - lr: 5.1034e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 5s - loss: 15.5068 - mse: 15.5068 - mae: 1.5938 - val_loss: 11.1426 - val_mse: 11.1426 - val_mae: 1.6507 - lr: 5.1034e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 5s - loss: 15.4676 - mse: 15.4676 - mae: 1.5885 - val_loss: 11.0298 - val_mse: 11.0298 - val_mae: 1.5263 - lr: 5.1034e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 4s - loss: 15.4306 - mse: 15.4306 - mae: 1.5947 - val_loss: 11.5104 - val_mse: 11.5104 - val_mae: 1.8828 - lr: 5.1034e-04 - 4s/epoch - 4ms/step\n",
            "Score for fold 2: loss of 11.510391235351562\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 5s - loss: 14.2083 - mse: 14.2083 - mae: 1.5718 - val_loss: 16.6033 - val_mse: 16.6033 - val_mae: 1.5450 - lr: 5.1034e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 5s - loss: 14.1452 - mse: 14.1452 - mae: 1.5715 - val_loss: 16.1465 - val_mse: 16.1465 - val_mae: 1.7448 - lr: 5.1034e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 4s - loss: 14.0472 - mse: 14.0472 - mae: 1.5747 - val_loss: 16.4719 - val_mse: 16.4719 - val_mae: 1.5540 - lr: 5.1034e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 5s - loss: 14.0463 - mse: 14.0463 - mae: 1.5794 - val_loss: 16.5845 - val_mse: 16.5845 - val_mae: 1.6671 - lr: 5.1034e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 5s - loss: 14.1230 - mse: 14.1230 - mae: 1.5884 - val_loss: 16.0985 - val_mse: 16.0985 - val_mae: 1.8358 - lr: 5.1034e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 5s - loss: 13.9510 - mse: 13.9510 - mae: 1.5802 - val_loss: 16.7355 - val_mse: 16.7355 - val_mae: 1.5238 - lr: 5.1034e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 5s - loss: 13.9826 - mse: 13.9826 - mae: 1.5831 - val_loss: 16.5942 - val_mse: 16.5942 - val_mae: 1.5975 - lr: 5.1034e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 5s - loss: 13.8537 - mse: 13.8537 - mae: 1.5882 - val_loss: 16.1765 - val_mse: 16.1765 - val_mae: 1.5401 - lr: 5.1034e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 4s - loss: 14.0036 - mse: 14.0036 - mae: 1.5939 - val_loss: 16.3859 - val_mse: 16.3859 - val_mae: 1.6325 - lr: 5.1034e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 5s - loss: 13.9396 - mse: 13.9396 - mae: 1.6034 - val_loss: 16.4489 - val_mse: 16.4489 - val_mae: 1.6489 - lr: 5.1034e-04 - 5s/epoch - 5ms/step\n",
            "Score for fold 3: loss of 16.44889259338379\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 5s - loss: 14.8635 - mse: 14.8635 - mae: 1.6327 - val_loss: 12.7633 - val_mse: 12.7633 - val_mae: 1.7911 - lr: 5.1034e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 7s - loss: 14.7580 - mse: 14.7580 - mae: 1.6275 - val_loss: 12.7317 - val_mse: 12.7317 - val_mae: 1.5615 - lr: 5.1034e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 5s - loss: 14.9042 - mse: 14.9042 - mae: 1.6406 - val_loss: 12.9563 - val_mse: 12.9563 - val_mae: 1.5362 - lr: 5.1034e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 5s - loss: 14.8453 - mse: 14.8453 - mae: 1.6472 - val_loss: 12.8343 - val_mse: 12.8343 - val_mae: 1.4598 - lr: 5.1034e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 5s - loss: 14.8179 - mse: 14.8179 - mae: 1.6521 - val_loss: 13.0618 - val_mse: 13.0618 - val_mae: 1.6679 - lr: 5.1034e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 5s - loss: 14.6179 - mse: 14.6179 - mae: 1.6671 - val_loss: 12.9710 - val_mse: 12.9710 - val_mae: 1.5456 - lr: 5.1034e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 5s - loss: 14.8489 - mse: 14.8489 - mae: 1.6863 - val_loss: 14.5409 - val_mse: 14.5409 - val_mae: 1.6760 - lr: 5.1034e-04 - 5s/epoch - 5ms/step\n",
            "Score for fold 4: loss of 14.540858268737793\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 5s - loss: 14.9498 - mse: 14.9498 - mae: 1.6897 - val_loss: 12.4859 - val_mse: 12.4859 - val_mae: 1.8574 - lr: 5.1034e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 4s - loss: 14.8012 - mse: 14.8012 - mae: 1.6970 - val_loss: 13.4737 - val_mse: 13.4737 - val_mae: 1.5311 - lr: 5.1034e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 5s - loss: 14.8272 - mse: 14.8272 - mae: 1.7121 - val_loss: 12.3473 - val_mse: 12.3473 - val_mae: 1.5534 - lr: 5.1034e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 5s - loss: 14.5735 - mse: 14.5735 - mae: 1.6669 - val_loss: 12.5175 - val_mse: 12.5175 - val_mae: 1.5090 - lr: 5.1034e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 5s - loss: 14.6888 - mse: 14.6888 - mae: 1.6603 - val_loss: 12.4821 - val_mse: 12.4821 - val_mae: 1.7636 - lr: 5.1034e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 5s - loss: 14.5811 - mse: 14.5811 - mae: 1.6527 - val_loss: 13.2442 - val_mse: 13.2442 - val_mae: 1.9550 - lr: 5.1034e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 5s - loss: 14.5143 - mse: 14.5143 - mae: 1.6446 - val_loss: 12.5705 - val_mse: 12.5705 - val_mae: 1.5500 - lr: 5.1034e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 5s - loss: 14.6659 - mse: 14.6659 - mae: 1.6713 - val_loss: 12.7901 - val_mse: 12.7901 - val_mae: 1.5132 - lr: 5.1034e-04 - 5s/epoch - 5ms/step\n",
            "Score for fold 5: loss of 12.790104866027832\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-10-15 14:23:02,212]\u001b[0m Finished trial#28 resulted in value: 15.181999999999999. Current best value is 14.618 with parameters: {'activation': 'relu', 'num_hidden_layer': 4, 'i': 10, 'learning_rate': 0.0001995902281877497}.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 14.7297 - mse: 14.7297 - mae: 1.6516 - val_loss: 24.9727 - val_mse: 24.9727 - val_mae: 1.6520 - lr: 2.3945e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 13.3519 - mse: 13.3519 - mae: 1.6078 - val_loss: 24.7124 - val_mse: 24.7124 - val_mae: 1.6310 - lr: 2.3945e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 13.1364 - mse: 13.1364 - mae: 1.5944 - val_loss: 24.5622 - val_mse: 24.5622 - val_mae: 1.6311 - lr: 2.3945e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 13.0212 - mse: 13.0212 - mae: 1.5874 - val_loss: 24.5445 - val_mse: 24.5445 - val_mae: 1.5992 - lr: 2.3945e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 12.9590 - mse: 12.9590 - mae: 1.5819 - val_loss: 24.4513 - val_mse: 24.4513 - val_mae: 1.6187 - lr: 2.3945e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 12.9218 - mse: 12.9218 - mae: 1.5816 - val_loss: 24.4229 - val_mse: 24.4229 - val_mae: 1.6312 - lr: 2.3945e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 12.8757 - mse: 12.8757 - mae: 1.5852 - val_loss: 24.3973 - val_mse: 24.3973 - val_mae: 1.6348 - lr: 2.3945e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 2s - loss: 12.8377 - mse: 12.8377 - mae: 1.5883 - val_loss: 24.4215 - val_mse: 24.4215 - val_mae: 1.6084 - lr: 2.3945e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 2s - loss: 12.8366 - mse: 12.8366 - mae: 1.5839 - val_loss: 24.4055 - val_mse: 24.4055 - val_mae: 1.6111 - lr: 2.3945e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 2s - loss: 12.8317 - mse: 12.8317 - mae: 1.5814 - val_loss: 24.4055 - val_mse: 24.4055 - val_mae: 1.6180 - lr: 2.3945e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 2s - loss: 12.8192 - mse: 12.8192 - mae: 1.5841 - val_loss: 24.4023 - val_mse: 24.4023 - val_mae: 1.6207 - lr: 2.3945e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 2s - loss: 12.8104 - mse: 12.8104 - mae: 1.5817 - val_loss: 24.3729 - val_mse: 24.3729 - val_mae: 1.6131 - lr: 2.3945e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 13/100\n",
            "1000/1000 - 2s - loss: 12.7829 - mse: 12.7829 - mae: 1.5839 - val_loss: 24.3980 - val_mse: 24.3980 - val_mae: 1.6103 - lr: 2.3945e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 14/100\n",
            "1000/1000 - 2s - loss: 12.7915 - mse: 12.7915 - mae: 1.5822 - val_loss: 24.3823 - val_mse: 24.3823 - val_mae: 1.6229 - lr: 2.3945e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 15/100\n",
            "1000/1000 - 2s - loss: 12.7797 - mse: 12.7797 - mae: 1.5845 - val_loss: 24.3743 - val_mse: 24.3743 - val_mae: 1.6221 - lr: 2.3945e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 16/100\n",
            "1000/1000 - 2s - loss: 12.7668 - mse: 12.7668 - mae: 1.5820 - val_loss: 24.3411 - val_mse: 24.3411 - val_mae: 1.6394 - lr: 2.3945e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 17/100\n",
            "1000/1000 - 2s - loss: 12.7334 - mse: 12.7334 - mae: 1.5846 - val_loss: 24.3943 - val_mse: 24.3943 - val_mae: 1.6220 - lr: 2.3945e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 18/100\n",
            "1000/1000 - 2s - loss: 12.7574 - mse: 12.7574 - mae: 1.5815 - val_loss: 24.3779 - val_mse: 24.3779 - val_mae: 1.6019 - lr: 2.3945e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 19/100\n",
            "1000/1000 - 2s - loss: 12.7376 - mse: 12.7376 - mae: 1.5810 - val_loss: 24.3376 - val_mse: 24.3376 - val_mae: 1.6186 - lr: 2.3945e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 20/100\n",
            "1000/1000 - 2s - loss: 12.7255 - mse: 12.7255 - mae: 1.5824 - val_loss: 24.3501 - val_mse: 24.3501 - val_mae: 1.6151 - lr: 2.3945e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 21/100\n",
            "1000/1000 - 2s - loss: 12.7221 - mse: 12.7221 - mae: 1.5809 - val_loss: 24.3476 - val_mse: 24.3476 - val_mae: 1.6243 - lr: 2.3945e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 22/100\n",
            "1000/1000 - 2s - loss: 12.7252 - mse: 12.7252 - mae: 1.5801 - val_loss: 24.3627 - val_mse: 24.3627 - val_mae: 1.6056 - lr: 2.3945e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 23/100\n",
            "1000/1000 - 2s - loss: 12.7169 - mse: 12.7169 - mae: 1.5801 - val_loss: 24.3741 - val_mse: 24.3741 - val_mae: 1.6124 - lr: 2.3945e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 24/100\n",
            "1000/1000 - 2s - loss: 12.7207 - mse: 12.7207 - mae: 1.5813 - val_loss: 24.3400 - val_mse: 24.3400 - val_mae: 1.6165 - lr: 2.3945e-04 - 2s/epoch - 2ms/step\n",
            "Score for fold 1: loss of 24.34002113342285\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 14.9847 - mse: 14.9847 - mae: 1.5771 - val_loss: 15.2932 - val_mse: 15.2932 - val_mae: 1.5996 - lr: 2.3945e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 14.9554 - mse: 14.9554 - mae: 1.5771 - val_loss: 15.2984 - val_mse: 15.2984 - val_mae: 1.6105 - lr: 2.3945e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 14.9689 - mse: 14.9689 - mae: 1.5766 - val_loss: 15.3132 - val_mse: 15.3132 - val_mae: 1.5973 - lr: 2.3945e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 14.9857 - mse: 14.9857 - mae: 1.5729 - val_loss: 15.2759 - val_mse: 15.2759 - val_mae: 1.6172 - lr: 2.3945e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 14.9595 - mse: 14.9595 - mae: 1.5765 - val_loss: 15.2992 - val_mse: 15.2992 - val_mae: 1.6111 - lr: 2.3945e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 14.9480 - mse: 14.9480 - mae: 1.5771 - val_loss: 15.3170 - val_mse: 15.3170 - val_mae: 1.5945 - lr: 2.3945e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 14.9518 - mse: 14.9518 - mae: 1.5756 - val_loss: 15.2934 - val_mse: 15.2934 - val_mae: 1.6142 - lr: 2.3945e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 2s - loss: 14.9502 - mse: 14.9502 - mae: 1.5760 - val_loss: 15.2805 - val_mse: 15.2805 - val_mae: 1.6093 - lr: 2.3945e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 2s - loss: 14.9264 - mse: 14.9264 - mae: 1.5768 - val_loss: 15.2936 - val_mse: 15.2936 - val_mae: 1.6106 - lr: 2.3945e-04 - 2s/epoch - 2ms/step\n",
            "Score for fold 2: loss of 15.29362678527832\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 15.8441 - mse: 15.8441 - mae: 1.5884 - val_loss: 11.6029 - val_mse: 11.6029 - val_mae: 1.5670 - lr: 2.3945e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 15.8546 - mse: 15.8546 - mae: 1.5853 - val_loss: 11.6039 - val_mse: 11.6039 - val_mae: 1.5713 - lr: 2.3945e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 15.8585 - mse: 15.8585 - mae: 1.5854 - val_loss: 11.6174 - val_mse: 11.6174 - val_mae: 1.5517 - lr: 2.3945e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 15.8477 - mse: 15.8477 - mae: 1.5852 - val_loss: 11.5935 - val_mse: 11.5935 - val_mae: 1.5722 - lr: 2.3945e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 15.8465 - mse: 15.8465 - mae: 1.5865 - val_loss: 11.6019 - val_mse: 11.6019 - val_mae: 1.5626 - lr: 2.3945e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 15.8447 - mse: 15.8447 - mae: 1.5873 - val_loss: 11.5942 - val_mse: 11.5942 - val_mae: 1.5601 - lr: 2.3945e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 15.8318 - mse: 15.8318 - mae: 1.5853 - val_loss: 11.6048 - val_mse: 11.6048 - val_mae: 1.5597 - lr: 2.3945e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 2s - loss: 15.8209 - mse: 15.8209 - mae: 1.5826 - val_loss: 11.6020 - val_mse: 11.6020 - val_mae: 1.5808 - lr: 2.3945e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 2s - loss: 15.8274 - mse: 15.8274 - mae: 1.5843 - val_loss: 11.5764 - val_mse: 11.5764 - val_mae: 1.5664 - lr: 2.3945e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 2s - loss: 15.8162 - mse: 15.8162 - mae: 1.5849 - val_loss: 11.6262 - val_mse: 11.6262 - val_mae: 1.5596 - lr: 2.3945e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 2s - loss: 15.8184 - mse: 15.8184 - mae: 1.5833 - val_loss: 11.6112 - val_mse: 11.6112 - val_mae: 1.5626 - lr: 2.3945e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 2s - loss: 15.8263 - mse: 15.8263 - mae: 1.5814 - val_loss: 11.5746 - val_mse: 11.5746 - val_mae: 1.5732 - lr: 2.3945e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 13/100\n",
            "1000/1000 - 2s - loss: 15.8002 - mse: 15.8002 - mae: 1.5853 - val_loss: 11.5735 - val_mse: 11.5735 - val_mae: 1.5663 - lr: 2.3945e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 14/100\n",
            "1000/1000 - 2s - loss: 15.7974 - mse: 15.7974 - mae: 1.5825 - val_loss: 11.6090 - val_mse: 11.6090 - val_mae: 1.5791 - lr: 2.3945e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 15/100\n",
            "1000/1000 - 2s - loss: 15.7987 - mse: 15.7987 - mae: 1.5859 - val_loss: 11.5595 - val_mse: 11.5595 - val_mae: 1.5797 - lr: 2.3945e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 16/100\n",
            "1000/1000 - 2s - loss: 15.8043 - mse: 15.8043 - mae: 1.5828 - val_loss: 11.5939 - val_mse: 11.5939 - val_mae: 1.5812 - lr: 2.3945e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 17/100\n",
            "1000/1000 - 2s - loss: 15.7946 - mse: 15.7946 - mae: 1.5858 - val_loss: 11.5770 - val_mse: 11.5770 - val_mae: 1.5733 - lr: 2.3945e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 18/100\n",
            "1000/1000 - 2s - loss: 15.7905 - mse: 15.7905 - mae: 1.5840 - val_loss: 11.6005 - val_mse: 11.6005 - val_mae: 1.5711 - lr: 2.3945e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 19/100\n",
            "1000/1000 - 2s - loss: 15.7982 - mse: 15.7982 - mae: 1.5828 - val_loss: 11.5891 - val_mse: 11.5891 - val_mae: 1.5785 - lr: 2.3945e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 20/100\n",
            "1000/1000 - 2s - loss: 15.8018 - mse: 15.8018 - mae: 1.5802 - val_loss: 11.5744 - val_mse: 11.5744 - val_mae: 1.5761 - lr: 2.3945e-04 - 2s/epoch - 2ms/step\n",
            "Score for fold 3: loss of 11.574349403381348\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 3s - loss: 15.7666 - mse: 15.7666 - mae: 1.5943 - val_loss: 11.6581 - val_mse: 11.6581 - val_mae: 1.5426 - lr: 2.3945e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 3s - loss: 15.7549 - mse: 15.7549 - mae: 1.5960 - val_loss: 11.6912 - val_mse: 11.6912 - val_mae: 1.5256 - lr: 2.3945e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 15.7702 - mse: 15.7702 - mae: 1.5924 - val_loss: 11.6779 - val_mse: 11.6779 - val_mae: 1.5240 - lr: 2.3945e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 15.7480 - mse: 15.7480 - mae: 1.5935 - val_loss: 11.6895 - val_mse: 11.6895 - val_mae: 1.5421 - lr: 2.3945e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 15.7670 - mse: 15.7670 - mae: 1.5891 - val_loss: 11.6731 - val_mse: 11.6731 - val_mae: 1.5518 - lr: 2.3945e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 15.7591 - mse: 15.7591 - mae: 1.5922 - val_loss: 11.6808 - val_mse: 11.6808 - val_mae: 1.5406 - lr: 2.3945e-04 - 2s/epoch - 2ms/step\n",
            "Score for fold 4: loss of 11.680832862854004\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 15.6663 - mse: 15.6663 - mae: 1.5831 - val_loss: 12.0577 - val_mse: 12.0577 - val_mae: 1.5889 - lr: 2.3945e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 15.6578 - mse: 15.6578 - mae: 1.5810 - val_loss: 12.0889 - val_mse: 12.0889 - val_mae: 1.5649 - lr: 2.3945e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 15.6685 - mse: 15.6685 - mae: 1.5752 - val_loss: 12.0372 - val_mse: 12.0372 - val_mae: 1.6108 - lr: 2.3945e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 15.6464 - mse: 15.6464 - mae: 1.5812 - val_loss: 12.0874 - val_mse: 12.0874 - val_mae: 1.5836 - lr: 2.3945e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 15.6733 - mse: 15.6733 - mae: 1.5779 - val_loss: 12.0593 - val_mse: 12.0593 - val_mae: 1.5995 - lr: 2.3945e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 15.6367 - mse: 15.6367 - mae: 1.5796 - val_loss: 12.0924 - val_mse: 12.0924 - val_mae: 1.6014 - lr: 2.3945e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 15.6509 - mse: 15.6509 - mae: 1.5786 - val_loss: 12.0699 - val_mse: 12.0699 - val_mae: 1.5788 - lr: 2.3945e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 2s - loss: 15.6380 - mse: 15.6380 - mae: 1.5793 - val_loss: 12.1034 - val_mse: 12.1034 - val_mae: 1.5721 - lr: 2.3945e-04 - 2s/epoch - 2ms/step\n",
            "Score for fold 5: loss of 12.103399276733398\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-10-15 14:25:25,216]\u001b[0m Finished trial#29 resulted in value: 14.995999999999999. Current best value is 14.618 with parameters: {'activation': 'relu', 'num_hidden_layer': 4, 'i': 10, 'learning_rate': 0.0001995902281877497}.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 3s - loss: 16.9296 - mse: 16.9296 - mae: 1.6324 - val_loss: 9.6294 - val_mse: 9.6294 - val_mae: 1.5477 - lr: 0.0020 - 3s/epoch - 3ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 16.6382 - mse: 16.6382 - mae: 1.6154 - val_loss: 9.5388 - val_mse: 9.5388 - val_mae: 1.5495 - lr: 0.0020 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 16.5307 - mse: 16.5307 - mae: 1.6150 - val_loss: 9.6069 - val_mse: 9.6069 - val_mae: 1.6419 - lr: 0.0020 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 16.5258 - mse: 16.5258 - mae: 1.6091 - val_loss: 9.5507 - val_mse: 9.5507 - val_mae: 1.5111 - lr: 0.0020 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 16.4978 - mse: 16.4978 - mae: 1.6085 - val_loss: 9.5051 - val_mse: 9.5051 - val_mae: 1.5294 - lr: 0.0020 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 16.4683 - mse: 16.4683 - mae: 1.6104 - val_loss: 9.4586 - val_mse: 9.4586 - val_mae: 1.5289 - lr: 0.0020 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 16.4046 - mse: 16.4046 - mae: 1.6066 - val_loss: 9.5621 - val_mse: 9.5621 - val_mae: 1.5477 - lr: 0.0020 - 2s/epoch - 2ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 2s - loss: 16.4258 - mse: 16.4258 - mae: 1.6048 - val_loss: 9.4323 - val_mse: 9.4323 - val_mae: 1.5232 - lr: 0.0020 - 2s/epoch - 2ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 2s - loss: 16.3856 - mse: 16.3856 - mae: 1.6048 - val_loss: 9.5787 - val_mse: 9.5787 - val_mae: 1.5560 - lr: 0.0020 - 2s/epoch - 2ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 2s - loss: 16.4271 - mse: 16.4271 - mae: 1.6043 - val_loss: 9.4177 - val_mse: 9.4177 - val_mae: 1.5307 - lr: 0.0020 - 2s/epoch - 2ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 2s - loss: 16.4071 - mse: 16.4071 - mae: 1.6097 - val_loss: 9.4314 - val_mse: 9.4314 - val_mae: 1.5635 - lr: 0.0020 - 2s/epoch - 2ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 2s - loss: 16.4235 - mse: 16.4235 - mae: 1.6031 - val_loss: 9.4513 - val_mse: 9.4513 - val_mae: 1.5433 - lr: 0.0020 - 2s/epoch - 2ms/step\n",
            "Epoch 13/100\n",
            "1000/1000 - 2s - loss: 16.3696 - mse: 16.3696 - mae: 1.6022 - val_loss: 9.4652 - val_mse: 9.4652 - val_mae: 1.5071 - lr: 0.0020 - 2s/epoch - 2ms/step\n",
            "Epoch 14/100\n",
            "1000/1000 - 2s - loss: 16.3781 - mse: 16.3781 - mae: 1.6022 - val_loss: 9.5636 - val_mse: 9.5636 - val_mae: 1.5935 - lr: 0.0020 - 2s/epoch - 2ms/step\n",
            "Epoch 15/100\n",
            "1000/1000 - 2s - loss: 16.4079 - mse: 16.4079 - mae: 1.6082 - val_loss: 9.4408 - val_mse: 9.4408 - val_mae: 1.5345 - lr: 0.0020 - 2s/epoch - 2ms/step\n",
            "Score for fold 1: loss of 9.440796852111816\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 14.9950 - mse: 14.9950 - mae: 1.5743 - val_loss: 14.7731 - val_mse: 14.7731 - val_mae: 1.6181 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 14.9946 - mse: 14.9946 - mae: 1.5728 - val_loss: 14.9091 - val_mse: 14.9091 - val_mae: 1.5965 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 15.0047 - mse: 15.0047 - mae: 1.5703 - val_loss: 14.8121 - val_mse: 14.8121 - val_mae: 1.6347 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 14.9904 - mse: 14.9904 - mae: 1.5703 - val_loss: 14.9227 - val_mse: 14.9227 - val_mae: 1.6120 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 14.9853 - mse: 14.9853 - mae: 1.5702 - val_loss: 14.8762 - val_mse: 14.8762 - val_mae: 1.6026 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 14.9768 - mse: 14.9768 - mae: 1.5682 - val_loss: 14.7535 - val_mse: 14.7535 - val_mae: 1.6780 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 14.9713 - mse: 14.9713 - mae: 1.5711 - val_loss: 14.9640 - val_mse: 14.9640 - val_mae: 1.6154 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 2s - loss: 14.9564 - mse: 14.9564 - mae: 1.5716 - val_loss: 14.6957 - val_mse: 14.6957 - val_mae: 1.6212 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 2s - loss: 14.9617 - mse: 14.9617 - mae: 1.5660 - val_loss: 14.8320 - val_mse: 14.8320 - val_mae: 1.6013 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 2s - loss: 14.9505 - mse: 14.9505 - mae: 1.5688 - val_loss: 14.9045 - val_mse: 14.9045 - val_mae: 1.5839 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 2s - loss: 14.9752 - mse: 14.9752 - mae: 1.5671 - val_loss: 14.7196 - val_mse: 14.7196 - val_mae: 1.6734 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 2s - loss: 14.9442 - mse: 14.9442 - mae: 1.5687 - val_loss: 14.8870 - val_mse: 14.8870 - val_mae: 1.6109 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 13/100\n",
            "1000/1000 - 2s - loss: 14.9456 - mse: 14.9456 - mae: 1.5662 - val_loss: 14.7848 - val_mse: 14.7848 - val_mae: 1.6489 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Score for fold 2: loss of 14.784807205200195\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 14.3086 - mse: 14.3086 - mae: 1.5938 - val_loss: 17.2184 - val_mse: 17.2184 - val_mae: 1.5191 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 14.3079 - mse: 14.3079 - mae: 1.5932 - val_loss: 17.2238 - val_mse: 17.2238 - val_mae: 1.4914 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 14.2893 - mse: 14.2893 - mae: 1.5937 - val_loss: 17.1749 - val_mse: 17.1749 - val_mae: 1.5403 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 14.3173 - mse: 14.3173 - mae: 1.5909 - val_loss: 17.2593 - val_mse: 17.2593 - val_mae: 1.5133 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 14.3187 - mse: 14.3187 - mae: 1.5910 - val_loss: 17.2189 - val_mse: 17.2189 - val_mae: 1.5422 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 14.2580 - mse: 14.2580 - mae: 1.5906 - val_loss: 17.2089 - val_mse: 17.2089 - val_mae: 1.5278 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 14.2784 - mse: 14.2784 - mae: 1.5918 - val_loss: 17.2386 - val_mse: 17.2386 - val_mae: 1.5267 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 2s - loss: 14.2633 - mse: 14.2633 - mae: 1.5894 - val_loss: 17.1982 - val_mse: 17.1982 - val_mae: 1.5415 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Score for fold 3: loss of 17.19816780090332\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 13.0349 - mse: 13.0349 - mae: 1.5550 - val_loss: 22.1704 - val_mse: 22.1704 - val_mae: 1.6552 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 13.0373 - mse: 13.0373 - mae: 1.5561 - val_loss: 22.1690 - val_mse: 22.1690 - val_mae: 1.6668 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 13.0117 - mse: 13.0117 - mae: 1.5509 - val_loss: 22.3163 - val_mse: 22.3163 - val_mae: 1.6538 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 13.0381 - mse: 13.0381 - mae: 1.5540 - val_loss: 22.2653 - val_mse: 22.2653 - val_mae: 1.6449 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 13.0456 - mse: 13.0456 - mae: 1.5546 - val_loss: 22.2825 - val_mse: 22.2825 - val_mae: 1.6409 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 13.0251 - mse: 13.0251 - mae: 1.5568 - val_loss: 22.1763 - val_mse: 22.1763 - val_mae: 1.6692 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 13.0341 - mse: 13.0341 - mae: 1.5550 - val_loss: 22.3439 - val_mse: 22.3439 - val_mae: 1.6591 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Score for fold 4: loss of 22.343902587890625\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 15.8662 - mse: 15.8662 - mae: 1.5870 - val_loss: 10.9130 - val_mse: 10.9130 - val_mae: 1.5661 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 3s - loss: 15.8670 - mse: 15.8670 - mae: 1.5848 - val_loss: 10.8870 - val_mse: 10.8870 - val_mae: 1.5584 - lr: 0.0010 - 3s/epoch - 3ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 3s - loss: 15.8547 - mse: 15.8547 - mae: 1.5847 - val_loss: 10.8818 - val_mse: 10.8818 - val_mae: 1.5314 - lr: 0.0010 - 3s/epoch - 3ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 15.8590 - mse: 15.8590 - mae: 1.5843 - val_loss: 10.8705 - val_mse: 10.8705 - val_mae: 1.5681 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 15.8754 - mse: 15.8754 - mae: 1.5851 - val_loss: 10.9209 - val_mse: 10.9209 - val_mae: 1.5311 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 15.8244 - mse: 15.8244 - mae: 1.5835 - val_loss: 10.9520 - val_mse: 10.9520 - val_mae: 1.6150 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 15.8433 - mse: 15.8433 - mae: 1.5854 - val_loss: 11.0658 - val_mse: 11.0658 - val_mae: 1.5545 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 2s - loss: 15.8661 - mse: 15.8661 - mae: 1.5830 - val_loss: 10.9465 - val_mse: 10.9465 - val_mae: 1.5419 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 2s - loss: 15.8513 - mse: 15.8513 - mae: 1.5878 - val_loss: 10.8742 - val_mse: 10.8742 - val_mae: 1.5530 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Score for fold 5: loss of 10.87419319152832\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-10-15 14:27:53,557]\u001b[0m Finished trial#30 resulted in value: 14.926000000000002. Current best value is 14.618 with parameters: {'activation': 'relu', 'num_hidden_layer': 4, 'i': 10, 'learning_rate': 0.0001995902281877497}.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 22s - loss: 14.5294 - mse: 14.5294 - mae: 1.5997 - val_loss: 19.1974 - val_mse: 19.1974 - val_mae: 1.6287 - lr: 1.6457e-04 - 22s/epoch - 22ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 21s - loss: 14.1192 - mse: 14.1192 - mae: 1.5782 - val_loss: 18.9944 - val_mse: 18.9944 - val_mae: 1.6434 - lr: 1.6457e-04 - 21s/epoch - 21ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 22s - loss: 14.0063 - mse: 14.0063 - mae: 1.5761 - val_loss: 19.1608 - val_mse: 19.1608 - val_mae: 1.6521 - lr: 1.6457e-04 - 22s/epoch - 22ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 21s - loss: 13.9692 - mse: 13.9692 - mae: 1.5695 - val_loss: 19.0117 - val_mse: 19.0117 - val_mae: 1.6692 - lr: 1.6457e-04 - 21s/epoch - 21ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 21s - loss: 13.9470 - mse: 13.9470 - mae: 1.5724 - val_loss: 18.9657 - val_mse: 18.9657 - val_mae: 1.6637 - lr: 1.6457e-04 - 21s/epoch - 21ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 21s - loss: 13.9109 - mse: 13.9109 - mae: 1.5660 - val_loss: 19.0308 - val_mse: 19.0308 - val_mae: 1.7482 - lr: 1.6457e-04 - 21s/epoch - 21ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 21s - loss: 13.9029 - mse: 13.9029 - mae: 1.5639 - val_loss: 19.0722 - val_mse: 19.0722 - val_mae: 1.6152 - lr: 1.6457e-04 - 21s/epoch - 21ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 23s - loss: 13.8564 - mse: 13.8564 - mae: 1.5638 - val_loss: 19.0688 - val_mse: 19.0688 - val_mae: 1.5821 - lr: 1.6457e-04 - 23s/epoch - 23ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 21s - loss: 13.7966 - mse: 13.7966 - mae: 1.5626 - val_loss: 19.0652 - val_mse: 19.0652 - val_mae: 1.6041 - lr: 1.6457e-04 - 21s/epoch - 21ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 21s - loss: 13.7911 - mse: 13.7911 - mae: 1.5582 - val_loss: 19.0578 - val_mse: 19.0578 - val_mae: 1.5967 - lr: 1.6457e-04 - 21s/epoch - 21ms/step\n",
            "Score for fold 1: loss of 19.057790756225586\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 22s - loss: 15.3602 - mse: 15.3602 - mae: 1.5777 - val_loss: 12.7483 - val_mse: 12.7483 - val_mae: 1.5096 - lr: 1.6457e-04 - 22s/epoch - 22ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 22s - loss: 15.3411 - mse: 15.3411 - mae: 1.5733 - val_loss: 12.7821 - val_mse: 12.7821 - val_mae: 1.5530 - lr: 1.6457e-04 - 22s/epoch - 22ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 22s - loss: 15.3576 - mse: 15.3576 - mae: 1.5705 - val_loss: 12.6616 - val_mse: 12.6616 - val_mae: 1.6159 - lr: 1.6457e-04 - 22s/epoch - 22ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 22s - loss: 15.3230 - mse: 15.3230 - mae: 1.5699 - val_loss: 12.7952 - val_mse: 12.7952 - val_mae: 1.5101 - lr: 1.6457e-04 - 22s/epoch - 22ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 22s - loss: 15.2654 - mse: 15.2654 - mae: 1.5688 - val_loss: 12.7840 - val_mse: 12.7840 - val_mae: 1.5385 - lr: 1.6457e-04 - 22s/epoch - 22ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 24s - loss: 15.3000 - mse: 15.3000 - mae: 1.5695 - val_loss: 12.6961 - val_mse: 12.6961 - val_mae: 1.5534 - lr: 1.6457e-04 - 24s/epoch - 24ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 22s - loss: 15.2343 - mse: 15.2343 - mae: 1.5677 - val_loss: 12.8480 - val_mse: 12.8480 - val_mae: 1.5239 - lr: 1.6457e-04 - 22s/epoch - 22ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 22s - loss: 15.2596 - mse: 15.2596 - mae: 1.5673 - val_loss: 12.6750 - val_mse: 12.6750 - val_mae: 1.6176 - lr: 1.6457e-04 - 22s/epoch - 22ms/step\n",
            "Score for fold 2: loss of 12.674958229064941\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 22s - loss: 13.2991 - mse: 13.2991 - mae: 1.5565 - val_loss: 20.9836 - val_mse: 20.9836 - val_mae: 1.5850 - lr: 1.6457e-04 - 22s/epoch - 22ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 21s - loss: 13.2961 - mse: 13.2961 - mae: 1.5531 - val_loss: 20.5950 - val_mse: 20.5950 - val_mae: 1.5747 - lr: 1.6457e-04 - 21s/epoch - 21ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 22s - loss: 13.2502 - mse: 13.2502 - mae: 1.5451 - val_loss: 20.5198 - val_mse: 20.5198 - val_mae: 1.6272 - lr: 1.6457e-04 - 22s/epoch - 22ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 22s - loss: 13.2103 - mse: 13.2103 - mae: 1.5482 - val_loss: 20.8305 - val_mse: 20.8305 - val_mae: 1.5685 - lr: 1.6457e-04 - 22s/epoch - 22ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 23s - loss: 13.1923 - mse: 13.1923 - mae: 1.5452 - val_loss: 20.5373 - val_mse: 20.5373 - val_mae: 1.6128 - lr: 1.6457e-04 - 23s/epoch - 23ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 22s - loss: 13.2068 - mse: 13.2068 - mae: 1.5455 - val_loss: 20.5734 - val_mse: 20.5734 - val_mae: 1.6254 - lr: 1.6457e-04 - 22s/epoch - 22ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 21s - loss: 13.1885 - mse: 13.1885 - mae: 1.5417 - val_loss: 20.6756 - val_mse: 20.6756 - val_mae: 1.6352 - lr: 1.6457e-04 - 21s/epoch - 21ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 21s - loss: 13.1224 - mse: 13.1224 - mae: 1.5458 - val_loss: 21.0123 - val_mse: 21.0123 - val_mae: 1.5690 - lr: 1.6457e-04 - 21s/epoch - 21ms/step\n",
            "Score for fold 3: loss of 21.012340545654297\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 21s - loss: 15.5778 - mse: 15.5778 - mae: 1.5644 - val_loss: 10.6560 - val_mse: 10.6560 - val_mae: 1.5312 - lr: 1.6457e-04 - 21s/epoch - 21ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 21s - loss: 15.4733 - mse: 15.4733 - mae: 1.5646 - val_loss: 10.9748 - val_mse: 10.9748 - val_mae: 1.5027 - lr: 1.6457e-04 - 21s/epoch - 21ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 23s - loss: 15.5306 - mse: 15.5306 - mae: 1.5567 - val_loss: 10.9650 - val_mse: 10.9650 - val_mae: 1.5008 - lr: 1.6457e-04 - 23s/epoch - 23ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 21s - loss: 15.4932 - mse: 15.4932 - mae: 1.5593 - val_loss: 10.8962 - val_mse: 10.8962 - val_mae: 1.5142 - lr: 1.6457e-04 - 21s/epoch - 21ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 21s - loss: 15.4620 - mse: 15.4620 - mae: 1.5561 - val_loss: 11.1146 - val_mse: 11.1146 - val_mae: 1.5219 - lr: 1.6457e-04 - 21s/epoch - 21ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 21s - loss: 15.4451 - mse: 15.4451 - mae: 1.5596 - val_loss: 11.0047 - val_mse: 11.0047 - val_mae: 1.6036 - lr: 1.6457e-04 - 21s/epoch - 21ms/step\n",
            "Score for fold 4: loss of 11.00468635559082\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 21s - loss: 15.6099 - mse: 15.6099 - mae: 1.5586 - val_loss: 10.4385 - val_mse: 10.4385 - val_mae: 1.5779 - lr: 1.6457e-04 - 21s/epoch - 21ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 21s - loss: 15.4858 - mse: 15.4858 - mae: 1.5583 - val_loss: 10.4913 - val_mse: 10.4913 - val_mae: 1.5139 - lr: 1.6457e-04 - 21s/epoch - 21ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 21s - loss: 15.5290 - mse: 15.5290 - mae: 1.5566 - val_loss: 10.4900 - val_mse: 10.4900 - val_mae: 1.5122 - lr: 1.6457e-04 - 21s/epoch - 21ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 21s - loss: 15.4340 - mse: 15.4340 - mae: 1.5531 - val_loss: 10.4958 - val_mse: 10.4958 - val_mae: 1.5751 - lr: 1.6457e-04 - 21s/epoch - 21ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 23s - loss: 15.4448 - mse: 15.4448 - mae: 1.5517 - val_loss: 10.5567 - val_mse: 10.5567 - val_mae: 1.5692 - lr: 1.6457e-04 - 23s/epoch - 23ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 21s - loss: 15.3731 - mse: 15.3731 - mae: 1.5513 - val_loss: 10.4680 - val_mse: 10.4680 - val_mae: 1.5671 - lr: 1.6457e-04 - 21s/epoch - 21ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-10-15 14:43:06,458]\u001b[0m Finished trial#31 resulted in value: 14.841999999999999. Current best value is 14.618 with parameters: {'activation': 'relu', 'num_hidden_layer': 4, 'i': 10, 'learning_rate': 0.0001995902281877497}.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Score for fold 5: loss of 10.46799373626709\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 22s - loss: 16.4316 - mse: 16.4316 - mae: 1.6323 - val_loss: 11.8109 - val_mse: 11.8109 - val_mae: 1.4977 - lr: 1.3021e-04 - 22s/epoch - 22ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 21s - loss: 15.9599 - mse: 15.9599 - mae: 1.6084 - val_loss: 11.7906 - val_mse: 11.7906 - val_mae: 1.6152 - lr: 1.3021e-04 - 21s/epoch - 21ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 21s - loss: 15.8306 - mse: 15.8306 - mae: 1.6042 - val_loss: 11.6836 - val_mse: 11.6836 - val_mae: 1.5860 - lr: 1.3021e-04 - 21s/epoch - 21ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 20s - loss: 15.7702 - mse: 15.7702 - mae: 1.6000 - val_loss: 11.7303 - val_mse: 11.7303 - val_mae: 1.6178 - lr: 1.3021e-04 - 20s/epoch - 20ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 20s - loss: 15.7866 - mse: 15.7866 - mae: 1.5964 - val_loss: 11.6972 - val_mse: 11.6972 - val_mae: 1.5173 - lr: 1.3021e-04 - 20s/epoch - 20ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 22s - loss: 15.7849 - mse: 15.7849 - mae: 1.5955 - val_loss: 11.5842 - val_mse: 11.5842 - val_mae: 1.4952 - lr: 1.3021e-04 - 22s/epoch - 22ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 21s - loss: 15.6928 - mse: 15.6928 - mae: 1.5947 - val_loss: 11.6506 - val_mse: 11.6506 - val_mae: 1.5640 - lr: 1.3021e-04 - 21s/epoch - 21ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 21s - loss: 15.6577 - mse: 15.6577 - mae: 1.5867 - val_loss: 11.5995 - val_mse: 11.5995 - val_mae: 1.5350 - lr: 1.3021e-04 - 21s/epoch - 21ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 21s - loss: 15.6802 - mse: 15.6802 - mae: 1.5902 - val_loss: 11.7288 - val_mse: 11.7288 - val_mae: 1.4875 - lr: 1.3021e-04 - 21s/epoch - 21ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 21s - loss: 15.6656 - mse: 15.6656 - mae: 1.5891 - val_loss: 11.5772 - val_mse: 11.5772 - val_mae: 1.4981 - lr: 1.3021e-04 - 21s/epoch - 21ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 21s - loss: 15.6425 - mse: 15.6425 - mae: 1.5819 - val_loss: 11.6082 - val_mse: 11.6082 - val_mae: 1.6273 - lr: 1.3021e-04 - 21s/epoch - 21ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 21s - loss: 15.6171 - mse: 15.6171 - mae: 1.5841 - val_loss: 11.6892 - val_mse: 11.6892 - val_mae: 1.4721 - lr: 1.3021e-04 - 21s/epoch - 21ms/step\n",
            "Epoch 13/100\n",
            "1000/1000 - 21s - loss: 15.5827 - mse: 15.5827 - mae: 1.5850 - val_loss: 11.5907 - val_mse: 11.5907 - val_mae: 1.5566 - lr: 1.3021e-04 - 21s/epoch - 21ms/step\n",
            "Epoch 14/100\n",
            "1000/1000 - 22s - loss: 15.6314 - mse: 15.6314 - mae: 1.5831 - val_loss: 11.5565 - val_mse: 11.5565 - val_mae: 1.5246 - lr: 1.3021e-04 - 22s/epoch - 22ms/step\n",
            "Epoch 15/100\n",
            "1000/1000 - 22s - loss: 15.5878 - mse: 15.5878 - mae: 1.5824 - val_loss: 11.5589 - val_mse: 11.5589 - val_mae: 1.5133 - lr: 1.3021e-04 - 22s/epoch - 22ms/step\n",
            "Epoch 16/100\n",
            "1000/1000 - 21s - loss: 15.5374 - mse: 15.5374 - mae: 1.5808 - val_loss: 11.5836 - val_mse: 11.5836 - val_mae: 1.5319 - lr: 1.3021e-04 - 21s/epoch - 21ms/step\n",
            "Epoch 17/100\n",
            "1000/1000 - 21s - loss: 15.5256 - mse: 15.5256 - mae: 1.5787 - val_loss: 11.6647 - val_mse: 11.6647 - val_mae: 1.5215 - lr: 1.3021e-04 - 21s/epoch - 21ms/step\n",
            "Epoch 18/100\n",
            "1000/1000 - 21s - loss: 15.5520 - mse: 15.5520 - mae: 1.5756 - val_loss: 11.5104 - val_mse: 11.5104 - val_mae: 1.5299 - lr: 1.3021e-04 - 21s/epoch - 21ms/step\n",
            "Epoch 19/100\n",
            "1000/1000 - 21s - loss: 15.4878 - mse: 15.4878 - mae: 1.5750 - val_loss: 11.5381 - val_mse: 11.5381 - val_mae: 1.5342 - lr: 1.3021e-04 - 21s/epoch - 21ms/step\n",
            "Epoch 20/100\n",
            "1000/1000 - 21s - loss: 15.4843 - mse: 15.4843 - mae: 1.5748 - val_loss: 11.6560 - val_mse: 11.6560 - val_mae: 1.4779 - lr: 1.3021e-04 - 21s/epoch - 21ms/step\n",
            "Epoch 21/100\n",
            "1000/1000 - 21s - loss: 15.4942 - mse: 15.4942 - mae: 1.5724 - val_loss: 11.7548 - val_mse: 11.7548 - val_mae: 1.4687 - lr: 1.3021e-04 - 21s/epoch - 21ms/step\n",
            "Epoch 22/100\n",
            "1000/1000 - 21s - loss: 15.4270 - mse: 15.4270 - mae: 1.5728 - val_loss: 11.4793 - val_mse: 11.4793 - val_mae: 1.5325 - lr: 1.3021e-04 - 21s/epoch - 21ms/step\n",
            "Epoch 23/100\n",
            "1000/1000 - 23s - loss: 15.4201 - mse: 15.4201 - mae: 1.5701 - val_loss: 11.5369 - val_mse: 11.5369 - val_mae: 1.5759 - lr: 1.3021e-04 - 23s/epoch - 23ms/step\n",
            "Epoch 24/100\n",
            "1000/1000 - 21s - loss: 15.4159 - mse: 15.4159 - mae: 1.5685 - val_loss: 11.4837 - val_mse: 11.4837 - val_mae: 1.5184 - lr: 1.3021e-04 - 21s/epoch - 21ms/step\n",
            "Epoch 25/100\n",
            "1000/1000 - 21s - loss: 15.4029 - mse: 15.4029 - mae: 1.5695 - val_loss: 11.5182 - val_mse: 11.5182 - val_mae: 1.4961 - lr: 1.3021e-04 - 21s/epoch - 21ms/step\n",
            "Epoch 26/100\n",
            "1000/1000 - 21s - loss: 15.3959 - mse: 15.3959 - mae: 1.5692 - val_loss: 11.5556 - val_mse: 11.5556 - val_mae: 1.4861 - lr: 1.3021e-04 - 21s/epoch - 21ms/step\n",
            "Epoch 27/100\n",
            "1000/1000 - 21s - loss: 15.3742 - mse: 15.3742 - mae: 1.5682 - val_loss: 11.6228 - val_mse: 11.6228 - val_mae: 1.4837 - lr: 1.3021e-04 - 21s/epoch - 21ms/step\n",
            "Score for fold 1: loss of 11.622770309448242\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 21s - loss: 15.8871 - mse: 15.8871 - mae: 1.5608 - val_loss: 9.3744 - val_mse: 9.3744 - val_mae: 1.5169 - lr: 1.3021e-04 - 21s/epoch - 21ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 23s - loss: 15.9120 - mse: 15.9120 - mae: 1.5616 - val_loss: 9.4063 - val_mse: 9.4063 - val_mae: 1.5853 - lr: 1.3021e-04 - 23s/epoch - 23ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 21s - loss: 15.8681 - mse: 15.8681 - mae: 1.5591 - val_loss: 9.3153 - val_mse: 9.3153 - val_mae: 1.5886 - lr: 1.3021e-04 - 21s/epoch - 21ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 21s - loss: 15.8710 - mse: 15.8710 - mae: 1.5577 - val_loss: 9.3772 - val_mse: 9.3772 - val_mae: 1.5533 - lr: 1.3021e-04 - 21s/epoch - 21ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 21s - loss: 15.8239 - mse: 15.8239 - mae: 1.5548 - val_loss: 9.3866 - val_mse: 9.3866 - val_mae: 1.5781 - lr: 1.3021e-04 - 21s/epoch - 21ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 21s - loss: 15.7788 - mse: 15.7788 - mae: 1.5525 - val_loss: 9.4505 - val_mse: 9.4505 - val_mae: 1.5480 - lr: 1.3021e-04 - 21s/epoch - 21ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 21s - loss: 15.7921 - mse: 15.7921 - mae: 1.5521 - val_loss: 9.5255 - val_mse: 9.5255 - val_mae: 1.4938 - lr: 1.3021e-04 - 21s/epoch - 21ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 22s - loss: 15.8003 - mse: 15.8003 - mae: 1.5505 - val_loss: 9.4350 - val_mse: 9.4350 - val_mae: 1.5792 - lr: 1.3021e-04 - 22s/epoch - 22ms/step\n",
            "Score for fold 2: loss of 9.434978485107422\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 21s - loss: 14.2732 - mse: 14.2732 - mae: 1.5399 - val_loss: 16.0102 - val_mse: 16.0102 - val_mae: 1.5740 - lr: 1.3021e-04 - 21s/epoch - 21ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 23s - loss: 14.1979 - mse: 14.1979 - mae: 1.5383 - val_loss: 15.6318 - val_mse: 15.6318 - val_mae: 1.6032 - lr: 1.3021e-04 - 23s/epoch - 23ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 20s - loss: 14.1678 - mse: 14.1678 - mae: 1.5341 - val_loss: 15.4941 - val_mse: 15.4941 - val_mae: 1.6600 - lr: 1.3021e-04 - 20s/epoch - 20ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 21s - loss: 14.1159 - mse: 14.1159 - mae: 1.5349 - val_loss: 16.2035 - val_mse: 16.2035 - val_mae: 1.6034 - lr: 1.3021e-04 - 21s/epoch - 21ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 21s - loss: 14.1298 - mse: 14.1298 - mae: 1.5297 - val_loss: 15.9174 - val_mse: 15.9174 - val_mae: 1.5727 - lr: 1.3021e-04 - 21s/epoch - 21ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 20s - loss: 14.1232 - mse: 14.1232 - mae: 1.5271 - val_loss: 16.0352 - val_mse: 16.0352 - val_mae: 1.6370 - lr: 1.3021e-04 - 20s/epoch - 20ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 20s - loss: 14.0845 - mse: 14.0845 - mae: 1.5242 - val_loss: 15.9516 - val_mse: 15.9516 - val_mae: 1.5876 - lr: 1.3021e-04 - 20s/epoch - 20ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 20s - loss: 14.0423 - mse: 14.0423 - mae: 1.5288 - val_loss: 16.4102 - val_mse: 16.4102 - val_mae: 1.5959 - lr: 1.3021e-04 - 20s/epoch - 20ms/step\n",
            "Score for fold 3: loss of 16.410207748413086\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 21s - loss: 12.9906 - mse: 12.9906 - mae: 1.5361 - val_loss: 20.1798 - val_mse: 20.1798 - val_mae: 1.5507 - lr: 1.3021e-04 - 21s/epoch - 21ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 21s - loss: 12.8887 - mse: 12.8887 - mae: 1.5316 - val_loss: 20.4672 - val_mse: 20.4672 - val_mae: 1.5451 - lr: 1.3021e-04 - 21s/epoch - 21ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 24s - loss: 12.7668 - mse: 12.7668 - mae: 1.5263 - val_loss: 20.3729 - val_mse: 20.3729 - val_mae: 1.6587 - lr: 1.3021e-04 - 24s/epoch - 24ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 22s - loss: 12.7783 - mse: 12.7783 - mae: 1.5279 - val_loss: 20.3972 - val_mse: 20.3972 - val_mae: 1.5461 - lr: 1.3021e-04 - 22s/epoch - 22ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 22s - loss: 12.7642 - mse: 12.7642 - mae: 1.5258 - val_loss: 20.3846 - val_mse: 20.3846 - val_mae: 1.6119 - lr: 1.3021e-04 - 22s/epoch - 22ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 22s - loss: 12.7778 - mse: 12.7778 - mae: 1.5230 - val_loss: 20.3996 - val_mse: 20.3996 - val_mae: 1.5587 - lr: 1.3021e-04 - 22s/epoch - 22ms/step\n",
            "Score for fold 4: loss of 20.399551391601562\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 21s - loss: 14.0077 - mse: 14.0077 - mae: 1.5365 - val_loss: 15.7567 - val_mse: 15.7567 - val_mae: 1.5522 - lr: 1.3021e-04 - 21s/epoch - 21ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 22s - loss: 13.9039 - mse: 13.9039 - mae: 1.5322 - val_loss: 16.0314 - val_mse: 16.0314 - val_mae: 1.5196 - lr: 1.3021e-04 - 22s/epoch - 22ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 21s - loss: 13.9203 - mse: 13.9203 - mae: 1.5306 - val_loss: 15.8470 - val_mse: 15.8470 - val_mae: 1.5657 - lr: 1.3021e-04 - 21s/epoch - 21ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 22s - loss: 13.9302 - mse: 13.9302 - mae: 1.5291 - val_loss: 15.7425 - val_mse: 15.7425 - val_mae: 1.5302 - lr: 1.3021e-04 - 22s/epoch - 22ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 24s - loss: 13.9149 - mse: 13.9149 - mae: 1.5297 - val_loss: 15.7446 - val_mse: 15.7446 - val_mae: 1.5271 - lr: 1.3021e-04 - 24s/epoch - 24ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 22s - loss: 13.8075 - mse: 13.8075 - mae: 1.5205 - val_loss: 15.8752 - val_mse: 15.8752 - val_mae: 1.5534 - lr: 1.3021e-04 - 22s/epoch - 22ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 21s - loss: 13.7273 - mse: 13.7273 - mae: 1.5182 - val_loss: 15.6271 - val_mse: 15.6271 - val_mae: 1.5708 - lr: 1.3021e-04 - 21s/epoch - 21ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 22s - loss: 13.8014 - mse: 13.8014 - mae: 1.5211 - val_loss: 15.7700 - val_mse: 15.7700 - val_mae: 1.5757 - lr: 1.3021e-04 - 22s/epoch - 22ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 21s - loss: 13.7205 - mse: 13.7205 - mae: 1.5171 - val_loss: 15.9791 - val_mse: 15.9791 - val_mae: 1.5520 - lr: 1.3021e-04 - 21s/epoch - 21ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 21s - loss: 13.7296 - mse: 13.7296 - mae: 1.5159 - val_loss: 15.8020 - val_mse: 15.8020 - val_mae: 1.5740 - lr: 1.3021e-04 - 21s/epoch - 21ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 21s - loss: 13.6875 - mse: 13.6875 - mae: 1.5130 - val_loss: 15.8582 - val_mse: 15.8582 - val_mae: 1.5448 - lr: 1.3021e-04 - 21s/epoch - 21ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 21s - loss: 13.6458 - mse: 13.6458 - mae: 1.5131 - val_loss: 15.9082 - val_mse: 15.9082 - val_mae: 1.5335 - lr: 1.3021e-04 - 21s/epoch - 21ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-10-15 15:05:49,086]\u001b[0m Finished trial#32 resulted in value: 14.754. Current best value is 14.618 with parameters: {'activation': 'relu', 'num_hidden_layer': 4, 'i': 10, 'learning_rate': 0.0001995902281877497}.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Score for fold 5: loss of 15.908158302307129\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 24s - loss: 15.2307 - mse: 15.2307 - mae: 1.6079 - val_loss: 16.6714 - val_mse: 16.6714 - val_mae: 1.5754 - lr: 1.0421e-04 - 24s/epoch - 24ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 21s - loss: 14.8240 - mse: 14.8240 - mae: 1.5858 - val_loss: 16.4668 - val_mse: 16.4668 - val_mae: 1.5689 - lr: 1.0421e-04 - 21s/epoch - 21ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 21s - loss: 14.7390 - mse: 14.7390 - mae: 1.5796 - val_loss: 16.3120 - val_mse: 16.3120 - val_mae: 1.6420 - lr: 1.0421e-04 - 21s/epoch - 21ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 21s - loss: 14.6966 - mse: 14.6966 - mae: 1.5800 - val_loss: 16.4661 - val_mse: 16.4661 - val_mae: 1.6163 - lr: 1.0421e-04 - 21s/epoch - 21ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 22s - loss: 14.6636 - mse: 14.6636 - mae: 1.5793 - val_loss: 16.3824 - val_mse: 16.3824 - val_mae: 1.6020 - lr: 1.0421e-04 - 22s/epoch - 22ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 22s - loss: 14.6317 - mse: 14.6317 - mae: 1.5756 - val_loss: 16.4302 - val_mse: 16.4302 - val_mae: 1.5939 - lr: 1.0421e-04 - 22s/epoch - 22ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 21s - loss: 14.6426 - mse: 14.6426 - mae: 1.5708 - val_loss: 16.9108 - val_mse: 16.9108 - val_mae: 1.5575 - lr: 1.0421e-04 - 21s/epoch - 21ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 22s - loss: 14.5843 - mse: 14.5843 - mae: 1.5727 - val_loss: 16.3394 - val_mse: 16.3394 - val_mae: 1.5870 - lr: 1.0421e-04 - 22s/epoch - 22ms/step\n",
            "Score for fold 1: loss of 16.339370727539062\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 21s - loss: 14.9244 - mse: 14.9244 - mae: 1.5625 - val_loss: 14.7260 - val_mse: 14.7260 - val_mae: 1.6211 - lr: 1.0421e-04 - 21s/epoch - 21ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 21s - loss: 14.9131 - mse: 14.9131 - mae: 1.5623 - val_loss: 14.9546 - val_mse: 14.9546 - val_mae: 1.5680 - lr: 1.0421e-04 - 21s/epoch - 21ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 21s - loss: 14.8639 - mse: 14.8639 - mae: 1.5603 - val_loss: 14.7570 - val_mse: 14.7570 - val_mae: 1.5849 - lr: 1.0421e-04 - 21s/epoch - 21ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 21s - loss: 14.8417 - mse: 14.8417 - mae: 1.5578 - val_loss: 14.8783 - val_mse: 14.8783 - val_mae: 1.6183 - lr: 1.0421e-04 - 21s/epoch - 21ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 21s - loss: 14.7818 - mse: 14.7818 - mae: 1.5551 - val_loss: 14.7281 - val_mse: 14.7281 - val_mae: 1.5987 - lr: 1.0421e-04 - 21s/epoch - 21ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 22s - loss: 14.7804 - mse: 14.7804 - mae: 1.5542 - val_loss: 14.8381 - val_mse: 14.8381 - val_mae: 1.5990 - lr: 1.0421e-04 - 22s/epoch - 22ms/step\n",
            "Score for fold 2: loss of 14.838128089904785\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 22s - loss: 15.1434 - mse: 15.1434 - mae: 1.5652 - val_loss: 13.4211 - val_mse: 13.4211 - val_mae: 1.5667 - lr: 1.0421e-04 - 22s/epoch - 22ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 23s - loss: 15.0817 - mse: 15.0817 - mae: 1.5652 - val_loss: 13.5212 - val_mse: 13.5212 - val_mae: 1.5552 - lr: 1.0421e-04 - 23s/epoch - 23ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 22s - loss: 15.0755 - mse: 15.0755 - mae: 1.5627 - val_loss: 13.3756 - val_mse: 13.3756 - val_mae: 1.6057 - lr: 1.0421e-04 - 22s/epoch - 22ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 22s - loss: 15.0828 - mse: 15.0828 - mae: 1.5643 - val_loss: 13.3514 - val_mse: 13.3514 - val_mae: 1.6098 - lr: 1.0421e-04 - 22s/epoch - 22ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 22s - loss: 15.0252 - mse: 15.0252 - mae: 1.5631 - val_loss: 13.4473 - val_mse: 13.4473 - val_mae: 1.5853 - lr: 1.0421e-04 - 22s/epoch - 22ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 22s - loss: 15.0149 - mse: 15.0149 - mae: 1.5587 - val_loss: 13.4986 - val_mse: 13.4986 - val_mae: 1.5570 - lr: 1.0421e-04 - 22s/epoch - 22ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 22s - loss: 14.9496 - mse: 14.9496 - mae: 1.5564 - val_loss: 13.5803 - val_mse: 13.5803 - val_mae: 1.5737 - lr: 1.0421e-04 - 22s/epoch - 22ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 22s - loss: 15.0366 - mse: 15.0366 - mae: 1.5593 - val_loss: 13.5074 - val_mse: 13.5074 - val_mae: 1.5885 - lr: 1.0421e-04 - 22s/epoch - 22ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 22s - loss: 14.9780 - mse: 14.9780 - mae: 1.5568 - val_loss: 13.6346 - val_mse: 13.6346 - val_mae: 1.5076 - lr: 1.0421e-04 - 22s/epoch - 22ms/step\n",
            "Score for fold 3: loss of 13.63463306427002\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 23s - loss: 13.8165 - mse: 13.8165 - mae: 1.5664 - val_loss: 18.1954 - val_mse: 18.1954 - val_mae: 1.5260 - lr: 1.0421e-04 - 23s/epoch - 23ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 21s - loss: 13.7717 - mse: 13.7717 - mae: 1.5651 - val_loss: 18.2719 - val_mse: 18.2719 - val_mae: 1.6005 - lr: 1.0421e-04 - 21s/epoch - 21ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 21s - loss: 13.7539 - mse: 13.7539 - mae: 1.5652 - val_loss: 18.2935 - val_mse: 18.2935 - val_mae: 1.5735 - lr: 1.0421e-04 - 21s/epoch - 21ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 21s - loss: 13.6743 - mse: 13.6743 - mae: 1.5623 - val_loss: 18.3436 - val_mse: 18.3436 - val_mae: 1.5654 - lr: 1.0421e-04 - 21s/epoch - 21ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 21s - loss: 13.7408 - mse: 13.7408 - mae: 1.5624 - val_loss: 18.4482 - val_mse: 18.4482 - val_mae: 1.5708 - lr: 1.0421e-04 - 21s/epoch - 21ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 21s - loss: 13.7203 - mse: 13.7203 - mae: 1.5615 - val_loss: 18.3426 - val_mse: 18.3426 - val_mae: 1.5523 - lr: 1.0421e-04 - 21s/epoch - 21ms/step\n",
            "Score for fold 4: loss of 18.342601776123047\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 21s - loss: 15.5048 - mse: 15.5048 - mae: 1.5645 - val_loss: 10.9802 - val_mse: 10.9802 - val_mae: 1.4898 - lr: 1.0421e-04 - 21s/epoch - 21ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 21s - loss: 15.4871 - mse: 15.4871 - mae: 1.5615 - val_loss: 10.8776 - val_mse: 10.8776 - val_mae: 1.5630 - lr: 1.0421e-04 - 21s/epoch - 21ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 21s - loss: 15.4871 - mse: 15.4871 - mae: 1.5612 - val_loss: 10.9220 - val_mse: 10.9220 - val_mae: 1.5244 - lr: 1.0421e-04 - 21s/epoch - 21ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 23s - loss: 15.4527 - mse: 15.4527 - mae: 1.5564 - val_loss: 11.0900 - val_mse: 11.0900 - val_mae: 1.5227 - lr: 1.0421e-04 - 23s/epoch - 23ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 21s - loss: 15.4423 - mse: 15.4423 - mae: 1.5562 - val_loss: 11.0268 - val_mse: 11.0268 - val_mae: 1.5177 - lr: 1.0421e-04 - 21s/epoch - 21ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 21s - loss: 15.4252 - mse: 15.4252 - mae: 1.5552 - val_loss: 11.1834 - val_mse: 11.1834 - val_mae: 1.5130 - lr: 1.0421e-04 - 21s/epoch - 21ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 21s - loss: 15.3857 - mse: 15.3857 - mae: 1.5573 - val_loss: 11.1708 - val_mse: 11.1708 - val_mae: 1.5465 - lr: 1.0421e-04 - 21s/epoch - 21ms/step\n",
            "Score for fold 5: loss of 11.170797348022461\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-10-15 15:20:32,407]\u001b[0m Finished trial#33 resulted in value: 14.864. Current best value is 14.618 with parameters: {'activation': 'relu', 'num_hidden_layer': 4, 'i': 10, 'learning_rate': 0.0001995902281877497}.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 7s - loss: 16.8256 - mse: 16.8256 - mae: 1.6188 - val_loss: 10.4362 - val_mse: 10.4362 - val_mae: 1.5298 - lr: 2.1160e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 6s - loss: 16.3329 - mse: 16.3329 - mae: 1.5962 - val_loss: 10.2352 - val_mse: 10.2352 - val_mae: 1.5607 - lr: 2.1160e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 6s - loss: 16.2654 - mse: 16.2654 - mae: 1.5940 - val_loss: 10.2026 - val_mse: 10.2026 - val_mae: 1.6312 - lr: 2.1160e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 6s - loss: 16.2101 - mse: 16.2101 - mae: 1.5897 - val_loss: 10.1899 - val_mse: 10.1899 - val_mae: 1.5272 - lr: 2.1160e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 6s - loss: 16.1772 - mse: 16.1772 - mae: 1.5864 - val_loss: 10.1410 - val_mse: 10.1410 - val_mae: 1.5765 - lr: 2.1160e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 6s - loss: 16.1831 - mse: 16.1831 - mae: 1.5834 - val_loss: 10.1598 - val_mse: 10.1598 - val_mae: 1.5403 - lr: 2.1160e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 7s - loss: 16.1658 - mse: 16.1658 - mae: 1.5800 - val_loss: 10.1121 - val_mse: 10.1121 - val_mae: 1.5332 - lr: 2.1160e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 7s - loss: 16.0774 - mse: 16.0774 - mae: 1.5836 - val_loss: 10.1541 - val_mse: 10.1541 - val_mae: 1.5370 - lr: 2.1160e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 6s - loss: 16.0887 - mse: 16.0887 - mae: 1.5789 - val_loss: 10.0976 - val_mse: 10.0976 - val_mae: 1.5376 - lr: 2.1160e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 6s - loss: 16.0989 - mse: 16.0989 - mae: 1.5789 - val_loss: 10.0635 - val_mse: 10.0635 - val_mae: 1.5482 - lr: 2.1160e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 6s - loss: 16.0740 - mse: 16.0740 - mae: 1.5772 - val_loss: 10.0696 - val_mse: 10.0696 - val_mae: 1.5641 - lr: 2.1160e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 6s - loss: 16.0341 - mse: 16.0341 - mae: 1.5730 - val_loss: 10.0517 - val_mse: 10.0517 - val_mae: 1.5555 - lr: 2.1160e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 13/100\n",
            "1000/1000 - 6s - loss: 16.0508 - mse: 16.0508 - mae: 1.5721 - val_loss: 10.0490 - val_mse: 10.0490 - val_mae: 1.5736 - lr: 2.1160e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 14/100\n",
            "1000/1000 - 6s - loss: 16.0260 - mse: 16.0260 - mae: 1.5709 - val_loss: 10.2127 - val_mse: 10.2127 - val_mae: 1.5707 - lr: 2.1160e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 15/100\n",
            "1000/1000 - 6s - loss: 15.9827 - mse: 15.9827 - mae: 1.5704 - val_loss: 10.1505 - val_mse: 10.1505 - val_mae: 1.5847 - lr: 2.1160e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 16/100\n",
            "1000/1000 - 6s - loss: 16.0027 - mse: 16.0027 - mae: 1.5698 - val_loss: 9.9976 - val_mse: 9.9976 - val_mae: 1.5270 - lr: 2.1160e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 17/100\n",
            "1000/1000 - 6s - loss: 16.0179 - mse: 16.0179 - mae: 1.5676 - val_loss: 9.9954 - val_mse: 9.9954 - val_mae: 1.5366 - lr: 2.1160e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 18/100\n",
            "1000/1000 - 6s - loss: 15.9956 - mse: 15.9956 - mae: 1.5653 - val_loss: 10.0003 - val_mse: 10.0003 - val_mae: 1.5415 - lr: 2.1160e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 19/100\n",
            "1000/1000 - 6s - loss: 15.9918 - mse: 15.9918 - mae: 1.5675 - val_loss: 10.0815 - val_mse: 10.0815 - val_mae: 1.5218 - lr: 2.1160e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 20/100\n",
            "1000/1000 - 7s - loss: 15.9469 - mse: 15.9469 - mae: 1.5679 - val_loss: 10.0121 - val_mse: 10.0121 - val_mae: 1.5565 - lr: 2.1160e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 21/100\n",
            "1000/1000 - 6s - loss: 15.9745 - mse: 15.9745 - mae: 1.5650 - val_loss: 10.0515 - val_mse: 10.0515 - val_mae: 1.5434 - lr: 2.1160e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 22/100\n",
            "1000/1000 - 6s - loss: 15.8846 - mse: 15.8846 - mae: 1.5652 - val_loss: 10.1025 - val_mse: 10.1025 - val_mae: 1.5218 - lr: 2.1160e-04 - 6s/epoch - 6ms/step\n",
            "Score for fold 1: loss of 10.102444648742676\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 6s - loss: 11.4389 - mse: 11.4389 - mae: 1.5469 - val_loss: 28.4282 - val_mse: 28.4282 - val_mae: 1.5840 - lr: 2.1160e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 6s - loss: 11.3905 - mse: 11.3905 - mae: 1.5456 - val_loss: 28.2025 - val_mse: 28.2025 - val_mae: 1.5943 - lr: 2.1160e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 6s - loss: 11.3338 - mse: 11.3338 - mae: 1.5460 - val_loss: 28.1258 - val_mse: 28.1258 - val_mae: 1.5938 - lr: 2.1160e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 7s - loss: 11.2971 - mse: 11.2971 - mae: 1.5424 - val_loss: 27.8628 - val_mse: 27.8628 - val_mae: 1.6594 - lr: 2.1160e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 8s - loss: 11.2766 - mse: 11.2766 - mae: 1.5403 - val_loss: 28.2738 - val_mse: 28.2738 - val_mae: 1.5766 - lr: 2.1160e-04 - 8s/epoch - 8ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 6s - loss: 11.2868 - mse: 11.2868 - mae: 1.5367 - val_loss: 28.2017 - val_mse: 28.2017 - val_mae: 1.6229 - lr: 2.1160e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 6s - loss: 11.2207 - mse: 11.2207 - mae: 1.5367 - val_loss: 28.2551 - val_mse: 28.2551 - val_mae: 1.6694 - lr: 2.1160e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 7s - loss: 11.2817 - mse: 11.2817 - mae: 1.5396 - val_loss: 28.4033 - val_mse: 28.4033 - val_mae: 1.5667 - lr: 2.1160e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 6s - loss: 11.2589 - mse: 11.2589 - mae: 1.5384 - val_loss: 28.4905 - val_mse: 28.4905 - val_mae: 1.6196 - lr: 2.1160e-04 - 6s/epoch - 6ms/step\n",
            "Score for fold 2: loss of 28.49047088623047\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 7s - loss: 15.5710 - mse: 15.5710 - mae: 1.5581 - val_loss: 10.8817 - val_mse: 10.8817 - val_mae: 1.5548 - lr: 2.1160e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 6s - loss: 15.5715 - mse: 15.5715 - mae: 1.5563 - val_loss: 11.0314 - val_mse: 11.0314 - val_mae: 1.5252 - lr: 2.1160e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 6s - loss: 15.5552 - mse: 15.5552 - mae: 1.5539 - val_loss: 10.9994 - val_mse: 10.9994 - val_mae: 1.5488 - lr: 2.1160e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 6s - loss: 15.5149 - mse: 15.5149 - mae: 1.5502 - val_loss: 11.2181 - val_mse: 11.2181 - val_mae: 1.5178 - lr: 2.1160e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 6s - loss: 15.5228 - mse: 15.5228 - mae: 1.5495 - val_loss: 11.0632 - val_mse: 11.0632 - val_mae: 1.5656 - lr: 2.1160e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 6s - loss: 15.4221 - mse: 15.4221 - mae: 1.5482 - val_loss: 11.0198 - val_mse: 11.0198 - val_mae: 1.5684 - lr: 2.1160e-04 - 6s/epoch - 6ms/step\n",
            "Score for fold 3: loss of 11.019762992858887\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 6s - loss: 14.9021 - mse: 14.9021 - mae: 1.5515 - val_loss: 13.2677 - val_mse: 13.2677 - val_mae: 1.6272 - lr: 2.1160e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 6s - loss: 14.7815 - mse: 14.7815 - mae: 1.5465 - val_loss: 13.3469 - val_mse: 13.3469 - val_mae: 1.5111 - lr: 2.1160e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 7s - loss: 14.7726 - mse: 14.7726 - mae: 1.5476 - val_loss: 13.2934 - val_mse: 13.2934 - val_mae: 1.5753 - lr: 2.1160e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 7s - loss: 14.7673 - mse: 14.7673 - mae: 1.5451 - val_loss: 13.3262 - val_mse: 13.3262 - val_mae: 1.5581 - lr: 2.1160e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 6s - loss: 14.7647 - mse: 14.7647 - mae: 1.5419 - val_loss: 13.6059 - val_mse: 13.6059 - val_mae: 1.5252 - lr: 2.1160e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 6s - loss: 14.7651 - mse: 14.7651 - mae: 1.5383 - val_loss: 13.2580 - val_mse: 13.2580 - val_mae: 1.5504 - lr: 2.1160e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 7s - loss: 14.7174 - mse: 14.7174 - mae: 1.5403 - val_loss: 13.4000 - val_mse: 13.4000 - val_mae: 1.5393 - lr: 2.1160e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 6s - loss: 14.6814 - mse: 14.6814 - mae: 1.5379 - val_loss: 13.3842 - val_mse: 13.3842 - val_mae: 1.5526 - lr: 2.1160e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 6s - loss: 14.6273 - mse: 14.6273 - mae: 1.5326 - val_loss: 13.7462 - val_mse: 13.7462 - val_mae: 1.5540 - lr: 2.1160e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 7s - loss: 14.6714 - mse: 14.6714 - mae: 1.5355 - val_loss: 13.3914 - val_mse: 13.3914 - val_mae: 1.5705 - lr: 2.1160e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 6s - loss: 14.6345 - mse: 14.6345 - mae: 1.5335 - val_loss: 13.5324 - val_mse: 13.5324 - val_mae: 1.5697 - lr: 2.1160e-04 - 6s/epoch - 6ms/step\n",
            "Score for fold 4: loss of 13.532392501831055\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 6s - loss: 15.3333 - mse: 15.3333 - mae: 1.5479 - val_loss: 10.7625 - val_mse: 10.7625 - val_mae: 1.4708 - lr: 2.1160e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 6s - loss: 15.2574 - mse: 15.2574 - mae: 1.5427 - val_loss: 10.8400 - val_mse: 10.8400 - val_mae: 1.5052 - lr: 2.1160e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 6s - loss: 15.2696 - mse: 15.2696 - mae: 1.5397 - val_loss: 10.8608 - val_mse: 10.8608 - val_mae: 1.5033 - lr: 2.1160e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 6s - loss: 15.1836 - mse: 15.1836 - mae: 1.5440 - val_loss: 10.7277 - val_mse: 10.7277 - val_mae: 1.5628 - lr: 2.1160e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 6s - loss: 15.1676 - mse: 15.1676 - mae: 1.5385 - val_loss: 10.8143 - val_mse: 10.8143 - val_mae: 1.5737 - lr: 2.1160e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 8s - loss: 15.1889 - mse: 15.1889 - mae: 1.5403 - val_loss: 10.9530 - val_mse: 10.9530 - val_mae: 1.5244 - lr: 2.1160e-04 - 8s/epoch - 8ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 6s - loss: 15.1352 - mse: 15.1352 - mae: 1.5377 - val_loss: 11.1482 - val_mse: 11.1482 - val_mae: 1.4868 - lr: 2.1160e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 6s - loss: 15.1139 - mse: 15.1139 - mae: 1.5350 - val_loss: 10.8985 - val_mse: 10.8985 - val_mae: 1.5528 - lr: 2.1160e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 7s - loss: 15.0814 - mse: 15.0814 - mae: 1.5307 - val_loss: 10.9152 - val_mse: 10.9152 - val_mae: 1.5289 - lr: 2.1160e-04 - 7s/epoch - 7ms/step\n",
            "Score for fold 5: loss of 10.915213584899902\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-10-15 15:27:53,619]\u001b[0m Finished trial#34 resulted in value: 14.812000000000001. Current best value is 14.618 with parameters: {'activation': 'relu', 'num_hidden_layer': 4, 'i': 10, 'learning_rate': 0.0001995902281877497}.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 22s - loss: 17.2597 - mse: 17.2597 - mae: 1.7069 - val_loss: 11.6111 - val_mse: 11.6111 - val_mae: 1.5827 - lr: 1.2955e-04 - 22s/epoch - 22ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 22s - loss: 16.8338 - mse: 16.8338 - mae: 1.6449 - val_loss: 11.7108 - val_mse: 11.7108 - val_mae: 1.6193 - lr: 1.2955e-04 - 22s/epoch - 22ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 22s - loss: 16.7688 - mse: 16.7688 - mae: 1.6409 - val_loss: 11.7088 - val_mse: 11.7088 - val_mae: 1.5596 - lr: 1.2955e-04 - 22s/epoch - 22ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 22s - loss: 16.8069 - mse: 16.8069 - mae: 1.6429 - val_loss: 11.6092 - val_mse: 11.6092 - val_mae: 1.5705 - lr: 1.2955e-04 - 22s/epoch - 22ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 22s - loss: 16.7805 - mse: 16.7805 - mae: 1.6421 - val_loss: 11.6104 - val_mse: 11.6104 - val_mae: 1.6124 - lr: 1.2955e-04 - 22s/epoch - 22ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 22s - loss: 16.7806 - mse: 16.7806 - mae: 1.6406 - val_loss: 11.5794 - val_mse: 11.5794 - val_mae: 1.6132 - lr: 1.2955e-04 - 22s/epoch - 22ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 22s - loss: 16.6823 - mse: 16.6823 - mae: 1.6476 - val_loss: 11.5841 - val_mse: 11.5841 - val_mae: 1.6427 - lr: 1.2955e-04 - 22s/epoch - 22ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 24s - loss: 16.7163 - mse: 16.7163 - mae: 1.6426 - val_loss: 11.5766 - val_mse: 11.5766 - val_mae: 1.6377 - lr: 1.2955e-04 - 24s/epoch - 24ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 22s - loss: 16.7105 - mse: 16.7105 - mae: 1.6398 - val_loss: 11.6338 - val_mse: 11.6338 - val_mae: 1.5869 - lr: 1.2955e-04 - 22s/epoch - 22ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 23s - loss: 16.7183 - mse: 16.7183 - mae: 1.6444 - val_loss: 11.5389 - val_mse: 11.5389 - val_mae: 1.5881 - lr: 1.2955e-04 - 23s/epoch - 23ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 21s - loss: 16.7038 - mse: 16.7038 - mae: 1.6396 - val_loss: 11.5706 - val_mse: 11.5706 - val_mae: 1.6096 - lr: 1.2955e-04 - 21s/epoch - 21ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 22s - loss: 16.7125 - mse: 16.7125 - mae: 1.6390 - val_loss: 11.5376 - val_mse: 11.5376 - val_mae: 1.6447 - lr: 1.2955e-04 - 22s/epoch - 22ms/step\n",
            "Epoch 13/100\n",
            "1000/1000 - 21s - loss: 16.7000 - mse: 16.7000 - mae: 1.6426 - val_loss: 11.6356 - val_mse: 11.6356 - val_mae: 1.6005 - lr: 1.2955e-04 - 21s/epoch - 21ms/step\n",
            "Epoch 14/100\n",
            "1000/1000 - 22s - loss: 16.6565 - mse: 16.6565 - mae: 1.6396 - val_loss: 11.5683 - val_mse: 11.5683 - val_mae: 1.6403 - lr: 1.2955e-04 - 22s/epoch - 22ms/step\n",
            "Epoch 15/100\n",
            "1000/1000 - 22s - loss: 16.6591 - mse: 16.6591 - mae: 1.6397 - val_loss: 11.5364 - val_mse: 11.5364 - val_mae: 1.6578 - lr: 1.2955e-04 - 22s/epoch - 22ms/step\n",
            "Epoch 16/100\n",
            "1000/1000 - 22s - loss: 16.6575 - mse: 16.6575 - mae: 1.6391 - val_loss: 11.5532 - val_mse: 11.5532 - val_mae: 1.5836 - lr: 1.2955e-04 - 22s/epoch - 22ms/step\n",
            "Epoch 17/100\n",
            "1000/1000 - 24s - loss: 16.6734 - mse: 16.6734 - mae: 1.6444 - val_loss: 11.5807 - val_mse: 11.5807 - val_mae: 1.5851 - lr: 1.2955e-04 - 24s/epoch - 24ms/step\n",
            "Epoch 18/100\n",
            "1000/1000 - 22s - loss: 16.6453 - mse: 16.6453 - mae: 1.6380 - val_loss: 11.5354 - val_mse: 11.5354 - val_mae: 1.6371 - lr: 1.2955e-04 - 22s/epoch - 22ms/step\n",
            "Epoch 19/100\n",
            "1000/1000 - 22s - loss: 16.6594 - mse: 16.6594 - mae: 1.6384 - val_loss: 11.5973 - val_mse: 11.5973 - val_mae: 1.5713 - lr: 1.2955e-04 - 22s/epoch - 22ms/step\n",
            "Epoch 20/100\n",
            "1000/1000 - 21s - loss: 16.6450 - mse: 16.6450 - mae: 1.6366 - val_loss: 11.5311 - val_mse: 11.5311 - val_mae: 1.6231 - lr: 1.2955e-04 - 21s/epoch - 21ms/step\n",
            "Epoch 21/100\n",
            "1000/1000 - 22s - loss: 16.6561 - mse: 16.6561 - mae: 1.6407 - val_loss: 11.5432 - val_mse: 11.5432 - val_mae: 1.6175 - lr: 1.2955e-04 - 22s/epoch - 22ms/step\n",
            "Epoch 22/100\n",
            "1000/1000 - 22s - loss: 16.6708 - mse: 16.6708 - mae: 1.6381 - val_loss: 11.5681 - val_mse: 11.5681 - val_mae: 1.5955 - lr: 1.2955e-04 - 22s/epoch - 22ms/step\n",
            "Epoch 23/100\n",
            "1000/1000 - 22s - loss: 16.6545 - mse: 16.6545 - mae: 1.6368 - val_loss: 11.5408 - val_mse: 11.5408 - val_mae: 1.5955 - lr: 1.2955e-04 - 22s/epoch - 22ms/step\n",
            "Epoch 24/100\n",
            "1000/1000 - 22s - loss: 16.6370 - mse: 16.6370 - mae: 1.6383 - val_loss: 11.5440 - val_mse: 11.5440 - val_mae: 1.6081 - lr: 1.2955e-04 - 22s/epoch - 22ms/step\n",
            "Epoch 25/100\n",
            "1000/1000 - 21s - loss: 16.6411 - mse: 16.6411 - mae: 1.6431 - val_loss: 11.6137 - val_mse: 11.6137 - val_mae: 1.5758 - lr: 1.2955e-04 - 21s/epoch - 21ms/step\n",
            "Score for fold 1: loss of 11.613700866699219\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 22s - loss: 16.7429 - mse: 16.7429 - mae: 1.6438 - val_loss: 11.2146 - val_mse: 11.2146 - val_mae: 1.6278 - lr: 1.2955e-04 - 22s/epoch - 22ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 22s - loss: 16.7629 - mse: 16.7629 - mae: 1.6363 - val_loss: 11.2508 - val_mse: 11.2508 - val_mae: 1.5489 - lr: 1.2955e-04 - 22s/epoch - 22ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 22s - loss: 16.7582 - mse: 16.7582 - mae: 1.6476 - val_loss: 11.2005 - val_mse: 11.2005 - val_mae: 1.6352 - lr: 1.2955e-04 - 22s/epoch - 22ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 21s - loss: 16.7733 - mse: 16.7733 - mae: 1.6481 - val_loss: 11.2587 - val_mse: 11.2587 - val_mae: 1.5353 - lr: 1.2955e-04 - 21s/epoch - 21ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 22s - loss: 16.7555 - mse: 16.7555 - mae: 1.6463 - val_loss: 11.2474 - val_mse: 11.2474 - val_mae: 1.5452 - lr: 1.2955e-04 - 22s/epoch - 22ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 22s - loss: 16.7263 - mse: 16.7263 - mae: 1.6489 - val_loss: 11.2165 - val_mse: 11.2165 - val_mae: 1.5625 - lr: 1.2955e-04 - 22s/epoch - 22ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 22s - loss: 16.7695 - mse: 16.7695 - mae: 1.6442 - val_loss: 11.2073 - val_mse: 11.2073 - val_mae: 1.6473 - lr: 1.2955e-04 - 22s/epoch - 22ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 22s - loss: 16.7839 - mse: 16.7839 - mae: 1.6459 - val_loss: 11.2172 - val_mse: 11.2172 - val_mae: 1.5817 - lr: 1.2955e-04 - 22s/epoch - 22ms/step\n",
            "Score for fold 2: loss of 11.217207908630371\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 24s - loss: 14.0667 - mse: 14.0667 - mae: 1.6293 - val_loss: 21.9257 - val_mse: 21.9257 - val_mae: 1.6500 - lr: 1.2955e-04 - 24s/epoch - 24ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 22s - loss: 14.0427 - mse: 14.0427 - mae: 1.6302 - val_loss: 21.9914 - val_mse: 21.9914 - val_mae: 1.6606 - lr: 1.2955e-04 - 22s/epoch - 22ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 22s - loss: 14.0672 - mse: 14.0672 - mae: 1.6337 - val_loss: 21.9320 - val_mse: 21.9320 - val_mae: 1.6790 - lr: 1.2955e-04 - 22s/epoch - 22ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 22s - loss: 14.0780 - mse: 14.0780 - mae: 1.6369 - val_loss: 22.0127 - val_mse: 22.0127 - val_mae: 1.6156 - lr: 1.2955e-04 - 22s/epoch - 22ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 22s - loss: 14.1087 - mse: 14.1087 - mae: 1.6315 - val_loss: 22.0378 - val_mse: 22.0378 - val_mae: 1.6072 - lr: 1.2955e-04 - 22s/epoch - 22ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 22s - loss: 14.0753 - mse: 14.0753 - mae: 1.6322 - val_loss: 21.8957 - val_mse: 21.8957 - val_mae: 1.6950 - lr: 1.2955e-04 - 22s/epoch - 22ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 22s - loss: 14.0662 - mse: 14.0662 - mae: 1.6381 - val_loss: 21.9333 - val_mse: 21.9333 - val_mae: 1.7196 - lr: 1.2955e-04 - 22s/epoch - 22ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 22s - loss: 14.0964 - mse: 14.0964 - mae: 1.6356 - val_loss: 22.0172 - val_mse: 22.0172 - val_mae: 1.6263 - lr: 1.2955e-04 - 22s/epoch - 22ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 23s - loss: 14.0988 - mse: 14.0988 - mae: 1.6309 - val_loss: 22.0369 - val_mse: 22.0369 - val_mae: 1.6117 - lr: 1.2955e-04 - 23s/epoch - 23ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 22s - loss: 14.1057 - mse: 14.1057 - mae: 1.6328 - val_loss: 22.0379 - val_mse: 22.0379 - val_mae: 1.6238 - lr: 1.2955e-04 - 22s/epoch - 22ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 22s - loss: 14.1089 - mse: 14.1089 - mae: 1.6296 - val_loss: 22.0476 - val_mse: 22.0476 - val_mae: 1.6048 - lr: 1.2955e-04 - 22s/epoch - 22ms/step\n",
            "Score for fold 3: loss of 22.047616958618164\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 21s - loss: 15.7271 - mse: 15.7271 - mae: 1.6289 - val_loss: 15.5170 - val_mse: 15.5170 - val_mae: 1.5959 - lr: 1.2955e-04 - 21s/epoch - 21ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 21s - loss: 15.7307 - mse: 15.7307 - mae: 1.6394 - val_loss: 15.4448 - val_mse: 15.4448 - val_mae: 1.7275 - lr: 1.2955e-04 - 21s/epoch - 21ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 21s - loss: 15.7242 - mse: 15.7242 - mae: 1.6406 - val_loss: 15.4514 - val_mse: 15.4514 - val_mae: 1.6287 - lr: 1.2955e-04 - 21s/epoch - 21ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 21s - loss: 15.7281 - mse: 15.7281 - mae: 1.6403 - val_loss: 15.6673 - val_mse: 15.6673 - val_mae: 1.5566 - lr: 1.2955e-04 - 21s/epoch - 21ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 21s - loss: 15.7281 - mse: 15.7281 - mae: 1.6357 - val_loss: 15.4274 - val_mse: 15.4274 - val_mae: 1.6367 - lr: 1.2955e-04 - 21s/epoch - 21ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 23s - loss: 15.7087 - mse: 15.7087 - mae: 1.6364 - val_loss: 15.4620 - val_mse: 15.4620 - val_mae: 1.6160 - lr: 1.2955e-04 - 23s/epoch - 23ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 21s - loss: 15.7431 - mse: 15.7431 - mae: 1.6311 - val_loss: 15.6615 - val_mse: 15.6615 - val_mae: 1.5494 - lr: 1.2955e-04 - 21s/epoch - 21ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 21s - loss: 15.7790 - mse: 15.7790 - mae: 1.6389 - val_loss: 15.6197 - val_mse: 15.6197 - val_mae: 1.5652 - lr: 1.2955e-04 - 21s/epoch - 21ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 21s - loss: 15.7900 - mse: 15.7900 - mae: 1.6441 - val_loss: 15.4183 - val_mse: 15.4183 - val_mae: 1.7148 - lr: 1.2955e-04 - 21s/epoch - 21ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 21s - loss: 15.7711 - mse: 15.7711 - mae: 1.6392 - val_loss: 15.5291 - val_mse: 15.5291 - val_mae: 1.7926 - lr: 1.2955e-04 - 21s/epoch - 21ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 21s - loss: 15.7191 - mse: 15.7191 - mae: 1.6348 - val_loss: 15.4566 - val_mse: 15.4566 - val_mae: 1.6173 - lr: 1.2955e-04 - 21s/epoch - 21ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 21s - loss: 15.7736 - mse: 15.7736 - mae: 1.6411 - val_loss: 15.7525 - val_mse: 15.7525 - val_mae: 1.5394 - lr: 1.2955e-04 - 21s/epoch - 21ms/step\n",
            "Epoch 13/100\n",
            "1000/1000 - 21s - loss: 15.7949 - mse: 15.7949 - mae: 1.6407 - val_loss: 15.5756 - val_mse: 15.5756 - val_mae: 1.8047 - lr: 1.2955e-04 - 21s/epoch - 21ms/step\n",
            "Epoch 14/100\n",
            "1000/1000 - 21s - loss: 15.7001 - mse: 15.7001 - mae: 1.6442 - val_loss: 15.4868 - val_mse: 15.4868 - val_mae: 1.5935 - lr: 1.2955e-04 - 21s/epoch - 21ms/step\n",
            "Score for fold 4: loss of 15.486798286437988\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 23s - loss: 15.1557 - mse: 15.1557 - mae: 1.6325 - val_loss: 18.0368 - val_mse: 18.0368 - val_mae: 1.6301 - lr: 1.2955e-04 - 23s/epoch - 23ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 22s - loss: 15.1586 - mse: 15.1586 - mae: 1.6387 - val_loss: 18.2098 - val_mse: 18.2098 - val_mae: 1.5856 - lr: 1.2955e-04 - 22s/epoch - 22ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 22s - loss: 15.1362 - mse: 15.1362 - mae: 1.6392 - val_loss: 17.9807 - val_mse: 17.9807 - val_mae: 1.6580 - lr: 1.2955e-04 - 22s/epoch - 22ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 22s - loss: 15.1707 - mse: 15.1707 - mae: 1.6410 - val_loss: 17.9047 - val_mse: 17.9047 - val_mae: 1.7070 - lr: 1.2955e-04 - 22s/epoch - 22ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 22s - loss: 15.0834 - mse: 15.0834 - mae: 1.6395 - val_loss: 17.9891 - val_mse: 17.9891 - val_mae: 1.6381 - lr: 1.2955e-04 - 22s/epoch - 22ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 22s - loss: 15.1390 - mse: 15.1390 - mae: 1.6366 - val_loss: 17.9625 - val_mse: 17.9625 - val_mae: 1.6596 - lr: 1.2955e-04 - 22s/epoch - 22ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 22s - loss: 15.1200 - mse: 15.1200 - mae: 1.6412 - val_loss: 18.2432 - val_mse: 18.2432 - val_mae: 1.5616 - lr: 1.2955e-04 - 22s/epoch - 22ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 22s - loss: 15.1191 - mse: 15.1191 - mae: 1.6426 - val_loss: 17.8601 - val_mse: 17.8601 - val_mae: 1.7350 - lr: 1.2955e-04 - 22s/epoch - 22ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 22s - loss: 15.1149 - mse: 15.1149 - mae: 1.6401 - val_loss: 18.1260 - val_mse: 18.1260 - val_mae: 1.5952 - lr: 1.2955e-04 - 22s/epoch - 22ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 24s - loss: 15.1497 - mse: 15.1497 - mae: 1.6356 - val_loss: 18.0323 - val_mse: 18.0323 - val_mae: 1.6265 - lr: 1.2955e-04 - 24s/epoch - 24ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 22s - loss: 15.1617 - mse: 15.1617 - mae: 1.6375 - val_loss: 17.8834 - val_mse: 17.8834 - val_mae: 1.7570 - lr: 1.2955e-04 - 22s/epoch - 22ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 22s - loss: 15.1649 - mse: 15.1649 - mae: 1.6508 - val_loss: 17.9596 - val_mse: 17.9596 - val_mae: 1.6766 - lr: 1.2955e-04 - 22s/epoch - 22ms/step\n",
            "Epoch 13/100\n",
            "1000/1000 - 22s - loss: 15.1204 - mse: 15.1204 - mae: 1.6313 - val_loss: 17.9599 - val_mse: 17.9599 - val_mae: 1.6650 - lr: 1.2955e-04 - 22s/epoch - 22ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-10-15 15:54:58,682]\u001b[0m Finished trial#35 resulted in value: 15.666. Current best value is 14.618 with parameters: {'activation': 'relu', 'num_hidden_layer': 4, 'i': 10, 'learning_rate': 0.0001995902281877497}.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Score for fold 5: loss of 17.959928512573242\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 22s - loss: 15.9842 - mse: 15.9842 - mae: 1.6177 - val_loss: 13.1923 - val_mse: 13.1923 - val_mae: 1.6046 - lr: 2.9529e-04 - 22s/epoch - 22ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 21s - loss: 15.7038 - mse: 15.7038 - mae: 1.6011 - val_loss: 13.0203 - val_mse: 13.0203 - val_mae: 1.5085 - lr: 2.9529e-04 - 21s/epoch - 21ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 21s - loss: 15.5791 - mse: 15.5791 - mae: 1.5963 - val_loss: 13.2607 - val_mse: 13.2607 - val_mae: 1.5093 - lr: 2.9529e-04 - 21s/epoch - 21ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 23s - loss: 15.5584 - mse: 15.5584 - mae: 1.5921 - val_loss: 12.9231 - val_mse: 12.9231 - val_mae: 1.5009 - lr: 2.9529e-04 - 23s/epoch - 23ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 21s - loss: 15.5509 - mse: 15.5509 - mae: 1.5905 - val_loss: 12.9780 - val_mse: 12.9780 - val_mae: 1.5594 - lr: 2.9529e-04 - 21s/epoch - 21ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 21s - loss: 15.4565 - mse: 15.4565 - mae: 1.5844 - val_loss: 12.7839 - val_mse: 12.7839 - val_mae: 1.5275 - lr: 2.9529e-04 - 21s/epoch - 21ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 21s - loss: 15.4318 - mse: 15.4318 - mae: 1.5764 - val_loss: 12.8457 - val_mse: 12.8457 - val_mae: 1.5990 - lr: 2.9529e-04 - 21s/epoch - 21ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 21s - loss: 15.3996 - mse: 15.3996 - mae: 1.5798 - val_loss: 12.7987 - val_mse: 12.7987 - val_mae: 1.6384 - lr: 2.9529e-04 - 21s/epoch - 21ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 22s - loss: 15.4224 - mse: 15.4224 - mae: 1.5792 - val_loss: 12.9140 - val_mse: 12.9140 - val_mae: 1.5518 - lr: 2.9529e-04 - 22s/epoch - 22ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 22s - loss: 15.3513 - mse: 15.3513 - mae: 1.5778 - val_loss: 12.8557 - val_mse: 12.8557 - val_mae: 1.5801 - lr: 2.9529e-04 - 22s/epoch - 22ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 22s - loss: 15.2995 - mse: 15.2995 - mae: 1.5701 - val_loss: 13.3348 - val_mse: 13.3348 - val_mae: 1.5164 - lr: 2.9529e-04 - 22s/epoch - 22ms/step\n",
            "Score for fold 1: loss of 13.3347749710083\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 23s - loss: 15.1916 - mse: 15.1916 - mae: 1.5668 - val_loss: 13.7553 - val_mse: 13.7553 - val_mae: 1.6345 - lr: 2.9529e-04 - 23s/epoch - 23ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 23s - loss: 15.1590 - mse: 15.1590 - mae: 1.5658 - val_loss: 13.8055 - val_mse: 13.8055 - val_mae: 1.5767 - lr: 2.9529e-04 - 23s/epoch - 23ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 22s - loss: 15.0526 - mse: 15.0526 - mae: 1.5612 - val_loss: 13.8186 - val_mse: 13.8186 - val_mae: 1.5686 - lr: 2.9529e-04 - 22s/epoch - 22ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 22s - loss: 15.0716 - mse: 15.0716 - mae: 1.5612 - val_loss: 13.7040 - val_mse: 13.7040 - val_mae: 1.6153 - lr: 2.9529e-04 - 22s/epoch - 22ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 23s - loss: 14.9966 - mse: 14.9966 - mae: 1.5618 - val_loss: 13.9340 - val_mse: 13.9340 - val_mae: 1.6038 - lr: 2.9529e-04 - 23s/epoch - 23ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 23s - loss: 15.0169 - mse: 15.0169 - mae: 1.5620 - val_loss: 13.7365 - val_mse: 13.7365 - val_mae: 1.6357 - lr: 2.9529e-04 - 23s/epoch - 23ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 23s - loss: 14.9527 - mse: 14.9527 - mae: 1.5547 - val_loss: 14.0783 - val_mse: 14.0783 - val_mae: 1.6238 - lr: 2.9529e-04 - 23s/epoch - 23ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 22s - loss: 14.9362 - mse: 14.9362 - mae: 1.5518 - val_loss: 13.9279 - val_mse: 13.9279 - val_mae: 1.6079 - lr: 2.9529e-04 - 22s/epoch - 22ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 24s - loss: 14.9824 - mse: 14.9824 - mae: 1.5563 - val_loss: 13.6533 - val_mse: 13.6533 - val_mae: 1.6192 - lr: 2.9529e-04 - 24s/epoch - 24ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 23s - loss: 14.9217 - mse: 14.9217 - mae: 1.5495 - val_loss: 13.7050 - val_mse: 13.7050 - val_mae: 1.5730 - lr: 2.9529e-04 - 23s/epoch - 23ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 23s - loss: 14.9513 - mse: 14.9513 - mae: 1.5507 - val_loss: 13.6706 - val_mse: 13.6706 - val_mae: 1.5653 - lr: 2.9529e-04 - 23s/epoch - 23ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 23s - loss: 14.7616 - mse: 14.7616 - mae: 1.5451 - val_loss: 13.8091 - val_mse: 13.8091 - val_mae: 1.5985 - lr: 2.9529e-04 - 23s/epoch - 23ms/step\n",
            "Epoch 13/100\n",
            "1000/1000 - 23s - loss: 14.7729 - mse: 14.7729 - mae: 1.5473 - val_loss: 13.7543 - val_mse: 13.7543 - val_mae: 1.6624 - lr: 2.9529e-04 - 23s/epoch - 23ms/step\n",
            "Epoch 14/100\n",
            "1000/1000 - 23s - loss: 14.7446 - mse: 14.7446 - mae: 1.5439 - val_loss: 13.9075 - val_mse: 13.9075 - val_mae: 1.5528 - lr: 2.9529e-04 - 23s/epoch - 23ms/step\n",
            "Score for fold 2: loss of 13.907546997070312\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 23s - loss: 15.2089 - mse: 15.2089 - mae: 1.5533 - val_loss: 12.2886 - val_mse: 12.2886 - val_mae: 1.5676 - lr: 2.9529e-04 - 23s/epoch - 23ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 23s - loss: 15.1272 - mse: 15.1272 - mae: 1.5506 - val_loss: 12.0115 - val_mse: 12.0115 - val_mae: 1.5234 - lr: 2.9529e-04 - 23s/epoch - 23ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 25s - loss: 15.1148 - mse: 15.1148 - mae: 1.5522 - val_loss: 12.4964 - val_mse: 12.4964 - val_mae: 1.5531 - lr: 2.9529e-04 - 25s/epoch - 25ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 23s - loss: 15.0268 - mse: 15.0268 - mae: 1.5498 - val_loss: 12.2681 - val_mse: 12.2681 - val_mae: 1.4966 - lr: 2.9529e-04 - 23s/epoch - 23ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 22s - loss: 14.9191 - mse: 14.9191 - mae: 1.5421 - val_loss: 12.2698 - val_mse: 12.2698 - val_mae: 1.5755 - lr: 2.9529e-04 - 22s/epoch - 22ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 22s - loss: 14.9620 - mse: 14.9620 - mae: 1.5418 - val_loss: 12.5259 - val_mse: 12.5259 - val_mae: 1.5916 - lr: 2.9529e-04 - 22s/epoch - 22ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 22s - loss: 14.9979 - mse: 14.9979 - mae: 1.5444 - val_loss: 12.2950 - val_mse: 12.2950 - val_mae: 1.5327 - lr: 2.9529e-04 - 22s/epoch - 22ms/step\n",
            "Score for fold 3: loss of 12.294953346252441\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 22s - loss: 12.8183 - mse: 12.8183 - mae: 1.5447 - val_loss: 21.0503 - val_mse: 21.0503 - val_mae: 1.5547 - lr: 2.9529e-04 - 22s/epoch - 22ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 22s - loss: 12.8041 - mse: 12.8041 - mae: 1.5398 - val_loss: 21.1041 - val_mse: 21.1041 - val_mae: 1.5628 - lr: 2.9529e-04 - 22s/epoch - 22ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 22s - loss: 12.5575 - mse: 12.5575 - mae: 1.5370 - val_loss: 20.9213 - val_mse: 20.9213 - val_mae: 1.5715 - lr: 2.9529e-04 - 22s/epoch - 22ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 24s - loss: 12.5225 - mse: 12.5225 - mae: 1.5356 - val_loss: 20.8612 - val_mse: 20.8612 - val_mae: 1.5882 - lr: 2.9529e-04 - 24s/epoch - 24ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 22s - loss: 12.5007 - mse: 12.5007 - mae: 1.5368 - val_loss: 20.9979 - val_mse: 20.9979 - val_mae: 1.5966 - lr: 2.9529e-04 - 22s/epoch - 22ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 22s - loss: 12.5947 - mse: 12.5947 - mae: 1.5302 - val_loss: 21.3468 - val_mse: 21.3468 - val_mae: 1.5455 - lr: 2.9529e-04 - 22s/epoch - 22ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 22s - loss: 12.4278 - mse: 12.4278 - mae: 1.5281 - val_loss: 21.0855 - val_mse: 21.0855 - val_mae: 1.5988 - lr: 2.9529e-04 - 22s/epoch - 22ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 22s - loss: 12.4846 - mse: 12.4846 - mae: 1.5286 - val_loss: 21.1033 - val_mse: 21.1033 - val_mae: 1.6065 - lr: 2.9529e-04 - 22s/epoch - 22ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 22s - loss: 12.3575 - mse: 12.3575 - mae: 1.5221 - val_loss: 21.2882 - val_mse: 21.2882 - val_mae: 1.6375 - lr: 2.9529e-04 - 22s/epoch - 22ms/step\n",
            "Score for fold 4: loss of 21.288196563720703\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 23s - loss: 14.4026 - mse: 14.4026 - mae: 1.5296 - val_loss: 13.0997 - val_mse: 13.0997 - val_mae: 1.5020 - lr: 2.9529e-04 - 23s/epoch - 23ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 23s - loss: 14.4126 - mse: 14.4126 - mae: 1.5304 - val_loss: 13.1149 - val_mse: 13.1149 - val_mae: 1.5160 - lr: 2.9529e-04 - 23s/epoch - 23ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 24s - loss: 14.2519 - mse: 14.2519 - mae: 1.5228 - val_loss: 13.2151 - val_mse: 13.2151 - val_mae: 1.5057 - lr: 2.9529e-04 - 24s/epoch - 24ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 25s - loss: 14.1806 - mse: 14.1806 - mae: 1.5203 - val_loss: 13.1729 - val_mse: 13.1729 - val_mae: 1.5436 - lr: 2.9529e-04 - 25s/epoch - 25ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 25s - loss: 14.0957 - mse: 14.0957 - mae: 1.5147 - val_loss: 13.5534 - val_mse: 13.5534 - val_mae: 1.5043 - lr: 2.9529e-04 - 25s/epoch - 25ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 23s - loss: 14.0938 - mse: 14.0938 - mae: 1.5126 - val_loss: 13.3638 - val_mse: 13.3638 - val_mae: 1.5562 - lr: 2.9529e-04 - 23s/epoch - 23ms/step\n",
            "Score for fold 5: loss of 13.363832473754883\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-10-15 16:13:16,253]\u001b[0m Finished trial#36 resulted in value: 14.836000000000002. Current best value is 14.618 with parameters: {'activation': 'relu', 'num_hidden_layer': 4, 'i': 10, 'learning_rate': 0.0001995902281877497}.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 12s - loss: 16.2249 - mse: 16.2249 - mae: 1.6272 - val_loss: 11.8092 - val_mse: 11.8092 - val_mae: 1.6064 - lr: 6.7191e-04 - 12s/epoch - 12ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 11s - loss: 15.9243 - mse: 15.9243 - mae: 1.6124 - val_loss: 11.9248 - val_mse: 11.9248 - val_mae: 1.5398 - lr: 6.7191e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 11s - loss: 15.8802 - mse: 15.8802 - mae: 1.6066 - val_loss: 12.0394 - val_mse: 12.0394 - val_mae: 1.5828 - lr: 6.7191e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 11s - loss: 15.8268 - mse: 15.8268 - mae: 1.6006 - val_loss: 11.7042 - val_mse: 11.7042 - val_mae: 1.5506 - lr: 6.7191e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 12s - loss: 15.7745 - mse: 15.7745 - mae: 1.5958 - val_loss: 12.1755 - val_mse: 12.1755 - val_mae: 1.5182 - lr: 6.7191e-04 - 12s/epoch - 12ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 12s - loss: 15.7887 - mse: 15.7887 - mae: 1.5914 - val_loss: 12.2207 - val_mse: 12.2207 - val_mae: 1.7281 - lr: 6.7191e-04 - 12s/epoch - 12ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 11s - loss: 15.7826 - mse: 15.7826 - mae: 1.5939 - val_loss: 12.0400 - val_mse: 12.0400 - val_mae: 1.5579 - lr: 6.7191e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 11s - loss: 15.7184 - mse: 15.7184 - mae: 1.5946 - val_loss: 11.9487 - val_mse: 11.9487 - val_mae: 1.6644 - lr: 6.7191e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 11s - loss: 15.7266 - mse: 15.7266 - mae: 1.5979 - val_loss: 11.9381 - val_mse: 11.9381 - val_mae: 1.5439 - lr: 6.7191e-04 - 11s/epoch - 11ms/step\n",
            "Score for fold 1: loss of 11.938126564025879\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 11s - loss: 14.9676 - mse: 14.9676 - mae: 1.5835 - val_loss: 15.0172 - val_mse: 15.0172 - val_mae: 1.5548 - lr: 6.7191e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 11s - loss: 14.9786 - mse: 14.9786 - mae: 1.5856 - val_loss: 14.5804 - val_mse: 14.5804 - val_mae: 1.6081 - lr: 6.7191e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 11s - loss: 15.0233 - mse: 15.0233 - mae: 1.5842 - val_loss: 14.6917 - val_mse: 14.6917 - val_mae: 1.6738 - lr: 6.7191e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 11s - loss: 14.9988 - mse: 14.9988 - mae: 1.5894 - val_loss: 14.7835 - val_mse: 14.7835 - val_mae: 1.6031 - lr: 6.7191e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 12s - loss: 14.9544 - mse: 14.9544 - mae: 1.5818 - val_loss: 14.7297 - val_mse: 14.7297 - val_mae: 1.8085 - lr: 6.7191e-04 - 12s/epoch - 12ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 12s - loss: 14.9772 - mse: 14.9772 - mae: 1.5880 - val_loss: 14.5238 - val_mse: 14.5238 - val_mae: 1.6863 - lr: 6.7191e-04 - 12s/epoch - 12ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 12s - loss: 14.7067 - mse: 14.7067 - mae: 1.5809 - val_loss: 14.7823 - val_mse: 14.7823 - val_mae: 1.6231 - lr: 6.7191e-04 - 12s/epoch - 12ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 11s - loss: 14.8703 - mse: 14.8703 - mae: 1.5816 - val_loss: 14.4957 - val_mse: 14.4957 - val_mae: 1.6512 - lr: 6.7191e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 11s - loss: 14.7283 - mse: 14.7283 - mae: 1.5859 - val_loss: 14.6691 - val_mse: 14.6691 - val_mae: 1.6349 - lr: 6.7191e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 11s - loss: 14.9356 - mse: 14.9356 - mae: 1.5892 - val_loss: 14.9778 - val_mse: 14.9778 - val_mae: 1.4815 - lr: 6.7191e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 11s - loss: 14.7947 - mse: 14.7947 - mae: 1.5838 - val_loss: 14.4788 - val_mse: 14.4788 - val_mae: 1.6385 - lr: 6.7191e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 11s - loss: 14.7245 - mse: 14.7245 - mae: 1.5877 - val_loss: 14.6341 - val_mse: 14.6341 - val_mae: 1.6074 - lr: 6.7191e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 13/100\n",
            "1000/1000 - 11s - loss: 14.7617 - mse: 14.7617 - mae: 1.5842 - val_loss: 14.8530 - val_mse: 14.8530 - val_mae: 1.5570 - lr: 6.7191e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 14/100\n",
            "1000/1000 - 13s - loss: 14.6929 - mse: 14.6929 - mae: 1.5876 - val_loss: 14.6508 - val_mse: 14.6508 - val_mae: 1.6233 - lr: 6.7191e-04 - 13s/epoch - 13ms/step\n",
            "Epoch 15/100\n",
            "1000/1000 - 11s - loss: 14.7239 - mse: 14.7239 - mae: 1.5823 - val_loss: 14.5897 - val_mse: 14.5897 - val_mae: 1.7277 - lr: 6.7191e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 16/100\n",
            "1000/1000 - 11s - loss: 14.6419 - mse: 14.6419 - mae: 1.5873 - val_loss: 14.7353 - val_mse: 14.7353 - val_mae: 1.5184 - lr: 6.7191e-04 - 11s/epoch - 11ms/step\n",
            "Score for fold 2: loss of 14.735328674316406\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 11s - loss: 16.0625 - mse: 16.0625 - mae: 1.5898 - val_loss: 9.6356 - val_mse: 9.6356 - val_mae: 1.5173 - lr: 6.7191e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 11s - loss: 15.8608 - mse: 15.8608 - mae: 1.5885 - val_loss: 9.4810 - val_mse: 9.4810 - val_mae: 1.5672 - lr: 6.7191e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 11s - loss: 15.8747 - mse: 15.8747 - mae: 1.5804 - val_loss: 9.3814 - val_mse: 9.3814 - val_mae: 1.6489 - lr: 6.7191e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 11s - loss: 15.7882 - mse: 15.7882 - mae: 1.5868 - val_loss: 9.5803 - val_mse: 9.5803 - val_mae: 1.5143 - lr: 6.7191e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 11s - loss: 15.8628 - mse: 15.8628 - mae: 1.5805 - val_loss: 9.6062 - val_mse: 9.6062 - val_mae: 1.6862 - lr: 6.7191e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 11s - loss: 15.7832 - mse: 15.7832 - mae: 1.5820 - val_loss: 9.7218 - val_mse: 9.7218 - val_mae: 1.5929 - lr: 6.7191e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 11s - loss: 15.6064 - mse: 15.6064 - mae: 1.5788 - val_loss: 10.1330 - val_mse: 10.1330 - val_mae: 1.7316 - lr: 6.7191e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 11s - loss: 15.7743 - mse: 15.7743 - mae: 1.5765 - val_loss: 10.0697 - val_mse: 10.0697 - val_mae: 1.4882 - lr: 6.7191e-04 - 11s/epoch - 11ms/step\n",
            "Score for fold 3: loss of 10.069652557373047\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 11s - loss: 13.7794 - mse: 13.7794 - mae: 1.5713 - val_loss: 17.4822 - val_mse: 17.4822 - val_mae: 1.6567 - lr: 6.7191e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 11s - loss: 13.7278 - mse: 13.7278 - mae: 1.5684 - val_loss: 17.0400 - val_mse: 17.0400 - val_mae: 1.5703 - lr: 6.7191e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 12s - loss: 13.5565 - mse: 13.5565 - mae: 1.5750 - val_loss: 17.1436 - val_mse: 17.1436 - val_mae: 1.5651 - lr: 6.7191e-04 - 12s/epoch - 12ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 11s - loss: 13.6055 - mse: 13.6055 - mae: 1.5717 - val_loss: 17.3122 - val_mse: 17.3122 - val_mae: 1.4827 - lr: 6.7191e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 11s - loss: 13.7591 - mse: 13.7591 - mae: 1.5731 - val_loss: 17.1628 - val_mse: 17.1628 - val_mae: 1.5700 - lr: 6.7191e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 11s - loss: 13.7134 - mse: 13.7134 - mae: 1.5732 - val_loss: 17.2488 - val_mse: 17.2488 - val_mae: 1.5062 - lr: 6.7191e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 11s - loss: 13.5878 - mse: 13.5878 - mae: 1.5730 - val_loss: 17.4497 - val_mse: 17.4497 - val_mae: 1.5503 - lr: 6.7191e-04 - 11s/epoch - 11ms/step\n",
            "Score for fold 4: loss of 17.449748992919922\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 12s - loss: 12.8504 - mse: 12.8504 - mae: 1.5813 - val_loss: 21.2761 - val_mse: 21.2761 - val_mae: 1.8421 - lr: 6.7191e-04 - 12s/epoch - 12ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 11s - loss: 12.6886 - mse: 12.6886 - mae: 1.5944 - val_loss: 20.7417 - val_mse: 20.7417 - val_mae: 1.5735 - lr: 6.7191e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 11s - loss: 12.7902 - mse: 12.7902 - mae: 1.5929 - val_loss: 20.7297 - val_mse: 20.7297 - val_mae: 1.5413 - lr: 6.7191e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 11s - loss: 12.6583 - mse: 12.6583 - mae: 1.5974 - val_loss: 21.4077 - val_mse: 21.4077 - val_mae: 1.6118 - lr: 6.7191e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 11s - loss: 12.5639 - mse: 12.5639 - mae: 1.5988 - val_loss: 20.6670 - val_mse: 20.6670 - val_mae: 1.5570 - lr: 6.7191e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 11s - loss: 12.7302 - mse: 12.7302 - mae: 1.6004 - val_loss: 20.8711 - val_mse: 20.8711 - val_mae: 1.5962 - lr: 6.7191e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 11s - loss: 12.5513 - mse: 12.5513 - mae: 1.5976 - val_loss: 21.0677 - val_mse: 21.0677 - val_mae: 1.5233 - lr: 6.7191e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 11s - loss: 12.7048 - mse: 12.7048 - mae: 1.5924 - val_loss: 20.7823 - val_mse: 20.7823 - val_mae: 1.5904 - lr: 6.7191e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 13s - loss: 12.5250 - mse: 12.5250 - mae: 1.6013 - val_loss: 20.9532 - val_mse: 20.9532 - val_mae: 1.5432 - lr: 6.7191e-04 - 13s/epoch - 13ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 11s - loss: 12.5862 - mse: 12.5862 - mae: 1.6067 - val_loss: 20.8715 - val_mse: 20.8715 - val_mae: 1.6436 - lr: 6.7191e-04 - 11s/epoch - 11ms/step\n",
            "Score for fold 5: loss of 20.871435165405273\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-10-15 16:24:48,140]\u001b[0m Finished trial#37 resulted in value: 15.014000000000001. Current best value is 14.618 with parameters: {'activation': 'relu', 'num_hidden_layer': 4, 'i': 10, 'learning_rate': 0.0001995902281877497}.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 3s - loss: 16.2788 - mse: 16.2788 - mae: 1.6894 - val_loss: 15.4463 - val_mse: 15.4463 - val_mae: 1.6325 - lr: 4.3473e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 3s - loss: 15.7961 - mse: 15.7961 - mae: 1.6352 - val_loss: 15.2647 - val_mse: 15.2647 - val_mae: 1.6381 - lr: 4.3473e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 15.7556 - mse: 15.7556 - mae: 1.6326 - val_loss: 15.3146 - val_mse: 15.3146 - val_mae: 1.6401 - lr: 4.3473e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 3s - loss: 15.7658 - mse: 15.7658 - mae: 1.6376 - val_loss: 15.2781 - val_mse: 15.2781 - val_mae: 1.6650 - lr: 4.3473e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 15.7257 - mse: 15.7257 - mae: 1.6387 - val_loss: 15.3099 - val_mse: 15.3099 - val_mae: 1.6099 - lr: 4.3473e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 15.7544 - mse: 15.7544 - mae: 1.6402 - val_loss: 15.3185 - val_mse: 15.3185 - val_mae: 1.6642 - lr: 4.3473e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 15.7539 - mse: 15.7539 - mae: 1.6385 - val_loss: 15.2525 - val_mse: 15.2525 - val_mae: 1.6491 - lr: 4.3473e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 2s - loss: 15.7517 - mse: 15.7517 - mae: 1.6387 - val_loss: 15.2507 - val_mse: 15.2507 - val_mae: 1.6657 - lr: 4.3473e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 2s - loss: 15.7667 - mse: 15.7667 - mae: 1.6413 - val_loss: 15.3146 - val_mse: 15.3146 - val_mae: 1.6310 - lr: 4.3473e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 2s - loss: 15.7398 - mse: 15.7398 - mae: 1.6399 - val_loss: 15.3400 - val_mse: 15.3400 - val_mae: 1.5833 - lr: 4.3473e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 3s - loss: 15.7612 - mse: 15.7612 - mae: 1.6400 - val_loss: 15.3024 - val_mse: 15.3024 - val_mae: 1.6194 - lr: 4.3473e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 2s - loss: 15.7569 - mse: 15.7569 - mae: 1.6362 - val_loss: 15.3112 - val_mse: 15.3112 - val_mae: 1.6055 - lr: 4.3473e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 13/100\n",
            "1000/1000 - 2s - loss: 15.7420 - mse: 15.7420 - mae: 1.6388 - val_loss: 15.2730 - val_mse: 15.2730 - val_mae: 1.6172 - lr: 4.3473e-04 - 2s/epoch - 2ms/step\n",
            "Score for fold 1: loss of 15.273025512695312\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 3s - loss: 16.0560 - mse: 16.0560 - mae: 1.6383 - val_loss: 13.9905 - val_mse: 13.9905 - val_mae: 1.6764 - lr: 4.3473e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 3s - loss: 16.0575 - mse: 16.0575 - mae: 1.6393 - val_loss: 14.0973 - val_mse: 14.0973 - val_mae: 1.6090 - lr: 4.3473e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 16.0748 - mse: 16.0748 - mae: 1.6363 - val_loss: 13.9956 - val_mse: 13.9956 - val_mae: 1.6790 - lr: 4.3473e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 3s - loss: 16.0686 - mse: 16.0686 - mae: 1.6401 - val_loss: 14.0509 - val_mse: 14.0509 - val_mae: 1.6417 - lr: 4.3473e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 16.0362 - mse: 16.0362 - mae: 1.6355 - val_loss: 14.0166 - val_mse: 14.0166 - val_mae: 1.6605 - lr: 4.3473e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 16.0516 - mse: 16.0516 - mae: 1.6359 - val_loss: 14.0887 - val_mse: 14.0887 - val_mae: 1.6570 - lr: 4.3473e-04 - 2s/epoch - 2ms/step\n",
            "Score for fold 2: loss of 14.088648796081543\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 3s - loss: 16.0837 - mse: 16.0837 - mae: 1.6396 - val_loss: 14.0345 - val_mse: 14.0345 - val_mae: 1.6063 - lr: 4.3473e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 16.0894 - mse: 16.0894 - mae: 1.6443 - val_loss: 13.9900 - val_mse: 13.9900 - val_mae: 1.6006 - lr: 4.3473e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 3s - loss: 16.0582 - mse: 16.0582 - mae: 1.6421 - val_loss: 14.1477 - val_mse: 14.1477 - val_mae: 1.6010 - lr: 4.3473e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 3s - loss: 16.0741 - mse: 16.0741 - mae: 1.6442 - val_loss: 14.0397 - val_mse: 14.0397 - val_mae: 1.6243 - lr: 4.3473e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 3s - loss: 16.0795 - mse: 16.0795 - mae: 1.6426 - val_loss: 13.9525 - val_mse: 13.9525 - val_mae: 1.6229 - lr: 4.3473e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 3s - loss: 16.0950 - mse: 16.0950 - mae: 1.6353 - val_loss: 14.0341 - val_mse: 14.0341 - val_mae: 1.5956 - lr: 4.3473e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 16.0532 - mse: 16.0532 - mae: 1.6437 - val_loss: 13.8960 - val_mse: 13.8960 - val_mae: 1.6192 - lr: 4.3473e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 2s - loss: 16.0491 - mse: 16.0491 - mae: 1.6398 - val_loss: 14.1876 - val_mse: 14.1876 - val_mae: 1.5479 - lr: 4.3473e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 3s - loss: 16.0865 - mse: 16.0865 - mae: 1.6360 - val_loss: 13.9767 - val_mse: 13.9767 - val_mae: 1.6282 - lr: 4.3473e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 3s - loss: 16.0551 - mse: 16.0551 - mae: 1.6419 - val_loss: 13.9269 - val_mse: 13.9269 - val_mae: 1.7255 - lr: 4.3473e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 3s - loss: 16.0720 - mse: 16.0720 - mae: 1.6416 - val_loss: 14.1439 - val_mse: 14.1439 - val_mae: 1.5765 - lr: 4.3473e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 3s - loss: 16.0923 - mse: 16.0923 - mae: 1.6398 - val_loss: 13.9483 - val_mse: 13.9483 - val_mae: 1.6615 - lr: 4.3473e-04 - 3s/epoch - 3ms/step\n",
            "Score for fold 3: loss of 13.94825553894043\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 14.2815 - mse: 14.2815 - mae: 1.6341 - val_loss: 21.1856 - val_mse: 21.1856 - val_mae: 1.6627 - lr: 4.3473e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 3s - loss: 14.2698 - mse: 14.2698 - mae: 1.6324 - val_loss: 21.1849 - val_mse: 21.1849 - val_mae: 1.7076 - lr: 4.3473e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 3s - loss: 14.2452 - mse: 14.2452 - mae: 1.6355 - val_loss: 21.1898 - val_mse: 21.1898 - val_mae: 1.6724 - lr: 4.3473e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 14.2954 - mse: 14.2954 - mae: 1.6339 - val_loss: 21.2219 - val_mse: 21.2219 - val_mae: 1.6220 - lr: 4.3473e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 14.2560 - mse: 14.2560 - mae: 1.6287 - val_loss: 21.2039 - val_mse: 21.2039 - val_mae: 1.6489 - lr: 4.3473e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 14.2564 - mse: 14.2564 - mae: 1.6350 - val_loss: 21.2295 - val_mse: 21.2295 - val_mae: 1.7450 - lr: 4.3473e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 14.2540 - mse: 14.2540 - mae: 1.6327 - val_loss: 21.2682 - val_mse: 21.2682 - val_mae: 1.6136 - lr: 4.3473e-04 - 2s/epoch - 2ms/step\n",
            "Score for fold 4: loss of 21.26824951171875\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 16.1515 - mse: 16.1515 - mae: 1.6270 - val_loss: 13.6522 - val_mse: 13.6522 - val_mae: 1.7060 - lr: 4.3473e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 16.1463 - mse: 16.1463 - mae: 1.6301 - val_loss: 13.6535 - val_mse: 13.6535 - val_mae: 1.6664 - lr: 4.3473e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 3s - loss: 16.1324 - mse: 16.1324 - mae: 1.6356 - val_loss: 13.7110 - val_mse: 13.7110 - val_mae: 1.6196 - lr: 4.3473e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 16.1556 - mse: 16.1556 - mae: 1.6303 - val_loss: 13.6507 - val_mse: 13.6507 - val_mae: 1.6716 - lr: 4.3473e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 3s - loss: 16.1397 - mse: 16.1397 - mae: 1.6281 - val_loss: 13.6908 - val_mse: 13.6908 - val_mae: 1.6257 - lr: 4.3473e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 16.1451 - mse: 16.1451 - mae: 1.6307 - val_loss: 13.6472 - val_mse: 13.6472 - val_mae: 1.6701 - lr: 4.3473e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 16.1822 - mse: 16.1822 - mae: 1.6256 - val_loss: 13.8083 - val_mse: 13.8083 - val_mae: 1.5842 - lr: 4.3473e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 2s - loss: 16.1484 - mse: 16.1484 - mae: 1.6223 - val_loss: 13.6832 - val_mse: 13.6832 - val_mae: 1.6352 - lr: 4.3473e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 2s - loss: 16.1403 - mse: 16.1403 - mae: 1.6351 - val_loss: 13.7103 - val_mse: 13.7103 - val_mae: 1.7562 - lr: 4.3473e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 3s - loss: 16.1013 - mse: 16.1013 - mae: 1.6355 - val_loss: 13.6822 - val_mse: 13.6822 - val_mae: 1.6309 - lr: 4.3473e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 3s - loss: 16.1573 - mse: 16.1573 - mae: 1.6260 - val_loss: 13.6675 - val_mse: 13.6675 - val_mae: 1.7158 - lr: 4.3473e-04 - 3s/epoch - 3ms/step\n",
            "Score for fold 5: loss of 13.667549133300781\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-10-15 16:27:19,408]\u001b[0m Finished trial#38 resulted in value: 15.65. Current best value is 14.618 with parameters: {'activation': 'relu', 'num_hidden_layer': 4, 'i': 10, 'learning_rate': 0.0001995902281877497}.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 3s - loss: 18.2848 - mse: 18.2848 - mae: 1.6777 - val_loss: 11.2994 - val_mse: 11.2994 - val_mae: 1.6156 - lr: 1.7620e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 3s - loss: 16.6851 - mse: 16.6851 - mae: 1.6191 - val_loss: 11.1023 - val_mse: 11.1023 - val_mae: 1.5803 - lr: 1.7620e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 16.5034 - mse: 16.5034 - mae: 1.6036 - val_loss: 10.9811 - val_mse: 10.9811 - val_mae: 1.5804 - lr: 1.7620e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 16.3820 - mse: 16.3820 - mae: 1.6004 - val_loss: 10.9091 - val_mse: 10.9091 - val_mae: 1.5663 - lr: 1.7620e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 16.3107 - mse: 16.3107 - mae: 1.5960 - val_loss: 10.8459 - val_mse: 10.8459 - val_mae: 1.5624 - lr: 1.7620e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 16.3045 - mse: 16.3045 - mae: 1.5953 - val_loss: 10.8205 - val_mse: 10.8205 - val_mae: 1.5706 - lr: 1.7620e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 16.2609 - mse: 16.2609 - mae: 1.5969 - val_loss: 10.8043 - val_mse: 10.8043 - val_mae: 1.5571 - lr: 1.7620e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 2s - loss: 16.2485 - mse: 16.2485 - mae: 1.5970 - val_loss: 10.7929 - val_mse: 10.7929 - val_mae: 1.5731 - lr: 1.7620e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 2s - loss: 16.2251 - mse: 16.2251 - mae: 1.5993 - val_loss: 10.7903 - val_mse: 10.7903 - val_mae: 1.5562 - lr: 1.7620e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 2s - loss: 16.2083 - mse: 16.2083 - mae: 1.5977 - val_loss: 10.7891 - val_mse: 10.7891 - val_mae: 1.5412 - lr: 1.7620e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 2s - loss: 16.2099 - mse: 16.2099 - mae: 1.5981 - val_loss: 10.7712 - val_mse: 10.7712 - val_mae: 1.5716 - lr: 1.7620e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 2s - loss: 16.1988 - mse: 16.1988 - mae: 1.5971 - val_loss: 10.7669 - val_mse: 10.7669 - val_mae: 1.5624 - lr: 1.7620e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 13/100\n",
            "1000/1000 - 2s - loss: 16.1885 - mse: 16.1885 - mae: 1.5975 - val_loss: 10.7664 - val_mse: 10.7664 - val_mae: 1.5553 - lr: 1.7620e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 14/100\n",
            "1000/1000 - 2s - loss: 16.1985 - mse: 16.1985 - mae: 1.5977 - val_loss: 10.7598 - val_mse: 10.7598 - val_mae: 1.5532 - lr: 1.7620e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 15/100\n",
            "1000/1000 - 2s - loss: 16.1631 - mse: 16.1631 - mae: 1.5949 - val_loss: 10.7428 - val_mse: 10.7428 - val_mae: 1.5666 - lr: 1.7620e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 16/100\n",
            "1000/1000 - 2s - loss: 16.1621 - mse: 16.1621 - mae: 1.5963 - val_loss: 10.7477 - val_mse: 10.7477 - val_mae: 1.5460 - lr: 1.7620e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 17/100\n",
            "1000/1000 - 2s - loss: 16.1433 - mse: 16.1433 - mae: 1.5955 - val_loss: 10.7439 - val_mse: 10.7439 - val_mae: 1.5626 - lr: 1.7620e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 18/100\n",
            "1000/1000 - 2s - loss: 16.1420 - mse: 16.1420 - mae: 1.5973 - val_loss: 10.7471 - val_mse: 10.7471 - val_mae: 1.5588 - lr: 1.7620e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 19/100\n",
            "1000/1000 - 2s - loss: 16.1364 - mse: 16.1364 - mae: 1.5943 - val_loss: 10.7603 - val_mse: 10.7603 - val_mae: 1.5346 - lr: 1.7620e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 20/100\n",
            "1000/1000 - 2s - loss: 16.1168 - mse: 16.1168 - mae: 1.5926 - val_loss: 10.7269 - val_mse: 10.7269 - val_mae: 1.5735 - lr: 1.7620e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 21/100\n",
            "1000/1000 - 2s - loss: 16.1154 - mse: 16.1154 - mae: 1.5949 - val_loss: 10.7314 - val_mse: 10.7314 - val_mae: 1.5534 - lr: 1.7620e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 22/100\n",
            "1000/1000 - 2s - loss: 16.1068 - mse: 16.1068 - mae: 1.5937 - val_loss: 10.7259 - val_mse: 10.7259 - val_mae: 1.5552 - lr: 1.7620e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 23/100\n",
            "1000/1000 - 2s - loss: 16.1121 - mse: 16.1121 - mae: 1.5929 - val_loss: 10.7457 - val_mse: 10.7457 - val_mae: 1.5438 - lr: 1.7620e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 24/100\n",
            "1000/1000 - 2s - loss: 16.0848 - mse: 16.0848 - mae: 1.5988 - val_loss: 10.7300 - val_mse: 10.7300 - val_mae: 1.5441 - lr: 1.7620e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 25/100\n",
            "1000/1000 - 2s - loss: 16.0828 - mse: 16.0828 - mae: 1.5943 - val_loss: 10.7298 - val_mse: 10.7298 - val_mae: 1.5599 - lr: 1.7620e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 26/100\n",
            "1000/1000 - 2s - loss: 16.0958 - mse: 16.0958 - mae: 1.5939 - val_loss: 10.7188 - val_mse: 10.7188 - val_mae: 1.5672 - lr: 1.7620e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 27/100\n",
            "1000/1000 - 2s - loss: 16.0820 - mse: 16.0820 - mae: 1.5952 - val_loss: 10.7158 - val_mse: 10.7158 - val_mae: 1.5683 - lr: 1.7620e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 28/100\n",
            "1000/1000 - 2s - loss: 16.0731 - mse: 16.0731 - mae: 1.5935 - val_loss: 10.7215 - val_mse: 10.7215 - val_mae: 1.5703 - lr: 1.7620e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 29/100\n",
            "1000/1000 - 2s - loss: 16.0408 - mse: 16.0408 - mae: 1.5939 - val_loss: 10.7109 - val_mse: 10.7109 - val_mae: 1.5542 - lr: 1.7620e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 30/100\n",
            "1000/1000 - 2s - loss: 16.0601 - mse: 16.0601 - mae: 1.5919 - val_loss: 10.7226 - val_mse: 10.7226 - val_mae: 1.5442 - lr: 1.7620e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 31/100\n",
            "1000/1000 - 2s - loss: 16.0636 - mse: 16.0636 - mae: 1.5947 - val_loss: 10.7139 - val_mse: 10.7139 - val_mae: 1.5468 - lr: 1.7620e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 32/100\n",
            "1000/1000 - 2s - loss: 16.0517 - mse: 16.0517 - mae: 1.5916 - val_loss: 10.7045 - val_mse: 10.7045 - val_mae: 1.5624 - lr: 1.7620e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 33/100\n",
            "1000/1000 - 2s - loss: 16.0452 - mse: 16.0452 - mae: 1.5913 - val_loss: 10.6953 - val_mse: 10.6953 - val_mae: 1.5591 - lr: 1.7620e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 34/100\n",
            "1000/1000 - 2s - loss: 16.0357 - mse: 16.0357 - mae: 1.5923 - val_loss: 10.6971 - val_mse: 10.6971 - val_mae: 1.5725 - lr: 1.7620e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 35/100\n",
            "1000/1000 - 2s - loss: 16.0531 - mse: 16.0531 - mae: 1.5910 - val_loss: 10.7053 - val_mse: 10.7053 - val_mae: 1.5452 - lr: 1.7620e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 36/100\n",
            "1000/1000 - 2s - loss: 16.0426 - mse: 16.0426 - mae: 1.5870 - val_loss: 10.6930 - val_mse: 10.6930 - val_mae: 1.5540 - lr: 1.7620e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 37/100\n",
            "1000/1000 - 2s - loss: 16.0268 - mse: 16.0268 - mae: 1.5937 - val_loss: 10.6982 - val_mse: 10.6982 - val_mae: 1.5394 - lr: 1.7620e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 38/100\n",
            "1000/1000 - 2s - loss: 16.0247 - mse: 16.0247 - mae: 1.5881 - val_loss: 10.6939 - val_mse: 10.6939 - val_mae: 1.5590 - lr: 1.7620e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 39/100\n",
            "1000/1000 - 2s - loss: 16.0228 - mse: 16.0228 - mae: 1.5906 - val_loss: 10.7033 - val_mse: 10.7033 - val_mae: 1.5494 - lr: 1.7620e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 40/100\n",
            "1000/1000 - 2s - loss: 16.0104 - mse: 16.0104 - mae: 1.5906 - val_loss: 10.6904 - val_mse: 10.6904 - val_mae: 1.5553 - lr: 1.7620e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 41/100\n",
            "1000/1000 - 2s - loss: 16.0215 - mse: 16.0215 - mae: 1.5867 - val_loss: 10.6736 - val_mse: 10.6736 - val_mae: 1.5750 - lr: 1.7620e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 42/100\n",
            "1000/1000 - 2s - loss: 16.0022 - mse: 16.0022 - mae: 1.5944 - val_loss: 10.6841 - val_mse: 10.6841 - val_mae: 1.5527 - lr: 1.7620e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 43/100\n",
            "1000/1000 - 2s - loss: 16.0106 - mse: 16.0106 - mae: 1.5894 - val_loss: 10.6845 - val_mse: 10.6845 - val_mae: 1.5540 - lr: 1.7620e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 44/100\n",
            "1000/1000 - 2s - loss: 15.9950 - mse: 15.9950 - mae: 1.5876 - val_loss: 10.6863 - val_mse: 10.6863 - val_mae: 1.5625 - lr: 1.7620e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 45/100\n",
            "1000/1000 - 2s - loss: 16.0029 - mse: 16.0029 - mae: 1.5880 - val_loss: 10.6901 - val_mse: 10.6901 - val_mae: 1.5505 - lr: 1.7620e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 46/100\n",
            "1000/1000 - 2s - loss: 16.0021 - mse: 16.0021 - mae: 1.5870 - val_loss: 10.6711 - val_mse: 10.6711 - val_mae: 1.5567 - lr: 1.7620e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 47/100\n",
            "1000/1000 - 2s - loss: 15.9989 - mse: 15.9989 - mae: 1.5894 - val_loss: 10.7008 - val_mse: 10.7008 - val_mae: 1.5226 - lr: 1.7620e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 48/100\n",
            "1000/1000 - 2s - loss: 15.9985 - mse: 15.9985 - mae: 1.5842 - val_loss: 10.6622 - val_mse: 10.6622 - val_mae: 1.5607 - lr: 1.7620e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 49/100\n",
            "1000/1000 - 2s - loss: 15.9873 - mse: 15.9873 - mae: 1.5878 - val_loss: 10.6804 - val_mse: 10.6804 - val_mae: 1.5506 - lr: 1.7620e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 50/100\n",
            "1000/1000 - 2s - loss: 15.9823 - mse: 15.9823 - mae: 1.5856 - val_loss: 10.6937 - val_mse: 10.6937 - val_mae: 1.5396 - lr: 1.7620e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 51/100\n",
            "1000/1000 - 2s - loss: 15.9878 - mse: 15.9878 - mae: 1.5883 - val_loss: 10.6722 - val_mse: 10.6722 - val_mae: 1.5515 - lr: 1.7620e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 52/100\n",
            "1000/1000 - 2s - loss: 15.9894 - mse: 15.9894 - mae: 1.5849 - val_loss: 10.6682 - val_mse: 10.6682 - val_mae: 1.5599 - lr: 1.7620e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 53/100\n",
            "1000/1000 - 2s - loss: 15.9811 - mse: 15.9811 - mae: 1.5840 - val_loss: 10.6770 - val_mse: 10.6770 - val_mae: 1.5439 - lr: 1.7620e-04 - 2s/epoch - 2ms/step\n",
            "Score for fold 1: loss of 10.676985740661621\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 11.9353 - mse: 11.9353 - mae: 1.5696 - val_loss: 26.8802 - val_mse: 26.8802 - val_mae: 1.6092 - lr: 1.7620e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 11.9362 - mse: 11.9362 - mae: 1.5671 - val_loss: 26.8589 - val_mse: 26.8589 - val_mae: 1.6236 - lr: 1.7620e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 11.9220 - mse: 11.9220 - mae: 1.5681 - val_loss: 26.8823 - val_mse: 26.8823 - val_mae: 1.6163 - lr: 1.7620e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 11.9173 - mse: 11.9173 - mae: 1.5670 - val_loss: 26.8747 - val_mse: 26.8747 - val_mae: 1.6119 - lr: 1.7620e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 11.9114 - mse: 11.9114 - mae: 1.5670 - val_loss: 26.8738 - val_mse: 26.8738 - val_mae: 1.6176 - lr: 1.7620e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 11.9219 - mse: 11.9219 - mae: 1.5652 - val_loss: 26.8723 - val_mse: 26.8723 - val_mae: 1.6205 - lr: 1.7620e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 11.9107 - mse: 11.9107 - mae: 1.5704 - val_loss: 26.8982 - val_mse: 26.8982 - val_mae: 1.5986 - lr: 1.7620e-04 - 2s/epoch - 2ms/step\n",
            "Score for fold 2: loss of 26.898183822631836\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 15.7589 - mse: 15.7589 - mae: 1.5705 - val_loss: 11.5699 - val_mse: 11.5699 - val_mae: 1.5976 - lr: 1.7620e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 15.7694 - mse: 15.7694 - mae: 1.5719 - val_loss: 11.5793 - val_mse: 11.5793 - val_mae: 1.5763 - lr: 1.7620e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 15.7451 - mse: 15.7451 - mae: 1.5754 - val_loss: 11.6055 - val_mse: 11.6055 - val_mae: 1.5701 - lr: 1.7620e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 15.7495 - mse: 15.7495 - mae: 1.5731 - val_loss: 11.6104 - val_mse: 11.6104 - val_mae: 1.5646 - lr: 1.7620e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 15.7514 - mse: 15.7514 - mae: 1.5710 - val_loss: 11.6067 - val_mse: 11.6067 - val_mae: 1.5781 - lr: 1.7620e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 15.7556 - mse: 15.7556 - mae: 1.5726 - val_loss: 11.6026 - val_mse: 11.6026 - val_mae: 1.5671 - lr: 1.7620e-04 - 2s/epoch - 2ms/step\n",
            "Score for fold 3: loss of 11.60254955291748\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 14.9222 - mse: 14.9222 - mae: 1.5682 - val_loss: 14.9647 - val_mse: 14.9647 - val_mae: 1.5702 - lr: 1.7620e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 14.9225 - mse: 14.9225 - mae: 1.5668 - val_loss: 14.8846 - val_mse: 14.8846 - val_mae: 1.5911 - lr: 1.7620e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 14.9172 - mse: 14.9172 - mae: 1.5699 - val_loss: 14.9456 - val_mse: 14.9456 - val_mae: 1.5863 - lr: 1.7620e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 14.9185 - mse: 14.9185 - mae: 1.5686 - val_loss: 14.9179 - val_mse: 14.9179 - val_mae: 1.6038 - lr: 1.7620e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 14.9040 - mse: 14.9040 - mae: 1.5700 - val_loss: 14.9609 - val_mse: 14.9609 - val_mae: 1.5976 - lr: 1.7620e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 14.9085 - mse: 14.9085 - mae: 1.5698 - val_loss: 14.9743 - val_mse: 14.9743 - val_mae: 1.5723 - lr: 1.7620e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 14.9072 - mse: 14.9072 - mae: 1.5665 - val_loss: 14.9685 - val_mse: 14.9685 - val_mae: 1.6103 - lr: 1.7620e-04 - 2s/epoch - 2ms/step\n",
            "Score for fold 4: loss of 14.96853256225586\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 15.9937 - mse: 15.9937 - mae: 1.5833 - val_loss: 10.5878 - val_mse: 10.5878 - val_mae: 1.5410 - lr: 1.7620e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 16.0028 - mse: 16.0028 - mae: 1.5809 - val_loss: 10.5473 - val_mse: 10.5473 - val_mae: 1.5510 - lr: 1.7620e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 15.9904 - mse: 15.9904 - mae: 1.5797 - val_loss: 10.5487 - val_mse: 10.5487 - val_mae: 1.5597 - lr: 1.7620e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 15.9866 - mse: 15.9866 - mae: 1.5822 - val_loss: 10.5739 - val_mse: 10.5739 - val_mae: 1.5351 - lr: 1.7620e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 15.9856 - mse: 15.9856 - mae: 1.5787 - val_loss: 10.5566 - val_mse: 10.5566 - val_mae: 1.5617 - lr: 1.7620e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 15.9748 - mse: 15.9748 - mae: 1.5821 - val_loss: 10.5621 - val_mse: 10.5621 - val_mae: 1.5526 - lr: 1.7620e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 15.9780 - mse: 15.9780 - mae: 1.5813 - val_loss: 10.5879 - val_mse: 10.5879 - val_mae: 1.5577 - lr: 1.7620e-04 - 2s/epoch - 2ms/step\n",
            "Score for fold 5: loss of 10.587946891784668\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-10-15 16:31:10,186]\u001b[0m Finished trial#39 resulted in value: 14.948000000000002. Current best value is 14.618 with parameters: {'activation': 'relu', 'num_hidden_layer': 4, 'i': 10, 'learning_rate': 0.0001995902281877497}.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 5s - loss: 16.2610 - mse: 16.2610 - mae: 1.6704 - val_loss: 13.5675 - val_mse: 13.5675 - val_mae: 1.6505 - lr: 9.4759e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 4s - loss: 15.9353 - mse: 15.9353 - mae: 1.6253 - val_loss: 13.4372 - val_mse: 13.4372 - val_mae: 1.5761 - lr: 9.4759e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 4s - loss: 15.8798 - mse: 15.8798 - mae: 1.6202 - val_loss: 13.4884 - val_mse: 13.4884 - val_mae: 1.6539 - lr: 9.4759e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 4s - loss: 15.7863 - mse: 15.7863 - mae: 1.6178 - val_loss: 13.4922 - val_mse: 13.4922 - val_mae: 1.6104 - lr: 9.4759e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 4s - loss: 15.7396 - mse: 15.7396 - mae: 1.6107 - val_loss: 13.3392 - val_mse: 13.3392 - val_mae: 1.5617 - lr: 9.4759e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 4s - loss: 15.7200 - mse: 15.7200 - mae: 1.6099 - val_loss: 13.5370 - val_mse: 13.5370 - val_mae: 1.5489 - lr: 9.4759e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 4s - loss: 15.6903 - mse: 15.6903 - mae: 1.6096 - val_loss: 13.5032 - val_mse: 13.5032 - val_mae: 1.5492 - lr: 9.4759e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 4s - loss: 15.6527 - mse: 15.6527 - mae: 1.6029 - val_loss: 13.3120 - val_mse: 13.3120 - val_mae: 1.5437 - lr: 9.4759e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 4s - loss: 15.5864 - mse: 15.5864 - mae: 1.6032 - val_loss: 13.3486 - val_mse: 13.3486 - val_mae: 1.5687 - lr: 9.4759e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 4s - loss: 15.5800 - mse: 15.5800 - mae: 1.6021 - val_loss: 13.2757 - val_mse: 13.2757 - val_mae: 1.6443 - lr: 9.4759e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 4s - loss: 15.5452 - mse: 15.5452 - mae: 1.5982 - val_loss: 13.2176 - val_mse: 13.2176 - val_mae: 1.5914 - lr: 9.4759e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 4s - loss: 15.4935 - mse: 15.4935 - mae: 1.5979 - val_loss: 13.2987 - val_mse: 13.2987 - val_mae: 1.5681 - lr: 9.4759e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 13/100\n",
            "1000/1000 - 6s - loss: 15.4937 - mse: 15.4937 - mae: 1.5944 - val_loss: 13.2214 - val_mse: 13.2214 - val_mae: 1.5690 - lr: 9.4759e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 14/100\n",
            "1000/1000 - 4s - loss: 15.4613 - mse: 15.4613 - mae: 1.5932 - val_loss: 13.3016 - val_mse: 13.3016 - val_mae: 1.5094 - lr: 9.4759e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 15/100\n",
            "1000/1000 - 4s - loss: 15.4443 - mse: 15.4443 - mae: 1.5919 - val_loss: 13.0838 - val_mse: 13.0838 - val_mae: 1.5709 - lr: 9.4759e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 16/100\n",
            "1000/1000 - 4s - loss: 15.4262 - mse: 15.4262 - mae: 1.5880 - val_loss: 13.2125 - val_mse: 13.2125 - val_mae: 1.5789 - lr: 9.4759e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 17/100\n",
            "1000/1000 - 4s - loss: 15.3901 - mse: 15.3901 - mae: 1.5891 - val_loss: 13.1203 - val_mse: 13.1203 - val_mae: 1.5850 - lr: 9.4759e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 18/100\n",
            "1000/1000 - 4s - loss: 15.3777 - mse: 15.3777 - mae: 1.5852 - val_loss: 13.1485 - val_mse: 13.1485 - val_mae: 1.5541 - lr: 9.4759e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 19/100\n",
            "1000/1000 - 4s - loss: 15.3687 - mse: 15.3687 - mae: 1.5849 - val_loss: 13.3146 - val_mse: 13.3146 - val_mae: 1.6555 - lr: 9.4759e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 20/100\n",
            "1000/1000 - 4s - loss: 15.3470 - mse: 15.3470 - mae: 1.5865 - val_loss: 13.1306 - val_mse: 13.1306 - val_mae: 1.6008 - lr: 9.4759e-04 - 4s/epoch - 4ms/step\n",
            "Score for fold 1: loss of 13.13061237335205\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 4s - loss: 15.7180 - mse: 15.7180 - mae: 1.5879 - val_loss: 11.8078 - val_mse: 11.8078 - val_mae: 1.6465 - lr: 9.4759e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 4s - loss: 15.7067 - mse: 15.7067 - mae: 1.5806 - val_loss: 11.6963 - val_mse: 11.6963 - val_mae: 1.5912 - lr: 9.4759e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 4s - loss: 15.6551 - mse: 15.6551 - mae: 1.5834 - val_loss: 11.7336 - val_mse: 11.7336 - val_mae: 1.5994 - lr: 9.4759e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 4s - loss: 15.6754 - mse: 15.6754 - mae: 1.5836 - val_loss: 11.7640 - val_mse: 11.7640 - val_mae: 1.5991 - lr: 9.4759e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 4s - loss: 15.5931 - mse: 15.5931 - mae: 1.5785 - val_loss: 11.7714 - val_mse: 11.7714 - val_mae: 1.5524 - lr: 9.4759e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 4s - loss: 15.6190 - mse: 15.6190 - mae: 1.5810 - val_loss: 11.7543 - val_mse: 11.7543 - val_mae: 1.5679 - lr: 9.4759e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 4s - loss: 15.5834 - mse: 15.5834 - mae: 1.5791 - val_loss: 11.8356 - val_mse: 11.8356 - val_mae: 1.6482 - lr: 9.4759e-04 - 4s/epoch - 4ms/step\n",
            "Score for fold 2: loss of 11.83564281463623\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 4s - loss: 13.8557 - mse: 13.8557 - mae: 1.5726 - val_loss: 18.7635 - val_mse: 18.7635 - val_mae: 1.6213 - lr: 9.4759e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 4s - loss: 13.8475 - mse: 13.8475 - mae: 1.5722 - val_loss: 18.7608 - val_mse: 18.7608 - val_mae: 1.6235 - lr: 9.4759e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 4s - loss: 13.7979 - mse: 13.7979 - mae: 1.5688 - val_loss: 19.0659 - val_mse: 19.0659 - val_mae: 1.6093 - lr: 9.4759e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 4s - loss: 13.8170 - mse: 13.8170 - mae: 1.5684 - val_loss: 18.8535 - val_mse: 18.8535 - val_mae: 1.5889 - lr: 9.4759e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 4s - loss: 13.7459 - mse: 13.7459 - mae: 1.5679 - val_loss: 18.9311 - val_mse: 18.9311 - val_mae: 1.6640 - lr: 9.4759e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 4s - loss: 13.8077 - mse: 13.8077 - mae: 1.5653 - val_loss: 19.2112 - val_mse: 19.2112 - val_mae: 1.6214 - lr: 9.4759e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 4s - loss: 13.7832 - mse: 13.7832 - mae: 1.5727 - val_loss: 19.0669 - val_mse: 19.0669 - val_mae: 1.6533 - lr: 9.4759e-04 - 4s/epoch - 4ms/step\n",
            "Score for fold 3: loss of 19.066898345947266\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 4s - loss: 13.6480 - mse: 13.6480 - mae: 1.5823 - val_loss: 19.3141 - val_mse: 19.3141 - val_mae: 1.6153 - lr: 9.4759e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 4s - loss: 13.6224 - mse: 13.6224 - mae: 1.5802 - val_loss: 19.4462 - val_mse: 19.4462 - val_mae: 1.5377 - lr: 9.4759e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 4s - loss: 13.6062 - mse: 13.6062 - mae: 1.5798 - val_loss: 19.5632 - val_mse: 19.5632 - val_mae: 1.5263 - lr: 9.4759e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 4s - loss: 13.5697 - mse: 13.5697 - mae: 1.5784 - val_loss: 19.4267 - val_mse: 19.4267 - val_mae: 1.5824 - lr: 9.4759e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 4s - loss: 13.6018 - mse: 13.6018 - mae: 1.5787 - val_loss: 19.4276 - val_mse: 19.4276 - val_mae: 1.5884 - lr: 9.4759e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 4s - loss: 13.5522 - mse: 13.5522 - mae: 1.5755 - val_loss: 19.4482 - val_mse: 19.4482 - val_mae: 1.5525 - lr: 9.4759e-04 - 4s/epoch - 4ms/step\n",
            "Score for fold 4: loss of 19.448240280151367\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 4s - loss: 15.5991 - mse: 15.5991 - mae: 1.5798 - val_loss: 11.2690 - val_mse: 11.2690 - val_mae: 1.5800 - lr: 9.4759e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 4s - loss: 15.6190 - mse: 15.6190 - mae: 1.5804 - val_loss: 11.3246 - val_mse: 11.3246 - val_mae: 1.6126 - lr: 9.4759e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 4s - loss: 15.5535 - mse: 15.5535 - mae: 1.5746 - val_loss: 11.1761 - val_mse: 11.1761 - val_mae: 1.5721 - lr: 9.4759e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 4s - loss: 15.5413 - mse: 15.5413 - mae: 1.5774 - val_loss: 11.4210 - val_mse: 11.4210 - val_mae: 1.5069 - lr: 9.4759e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 4s - loss: 15.6023 - mse: 15.6023 - mae: 1.5801 - val_loss: 11.4583 - val_mse: 11.4583 - val_mae: 1.5680 - lr: 9.4759e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 4s - loss: 15.4936 - mse: 15.4936 - mae: 1.5757 - val_loss: 11.3100 - val_mse: 11.3100 - val_mae: 1.5473 - lr: 9.4759e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 4s - loss: 15.4949 - mse: 15.4949 - mae: 1.5729 - val_loss: 11.2575 - val_mse: 11.2575 - val_mae: 1.5836 - lr: 9.4759e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 4s - loss: 15.4719 - mse: 15.4719 - mae: 1.5751 - val_loss: 11.3612 - val_mse: 11.3612 - val_mae: 1.5450 - lr: 9.4759e-04 - 4s/epoch - 4ms/step\n",
            "Score for fold 5: loss of 11.361194610595703\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-10-15 16:34:41,047]\u001b[0m Finished trial#40 resulted in value: 14.969999999999999. Current best value is 14.618 with parameters: {'activation': 'relu', 'num_hidden_layer': 4, 'i': 10, 'learning_rate': 0.0001995902281877497}.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 23s - loss: 15.8075 - mse: 15.8075 - mae: 1.6097 - val_loss: 14.0953 - val_mse: 14.0953 - val_mae: 1.5462 - lr: 1.4676e-04 - 23s/epoch - 23ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 23s - loss: 15.3505 - mse: 15.3505 - mae: 1.5852 - val_loss: 14.0805 - val_mse: 14.0805 - val_mae: 1.6961 - lr: 1.4676e-04 - 23s/epoch - 23ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 23s - loss: 15.3053 - mse: 15.3053 - mae: 1.5819 - val_loss: 14.0588 - val_mse: 14.0588 - val_mae: 1.5958 - lr: 1.4676e-04 - 23s/epoch - 23ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 24s - loss: 15.2440 - mse: 15.2440 - mae: 1.5818 - val_loss: 14.4607 - val_mse: 14.4607 - val_mae: 1.5577 - lr: 1.4676e-04 - 24s/epoch - 24ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 23s - loss: 15.1784 - mse: 15.1784 - mae: 1.5812 - val_loss: 13.9056 - val_mse: 13.9056 - val_mae: 1.5871 - lr: 1.4676e-04 - 23s/epoch - 23ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 23s - loss: 15.1614 - mse: 15.1614 - mae: 1.5779 - val_loss: 14.2308 - val_mse: 14.2308 - val_mae: 1.5883 - lr: 1.4676e-04 - 23s/epoch - 23ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 24s - loss: 15.1520 - mse: 15.1520 - mae: 1.5737 - val_loss: 13.9681 - val_mse: 13.9681 - val_mae: 1.5860 - lr: 1.4676e-04 - 24s/epoch - 24ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 23s - loss: 15.1566 - mse: 15.1566 - mae: 1.5712 - val_loss: 14.0223 - val_mse: 14.0223 - val_mae: 1.5659 - lr: 1.4676e-04 - 23s/epoch - 23ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 22s - loss: 15.0909 - mse: 15.0909 - mae: 1.5722 - val_loss: 13.9708 - val_mse: 13.9708 - val_mae: 1.6037 - lr: 1.4676e-04 - 22s/epoch - 22ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 23s - loss: 15.1233 - mse: 15.1233 - mae: 1.5667 - val_loss: 13.8376 - val_mse: 13.8376 - val_mae: 1.6033 - lr: 1.4676e-04 - 23s/epoch - 23ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 23s - loss: 15.0478 - mse: 15.0478 - mae: 1.5670 - val_loss: 14.0371 - val_mse: 14.0371 - val_mae: 1.5721 - lr: 1.4676e-04 - 23s/epoch - 23ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 24s - loss: 15.0088 - mse: 15.0088 - mae: 1.5630 - val_loss: 13.9397 - val_mse: 13.9397 - val_mae: 1.6232 - lr: 1.4676e-04 - 24s/epoch - 24ms/step\n",
            "Epoch 13/100\n",
            "1000/1000 - 23s - loss: 15.0252 - mse: 15.0252 - mae: 1.5622 - val_loss: 13.9441 - val_mse: 13.9441 - val_mae: 1.5947 - lr: 1.4676e-04 - 23s/epoch - 23ms/step\n",
            "Epoch 14/100\n",
            "1000/1000 - 23s - loss: 14.9494 - mse: 14.9494 - mae: 1.5621 - val_loss: 14.2046 - val_mse: 14.2046 - val_mae: 1.5483 - lr: 1.4676e-04 - 23s/epoch - 23ms/step\n",
            "Epoch 15/100\n",
            "1000/1000 - 22s - loss: 14.9647 - mse: 14.9647 - mae: 1.5624 - val_loss: 14.0527 - val_mse: 14.0527 - val_mae: 1.7015 - lr: 1.4676e-04 - 22s/epoch - 22ms/step\n",
            "Score for fold 1: loss of 14.052751541137695\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 23s - loss: 15.1981 - mse: 15.1981 - mae: 1.5682 - val_loss: 13.1191 - val_mse: 13.1191 - val_mae: 1.5650 - lr: 1.4676e-04 - 23s/epoch - 23ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 23s - loss: 15.1953 - mse: 15.1953 - mae: 1.5632 - val_loss: 13.1050 - val_mse: 13.1050 - val_mae: 1.6111 - lr: 1.4676e-04 - 23s/epoch - 23ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 26s - loss: 15.1767 - mse: 15.1767 - mae: 1.5596 - val_loss: 13.1274 - val_mse: 13.1274 - val_mae: 1.5838 - lr: 1.4676e-04 - 26s/epoch - 26ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 23s - loss: 15.1519 - mse: 15.1519 - mae: 1.5551 - val_loss: 13.0673 - val_mse: 13.0673 - val_mae: 1.5746 - lr: 1.4676e-04 - 23s/epoch - 23ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 23s - loss: 15.0398 - mse: 15.0398 - mae: 1.5562 - val_loss: 13.1458 - val_mse: 13.1458 - val_mae: 1.6191 - lr: 1.4676e-04 - 23s/epoch - 23ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 22s - loss: 15.0231 - mse: 15.0231 - mae: 1.5566 - val_loss: 13.1943 - val_mse: 13.1943 - val_mae: 1.5799 - lr: 1.4676e-04 - 22s/epoch - 22ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 23s - loss: 15.0271 - mse: 15.0271 - mae: 1.5556 - val_loss: 13.2073 - val_mse: 13.2073 - val_mae: 1.5577 - lr: 1.4676e-04 - 23s/epoch - 23ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 23s - loss: 15.0478 - mse: 15.0478 - mae: 1.5517 - val_loss: 13.3684 - val_mse: 13.3684 - val_mae: 1.5380 - lr: 1.4676e-04 - 23s/epoch - 23ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 23s - loss: 15.0367 - mse: 15.0367 - mae: 1.5541 - val_loss: 13.2391 - val_mse: 13.2391 - val_mae: 1.5427 - lr: 1.4676e-04 - 23s/epoch - 23ms/step\n",
            "Score for fold 2: loss of 13.23912525177002\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 23s - loss: 15.2595 - mse: 15.2595 - mae: 1.5522 - val_loss: 11.9686 - val_mse: 11.9686 - val_mae: 1.5947 - lr: 1.4676e-04 - 23s/epoch - 23ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 23s - loss: 15.2580 - mse: 15.2580 - mae: 1.5514 - val_loss: 12.2864 - val_mse: 12.2864 - val_mae: 1.5026 - lr: 1.4676e-04 - 23s/epoch - 23ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 23s - loss: 15.2376 - mse: 15.2376 - mae: 1.5488 - val_loss: 12.2459 - val_mse: 12.2459 - val_mae: 1.5500 - lr: 1.4676e-04 - 23s/epoch - 23ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 23s - loss: 15.2215 - mse: 15.2215 - mae: 1.5486 - val_loss: 12.2380 - val_mse: 12.2380 - val_mae: 1.6300 - lr: 1.4676e-04 - 23s/epoch - 23ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 24s - loss: 15.2128 - mse: 15.2128 - mae: 1.5500 - val_loss: 12.1852 - val_mse: 12.1852 - val_mae: 1.5941 - lr: 1.4676e-04 - 24s/epoch - 24ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 23s - loss: 15.1709 - mse: 15.1709 - mae: 1.5455 - val_loss: 12.5289 - val_mse: 12.5289 - val_mae: 1.5312 - lr: 1.4676e-04 - 23s/epoch - 23ms/step\n",
            "Score for fold 3: loss of 12.528908729553223\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 23s - loss: 13.8471 - mse: 13.8471 - mae: 1.5570 - val_loss: 17.6360 - val_mse: 17.6360 - val_mae: 1.5595 - lr: 1.4676e-04 - 23s/epoch - 23ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 22s - loss: 13.7573 - mse: 13.7573 - mae: 1.5570 - val_loss: 17.8391 - val_mse: 17.8391 - val_mae: 1.4835 - lr: 1.4676e-04 - 22s/epoch - 22ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 23s - loss: 13.8839 - mse: 13.8839 - mae: 1.5495 - val_loss: 17.7415 - val_mse: 17.7415 - val_mae: 1.5481 - lr: 1.4676e-04 - 23s/epoch - 23ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 22s - loss: 13.7680 - mse: 13.7680 - mae: 1.5458 - val_loss: 18.0468 - val_mse: 18.0468 - val_mae: 1.5403 - lr: 1.4676e-04 - 22s/epoch - 22ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 23s - loss: 13.6275 - mse: 13.6275 - mae: 1.5477 - val_loss: 17.7600 - val_mse: 17.7600 - val_mae: 1.5520 - lr: 1.4676e-04 - 23s/epoch - 23ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 23s - loss: 13.7589 - mse: 13.7589 - mae: 1.5453 - val_loss: 17.7417 - val_mse: 17.7417 - val_mae: 1.5584 - lr: 1.4676e-04 - 23s/epoch - 23ms/step\n",
            "Score for fold 4: loss of 17.741714477539062\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 23s - loss: 14.0215 - mse: 14.0215 - mae: 1.5586 - val_loss: 16.4500 - val_mse: 16.4500 - val_mae: 1.5348 - lr: 1.4676e-04 - 23s/epoch - 23ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 24s - loss: 14.0143 - mse: 14.0143 - mae: 1.5547 - val_loss: 16.5066 - val_mse: 16.5066 - val_mae: 1.4880 - lr: 1.4676e-04 - 24s/epoch - 24ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 23s - loss: 13.9430 - mse: 13.9430 - mae: 1.5525 - val_loss: 16.4672 - val_mse: 16.4672 - val_mae: 1.5647 - lr: 1.4676e-04 - 23s/epoch - 23ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 23s - loss: 13.9036 - mse: 13.9036 - mae: 1.5485 - val_loss: 16.6062 - val_mse: 16.6062 - val_mae: 1.4899 - lr: 1.4676e-04 - 23s/epoch - 23ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 23s - loss: 13.8250 - mse: 13.8250 - mae: 1.5458 - val_loss: 16.7439 - val_mse: 16.7439 - val_mae: 1.5307 - lr: 1.4676e-04 - 23s/epoch - 23ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 23s - loss: 13.8897 - mse: 13.8897 - mae: 1.5485 - val_loss: 16.5053 - val_mse: 16.5053 - val_mae: 1.5508 - lr: 1.4676e-04 - 23s/epoch - 23ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-10-15 16:52:35,542]\u001b[0m Finished trial#41 resulted in value: 14.814000000000002. Current best value is 14.618 with parameters: {'activation': 'relu', 'num_hidden_layer': 4, 'i': 10, 'learning_rate': 0.0001995902281877497}.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Score for fold 5: loss of 16.505332946777344\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 23s - loss: 16.2600 - mse: 16.2600 - mae: 1.6255 - val_loss: 12.6688 - val_mse: 12.6688 - val_mae: 1.5479 - lr: 1.3716e-04 - 23s/epoch - 23ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 23s - loss: 15.7533 - mse: 15.7533 - mae: 1.6028 - val_loss: 12.5402 - val_mse: 12.5402 - val_mae: 1.5979 - lr: 1.3716e-04 - 23s/epoch - 23ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 25s - loss: 15.6940 - mse: 15.6940 - mae: 1.5957 - val_loss: 12.7156 - val_mse: 12.7156 - val_mae: 1.4922 - lr: 1.3716e-04 - 25s/epoch - 25ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 23s - loss: 15.6376 - mse: 15.6376 - mae: 1.5973 - val_loss: 12.4873 - val_mse: 12.4873 - val_mae: 1.5917 - lr: 1.3716e-04 - 23s/epoch - 23ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 23s - loss: 15.5805 - mse: 15.5805 - mae: 1.5922 - val_loss: 12.4884 - val_mse: 12.4884 - val_mae: 1.5898 - lr: 1.3716e-04 - 23s/epoch - 23ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 23s - loss: 15.5507 - mse: 15.5507 - mae: 1.5881 - val_loss: 12.5031 - val_mse: 12.5031 - val_mae: 1.5092 - lr: 1.3716e-04 - 23s/epoch - 23ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 23s - loss: 15.5656 - mse: 15.5656 - mae: 1.5850 - val_loss: 12.5022 - val_mse: 12.5022 - val_mae: 1.5226 - lr: 1.3716e-04 - 23s/epoch - 23ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 23s - loss: 15.4988 - mse: 15.4988 - mae: 1.5847 - val_loss: 12.4540 - val_mse: 12.4540 - val_mae: 1.5422 - lr: 1.3716e-04 - 23s/epoch - 23ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 23s - loss: 15.4343 - mse: 15.4343 - mae: 1.5840 - val_loss: 12.4503 - val_mse: 12.4503 - val_mae: 1.6001 - lr: 1.3716e-04 - 23s/epoch - 23ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 23s - loss: 15.4305 - mse: 15.4305 - mae: 1.5807 - val_loss: 12.4935 - val_mse: 12.4935 - val_mae: 1.5219 - lr: 1.3716e-04 - 23s/epoch - 23ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 24s - loss: 15.4304 - mse: 15.4304 - mae: 1.5784 - val_loss: 12.4835 - val_mse: 12.4835 - val_mae: 1.5277 - lr: 1.3716e-04 - 24s/epoch - 24ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 22s - loss: 15.3797 - mse: 15.3797 - mae: 1.5792 - val_loss: 12.5640 - val_mse: 12.5640 - val_mae: 1.5091 - lr: 1.3716e-04 - 22s/epoch - 22ms/step\n",
            "Epoch 13/100\n",
            "1000/1000 - 23s - loss: 15.4028 - mse: 15.4028 - mae: 1.5764 - val_loss: 12.5055 - val_mse: 12.5055 - val_mae: 1.5498 - lr: 1.3716e-04 - 23s/epoch - 23ms/step\n",
            "Epoch 14/100\n",
            "1000/1000 - 22s - loss: 15.3251 - mse: 15.3251 - mae: 1.5739 - val_loss: 12.4539 - val_mse: 12.4539 - val_mae: 1.5121 - lr: 1.3716e-04 - 22s/epoch - 22ms/step\n",
            "Score for fold 1: loss of 12.453895568847656\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 24s - loss: 15.0005 - mse: 15.0005 - mae: 1.5641 - val_loss: 13.6845 - val_mse: 13.6845 - val_mae: 1.5794 - lr: 1.3716e-04 - 24s/epoch - 24ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 23s - loss: 14.9602 - mse: 14.9602 - mae: 1.5629 - val_loss: 13.7356 - val_mse: 13.7356 - val_mae: 1.5903 - lr: 1.3716e-04 - 23s/epoch - 23ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 23s - loss: 14.9764 - mse: 14.9764 - mae: 1.5627 - val_loss: 13.9580 - val_mse: 13.9580 - val_mae: 1.5375 - lr: 1.3716e-04 - 23s/epoch - 23ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 23s - loss: 14.9455 - mse: 14.9455 - mae: 1.5604 - val_loss: 13.8975 - val_mse: 13.8975 - val_mae: 1.5329 - lr: 1.3716e-04 - 23s/epoch - 23ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 23s - loss: 14.9341 - mse: 14.9341 - mae: 1.5576 - val_loss: 14.0373 - val_mse: 14.0373 - val_mae: 1.5254 - lr: 1.3716e-04 - 23s/epoch - 23ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 23s - loss: 14.8977 - mse: 14.8977 - mae: 1.5560 - val_loss: 14.0767 - val_mse: 14.0767 - val_mae: 1.5161 - lr: 1.3716e-04 - 23s/epoch - 23ms/step\n",
            "Score for fold 2: loss of 14.076699256896973\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 22s - loss: 15.2319 - mse: 15.2319 - mae: 1.5618 - val_loss: 12.5246 - val_mse: 12.5246 - val_mae: 1.5375 - lr: 1.3716e-04 - 22s/epoch - 22ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 22s - loss: 15.2295 - mse: 15.2295 - mae: 1.5608 - val_loss: 12.4129 - val_mse: 12.4129 - val_mae: 1.5653 - lr: 1.3716e-04 - 22s/epoch - 22ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 25s - loss: 15.2061 - mse: 15.2061 - mae: 1.5595 - val_loss: 12.4445 - val_mse: 12.4445 - val_mae: 1.5611 - lr: 1.3716e-04 - 25s/epoch - 25ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 22s - loss: 15.1839 - mse: 15.1839 - mae: 1.5582 - val_loss: 12.6750 - val_mse: 12.6750 - val_mae: 1.5858 - lr: 1.3716e-04 - 22s/epoch - 22ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 23s - loss: 15.1042 - mse: 15.1042 - mae: 1.5594 - val_loss: 12.8244 - val_mse: 12.8244 - val_mae: 1.4889 - lr: 1.3716e-04 - 23s/epoch - 23ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 22s - loss: 15.1439 - mse: 15.1439 - mae: 1.5545 - val_loss: 12.4951 - val_mse: 12.4951 - val_mae: 1.5438 - lr: 1.3716e-04 - 22s/epoch - 22ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 23s - loss: 15.0547 - mse: 15.0547 - mae: 1.5515 - val_loss: 12.6486 - val_mse: 12.6486 - val_mae: 1.5690 - lr: 1.3716e-04 - 23s/epoch - 23ms/step\n",
            "Score for fold 3: loss of 12.648605346679688\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 22s - loss: 12.4823 - mse: 12.4823 - mae: 1.5501 - val_loss: 22.7162 - val_mse: 22.7162 - val_mae: 1.5983 - lr: 1.3716e-04 - 22s/epoch - 22ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 23s - loss: 12.5461 - mse: 12.5461 - mae: 1.5497 - val_loss: 22.9195 - val_mse: 22.9195 - val_mae: 1.5799 - lr: 1.3716e-04 - 23s/epoch - 23ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 24s - loss: 12.5167 - mse: 12.5167 - mae: 1.5482 - val_loss: 22.8451 - val_mse: 22.8451 - val_mae: 1.5471 - lr: 1.3716e-04 - 24s/epoch - 24ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 23s - loss: 12.4337 - mse: 12.4337 - mae: 1.5467 - val_loss: 22.7979 - val_mse: 22.7979 - val_mae: 1.6228 - lr: 1.3716e-04 - 23s/epoch - 23ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 23s - loss: 12.4724 - mse: 12.4724 - mae: 1.5455 - val_loss: 22.8189 - val_mse: 22.8189 - val_mae: 1.5408 - lr: 1.3716e-04 - 23s/epoch - 23ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 23s - loss: 12.4435 - mse: 12.4435 - mae: 1.5399 - val_loss: 22.8840 - val_mse: 22.8840 - val_mae: 1.6331 - lr: 1.3716e-04 - 23s/epoch - 23ms/step\n",
            "Score for fold 4: loss of 22.88397216796875\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 23s - loss: 15.2142 - mse: 15.2142 - mae: 1.5443 - val_loss: 11.7161 - val_mse: 11.7161 - val_mae: 1.5865 - lr: 1.3716e-04 - 23s/epoch - 23ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 22s - loss: 15.1952 - mse: 15.1952 - mae: 1.5420 - val_loss: 11.7340 - val_mse: 11.7340 - val_mae: 1.5948 - lr: 1.3716e-04 - 22s/epoch - 22ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 23s - loss: 15.1663 - mse: 15.1663 - mae: 1.5414 - val_loss: 12.0059 - val_mse: 12.0059 - val_mae: 1.5526 - lr: 1.3716e-04 - 23s/epoch - 23ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 24s - loss: 15.1251 - mse: 15.1251 - mae: 1.5342 - val_loss: 11.9214 - val_mse: 11.9214 - val_mae: 1.5607 - lr: 1.3716e-04 - 24s/epoch - 24ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 22s - loss: 15.0944 - mse: 15.0944 - mae: 1.5394 - val_loss: 11.9792 - val_mse: 11.9792 - val_mae: 1.5913 - lr: 1.3716e-04 - 22s/epoch - 22ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 22s - loss: 15.1159 - mse: 15.1159 - mae: 1.5352 - val_loss: 12.1861 - val_mse: 12.1861 - val_mae: 1.5721 - lr: 1.3716e-04 - 22s/epoch - 22ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-10-15 17:08:49,469]\u001b[0m Finished trial#42 resulted in value: 14.85. Current best value is 14.618 with parameters: {'activation': 'relu', 'num_hidden_layer': 4, 'i': 10, 'learning_rate': 0.0001995902281877497}.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Score for fold 5: loss of 12.186136245727539\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 23s - loss: 14.4604 - mse: 14.4604 - mae: 1.6079 - val_loss: 19.9005 - val_mse: 19.9005 - val_mae: 1.6113 - lr: 1.0264e-04 - 23s/epoch - 23ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 22s - loss: 13.9553 - mse: 13.9553 - mae: 1.5839 - val_loss: 19.6707 - val_mse: 19.6707 - val_mae: 1.6386 - lr: 1.0264e-04 - 22s/epoch - 22ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 22s - loss: 13.8736 - mse: 13.8736 - mae: 1.5774 - val_loss: 19.7909 - val_mse: 19.7909 - val_mae: 1.5641 - lr: 1.0264e-04 - 22s/epoch - 22ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 22s - loss: 13.8360 - mse: 13.8360 - mae: 1.5758 - val_loss: 19.7013 - val_mse: 19.7013 - val_mae: 1.6005 - lr: 1.0264e-04 - 22s/epoch - 22ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 23s - loss: 13.7714 - mse: 13.7714 - mae: 1.5768 - val_loss: 19.6107 - val_mse: 19.6107 - val_mae: 1.6390 - lr: 1.0264e-04 - 23s/epoch - 23ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 23s - loss: 13.7815 - mse: 13.7815 - mae: 1.5719 - val_loss: 19.6502 - val_mse: 19.6502 - val_mae: 1.5720 - lr: 1.0264e-04 - 23s/epoch - 23ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 22s - loss: 13.7232 - mse: 13.7232 - mae: 1.5699 - val_loss: 19.5931 - val_mse: 19.5931 - val_mae: 1.6422 - lr: 1.0264e-04 - 22s/epoch - 22ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 22s - loss: 13.6877 - mse: 13.6877 - mae: 1.5704 - val_loss: 19.8329 - val_mse: 19.8329 - val_mae: 1.5126 - lr: 1.0264e-04 - 22s/epoch - 22ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 22s - loss: 13.6884 - mse: 13.6884 - mae: 1.5639 - val_loss: 19.5743 - val_mse: 19.5743 - val_mae: 1.6500 - lr: 1.0264e-04 - 22s/epoch - 22ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 23s - loss: 13.6513 - mse: 13.6513 - mae: 1.5653 - val_loss: 19.5812 - val_mse: 19.5812 - val_mae: 1.6510 - lr: 1.0264e-04 - 23s/epoch - 23ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 22s - loss: 13.6784 - mse: 13.6784 - mae: 1.5633 - val_loss: 19.5632 - val_mse: 19.5632 - val_mae: 1.6030 - lr: 1.0264e-04 - 22s/epoch - 22ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 22s - loss: 13.6051 - mse: 13.6051 - mae: 1.5639 - val_loss: 19.5415 - val_mse: 19.5415 - val_mae: 1.6050 - lr: 1.0264e-04 - 22s/epoch - 22ms/step\n",
            "Epoch 13/100\n",
            "1000/1000 - 24s - loss: 13.6061 - mse: 13.6061 - mae: 1.5585 - val_loss: 19.5569 - val_mse: 19.5569 - val_mae: 1.6075 - lr: 1.0264e-04 - 24s/epoch - 24ms/step\n",
            "Epoch 14/100\n",
            "1000/1000 - 22s - loss: 13.6140 - mse: 13.6140 - mae: 1.5555 - val_loss: 19.6240 - val_mse: 19.6240 - val_mae: 1.5951 - lr: 1.0264e-04 - 22s/epoch - 22ms/step\n",
            "Epoch 15/100\n",
            "1000/1000 - 22s - loss: 13.5809 - mse: 13.5809 - mae: 1.5574 - val_loss: 19.5648 - val_mse: 19.5648 - val_mae: 1.5961 - lr: 1.0264e-04 - 22s/epoch - 22ms/step\n",
            "Epoch 16/100\n",
            "1000/1000 - 22s - loss: 13.5322 - mse: 13.5322 - mae: 1.5583 - val_loss: 19.4991 - val_mse: 19.4991 - val_mae: 1.6219 - lr: 1.0264e-04 - 22s/epoch - 22ms/step\n",
            "Epoch 17/100\n",
            "1000/1000 - 22s - loss: 13.5377 - mse: 13.5377 - mae: 1.5543 - val_loss: 19.5226 - val_mse: 19.5226 - val_mae: 1.6496 - lr: 1.0264e-04 - 22s/epoch - 22ms/step\n",
            "Epoch 18/100\n",
            "1000/1000 - 22s - loss: 13.5304 - mse: 13.5304 - mae: 1.5575 - val_loss: 19.4970 - val_mse: 19.4970 - val_mae: 1.6118 - lr: 1.0264e-04 - 22s/epoch - 22ms/step\n",
            "Epoch 19/100\n",
            "1000/1000 - 22s - loss: 13.4746 - mse: 13.4746 - mae: 1.5544 - val_loss: 19.6879 - val_mse: 19.6879 - val_mae: 1.5773 - lr: 1.0264e-04 - 22s/epoch - 22ms/step\n",
            "Epoch 20/100\n",
            "1000/1000 - 23s - loss: 13.4730 - mse: 13.4730 - mae: 1.5538 - val_loss: 19.5075 - val_mse: 19.5075 - val_mae: 1.6091 - lr: 1.0264e-04 - 23s/epoch - 23ms/step\n",
            "Epoch 21/100\n",
            "1000/1000 - 23s - loss: 13.5109 - mse: 13.5109 - mae: 1.5511 - val_loss: 19.6066 - val_mse: 19.6066 - val_mae: 1.5648 - lr: 1.0264e-04 - 23s/epoch - 23ms/step\n",
            "Epoch 22/100\n",
            "1000/1000 - 22s - loss: 13.5250 - mse: 13.5250 - mae: 1.5509 - val_loss: 19.6771 - val_mse: 19.6771 - val_mae: 1.5481 - lr: 1.0264e-04 - 22s/epoch - 22ms/step\n",
            "Epoch 23/100\n",
            "1000/1000 - 23s - loss: 13.5127 - mse: 13.5127 - mae: 1.5492 - val_loss: 19.4940 - val_mse: 19.4940 - val_mae: 1.6576 - lr: 1.0264e-04 - 23s/epoch - 23ms/step\n",
            "Epoch 24/100\n",
            "1000/1000 - 22s - loss: 13.4374 - mse: 13.4374 - mae: 1.5501 - val_loss: 19.6389 - val_mse: 19.6389 - val_mae: 1.5540 - lr: 1.0264e-04 - 22s/epoch - 22ms/step\n",
            "Epoch 25/100\n",
            "1000/1000 - 22s - loss: 13.4037 - mse: 13.4037 - mae: 1.5503 - val_loss: 19.5577 - val_mse: 19.5577 - val_mae: 1.6179 - lr: 1.0264e-04 - 22s/epoch - 22ms/step\n",
            "Epoch 26/100\n",
            "1000/1000 - 22s - loss: 13.4619 - mse: 13.4619 - mae: 1.5443 - val_loss: 19.5385 - val_mse: 19.5385 - val_mae: 1.6480 - lr: 1.0264e-04 - 22s/epoch - 22ms/step\n",
            "Epoch 27/100\n",
            "1000/1000 - 22s - loss: 13.4297 - mse: 13.4297 - mae: 1.5467 - val_loss: 19.4824 - val_mse: 19.4824 - val_mae: 1.6407 - lr: 1.0264e-04 - 22s/epoch - 22ms/step\n",
            "Epoch 28/100\n",
            "1000/1000 - 24s - loss: 13.4249 - mse: 13.4249 - mae: 1.5475 - val_loss: 19.4536 - val_mse: 19.4536 - val_mae: 1.6209 - lr: 1.0264e-04 - 24s/epoch - 24ms/step\n",
            "Epoch 29/100\n",
            "1000/1000 - 22s - loss: 13.3862 - mse: 13.3862 - mae: 1.5436 - val_loss: 19.5043 - val_mse: 19.5043 - val_mae: 1.6498 - lr: 1.0264e-04 - 22s/epoch - 22ms/step\n",
            "Epoch 30/100\n",
            "1000/1000 - 22s - loss: 13.3198 - mse: 13.3198 - mae: 1.5440 - val_loss: 19.5390 - val_mse: 19.5390 - val_mae: 1.5734 - lr: 1.0264e-04 - 22s/epoch - 22ms/step\n",
            "Epoch 31/100\n",
            "1000/1000 - 22s - loss: 13.3320 - mse: 13.3320 - mae: 1.5442 - val_loss: 19.4926 - val_mse: 19.4926 - val_mae: 1.5915 - lr: 1.0264e-04 - 22s/epoch - 22ms/step\n",
            "Epoch 32/100\n",
            "1000/1000 - 22s - loss: 13.3141 - mse: 13.3141 - mae: 1.5417 - val_loss: 19.6406 - val_mse: 19.6406 - val_mae: 1.5837 - lr: 1.0264e-04 - 22s/epoch - 22ms/step\n",
            "Epoch 33/100\n",
            "1000/1000 - 22s - loss: 13.2989 - mse: 13.2989 - mae: 1.5412 - val_loss: 19.5125 - val_mse: 19.5125 - val_mae: 1.6024 - lr: 1.0264e-04 - 22s/epoch - 22ms/step\n",
            "Score for fold 1: loss of 19.512535095214844\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 22s - loss: 15.8372 - mse: 15.8372 - mae: 1.5676 - val_loss: 9.4483 - val_mse: 9.4483 - val_mae: 1.4545 - lr: 1.0264e-04 - 22s/epoch - 22ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 24s - loss: 15.8382 - mse: 15.8382 - mae: 1.5643 - val_loss: 9.4538 - val_mse: 9.4538 - val_mae: 1.5105 - lr: 1.0264e-04 - 24s/epoch - 24ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 22s - loss: 15.8337 - mse: 15.8337 - mae: 1.5636 - val_loss: 9.6751 - val_mse: 9.6751 - val_mae: 1.4833 - lr: 1.0264e-04 - 22s/epoch - 22ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 23s - loss: 15.7939 - mse: 15.7939 - mae: 1.5661 - val_loss: 9.4363 - val_mse: 9.4363 - val_mae: 1.4956 - lr: 1.0264e-04 - 23s/epoch - 23ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 22s - loss: 15.7443 - mse: 15.7443 - mae: 1.5597 - val_loss: 9.5187 - val_mse: 9.5187 - val_mae: 1.5290 - lr: 1.0264e-04 - 22s/epoch - 22ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 22s - loss: 15.6844 - mse: 15.6844 - mae: 1.5627 - val_loss: 9.8127 - val_mse: 9.8127 - val_mae: 1.4478 - lr: 1.0264e-04 - 22s/epoch - 22ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 22s - loss: 15.7372 - mse: 15.7372 - mae: 1.5628 - val_loss: 9.6152 - val_mse: 9.6152 - val_mae: 1.5758 - lr: 1.0264e-04 - 22s/epoch - 22ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 22s - loss: 15.7049 - mse: 15.7049 - mae: 1.5596 - val_loss: 9.5602 - val_mse: 9.5602 - val_mae: 1.4832 - lr: 1.0264e-04 - 22s/epoch - 22ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 22s - loss: 15.7194 - mse: 15.7194 - mae: 1.5586 - val_loss: 9.6241 - val_mse: 9.6241 - val_mae: 1.5004 - lr: 1.0264e-04 - 22s/epoch - 22ms/step\n",
            "Score for fold 2: loss of 9.624123573303223\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 23s - loss: 14.5020 - mse: 14.5020 - mae: 1.5357 - val_loss: 14.2725 - val_mse: 14.2725 - val_mae: 1.5872 - lr: 1.0264e-04 - 23s/epoch - 23ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 22s - loss: 14.5307 - mse: 14.5307 - mae: 1.5350 - val_loss: 14.6432 - val_mse: 14.6432 - val_mae: 1.5575 - lr: 1.0264e-04 - 22s/epoch - 22ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 22s - loss: 14.4445 - mse: 14.4445 - mae: 1.5328 - val_loss: 14.1571 - val_mse: 14.1571 - val_mae: 1.6117 - lr: 1.0264e-04 - 22s/epoch - 22ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 22s - loss: 14.4169 - mse: 14.4169 - mae: 1.5311 - val_loss: 14.5977 - val_mse: 14.5977 - val_mae: 1.5444 - lr: 1.0264e-04 - 22s/epoch - 22ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 22s - loss: 14.3870 - mse: 14.3870 - mae: 1.5264 - val_loss: 14.1296 - val_mse: 14.1296 - val_mae: 1.6048 - lr: 1.0264e-04 - 22s/epoch - 22ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 22s - loss: 14.3824 - mse: 14.3824 - mae: 1.5272 - val_loss: 14.5114 - val_mse: 14.5114 - val_mae: 1.5373 - lr: 1.0264e-04 - 22s/epoch - 22ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 22s - loss: 14.3690 - mse: 14.3690 - mae: 1.5277 - val_loss: 14.4989 - val_mse: 14.4989 - val_mae: 1.5784 - lr: 1.0264e-04 - 22s/epoch - 22ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 24s - loss: 14.3241 - mse: 14.3241 - mae: 1.5263 - val_loss: 14.6067 - val_mse: 14.6067 - val_mae: 1.5476 - lr: 1.0264e-04 - 24s/epoch - 24ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 21s - loss: 14.3243 - mse: 14.3243 - mae: 1.5209 - val_loss: 14.2431 - val_mse: 14.2431 - val_mae: 1.6486 - lr: 1.0264e-04 - 21s/epoch - 21ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 21s - loss: 14.3174 - mse: 14.3174 - mae: 1.5209 - val_loss: 14.1741 - val_mse: 14.1741 - val_mae: 1.6324 - lr: 1.0264e-04 - 21s/epoch - 21ms/step\n",
            "Score for fold 3: loss of 14.174087524414062\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 21s - loss: 13.4516 - mse: 13.4516 - mae: 1.5355 - val_loss: 17.6944 - val_mse: 17.6944 - val_mae: 1.5197 - lr: 1.0264e-04 - 21s/epoch - 21ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 21s - loss: 13.3781 - mse: 13.3781 - mae: 1.5322 - val_loss: 17.7698 - val_mse: 17.7698 - val_mae: 1.5315 - lr: 1.0264e-04 - 21s/epoch - 21ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 21s - loss: 13.3761 - mse: 13.3761 - mae: 1.5309 - val_loss: 18.0384 - val_mse: 18.0384 - val_mae: 1.5390 - lr: 1.0264e-04 - 21s/epoch - 21ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 21s - loss: 13.4190 - mse: 13.4190 - mae: 1.5280 - val_loss: 17.9023 - val_mse: 17.9023 - val_mae: 1.5160 - lr: 1.0264e-04 - 21s/epoch - 21ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 21s - loss: 13.3075 - mse: 13.3075 - mae: 1.5253 - val_loss: 17.9359 - val_mse: 17.9359 - val_mae: 1.5361 - lr: 1.0264e-04 - 21s/epoch - 21ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 23s - loss: 13.3120 - mse: 13.3120 - mae: 1.5257 - val_loss: 17.9042 - val_mse: 17.9042 - val_mae: 1.5302 - lr: 1.0264e-04 - 23s/epoch - 23ms/step\n",
            "Score for fold 4: loss of 17.904172897338867\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 22s - loss: 14.9731 - mse: 14.9731 - mae: 1.5383 - val_loss: 11.2823 - val_mse: 11.2823 - val_mae: 1.4487 - lr: 1.0264e-04 - 22s/epoch - 22ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 22s - loss: 14.9735 - mse: 14.9735 - mae: 1.5346 - val_loss: 11.1713 - val_mse: 11.1713 - val_mae: 1.5212 - lr: 1.0264e-04 - 22s/epoch - 22ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 22s - loss: 14.9223 - mse: 14.9223 - mae: 1.5340 - val_loss: 11.2078 - val_mse: 11.2078 - val_mae: 1.5454 - lr: 1.0264e-04 - 22s/epoch - 22ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 22s - loss: 14.9881 - mse: 14.9881 - mae: 1.5341 - val_loss: 11.2723 - val_mse: 11.2723 - val_mae: 1.5262 - lr: 1.0264e-04 - 22s/epoch - 22ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 22s - loss: 14.9304 - mse: 14.9304 - mae: 1.5293 - val_loss: 11.5462 - val_mse: 11.5462 - val_mae: 1.4820 - lr: 1.0264e-04 - 22s/epoch - 22ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 23s - loss: 14.8163 - mse: 14.8163 - mae: 1.5327 - val_loss: 11.4101 - val_mse: 11.4101 - val_mae: 1.5024 - lr: 1.0264e-04 - 23s/epoch - 23ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 24s - loss: 14.8268 - mse: 14.8268 - mae: 1.5264 - val_loss: 11.4925 - val_mse: 11.4925 - val_mae: 1.5351 - lr: 1.0264e-04 - 24s/epoch - 24ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-10-15 17:33:55,096]\u001b[0m Finished trial#43 resulted in value: 14.538. Current best value is 14.538 with parameters: {'activation': 'relu', 'num_hidden_layer': 3, 'i': 10, 'learning_rate': 0.00010263970460536851}.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Score for fold 5: loss of 11.492536544799805\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 22s - loss: 14.3027 - mse: 14.3027 - mae: 1.6092 - val_loss: 20.1249 - val_mse: 20.1249 - val_mae: 1.6391 - lr: 1.0882e-04 - 22s/epoch - 22ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 21s - loss: 13.8744 - mse: 13.8744 - mae: 1.5873 - val_loss: 20.0968 - val_mse: 20.0968 - val_mae: 1.5803 - lr: 1.0882e-04 - 21s/epoch - 21ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 21s - loss: 13.8065 - mse: 13.8065 - mae: 1.5799 - val_loss: 20.1254 - val_mse: 20.1254 - val_mae: 1.5770 - lr: 1.0882e-04 - 21s/epoch - 21ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 21s - loss: 13.7699 - mse: 13.7699 - mae: 1.5805 - val_loss: 19.9871 - val_mse: 19.9871 - val_mae: 1.6359 - lr: 1.0882e-04 - 21s/epoch - 21ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 21s - loss: 13.7227 - mse: 13.7227 - mae: 1.5765 - val_loss: 20.0853 - val_mse: 20.0853 - val_mae: 1.5969 - lr: 1.0882e-04 - 21s/epoch - 21ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 23s - loss: 13.7285 - mse: 13.7285 - mae: 1.5727 - val_loss: 19.9433 - val_mse: 19.9433 - val_mae: 1.6307 - lr: 1.0882e-04 - 23s/epoch - 23ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 21s - loss: 13.6757 - mse: 13.6757 - mae: 1.5693 - val_loss: 19.9999 - val_mse: 19.9999 - val_mae: 1.5858 - lr: 1.0882e-04 - 21s/epoch - 21ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 21s - loss: 13.6490 - mse: 13.6490 - mae: 1.5676 - val_loss: 20.1276 - val_mse: 20.1276 - val_mae: 1.6113 - lr: 1.0882e-04 - 21s/epoch - 21ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 21s - loss: 13.6087 - mse: 13.6087 - mae: 1.5735 - val_loss: 20.0049 - val_mse: 20.0049 - val_mae: 1.5960 - lr: 1.0882e-04 - 21s/epoch - 21ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 21s - loss: 13.6120 - mse: 13.6120 - mae: 1.5664 - val_loss: 20.0744 - val_mse: 20.0744 - val_mae: 1.5749 - lr: 1.0882e-04 - 21s/epoch - 21ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 21s - loss: 13.5849 - mse: 13.5849 - mae: 1.5630 - val_loss: 19.9018 - val_mse: 19.9018 - val_mae: 1.6207 - lr: 1.0882e-04 - 21s/epoch - 21ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 21s - loss: 13.5607 - mse: 13.5607 - mae: 1.5618 - val_loss: 19.9043 - val_mse: 19.9043 - val_mae: 1.6715 - lr: 1.0882e-04 - 21s/epoch - 21ms/step\n",
            "Epoch 13/100\n",
            "1000/1000 - 22s - loss: 13.5371 - mse: 13.5371 - mae: 1.5642 - val_loss: 20.0625 - val_mse: 20.0625 - val_mae: 1.6162 - lr: 1.0882e-04 - 22s/epoch - 22ms/step\n",
            "Epoch 14/100\n",
            "1000/1000 - 22s - loss: 13.5277 - mse: 13.5277 - mae: 1.5585 - val_loss: 20.1305 - val_mse: 20.1305 - val_mae: 1.5868 - lr: 1.0882e-04 - 22s/epoch - 22ms/step\n",
            "Epoch 15/100\n",
            "1000/1000 - 21s - loss: 13.5038 - mse: 13.5038 - mae: 1.5577 - val_loss: 20.0704 - val_mse: 20.0704 - val_mae: 1.6147 - lr: 1.0882e-04 - 21s/epoch - 21ms/step\n",
            "Epoch 16/100\n",
            "1000/1000 - 21s - loss: 13.5056 - mse: 13.5056 - mae: 1.5555 - val_loss: 20.0355 - val_mse: 20.0355 - val_mae: 1.5964 - lr: 1.0882e-04 - 21s/epoch - 21ms/step\n",
            "Score for fold 1: loss of 20.03545570373535\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 22s - loss: 15.8631 - mse: 15.8631 - mae: 1.5699 - val_loss: 10.4854 - val_mse: 10.4854 - val_mae: 1.5533 - lr: 1.0882e-04 - 22s/epoch - 22ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 21s - loss: 15.7757 - mse: 15.7757 - mae: 1.5693 - val_loss: 10.5377 - val_mse: 10.5377 - val_mae: 1.5407 - lr: 1.0882e-04 - 21s/epoch - 21ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 23s - loss: 15.8032 - mse: 15.8032 - mae: 1.5677 - val_loss: 10.5404 - val_mse: 10.5404 - val_mae: 1.5217 - lr: 1.0882e-04 - 23s/epoch - 23ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 21s - loss: 15.7406 - mse: 15.7406 - mae: 1.5659 - val_loss: 10.5597 - val_mse: 10.5597 - val_mae: 1.6392 - lr: 1.0882e-04 - 21s/epoch - 21ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 21s - loss: 15.7158 - mse: 15.7158 - mae: 1.5647 - val_loss: 10.6839 - val_mse: 10.6839 - val_mae: 1.5213 - lr: 1.0882e-04 - 21s/epoch - 21ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 21s - loss: 15.7145 - mse: 15.7145 - mae: 1.5666 - val_loss: 10.5628 - val_mse: 10.5628 - val_mae: 1.5626 - lr: 1.0882e-04 - 21s/epoch - 21ms/step\n",
            "Score for fold 2: loss of 10.562766075134277\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 22s - loss: 13.7318 - mse: 13.7318 - mae: 1.5581 - val_loss: 18.6320 - val_mse: 18.6320 - val_mae: 1.5404 - lr: 1.0882e-04 - 22s/epoch - 22ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 21s - loss: 13.6587 - mse: 13.6587 - mae: 1.5562 - val_loss: 18.8365 - val_mse: 18.8365 - val_mae: 1.5266 - lr: 1.0882e-04 - 21s/epoch - 21ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 21s - loss: 13.6794 - mse: 13.6794 - mae: 1.5567 - val_loss: 18.7699 - val_mse: 18.7699 - val_mae: 1.6038 - lr: 1.0882e-04 - 21s/epoch - 21ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 22s - loss: 13.6608 - mse: 13.6608 - mae: 1.5534 - val_loss: 18.7822 - val_mse: 18.7822 - val_mae: 1.5770 - lr: 1.0882e-04 - 22s/epoch - 22ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 23s - loss: 13.6421 - mse: 13.6421 - mae: 1.5553 - val_loss: 18.8625 - val_mse: 18.8625 - val_mae: 1.5135 - lr: 1.0882e-04 - 23s/epoch - 23ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 21s - loss: 13.5879 - mse: 13.5879 - mae: 1.5502 - val_loss: 18.9009 - val_mse: 18.9009 - val_mae: 1.5315 - lr: 1.0882e-04 - 21s/epoch - 21ms/step\n",
            "Score for fold 3: loss of 18.900903701782227\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 22s - loss: 15.5612 - mse: 15.5612 - mae: 1.5617 - val_loss: 10.9918 - val_mse: 10.9918 - val_mae: 1.5265 - lr: 1.0882e-04 - 22s/epoch - 22ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 21s - loss: 15.5375 - mse: 15.5375 - mae: 1.5594 - val_loss: 11.0269 - val_mse: 11.0269 - val_mae: 1.5062 - lr: 1.0882e-04 - 21s/epoch - 21ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 22s - loss: 15.5318 - mse: 15.5318 - mae: 1.5573 - val_loss: 11.0591 - val_mse: 11.0591 - val_mae: 1.5128 - lr: 1.0882e-04 - 22s/epoch - 22ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 22s - loss: 15.4325 - mse: 15.4325 - mae: 1.5565 - val_loss: 10.9693 - val_mse: 10.9693 - val_mae: 1.6317 - lr: 1.0882e-04 - 22s/epoch - 22ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 21s - loss: 15.4764 - mse: 15.4764 - mae: 1.5517 - val_loss: 11.0040 - val_mse: 11.0040 - val_mae: 1.5279 - lr: 1.0882e-04 - 21s/epoch - 21ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 23s - loss: 15.4360 - mse: 15.4360 - mae: 1.5531 - val_loss: 10.9745 - val_mse: 10.9745 - val_mae: 1.6085 - lr: 1.0882e-04 - 23s/epoch - 23ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 21s - loss: 15.3916 - mse: 15.3916 - mae: 1.5505 - val_loss: 11.1292 - val_mse: 11.1292 - val_mae: 1.6448 - lr: 1.0882e-04 - 21s/epoch - 21ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 21s - loss: 15.4014 - mse: 15.4014 - mae: 1.5475 - val_loss: 11.1147 - val_mse: 11.1147 - val_mae: 1.5746 - lr: 1.0882e-04 - 21s/epoch - 21ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 22s - loss: 15.2992 - mse: 15.2992 - mae: 1.5533 - val_loss: 11.0597 - val_mse: 11.0597 - val_mae: 1.5851 - lr: 1.0882e-04 - 22s/epoch - 22ms/step\n",
            "Score for fold 4: loss of 11.059733390808105\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 21s - loss: 14.8700 - mse: 14.8700 - mae: 1.5491 - val_loss: 13.0817 - val_mse: 13.0817 - val_mae: 1.5534 - lr: 1.0882e-04 - 21s/epoch - 21ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 21s - loss: 14.8095 - mse: 14.8095 - mae: 1.5463 - val_loss: 13.1321 - val_mse: 13.1321 - val_mae: 1.5517 - lr: 1.0882e-04 - 21s/epoch - 21ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 21s - loss: 14.7900 - mse: 14.7900 - mae: 1.5441 - val_loss: 13.2412 - val_mse: 13.2412 - val_mae: 1.5561 - lr: 1.0882e-04 - 21s/epoch - 21ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 23s - loss: 14.7735 - mse: 14.7735 - mae: 1.5445 - val_loss: 13.4404 - val_mse: 13.4404 - val_mae: 1.4862 - lr: 1.0882e-04 - 23s/epoch - 23ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 21s - loss: 14.7511 - mse: 14.7511 - mae: 1.5412 - val_loss: 13.1895 - val_mse: 13.1895 - val_mae: 1.5933 - lr: 1.0882e-04 - 21s/epoch - 21ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 21s - loss: 14.7187 - mse: 14.7187 - mae: 1.5403 - val_loss: 13.2353 - val_mse: 13.2353 - val_mae: 1.5750 - lr: 1.0882e-04 - 21s/epoch - 21ms/step\n",
            "Score for fold 5: loss of 13.235342025756836\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-10-15 17:50:31,184]\u001b[0m Finished trial#44 resulted in value: 14.76. Current best value is 14.538 with parameters: {'activation': 'relu', 'num_hidden_layer': 3, 'i': 10, 'learning_rate': 0.00010263970460536851}.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 11s - loss: 15.3304 - mse: 15.3304 - mae: 1.6755 - val_loss: 19.0222 - val_mse: 19.0222 - val_mae: 1.5950 - lr: 1.9688e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 10s - loss: 15.0071 - mse: 15.0071 - mae: 1.6291 - val_loss: 18.8822 - val_mse: 18.8822 - val_mae: 1.6560 - lr: 1.9688e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 10s - loss: 14.9594 - mse: 14.9594 - mae: 1.6248 - val_loss: 18.8448 - val_mse: 18.8448 - val_mae: 1.6404 - lr: 1.9688e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 11s - loss: 14.9245 - mse: 14.9245 - mae: 1.6242 - val_loss: 18.7346 - val_mse: 18.7346 - val_mae: 1.6454 - lr: 1.9688e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 10s - loss: 14.9118 - mse: 14.9118 - mae: 1.6148 - val_loss: 18.9267 - val_mse: 18.9267 - val_mae: 1.6465 - lr: 1.9688e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 11s - loss: 14.9164 - mse: 14.9164 - mae: 1.6193 - val_loss: 18.8137 - val_mse: 18.8137 - val_mae: 1.6904 - lr: 1.9688e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 11s - loss: 14.9328 - mse: 14.9328 - mae: 1.6202 - val_loss: 18.7809 - val_mse: 18.7809 - val_mae: 1.6692 - lr: 1.9688e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 10s - loss: 14.9144 - mse: 14.9144 - mae: 1.6226 - val_loss: 18.8716 - val_mse: 18.8716 - val_mae: 1.6449 - lr: 1.9688e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 12s - loss: 14.9078 - mse: 14.9078 - mae: 1.6201 - val_loss: 18.9446 - val_mse: 18.9446 - val_mae: 1.6377 - lr: 1.9688e-04 - 12s/epoch - 12ms/step\n",
            "Score for fold 1: loss of 18.94457244873047\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 10s - loss: 15.3296 - mse: 15.3296 - mae: 1.6318 - val_loss: 17.0689 - val_mse: 17.0689 - val_mae: 1.6262 - lr: 1.9688e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 11s - loss: 15.2950 - mse: 15.2950 - mae: 1.6284 - val_loss: 17.0783 - val_mse: 17.0783 - val_mae: 1.7203 - lr: 1.9688e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 10s - loss: 15.2998 - mse: 15.2998 - mae: 1.6283 - val_loss: 17.0359 - val_mse: 17.0359 - val_mae: 1.6542 - lr: 1.9688e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 11s - loss: 15.3357 - mse: 15.3357 - mae: 1.6281 - val_loss: 17.2536 - val_mse: 17.2536 - val_mae: 1.6145 - lr: 1.9688e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 10s - loss: 15.3157 - mse: 15.3157 - mae: 1.6305 - val_loss: 17.0780 - val_mse: 17.0780 - val_mae: 1.7662 - lr: 1.9688e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 10s - loss: 15.3305 - mse: 15.3305 - mae: 1.6339 - val_loss: 17.0775 - val_mse: 17.0775 - val_mae: 1.6377 - lr: 1.9688e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 10s - loss: 15.3373 - mse: 15.3373 - mae: 1.6293 - val_loss: 17.1314 - val_mse: 17.1314 - val_mae: 1.6377 - lr: 1.9688e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 10s - loss: 15.3111 - mse: 15.3111 - mae: 1.6315 - val_loss: 17.1475 - val_mse: 17.1475 - val_mae: 1.6737 - lr: 1.9688e-04 - 10s/epoch - 10ms/step\n",
            "Score for fold 2: loss of 17.14751625061035\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 11s - loss: 14.2093 - mse: 14.2093 - mae: 1.6281 - val_loss: 21.4900 - val_mse: 21.4900 - val_mae: 1.6742 - lr: 1.9688e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 10s - loss: 14.1930 - mse: 14.1930 - mae: 1.6247 - val_loss: 21.4501 - val_mse: 21.4501 - val_mae: 1.6497 - lr: 1.9688e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 10s - loss: 14.2125 - mse: 14.2125 - mae: 1.6268 - val_loss: 21.4493 - val_mse: 21.4493 - val_mae: 1.6742 - lr: 1.9688e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 10s - loss: 14.2107 - mse: 14.2107 - mae: 1.6238 - val_loss: 21.5148 - val_mse: 21.5148 - val_mae: 1.6596 - lr: 1.9688e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 10s - loss: 14.2219 - mse: 14.2219 - mae: 1.6262 - val_loss: 21.4844 - val_mse: 21.4844 - val_mae: 1.6539 - lr: 1.9688e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 10s - loss: 14.1904 - mse: 14.1904 - mae: 1.6275 - val_loss: 21.4342 - val_mse: 21.4342 - val_mae: 1.6706 - lr: 1.9688e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 11s - loss: 14.1694 - mse: 14.1694 - mae: 1.6215 - val_loss: 21.4364 - val_mse: 21.4364 - val_mae: 1.7169 - lr: 1.9688e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 10s - loss: 14.1918 - mse: 14.1918 - mae: 1.6253 - val_loss: 21.4466 - val_mse: 21.4466 - val_mae: 1.6983 - lr: 1.9688e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 11s - loss: 14.2009 - mse: 14.2009 - mae: 1.6238 - val_loss: 21.4455 - val_mse: 21.4455 - val_mae: 1.6649 - lr: 1.9688e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 10s - loss: 14.1937 - mse: 14.1937 - mae: 1.6216 - val_loss: 21.6290 - val_mse: 21.6290 - val_mae: 1.6187 - lr: 1.9688e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 11s - loss: 14.1850 - mse: 14.1850 - mae: 1.6274 - val_loss: 21.4171 - val_mse: 21.4171 - val_mae: 1.6865 - lr: 1.9688e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 13s - loss: 14.2088 - mse: 14.2088 - mae: 1.6249 - val_loss: 21.4810 - val_mse: 21.4810 - val_mae: 1.6591 - lr: 1.9688e-04 - 13s/epoch - 13ms/step\n",
            "Epoch 13/100\n",
            "1000/1000 - 10s - loss: 14.2044 - mse: 14.2044 - mae: 1.6321 - val_loss: 21.4248 - val_mse: 21.4248 - val_mae: 1.6939 - lr: 1.9688e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 14/100\n",
            "1000/1000 - 11s - loss: 14.1885 - mse: 14.1885 - mae: 1.6247 - val_loss: 21.5244 - val_mse: 21.5244 - val_mae: 1.6518 - lr: 1.9688e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 15/100\n",
            "1000/1000 - 11s - loss: 14.1971 - mse: 14.1971 - mae: 1.6230 - val_loss: 21.4057 - val_mse: 21.4057 - val_mae: 1.7350 - lr: 1.9688e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 16/100\n",
            "1000/1000 - 10s - loss: 14.1779 - mse: 14.1779 - mae: 1.6239 - val_loss: 21.5043 - val_mse: 21.5043 - val_mae: 1.6452 - lr: 1.9688e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 17/100\n",
            "1000/1000 - 10s - loss: 14.1889 - mse: 14.1889 - mae: 1.6233 - val_loss: 21.9368 - val_mse: 21.9368 - val_mae: 1.5662 - lr: 1.9688e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 18/100\n",
            "1000/1000 - 11s - loss: 14.1834 - mse: 14.1834 - mae: 1.6233 - val_loss: 21.4079 - val_mse: 21.4079 - val_mae: 1.7545 - lr: 1.9688e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 19/100\n",
            "1000/1000 - 10s - loss: 14.1834 - mse: 14.1834 - mae: 1.6300 - val_loss: 21.4308 - val_mse: 21.4308 - val_mae: 1.6864 - lr: 1.9688e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 20/100\n",
            "1000/1000 - 11s - loss: 14.1845 - mse: 14.1845 - mae: 1.6187 - val_loss: 21.4113 - val_mse: 21.4113 - val_mae: 1.7045 - lr: 1.9688e-04 - 11s/epoch - 11ms/step\n",
            "Score for fold 3: loss of 21.411306381225586\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 11s - loss: 17.2888 - mse: 17.2888 - mae: 1.6692 - val_loss: 8.9452 - val_mse: 8.9452 - val_mae: 1.5280 - lr: 1.9688e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 12s - loss: 17.3165 - mse: 17.3165 - mae: 1.6645 - val_loss: 8.9605 - val_mse: 8.9605 - val_mae: 1.5695 - lr: 1.9688e-04 - 12s/epoch - 12ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 11s - loss: 17.2790 - mse: 17.2790 - mae: 1.6682 - val_loss: 8.9471 - val_mse: 8.9471 - val_mae: 1.4964 - lr: 1.9688e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 10s - loss: 17.3271 - mse: 17.3271 - mae: 1.6697 - val_loss: 9.0729 - val_mse: 9.0729 - val_mae: 1.6479 - lr: 1.9688e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 11s - loss: 17.3182 - mse: 17.3182 - mae: 1.6717 - val_loss: 8.9388 - val_mse: 8.9388 - val_mae: 1.5356 - lr: 1.9688e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 10s - loss: 17.2809 - mse: 17.2809 - mae: 1.6660 - val_loss: 9.0390 - val_mse: 9.0390 - val_mae: 1.6268 - lr: 1.9688e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 10s - loss: 17.2984 - mse: 17.2984 - mae: 1.6689 - val_loss: 8.9416 - val_mse: 8.9416 - val_mae: 1.5025 - lr: 1.9688e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 10s - loss: 17.3046 - mse: 17.3046 - mae: 1.6685 - val_loss: 8.9407 - val_mse: 8.9407 - val_mae: 1.5307 - lr: 1.9688e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 11s - loss: 17.2991 - mse: 17.2991 - mae: 1.6710 - val_loss: 8.9443 - val_mse: 8.9443 - val_mae: 1.4985 - lr: 1.9688e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 10s - loss: 17.3143 - mse: 17.3143 - mae: 1.6677 - val_loss: 8.9690 - val_mse: 8.9690 - val_mae: 1.5703 - lr: 1.9688e-04 - 10s/epoch - 10ms/step\n",
            "Score for fold 4: loss of 8.968962669372559\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 10s - loss: 16.5737 - mse: 16.5737 - mae: 1.6423 - val_loss: 11.9199 - val_mse: 11.9199 - val_mae: 1.6274 - lr: 1.9688e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 11s - loss: 16.5652 - mse: 16.5652 - mae: 1.6365 - val_loss: 11.9104 - val_mse: 11.9104 - val_mae: 1.6494 - lr: 1.9688e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 10s - loss: 16.6140 - mse: 16.6140 - mae: 1.6277 - val_loss: 11.9743 - val_mse: 11.9743 - val_mae: 1.6021 - lr: 1.9688e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 10s - loss: 16.5670 - mse: 16.5670 - mae: 1.6342 - val_loss: 11.9171 - val_mse: 11.9171 - val_mae: 1.7038 - lr: 1.9688e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 10s - loss: 16.5649 - mse: 16.5649 - mae: 1.6340 - val_loss: 11.9359 - val_mse: 11.9359 - val_mae: 1.7274 - lr: 1.9688e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 10s - loss: 16.5699 - mse: 16.5699 - mae: 1.6371 - val_loss: 12.0873 - val_mse: 12.0873 - val_mae: 1.5609 - lr: 1.9688e-04 - 10s/epoch - 10ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 13s - loss: 16.5883 - mse: 16.5883 - mae: 1.6321 - val_loss: 11.9572 - val_mse: 11.9572 - val_mae: 1.7473 - lr: 1.9688e-04 - 13s/epoch - 13ms/step\n",
            "Score for fold 5: loss of 11.957221031188965\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-10-15 18:02:52,097]\u001b[0m Finished trial#45 resulted in value: 15.686000000000002. Current best value is 14.538 with parameters: {'activation': 'relu', 'num_hidden_layer': 3, 'i': 10, 'learning_rate': 0.00010263970460536851}.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 22s - loss: 14.3986 - mse: 14.3986 - mae: 1.6165 - val_loss: 19.3123 - val_mse: 19.3123 - val_mae: 1.6249 - lr: 2.7495e-04 - 22s/epoch - 22ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 21s - loss: 14.0206 - mse: 14.0206 - mae: 1.6011 - val_loss: 19.2786 - val_mse: 19.2786 - val_mae: 1.6351 - lr: 2.7495e-04 - 21s/epoch - 21ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 22s - loss: 13.9984 - mse: 13.9984 - mae: 1.5896 - val_loss: 19.4238 - val_mse: 19.4238 - val_mae: 1.4994 - lr: 2.7495e-04 - 22s/epoch - 22ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 21s - loss: 14.0146 - mse: 14.0146 - mae: 1.5915 - val_loss: 19.5928 - val_mse: 19.5928 - val_mae: 1.4947 - lr: 2.7495e-04 - 21s/epoch - 21ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 21s - loss: 13.9718 - mse: 13.9718 - mae: 1.5908 - val_loss: 19.1886 - val_mse: 19.1886 - val_mae: 1.5438 - lr: 2.7495e-04 - 21s/epoch - 21ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 21s - loss: 13.9422 - mse: 13.9422 - mae: 1.5856 - val_loss: 19.1800 - val_mse: 19.1800 - val_mae: 1.5026 - lr: 2.7495e-04 - 21s/epoch - 21ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 23s - loss: 13.9129 - mse: 13.9129 - mae: 1.5835 - val_loss: 19.3357 - val_mse: 19.3357 - val_mae: 1.5697 - lr: 2.7495e-04 - 23s/epoch - 23ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 22s - loss: 13.8884 - mse: 13.8884 - mae: 1.5781 - val_loss: 19.1912 - val_mse: 19.1912 - val_mae: 1.5679 - lr: 2.7495e-04 - 22s/epoch - 22ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 22s - loss: 13.8342 - mse: 13.8342 - mae: 1.5804 - val_loss: 19.2142 - val_mse: 19.2142 - val_mae: 1.5442 - lr: 2.7495e-04 - 22s/epoch - 22ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 21s - loss: 13.8077 - mse: 13.8077 - mae: 1.5767 - val_loss: 19.0720 - val_mse: 19.0720 - val_mae: 1.6111 - lr: 2.7495e-04 - 21s/epoch - 21ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 22s - loss: 13.8202 - mse: 13.8202 - mae: 1.5781 - val_loss: 19.1960 - val_mse: 19.1960 - val_mae: 1.5157 - lr: 2.7495e-04 - 22s/epoch - 22ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 21s - loss: 13.7379 - mse: 13.7379 - mae: 1.5750 - val_loss: 19.0622 - val_mse: 19.0622 - val_mae: 1.5714 - lr: 2.7495e-04 - 21s/epoch - 21ms/step\n",
            "Epoch 13/100\n",
            "1000/1000 - 21s - loss: 13.7962 - mse: 13.7962 - mae: 1.5717 - val_loss: 19.1597 - val_mse: 19.1597 - val_mae: 1.5740 - lr: 2.7495e-04 - 21s/epoch - 21ms/step\n",
            "Epoch 14/100\n",
            "1000/1000 - 22s - loss: 13.6973 - mse: 13.6973 - mae: 1.5743 - val_loss: 19.2620 - val_mse: 19.2620 - val_mae: 1.6184 - lr: 2.7495e-04 - 22s/epoch - 22ms/step\n",
            "Epoch 15/100\n",
            "1000/1000 - 24s - loss: 13.7083 - mse: 13.7083 - mae: 1.5687 - val_loss: 19.1572 - val_mse: 19.1572 - val_mae: 1.6314 - lr: 2.7495e-04 - 24s/epoch - 24ms/step\n",
            "Epoch 16/100\n",
            "1000/1000 - 21s - loss: 13.6262 - mse: 13.6262 - mae: 1.5685 - val_loss: 19.3041 - val_mse: 19.3041 - val_mae: 1.5865 - lr: 2.7495e-04 - 21s/epoch - 21ms/step\n",
            "Epoch 17/100\n",
            "1000/1000 - 22s - loss: 13.7195 - mse: 13.7195 - mae: 1.5708 - val_loss: 19.3322 - val_mse: 19.3322 - val_mae: 1.5376 - lr: 2.7495e-04 - 22s/epoch - 22ms/step\n",
            "Score for fold 1: loss of 19.332231521606445\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 22s - loss: 13.5291 - mse: 13.5291 - mae: 1.5417 - val_loss: 19.8262 - val_mse: 19.8262 - val_mae: 1.6299 - lr: 2.7495e-04 - 22s/epoch - 22ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 22s - loss: 13.4846 - mse: 13.4846 - mae: 1.5428 - val_loss: 19.7380 - val_mse: 19.7380 - val_mae: 1.6236 - lr: 2.7495e-04 - 22s/epoch - 22ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 22s - loss: 13.4515 - mse: 13.4515 - mae: 1.5397 - val_loss: 19.6092 - val_mse: 19.6092 - val_mae: 1.6271 - lr: 2.7495e-04 - 22s/epoch - 22ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 24s - loss: 13.4387 - mse: 13.4387 - mae: 1.5394 - val_loss: 19.6601 - val_mse: 19.6601 - val_mae: 1.6099 - lr: 2.7495e-04 - 24s/epoch - 24ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 22s - loss: 13.4644 - mse: 13.4644 - mae: 1.5384 - val_loss: 19.6404 - val_mse: 19.6404 - val_mae: 1.7096 - lr: 2.7495e-04 - 22s/epoch - 22ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 22s - loss: 13.3766 - mse: 13.3766 - mae: 1.5355 - val_loss: 20.1535 - val_mse: 20.1535 - val_mae: 1.6260 - lr: 2.7495e-04 - 22s/epoch - 22ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 22s - loss: 13.3793 - mse: 13.3793 - mae: 1.5346 - val_loss: 19.9493 - val_mse: 19.9493 - val_mae: 1.7012 - lr: 2.7495e-04 - 22s/epoch - 22ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 22s - loss: 13.3146 - mse: 13.3146 - mae: 1.5313 - val_loss: 19.6845 - val_mse: 19.6845 - val_mae: 1.6680 - lr: 2.7495e-04 - 22s/epoch - 22ms/step\n",
            "Score for fold 2: loss of 19.684446334838867\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 23s - loss: 15.3852 - mse: 15.3852 - mae: 1.5634 - val_loss: 12.1582 - val_mse: 12.1582 - val_mae: 1.5231 - lr: 2.7495e-04 - 23s/epoch - 23ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 23s - loss: 15.1630 - mse: 15.1630 - mae: 1.5569 - val_loss: 12.2271 - val_mse: 12.2271 - val_mae: 1.5435 - lr: 2.7495e-04 - 23s/epoch - 23ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 21s - loss: 15.1532 - mse: 15.1532 - mae: 1.5541 - val_loss: 12.4231 - val_mse: 12.4231 - val_mae: 1.5317 - lr: 2.7495e-04 - 21s/epoch - 21ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 21s - loss: 15.0692 - mse: 15.0692 - mae: 1.5529 - val_loss: 12.2492 - val_mse: 12.2492 - val_mae: 1.5425 - lr: 2.7495e-04 - 21s/epoch - 21ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 21s - loss: 15.1320 - mse: 15.1320 - mae: 1.5503 - val_loss: 12.4263 - val_mse: 12.4263 - val_mae: 1.5351 - lr: 2.7495e-04 - 21s/epoch - 21ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 21s - loss: 15.0969 - mse: 15.0969 - mae: 1.5476 - val_loss: 12.2911 - val_mse: 12.2911 - val_mae: 1.5231 - lr: 2.7495e-04 - 21s/epoch - 21ms/step\n",
            "Score for fold 3: loss of 12.291155815124512\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 22s - loss: 15.0315 - mse: 15.0315 - mae: 1.5543 - val_loss: 12.3824 - val_mse: 12.3824 - val_mae: 1.4995 - lr: 2.7495e-04 - 22s/epoch - 22ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 23s - loss: 15.0994 - mse: 15.0994 - mae: 1.5485 - val_loss: 12.2641 - val_mse: 12.2641 - val_mae: 1.6267 - lr: 2.7495e-04 - 23s/epoch - 23ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 22s - loss: 15.0258 - mse: 15.0258 - mae: 1.5498 - val_loss: 12.6179 - val_mse: 12.6179 - val_mae: 1.6571 - lr: 2.7495e-04 - 22s/epoch - 22ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 22s - loss: 14.8709 - mse: 14.8709 - mae: 1.5399 - val_loss: 12.6063 - val_mse: 12.6063 - val_mae: 1.6116 - lr: 2.7495e-04 - 22s/epoch - 22ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 22s - loss: 14.8593 - mse: 14.8593 - mae: 1.5397 - val_loss: 12.3519 - val_mse: 12.3519 - val_mae: 1.5694 - lr: 2.7495e-04 - 22s/epoch - 22ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 22s - loss: 14.9176 - mse: 14.9176 - mae: 1.5367 - val_loss: 12.4861 - val_mse: 12.4861 - val_mae: 1.5458 - lr: 2.7495e-04 - 22s/epoch - 22ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 22s - loss: 14.8569 - mse: 14.8569 - mae: 1.5356 - val_loss: 12.5220 - val_mse: 12.5220 - val_mae: 1.5635 - lr: 2.7495e-04 - 22s/epoch - 22ms/step\n",
            "Score for fold 4: loss of 12.522002220153809\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 24s - loss: 15.4661 - mse: 15.4661 - mae: 1.5545 - val_loss: 9.8444 - val_mse: 9.8444 - val_mae: 1.5105 - lr: 2.7495e-04 - 24s/epoch - 24ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 22s - loss: 15.4783 - mse: 15.4783 - mae: 1.5503 - val_loss: 9.8111 - val_mse: 9.8111 - val_mae: 1.5165 - lr: 2.7495e-04 - 22s/epoch - 22ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 22s - loss: 15.3648 - mse: 15.3648 - mae: 1.5460 - val_loss: 10.0094 - val_mse: 10.0094 - val_mae: 1.4866 - lr: 2.7495e-04 - 22s/epoch - 22ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 22s - loss: 15.3338 - mse: 15.3338 - mae: 1.5445 - val_loss: 9.9988 - val_mse: 9.9988 - val_mae: 1.5187 - lr: 2.7495e-04 - 22s/epoch - 22ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 22s - loss: 15.3130 - mse: 15.3130 - mae: 1.5400 - val_loss: 10.2378 - val_mse: 10.2378 - val_mae: 1.4831 - lr: 2.7495e-04 - 22s/epoch - 22ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 22s - loss: 15.2657 - mse: 15.2657 - mae: 1.5391 - val_loss: 10.1222 - val_mse: 10.1222 - val_mae: 1.5148 - lr: 2.7495e-04 - 22s/epoch - 22ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 23s - loss: 15.1970 - mse: 15.1970 - mae: 1.5329 - val_loss: 10.3875 - val_mse: 10.3875 - val_mae: 1.5029 - lr: 2.7495e-04 - 23s/epoch - 23ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-10-15 18:21:52,655]\u001b[0m Finished trial#46 resulted in value: 14.841999999999999. Current best value is 14.538 with parameters: {'activation': 'relu', 'num_hidden_layer': 3, 'i': 10, 'learning_rate': 0.00010263970460536851}.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Score for fold 5: loss of 10.38752269744873\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 6s - loss: 14.8205 - mse: 14.8205 - mae: 1.6252 - val_loss: 18.1018 - val_mse: 18.1018 - val_mae: 1.4670 - lr: 3.3554e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 5s - loss: 14.4523 - mse: 14.4523 - mae: 1.5996 - val_loss: 17.8666 - val_mse: 17.8666 - val_mae: 1.6147 - lr: 3.3554e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 5s - loss: 14.3180 - mse: 14.3180 - mae: 1.6007 - val_loss: 17.8600 - val_mse: 17.8600 - val_mae: 1.5360 - lr: 3.3554e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 5s - loss: 14.3201 - mse: 14.3201 - mae: 1.5997 - val_loss: 17.6490 - val_mse: 17.6490 - val_mae: 1.6226 - lr: 3.3554e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 5s - loss: 14.3424 - mse: 14.3424 - mae: 1.5958 - val_loss: 17.8707 - val_mse: 17.8707 - val_mae: 1.5432 - lr: 3.3554e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 5s - loss: 14.2666 - mse: 14.2666 - mae: 1.5901 - val_loss: 17.8149 - val_mse: 17.8149 - val_mae: 1.5602 - lr: 3.3554e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 5s - loss: 14.2450 - mse: 14.2450 - mae: 1.5898 - val_loss: 17.7770 - val_mse: 17.7770 - val_mae: 1.5658 - lr: 3.3554e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 5s - loss: 14.1633 - mse: 14.1633 - mae: 1.5871 - val_loss: 17.6340 - val_mse: 17.6340 - val_mae: 1.5590 - lr: 3.3554e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 5s - loss: 14.1436 - mse: 14.1436 - mae: 1.5889 - val_loss: 17.7913 - val_mse: 17.7913 - val_mae: 1.5150 - lr: 3.3554e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 5s - loss: 14.1771 - mse: 14.1771 - mae: 1.5863 - val_loss: 17.5972 - val_mse: 17.5972 - val_mae: 1.5592 - lr: 3.3554e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 5s - loss: 14.0863 - mse: 14.0863 - mae: 1.5852 - val_loss: 17.6291 - val_mse: 17.6291 - val_mae: 1.6024 - lr: 3.3554e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 5s - loss: 14.0491 - mse: 14.0491 - mae: 1.5819 - val_loss: 17.7012 - val_mse: 17.7012 - val_mae: 1.5197 - lr: 3.3554e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 13/100\n",
            "1000/1000 - 5s - loss: 14.0376 - mse: 14.0376 - mae: 1.5796 - val_loss: 17.8772 - val_mse: 17.8772 - val_mae: 1.5313 - lr: 3.3554e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 14/100\n",
            "1000/1000 - 6s - loss: 13.8622 - mse: 13.8622 - mae: 1.5770 - val_loss: 17.6512 - val_mse: 17.6512 - val_mae: 1.6230 - lr: 3.3554e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 15/100\n",
            "1000/1000 - 5s - loss: 13.9160 - mse: 13.9160 - mae: 1.5798 - val_loss: 17.7230 - val_mse: 17.7230 - val_mae: 1.6348 - lr: 3.3554e-04 - 5s/epoch - 5ms/step\n",
            "Score for fold 1: loss of 17.723033905029297\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 5s - loss: 15.9867 - mse: 15.9867 - mae: 1.5914 - val_loss: 9.3929 - val_mse: 9.3929 - val_mae: 1.4309 - lr: 3.3554e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 5s - loss: 15.7586 - mse: 15.7586 - mae: 1.5808 - val_loss: 9.3454 - val_mse: 9.3454 - val_mae: 1.6011 - lr: 3.3554e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 5s - loss: 15.7680 - mse: 15.7680 - mae: 1.5862 - val_loss: 9.3631 - val_mse: 9.3631 - val_mae: 1.5330 - lr: 3.3554e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 5s - loss: 15.7057 - mse: 15.7057 - mae: 1.5804 - val_loss: 9.6379 - val_mse: 9.6379 - val_mae: 1.6336 - lr: 3.3554e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 5s - loss: 15.4867 - mse: 15.4867 - mae: 1.5801 - val_loss: 9.6287 - val_mse: 9.6287 - val_mae: 1.6497 - lr: 3.3554e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 7s - loss: 15.5523 - mse: 15.5523 - mae: 1.5773 - val_loss: 9.2313 - val_mse: 9.2313 - val_mae: 1.5537 - lr: 3.3554e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 5s - loss: 15.3453 - mse: 15.3453 - mae: 1.5748 - val_loss: 9.4887 - val_mse: 9.4887 - val_mae: 1.5637 - lr: 3.3554e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 5s - loss: 15.3977 - mse: 15.3977 - mae: 1.5849 - val_loss: 9.5630 - val_mse: 9.5630 - val_mae: 1.4331 - lr: 3.3554e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 5s - loss: 15.3602 - mse: 15.3602 - mae: 1.5826 - val_loss: 9.5504 - val_mse: 9.5504 - val_mae: 1.7189 - lr: 3.3554e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 5s - loss: 15.1828 - mse: 15.1828 - mae: 1.5687 - val_loss: 9.6214 - val_mse: 9.6214 - val_mae: 1.5822 - lr: 3.3554e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 5s - loss: 15.1573 - mse: 15.1573 - mae: 1.5815 - val_loss: 9.6861 - val_mse: 9.6861 - val_mae: 1.5915 - lr: 3.3554e-04 - 5s/epoch - 5ms/step\n",
            "Score for fold 2: loss of 9.68610954284668\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 5s - loss: 13.3743 - mse: 13.3743 - mae: 1.5654 - val_loss: 17.0307 - val_mse: 17.0307 - val_mae: 1.6003 - lr: 3.3554e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 5s - loss: 13.1911 - mse: 13.1911 - mae: 1.5634 - val_loss: 18.1405 - val_mse: 18.1405 - val_mae: 1.5632 - lr: 3.3554e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 5s - loss: 13.2479 - mse: 13.2479 - mae: 1.5692 - val_loss: 17.9815 - val_mse: 17.9815 - val_mae: 1.5032 - lr: 3.3554e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 5s - loss: 13.1311 - mse: 13.1311 - mae: 1.5736 - val_loss: 17.8636 - val_mse: 17.8636 - val_mae: 1.5808 - lr: 3.3554e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 5s - loss: 13.0221 - mse: 13.0221 - mae: 1.5751 - val_loss: 18.3529 - val_mse: 18.3529 - val_mae: 1.9112 - lr: 3.3554e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 5s - loss: 13.1042 - mse: 13.1042 - mae: 1.5855 - val_loss: 17.9794 - val_mse: 17.9794 - val_mae: 1.8734 - lr: 3.3554e-04 - 5s/epoch - 5ms/step\n",
            "Score for fold 3: loss of 17.979408264160156\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 5s - loss: 15.3799 - mse: 15.3799 - mae: 1.6185 - val_loss: 9.4598 - val_mse: 9.4598 - val_mae: 1.5479 - lr: 3.3554e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 5s - loss: 15.2491 - mse: 15.2491 - mae: 1.6143 - val_loss: 9.4728 - val_mse: 9.4728 - val_mae: 1.4609 - lr: 3.3554e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 5s - loss: 15.0088 - mse: 15.0088 - mae: 1.6273 - val_loss: 9.7347 - val_mse: 9.7347 - val_mae: 1.4754 - lr: 3.3554e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 5s - loss: 15.0902 - mse: 15.0902 - mae: 1.6229 - val_loss: 9.6188 - val_mse: 9.6188 - val_mae: 1.6187 - lr: 3.3554e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 5s - loss: 15.0924 - mse: 15.0924 - mae: 1.6285 - val_loss: 10.4919 - val_mse: 10.4919 - val_mae: 1.5000 - lr: 3.3554e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 5s - loss: 14.9364 - mse: 14.9364 - mae: 1.6251 - val_loss: 9.5428 - val_mse: 9.5428 - val_mae: 1.5587 - lr: 3.3554e-04 - 5s/epoch - 5ms/step\n",
            "Score for fold 4: loss of 9.54283332824707\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 5s - loss: 13.0903 - mse: 13.0903 - mae: 1.6101 - val_loss: 18.9900 - val_mse: 18.9900 - val_mae: 2.4426 - lr: 3.3554e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 5s - loss: 13.1024 - mse: 13.1024 - mae: 1.6343 - val_loss: 17.6571 - val_mse: 17.6571 - val_mae: 1.4706 - lr: 3.3554e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 5s - loss: 13.0048 - mse: 13.0048 - mae: 1.6019 - val_loss: 17.1858 - val_mse: 17.1858 - val_mae: 1.5456 - lr: 3.3554e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 5s - loss: 12.8742 - mse: 12.8742 - mae: 1.5988 - val_loss: 17.2494 - val_mse: 17.2494 - val_mae: 1.5958 - lr: 3.3554e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 5s - loss: 12.8862 - mse: 12.8862 - mae: 1.6007 - val_loss: 18.0407 - val_mse: 18.0407 - val_mae: 1.5288 - lr: 3.3554e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 5s - loss: 12.9116 - mse: 12.9116 - mae: 1.6108 - val_loss: 18.3280 - val_mse: 18.3280 - val_mae: 1.6198 - lr: 3.3554e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 5s - loss: 12.7835 - mse: 12.7835 - mae: 1.6011 - val_loss: 17.6888 - val_mse: 17.6888 - val_mae: 1.5980 - lr: 3.3554e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 5s - loss: 12.6955 - mse: 12.6955 - mae: 1.5864 - val_loss: 17.8497 - val_mse: 17.8497 - val_mae: 1.5792 - lr: 3.3554e-04 - 5s/epoch - 5ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-10-15 18:26:05,710]\u001b[0m Finished trial#47 resulted in value: 14.556000000000001. Current best value is 14.538 with parameters: {'activation': 'relu', 'num_hidden_layer': 3, 'i': 10, 'learning_rate': 0.00010263970460536851}.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Score for fold 5: loss of 17.849748611450195\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 6s - loss: 16.4922 - mse: 16.4922 - mae: 1.6235 - val_loss: 11.0034 - val_mse: 11.0034 - val_mae: 1.5637 - lr: 4.7069e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 7s - loss: 16.1381 - mse: 16.1381 - mae: 1.6068 - val_loss: 10.8518 - val_mse: 10.8518 - val_mae: 1.6322 - lr: 4.7069e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 5s - loss: 16.0696 - mse: 16.0696 - mae: 1.6055 - val_loss: 11.0517 - val_mse: 11.0517 - val_mae: 1.5327 - lr: 4.7069e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 5s - loss: 16.0402 - mse: 16.0402 - mae: 1.6025 - val_loss: 10.9720 - val_mse: 10.9720 - val_mae: 1.5187 - lr: 4.7069e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 5s - loss: 15.9781 - mse: 15.9781 - mae: 1.5898 - val_loss: 10.8487 - val_mse: 10.8487 - val_mae: 1.5927 - lr: 4.7069e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 5s - loss: 15.9501 - mse: 15.9501 - mae: 1.5908 - val_loss: 10.7826 - val_mse: 10.7826 - val_mae: 1.5759 - lr: 4.7069e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 5s - loss: 15.9086 - mse: 15.9086 - mae: 1.5849 - val_loss: 10.7914 - val_mse: 10.7914 - val_mae: 1.5240 - lr: 4.7069e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 5s - loss: 15.8229 - mse: 15.8229 - mae: 1.5827 - val_loss: 10.9140 - val_mse: 10.9140 - val_mae: 1.5481 - lr: 4.7069e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 5s - loss: 15.8281 - mse: 15.8281 - mae: 1.5862 - val_loss: 10.9270 - val_mse: 10.9270 - val_mae: 1.5820 - lr: 4.7069e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 5s - loss: 15.7781 - mse: 15.7781 - mae: 1.5872 - val_loss: 10.8345 - val_mse: 10.8345 - val_mae: 1.5253 - lr: 4.7069e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 5s - loss: 15.7562 - mse: 15.7562 - mae: 1.5835 - val_loss: 10.8132 - val_mse: 10.8132 - val_mae: 1.5799 - lr: 4.7069e-04 - 5s/epoch - 5ms/step\n",
            "Score for fold 1: loss of 10.813163757324219\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 5s - loss: 13.1629 - mse: 13.1629 - mae: 1.5693 - val_loss: 20.8427 - val_mse: 20.8427 - val_mae: 1.5513 - lr: 4.7069e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 5s - loss: 13.1778 - mse: 13.1778 - mae: 1.5660 - val_loss: 21.1787 - val_mse: 21.1787 - val_mae: 1.5646 - lr: 4.7069e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 5s - loss: 13.0931 - mse: 13.0931 - mae: 1.5636 - val_loss: 20.9687 - val_mse: 20.9687 - val_mae: 1.6878 - lr: 4.7069e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 5s - loss: 13.0645 - mse: 13.0645 - mae: 1.5672 - val_loss: 20.9944 - val_mse: 20.9944 - val_mae: 1.5883 - lr: 4.7069e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 5s - loss: 12.9447 - mse: 12.9447 - mae: 1.5672 - val_loss: 20.9828 - val_mse: 20.9828 - val_mae: 1.6007 - lr: 4.7069e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 5s - loss: 12.9648 - mse: 12.9648 - mae: 1.5575 - val_loss: 20.8161 - val_mse: 20.8161 - val_mae: 1.6398 - lr: 4.7069e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 5s - loss: 12.8503 - mse: 12.8503 - mae: 1.5648 - val_loss: 21.1594 - val_mse: 21.1594 - val_mae: 1.6216 - lr: 4.7069e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 5s - loss: 12.8518 - mse: 12.8518 - mae: 1.5625 - val_loss: 20.9805 - val_mse: 20.9805 - val_mae: 1.6671 - lr: 4.7069e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 5s - loss: 12.7733 - mse: 12.7733 - mae: 1.5607 - val_loss: 21.2595 - val_mse: 21.2595 - val_mae: 1.6057 - lr: 4.7069e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 5s - loss: 12.7265 - mse: 12.7265 - mae: 1.5592 - val_loss: 21.3457 - val_mse: 21.3457 - val_mae: 1.5361 - lr: 4.7069e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 5s - loss: 12.6857 - mse: 12.6857 - mae: 1.5681 - val_loss: 21.0533 - val_mse: 21.0533 - val_mae: 1.5677 - lr: 4.7069e-04 - 5s/epoch - 5ms/step\n",
            "Score for fold 2: loss of 21.053359985351562\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 5s - loss: 14.4653 - mse: 14.4653 - mae: 1.5778 - val_loss: 13.3776 - val_mse: 13.3776 - val_mae: 1.5546 - lr: 4.7069e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 5s - loss: 14.3177 - mse: 14.3177 - mae: 1.5780 - val_loss: 13.9725 - val_mse: 13.9725 - val_mae: 1.5881 - lr: 4.7069e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 5s - loss: 14.2107 - mse: 14.2107 - mae: 1.5775 - val_loss: 13.6886 - val_mse: 13.6886 - val_mae: 1.6142 - lr: 4.7069e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 5s - loss: 14.3311 - mse: 14.3311 - mae: 1.5692 - val_loss: 14.1752 - val_mse: 14.1752 - val_mae: 1.5931 - lr: 4.7069e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 5s - loss: 14.2276 - mse: 14.2276 - mae: 1.5740 - val_loss: 14.2889 - val_mse: 14.2889 - val_mae: 1.5550 - lr: 4.7069e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 5s - loss: 14.0967 - mse: 14.0967 - mae: 1.5772 - val_loss: 14.2134 - val_mse: 14.2134 - val_mae: 1.4996 - lr: 4.7069e-04 - 5s/epoch - 5ms/step\n",
            "Score for fold 3: loss of 14.213445663452148\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 5s - loss: 14.8895 - mse: 14.8895 - mae: 1.5962 - val_loss: 11.8967 - val_mse: 11.8967 - val_mae: 1.7374 - lr: 4.7069e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 5s - loss: 14.7974 - mse: 14.7974 - mae: 1.6137 - val_loss: 11.5966 - val_mse: 11.5966 - val_mae: 1.6252 - lr: 4.7069e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 5s - loss: 14.6252 - mse: 14.6252 - mae: 1.6042 - val_loss: 12.5032 - val_mse: 12.5032 - val_mae: 1.5491 - lr: 4.7069e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 5s - loss: 14.7208 - mse: 14.7208 - mae: 1.6015 - val_loss: 11.8002 - val_mse: 11.8002 - val_mae: 1.5325 - lr: 4.7069e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 5s - loss: 14.5551 - mse: 14.5551 - mae: 1.5896 - val_loss: 11.8867 - val_mse: 11.8867 - val_mae: 1.5550 - lr: 4.7069e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 5s - loss: 14.3284 - mse: 14.3284 - mae: 1.5960 - val_loss: 11.8996 - val_mse: 11.8996 - val_mae: 1.5892 - lr: 4.7069e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 5s - loss: 14.3899 - mse: 14.3899 - mae: 1.5962 - val_loss: 12.1564 - val_mse: 12.1564 - val_mae: 1.7976 - lr: 4.7069e-04 - 5s/epoch - 5ms/step\n",
            "Score for fold 4: loss of 12.156448364257812\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 5s - loss: 13.8346 - mse: 13.8346 - mae: 1.6327 - val_loss: 14.3142 - val_mse: 14.3142 - val_mae: 1.5520 - lr: 4.7069e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 5s - loss: 13.8421 - mse: 13.8421 - mae: 1.6268 - val_loss: 14.9927 - val_mse: 14.9927 - val_mae: 1.4536 - lr: 4.7069e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 5s - loss: 13.7279 - mse: 13.7279 - mae: 1.6231 - val_loss: 15.4058 - val_mse: 15.4058 - val_mae: 1.8870 - lr: 4.7069e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 5s - loss: 13.7295 - mse: 13.7295 - mae: 1.6294 - val_loss: 14.5909 - val_mse: 14.5909 - val_mae: 1.5096 - lr: 4.7069e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 5s - loss: 13.7418 - mse: 13.7418 - mae: 1.6597 - val_loss: 15.7270 - val_mse: 15.7270 - val_mae: 1.5285 - lr: 4.7069e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 5s - loss: 13.7812 - mse: 13.7812 - mae: 1.6846 - val_loss: 15.6564 - val_mse: 15.6564 - val_mae: 2.1176 - lr: 4.7069e-04 - 5s/epoch - 5ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-10-15 18:30:53,621]\u001b[0m Finished trial#48 resulted in value: 14.778. Current best value is 14.538 with parameters: {'activation': 'relu', 'num_hidden_layer': 3, 'i': 10, 'learning_rate': 0.00010263970460536851}.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Score for fold 5: loss of 15.656357765197754\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 3s - loss: 16.2725 - mse: 16.2725 - mae: 1.6726 - val_loss: 14.6213 - val_mse: 14.6213 - val_mae: 1.5838 - lr: 3.4584e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 3s - loss: 15.6579 - mse: 15.6579 - mae: 1.6170 - val_loss: 14.5461 - val_mse: 14.5461 - val_mae: 1.6006 - lr: 3.4584e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 3s - loss: 15.5254 - mse: 15.5254 - mae: 1.6067 - val_loss: 14.5628 - val_mse: 14.5628 - val_mae: 1.5554 - lr: 3.4584e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 3s - loss: 15.4338 - mse: 15.4338 - mae: 1.6017 - val_loss: 14.4584 - val_mse: 14.4584 - val_mae: 1.5837 - lr: 3.4584e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 3s - loss: 15.3492 - mse: 15.3492 - mae: 1.5937 - val_loss: 14.4173 - val_mse: 14.4173 - val_mae: 1.6170 - lr: 3.4584e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 3s - loss: 15.2951 - mse: 15.2951 - mae: 1.5961 - val_loss: 14.4108 - val_mse: 14.4108 - val_mae: 1.5914 - lr: 3.4584e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 3s - loss: 15.2768 - mse: 15.2768 - mae: 1.5844 - val_loss: 14.4562 - val_mse: 14.4562 - val_mae: 1.5799 - lr: 3.4584e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 3s - loss: 15.2423 - mse: 15.2423 - mae: 1.5923 - val_loss: 14.4067 - val_mse: 14.4067 - val_mae: 1.5852 - lr: 3.4584e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 3s - loss: 15.2083 - mse: 15.2083 - mae: 1.5892 - val_loss: 14.5401 - val_mse: 14.5401 - val_mae: 1.5491 - lr: 3.4584e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 3s - loss: 15.1844 - mse: 15.1844 - mae: 1.5880 - val_loss: 14.5752 - val_mse: 14.5752 - val_mae: 1.5403 - lr: 3.4584e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 4s - loss: 15.1701 - mse: 15.1701 - mae: 1.5866 - val_loss: 14.5697 - val_mse: 14.5697 - val_mae: 1.5770 - lr: 3.4584e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 3s - loss: 15.1630 - mse: 15.1630 - mae: 1.5844 - val_loss: 14.4227 - val_mse: 14.4227 - val_mae: 1.5625 - lr: 3.4584e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 13/100\n",
            "1000/1000 - 3s - loss: 15.1528 - mse: 15.1528 - mae: 1.5846 - val_loss: 14.3636 - val_mse: 14.3636 - val_mae: 1.5636 - lr: 3.4584e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 14/100\n",
            "1000/1000 - 3s - loss: 15.1431 - mse: 15.1431 - mae: 1.5867 - val_loss: 14.4499 - val_mse: 14.4499 - val_mae: 1.5658 - lr: 3.4584e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 15/100\n",
            "1000/1000 - 3s - loss: 15.1206 - mse: 15.1206 - mae: 1.5892 - val_loss: 14.4274 - val_mse: 14.4274 - val_mae: 1.6345 - lr: 3.4584e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 16/100\n",
            "1000/1000 - 3s - loss: 15.0957 - mse: 15.0957 - mae: 1.5818 - val_loss: 14.3521 - val_mse: 14.3521 - val_mae: 1.5801 - lr: 3.4584e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 17/100\n",
            "1000/1000 - 3s - loss: 15.0811 - mse: 15.0811 - mae: 1.5825 - val_loss: 14.3573 - val_mse: 14.3573 - val_mae: 1.6206 - lr: 3.4584e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 18/100\n",
            "1000/1000 - 3s - loss: 15.0720 - mse: 15.0720 - mae: 1.5827 - val_loss: 14.5018 - val_mse: 14.5018 - val_mae: 1.5662 - lr: 3.4584e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 19/100\n",
            "1000/1000 - 3s - loss: 15.0528 - mse: 15.0528 - mae: 1.5818 - val_loss: 14.3217 - val_mse: 14.3217 - val_mae: 1.5664 - lr: 3.4584e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 20/100\n",
            "1000/1000 - 3s - loss: 15.0506 - mse: 15.0506 - mae: 1.5825 - val_loss: 14.3900 - val_mse: 14.3900 - val_mae: 1.5835 - lr: 3.4584e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 21/100\n",
            "1000/1000 - 3s - loss: 15.0457 - mse: 15.0457 - mae: 1.5826 - val_loss: 14.2936 - val_mse: 14.2936 - val_mae: 1.5824 - lr: 3.4584e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 22/100\n",
            "1000/1000 - 3s - loss: 15.0339 - mse: 15.0339 - mae: 1.5751 - val_loss: 14.3509 - val_mse: 14.3509 - val_mae: 1.5913 - lr: 3.4584e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 23/100\n",
            "1000/1000 - 3s - loss: 15.0203 - mse: 15.0203 - mae: 1.5804 - val_loss: 14.3329 - val_mse: 14.3329 - val_mae: 1.5781 - lr: 3.4584e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 24/100\n",
            "1000/1000 - 3s - loss: 14.9881 - mse: 14.9881 - mae: 1.5779 - val_loss: 14.3217 - val_mse: 14.3217 - val_mae: 1.5828 - lr: 3.4584e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 25/100\n",
            "1000/1000 - 3s - loss: 14.9890 - mse: 14.9890 - mae: 1.5805 - val_loss: 14.3976 - val_mse: 14.3976 - val_mae: 1.5635 - lr: 3.4584e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 26/100\n",
            "1000/1000 - 3s - loss: 14.9796 - mse: 14.9796 - mae: 1.5731 - val_loss: 14.3047 - val_mse: 14.3047 - val_mae: 1.6117 - lr: 3.4584e-04 - 3s/epoch - 3ms/step\n",
            "Score for fold 1: loss of 14.30472469329834\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 3s - loss: 14.7020 - mse: 14.7020 - mae: 1.5830 - val_loss: 15.2875 - val_mse: 15.2875 - val_mae: 1.6014 - lr: 3.4584e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 3s - loss: 14.7088 - mse: 14.7088 - mae: 1.5812 - val_loss: 15.4063 - val_mse: 15.4063 - val_mae: 1.5550 - lr: 3.4584e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 3s - loss: 14.6445 - mse: 14.6445 - mae: 1.5823 - val_loss: 15.3926 - val_mse: 15.3926 - val_mae: 1.5446 - lr: 3.4584e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 3s - loss: 14.6347 - mse: 14.6347 - mae: 1.5784 - val_loss: 15.2276 - val_mse: 15.2276 - val_mae: 1.5595 - lr: 3.4584e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 3s - loss: 14.6287 - mse: 14.6287 - mae: 1.5769 - val_loss: 15.2837 - val_mse: 15.2837 - val_mae: 1.5491 - lr: 3.4584e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 3s - loss: 14.6071 - mse: 14.6071 - mae: 1.5774 - val_loss: 15.3639 - val_mse: 15.3639 - val_mae: 1.5783 - lr: 3.4584e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 3s - loss: 14.5614 - mse: 14.5614 - mae: 1.5753 - val_loss: 15.6343 - val_mse: 15.6343 - val_mae: 1.5764 - lr: 3.4584e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 3s - loss: 14.6050 - mse: 14.6050 - mae: 1.5754 - val_loss: 15.3969 - val_mse: 15.3969 - val_mae: 1.6298 - lr: 3.4584e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 3s - loss: 14.5823 - mse: 14.5823 - mae: 1.5747 - val_loss: 15.4638 - val_mse: 15.4638 - val_mae: 1.5858 - lr: 3.4584e-04 - 3s/epoch - 3ms/step\n",
            "Score for fold 2: loss of 15.463778495788574\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 3s - loss: 14.5868 - mse: 14.5868 - mae: 1.5551 - val_loss: 15.5071 - val_mse: 15.5071 - val_mae: 1.6175 - lr: 3.4584e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 3s - loss: 14.5423 - mse: 14.5423 - mae: 1.5553 - val_loss: 15.4803 - val_mse: 15.4803 - val_mae: 1.6224 - lr: 3.4584e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 3s - loss: 14.4830 - mse: 14.4830 - mae: 1.5526 - val_loss: 15.5202 - val_mse: 15.5202 - val_mae: 1.6735 - lr: 3.4584e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 3s - loss: 14.5089 - mse: 14.5089 - mae: 1.5472 - val_loss: 15.4980 - val_mse: 15.4980 - val_mae: 1.6331 - lr: 3.4584e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 3s - loss: 14.4960 - mse: 14.4960 - mae: 1.5497 - val_loss: 15.5321 - val_mse: 15.5321 - val_mae: 1.6102 - lr: 3.4584e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 3s - loss: 14.4344 - mse: 14.4344 - mae: 1.5443 - val_loss: 15.5317 - val_mse: 15.5317 - val_mae: 1.6479 - lr: 3.4584e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 3s - loss: 14.4358 - mse: 14.4358 - mae: 1.5471 - val_loss: 15.7490 - val_mse: 15.7490 - val_mae: 1.6421 - lr: 3.4584e-04 - 3s/epoch - 3ms/step\n",
            "Score for fold 3: loss of 15.748956680297852\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 3s - loss: 14.0504 - mse: 14.0504 - mae: 1.5792 - val_loss: 17.0669 - val_mse: 17.0669 - val_mae: 1.5443 - lr: 3.4584e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 3s - loss: 14.0238 - mse: 14.0238 - mae: 1.5762 - val_loss: 16.9826 - val_mse: 16.9826 - val_mae: 1.5314 - lr: 3.4584e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 3s - loss: 13.9885 - mse: 13.9885 - mae: 1.5788 - val_loss: 17.0572 - val_mse: 17.0572 - val_mae: 1.5267 - lr: 3.4584e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 3s - loss: 13.9679 - mse: 13.9679 - mae: 1.5742 - val_loss: 17.1102 - val_mse: 17.1102 - val_mae: 1.5208 - lr: 3.4584e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 3s - loss: 13.8916 - mse: 13.8916 - mae: 1.5761 - val_loss: 17.0176 - val_mse: 17.0176 - val_mae: 1.5366 - lr: 3.4584e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 3s - loss: 13.9223 - mse: 13.9223 - mae: 1.5706 - val_loss: 17.1411 - val_mse: 17.1411 - val_mae: 1.5420 - lr: 3.4584e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 3s - loss: 13.9240 - mse: 13.9240 - mae: 1.5730 - val_loss: 17.0791 - val_mse: 17.0791 - val_mae: 1.5523 - lr: 3.4584e-04 - 3s/epoch - 3ms/step\n",
            "Score for fold 4: loss of 17.079063415527344\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 3s - loss: 15.2506 - mse: 15.2506 - mae: 1.5628 - val_loss: 11.5951 - val_mse: 11.5951 - val_mae: 1.5354 - lr: 3.4584e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 3s - loss: 15.2485 - mse: 15.2485 - mae: 1.5605 - val_loss: 11.5079 - val_mse: 11.5079 - val_mae: 1.5442 - lr: 3.4584e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 3s - loss: 15.2641 - mse: 15.2641 - mae: 1.5589 - val_loss: 11.7654 - val_mse: 11.7654 - val_mae: 1.5937 - lr: 3.4584e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 3s - loss: 15.2265 - mse: 15.2265 - mae: 1.5591 - val_loss: 11.6355 - val_mse: 11.6355 - val_mae: 1.5798 - lr: 3.4584e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 3s - loss: 15.1930 - mse: 15.1930 - mae: 1.5563 - val_loss: 11.5342 - val_mse: 11.5342 - val_mae: 1.5908 - lr: 3.4584e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 3s - loss: 15.1775 - mse: 15.1775 - mae: 1.5548 - val_loss: 11.5819 - val_mse: 11.5819 - val_mae: 1.5595 - lr: 3.4584e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 3s - loss: 15.1455 - mse: 15.1455 - mae: 1.5563 - val_loss: 11.5087 - val_mse: 11.5087 - val_mae: 1.6099 - lr: 3.4584e-04 - 3s/epoch - 3ms/step\n",
            "Score for fold 5: loss of 11.508700370788574\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-10-15 18:33:49,687]\u001b[0m Finished trial#49 resulted in value: 14.820000000000002. Current best value is 14.538 with parameters: {'activation': 'relu', 'num_hidden_layer': 3, 'i': 10, 'learning_rate': 0.00010263970460536851}.\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "model_list=[]\n",
        "history_list=[]\n",
        "study_name = 'NN_study_5'\n",
        "study = optuna.create_study(study_name=study_name, load_if_exists=True)\n",
        "func = lambda trial: objective(trial,training_shuffled4,labelsForTrain_shuffled4)\n",
        "study.optimize(func, n_trials=50,)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#{'activation': 'relu', 'num_hidden_layer': 3, 'i': 10, 'learning_rate': 0.00010263970460536851}.\n",
        "optimizer = Adam(learning_rate=0.00010263970460536851 ,clipnorm=1.0)\n",
        "model_5 = create_model(activation=\"relu\",num_hidden_layer=3,num_hidden_unit=1024)\n",
        "\n",
        "es = EarlyStopping(monitor='mse', patience=5)\n",
        "reduce_lr = ReduceLROnPlateau(monitor='val_mse', factor=0.2,\n",
        "                              patience=20, min_lr=0.001)\n",
        "model_5.compile(loss='mse',optimizer=optimizer,metrics=['mse','mae'])\n",
        "history = model_5.fit(training,labelsForTrain,\n",
        "                batch_size=64,\n",
        "                epochs=20,\n",
        "                verbose=2,\n",
        "                validation_data=(valing,labelsForVal),\n",
        "                validation_batch_size=64,\n",
        "                callbacks=[es,reduce_lr])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dI6V1X2t6ZoY",
        "outputId": "8c5cc90f-0979-4399-8235-03a856a6ba5d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "1250/1250 - 7s - loss: 15.5073 - mse: 15.5073 - mae: 1.6090 - val_loss: 13.5538 - val_mse: 13.5538 - val_mae: 1.6833 - lr: 1.0264e-04 - 7s/epoch - 5ms/step\n",
            "Epoch 2/20\n",
            "1250/1250 - 6s - loss: 15.0832 - mse: 15.0832 - mae: 1.5857 - val_loss: 13.5070 - val_mse: 13.5070 - val_mae: 1.6696 - lr: 1.0264e-04 - 6s/epoch - 5ms/step\n",
            "Epoch 3/20\n",
            "1250/1250 - 6s - loss: 15.0662 - mse: 15.0662 - mae: 1.5820 - val_loss: 13.6071 - val_mse: 13.6071 - val_mae: 1.5636 - lr: 1.0264e-04 - 6s/epoch - 5ms/step\n",
            "Epoch 4/20\n",
            "1250/1250 - 6s - loss: 15.0110 - mse: 15.0110 - mae: 1.5838 - val_loss: 13.3334 - val_mse: 13.3334 - val_mae: 1.6444 - lr: 1.0264e-04 - 6s/epoch - 5ms/step\n",
            "Epoch 5/20\n",
            "1250/1250 - 6s - loss: 14.9340 - mse: 14.9340 - mae: 1.5800 - val_loss: 13.5074 - val_mse: 13.5074 - val_mae: 1.5521 - lr: 1.0264e-04 - 6s/epoch - 5ms/step\n",
            "Epoch 6/20\n",
            "1250/1250 - 6s - loss: 14.9375 - mse: 14.9375 - mae: 1.5786 - val_loss: 13.3518 - val_mse: 13.3518 - val_mae: 1.6075 - lr: 1.0264e-04 - 6s/epoch - 5ms/step\n",
            "Epoch 7/20\n",
            "1250/1250 - 6s - loss: 14.9289 - mse: 14.9289 - mae: 1.5738 - val_loss: 13.3774 - val_mse: 13.3774 - val_mae: 1.5949 - lr: 1.0264e-04 - 6s/epoch - 5ms/step\n",
            "Epoch 8/20\n",
            "1250/1250 - 6s - loss: 14.9168 - mse: 14.9168 - mae: 1.5746 - val_loss: 13.3120 - val_mse: 13.3120 - val_mae: 1.5712 - lr: 1.0264e-04 - 6s/epoch - 5ms/step\n",
            "Epoch 9/20\n",
            "1250/1250 - 6s - loss: 14.8771 - mse: 14.8771 - mae: 1.5727 - val_loss: 13.2736 - val_mse: 13.2736 - val_mae: 1.6116 - lr: 1.0264e-04 - 6s/epoch - 5ms/step\n",
            "Epoch 10/20\n",
            "1250/1250 - 6s - loss: 14.8522 - mse: 14.8522 - mae: 1.5695 - val_loss: 13.2383 - val_mse: 13.2383 - val_mae: 1.5654 - lr: 1.0264e-04 - 6s/epoch - 5ms/step\n",
            "Epoch 11/20\n",
            "1250/1250 - 6s - loss: 14.8412 - mse: 14.8412 - mae: 1.5678 - val_loss: 13.2550 - val_mse: 13.2550 - val_mae: 1.7235 - lr: 1.0264e-04 - 6s/epoch - 5ms/step\n",
            "Epoch 12/20\n",
            "1250/1250 - 6s - loss: 14.7974 - mse: 14.7974 - mae: 1.5634 - val_loss: 13.2698 - val_mse: 13.2698 - val_mae: 1.5832 - lr: 1.0264e-04 - 6s/epoch - 5ms/step\n",
            "Epoch 13/20\n",
            "1250/1250 - 6s - loss: 14.7971 - mse: 14.7971 - mae: 1.5646 - val_loss: 13.2305 - val_mse: 13.2305 - val_mae: 1.5716 - lr: 1.0264e-04 - 6s/epoch - 5ms/step\n",
            "Epoch 14/20\n",
            "1250/1250 - 6s - loss: 14.7743 - mse: 14.7743 - mae: 1.5691 - val_loss: 13.2906 - val_mse: 13.2906 - val_mae: 1.5899 - lr: 1.0264e-04 - 6s/epoch - 5ms/step\n",
            "Epoch 15/20\n",
            "1250/1250 - 6s - loss: 14.7738 - mse: 14.7738 - mae: 1.5667 - val_loss: 13.2725 - val_mse: 13.2725 - val_mae: 1.5823 - lr: 1.0264e-04 - 6s/epoch - 5ms/step\n",
            "Epoch 16/20\n",
            "1250/1250 - 6s - loss: 14.7256 - mse: 14.7256 - mae: 1.5610 - val_loss: 13.3436 - val_mse: 13.3436 - val_mae: 1.5379 - lr: 1.0264e-04 - 6s/epoch - 5ms/step\n",
            "Epoch 17/20\n",
            "1250/1250 - 6s - loss: 14.7515 - mse: 14.7515 - mae: 1.5592 - val_loss: 13.2211 - val_mse: 13.2211 - val_mae: 1.5917 - lr: 1.0264e-04 - 6s/epoch - 5ms/step\n",
            "Epoch 18/20\n",
            "1250/1250 - 6s - loss: 14.7123 - mse: 14.7123 - mae: 1.5592 - val_loss: 13.4122 - val_mse: 13.4122 - val_mae: 1.6014 - lr: 1.0264e-04 - 6s/epoch - 5ms/step\n",
            "Epoch 19/20\n",
            "1250/1250 - 6s - loss: 14.6926 - mse: 14.6926 - mae: 1.5615 - val_loss: 13.1888 - val_mse: 13.1888 - val_mae: 1.6489 - lr: 1.0264e-04 - 6s/epoch - 5ms/step\n",
            "Epoch 20/20\n",
            "1250/1250 - 6s - loss: 14.7014 - mse: 14.7014 - mae: 1.5570 - val_loss: 13.1511 - val_mse: 13.1511 - val_mae: 1.5930 - lr: 1.0264e-04 - 6s/epoch - 5ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "results_model5 = model_5.evaluate(testing, labelsForTest, batch_size=32)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qMw3_s1a7iev",
        "outputId": "3c2c0c72-a6c0-43f3-9996-f95b61890edf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "313/313 [==============================] - 1s 2ms/step - loss: 11.6360 - mse: 11.6360 - mae: 1.5715\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PIN_hslsf2Xw"
      },
      "source": [
        "##Shuffle Repetation 6"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t2JRYxcVf0K_"
      },
      "outputs": [],
      "source": [
        "# Shuffle the data after train\n",
        "shuffled5 = shuffle(train_df, random_state=5)\n",
        "training_shuffled5,labelsForTrain_shuffled5=process_shuffle_dataset(shuffled5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WtAarMOHf6v_",
        "outputId": "568f66c9-3f61-476c-8594-465da9bc2949"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 3s - loss: 16.2545 - mse: 16.2545 - mae: 1.6608 - val_loss: 16.6368 - val_mse: 16.6368 - val_mae: 1.5995 - lr: 2.1064e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 15.1146 - mse: 15.1146 - mae: 1.5999 - val_loss: 16.3394 - val_mse: 16.3394 - val_mae: 1.5698 - lr: 2.1064e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 14.9362 - mse: 14.9362 - mae: 1.5920 - val_loss: 16.2105 - val_mse: 16.2105 - val_mae: 1.6078 - lr: 2.1064e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 14.8638 - mse: 14.8638 - mae: 1.5949 - val_loss: 16.3082 - val_mse: 16.3082 - val_mae: 1.5623 - lr: 2.1064e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 14.8344 - mse: 14.8344 - mae: 1.5914 - val_loss: 16.2764 - val_mse: 16.2764 - val_mae: 1.5568 - lr: 2.1064e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 14.8199 - mse: 14.8199 - mae: 1.5926 - val_loss: 16.2294 - val_mse: 16.2294 - val_mae: 1.5635 - lr: 2.1064e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 14.7959 - mse: 14.7959 - mae: 1.5906 - val_loss: 16.1780 - val_mse: 16.1780 - val_mae: 1.5588 - lr: 2.1064e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 2s - loss: 14.7671 - mse: 14.7671 - mae: 1.5901 - val_loss: 16.0859 - val_mse: 16.0859 - val_mae: 1.5852 - lr: 2.1064e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 2s - loss: 14.7520 - mse: 14.7520 - mae: 1.5874 - val_loss: 16.1405 - val_mse: 16.1405 - val_mae: 1.5566 - lr: 2.1064e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 2s - loss: 14.7421 - mse: 14.7421 - mae: 1.5856 - val_loss: 16.1933 - val_mse: 16.1933 - val_mae: 1.5482 - lr: 2.1064e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 2s - loss: 14.7384 - mse: 14.7384 - mae: 1.5871 - val_loss: 16.2395 - val_mse: 16.2395 - val_mae: 1.5449 - lr: 2.1064e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 2s - loss: 14.7342 - mse: 14.7342 - mae: 1.5803 - val_loss: 16.1055 - val_mse: 16.1055 - val_mae: 1.5820 - lr: 2.1064e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 13/100\n",
            "1000/1000 - 2s - loss: 14.7020 - mse: 14.7020 - mae: 1.5838 - val_loss: 16.1598 - val_mse: 16.1598 - val_mae: 1.5462 - lr: 2.1064e-04 - 2s/epoch - 2ms/step\n",
            "Score for fold 1: loss of 16.159751892089844\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 16.0000 - mse: 16.0000 - mae: 1.5884 - val_loss: 10.8732 - val_mse: 10.8732 - val_mae: 1.5905 - lr: 2.1064e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 15.9700 - mse: 15.9700 - mae: 1.5866 - val_loss: 10.9238 - val_mse: 10.9238 - val_mae: 1.5578 - lr: 2.1064e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 15.9505 - mse: 15.9505 - mae: 1.5866 - val_loss: 10.9004 - val_mse: 10.9004 - val_mae: 1.5578 - lr: 2.1064e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 15.9449 - mse: 15.9449 - mae: 1.5860 - val_loss: 11.0316 - val_mse: 11.0316 - val_mae: 1.5063 - lr: 2.1064e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 15.9460 - mse: 15.9460 - mae: 1.5829 - val_loss: 10.9231 - val_mse: 10.9231 - val_mae: 1.5394 - lr: 2.1064e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 15.9469 - mse: 15.9469 - mae: 1.5803 - val_loss: 10.8726 - val_mse: 10.8726 - val_mae: 1.5838 - lr: 2.1064e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 15.9198 - mse: 15.9198 - mae: 1.5829 - val_loss: 10.8843 - val_mse: 10.8843 - val_mae: 1.5659 - lr: 2.1064e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 2s - loss: 15.9213 - mse: 15.9213 - mae: 1.5826 - val_loss: 10.8802 - val_mse: 10.8802 - val_mae: 1.5668 - lr: 2.1064e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 2s - loss: 15.8970 - mse: 15.8970 - mae: 1.5807 - val_loss: 10.9868 - val_mse: 10.9868 - val_mae: 1.5378 - lr: 2.1064e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 2s - loss: 15.9156 - mse: 15.9156 - mae: 1.5845 - val_loss: 10.8778 - val_mse: 10.8778 - val_mae: 1.5522 - lr: 2.1064e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 2s - loss: 15.9033 - mse: 15.9033 - mae: 1.5765 - val_loss: 10.8485 - val_mse: 10.8485 - val_mae: 1.5943 - lr: 2.1064e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 2s - loss: 15.8753 - mse: 15.8753 - mae: 1.5812 - val_loss: 10.8961 - val_mse: 10.8961 - val_mae: 1.5478 - lr: 2.1064e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 13/100\n",
            "1000/1000 - 2s - loss: 15.9005 - mse: 15.9005 - mae: 1.5784 - val_loss: 10.8844 - val_mse: 10.8844 - val_mae: 1.5412 - lr: 2.1064e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 14/100\n",
            "1000/1000 - 2s - loss: 15.8888 - mse: 15.8888 - mae: 1.5775 - val_loss: 10.8597 - val_mse: 10.8597 - val_mae: 1.5706 - lr: 2.1064e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 15/100\n",
            "1000/1000 - 2s - loss: 15.8646 - mse: 15.8646 - mae: 1.5805 - val_loss: 10.8531 - val_mse: 10.8531 - val_mae: 1.5539 - lr: 2.1064e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 16/100\n",
            "1000/1000 - 2s - loss: 15.8823 - mse: 15.8823 - mae: 1.5775 - val_loss: 10.8360 - val_mse: 10.8360 - val_mae: 1.5742 - lr: 2.1064e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 17/100\n",
            "1000/1000 - 2s - loss: 15.8519 - mse: 15.8519 - mae: 1.5753 - val_loss: 10.8685 - val_mse: 10.8685 - val_mae: 1.5570 - lr: 2.1064e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 18/100\n",
            "1000/1000 - 2s - loss: 15.8238 - mse: 15.8238 - mae: 1.5758 - val_loss: 10.8312 - val_mse: 10.8312 - val_mae: 1.5624 - lr: 2.1064e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 19/100\n",
            "1000/1000 - 2s - loss: 15.8130 - mse: 15.8130 - mae: 1.5722 - val_loss: 10.8672 - val_mse: 10.8672 - val_mae: 1.5524 - lr: 2.1064e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 20/100\n",
            "1000/1000 - 2s - loss: 15.8515 - mse: 15.8515 - mae: 1.5746 - val_loss: 10.8235 - val_mse: 10.8235 - val_mae: 1.5544 - lr: 2.1064e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 21/100\n",
            "1000/1000 - 2s - loss: 15.8220 - mse: 15.8220 - mae: 1.5728 - val_loss: 10.8542 - val_mse: 10.8542 - val_mae: 1.5461 - lr: 2.1064e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 22/100\n",
            "1000/1000 - 2s - loss: 15.8556 - mse: 15.8556 - mae: 1.5729 - val_loss: 10.8192 - val_mse: 10.8192 - val_mae: 1.5510 - lr: 2.1064e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 23/100\n",
            "1000/1000 - 2s - loss: 15.8110 - mse: 15.8110 - mae: 1.5701 - val_loss: 10.8312 - val_mse: 10.8312 - val_mae: 1.6030 - lr: 2.1064e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 24/100\n",
            "1000/1000 - 2s - loss: 15.8209 - mse: 15.8209 - mae: 1.5753 - val_loss: 10.8341 - val_mse: 10.8341 - val_mae: 1.5487 - lr: 2.1064e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 25/100\n",
            "1000/1000 - 2s - loss: 15.8200 - mse: 15.8200 - mae: 1.5722 - val_loss: 10.8219 - val_mse: 10.8219 - val_mae: 1.5424 - lr: 2.1064e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 26/100\n",
            "1000/1000 - 2s - loss: 15.8068 - mse: 15.8068 - mae: 1.5706 - val_loss: 10.8440 - val_mse: 10.8440 - val_mae: 1.5409 - lr: 2.1064e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 27/100\n",
            "1000/1000 - 2s - loss: 15.8065 - mse: 15.8065 - mae: 1.5706 - val_loss: 10.8593 - val_mse: 10.8593 - val_mae: 1.5397 - lr: 2.1064e-04 - 2s/epoch - 2ms/step\n",
            "Score for fold 2: loss of 10.859277725219727\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 15.2039 - mse: 15.2039 - mae: 1.5699 - val_loss: 13.2224 - val_mse: 13.2224 - val_mae: 1.5528 - lr: 2.1064e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 15.2098 - mse: 15.2098 - mae: 1.5700 - val_loss: 13.1849 - val_mse: 13.1849 - val_mae: 1.5827 - lr: 2.1064e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 15.2223 - mse: 15.2223 - mae: 1.5674 - val_loss: 13.2230 - val_mse: 13.2230 - val_mae: 1.5658 - lr: 2.1064e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 15.1885 - mse: 15.1885 - mae: 1.5672 - val_loss: 13.2823 - val_mse: 13.2823 - val_mae: 1.5630 - lr: 2.1064e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 15.1930 - mse: 15.1930 - mae: 1.5653 - val_loss: 13.2673 - val_mse: 13.2673 - val_mae: 1.5786 - lr: 2.1064e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 15.1798 - mse: 15.1798 - mae: 1.5648 - val_loss: 13.3012 - val_mse: 13.3012 - val_mae: 1.5766 - lr: 2.1064e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 15.1716 - mse: 15.1716 - mae: 1.5672 - val_loss: 13.2489 - val_mse: 13.2489 - val_mae: 1.5837 - lr: 2.1064e-04 - 2s/epoch - 2ms/step\n",
            "Score for fold 3: loss of 13.248917579650879\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 13.8465 - mse: 13.8465 - mae: 1.5654 - val_loss: 18.5864 - val_mse: 18.5864 - val_mae: 1.5268 - lr: 2.1064e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 13.8702 - mse: 13.8702 - mae: 1.5682 - val_loss: 18.4746 - val_mse: 18.4746 - val_mae: 1.5495 - lr: 2.1064e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 13.8604 - mse: 13.8604 - mae: 1.5693 - val_loss: 18.5136 - val_mse: 18.5136 - val_mae: 1.5657 - lr: 2.1064e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 13.8369 - mse: 13.8369 - mae: 1.5644 - val_loss: 18.5317 - val_mse: 18.5317 - val_mae: 1.5632 - lr: 2.1064e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 13.8612 - mse: 13.8612 - mae: 1.5663 - val_loss: 18.5133 - val_mse: 18.5133 - val_mae: 1.5708 - lr: 2.1064e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 13.8390 - mse: 13.8390 - mae: 1.5676 - val_loss: 18.5202 - val_mse: 18.5202 - val_mae: 1.5807 - lr: 2.1064e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 13.8232 - mse: 13.8232 - mae: 1.5659 - val_loss: 18.5318 - val_mse: 18.5318 - val_mae: 1.5721 - lr: 2.1064e-04 - 2s/epoch - 2ms/step\n",
            "Score for fold 4: loss of 18.531845092773438\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 14.5889 - mse: 14.5889 - mae: 1.5607 - val_loss: 15.5741 - val_mse: 15.5741 - val_mae: 1.5805 - lr: 2.1064e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 14.5813 - mse: 14.5813 - mae: 1.5584 - val_loss: 15.5410 - val_mse: 15.5410 - val_mae: 1.6061 - lr: 2.1064e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 14.5931 - mse: 14.5931 - mae: 1.5603 - val_loss: 15.6421 - val_mse: 15.6421 - val_mae: 1.5824 - lr: 2.1064e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 14.5806 - mse: 14.5806 - mae: 1.5589 - val_loss: 15.5384 - val_mse: 15.5384 - val_mae: 1.6096 - lr: 2.1064e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 14.5478 - mse: 14.5478 - mae: 1.5607 - val_loss: 15.7781 - val_mse: 15.7781 - val_mae: 1.5667 - lr: 2.1064e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 14.5577 - mse: 14.5577 - mae: 1.5604 - val_loss: 15.7248 - val_mse: 15.7248 - val_mae: 1.5754 - lr: 2.1064e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 14.5717 - mse: 14.5717 - mae: 1.5604 - val_loss: 15.7419 - val_mse: 15.7419 - val_mae: 1.5863 - lr: 2.1064e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 2s - loss: 14.5745 - mse: 14.5745 - mae: 1.5551 - val_loss: 15.6586 - val_mse: 15.6586 - val_mae: 1.6269 - lr: 2.1064e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 2s - loss: 14.5575 - mse: 14.5575 - mae: 1.5582 - val_loss: 15.6037 - val_mse: 15.6037 - val_mae: 1.5901 - lr: 2.1064e-04 - 2s/epoch - 2ms/step\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-10-16 05:45:07,257]\u001b[0m Finished trial#0 resulted in value: 14.879999999999999. Current best value is 14.879999999999999 with parameters: {'activation': 'relu', 'num_hidden_layer': 3, 'i': 6, 'learning_rate': 0.00021063978488417066}.\u001b[0m\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Score for fold 5: loss of 15.603734016418457\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 4s - loss: 16.7996 - mse: 16.7996 - mae: 1.6747 - val_loss: 11.8674 - val_mse: 11.8674 - val_mae: 1.6322 - lr: 6.6448e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 4s - loss: 16.4184 - mse: 16.4184 - mae: 1.6336 - val_loss: 12.2021 - val_mse: 12.2021 - val_mae: 1.5819 - lr: 6.6448e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 4s - loss: 16.3251 - mse: 16.3251 - mae: 1.6265 - val_loss: 11.8450 - val_mse: 11.8450 - val_mae: 1.6159 - lr: 6.6448e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 4s - loss: 16.2582 - mse: 16.2582 - mae: 1.6213 - val_loss: 11.6132 - val_mse: 11.6132 - val_mae: 1.5805 - lr: 6.6448e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 4s - loss: 16.1834 - mse: 16.1834 - mae: 1.6166 - val_loss: 11.8516 - val_mse: 11.8516 - val_mae: 1.5262 - lr: 6.6448e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 4s - loss: 16.1652 - mse: 16.1652 - mae: 1.6117 - val_loss: 11.5739 - val_mse: 11.5739 - val_mae: 1.6193 - lr: 6.6448e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 4s - loss: 16.1247 - mse: 16.1247 - mae: 1.6074 - val_loss: 11.6325 - val_mse: 11.6325 - val_mae: 1.6288 - lr: 6.6448e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 4s - loss: 16.1303 - mse: 16.1303 - mae: 1.6090 - val_loss: 11.5910 - val_mse: 11.5910 - val_mae: 1.5269 - lr: 6.6448e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 4s - loss: 16.1094 - mse: 16.1094 - mae: 1.6038 - val_loss: 11.5562 - val_mse: 11.5562 - val_mae: 1.6396 - lr: 6.6448e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 4s - loss: 16.0836 - mse: 16.0836 - mae: 1.6095 - val_loss: 11.4062 - val_mse: 11.4062 - val_mae: 1.5601 - lr: 6.6448e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 4s - loss: 16.0322 - mse: 16.0322 - mae: 1.6033 - val_loss: 11.7135 - val_mse: 11.7135 - val_mae: 1.6418 - lr: 6.6448e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 4s - loss: 16.0319 - mse: 16.0319 - mae: 1.6078 - val_loss: 11.4486 - val_mse: 11.4486 - val_mae: 1.5832 - lr: 6.6448e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 13/100\n",
            "1000/1000 - 4s - loss: 15.9727 - mse: 15.9727 - mae: 1.5970 - val_loss: 11.6651 - val_mse: 11.6651 - val_mae: 1.5449 - lr: 6.6448e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 14/100\n",
            "1000/1000 - 4s - loss: 15.9628 - mse: 15.9628 - mae: 1.5921 - val_loss: 11.8952 - val_mse: 11.8952 - val_mae: 1.5501 - lr: 6.6448e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 15/100\n",
            "1000/1000 - 4s - loss: 15.9054 - mse: 15.9054 - mae: 1.6016 - val_loss: 11.7248 - val_mse: 11.7248 - val_mae: 1.4953 - lr: 6.6448e-04 - 4s/epoch - 4ms/step\n",
            "Score for fold 1: loss of 11.724757194519043\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 4s - loss: 13.6140 - mse: 13.6140 - mae: 1.5853 - val_loss: 20.6810 - val_mse: 20.6810 - val_mae: 1.7014 - lr: 6.6448e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 4s - loss: 13.5788 - mse: 13.5788 - mae: 1.5845 - val_loss: 20.7806 - val_mse: 20.7806 - val_mae: 1.5710 - lr: 6.6448e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 4s - loss: 13.5485 - mse: 13.5485 - mae: 1.5822 - val_loss: 20.6394 - val_mse: 20.6394 - val_mae: 1.6071 - lr: 6.6448e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 4s - loss: 13.5112 - mse: 13.5112 - mae: 1.5788 - val_loss: 20.7782 - val_mse: 20.7782 - val_mae: 1.6584 - lr: 6.6448e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 4s - loss: 13.5085 - mse: 13.5085 - mae: 1.5791 - val_loss: 20.7265 - val_mse: 20.7265 - val_mae: 1.6200 - lr: 6.6448e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 4s - loss: 13.5194 - mse: 13.5194 - mae: 1.5780 - val_loss: 20.7799 - val_mse: 20.7799 - val_mae: 1.6022 - lr: 6.6448e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 4s - loss: 13.4363 - mse: 13.4363 - mae: 1.5786 - val_loss: 20.7330 - val_mse: 20.7330 - val_mae: 1.6208 - lr: 6.6448e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 4s - loss: 13.4074 - mse: 13.4074 - mae: 1.5772 - val_loss: 20.6802 - val_mse: 20.6802 - val_mae: 1.6274 - lr: 6.6448e-04 - 4s/epoch - 4ms/step\n",
            "Score for fold 2: loss of 20.68017578125\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 4s - loss: 15.9752 - mse: 15.9752 - mae: 1.5964 - val_loss: 10.8185 - val_mse: 10.8185 - val_mae: 1.5722 - lr: 6.6448e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 4s - loss: 15.9263 - mse: 15.9263 - mae: 1.5944 - val_loss: 10.9287 - val_mse: 10.9287 - val_mae: 1.5435 - lr: 6.6448e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 4s - loss: 15.8891 - mse: 15.8891 - mae: 1.5955 - val_loss: 10.6530 - val_mse: 10.6530 - val_mae: 1.6037 - lr: 6.6448e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 4s - loss: 15.8994 - mse: 15.8994 - mae: 1.5918 - val_loss: 10.6439 - val_mse: 10.6439 - val_mae: 1.5411 - lr: 6.6448e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 4s - loss: 15.8472 - mse: 15.8472 - mae: 1.5926 - val_loss: 10.7267 - val_mse: 10.7267 - val_mae: 1.5647 - lr: 6.6448e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 4s - loss: 15.7841 - mse: 15.7841 - mae: 1.5910 - val_loss: 10.8213 - val_mse: 10.8213 - val_mae: 1.6060 - lr: 6.6448e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 4s - loss: 15.8202 - mse: 15.8202 - mae: 1.5852 - val_loss: 10.7807 - val_mse: 10.7807 - val_mae: 1.5594 - lr: 6.6448e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 4s - loss: 15.8069 - mse: 15.8069 - mae: 1.5881 - val_loss: 10.8145 - val_mse: 10.8145 - val_mae: 1.5645 - lr: 6.6448e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 4s - loss: 15.7335 - mse: 15.7335 - mae: 1.5859 - val_loss: 10.6967 - val_mse: 10.6967 - val_mae: 1.5496 - lr: 6.6448e-04 - 4s/epoch - 4ms/step\n",
            "Score for fold 3: loss of 10.696678161621094\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 4s - loss: 15.1803 - mse: 15.1803 - mae: 1.5895 - val_loss: 13.0239 - val_mse: 13.0239 - val_mae: 1.5438 - lr: 6.6448e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 4s - loss: 15.1613 - mse: 15.1613 - mae: 1.5823 - val_loss: 13.1176 - val_mse: 13.1176 - val_mae: 1.5651 - lr: 6.6448e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 4s - loss: 15.1394 - mse: 15.1394 - mae: 1.5826 - val_loss: 13.2390 - val_mse: 13.2390 - val_mae: 1.5882 - lr: 6.6448e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 4s - loss: 15.0399 - mse: 15.0399 - mae: 1.5836 - val_loss: 13.2005 - val_mse: 13.2005 - val_mae: 1.5251 - lr: 6.6448e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 4s - loss: 14.9784 - mse: 14.9784 - mae: 1.5762 - val_loss: 13.3264 - val_mse: 13.3264 - val_mae: 1.6295 - lr: 6.6448e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 4s - loss: 15.0243 - mse: 15.0243 - mae: 1.5772 - val_loss: 13.2764 - val_mse: 13.2764 - val_mae: 1.6047 - lr: 6.6448e-04 - 4s/epoch - 4ms/step\n",
            "Score for fold 4: loss of 13.276405334472656\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 4s - loss: 13.8234 - mse: 13.8234 - mae: 1.5592 - val_loss: 18.1851 - val_mse: 18.1851 - val_mae: 1.6303 - lr: 6.6448e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 4s - loss: 13.8144 - mse: 13.8144 - mae: 1.5571 - val_loss: 18.0732 - val_mse: 18.0732 - val_mae: 1.6345 - lr: 6.6448e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 4s - loss: 13.7744 - mse: 13.7744 - mae: 1.5532 - val_loss: 18.5211 - val_mse: 18.5211 - val_mae: 1.6509 - lr: 6.6448e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 4s - loss: 13.7259 - mse: 13.7259 - mae: 1.5492 - val_loss: 18.3091 - val_mse: 18.3091 - val_mae: 1.6817 - lr: 6.6448e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 4s - loss: 13.6601 - mse: 13.6601 - mae: 1.5551 - val_loss: 18.0103 - val_mse: 18.0103 - val_mae: 1.6347 - lr: 6.6448e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 4s - loss: 13.6493 - mse: 13.6493 - mae: 1.5534 - val_loss: 18.1715 - val_mse: 18.1715 - val_mae: 1.7396 - lr: 6.6448e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 4s - loss: 13.6253 - mse: 13.6253 - mae: 1.5485 - val_loss: 18.6677 - val_mse: 18.6677 - val_mae: 1.5906 - lr: 6.6448e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 4s - loss: 13.6674 - mse: 13.6674 - mae: 1.5483 - val_loss: 18.7389 - val_mse: 18.7389 - val_mae: 1.6746 - lr: 6.6448e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 4s - loss: 13.5887 - mse: 13.5887 - mae: 1.5494 - val_loss: 18.3045 - val_mse: 18.3045 - val_mae: 1.6880 - lr: 6.6448e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 4s - loss: 13.5543 - mse: 13.5543 - mae: 1.5421 - val_loss: 18.4527 - val_mse: 18.4527 - val_mae: 1.5939 - lr: 6.6448e-04 - 4s/epoch - 4ms/step\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-10-16 05:48:07,734]\u001b[0m Finished trial#1 resulted in value: 14.966. Current best value is 14.879999999999999 with parameters: {'activation': 'relu', 'num_hidden_layer': 3, 'i': 6, 'learning_rate': 0.00021063978488417066}.\u001b[0m\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Score for fold 5: loss of 18.452739715576172\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 7s - loss: 16.1393 - mse: 16.1393 - mae: 1.8656 - val_loss: 22.1841 - val_mse: 22.1841 - val_mae: 1.8592 - lr: 0.0077 - 7s/epoch - 7ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 6s - loss: 15.5685 - mse: 15.5685 - mae: 1.8162 - val_loss: 21.2417 - val_mse: 21.2417 - val_mae: 1.8389 - lr: 0.0077 - 6s/epoch - 6ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 7s - loss: 15.3709 - mse: 15.3709 - mae: 1.7837 - val_loss: 21.7404 - val_mse: 21.7404 - val_mae: 1.7751 - lr: 0.0077 - 7s/epoch - 7ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 6s - loss: 15.3673 - mse: 15.3673 - mae: 1.8026 - val_loss: 23.2597 - val_mse: 23.2597 - val_mae: 1.7769 - lr: 0.0077 - 6s/epoch - 6ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 7s - loss: 15.4532 - mse: 15.4532 - mae: 1.8033 - val_loss: 22.0814 - val_mse: 22.0814 - val_mae: 1.8589 - lr: 0.0077 - 7s/epoch - 7ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 6s - loss: 15.4324 - mse: 15.4324 - mae: 1.7993 - val_loss: 22.0694 - val_mse: 22.0694 - val_mae: 1.7800 - lr: 0.0077 - 6s/epoch - 6ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 6s - loss: 15.2969 - mse: 15.2969 - mae: 1.7669 - val_loss: 21.3009 - val_mse: 21.3009 - val_mae: 1.7299 - lr: 0.0077 - 6s/epoch - 6ms/step\n",
            "Score for fold 1: loss of 21.300899505615234\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 7s - loss: 13.9160 - mse: 13.9160 - mae: 1.6342 - val_loss: 23.3000 - val_mse: 23.3000 - val_mae: 1.7345 - lr: 0.0015 - 7s/epoch - 7ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 6s - loss: 13.8643 - mse: 13.8643 - mae: 1.6303 - val_loss: 22.9824 - val_mse: 22.9824 - val_mae: 1.7240 - lr: 0.0015 - 6s/epoch - 6ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 7s - loss: 13.9175 - mse: 13.9175 - mae: 1.6354 - val_loss: 23.0573 - val_mse: 23.0573 - val_mae: 1.6943 - lr: 0.0015 - 7s/epoch - 7ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 6s - loss: 13.9190 - mse: 13.9190 - mae: 1.6367 - val_loss: 23.2201 - val_mse: 23.2201 - val_mae: 1.6502 - lr: 0.0015 - 6s/epoch - 6ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 7s - loss: 13.9132 - mse: 13.9132 - mae: 1.6306 - val_loss: 23.0600 - val_mse: 23.0600 - val_mae: 1.6862 - lr: 0.0015 - 7s/epoch - 7ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 6s - loss: 13.8778 - mse: 13.8778 - mae: 1.6356 - val_loss: 23.0776 - val_mse: 23.0776 - val_mae: 1.6712 - lr: 0.0015 - 6s/epoch - 6ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 6s - loss: 13.8924 - mse: 13.8924 - mae: 1.6348 - val_loss: 23.9423 - val_mse: 23.9423 - val_mae: 1.6846 - lr: 0.0015 - 6s/epoch - 6ms/step\n",
            "Score for fold 2: loss of 23.942312240600586\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 6s - loss: 16.8822 - mse: 16.8822 - mae: 1.6454 - val_loss: 11.5647 - val_mse: 11.5647 - val_mae: 1.5944 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 6s - loss: 16.9463 - mse: 16.9463 - mae: 1.6465 - val_loss: 10.9533 - val_mse: 10.9533 - val_mae: 1.6455 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 7s - loss: 16.9197 - mse: 16.9197 - mae: 1.6469 - val_loss: 10.8584 - val_mse: 10.8584 - val_mae: 1.6253 - lr: 0.0010 - 7s/epoch - 7ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 7s - loss: 16.9365 - mse: 16.9365 - mae: 1.6501 - val_loss: 10.8575 - val_mse: 10.8575 - val_mae: 1.6557 - lr: 0.0010 - 7s/epoch - 7ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 7s - loss: 16.8706 - mse: 16.8706 - mae: 1.6473 - val_loss: 10.8754 - val_mse: 10.8754 - val_mae: 1.6900 - lr: 0.0010 - 7s/epoch - 7ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 7s - loss: 16.8689 - mse: 16.8689 - mae: 1.6511 - val_loss: 11.0223 - val_mse: 11.0223 - val_mae: 1.6585 - lr: 0.0010 - 7s/epoch - 7ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 7s - loss: 16.9045 - mse: 16.9045 - mae: 1.6436 - val_loss: 10.8578 - val_mse: 10.8578 - val_mae: 1.6467 - lr: 0.0010 - 7s/epoch - 7ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 7s - loss: 16.9824 - mse: 16.9824 - mae: 1.6499 - val_loss: 10.8616 - val_mse: 10.8616 - val_mae: 1.6623 - lr: 0.0010 - 7s/epoch - 7ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 6s - loss: 16.9407 - mse: 16.9407 - mae: 1.6484 - val_loss: 11.1266 - val_mse: 11.1266 - val_mae: 1.5985 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
            "Score for fold 3: loss of 11.126564025878906\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 6s - loss: 16.7600 - mse: 16.7600 - mae: 1.6628 - val_loss: 11.8589 - val_mse: 11.8589 - val_mae: 1.6510 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 6s - loss: 16.6838 - mse: 16.6838 - mae: 1.6595 - val_loss: 11.8960 - val_mse: 11.8960 - val_mae: 1.6601 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 7s - loss: 16.6905 - mse: 16.6905 - mae: 1.6577 - val_loss: 12.0635 - val_mse: 12.0635 - val_mae: 1.6575 - lr: 0.0010 - 7s/epoch - 7ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 7s - loss: 16.6984 - mse: 16.6984 - mae: 1.6646 - val_loss: 12.3641 - val_mse: 12.3641 - val_mae: 1.6961 - lr: 0.0010 - 7s/epoch - 7ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 6s - loss: 16.7232 - mse: 16.7232 - mae: 1.6612 - val_loss: 11.8542 - val_mse: 11.8542 - val_mae: 1.6187 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 7s - loss: 16.6712 - mse: 16.6712 - mae: 1.6629 - val_loss: 11.8501 - val_mse: 11.8501 - val_mae: 1.6383 - lr: 0.0010 - 7s/epoch - 7ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 6s - loss: 16.6608 - mse: 16.6608 - mae: 1.6614 - val_loss: 12.2411 - val_mse: 12.2411 - val_mae: 1.7584 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 7s - loss: 16.6895 - mse: 16.6895 - mae: 1.6574 - val_loss: 11.8962 - val_mse: 11.8962 - val_mae: 1.6744 - lr: 0.0010 - 7s/epoch - 7ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 7s - loss: 16.6184 - mse: 16.6184 - mae: 1.6609 - val_loss: 11.8554 - val_mse: 11.8554 - val_mae: 1.6126 - lr: 0.0010 - 7s/epoch - 7ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 7s - loss: 16.6392 - mse: 16.6392 - mae: 1.6601 - val_loss: 12.4322 - val_mse: 12.4322 - val_mae: 1.5510 - lr: 0.0010 - 7s/epoch - 7ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 7s - loss: 16.6984 - mse: 16.6984 - mae: 1.6638 - val_loss: 12.0430 - val_mse: 12.0430 - val_mae: 1.5882 - lr: 0.0010 - 7s/epoch - 7ms/step\n",
            "Score for fold 4: loss of 12.043013572692871\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 7s - loss: 16.8391 - mse: 16.8391 - mae: 1.6587 - val_loss: 11.2601 - val_mse: 11.2601 - val_mae: 1.6039 - lr: 0.0010 - 7s/epoch - 7ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 6s - loss: 16.7976 - mse: 16.7976 - mae: 1.6597 - val_loss: 11.2819 - val_mse: 11.2819 - val_mae: 1.6344 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 6s - loss: 16.8011 - mse: 16.8011 - mae: 1.6628 - val_loss: 11.5797 - val_mse: 11.5797 - val_mae: 1.6993 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 7s - loss: 16.7851 - mse: 16.7851 - mae: 1.6615 - val_loss: 11.4330 - val_mse: 11.4330 - val_mae: 1.5553 - lr: 0.0010 - 7s/epoch - 7ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 7s - loss: 16.8880 - mse: 16.8880 - mae: 1.6651 - val_loss: 11.3866 - val_mse: 11.3866 - val_mae: 1.5736 - lr: 0.0010 - 7s/epoch - 7ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 7s - loss: 16.8340 - mse: 16.8340 - mae: 1.6600 - val_loss: 11.4437 - val_mse: 11.4437 - val_mae: 1.6602 - lr: 0.0010 - 7s/epoch - 7ms/step\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-10-16 05:52:35,698]\u001b[0m Finished trial#2 resulted in value: 15.969999999999999. Current best value is 14.879999999999999 with parameters: {'activation': 'relu', 'num_hidden_layer': 3, 'i': 6, 'learning_rate': 0.00021063978488417066}.\u001b[0m\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Score for fold 5: loss of 11.443719863891602\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 15.6368 - mse: 15.6368 - mae: 1.6652 - val_loss: 17.8072 - val_mse: 17.8072 - val_mae: 1.6463 - lr: 8.1238e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 15.1351 - mse: 15.1351 - mae: 1.6268 - val_loss: 17.7556 - val_mse: 17.7556 - val_mae: 1.6256 - lr: 8.1238e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 15.1601 - mse: 15.1601 - mae: 1.6252 - val_loss: 17.7815 - val_mse: 17.7815 - val_mae: 1.6404 - lr: 8.1238e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 15.1249 - mse: 15.1249 - mae: 1.6239 - val_loss: 17.8654 - val_mse: 17.8654 - val_mae: 1.6224 - lr: 8.1238e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 15.1084 - mse: 15.1084 - mae: 1.6202 - val_loss: 17.8473 - val_mse: 17.8473 - val_mae: 1.6160 - lr: 8.1238e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 15.1266 - mse: 15.1266 - mae: 1.6220 - val_loss: 17.7400 - val_mse: 17.7400 - val_mae: 1.6786 - lr: 8.1238e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 15.1279 - mse: 15.1279 - mae: 1.6256 - val_loss: 17.7440 - val_mse: 17.7440 - val_mae: 1.6649 - lr: 8.1238e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 2s - loss: 15.1424 - mse: 15.1424 - mae: 1.6264 - val_loss: 17.7280 - val_mse: 17.7280 - val_mae: 1.6618 - lr: 8.1238e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 2s - loss: 15.1291 - mse: 15.1291 - mae: 1.6281 - val_loss: 17.8040 - val_mse: 17.8040 - val_mae: 1.6231 - lr: 8.1238e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 2s - loss: 15.1420 - mse: 15.1420 - mae: 1.6259 - val_loss: 17.8280 - val_mse: 17.8280 - val_mae: 1.6237 - lr: 8.1238e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 2s - loss: 15.1361 - mse: 15.1361 - mae: 1.6237 - val_loss: 17.7930 - val_mse: 17.7930 - val_mae: 1.6475 - lr: 8.1238e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 2s - loss: 15.1228 - mse: 15.1228 - mae: 1.6299 - val_loss: 17.7323 - val_mse: 17.7323 - val_mae: 1.6967 - lr: 8.1238e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 13/100\n",
            "1000/1000 - 2s - loss: 15.1443 - mse: 15.1443 - mae: 1.6296 - val_loss: 17.8249 - val_mse: 17.8249 - val_mae: 1.5961 - lr: 8.1238e-04 - 2s/epoch - 2ms/step\n",
            "Score for fold 1: loss of 17.824914932250977\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 16.2041 - mse: 16.2041 - mae: 1.6415 - val_loss: 13.4402 - val_mse: 13.4402 - val_mae: 1.6522 - lr: 8.1238e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 16.1964 - mse: 16.1964 - mae: 1.6426 - val_loss: 13.4858 - val_mse: 13.4858 - val_mae: 1.6153 - lr: 8.1238e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 16.2231 - mse: 16.2231 - mae: 1.6385 - val_loss: 13.5251 - val_mse: 13.5251 - val_mae: 1.5759 - lr: 8.1238e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 16.2339 - mse: 16.2339 - mae: 1.6395 - val_loss: 13.4801 - val_mse: 13.4801 - val_mae: 1.6769 - lr: 8.1238e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 16.1956 - mse: 16.1956 - mae: 1.6452 - val_loss: 13.4521 - val_mse: 13.4521 - val_mae: 1.6508 - lr: 8.1238e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 16.1819 - mse: 16.1819 - mae: 1.6434 - val_loss: 13.4764 - val_mse: 13.4764 - val_mae: 1.6383 - lr: 8.1238e-04 - 2s/epoch - 2ms/step\n",
            "Score for fold 2: loss of 13.476418495178223\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 14.3590 - mse: 14.3590 - mae: 1.6420 - val_loss: 20.8057 - val_mse: 20.8057 - val_mae: 1.6249 - lr: 8.1238e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 14.3904 - mse: 14.3904 - mae: 1.6367 - val_loss: 20.7873 - val_mse: 20.7873 - val_mae: 1.6148 - lr: 8.1238e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 14.3645 - mse: 14.3645 - mae: 1.6382 - val_loss: 20.7840 - val_mse: 20.7840 - val_mae: 1.6323 - lr: 8.1238e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 14.3744 - mse: 14.3744 - mae: 1.6346 - val_loss: 20.7714 - val_mse: 20.7714 - val_mae: 1.6233 - lr: 8.1238e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 14.3876 - mse: 14.3876 - mae: 1.6338 - val_loss: 20.8430 - val_mse: 20.8430 - val_mae: 1.6083 - lr: 8.1238e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 14.3716 - mse: 14.3716 - mae: 1.6352 - val_loss: 20.7631 - val_mse: 20.7631 - val_mae: 1.6578 - lr: 8.1238e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 14.3929 - mse: 14.3929 - mae: 1.6421 - val_loss: 20.7706 - val_mse: 20.7706 - val_mae: 1.6303 - lr: 8.1238e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 2s - loss: 14.3558 - mse: 14.3558 - mae: 1.6394 - val_loss: 20.8176 - val_mse: 20.8176 - val_mae: 1.6140 - lr: 8.1238e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 2s - loss: 14.3596 - mse: 14.3596 - mae: 1.6375 - val_loss: 20.7584 - val_mse: 20.7584 - val_mae: 1.7016 - lr: 8.1238e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 2s - loss: 14.3694 - mse: 14.3694 - mae: 1.6438 - val_loss: 20.8088 - val_mse: 20.8088 - val_mae: 1.5974 - lr: 8.1238e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 2s - loss: 14.3610 - mse: 14.3610 - mae: 1.6366 - val_loss: 20.7486 - val_mse: 20.7486 - val_mae: 1.6950 - lr: 8.1238e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 2s - loss: 14.3835 - mse: 14.3835 - mae: 1.6360 - val_loss: 20.9659 - val_mse: 20.9659 - val_mae: 1.5697 - lr: 8.1238e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 13/100\n",
            "1000/1000 - 2s - loss: 14.3809 - mse: 14.3809 - mae: 1.6422 - val_loss: 20.7543 - val_mse: 20.7543 - val_mae: 1.6692 - lr: 8.1238e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 14/100\n",
            "1000/1000 - 2s - loss: 14.3817 - mse: 14.3817 - mae: 1.6331 - val_loss: 20.7895 - val_mse: 20.7895 - val_mae: 1.6387 - lr: 8.1238e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 15/100\n",
            "1000/1000 - 2s - loss: 14.3859 - mse: 14.3859 - mae: 1.6356 - val_loss: 20.7585 - val_mse: 20.7585 - val_mae: 1.6885 - lr: 8.1238e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 16/100\n",
            "1000/1000 - 2s - loss: 14.3794 - mse: 14.3794 - mae: 1.6345 - val_loss: 20.8259 - val_mse: 20.8259 - val_mae: 1.6185 - lr: 8.1238e-04 - 2s/epoch - 2ms/step\n",
            "Score for fold 3: loss of 20.82585906982422\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 16.1206 - mse: 16.1206 - mae: 1.6359 - val_loss: 13.8894 - val_mse: 13.8894 - val_mae: 1.6469 - lr: 8.1238e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 16.1033 - mse: 16.1033 - mae: 1.6370 - val_loss: 13.9427 - val_mse: 13.9427 - val_mae: 1.6656 - lr: 8.1238e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 16.1226 - mse: 16.1226 - mae: 1.6352 - val_loss: 13.8808 - val_mse: 13.8808 - val_mae: 1.6924 - lr: 8.1238e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 16.0747 - mse: 16.0747 - mae: 1.6404 - val_loss: 13.9005 - val_mse: 13.9005 - val_mae: 1.6507 - lr: 8.1238e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 16.0882 - mse: 16.0882 - mae: 1.6364 - val_loss: 13.9384 - val_mse: 13.9384 - val_mae: 1.6293 - lr: 8.1238e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 16.0955 - mse: 16.0955 - mae: 1.6399 - val_loss: 13.8405 - val_mse: 13.8405 - val_mae: 1.6536 - lr: 8.1238e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 16.0676 - mse: 16.0676 - mae: 1.6360 - val_loss: 13.9446 - val_mse: 13.9446 - val_mae: 1.6095 - lr: 8.1238e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 2s - loss: 16.1022 - mse: 16.1022 - mae: 1.6348 - val_loss: 13.8481 - val_mse: 13.8481 - val_mae: 1.6571 - lr: 8.1238e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 2s - loss: 16.0619 - mse: 16.0619 - mae: 1.6384 - val_loss: 13.9529 - val_mse: 13.9529 - val_mae: 1.6200 - lr: 8.1238e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 2s - loss: 16.0925 - mse: 16.0925 - mae: 1.6368 - val_loss: 14.1715 - val_mse: 14.1715 - val_mae: 1.5645 - lr: 8.1238e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 2s - loss: 16.0972 - mse: 16.0972 - mae: 1.6370 - val_loss: 14.0838 - val_mse: 14.0838 - val_mae: 1.5820 - lr: 8.1238e-04 - 2s/epoch - 2ms/step\n",
            "Score for fold 4: loss of 14.083759307861328\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 16.4668 - mse: 16.4668 - mae: 1.6398 - val_loss: 12.2612 - val_mse: 12.2612 - val_mae: 1.6164 - lr: 8.1238e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 16.5138 - mse: 16.5138 - mae: 1.6355 - val_loss: 12.3675 - val_mse: 12.3675 - val_mae: 1.5904 - lr: 8.1238e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 16.5205 - mse: 16.5205 - mae: 1.6323 - val_loss: 12.2428 - val_mse: 12.2428 - val_mae: 1.7011 - lr: 8.1238e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 16.5466 - mse: 16.5466 - mae: 1.6319 - val_loss: 12.3496 - val_mse: 12.3496 - val_mae: 1.6014 - lr: 8.1238e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 16.4721 - mse: 16.4721 - mae: 1.6338 - val_loss: 12.4157 - val_mse: 12.4157 - val_mae: 1.5705 - lr: 8.1238e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 16.5303 - mse: 16.5303 - mae: 1.6350 - val_loss: 12.2636 - val_mse: 12.2636 - val_mae: 1.6723 - lr: 8.1238e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 16.5041 - mse: 16.5041 - mae: 1.6378 - val_loss: 12.2890 - val_mse: 12.2890 - val_mae: 1.5989 - lr: 8.1238e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 2s - loss: 16.5061 - mse: 16.5061 - mae: 1.6327 - val_loss: 12.4471 - val_mse: 12.4471 - val_mae: 1.5942 - lr: 8.1238e-04 - 2s/epoch - 2ms/step\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-10-16 05:54:22,261]\u001b[0m Finished trial#3 resulted in value: 15.732. Current best value is 14.879999999999999 with parameters: {'activation': 'relu', 'num_hidden_layer': 3, 'i': 6, 'learning_rate': 0.00021063978488417066}.\u001b[0m\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Score for fold 5: loss of 12.447153091430664\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 3s - loss: 15.5334 - mse: 15.5334 - mae: 1.6259 - val_loss: 15.1640 - val_mse: 15.1640 - val_mae: 1.5958 - lr: 0.0015 - 3s/epoch - 3ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 15.2322 - mse: 15.2322 - mae: 1.6044 - val_loss: 15.1782 - val_mse: 15.1782 - val_mae: 1.5904 - lr: 0.0015 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 15.0810 - mse: 15.0810 - mae: 1.5954 - val_loss: 15.1140 - val_mse: 15.1140 - val_mae: 1.5636 - lr: 0.0015 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 15.0843 - mse: 15.0843 - mae: 1.5881 - val_loss: 15.3673 - val_mse: 15.3673 - val_mae: 1.5075 - lr: 0.0015 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 14.9917 - mse: 14.9917 - mae: 1.5839 - val_loss: 14.9871 - val_mse: 14.9871 - val_mae: 1.6171 - lr: 0.0015 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 14.9698 - mse: 14.9698 - mae: 1.5871 - val_loss: 15.1220 - val_mse: 15.1220 - val_mae: 1.5412 - lr: 0.0015 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 14.9551 - mse: 14.9551 - mae: 1.5898 - val_loss: 15.3515 - val_mse: 15.3515 - val_mae: 1.6257 - lr: 0.0015 - 2s/epoch - 2ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 2s - loss: 15.0166 - mse: 15.0166 - mae: 1.5940 - val_loss: 15.0691 - val_mse: 15.0691 - val_mae: 1.6739 - lr: 0.0015 - 2s/epoch - 2ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 2s - loss: 14.9813 - mse: 14.9813 - mae: 1.5917 - val_loss: 15.1146 - val_mse: 15.1146 - val_mae: 1.5813 - lr: 0.0015 - 2s/epoch - 2ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 2s - loss: 15.0002 - mse: 15.0002 - mae: 1.5935 - val_loss: 15.0482 - val_mse: 15.0482 - val_mae: 1.6056 - lr: 0.0015 - 2s/epoch - 2ms/step\n",
            "Score for fold 1: loss of 15.048195838928223\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 15.0532 - mse: 15.0532 - mae: 1.5772 - val_loss: 14.6923 - val_mse: 14.6923 - val_mae: 1.5833 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 15.0339 - mse: 15.0339 - mae: 1.5785 - val_loss: 14.2122 - val_mse: 14.2122 - val_mae: 1.6533 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 15.0800 - mse: 15.0800 - mae: 1.5840 - val_loss: 14.6847 - val_mse: 14.6847 - val_mae: 1.6225 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 14.9857 - mse: 14.9857 - mae: 1.5821 - val_loss: 14.3205 - val_mse: 14.3205 - val_mae: 1.5494 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 14.8972 - mse: 14.8972 - mae: 1.5823 - val_loss: 14.3660 - val_mse: 14.3660 - val_mae: 1.7059 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 14.9701 - mse: 14.9701 - mae: 1.5778 - val_loss: 14.6362 - val_mse: 14.6362 - val_mae: 1.5522 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 14.8076 - mse: 14.8076 - mae: 1.5786 - val_loss: 14.3551 - val_mse: 14.3551 - val_mae: 1.5282 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Score for fold 2: loss of 14.355055809020996\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 15.3247 - mse: 15.3247 - mae: 1.5983 - val_loss: 12.9789 - val_mse: 12.9789 - val_mae: 1.6405 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 14.9826 - mse: 14.9826 - mae: 1.5964 - val_loss: 12.9970 - val_mse: 12.9970 - val_mae: 1.5513 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 14.7620 - mse: 14.7620 - mae: 1.5906 - val_loss: 13.2729 - val_mse: 13.2729 - val_mae: 1.6588 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 14.9805 - mse: 14.9805 - mae: 1.5940 - val_loss: 13.1566 - val_mse: 13.1566 - val_mae: 1.4778 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 15.2315 - mse: 15.2315 - mae: 1.5940 - val_loss: 12.8663 - val_mse: 12.8663 - val_mae: 1.6032 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 15.1280 - mse: 15.1280 - mae: 1.5957 - val_loss: 12.8257 - val_mse: 12.8257 - val_mae: 1.5166 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 15.0892 - mse: 15.0892 - mae: 1.5945 - val_loss: 12.7947 - val_mse: 12.7947 - val_mae: 1.6065 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 2s - loss: 15.0092 - mse: 15.0092 - mae: 1.5939 - val_loss: 12.9077 - val_mse: 12.9077 - val_mae: 1.6189 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 2s - loss: 15.2080 - mse: 15.2080 - mae: 1.5936 - val_loss: 13.0013 - val_mse: 13.0013 - val_mae: 1.6692 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 2s - loss: 15.0928 - mse: 15.0928 - mae: 1.5864 - val_loss: 13.0909 - val_mse: 13.0909 - val_mae: 1.6734 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 2s - loss: 15.2173 - mse: 15.2173 - mae: 1.5870 - val_loss: 13.1048 - val_mse: 13.1048 - val_mae: 1.6053 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 3s - loss: 15.0827 - mse: 15.0827 - mae: 1.5790 - val_loss: 13.1623 - val_mse: 13.1623 - val_mae: 1.5013 - lr: 0.0010 - 3s/epoch - 3ms/step\n",
            "Score for fold 3: loss of 13.162269592285156\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 3s - loss: 13.8218 - mse: 13.8218 - mae: 1.5878 - val_loss: 17.8284 - val_mse: 17.8284 - val_mae: 1.5298 - lr: 0.0010 - 3s/epoch - 3ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 13.8478 - mse: 13.8478 - mae: 1.5913 - val_loss: 18.0803 - val_mse: 18.0803 - val_mae: 1.4544 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 13.8631 - mse: 13.8631 - mae: 1.5840 - val_loss: 18.0393 - val_mse: 18.0393 - val_mae: 1.5384 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 13.7603 - mse: 13.7603 - mae: 1.5854 - val_loss: 18.1744 - val_mse: 18.1744 - val_mae: 1.4567 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 13.8415 - mse: 13.8415 - mae: 1.5878 - val_loss: 18.0046 - val_mse: 18.0046 - val_mae: 1.5169 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 13.8329 - mse: 13.8329 - mae: 1.5857 - val_loss: 17.9571 - val_mse: 17.9571 - val_mae: 1.4966 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Score for fold 4: loss of 17.957088470458984\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 14.8385 - mse: 14.8385 - mae: 1.5773 - val_loss: 13.8367 - val_mse: 13.8367 - val_mae: 1.8218 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 14.8203 - mse: 14.8203 - mae: 1.5797 - val_loss: 14.3509 - val_mse: 14.3509 - val_mae: 1.6057 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 14.6714 - mse: 14.6714 - mae: 1.5719 - val_loss: 13.4570 - val_mse: 13.4570 - val_mae: 1.6702 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 14.8317 - mse: 14.8317 - mae: 1.5847 - val_loss: 13.9272 - val_mse: 13.9272 - val_mae: 1.5576 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 14.7843 - mse: 14.7843 - mae: 1.5719 - val_loss: 13.9860 - val_mse: 13.9860 - val_mae: 1.5417 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 14.6057 - mse: 14.6057 - mae: 1.5717 - val_loss: 14.0694 - val_mse: 14.0694 - val_mae: 1.8533 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 14.7333 - mse: 14.7333 - mae: 1.5730 - val_loss: 14.0498 - val_mse: 14.0498 - val_mae: 1.5918 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 2s - loss: 14.4842 - mse: 14.4842 - mae: 1.5675 - val_loss: 14.0147 - val_mse: 14.0147 - val_mae: 1.6943 - lr: 0.0010 - 2s/epoch - 2ms/step\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-10-16 05:56:11,594]\u001b[0m Finished trial#4 resulted in value: 14.908000000000001. Current best value is 14.879999999999999 with parameters: {'activation': 'relu', 'num_hidden_layer': 3, 'i': 6, 'learning_rate': 0.00021063978488417066}.\u001b[0m\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Score for fold 5: loss of 14.014738082885742\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 14.5383 - mse: 14.5383 - mae: 1.6354 - val_loss: 19.0330 - val_mse: 19.0330 - val_mae: 1.5466 - lr: 0.0070 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 14.3342 - mse: 14.3342 - mae: 1.6263 - val_loss: 18.8962 - val_mse: 18.8962 - val_mae: 1.6392 - lr: 0.0070 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 14.2444 - mse: 14.2444 - mae: 1.6234 - val_loss: 19.0391 - val_mse: 19.0391 - val_mae: 1.5940 - lr: 0.0070 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 14.2985 - mse: 14.2985 - mae: 1.6213 - val_loss: 19.0699 - val_mse: 19.0699 - val_mae: 1.5632 - lr: 0.0070 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 14.2307 - mse: 14.2307 - mae: 1.6199 - val_loss: 18.8952 - val_mse: 18.8952 - val_mae: 1.6639 - lr: 0.0070 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 14.2043 - mse: 14.2043 - mae: 1.6178 - val_loss: 18.9796 - val_mse: 18.9796 - val_mae: 1.5699 - lr: 0.0070 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 14.2443 - mse: 14.2443 - mae: 1.6215 - val_loss: 18.9372 - val_mse: 18.9372 - val_mae: 1.6280 - lr: 0.0070 - 2s/epoch - 2ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 2s - loss: 14.2239 - mse: 14.2239 - mae: 1.6195 - val_loss: 19.0316 - val_mse: 19.0316 - val_mae: 1.5406 - lr: 0.0070 - 2s/epoch - 2ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 2s - loss: 14.1936 - mse: 14.1936 - mae: 1.6149 - val_loss: 18.9199 - val_mse: 18.9199 - val_mae: 1.6103 - lr: 0.0070 - 2s/epoch - 2ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 2s - loss: 14.2222 - mse: 14.2222 - mae: 1.6184 - val_loss: 19.2681 - val_mse: 19.2681 - val_mae: 1.5700 - lr: 0.0070 - 2s/epoch - 2ms/step\n",
            "Score for fold 1: loss of 19.268091201782227\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 15.4588 - mse: 15.4588 - mae: 1.5964 - val_loss: 13.4652 - val_mse: 13.4652 - val_mae: 1.5833 - lr: 0.0014 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 15.4413 - mse: 15.4413 - mae: 1.5915 - val_loss: 13.4887 - val_mse: 13.4887 - val_mae: 1.5896 - lr: 0.0014 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 15.4328 - mse: 15.4328 - mae: 1.5961 - val_loss: 13.4829 - val_mse: 13.4829 - val_mae: 1.6033 - lr: 0.0014 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 15.4299 - mse: 15.4299 - mae: 1.5952 - val_loss: 13.5452 - val_mse: 13.5452 - val_mae: 1.5809 - lr: 0.0014 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 15.4349 - mse: 15.4349 - mae: 1.5964 - val_loss: 13.5763 - val_mse: 13.5763 - val_mae: 1.5793 - lr: 0.0014 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 15.4764 - mse: 15.4764 - mae: 1.5908 - val_loss: 13.5759 - val_mse: 13.5759 - val_mae: 1.5836 - lr: 0.0014 - 2s/epoch - 2ms/step\n",
            "Score for fold 2: loss of 13.575878143310547\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 15.6310 - mse: 15.6310 - mae: 1.5959 - val_loss: 12.9459 - val_mse: 12.9459 - val_mae: 1.5918 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 15.6096 - mse: 15.6096 - mae: 1.5950 - val_loss: 12.8611 - val_mse: 12.8611 - val_mae: 1.6164 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 15.6273 - mse: 15.6273 - mae: 1.5961 - val_loss: 12.8966 - val_mse: 12.8966 - val_mae: 1.5839 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 15.5820 - mse: 15.5820 - mae: 1.5968 - val_loss: 12.8567 - val_mse: 12.8567 - val_mae: 1.5901 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 15.5930 - mse: 15.5930 - mae: 1.5926 - val_loss: 12.8607 - val_mse: 12.8607 - val_mae: 1.6251 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 15.5719 - mse: 15.5719 - mae: 1.5965 - val_loss: 12.9309 - val_mse: 12.9309 - val_mae: 1.5739 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 15.6120 - mse: 15.6120 - mae: 1.5929 - val_loss: 12.7834 - val_mse: 12.7834 - val_mae: 1.6139 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 2s - loss: 15.5812 - mse: 15.5812 - mae: 1.5950 - val_loss: 12.7942 - val_mse: 12.7942 - val_mae: 1.5986 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 2s - loss: 15.5927 - mse: 15.5927 - mae: 1.5925 - val_loss: 13.1406 - val_mse: 13.1406 - val_mae: 1.5702 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 2s - loss: 15.6303 - mse: 15.6303 - mae: 1.5929 - val_loss: 12.8554 - val_mse: 12.8554 - val_mae: 1.5880 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 2s - loss: 15.5937 - mse: 15.5937 - mae: 1.5919 - val_loss: 12.8326 - val_mse: 12.8326 - val_mae: 1.5994 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 2s - loss: 15.5846 - mse: 15.5846 - mae: 1.5931 - val_loss: 12.9286 - val_mse: 12.9286 - val_mae: 1.5733 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Score for fold 3: loss of 12.92859172821045\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 14.8489 - mse: 14.8489 - mae: 1.5859 - val_loss: 15.7156 - val_mse: 15.7156 - val_mae: 1.5900 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 14.8415 - mse: 14.8415 - mae: 1.5892 - val_loss: 15.6354 - val_mse: 15.6354 - val_mae: 1.6388 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 14.8614 - mse: 14.8614 - mae: 1.5834 - val_loss: 15.8102 - val_mse: 15.8102 - val_mae: 1.6111 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 14.8564 - mse: 14.8564 - mae: 1.5842 - val_loss: 15.7313 - val_mse: 15.7313 - val_mae: 1.5993 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 14.8705 - mse: 14.8705 - mae: 1.5853 - val_loss: 15.6905 - val_mse: 15.6905 - val_mae: 1.6040 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 14.8271 - mse: 14.8271 - mae: 1.5857 - val_loss: 15.7530 - val_mse: 15.7530 - val_mae: 1.5929 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 14.8669 - mse: 14.8669 - mae: 1.5830 - val_loss: 15.7266 - val_mse: 15.7266 - val_mae: 1.6387 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Score for fold 4: loss of 15.7266206741333\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 15.2468 - mse: 15.2468 - mae: 1.5882 - val_loss: 14.1900 - val_mse: 14.1900 - val_mae: 1.5939 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 15.3075 - mse: 15.3075 - mae: 1.5881 - val_loss: 14.2471 - val_mse: 14.2471 - val_mae: 1.5812 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 15.2645 - mse: 15.2645 - mae: 1.5928 - val_loss: 14.2008 - val_mse: 14.2008 - val_mae: 1.6245 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 15.2723 - mse: 15.2723 - mae: 1.5918 - val_loss: 14.2042 - val_mse: 14.2042 - val_mae: 1.6124 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 15.2409 - mse: 15.2409 - mae: 1.5890 - val_loss: 14.2263 - val_mse: 14.2263 - val_mae: 1.6120 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 15.3152 - mse: 15.3152 - mae: 1.5907 - val_loss: 14.3543 - val_mse: 14.3543 - val_mae: 1.5781 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Score for fold 5: loss of 14.354305267333984\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-10-16 05:57:29,146]\u001b[0m Finished trial#5 resulted in value: 15.172. Current best value is 14.879999999999999 with parameters: {'activation': 'relu', 'num_hidden_layer': 3, 'i': 6, 'learning_rate': 0.00021063978488417066}.\u001b[0m\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 6s - loss: 15.5150 - mse: 15.5150 - mae: 1.6134 - val_loss: 14.9528 - val_mse: 14.9528 - val_mae: 1.6192 - lr: 4.4016e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 5s - loss: 15.1816 - mse: 15.1816 - mae: 1.5971 - val_loss: 15.1620 - val_mse: 15.1620 - val_mae: 1.5828 - lr: 4.4016e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 5s - loss: 15.1402 - mse: 15.1402 - mae: 1.5937 - val_loss: 14.7466 - val_mse: 14.7466 - val_mae: 1.6029 - lr: 4.4016e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 5s - loss: 14.9817 - mse: 14.9817 - mae: 1.5876 - val_loss: 14.9192 - val_mse: 14.9192 - val_mae: 1.5713 - lr: 4.4016e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 5s - loss: 14.9931 - mse: 14.9931 - mae: 1.5808 - val_loss: 15.0287 - val_mse: 15.0287 - val_mae: 1.5708 - lr: 4.4016e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 5s - loss: 14.9330 - mse: 14.9330 - mae: 1.5797 - val_loss: 14.7466 - val_mse: 14.7466 - val_mae: 1.6046 - lr: 4.4016e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 5s - loss: 14.9244 - mse: 14.9244 - mae: 1.5794 - val_loss: 14.6900 - val_mse: 14.6900 - val_mae: 1.6203 - lr: 4.4016e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 5s - loss: 14.8805 - mse: 14.8805 - mae: 1.5809 - val_loss: 14.6532 - val_mse: 14.6532 - val_mae: 1.5671 - lr: 4.4016e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 5s - loss: 14.8650 - mse: 14.8650 - mae: 1.5817 - val_loss: 14.6791 - val_mse: 14.6791 - val_mae: 1.6073 - lr: 4.4016e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 5s - loss: 14.6568 - mse: 14.6568 - mae: 1.5791 - val_loss: 14.6451 - val_mse: 14.6451 - val_mae: 1.6067 - lr: 4.4016e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 5s - loss: 14.7520 - mse: 14.7520 - mae: 1.5788 - val_loss: 14.7189 - val_mse: 14.7189 - val_mae: 1.6842 - lr: 4.4016e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 5s - loss: 14.6243 - mse: 14.6243 - mae: 1.5856 - val_loss: 14.6690 - val_mse: 14.6690 - val_mae: 1.6659 - lr: 4.4016e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 13/100\n",
            "1000/1000 - 5s - loss: 14.7219 - mse: 14.7219 - mae: 1.5928 - val_loss: 15.1385 - val_mse: 15.1385 - val_mae: 1.7855 - lr: 4.4016e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 14/100\n",
            "1000/1000 - 5s - loss: 14.6291 - mse: 14.6291 - mae: 1.5850 - val_loss: 14.7029 - val_mse: 14.7029 - val_mae: 1.5494 - lr: 4.4016e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 15/100\n",
            "1000/1000 - 5s - loss: 14.6584 - mse: 14.6584 - mae: 1.5835 - val_loss: 14.6706 - val_mse: 14.6706 - val_mae: 1.5293 - lr: 4.4016e-04 - 5s/epoch - 5ms/step\n",
            "Score for fold 1: loss of 14.67058277130127\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 5s - loss: 15.5509 - mse: 15.5509 - mae: 1.5986 - val_loss: 11.1640 - val_mse: 11.1640 - val_mae: 1.5963 - lr: 4.4016e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 5s - loss: 15.3663 - mse: 15.3663 - mae: 1.5856 - val_loss: 11.4551 - val_mse: 11.4551 - val_mae: 1.5045 - lr: 4.4016e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 5s - loss: 15.4754 - mse: 15.4754 - mae: 1.5862 - val_loss: 11.3008 - val_mse: 11.3008 - val_mae: 1.4601 - lr: 4.4016e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 5s - loss: 15.3401 - mse: 15.3401 - mae: 1.5847 - val_loss: 11.3373 - val_mse: 11.3373 - val_mae: 1.6027 - lr: 4.4016e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 5s - loss: 15.3842 - mse: 15.3842 - mae: 1.5996 - val_loss: 11.8003 - val_mse: 11.8003 - val_mae: 1.4922 - lr: 4.4016e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 5s - loss: 15.2824 - mse: 15.2824 - mae: 1.5944 - val_loss: 11.3317 - val_mse: 11.3317 - val_mae: 1.5355 - lr: 4.4016e-04 - 5s/epoch - 5ms/step\n",
            "Score for fold 2: loss of 11.33173942565918\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 5s - loss: 14.0163 - mse: 14.0163 - mae: 1.5912 - val_loss: 16.4731 - val_mse: 16.4731 - val_mae: 1.5635 - lr: 4.4016e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 5s - loss: 13.7965 - mse: 13.7965 - mae: 1.6010 - val_loss: 16.7855 - val_mse: 16.7855 - val_mae: 1.7882 - lr: 4.4016e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 5s - loss: 13.7434 - mse: 13.7434 - mae: 1.6056 - val_loss: 17.0646 - val_mse: 17.0646 - val_mae: 1.7868 - lr: 4.4016e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 5s - loss: 13.8555 - mse: 13.8555 - mae: 1.5886 - val_loss: 17.1495 - val_mse: 17.1495 - val_mae: 1.5412 - lr: 4.4016e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 5s - loss: 13.6883 - mse: 13.6883 - mae: 1.5949 - val_loss: 16.9032 - val_mse: 16.9032 - val_mae: 1.5856 - lr: 4.4016e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 5s - loss: 13.6364 - mse: 13.6364 - mae: 1.5844 - val_loss: 17.2307 - val_mse: 17.2307 - val_mae: 1.9273 - lr: 4.4016e-04 - 5s/epoch - 5ms/step\n",
            "Score for fold 3: loss of 17.230670928955078\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 5s - loss: 15.3043 - mse: 15.3043 - mae: 1.6114 - val_loss: 10.3646 - val_mse: 10.3646 - val_mae: 1.5995 - lr: 4.4016e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 5s - loss: 15.1374 - mse: 15.1374 - mae: 1.6026 - val_loss: 11.2549 - val_mse: 11.2549 - val_mae: 1.5050 - lr: 4.4016e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 5s - loss: 15.1301 - mse: 15.1301 - mae: 1.5982 - val_loss: 10.9552 - val_mse: 10.9552 - val_mae: 1.8261 - lr: 4.4016e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 5s - loss: 15.0841 - mse: 15.0841 - mae: 1.5997 - val_loss: 11.0083 - val_mse: 11.0083 - val_mae: 1.6174 - lr: 4.4016e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 5s - loss: 14.9926 - mse: 14.9926 - mae: 1.6021 - val_loss: 10.9480 - val_mse: 10.9480 - val_mae: 1.5860 - lr: 4.4016e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 5s - loss: 15.1211 - mse: 15.1211 - mae: 1.6282 - val_loss: 11.2580 - val_mse: 11.2580 - val_mae: 1.5961 - lr: 4.4016e-04 - 5s/epoch - 5ms/step\n",
            "Score for fold 4: loss of 11.258010864257812\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 5s - loss: 13.2650 - mse: 13.2650 - mae: 1.6294 - val_loss: 19.1051 - val_mse: 19.1051 - val_mae: 1.4438 - lr: 4.4016e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 5s - loss: 13.0074 - mse: 13.0074 - mae: 1.6273 - val_loss: 18.6583 - val_mse: 18.6583 - val_mae: 1.5536 - lr: 4.4016e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 5s - loss: 13.0369 - mse: 13.0369 - mae: 1.6242 - val_loss: 18.7479 - val_mse: 18.7479 - val_mae: 1.5370 - lr: 4.4016e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 5s - loss: 12.9971 - mse: 12.9971 - mae: 1.6225 - val_loss: 19.7588 - val_mse: 19.7588 - val_mae: 1.6108 - lr: 4.4016e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 6s - loss: 13.0702 - mse: 13.0702 - mae: 1.6600 - val_loss: 20.1261 - val_mse: 20.1261 - val_mae: 2.1240 - lr: 4.4016e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 6s - loss: 13.0033 - mse: 13.0033 - mae: 1.6589 - val_loss: 20.5690 - val_mse: 20.5690 - val_mae: 2.3446 - lr: 4.4016e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 5s - loss: 12.9758 - mse: 12.9758 - mae: 1.6385 - val_loss: 18.8566 - val_mse: 18.8566 - val_mae: 1.6117 - lr: 4.4016e-04 - 5s/epoch - 5ms/step\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-10-16 06:01:04,647]\u001b[0m Finished trial#6 resulted in value: 14.669999999999998. Current best value is 14.669999999999998 with parameters: {'activation': 'relu', 'num_hidden_layer': 4, 'i': 9, 'learning_rate': 0.00044016359204388243}.\u001b[0m\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Score for fold 5: loss of 18.856552124023438\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 4s - loss: 16.7051 - mse: 16.7051 - mae: 1.7118 - val_loss: 14.2885 - val_mse: 14.2885 - val_mae: 1.6023 - lr: 1.3037e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 4s - loss: 16.1217 - mse: 16.1217 - mae: 1.6421 - val_loss: 14.0411 - val_mse: 14.0411 - val_mae: 1.5812 - lr: 1.3037e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 4s - loss: 16.1090 - mse: 16.1090 - mae: 1.6378 - val_loss: 14.3202 - val_mse: 14.3202 - val_mae: 1.5740 - lr: 1.3037e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 4s - loss: 16.0809 - mse: 16.0809 - mae: 1.6340 - val_loss: 14.0870 - val_mse: 14.0870 - val_mae: 1.6268 - lr: 1.3037e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 4s - loss: 16.1330 - mse: 16.1330 - mae: 1.6396 - val_loss: 13.9898 - val_mse: 13.9898 - val_mae: 1.5863 - lr: 1.3037e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 4s - loss: 16.1009 - mse: 16.1009 - mae: 1.6404 - val_loss: 14.2754 - val_mse: 14.2754 - val_mae: 1.5486 - lr: 1.3037e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 4s - loss: 16.0978 - mse: 16.0978 - mae: 1.6446 - val_loss: 14.1715 - val_mse: 14.1715 - val_mae: 1.5685 - lr: 1.3037e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 4s - loss: 16.0923 - mse: 16.0923 - mae: 1.6450 - val_loss: 13.9580 - val_mse: 13.9580 - val_mae: 1.6661 - lr: 1.3037e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 4s - loss: 16.1090 - mse: 16.1090 - mae: 1.6405 - val_loss: 14.2229 - val_mse: 14.2229 - val_mae: 1.5563 - lr: 1.3037e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 4s - loss: 16.1443 - mse: 16.1443 - mae: 1.6400 - val_loss: 13.9749 - val_mse: 13.9749 - val_mae: 1.6197 - lr: 1.3037e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 4s - loss: 16.1216 - mse: 16.1216 - mae: 1.6418 - val_loss: 14.1259 - val_mse: 14.1259 - val_mae: 1.5932 - lr: 1.3037e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 4s - loss: 16.0923 - mse: 16.0923 - mae: 1.6425 - val_loss: 14.1284 - val_mse: 14.1284 - val_mae: 1.5695 - lr: 1.3037e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 13/100\n",
            "1000/1000 - 4s - loss: 16.0911 - mse: 16.0911 - mae: 1.6365 - val_loss: 14.0308 - val_mse: 14.0308 - val_mae: 1.6022 - lr: 1.3037e-04 - 4s/epoch - 4ms/step\n",
            "Score for fold 1: loss of 14.03083610534668\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 4s - loss: 16.4345 - mse: 16.4345 - mae: 1.6309 - val_loss: 12.6537 - val_mse: 12.6537 - val_mae: 1.6205 - lr: 1.3037e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 4s - loss: 16.4549 - mse: 16.4549 - mae: 1.6285 - val_loss: 12.5612 - val_mse: 12.5612 - val_mae: 1.6634 - lr: 1.3037e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 4s - loss: 16.4426 - mse: 16.4426 - mae: 1.6341 - val_loss: 12.5520 - val_mse: 12.5520 - val_mae: 1.6390 - lr: 1.3037e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 4s - loss: 16.4385 - mse: 16.4385 - mae: 1.6330 - val_loss: 12.5452 - val_mse: 12.5452 - val_mae: 1.6925 - lr: 1.3037e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 4s - loss: 16.4330 - mse: 16.4330 - mae: 1.6321 - val_loss: 12.5516 - val_mse: 12.5516 - val_mae: 1.6237 - lr: 1.3037e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 4s - loss: 16.4514 - mse: 16.4514 - mae: 1.6288 - val_loss: 12.5562 - val_mse: 12.5562 - val_mae: 1.6491 - lr: 1.3037e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 4s - loss: 16.4226 - mse: 16.4226 - mae: 1.6325 - val_loss: 12.5740 - val_mse: 12.5740 - val_mae: 1.6548 - lr: 1.3037e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 4s - loss: 16.4207 - mse: 16.4207 - mae: 1.6313 - val_loss: 12.5778 - val_mse: 12.5778 - val_mae: 1.6401 - lr: 1.3037e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 4s - loss: 16.4262 - mse: 16.4262 - mae: 1.6300 - val_loss: 12.6100 - val_mse: 12.6100 - val_mae: 1.6039 - lr: 1.3037e-04 - 4s/epoch - 4ms/step\n",
            "Score for fold 2: loss of 12.609955787658691\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 4s - loss: 13.9796 - mse: 13.9796 - mae: 1.6169 - val_loss: 22.1686 - val_mse: 22.1686 - val_mae: 1.6859 - lr: 1.3037e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 4s - loss: 13.9817 - mse: 13.9817 - mae: 1.6167 - val_loss: 22.2247 - val_mse: 22.2247 - val_mae: 1.6590 - lr: 1.3037e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 4s - loss: 14.0184 - mse: 14.0184 - mae: 1.6167 - val_loss: 22.2227 - val_mse: 22.2227 - val_mae: 1.6639 - lr: 1.3037e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 4s - loss: 14.0054 - mse: 14.0054 - mae: 1.6190 - val_loss: 22.2031 - val_mse: 22.2031 - val_mae: 1.6689 - lr: 1.3037e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 4s - loss: 13.9846 - mse: 13.9846 - mae: 1.6197 - val_loss: 22.2443 - val_mse: 22.2443 - val_mae: 1.6452 - lr: 1.3037e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 4s - loss: 13.9651 - mse: 13.9651 - mae: 1.6217 - val_loss: 22.2024 - val_mse: 22.2024 - val_mae: 1.7369 - lr: 1.3037e-04 - 4s/epoch - 4ms/step\n",
            "Score for fold 3: loss of 22.20234489440918\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 4s - loss: 15.6464 - mse: 15.6464 - mae: 1.6352 - val_loss: 15.4739 - val_mse: 15.4739 - val_mae: 1.6218 - lr: 1.3037e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 4s - loss: 15.6562 - mse: 15.6562 - mae: 1.6350 - val_loss: 15.5016 - val_mse: 15.5016 - val_mae: 1.6440 - lr: 1.3037e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 4s - loss: 15.6595 - mse: 15.6595 - mae: 1.6390 - val_loss: 15.5264 - val_mse: 15.5264 - val_mae: 1.6190 - lr: 1.3037e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 4s - loss: 15.6660 - mse: 15.6660 - mae: 1.6351 - val_loss: 15.5212 - val_mse: 15.5212 - val_mae: 1.6405 - lr: 1.3037e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 4s - loss: 15.6504 - mse: 15.6504 - mae: 1.6313 - val_loss: 15.4876 - val_mse: 15.4876 - val_mae: 1.6489 - lr: 1.3037e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 4s - loss: 15.6617 - mse: 15.6617 - mae: 1.6343 - val_loss: 15.6117 - val_mse: 15.6117 - val_mae: 1.5773 - lr: 1.3037e-04 - 4s/epoch - 4ms/step\n",
            "Score for fold 4: loss of 15.611651420593262\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 4s - loss: 16.0700 - mse: 16.0700 - mae: 1.6368 - val_loss: 13.8733 - val_mse: 13.8733 - val_mae: 1.6154 - lr: 1.3037e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 4s - loss: 16.0665 - mse: 16.0665 - mae: 1.6317 - val_loss: 13.8491 - val_mse: 13.8491 - val_mae: 1.6189 - lr: 1.3037e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 4s - loss: 16.0625 - mse: 16.0625 - mae: 1.6262 - val_loss: 13.8616 - val_mse: 13.8616 - val_mae: 1.6385 - lr: 1.3037e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 4s - loss: 16.0607 - mse: 16.0607 - mae: 1.6348 - val_loss: 13.9114 - val_mse: 13.9114 - val_mae: 1.5875 - lr: 1.3037e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 4s - loss: 16.0555 - mse: 16.0555 - mae: 1.6340 - val_loss: 13.8782 - val_mse: 13.8782 - val_mae: 1.6091 - lr: 1.3037e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 4s - loss: 16.0383 - mse: 16.0383 - mae: 1.6327 - val_loss: 13.8494 - val_mse: 13.8494 - val_mae: 1.6219 - lr: 1.3037e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 4s - loss: 16.0472 - mse: 16.0472 - mae: 1.6299 - val_loss: 13.8877 - val_mse: 13.8877 - val_mae: 1.5797 - lr: 1.3037e-04 - 4s/epoch - 4ms/step\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-10-16 06:03:45,122]\u001b[0m Finished trial#7 resulted in value: 15.668000000000001. Current best value is 14.669999999999998 with parameters: {'activation': 'relu', 'num_hidden_layer': 4, 'i': 9, 'learning_rate': 0.00044016359204388243}.\u001b[0m\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Score for fold 5: loss of 13.887741088867188\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 3s - loss: 15.9535 - mse: 15.9535 - mae: 1.6466 - val_loss: 14.9833 - val_mse: 14.9833 - val_mae: 1.6974 - lr: 0.0010 - 3s/epoch - 3ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 15.5818 - mse: 15.5818 - mae: 1.6192 - val_loss: 14.9539 - val_mse: 14.9539 - val_mae: 1.6832 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 15.5801 - mse: 15.5801 - mae: 1.6130 - val_loss: 15.0372 - val_mse: 15.0372 - val_mae: 1.5953 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 15.5678 - mse: 15.5678 - mae: 1.6107 - val_loss: 14.9682 - val_mse: 14.9682 - val_mae: 1.5957 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 15.4956 - mse: 15.4956 - mae: 1.6049 - val_loss: 14.8662 - val_mse: 14.8662 - val_mae: 1.6841 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 15.4947 - mse: 15.4947 - mae: 1.6041 - val_loss: 14.9559 - val_mse: 14.9559 - val_mae: 1.6338 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 15.4925 - mse: 15.4925 - mae: 1.6029 - val_loss: 14.9538 - val_mse: 14.9538 - val_mae: 1.6095 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 2s - loss: 15.4644 - mse: 15.4644 - mae: 1.6013 - val_loss: 15.0216 - val_mse: 15.0216 - val_mae: 1.5814 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 2s - loss: 15.4357 - mse: 15.4357 - mae: 1.5990 - val_loss: 14.9534 - val_mse: 14.9534 - val_mae: 1.5878 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 2s - loss: 15.4368 - mse: 15.4368 - mae: 1.5969 - val_loss: 14.9914 - val_mse: 14.9914 - val_mae: 1.5848 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Score for fold 1: loss of 14.991398811340332\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 15.9169 - mse: 15.9169 - mae: 1.5941 - val_loss: 12.9046 - val_mse: 12.9046 - val_mae: 1.6036 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 15.9003 - mse: 15.9003 - mae: 1.5976 - val_loss: 12.8006 - val_mse: 12.8006 - val_mae: 1.6552 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 15.8608 - mse: 15.8608 - mae: 1.5961 - val_loss: 12.8363 - val_mse: 12.8363 - val_mae: 1.6052 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 15.8709 - mse: 15.8709 - mae: 1.5943 - val_loss: 12.7376 - val_mse: 12.7376 - val_mae: 1.6439 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 15.8553 - mse: 15.8553 - mae: 1.5908 - val_loss: 12.7651 - val_mse: 12.7651 - val_mae: 1.6006 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 15.8484 - mse: 15.8484 - mae: 1.5887 - val_loss: 12.8114 - val_mse: 12.8114 - val_mae: 1.6064 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 15.8503 - mse: 15.8503 - mae: 1.5870 - val_loss: 12.8836 - val_mse: 12.8836 - val_mae: 1.5760 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 2s - loss: 15.7878 - mse: 15.7878 - mae: 1.5942 - val_loss: 12.9680 - val_mse: 12.9680 - val_mae: 1.5427 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 2s - loss: 15.8277 - mse: 15.8277 - mae: 1.5878 - val_loss: 12.8397 - val_mse: 12.8397 - val_mae: 1.5613 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Score for fold 2: loss of 12.839730262756348\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 15.5191 - mse: 15.5191 - mae: 1.5913 - val_loss: 13.9848 - val_mse: 13.9848 - val_mae: 1.5801 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 15.5195 - mse: 15.5195 - mae: 1.5892 - val_loss: 13.9605 - val_mse: 13.9605 - val_mae: 1.5930 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 15.5027 - mse: 15.5027 - mae: 1.5929 - val_loss: 13.9640 - val_mse: 13.9640 - val_mae: 1.5484 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 15.5031 - mse: 15.5031 - mae: 1.5891 - val_loss: 14.0055 - val_mse: 14.0055 - val_mae: 1.5843 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 15.5142 - mse: 15.5142 - mae: 1.5877 - val_loss: 13.9292 - val_mse: 13.9292 - val_mae: 1.6077 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 15.4926 - mse: 15.4926 - mae: 1.5881 - val_loss: 13.9823 - val_mse: 13.9823 - val_mae: 1.5585 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 15.4804 - mse: 15.4804 - mae: 1.5890 - val_loss: 13.9394 - val_mse: 13.9394 - val_mae: 1.5775 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 2s - loss: 15.4700 - mse: 15.4700 - mae: 1.5886 - val_loss: 13.8999 - val_mse: 13.8999 - val_mae: 1.6098 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 2s - loss: 15.4829 - mse: 15.4829 - mae: 1.5885 - val_loss: 13.8838 - val_mse: 13.8838 - val_mae: 1.5753 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 2s - loss: 15.4417 - mse: 15.4417 - mae: 1.5868 - val_loss: 13.8814 - val_mse: 13.8814 - val_mae: 1.5822 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 2s - loss: 15.4310 - mse: 15.4310 - mae: 1.5831 - val_loss: 13.9095 - val_mse: 13.9095 - val_mae: 1.5826 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 2s - loss: 15.4540 - mse: 15.4540 - mae: 1.5890 - val_loss: 13.8745 - val_mse: 13.8745 - val_mae: 1.5889 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 13/100\n",
            "1000/1000 - 2s - loss: 15.4397 - mse: 15.4397 - mae: 1.5862 - val_loss: 13.9193 - val_mse: 13.9193 - val_mae: 1.6121 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 14/100\n",
            "1000/1000 - 2s - loss: 15.4492 - mse: 15.4492 - mae: 1.5836 - val_loss: 13.9260 - val_mse: 13.9260 - val_mae: 1.5886 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 15/100\n",
            "1000/1000 - 2s - loss: 15.4410 - mse: 15.4410 - mae: 1.5827 - val_loss: 13.9000 - val_mse: 13.9000 - val_mae: 1.5862 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 16/100\n",
            "1000/1000 - 2s - loss: 15.4146 - mse: 15.4146 - mae: 1.5847 - val_loss: 13.9230 - val_mse: 13.9230 - val_mae: 1.5802 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 17/100\n",
            "1000/1000 - 2s - loss: 15.4271 - mse: 15.4271 - mae: 1.5833 - val_loss: 13.8404 - val_mse: 13.8404 - val_mae: 1.5603 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 18/100\n",
            "1000/1000 - 2s - loss: 15.4053 - mse: 15.4053 - mae: 1.5824 - val_loss: 14.0482 - val_mse: 14.0482 - val_mae: 1.5921 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 19/100\n",
            "1000/1000 - 2s - loss: 15.4201 - mse: 15.4201 - mae: 1.5814 - val_loss: 13.7958 - val_mse: 13.7958 - val_mae: 1.5932 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 20/100\n",
            "1000/1000 - 2s - loss: 15.3991 - mse: 15.3991 - mae: 1.5815 - val_loss: 13.9042 - val_mse: 13.9042 - val_mae: 1.5422 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 21/100\n",
            "1000/1000 - 2s - loss: 15.3908 - mse: 15.3908 - mae: 1.5814 - val_loss: 13.8685 - val_mse: 13.8685 - val_mae: 1.5659 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 22/100\n",
            "1000/1000 - 2s - loss: 15.3749 - mse: 15.3749 - mae: 1.5805 - val_loss: 13.8356 - val_mse: 13.8356 - val_mae: 1.6062 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 23/100\n",
            "1000/1000 - 2s - loss: 15.3678 - mse: 15.3678 - mae: 1.5800 - val_loss: 13.8554 - val_mse: 13.8554 - val_mae: 1.5872 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 24/100\n",
            "1000/1000 - 2s - loss: 15.3604 - mse: 15.3604 - mae: 1.5831 - val_loss: 13.7512 - val_mse: 13.7512 - val_mae: 1.5922 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 25/100\n",
            "1000/1000 - 2s - loss: 15.3446 - mse: 15.3446 - mae: 1.5788 - val_loss: 13.9816 - val_mse: 13.9816 - val_mae: 1.5356 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 26/100\n",
            "1000/1000 - 2s - loss: 15.3407 - mse: 15.3407 - mae: 1.5785 - val_loss: 13.8064 - val_mse: 13.8064 - val_mae: 1.5591 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 27/100\n",
            "1000/1000 - 2s - loss: 15.3236 - mse: 15.3236 - mae: 1.5802 - val_loss: 13.7290 - val_mse: 13.7290 - val_mae: 1.5865 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 28/100\n",
            "1000/1000 - 2s - loss: 15.3188 - mse: 15.3188 - mae: 1.5813 - val_loss: 13.7746 - val_mse: 13.7746 - val_mae: 1.5971 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 29/100\n",
            "1000/1000 - 2s - loss: 15.3154 - mse: 15.3154 - mae: 1.5796 - val_loss: 13.7652 - val_mse: 13.7652 - val_mae: 1.5846 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 30/100\n",
            "1000/1000 - 2s - loss: 15.2650 - mse: 15.2650 - mae: 1.5798 - val_loss: 13.8088 - val_mse: 13.8088 - val_mae: 1.5705 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 31/100\n",
            "1000/1000 - 2s - loss: 15.2945 - mse: 15.2945 - mae: 1.5824 - val_loss: 13.7119 - val_mse: 13.7119 - val_mae: 1.5797 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 32/100\n",
            "1000/1000 - 2s - loss: 15.2947 - mse: 15.2947 - mae: 1.5821 - val_loss: 13.7588 - val_mse: 13.7588 - val_mae: 1.5941 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 33/100\n",
            "1000/1000 - 2s - loss: 15.2690 - mse: 15.2690 - mae: 1.5789 - val_loss: 13.7403 - val_mse: 13.7403 - val_mae: 1.6183 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 34/100\n",
            "1000/1000 - 2s - loss: 15.2769 - mse: 15.2769 - mae: 1.5784 - val_loss: 13.7523 - val_mse: 13.7523 - val_mae: 1.6022 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 35/100\n",
            "1000/1000 - 2s - loss: 15.2714 - mse: 15.2714 - mae: 1.5806 - val_loss: 13.8182 - val_mse: 13.8182 - val_mae: 1.6044 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 36/100\n",
            "1000/1000 - 2s - loss: 15.2713 - mse: 15.2713 - mae: 1.5842 - val_loss: 13.7634 - val_mse: 13.7634 - val_mae: 1.6305 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Score for fold 3: loss of 13.763392448425293\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 15.9078 - mse: 15.9078 - mae: 1.5803 - val_loss: 11.2451 - val_mse: 11.2451 - val_mae: 1.5450 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 15.9069 - mse: 15.9069 - mae: 1.5799 - val_loss: 11.1912 - val_mse: 11.1912 - val_mae: 1.5600 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 15.8923 - mse: 15.8923 - mae: 1.5786 - val_loss: 11.3081 - val_mse: 11.3081 - val_mae: 1.5410 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 15.8963 - mse: 15.8963 - mae: 1.5788 - val_loss: 11.2882 - val_mse: 11.2882 - val_mae: 1.5345 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 15.8798 - mse: 15.8798 - mae: 1.5835 - val_loss: 11.2405 - val_mse: 11.2405 - val_mae: 1.5500 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 15.8817 - mse: 15.8817 - mae: 1.5816 - val_loss: 11.2239 - val_mse: 11.2239 - val_mae: 1.5548 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 15.8714 - mse: 15.8714 - mae: 1.5800 - val_loss: 11.3174 - val_mse: 11.3174 - val_mae: 1.5329 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Score for fold 4: loss of 11.31741714477539\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 12.9700 - mse: 12.9700 - mae: 1.5762 - val_loss: 22.7949 - val_mse: 22.7949 - val_mae: 1.6015 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 12.9488 - mse: 12.9488 - mae: 1.5766 - val_loss: 22.7717 - val_mse: 22.7717 - val_mae: 1.5771 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 12.9320 - mse: 12.9320 - mae: 1.5751 - val_loss: 22.8173 - val_mse: 22.8173 - val_mae: 1.5937 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 12.9218 - mse: 12.9218 - mae: 1.5795 - val_loss: 22.7851 - val_mse: 22.7851 - val_mae: 1.5778 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 12.9175 - mse: 12.9175 - mae: 1.5782 - val_loss: 22.7607 - val_mse: 22.7607 - val_mae: 1.5853 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 12.9124 - mse: 12.9124 - mae: 1.5747 - val_loss: 22.8129 - val_mse: 22.8129 - val_mae: 1.5936 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 12.9232 - mse: 12.9232 - mae: 1.5732 - val_loss: 22.8242 - val_mse: 22.8242 - val_mae: 1.6084 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 2s - loss: 12.9368 - mse: 12.9368 - mae: 1.5753 - val_loss: 22.7721 - val_mse: 22.7721 - val_mae: 1.6078 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 2s - loss: 12.9077 - mse: 12.9077 - mae: 1.5730 - val_loss: 22.7902 - val_mse: 22.7902 - val_mae: 1.6156 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 2s - loss: 12.8991 - mse: 12.8991 - mae: 1.5802 - val_loss: 22.8146 - val_mse: 22.8146 - val_mae: 1.5612 - lr: 0.0010 - 2s/epoch - 2ms/step\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-10-16 06:06:23,147]\u001b[0m Finished trial#8 resulted in value: 15.144. Current best value is 14.669999999999998 with parameters: {'activation': 'relu', 'num_hidden_layer': 4, 'i': 9, 'learning_rate': 0.00044016359204388243}.\u001b[0m\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Score for fold 5: loss of 22.814586639404297\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 3s - loss: 15.8596 - mse: 15.8596 - mae: 1.6490 - val_loss: 14.9266 - val_mse: 14.9266 - val_mae: 1.6542 - lr: 0.0029 - 3s/epoch - 3ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 15.5720 - mse: 15.5720 - mae: 1.6185 - val_loss: 14.8273 - val_mse: 14.8273 - val_mae: 1.6608 - lr: 0.0029 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 15.4882 - mse: 15.4882 - mae: 1.6138 - val_loss: 14.6942 - val_mse: 14.6942 - val_mae: 1.6712 - lr: 0.0029 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 15.4499 - mse: 15.4499 - mae: 1.6091 - val_loss: 14.7637 - val_mse: 14.7637 - val_mae: 1.5803 - lr: 0.0029 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 15.4653 - mse: 15.4653 - mae: 1.6054 - val_loss: 14.7019 - val_mse: 14.7019 - val_mae: 1.5887 - lr: 0.0029 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 15.3702 - mse: 15.3702 - mae: 1.6013 - val_loss: 14.8002 - val_mse: 14.8002 - val_mae: 1.5780 - lr: 0.0029 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 15.3546 - mse: 15.3546 - mae: 1.5995 - val_loss: 14.6598 - val_mse: 14.6598 - val_mae: 1.6364 - lr: 0.0029 - 2s/epoch - 2ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 2s - loss: 15.3357 - mse: 15.3357 - mae: 1.5981 - val_loss: 14.6739 - val_mse: 14.6739 - val_mae: 1.6084 - lr: 0.0029 - 2s/epoch - 2ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 2s - loss: 15.3056 - mse: 15.3056 - mae: 1.5941 - val_loss: 14.7320 - val_mse: 14.7320 - val_mae: 1.6487 - lr: 0.0029 - 2s/epoch - 2ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 2s - loss: 15.3104 - mse: 15.3104 - mae: 1.5919 - val_loss: 14.8416 - val_mse: 14.8416 - val_mae: 1.6074 - lr: 0.0029 - 2s/epoch - 2ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 2s - loss: 15.2741 - mse: 15.2741 - mae: 1.5922 - val_loss: 14.5700 - val_mse: 14.5700 - val_mae: 1.6172 - lr: 0.0029 - 2s/epoch - 2ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 2s - loss: 15.2359 - mse: 15.2359 - mae: 1.5923 - val_loss: 14.5679 - val_mse: 14.5679 - val_mae: 1.6132 - lr: 0.0029 - 2s/epoch - 2ms/step\n",
            "Epoch 13/100\n",
            "1000/1000 - 2s - loss: 15.2329 - mse: 15.2329 - mae: 1.5932 - val_loss: 14.5787 - val_mse: 14.5787 - val_mae: 1.5615 - lr: 0.0029 - 2s/epoch - 2ms/step\n",
            "Epoch 14/100\n",
            "1000/1000 - 2s - loss: 15.2150 - mse: 15.2150 - mae: 1.5909 - val_loss: 14.6003 - val_mse: 14.6003 - val_mae: 1.6184 - lr: 0.0029 - 2s/epoch - 2ms/step\n",
            "Epoch 15/100\n",
            "1000/1000 - 2s - loss: 15.2009 - mse: 15.2009 - mae: 1.5926 - val_loss: 14.6254 - val_mse: 14.6254 - val_mae: 1.5869 - lr: 0.0029 - 2s/epoch - 2ms/step\n",
            "Epoch 16/100\n",
            "1000/1000 - 2s - loss: 15.2142 - mse: 15.2142 - mae: 1.5906 - val_loss: 14.5255 - val_mse: 14.5255 - val_mae: 1.5878 - lr: 0.0029 - 2s/epoch - 2ms/step\n",
            "Epoch 17/100\n",
            "1000/1000 - 2s - loss: 15.2045 - mse: 15.2045 - mae: 1.5915 - val_loss: 14.4531 - val_mse: 14.4531 - val_mae: 1.6545 - lr: 0.0029 - 2s/epoch - 2ms/step\n",
            "Epoch 18/100\n",
            "1000/1000 - 2s - loss: 15.1670 - mse: 15.1670 - mae: 1.5889 - val_loss: 14.5145 - val_mse: 14.5145 - val_mae: 1.6077 - lr: 0.0029 - 2s/epoch - 2ms/step\n",
            "Epoch 19/100\n",
            "1000/1000 - 2s - loss: 15.1934 - mse: 15.1934 - mae: 1.5894 - val_loss: 14.5744 - val_mse: 14.5744 - val_mae: 1.6692 - lr: 0.0029 - 2s/epoch - 2ms/step\n",
            "Epoch 20/100\n",
            "1000/1000 - 2s - loss: 15.1606 - mse: 15.1606 - mae: 1.5911 - val_loss: 14.5308 - val_mse: 14.5308 - val_mae: 1.5938 - lr: 0.0029 - 2s/epoch - 2ms/step\n",
            "Epoch 21/100\n",
            "1000/1000 - 2s - loss: 15.1571 - mse: 15.1571 - mae: 1.5906 - val_loss: 14.5004 - val_mse: 14.5004 - val_mae: 1.5805 - lr: 0.0029 - 2s/epoch - 2ms/step\n",
            "Epoch 22/100\n",
            "1000/1000 - 2s - loss: 15.1670 - mse: 15.1670 - mae: 1.5943 - val_loss: 14.4702 - val_mse: 14.4702 - val_mae: 1.5952 - lr: 0.0029 - 2s/epoch - 2ms/step\n",
            "Score for fold 1: loss of 14.47021770477295\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 14.6714 - mse: 14.6714 - mae: 1.5787 - val_loss: 16.0111 - val_mse: 16.0111 - val_mae: 1.6051 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 14.6669 - mse: 14.6669 - mae: 1.5747 - val_loss: 16.0345 - val_mse: 16.0345 - val_mae: 1.5927 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 14.6489 - mse: 14.6489 - mae: 1.5758 - val_loss: 16.0497 - val_mse: 16.0497 - val_mae: 1.5817 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 14.6359 - mse: 14.6359 - mae: 1.5726 - val_loss: 16.0278 - val_mse: 16.0278 - val_mae: 1.5972 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 14.6541 - mse: 14.6541 - mae: 1.5737 - val_loss: 16.0473 - val_mse: 16.0473 - val_mae: 1.6367 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 14.6280 - mse: 14.6280 - mae: 1.5751 - val_loss: 16.0816 - val_mse: 16.0816 - val_mae: 1.5805 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Score for fold 2: loss of 16.081575393676758\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 15.4666 - mse: 15.4666 - mae: 1.5758 - val_loss: 12.7847 - val_mse: 12.7847 - val_mae: 1.5718 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 15.4807 - mse: 15.4807 - mae: 1.5758 - val_loss: 12.8121 - val_mse: 12.8121 - val_mae: 1.5928 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 15.4581 - mse: 15.4581 - mae: 1.5718 - val_loss: 12.7340 - val_mse: 12.7340 - val_mae: 1.5713 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 15.4591 - mse: 15.4591 - mae: 1.5739 - val_loss: 12.8358 - val_mse: 12.8358 - val_mae: 1.5977 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 15.4723 - mse: 15.4723 - mae: 1.5744 - val_loss: 12.7530 - val_mse: 12.7530 - val_mae: 1.5829 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 15.4404 - mse: 15.4404 - mae: 1.5751 - val_loss: 12.8017 - val_mse: 12.8017 - val_mae: 1.5825 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 15.4446 - mse: 15.4446 - mae: 1.5756 - val_loss: 12.7776 - val_mse: 12.7776 - val_mae: 1.6017 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 2s - loss: 15.4498 - mse: 15.4498 - mae: 1.5744 - val_loss: 12.8439 - val_mse: 12.8439 - val_mae: 1.5588 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Score for fold 3: loss of 12.843849182128906\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 13.1097 - mse: 13.1097 - mae: 1.5734 - val_loss: 22.0828 - val_mse: 22.0828 - val_mae: 1.5914 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 13.1095 - mse: 13.1095 - mae: 1.5730 - val_loss: 22.0392 - val_mse: 22.0392 - val_mae: 1.6054 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 13.1127 - mse: 13.1127 - mae: 1.5675 - val_loss: 22.0767 - val_mse: 22.0767 - val_mae: 1.6112 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 13.1083 - mse: 13.1083 - mae: 1.5701 - val_loss: 22.0506 - val_mse: 22.0506 - val_mae: 1.6086 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 13.0961 - mse: 13.0961 - mae: 1.5707 - val_loss: 22.1707 - val_mse: 22.1707 - val_mae: 1.5941 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 13.0815 - mse: 13.0815 - mae: 1.5702 - val_loss: 22.1077 - val_mse: 22.1077 - val_mae: 1.6348 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 13.1121 - mse: 13.1121 - mae: 1.5688 - val_loss: 22.0500 - val_mse: 22.0500 - val_mae: 1.6220 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Score for fold 4: loss of 22.050006866455078\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 2s - loss: 16.2730 - mse: 16.2730 - mae: 1.5964 - val_loss: 9.3833 - val_mse: 9.3833 - val_mae: 1.5422 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 16.2716 - mse: 16.2716 - mae: 1.5958 - val_loss: 9.3733 - val_mse: 9.3733 - val_mae: 1.5106 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 2s - loss: 16.2460 - mse: 16.2460 - mae: 1.5925 - val_loss: 9.3050 - val_mse: 9.3050 - val_mae: 1.5248 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 2s - loss: 16.2465 - mse: 16.2465 - mae: 1.5945 - val_loss: 9.3146 - val_mse: 9.3146 - val_mae: 1.4979 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 2s - loss: 16.2292 - mse: 16.2292 - mae: 1.5934 - val_loss: 9.3780 - val_mse: 9.3780 - val_mae: 1.5340 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 2s - loss: 16.2594 - mse: 16.2594 - mae: 1.5889 - val_loss: 9.3959 - val_mse: 9.3959 - val_mae: 1.5614 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 2s - loss: 16.2374 - mse: 16.2374 - mae: 1.5933 - val_loss: 9.4128 - val_mse: 9.4128 - val_mae: 1.5287 - lr: 0.0010 - 2s/epoch - 2ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 2s - loss: 16.2486 - mse: 16.2486 - mae: 1.5953 - val_loss: 9.4317 - val_mse: 9.4317 - val_mae: 1.5154 - lr: 0.0010 - 2s/epoch - 2ms/step\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-10-16 06:08:09,347]\u001b[0m Finished trial#9 resulted in value: 14.974. Current best value is 14.669999999999998 with parameters: {'activation': 'relu', 'num_hidden_layer': 4, 'i': 9, 'learning_rate': 0.00044016359204388243}.\u001b[0m\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Score for fold 5: loss of 9.431678771972656\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 13s - loss: 14.9078 - mse: 14.9078 - mae: 1.6153 - val_loss: 17.5610 - val_mse: 17.5610 - val_mae: 1.5722 - lr: 3.1447e-04 - 13s/epoch - 13ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 11s - loss: 14.6274 - mse: 14.6274 - mae: 1.6001 - val_loss: 17.1723 - val_mse: 17.1723 - val_mae: 1.6943 - lr: 3.1447e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 12s - loss: 14.5373 - mse: 14.5373 - mae: 1.5926 - val_loss: 17.6038 - val_mse: 17.6038 - val_mae: 1.5765 - lr: 3.1447e-04 - 12s/epoch - 12ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 12s - loss: 14.4695 - mse: 14.4695 - mae: 1.5880 - val_loss: 17.0575 - val_mse: 17.0575 - val_mae: 1.5788 - lr: 3.1447e-04 - 12s/epoch - 12ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 11s - loss: 14.4195 - mse: 14.4195 - mae: 1.5869 - val_loss: 17.3054 - val_mse: 17.3054 - val_mae: 1.6037 - lr: 3.1447e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 11s - loss: 14.3140 - mse: 14.3140 - mae: 1.5811 - val_loss: 18.0282 - val_mse: 18.0282 - val_mae: 1.5663 - lr: 3.1447e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 11s - loss: 14.3923 - mse: 14.3923 - mae: 1.5809 - val_loss: 17.2324 - val_mse: 17.2324 - val_mae: 1.5798 - lr: 3.1447e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 11s - loss: 14.3314 - mse: 14.3314 - mae: 1.5788 - val_loss: 17.4216 - val_mse: 17.4216 - val_mae: 1.6220 - lr: 3.1447e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 12s - loss: 14.2962 - mse: 14.2962 - mae: 1.5852 - val_loss: 17.1815 - val_mse: 17.1815 - val_mae: 1.6997 - lr: 3.1447e-04 - 12s/epoch - 12ms/step\n",
            "Score for fold 1: loss of 17.1815128326416\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 12s - loss: 14.9679 - mse: 14.9679 - mae: 1.5902 - val_loss: 14.2437 - val_mse: 14.2437 - val_mae: 1.4986 - lr: 3.1447e-04 - 12s/epoch - 12ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 11s - loss: 14.9833 - mse: 14.9833 - mae: 1.5942 - val_loss: 14.4130 - val_mse: 14.4130 - val_mae: 1.5389 - lr: 3.1447e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 11s - loss: 14.9009 - mse: 14.9009 - mae: 1.5912 - val_loss: 14.4145 - val_mse: 14.4145 - val_mae: 1.4944 - lr: 3.1447e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 12s - loss: 14.8730 - mse: 14.8730 - mae: 1.5853 - val_loss: 14.4407 - val_mse: 14.4407 - val_mae: 1.5291 - lr: 3.1447e-04 - 12s/epoch - 12ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 12s - loss: 14.7913 - mse: 14.7913 - mae: 1.5904 - val_loss: 14.2768 - val_mse: 14.2768 - val_mae: 1.5566 - lr: 3.1447e-04 - 12s/epoch - 12ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 12s - loss: 14.7065 - mse: 14.7065 - mae: 1.5895 - val_loss: 14.3750 - val_mse: 14.3750 - val_mae: 1.5055 - lr: 3.1447e-04 - 12s/epoch - 12ms/step\n",
            "Score for fold 2: loss of 14.37498950958252\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 12s - loss: 15.5997 - mse: 15.5997 - mae: 1.5817 - val_loss: 10.4077 - val_mse: 10.4077 - val_mae: 1.6426 - lr: 3.1447e-04 - 12s/epoch - 12ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 12s - loss: 15.6016 - mse: 15.6016 - mae: 1.5865 - val_loss: 10.4086 - val_mse: 10.4086 - val_mae: 1.5969 - lr: 3.1447e-04 - 12s/epoch - 12ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 12s - loss: 15.5365 - mse: 15.5365 - mae: 1.5799 - val_loss: 10.7864 - val_mse: 10.7864 - val_mae: 1.4984 - lr: 3.1447e-04 - 12s/epoch - 12ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 12s - loss: 15.5378 - mse: 15.5378 - mae: 1.5836 - val_loss: 10.8161 - val_mse: 10.8161 - val_mae: 1.7385 - lr: 3.1447e-04 - 12s/epoch - 12ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 12s - loss: 15.4868 - mse: 15.4868 - mae: 1.5768 - val_loss: 10.8374 - val_mse: 10.8374 - val_mae: 1.4798 - lr: 3.1447e-04 - 12s/epoch - 12ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 12s - loss: 15.4425 - mse: 15.4425 - mae: 1.5824 - val_loss: 10.8077 - val_mse: 10.8077 - val_mae: 1.7104 - lr: 3.1447e-04 - 12s/epoch - 12ms/step\n",
            "Score for fold 3: loss of 10.807695388793945\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 11s - loss: 14.7697 - mse: 14.7697 - mae: 1.6080 - val_loss: 13.1521 - val_mse: 13.1521 - val_mae: 1.4348 - lr: 3.1447e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 11s - loss: 14.7580 - mse: 14.7580 - mae: 1.6014 - val_loss: 12.9666 - val_mse: 12.9666 - val_mae: 1.6881 - lr: 3.1447e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 11s - loss: 14.7893 - mse: 14.7893 - mae: 1.5958 - val_loss: 13.7813 - val_mse: 13.7813 - val_mae: 2.0410 - lr: 3.1447e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 12s - loss: 14.5827 - mse: 14.5827 - mae: 1.6003 - val_loss: 12.7634 - val_mse: 12.7634 - val_mae: 1.5278 - lr: 3.1447e-04 - 12s/epoch - 12ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 12s - loss: 14.6053 - mse: 14.6053 - mae: 1.6024 - val_loss: 14.4082 - val_mse: 14.4082 - val_mae: 2.1586 - lr: 3.1447e-04 - 12s/epoch - 12ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 12s - loss: 14.4096 - mse: 14.4096 - mae: 1.6020 - val_loss: 13.3768 - val_mse: 13.3768 - val_mae: 1.5179 - lr: 3.1447e-04 - 12s/epoch - 12ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 12s - loss: 14.6988 - mse: 14.6988 - mae: 1.6454 - val_loss: 13.7171 - val_mse: 13.7171 - val_mae: 1.4698 - lr: 3.1447e-04 - 12s/epoch - 12ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 12s - loss: 14.5329 - mse: 14.5329 - mae: 1.6351 - val_loss: 13.8066 - val_mse: 13.8066 - val_mae: 1.8699 - lr: 3.1447e-04 - 12s/epoch - 12ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 12s - loss: 14.6730 - mse: 14.6730 - mae: 1.6709 - val_loss: 13.3508 - val_mse: 13.3508 - val_mae: 1.5715 - lr: 3.1447e-04 - 12s/epoch - 12ms/step\n",
            "Score for fold 4: loss of 13.350794792175293\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 11s - loss: 13.6075 - mse: 13.6075 - mae: 1.6697 - val_loss: 18.3940 - val_mse: 18.3940 - val_mae: 1.4893 - lr: 3.1447e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 11s - loss: 13.4171 - mse: 13.4171 - mae: 1.6419 - val_loss: 18.4158 - val_mse: 18.4158 - val_mae: 1.8695 - lr: 3.1447e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 12s - loss: 13.4395 - mse: 13.4395 - mae: 1.6859 - val_loss: 18.4504 - val_mse: 18.4504 - val_mae: 1.7063 - lr: 3.1447e-04 - 12s/epoch - 12ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 12s - loss: 13.3671 - mse: 13.3671 - mae: 1.6721 - val_loss: 18.2423 - val_mse: 18.2423 - val_mae: 1.6766 - lr: 3.1447e-04 - 12s/epoch - 12ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 12s - loss: 13.2330 - mse: 13.2330 - mae: 1.6633 - val_loss: 18.1333 - val_mse: 18.1333 - val_mae: 1.5815 - lr: 3.1447e-04 - 12s/epoch - 12ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 12s - loss: 13.4103 - mse: 13.4103 - mae: 1.6578 - val_loss: 18.3677 - val_mse: 18.3677 - val_mae: 1.4964 - lr: 3.1447e-04 - 12s/epoch - 12ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 12s - loss: 13.1186 - mse: 13.1186 - mae: 1.6634 - val_loss: 19.0723 - val_mse: 19.0723 - val_mae: 1.8481 - lr: 3.1447e-04 - 12s/epoch - 12ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 12s - loss: 12.9717 - mse: 12.9717 - mae: 1.6547 - val_loss: 18.8130 - val_mse: 18.8130 - val_mae: 2.0111 - lr: 3.1447e-04 - 12s/epoch - 12ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 13s - loss: 13.1124 - mse: 13.1124 - mae: 1.6484 - val_loss: 18.7131 - val_mse: 18.7131 - val_mae: 1.5272 - lr: 3.1447e-04 - 13s/epoch - 13ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 12s - loss: 13.0339 - mse: 13.0339 - mae: 1.6838 - val_loss: 18.4065 - val_mse: 18.4065 - val_mae: 1.7405 - lr: 3.1447e-04 - 12s/epoch - 12ms/step\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-10-16 06:16:07,627]\u001b[0m Finished trial#10 resulted in value: 14.824000000000002. Current best value is 14.669999999999998 with parameters: {'activation': 'relu', 'num_hidden_layer': 4, 'i': 9, 'learning_rate': 0.00044016359204388243}.\u001b[0m\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Score for fold 5: loss of 18.4064998626709\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 12s - loss: 13.7470 - mse: 13.7470 - mae: 1.6195 - val_loss: 22.1945 - val_mse: 22.1945 - val_mae: 1.5780 - lr: 3.1068e-04 - 12s/epoch - 12ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 12s - loss: 13.4248 - mse: 13.4248 - mae: 1.5953 - val_loss: 22.2153 - val_mse: 22.2153 - val_mae: 1.5389 - lr: 3.1068e-04 - 12s/epoch - 12ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 12s - loss: 13.2851 - mse: 13.2851 - mae: 1.5886 - val_loss: 22.1607 - val_mse: 22.1607 - val_mae: 1.5856 - lr: 3.1068e-04 - 12s/epoch - 12ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 12s - loss: 13.2141 - mse: 13.2141 - mae: 1.5870 - val_loss: 22.1096 - val_mse: 22.1096 - val_mae: 1.5836 - lr: 3.1068e-04 - 12s/epoch - 12ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 12s - loss: 13.2350 - mse: 13.2350 - mae: 1.5866 - val_loss: 22.1179 - val_mse: 22.1179 - val_mae: 1.5545 - lr: 3.1068e-04 - 12s/epoch - 12ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 12s - loss: 13.1489 - mse: 13.1489 - mae: 1.5816 - val_loss: 22.1539 - val_mse: 22.1539 - val_mae: 1.5101 - lr: 3.1068e-04 - 12s/epoch - 12ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 12s - loss: 13.1838 - mse: 13.1838 - mae: 1.5781 - val_loss: 21.9641 - val_mse: 21.9641 - val_mae: 1.6945 - lr: 3.1068e-04 - 12s/epoch - 12ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 12s - loss: 13.2261 - mse: 13.2261 - mae: 1.5810 - val_loss: 21.9710 - val_mse: 21.9710 - val_mae: 1.6521 - lr: 3.1068e-04 - 12s/epoch - 12ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 12s - loss: 13.1110 - mse: 13.1110 - mae: 1.5827 - val_loss: 21.9240 - val_mse: 21.9240 - val_mae: 1.7104 - lr: 3.1068e-04 - 12s/epoch - 12ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 12s - loss: 13.1291 - mse: 13.1291 - mae: 1.5729 - val_loss: 22.0505 - val_mse: 22.0505 - val_mae: 1.7595 - lr: 3.1068e-04 - 12s/epoch - 12ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 12s - loss: 13.1176 - mse: 13.1176 - mae: 1.5807 - val_loss: 22.3148 - val_mse: 22.3148 - val_mae: 1.5773 - lr: 3.1068e-04 - 12s/epoch - 12ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 12s - loss: 13.0312 - mse: 13.0312 - mae: 1.5821 - val_loss: 21.9013 - val_mse: 21.9013 - val_mae: 1.5524 - lr: 3.1068e-04 - 12s/epoch - 12ms/step\n",
            "Epoch 13/100\n",
            "1000/1000 - 12s - loss: 12.9836 - mse: 12.9836 - mae: 1.5782 - val_loss: 21.8455 - val_mse: 21.8455 - val_mae: 1.6837 - lr: 3.1068e-04 - 12s/epoch - 12ms/step\n",
            "Epoch 14/100\n",
            "1000/1000 - 12s - loss: 12.9190 - mse: 12.9190 - mae: 1.5765 - val_loss: 21.5798 - val_mse: 21.5798 - val_mae: 1.5975 - lr: 3.1068e-04 - 12s/epoch - 12ms/step\n",
            "Epoch 15/100\n",
            "1000/1000 - 12s - loss: 12.8306 - mse: 12.8306 - mae: 1.5810 - val_loss: 22.0775 - val_mse: 22.0775 - val_mae: 1.5103 - lr: 3.1068e-04 - 12s/epoch - 12ms/step\n",
            "Epoch 16/100\n",
            "1000/1000 - 12s - loss: 12.9019 - mse: 12.9019 - mae: 1.5735 - val_loss: 22.0657 - val_mse: 22.0657 - val_mae: 1.5564 - lr: 3.1068e-04 - 12s/epoch - 12ms/step\n",
            "Epoch 17/100\n",
            "1000/1000 - 12s - loss: 12.6952 - mse: 12.6952 - mae: 1.5775 - val_loss: 21.7196 - val_mse: 21.7196 - val_mae: 1.6536 - lr: 3.1068e-04 - 12s/epoch - 12ms/step\n",
            "Epoch 18/100\n",
            "1000/1000 - 13s - loss: 12.8215 - mse: 12.8215 - mae: 1.5791 - val_loss: 22.0157 - val_mse: 22.0157 - val_mae: 1.5077 - lr: 3.1068e-04 - 13s/epoch - 13ms/step\n",
            "Epoch 19/100\n",
            "1000/1000 - 13s - loss: 12.9093 - mse: 12.9093 - mae: 1.5773 - val_loss: 21.9481 - val_mse: 21.9481 - val_mae: 1.5964 - lr: 3.1068e-04 - 13s/epoch - 13ms/step\n",
            "Score for fold 1: loss of 21.94810676574707\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 12s - loss: 15.6451 - mse: 15.6451 - mae: 1.5923 - val_loss: 10.0344 - val_mse: 10.0344 - val_mae: 1.5814 - lr: 3.1068e-04 - 12s/epoch - 12ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 12s - loss: 15.5179 - mse: 15.5179 - mae: 1.5856 - val_loss: 10.0360 - val_mse: 10.0360 - val_mae: 1.5611 - lr: 3.1068e-04 - 12s/epoch - 12ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 12s - loss: 15.4551 - mse: 15.4551 - mae: 1.5882 - val_loss: 10.1404 - val_mse: 10.1404 - val_mae: 1.5450 - lr: 3.1068e-04 - 12s/epoch - 12ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 12s - loss: 15.4762 - mse: 15.4762 - mae: 1.5830 - val_loss: 10.3914 - val_mse: 10.3914 - val_mae: 1.4413 - lr: 3.1068e-04 - 12s/epoch - 12ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 12s - loss: 15.4597 - mse: 15.4597 - mae: 1.5763 - val_loss: 10.1687 - val_mse: 10.1687 - val_mae: 1.5611 - lr: 3.1068e-04 - 12s/epoch - 12ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 12s - loss: 15.2344 - mse: 15.2344 - mae: 1.5814 - val_loss: 10.1320 - val_mse: 10.1320 - val_mae: 1.6350 - lr: 3.1068e-04 - 12s/epoch - 12ms/step\n",
            "Score for fold 2: loss of 10.132040023803711\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 12s - loss: 14.7113 - mse: 14.7113 - mae: 1.5855 - val_loss: 13.3897 - val_mse: 13.3897 - val_mae: 1.5382 - lr: 3.1068e-04 - 12s/epoch - 12ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 12s - loss: 14.6773 - mse: 14.6773 - mae: 1.5868 - val_loss: 13.1552 - val_mse: 13.1552 - val_mae: 1.4850 - lr: 3.1068e-04 - 12s/epoch - 12ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 12s - loss: 14.4779 - mse: 14.4779 - mae: 1.5798 - val_loss: 13.6618 - val_mse: 13.6618 - val_mae: 1.4680 - lr: 3.1068e-04 - 12s/epoch - 12ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 12s - loss: 14.4813 - mse: 14.4813 - mae: 1.5842 - val_loss: 12.3374 - val_mse: 12.3374 - val_mae: 1.4855 - lr: 3.1068e-04 - 12s/epoch - 12ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 13s - loss: 14.4639 - mse: 14.4639 - mae: 1.5909 - val_loss: 13.4054 - val_mse: 13.4054 - val_mae: 1.6663 - lr: 3.1068e-04 - 13s/epoch - 13ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 12s - loss: 14.4888 - mse: 14.4888 - mae: 1.5859 - val_loss: 13.7737 - val_mse: 13.7737 - val_mae: 1.5766 - lr: 3.1068e-04 - 12s/epoch - 12ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 13s - loss: 14.4337 - mse: 14.4337 - mae: 1.5873 - val_loss: 13.5515 - val_mse: 13.5515 - val_mae: 1.4573 - lr: 3.1068e-04 - 13s/epoch - 13ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 12s - loss: 14.3383 - mse: 14.3383 - mae: 1.5943 - val_loss: 13.7771 - val_mse: 13.7771 - val_mae: 1.5688 - lr: 3.1068e-04 - 12s/epoch - 12ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 12s - loss: 14.2224 - mse: 14.2224 - mae: 1.5836 - val_loss: 13.5533 - val_mse: 13.5533 - val_mae: 1.6479 - lr: 3.1068e-04 - 12s/epoch - 12ms/step\n",
            "Score for fold 3: loss of 13.55331802368164\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 12s - loss: 14.5499 - mse: 14.5499 - mae: 1.5940 - val_loss: 12.8332 - val_mse: 12.8332 - val_mae: 1.5393 - lr: 3.1068e-04 - 12s/epoch - 12ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 12s - loss: 14.4415 - mse: 14.4415 - mae: 1.6093 - val_loss: 12.9184 - val_mse: 12.9184 - val_mae: 1.5184 - lr: 3.1068e-04 - 12s/epoch - 12ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 12s - loss: 14.5802 - mse: 14.5802 - mae: 1.6081 - val_loss: 13.4485 - val_mse: 13.4485 - val_mae: 1.4435 - lr: 3.1068e-04 - 12s/epoch - 12ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 12s - loss: 14.3903 - mse: 14.3903 - mae: 1.6073 - val_loss: 13.3914 - val_mse: 13.3914 - val_mae: 1.7503 - lr: 3.1068e-04 - 12s/epoch - 12ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 12s - loss: 14.2584 - mse: 14.2584 - mae: 1.5953 - val_loss: 13.2587 - val_mse: 13.2587 - val_mae: 1.6166 - lr: 3.1068e-04 - 12s/epoch - 12ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 12s - loss: 14.1949 - mse: 14.1949 - mae: 1.6014 - val_loss: 13.3879 - val_mse: 13.3879 - val_mae: 1.7416 - lr: 3.1068e-04 - 12s/epoch - 12ms/step\n",
            "Score for fold 4: loss of 13.38785171508789\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 11s - loss: 14.1778 - mse: 14.1778 - mae: 1.6143 - val_loss: 13.9930 - val_mse: 13.9930 - val_mae: 1.4855 - lr: 3.1068e-04 - 11s/epoch - 11ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 12s - loss: 14.0632 - mse: 14.0632 - mae: 1.6056 - val_loss: 14.4526 - val_mse: 14.4526 - val_mae: 1.8042 - lr: 3.1068e-04 - 12s/epoch - 12ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 12s - loss: 14.0552 - mse: 14.0552 - mae: 1.6080 - val_loss: 14.4019 - val_mse: 14.4019 - val_mae: 1.5497 - lr: 3.1068e-04 - 12s/epoch - 12ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 12s - loss: 13.7068 - mse: 13.7068 - mae: 1.6170 - val_loss: 14.0307 - val_mse: 14.0307 - val_mae: 1.5529 - lr: 3.1068e-04 - 12s/epoch - 12ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 12s - loss: 13.9380 - mse: 13.9380 - mae: 1.6034 - val_loss: 14.5411 - val_mse: 14.5411 - val_mae: 1.5020 - lr: 3.1068e-04 - 12s/epoch - 12ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 13s - loss: 13.8849 - mse: 13.8849 - mae: 1.5909 - val_loss: 14.5024 - val_mse: 14.5024 - val_mae: 1.5775 - lr: 3.1068e-04 - 13s/epoch - 13ms/step\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-10-16 06:25:33,615]\u001b[0m Finished trial#11 resulted in value: 14.703999999999999. Current best value is 14.669999999999998 with parameters: {'activation': 'relu', 'num_hidden_layer': 4, 'i': 9, 'learning_rate': 0.00044016359204388243}.\u001b[0m\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Score for fold 5: loss of 14.50245189666748\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 6s - loss: 15.8848 - mse: 15.8848 - mae: 1.6285 - val_loss: 13.4056 - val_mse: 13.4056 - val_mae: 1.6093 - lr: 3.6924e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 6s - loss: 15.5105 - mse: 15.5105 - mae: 1.6113 - val_loss: 13.5124 - val_mse: 13.5124 - val_mae: 1.5127 - lr: 3.6924e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 6s - loss: 15.4482 - mse: 15.4482 - mae: 1.6104 - val_loss: 13.4643 - val_mse: 13.4643 - val_mae: 1.5590 - lr: 3.6924e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 6s - loss: 15.3778 - mse: 15.3778 - mae: 1.5998 - val_loss: 13.4204 - val_mse: 13.4204 - val_mae: 1.5109 - lr: 3.6924e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 6s - loss: 15.2721 - mse: 15.2721 - mae: 1.5921 - val_loss: 13.3181 - val_mse: 13.3181 - val_mae: 1.5386 - lr: 3.6924e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 6s - loss: 15.3464 - mse: 15.3464 - mae: 1.5910 - val_loss: 13.3417 - val_mse: 13.3417 - val_mae: 1.6598 - lr: 3.6924e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 6s - loss: 15.2709 - mse: 15.2709 - mae: 1.5908 - val_loss: 13.2998 - val_mse: 13.2998 - val_mae: 1.5685 - lr: 3.6924e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 6s - loss: 15.2187 - mse: 15.2187 - mae: 1.5891 - val_loss: 13.5176 - val_mse: 13.5176 - val_mae: 1.6709 - lr: 3.6924e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 6s - loss: 15.1053 - mse: 15.1053 - mae: 1.5861 - val_loss: 13.4109 - val_mse: 13.4109 - val_mae: 1.4964 - lr: 3.6924e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 6s - loss: 15.0728 - mse: 15.0728 - mae: 1.5841 - val_loss: 13.3441 - val_mse: 13.3441 - val_mae: 1.6241 - lr: 3.6924e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 6s - loss: 14.9956 - mse: 14.9956 - mae: 1.5817 - val_loss: 13.2744 - val_mse: 13.2744 - val_mae: 1.5403 - lr: 3.6924e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 6s - loss: 14.9914 - mse: 14.9914 - mae: 1.5837 - val_loss: 13.2596 - val_mse: 13.2596 - val_mae: 1.5802 - lr: 3.6924e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 13/100\n",
            "1000/1000 - 6s - loss: 14.9137 - mse: 14.9137 - mae: 1.5839 - val_loss: 13.3198 - val_mse: 13.3198 - val_mae: 1.5963 - lr: 3.6924e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 14/100\n",
            "1000/1000 - 6s - loss: 14.9354 - mse: 14.9354 - mae: 1.5824 - val_loss: 13.3788 - val_mse: 13.3788 - val_mae: 1.5875 - lr: 3.6924e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 15/100\n",
            "1000/1000 - 6s - loss: 14.7715 - mse: 14.7715 - mae: 1.5723 - val_loss: 13.4690 - val_mse: 13.4690 - val_mae: 1.5938 - lr: 3.6924e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 16/100\n",
            "1000/1000 - 6s - loss: 14.6246 - mse: 14.6246 - mae: 1.5750 - val_loss: 13.3124 - val_mse: 13.3124 - val_mae: 1.5571 - lr: 3.6924e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 17/100\n",
            "1000/1000 - 6s - loss: 14.6246 - mse: 14.6246 - mae: 1.5655 - val_loss: 13.6764 - val_mse: 13.6764 - val_mae: 1.4916 - lr: 3.6924e-04 - 6s/epoch - 6ms/step\n",
            "Score for fold 1: loss of 13.676385879516602\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 6s - loss: 15.0356 - mse: 15.0356 - mae: 1.5690 - val_loss: 12.0970 - val_mse: 12.0970 - val_mae: 1.6081 - lr: 3.6924e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 6s - loss: 14.8808 - mse: 14.8808 - mae: 1.5675 - val_loss: 12.1852 - val_mse: 12.1852 - val_mae: 1.5330 - lr: 3.6924e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 6s - loss: 14.7614 - mse: 14.7614 - mae: 1.5665 - val_loss: 12.5429 - val_mse: 12.5429 - val_mae: 1.5540 - lr: 3.6924e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 6s - loss: 14.6950 - mse: 14.6950 - mae: 1.5662 - val_loss: 12.6973 - val_mse: 12.6973 - val_mae: 1.7008 - lr: 3.6924e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 6s - loss: 14.8065 - mse: 14.8065 - mae: 1.5649 - val_loss: 12.6149 - val_mse: 12.6149 - val_mae: 1.6803 - lr: 3.6924e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 6s - loss: 14.6175 - mse: 14.6175 - mae: 1.5646 - val_loss: 12.7544 - val_mse: 12.7544 - val_mae: 1.5089 - lr: 3.6924e-04 - 6s/epoch - 6ms/step\n",
            "Score for fold 2: loss of 12.754435539245605\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 6s - loss: 14.3868 - mse: 14.3868 - mae: 1.5778 - val_loss: 14.2896 - val_mse: 14.2896 - val_mae: 1.6207 - lr: 3.6924e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 6s - loss: 14.3601 - mse: 14.3601 - mae: 1.5756 - val_loss: 14.0977 - val_mse: 14.0977 - val_mae: 1.5955 - lr: 3.6924e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 6s - loss: 14.1007 - mse: 14.1007 - mae: 1.5684 - val_loss: 14.5177 - val_mse: 14.5177 - val_mae: 1.5558 - lr: 3.6924e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 6s - loss: 14.0950 - mse: 14.0950 - mae: 1.5718 - val_loss: 15.0838 - val_mse: 15.0838 - val_mae: 1.4464 - lr: 3.6924e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 6s - loss: 13.9957 - mse: 13.9957 - mae: 1.5597 - val_loss: 14.7746 - val_mse: 14.7746 - val_mae: 1.6230 - lr: 3.6924e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 6s - loss: 13.9978 - mse: 13.9978 - mae: 1.5646 - val_loss: 14.4381 - val_mse: 14.4381 - val_mae: 1.5497 - lr: 3.6924e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 6s - loss: 13.9281 - mse: 13.9281 - mae: 1.5762 - val_loss: 14.5047 - val_mse: 14.5047 - val_mae: 1.6892 - lr: 3.6924e-04 - 6s/epoch - 6ms/step\n",
            "Score for fold 3: loss of 14.504694938659668\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 6s - loss: 12.8038 - mse: 12.8038 - mae: 1.5890 - val_loss: 19.9476 - val_mse: 19.9476 - val_mae: 1.4532 - lr: 3.6924e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 6s - loss: 12.7558 - mse: 12.7558 - mae: 1.5702 - val_loss: 18.8851 - val_mse: 18.8851 - val_mae: 1.5999 - lr: 3.6924e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 6s - loss: 12.6004 - mse: 12.6004 - mae: 1.5902 - val_loss: 19.4755 - val_mse: 19.4755 - val_mae: 1.4926 - lr: 3.6924e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 6s - loss: 12.6433 - mse: 12.6433 - mae: 1.5987 - val_loss: 19.5038 - val_mse: 19.5038 - val_mae: 1.5230 - lr: 3.6924e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 6s - loss: 12.4636 - mse: 12.4636 - mae: 1.5849 - val_loss: 19.4294 - val_mse: 19.4294 - val_mae: 1.5522 - lr: 3.6924e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 6s - loss: 12.4698 - mse: 12.4698 - mae: 1.5936 - val_loss: 19.0718 - val_mse: 19.0718 - val_mae: 1.5279 - lr: 3.6924e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 6s - loss: 12.5703 - mse: 12.5703 - mae: 1.6124 - val_loss: 19.1762 - val_mse: 19.1762 - val_mae: 1.5237 - lr: 3.6924e-04 - 6s/epoch - 6ms/step\n",
            "Score for fold 4: loss of 19.176237106323242\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 6s - loss: 14.4178 - mse: 14.4178 - mae: 1.6203 - val_loss: 11.7014 - val_mse: 11.7014 - val_mae: 1.5341 - lr: 3.6924e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 6s - loss: 14.3062 - mse: 14.3062 - mae: 1.6303 - val_loss: 12.2090 - val_mse: 12.2090 - val_mae: 1.4790 - lr: 3.6924e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 6s - loss: 14.2731 - mse: 14.2731 - mae: 1.6389 - val_loss: 12.0375 - val_mse: 12.0375 - val_mae: 1.4780 - lr: 3.6924e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 6s - loss: 14.1936 - mse: 14.1936 - mae: 1.6174 - val_loss: 11.8175 - val_mse: 11.8175 - val_mae: 1.6071 - lr: 3.6924e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 6s - loss: 14.1519 - mse: 14.1519 - mae: 1.6281 - val_loss: 12.4116 - val_mse: 12.4116 - val_mae: 1.5794 - lr: 3.6924e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 6s - loss: 14.0155 - mse: 14.0155 - mae: 1.6231 - val_loss: 12.5709 - val_mse: 12.5709 - val_mae: 1.4562 - lr: 3.6924e-04 - 6s/epoch - 6ms/step\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-10-16 06:29:49,221]\u001b[0m Finished trial#12 resulted in value: 14.536000000000001. Current best value is 14.536000000000001 with parameters: {'activation': 'relu', 'num_hidden_layer': 4, 'i': 9, 'learning_rate': 0.00036924207662998674}.\u001b[0m\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Score for fold 5: loss of 12.570920944213867\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 6s - loss: 14.8953 - mse: 14.8953 - mae: 1.6103 - val_loss: 17.7606 - val_mse: 17.7606 - val_mae: 1.6862 - lr: 4.6133e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 6s - loss: 14.6182 - mse: 14.6182 - mae: 1.5895 - val_loss: 17.2881 - val_mse: 17.2881 - val_mae: 1.7119 - lr: 4.6133e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 6s - loss: 14.5845 - mse: 14.5845 - mae: 1.5883 - val_loss: 17.0376 - val_mse: 17.0376 - val_mae: 1.6011 - lr: 4.6133e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 6s - loss: 14.4801 - mse: 14.4801 - mae: 1.5795 - val_loss: 16.8853 - val_mse: 16.8853 - val_mae: 1.6238 - lr: 4.6133e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 6s - loss: 14.4344 - mse: 14.4344 - mae: 1.5772 - val_loss: 16.7919 - val_mse: 16.7919 - val_mae: 1.6434 - lr: 4.6133e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 6s - loss: 14.4218 - mse: 14.4218 - mae: 1.5739 - val_loss: 16.5955 - val_mse: 16.5955 - val_mae: 1.6255 - lr: 4.6133e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 6s - loss: 14.3524 - mse: 14.3524 - mae: 1.5736 - val_loss: 16.4780 - val_mse: 16.4780 - val_mae: 1.6445 - lr: 4.6133e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 6s - loss: 14.2984 - mse: 14.2984 - mae: 1.5682 - val_loss: 16.9961 - val_mse: 16.9961 - val_mae: 1.6385 - lr: 4.6133e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 6s - loss: 14.2204 - mse: 14.2204 - mae: 1.5710 - val_loss: 16.7508 - val_mse: 16.7508 - val_mae: 1.6061 - lr: 4.6133e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 6s - loss: 14.1727 - mse: 14.1727 - mae: 1.5663 - val_loss: 16.9837 - val_mse: 16.9837 - val_mae: 1.7489 - lr: 4.6133e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 6s - loss: 14.1480 - mse: 14.1480 - mae: 1.5678 - val_loss: 17.2003 - val_mse: 17.2003 - val_mae: 1.6097 - lr: 4.6133e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 6s - loss: 14.0879 - mse: 14.0879 - mae: 1.5639 - val_loss: 16.9666 - val_mse: 16.9666 - val_mae: 1.5612 - lr: 4.6133e-04 - 6s/epoch - 6ms/step\n",
            "Score for fold 1: loss of 16.966598510742188\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 6s - loss: 15.4465 - mse: 15.4465 - mae: 1.5852 - val_loss: 12.3172 - val_mse: 12.3172 - val_mae: 1.4885 - lr: 4.6133e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 6s - loss: 15.2400 - mse: 15.2400 - mae: 1.5838 - val_loss: 12.5885 - val_mse: 12.5885 - val_mae: 1.5612 - lr: 4.6133e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 6s - loss: 15.1699 - mse: 15.1699 - mae: 1.5866 - val_loss: 12.4423 - val_mse: 12.4423 - val_mae: 1.5771 - lr: 4.6133e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 6s - loss: 15.2192 - mse: 15.2192 - mae: 1.5831 - val_loss: 12.4267 - val_mse: 12.4267 - val_mae: 1.6554 - lr: 4.6133e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 6s - loss: 15.1025 - mse: 15.1025 - mae: 1.5841 - val_loss: 12.4525 - val_mse: 12.4525 - val_mae: 1.5186 - lr: 4.6133e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 6s - loss: 14.9537 - mse: 14.9537 - mae: 1.5766 - val_loss: 12.4761 - val_mse: 12.4761 - val_mae: 1.6145 - lr: 4.6133e-04 - 6s/epoch - 6ms/step\n",
            "Score for fold 2: loss of 12.476142883300781\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 6s - loss: 13.1984 - mse: 13.1984 - mae: 1.5782 - val_loss: 20.2873 - val_mse: 20.2873 - val_mae: 1.5892 - lr: 4.6133e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 6s - loss: 12.9698 - mse: 12.9698 - mae: 1.5877 - val_loss: 20.2644 - val_mse: 20.2644 - val_mae: 1.7982 - lr: 4.6133e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 6s - loss: 13.0120 - mse: 13.0120 - mae: 1.5778 - val_loss: 20.2709 - val_mse: 20.2709 - val_mae: 1.8149 - lr: 4.6133e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 6s - loss: 13.0365 - mse: 13.0365 - mae: 1.5803 - val_loss: 20.2417 - val_mse: 20.2417 - val_mae: 1.6947 - lr: 4.6133e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 6s - loss: 13.0423 - mse: 13.0423 - mae: 1.5750 - val_loss: 20.2818 - val_mse: 20.2818 - val_mae: 1.5947 - lr: 4.6133e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 6s - loss: 12.8061 - mse: 12.8061 - mae: 1.5628 - val_loss: 20.4323 - val_mse: 20.4323 - val_mae: 1.7289 - lr: 4.6133e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 6s - loss: 12.8951 - mse: 12.8951 - mae: 1.5734 - val_loss: 20.3278 - val_mse: 20.3278 - val_mae: 1.7558 - lr: 4.6133e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 6s - loss: 12.7615 - mse: 12.7615 - mae: 1.5730 - val_loss: 20.6597 - val_mse: 20.6597 - val_mae: 1.6329 - lr: 4.6133e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 6s - loss: 12.8250 - mse: 12.8250 - mae: 1.5800 - val_loss: 20.6726 - val_mse: 20.6726 - val_mae: 1.5096 - lr: 4.6133e-04 - 6s/epoch - 6ms/step\n",
            "Score for fold 3: loss of 20.672555923461914\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 6s - loss: 15.6700 - mse: 15.6700 - mae: 1.5936 - val_loss: 8.7806 - val_mse: 8.7806 - val_mae: 1.6261 - lr: 4.6133e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 6s - loss: 15.4007 - mse: 15.4007 - mae: 1.5896 - val_loss: 9.0085 - val_mse: 9.0085 - val_mae: 1.5356 - lr: 4.6133e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 6s - loss: 15.3886 - mse: 15.3886 - mae: 1.5899 - val_loss: 9.1437 - val_mse: 9.1437 - val_mae: 1.4540 - lr: 4.6133e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 6s - loss: 15.3298 - mse: 15.3298 - mae: 1.5754 - val_loss: 9.2331 - val_mse: 9.2331 - val_mae: 1.4862 - lr: 4.6133e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 6s - loss: 15.3915 - mse: 15.3915 - mae: 1.5795 - val_loss: 9.1176 - val_mse: 9.1176 - val_mae: 1.5325 - lr: 4.6133e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 6s - loss: 15.1866 - mse: 15.1866 - mae: 1.5686 - val_loss: 9.1731 - val_mse: 9.1731 - val_mae: 1.5306 - lr: 4.6133e-04 - 6s/epoch - 6ms/step\n",
            "Score for fold 4: loss of 9.173062324523926\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 6s - loss: 14.1858 - mse: 14.1858 - mae: 1.5782 - val_loss: 13.8057 - val_mse: 13.8057 - val_mae: 1.5722 - lr: 4.6133e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 6s - loss: 13.9185 - mse: 13.9185 - mae: 1.5817 - val_loss: 13.9955 - val_mse: 13.9955 - val_mae: 1.5028 - lr: 4.6133e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 6s - loss: 13.9022 - mse: 13.9022 - mae: 1.5812 - val_loss: 13.9349 - val_mse: 13.9349 - val_mae: 1.6020 - lr: 4.6133e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 6s - loss: 14.0130 - mse: 14.0130 - mae: 1.5721 - val_loss: 14.0771 - val_mse: 14.0771 - val_mae: 1.7116 - lr: 4.6133e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 6s - loss: 13.7894 - mse: 13.7894 - mae: 1.5874 - val_loss: 14.6282 - val_mse: 14.6282 - val_mae: 1.9307 - lr: 4.6133e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 6s - loss: 13.7990 - mse: 13.7990 - mae: 1.5965 - val_loss: 13.9242 - val_mse: 13.9242 - val_mae: 1.5260 - lr: 4.6133e-04 - 6s/epoch - 6ms/step\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-10-16 06:33:45,643]\u001b[0m Finished trial#13 resulted in value: 14.642000000000001. Current best value is 14.536000000000001 with parameters: {'activation': 'relu', 'num_hidden_layer': 4, 'i': 9, 'learning_rate': 0.00036924207662998674}.\u001b[0m\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Score for fold 5: loss of 13.924233436584473\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 7s - loss: 15.5270 - mse: 15.5270 - mae: 1.6264 - val_loss: 15.1116 - val_mse: 15.1116 - val_mae: 1.5534 - lr: 1.5522e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 6s - loss: 15.1457 - mse: 15.1457 - mae: 1.5955 - val_loss: 15.0315 - val_mse: 15.0315 - val_mae: 1.6035 - lr: 1.5522e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 6s - loss: 15.0772 - mse: 15.0772 - mae: 1.5978 - val_loss: 15.0662 - val_mse: 15.0662 - val_mae: 1.6524 - lr: 1.5522e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 6s - loss: 14.9498 - mse: 14.9498 - mae: 1.5966 - val_loss: 15.2409 - val_mse: 15.2409 - val_mae: 1.5077 - lr: 1.5522e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 6s - loss: 14.9587 - mse: 14.9587 - mae: 1.5887 - val_loss: 15.0253 - val_mse: 15.0253 - val_mae: 1.6420 - lr: 1.5522e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 6s - loss: 14.9014 - mse: 14.9014 - mae: 1.5895 - val_loss: 15.0908 - val_mse: 15.0908 - val_mae: 1.5499 - lr: 1.5522e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 6s - loss: 14.8475 - mse: 14.8475 - mae: 1.5885 - val_loss: 15.0670 - val_mse: 15.0670 - val_mae: 1.5855 - lr: 1.5522e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 6s - loss: 14.8322 - mse: 14.8322 - mae: 1.5858 - val_loss: 15.2186 - val_mse: 15.2186 - val_mae: 1.5399 - lr: 1.5522e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 6s - loss: 14.8036 - mse: 14.8036 - mae: 1.5786 - val_loss: 15.2077 - val_mse: 15.2077 - val_mae: 1.5364 - lr: 1.5522e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 6s - loss: 14.7015 - mse: 14.7015 - mae: 1.5794 - val_loss: 15.1438 - val_mse: 15.1438 - val_mae: 1.5121 - lr: 1.5522e-04 - 6s/epoch - 6ms/step\n",
            "Score for fold 1: loss of 15.143798828125\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 6s - loss: 15.3127 - mse: 15.3127 - mae: 1.5629 - val_loss: 12.7750 - val_mse: 12.7750 - val_mae: 1.5894 - lr: 1.5522e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 6s - loss: 15.1139 - mse: 15.1139 - mae: 1.5629 - val_loss: 12.9407 - val_mse: 12.9407 - val_mae: 1.6259 - lr: 1.5522e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 6s - loss: 15.1070 - mse: 15.1070 - mae: 1.5563 - val_loss: 13.4749 - val_mse: 13.4749 - val_mae: 1.5822 - lr: 1.5522e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 6s - loss: 15.0851 - mse: 15.0851 - mae: 1.5576 - val_loss: 13.0930 - val_mse: 13.0930 - val_mae: 1.5573 - lr: 1.5522e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 6s - loss: 15.0636 - mse: 15.0636 - mae: 1.5520 - val_loss: 12.9852 - val_mse: 12.9852 - val_mae: 1.5723 - lr: 1.5522e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 6s - loss: 15.0447 - mse: 15.0447 - mae: 1.5506 - val_loss: 13.1786 - val_mse: 13.1786 - val_mae: 1.5751 - lr: 1.5522e-04 - 6s/epoch - 6ms/step\n",
            "Score for fold 2: loss of 13.178620338439941\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 5s - loss: 14.9448 - mse: 14.9448 - mae: 1.5606 - val_loss: 13.0821 - val_mse: 13.0821 - val_mae: 1.5005 - lr: 1.5522e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 6s - loss: 14.8362 - mse: 14.8362 - mae: 1.5610 - val_loss: 13.0932 - val_mse: 13.0932 - val_mae: 1.5649 - lr: 1.5522e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 5s - loss: 14.7972 - mse: 14.7972 - mae: 1.5597 - val_loss: 13.1026 - val_mse: 13.1026 - val_mae: 1.6412 - lr: 1.5522e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 5s - loss: 14.6699 - mse: 14.6699 - mae: 1.5530 - val_loss: 13.2568 - val_mse: 13.2568 - val_mae: 1.5000 - lr: 1.5522e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 6s - loss: 14.6191 - mse: 14.6191 - mae: 1.5505 - val_loss: 13.2357 - val_mse: 13.2357 - val_mae: 1.5659 - lr: 1.5522e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 6s - loss: 14.6217 - mse: 14.6217 - mae: 1.5505 - val_loss: 13.4216 - val_mse: 13.4216 - val_mae: 1.4693 - lr: 1.5522e-04 - 6s/epoch - 6ms/step\n",
            "Score for fold 3: loss of 13.421642303466797\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 6s - loss: 14.9538 - mse: 14.9538 - mae: 1.5587 - val_loss: 11.1943 - val_mse: 11.1943 - val_mae: 1.6218 - lr: 1.5522e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 6s - loss: 15.0201 - mse: 15.0201 - mae: 1.5599 - val_loss: 11.4997 - val_mse: 11.4997 - val_mae: 1.4907 - lr: 1.5522e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 6s - loss: 14.8799 - mse: 14.8799 - mae: 1.5602 - val_loss: 11.5082 - val_mse: 11.5082 - val_mae: 1.6493 - lr: 1.5522e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 6s - loss: 14.7848 - mse: 14.7848 - mae: 1.5548 - val_loss: 11.1214 - val_mse: 11.1214 - val_mae: 1.5500 - lr: 1.5522e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 6s - loss: 14.7265 - mse: 14.7265 - mae: 1.5582 - val_loss: 11.4475 - val_mse: 11.4475 - val_mae: 1.6321 - lr: 1.5522e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 6s - loss: 14.6203 - mse: 14.6203 - mae: 1.5537 - val_loss: 11.6163 - val_mse: 11.6163 - val_mae: 1.4830 - lr: 1.5522e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 6s - loss: 14.5830 - mse: 14.5830 - mae: 1.5412 - val_loss: 11.2183 - val_mse: 11.2183 - val_mae: 1.5367 - lr: 1.5522e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 6s - loss: 14.4326 - mse: 14.4326 - mae: 1.5508 - val_loss: 11.3903 - val_mse: 11.3903 - val_mae: 1.5731 - lr: 1.5522e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 6s - loss: 14.4609 - mse: 14.4609 - mae: 1.5356 - val_loss: 11.2636 - val_mse: 11.2636 - val_mae: 1.5510 - lr: 1.5522e-04 - 6s/epoch - 6ms/step\n",
            "Score for fold 4: loss of 11.263620376586914\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 6s - loss: 12.3041 - mse: 12.3041 - mae: 1.5427 - val_loss: 19.6041 - val_mse: 19.6041 - val_mae: 1.5947 - lr: 1.5522e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 6s - loss: 12.1110 - mse: 12.1110 - mae: 1.5418 - val_loss: 20.4153 - val_mse: 20.4153 - val_mae: 1.4788 - lr: 1.5522e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 6s - loss: 12.0845 - mse: 12.0845 - mae: 1.5366 - val_loss: 19.8810 - val_mse: 19.8810 - val_mae: 1.6785 - lr: 1.5522e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 6s - loss: 11.9962 - mse: 11.9962 - mae: 1.5357 - val_loss: 20.2035 - val_mse: 20.2035 - val_mae: 1.5107 - lr: 1.5522e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 6s - loss: 11.9709 - mse: 11.9709 - mae: 1.5311 - val_loss: 20.3775 - val_mse: 20.3775 - val_mae: 1.5629 - lr: 1.5522e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 6s - loss: 11.8459 - mse: 11.8459 - mae: 1.5233 - val_loss: 20.0065 - val_mse: 20.0065 - val_mae: 1.5747 - lr: 1.5522e-04 - 6s/epoch - 6ms/step\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-10-16 06:37:22,719]\u001b[0m Finished trial#14 resulted in value: 14.602. Current best value is 14.536000000000001 with parameters: {'activation': 'relu', 'num_hidden_layer': 4, 'i': 9, 'learning_rate': 0.00036924207662998674}.\u001b[0m\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Score for fold 5: loss of 20.006519317626953\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 4s - loss: 15.5203 - mse: 15.5203 - mae: 1.6114 - val_loss: 17.1605 - val_mse: 17.1605 - val_mae: 1.6631 - lr: 1.2350e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 3s - loss: 14.8115 - mse: 14.8115 - mae: 1.5696 - val_loss: 16.9211 - val_mse: 16.9211 - val_mae: 1.6748 - lr: 1.2350e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 3s - loss: 14.6722 - mse: 14.6722 - mae: 1.5671 - val_loss: 16.8537 - val_mse: 16.8537 - val_mae: 1.7065 - lr: 1.2350e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 3s - loss: 14.6344 - mse: 14.6344 - mae: 1.5689 - val_loss: 16.9374 - val_mse: 16.9374 - val_mae: 1.6191 - lr: 1.2350e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 3s - loss: 14.6018 - mse: 14.6018 - mae: 1.5717 - val_loss: 16.8034 - val_mse: 16.8034 - val_mae: 1.6626 - lr: 1.2350e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 3s - loss: 14.6018 - mse: 14.6018 - mae: 1.5646 - val_loss: 16.8555 - val_mse: 16.8555 - val_mae: 1.6301 - lr: 1.2350e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 3s - loss: 14.5605 - mse: 14.5605 - mae: 1.5654 - val_loss: 16.8501 - val_mse: 16.8501 - val_mae: 1.6102 - lr: 1.2350e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 3s - loss: 14.5520 - mse: 14.5520 - mae: 1.5641 - val_loss: 16.7856 - val_mse: 16.7856 - val_mae: 1.6379 - lr: 1.2350e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 3s - loss: 14.5232 - mse: 14.5232 - mae: 1.5633 - val_loss: 16.8340 - val_mse: 16.8340 - val_mae: 1.6538 - lr: 1.2350e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 3s - loss: 14.5329 - mse: 14.5329 - mae: 1.5651 - val_loss: 16.8310 - val_mse: 16.8310 - val_mae: 1.6043 - lr: 1.2350e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 3s - loss: 14.5028 - mse: 14.5028 - mae: 1.5628 - val_loss: 16.8107 - val_mse: 16.8107 - val_mae: 1.6107 - lr: 1.2350e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 3s - loss: 14.4964 - mse: 14.4964 - mae: 1.5640 - val_loss: 16.6804 - val_mse: 16.6804 - val_mae: 1.6695 - lr: 1.2350e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 13/100\n",
            "1000/1000 - 3s - loss: 14.4995 - mse: 14.4995 - mae: 1.5596 - val_loss: 16.6754 - val_mse: 16.6754 - val_mae: 1.6420 - lr: 1.2350e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 14/100\n",
            "1000/1000 - 3s - loss: 14.4537 - mse: 14.4537 - mae: 1.5627 - val_loss: 16.6391 - val_mse: 16.6391 - val_mae: 1.6674 - lr: 1.2350e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 15/100\n",
            "1000/1000 - 3s - loss: 14.4379 - mse: 14.4379 - mae: 1.5574 - val_loss: 16.6928 - val_mse: 16.6928 - val_mae: 1.6166 - lr: 1.2350e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 16/100\n",
            "1000/1000 - 3s - loss: 14.4474 - mse: 14.4474 - mae: 1.5563 - val_loss: 16.8227 - val_mse: 16.8227 - val_mae: 1.6317 - lr: 1.2350e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 17/100\n",
            "1000/1000 - 3s - loss: 14.4128 - mse: 14.4128 - mae: 1.5580 - val_loss: 16.6320 - val_mse: 16.6320 - val_mae: 1.6607 - lr: 1.2350e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 18/100\n",
            "1000/1000 - 3s - loss: 14.4408 - mse: 14.4408 - mae: 1.5548 - val_loss: 16.6224 - val_mse: 16.6224 - val_mae: 1.6825 - lr: 1.2350e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 19/100\n",
            "1000/1000 - 3s - loss: 14.4216 - mse: 14.4216 - mae: 1.5540 - val_loss: 16.6803 - val_mse: 16.6803 - val_mae: 1.6398 - lr: 1.2350e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 20/100\n",
            "1000/1000 - 3s - loss: 14.3798 - mse: 14.3798 - mae: 1.5580 - val_loss: 16.7097 - val_mse: 16.7097 - val_mae: 1.5965 - lr: 1.2350e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 21/100\n",
            "1000/1000 - 3s - loss: 14.4117 - mse: 14.4117 - mae: 1.5525 - val_loss: 16.6624 - val_mse: 16.6624 - val_mae: 1.6171 - lr: 1.2350e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 22/100\n",
            "1000/1000 - 3s - loss: 14.3757 - mse: 14.3757 - mae: 1.5544 - val_loss: 16.6930 - val_mse: 16.6930 - val_mae: 1.6153 - lr: 1.2350e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 23/100\n",
            "1000/1000 - 3s - loss: 14.3623 - mse: 14.3623 - mae: 1.5551 - val_loss: 16.6584 - val_mse: 16.6584 - val_mae: 1.6235 - lr: 1.2350e-04 - 3s/epoch - 3ms/step\n",
            "Score for fold 1: loss of 16.658411026000977\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 3s - loss: 15.8535 - mse: 15.8535 - mae: 1.5820 - val_loss: 10.6050 - val_mse: 10.6050 - val_mae: 1.4943 - lr: 1.2350e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 3s - loss: 15.8297 - mse: 15.8297 - mae: 1.5811 - val_loss: 10.6300 - val_mse: 10.6300 - val_mae: 1.5896 - lr: 1.2350e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 3s - loss: 15.8077 - mse: 15.8077 - mae: 1.5817 - val_loss: 10.6727 - val_mse: 10.6727 - val_mae: 1.5797 - lr: 1.2350e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 3s - loss: 15.8202 - mse: 15.8202 - mae: 1.5793 - val_loss: 10.6634 - val_mse: 10.6634 - val_mae: 1.5263 - lr: 1.2350e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 3s - loss: 15.7652 - mse: 15.7652 - mae: 1.5780 - val_loss: 10.7495 - val_mse: 10.7495 - val_mae: 1.4884 - lr: 1.2350e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 3s - loss: 15.7926 - mse: 15.7926 - mae: 1.5762 - val_loss: 10.7104 - val_mse: 10.7104 - val_mae: 1.5056 - lr: 1.2350e-04 - 3s/epoch - 3ms/step\n",
            "Score for fold 2: loss of 10.710376739501953\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 3s - loss: 15.2956 - mse: 15.2956 - mae: 1.5714 - val_loss: 12.5099 - val_mse: 12.5099 - val_mae: 1.5772 - lr: 1.2350e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 3s - loss: 15.3132 - mse: 15.3132 - mae: 1.5727 - val_loss: 12.4916 - val_mse: 12.4916 - val_mae: 1.5329 - lr: 1.2350e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 3s - loss: 15.2750 - mse: 15.2750 - mae: 1.5711 - val_loss: 12.5303 - val_mse: 12.5303 - val_mae: 1.5436 - lr: 1.2350e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 3s - loss: 15.2712 - mse: 15.2712 - mae: 1.5702 - val_loss: 12.7176 - val_mse: 12.7176 - val_mae: 1.4917 - lr: 1.2350e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 3s - loss: 15.2727 - mse: 15.2727 - mae: 1.5679 - val_loss: 12.6458 - val_mse: 12.6458 - val_mae: 1.5125 - lr: 1.2350e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 3s - loss: 15.2739 - mse: 15.2739 - mae: 1.5669 - val_loss: 12.5697 - val_mse: 12.5697 - val_mae: 1.5798 - lr: 1.2350e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 3s - loss: 15.2166 - mse: 15.2166 - mae: 1.5686 - val_loss: 12.5353 - val_mse: 12.5353 - val_mae: 1.6160 - lr: 1.2350e-04 - 3s/epoch - 3ms/step\n",
            "Score for fold 3: loss of 12.535282135009766\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 3s - loss: 14.1625 - mse: 14.1625 - mae: 1.5685 - val_loss: 16.8519 - val_mse: 16.8519 - val_mae: 1.5524 - lr: 1.2350e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 3s - loss: 14.1618 - mse: 14.1618 - mae: 1.5686 - val_loss: 16.9306 - val_mse: 16.9306 - val_mae: 1.5782 - lr: 1.2350e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 3s - loss: 14.1786 - mse: 14.1786 - mae: 1.5650 - val_loss: 16.8830 - val_mse: 16.8830 - val_mae: 1.5772 - lr: 1.2350e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 3s - loss: 14.1320 - mse: 14.1320 - mae: 1.5688 - val_loss: 16.8984 - val_mse: 16.8984 - val_mae: 1.5649 - lr: 1.2350e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 3s - loss: 14.1023 - mse: 14.1023 - mae: 1.5650 - val_loss: 16.9950 - val_mse: 16.9950 - val_mae: 1.5337 - lr: 1.2350e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 3s - loss: 14.1195 - mse: 14.1195 - mae: 1.5675 - val_loss: 16.9763 - val_mse: 16.9763 - val_mae: 1.5594 - lr: 1.2350e-04 - 3s/epoch - 3ms/step\n",
            "Score for fold 4: loss of 16.976289749145508\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 3s - loss: 14.1086 - mse: 14.1086 - mae: 1.5456 - val_loss: 16.9054 - val_mse: 16.9054 - val_mae: 1.6411 - lr: 1.2350e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 3s - loss: 14.0767 - mse: 14.0767 - mae: 1.5506 - val_loss: 16.8904 - val_mse: 16.8904 - val_mae: 1.5953 - lr: 1.2350e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 3s - loss: 14.0725 - mse: 14.0725 - mae: 1.5498 - val_loss: 17.0784 - val_mse: 17.0784 - val_mae: 1.5716 - lr: 1.2350e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 3s - loss: 14.0719 - mse: 14.0719 - mae: 1.5466 - val_loss: 17.2504 - val_mse: 17.2504 - val_mae: 1.5579 - lr: 1.2350e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 3s - loss: 14.0459 - mse: 14.0459 - mae: 1.5477 - val_loss: 17.0054 - val_mse: 17.0054 - val_mae: 1.6106 - lr: 1.2350e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 3s - loss: 14.0403 - mse: 14.0403 - mae: 1.5459 - val_loss: 17.1436 - val_mse: 17.1436 - val_mae: 1.5873 - lr: 1.2350e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 3s - loss: 14.0385 - mse: 14.0385 - mae: 1.5430 - val_loss: 16.9571 - val_mse: 16.9571 - val_mae: 1.5951 - lr: 1.2350e-04 - 3s/epoch - 3ms/step\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-10-16 06:39:56,474]\u001b[0m Finished trial#15 resulted in value: 14.77. Current best value is 14.536000000000001 with parameters: {'activation': 'relu', 'num_hidden_layer': 4, 'i': 9, 'learning_rate': 0.00036924207662998674}.\u001b[0m\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Score for fold 5: loss of 16.957067489624023\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 6s - loss: 15.0154 - mse: 15.0154 - mae: 1.6006 - val_loss: 16.8333 - val_mse: 16.8333 - val_mae: 1.6050 - lr: 1.7436e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 5s - loss: 14.6141 - mse: 14.6141 - mae: 1.5780 - val_loss: 16.9589 - val_mse: 16.9589 - val_mae: 1.6528 - lr: 1.7436e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 6s - loss: 14.5639 - mse: 14.5639 - mae: 1.5748 - val_loss: 17.1814 - val_mse: 17.1814 - val_mae: 1.5904 - lr: 1.7436e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 6s - loss: 14.5453 - mse: 14.5453 - mae: 1.5776 - val_loss: 16.7837 - val_mse: 16.7837 - val_mae: 1.6002 - lr: 1.7436e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 6s - loss: 14.5290 - mse: 14.5290 - mae: 1.5752 - val_loss: 16.8942 - val_mse: 16.8942 - val_mae: 1.5739 - lr: 1.7436e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 6s - loss: 14.4636 - mse: 14.4636 - mae: 1.5731 - val_loss: 16.6628 - val_mse: 16.6628 - val_mae: 1.6216 - lr: 1.7436e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 6s - loss: 14.3803 - mse: 14.3803 - mae: 1.5670 - val_loss: 16.6994 - val_mse: 16.6994 - val_mae: 1.5899 - lr: 1.7436e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 6s - loss: 14.3205 - mse: 14.3205 - mae: 1.5636 - val_loss: 16.8153 - val_mse: 16.8153 - val_mae: 1.5948 - lr: 1.7436e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 6s - loss: 14.3276 - mse: 14.3276 - mae: 1.5623 - val_loss: 16.6052 - val_mse: 16.6052 - val_mae: 1.6577 - lr: 1.7436e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 6s - loss: 14.2668 - mse: 14.2668 - mae: 1.5606 - val_loss: 16.4682 - val_mse: 16.4682 - val_mae: 1.5936 - lr: 1.7436e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 6s - loss: 14.2305 - mse: 14.2305 - mae: 1.5607 - val_loss: 17.3664 - val_mse: 17.3664 - val_mae: 1.7495 - lr: 1.7436e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 6s - loss: 14.1920 - mse: 14.1920 - mae: 1.5573 - val_loss: 17.0279 - val_mse: 17.0279 - val_mae: 1.5867 - lr: 1.7436e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 13/100\n",
            "1000/1000 - 6s - loss: 14.1356 - mse: 14.1356 - mae: 1.5532 - val_loss: 17.2428 - val_mse: 17.2428 - val_mae: 1.6840 - lr: 1.7436e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 14/100\n",
            "1000/1000 - 6s - loss: 14.0805 - mse: 14.0805 - mae: 1.5569 - val_loss: 16.7464 - val_mse: 16.7464 - val_mae: 1.6788 - lr: 1.7436e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 15/100\n",
            "1000/1000 - 6s - loss: 13.9713 - mse: 13.9713 - mae: 1.5472 - val_loss: 16.8739 - val_mse: 16.8739 - val_mae: 1.5908 - lr: 1.7436e-04 - 6s/epoch - 6ms/step\n",
            "Score for fold 1: loss of 16.873952865600586\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 6s - loss: 15.0455 - mse: 15.0455 - mae: 1.5632 - val_loss: 12.4573 - val_mse: 12.4573 - val_mae: 1.7125 - lr: 1.7436e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 6s - loss: 14.8329 - mse: 14.8329 - mae: 1.5663 - val_loss: 12.3648 - val_mse: 12.3648 - val_mae: 1.6731 - lr: 1.7436e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 6s - loss: 14.8225 - mse: 14.8225 - mae: 1.5605 - val_loss: 12.3443 - val_mse: 12.3443 - val_mae: 1.5556 - lr: 1.7436e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 6s - loss: 14.7377 - mse: 14.7377 - mae: 1.5581 - val_loss: 12.5428 - val_mse: 12.5428 - val_mae: 1.5200 - lr: 1.7436e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 6s - loss: 14.7661 - mse: 14.7661 - mae: 1.5523 - val_loss: 12.5215 - val_mse: 12.5215 - val_mae: 1.6045 - lr: 1.7436e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 6s - loss: 14.6252 - mse: 14.6252 - mae: 1.5481 - val_loss: 12.5097 - val_mse: 12.5097 - val_mae: 1.5613 - lr: 1.7436e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 6s - loss: 14.4811 - mse: 14.4811 - mae: 1.5485 - val_loss: 12.6699 - val_mse: 12.6699 - val_mae: 1.5763 - lr: 1.7436e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 6s - loss: 14.4899 - mse: 14.4899 - mae: 1.5478 - val_loss: 12.7432 - val_mse: 12.7432 - val_mae: 1.5673 - lr: 1.7436e-04 - 6s/epoch - 6ms/step\n",
            "Score for fold 2: loss of 12.74319076538086\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 6s - loss: 13.3203 - mse: 13.3203 - mae: 1.5560 - val_loss: 16.5438 - val_mse: 16.5438 - val_mae: 1.4990 - lr: 1.7436e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 6s - loss: 13.4200 - mse: 13.4200 - mae: 1.5559 - val_loss: 16.6859 - val_mse: 16.6859 - val_mae: 1.6482 - lr: 1.7436e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 6s - loss: 13.2587 - mse: 13.2587 - mae: 1.5453 - val_loss: 16.6264 - val_mse: 16.6264 - val_mae: 1.5192 - lr: 1.7436e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 6s - loss: 13.3236 - mse: 13.3236 - mae: 1.5500 - val_loss: 16.6028 - val_mse: 16.6028 - val_mae: 1.5533 - lr: 1.7436e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 6s - loss: 13.1383 - mse: 13.1383 - mae: 1.5550 - val_loss: 17.0147 - val_mse: 17.0147 - val_mae: 1.6551 - lr: 1.7436e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 6s - loss: 12.9731 - mse: 12.9731 - mae: 1.5366 - val_loss: 16.9670 - val_mse: 16.9670 - val_mae: 1.4879 - lr: 1.7436e-04 - 6s/epoch - 6ms/step\n",
            "Score for fold 3: loss of 16.96696662902832\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 5s - loss: 13.5864 - mse: 13.5864 - mae: 1.5576 - val_loss: 15.2107 - val_mse: 15.2107 - val_mae: 1.6840 - lr: 1.7436e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 6s - loss: 13.3723 - mse: 13.3723 - mae: 1.5617 - val_loss: 15.1312 - val_mse: 15.1312 - val_mae: 1.5377 - lr: 1.7436e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 6s - loss: 13.2612 - mse: 13.2612 - mae: 1.5522 - val_loss: 14.9346 - val_mse: 14.9346 - val_mae: 1.5263 - lr: 1.7436e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 6s - loss: 13.0925 - mse: 13.0925 - mae: 1.5552 - val_loss: 15.2806 - val_mse: 15.2806 - val_mae: 1.6591 - lr: 1.7436e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 6s - loss: 13.2505 - mse: 13.2505 - mae: 1.5574 - val_loss: 15.5795 - val_mse: 15.5795 - val_mae: 1.7980 - lr: 1.7436e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 6s - loss: 12.9162 - mse: 12.9162 - mae: 1.5451 - val_loss: 15.6861 - val_mse: 15.6861 - val_mae: 1.4750 - lr: 1.7436e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 6s - loss: 12.8886 - mse: 12.8886 - mae: 1.5647 - val_loss: 15.6705 - val_mse: 15.6705 - val_mae: 1.8185 - lr: 1.7436e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 6s - loss: 12.9115 - mse: 12.9115 - mae: 1.5669 - val_loss: 15.6676 - val_mse: 15.6676 - val_mae: 1.4820 - lr: 1.7436e-04 - 6s/epoch - 6ms/step\n",
            "Score for fold 4: loss of 15.667611122131348\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 6s - loss: 14.5194 - mse: 14.5194 - mae: 1.5859 - val_loss: 9.6749 - val_mse: 9.6749 - val_mae: 1.3684 - lr: 1.7436e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 6s - loss: 14.3150 - mse: 14.3150 - mae: 1.5942 - val_loss: 9.6756 - val_mse: 9.6756 - val_mae: 1.4306 - lr: 1.7436e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 6s - loss: 14.3383 - mse: 14.3383 - mae: 1.5831 - val_loss: 9.7141 - val_mse: 9.7141 - val_mae: 1.4187 - lr: 1.7436e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 6s - loss: 14.2218 - mse: 14.2218 - mae: 1.5748 - val_loss: 10.1022 - val_mse: 10.1022 - val_mae: 1.4453 - lr: 1.7436e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 6s - loss: 14.1473 - mse: 14.1473 - mae: 1.5881 - val_loss: 9.9907 - val_mse: 9.9907 - val_mae: 1.5231 - lr: 1.7436e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 6s - loss: 14.1654 - mse: 14.1654 - mae: 1.6070 - val_loss: 9.9864 - val_mse: 9.9864 - val_mae: 1.4163 - lr: 1.7436e-04 - 6s/epoch - 6ms/step\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-10-16 06:44:11,633]\u001b[0m Finished trial#16 resulted in value: 14.447999999999999. Current best value is 14.447999999999999 with parameters: {'activation': 'relu', 'num_hidden_layer': 4, 'i': 9, 'learning_rate': 0.00017436499225447727}.\u001b[0m\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Score for fold 5: loss of 9.986413955688477\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 8s - loss: 16.2939 - mse: 16.2939 - mae: 1.6187 - val_loss: 12.1599 - val_mse: 12.1599 - val_mae: 1.5726 - lr: 2.2560e-04 - 8s/epoch - 8ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 8s - loss: 15.8936 - mse: 15.8936 - mae: 1.5934 - val_loss: 12.1700 - val_mse: 12.1700 - val_mae: 1.6333 - lr: 2.2560e-04 - 8s/epoch - 8ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 8s - loss: 15.7866 - mse: 15.7866 - mae: 1.5938 - val_loss: 12.1320 - val_mse: 12.1320 - val_mae: 1.5549 - lr: 2.2560e-04 - 8s/epoch - 8ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 8s - loss: 15.7328 - mse: 15.7328 - mae: 1.5893 - val_loss: 12.0757 - val_mse: 12.0757 - val_mae: 1.6503 - lr: 2.2560e-04 - 8s/epoch - 8ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 8s - loss: 15.6808 - mse: 15.6808 - mae: 1.5878 - val_loss: 12.4335 - val_mse: 12.4335 - val_mae: 1.5543 - lr: 2.2560e-04 - 8s/epoch - 8ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 8s - loss: 15.6921 - mse: 15.6921 - mae: 1.5822 - val_loss: 12.3461 - val_mse: 12.3461 - val_mae: 1.5893 - lr: 2.2560e-04 - 8s/epoch - 8ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 7s - loss: 15.6585 - mse: 15.6585 - mae: 1.5816 - val_loss: 12.0877 - val_mse: 12.0877 - val_mae: 1.5537 - lr: 2.2560e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 7s - loss: 15.6452 - mse: 15.6452 - mae: 1.5775 - val_loss: 12.0495 - val_mse: 12.0495 - val_mae: 1.5588 - lr: 2.2560e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 7s - loss: 15.6100 - mse: 15.6100 - mae: 1.5760 - val_loss: 12.2542 - val_mse: 12.2542 - val_mae: 1.5229 - lr: 2.2560e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 7s - loss: 15.6254 - mse: 15.6254 - mae: 1.5705 - val_loss: 11.9941 - val_mse: 11.9941 - val_mae: 1.5985 - lr: 2.2560e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 7s - loss: 15.5301 - mse: 15.5301 - mae: 1.5726 - val_loss: 12.1040 - val_mse: 12.1040 - val_mae: 1.5835 - lr: 2.2560e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 8s - loss: 15.5256 - mse: 15.5256 - mae: 1.5697 - val_loss: 12.0045 - val_mse: 12.0045 - val_mae: 1.6836 - lr: 2.2560e-04 - 8s/epoch - 8ms/step\n",
            "Epoch 13/100\n",
            "1000/1000 - 8s - loss: 15.5572 - mse: 15.5572 - mae: 1.5692 - val_loss: 12.2041 - val_mse: 12.2041 - val_mae: 1.5845 - lr: 2.2560e-04 - 8s/epoch - 8ms/step\n",
            "Epoch 14/100\n",
            "1000/1000 - 8s - loss: 15.4933 - mse: 15.4933 - mae: 1.5667 - val_loss: 11.9565 - val_mse: 11.9565 - val_mae: 1.6250 - lr: 2.2560e-04 - 8s/epoch - 8ms/step\n",
            "Epoch 15/100\n",
            "1000/1000 - 7s - loss: 15.4014 - mse: 15.4014 - mae: 1.5646 - val_loss: 12.2121 - val_mse: 12.2121 - val_mae: 1.5698 - lr: 2.2560e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 16/100\n",
            "1000/1000 - 7s - loss: 15.4983 - mse: 15.4983 - mae: 1.5622 - val_loss: 12.4165 - val_mse: 12.4165 - val_mae: 1.5287 - lr: 2.2560e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 17/100\n",
            "1000/1000 - 8s - loss: 15.4383 - mse: 15.4383 - mae: 1.5627 - val_loss: 12.1797 - val_mse: 12.1797 - val_mae: 1.5585 - lr: 2.2560e-04 - 8s/epoch - 8ms/step\n",
            "Epoch 18/100\n",
            "1000/1000 - 7s - loss: 15.4125 - mse: 15.4125 - mae: 1.5618 - val_loss: 12.1421 - val_mse: 12.1421 - val_mae: 1.5840 - lr: 2.2560e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 19/100\n",
            "1000/1000 - 7s - loss: 15.5032 - mse: 15.5032 - mae: 1.5600 - val_loss: 12.0674 - val_mse: 12.0674 - val_mae: 1.5706 - lr: 2.2560e-04 - 7s/epoch - 7ms/step\n",
            "Score for fold 1: loss of 12.067401885986328\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 7s - loss: 13.2105 - mse: 13.2105 - mae: 1.5658 - val_loss: 20.8992 - val_mse: 20.8992 - val_mae: 1.6053 - lr: 2.2560e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 7s - loss: 13.0688 - mse: 13.0688 - mae: 1.5630 - val_loss: 20.8162 - val_mse: 20.8162 - val_mae: 1.5599 - lr: 2.2560e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 7s - loss: 13.0127 - mse: 13.0127 - mae: 1.5580 - val_loss: 21.0064 - val_mse: 21.0064 - val_mae: 1.5130 - lr: 2.2560e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 8s - loss: 13.1149 - mse: 13.1149 - mae: 1.5636 - val_loss: 20.9158 - val_mse: 20.9158 - val_mae: 1.6352 - lr: 2.2560e-04 - 8s/epoch - 8ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 7s - loss: 12.9909 - mse: 12.9909 - mae: 1.5599 - val_loss: 20.9352 - val_mse: 20.9352 - val_mae: 1.5942 - lr: 2.2560e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 8s - loss: 12.9609 - mse: 12.9609 - mae: 1.5537 - val_loss: 20.8400 - val_mse: 20.8400 - val_mae: 1.5545 - lr: 2.2560e-04 - 8s/epoch - 8ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 7s - loss: 12.9254 - mse: 12.9254 - mae: 1.5577 - val_loss: 21.1133 - val_mse: 21.1133 - val_mae: 1.4954 - lr: 2.2560e-04 - 7s/epoch - 7ms/step\n",
            "Score for fold 2: loss of 21.11332893371582\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 7s - loss: 14.5951 - mse: 14.5951 - mae: 1.5510 - val_loss: 14.6912 - val_mse: 14.6912 - val_mae: 1.5432 - lr: 2.2560e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 7s - loss: 14.5144 - mse: 14.5144 - mae: 1.5464 - val_loss: 14.8710 - val_mse: 14.8710 - val_mae: 1.6080 - lr: 2.2560e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 8s - loss: 14.5370 - mse: 14.5370 - mae: 1.5429 - val_loss: 14.7419 - val_mse: 14.7419 - val_mae: 1.5478 - lr: 2.2560e-04 - 8s/epoch - 8ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 8s - loss: 14.4786 - mse: 14.4786 - mae: 1.5437 - val_loss: 14.6717 - val_mse: 14.6717 - val_mae: 1.5986 - lr: 2.2560e-04 - 8s/epoch - 8ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 8s - loss: 14.4338 - mse: 14.4338 - mae: 1.5407 - val_loss: 14.9317 - val_mse: 14.9317 - val_mae: 1.5617 - lr: 2.2560e-04 - 8s/epoch - 8ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 8s - loss: 14.4535 - mse: 14.4535 - mae: 1.5395 - val_loss: 14.6073 - val_mse: 14.6073 - val_mae: 1.5735 - lr: 2.2560e-04 - 8s/epoch - 8ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 8s - loss: 14.4435 - mse: 14.4435 - mae: 1.5406 - val_loss: 14.5504 - val_mse: 14.5504 - val_mae: 1.5746 - lr: 2.2560e-04 - 8s/epoch - 8ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 8s - loss: 14.4293 - mse: 14.4293 - mae: 1.5343 - val_loss: 14.5579 - val_mse: 14.5579 - val_mae: 1.5550 - lr: 2.2560e-04 - 8s/epoch - 8ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 8s - loss: 14.2846 - mse: 14.2846 - mae: 1.5322 - val_loss: 14.8235 - val_mse: 14.8235 - val_mae: 1.5974 - lr: 2.2560e-04 - 8s/epoch - 8ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 8s - loss: 14.3214 - mse: 14.3214 - mae: 1.5350 - val_loss: 14.6518 - val_mse: 14.6518 - val_mae: 1.6057 - lr: 2.2560e-04 - 8s/epoch - 8ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 8s - loss: 14.2179 - mse: 14.2179 - mae: 1.5306 - val_loss: 15.0370 - val_mse: 15.0370 - val_mae: 1.6067 - lr: 2.2560e-04 - 8s/epoch - 8ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 8s - loss: 14.2466 - mse: 14.2466 - mae: 1.5287 - val_loss: 14.8320 - val_mse: 14.8320 - val_mae: 1.5631 - lr: 2.2560e-04 - 8s/epoch - 8ms/step\n",
            "Score for fold 3: loss of 14.831955909729004\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 8s - loss: 14.4044 - mse: 14.4044 - mae: 1.5433 - val_loss: 13.7973 - val_mse: 13.7973 - val_mae: 1.5137 - lr: 2.2560e-04 - 8s/epoch - 8ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 7s - loss: 14.5090 - mse: 14.5090 - mae: 1.5417 - val_loss: 13.9974 - val_mse: 13.9974 - val_mae: 1.5414 - lr: 2.2560e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 8s - loss: 14.3172 - mse: 14.3172 - mae: 1.5373 - val_loss: 13.9751 - val_mse: 13.9751 - val_mae: 1.5527 - lr: 2.2560e-04 - 8s/epoch - 8ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 7s - loss: 14.2890 - mse: 14.2890 - mae: 1.5298 - val_loss: 14.1970 - val_mse: 14.1970 - val_mae: 1.5130 - lr: 2.2560e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 8s - loss: 14.2171 - mse: 14.2171 - mae: 1.5378 - val_loss: 14.0995 - val_mse: 14.0995 - val_mae: 1.5552 - lr: 2.2560e-04 - 8s/epoch - 8ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 7s - loss: 14.1048 - mse: 14.1048 - mae: 1.5279 - val_loss: 14.1712 - val_mse: 14.1712 - val_mae: 1.5434 - lr: 2.2560e-04 - 7s/epoch - 7ms/step\n",
            "Score for fold 4: loss of 14.171189308166504\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 8s - loss: 15.0367 - mse: 15.0367 - mae: 1.5326 - val_loss: 11.1514 - val_mse: 11.1514 - val_mae: 1.5722 - lr: 2.2560e-04 - 8s/epoch - 8ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 7s - loss: 14.9119 - mse: 14.9119 - mae: 1.5253 - val_loss: 11.5039 - val_mse: 11.5039 - val_mae: 1.5063 - lr: 2.2560e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 7s - loss: 14.8786 - mse: 14.8786 - mae: 1.5179 - val_loss: 11.3691 - val_mse: 11.3691 - val_mae: 1.5543 - lr: 2.2560e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 7s - loss: 14.7213 - mse: 14.7213 - mae: 1.5198 - val_loss: 11.6697 - val_mse: 11.6697 - val_mae: 1.5078 - lr: 2.2560e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 8s - loss: 14.8082 - mse: 14.8082 - mae: 1.5160 - val_loss: 11.6602 - val_mse: 11.6602 - val_mae: 1.5553 - lr: 2.2560e-04 - 8s/epoch - 8ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 7s - loss: 14.7360 - mse: 14.7360 - mae: 1.5137 - val_loss: 11.6576 - val_mse: 11.6576 - val_mae: 1.5555 - lr: 2.2560e-04 - 7s/epoch - 7ms/step\n",
            "Score for fold 5: loss of 11.65762710571289\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-10-16 06:50:38,081]\u001b[0m Finished trial#17 resulted in value: 14.768. Current best value is 14.447999999999999 with parameters: {'activation': 'relu', 'num_hidden_layer': 4, 'i': 9, 'learning_rate': 0.00017436499225447727}.\u001b[0m\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 3s - loss: 16.1588 - mse: 16.1588 - mae: 1.6336 - val_loss: 12.3531 - val_mse: 12.3531 - val_mae: 1.6063 - lr: 0.0019 - 3s/epoch - 3ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 2s - loss: 15.7893 - mse: 15.7893 - mae: 1.6119 - val_loss: 12.3812 - val_mse: 12.3812 - val_mae: 1.6217 - lr: 0.0019 - 2s/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 3s - loss: 15.6767 - mse: 15.6767 - mae: 1.6051 - val_loss: 12.4853 - val_mse: 12.4853 - val_mae: 1.5354 - lr: 0.0019 - 3s/epoch - 3ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 3s - loss: 15.7045 - mse: 15.7045 - mae: 1.6013 - val_loss: 12.5719 - val_mse: 12.5719 - val_mae: 1.5654 - lr: 0.0019 - 3s/epoch - 3ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 3s - loss: 15.7596 - mse: 15.7596 - mae: 1.5985 - val_loss: 12.3480 - val_mse: 12.3480 - val_mae: 1.7146 - lr: 0.0019 - 3s/epoch - 3ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 3s - loss: 15.6909 - mse: 15.6909 - mae: 1.5940 - val_loss: 12.5794 - val_mse: 12.5794 - val_mae: 1.5089 - lr: 0.0019 - 3s/epoch - 3ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 3s - loss: 15.7178 - mse: 15.7178 - mae: 1.5977 - val_loss: 12.2926 - val_mse: 12.2926 - val_mae: 1.5463 - lr: 0.0019 - 3s/epoch - 3ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 3s - loss: 15.6974 - mse: 15.6974 - mae: 1.5990 - val_loss: 12.3612 - val_mse: 12.3612 - val_mae: 1.5165 - lr: 0.0019 - 3s/epoch - 3ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 3s - loss: 15.6713 - mse: 15.6713 - mae: 1.6050 - val_loss: 12.2848 - val_mse: 12.2848 - val_mae: 1.6537 - lr: 0.0019 - 3s/epoch - 3ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 3s - loss: 15.7206 - mse: 15.7206 - mae: 1.5991 - val_loss: 12.4655 - val_mse: 12.4655 - val_mae: 1.5750 - lr: 0.0019 - 3s/epoch - 3ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 3s - loss: 15.6725 - mse: 15.6725 - mae: 1.6078 - val_loss: 12.3764 - val_mse: 12.3764 - val_mae: 1.6660 - lr: 0.0019 - 3s/epoch - 3ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 3s - loss: 15.6676 - mse: 15.6676 - mae: 1.6075 - val_loss: 12.4164 - val_mse: 12.4164 - val_mae: 1.6544 - lr: 0.0019 - 3s/epoch - 3ms/step\n",
            "Epoch 13/100\n",
            "1000/1000 - 3s - loss: 15.5955 - mse: 15.5955 - mae: 1.6086 - val_loss: 12.4108 - val_mse: 12.4108 - val_mae: 1.5268 - lr: 0.0019 - 3s/epoch - 3ms/step\n",
            "Epoch 14/100\n",
            "1000/1000 - 3s - loss: 15.3991 - mse: 15.3991 - mae: 1.6104 - val_loss: 12.3974 - val_mse: 12.3974 - val_mae: 1.6523 - lr: 0.0019 - 3s/epoch - 3ms/step\n",
            "Score for fold 1: loss of 12.397398948669434\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 3s - loss: 14.5418 - mse: 14.5418 - mae: 1.5876 - val_loss: 16.3464 - val_mse: 16.3464 - val_mae: 1.5281 - lr: 0.0010 - 3s/epoch - 3ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 3s - loss: 14.5013 - mse: 14.5013 - mae: 1.5787 - val_loss: 16.2091 - val_mse: 16.2091 - val_mae: 1.6182 - lr: 0.0010 - 3s/epoch - 3ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 3s - loss: 14.4590 - mse: 14.4590 - mae: 1.5795 - val_loss: 16.9935 - val_mse: 16.9935 - val_mae: 1.5925 - lr: 0.0010 - 3s/epoch - 3ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 3s - loss: 14.4164 - mse: 14.4164 - mae: 1.5825 - val_loss: 16.1363 - val_mse: 16.1363 - val_mae: 1.6583 - lr: 0.0010 - 3s/epoch - 3ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 3s - loss: 14.3791 - mse: 14.3791 - mae: 1.5773 - val_loss: 16.3449 - val_mse: 16.3449 - val_mae: 1.7336 - lr: 0.0010 - 3s/epoch - 3ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 3s - loss: 14.4038 - mse: 14.4038 - mae: 1.5866 - val_loss: 16.2528 - val_mse: 16.2528 - val_mae: 1.6829 - lr: 0.0010 - 3s/epoch - 3ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 3s - loss: 14.3655 - mse: 14.3655 - mae: 1.5835 - val_loss: 16.0857 - val_mse: 16.0857 - val_mae: 1.7099 - lr: 0.0010 - 3s/epoch - 3ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 3s - loss: 14.3179 - mse: 14.3179 - mae: 1.5766 - val_loss: 15.9037 - val_mse: 15.9037 - val_mae: 1.6492 - lr: 0.0010 - 3s/epoch - 3ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 3s - loss: 14.2601 - mse: 14.2601 - mae: 1.5764 - val_loss: 16.3463 - val_mse: 16.3463 - val_mae: 1.4981 - lr: 0.0010 - 3s/epoch - 3ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 3s - loss: 14.2667 - mse: 14.2667 - mae: 1.5780 - val_loss: 16.7083 - val_mse: 16.7083 - val_mae: 1.6812 - lr: 0.0010 - 3s/epoch - 3ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 3s - loss: 14.2738 - mse: 14.2738 - mae: 1.5813 - val_loss: 15.8759 - val_mse: 15.8759 - val_mae: 1.6007 - lr: 0.0010 - 3s/epoch - 3ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 3s - loss: 14.2661 - mse: 14.2661 - mae: 1.5760 - val_loss: 16.6005 - val_mse: 16.6005 - val_mae: 1.5663 - lr: 0.0010 - 3s/epoch - 3ms/step\n",
            "Epoch 13/100\n",
            "1000/1000 - 3s - loss: 14.2660 - mse: 14.2660 - mae: 1.5781 - val_loss: 15.8676 - val_mse: 15.8676 - val_mae: 1.6747 - lr: 0.0010 - 3s/epoch - 3ms/step\n",
            "Epoch 14/100\n",
            "1000/1000 - 3s - loss: 14.2392 - mse: 14.2392 - mae: 1.5763 - val_loss: 15.9882 - val_mse: 15.9882 - val_mae: 1.5271 - lr: 0.0010 - 3s/epoch - 3ms/step\n",
            "Epoch 15/100\n",
            "1000/1000 - 3s - loss: 14.1845 - mse: 14.1845 - mae: 1.5709 - val_loss: 16.5873 - val_mse: 16.5873 - val_mae: 1.5808 - lr: 0.0010 - 3s/epoch - 3ms/step\n",
            "Epoch 16/100\n",
            "1000/1000 - 3s - loss: 14.2547 - mse: 14.2547 - mae: 1.5749 - val_loss: 16.2574 - val_mse: 16.2574 - val_mae: 1.5382 - lr: 0.0010 - 3s/epoch - 3ms/step\n",
            "Epoch 17/100\n",
            "1000/1000 - 3s - loss: 14.2548 - mse: 14.2548 - mae: 1.5740 - val_loss: 16.5319 - val_mse: 16.5319 - val_mae: 1.5086 - lr: 0.0010 - 3s/epoch - 3ms/step\n",
            "Epoch 18/100\n",
            "1000/1000 - 3s - loss: 14.2968 - mse: 14.2968 - mae: 1.5823 - val_loss: 16.3962 - val_mse: 16.3962 - val_mae: 1.6768 - lr: 0.0010 - 3s/epoch - 3ms/step\n",
            "Score for fold 2: loss of 16.396240234375\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 3s - loss: 14.6736 - mse: 14.6736 - mae: 1.5960 - val_loss: 14.8465 - val_mse: 14.8465 - val_mae: 1.6167 - lr: 0.0010 - 3s/epoch - 3ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 3s - loss: 14.4813 - mse: 14.4813 - mae: 1.6093 - val_loss: 14.7247 - val_mse: 14.7247 - val_mae: 1.6244 - lr: 0.0010 - 3s/epoch - 3ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 3s - loss: 14.4757 - mse: 14.4757 - mae: 1.5953 - val_loss: 14.7399 - val_mse: 14.7399 - val_mae: 1.5499 - lr: 0.0010 - 3s/epoch - 3ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 3s - loss: 14.4266 - mse: 14.4266 - mae: 1.5865 - val_loss: 14.9025 - val_mse: 14.9025 - val_mae: 1.5652 - lr: 0.0010 - 3s/epoch - 3ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 3s - loss: 14.5383 - mse: 14.5383 - mae: 1.5867 - val_loss: 14.8751 - val_mse: 14.8751 - val_mae: 1.5411 - lr: 0.0010 - 3s/epoch - 3ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 3s - loss: 14.4962 - mse: 14.4962 - mae: 1.6090 - val_loss: 15.2673 - val_mse: 15.2673 - val_mae: 1.4743 - lr: 0.0010 - 3s/epoch - 3ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 3s - loss: 14.2434 - mse: 14.2434 - mae: 1.5981 - val_loss: 14.9285 - val_mse: 14.9285 - val_mae: 1.6290 - lr: 0.0010 - 3s/epoch - 3ms/step\n",
            "Score for fold 3: loss of 14.92847728729248\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 3s - loss: 15.1675 - mse: 15.1675 - mae: 1.6032 - val_loss: 11.7695 - val_mse: 11.7695 - val_mae: 1.5712 - lr: 0.0010 - 3s/epoch - 3ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 3s - loss: 15.1558 - mse: 15.1558 - mae: 1.6190 - val_loss: 12.2483 - val_mse: 12.2483 - val_mae: 1.5108 - lr: 0.0010 - 3s/epoch - 3ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 3s - loss: 15.3267 - mse: 15.3267 - mae: 1.6132 - val_loss: 11.9954 - val_mse: 11.9954 - val_mae: 1.6296 - lr: 0.0010 - 3s/epoch - 3ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 3s - loss: 15.3790 - mse: 15.3790 - mae: 1.6305 - val_loss: 12.9807 - val_mse: 12.9807 - val_mae: 1.5514 - lr: 0.0010 - 3s/epoch - 3ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 3s - loss: 15.2936 - mse: 15.2936 - mae: 1.6300 - val_loss: 12.0116 - val_mse: 12.0116 - val_mae: 1.5808 - lr: 0.0010 - 3s/epoch - 3ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 3s - loss: 15.1948 - mse: 15.1948 - mae: 1.6262 - val_loss: 12.3292 - val_mse: 12.3292 - val_mae: 1.6978 - lr: 0.0010 - 3s/epoch - 3ms/step\n",
            "Score for fold 4: loss of 12.329181671142578\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 3s - loss: 13.7039 - mse: 13.7039 - mae: 1.6398 - val_loss: 18.1737 - val_mse: 18.1737 - val_mae: 1.5596 - lr: 0.0010 - 3s/epoch - 3ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 3s - loss: 13.8148 - mse: 13.8148 - mae: 1.6389 - val_loss: 18.1206 - val_mse: 18.1206 - val_mae: 1.7119 - lr: 0.0010 - 3s/epoch - 3ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 3s - loss: 13.7812 - mse: 13.7812 - mae: 1.6583 - val_loss: 18.9250 - val_mse: 18.9250 - val_mae: 1.4803 - lr: 0.0010 - 3s/epoch - 3ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 3s - loss: 13.8161 - mse: 13.8161 - mae: 1.6488 - val_loss: 18.1956 - val_mse: 18.1956 - val_mae: 1.5746 - lr: 0.0010 - 3s/epoch - 3ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 3s - loss: 13.8648 - mse: 13.8648 - mae: 1.6565 - val_loss: 18.2415 - val_mse: 18.2415 - val_mae: 1.6968 - lr: 0.0010 - 3s/epoch - 3ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 3s - loss: 13.8499 - mse: 13.8499 - mae: 1.6450 - val_loss: 18.3331 - val_mse: 18.3331 - val_mae: 1.7220 - lr: 0.0010 - 3s/epoch - 3ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 3s - loss: 13.9350 - mse: 13.9350 - mae: 1.6604 - val_loss: 18.4952 - val_mse: 18.4952 - val_mae: 1.4728 - lr: 0.0010 - 3s/epoch - 3ms/step\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-10-16 06:53:05,544]\u001b[0m Finished trial#18 resulted in value: 14.912. Current best value is 14.447999999999999 with parameters: {'activation': 'relu', 'num_hidden_layer': 4, 'i': 9, 'learning_rate': 0.00017436499225447727}.\u001b[0m\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Score for fold 5: loss of 18.495206832885742\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 6s - loss: 16.4819 - mse: 16.4819 - mae: 1.6214 - val_loss: 11.7314 - val_mse: 11.7314 - val_mae: 1.6154 - lr: 1.0214e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 6s - loss: 15.9226 - mse: 15.9226 - mae: 1.5875 - val_loss: 11.7174 - val_mse: 11.7174 - val_mae: 1.6046 - lr: 1.0214e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 6s - loss: 15.8628 - mse: 15.8628 - mae: 1.5843 - val_loss: 11.6863 - val_mse: 11.6863 - val_mae: 1.5894 - lr: 1.0214e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 6s - loss: 15.8347 - mse: 15.8347 - mae: 1.5822 - val_loss: 11.6153 - val_mse: 11.6153 - val_mae: 1.6607 - lr: 1.0214e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 6s - loss: 15.7609 - mse: 15.7609 - mae: 1.5830 - val_loss: 11.6013 - val_mse: 11.6013 - val_mae: 1.5901 - lr: 1.0214e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 6s - loss: 15.7378 - mse: 15.7378 - mae: 1.5776 - val_loss: 11.5383 - val_mse: 11.5383 - val_mae: 1.5994 - lr: 1.0214e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 6s - loss: 15.7213 - mse: 15.7213 - mae: 1.5802 - val_loss: 11.6045 - val_mse: 11.6045 - val_mae: 1.6844 - lr: 1.0214e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 6s - loss: 15.7379 - mse: 15.7379 - mae: 1.5785 - val_loss: 11.5248 - val_mse: 11.5248 - val_mae: 1.5785 - lr: 1.0214e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 6s - loss: 15.7113 - mse: 15.7113 - mae: 1.5726 - val_loss: 11.5601 - val_mse: 11.5601 - val_mae: 1.5971 - lr: 1.0214e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 6s - loss: 15.6583 - mse: 15.6583 - mae: 1.5721 - val_loss: 11.6000 - val_mse: 11.6000 - val_mae: 1.5669 - lr: 1.0214e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 6s - loss: 15.5769 - mse: 15.5769 - mae: 1.5644 - val_loss: 11.4712 - val_mse: 11.4712 - val_mae: 1.6138 - lr: 1.0214e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 6s - loss: 15.5745 - mse: 15.5745 - mae: 1.5668 - val_loss: 11.5276 - val_mse: 11.5276 - val_mae: 1.5720 - lr: 1.0214e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 13/100\n",
            "1000/1000 - 6s - loss: 15.5275 - mse: 15.5275 - mae: 1.5662 - val_loss: 11.4925 - val_mse: 11.4925 - val_mae: 1.5945 - lr: 1.0214e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 14/100\n",
            "1000/1000 - 6s - loss: 15.4932 - mse: 15.4932 - mae: 1.5622 - val_loss: 11.5545 - val_mse: 11.5545 - val_mae: 1.5692 - lr: 1.0214e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 15/100\n",
            "1000/1000 - 6s - loss: 15.4747 - mse: 15.4747 - mae: 1.5584 - val_loss: 11.4308 - val_mse: 11.4308 - val_mae: 1.5979 - lr: 1.0214e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 16/100\n",
            "1000/1000 - 6s - loss: 15.3628 - mse: 15.3628 - mae: 1.5584 - val_loss: 11.5427 - val_mse: 11.5427 - val_mae: 1.5863 - lr: 1.0214e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 17/100\n",
            "1000/1000 - 6s - loss: 15.3797 - mse: 15.3797 - mae: 1.5566 - val_loss: 11.5284 - val_mse: 11.5284 - val_mae: 1.5764 - lr: 1.0214e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 18/100\n",
            "1000/1000 - 6s - loss: 15.3672 - mse: 15.3672 - mae: 1.5578 - val_loss: 11.5993 - val_mse: 11.5993 - val_mae: 1.5484 - lr: 1.0214e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 19/100\n",
            "1000/1000 - 6s - loss: 15.3386 - mse: 15.3386 - mae: 1.5532 - val_loss: 11.4809 - val_mse: 11.4809 - val_mae: 1.5645 - lr: 1.0214e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 20/100\n",
            "1000/1000 - 6s - loss: 15.2929 - mse: 15.2929 - mae: 1.5532 - val_loss: 11.4958 - val_mse: 11.4958 - val_mae: 1.5527 - lr: 1.0214e-04 - 6s/epoch - 6ms/step\n",
            "Score for fold 1: loss of 11.495759963989258\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 6s - loss: 13.4718 - mse: 13.4718 - mae: 1.5698 - val_loss: 18.7038 - val_mse: 18.7038 - val_mae: 1.4851 - lr: 1.0214e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 6s - loss: 13.3618 - mse: 13.3618 - mae: 1.5666 - val_loss: 19.0209 - val_mse: 19.0209 - val_mae: 1.5846 - lr: 1.0214e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 6s - loss: 13.3769 - mse: 13.3769 - mae: 1.5682 - val_loss: 18.8818 - val_mse: 18.8818 - val_mae: 1.5516 - lr: 1.0214e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 6s - loss: 13.2952 - mse: 13.2952 - mae: 1.5632 - val_loss: 18.8586 - val_mse: 18.8586 - val_mae: 1.5080 - lr: 1.0214e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 6s - loss: 13.2273 - mse: 13.2273 - mae: 1.5571 - val_loss: 19.0444 - val_mse: 19.0444 - val_mae: 1.5716 - lr: 1.0214e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 6s - loss: 13.2008 - mse: 13.2008 - mae: 1.5571 - val_loss: 18.9943 - val_mse: 18.9943 - val_mae: 1.4741 - lr: 1.0214e-04 - 6s/epoch - 6ms/step\n",
            "Score for fold 2: loss of 18.99428939819336\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 6s - loss: 14.8571 - mse: 14.8571 - mae: 1.5475 - val_loss: 12.4851 - val_mse: 12.4851 - val_mae: 1.4770 - lr: 1.0214e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 6s - loss: 14.7296 - mse: 14.7296 - mae: 1.5443 - val_loss: 12.3652 - val_mse: 12.3652 - val_mae: 1.5238 - lr: 1.0214e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 6s - loss: 14.6528 - mse: 14.6528 - mae: 1.5411 - val_loss: 12.4184 - val_mse: 12.4184 - val_mae: 1.4998 - lr: 1.0214e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 6s - loss: 14.6014 - mse: 14.6014 - mae: 1.5346 - val_loss: 12.7007 - val_mse: 12.7007 - val_mae: 1.5859 - lr: 1.0214e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 6s - loss: 14.5142 - mse: 14.5142 - mae: 1.5350 - val_loss: 12.6291 - val_mse: 12.6291 - val_mae: 1.5868 - lr: 1.0214e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 6s - loss: 14.4355 - mse: 14.4355 - mae: 1.5336 - val_loss: 12.5548 - val_mse: 12.5548 - val_mae: 1.6381 - lr: 1.0214e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 6s - loss: 14.3703 - mse: 14.3703 - mae: 1.5314 - val_loss: 12.8390 - val_mse: 12.8390 - val_mae: 1.5535 - lr: 1.0214e-04 - 6s/epoch - 6ms/step\n",
            "Score for fold 3: loss of 12.838968276977539\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 6s - loss: 14.6062 - mse: 14.6062 - mae: 1.5484 - val_loss: 11.8002 - val_mse: 11.8002 - val_mae: 1.5329 - lr: 1.0214e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 6s - loss: 14.5852 - mse: 14.5852 - mae: 1.5379 - val_loss: 11.7529 - val_mse: 11.7529 - val_mae: 1.4734 - lr: 1.0214e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 6s - loss: 14.5173 - mse: 14.5173 - mae: 1.5352 - val_loss: 11.9549 - val_mse: 11.9549 - val_mae: 1.4884 - lr: 1.0214e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 6s - loss: 14.3266 - mse: 14.3266 - mae: 1.5277 - val_loss: 11.9174 - val_mse: 11.9174 - val_mae: 1.5558 - lr: 1.0214e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 6s - loss: 14.2917 - mse: 14.2917 - mae: 1.5329 - val_loss: 12.0846 - val_mse: 12.0846 - val_mae: 1.4589 - lr: 1.0214e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 6s - loss: 14.1746 - mse: 14.1746 - mae: 1.5161 - val_loss: 12.1949 - val_mse: 12.1949 - val_mae: 1.6515 - lr: 1.0214e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 6s - loss: 14.0489 - mse: 14.0489 - mae: 1.5219 - val_loss: 12.2129 - val_mse: 12.2129 - val_mae: 1.5623 - lr: 1.0214e-04 - 6s/epoch - 6ms/step\n",
            "Score for fold 4: loss of 12.212935447692871\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 5s - loss: 13.0497 - mse: 13.0497 - mae: 1.5213 - val_loss: 16.1681 - val_mse: 16.1681 - val_mae: 1.5122 - lr: 1.0214e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 6s - loss: 12.8405 - mse: 12.8405 - mae: 1.5168 - val_loss: 16.7094 - val_mse: 16.7094 - val_mae: 1.5839 - lr: 1.0214e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 6s - loss: 12.8423 - mse: 12.8423 - mae: 1.5123 - val_loss: 16.4327 - val_mse: 16.4327 - val_mae: 1.5449 - lr: 1.0214e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 6s - loss: 12.7628 - mse: 12.7628 - mae: 1.5102 - val_loss: 16.8248 - val_mse: 16.8248 - val_mae: 1.4929 - lr: 1.0214e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 6s - loss: 12.6200 - mse: 12.6200 - mae: 1.5033 - val_loss: 16.6434 - val_mse: 16.6434 - val_mae: 1.5206 - lr: 1.0214e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 6s - loss: 12.6353 - mse: 12.6353 - mae: 1.4965 - val_loss: 17.1547 - val_mse: 17.1547 - val_mae: 1.5486 - lr: 1.0214e-04 - 6s/epoch - 6ms/step\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-10-16 06:57:37,995]\u001b[0m Finished trial#19 resulted in value: 14.538. Current best value is 14.447999999999999 with parameters: {'activation': 'relu', 'num_hidden_layer': 4, 'i': 9, 'learning_rate': 0.00017436499225447727}.\u001b[0m\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Score for fold 5: loss of 17.154666900634766\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 4s - loss: 16.3036 - mse: 16.3036 - mae: 1.6866 - val_loss: 15.1839 - val_mse: 15.1839 - val_mae: 1.6527 - lr: 2.1695e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 3s - loss: 15.8297 - mse: 15.8297 - mae: 1.6230 - val_loss: 15.2426 - val_mse: 15.2426 - val_mae: 1.6722 - lr: 2.1695e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 3s - loss: 15.8543 - mse: 15.8543 - mae: 1.6251 - val_loss: 15.1022 - val_mse: 15.1022 - val_mae: 1.7084 - lr: 2.1695e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 3s - loss: 15.8041 - mse: 15.8041 - mae: 1.6211 - val_loss: 15.1717 - val_mse: 15.1717 - val_mae: 1.6816 - lr: 2.1695e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 3s - loss: 15.8041 - mse: 15.8041 - mae: 1.6245 - val_loss: 15.5220 - val_mse: 15.5220 - val_mae: 1.6753 - lr: 2.1695e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 3s - loss: 15.8014 - mse: 15.8014 - mae: 1.6241 - val_loss: 15.1425 - val_mse: 15.1425 - val_mae: 1.6513 - lr: 2.1695e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 3s - loss: 15.7757 - mse: 15.7757 - mae: 1.6239 - val_loss: 15.2416 - val_mse: 15.2416 - val_mae: 1.6626 - lr: 2.1695e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 3s - loss: 15.7888 - mse: 15.7888 - mae: 1.6244 - val_loss: 15.2828 - val_mse: 15.2828 - val_mae: 1.6424 - lr: 2.1695e-04 - 3s/epoch - 3ms/step\n",
            "Score for fold 1: loss of 15.282751083374023\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 3s - loss: 16.0561 - mse: 16.0561 - mae: 1.6323 - val_loss: 14.1967 - val_mse: 14.1967 - val_mae: 1.6213 - lr: 2.1695e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 3s - loss: 16.0615 - mse: 16.0615 - mae: 1.6342 - val_loss: 14.2933 - val_mse: 14.2933 - val_mae: 1.5980 - lr: 2.1695e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 3s - loss: 16.0631 - mse: 16.0631 - mae: 1.6321 - val_loss: 14.3872 - val_mse: 14.3872 - val_mae: 1.6147 - lr: 2.1695e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 3s - loss: 16.0384 - mse: 16.0384 - mae: 1.6323 - val_loss: 14.1585 - val_mse: 14.1585 - val_mae: 1.6457 - lr: 2.1695e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 3s - loss: 16.0560 - mse: 16.0560 - mae: 1.6323 - val_loss: 14.1791 - val_mse: 14.1791 - val_mae: 1.6320 - lr: 2.1695e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 3s - loss: 16.0178 - mse: 16.0178 - mae: 1.6319 - val_loss: 14.1918 - val_mse: 14.1918 - val_mae: 1.6414 - lr: 2.1695e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 3s - loss: 16.0334 - mse: 16.0334 - mae: 1.6306 - val_loss: 14.1418 - val_mse: 14.1418 - val_mae: 1.6424 - lr: 2.1695e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 3s - loss: 16.0203 - mse: 16.0203 - mae: 1.6297 - val_loss: 14.2183 - val_mse: 14.2183 - val_mae: 1.6399 - lr: 2.1695e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 3s - loss: 16.0213 - mse: 16.0213 - mae: 1.6320 - val_loss: 14.1877 - val_mse: 14.1877 - val_mae: 1.6280 - lr: 2.1695e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 3s - loss: 16.0327 - mse: 16.0327 - mae: 1.6308 - val_loss: 14.1957 - val_mse: 14.1957 - val_mae: 1.6254 - lr: 2.1695e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 3s - loss: 16.0270 - mse: 16.0270 - mae: 1.6291 - val_loss: 14.2939 - val_mse: 14.2939 - val_mae: 1.6298 - lr: 2.1695e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 3s - loss: 16.0198 - mse: 16.0198 - mae: 1.6290 - val_loss: 14.0891 - val_mse: 14.0891 - val_mae: 1.6554 - lr: 2.1695e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 13/100\n",
            "1000/1000 - 3s - loss: 16.0080 - mse: 16.0080 - mae: 1.6290 - val_loss: 14.1509 - val_mse: 14.1509 - val_mae: 1.6364 - lr: 2.1695e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 14/100\n",
            "1000/1000 - 3s - loss: 16.0170 - mse: 16.0170 - mae: 1.6284 - val_loss: 14.0837 - val_mse: 14.0837 - val_mae: 1.6790 - lr: 2.1695e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 15/100\n",
            "1000/1000 - 3s - loss: 16.0068 - mse: 16.0068 - mae: 1.6320 - val_loss: 14.1493 - val_mse: 14.1493 - val_mae: 1.6467 - lr: 2.1695e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 16/100\n",
            "1000/1000 - 3s - loss: 16.0342 - mse: 16.0342 - mae: 1.6274 - val_loss: 14.1652 - val_mse: 14.1652 - val_mae: 1.6630 - lr: 2.1695e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 17/100\n",
            "1000/1000 - 3s - loss: 16.0288 - mse: 16.0288 - mae: 1.6316 - val_loss: 14.0983 - val_mse: 14.0983 - val_mae: 1.6576 - lr: 2.1695e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 18/100\n",
            "1000/1000 - 3s - loss: 16.0108 - mse: 16.0108 - mae: 1.6319 - val_loss: 14.2653 - val_mse: 14.2653 - val_mae: 1.6163 - lr: 2.1695e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 19/100\n",
            "1000/1000 - 3s - loss: 16.0143 - mse: 16.0143 - mae: 1.6336 - val_loss: 14.2590 - val_mse: 14.2590 - val_mae: 1.5982 - lr: 2.1695e-04 - 3s/epoch - 3ms/step\n",
            "Score for fold 2: loss of 14.25895881652832\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 3s - loss: 14.7877 - mse: 14.7877 - mae: 1.6351 - val_loss: 18.9755 - val_mse: 18.9755 - val_mae: 1.6285 - lr: 2.1695e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 3s - loss: 14.7930 - mse: 14.7930 - mae: 1.6345 - val_loss: 18.9957 - val_mse: 18.9957 - val_mae: 1.6194 - lr: 2.1695e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 3s - loss: 14.7987 - mse: 14.7987 - mae: 1.6323 - val_loss: 18.9507 - val_mse: 18.9507 - val_mae: 1.6610 - lr: 2.1695e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 3s - loss: 14.7897 - mse: 14.7897 - mae: 1.6356 - val_loss: 18.9570 - val_mse: 18.9570 - val_mae: 1.6370 - lr: 2.1695e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 3s - loss: 14.7964 - mse: 14.7964 - mae: 1.6365 - val_loss: 18.9900 - val_mse: 18.9900 - val_mae: 1.6225 - lr: 2.1695e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 3s - loss: 14.7881 - mse: 14.7881 - mae: 1.6337 - val_loss: 19.0496 - val_mse: 19.0496 - val_mae: 1.5839 - lr: 2.1695e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 3s - loss: 14.8070 - mse: 14.8070 - mae: 1.6397 - val_loss: 18.9599 - val_mse: 18.9599 - val_mae: 1.6392 - lr: 2.1695e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 3s - loss: 14.7998 - mse: 14.7998 - mae: 1.6319 - val_loss: 19.0143 - val_mse: 19.0143 - val_mae: 1.6100 - lr: 2.1695e-04 - 3s/epoch - 3ms/step\n",
            "Score for fold 3: loss of 19.0142765045166\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 3s - loss: 16.7108 - mse: 16.7108 - mae: 1.6432 - val_loss: 11.2003 - val_mse: 11.2003 - val_mae: 1.6325 - lr: 2.1695e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 3s - loss: 16.7214 - mse: 16.7214 - mae: 1.6419 - val_loss: 11.2694 - val_mse: 11.2694 - val_mae: 1.5530 - lr: 2.1695e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 3s - loss: 16.7143 - mse: 16.7143 - mae: 1.6479 - val_loss: 11.2936 - val_mse: 11.2936 - val_mae: 1.5395 - lr: 2.1695e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 3s - loss: 16.7460 - mse: 16.7460 - mae: 1.6418 - val_loss: 11.3135 - val_mse: 11.3135 - val_mae: 1.5507 - lr: 2.1695e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 3s - loss: 16.7570 - mse: 16.7570 - mae: 1.6491 - val_loss: 11.2599 - val_mse: 11.2599 - val_mae: 1.5444 - lr: 2.1695e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 3s - loss: 16.7524 - mse: 16.7524 - mae: 1.6377 - val_loss: 11.1963 - val_mse: 11.1963 - val_mae: 1.5951 - lr: 2.1695e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 3s - loss: 16.7382 - mse: 16.7382 - mae: 1.6432 - val_loss: 11.1968 - val_mse: 11.1968 - val_mae: 1.6310 - lr: 2.1695e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 3s - loss: 16.7365 - mse: 16.7365 - mae: 1.6433 - val_loss: 11.1959 - val_mse: 11.1959 - val_mae: 1.6181 - lr: 2.1695e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 3s - loss: 16.7341 - mse: 16.7341 - mae: 1.6451 - val_loss: 11.2455 - val_mse: 11.2455 - val_mae: 1.5690 - lr: 2.1695e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 3s - loss: 16.7254 - mse: 16.7254 - mae: 1.6506 - val_loss: 11.2187 - val_mse: 11.2187 - val_mae: 1.5692 - lr: 2.1695e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 3s - loss: 16.7585 - mse: 16.7585 - mae: 1.6401 - val_loss: 11.2595 - val_mse: 11.2595 - val_mae: 1.5470 - lr: 2.1695e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 3s - loss: 16.7281 - mse: 16.7281 - mae: 1.6530 - val_loss: 11.2876 - val_mse: 11.2876 - val_mae: 1.5373 - lr: 2.1695e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 13/100\n",
            "1000/1000 - 3s - loss: 16.7653 - mse: 16.7653 - mae: 1.6474 - val_loss: 11.4768 - val_mse: 11.4768 - val_mae: 1.4935 - lr: 2.1695e-04 - 3s/epoch - 3ms/step\n",
            "Score for fold 4: loss of 11.47678279876709\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 3s - loss: 14.8996 - mse: 14.8996 - mae: 1.6353 - val_loss: 18.6612 - val_mse: 18.6612 - val_mae: 1.6002 - lr: 2.1695e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 3s - loss: 14.9642 - mse: 14.9642 - mae: 1.6353 - val_loss: 18.6625 - val_mse: 18.6625 - val_mae: 1.5932 - lr: 2.1695e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 3s - loss: 14.9262 - mse: 14.9262 - mae: 1.6332 - val_loss: 18.5946 - val_mse: 18.5946 - val_mae: 1.6295 - lr: 2.1695e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 3s - loss: 14.9362 - mse: 14.9362 - mae: 1.6348 - val_loss: 18.6109 - val_mse: 18.6109 - val_mae: 1.6377 - lr: 2.1695e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 3s - loss: 14.9139 - mse: 14.9139 - mae: 1.6383 - val_loss: 18.5794 - val_mse: 18.5794 - val_mae: 1.6402 - lr: 2.1695e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 3s - loss: 14.9399 - mse: 14.9399 - mae: 1.6359 - val_loss: 18.5836 - val_mse: 18.5836 - val_mae: 1.7096 - lr: 2.1695e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 3s - loss: 14.9291 - mse: 14.9291 - mae: 1.6390 - val_loss: 18.5698 - val_mse: 18.5698 - val_mae: 1.6567 - lr: 2.1695e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 3s - loss: 14.9125 - mse: 14.9125 - mae: 1.6314 - val_loss: 18.6319 - val_mse: 18.6319 - val_mae: 1.5934 - lr: 2.1695e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 3s - loss: 14.9245 - mse: 14.9245 - mae: 1.6378 - val_loss: 18.5590 - val_mse: 18.5590 - val_mae: 1.6604 - lr: 2.1695e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 3s - loss: 14.9167 - mse: 14.9167 - mae: 1.6351 - val_loss: 18.5786 - val_mse: 18.5786 - val_mae: 1.6563 - lr: 2.1695e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 3s - loss: 14.8783 - mse: 14.8783 - mae: 1.6433 - val_loss: 18.5872 - val_mse: 18.5872 - val_mae: 1.6277 - lr: 2.1695e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 3s - loss: 14.9508 - mse: 14.9508 - mae: 1.6437 - val_loss: 18.5946 - val_mse: 18.5946 - val_mae: 1.6139 - lr: 2.1695e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 13/100\n",
            "1000/1000 - 3s - loss: 14.9017 - mse: 14.9017 - mae: 1.6352 - val_loss: 18.8850 - val_mse: 18.8850 - val_mae: 1.5379 - lr: 2.1695e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 14/100\n",
            "1000/1000 - 3s - loss: 14.9253 - mse: 14.9253 - mae: 1.6499 - val_loss: 18.6086 - val_mse: 18.6086 - val_mae: 1.6184 - lr: 2.1695e-04 - 3s/epoch - 3ms/step\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-10-16 07:00:52,638]\u001b[0m Finished trial#20 resulted in value: 15.728. Current best value is 14.447999999999999 with parameters: {'activation': 'relu', 'num_hidden_layer': 4, 'i': 9, 'learning_rate': 0.00017436499225447727}.\u001b[0m\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Score for fold 5: loss of 18.60862159729004\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 6s - loss: 15.8764 - mse: 15.8764 - mae: 1.6181 - val_loss: 13.8599 - val_mse: 13.8599 - val_mae: 1.5803 - lr: 1.3622e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 6s - loss: 15.3598 - mse: 15.3598 - mae: 1.5876 - val_loss: 13.7851 - val_mse: 13.7851 - val_mae: 1.6087 - lr: 1.3622e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 6s - loss: 15.2919 - mse: 15.2919 - mae: 1.5849 - val_loss: 13.9256 - val_mse: 13.9256 - val_mae: 1.5564 - lr: 1.3622e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 6s - loss: 15.2916 - mse: 15.2916 - mae: 1.5810 - val_loss: 13.9775 - val_mse: 13.9775 - val_mae: 1.5877 - lr: 1.3622e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 6s - loss: 15.2337 - mse: 15.2337 - mae: 1.5828 - val_loss: 14.1513 - val_mse: 14.1513 - val_mae: 1.5968 - lr: 1.3622e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 6s - loss: 15.1836 - mse: 15.1836 - mae: 1.5765 - val_loss: 13.7877 - val_mse: 13.7877 - val_mae: 1.5726 - lr: 1.3622e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 6s - loss: 15.1975 - mse: 15.1975 - mae: 1.5750 - val_loss: 13.8457 - val_mse: 13.8457 - val_mae: 1.6018 - lr: 1.3622e-04 - 6s/epoch - 6ms/step\n",
            "Score for fold 1: loss of 13.845675468444824\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 6s - loss: 16.1403 - mse: 16.1403 - mae: 1.5878 - val_loss: 9.6686 - val_mse: 9.6686 - val_mae: 1.5503 - lr: 1.3622e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 6s - loss: 16.0577 - mse: 16.0577 - mae: 1.5834 - val_loss: 9.6814 - val_mse: 9.6814 - val_mae: 1.5817 - lr: 1.3622e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 6s - loss: 15.9955 - mse: 15.9955 - mae: 1.5842 - val_loss: 9.7715 - val_mse: 9.7715 - val_mae: 1.4930 - lr: 1.3622e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 6s - loss: 15.9984 - mse: 15.9984 - mae: 1.5826 - val_loss: 9.7430 - val_mse: 9.7430 - val_mae: 1.4582 - lr: 1.3622e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 6s - loss: 15.9621 - mse: 15.9621 - mae: 1.5756 - val_loss: 9.9959 - val_mse: 9.9959 - val_mae: 1.6248 - lr: 1.3622e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 6s - loss: 15.9132 - mse: 15.9132 - mae: 1.5782 - val_loss: 9.8228 - val_mse: 9.8228 - val_mae: 1.5087 - lr: 1.3622e-04 - 6s/epoch - 6ms/step\n",
            "Score for fold 2: loss of 9.8228120803833\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 6s - loss: 15.1419 - mse: 15.1419 - mae: 1.5629 - val_loss: 13.2947 - val_mse: 13.2947 - val_mae: 1.5898 - lr: 1.3622e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 6s - loss: 14.9765 - mse: 14.9765 - mae: 1.5553 - val_loss: 13.3302 - val_mse: 13.3302 - val_mae: 1.6106 - lr: 1.3622e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 6s - loss: 14.9707 - mse: 14.9707 - mae: 1.5566 - val_loss: 13.5205 - val_mse: 13.5205 - val_mae: 1.5391 - lr: 1.3622e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 6s - loss: 14.9172 - mse: 14.9172 - mae: 1.5523 - val_loss: 13.5016 - val_mse: 13.5016 - val_mae: 1.5722 - lr: 1.3622e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 6s - loss: 14.8851 - mse: 14.8851 - mae: 1.5528 - val_loss: 13.5489 - val_mse: 13.5489 - val_mae: 1.5386 - lr: 1.3622e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 6s - loss: 14.7506 - mse: 14.7506 - mae: 1.5498 - val_loss: 13.2484 - val_mse: 13.2484 - val_mae: 1.5275 - lr: 1.3622e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 6s - loss: 14.7480 - mse: 14.7480 - mae: 1.5478 - val_loss: 13.0229 - val_mse: 13.0229 - val_mae: 1.6028 - lr: 1.3622e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 6s - loss: 14.6837 - mse: 14.6837 - mae: 1.5420 - val_loss: 13.6512 - val_mse: 13.6512 - val_mae: 1.6390 - lr: 1.3622e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 6s - loss: 14.5860 - mse: 14.5860 - mae: 1.5361 - val_loss: 13.3137 - val_mse: 13.3137 - val_mae: 1.5643 - lr: 1.3622e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 6s - loss: 14.5200 - mse: 14.5200 - mae: 1.5392 - val_loss: 13.6022 - val_mse: 13.6022 - val_mae: 1.5896 - lr: 1.3622e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 6s - loss: 14.4996 - mse: 14.4996 - mae: 1.5356 - val_loss: 13.3005 - val_mse: 13.3005 - val_mae: 1.5861 - lr: 1.3622e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 6s - loss: 14.3801 - mse: 14.3801 - mae: 1.5335 - val_loss: 13.7109 - val_mse: 13.7109 - val_mae: 1.6450 - lr: 1.3622e-04 - 6s/epoch - 6ms/step\n",
            "Score for fold 3: loss of 13.71088695526123\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 5s - loss: 14.3947 - mse: 14.3947 - mae: 1.5481 - val_loss: 13.5924 - val_mse: 13.5924 - val_mae: 1.5194 - lr: 1.3622e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 6s - loss: 14.3813 - mse: 14.3813 - mae: 1.5428 - val_loss: 13.1928 - val_mse: 13.1928 - val_mae: 1.5445 - lr: 1.3622e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 6s - loss: 14.3193 - mse: 14.3193 - mae: 1.5388 - val_loss: 13.7448 - val_mse: 13.7448 - val_mae: 1.5307 - lr: 1.3622e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 6s - loss: 14.1708 - mse: 14.1708 - mae: 1.5331 - val_loss: 13.4065 - val_mse: 13.4065 - val_mae: 1.6035 - lr: 1.3622e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 6s - loss: 14.0736 - mse: 14.0736 - mae: 1.5346 - val_loss: 13.3984 - val_mse: 13.3984 - val_mae: 1.6425 - lr: 1.3622e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 6s - loss: 14.0153 - mse: 14.0153 - mae: 1.5311 - val_loss: 13.6241 - val_mse: 13.6241 - val_mae: 1.5563 - lr: 1.3622e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 6s - loss: 13.8360 - mse: 13.8360 - mae: 1.5252 - val_loss: 13.8817 - val_mse: 13.8817 - val_mae: 1.6232 - lr: 1.3622e-04 - 6s/epoch - 6ms/step\n",
            "Score for fold 4: loss of 13.881749153137207\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 6s - loss: 11.8201 - mse: 11.8201 - mae: 1.5372 - val_loss: 21.9507 - val_mse: 21.9507 - val_mae: 1.7748 - lr: 1.3622e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 6s - loss: 11.7842 - mse: 11.7842 - mae: 1.5305 - val_loss: 21.8089 - val_mse: 21.8089 - val_mae: 1.5344 - lr: 1.3622e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 6s - loss: 11.7286 - mse: 11.7286 - mae: 1.5339 - val_loss: 21.8751 - val_mse: 21.8751 - val_mae: 1.5315 - lr: 1.3622e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 6s - loss: 11.6587 - mse: 11.6587 - mae: 1.5259 - val_loss: 22.0981 - val_mse: 22.0981 - val_mae: 1.4830 - lr: 1.3622e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 6s - loss: 11.5184 - mse: 11.5184 - mae: 1.5261 - val_loss: 22.3057 - val_mse: 22.3057 - val_mae: 1.7810 - lr: 1.3622e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 6s - loss: 11.4368 - mse: 11.4368 - mae: 1.5243 - val_loss: 22.6416 - val_mse: 22.6416 - val_mae: 1.8904 - lr: 1.3622e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 6s - loss: 11.3712 - mse: 11.3712 - mae: 1.5337 - val_loss: 22.2761 - val_mse: 22.2761 - val_mae: 1.5836 - lr: 1.3622e-04 - 6s/epoch - 6ms/step\n",
            "Score for fold 5: loss of 22.276071548461914\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-10-16 07:04:44,145]\u001b[0m Finished trial#21 resulted in value: 14.708000000000002. Current best value is 14.447999999999999 with parameters: {'activation': 'relu', 'num_hidden_layer': 4, 'i': 9, 'learning_rate': 0.00017436499225447727}.\u001b[0m\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 6s - loss: 16.6094 - mse: 16.6094 - mae: 1.6167 - val_loss: 11.0492 - val_mse: 11.0492 - val_mae: 1.6506 - lr: 1.0650e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 6s - loss: 16.1110 - mse: 16.1110 - mae: 1.5948 - val_loss: 11.0804 - val_mse: 11.0804 - val_mae: 1.5197 - lr: 1.0650e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 6s - loss: 16.0331 - mse: 16.0331 - mae: 1.5906 - val_loss: 10.9559 - val_mse: 10.9559 - val_mae: 1.5856 - lr: 1.0650e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 6s - loss: 15.9818 - mse: 15.9818 - mae: 1.5871 - val_loss: 10.9657 - val_mse: 10.9657 - val_mae: 1.5918 - lr: 1.0650e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 6s - loss: 15.9244 - mse: 15.9244 - mae: 1.5855 - val_loss: 10.9170 - val_mse: 10.9170 - val_mae: 1.5897 - lr: 1.0650e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 6s - loss: 15.8686 - mse: 15.8686 - mae: 1.5873 - val_loss: 10.9154 - val_mse: 10.9154 - val_mae: 1.5847 - lr: 1.0650e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 6s - loss: 15.8219 - mse: 15.8219 - mae: 1.5778 - val_loss: 10.9463 - val_mse: 10.9463 - val_mae: 1.5870 - lr: 1.0650e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 6s - loss: 15.8755 - mse: 15.8755 - mae: 1.5801 - val_loss: 10.8982 - val_mse: 10.8982 - val_mae: 1.5943 - lr: 1.0650e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 6s - loss: 15.8004 - mse: 15.8004 - mae: 1.5758 - val_loss: 10.8441 - val_mse: 10.8441 - val_mae: 1.5951 - lr: 1.0650e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 6s - loss: 15.7689 - mse: 15.7689 - mae: 1.5742 - val_loss: 10.8474 - val_mse: 10.8474 - val_mae: 1.5568 - lr: 1.0650e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 6s - loss: 15.7480 - mse: 15.7480 - mae: 1.5711 - val_loss: 10.8399 - val_mse: 10.8399 - val_mae: 1.5499 - lr: 1.0650e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 6s - loss: 15.7214 - mse: 15.7214 - mae: 1.5689 - val_loss: 10.9529 - val_mse: 10.9529 - val_mae: 1.5237 - lr: 1.0650e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 13/100\n",
            "1000/1000 - 6s - loss: 15.6046 - mse: 15.6046 - mae: 1.5676 - val_loss: 10.8832 - val_mse: 10.8832 - val_mae: 1.5654 - lr: 1.0650e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 14/100\n",
            "1000/1000 - 6s - loss: 15.5917 - mse: 15.5917 - mae: 1.5666 - val_loss: 10.8391 - val_mse: 10.8391 - val_mae: 1.6277 - lr: 1.0650e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 15/100\n",
            "1000/1000 - 6s - loss: 15.6066 - mse: 15.6066 - mae: 1.5653 - val_loss: 10.7841 - val_mse: 10.7841 - val_mae: 1.5723 - lr: 1.0650e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 16/100\n",
            "1000/1000 - 6s - loss: 15.4869 - mse: 15.4869 - mae: 1.5655 - val_loss: 10.7573 - val_mse: 10.7573 - val_mae: 1.5624 - lr: 1.0650e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 17/100\n",
            "1000/1000 - 6s - loss: 15.4599 - mse: 15.4599 - mae: 1.5602 - val_loss: 11.0047 - val_mse: 11.0047 - val_mae: 1.5342 - lr: 1.0650e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 18/100\n",
            "1000/1000 - 6s - loss: 15.5466 - mse: 15.5466 - mae: 1.5626 - val_loss: 10.8008 - val_mse: 10.8008 - val_mae: 1.6087 - lr: 1.0650e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 19/100\n",
            "1000/1000 - 6s - loss: 15.4908 - mse: 15.4908 - mae: 1.5603 - val_loss: 10.8473 - val_mse: 10.8473 - val_mae: 1.5877 - lr: 1.0650e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 20/100\n",
            "1000/1000 - 6s - loss: 15.3604 - mse: 15.3604 - mae: 1.5635 - val_loss: 10.8849 - val_mse: 10.8849 - val_mae: 1.6134 - lr: 1.0650e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 21/100\n",
            "1000/1000 - 6s - loss: 15.3586 - mse: 15.3586 - mae: 1.5511 - val_loss: 10.8739 - val_mse: 10.8739 - val_mae: 1.5548 - lr: 1.0650e-04 - 6s/epoch - 6ms/step\n",
            "Score for fold 1: loss of 10.873875617980957\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 5s - loss: 13.4171 - mse: 13.4171 - mae: 1.5620 - val_loss: 18.9511 - val_mse: 18.9511 - val_mae: 1.5785 - lr: 1.0650e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 6s - loss: 13.3159 - mse: 13.3159 - mae: 1.5557 - val_loss: 19.1114 - val_mse: 19.1114 - val_mae: 1.5899 - lr: 1.0650e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 6s - loss: 13.2903 - mse: 13.2903 - mae: 1.5534 - val_loss: 18.7368 - val_mse: 18.7368 - val_mae: 1.5181 - lr: 1.0650e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 6s - loss: 13.1619 - mse: 13.1619 - mae: 1.5462 - val_loss: 19.2238 - val_mse: 19.2238 - val_mae: 1.5415 - lr: 1.0650e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 6s - loss: 13.0660 - mse: 13.0660 - mae: 1.5450 - val_loss: 18.9303 - val_mse: 18.9303 - val_mae: 1.5604 - lr: 1.0650e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 6s - loss: 12.9703 - mse: 12.9703 - mae: 1.5422 - val_loss: 19.5081 - val_mse: 19.5081 - val_mae: 1.5347 - lr: 1.0650e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 6s - loss: 12.8950 - mse: 12.8950 - mae: 1.5385 - val_loss: 19.2333 - val_mse: 19.2333 - val_mae: 1.5506 - lr: 1.0650e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 6s - loss: 12.8499 - mse: 12.8499 - mae: 1.5318 - val_loss: 19.1213 - val_mse: 19.1213 - val_mae: 1.5977 - lr: 1.0650e-04 - 6s/epoch - 6ms/step\n",
            "Score for fold 2: loss of 19.121322631835938\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 6s - loss: 13.3118 - mse: 13.3118 - mae: 1.5281 - val_loss: 17.9430 - val_mse: 17.9430 - val_mae: 1.5051 - lr: 1.0650e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 6s - loss: 13.2603 - mse: 13.2603 - mae: 1.5303 - val_loss: 17.3894 - val_mse: 17.3894 - val_mae: 1.6004 - lr: 1.0650e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 6s - loss: 13.2026 - mse: 13.2026 - mae: 1.5251 - val_loss: 17.4190 - val_mse: 17.4190 - val_mae: 1.6396 - lr: 1.0650e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 6s - loss: 13.0425 - mse: 13.0425 - mae: 1.5176 - val_loss: 17.4705 - val_mse: 17.4705 - val_mae: 1.6019 - lr: 1.0650e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 6s - loss: 13.0073 - mse: 13.0073 - mae: 1.5157 - val_loss: 17.9127 - val_mse: 17.9127 - val_mae: 1.6396 - lr: 1.0650e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 6s - loss: 12.8388 - mse: 12.8388 - mae: 1.5094 - val_loss: 17.2903 - val_mse: 17.2903 - val_mae: 1.5893 - lr: 1.0650e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 6s - loss: 12.7976 - mse: 12.7976 - mae: 1.5032 - val_loss: 17.8955 - val_mse: 17.8955 - val_mae: 1.6638 - lr: 1.0650e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 6s - loss: 12.7237 - mse: 12.7237 - mae: 1.5092 - val_loss: 17.5148 - val_mse: 17.5148 - val_mae: 1.6277 - lr: 1.0650e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 6s - loss: 12.6141 - mse: 12.6141 - mae: 1.5015 - val_loss: 17.9132 - val_mse: 17.9132 - val_mae: 1.5596 - lr: 1.0650e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 6s - loss: 12.5815 - mse: 12.5815 - mae: 1.5000 - val_loss: 17.9779 - val_mse: 17.9779 - val_mae: 1.5762 - lr: 1.0650e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 6s - loss: 12.5167 - mse: 12.5167 - mae: 1.4890 - val_loss: 17.7884 - val_mse: 17.7884 - val_mae: 1.6505 - lr: 1.0650e-04 - 6s/epoch - 6ms/step\n",
            "Score for fold 3: loss of 17.788387298583984\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 5s - loss: 13.7960 - mse: 13.7960 - mae: 1.5275 - val_loss: 12.7794 - val_mse: 12.7794 - val_mae: 1.5232 - lr: 1.0650e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 6s - loss: 13.6707 - mse: 13.6707 - mae: 1.5180 - val_loss: 13.0740 - val_mse: 13.0740 - val_mae: 1.5507 - lr: 1.0650e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 6s - loss: 13.6449 - mse: 13.6449 - mae: 1.5121 - val_loss: 13.1174 - val_mse: 13.1174 - val_mae: 1.6485 - lr: 1.0650e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 6s - loss: 13.4383 - mse: 13.4383 - mae: 1.5106 - val_loss: 13.5027 - val_mse: 13.5027 - val_mae: 1.4533 - lr: 1.0650e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 6s - loss: 13.3334 - mse: 13.3334 - mae: 1.4991 - val_loss: 13.4136 - val_mse: 13.4136 - val_mae: 1.4510 - lr: 1.0650e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 6s - loss: 13.3012 - mse: 13.3012 - mae: 1.4957 - val_loss: 13.5332 - val_mse: 13.5332 - val_mae: 1.7051 - lr: 1.0650e-04 - 6s/epoch - 6ms/step\n",
            "Score for fold 4: loss of 13.533209800720215\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 6s - loss: 14.2330 - mse: 14.2330 - mae: 1.5344 - val_loss: 9.3772 - val_mse: 9.3772 - val_mae: 1.4463 - lr: 1.0650e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 6s - loss: 14.2017 - mse: 14.2017 - mae: 1.5155 - val_loss: 9.4994 - val_mse: 9.4994 - val_mae: 1.5077 - lr: 1.0650e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 6s - loss: 14.0319 - mse: 14.0319 - mae: 1.5144 - val_loss: 9.9413 - val_mse: 9.9413 - val_mae: 1.4461 - lr: 1.0650e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 6s - loss: 13.9115 - mse: 13.9115 - mae: 1.5087 - val_loss: 9.8158 - val_mse: 9.8158 - val_mae: 1.4583 - lr: 1.0650e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 6s - loss: 13.8518 - mse: 13.8518 - mae: 1.4979 - val_loss: 9.8869 - val_mse: 9.8869 - val_mae: 1.5085 - lr: 1.0650e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 6s - loss: 13.6439 - mse: 13.6439 - mae: 1.4952 - val_loss: 9.9580 - val_mse: 9.9580 - val_mae: 1.5919 - lr: 1.0650e-04 - 6s/epoch - 6ms/step\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-10-16 07:09:49,957]\u001b[0m Finished trial#22 resulted in value: 14.254000000000001. Current best value is 14.254000000000001 with parameters: {'activation': 'relu', 'num_hidden_layer': 4, 'i': 9, 'learning_rate': 0.00010649836452363096}.\u001b[0m\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Score for fold 5: loss of 9.95803165435791\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 13s - loss: 15.7431 - mse: 15.7431 - mae: 1.6133 - val_loss: 13.9237 - val_mse: 13.9237 - val_mae: 1.6585 - lr: 3.3377e-04 - 13s/epoch - 13ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 12s - loss: 15.4872 - mse: 15.4872 - mae: 1.6007 - val_loss: 13.9473 - val_mse: 13.9473 - val_mae: 1.5802 - lr: 3.3377e-04 - 12s/epoch - 12ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 12s - loss: 15.4243 - mse: 15.4243 - mae: 1.6008 - val_loss: 13.8110 - val_mse: 13.8110 - val_mae: 1.6243 - lr: 3.3377e-04 - 12s/epoch - 12ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 12s - loss: 15.3111 - mse: 15.3111 - mae: 1.5915 - val_loss: 13.9748 - val_mse: 13.9748 - val_mae: 1.5624 - lr: 3.3377e-04 - 12s/epoch - 12ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 13s - loss: 15.1891 - mse: 15.1891 - mae: 1.5849 - val_loss: 13.8512 - val_mse: 13.8512 - val_mae: 1.6378 - lr: 3.3377e-04 - 13s/epoch - 13ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 13s - loss: 15.2738 - mse: 15.2738 - mae: 1.5824 - val_loss: 13.8705 - val_mse: 13.8705 - val_mae: 1.5898 - lr: 3.3377e-04 - 13s/epoch - 13ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 13s - loss: 15.2198 - mse: 15.2198 - mae: 1.5825 - val_loss: 13.9616 - val_mse: 13.9616 - val_mae: 1.6822 - lr: 3.3377e-04 - 13s/epoch - 13ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 13s - loss: 15.0751 - mse: 15.0751 - mae: 1.5805 - val_loss: 13.8117 - val_mse: 13.8117 - val_mae: 1.6496 - lr: 3.3377e-04 - 13s/epoch - 13ms/step\n",
            "Score for fold 1: loss of 13.811732292175293\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 12s - loss: 15.2496 - mse: 15.2496 - mae: 1.5949 - val_loss: 13.0105 - val_mse: 13.0105 - val_mae: 1.6365 - lr: 3.3377e-04 - 12s/epoch - 12ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 12s - loss: 15.1063 - mse: 15.1063 - mae: 1.5884 - val_loss: 13.3012 - val_mse: 13.3012 - val_mae: 1.4948 - lr: 3.3377e-04 - 12s/epoch - 12ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 12s - loss: 15.2256 - mse: 15.2256 - mae: 1.5894 - val_loss: 13.1162 - val_mse: 13.1162 - val_mae: 1.6261 - lr: 3.3377e-04 - 12s/epoch - 12ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 12s - loss: 15.1916 - mse: 15.1916 - mae: 1.5933 - val_loss: 13.2722 - val_mse: 13.2722 - val_mae: 1.4967 - lr: 3.3377e-04 - 12s/epoch - 12ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 12s - loss: 15.1308 - mse: 15.1308 - mae: 1.5923 - val_loss: 13.3354 - val_mse: 13.3354 - val_mae: 1.7495 - lr: 3.3377e-04 - 12s/epoch - 12ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 12s - loss: 15.0537 - mse: 15.0537 - mae: 1.5934 - val_loss: 13.5775 - val_mse: 13.5775 - val_mae: 1.5214 - lr: 3.3377e-04 - 12s/epoch - 12ms/step\n",
            "Score for fold 2: loss of 13.577486038208008\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 12s - loss: 15.2808 - mse: 15.2808 - mae: 1.5986 - val_loss: 11.8890 - val_mse: 11.8890 - val_mae: 1.5139 - lr: 3.3377e-04 - 12s/epoch - 12ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 12s - loss: 15.2281 - mse: 15.2281 - mae: 1.5925 - val_loss: 12.4700 - val_mse: 12.4700 - val_mae: 1.4826 - lr: 3.3377e-04 - 12s/epoch - 12ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 12s - loss: 15.2094 - mse: 15.2094 - mae: 1.5990 - val_loss: 12.5779 - val_mse: 12.5779 - val_mae: 1.5271 - lr: 3.3377e-04 - 12s/epoch - 12ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 12s - loss: 15.1395 - mse: 15.1395 - mae: 1.5927 - val_loss: 12.6890 - val_mse: 12.6890 - val_mae: 1.7752 - lr: 3.3377e-04 - 12s/epoch - 12ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 12s - loss: 15.1704 - mse: 15.1704 - mae: 1.6039 - val_loss: 12.0591 - val_mse: 12.0591 - val_mae: 1.5171 - lr: 3.3377e-04 - 12s/epoch - 12ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 12s - loss: 14.9499 - mse: 14.9499 - mae: 1.5994 - val_loss: 12.9780 - val_mse: 12.9780 - val_mae: 1.7662 - lr: 3.3377e-04 - 12s/epoch - 12ms/step\n",
            "Score for fold 3: loss of 12.978004455566406\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 12s - loss: 13.3998 - mse: 13.3998 - mae: 1.5955 - val_loss: 19.5775 - val_mse: 19.5775 - val_mae: 1.6154 - lr: 3.3377e-04 - 12s/epoch - 12ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 12s - loss: 13.2585 - mse: 13.2585 - mae: 1.5947 - val_loss: 19.7692 - val_mse: 19.7692 - val_mae: 1.4850 - lr: 3.3377e-04 - 12s/epoch - 12ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 12s - loss: 13.1539 - mse: 13.1539 - mae: 1.5934 - val_loss: 19.7632 - val_mse: 19.7632 - val_mae: 1.4876 - lr: 3.3377e-04 - 12s/epoch - 12ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 12s - loss: 13.1059 - mse: 13.1059 - mae: 1.5919 - val_loss: 19.7765 - val_mse: 19.7765 - val_mae: 1.8124 - lr: 3.3377e-04 - 12s/epoch - 12ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 12s - loss: 13.1390 - mse: 13.1390 - mae: 1.5927 - val_loss: 19.8612 - val_mse: 19.8612 - val_mae: 1.6241 - lr: 3.3377e-04 - 12s/epoch - 12ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 13s - loss: 13.0736 - mse: 13.0736 - mae: 1.6022 - val_loss: 20.0567 - val_mse: 20.0567 - val_mae: 1.8050 - lr: 3.3377e-04 - 13s/epoch - 13ms/step\n",
            "Score for fold 4: loss of 20.056766510009766\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 12s - loss: 14.4503 - mse: 14.4503 - mae: 1.6055 - val_loss: 14.1948 - val_mse: 14.1948 - val_mae: 1.6229 - lr: 3.3377e-04 - 12s/epoch - 12ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 12s - loss: 14.4636 - mse: 14.4636 - mae: 1.6219 - val_loss: 14.3624 - val_mse: 14.3624 - val_mae: 1.8500 - lr: 3.3377e-04 - 12s/epoch - 12ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 13s - loss: 14.2265 - mse: 14.2265 - mae: 1.6020 - val_loss: 14.3294 - val_mse: 14.3294 - val_mae: 1.6432 - lr: 3.3377e-04 - 13s/epoch - 13ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 13s - loss: 14.3293 - mse: 14.3293 - mae: 1.6273 - val_loss: 14.4387 - val_mse: 14.4387 - val_mae: 1.4977 - lr: 3.3377e-04 - 13s/epoch - 13ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 12s - loss: 14.2629 - mse: 14.2629 - mae: 1.6392 - val_loss: 14.8709 - val_mse: 14.8709 - val_mae: 1.5584 - lr: 3.3377e-04 - 12s/epoch - 12ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 12s - loss: 14.1788 - mse: 14.1788 - mae: 1.6329 - val_loss: 14.4123 - val_mse: 14.4123 - val_mae: 1.6894 - lr: 3.3377e-04 - 12s/epoch - 12ms/step\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-10-16 07:16:28,984]\u001b[0m Finished trial#23 resulted in value: 14.968. Current best value is 14.254000000000001 with parameters: {'activation': 'relu', 'num_hidden_layer': 4, 'i': 9, 'learning_rate': 0.00010649836452363096}.\u001b[0m\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Score for fold 5: loss of 14.412344932556152\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 6s - loss: 16.1079 - mse: 16.1079 - mae: 1.6171 - val_loss: 12.5654 - val_mse: 12.5654 - val_mae: 1.6488 - lr: 1.8064e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 5s - loss: 15.6693 - mse: 15.6693 - mae: 1.5909 - val_loss: 12.5404 - val_mse: 12.5404 - val_mae: 1.6729 - lr: 1.8064e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 6s - loss: 15.7047 - mse: 15.7047 - mae: 1.5874 - val_loss: 12.5153 - val_mse: 12.5153 - val_mae: 1.6203 - lr: 1.8064e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 6s - loss: 15.6390 - mse: 15.6390 - mae: 1.5880 - val_loss: 12.5624 - val_mse: 12.5624 - val_mae: 1.5584 - lr: 1.8064e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 6s - loss: 15.5182 - mse: 15.5182 - mae: 1.5859 - val_loss: 12.4153 - val_mse: 12.4153 - val_mae: 1.5963 - lr: 1.8064e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 6s - loss: 15.5186 - mse: 15.5186 - mae: 1.5827 - val_loss: 12.4804 - val_mse: 12.4804 - val_mae: 1.5651 - lr: 1.8064e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 6s - loss: 15.4937 - mse: 15.4937 - mae: 1.5754 - val_loss: 12.5848 - val_mse: 12.5848 - val_mae: 1.6272 - lr: 1.8064e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 6s - loss: 15.4139 - mse: 15.4139 - mae: 1.5744 - val_loss: 12.4610 - val_mse: 12.4610 - val_mae: 1.5704 - lr: 1.8064e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 6s - loss: 15.4187 - mse: 15.4187 - mae: 1.5717 - val_loss: 12.3918 - val_mse: 12.3918 - val_mae: 1.6036 - lr: 1.8064e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 6s - loss: 15.2998 - mse: 15.2998 - mae: 1.5649 - val_loss: 12.4009 - val_mse: 12.4009 - val_mae: 1.5933 - lr: 1.8064e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 6s - loss: 15.2987 - mse: 15.2987 - mae: 1.5643 - val_loss: 12.3710 - val_mse: 12.3710 - val_mae: 1.5867 - lr: 1.8064e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 6s - loss: 15.3226 - mse: 15.3226 - mae: 1.5626 - val_loss: 12.3694 - val_mse: 12.3694 - val_mae: 1.5402 - lr: 1.8064e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 13/100\n",
            "1000/1000 - 6s - loss: 15.2289 - mse: 15.2289 - mae: 1.5574 - val_loss: 12.4262 - val_mse: 12.4262 - val_mae: 1.5881 - lr: 1.8064e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 14/100\n",
            "1000/1000 - 6s - loss: 15.1578 - mse: 15.1578 - mae: 1.5608 - val_loss: 12.4351 - val_mse: 12.4351 - val_mae: 1.5383 - lr: 1.8064e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 15/100\n",
            "1000/1000 - 6s - loss: 15.0515 - mse: 15.0515 - mae: 1.5549 - val_loss: 12.3661 - val_mse: 12.3661 - val_mae: 1.5461 - lr: 1.8064e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 16/100\n",
            "1000/1000 - 6s - loss: 14.9986 - mse: 14.9986 - mae: 1.5556 - val_loss: 12.3738 - val_mse: 12.3738 - val_mae: 1.6250 - lr: 1.8064e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 17/100\n",
            "1000/1000 - 6s - loss: 14.9832 - mse: 14.9832 - mae: 1.5530 - val_loss: 12.6547 - val_mse: 12.6547 - val_mae: 1.5627 - lr: 1.8064e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 18/100\n",
            "1000/1000 - 6s - loss: 14.8583 - mse: 14.8583 - mae: 1.5470 - val_loss: 12.3940 - val_mse: 12.3940 - val_mae: 1.5286 - lr: 1.8064e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 19/100\n",
            "1000/1000 - 6s - loss: 14.7990 - mse: 14.7990 - mae: 1.5397 - val_loss: 12.5796 - val_mse: 12.5796 - val_mae: 1.6180 - lr: 1.8064e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 20/100\n",
            "1000/1000 - 6s - loss: 14.8034 - mse: 14.8034 - mae: 1.5449 - val_loss: 12.3364 - val_mse: 12.3364 - val_mae: 1.5662 - lr: 1.8064e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 21/100\n",
            "1000/1000 - 6s - loss: 14.6146 - mse: 14.6146 - mae: 1.5362 - val_loss: 12.7289 - val_mse: 12.7289 - val_mae: 1.5852 - lr: 1.8064e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 22/100\n",
            "1000/1000 - 6s - loss: 14.4995 - mse: 14.4995 - mae: 1.5428 - val_loss: 12.4030 - val_mse: 12.4030 - val_mae: 1.6053 - lr: 1.8064e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 23/100\n",
            "1000/1000 - 6s - loss: 14.5399 - mse: 14.5399 - mae: 1.5365 - val_loss: 12.6280 - val_mse: 12.6280 - val_mae: 1.5797 - lr: 1.8064e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 24/100\n",
            "1000/1000 - 6s - loss: 14.3777 - mse: 14.3777 - mae: 1.5271 - val_loss: 12.7220 - val_mse: 12.7220 - val_mae: 1.7282 - lr: 1.8064e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 25/100\n",
            "1000/1000 - 6s - loss: 14.3777 - mse: 14.3777 - mae: 1.5302 - val_loss: 12.5642 - val_mse: 12.5642 - val_mae: 1.6691 - lr: 1.8064e-04 - 6s/epoch - 6ms/step\n",
            "Score for fold 1: loss of 12.56420612335205\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 6s - loss: 14.3979 - mse: 14.3979 - mae: 1.5470 - val_loss: 12.5026 - val_mse: 12.5026 - val_mae: 1.4933 - lr: 1.8064e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 5s - loss: 14.2628 - mse: 14.2628 - mae: 1.5348 - val_loss: 12.5578 - val_mse: 12.5578 - val_mae: 1.5879 - lr: 1.8064e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 6s - loss: 14.1226 - mse: 14.1226 - mae: 1.5334 - val_loss: 12.6530 - val_mse: 12.6530 - val_mae: 1.6425 - lr: 1.8064e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 6s - loss: 14.0486 - mse: 14.0486 - mae: 1.5284 - val_loss: 13.2764 - val_mse: 13.2764 - val_mae: 1.5148 - lr: 1.8064e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 6s - loss: 13.9679 - mse: 13.9679 - mae: 1.5330 - val_loss: 13.0534 - val_mse: 13.0534 - val_mae: 1.6839 - lr: 1.8064e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 6s - loss: 13.8697 - mse: 13.8697 - mae: 1.5250 - val_loss: 13.0282 - val_mse: 13.0282 - val_mae: 1.5421 - lr: 1.8064e-04 - 6s/epoch - 6ms/step\n",
            "Score for fold 2: loss of 13.028176307678223\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 5s - loss: 13.9763 - mse: 13.9763 - mae: 1.5495 - val_loss: 13.1207 - val_mse: 13.1207 - val_mae: 1.5218 - lr: 1.8064e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 6s - loss: 13.9661 - mse: 13.9661 - mae: 1.5503 - val_loss: 13.2836 - val_mse: 13.2836 - val_mae: 1.3901 - lr: 1.8064e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 6s - loss: 13.7710 - mse: 13.7710 - mae: 1.5487 - val_loss: 13.1543 - val_mse: 13.1543 - val_mae: 1.5116 - lr: 1.8064e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 6s - loss: 13.6695 - mse: 13.6695 - mae: 1.5480 - val_loss: 13.0529 - val_mse: 13.0529 - val_mae: 1.4976 - lr: 1.8064e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 6s - loss: 13.5401 - mse: 13.5401 - mae: 1.5350 - val_loss: 13.6987 - val_mse: 13.6987 - val_mae: 1.5566 - lr: 1.8064e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 6s - loss: 13.5295 - mse: 13.5295 - mae: 1.5456 - val_loss: 13.5448 - val_mse: 13.5448 - val_mae: 1.4666 - lr: 1.8064e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 6s - loss: 13.3778 - mse: 13.3778 - mae: 1.5406 - val_loss: 13.4210 - val_mse: 13.4210 - val_mae: 1.5104 - lr: 1.8064e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 6s - loss: 13.3968 - mse: 13.3968 - mae: 1.5326 - val_loss: 13.2064 - val_mse: 13.2064 - val_mae: 1.5657 - lr: 1.8064e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 6s - loss: 13.2470 - mse: 13.2470 - mae: 1.5355 - val_loss: 13.3740 - val_mse: 13.3740 - val_mae: 1.6307 - lr: 1.8064e-04 - 6s/epoch - 6ms/step\n",
            "Score for fold 3: loss of 13.373995780944824\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 6s - loss: 12.1293 - mse: 12.1293 - mae: 1.5496 - val_loss: 17.7017 - val_mse: 17.7017 - val_mae: 1.5341 - lr: 1.8064e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 6s - loss: 12.0540 - mse: 12.0540 - mae: 1.5440 - val_loss: 18.1296 - val_mse: 18.1296 - val_mae: 1.4894 - lr: 1.8064e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 6s - loss: 11.9510 - mse: 11.9510 - mae: 1.5319 - val_loss: 17.9005 - val_mse: 17.9005 - val_mae: 1.6004 - lr: 1.8064e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 6s - loss: 11.8095 - mse: 11.8095 - mae: 1.5321 - val_loss: 18.1532 - val_mse: 18.1532 - val_mae: 1.5414 - lr: 1.8064e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 6s - loss: 11.7451 - mse: 11.7451 - mae: 1.5300 - val_loss: 18.5696 - val_mse: 18.5696 - val_mae: 1.7404 - lr: 1.8064e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 6s - loss: 11.6773 - mse: 11.6773 - mae: 1.5302 - val_loss: 18.3671 - val_mse: 18.3671 - val_mae: 1.5187 - lr: 1.8064e-04 - 6s/epoch - 6ms/step\n",
            "Score for fold 4: loss of 18.367090225219727\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 6s - loss: 13.3790 - mse: 13.3790 - mae: 1.5705 - val_loss: 12.4125 - val_mse: 12.4125 - val_mae: 1.7958 - lr: 1.8064e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 6s - loss: 13.2443 - mse: 13.2443 - mae: 1.5525 - val_loss: 11.9428 - val_mse: 11.9428 - val_mae: 1.6127 - lr: 1.8064e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 6s - loss: 13.1688 - mse: 13.1688 - mae: 1.5500 - val_loss: 12.4237 - val_mse: 12.4237 - val_mae: 1.5491 - lr: 1.8064e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 6s - loss: 12.9761 - mse: 12.9761 - mae: 1.5377 - val_loss: 12.4371 - val_mse: 12.4371 - val_mae: 1.3785 - lr: 1.8064e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 6s - loss: 12.8243 - mse: 12.8243 - mae: 1.5271 - val_loss: 12.2588 - val_mse: 12.2588 - val_mae: 1.4998 - lr: 1.8064e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 6s - loss: 12.9092 - mse: 12.9092 - mae: 1.5419 - val_loss: 12.6788 - val_mse: 12.6788 - val_mae: 1.6393 - lr: 1.8064e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 6s - loss: 12.7534 - mse: 12.7534 - mae: 1.5334 - val_loss: 12.7925 - val_mse: 12.7925 - val_mae: 1.5325 - lr: 1.8064e-04 - 6s/epoch - 6ms/step\n",
            "Score for fold 5: loss of 12.7925443649292\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-10-16 07:21:38,715]\u001b[0m Finished trial#24 resulted in value: 14.024000000000001. Current best value is 14.024000000000001 with parameters: {'activation': 'relu', 'num_hidden_layer': 4, 'i': 9, 'learning_rate': 0.00018064149922822524}.\u001b[0m\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 5s - loss: 17.6555 - mse: 17.6555 - mae: 1.7177 - val_loss: 10.5651 - val_mse: 10.5651 - val_mae: 1.6135 - lr: 1.0680e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 4s - loss: 16.7388 - mse: 16.7388 - mae: 1.6232 - val_loss: 10.3775 - val_mse: 10.3775 - val_mae: 1.5881 - lr: 1.0680e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 4s - loss: 16.6373 - mse: 16.6373 - mae: 1.6147 - val_loss: 10.2645 - val_mse: 10.2645 - val_mae: 1.5283 - lr: 1.0680e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 4s - loss: 16.5495 - mse: 16.5495 - mae: 1.6119 - val_loss: 10.1876 - val_mse: 10.1876 - val_mae: 1.5839 - lr: 1.0680e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 4s - loss: 16.4963 - mse: 16.4963 - mae: 1.6062 - val_loss: 10.3844 - val_mse: 10.3844 - val_mae: 1.5613 - lr: 1.0680e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 4s - loss: 16.4477 - mse: 16.4477 - mae: 1.6017 - val_loss: 10.1235 - val_mse: 10.1235 - val_mae: 1.5762 - lr: 1.0680e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 4s - loss: 16.4011 - mse: 16.4011 - mae: 1.6007 - val_loss: 10.1774 - val_mse: 10.1774 - val_mae: 1.5261 - lr: 1.0680e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 4s - loss: 16.3880 - mse: 16.3880 - mae: 1.5970 - val_loss: 10.1268 - val_mse: 10.1268 - val_mae: 1.5253 - lr: 1.0680e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 4s - loss: 16.3620 - mse: 16.3620 - mae: 1.5971 - val_loss: 10.0779 - val_mse: 10.0779 - val_mae: 1.5391 - lr: 1.0680e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 4s - loss: 16.3187 - mse: 16.3187 - mae: 1.5966 - val_loss: 10.0531 - val_mse: 10.0531 - val_mae: 1.5445 - lr: 1.0680e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 4s - loss: 16.3420 - mse: 16.3420 - mae: 1.5946 - val_loss: 10.1034 - val_mse: 10.1034 - val_mae: 1.5621 - lr: 1.0680e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 4s - loss: 16.2958 - mse: 16.2958 - mae: 1.5956 - val_loss: 10.1386 - val_mse: 10.1386 - val_mae: 1.5547 - lr: 1.0680e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 13/100\n",
            "1000/1000 - 4s - loss: 16.3111 - mse: 16.3111 - mae: 1.5922 - val_loss: 10.0682 - val_mse: 10.0682 - val_mae: 1.5558 - lr: 1.0680e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 14/100\n",
            "1000/1000 - 4s - loss: 16.2967 - mse: 16.2967 - mae: 1.5936 - val_loss: 10.0720 - val_mse: 10.0720 - val_mae: 1.5474 - lr: 1.0680e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 15/100\n",
            "1000/1000 - 4s - loss: 16.2739 - mse: 16.2739 - mae: 1.5919 - val_loss: 10.1587 - val_mse: 10.1587 - val_mae: 1.5784 - lr: 1.0680e-04 - 4s/epoch - 4ms/step\n",
            "Score for fold 1: loss of 10.15866470336914\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 4s - loss: 14.8231 - mse: 14.8231 - mae: 1.5721 - val_loss: 15.8375 - val_mse: 15.8375 - val_mae: 1.6352 - lr: 1.0680e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 4s - loss: 14.8201 - mse: 14.8201 - mae: 1.5711 - val_loss: 15.8987 - val_mse: 15.8987 - val_mae: 1.6676 - lr: 1.0680e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 4s - loss: 14.8054 - mse: 14.8054 - mae: 1.5685 - val_loss: 15.9569 - val_mse: 15.9569 - val_mae: 1.6035 - lr: 1.0680e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 4s - loss: 14.8035 - mse: 14.8035 - mae: 1.5711 - val_loss: 15.9631 - val_mse: 15.9631 - val_mae: 1.6019 - lr: 1.0680e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 4s - loss: 14.7974 - mse: 14.7974 - mae: 1.5696 - val_loss: 15.8753 - val_mse: 15.8753 - val_mae: 1.6325 - lr: 1.0680e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 4s - loss: 14.7925 - mse: 14.7925 - mae: 1.5673 - val_loss: 15.8855 - val_mse: 15.8855 - val_mae: 1.6097 - lr: 1.0680e-04 - 4s/epoch - 4ms/step\n",
            "Score for fold 2: loss of 15.885540008544922\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 4s - loss: 15.1689 - mse: 15.1689 - mae: 1.5883 - val_loss: 14.4148 - val_mse: 14.4148 - val_mae: 1.5415 - lr: 1.0680e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 4s - loss: 15.1630 - mse: 15.1630 - mae: 1.5854 - val_loss: 14.2298 - val_mse: 14.2298 - val_mae: 1.5673 - lr: 1.0680e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 4s - loss: 15.1610 - mse: 15.1610 - mae: 1.5848 - val_loss: 14.2285 - val_mse: 14.2285 - val_mae: 1.5813 - lr: 1.0680e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 4s - loss: 15.1448 - mse: 15.1448 - mae: 1.5850 - val_loss: 14.2829 - val_mse: 14.2829 - val_mae: 1.5619 - lr: 1.0680e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 4s - loss: 15.1175 - mse: 15.1175 - mae: 1.5822 - val_loss: 14.2195 - val_mse: 14.2195 - val_mae: 1.5721 - lr: 1.0680e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 4s - loss: 15.1230 - mse: 15.1230 - mae: 1.5793 - val_loss: 14.2278 - val_mse: 14.2278 - val_mae: 1.5994 - lr: 1.0680e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 4s - loss: 15.1105 - mse: 15.1105 - mae: 1.5816 - val_loss: 14.3862 - val_mse: 14.3862 - val_mae: 1.5642 - lr: 1.0680e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 4s - loss: 15.1142 - mse: 15.1142 - mae: 1.5828 - val_loss: 14.2881 - val_mse: 14.2881 - val_mae: 1.5924 - lr: 1.0680e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 4s - loss: 15.1155 - mse: 15.1155 - mae: 1.5815 - val_loss: 14.3498 - val_mse: 14.3498 - val_mae: 1.5616 - lr: 1.0680e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 4s - loss: 15.1109 - mse: 15.1109 - mae: 1.5770 - val_loss: 14.2270 - val_mse: 14.2270 - val_mae: 1.6140 - lr: 1.0680e-04 - 4s/epoch - 4ms/step\n",
            "Score for fold 3: loss of 14.227011680603027\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 4s - loss: 13.3919 - mse: 13.3919 - mae: 1.5706 - val_loss: 21.1430 - val_mse: 21.1430 - val_mae: 1.6308 - lr: 1.0680e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 4s - loss: 13.3879 - mse: 13.3879 - mae: 1.5703 - val_loss: 21.1419 - val_mse: 21.1419 - val_mae: 1.5863 - lr: 1.0680e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 4s - loss: 13.3807 - mse: 13.3807 - mae: 1.5679 - val_loss: 21.1404 - val_mse: 21.1404 - val_mae: 1.5838 - lr: 1.0680e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 4s - loss: 13.3536 - mse: 13.3536 - mae: 1.5701 - val_loss: 21.1160 - val_mse: 21.1160 - val_mae: 1.5916 - lr: 1.0680e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 4s - loss: 13.3390 - mse: 13.3390 - mae: 1.5665 - val_loss: 21.1717 - val_mse: 21.1717 - val_mae: 1.5814 - lr: 1.0680e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 4s - loss: 13.3482 - mse: 13.3482 - mae: 1.5677 - val_loss: 21.1752 - val_mse: 21.1752 - val_mae: 1.6373 - lr: 1.0680e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 4s - loss: 13.3252 - mse: 13.3252 - mae: 1.5639 - val_loss: 21.2037 - val_mse: 21.2037 - val_mae: 1.5442 - lr: 1.0680e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 4s - loss: 13.3335 - mse: 13.3335 - mae: 1.5625 - val_loss: 21.1336 - val_mse: 21.1336 - val_mae: 1.6204 - lr: 1.0680e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 4s - loss: 13.3055 - mse: 13.3055 - mae: 1.5635 - val_loss: 21.2044 - val_mse: 21.2044 - val_mae: 1.5919 - lr: 1.0680e-04 - 4s/epoch - 4ms/step\n",
            "Score for fold 4: loss of 21.20443344116211\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 4s - loss: 15.2594 - mse: 15.2594 - mae: 1.5784 - val_loss: 13.3833 - val_mse: 13.3833 - val_mae: 1.5184 - lr: 1.0680e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 4s - loss: 15.2191 - mse: 15.2191 - mae: 1.5778 - val_loss: 13.3876 - val_mse: 13.3876 - val_mae: 1.5383 - lr: 1.0680e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 4s - loss: 15.1993 - mse: 15.1993 - mae: 1.5801 - val_loss: 13.4053 - val_mse: 13.4053 - val_mae: 1.5384 - lr: 1.0680e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 4s - loss: 15.1709 - mse: 15.1709 - mae: 1.5770 - val_loss: 13.3598 - val_mse: 13.3598 - val_mae: 1.5627 - lr: 1.0680e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 4s - loss: 15.1735 - mse: 15.1735 - mae: 1.5764 - val_loss: 13.3851 - val_mse: 13.3851 - val_mae: 1.5680 - lr: 1.0680e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 4s - loss: 15.1697 - mse: 15.1697 - mae: 1.5800 - val_loss: 13.4619 - val_mse: 13.4619 - val_mae: 1.5040 - lr: 1.0680e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 4s - loss: 15.1745 - mse: 15.1745 - mae: 1.5748 - val_loss: 13.3885 - val_mse: 13.3885 - val_mae: 1.5613 - lr: 1.0680e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 4s - loss: 15.1647 - mse: 15.1647 - mae: 1.5745 - val_loss: 13.4971 - val_mse: 13.4971 - val_mae: 1.5259 - lr: 1.0680e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 4s - loss: 15.1665 - mse: 15.1665 - mae: 1.5732 - val_loss: 13.4546 - val_mse: 13.4546 - val_mae: 1.5909 - lr: 1.0680e-04 - 4s/epoch - 4ms/step\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-10-16 07:24:59,637]\u001b[0m Finished trial#25 resulted in value: 14.986. Current best value is 14.024000000000001 with parameters: {'activation': 'relu', 'num_hidden_layer': 4, 'i': 9, 'learning_rate': 0.00018064149922822524}.\u001b[0m\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Score for fold 5: loss of 13.454580307006836\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 6s - loss: 15.4623 - mse: 15.4623 - mae: 1.6217 - val_loss: 15.2653 - val_mse: 15.2653 - val_mae: 1.5834 - lr: 1.8390e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 6s - loss: 15.0175 - mse: 15.0175 - mae: 1.6018 - val_loss: 15.3566 - val_mse: 15.3566 - val_mae: 1.5473 - lr: 1.8390e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 6s - loss: 14.9846 - mse: 14.9846 - mae: 1.5984 - val_loss: 15.4573 - val_mse: 15.4573 - val_mae: 1.5838 - lr: 1.8390e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 6s - loss: 14.8846 - mse: 14.8846 - mae: 1.5998 - val_loss: 15.3845 - val_mse: 15.3845 - val_mae: 1.5361 - lr: 1.8390e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 6s - loss: 14.8044 - mse: 14.8044 - mae: 1.5900 - val_loss: 15.5384 - val_mse: 15.5384 - val_mae: 1.5119 - lr: 1.8390e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 6s - loss: 14.8250 - mse: 14.8250 - mae: 1.5888 - val_loss: 15.2001 - val_mse: 15.2001 - val_mae: 1.5725 - lr: 1.8390e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 6s - loss: 14.7702 - mse: 14.7702 - mae: 1.5848 - val_loss: 15.2797 - val_mse: 15.2797 - val_mae: 1.5550 - lr: 1.8390e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 6s - loss: 14.7403 - mse: 14.7403 - mae: 1.5816 - val_loss: 15.2951 - val_mse: 15.2951 - val_mae: 1.5596 - lr: 1.8390e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 6s - loss: 14.6147 - mse: 14.6147 - mae: 1.5760 - val_loss: 15.7068 - val_mse: 15.7068 - val_mae: 1.6429 - lr: 1.8390e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 6s - loss: 14.6286 - mse: 14.6286 - mae: 1.5782 - val_loss: 15.3407 - val_mse: 15.3407 - val_mae: 1.5955 - lr: 1.8390e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 6s - loss: 14.6216 - mse: 14.6216 - mae: 1.5693 - val_loss: 15.5322 - val_mse: 15.5322 - val_mae: 1.5577 - lr: 1.8390e-04 - 6s/epoch - 6ms/step\n",
            "Score for fold 1: loss of 15.53217887878418\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 6s - loss: 14.2909 - mse: 14.2909 - mae: 1.5529 - val_loss: 16.4788 - val_mse: 16.4788 - val_mae: 1.5950 - lr: 1.8390e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 6s - loss: 14.2947 - mse: 14.2947 - mae: 1.5461 - val_loss: 16.7773 - val_mse: 16.7773 - val_mae: 1.6608 - lr: 1.8390e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 6s - loss: 14.1956 - mse: 14.1956 - mae: 1.5487 - val_loss: 16.1031 - val_mse: 16.1031 - val_mae: 1.6625 - lr: 1.8390e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 6s - loss: 14.1797 - mse: 14.1797 - mae: 1.5399 - val_loss: 16.3890 - val_mse: 16.3890 - val_mae: 1.6426 - lr: 1.8390e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 6s - loss: 14.1165 - mse: 14.1165 - mae: 1.5449 - val_loss: 16.1597 - val_mse: 16.1597 - val_mae: 1.7120 - lr: 1.8390e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 6s - loss: 14.0295 - mse: 14.0295 - mae: 1.5403 - val_loss: 16.1203 - val_mse: 16.1203 - val_mae: 1.6378 - lr: 1.8390e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 6s - loss: 13.9727 - mse: 13.9727 - mae: 1.5352 - val_loss: 16.6257 - val_mse: 16.6257 - val_mae: 1.5845 - lr: 1.8390e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 6s - loss: 13.9424 - mse: 13.9424 - mae: 1.5317 - val_loss: 16.6408 - val_mse: 16.6408 - val_mae: 1.5706 - lr: 1.8390e-04 - 6s/epoch - 6ms/step\n",
            "Score for fold 2: loss of 16.640798568725586\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 6s - loss: 13.3276 - mse: 13.3276 - mae: 1.5678 - val_loss: 18.7684 - val_mse: 18.7684 - val_mae: 1.5125 - lr: 1.8390e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 6s - loss: 13.2281 - mse: 13.2281 - mae: 1.5639 - val_loss: 18.6435 - val_mse: 18.6435 - val_mae: 1.4943 - lr: 1.8390e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 6s - loss: 13.1451 - mse: 13.1451 - mae: 1.5570 - val_loss: 18.4535 - val_mse: 18.4535 - val_mae: 1.4714 - lr: 1.8390e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 6s - loss: 12.9760 - mse: 12.9760 - mae: 1.5511 - val_loss: 18.5294 - val_mse: 18.5294 - val_mae: 1.5974 - lr: 1.8390e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 6s - loss: 12.9065 - mse: 12.9065 - mae: 1.5443 - val_loss: 18.7108 - val_mse: 18.7108 - val_mae: 1.5772 - lr: 1.8390e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 6s - loss: 12.8157 - mse: 12.8157 - mae: 1.5443 - val_loss: 19.0802 - val_mse: 19.0802 - val_mae: 1.7275 - lr: 1.8390e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 6s - loss: 12.7820 - mse: 12.7820 - mae: 1.5442 - val_loss: 18.6776 - val_mse: 18.6776 - val_mae: 1.5443 - lr: 1.8390e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 6s - loss: 12.5939 - mse: 12.5939 - mae: 1.5466 - val_loss: 18.9811 - val_mse: 18.9811 - val_mae: 1.5943 - lr: 1.8390e-04 - 6s/epoch - 6ms/step\n",
            "Score for fold 3: loss of 18.98106575012207\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 6s - loss: 14.7779 - mse: 14.7779 - mae: 1.5483 - val_loss: 10.4417 - val_mse: 10.4417 - val_mae: 1.5939 - lr: 1.8390e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 6s - loss: 14.6060 - mse: 14.6060 - mae: 1.5432 - val_loss: 10.4402 - val_mse: 10.4402 - val_mae: 1.6006 - lr: 1.8390e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 6s - loss: 14.5519 - mse: 14.5519 - mae: 1.5395 - val_loss: 10.7409 - val_mse: 10.7409 - val_mae: 1.4688 - lr: 1.8390e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 6s - loss: 14.3820 - mse: 14.3820 - mae: 1.5439 - val_loss: 10.9704 - val_mse: 10.9704 - val_mae: 1.4659 - lr: 1.8390e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 6s - loss: 14.3311 - mse: 14.3311 - mae: 1.5354 - val_loss: 11.1240 - val_mse: 11.1240 - val_mae: 1.4973 - lr: 1.8390e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 6s - loss: 14.2520 - mse: 14.2520 - mae: 1.5498 - val_loss: 11.0206 - val_mse: 11.0206 - val_mae: 1.6129 - lr: 1.8390e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 6s - loss: 14.2422 - mse: 14.2422 - mae: 1.5312 - val_loss: 11.1207 - val_mse: 11.1207 - val_mae: 1.6033 - lr: 1.8390e-04 - 6s/epoch - 6ms/step\n",
            "Score for fold 4: loss of 11.120701789855957\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 6s - loss: 14.2891 - mse: 14.2891 - mae: 1.5646 - val_loss: 10.6941 - val_mse: 10.6941 - val_mae: 1.3879 - lr: 1.8390e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 6s - loss: 14.2466 - mse: 14.2466 - mae: 1.5666 - val_loss: 10.7095 - val_mse: 10.7095 - val_mae: 1.4412 - lr: 1.8390e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 6s - loss: 14.2415 - mse: 14.2415 - mae: 1.5614 - val_loss: 10.8084 - val_mse: 10.8084 - val_mae: 1.6688 - lr: 1.8390e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 6s - loss: 14.0824 - mse: 14.0824 - mae: 1.5535 - val_loss: 10.6513 - val_mse: 10.6513 - val_mae: 1.5224 - lr: 1.8390e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 6s - loss: 13.8322 - mse: 13.8322 - mae: 1.5560 - val_loss: 11.6304 - val_mse: 11.6304 - val_mae: 1.3941 - lr: 1.8390e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 6s - loss: 14.0292 - mse: 14.0292 - mae: 1.5540 - val_loss: 10.9930 - val_mse: 10.9930 - val_mae: 1.6569 - lr: 1.8390e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 6s - loss: 13.8106 - mse: 13.8106 - mae: 1.5547 - val_loss: 11.7013 - val_mse: 11.7013 - val_mae: 1.4629 - lr: 1.8390e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 6s - loss: 13.7494 - mse: 13.7494 - mae: 1.5434 - val_loss: 11.3144 - val_mse: 11.3144 - val_mae: 1.5460 - lr: 1.8390e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 6s - loss: 13.6795 - mse: 13.6795 - mae: 1.5306 - val_loss: 11.2439 - val_mse: 11.2439 - val_mae: 1.4308 - lr: 1.8390e-04 - 6s/epoch - 6ms/step\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-10-16 07:29:16,066]\u001b[0m Finished trial#26 resulted in value: 14.702000000000002. Current best value is 14.024000000000001 with parameters: {'activation': 'relu', 'num_hidden_layer': 4, 'i': 9, 'learning_rate': 0.00018064149922822524}.\u001b[0m\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Score for fold 5: loss of 11.243939399719238\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 8s - loss: 16.5642 - mse: 16.5642 - mae: 1.6254 - val_loss: 11.6627 - val_mse: 11.6627 - val_mae: 1.5097 - lr: 1.0166e-04 - 8s/epoch - 8ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 7s - loss: 16.0558 - mse: 16.0558 - mae: 1.5965 - val_loss: 11.7568 - val_mse: 11.7568 - val_mae: 1.5812 - lr: 1.0166e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 7s - loss: 15.9862 - mse: 15.9862 - mae: 1.5941 - val_loss: 11.5856 - val_mse: 11.5856 - val_mae: 1.5645 - lr: 1.0166e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 7s - loss: 15.9232 - mse: 15.9232 - mae: 1.5889 - val_loss: 11.4376 - val_mse: 11.4376 - val_mae: 1.5417 - lr: 1.0166e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 7s - loss: 15.8867 - mse: 15.8867 - mae: 1.5908 - val_loss: 11.3843 - val_mse: 11.3843 - val_mae: 1.5581 - lr: 1.0166e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 7s - loss: 15.8829 - mse: 15.8829 - mae: 1.5863 - val_loss: 11.4093 - val_mse: 11.4093 - val_mae: 1.5596 - lr: 1.0166e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 7s - loss: 15.8396 - mse: 15.8396 - mae: 1.5851 - val_loss: 11.5035 - val_mse: 11.5035 - val_mae: 1.6534 - lr: 1.0166e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 7s - loss: 15.7875 - mse: 15.7875 - mae: 1.5832 - val_loss: 11.4063 - val_mse: 11.4063 - val_mae: 1.5367 - lr: 1.0166e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 7s - loss: 15.7591 - mse: 15.7591 - mae: 1.5830 - val_loss: 11.4815 - val_mse: 11.4815 - val_mae: 1.5238 - lr: 1.0166e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 7s - loss: 15.7162 - mse: 15.7162 - mae: 1.5792 - val_loss: 11.3273 - val_mse: 11.3273 - val_mae: 1.5671 - lr: 1.0166e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 7s - loss: 15.7468 - mse: 15.7468 - mae: 1.5770 - val_loss: 11.3398 - val_mse: 11.3398 - val_mae: 1.5684 - lr: 1.0166e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 8s - loss: 15.6983 - mse: 15.6983 - mae: 1.5751 - val_loss: 11.4223 - val_mse: 11.4223 - val_mae: 1.5478 - lr: 1.0166e-04 - 8s/epoch - 8ms/step\n",
            "Epoch 13/100\n",
            "1000/1000 - 8s - loss: 15.6911 - mse: 15.6911 - mae: 1.5757 - val_loss: 11.4525 - val_mse: 11.4525 - val_mae: 1.5430 - lr: 1.0166e-04 - 8s/epoch - 8ms/step\n",
            "Epoch 14/100\n",
            "1000/1000 - 8s - loss: 15.6605 - mse: 15.6605 - mae: 1.5752 - val_loss: 11.3760 - val_mse: 11.3760 - val_mae: 1.5868 - lr: 1.0166e-04 - 8s/epoch - 8ms/step\n",
            "Epoch 15/100\n",
            "1000/1000 - 8s - loss: 15.6079 - mse: 15.6079 - mae: 1.5726 - val_loss: 11.4514 - val_mse: 11.4514 - val_mae: 1.5315 - lr: 1.0166e-04 - 8s/epoch - 8ms/step\n",
            "Score for fold 1: loss of 11.451383590698242\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 7s - loss: 14.0611 - mse: 14.0611 - mae: 1.5704 - val_loss: 17.5322 - val_mse: 17.5322 - val_mae: 1.5448 - lr: 1.0166e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 8s - loss: 14.0853 - mse: 14.0853 - mae: 1.5633 - val_loss: 17.6151 - val_mse: 17.6151 - val_mae: 1.5473 - lr: 1.0166e-04 - 8s/epoch - 8ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 8s - loss: 13.9883 - mse: 13.9883 - mae: 1.5665 - val_loss: 17.5208 - val_mse: 17.5208 - val_mae: 1.5807 - lr: 1.0166e-04 - 8s/epoch - 8ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 8s - loss: 13.9898 - mse: 13.9898 - mae: 1.5672 - val_loss: 17.7058 - val_mse: 17.7058 - val_mae: 1.5315 - lr: 1.0166e-04 - 8s/epoch - 8ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 8s - loss: 14.0282 - mse: 14.0282 - mae: 1.5675 - val_loss: 17.6247 - val_mse: 17.6247 - val_mae: 1.5038 - lr: 1.0166e-04 - 8s/epoch - 8ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 8s - loss: 13.9859 - mse: 13.9859 - mae: 1.5624 - val_loss: 17.6557 - val_mse: 17.6557 - val_mae: 1.5120 - lr: 1.0166e-04 - 8s/epoch - 8ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 8s - loss: 13.9500 - mse: 13.9500 - mae: 1.5570 - val_loss: 17.8669 - val_mse: 17.8669 - val_mae: 1.5221 - lr: 1.0166e-04 - 8s/epoch - 8ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 8s - loss: 13.9710 - mse: 13.9710 - mae: 1.5630 - val_loss: 17.6289 - val_mse: 17.6289 - val_mae: 1.5513 - lr: 1.0166e-04 - 8s/epoch - 8ms/step\n",
            "Score for fold 2: loss of 17.628860473632812\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 7s - loss: 12.9071 - mse: 12.9071 - mae: 1.5456 - val_loss: 21.8180 - val_mse: 21.8180 - val_mae: 1.5438 - lr: 1.0166e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 8s - loss: 12.9448 - mse: 12.9448 - mae: 1.5453 - val_loss: 21.8217 - val_mse: 21.8217 - val_mae: 1.6189 - lr: 1.0166e-04 - 8s/epoch - 8ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 8s - loss: 12.9338 - mse: 12.9338 - mae: 1.5414 - val_loss: 21.8464 - val_mse: 21.8464 - val_mae: 1.6576 - lr: 1.0166e-04 - 8s/epoch - 8ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 8s - loss: 12.8923 - mse: 12.8923 - mae: 1.5406 - val_loss: 21.9063 - val_mse: 21.9063 - val_mae: 1.6058 - lr: 1.0166e-04 - 8s/epoch - 8ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 8s - loss: 12.8169 - mse: 12.8169 - mae: 1.5406 - val_loss: 22.0294 - val_mse: 22.0294 - val_mae: 1.6063 - lr: 1.0166e-04 - 8s/epoch - 8ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 8s - loss: 12.8127 - mse: 12.8127 - mae: 1.5391 - val_loss: 21.7704 - val_mse: 21.7704 - val_mae: 1.6159 - lr: 1.0166e-04 - 8s/epoch - 8ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 8s - loss: 12.7766 - mse: 12.7766 - mae: 1.5379 - val_loss: 21.7962 - val_mse: 21.7962 - val_mae: 1.5879 - lr: 1.0166e-04 - 8s/epoch - 8ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 8s - loss: 12.7867 - mse: 12.7867 - mae: 1.5334 - val_loss: 21.9307 - val_mse: 21.9307 - val_mae: 1.5831 - lr: 1.0166e-04 - 8s/epoch - 8ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 8s - loss: 12.7842 - mse: 12.7842 - mae: 1.5360 - val_loss: 21.9706 - val_mse: 21.9706 - val_mae: 1.5727 - lr: 1.0166e-04 - 8s/epoch - 8ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 8s - loss: 12.7131 - mse: 12.7131 - mae: 1.5356 - val_loss: 21.8220 - val_mse: 21.8220 - val_mae: 1.6662 - lr: 1.0166e-04 - 8s/epoch - 8ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 8s - loss: 12.7385 - mse: 12.7385 - mae: 1.5363 - val_loss: 22.3433 - val_mse: 22.3433 - val_mae: 1.6303 - lr: 1.0166e-04 - 8s/epoch - 8ms/step\n",
            "Score for fold 3: loss of 22.343259811401367\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 7s - loss: 15.6636 - mse: 15.6636 - mae: 1.5589 - val_loss: 10.3059 - val_mse: 10.3059 - val_mae: 1.4741 - lr: 1.0166e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 7s - loss: 15.5938 - mse: 15.5938 - mae: 1.5578 - val_loss: 10.1161 - val_mse: 10.1161 - val_mae: 1.5113 - lr: 1.0166e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 8s - loss: 15.5581 - mse: 15.5581 - mae: 1.5570 - val_loss: 10.1352 - val_mse: 10.1352 - val_mae: 1.5107 - lr: 1.0166e-04 - 8s/epoch - 8ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 8s - loss: 15.5487 - mse: 15.5487 - mae: 1.5496 - val_loss: 10.2531 - val_mse: 10.2531 - val_mae: 1.5109 - lr: 1.0166e-04 - 8s/epoch - 8ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 7s - loss: 15.5750 - mse: 15.5750 - mae: 1.5485 - val_loss: 10.3059 - val_mse: 10.3059 - val_mae: 1.5396 - lr: 1.0166e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 7s - loss: 15.5350 - mse: 15.5350 - mae: 1.5516 - val_loss: 10.3217 - val_mse: 10.3217 - val_mae: 1.5485 - lr: 1.0166e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 7s - loss: 15.5109 - mse: 15.5109 - mae: 1.5501 - val_loss: 10.2478 - val_mse: 10.2478 - val_mae: 1.5848 - lr: 1.0166e-04 - 7s/epoch - 7ms/step\n",
            "Score for fold 4: loss of 10.247815132141113\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 7s - loss: 14.9682 - mse: 14.9682 - mae: 1.5442 - val_loss: 12.2313 - val_mse: 12.2313 - val_mae: 1.5310 - lr: 1.0166e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 7s - loss: 14.9626 - mse: 14.9626 - mae: 1.5409 - val_loss: 12.0812 - val_mse: 12.0812 - val_mae: 1.6452 - lr: 1.0166e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 8s - loss: 14.9541 - mse: 14.9541 - mae: 1.5398 - val_loss: 12.2318 - val_mse: 12.2318 - val_mae: 1.5233 - lr: 1.0166e-04 - 8s/epoch - 8ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 7s - loss: 14.9612 - mse: 14.9612 - mae: 1.5367 - val_loss: 12.5715 - val_mse: 12.5715 - val_mae: 1.5436 - lr: 1.0166e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 8s - loss: 14.8838 - mse: 14.8838 - mae: 1.5348 - val_loss: 12.4973 - val_mse: 12.4973 - val_mae: 1.6134 - lr: 1.0166e-04 - 8s/epoch - 8ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 7s - loss: 14.9273 - mse: 14.9273 - mae: 1.5339 - val_loss: 12.4045 - val_mse: 12.4045 - val_mae: 1.5547 - lr: 1.0166e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 8s - loss: 14.8186 - mse: 14.8186 - mae: 1.5379 - val_loss: 12.7602 - val_mse: 12.7602 - val_mae: 1.5155 - lr: 1.0166e-04 - 8s/epoch - 8ms/step\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-10-16 07:35:27,460]\u001b[0m Finished trial#27 resulted in value: 14.886000000000001. Current best value is 14.024000000000001 with parameters: {'activation': 'relu', 'num_hidden_layer': 4, 'i': 9, 'learning_rate': 0.00018064149922822524}.\u001b[0m\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Score for fold 5: loss of 12.76025104522705\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 6s - loss: 16.2800 - mse: 16.2800 - mae: 1.6209 - val_loss: 11.9269 - val_mse: 11.9269 - val_mae: 1.5139 - lr: 5.7240e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 6s - loss: 15.9666 - mse: 15.9666 - mae: 1.6110 - val_loss: 12.1231 - val_mse: 12.1231 - val_mae: 1.5513 - lr: 5.7240e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 6s - loss: 15.9097 - mse: 15.9097 - mae: 1.5984 - val_loss: 11.4922 - val_mse: 11.4922 - val_mae: 1.5972 - lr: 5.7240e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 6s - loss: 15.8812 - mse: 15.8812 - mae: 1.5984 - val_loss: 11.8097 - val_mse: 11.8097 - val_mae: 1.5264 - lr: 5.7240e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 6s - loss: 15.8203 - mse: 15.8203 - mae: 1.5890 - val_loss: 12.1303 - val_mse: 12.1303 - val_mae: 1.5687 - lr: 5.7240e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 6s - loss: 15.7174 - mse: 15.7174 - mae: 1.5891 - val_loss: 12.0756 - val_mse: 12.0756 - val_mae: 1.4649 - lr: 5.7240e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 6s - loss: 15.7421 - mse: 15.7421 - mae: 1.5905 - val_loss: 12.0137 - val_mse: 12.0137 - val_mae: 1.5331 - lr: 5.7240e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 6s - loss: 15.7442 - mse: 15.7442 - mae: 1.5899 - val_loss: 11.5382 - val_mse: 11.5382 - val_mae: 1.5085 - lr: 5.7240e-04 - 6s/epoch - 6ms/step\n",
            "Score for fold 1: loss of 11.538247108459473\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 6s - loss: 15.9793 - mse: 15.9793 - mae: 1.6038 - val_loss: 10.2996 - val_mse: 10.2996 - val_mae: 1.5227 - lr: 5.7240e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 6s - loss: 16.0459 - mse: 16.0459 - mae: 1.6149 - val_loss: 10.4242 - val_mse: 10.4242 - val_mae: 1.6016 - lr: 5.7240e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 6s - loss: 16.0128 - mse: 16.0128 - mae: 1.6098 - val_loss: 10.7323 - val_mse: 10.7323 - val_mae: 1.5448 - lr: 5.7240e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 6s - loss: 16.0346 - mse: 16.0346 - mae: 1.6108 - val_loss: 10.7055 - val_mse: 10.7055 - val_mae: 1.7113 - lr: 5.7240e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 6s - loss: 15.8777 - mse: 15.8777 - mae: 1.6093 - val_loss: 10.3135 - val_mse: 10.3135 - val_mae: 1.4601 - lr: 5.7240e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 6s - loss: 15.8145 - mse: 15.8145 - mae: 1.6090 - val_loss: 10.7321 - val_mse: 10.7321 - val_mae: 1.7645 - lr: 5.7240e-04 - 6s/epoch - 6ms/step\n",
            "Score for fold 2: loss of 10.732137680053711\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 6s - loss: 13.7808 - mse: 13.7808 - mae: 1.5910 - val_loss: 19.0323 - val_mse: 19.0323 - val_mae: 1.8408 - lr: 5.7240e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 6s - loss: 13.7249 - mse: 13.7249 - mae: 1.5833 - val_loss: 19.4428 - val_mse: 19.4428 - val_mae: 1.7984 - lr: 5.7240e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 6s - loss: 13.5993 - mse: 13.5993 - mae: 1.5798 - val_loss: 18.9909 - val_mse: 18.9909 - val_mae: 1.7715 - lr: 5.7240e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 6s - loss: 13.5746 - mse: 13.5746 - mae: 1.5826 - val_loss: 19.2245 - val_mse: 19.2245 - val_mae: 1.7793 - lr: 5.7240e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 6s - loss: 13.5444 - mse: 13.5444 - mae: 1.5811 - val_loss: 18.9689 - val_mse: 18.9689 - val_mae: 1.6692 - lr: 5.7240e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 6s - loss: 13.3689 - mse: 13.3689 - mae: 1.5815 - val_loss: 18.9401 - val_mse: 18.9401 - val_mae: 1.7078 - lr: 5.7240e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 6s - loss: 13.3804 - mse: 13.3804 - mae: 1.5742 - val_loss: 19.3070 - val_mse: 19.3070 - val_mae: 1.5847 - lr: 5.7240e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 6s - loss: 13.4057 - mse: 13.4057 - mae: 1.5857 - val_loss: 19.1559 - val_mse: 19.1559 - val_mae: 1.5661 - lr: 5.7240e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 6s - loss: 13.2779 - mse: 13.2779 - mae: 1.5746 - val_loss: 18.9294 - val_mse: 18.9294 - val_mae: 1.6590 - lr: 5.7240e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 6s - loss: 13.1527 - mse: 13.1527 - mae: 1.5771 - val_loss: 19.3685 - val_mse: 19.3685 - val_mae: 1.7059 - lr: 5.7240e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 6s - loss: 13.2872 - mse: 13.2872 - mae: 1.5724 - val_loss: 19.3443 - val_mse: 19.3443 - val_mae: 1.5981 - lr: 5.7240e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 6s - loss: 13.1549 - mse: 13.1549 - mae: 1.5666 - val_loss: 18.7173 - val_mse: 18.7173 - val_mae: 1.6380 - lr: 5.7240e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 13/100\n",
            "1000/1000 - 6s - loss: 13.1400 - mse: 13.1400 - mae: 1.5675 - val_loss: 19.2232 - val_mse: 19.2232 - val_mae: 1.6723 - lr: 5.7240e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 14/100\n",
            "1000/1000 - 6s - loss: 13.1992 - mse: 13.1992 - mae: 1.5670 - val_loss: 19.1302 - val_mse: 19.1302 - val_mae: 1.7535 - lr: 5.7240e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 15/100\n",
            "1000/1000 - 6s - loss: 13.0641 - mse: 13.0641 - mae: 1.5731 - val_loss: 19.1080 - val_mse: 19.1080 - val_mae: 1.6047 - lr: 5.7240e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 16/100\n",
            "1000/1000 - 6s - loss: 13.0626 - mse: 13.0626 - mae: 1.5545 - val_loss: 19.0015 - val_mse: 19.0015 - val_mae: 1.7237 - lr: 5.7240e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 17/100\n",
            "1000/1000 - 6s - loss: 13.0400 - mse: 13.0400 - mae: 1.5704 - val_loss: 19.2857 - val_mse: 19.2857 - val_mae: 1.6059 - lr: 5.7240e-04 - 6s/epoch - 6ms/step\n",
            "Score for fold 3: loss of 19.28573226928711\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 6s - loss: 15.5349 - mse: 15.5349 - mae: 1.6116 - val_loss: 9.3649 - val_mse: 9.3649 - val_mae: 1.5827 - lr: 5.7240e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 6s - loss: 15.3760 - mse: 15.3760 - mae: 1.6126 - val_loss: 9.2165 - val_mse: 9.2165 - val_mae: 1.5068 - lr: 5.7240e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 6s - loss: 15.4342 - mse: 15.4342 - mae: 1.6001 - val_loss: 9.2568 - val_mse: 9.2568 - val_mae: 1.6532 - lr: 5.7240e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 6s - loss: 15.5704 - mse: 15.5704 - mae: 1.6207 - val_loss: 10.0423 - val_mse: 10.0423 - val_mae: 1.4722 - lr: 5.7240e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 6s - loss: 15.4495 - mse: 15.4495 - mae: 1.6019 - val_loss: 9.5002 - val_mse: 9.5002 - val_mae: 1.4885 - lr: 5.7240e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 6s - loss: 15.4042 - mse: 15.4042 - mae: 1.6014 - val_loss: 9.4005 - val_mse: 9.4005 - val_mae: 1.6114 - lr: 5.7240e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 6s - loss: 15.5061 - mse: 15.5061 - mae: 1.6139 - val_loss: 10.1016 - val_mse: 10.1016 - val_mae: 2.0005 - lr: 5.7240e-04 - 6s/epoch - 6ms/step\n",
            "Score for fold 4: loss of 10.10158920288086\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 6s - loss: 12.1919 - mse: 12.1919 - mae: 1.6194 - val_loss: 22.9512 - val_mse: 22.9512 - val_mae: 1.6674 - lr: 5.7240e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 6s - loss: 12.1957 - mse: 12.1957 - mae: 1.6098 - val_loss: 22.6812 - val_mse: 22.6812 - val_mae: 1.5076 - lr: 5.7240e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 6s - loss: 12.0458 - mse: 12.0458 - mae: 1.6164 - val_loss: 23.3221 - val_mse: 23.3221 - val_mae: 1.5039 - lr: 5.7240e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 6s - loss: 12.1166 - mse: 12.1166 - mae: 1.6173 - val_loss: 22.8043 - val_mse: 22.8043 - val_mae: 1.7158 - lr: 5.7240e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 6s - loss: 12.0134 - mse: 12.0134 - mae: 1.6107 - val_loss: 23.8933 - val_mse: 23.8933 - val_mae: 2.2325 - lr: 5.7240e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 6s - loss: 11.8857 - mse: 11.8857 - mae: 1.6122 - val_loss: 22.8617 - val_mse: 22.8617 - val_mae: 1.8003 - lr: 5.7240e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 6s - loss: 11.8323 - mse: 11.8323 - mae: 1.6070 - val_loss: 22.5400 - val_mse: 22.5400 - val_mae: 1.6297 - lr: 5.7240e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 6s - loss: 11.8718 - mse: 11.8718 - mae: 1.6212 - val_loss: 26.6448 - val_mse: 26.6448 - val_mae: 2.2194 - lr: 5.7240e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 6s - loss: 11.8128 - mse: 11.8128 - mae: 1.6389 - val_loss: 22.9316 - val_mse: 22.9316 - val_mae: 1.7867 - lr: 5.7240e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 6s - loss: 11.9484 - mse: 11.9484 - mae: 1.6246 - val_loss: 22.6871 - val_mse: 22.6871 - val_mae: 1.6592 - lr: 5.7240e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 6s - loss: 11.7382 - mse: 11.7382 - mae: 1.6246 - val_loss: 22.7378 - val_mse: 22.7378 - val_mae: 1.5629 - lr: 5.7240e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 6s - loss: 11.6724 - mse: 11.6724 - mae: 1.6225 - val_loss: 22.7887 - val_mse: 22.7887 - val_mae: 1.6547 - lr: 5.7240e-04 - 6s/epoch - 6ms/step\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-10-16 07:40:27,540]\u001b[0m Finished trial#28 resulted in value: 14.89. Current best value is 14.024000000000001 with parameters: {'activation': 'relu', 'num_hidden_layer': 4, 'i': 9, 'learning_rate': 0.00018064149922822524}.\u001b[0m\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Score for fold 5: loss of 22.788658142089844\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 4s - loss: 15.7464 - mse: 15.7464 - mae: 1.6154 - val_loss: 15.7369 - val_mse: 15.7369 - val_mae: 1.6569 - lr: 2.3092e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 3s - loss: 14.9893 - mse: 14.9893 - mae: 1.5815 - val_loss: 15.7057 - val_mse: 15.7057 - val_mae: 1.5913 - lr: 2.3092e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 3s - loss: 14.9404 - mse: 14.9404 - mae: 1.5823 - val_loss: 15.8244 - val_mse: 15.8244 - val_mae: 1.5524 - lr: 2.3092e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 3s - loss: 14.8658 - mse: 14.8658 - mae: 1.5801 - val_loss: 15.7069 - val_mse: 15.7069 - val_mae: 1.5864 - lr: 2.3092e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 3s - loss: 14.8649 - mse: 14.8649 - mae: 1.5739 - val_loss: 15.6526 - val_mse: 15.6526 - val_mae: 1.5885 - lr: 2.3092e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 3s - loss: 14.8261 - mse: 14.8261 - mae: 1.5716 - val_loss: 15.6286 - val_mse: 15.6286 - val_mae: 1.5971 - lr: 2.3092e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 3s - loss: 14.7894 - mse: 14.7894 - mae: 1.5717 - val_loss: 15.6442 - val_mse: 15.6442 - val_mae: 1.6070 - lr: 2.3092e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 3s - loss: 14.7524 - mse: 14.7524 - mae: 1.5704 - val_loss: 15.6282 - val_mse: 15.6282 - val_mae: 1.5673 - lr: 2.3092e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 3s - loss: 14.7142 - mse: 14.7142 - mae: 1.5681 - val_loss: 15.4890 - val_mse: 15.4890 - val_mae: 1.6017 - lr: 2.3092e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 3s - loss: 14.7139 - mse: 14.7139 - mae: 1.5633 - val_loss: 15.6331 - val_mse: 15.6331 - val_mae: 1.5592 - lr: 2.3092e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 3s - loss: 14.6859 - mse: 14.6859 - mae: 1.5662 - val_loss: 15.6735 - val_mse: 15.6735 - val_mae: 1.5909 - lr: 2.3092e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 3s - loss: 14.6805 - mse: 14.6805 - mae: 1.5645 - val_loss: 15.4769 - val_mse: 15.4769 - val_mae: 1.5803 - lr: 2.3092e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 13/100\n",
            "1000/1000 - 3s - loss: 14.6833 - mse: 14.6833 - mae: 1.5627 - val_loss: 15.4620 - val_mse: 15.4620 - val_mae: 1.6261 - lr: 2.3092e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 14/100\n",
            "1000/1000 - 3s - loss: 14.6561 - mse: 14.6561 - mae: 1.5597 - val_loss: 15.6995 - val_mse: 15.6995 - val_mae: 1.5664 - lr: 2.3092e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 15/100\n",
            "1000/1000 - 3s - loss: 14.6481 - mse: 14.6481 - mae: 1.5631 - val_loss: 15.5391 - val_mse: 15.5391 - val_mae: 1.6082 - lr: 2.3092e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 16/100\n",
            "1000/1000 - 3s - loss: 14.6433 - mse: 14.6433 - mae: 1.5627 - val_loss: 15.4241 - val_mse: 15.4241 - val_mae: 1.6447 - lr: 2.3092e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 17/100\n",
            "1000/1000 - 3s - loss: 14.6491 - mse: 14.6491 - mae: 1.5589 - val_loss: 15.4098 - val_mse: 15.4098 - val_mae: 1.6430 - lr: 2.3092e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 18/100\n",
            "1000/1000 - 3s - loss: 14.6202 - mse: 14.6202 - mae: 1.5560 - val_loss: 15.4796 - val_mse: 15.4796 - val_mae: 1.6317 - lr: 2.3092e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 19/100\n",
            "1000/1000 - 3s - loss: 14.5768 - mse: 14.5768 - mae: 1.5549 - val_loss: 15.5318 - val_mse: 15.5318 - val_mae: 1.6164 - lr: 2.3092e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 20/100\n",
            "1000/1000 - 3s - loss: 14.5984 - mse: 14.5984 - mae: 1.5575 - val_loss: 15.5867 - val_mse: 15.5867 - val_mae: 1.5535 - lr: 2.3092e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 21/100\n",
            "1000/1000 - 3s - loss: 14.5695 - mse: 14.5695 - mae: 1.5543 - val_loss: 15.4836 - val_mse: 15.4836 - val_mae: 1.6039 - lr: 2.3092e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 22/100\n",
            "1000/1000 - 3s - loss: 14.5884 - mse: 14.5884 - mae: 1.5549 - val_loss: 15.5582 - val_mse: 15.5582 - val_mae: 1.5646 - lr: 2.3092e-04 - 3s/epoch - 3ms/step\n",
            "Score for fold 1: loss of 15.558216094970703\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 3s - loss: 15.5850 - mse: 15.5850 - mae: 1.5705 - val_loss: 11.3138 - val_mse: 11.3138 - val_mae: 1.5992 - lr: 2.3092e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 3s - loss: 15.6296 - mse: 15.6296 - mae: 1.5691 - val_loss: 11.5821 - val_mse: 11.5821 - val_mae: 1.5116 - lr: 2.3092e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 3s - loss: 15.5484 - mse: 15.5484 - mae: 1.5671 - val_loss: 11.4789 - val_mse: 11.4789 - val_mae: 1.5417 - lr: 2.3092e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 3s - loss: 15.5275 - mse: 15.5275 - mae: 1.5605 - val_loss: 11.3747 - val_mse: 11.3747 - val_mae: 1.5964 - lr: 2.3092e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 3s - loss: 15.5527 - mse: 15.5527 - mae: 1.5647 - val_loss: 11.4246 - val_mse: 11.4246 - val_mae: 1.5406 - lr: 2.3092e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 3s - loss: 15.5638 - mse: 15.5638 - mae: 1.5608 - val_loss: 11.4070 - val_mse: 11.4070 - val_mae: 1.5822 - lr: 2.3092e-04 - 3s/epoch - 3ms/step\n",
            "Score for fold 2: loss of 11.406967163085938\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 3s - loss: 14.9365 - mse: 14.9365 - mae: 1.5682 - val_loss: 13.8898 - val_mse: 13.8898 - val_mae: 1.5211 - lr: 2.3092e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 3s - loss: 14.9015 - mse: 14.9015 - mae: 1.5653 - val_loss: 13.5864 - val_mse: 13.5864 - val_mae: 1.5442 - lr: 2.3092e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 3s - loss: 14.8460 - mse: 14.8460 - mae: 1.5628 - val_loss: 13.9273 - val_mse: 13.9273 - val_mae: 1.5016 - lr: 2.3092e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 3s - loss: 14.8468 - mse: 14.8468 - mae: 1.5604 - val_loss: 13.9767 - val_mse: 13.9767 - val_mae: 1.5160 - lr: 2.3092e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 3s - loss: 14.8457 - mse: 14.8457 - mae: 1.5596 - val_loss: 14.1124 - val_mse: 14.1124 - val_mae: 1.5265 - lr: 2.3092e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 3s - loss: 14.8158 - mse: 14.8158 - mae: 1.5579 - val_loss: 13.9078 - val_mse: 13.9078 - val_mae: 1.5807 - lr: 2.3092e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 3s - loss: 14.8045 - mse: 14.8045 - mae: 1.5597 - val_loss: 14.0914 - val_mse: 14.0914 - val_mae: 1.5917 - lr: 2.3092e-04 - 3s/epoch - 3ms/step\n",
            "Score for fold 3: loss of 14.0913724899292\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 3s - loss: 15.2553 - mse: 15.2553 - mae: 1.5599 - val_loss: 12.1707 - val_mse: 12.1707 - val_mae: 1.5275 - lr: 2.3092e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 3s - loss: 15.2318 - mse: 15.2318 - mae: 1.5582 - val_loss: 12.3137 - val_mse: 12.3137 - val_mae: 1.5299 - lr: 2.3092e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 3s - loss: 15.1571 - mse: 15.1571 - mae: 1.5552 - val_loss: 12.2666 - val_mse: 12.2666 - val_mae: 1.5776 - lr: 2.3092e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 3s - loss: 15.1918 - mse: 15.1918 - mae: 1.5506 - val_loss: 12.4821 - val_mse: 12.4821 - val_mae: 1.5662 - lr: 2.3092e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 3s - loss: 15.1050 - mse: 15.1050 - mae: 1.5503 - val_loss: 12.5951 - val_mse: 12.5951 - val_mae: 1.5392 - lr: 2.3092e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 3s - loss: 15.0999 - mse: 15.0999 - mae: 1.5534 - val_loss: 12.3519 - val_mse: 12.3519 - val_mae: 1.5662 - lr: 2.3092e-04 - 3s/epoch - 3ms/step\n",
            "Score for fold 4: loss of 12.35188102722168\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 3s - loss: 13.1002 - mse: 13.1002 - mae: 1.5489 - val_loss: 20.4488 - val_mse: 20.4488 - val_mae: 1.6248 - lr: 2.3092e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 3s - loss: 13.1476 - mse: 13.1476 - mae: 1.5536 - val_loss: 20.4812 - val_mse: 20.4812 - val_mae: 1.5426 - lr: 2.3092e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 3s - loss: 13.0631 - mse: 13.0631 - mae: 1.5460 - val_loss: 20.7193 - val_mse: 20.7193 - val_mae: 1.5595 - lr: 2.3092e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 3s - loss: 13.0909 - mse: 13.0909 - mae: 1.5501 - val_loss: 20.7917 - val_mse: 20.7917 - val_mae: 1.5497 - lr: 2.3092e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 3s - loss: 13.0933 - mse: 13.0933 - mae: 1.5467 - val_loss: 20.7478 - val_mse: 20.7478 - val_mae: 1.5425 - lr: 2.3092e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 3s - loss: 13.0807 - mse: 13.0807 - mae: 1.5455 - val_loss: 20.6758 - val_mse: 20.6758 - val_mae: 1.5716 - lr: 2.3092e-04 - 3s/epoch - 3ms/step\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-10-16 07:42:59,077]\u001b[0m Finished trial#29 resulted in value: 14.818000000000001. Current best value is 14.024000000000001 with parameters: {'activation': 'relu', 'num_hidden_layer': 4, 'i': 9, 'learning_rate': 0.00018064149922822524}.\u001b[0m\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Score for fold 5: loss of 20.675830841064453\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 13s - loss: 14.5621 - mse: 14.5621 - mae: 1.6053 - val_loss: 18.8942 - val_mse: 18.8942 - val_mae: 1.6135 - lr: 1.8532e-04 - 13s/epoch - 13ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 12s - loss: 14.2062 - mse: 14.2062 - mae: 1.5879 - val_loss: 18.5972 - val_mse: 18.5972 - val_mae: 1.6726 - lr: 1.8532e-04 - 12s/epoch - 12ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 12s - loss: 14.1015 - mse: 14.1015 - mae: 1.5873 - val_loss: 18.8162 - val_mse: 18.8162 - val_mae: 1.5649 - lr: 1.8532e-04 - 12s/epoch - 12ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 12s - loss: 14.0900 - mse: 14.0900 - mae: 1.5814 - val_loss: 18.8726 - val_mse: 18.8726 - val_mae: 1.5935 - lr: 1.8532e-04 - 12s/epoch - 12ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 12s - loss: 14.0468 - mse: 14.0468 - mae: 1.5801 - val_loss: 18.6376 - val_mse: 18.6376 - val_mae: 1.6524 - lr: 1.8532e-04 - 12s/epoch - 12ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 12s - loss: 13.9766 - mse: 13.9766 - mae: 1.5744 - val_loss: 18.6287 - val_mse: 18.6287 - val_mae: 1.6184 - lr: 1.8532e-04 - 12s/epoch - 12ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 12s - loss: 13.9622 - mse: 13.9622 - mae: 1.5737 - val_loss: 18.5371 - val_mse: 18.5371 - val_mae: 1.7491 - lr: 1.8532e-04 - 12s/epoch - 12ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 12s - loss: 13.8397 - mse: 13.8397 - mae: 1.5702 - val_loss: 18.5595 - val_mse: 18.5595 - val_mae: 1.5653 - lr: 1.8532e-04 - 12s/epoch - 12ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 12s - loss: 13.8270 - mse: 13.8270 - mae: 1.5707 - val_loss: 18.6690 - val_mse: 18.6690 - val_mae: 1.6434 - lr: 1.8532e-04 - 12s/epoch - 12ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 12s - loss: 13.7726 - mse: 13.7726 - mae: 1.5655 - val_loss: 18.7642 - val_mse: 18.7642 - val_mae: 1.7690 - lr: 1.8532e-04 - 12s/epoch - 12ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 12s - loss: 13.6481 - mse: 13.6481 - mae: 1.5652 - val_loss: 18.7797 - val_mse: 18.7797 - val_mae: 1.5892 - lr: 1.8532e-04 - 12s/epoch - 12ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 12s - loss: 13.6087 - mse: 13.6087 - mae: 1.5611 - val_loss: 18.7862 - val_mse: 18.7862 - val_mae: 1.5983 - lr: 1.8532e-04 - 12s/epoch - 12ms/step\n",
            "Score for fold 1: loss of 18.78618621826172\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 12s - loss: 15.4672 - mse: 15.4672 - mae: 1.5766 - val_loss: 11.5054 - val_mse: 11.5054 - val_mae: 1.5574 - lr: 1.8532e-04 - 12s/epoch - 12ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 12s - loss: 15.4957 - mse: 15.4957 - mae: 1.5771 - val_loss: 11.3969 - val_mse: 11.3969 - val_mae: 1.5723 - lr: 1.8532e-04 - 12s/epoch - 12ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 12s - loss: 15.2907 - mse: 15.2907 - mae: 1.5719 - val_loss: 12.1125 - val_mse: 12.1125 - val_mae: 1.4750 - lr: 1.8532e-04 - 12s/epoch - 12ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 12s - loss: 15.1562 - mse: 15.1562 - mae: 1.5775 - val_loss: 11.8911 - val_mse: 11.8911 - val_mae: 1.5027 - lr: 1.8532e-04 - 12s/epoch - 12ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 12s - loss: 15.2293 - mse: 15.2293 - mae: 1.5686 - val_loss: 11.5623 - val_mse: 11.5623 - val_mae: 1.5870 - lr: 1.8532e-04 - 12s/epoch - 12ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 12s - loss: 15.1215 - mse: 15.1215 - mae: 1.5809 - val_loss: 12.2650 - val_mse: 12.2650 - val_mae: 1.5286 - lr: 1.8532e-04 - 12s/epoch - 12ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 12s - loss: 15.1603 - mse: 15.1603 - mae: 1.5761 - val_loss: 11.6639 - val_mse: 11.6639 - val_mae: 1.5727 - lr: 1.8532e-04 - 12s/epoch - 12ms/step\n",
            "Score for fold 2: loss of 11.663878440856934\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 12s - loss: 12.7355 - mse: 12.7355 - mae: 1.5785 - val_loss: 21.1742 - val_mse: 21.1742 - val_mae: 1.4672 - lr: 1.8532e-04 - 12s/epoch - 12ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 12s - loss: 12.6731 - mse: 12.6731 - mae: 1.5877 - val_loss: 21.1737 - val_mse: 21.1737 - val_mae: 1.7884 - lr: 1.8532e-04 - 12s/epoch - 12ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 12s - loss: 12.4887 - mse: 12.4887 - mae: 1.5834 - val_loss: 21.3020 - val_mse: 21.3020 - val_mae: 1.7293 - lr: 1.8532e-04 - 12s/epoch - 12ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 12s - loss: 12.4070 - mse: 12.4070 - mae: 1.5929 - val_loss: 21.0343 - val_mse: 21.0343 - val_mae: 1.6174 - lr: 1.8532e-04 - 12s/epoch - 12ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 12s - loss: 12.3826 - mse: 12.3826 - mae: 1.6044 - val_loss: 21.0907 - val_mse: 21.0907 - val_mae: 1.5912 - lr: 1.8532e-04 - 12s/epoch - 12ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 12s - loss: 12.3344 - mse: 12.3344 - mae: 1.5960 - val_loss: 21.2737 - val_mse: 21.2737 - val_mae: 1.5435 - lr: 1.8532e-04 - 12s/epoch - 12ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 12s - loss: 12.0432 - mse: 12.0432 - mae: 1.6096 - val_loss: 21.7078 - val_mse: 21.7078 - val_mae: 1.5122 - lr: 1.8532e-04 - 12s/epoch - 12ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 12s - loss: 12.1568 - mse: 12.1568 - mae: 1.6025 - val_loss: 22.6388 - val_mse: 22.6388 - val_mae: 1.6106 - lr: 1.8532e-04 - 12s/epoch - 12ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 12s - loss: 12.2049 - mse: 12.2049 - mae: 1.6357 - val_loss: 21.3590 - val_mse: 21.3590 - val_mae: 1.5647 - lr: 1.8532e-04 - 12s/epoch - 12ms/step\n",
            "Score for fold 3: loss of 21.3590145111084\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 12s - loss: 15.0070 - mse: 15.0070 - mae: 1.6464 - val_loss: 9.3476 - val_mse: 9.3476 - val_mae: 1.4425 - lr: 1.8532e-04 - 12s/epoch - 12ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 12s - loss: 15.0701 - mse: 15.0701 - mae: 1.6528 - val_loss: 9.2137 - val_mse: 9.2137 - val_mae: 1.6840 - lr: 1.8532e-04 - 12s/epoch - 12ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 12s - loss: 15.1115 - mse: 15.1115 - mae: 1.6605 - val_loss: 9.3201 - val_mse: 9.3201 - val_mae: 1.6596 - lr: 1.8532e-04 - 12s/epoch - 12ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 12s - loss: 15.1339 - mse: 15.1339 - mae: 1.6471 - val_loss: 9.6189 - val_mse: 9.6189 - val_mae: 1.6765 - lr: 1.8532e-04 - 12s/epoch - 12ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 12s - loss: 14.8827 - mse: 14.8827 - mae: 1.6679 - val_loss: 10.0440 - val_mse: 10.0440 - val_mae: 1.5703 - lr: 1.8532e-04 - 12s/epoch - 12ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 12s - loss: 14.7612 - mse: 14.7612 - mae: 1.6481 - val_loss: 9.3041 - val_mse: 9.3041 - val_mae: 1.5056 - lr: 1.8532e-04 - 12s/epoch - 12ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 12s - loss: 14.8015 - mse: 14.8015 - mae: 1.6459 - val_loss: 10.7966 - val_mse: 10.7966 - val_mae: 2.0978 - lr: 1.8532e-04 - 12s/epoch - 12ms/step\n",
            "Score for fold 4: loss of 10.796599388122559\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 12s - loss: 14.4783 - mse: 14.4783 - mae: 1.6453 - val_loss: 11.8027 - val_mse: 11.8027 - val_mae: 1.4901 - lr: 1.8532e-04 - 12s/epoch - 12ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 12s - loss: 14.3007 - mse: 14.3007 - mae: 1.6558 - val_loss: 11.6319 - val_mse: 11.6319 - val_mae: 1.9144 - lr: 1.8532e-04 - 12s/epoch - 12ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 12s - loss: 14.3305 - mse: 14.3305 - mae: 1.6509 - val_loss: 11.2872 - val_mse: 11.2872 - val_mae: 1.5191 - lr: 1.8532e-04 - 12s/epoch - 12ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 12s - loss: 14.2097 - mse: 14.2097 - mae: 1.6429 - val_loss: 11.3372 - val_mse: 11.3372 - val_mae: 1.5408 - lr: 1.8532e-04 - 12s/epoch - 12ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 12s - loss: 14.2627 - mse: 14.2627 - mae: 1.6479 - val_loss: 11.2915 - val_mse: 11.2915 - val_mae: 1.4719 - lr: 1.8532e-04 - 12s/epoch - 12ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 12s - loss: 14.0657 - mse: 14.0657 - mae: 1.6175 - val_loss: 11.9221 - val_mse: 11.9221 - val_mae: 1.4990 - lr: 1.8532e-04 - 12s/epoch - 12ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 12s - loss: 13.7842 - mse: 13.7842 - mae: 1.6141 - val_loss: 11.7985 - val_mse: 11.7985 - val_mae: 1.4600 - lr: 1.8532e-04 - 12s/epoch - 12ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 12s - loss: 13.7583 - mse: 13.7583 - mae: 1.6310 - val_loss: 12.1560 - val_mse: 12.1560 - val_mae: 1.9094 - lr: 1.8532e-04 - 12s/epoch - 12ms/step\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-10-16 07:51:50,962]\u001b[0m Finished trial#30 resulted in value: 14.953999999999999. Current best value is 14.024000000000001 with parameters: {'activation': 'relu', 'num_hidden_layer': 4, 'i': 9, 'learning_rate': 0.00018064149922822524}.\u001b[0m\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Score for fold 5: loss of 12.155999183654785\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 6s - loss: 16.4921 - mse: 16.4921 - mae: 1.6207 - val_loss: 11.3243 - val_mse: 11.3243 - val_mae: 1.7394 - lr: 2.6815e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 6s - loss: 16.1168 - mse: 16.1168 - mae: 1.6008 - val_loss: 11.2038 - val_mse: 11.2038 - val_mae: 1.5616 - lr: 2.6815e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 6s - loss: 16.0137 - mse: 16.0137 - mae: 1.5984 - val_loss: 11.2654 - val_mse: 11.2654 - val_mae: 1.5287 - lr: 2.6815e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 6s - loss: 15.9710 - mse: 15.9710 - mae: 1.5959 - val_loss: 11.1072 - val_mse: 11.1072 - val_mae: 1.6072 - lr: 2.6815e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 6s - loss: 15.8939 - mse: 15.8939 - mae: 1.5882 - val_loss: 11.3400 - val_mse: 11.3400 - val_mae: 1.5621 - lr: 2.6815e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 6s - loss: 15.8040 - mse: 15.8040 - mae: 1.5892 - val_loss: 11.1454 - val_mse: 11.1454 - val_mae: 1.6171 - lr: 2.6815e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 6s - loss: 15.7945 - mse: 15.7945 - mae: 1.5812 - val_loss: 11.1899 - val_mse: 11.1899 - val_mae: 1.5264 - lr: 2.6815e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 6s - loss: 15.7662 - mse: 15.7662 - mae: 1.5770 - val_loss: 11.3523 - val_mse: 11.3523 - val_mae: 1.6120 - lr: 2.6815e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 6s - loss: 15.7538 - mse: 15.7538 - mae: 1.5798 - val_loss: 11.1515 - val_mse: 11.1515 - val_mae: 1.5080 - lr: 2.6815e-04 - 6s/epoch - 6ms/step\n",
            "Score for fold 1: loss of 11.151493072509766\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 6s - loss: 13.4017 - mse: 13.4017 - mae: 1.5827 - val_loss: 21.0912 - val_mse: 21.0912 - val_mae: 1.5409 - lr: 2.6815e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 6s - loss: 13.2635 - mse: 13.2635 - mae: 1.5776 - val_loss: 20.6204 - val_mse: 20.6204 - val_mae: 1.6462 - lr: 2.6815e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 6s - loss: 13.2675 - mse: 13.2675 - mae: 1.5731 - val_loss: 20.8818 - val_mse: 20.8818 - val_mae: 1.5502 - lr: 2.6815e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 6s - loss: 13.1085 - mse: 13.1085 - mae: 1.5779 - val_loss: 20.8148 - val_mse: 20.8148 - val_mae: 1.5161 - lr: 2.6815e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 6s - loss: 13.0573 - mse: 13.0573 - mae: 1.5749 - val_loss: 20.9283 - val_mse: 20.9283 - val_mae: 1.6436 - lr: 2.6815e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 6s - loss: 12.9164 - mse: 12.9164 - mae: 1.5726 - val_loss: 21.0905 - val_mse: 21.0905 - val_mae: 1.5436 - lr: 2.6815e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 6s - loss: 12.9431 - mse: 12.9431 - mae: 1.5699 - val_loss: 20.9728 - val_mse: 20.9728 - val_mae: 1.5543 - lr: 2.6815e-04 - 6s/epoch - 6ms/step\n",
            "Score for fold 2: loss of 20.972854614257812\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 5s - loss: 13.9050 - mse: 13.9050 - mae: 1.5646 - val_loss: 16.7131 - val_mse: 16.7131 - val_mae: 1.6058 - lr: 2.6815e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 5s - loss: 13.8426 - mse: 13.8426 - mae: 1.5653 - val_loss: 16.5472 - val_mse: 16.5472 - val_mae: 1.6313 - lr: 2.6815e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 6s - loss: 13.8247 - mse: 13.8247 - mae: 1.5655 - val_loss: 17.0346 - val_mse: 17.0346 - val_mae: 1.5255 - lr: 2.6815e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 6s - loss: 13.7549 - mse: 13.7549 - mae: 1.5613 - val_loss: 16.5282 - val_mse: 16.5282 - val_mae: 1.7027 - lr: 2.6815e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 6s - loss: 13.5814 - mse: 13.5814 - mae: 1.5579 - val_loss: 17.1270 - val_mse: 17.1270 - val_mae: 1.7947 - lr: 2.6815e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 6s - loss: 13.4848 - mse: 13.4848 - mae: 1.5604 - val_loss: 16.5011 - val_mse: 16.5011 - val_mae: 1.6104 - lr: 2.6815e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 6s - loss: 13.2665 - mse: 13.2665 - mae: 1.5459 - val_loss: 17.0618 - val_mse: 17.0618 - val_mae: 1.6303 - lr: 2.6815e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 6s - loss: 13.4437 - mse: 13.4437 - mae: 1.5571 - val_loss: 16.7535 - val_mse: 16.7535 - val_mae: 1.5312 - lr: 2.6815e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 6s - loss: 13.2161 - mse: 13.2161 - mae: 1.5435 - val_loss: 16.8913 - val_mse: 16.8913 - val_mae: 1.8932 - lr: 2.6815e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 6s - loss: 13.2285 - mse: 13.2285 - mae: 1.5548 - val_loss: 17.1140 - val_mse: 17.1140 - val_mae: 1.5413 - lr: 2.6815e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 6s - loss: 13.0876 - mse: 13.0876 - mae: 1.5573 - val_loss: 16.9843 - val_mse: 16.9843 - val_mae: 1.6178 - lr: 2.6815e-04 - 6s/epoch - 6ms/step\n",
            "Score for fold 3: loss of 16.984296798706055\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 5s - loss: 14.2289 - mse: 14.2289 - mae: 1.5749 - val_loss: 12.1511 - val_mse: 12.1511 - val_mae: 1.4745 - lr: 2.6815e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 6s - loss: 14.2477 - mse: 14.2477 - mae: 1.5800 - val_loss: 12.3021 - val_mse: 12.3021 - val_mae: 1.4577 - lr: 2.6815e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 6s - loss: 14.0704 - mse: 14.0704 - mae: 1.5750 - val_loss: 12.7087 - val_mse: 12.7087 - val_mae: 1.4789 - lr: 2.6815e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 6s - loss: 14.1081 - mse: 14.1081 - mae: 1.5917 - val_loss: 12.4416 - val_mse: 12.4416 - val_mae: 1.7267 - lr: 2.6815e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 6s - loss: 14.0063 - mse: 14.0063 - mae: 1.6033 - val_loss: 12.4427 - val_mse: 12.4427 - val_mae: 1.4820 - lr: 2.6815e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 6s - loss: 13.9149 - mse: 13.9149 - mae: 1.5707 - val_loss: 13.1595 - val_mse: 13.1595 - val_mae: 1.4760 - lr: 2.6815e-04 - 6s/epoch - 6ms/step\n",
            "Score for fold 4: loss of 13.159547805786133\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 5s - loss: 14.5595 - mse: 14.5595 - mae: 1.5914 - val_loss: 10.2009 - val_mse: 10.2009 - val_mae: 1.5663 - lr: 2.6815e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 6s - loss: 14.3878 - mse: 14.3878 - mae: 1.6052 - val_loss: 9.9375 - val_mse: 9.9375 - val_mae: 1.6040 - lr: 2.6815e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 6s - loss: 14.2278 - mse: 14.2278 - mae: 1.6019 - val_loss: 10.3203 - val_mse: 10.3203 - val_mae: 1.6499 - lr: 2.6815e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 6s - loss: 14.3697 - mse: 14.3697 - mae: 1.6007 - val_loss: 11.7785 - val_mse: 11.7785 - val_mae: 2.0960 - lr: 2.6815e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 6s - loss: 14.2056 - mse: 14.2056 - mae: 1.6002 - val_loss: 11.2584 - val_mse: 11.2584 - val_mae: 1.4501 - lr: 2.6815e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 6s - loss: 14.2472 - mse: 14.2472 - mae: 1.6076 - val_loss: 11.1076 - val_mse: 11.1076 - val_mae: 1.4485 - lr: 2.6815e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 6s - loss: 14.1871 - mse: 14.1871 - mae: 1.6041 - val_loss: 11.3874 - val_mse: 11.3874 - val_mae: 1.5827 - lr: 2.6815e-04 - 6s/epoch - 6ms/step\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-10-16 07:55:48,539]\u001b[0m Finished trial#31 resulted in value: 14.729999999999999. Current best value is 14.024000000000001 with parameters: {'activation': 'relu', 'num_hidden_layer': 4, 'i': 9, 'learning_rate': 0.00018064149922822524}.\u001b[0m\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Score for fold 5: loss of 11.387423515319824\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 6s - loss: 15.7263 - mse: 15.7263 - mae: 1.6067 - val_loss: 14.5607 - val_mse: 14.5607 - val_mae: 1.6001 - lr: 3.9984e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 6s - loss: 15.3392 - mse: 15.3392 - mae: 1.5933 - val_loss: 14.1989 - val_mse: 14.1989 - val_mae: 1.5857 - lr: 3.9984e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 6s - loss: 15.2203 - mse: 15.2203 - mae: 1.5839 - val_loss: 14.7416 - val_mse: 14.7416 - val_mae: 1.6111 - lr: 3.9984e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 6s - loss: 15.1871 - mse: 15.1871 - mae: 1.5796 - val_loss: 14.2037 - val_mse: 14.2037 - val_mae: 1.6392 - lr: 3.9984e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 6s - loss: 15.1330 - mse: 15.1330 - mae: 1.5786 - val_loss: 14.1029 - val_mse: 14.1029 - val_mae: 1.6210 - lr: 3.9984e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 6s - loss: 15.1479 - mse: 15.1479 - mae: 1.5757 - val_loss: 14.2286 - val_mse: 14.2286 - val_mae: 1.6370 - lr: 3.9984e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 6s - loss: 15.1063 - mse: 15.1063 - mae: 1.5733 - val_loss: 14.1410 - val_mse: 14.1410 - val_mae: 1.5601 - lr: 3.9984e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 6s - loss: 15.1343 - mse: 15.1343 - mae: 1.5690 - val_loss: 13.9615 - val_mse: 13.9615 - val_mae: 1.6262 - lr: 3.9984e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 6s - loss: 15.0347 - mse: 15.0347 - mae: 1.5612 - val_loss: 14.0628 - val_mse: 14.0628 - val_mae: 1.6474 - lr: 3.9984e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 6s - loss: 14.9479 - mse: 14.9479 - mae: 1.5672 - val_loss: 14.0983 - val_mse: 14.0983 - val_mae: 1.7831 - lr: 3.9984e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 6s - loss: 14.9759 - mse: 14.9759 - mae: 1.5690 - val_loss: 14.2891 - val_mse: 14.2891 - val_mae: 1.5338 - lr: 3.9984e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 6s - loss: 14.8574 - mse: 14.8574 - mae: 1.5649 - val_loss: 14.1827 - val_mse: 14.1827 - val_mae: 1.7128 - lr: 3.9984e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 13/100\n",
            "1000/1000 - 6s - loss: 14.7444 - mse: 14.7444 - mae: 1.5652 - val_loss: 14.3391 - val_mse: 14.3391 - val_mae: 1.4973 - lr: 3.9984e-04 - 6s/epoch - 6ms/step\n",
            "Score for fold 1: loss of 14.339096069335938\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 6s - loss: 14.6403 - mse: 14.6403 - mae: 1.5787 - val_loss: 14.8648 - val_mse: 14.8648 - val_mae: 1.5388 - lr: 3.9984e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 6s - loss: 14.6653 - mse: 14.6653 - mae: 1.5807 - val_loss: 14.8826 - val_mse: 14.8826 - val_mae: 1.7149 - lr: 3.9984e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 6s - loss: 14.6218 - mse: 14.6218 - mae: 1.5654 - val_loss: 14.8616 - val_mse: 14.8616 - val_mae: 1.5387 - lr: 3.9984e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 6s - loss: 14.5386 - mse: 14.5386 - mae: 1.5917 - val_loss: 15.1117 - val_mse: 15.1117 - val_mae: 1.5679 - lr: 3.9984e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 6s - loss: 14.4581 - mse: 14.4581 - mae: 1.5785 - val_loss: 15.0044 - val_mse: 15.0044 - val_mae: 1.6471 - lr: 3.9984e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 6s - loss: 14.3797 - mse: 14.3797 - mae: 1.5790 - val_loss: 15.0881 - val_mse: 15.0881 - val_mae: 1.5341 - lr: 3.9984e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 6s - loss: 14.4210 - mse: 14.4210 - mae: 1.5810 - val_loss: 14.9063 - val_mse: 14.9063 - val_mae: 1.5963 - lr: 3.9984e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 6s - loss: 14.3157 - mse: 14.3157 - mae: 1.5857 - val_loss: 15.5638 - val_mse: 15.5638 - val_mae: 1.4868 - lr: 3.9984e-04 - 6s/epoch - 6ms/step\n",
            "Score for fold 2: loss of 15.563764572143555\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 6s - loss: 12.9627 - mse: 12.9627 - mae: 1.5942 - val_loss: 20.4732 - val_mse: 20.4732 - val_mae: 1.5091 - lr: 3.9984e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 6s - loss: 12.8149 - mse: 12.8149 - mae: 1.5899 - val_loss: 20.5074 - val_mse: 20.5074 - val_mae: 1.5027 - lr: 3.9984e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 6s - loss: 13.0052 - mse: 13.0052 - mae: 1.5983 - val_loss: 20.5439 - val_mse: 20.5439 - val_mae: 1.7478 - lr: 3.9984e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 6s - loss: 12.7172 - mse: 12.7172 - mae: 1.5851 - val_loss: 20.6373 - val_mse: 20.6373 - val_mae: 1.5998 - lr: 3.9984e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 6s - loss: 12.8316 - mse: 12.8316 - mae: 1.6084 - val_loss: 20.8549 - val_mse: 20.8549 - val_mae: 1.4891 - lr: 3.9984e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 6s - loss: 12.6855 - mse: 12.6855 - mae: 1.5926 - val_loss: 21.0348 - val_mse: 21.0348 - val_mae: 1.5788 - lr: 3.9984e-04 - 6s/epoch - 6ms/step\n",
            "Score for fold 3: loss of 21.03481101989746\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 6s - loss: 15.1739 - mse: 15.1739 - mae: 1.6192 - val_loss: 10.9025 - val_mse: 10.9025 - val_mae: 1.4115 - lr: 3.9984e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 6s - loss: 15.0647 - mse: 15.0647 - mae: 1.6233 - val_loss: 10.9634 - val_mse: 10.9634 - val_mae: 1.6149 - lr: 3.9984e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 6s - loss: 14.9898 - mse: 14.9898 - mae: 1.6131 - val_loss: 11.0627 - val_mse: 11.0627 - val_mae: 1.5266 - lr: 3.9984e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 6s - loss: 14.9312 - mse: 14.9312 - mae: 1.6239 - val_loss: 11.2399 - val_mse: 11.2399 - val_mae: 1.4782 - lr: 3.9984e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 6s - loss: 14.7167 - mse: 14.7167 - mae: 1.6108 - val_loss: 11.0521 - val_mse: 11.0521 - val_mae: 1.5329 - lr: 3.9984e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 6s - loss: 14.9535 - mse: 14.9535 - mae: 1.6429 - val_loss: 11.4421 - val_mse: 11.4421 - val_mae: 1.4311 - lr: 3.9984e-04 - 6s/epoch - 6ms/step\n",
            "Score for fold 4: loss of 11.442060470581055\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 6s - loss: 14.7483 - mse: 14.7483 - mae: 1.6352 - val_loss: 11.4066 - val_mse: 11.4066 - val_mae: 1.4437 - lr: 3.9984e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 6s - loss: 14.6146 - mse: 14.6146 - mae: 1.6258 - val_loss: 12.5242 - val_mse: 12.5242 - val_mae: 1.9040 - lr: 3.9984e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 6s - loss: 14.6817 - mse: 14.6817 - mae: 1.6277 - val_loss: 12.2363 - val_mse: 12.2363 - val_mae: 1.4864 - lr: 3.9984e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 6s - loss: 14.6242 - mse: 14.6242 - mae: 1.6411 - val_loss: 12.6871 - val_mse: 12.6871 - val_mae: 1.5415 - lr: 3.9984e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 6s - loss: 14.6571 - mse: 14.6571 - mae: 1.6344 - val_loss: 12.0361 - val_mse: 12.0361 - val_mae: 1.5590 - lr: 3.9984e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 6s - loss: 14.6306 - mse: 14.6306 - mae: 1.6267 - val_loss: 11.9633 - val_mse: 11.9633 - val_mae: 1.6916 - lr: 3.9984e-04 - 6s/epoch - 6ms/step\n",
            "Score for fold 5: loss of 11.96329402923584\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-10-16 07:59:41,082]\u001b[0m Finished trial#32 resulted in value: 14.866. Current best value is 14.024000000000001 with parameters: {'activation': 'relu', 'num_hidden_layer': 4, 'i': 9, 'learning_rate': 0.00018064149922822524}.\u001b[0m\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 6s - loss: 13.9525 - mse: 13.9525 - mae: 1.6078 - val_loss: 21.3229 - val_mse: 21.3229 - val_mae: 1.7005 - lr: 1.6652e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 6s - loss: 13.5363 - mse: 13.5363 - mae: 1.5831 - val_loss: 21.2314 - val_mse: 21.2314 - val_mae: 1.6458 - lr: 1.6652e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 6s - loss: 13.4489 - mse: 13.4489 - mae: 1.5775 - val_loss: 21.6692 - val_mse: 21.6692 - val_mae: 1.5485 - lr: 1.6652e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 6s - loss: 13.4269 - mse: 13.4269 - mae: 1.5768 - val_loss: 21.4414 - val_mse: 21.4414 - val_mae: 1.6129 - lr: 1.6652e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 6s - loss: 13.4382 - mse: 13.4382 - mae: 1.5781 - val_loss: 21.1493 - val_mse: 21.1493 - val_mae: 1.6039 - lr: 1.6652e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 6s - loss: 13.3617 - mse: 13.3617 - mae: 1.5774 - val_loss: 21.2051 - val_mse: 21.2051 - val_mae: 1.5981 - lr: 1.6652e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 6s - loss: 13.2840 - mse: 13.2840 - mae: 1.5680 - val_loss: 21.3130 - val_mse: 21.3130 - val_mae: 1.5656 - lr: 1.6652e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 6s - loss: 13.3436 - mse: 13.3436 - mae: 1.5680 - val_loss: 21.1425 - val_mse: 21.1425 - val_mae: 1.5798 - lr: 1.6652e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 6s - loss: 13.2438 - mse: 13.2438 - mae: 1.5626 - val_loss: 21.1411 - val_mse: 21.1411 - val_mae: 1.6242 - lr: 1.6652e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 6s - loss: 13.1679 - mse: 13.1679 - mae: 1.5627 - val_loss: 21.1729 - val_mse: 21.1729 - val_mae: 1.6575 - lr: 1.6652e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 6s - loss: 13.0966 - mse: 13.0966 - mae: 1.5567 - val_loss: 21.0208 - val_mse: 21.0208 - val_mae: 1.6673 - lr: 1.6652e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 6s - loss: 13.1649 - mse: 13.1649 - mae: 1.5615 - val_loss: 21.0868 - val_mse: 21.0868 - val_mae: 1.6486 - lr: 1.6652e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 13/100\n",
            "1000/1000 - 6s - loss: 13.0814 - mse: 13.0814 - mae: 1.5582 - val_loss: 20.9846 - val_mse: 20.9846 - val_mae: 1.6690 - lr: 1.6652e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 14/100\n",
            "1000/1000 - 6s - loss: 12.9903 - mse: 12.9903 - mae: 1.5530 - val_loss: 21.0509 - val_mse: 21.0509 - val_mae: 1.6432 - lr: 1.6652e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 15/100\n",
            "1000/1000 - 6s - loss: 12.9541 - mse: 12.9541 - mae: 1.5497 - val_loss: 21.0540 - val_mse: 21.0540 - val_mae: 1.6266 - lr: 1.6652e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 16/100\n",
            "1000/1000 - 6s - loss: 12.9358 - mse: 12.9358 - mae: 1.5465 - val_loss: 20.9015 - val_mse: 20.9015 - val_mae: 1.6337 - lr: 1.6652e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 17/100\n",
            "1000/1000 - 6s - loss: 12.8566 - mse: 12.8566 - mae: 1.5456 - val_loss: 21.0206 - val_mse: 21.0206 - val_mae: 1.6071 - lr: 1.6652e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 18/100\n",
            "1000/1000 - 6s - loss: 12.8152 - mse: 12.8152 - mae: 1.5445 - val_loss: 20.8217 - val_mse: 20.8217 - val_mae: 1.5754 - lr: 1.6652e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 19/100\n",
            "1000/1000 - 6s - loss: 12.7448 - mse: 12.7448 - mae: 1.5411 - val_loss: 21.0276 - val_mse: 21.0276 - val_mae: 1.6167 - lr: 1.6652e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 20/100\n",
            "1000/1000 - 6s - loss: 12.6216 - mse: 12.6216 - mae: 1.5406 - val_loss: 20.7823 - val_mse: 20.7823 - val_mae: 1.6440 - lr: 1.6652e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 21/100\n",
            "1000/1000 - 6s - loss: 12.4881 - mse: 12.4881 - mae: 1.5375 - val_loss: 20.9325 - val_mse: 20.9325 - val_mae: 1.5687 - lr: 1.6652e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 22/100\n",
            "1000/1000 - 6s - loss: 12.4812 - mse: 12.4812 - mae: 1.5361 - val_loss: 20.9434 - val_mse: 20.9434 - val_mae: 1.5983 - lr: 1.6652e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 23/100\n",
            "1000/1000 - 6s - loss: 12.4394 - mse: 12.4394 - mae: 1.5304 - val_loss: 20.9029 - val_mse: 20.9029 - val_mae: 1.5998 - lr: 1.6652e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 24/100\n",
            "1000/1000 - 6s - loss: 12.3331 - mse: 12.3331 - mae: 1.5246 - val_loss: 21.3481 - val_mse: 21.3481 - val_mae: 1.5395 - lr: 1.6652e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 25/100\n",
            "1000/1000 - 6s - loss: 12.3784 - mse: 12.3784 - mae: 1.5213 - val_loss: 20.9175 - val_mse: 20.9175 - val_mae: 1.6197 - lr: 1.6652e-04 - 6s/epoch - 6ms/step\n",
            "Score for fold 1: loss of 20.917482376098633\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 6s - loss: 15.0619 - mse: 15.0619 - mae: 1.5548 - val_loss: 10.3539 - val_mse: 10.3539 - val_mae: 1.5452 - lr: 1.6652e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 6s - loss: 14.7977 - mse: 14.7977 - mae: 1.5513 - val_loss: 10.5685 - val_mse: 10.5685 - val_mae: 1.4625 - lr: 1.6652e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 6s - loss: 14.6944 - mse: 14.6944 - mae: 1.5371 - val_loss: 10.8872 - val_mse: 10.8872 - val_mae: 1.4810 - lr: 1.6652e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 6s - loss: 14.6890 - mse: 14.6890 - mae: 1.5394 - val_loss: 10.7152 - val_mse: 10.7152 - val_mae: 1.5088 - lr: 1.6652e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 6s - loss: 14.3558 - mse: 14.3558 - mae: 1.5368 - val_loss: 10.9047 - val_mse: 10.9047 - val_mae: 1.4670 - lr: 1.6652e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 6s - loss: 14.5175 - mse: 14.5175 - mae: 1.5330 - val_loss: 10.8015 - val_mse: 10.8015 - val_mae: 1.6036 - lr: 1.6652e-04 - 6s/epoch - 6ms/step\n",
            "Score for fold 2: loss of 10.801536560058594\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 6s - loss: 13.6719 - mse: 13.6719 - mae: 1.5394 - val_loss: 14.3949 - val_mse: 14.3949 - val_mae: 1.5449 - lr: 1.6652e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 6s - loss: 13.5145 - mse: 13.5145 - mae: 1.5321 - val_loss: 14.3685 - val_mse: 14.3685 - val_mae: 1.5479 - lr: 1.6652e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 6s - loss: 13.4233 - mse: 13.4233 - mae: 1.5283 - val_loss: 14.1786 - val_mse: 14.1786 - val_mae: 1.6861 - lr: 1.6652e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 6s - loss: 13.3269 - mse: 13.3269 - mae: 1.5219 - val_loss: 14.4872 - val_mse: 14.4872 - val_mae: 1.6391 - lr: 1.6652e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 6s - loss: 13.2338 - mse: 13.2338 - mae: 1.5215 - val_loss: 14.7132 - val_mse: 14.7132 - val_mae: 1.5153 - lr: 1.6652e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 6s - loss: 13.1229 - mse: 13.1229 - mae: 1.5178 - val_loss: 14.4070 - val_mse: 14.4070 - val_mae: 1.5327 - lr: 1.6652e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 6s - loss: 13.0038 - mse: 13.0038 - mae: 1.5122 - val_loss: 14.7791 - val_mse: 14.7791 - val_mae: 1.5630 - lr: 1.6652e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 6s - loss: 12.9147 - mse: 12.9147 - mae: 1.5050 - val_loss: 14.9093 - val_mse: 14.9093 - val_mae: 1.4540 - lr: 1.6652e-04 - 6s/epoch - 6ms/step\n",
            "Score for fold 3: loss of 14.909314155578613\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 6s - loss: 13.1887 - mse: 13.1887 - mae: 1.5279 - val_loss: 14.0719 - val_mse: 14.0719 - val_mae: 1.4512 - lr: 1.6652e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 6s - loss: 12.9707 - mse: 12.9707 - mae: 1.5218 - val_loss: 14.3954 - val_mse: 14.3954 - val_mae: 1.4055 - lr: 1.6652e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 6s - loss: 12.9153 - mse: 12.9153 - mae: 1.5222 - val_loss: 14.7500 - val_mse: 14.7500 - val_mae: 1.7624 - lr: 1.6652e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 6s - loss: 12.6026 - mse: 12.6026 - mae: 1.5134 - val_loss: 14.6186 - val_mse: 14.6186 - val_mae: 1.6581 - lr: 1.6652e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 6s - loss: 12.5707 - mse: 12.5707 - mae: 1.5112 - val_loss: 14.6845 - val_mse: 14.6845 - val_mae: 1.6470 - lr: 1.6652e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 6s - loss: 12.5447 - mse: 12.5447 - mae: 1.5041 - val_loss: 14.7224 - val_mse: 14.7224 - val_mae: 1.5586 - lr: 1.6652e-04 - 6s/epoch - 6ms/step\n",
            "Score for fold 4: loss of 14.722400665283203\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 6s - loss: 14.2321 - mse: 14.2321 - mae: 1.5341 - val_loss: 8.5475 - val_mse: 8.5475 - val_mae: 1.3635 - lr: 1.6652e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 6s - loss: 14.0519 - mse: 14.0519 - mae: 1.5215 - val_loss: 8.8478 - val_mse: 8.8478 - val_mae: 1.4284 - lr: 1.6652e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 6s - loss: 13.9282 - mse: 13.9282 - mae: 1.5139 - val_loss: 9.2543 - val_mse: 9.2543 - val_mae: 1.3936 - lr: 1.6652e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 6s - loss: 13.9019 - mse: 13.9019 - mae: 1.5210 - val_loss: 8.8400 - val_mse: 8.8400 - val_mae: 1.4013 - lr: 1.6652e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 6s - loss: 13.6926 - mse: 13.6926 - mae: 1.5077 - val_loss: 8.8401 - val_mse: 8.8401 - val_mae: 1.6134 - lr: 1.6652e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 6s - loss: 13.5952 - mse: 13.5952 - mae: 1.5149 - val_loss: 8.8332 - val_mse: 8.8332 - val_mae: 1.5426 - lr: 1.6652e-04 - 6s/epoch - 6ms/step\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-10-16 08:04:46,655]\u001b[0m Finished trial#33 resulted in value: 14.036000000000001. Current best value is 14.024000000000001 with parameters: {'activation': 'relu', 'num_hidden_layer': 4, 'i': 9, 'learning_rate': 0.00018064149922822524}.\u001b[0m\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Score for fold 5: loss of 8.833152770996094\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 5s - loss: 14.5370 - mse: 14.5370 - mae: 1.6836 - val_loss: 21.7978 - val_mse: 21.7978 - val_mae: 1.6317 - lr: 1.5223e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 4s - loss: 13.8327 - mse: 13.8327 - mae: 1.6040 - val_loss: 21.8000 - val_mse: 21.8000 - val_mae: 1.6322 - lr: 1.5223e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 4s - loss: 13.7125 - mse: 13.7125 - mae: 1.6051 - val_loss: 21.6512 - val_mse: 21.6512 - val_mae: 1.6634 - lr: 1.5223e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 4s - loss: 13.6548 - mse: 13.6548 - mae: 1.5940 - val_loss: 21.6601 - val_mse: 21.6601 - val_mae: 1.6607 - lr: 1.5223e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 4s - loss: 13.5835 - mse: 13.5835 - mae: 1.5939 - val_loss: 21.7432 - val_mse: 21.7432 - val_mae: 1.6590 - lr: 1.5223e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 4s - loss: 13.5872 - mse: 13.5872 - mae: 1.5876 - val_loss: 21.7074 - val_mse: 21.7074 - val_mae: 1.6096 - lr: 1.5223e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 4s - loss: 13.5711 - mse: 13.5711 - mae: 1.5856 - val_loss: 21.6704 - val_mse: 21.6704 - val_mae: 1.5623 - lr: 1.5223e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 4s - loss: 13.5515 - mse: 13.5515 - mae: 1.5837 - val_loss: 21.5602 - val_mse: 21.5602 - val_mae: 1.6316 - lr: 1.5223e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 4s - loss: 13.5251 - mse: 13.5251 - mae: 1.5850 - val_loss: 21.5696 - val_mse: 21.5696 - val_mae: 1.6441 - lr: 1.5223e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 4s - loss: 13.4841 - mse: 13.4841 - mae: 1.5827 - val_loss: 21.7032 - val_mse: 21.7032 - val_mae: 1.5987 - lr: 1.5223e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 4s - loss: 13.4832 - mse: 13.4832 - mae: 1.5850 - val_loss: 21.6171 - val_mse: 21.6171 - val_mae: 1.6485 - lr: 1.5223e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 4s - loss: 13.4786 - mse: 13.4786 - mae: 1.5827 - val_loss: 21.5726 - val_mse: 21.5726 - val_mae: 1.5846 - lr: 1.5223e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 13/100\n",
            "1000/1000 - 4s - loss: 13.4326 - mse: 13.4326 - mae: 1.5816 - val_loss: 21.7423 - val_mse: 21.7423 - val_mae: 1.6714 - lr: 1.5223e-04 - 4s/epoch - 4ms/step\n",
            "Score for fold 1: loss of 21.742271423339844\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 4s - loss: 15.2026 - mse: 15.2026 - mae: 1.5914 - val_loss: 14.5757 - val_mse: 14.5757 - val_mae: 1.6019 - lr: 1.5223e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 4s - loss: 15.1738 - mse: 15.1738 - mae: 1.5874 - val_loss: 14.4685 - val_mse: 14.4685 - val_mae: 1.6020 - lr: 1.5223e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 4s - loss: 15.1795 - mse: 15.1795 - mae: 1.5860 - val_loss: 14.5135 - val_mse: 14.5135 - val_mae: 1.6385 - lr: 1.5223e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 4s - loss: 15.1701 - mse: 15.1701 - mae: 1.5830 - val_loss: 14.4936 - val_mse: 14.4936 - val_mae: 1.5919 - lr: 1.5223e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 4s - loss: 15.1525 - mse: 15.1525 - mae: 1.5848 - val_loss: 14.6450 - val_mse: 14.6450 - val_mae: 1.5735 - lr: 1.5223e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 4s - loss: 15.1154 - mse: 15.1154 - mae: 1.5840 - val_loss: 14.6697 - val_mse: 14.6697 - val_mae: 1.5710 - lr: 1.5223e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 4s - loss: 15.1070 - mse: 15.1070 - mae: 1.5822 - val_loss: 14.4817 - val_mse: 14.4817 - val_mae: 1.6045 - lr: 1.5223e-04 - 4s/epoch - 4ms/step\n",
            "Score for fold 2: loss of 14.481674194335938\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 4s - loss: 15.2778 - mse: 15.2778 - mae: 1.5778 - val_loss: 13.7967 - val_mse: 13.7967 - val_mae: 1.6081 - lr: 1.5223e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 4s - loss: 15.2880 - mse: 15.2880 - mae: 1.5765 - val_loss: 14.0525 - val_mse: 14.0525 - val_mae: 1.5415 - lr: 1.5223e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 4s - loss: 15.2568 - mse: 15.2568 - mae: 1.5767 - val_loss: 14.0441 - val_mse: 14.0441 - val_mae: 1.6523 - lr: 1.5223e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 4s - loss: 15.2713 - mse: 15.2713 - mae: 1.5771 - val_loss: 13.8659 - val_mse: 13.8659 - val_mae: 1.5928 - lr: 1.5223e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 4s - loss: 15.2465 - mse: 15.2465 - mae: 1.5747 - val_loss: 13.9603 - val_mse: 13.9603 - val_mae: 1.6068 - lr: 1.5223e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 4s - loss: 15.2501 - mse: 15.2501 - mae: 1.5762 - val_loss: 13.9009 - val_mse: 13.9009 - val_mae: 1.5793 - lr: 1.5223e-04 - 4s/epoch - 4ms/step\n",
            "Score for fold 3: loss of 13.900918006896973\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 4s - loss: 15.6320 - mse: 15.6320 - mae: 1.5911 - val_loss: 12.4058 - val_mse: 12.4058 - val_mae: 1.5226 - lr: 1.5223e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 4s - loss: 15.5886 - mse: 15.5886 - mae: 1.5836 - val_loss: 12.2956 - val_mse: 12.2956 - val_mae: 1.5575 - lr: 1.5223e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 4s - loss: 15.5738 - mse: 15.5738 - mae: 1.5832 - val_loss: 12.3766 - val_mse: 12.3766 - val_mae: 1.5484 - lr: 1.5223e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 4s - loss: 15.5627 - mse: 15.5627 - mae: 1.5870 - val_loss: 12.3964 - val_mse: 12.3964 - val_mae: 1.5913 - lr: 1.5223e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 4s - loss: 15.5251 - mse: 15.5251 - mae: 1.5841 - val_loss: 12.3428 - val_mse: 12.3428 - val_mae: 1.5560 - lr: 1.5223e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 4s - loss: 15.5392 - mse: 15.5392 - mae: 1.5838 - val_loss: 12.4110 - val_mse: 12.4110 - val_mae: 1.5105 - lr: 1.5223e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 4s - loss: 15.5157 - mse: 15.5157 - mae: 1.5830 - val_loss: 12.2872 - val_mse: 12.2872 - val_mae: 1.5664 - lr: 1.5223e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 4s - loss: 15.5075 - mse: 15.5075 - mae: 1.5827 - val_loss: 12.4067 - val_mse: 12.4067 - val_mae: 1.5476 - lr: 1.5223e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 4s - loss: 15.4857 - mse: 15.4857 - mae: 1.5798 - val_loss: 12.3346 - val_mse: 12.3346 - val_mae: 1.5693 - lr: 1.5223e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 4s - loss: 15.4749 - mse: 15.4749 - mae: 1.5817 - val_loss: 12.4174 - val_mse: 12.4174 - val_mae: 1.5549 - lr: 1.5223e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 4s - loss: 15.4637 - mse: 15.4637 - mae: 1.5836 - val_loss: 12.4465 - val_mse: 12.4465 - val_mae: 1.5689 - lr: 1.5223e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 4s - loss: 15.4514 - mse: 15.4514 - mae: 1.5829 - val_loss: 12.4202 - val_mse: 12.4202 - val_mae: 1.5802 - lr: 1.5223e-04 - 4s/epoch - 4ms/step\n",
            "Score for fold 4: loss of 12.420157432556152\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 4s - loss: 15.3925 - mse: 15.3925 - mae: 1.5773 - val_loss: 12.6080 - val_mse: 12.6080 - val_mae: 1.5991 - lr: 1.5223e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 4s - loss: 15.3490 - mse: 15.3490 - mae: 1.5799 - val_loss: 12.5536 - val_mse: 12.5536 - val_mae: 1.5923 - lr: 1.5223e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 4s - loss: 15.3405 - mse: 15.3405 - mae: 1.5811 - val_loss: 12.6384 - val_mse: 12.6384 - val_mae: 1.5439 - lr: 1.5223e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 4s - loss: 15.3010 - mse: 15.3010 - mae: 1.5797 - val_loss: 12.5995 - val_mse: 12.5995 - val_mae: 1.5585 - lr: 1.5223e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 4s - loss: 15.3299 - mse: 15.3299 - mae: 1.5760 - val_loss: 12.6226 - val_mse: 12.6226 - val_mae: 1.5116 - lr: 1.5223e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 4s - loss: 15.2795 - mse: 15.2795 - mae: 1.5739 - val_loss: 12.5238 - val_mse: 12.5238 - val_mae: 1.5429 - lr: 1.5223e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 4s - loss: 15.2469 - mse: 15.2469 - mae: 1.5746 - val_loss: 12.5795 - val_mse: 12.5795 - val_mae: 1.5914 - lr: 1.5223e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 4s - loss: 15.2649 - mse: 15.2649 - mae: 1.5742 - val_loss: 12.6210 - val_mse: 12.6210 - val_mae: 1.5636 - lr: 1.5223e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 4s - loss: 15.2481 - mse: 15.2481 - mae: 1.5713 - val_loss: 12.5882 - val_mse: 12.5882 - val_mae: 1.5989 - lr: 1.5223e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 4s - loss: 15.1974 - mse: 15.1974 - mae: 1.5727 - val_loss: 12.6603 - val_mse: 12.6603 - val_mae: 1.6291 - lr: 1.5223e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 4s - loss: 15.1943 - mse: 15.1943 - mae: 1.5722 - val_loss: 12.6389 - val_mse: 12.6389 - val_mae: 1.5256 - lr: 1.5223e-04 - 4s/epoch - 4ms/step\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-10-16 08:08:10,807]\u001b[0m Finished trial#34 resulted in value: 15.036000000000001. Current best value is 14.024000000000001 with parameters: {'activation': 'relu', 'num_hidden_layer': 4, 'i': 9, 'learning_rate': 0.00018064149922822524}.\u001b[0m\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Score for fold 5: loss of 12.638935089111328\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 7s - loss: 17.5403 - mse: 17.5403 - mae: 1.7001 - val_loss: 10.3138 - val_mse: 10.3138 - val_mae: 1.6309 - lr: 1.6660e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 6s - loss: 17.1157 - mse: 17.1157 - mae: 1.6453 - val_loss: 10.2859 - val_mse: 10.2859 - val_mae: 1.6666 - lr: 1.6660e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 5s - loss: 17.1073 - mse: 17.1073 - mae: 1.6428 - val_loss: 10.3228 - val_mse: 10.3228 - val_mae: 1.6305 - lr: 1.6660e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 5s - loss: 17.0655 - mse: 17.0655 - mae: 1.6438 - val_loss: 10.2511 - val_mse: 10.2511 - val_mae: 1.6047 - lr: 1.6660e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 6s - loss: 17.0230 - mse: 17.0230 - mae: 1.6431 - val_loss: 10.2302 - val_mse: 10.2302 - val_mae: 1.6056 - lr: 1.6660e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 5s - loss: 17.0407 - mse: 17.0407 - mae: 1.6422 - val_loss: 10.4011 - val_mse: 10.4011 - val_mae: 1.5433 - lr: 1.6660e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 6s - loss: 17.0261 - mse: 17.0261 - mae: 1.6402 - val_loss: 10.2761 - val_mse: 10.2761 - val_mae: 1.5981 - lr: 1.6660e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 6s - loss: 17.0338 - mse: 17.0338 - mae: 1.6428 - val_loss: 10.2359 - val_mse: 10.2359 - val_mae: 1.6124 - lr: 1.6660e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 6s - loss: 17.0068 - mse: 17.0068 - mae: 1.6444 - val_loss: 10.3469 - val_mse: 10.3469 - val_mae: 1.5551 - lr: 1.6660e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 6s - loss: 17.0216 - mse: 17.0216 - mae: 1.6453 - val_loss: 10.2329 - val_mse: 10.2329 - val_mae: 1.6149 - lr: 1.6660e-04 - 6s/epoch - 6ms/step\n",
            "Score for fold 1: loss of 10.232945442199707\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 5s - loss: 16.7446 - mse: 16.7446 - mae: 1.6406 - val_loss: 11.6158 - val_mse: 11.6158 - val_mae: 1.5815 - lr: 1.6660e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 6s - loss: 16.6947 - mse: 16.6947 - mae: 1.6422 - val_loss: 11.5089 - val_mse: 11.5089 - val_mae: 1.6094 - lr: 1.6660e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 6s - loss: 16.7017 - mse: 16.7017 - mae: 1.6383 - val_loss: 11.6624 - val_mse: 11.6624 - val_mae: 1.5647 - lr: 1.6660e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 5s - loss: 16.6955 - mse: 16.6955 - mae: 1.6399 - val_loss: 11.5253 - val_mse: 11.5253 - val_mae: 1.6196 - lr: 1.6660e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 6s - loss: 16.7237 - mse: 16.7237 - mae: 1.6400 - val_loss: 11.4358 - val_mse: 11.4358 - val_mae: 1.6408 - lr: 1.6660e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 6s - loss: 16.7155 - mse: 16.7155 - mae: 1.6427 - val_loss: 11.5016 - val_mse: 11.5016 - val_mae: 1.6112 - lr: 1.6660e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 6s - loss: 16.6999 - mse: 16.6999 - mae: 1.6426 - val_loss: 11.8702 - val_mse: 11.8702 - val_mae: 1.5707 - lr: 1.6660e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 6s - loss: 16.7144 - mse: 16.7144 - mae: 1.6407 - val_loss: 11.5006 - val_mse: 11.5006 - val_mae: 1.6425 - lr: 1.6660e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 6s - loss: 16.7099 - mse: 16.7099 - mae: 1.6401 - val_loss: 11.4282 - val_mse: 11.4282 - val_mae: 1.6332 - lr: 1.6660e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 6s - loss: 16.7134 - mse: 16.7134 - mae: 1.6376 - val_loss: 11.4529 - val_mse: 11.4529 - val_mae: 1.6217 - lr: 1.6660e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 6s - loss: 16.7137 - mse: 16.7137 - mae: 1.6375 - val_loss: 11.4879 - val_mse: 11.4879 - val_mae: 1.6446 - lr: 1.6660e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 5s - loss: 16.7116 - mse: 16.7116 - mae: 1.6463 - val_loss: 11.6431 - val_mse: 11.6431 - val_mae: 1.5741 - lr: 1.6660e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 13/100\n",
            "1000/1000 - 5s - loss: 16.7123 - mse: 16.7123 - mae: 1.6434 - val_loss: 11.4611 - val_mse: 11.4611 - val_mae: 1.6207 - lr: 1.6660e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 14/100\n",
            "1000/1000 - 6s - loss: 16.7156 - mse: 16.7156 - mae: 1.6394 - val_loss: 11.4831 - val_mse: 11.4831 - val_mae: 1.5912 - lr: 1.6660e-04 - 6s/epoch - 6ms/step\n",
            "Score for fold 2: loss of 11.483064651489258\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 6s - loss: 14.6018 - mse: 14.6018 - mae: 1.6259 - val_loss: 19.9032 - val_mse: 19.9032 - val_mae: 1.6595 - lr: 1.6660e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 6s - loss: 14.5873 - mse: 14.5873 - mae: 1.6274 - val_loss: 19.9430 - val_mse: 19.9430 - val_mae: 1.6625 - lr: 1.6660e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 6s - loss: 14.5795 - mse: 14.5795 - mae: 1.6286 - val_loss: 19.9341 - val_mse: 19.9341 - val_mae: 1.6442 - lr: 1.6660e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 6s - loss: 14.6012 - mse: 14.6012 - mae: 1.6286 - val_loss: 19.8309 - val_mse: 19.8309 - val_mae: 1.7036 - lr: 1.6660e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 6s - loss: 14.5960 - mse: 14.5960 - mae: 1.6262 - val_loss: 19.9374 - val_mse: 19.9374 - val_mae: 1.6561 - lr: 1.6660e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 6s - loss: 14.5979 - mse: 14.5979 - mae: 1.6262 - val_loss: 19.8465 - val_mse: 19.8465 - val_mae: 1.7320 - lr: 1.6660e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 6s - loss: 14.5477 - mse: 14.5477 - mae: 1.6318 - val_loss: 19.8416 - val_mse: 19.8416 - val_mae: 1.6795 - lr: 1.6660e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 6s - loss: 14.5858 - mse: 14.5858 - mae: 1.6225 - val_loss: 19.9254 - val_mse: 19.9254 - val_mae: 1.6772 - lr: 1.6660e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 6s - loss: 14.5884 - mse: 14.5884 - mae: 1.6280 - val_loss: 19.8317 - val_mse: 19.8317 - val_mae: 1.6831 - lr: 1.6660e-04 - 6s/epoch - 6ms/step\n",
            "Score for fold 3: loss of 19.8316707611084\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 6s - loss: 14.1507 - mse: 14.1507 - mae: 1.6294 - val_loss: 21.4969 - val_mse: 21.4969 - val_mae: 1.6723 - lr: 1.6660e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 6s - loss: 14.2057 - mse: 14.2057 - mae: 1.6326 - val_loss: 21.5064 - val_mse: 21.5064 - val_mae: 1.7002 - lr: 1.6660e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 6s - loss: 14.1719 - mse: 14.1719 - mae: 1.6293 - val_loss: 21.6066 - val_mse: 21.6066 - val_mae: 1.6088 - lr: 1.6660e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 6s - loss: 14.1958 - mse: 14.1958 - mae: 1.6297 - val_loss: 21.5037 - val_mse: 21.5037 - val_mae: 1.6868 - lr: 1.6660e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 6s - loss: 14.1708 - mse: 14.1708 - mae: 1.6330 - val_loss: 21.4936 - val_mse: 21.4936 - val_mae: 1.6608 - lr: 1.6660e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 6s - loss: 14.2037 - mse: 14.2037 - mae: 1.6277 - val_loss: 21.5407 - val_mse: 21.5407 - val_mae: 1.6453 - lr: 1.6660e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 6s - loss: 14.2122 - mse: 14.2122 - mae: 1.6262 - val_loss: 21.5101 - val_mse: 21.5101 - val_mae: 1.6918 - lr: 1.6660e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 6s - loss: 14.1899 - mse: 14.1899 - mae: 1.6389 - val_loss: 21.4962 - val_mse: 21.4962 - val_mae: 1.6844 - lr: 1.6660e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 6s - loss: 14.1680 - mse: 14.1680 - mae: 1.6332 - val_loss: 21.5515 - val_mse: 21.5515 - val_mae: 1.6234 - lr: 1.6660e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 6s - loss: 14.1762 - mse: 14.1762 - mae: 1.6309 - val_loss: 21.6429 - val_mse: 21.6429 - val_mae: 1.5905 - lr: 1.6660e-04 - 6s/epoch - 6ms/step\n",
            "Score for fold 4: loss of 21.642934799194336\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 6s - loss: 15.8112 - mse: 15.8112 - mae: 1.6302 - val_loss: 15.0280 - val_mse: 15.0280 - val_mae: 1.7712 - lr: 1.6660e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 6s - loss: 15.8007 - mse: 15.8007 - mae: 1.6400 - val_loss: 15.0479 - val_mse: 15.0479 - val_mae: 1.6226 - lr: 1.6660e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 6s - loss: 15.7995 - mse: 15.7995 - mae: 1.6295 - val_loss: 14.9850 - val_mse: 14.9850 - val_mae: 1.6626 - lr: 1.6660e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 6s - loss: 15.7753 - mse: 15.7753 - mae: 1.6407 - val_loss: 14.9891 - val_mse: 14.9891 - val_mae: 1.6782 - lr: 1.6660e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 6s - loss: 15.7934 - mse: 15.7934 - mae: 1.6381 - val_loss: 14.9846 - val_mse: 14.9846 - val_mae: 1.6511 - lr: 1.6660e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 6s - loss: 15.7843 - mse: 15.7843 - mae: 1.6366 - val_loss: 15.0041 - val_mse: 15.0041 - val_mae: 1.6369 - lr: 1.6660e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 6s - loss: 15.7839 - mse: 15.7839 - mae: 1.6372 - val_loss: 14.9718 - val_mse: 14.9718 - val_mae: 1.6554 - lr: 1.6660e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 6s - loss: 15.7929 - mse: 15.7929 - mae: 1.6341 - val_loss: 14.9841 - val_mse: 14.9841 - val_mae: 1.6563 - lr: 1.6660e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 6s - loss: 15.7944 - mse: 15.7944 - mae: 1.6384 - val_loss: 14.9989 - val_mse: 14.9989 - val_mae: 1.6437 - lr: 1.6660e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 6s - loss: 15.7836 - mse: 15.7836 - mae: 1.6339 - val_loss: 14.9713 - val_mse: 14.9713 - val_mae: 1.7036 - lr: 1.6660e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 6s - loss: 15.7852 - mse: 15.7852 - mae: 1.6371 - val_loss: 14.9990 - val_mse: 14.9990 - val_mae: 1.6272 - lr: 1.6660e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 6s - loss: 15.7738 - mse: 15.7738 - mae: 1.6393 - val_loss: 14.9908 - val_mse: 14.9908 - val_mae: 1.6367 - lr: 1.6660e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 13/100\n",
            "1000/1000 - 6s - loss: 15.7972 - mse: 15.7972 - mae: 1.6295 - val_loss: 14.9695 - val_mse: 14.9695 - val_mae: 1.6786 - lr: 1.6660e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 14/100\n",
            "1000/1000 - 6s - loss: 15.7899 - mse: 15.7899 - mae: 1.6356 - val_loss: 14.9904 - val_mse: 14.9904 - val_mae: 1.6430 - lr: 1.6660e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 15/100\n",
            "1000/1000 - 6s - loss: 15.7895 - mse: 15.7895 - mae: 1.6313 - val_loss: 15.0712 - val_mse: 15.0712 - val_mae: 1.5969 - lr: 1.6660e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 16/100\n",
            "1000/1000 - 6s - loss: 15.7783 - mse: 15.7783 - mae: 1.6385 - val_loss: 14.9786 - val_mse: 14.9786 - val_mae: 1.6495 - lr: 1.6660e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 17/100\n",
            "1000/1000 - 6s - loss: 15.7786 - mse: 15.7786 - mae: 1.6324 - val_loss: 15.0297 - val_mse: 15.0297 - val_mae: 1.6175 - lr: 1.6660e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 18/100\n",
            "1000/1000 - 6s - loss: 15.8011 - mse: 15.8011 - mae: 1.6303 - val_loss: 14.9721 - val_mse: 14.9721 - val_mae: 1.6671 - lr: 1.6660e-04 - 6s/epoch - 6ms/step\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-10-16 08:14:03,578]\u001b[0m Finished trial#35 resulted in value: 15.63. Current best value is 14.024000000000001 with parameters: {'activation': 'relu', 'num_hidden_layer': 4, 'i': 9, 'learning_rate': 0.00018064149922822524}.\u001b[0m\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Score for fold 5: loss of 14.97208309173584\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 13s - loss: 16.6335 - mse: 16.6335 - mae: 1.6247 - val_loss: 10.9070 - val_mse: 10.9070 - val_mae: 1.6396 - lr: 1.0296e-04 - 13s/epoch - 13ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 12s - loss: 16.2113 - mse: 16.2113 - mae: 1.5990 - val_loss: 11.0327 - val_mse: 11.0327 - val_mae: 1.4823 - lr: 1.0296e-04 - 12s/epoch - 12ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 13s - loss: 16.1454 - mse: 16.1454 - mae: 1.5947 - val_loss: 10.5862 - val_mse: 10.5862 - val_mae: 1.5372 - lr: 1.0296e-04 - 13s/epoch - 13ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 13s - loss: 16.0742 - mse: 16.0742 - mae: 1.5973 - val_loss: 11.0149 - val_mse: 11.0149 - val_mae: 1.4984 - lr: 1.0296e-04 - 13s/epoch - 13ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 13s - loss: 16.0730 - mse: 16.0730 - mae: 1.5897 - val_loss: 10.5799 - val_mse: 10.5799 - val_mae: 1.5626 - lr: 1.0296e-04 - 13s/epoch - 13ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 13s - loss: 16.0395 - mse: 16.0395 - mae: 1.5889 - val_loss: 10.7621 - val_mse: 10.7621 - val_mae: 1.5200 - lr: 1.0296e-04 - 13s/epoch - 13ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 14s - loss: 15.9521 - mse: 15.9521 - mae: 1.5868 - val_loss: 10.6347 - val_mse: 10.6347 - val_mae: 1.6075 - lr: 1.0296e-04 - 14s/epoch - 14ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 15s - loss: 15.9329 - mse: 15.9329 - mae: 1.5834 - val_loss: 10.6614 - val_mse: 10.6614 - val_mae: 1.5494 - lr: 1.0296e-04 - 15s/epoch - 15ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 14s - loss: 15.8766 - mse: 15.8766 - mae: 1.5803 - val_loss: 10.7407 - val_mse: 10.7407 - val_mae: 1.5236 - lr: 1.0296e-04 - 14s/epoch - 14ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 14s - loss: 15.8644 - mse: 15.8644 - mae: 1.5736 - val_loss: 10.6756 - val_mse: 10.6756 - val_mae: 1.5435 - lr: 1.0296e-04 - 14s/epoch - 14ms/step\n",
            "Score for fold 1: loss of 10.675619125366211\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 13s - loss: 13.4224 - mse: 13.4224 - mae: 1.5657 - val_loss: 19.9245 - val_mse: 19.9245 - val_mae: 1.5477 - lr: 1.0296e-04 - 13s/epoch - 13ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 13s - loss: 13.3727 - mse: 13.3727 - mae: 1.5631 - val_loss: 20.0207 - val_mse: 20.0207 - val_mae: 1.6190 - lr: 1.0296e-04 - 13s/epoch - 13ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 14s - loss: 13.2531 - mse: 13.2531 - mae: 1.5610 - val_loss: 20.0087 - val_mse: 20.0087 - val_mae: 1.6005 - lr: 1.0296e-04 - 14s/epoch - 14ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 14s - loss: 13.2231 - mse: 13.2231 - mae: 1.5566 - val_loss: 20.5011 - val_mse: 20.5011 - val_mae: 1.5240 - lr: 1.0296e-04 - 14s/epoch - 14ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 14s - loss: 13.1680 - mse: 13.1680 - mae: 1.5560 - val_loss: 20.0751 - val_mse: 20.0751 - val_mae: 1.6246 - lr: 1.0296e-04 - 14s/epoch - 14ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 13s - loss: 13.1850 - mse: 13.1850 - mae: 1.5541 - val_loss: 20.0831 - val_mse: 20.0831 - val_mae: 1.6388 - lr: 1.0296e-04 - 13s/epoch - 13ms/step\n",
            "Score for fold 2: loss of 20.08310317993164\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 13s - loss: 13.3025 - mse: 13.3025 - mae: 1.5555 - val_loss: 19.3497 - val_mse: 19.3497 - val_mae: 1.6115 - lr: 1.0296e-04 - 13s/epoch - 13ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 13s - loss: 13.2737 - mse: 13.2737 - mae: 1.5510 - val_loss: 19.3268 - val_mse: 19.3268 - val_mae: 1.6414 - lr: 1.0296e-04 - 13s/epoch - 13ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 13s - loss: 13.1627 - mse: 13.1627 - mae: 1.5469 - val_loss: 19.1251 - val_mse: 19.1251 - val_mae: 1.6141 - lr: 1.0296e-04 - 13s/epoch - 13ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 13s - loss: 13.0355 - mse: 13.0355 - mae: 1.5402 - val_loss: 19.3204 - val_mse: 19.3204 - val_mae: 1.5690 - lr: 1.0296e-04 - 13s/epoch - 13ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 13s - loss: 12.9920 - mse: 12.9920 - mae: 1.5420 - val_loss: 19.0854 - val_mse: 19.0854 - val_mae: 1.5487 - lr: 1.0296e-04 - 13s/epoch - 13ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 14s - loss: 12.7777 - mse: 12.7777 - mae: 1.5299 - val_loss: 19.2378 - val_mse: 19.2378 - val_mae: 1.7091 - lr: 1.0296e-04 - 14s/epoch - 14ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 13s - loss: 12.8330 - mse: 12.8330 - mae: 1.5350 - val_loss: 19.1801 - val_mse: 19.1801 - val_mae: 1.5536 - lr: 1.0296e-04 - 13s/epoch - 13ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 13s - loss: 12.7278 - mse: 12.7278 - mae: 1.5317 - val_loss: 19.5397 - val_mse: 19.5397 - val_mae: 1.7395 - lr: 1.0296e-04 - 13s/epoch - 13ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 13s - loss: 12.5765 - mse: 12.5765 - mae: 1.5222 - val_loss: 19.3130 - val_mse: 19.3130 - val_mae: 1.6460 - lr: 1.0296e-04 - 13s/epoch - 13ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 13s - loss: 12.4664 - mse: 12.4664 - mae: 1.5161 - val_loss: 19.4157 - val_mse: 19.4157 - val_mae: 1.5732 - lr: 1.0296e-04 - 13s/epoch - 13ms/step\n",
            "Score for fold 3: loss of 19.415660858154297\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 13s - loss: 14.7719 - mse: 14.7719 - mae: 1.5496 - val_loss: 10.0711 - val_mse: 10.0711 - val_mae: 1.5925 - lr: 1.0296e-04 - 13s/epoch - 13ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 13s - loss: 14.4458 - mse: 14.4458 - mae: 1.5364 - val_loss: 10.2437 - val_mse: 10.2437 - val_mae: 1.4733 - lr: 1.0296e-04 - 13s/epoch - 13ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 13s - loss: 14.5931 - mse: 14.5931 - mae: 1.5357 - val_loss: 10.4097 - val_mse: 10.4097 - val_mae: 1.6654 - lr: 1.0296e-04 - 13s/epoch - 13ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 13s - loss: 14.4530 - mse: 14.4530 - mae: 1.5339 - val_loss: 10.3483 - val_mse: 10.3483 - val_mae: 1.5946 - lr: 1.0296e-04 - 13s/epoch - 13ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 13s - loss: 14.2475 - mse: 14.2475 - mae: 1.5280 - val_loss: 10.8418 - val_mse: 10.8418 - val_mae: 1.5526 - lr: 1.0296e-04 - 13s/epoch - 13ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 13s - loss: 14.2675 - mse: 14.2675 - mae: 1.5227 - val_loss: 10.7684 - val_mse: 10.7684 - val_mae: 1.4859 - lr: 1.0296e-04 - 13s/epoch - 13ms/step\n",
            "Score for fold 4: loss of 10.7683687210083\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 13s - loss: 14.1033 - mse: 14.1033 - mae: 1.5332 - val_loss: 11.0975 - val_mse: 11.0975 - val_mae: 1.5553 - lr: 1.0296e-04 - 13s/epoch - 13ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 14s - loss: 13.9424 - mse: 13.9424 - mae: 1.5225 - val_loss: 11.2143 - val_mse: 11.2143 - val_mae: 1.5780 - lr: 1.0296e-04 - 14s/epoch - 14ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 14s - loss: 13.8128 - mse: 13.8128 - mae: 1.5274 - val_loss: 11.2623 - val_mse: 11.2623 - val_mae: 1.5634 - lr: 1.0296e-04 - 14s/epoch - 14ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 15s - loss: 13.6068 - mse: 13.6068 - mae: 1.5191 - val_loss: 11.6792 - val_mse: 11.6792 - val_mae: 1.7997 - lr: 1.0296e-04 - 15s/epoch - 15ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 14s - loss: 13.5054 - mse: 13.5054 - mae: 1.5166 - val_loss: 11.5519 - val_mse: 11.5519 - val_mae: 1.5726 - lr: 1.0296e-04 - 14s/epoch - 14ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 15s - loss: 13.4865 - mse: 13.4865 - mae: 1.5125 - val_loss: 11.7673 - val_mse: 11.7673 - val_mae: 1.4523 - lr: 1.0296e-04 - 15s/epoch - 15ms/step\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-10-16 08:22:40,596]\u001b[0m Finished trial#36 resulted in value: 14.544. Current best value is 14.024000000000001 with parameters: {'activation': 'relu', 'num_hidden_layer': 4, 'i': 9, 'learning_rate': 0.00018064149922822524}.\u001b[0m\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Score for fold 5: loss of 11.767285346984863\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 4s - loss: 15.7802 - mse: 15.7802 - mae: 1.6185 - val_loss: 14.2235 - val_mse: 14.2235 - val_mae: 1.6039 - lr: 6.2628e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 3s - loss: 15.3662 - mse: 15.3662 - mae: 1.5981 - val_loss: 14.3851 - val_mse: 14.3851 - val_mae: 1.6654 - lr: 6.2628e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 3s - loss: 15.2818 - mse: 15.2818 - mae: 1.5987 - val_loss: 14.0131 - val_mse: 14.0131 - val_mae: 1.5545 - lr: 6.2628e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 3s - loss: 15.2934 - mse: 15.2934 - mae: 1.5872 - val_loss: 14.1325 - val_mse: 14.1325 - val_mae: 1.5786 - lr: 6.2628e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 3s - loss: 15.2153 - mse: 15.2153 - mae: 1.5850 - val_loss: 14.3714 - val_mse: 14.3714 - val_mae: 1.5884 - lr: 6.2628e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 3s - loss: 15.2314 - mse: 15.2314 - mae: 1.5838 - val_loss: 14.1343 - val_mse: 14.1343 - val_mae: 1.5461 - lr: 6.2628e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 4s - loss: 15.2288 - mse: 15.2288 - mae: 1.5811 - val_loss: 14.0374 - val_mse: 14.0374 - val_mae: 1.5861 - lr: 6.2628e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 4s - loss: 15.1709 - mse: 15.1709 - mae: 1.5811 - val_loss: 14.3469 - val_mse: 14.3469 - val_mae: 1.5233 - lr: 6.2628e-04 - 4s/epoch - 4ms/step\n",
            "Score for fold 1: loss of 14.346882820129395\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 3s - loss: 13.6200 - mse: 13.6200 - mae: 1.5638 - val_loss: 20.5314 - val_mse: 20.5314 - val_mae: 1.5879 - lr: 6.2628e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 3s - loss: 13.5451 - mse: 13.5451 - mae: 1.5679 - val_loss: 20.4092 - val_mse: 20.4092 - val_mae: 1.6283 - lr: 6.2628e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 3s - loss: 13.5733 - mse: 13.5733 - mae: 1.5644 - val_loss: 20.5999 - val_mse: 20.5999 - val_mae: 1.5673 - lr: 6.2628e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 4s - loss: 13.5203 - mse: 13.5203 - mae: 1.5633 - val_loss: 20.3147 - val_mse: 20.3147 - val_mae: 1.6358 - lr: 6.2628e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 4s - loss: 13.5403 - mse: 13.5403 - mae: 1.5596 - val_loss: 20.3064 - val_mse: 20.3064 - val_mae: 1.5629 - lr: 6.2628e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 3s - loss: 13.5368 - mse: 13.5368 - mae: 1.5589 - val_loss: 20.4178 - val_mse: 20.4178 - val_mae: 1.5904 - lr: 6.2628e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 3s - loss: 13.5034 - mse: 13.5034 - mae: 1.5553 - val_loss: 20.3221 - val_mse: 20.3221 - val_mae: 1.6104 - lr: 6.2628e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 3s - loss: 13.4562 - mse: 13.4562 - mae: 1.5613 - val_loss: 20.3262 - val_mse: 20.3262 - val_mae: 1.6093 - lr: 6.2628e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 3s - loss: 13.4455 - mse: 13.4455 - mae: 1.5540 - val_loss: 20.2717 - val_mse: 20.2717 - val_mae: 1.6405 - lr: 6.2628e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 3s - loss: 13.4642 - mse: 13.4642 - mae: 1.5558 - val_loss: 20.3843 - val_mse: 20.3843 - val_mae: 1.5878 - lr: 6.2628e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 3s - loss: 13.4673 - mse: 13.4673 - mae: 1.5563 - val_loss: 20.3702 - val_mse: 20.3702 - val_mae: 1.5611 - lr: 6.2628e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 3s - loss: 13.3997 - mse: 13.3997 - mae: 1.5560 - val_loss: 20.2333 - val_mse: 20.2333 - val_mae: 1.6396 - lr: 6.2628e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 13/100\n",
            "1000/1000 - 3s - loss: 13.3522 - mse: 13.3522 - mae: 1.5503 - val_loss: 20.3885 - val_mse: 20.3885 - val_mae: 1.6026 - lr: 6.2628e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 14/100\n",
            "1000/1000 - 3s - loss: 13.3401 - mse: 13.3401 - mae: 1.5498 - val_loss: 20.3783 - val_mse: 20.3783 - val_mae: 1.6143 - lr: 6.2628e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 15/100\n",
            "1000/1000 - 3s - loss: 13.3289 - mse: 13.3289 - mae: 1.5485 - val_loss: 20.5338 - val_mse: 20.5338 - val_mae: 1.6374 - lr: 6.2628e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 16/100\n",
            "1000/1000 - 3s - loss: 13.3538 - mse: 13.3538 - mae: 1.5502 - val_loss: 20.7143 - val_mse: 20.7143 - val_mae: 1.6232 - lr: 6.2628e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 17/100\n",
            "1000/1000 - 3s - loss: 13.3381 - mse: 13.3381 - mae: 1.5496 - val_loss: 20.4821 - val_mse: 20.4821 - val_mae: 1.5964 - lr: 6.2628e-04 - 3s/epoch - 3ms/step\n",
            "Score for fold 2: loss of 20.48209571838379\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 3s - loss: 15.3987 - mse: 15.3987 - mae: 1.5592 - val_loss: 12.1176 - val_mse: 12.1176 - val_mae: 1.5420 - lr: 6.2628e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 3s - loss: 15.2768 - mse: 15.2768 - mae: 1.5570 - val_loss: 11.9116 - val_mse: 11.9116 - val_mae: 1.5350 - lr: 6.2628e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 3s - loss: 15.2665 - mse: 15.2665 - mae: 1.5544 - val_loss: 12.0173 - val_mse: 12.0173 - val_mae: 1.5533 - lr: 6.2628e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 3s - loss: 15.1788 - mse: 15.1788 - mae: 1.5514 - val_loss: 12.1895 - val_mse: 12.1895 - val_mae: 1.5369 - lr: 6.2628e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 3s - loss: 15.1399 - mse: 15.1399 - mae: 1.5470 - val_loss: 12.1649 - val_mse: 12.1649 - val_mae: 1.5699 - lr: 6.2628e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 3s - loss: 15.1447 - mse: 15.1447 - mae: 1.5472 - val_loss: 12.1120 - val_mse: 12.1120 - val_mae: 1.5727 - lr: 6.2628e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 3s - loss: 15.1330 - mse: 15.1330 - mae: 1.5432 - val_loss: 12.1935 - val_mse: 12.1935 - val_mae: 1.5712 - lr: 6.2628e-04 - 3s/epoch - 3ms/step\n",
            "Score for fold 3: loss of 12.193482398986816\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 3s - loss: 14.6095 - mse: 14.6095 - mae: 1.5605 - val_loss: 13.9642 - val_mse: 13.9642 - val_mae: 1.5181 - lr: 6.2628e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 3s - loss: 14.5221 - mse: 14.5221 - mae: 1.5587 - val_loss: 13.8768 - val_mse: 13.8768 - val_mae: 1.5106 - lr: 6.2628e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 3s - loss: 14.5708 - mse: 14.5708 - mae: 1.5564 - val_loss: 13.9992 - val_mse: 13.9992 - val_mae: 1.5101 - lr: 6.2628e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 3s - loss: 14.4746 - mse: 14.4746 - mae: 1.5530 - val_loss: 14.0468 - val_mse: 14.0468 - val_mae: 1.5085 - lr: 6.2628e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 3s - loss: 14.3950 - mse: 14.3950 - mae: 1.5537 - val_loss: 14.0492 - val_mse: 14.0492 - val_mae: 1.5022 - lr: 6.2628e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 3s - loss: 14.4308 - mse: 14.4308 - mae: 1.5505 - val_loss: 14.0970 - val_mse: 14.0970 - val_mae: 1.5190 - lr: 6.2628e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 3s - loss: 14.2587 - mse: 14.2587 - mae: 1.5455 - val_loss: 13.9509 - val_mse: 13.9509 - val_mae: 1.5332 - lr: 6.2628e-04 - 3s/epoch - 3ms/step\n",
            "Score for fold 4: loss of 13.950922012329102\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 3s - loss: 14.6189 - mse: 14.6189 - mae: 1.5378 - val_loss: 12.9920 - val_mse: 12.9920 - val_mae: 1.5470 - lr: 6.2628e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 3s - loss: 14.6012 - mse: 14.6012 - mae: 1.5363 - val_loss: 13.2317 - val_mse: 13.2317 - val_mae: 1.5660 - lr: 6.2628e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 3s - loss: 14.5575 - mse: 14.5575 - mae: 1.5345 - val_loss: 13.1814 - val_mse: 13.1814 - val_mae: 1.5329 - lr: 6.2628e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 3s - loss: 14.4397 - mse: 14.4397 - mae: 1.5280 - val_loss: 13.4729 - val_mse: 13.4729 - val_mae: 1.5477 - lr: 6.2628e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 3s - loss: 14.4542 - mse: 14.4542 - mae: 1.5287 - val_loss: 13.3245 - val_mse: 13.3245 - val_mae: 1.5785 - lr: 6.2628e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 3s - loss: 14.3627 - mse: 14.3627 - mae: 1.5237 - val_loss: 13.3483 - val_mse: 13.3483 - val_mae: 1.5771 - lr: 6.2628e-04 - 3s/epoch - 3ms/step\n",
            "Score for fold 5: loss of 13.34825611114502\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-10-16 08:25:17,569]\u001b[0m Finished trial#37 resulted in value: 14.863999999999999. Current best value is 14.024000000000001 with parameters: {'activation': 'relu', 'num_hidden_layer': 4, 'i': 9, 'learning_rate': 0.00018064149922822524}.\u001b[0m\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 3s - loss: 16.5621 - mse: 16.5621 - mae: 1.6924 - val_loss: 14.1569 - val_mse: 14.1569 - val_mae: 1.6323 - lr: 2.4644e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 3s - loss: 16.0650 - mse: 16.0650 - mae: 1.6329 - val_loss: 14.2925 - val_mse: 14.2925 - val_mae: 1.6282 - lr: 2.4644e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 3s - loss: 16.0799 - mse: 16.0799 - mae: 1.6344 - val_loss: 14.2047 - val_mse: 14.2047 - val_mae: 1.6263 - lr: 2.4644e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 3s - loss: 16.0473 - mse: 16.0473 - mae: 1.6328 - val_loss: 14.2564 - val_mse: 14.2564 - val_mae: 1.6302 - lr: 2.4644e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 3s - loss: 16.0191 - mse: 16.0191 - mae: 1.6358 - val_loss: 14.4050 - val_mse: 14.4050 - val_mae: 1.6280 - lr: 2.4644e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 3s - loss: 16.0799 - mse: 16.0799 - mae: 1.6399 - val_loss: 14.1692 - val_mse: 14.1692 - val_mae: 1.6922 - lr: 2.4644e-04 - 3s/epoch - 3ms/step\n",
            "Score for fold 1: loss of 14.169204711914062\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 3s - loss: 16.7037 - mse: 16.7037 - mae: 1.6358 - val_loss: 11.5595 - val_mse: 11.5595 - val_mae: 1.6626 - lr: 2.4644e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 3s - loss: 16.6995 - mse: 16.6995 - mae: 1.6368 - val_loss: 11.7438 - val_mse: 11.7438 - val_mae: 1.5787 - lr: 2.4644e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 3s - loss: 16.7235 - mse: 16.7235 - mae: 1.6340 - val_loss: 11.6089 - val_mse: 11.6089 - val_mae: 1.6314 - lr: 2.4644e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 3s - loss: 16.7149 - mse: 16.7149 - mae: 1.6340 - val_loss: 11.6510 - val_mse: 11.6510 - val_mae: 1.6837 - lr: 2.4644e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 3s - loss: 16.6868 - mse: 16.6868 - mae: 1.6372 - val_loss: 11.5520 - val_mse: 11.5520 - val_mae: 1.6468 - lr: 2.4644e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 3s - loss: 16.7409 - mse: 16.7409 - mae: 1.6370 - val_loss: 11.6633 - val_mse: 11.6633 - val_mae: 1.6254 - lr: 2.4644e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 3s - loss: 16.7095 - mse: 16.7095 - mae: 1.6362 - val_loss: 11.5736 - val_mse: 11.5736 - val_mae: 1.6581 - lr: 2.4644e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 3s - loss: 16.6593 - mse: 16.6593 - mae: 1.6363 - val_loss: 11.7140 - val_mse: 11.7140 - val_mae: 1.6014 - lr: 2.4644e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 3s - loss: 16.7084 - mse: 16.7084 - mae: 1.6379 - val_loss: 11.5761 - val_mse: 11.5761 - val_mae: 1.6344 - lr: 2.4644e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 3s - loss: 16.7033 - mse: 16.7033 - mae: 1.6375 - val_loss: 11.5945 - val_mse: 11.5945 - val_mae: 1.6162 - lr: 2.4644e-04 - 3s/epoch - 3ms/step\n",
            "Score for fold 2: loss of 11.594550132751465\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 3s - loss: 16.1621 - mse: 16.1621 - mae: 1.6255 - val_loss: 13.7233 - val_mse: 13.7233 - val_mae: 1.6614 - lr: 2.4644e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 3s - loss: 16.1591 - mse: 16.1591 - mae: 1.6261 - val_loss: 13.7616 - val_mse: 13.7616 - val_mae: 1.6457 - lr: 2.4644e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 3s - loss: 16.1769 - mse: 16.1769 - mae: 1.6279 - val_loss: 13.7579 - val_mse: 13.7579 - val_mae: 1.6456 - lr: 2.4644e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 3s - loss: 16.1363 - mse: 16.1363 - mae: 1.6287 - val_loss: 13.7160 - val_mse: 13.7160 - val_mae: 1.6466 - lr: 2.4644e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 3s - loss: 16.1483 - mse: 16.1483 - mae: 1.6232 - val_loss: 13.7930 - val_mse: 13.7930 - val_mae: 1.6244 - lr: 2.4644e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 3s - loss: 16.1427 - mse: 16.1427 - mae: 1.6261 - val_loss: 13.6750 - val_mse: 13.6750 - val_mae: 1.6829 - lr: 2.4644e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 3s - loss: 16.1528 - mse: 16.1528 - mae: 1.6319 - val_loss: 13.8064 - val_mse: 13.8064 - val_mae: 1.6350 - lr: 2.4644e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 3s - loss: 16.1638 - mse: 16.1638 - mae: 1.6291 - val_loss: 13.7771 - val_mse: 13.7771 - val_mae: 1.6289 - lr: 2.4644e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 3s - loss: 16.1478 - mse: 16.1478 - mae: 1.6288 - val_loss: 13.7042 - val_mse: 13.7042 - val_mae: 1.6665 - lr: 2.4644e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 3s - loss: 16.1399 - mse: 16.1399 - mae: 1.6269 - val_loss: 13.7279 - val_mse: 13.7279 - val_mae: 1.6859 - lr: 2.4644e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 3s - loss: 16.1489 - mse: 16.1489 - mae: 1.6313 - val_loss: 13.7430 - val_mse: 13.7430 - val_mae: 1.6517 - lr: 2.4644e-04 - 3s/epoch - 3ms/step\n",
            "Score for fold 3: loss of 13.742998123168945\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 3s - loss: 14.2019 - mse: 14.2019 - mae: 1.6411 - val_loss: 21.4449 - val_mse: 21.4449 - val_mae: 1.6803 - lr: 2.4644e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 3s - loss: 14.1829 - mse: 14.1829 - mae: 1.6425 - val_loss: 21.3883 - val_mse: 21.3883 - val_mae: 1.6531 - lr: 2.4644e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 3s - loss: 14.1935 - mse: 14.1935 - mae: 1.6399 - val_loss: 21.3692 - val_mse: 21.3692 - val_mae: 1.6125 - lr: 2.4644e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 3s - loss: 14.2075 - mse: 14.2075 - mae: 1.6380 - val_loss: 21.4487 - val_mse: 21.4487 - val_mae: 1.6651 - lr: 2.4644e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 3s - loss: 14.1937 - mse: 14.1937 - mae: 1.6401 - val_loss: 21.3934 - val_mse: 21.3934 - val_mae: 1.6032 - lr: 2.4644e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 3s - loss: 14.2270 - mse: 14.2270 - mae: 1.6361 - val_loss: 21.4286 - val_mse: 21.4286 - val_mae: 1.6602 - lr: 2.4644e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 3s - loss: 14.1611 - mse: 14.1611 - mae: 1.6471 - val_loss: 21.3838 - val_mse: 21.3838 - val_mae: 1.6000 - lr: 2.4644e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 3s - loss: 14.2015 - mse: 14.2015 - mae: 1.6417 - val_loss: 21.3725 - val_mse: 21.3725 - val_mae: 1.6527 - lr: 2.4644e-04 - 3s/epoch - 3ms/step\n",
            "Score for fold 4: loss of 21.372547149658203\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 3s - loss: 15.2269 - mse: 15.2269 - mae: 1.6349 - val_loss: 17.3696 - val_mse: 17.3696 - val_mae: 1.6044 - lr: 2.4644e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 3s - loss: 15.2381 - mse: 15.2381 - mae: 1.6292 - val_loss: 17.3725 - val_mse: 17.3725 - val_mae: 1.5914 - lr: 2.4644e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 3s - loss: 15.2186 - mse: 15.2186 - mae: 1.6379 - val_loss: 17.2339 - val_mse: 17.2339 - val_mae: 1.6422 - lr: 2.4644e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 3s - loss: 15.2310 - mse: 15.2310 - mae: 1.6263 - val_loss: 17.2564 - val_mse: 17.2564 - val_mae: 1.6516 - lr: 2.4644e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 3s - loss: 15.2285 - mse: 15.2285 - mae: 1.6369 - val_loss: 17.2781 - val_mse: 17.2781 - val_mae: 1.6413 - lr: 2.4644e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 3s - loss: 15.2370 - mse: 15.2370 - mae: 1.6299 - val_loss: 17.3809 - val_mse: 17.3809 - val_mae: 1.6112 - lr: 2.4644e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 3s - loss: 15.2249 - mse: 15.2249 - mae: 1.6309 - val_loss: 17.2065 - val_mse: 17.2065 - val_mae: 1.6700 - lr: 2.4644e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 3s - loss: 15.2282 - mse: 15.2282 - mae: 1.6348 - val_loss: 17.2678 - val_mse: 17.2678 - val_mae: 1.6345 - lr: 2.4644e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 3s - loss: 15.2327 - mse: 15.2327 - mae: 1.6304 - val_loss: 17.3179 - val_mse: 17.3179 - val_mae: 1.6306 - lr: 2.4644e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 3s - loss: 15.2282 - mse: 15.2282 - mae: 1.6367 - val_loss: 17.3052 - val_mse: 17.3052 - val_mae: 1.6385 - lr: 2.4644e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 3s - loss: 15.2244 - mse: 15.2244 - mae: 1.6341 - val_loss: 17.3564 - val_mse: 17.3564 - val_mae: 1.6024 - lr: 2.4644e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 3s - loss: 15.2351 - mse: 15.2351 - mae: 1.6294 - val_loss: 17.4420 - val_mse: 17.4420 - val_mae: 1.5957 - lr: 2.4644e-04 - 3s/epoch - 3ms/step\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-10-16 08:27:34,890]\u001b[0m Finished trial#38 resulted in value: 15.662. Current best value is 14.024000000000001 with parameters: {'activation': 'relu', 'num_hidden_layer': 4, 'i': 9, 'learning_rate': 0.00018064149922822524}.\u001b[0m\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Score for fold 5: loss of 17.441987991333008\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 7s - loss: 14.6450 - mse: 14.6450 - mae: 1.6138 - val_loss: 18.8849 - val_mse: 18.8849 - val_mae: 1.6090 - lr: 1.3446e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 6s - loss: 14.1269 - mse: 14.1269 - mae: 1.5926 - val_loss: 18.8318 - val_mse: 18.8318 - val_mae: 1.6143 - lr: 1.3446e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 6s - loss: 14.0385 - mse: 14.0385 - mae: 1.5860 - val_loss: 18.9676 - val_mse: 18.9676 - val_mae: 1.5390 - lr: 1.3446e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 6s - loss: 14.0103 - mse: 14.0103 - mae: 1.5830 - val_loss: 18.8842 - val_mse: 18.8842 - val_mae: 1.5852 - lr: 1.3446e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 6s - loss: 14.0063 - mse: 14.0063 - mae: 1.5828 - val_loss: 18.7990 - val_mse: 18.7990 - val_mae: 1.6176 - lr: 1.3446e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 7s - loss: 13.9396 - mse: 13.9396 - mae: 1.5808 - val_loss: 18.8071 - val_mse: 18.8071 - val_mae: 1.5921 - lr: 1.3446e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 6s - loss: 13.8651 - mse: 13.8651 - mae: 1.5748 - val_loss: 18.9167 - val_mse: 18.9167 - val_mae: 1.5162 - lr: 1.3446e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 6s - loss: 13.7948 - mse: 13.7948 - mae: 1.5762 - val_loss: 18.8221 - val_mse: 18.8221 - val_mae: 1.6074 - lr: 1.3446e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 6s - loss: 13.7917 - mse: 13.7917 - mae: 1.5709 - val_loss: 18.9448 - val_mse: 18.9448 - val_mae: 1.5742 - lr: 1.3446e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 6s - loss: 13.7397 - mse: 13.7397 - mae: 1.5735 - val_loss: 18.8754 - val_mse: 18.8754 - val_mae: 1.5550 - lr: 1.3446e-04 - 6s/epoch - 6ms/step\n",
            "Score for fold 1: loss of 18.87543296813965\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 7s - loss: 15.5783 - mse: 15.5783 - mae: 1.5775 - val_loss: 11.0209 - val_mse: 11.0209 - val_mae: 1.5643 - lr: 1.3446e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 7s - loss: 15.6383 - mse: 15.6383 - mae: 1.5745 - val_loss: 11.1692 - val_mse: 11.1692 - val_mae: 1.5994 - lr: 1.3446e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 7s - loss: 15.5006 - mse: 15.5006 - mae: 1.5714 - val_loss: 11.6003 - val_mse: 11.6003 - val_mae: 1.4985 - lr: 1.3446e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 7s - loss: 15.4950 - mse: 15.4950 - mae: 1.5721 - val_loss: 11.1279 - val_mse: 11.1279 - val_mae: 1.5540 - lr: 1.3446e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 7s - loss: 15.4367 - mse: 15.4367 - mae: 1.5697 - val_loss: 11.4387 - val_mse: 11.4387 - val_mae: 1.6230 - lr: 1.3446e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 7s - loss: 15.4467 - mse: 15.4467 - mae: 1.5672 - val_loss: 11.4256 - val_mse: 11.4256 - val_mae: 1.5204 - lr: 1.3446e-04 - 7s/epoch - 7ms/step\n",
            "Score for fold 2: loss of 11.42557430267334\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 7s - loss: 14.4934 - mse: 14.4934 - mae: 1.5633 - val_loss: 14.7388 - val_mse: 14.7388 - val_mae: 1.5544 - lr: 1.3446e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 7s - loss: 14.5032 - mse: 14.5032 - mae: 1.5634 - val_loss: 14.8448 - val_mse: 14.8448 - val_mae: 1.6431 - lr: 1.3446e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 7s - loss: 14.4584 - mse: 14.4584 - mae: 1.5600 - val_loss: 14.9120 - val_mse: 14.9120 - val_mae: 1.5471 - lr: 1.3446e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 7s - loss: 14.3462 - mse: 14.3462 - mae: 1.5542 - val_loss: 14.9457 - val_mse: 14.9457 - val_mae: 1.6422 - lr: 1.3446e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 7s - loss: 14.3229 - mse: 14.3229 - mae: 1.5520 - val_loss: 15.0538 - val_mse: 15.0538 - val_mae: 1.6565 - lr: 1.3446e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 7s - loss: 14.1998 - mse: 14.1998 - mae: 1.5537 - val_loss: 14.8032 - val_mse: 14.8032 - val_mae: 1.5493 - lr: 1.3446e-04 - 7s/epoch - 7ms/step\n",
            "Score for fold 3: loss of 14.803189277648926\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 7s - loss: 14.3578 - mse: 14.3578 - mae: 1.5532 - val_loss: 14.1271 - val_mse: 14.1271 - val_mae: 1.5673 - lr: 1.3446e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 7s - loss: 14.2996 - mse: 14.2996 - mae: 1.5483 - val_loss: 14.2385 - val_mse: 14.2385 - val_mae: 1.5343 - lr: 1.3446e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 7s - loss: 14.1174 - mse: 14.1174 - mae: 1.5408 - val_loss: 14.2650 - val_mse: 14.2650 - val_mae: 1.5262 - lr: 1.3446e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 7s - loss: 14.0963 - mse: 14.0963 - mae: 1.5372 - val_loss: 14.4730 - val_mse: 14.4730 - val_mae: 1.5683 - lr: 1.3446e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 7s - loss: 14.0547 - mse: 14.0547 - mae: 1.5388 - val_loss: 14.4073 - val_mse: 14.4073 - val_mae: 1.5657 - lr: 1.3446e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 7s - loss: 13.9307 - mse: 13.9307 - mae: 1.5371 - val_loss: 14.5578 - val_mse: 14.5578 - val_mae: 1.5552 - lr: 1.3446e-04 - 7s/epoch - 7ms/step\n",
            "Score for fold 4: loss of 14.557809829711914\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 7s - loss: 14.2759 - mse: 14.2759 - mae: 1.5476 - val_loss: 13.4601 - val_mse: 13.4601 - val_mae: 1.5204 - lr: 1.3446e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 7s - loss: 14.1072 - mse: 14.1072 - mae: 1.5408 - val_loss: 13.5651 - val_mse: 13.5651 - val_mae: 1.4577 - lr: 1.3446e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 7s - loss: 13.9406 - mse: 13.9406 - mae: 1.5356 - val_loss: 13.8915 - val_mse: 13.8915 - val_mae: 1.5692 - lr: 1.3446e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 7s - loss: 13.8673 - mse: 13.8673 - mae: 1.5287 - val_loss: 13.8852 - val_mse: 13.8852 - val_mae: 1.4551 - lr: 1.3446e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 7s - loss: 13.7897 - mse: 13.7897 - mae: 1.5263 - val_loss: 13.5330 - val_mse: 13.5330 - val_mae: 1.5424 - lr: 1.3446e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 7s - loss: 13.6335 - mse: 13.6335 - mae: 1.5200 - val_loss: 13.6231 - val_mse: 13.6231 - val_mae: 1.5549 - lr: 1.3446e-04 - 7s/epoch - 7ms/step\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-10-16 08:31:28,817]\u001b[0m Finished trial#39 resulted in value: 14.658000000000001. Current best value is 14.024000000000001 with parameters: {'activation': 'relu', 'num_hidden_layer': 4, 'i': 9, 'learning_rate': 0.00018064149922822524}.\u001b[0m\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Score for fold 5: loss of 13.623061180114746\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 5s - loss: 16.7009 - mse: 16.7009 - mae: 1.6540 - val_loss: 10.5751 - val_mse: 10.5751 - val_mae: 1.5066 - lr: 0.0036 - 5s/epoch - 5ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 5s - loss: 16.4206 - mse: 16.4206 - mae: 1.6284 - val_loss: 10.5113 - val_mse: 10.5113 - val_mae: 1.5247 - lr: 0.0036 - 5s/epoch - 5ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 5s - loss: 16.3085 - mse: 16.3085 - mae: 1.6187 - val_loss: 10.5875 - val_mse: 10.5875 - val_mae: 1.5163 - lr: 0.0036 - 5s/epoch - 5ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 5s - loss: 16.2602 - mse: 16.2602 - mae: 1.6151 - val_loss: 10.5207 - val_mse: 10.5207 - val_mae: 1.5878 - lr: 0.0036 - 5s/epoch - 5ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 5s - loss: 16.2408 - mse: 16.2408 - mae: 1.6123 - val_loss: 10.4547 - val_mse: 10.4547 - val_mae: 1.5565 - lr: 0.0036 - 5s/epoch - 5ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 5s - loss: 16.3103 - mse: 16.3103 - mae: 1.6195 - val_loss: 10.5131 - val_mse: 10.5131 - val_mae: 1.5427 - lr: 0.0036 - 5s/epoch - 5ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 5s - loss: 16.2121 - mse: 16.2121 - mae: 1.6287 - val_loss: 10.4701 - val_mse: 10.4701 - val_mae: 1.5667 - lr: 0.0036 - 5s/epoch - 5ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 5s - loss: 16.1852 - mse: 16.1852 - mae: 1.6248 - val_loss: 10.4357 - val_mse: 10.4357 - val_mae: 1.5547 - lr: 0.0036 - 5s/epoch - 5ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 5s - loss: 16.1555 - mse: 16.1555 - mae: 1.6205 - val_loss: 10.4133 - val_mse: 10.4133 - val_mae: 1.5652 - lr: 0.0036 - 5s/epoch - 5ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 5s - loss: 16.2284 - mse: 16.2284 - mae: 1.6219 - val_loss: 10.5751 - val_mse: 10.5751 - val_mae: 1.6013 - lr: 0.0036 - 5s/epoch - 5ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 5s - loss: 16.1806 - mse: 16.1806 - mae: 1.6279 - val_loss: 10.4185 - val_mse: 10.4185 - val_mae: 1.5690 - lr: 0.0036 - 5s/epoch - 5ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 5s - loss: 16.2429 - mse: 16.2429 - mae: 1.6265 - val_loss: 10.6066 - val_mse: 10.6066 - val_mae: 1.6589 - lr: 0.0036 - 5s/epoch - 5ms/step\n",
            "Epoch 13/100\n",
            "1000/1000 - 5s - loss: 16.1667 - mse: 16.1667 - mae: 1.6255 - val_loss: 10.4354 - val_mse: 10.4354 - val_mae: 1.5619 - lr: 0.0036 - 5s/epoch - 5ms/step\n",
            "Epoch 14/100\n",
            "1000/1000 - 5s - loss: 16.1425 - mse: 16.1425 - mae: 1.6232 - val_loss: 10.5406 - val_mse: 10.5406 - val_mae: 1.6177 - lr: 0.0036 - 5s/epoch - 5ms/step\n",
            "Score for fold 1: loss of 10.540631294250488\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 5s - loss: 15.8861 - mse: 15.8861 - mae: 1.6158 - val_loss: 11.3636 - val_mse: 11.3636 - val_mae: 1.5440 - lr: 0.0010 - 5s/epoch - 5ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 5s - loss: 15.8766 - mse: 15.8766 - mae: 1.6080 - val_loss: 11.2305 - val_mse: 11.2305 - val_mae: 1.5648 - lr: 0.0010 - 5s/epoch - 5ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 5s - loss: 15.8880 - mse: 15.8880 - mae: 1.6036 - val_loss: 11.3843 - val_mse: 11.3843 - val_mae: 1.5767 - lr: 0.0010 - 5s/epoch - 5ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 5s - loss: 15.8186 - mse: 15.8186 - mae: 1.6030 - val_loss: 11.3745 - val_mse: 11.3745 - val_mae: 1.5783 - lr: 0.0010 - 5s/epoch - 5ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 5s - loss: 15.8243 - mse: 15.8243 - mae: 1.6004 - val_loss: 11.3489 - val_mse: 11.3489 - val_mae: 1.5349 - lr: 0.0010 - 5s/epoch - 5ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 5s - loss: 15.8355 - mse: 15.8355 - mae: 1.5945 - val_loss: 11.3085 - val_mse: 11.3085 - val_mae: 1.5492 - lr: 0.0010 - 5s/epoch - 5ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 5s - loss: 15.7931 - mse: 15.7931 - mae: 1.5865 - val_loss: 11.2310 - val_mse: 11.2310 - val_mae: 1.5898 - lr: 0.0010 - 5s/epoch - 5ms/step\n",
            "Score for fold 2: loss of 11.231023788452148\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 5s - loss: 15.4666 - mse: 15.4666 - mae: 1.5915 - val_loss: 12.6539 - val_mse: 12.6539 - val_mae: 1.5957 - lr: 0.0010 - 5s/epoch - 5ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 5s - loss: 15.4482 - mse: 15.4482 - mae: 1.5911 - val_loss: 12.6171 - val_mse: 12.6171 - val_mae: 1.5284 - lr: 0.0010 - 5s/epoch - 5ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 5s - loss: 15.4538 - mse: 15.4538 - mae: 1.5865 - val_loss: 12.7528 - val_mse: 12.7528 - val_mae: 1.5247 - lr: 0.0010 - 5s/epoch - 5ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 5s - loss: 15.4520 - mse: 15.4520 - mae: 1.5867 - val_loss: 12.6147 - val_mse: 12.6147 - val_mae: 1.6076 - lr: 0.0010 - 5s/epoch - 5ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 5s - loss: 15.3878 - mse: 15.3878 - mae: 1.5889 - val_loss: 12.7198 - val_mse: 12.7198 - val_mae: 1.6704 - lr: 0.0010 - 5s/epoch - 5ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 5s - loss: 15.3865 - mse: 15.3865 - mae: 1.5829 - val_loss: 12.6334 - val_mse: 12.6334 - val_mae: 1.6231 - lr: 0.0010 - 5s/epoch - 5ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 5s - loss: 15.3759 - mse: 15.3759 - mae: 1.5855 - val_loss: 12.6457 - val_mse: 12.6457 - val_mae: 1.5433 - lr: 0.0010 - 5s/epoch - 5ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 5s - loss: 15.3821 - mse: 15.3821 - mae: 1.5848 - val_loss: 12.6337 - val_mse: 12.6337 - val_mae: 1.5792 - lr: 0.0010 - 5s/epoch - 5ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 5s - loss: 15.4046 - mse: 15.4046 - mae: 1.5833 - val_loss: 12.6726 - val_mse: 12.6726 - val_mae: 1.6139 - lr: 0.0010 - 5s/epoch - 5ms/step\n",
            "Score for fold 3: loss of 12.67259693145752\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 5s - loss: 13.7663 - mse: 13.7663 - mae: 1.5622 - val_loss: 19.1010 - val_mse: 19.1010 - val_mae: 1.6107 - lr: 0.0010 - 5s/epoch - 5ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 5s - loss: 13.7750 - mse: 13.7750 - mae: 1.5590 - val_loss: 18.9903 - val_mse: 18.9903 - val_mae: 1.6395 - lr: 0.0010 - 5s/epoch - 5ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 5s - loss: 13.7760 - mse: 13.7760 - mae: 1.5661 - val_loss: 19.2991 - val_mse: 19.2991 - val_mae: 1.5794 - lr: 0.0010 - 5s/epoch - 5ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 5s - loss: 13.7624 - mse: 13.7624 - mae: 1.5658 - val_loss: 19.0512 - val_mse: 19.0512 - val_mae: 1.6312 - lr: 0.0010 - 5s/epoch - 5ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 5s - loss: 13.7690 - mse: 13.7690 - mae: 1.5645 - val_loss: 19.1747 - val_mse: 19.1747 - val_mae: 1.5696 - lr: 0.0010 - 5s/epoch - 5ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 5s - loss: 13.7666 - mse: 13.7666 - mae: 1.5613 - val_loss: 19.1320 - val_mse: 19.1320 - val_mae: 1.6845 - lr: 0.0010 - 5s/epoch - 5ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 5s - loss: 13.7550 - mse: 13.7550 - mae: 1.5638 - val_loss: 19.1567 - val_mse: 19.1567 - val_mae: 1.5961 - lr: 0.0010 - 5s/epoch - 5ms/step\n",
            "Score for fold 4: loss of 19.156723022460938\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 5s - loss: 13.3184 - mse: 13.3184 - mae: 1.5702 - val_loss: 20.9694 - val_mse: 20.9694 - val_mae: 1.6060 - lr: 0.0010 - 5s/epoch - 5ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 5s - loss: 13.2744 - mse: 13.2744 - mae: 1.5688 - val_loss: 21.0297 - val_mse: 21.0297 - val_mae: 1.5965 - lr: 0.0010 - 5s/epoch - 5ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 5s - loss: 13.3130 - mse: 13.3130 - mae: 1.5683 - val_loss: 20.9702 - val_mse: 20.9702 - val_mae: 1.6709 - lr: 0.0010 - 5s/epoch - 5ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 5s - loss: 13.2835 - mse: 13.2835 - mae: 1.5723 - val_loss: 21.0731 - val_mse: 21.0731 - val_mae: 1.6073 - lr: 0.0010 - 5s/epoch - 5ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 5s - loss: 13.2737 - mse: 13.2737 - mae: 1.5672 - val_loss: 21.0009 - val_mse: 21.0009 - val_mae: 1.6391 - lr: 0.0010 - 5s/epoch - 5ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 5s - loss: 13.2216 - mse: 13.2216 - mae: 1.5656 - val_loss: 20.9768 - val_mse: 20.9768 - val_mae: 1.6450 - lr: 0.0010 - 5s/epoch - 5ms/step\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-10-16 08:34:56,075]\u001b[0m Finished trial#40 resulted in value: 14.916. Current best value is 14.024000000000001 with parameters: {'activation': 'relu', 'num_hidden_layer': 4, 'i': 9, 'learning_rate': 0.00018064149922822524}.\u001b[0m\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Score for fold 5: loss of 20.97678565979004\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 7s - loss: 16.8988 - mse: 16.8988 - mae: 1.6258 - val_loss: 9.8166 - val_mse: 9.8166 - val_mae: 1.5464 - lr: 1.8751e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 6s - loss: 16.4212 - mse: 16.4212 - mae: 1.6085 - val_loss: 9.7237 - val_mse: 9.7237 - val_mae: 1.5028 - lr: 1.8751e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 7s - loss: 16.3454 - mse: 16.3454 - mae: 1.6026 - val_loss: 9.8152 - val_mse: 9.8152 - val_mae: 1.5502 - lr: 1.8751e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 7s - loss: 16.2452 - mse: 16.2452 - mae: 1.6028 - val_loss: 9.8659 - val_mse: 9.8659 - val_mae: 1.4940 - lr: 1.8751e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 7s - loss: 16.3345 - mse: 16.3345 - mae: 1.6015 - val_loss: 9.5058 - val_mse: 9.5058 - val_mae: 1.5157 - lr: 1.8751e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 7s - loss: 16.2564 - mse: 16.2564 - mae: 1.5937 - val_loss: 9.7646 - val_mse: 9.7646 - val_mae: 1.5466 - lr: 1.8751e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 7s - loss: 16.2108 - mse: 16.2108 - mae: 1.5902 - val_loss: 9.6161 - val_mse: 9.6161 - val_mae: 1.5732 - lr: 1.8751e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 7s - loss: 16.2084 - mse: 16.2084 - mae: 1.5924 - val_loss: 9.4871 - val_mse: 9.4871 - val_mae: 1.5183 - lr: 1.8751e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 7s - loss: 16.1516 - mse: 16.1516 - mae: 1.5888 - val_loss: 9.4680 - val_mse: 9.4680 - val_mae: 1.5274 - lr: 1.8751e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 7s - loss: 16.0895 - mse: 16.0895 - mae: 1.5820 - val_loss: 9.6561 - val_mse: 9.6561 - val_mae: 1.5488 - lr: 1.8751e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 7s - loss: 16.0545 - mse: 16.0545 - mae: 1.5853 - val_loss: 9.6552 - val_mse: 9.6552 - val_mae: 1.5013 - lr: 1.8751e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 7s - loss: 16.1319 - mse: 16.1319 - mae: 1.5796 - val_loss: 9.4914 - val_mse: 9.4914 - val_mae: 1.5700 - lr: 1.8751e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 13/100\n",
            "1000/1000 - 7s - loss: 16.0768 - mse: 16.0768 - mae: 1.5797 - val_loss: 9.5842 - val_mse: 9.5842 - val_mae: 1.6151 - lr: 1.8751e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 14/100\n",
            "1000/1000 - 7s - loss: 15.9879 - mse: 15.9879 - mae: 1.5762 - val_loss: 9.5800 - val_mse: 9.5800 - val_mae: 1.6172 - lr: 1.8751e-04 - 7s/epoch - 7ms/step\n",
            "Score for fold 1: loss of 9.580026626586914\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 6s - loss: 14.3887 - mse: 14.3887 - mae: 1.5627 - val_loss: 15.5584 - val_mse: 15.5584 - val_mae: 1.6223 - lr: 1.8751e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 7s - loss: 14.3450 - mse: 14.3450 - mae: 1.5530 - val_loss: 15.6539 - val_mse: 15.6539 - val_mae: 1.5687 - lr: 1.8751e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 7s - loss: 14.1388 - mse: 14.1388 - mae: 1.5564 - val_loss: 15.8746 - val_mse: 15.8746 - val_mae: 1.5594 - lr: 1.8751e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 7s - loss: 14.0306 - mse: 14.0306 - mae: 1.5466 - val_loss: 15.9181 - val_mse: 15.9181 - val_mae: 1.5815 - lr: 1.8751e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 7s - loss: 13.9412 - mse: 13.9412 - mae: 1.5502 - val_loss: 15.7852 - val_mse: 15.7852 - val_mae: 1.5479 - lr: 1.8751e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 6s - loss: 13.8920 - mse: 13.8920 - mae: 1.5411 - val_loss: 15.8092 - val_mse: 15.8092 - val_mae: 1.6900 - lr: 1.8751e-04 - 6s/epoch - 6ms/step\n",
            "Score for fold 2: loss of 15.809173583984375\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 6s - loss: 14.8323 - mse: 14.8323 - mae: 1.5628 - val_loss: 12.2255 - val_mse: 12.2255 - val_mae: 1.4998 - lr: 1.8751e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 7s - loss: 14.7610 - mse: 14.7610 - mae: 1.5506 - val_loss: 12.5148 - val_mse: 12.5148 - val_mae: 1.4972 - lr: 1.8751e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 7s - loss: 14.6514 - mse: 14.6514 - mae: 1.5483 - val_loss: 12.2529 - val_mse: 12.2529 - val_mae: 1.5241 - lr: 1.8751e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 7s - loss: 14.5309 - mse: 14.5309 - mae: 1.5521 - val_loss: 12.2057 - val_mse: 12.2057 - val_mae: 1.6840 - lr: 1.8751e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 7s - loss: 14.3578 - mse: 14.3578 - mae: 1.5439 - val_loss: 12.2704 - val_mse: 12.2704 - val_mae: 1.8528 - lr: 1.8751e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 7s - loss: 14.3864 - mse: 14.3864 - mae: 1.5343 - val_loss: 12.8023 - val_mse: 12.8023 - val_mae: 1.6001 - lr: 1.8751e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 7s - loss: 14.4330 - mse: 14.4330 - mae: 1.5383 - val_loss: 12.2865 - val_mse: 12.2865 - val_mae: 1.6537 - lr: 1.8751e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 7s - loss: 14.2859 - mse: 14.2859 - mae: 1.5290 - val_loss: 13.0916 - val_mse: 13.0916 - val_mae: 1.5001 - lr: 1.8751e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 7s - loss: 14.1523 - mse: 14.1523 - mae: 1.5263 - val_loss: 12.4595 - val_mse: 12.4595 - val_mae: 1.5567 - lr: 1.8751e-04 - 7s/epoch - 7ms/step\n",
            "Score for fold 3: loss of 12.459532737731934\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 7s - loss: 13.0544 - mse: 13.0544 - mae: 1.5437 - val_loss: 17.5695 - val_mse: 17.5695 - val_mae: 1.5707 - lr: 1.8751e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 7s - loss: 12.8590 - mse: 12.8590 - mae: 1.5486 - val_loss: 17.3507 - val_mse: 17.3507 - val_mae: 1.4972 - lr: 1.8751e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 7s - loss: 12.6665 - mse: 12.6665 - mae: 1.5389 - val_loss: 17.8149 - val_mse: 17.8149 - val_mae: 1.5010 - lr: 1.8751e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 7s - loss: 12.5230 - mse: 12.5230 - mae: 1.5372 - val_loss: 17.7388 - val_mse: 17.7388 - val_mae: 1.4419 - lr: 1.8751e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 7s - loss: 12.5989 - mse: 12.5989 - mae: 1.5436 - val_loss: 17.7777 - val_mse: 17.7777 - val_mae: 1.5593 - lr: 1.8751e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 6s - loss: 12.4753 - mse: 12.4753 - mae: 1.5338 - val_loss: 17.8044 - val_mse: 17.8044 - val_mae: 1.6435 - lr: 1.8751e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 7s - loss: 12.4865 - mse: 12.4865 - mae: 1.5381 - val_loss: 17.6660 - val_mse: 17.6660 - val_mae: 1.5894 - lr: 1.8751e-04 - 7s/epoch - 7ms/step\n",
            "Score for fold 4: loss of 17.66598129272461\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 7s - loss: 12.9719 - mse: 12.9719 - mae: 1.5512 - val_loss: 15.5990 - val_mse: 15.5990 - val_mae: 1.5129 - lr: 1.8751e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 7s - loss: 12.7724 - mse: 12.7724 - mae: 1.5629 - val_loss: 15.7691 - val_mse: 15.7691 - val_mae: 1.7540 - lr: 1.8751e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 7s - loss: 12.6914 - mse: 12.6914 - mae: 1.5557 - val_loss: 15.8861 - val_mse: 15.8861 - val_mae: 1.5517 - lr: 1.8751e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 7s - loss: 12.8972 - mse: 12.8972 - mae: 1.5491 - val_loss: 15.8200 - val_mse: 15.8200 - val_mae: 1.5367 - lr: 1.8751e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 7s - loss: 12.6484 - mse: 12.6484 - mae: 1.5457 - val_loss: 16.1331 - val_mse: 16.1331 - val_mae: 1.6993 - lr: 1.8751e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 7s - loss: 12.4624 - mse: 12.4624 - mae: 1.5434 - val_loss: 16.1795 - val_mse: 16.1795 - val_mae: 1.6866 - lr: 1.8751e-04 - 7s/epoch - 7ms/step\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-10-16 08:39:42,080]\u001b[0m Finished trial#41 resulted in value: 14.34. Current best value is 14.024000000000001 with parameters: {'activation': 'relu', 'num_hidden_layer': 4, 'i': 9, 'learning_rate': 0.00018064149922822524}.\u001b[0m\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Score for fold 5: loss of 16.179515838623047\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 7s - loss: 15.3452 - mse: 15.3452 - mae: 1.6065 - val_loss: 15.7767 - val_mse: 15.7767 - val_mae: 1.6246 - lr: 1.7547e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 7s - loss: 15.0078 - mse: 15.0078 - mae: 1.5880 - val_loss: 15.8253 - val_mse: 15.8253 - val_mae: 1.6076 - lr: 1.7547e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 7s - loss: 14.8965 - mse: 14.8965 - mae: 1.5808 - val_loss: 15.5085 - val_mse: 15.5085 - val_mae: 1.6208 - lr: 1.7547e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 7s - loss: 14.8590 - mse: 14.8590 - mae: 1.5784 - val_loss: 15.4575 - val_mse: 15.4575 - val_mae: 1.6634 - lr: 1.7547e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 7s - loss: 14.8240 - mse: 14.8240 - mae: 1.5734 - val_loss: 15.4273 - val_mse: 15.4273 - val_mae: 1.6853 - lr: 1.7547e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 7s - loss: 14.7460 - mse: 14.7460 - mae: 1.5723 - val_loss: 15.4865 - val_mse: 15.4865 - val_mae: 1.6349 - lr: 1.7547e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 7s - loss: 14.7417 - mse: 14.7417 - mae: 1.5685 - val_loss: 15.4053 - val_mse: 15.4053 - val_mae: 1.5871 - lr: 1.7547e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 7s - loss: 14.7200 - mse: 14.7200 - mae: 1.5676 - val_loss: 15.3982 - val_mse: 15.3982 - val_mae: 1.6547 - lr: 1.7547e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 7s - loss: 14.6692 - mse: 14.6692 - mae: 1.5632 - val_loss: 15.3657 - val_mse: 15.3657 - val_mae: 1.6089 - lr: 1.7547e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 7s - loss: 14.6756 - mse: 14.6756 - mae: 1.5656 - val_loss: 15.4146 - val_mse: 15.4146 - val_mae: 1.5964 - lr: 1.7547e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 7s - loss: 14.6420 - mse: 14.6420 - mae: 1.5581 - val_loss: 15.4099 - val_mse: 15.4099 - val_mae: 1.6168 - lr: 1.7547e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 7s - loss: 14.6012 - mse: 14.6012 - mae: 1.5604 - val_loss: 15.3687 - val_mse: 15.3687 - val_mae: 1.6037 - lr: 1.7547e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 13/100\n",
            "1000/1000 - 7s - loss: 14.4769 - mse: 14.4769 - mae: 1.5567 - val_loss: 15.5579 - val_mse: 15.5579 - val_mae: 1.6319 - lr: 1.7547e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 14/100\n",
            "1000/1000 - 7s - loss: 14.5069 - mse: 14.5069 - mae: 1.5579 - val_loss: 15.3504 - val_mse: 15.3504 - val_mae: 1.7391 - lr: 1.7547e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 15/100\n",
            "1000/1000 - 7s - loss: 14.4690 - mse: 14.4690 - mae: 1.5543 - val_loss: 15.3419 - val_mse: 15.3419 - val_mae: 1.6641 - lr: 1.7547e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 16/100\n",
            "1000/1000 - 7s - loss: 14.3112 - mse: 14.3112 - mae: 1.5571 - val_loss: 15.3990 - val_mse: 15.3990 - val_mae: 1.6717 - lr: 1.7547e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 17/100\n",
            "1000/1000 - 7s - loss: 14.2808 - mse: 14.2808 - mae: 1.5532 - val_loss: 15.6381 - val_mse: 15.6381 - val_mae: 1.5657 - lr: 1.7547e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 18/100\n",
            "1000/1000 - 7s - loss: 14.1985 - mse: 14.1985 - mae: 1.5514 - val_loss: 15.5307 - val_mse: 15.5307 - val_mae: 1.7709 - lr: 1.7547e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 19/100\n",
            "1000/1000 - 7s - loss: 14.1809 - mse: 14.1809 - mae: 1.5522 - val_loss: 15.5721 - val_mse: 15.5721 - val_mae: 1.7326 - lr: 1.7547e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 20/100\n",
            "1000/1000 - 7s - loss: 14.1084 - mse: 14.1084 - mae: 1.5473 - val_loss: 15.6407 - val_mse: 15.6407 - val_mae: 1.6538 - lr: 1.7547e-04 - 7s/epoch - 7ms/step\n",
            "Score for fold 1: loss of 15.640671730041504\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 6s - loss: 13.1454 - mse: 13.1454 - mae: 1.5601 - val_loss: 19.0316 - val_mse: 19.0316 - val_mae: 1.5205 - lr: 1.7547e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 7s - loss: 13.0841 - mse: 13.0841 - mae: 1.5576 - val_loss: 19.1419 - val_mse: 19.1419 - val_mae: 1.4797 - lr: 1.7547e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 7s - loss: 13.0020 - mse: 13.0020 - mae: 1.5578 - val_loss: 19.1952 - val_mse: 19.1952 - val_mae: 1.7379 - lr: 1.7547e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 7s - loss: 12.8227 - mse: 12.8227 - mae: 1.5543 - val_loss: 19.4770 - val_mse: 19.4770 - val_mae: 1.4967 - lr: 1.7547e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 7s - loss: 12.7475 - mse: 12.7475 - mae: 1.5501 - val_loss: 19.2022 - val_mse: 19.2022 - val_mae: 1.6995 - lr: 1.7547e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 7s - loss: 12.7520 - mse: 12.7520 - mae: 1.5422 - val_loss: 19.2297 - val_mse: 19.2297 - val_mae: 1.6225 - lr: 1.7547e-04 - 7s/epoch - 7ms/step\n",
            "Score for fold 2: loss of 19.22968101501465\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 6s - loss: 14.0276 - mse: 14.0276 - mae: 1.5562 - val_loss: 13.7000 - val_mse: 13.7000 - val_mae: 1.6090 - lr: 1.7547e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 7s - loss: 14.0676 - mse: 14.0676 - mae: 1.5539 - val_loss: 13.9105 - val_mse: 13.9105 - val_mae: 1.6745 - lr: 1.7547e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 7s - loss: 13.9772 - mse: 13.9772 - mae: 1.5478 - val_loss: 13.8229 - val_mse: 13.8229 - val_mae: 1.5957 - lr: 1.7547e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 6s - loss: 13.8207 - mse: 13.8207 - mae: 1.5546 - val_loss: 14.0178 - val_mse: 14.0178 - val_mae: 1.6528 - lr: 1.7547e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 7s - loss: 13.8299 - mse: 13.8299 - mae: 1.5491 - val_loss: 14.1180 - val_mse: 14.1180 - val_mae: 1.7279 - lr: 1.7547e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 7s - loss: 13.7783 - mse: 13.7783 - mae: 1.5605 - val_loss: 14.2633 - val_mse: 14.2633 - val_mae: 1.6324 - lr: 1.7547e-04 - 7s/epoch - 7ms/step\n",
            "Score for fold 3: loss of 14.263273239135742\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 7s - loss: 15.3823 - mse: 15.3823 - mae: 1.5854 - val_loss: 7.7474 - val_mse: 7.7474 - val_mae: 1.6806 - lr: 1.7547e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 7s - loss: 15.1478 - mse: 15.1478 - mae: 1.5822 - val_loss: 8.0427 - val_mse: 8.0427 - val_mae: 1.3936 - lr: 1.7547e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 7s - loss: 15.1386 - mse: 15.1386 - mae: 1.5938 - val_loss: 7.8343 - val_mse: 7.8343 - val_mae: 1.5447 - lr: 1.7547e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 7s - loss: 14.9948 - mse: 14.9948 - mae: 1.5705 - val_loss: 7.9782 - val_mse: 7.9782 - val_mae: 1.4278 - lr: 1.7547e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 7s - loss: 14.9725 - mse: 14.9725 - mae: 1.5931 - val_loss: 8.4389 - val_mse: 8.4389 - val_mae: 1.6976 - lr: 1.7547e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 7s - loss: 14.9551 - mse: 14.9551 - mae: 1.5977 - val_loss: 8.7478 - val_mse: 8.7478 - val_mae: 1.7759 - lr: 1.7547e-04 - 7s/epoch - 7ms/step\n",
            "Score for fold 4: loss of 8.747747421264648\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 6s - loss: 13.5193 - mse: 13.5193 - mae: 1.5881 - val_loss: 14.2709 - val_mse: 14.2709 - val_mae: 1.6794 - lr: 1.7547e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 7s - loss: 13.3292 - mse: 13.3292 - mae: 1.5970 - val_loss: 14.5036 - val_mse: 14.5036 - val_mae: 1.6988 - lr: 1.7547e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 7s - loss: 13.2411 - mse: 13.2411 - mae: 1.5859 - val_loss: 13.9253 - val_mse: 13.9253 - val_mae: 1.4990 - lr: 1.7547e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 7s - loss: 13.2875 - mse: 13.2875 - mae: 1.6274 - val_loss: 14.2992 - val_mse: 14.2992 - val_mae: 1.5380 - lr: 1.7547e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 7s - loss: 13.0170 - mse: 13.0170 - mae: 1.5990 - val_loss: 15.6328 - val_mse: 15.6328 - val_mae: 2.1762 - lr: 1.7547e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 7s - loss: 12.8749 - mse: 12.8749 - mae: 1.5793 - val_loss: 14.6046 - val_mse: 14.6046 - val_mae: 1.4794 - lr: 1.7547e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 7s - loss: 12.9882 - mse: 12.9882 - mae: 1.5968 - val_loss: 14.4388 - val_mse: 14.4388 - val_mae: 1.6080 - lr: 1.7547e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 7s - loss: 12.8217 - mse: 12.8217 - mae: 1.5800 - val_loss: 14.2571 - val_mse: 14.2571 - val_mae: 1.5495 - lr: 1.7547e-04 - 7s/epoch - 7ms/step\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-10-16 08:44:59,362]\u001b[0m Finished trial#42 resulted in value: 14.428. Current best value is 14.024000000000001 with parameters: {'activation': 'relu', 'num_hidden_layer': 4, 'i': 9, 'learning_rate': 0.00018064149922822524}.\u001b[0m\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Score for fold 5: loss of 14.257126808166504\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 7s - loss: 14.5317 - mse: 14.5317 - mae: 1.6164 - val_loss: 19.6554 - val_mse: 19.6554 - val_mae: 1.5803 - lr: 1.2677e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 6s - loss: 13.9974 - mse: 13.9974 - mae: 1.5880 - val_loss: 19.6017 - val_mse: 19.6017 - val_mae: 1.5429 - lr: 1.2677e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 7s - loss: 13.9084 - mse: 13.9084 - mae: 1.5856 - val_loss: 19.4942 - val_mse: 19.4942 - val_mae: 1.5864 - lr: 1.2677e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 7s - loss: 13.8645 - mse: 13.8645 - mae: 1.5824 - val_loss: 19.4508 - val_mse: 19.4508 - val_mae: 1.5786 - lr: 1.2677e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 7s - loss: 13.7874 - mse: 13.7874 - mae: 1.5813 - val_loss: 19.6249 - val_mse: 19.6249 - val_mae: 1.5378 - lr: 1.2677e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 7s - loss: 13.7930 - mse: 13.7930 - mae: 1.5807 - val_loss: 19.3719 - val_mse: 19.3719 - val_mae: 1.6312 - lr: 1.2677e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 7s - loss: 13.7892 - mse: 13.7892 - mae: 1.5782 - val_loss: 19.4207 - val_mse: 19.4207 - val_mae: 1.6477 - lr: 1.2677e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 7s - loss: 13.6965 - mse: 13.6965 - mae: 1.5750 - val_loss: 19.5172 - val_mse: 19.5172 - val_mae: 1.6082 - lr: 1.2677e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 7s - loss: 13.6891 - mse: 13.6891 - mae: 1.5723 - val_loss: 19.4736 - val_mse: 19.4736 - val_mae: 1.6378 - lr: 1.2677e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 7s - loss: 13.6278 - mse: 13.6278 - mae: 1.5697 - val_loss: 19.3831 - val_mse: 19.3831 - val_mae: 1.5450 - lr: 1.2677e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 7s - loss: 13.5910 - mse: 13.5910 - mae: 1.5654 - val_loss: 19.5765 - val_mse: 19.5765 - val_mae: 1.5292 - lr: 1.2677e-04 - 7s/epoch - 7ms/step\n",
            "Score for fold 1: loss of 19.576457977294922\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 7s - loss: 15.4492 - mse: 15.4492 - mae: 1.5734 - val_loss: 12.1251 - val_mse: 12.1251 - val_mae: 1.5089 - lr: 1.2677e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 7s - loss: 15.3221 - mse: 15.3221 - mae: 1.5681 - val_loss: 12.1547 - val_mse: 12.1547 - val_mae: 1.5023 - lr: 1.2677e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 7s - loss: 15.2856 - mse: 15.2856 - mae: 1.5652 - val_loss: 12.1389 - val_mse: 12.1389 - val_mae: 1.6136 - lr: 1.2677e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 7s - loss: 15.2349 - mse: 15.2349 - mae: 1.5643 - val_loss: 12.0533 - val_mse: 12.0533 - val_mae: 1.5502 - lr: 1.2677e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 7s - loss: 15.1485 - mse: 15.1485 - mae: 1.5662 - val_loss: 12.1469 - val_mse: 12.1469 - val_mae: 1.5476 - lr: 1.2677e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 7s - loss: 15.0779 - mse: 15.0779 - mae: 1.5613 - val_loss: 12.2816 - val_mse: 12.2816 - val_mae: 1.5355 - lr: 1.2677e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 7s - loss: 15.1149 - mse: 15.1149 - mae: 1.5551 - val_loss: 12.2337 - val_mse: 12.2337 - val_mae: 1.5253 - lr: 1.2677e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 7s - loss: 15.0268 - mse: 15.0268 - mae: 1.5572 - val_loss: 12.2490 - val_mse: 12.2490 - val_mae: 1.5299 - lr: 1.2677e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 7s - loss: 14.9226 - mse: 14.9226 - mae: 1.5531 - val_loss: 12.2891 - val_mse: 12.2891 - val_mae: 1.5375 - lr: 1.2677e-04 - 7s/epoch - 7ms/step\n",
            "Score for fold 2: loss of 12.289071083068848\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 6s - loss: 14.1412 - mse: 14.1412 - mae: 1.5618 - val_loss: 15.8140 - val_mse: 15.8140 - val_mae: 1.4718 - lr: 1.2677e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 7s - loss: 14.0261 - mse: 14.0261 - mae: 1.5612 - val_loss: 15.8010 - val_mse: 15.8010 - val_mae: 1.5794 - lr: 1.2677e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 7s - loss: 13.9897 - mse: 13.9897 - mae: 1.5574 - val_loss: 15.8883 - val_mse: 15.8883 - val_mae: 1.5491 - lr: 1.2677e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 7s - loss: 13.9196 - mse: 13.9196 - mae: 1.5548 - val_loss: 16.1145 - val_mse: 16.1145 - val_mae: 1.6049 - lr: 1.2677e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 7s - loss: 13.7994 - mse: 13.7994 - mae: 1.5571 - val_loss: 15.8511 - val_mse: 15.8511 - val_mae: 1.5216 - lr: 1.2677e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 7s - loss: 13.6638 - mse: 13.6638 - mae: 1.5526 - val_loss: 15.9183 - val_mse: 15.9183 - val_mae: 1.5282 - lr: 1.2677e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 7s - loss: 13.6858 - mse: 13.6858 - mae: 1.5475 - val_loss: 16.1276 - val_mse: 16.1276 - val_mae: 1.4713 - lr: 1.2677e-04 - 7s/epoch - 7ms/step\n",
            "Score for fold 3: loss of 16.127609252929688\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 7s - loss: 14.4708 - mse: 14.4708 - mae: 1.5501 - val_loss: 12.2996 - val_mse: 12.2996 - val_mae: 1.4912 - lr: 1.2677e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 7s - loss: 14.3963 - mse: 14.3963 - mae: 1.5409 - val_loss: 12.2912 - val_mse: 12.2912 - val_mae: 1.5371 - lr: 1.2677e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 7s - loss: 14.3646 - mse: 14.3646 - mae: 1.5379 - val_loss: 12.4573 - val_mse: 12.4573 - val_mae: 1.5119 - lr: 1.2677e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 7s - loss: 14.3181 - mse: 14.3181 - mae: 1.5373 - val_loss: 12.2974 - val_mse: 12.2974 - val_mae: 1.5587 - lr: 1.2677e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 7s - loss: 14.1737 - mse: 14.1737 - mae: 1.5293 - val_loss: 12.4718 - val_mse: 12.4718 - val_mae: 1.6495 - lr: 1.2677e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 7s - loss: 14.1342 - mse: 14.1342 - mae: 1.5253 - val_loss: 12.9436 - val_mse: 12.9436 - val_mae: 1.4763 - lr: 1.2677e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 7s - loss: 14.0500 - mse: 14.0500 - mae: 1.5267 - val_loss: 12.4626 - val_mse: 12.4626 - val_mae: 1.5701 - lr: 1.2677e-04 - 7s/epoch - 7ms/step\n",
            "Score for fold 4: loss of 12.46256160736084\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 7s - loss: 14.2057 - mse: 14.2057 - mae: 1.5258 - val_loss: 11.7154 - val_mse: 11.7154 - val_mae: 1.5524 - lr: 1.2677e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 7s - loss: 14.1462 - mse: 14.1462 - mae: 1.5205 - val_loss: 12.6736 - val_mse: 12.6736 - val_mae: 1.6777 - lr: 1.2677e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 7s - loss: 14.0317 - mse: 14.0317 - mae: 1.5226 - val_loss: 12.4076 - val_mse: 12.4076 - val_mae: 1.5714 - lr: 1.2677e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 7s - loss: 13.9506 - mse: 13.9506 - mae: 1.5145 - val_loss: 12.8175 - val_mse: 12.8175 - val_mae: 1.5987 - lr: 1.2677e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 7s - loss: 13.8133 - mse: 13.8133 - mae: 1.5026 - val_loss: 13.1085 - val_mse: 13.1085 - val_mae: 1.5170 - lr: 1.2677e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 7s - loss: 13.7913 - mse: 13.7913 - mae: 1.5036 - val_loss: 12.7017 - val_mse: 12.7017 - val_mae: 1.6552 - lr: 1.2677e-04 - 7s/epoch - 7ms/step\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-10-16 08:49:39,586]\u001b[0m Finished trial#43 resulted in value: 14.632. Current best value is 14.024000000000001 with parameters: {'activation': 'relu', 'num_hidden_layer': 4, 'i': 9, 'learning_rate': 0.00018064149922822524}.\u001b[0m\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Score for fold 5: loss of 12.701650619506836\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 5s - loss: 16.2345 - mse: 16.2345 - mae: 1.6227 - val_loss: 12.4630 - val_mse: 12.4630 - val_mae: 1.6379 - lr: 2.0087e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 4s - loss: 15.7679 - mse: 15.7679 - mae: 1.5989 - val_loss: 12.5134 - val_mse: 12.5134 - val_mae: 1.5152 - lr: 2.0087e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 4s - loss: 15.7052 - mse: 15.7052 - mae: 1.5948 - val_loss: 12.3832 - val_mse: 12.3832 - val_mae: 1.5635 - lr: 2.0087e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 5s - loss: 15.6523 - mse: 15.6523 - mae: 1.5903 - val_loss: 12.3312 - val_mse: 12.3312 - val_mae: 1.5262 - lr: 2.0087e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 5s - loss: 15.5567 - mse: 15.5567 - mae: 1.5906 - val_loss: 12.2884 - val_mse: 12.2884 - val_mae: 1.5820 - lr: 2.0087e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 4s - loss: 15.5826 - mse: 15.5826 - mae: 1.5902 - val_loss: 12.3666 - val_mse: 12.3666 - val_mae: 1.5840 - lr: 2.0087e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 4s - loss: 15.5251 - mse: 15.5251 - mae: 1.5822 - val_loss: 12.3135 - val_mse: 12.3135 - val_mae: 1.6505 - lr: 2.0087e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 4s - loss: 15.4516 - mse: 15.4516 - mae: 1.5820 - val_loss: 12.2887 - val_mse: 12.2887 - val_mae: 1.5465 - lr: 2.0087e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 4s - loss: 15.4588 - mse: 15.4588 - mae: 1.5802 - val_loss: 12.2773 - val_mse: 12.2773 - val_mae: 1.5640 - lr: 2.0087e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 4s - loss: 15.3797 - mse: 15.3797 - mae: 1.5809 - val_loss: 12.3690 - val_mse: 12.3690 - val_mae: 1.4942 - lr: 2.0087e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 4s - loss: 15.3747 - mse: 15.3747 - mae: 1.5778 - val_loss: 12.3106 - val_mse: 12.3106 - val_mae: 1.5285 - lr: 2.0087e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 4s - loss: 15.3640 - mse: 15.3640 - mae: 1.5783 - val_loss: 12.3673 - val_mse: 12.3673 - val_mae: 1.5978 - lr: 2.0087e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 13/100\n",
            "1000/1000 - 4s - loss: 15.3710 - mse: 15.3710 - mae: 1.5741 - val_loss: 12.4574 - val_mse: 12.4574 - val_mae: 1.6583 - lr: 2.0087e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 14/100\n",
            "1000/1000 - 5s - loss: 15.3159 - mse: 15.3159 - mae: 1.5735 - val_loss: 12.2451 - val_mse: 12.2451 - val_mae: 1.6362 - lr: 2.0087e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 15/100\n",
            "1000/1000 - 4s - loss: 15.2806 - mse: 15.2806 - mae: 1.5737 - val_loss: 12.2494 - val_mse: 12.2494 - val_mae: 1.5756 - lr: 2.0087e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 16/100\n",
            "1000/1000 - 4s - loss: 15.2574 - mse: 15.2574 - mae: 1.5750 - val_loss: 12.3835 - val_mse: 12.3835 - val_mae: 1.5810 - lr: 2.0087e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 17/100\n",
            "1000/1000 - 4s - loss: 15.1802 - mse: 15.1802 - mae: 1.5734 - val_loss: 12.2443 - val_mse: 12.2443 - val_mae: 1.6231 - lr: 2.0087e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 18/100\n",
            "1000/1000 - 4s - loss: 15.1966 - mse: 15.1966 - mae: 1.5676 - val_loss: 12.6092 - val_mse: 12.6092 - val_mae: 1.4655 - lr: 2.0087e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 19/100\n",
            "1000/1000 - 4s - loss: 15.1379 - mse: 15.1379 - mae: 1.5651 - val_loss: 12.4159 - val_mse: 12.4159 - val_mae: 1.5720 - lr: 2.0087e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 20/100\n",
            "1000/1000 - 4s - loss: 15.0673 - mse: 15.0673 - mae: 1.5665 - val_loss: 12.5354 - val_mse: 12.5354 - val_mae: 1.5183 - lr: 2.0087e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 21/100\n",
            "1000/1000 - 4s - loss: 15.0208 - mse: 15.0208 - mae: 1.5643 - val_loss: 12.3493 - val_mse: 12.3493 - val_mae: 1.5252 - lr: 2.0087e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 22/100\n",
            "1000/1000 - 4s - loss: 14.9619 - mse: 14.9619 - mae: 1.5632 - val_loss: 12.4505 - val_mse: 12.4505 - val_mae: 1.6736 - lr: 2.0087e-04 - 4s/epoch - 4ms/step\n",
            "Score for fold 1: loss of 12.450512886047363\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 4s - loss: 12.7812 - mse: 12.7812 - mae: 1.5615 - val_loss: 21.4370 - val_mse: 21.4370 - val_mae: 1.5263 - lr: 2.0087e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 4s - loss: 12.7198 - mse: 12.7198 - mae: 1.5521 - val_loss: 21.1542 - val_mse: 21.1542 - val_mae: 1.6594 - lr: 2.0087e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 4s - loss: 12.7149 - mse: 12.7149 - mae: 1.5523 - val_loss: 21.3780 - val_mse: 21.3780 - val_mae: 1.6108 - lr: 2.0087e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 5s - loss: 12.6791 - mse: 12.6791 - mae: 1.5496 - val_loss: 21.3640 - val_mse: 21.3640 - val_mae: 1.6063 - lr: 2.0087e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 5s - loss: 12.5994 - mse: 12.5994 - mae: 1.5508 - val_loss: 21.2353 - val_mse: 21.2353 - val_mae: 1.5479 - lr: 2.0087e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 4s - loss: 12.5181 - mse: 12.5181 - mae: 1.5464 - val_loss: 21.3378 - val_mse: 21.3378 - val_mae: 1.5273 - lr: 2.0087e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 5s - loss: 12.3835 - mse: 12.3835 - mae: 1.5407 - val_loss: 21.5013 - val_mse: 21.5013 - val_mae: 1.7100 - lr: 2.0087e-04 - 5s/epoch - 5ms/step\n",
            "Score for fold 2: loss of 21.501298904418945\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 5s - loss: 15.1085 - mse: 15.1085 - mae: 1.5673 - val_loss: 10.7957 - val_mse: 10.7957 - val_mae: 1.6558 - lr: 2.0087e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 5s - loss: 15.0185 - mse: 15.0185 - mae: 1.5638 - val_loss: 10.6556 - val_mse: 10.6556 - val_mae: 1.4892 - lr: 2.0087e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 5s - loss: 14.9526 - mse: 14.9526 - mae: 1.5646 - val_loss: 10.7967 - val_mse: 10.7967 - val_mae: 1.5259 - lr: 2.0087e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 4s - loss: 14.9455 - mse: 14.9455 - mae: 1.5567 - val_loss: 10.9974 - val_mse: 10.9974 - val_mae: 1.6025 - lr: 2.0087e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 4s - loss: 14.7369 - mse: 14.7369 - mae: 1.5589 - val_loss: 10.7213 - val_mse: 10.7213 - val_mae: 1.5868 - lr: 2.0087e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 4s - loss: 14.7983 - mse: 14.7983 - mae: 1.5641 - val_loss: 11.0720 - val_mse: 11.0720 - val_mae: 1.7188 - lr: 2.0087e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 5s - loss: 14.7665 - mse: 14.7665 - mae: 1.5661 - val_loss: 10.9789 - val_mse: 10.9789 - val_mae: 1.5462 - lr: 2.0087e-04 - 5s/epoch - 5ms/step\n",
            "Score for fold 3: loss of 10.978951454162598\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 4s - loss: 14.6903 - mse: 14.6903 - mae: 1.5608 - val_loss: 11.0593 - val_mse: 11.0593 - val_mae: 1.6037 - lr: 2.0087e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 5s - loss: 14.6569 - mse: 14.6569 - mae: 1.5626 - val_loss: 11.4371 - val_mse: 11.4371 - val_mae: 1.4770 - lr: 2.0087e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 5s - loss: 14.4249 - mse: 14.4249 - mae: 1.5519 - val_loss: 11.7579 - val_mse: 11.7579 - val_mae: 1.5904 - lr: 2.0087e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 5s - loss: 14.3702 - mse: 14.3702 - mae: 1.5547 - val_loss: 11.8712 - val_mse: 11.8712 - val_mae: 1.4879 - lr: 2.0087e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 4s - loss: 14.3298 - mse: 14.3298 - mae: 1.5558 - val_loss: 11.7903 - val_mse: 11.7903 - val_mae: 1.6797 - lr: 2.0087e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 5s - loss: 14.2375 - mse: 14.2375 - mae: 1.5447 - val_loss: 12.1175 - val_mse: 12.1175 - val_mae: 1.6995 - lr: 2.0087e-04 - 5s/epoch - 5ms/step\n",
            "Score for fold 4: loss of 12.117465019226074\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 4s - loss: 13.5344 - mse: 13.5344 - mae: 1.5661 - val_loss: 15.3050 - val_mse: 15.3050 - val_mae: 1.5366 - lr: 2.0087e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 5s - loss: 13.4072 - mse: 13.4072 - mae: 1.5646 - val_loss: 15.3847 - val_mse: 15.3847 - val_mae: 1.6896 - lr: 2.0087e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 5s - loss: 13.2932 - mse: 13.2932 - mae: 1.5556 - val_loss: 15.4769 - val_mse: 15.4769 - val_mae: 1.5453 - lr: 2.0087e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 5s - loss: 13.1800 - mse: 13.1800 - mae: 1.5515 - val_loss: 15.5612 - val_mse: 15.5612 - val_mae: 1.6661 - lr: 2.0087e-04 - 5s/epoch - 5ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 4s - loss: 13.0947 - mse: 13.0947 - mae: 1.5536 - val_loss: 15.8653 - val_mse: 15.8653 - val_mae: 1.8585 - lr: 2.0087e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 4s - loss: 13.1137 - mse: 13.1137 - mae: 1.5458 - val_loss: 15.7285 - val_mse: 15.7285 - val_mae: 1.8108 - lr: 2.0087e-04 - 4s/epoch - 4ms/step\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-10-16 08:53:20,368]\u001b[0m Finished trial#44 resulted in value: 14.556000000000001. Current best value is 14.024000000000001 with parameters: {'activation': 'relu', 'num_hidden_layer': 4, 'i': 9, 'learning_rate': 0.00018064149922822524}.\u001b[0m\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Score for fold 5: loss of 15.728459358215332\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 16s - loss: 17.4235 - mse: 17.4235 - mae: 1.6982 - val_loss: 9.5476 - val_mse: 9.5476 - val_mae: 1.5704 - lr: 2.7865e-04 - 16s/epoch - 16ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 15s - loss: 17.0697 - mse: 17.0697 - mae: 1.6467 - val_loss: 9.6370 - val_mse: 9.6370 - val_mae: 1.5327 - lr: 2.7865e-04 - 15s/epoch - 15ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 15s - loss: 16.9564 - mse: 16.9564 - mae: 1.6356 - val_loss: 9.6111 - val_mse: 9.6111 - val_mae: 1.5427 - lr: 2.7865e-04 - 15s/epoch - 15ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 15s - loss: 16.8981 - mse: 16.8981 - mae: 1.6295 - val_loss: 9.5384 - val_mse: 9.5384 - val_mae: 1.6490 - lr: 2.7865e-04 - 15s/epoch - 15ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 15s - loss: 16.9584 - mse: 16.9584 - mae: 1.6274 - val_loss: 9.6680 - val_mse: 9.6680 - val_mae: 1.5910 - lr: 2.7865e-04 - 15s/epoch - 15ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 15s - loss: 16.8337 - mse: 16.8337 - mae: 1.6289 - val_loss: 9.4721 - val_mse: 9.4721 - val_mae: 1.5983 - lr: 2.7865e-04 - 15s/epoch - 15ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 15s - loss: 16.8567 - mse: 16.8567 - mae: 1.6200 - val_loss: 9.5723 - val_mse: 9.5723 - val_mae: 1.5549 - lr: 2.7865e-04 - 15s/epoch - 15ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 15s - loss: 16.7878 - mse: 16.7878 - mae: 1.6158 - val_loss: 9.3332 - val_mse: 9.3332 - val_mae: 1.5689 - lr: 2.7865e-04 - 15s/epoch - 15ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 15s - loss: 16.7974 - mse: 16.7974 - mae: 1.6189 - val_loss: 9.5966 - val_mse: 9.5966 - val_mae: 1.5594 - lr: 2.7865e-04 - 15s/epoch - 15ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 15s - loss: 16.7444 - mse: 16.7444 - mae: 1.6161 - val_loss: 9.3547 - val_mse: 9.3547 - val_mae: 1.5288 - lr: 2.7865e-04 - 15s/epoch - 15ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 16s - loss: 16.7530 - mse: 16.7530 - mae: 1.6100 - val_loss: 9.9023 - val_mse: 9.9023 - val_mae: 1.5425 - lr: 2.7865e-04 - 16s/epoch - 16ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 15s - loss: 16.7436 - mse: 16.7436 - mae: 1.6101 - val_loss: 9.3771 - val_mse: 9.3771 - val_mae: 1.5694 - lr: 2.7865e-04 - 15s/epoch - 15ms/step\n",
            "Epoch 13/100\n",
            "1000/1000 - 15s - loss: 16.7531 - mse: 16.7531 - mae: 1.6097 - val_loss: 9.4190 - val_mse: 9.4190 - val_mae: 1.5725 - lr: 2.7865e-04 - 15s/epoch - 15ms/step\n",
            "Score for fold 1: loss of 9.418991088867188\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 14s - loss: 14.7108 - mse: 14.7108 - mae: 1.6017 - val_loss: 17.5083 - val_mse: 17.5083 - val_mae: 1.6895 - lr: 2.7865e-04 - 14s/epoch - 14ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 15s - loss: 14.6783 - mse: 14.6783 - mae: 1.6022 - val_loss: 17.7055 - val_mse: 17.7055 - val_mae: 1.6529 - lr: 2.7865e-04 - 15s/epoch - 15ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 15s - loss: 14.7039 - mse: 14.7039 - mae: 1.5982 - val_loss: 17.6059 - val_mse: 17.6059 - val_mae: 1.6195 - lr: 2.7865e-04 - 15s/epoch - 15ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 15s - loss: 14.7043 - mse: 14.7043 - mae: 1.6043 - val_loss: 17.5890 - val_mse: 17.5890 - val_mae: 1.6256 - lr: 2.7865e-04 - 15s/epoch - 15ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 15s - loss: 14.6827 - mse: 14.6827 - mae: 1.6073 - val_loss: 17.6416 - val_mse: 17.6416 - val_mae: 1.6387 - lr: 2.7865e-04 - 15s/epoch - 15ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 15s - loss: 14.6840 - mse: 14.6840 - mae: 1.6005 - val_loss: 17.5175 - val_mse: 17.5175 - val_mae: 1.5684 - lr: 2.7865e-04 - 15s/epoch - 15ms/step\n",
            "Score for fold 2: loss of 17.51747703552246\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 15s - loss: 12.8688 - mse: 12.8688 - mae: 1.5955 - val_loss: 24.7321 - val_mse: 24.7321 - val_mae: 1.7067 - lr: 2.7865e-04 - 15s/epoch - 15ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 15s - loss: 12.8627 - mse: 12.8627 - mae: 1.5872 - val_loss: 24.8185 - val_mse: 24.8185 - val_mae: 1.5786 - lr: 2.7865e-04 - 15s/epoch - 15ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 15s - loss: 12.8349 - mse: 12.8349 - mae: 1.5855 - val_loss: 24.7708 - val_mse: 24.7708 - val_mae: 1.6685 - lr: 2.7865e-04 - 15s/epoch - 15ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 15s - loss: 12.8543 - mse: 12.8543 - mae: 1.5926 - val_loss: 24.7418 - val_mse: 24.7418 - val_mae: 1.5746 - lr: 2.7865e-04 - 15s/epoch - 15ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 15s - loss: 12.8551 - mse: 12.8551 - mae: 1.5871 - val_loss: 24.5875 - val_mse: 24.5875 - val_mae: 1.6134 - lr: 2.7865e-04 - 15s/epoch - 15ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 15s - loss: 12.8209 - mse: 12.8209 - mae: 1.5863 - val_loss: 24.6424 - val_mse: 24.6424 - val_mae: 1.6737 - lr: 2.7865e-04 - 15s/epoch - 15ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 15s - loss: 12.8450 - mse: 12.8450 - mae: 1.5907 - val_loss: 24.6336 - val_mse: 24.6336 - val_mae: 1.6892 - lr: 2.7865e-04 - 15s/epoch - 15ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 16s - loss: 12.7859 - mse: 12.7859 - mae: 1.5867 - val_loss: 25.1079 - val_mse: 25.1079 - val_mae: 1.5929 - lr: 2.7865e-04 - 16s/epoch - 16ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 16s - loss: 12.8026 - mse: 12.8026 - mae: 1.5838 - val_loss: 24.5519 - val_mse: 24.5519 - val_mae: 1.6252 - lr: 2.7865e-04 - 16s/epoch - 16ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 16s - loss: 12.8025 - mse: 12.8025 - mae: 1.5817 - val_loss: 24.6577 - val_mse: 24.6577 - val_mae: 1.7140 - lr: 2.7865e-04 - 16s/epoch - 16ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 16s - loss: 12.7775 - mse: 12.7775 - mae: 1.5894 - val_loss: 24.5002 - val_mse: 24.5002 - val_mae: 1.6412 - lr: 2.7865e-04 - 16s/epoch - 16ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 16s - loss: 12.7635 - mse: 12.7635 - mae: 1.5840 - val_loss: 24.5841 - val_mse: 24.5841 - val_mae: 1.6580 - lr: 2.7865e-04 - 16s/epoch - 16ms/step\n",
            "Epoch 13/100\n",
            "1000/1000 - 16s - loss: 12.7230 - mse: 12.7230 - mae: 1.5843 - val_loss: 24.4342 - val_mse: 24.4342 - val_mae: 1.6479 - lr: 2.7865e-04 - 16s/epoch - 16ms/step\n",
            "Epoch 14/100\n",
            "1000/1000 - 15s - loss: 12.7335 - mse: 12.7335 - mae: 1.5859 - val_loss: 24.5438 - val_mse: 24.5438 - val_mae: 1.5826 - lr: 2.7865e-04 - 15s/epoch - 15ms/step\n",
            "Epoch 15/100\n",
            "1000/1000 - 16s - loss: 12.7252 - mse: 12.7252 - mae: 1.5820 - val_loss: 24.8391 - val_mse: 24.8391 - val_mae: 1.6040 - lr: 2.7865e-04 - 16s/epoch - 16ms/step\n",
            "Epoch 16/100\n",
            "1000/1000 - 16s - loss: 12.7748 - mse: 12.7748 - mae: 1.5908 - val_loss: 24.7696 - val_mse: 24.7696 - val_mae: 1.6533 - lr: 2.7865e-04 - 16s/epoch - 16ms/step\n",
            "Epoch 17/100\n",
            "1000/1000 - 15s - loss: 12.7753 - mse: 12.7753 - mae: 1.5884 - val_loss: 24.7058 - val_mse: 24.7058 - val_mae: 1.6363 - lr: 2.7865e-04 - 15s/epoch - 15ms/step\n",
            "Epoch 18/100\n",
            "1000/1000 - 15s - loss: 12.7507 - mse: 12.7507 - mae: 1.5842 - val_loss: 24.7816 - val_mse: 24.7816 - val_mae: 1.6378 - lr: 2.7865e-04 - 15s/epoch - 15ms/step\n",
            "Score for fold 3: loss of 24.781545639038086\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 15s - loss: 15.5377 - mse: 15.5377 - mae: 1.5879 - val_loss: 13.4051 - val_mse: 13.4051 - val_mae: 1.5715 - lr: 2.7865e-04 - 15s/epoch - 15ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 16s - loss: 15.4929 - mse: 15.4929 - mae: 1.5874 - val_loss: 13.3186 - val_mse: 13.3186 - val_mae: 1.5951 - lr: 2.7865e-04 - 16s/epoch - 16ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 15s - loss: 15.5173 - mse: 15.5173 - mae: 1.5913 - val_loss: 13.1228 - val_mse: 13.1228 - val_mae: 1.6154 - lr: 2.7865e-04 - 15s/epoch - 15ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 16s - loss: 15.5083 - mse: 15.5083 - mae: 1.5843 - val_loss: 13.4657 - val_mse: 13.4657 - val_mae: 1.5803 - lr: 2.7865e-04 - 16s/epoch - 16ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 15s - loss: 15.5603 - mse: 15.5603 - mae: 1.5913 - val_loss: 13.1939 - val_mse: 13.1939 - val_mae: 1.5470 - lr: 2.7865e-04 - 15s/epoch - 15ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 15s - loss: 15.4980 - mse: 15.4980 - mae: 1.5884 - val_loss: 13.2060 - val_mse: 13.2060 - val_mae: 1.6002 - lr: 2.7865e-04 - 15s/epoch - 15ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 14s - loss: 15.5080 - mse: 15.5080 - mae: 1.5913 - val_loss: 13.3735 - val_mse: 13.3735 - val_mae: 1.5952 - lr: 2.7865e-04 - 14s/epoch - 14ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 14s - loss: 15.5106 - mse: 15.5106 - mae: 1.5876 - val_loss: 13.0800 - val_mse: 13.0800 - val_mae: 1.5850 - lr: 2.7865e-04 - 14s/epoch - 14ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 14s - loss: 15.5367 - mse: 15.5367 - mae: 1.5897 - val_loss: 13.7101 - val_mse: 13.7101 - val_mae: 1.5461 - lr: 2.7865e-04 - 14s/epoch - 14ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 14s - loss: 15.4518 - mse: 15.4518 - mae: 1.5858 - val_loss: 13.3428 - val_mse: 13.3428 - val_mae: 1.5550 - lr: 2.7865e-04 - 14s/epoch - 14ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 14s - loss: 15.5359 - mse: 15.5359 - mae: 1.5802 - val_loss: 13.1287 - val_mse: 13.1287 - val_mae: 1.6213 - lr: 2.7865e-04 - 14s/epoch - 14ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 14s - loss: 15.4872 - mse: 15.4872 - mae: 1.5808 - val_loss: 13.1807 - val_mse: 13.1807 - val_mae: 1.6266 - lr: 2.7865e-04 - 14s/epoch - 14ms/step\n",
            "Epoch 13/100\n",
            "1000/1000 - 14s - loss: 15.4832 - mse: 15.4832 - mae: 1.5833 - val_loss: 13.0385 - val_mse: 13.0385 - val_mae: 1.6674 - lr: 2.7865e-04 - 14s/epoch - 14ms/step\n",
            "Epoch 14/100\n",
            "1000/1000 - 14s - loss: 15.4666 - mse: 15.4666 - mae: 1.5826 - val_loss: 13.3912 - val_mse: 13.3912 - val_mae: 1.4971 - lr: 2.7865e-04 - 14s/epoch - 14ms/step\n",
            "Epoch 15/100\n",
            "1000/1000 - 14s - loss: 15.4043 - mse: 15.4043 - mae: 1.5851 - val_loss: 13.1532 - val_mse: 13.1532 - val_mae: 1.6175 - lr: 2.7865e-04 - 14s/epoch - 14ms/step\n",
            "Epoch 16/100\n",
            "1000/1000 - 14s - loss: 15.3723 - mse: 15.3723 - mae: 1.5813 - val_loss: 13.1263 - val_mse: 13.1263 - val_mae: 1.5625 - lr: 2.7865e-04 - 14s/epoch - 14ms/step\n",
            "Epoch 17/100\n",
            "1000/1000 - 14s - loss: 15.4930 - mse: 15.4930 - mae: 1.5824 - val_loss: 13.4709 - val_mse: 13.4709 - val_mae: 1.5307 - lr: 2.7865e-04 - 14s/epoch - 14ms/step\n",
            "Epoch 18/100\n",
            "1000/1000 - 14s - loss: 15.3844 - mse: 15.3844 - mae: 1.5796 - val_loss: 13.4038 - val_mse: 13.4038 - val_mae: 1.5707 - lr: 2.7865e-04 - 14s/epoch - 14ms/step\n",
            "Score for fold 4: loss of 13.403807640075684\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 13s - loss: 15.9794 - mse: 15.9794 - mae: 1.5953 - val_loss: 10.8570 - val_mse: 10.8570 - val_mae: 1.5029 - lr: 2.7865e-04 - 13s/epoch - 13ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 14s - loss: 16.0047 - mse: 16.0047 - mae: 1.5866 - val_loss: 10.8743 - val_mse: 10.8743 - val_mae: 1.5481 - lr: 2.7865e-04 - 14s/epoch - 14ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 14s - loss: 15.9408 - mse: 15.9408 - mae: 1.5931 - val_loss: 10.7297 - val_mse: 10.7297 - val_mae: 1.5553 - lr: 2.7865e-04 - 14s/epoch - 14ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 14s - loss: 15.9187 - mse: 15.9187 - mae: 1.5982 - val_loss: 10.8381 - val_mse: 10.8381 - val_mae: 1.5298 - lr: 2.7865e-04 - 14s/epoch - 14ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 14s - loss: 15.8931 - mse: 15.8931 - mae: 1.5928 - val_loss: 10.8009 - val_mse: 10.8009 - val_mae: 1.5705 - lr: 2.7865e-04 - 14s/epoch - 14ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 13s - loss: 15.9523 - mse: 15.9523 - mae: 1.5898 - val_loss: 10.8553 - val_mse: 10.8553 - val_mae: 1.6024 - lr: 2.7865e-04 - 13s/epoch - 13ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 13s - loss: 15.9663 - mse: 15.9663 - mae: 1.5915 - val_loss: 10.8746 - val_mse: 10.8746 - val_mae: 1.5238 - lr: 2.7865e-04 - 13s/epoch - 13ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 13s - loss: 15.9408 - mse: 15.9408 - mae: 1.5918 - val_loss: 10.7703 - val_mse: 10.7703 - val_mae: 1.5559 - lr: 2.7865e-04 - 13s/epoch - 13ms/step\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-10-16 09:08:59,868]\u001b[0m Finished trial#45 resulted in value: 15.178. Current best value is 14.024000000000001 with parameters: {'activation': 'relu', 'num_hidden_layer': 4, 'i': 9, 'learning_rate': 0.00018064149922822524}.\u001b[0m\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Score for fold 5: loss of 10.77028751373291\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 7s - loss: 16.0167 - mse: 16.0167 - mae: 1.6304 - val_loss: 13.0100 - val_mse: 13.0100 - val_mae: 1.6707 - lr: 8.4704e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 6s - loss: 15.6815 - mse: 15.6815 - mae: 1.6115 - val_loss: 13.5334 - val_mse: 13.5334 - val_mae: 1.5601 - lr: 8.4704e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 6s - loss: 15.5599 - mse: 15.5599 - mae: 1.5956 - val_loss: 13.1133 - val_mse: 13.1133 - val_mae: 1.5569 - lr: 8.4704e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 6s - loss: 15.5933 - mse: 15.5933 - mae: 1.5922 - val_loss: 12.9312 - val_mse: 12.9312 - val_mae: 1.5786 - lr: 8.4704e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 6s - loss: 15.5704 - mse: 15.5704 - mae: 1.5894 - val_loss: 12.9599 - val_mse: 12.9599 - val_mae: 1.5573 - lr: 8.4704e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 6s - loss: 15.4624 - mse: 15.4624 - mae: 1.5898 - val_loss: 12.8242 - val_mse: 12.8242 - val_mae: 1.5676 - lr: 8.4704e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 6s - loss: 15.4577 - mse: 15.4577 - mae: 1.5907 - val_loss: 13.2082 - val_mse: 13.2082 - val_mae: 1.6655 - lr: 8.4704e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 6s - loss: 15.4172 - mse: 15.4172 - mae: 1.5901 - val_loss: 13.1529 - val_mse: 13.1529 - val_mae: 1.5485 - lr: 8.4704e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 6s - loss: 15.4567 - mse: 15.4567 - mae: 1.5877 - val_loss: 12.8762 - val_mse: 12.8762 - val_mae: 1.5378 - lr: 8.4704e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 6s - loss: 15.4374 - mse: 15.4374 - mae: 1.5941 - val_loss: 13.3933 - val_mse: 13.3933 - val_mae: 1.5108 - lr: 8.4704e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 7s - loss: 15.4858 - mse: 15.4858 - mae: 1.5836 - val_loss: 12.9157 - val_mse: 12.9157 - val_mae: 1.7062 - lr: 8.4704e-04 - 7s/epoch - 7ms/step\n",
            "Score for fold 1: loss of 12.915684700012207\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 6s - loss: 15.1738 - mse: 15.1738 - mae: 1.5878 - val_loss: 14.2233 - val_mse: 14.2233 - val_mae: 1.5939 - lr: 8.4704e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 6s - loss: 15.1645 - mse: 15.1645 - mae: 1.5870 - val_loss: 14.5560 - val_mse: 14.5560 - val_mae: 1.5577 - lr: 8.4704e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 6s - loss: 15.0804 - mse: 15.0804 - mae: 1.5837 - val_loss: 14.3147 - val_mse: 14.3147 - val_mae: 1.5402 - lr: 8.4704e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 6s - loss: 15.0738 - mse: 15.0738 - mae: 1.5889 - val_loss: 14.7042 - val_mse: 14.7042 - val_mae: 1.5745 - lr: 8.4704e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 6s - loss: 14.9974 - mse: 14.9974 - mae: 1.5892 - val_loss: 14.6784 - val_mse: 14.6784 - val_mae: 1.7848 - lr: 8.4704e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 6s - loss: 14.9471 - mse: 14.9471 - mae: 1.5830 - val_loss: 14.4095 - val_mse: 14.4095 - val_mae: 1.6660 - lr: 8.4704e-04 - 6s/epoch - 6ms/step\n",
            "Score for fold 2: loss of 14.409497261047363\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 6s - loss: 13.3979 - mse: 13.3979 - mae: 1.5818 - val_loss: 21.4359 - val_mse: 21.4359 - val_mae: 1.5548 - lr: 8.4704e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 6s - loss: 13.2115 - mse: 13.2115 - mae: 1.5822 - val_loss: 21.1188 - val_mse: 21.1188 - val_mae: 1.6078 - lr: 8.4704e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 6s - loss: 13.3566 - mse: 13.3566 - mae: 1.5859 - val_loss: 21.2393 - val_mse: 21.2393 - val_mae: 1.6140 - lr: 8.4704e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 6s - loss: 13.3105 - mse: 13.3105 - mae: 1.5945 - val_loss: 21.4002 - val_mse: 21.4002 - val_mae: 1.9461 - lr: 8.4704e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 6s - loss: 13.3042 - mse: 13.3042 - mae: 1.5766 - val_loss: 21.0565 - val_mse: 21.0565 - val_mae: 1.5816 - lr: 8.4704e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 6s - loss: 13.3000 - mse: 13.3000 - mae: 1.5831 - val_loss: 21.1107 - val_mse: 21.1107 - val_mae: 1.6596 - lr: 8.4704e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 6s - loss: 13.1301 - mse: 13.1301 - mae: 1.5739 - val_loss: 20.9178 - val_mse: 20.9178 - val_mae: 1.7175 - lr: 8.4704e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 6s - loss: 13.1913 - mse: 13.1913 - mae: 1.5794 - val_loss: 21.0815 - val_mse: 21.0815 - val_mae: 1.7633 - lr: 8.4704e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 6s - loss: 13.2929 - mse: 13.2929 - mae: 1.5861 - val_loss: 21.2932 - val_mse: 21.2932 - val_mae: 1.5387 - lr: 8.4704e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 6s - loss: 13.2453 - mse: 13.2453 - mae: 1.5726 - val_loss: 20.9385 - val_mse: 20.9385 - val_mae: 1.5498 - lr: 8.4704e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 6s - loss: 13.1751 - mse: 13.1751 - mae: 1.5806 - val_loss: 21.1965 - val_mse: 21.1965 - val_mae: 1.5865 - lr: 8.4704e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 6s - loss: 13.2459 - mse: 13.2459 - mae: 1.5772 - val_loss: 21.1949 - val_mse: 21.1949 - val_mae: 1.6278 - lr: 8.4704e-04 - 6s/epoch - 6ms/step\n",
            "Score for fold 3: loss of 21.19487190246582\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 6s - loss: 15.5658 - mse: 15.5658 - mae: 1.6031 - val_loss: 11.7439 - val_mse: 11.7439 - val_mae: 1.6796 - lr: 8.4704e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 6s - loss: 15.5812 - mse: 15.5812 - mae: 1.6217 - val_loss: 12.1118 - val_mse: 12.1118 - val_mae: 1.4559 - lr: 8.4704e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 6s - loss: 15.4723 - mse: 15.4723 - mae: 1.6215 - val_loss: 12.3459 - val_mse: 12.3459 - val_mae: 1.6055 - lr: 8.4704e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 6s - loss: 15.5940 - mse: 15.5940 - mae: 1.6462 - val_loss: 12.3875 - val_mse: 12.3875 - val_mae: 1.9180 - lr: 8.4704e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 6s - loss: 15.5498 - mse: 15.5498 - mae: 1.6425 - val_loss: 11.7329 - val_mse: 11.7329 - val_mae: 1.6736 - lr: 8.4704e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 6s - loss: 15.4626 - mse: 15.4626 - mae: 1.6618 - val_loss: 11.7705 - val_mse: 11.7705 - val_mae: 1.5838 - lr: 8.4704e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 6s - loss: 15.4696 - mse: 15.4696 - mae: 1.6648 - val_loss: 12.4943 - val_mse: 12.4943 - val_mae: 1.6014 - lr: 8.4704e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 6s - loss: 15.2570 - mse: 15.2570 - mae: 1.6684 - val_loss: 11.8378 - val_mse: 11.8378 - val_mae: 1.7599 - lr: 8.4704e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 6s - loss: 15.3567 - mse: 15.3567 - mae: 1.6487 - val_loss: 11.9364 - val_mse: 11.9364 - val_mae: 1.6070 - lr: 8.4704e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 6s - loss: 15.4580 - mse: 15.4580 - mae: 1.6750 - val_loss: 12.1895 - val_mse: 12.1895 - val_mae: 1.4841 - lr: 8.4704e-04 - 6s/epoch - 6ms/step\n",
            "Score for fold 4: loss of 12.189532279968262\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 6s - loss: 15.0083 - mse: 15.0083 - mae: 1.7005 - val_loss: 14.6682 - val_mse: 14.6682 - val_mae: 1.9227 - lr: 8.4704e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 6s - loss: 14.6399 - mse: 14.6399 - mae: 1.6763 - val_loss: 15.0467 - val_mse: 15.0467 - val_mae: 1.5044 - lr: 8.4704e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 6s - loss: 14.6267 - mse: 14.6267 - mae: 1.6699 - val_loss: 14.2547 - val_mse: 14.2547 - val_mae: 1.5428 - lr: 8.4704e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 6s - loss: 14.8910 - mse: 14.8910 - mae: 1.6849 - val_loss: 14.4002 - val_mse: 14.4002 - val_mae: 1.6208 - lr: 8.4704e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 6s - loss: 15.1186 - mse: 15.1186 - mae: 1.7203 - val_loss: 14.2444 - val_mse: 14.2444 - val_mae: 1.5209 - lr: 8.4704e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 6s - loss: 14.8508 - mse: 14.8508 - mae: 1.6787 - val_loss: 14.2411 - val_mse: 14.2411 - val_mae: 1.4790 - lr: 8.4704e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 6s - loss: 14.8205 - mse: 14.8205 - mae: 1.6860 - val_loss: 14.8557 - val_mse: 14.8557 - val_mae: 1.4645 - lr: 8.4704e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 6s - loss: 14.8895 - mse: 14.8895 - mae: 1.7188 - val_loss: 14.5559 - val_mse: 14.5559 - val_mae: 1.4413 - lr: 8.4704e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 6s - loss: 14.8504 - mse: 14.8504 - mae: 1.7148 - val_loss: 14.3526 - val_mse: 14.3526 - val_mae: 1.6299 - lr: 8.4704e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 6s - loss: 14.8493 - mse: 14.8493 - mae: 1.7185 - val_loss: 14.5146 - val_mse: 14.5146 - val_mae: 1.7436 - lr: 8.4704e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 6s - loss: 14.9405 - mse: 14.9405 - mae: 1.7194 - val_loss: 14.6617 - val_mse: 14.6617 - val_mae: 1.7872 - lr: 8.4704e-04 - 6s/epoch - 6ms/step\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-10-16 09:14:19,745]\u001b[0m Finished trial#46 resulted in value: 15.073999999999998. Current best value is 14.024000000000001 with parameters: {'activation': 'relu', 'num_hidden_layer': 4, 'i': 9, 'learning_rate': 0.00018064149922822524}.\u001b[0m\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Score for fold 5: loss of 14.661703109741211\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 7s - loss: 15.4517 - mse: 15.4517 - mae: 1.6139 - val_loss: 14.8450 - val_mse: 14.8450 - val_mae: 1.6446 - lr: 5.0175e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 6s - loss: 15.2096 - mse: 15.2096 - mae: 1.5966 - val_loss: 15.0909 - val_mse: 15.0909 - val_mae: 1.5442 - lr: 5.0175e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 6s - loss: 15.1053 - mse: 15.1053 - mae: 1.5877 - val_loss: 14.9043 - val_mse: 14.9043 - val_mae: 1.5854 - lr: 5.0175e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 6s - loss: 15.0540 - mse: 15.0540 - mae: 1.5882 - val_loss: 14.6056 - val_mse: 14.6056 - val_mae: 1.5894 - lr: 5.0175e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 6s - loss: 15.0386 - mse: 15.0386 - mae: 1.5843 - val_loss: 14.7986 - val_mse: 14.7986 - val_mae: 1.5824 - lr: 5.0175e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 6s - loss: 15.0085 - mse: 15.0085 - mae: 1.5792 - val_loss: 14.6652 - val_mse: 14.6652 - val_mae: 1.5661 - lr: 5.0175e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 6s - loss: 14.8966 - mse: 14.8966 - mae: 1.5792 - val_loss: 14.8055 - val_mse: 14.8055 - val_mae: 1.6060 - lr: 5.0175e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 6s - loss: 14.8977 - mse: 14.8977 - mae: 1.5852 - val_loss: 14.7886 - val_mse: 14.7886 - val_mae: 1.5677 - lr: 5.0175e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 6s - loss: 14.8492 - mse: 14.8492 - mae: 1.5786 - val_loss: 14.2649 - val_mse: 14.2649 - val_mae: 1.6040 - lr: 5.0175e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 6s - loss: 14.7361 - mse: 14.7361 - mae: 1.5786 - val_loss: 14.8356 - val_mse: 14.8356 - val_mae: 1.6263 - lr: 5.0175e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 6s - loss: 14.8003 - mse: 14.8003 - mae: 1.5817 - val_loss: 14.5665 - val_mse: 14.5665 - val_mae: 1.6155 - lr: 5.0175e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 7s - loss: 14.7259 - mse: 14.7259 - mae: 1.5800 - val_loss: 14.7132 - val_mse: 14.7132 - val_mae: 1.6267 - lr: 5.0175e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 13/100\n",
            "1000/1000 - 7s - loss: 14.7254 - mse: 14.7254 - mae: 1.5783 - val_loss: 14.9383 - val_mse: 14.9383 - val_mae: 1.6182 - lr: 5.0175e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 14/100\n",
            "1000/1000 - 7s - loss: 14.6970 - mse: 14.6970 - mae: 1.5709 - val_loss: 14.5323 - val_mse: 14.5323 - val_mae: 1.6012 - lr: 5.0175e-04 - 7s/epoch - 7ms/step\n",
            "Score for fold 1: loss of 14.532280921936035\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 7s - loss: 15.2972 - mse: 15.2972 - mae: 1.5920 - val_loss: 12.6973 - val_mse: 12.6973 - val_mae: 1.4706 - lr: 5.0175e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 6s - loss: 15.1746 - mse: 15.1746 - mae: 1.5833 - val_loss: 12.6065 - val_mse: 12.6065 - val_mae: 1.5750 - lr: 5.0175e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 6s - loss: 15.3299 - mse: 15.3299 - mae: 1.5920 - val_loss: 12.1329 - val_mse: 12.1329 - val_mae: 1.5317 - lr: 5.0175e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 7s - loss: 15.2441 - mse: 15.2441 - mae: 1.5812 - val_loss: 12.3786 - val_mse: 12.3786 - val_mae: 1.5328 - lr: 5.0175e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 6s - loss: 15.3546 - mse: 15.3546 - mae: 1.5875 - val_loss: 12.8142 - val_mse: 12.8142 - val_mae: 1.6146 - lr: 5.0175e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 6s - loss: 15.2140 - mse: 15.2140 - mae: 1.5959 - val_loss: 12.6494 - val_mse: 12.6494 - val_mae: 1.5256 - lr: 5.0175e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 6s - loss: 15.1002 - mse: 15.1002 - mae: 1.5959 - val_loss: 12.5756 - val_mse: 12.5756 - val_mae: 1.6166 - lr: 5.0175e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 6s - loss: 15.1465 - mse: 15.1465 - mae: 1.5904 - val_loss: 12.3246 - val_mse: 12.3246 - val_mae: 1.6703 - lr: 5.0175e-04 - 6s/epoch - 6ms/step\n",
            "Score for fold 2: loss of 12.324633598327637\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 6s - loss: 15.5592 - mse: 15.5592 - mae: 1.5876 - val_loss: 10.9794 - val_mse: 10.9794 - val_mae: 1.6917 - lr: 5.0175e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 6s - loss: 15.4406 - mse: 15.4406 - mae: 1.5820 - val_loss: 11.0713 - val_mse: 11.0713 - val_mae: 1.5719 - lr: 5.0175e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 6s - loss: 15.3332 - mse: 15.3332 - mae: 1.5846 - val_loss: 11.1458 - val_mse: 11.1458 - val_mae: 1.6052 - lr: 5.0175e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 7s - loss: 15.2582 - mse: 15.2582 - mae: 1.5839 - val_loss: 11.1994 - val_mse: 11.1994 - val_mae: 1.7896 - lr: 5.0175e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 7s - loss: 15.3011 - mse: 15.3011 - mae: 1.5948 - val_loss: 11.3726 - val_mse: 11.3726 - val_mae: 1.7693 - lr: 5.0175e-04 - 7s/epoch - 7ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 7s - loss: 15.0819 - mse: 15.0819 - mae: 1.5828 - val_loss: 11.2721 - val_mse: 11.2721 - val_mae: 1.5446 - lr: 5.0175e-04 - 7s/epoch - 7ms/step\n",
            "Score for fold 3: loss of 11.27211856842041\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 6s - loss: 15.4872 - mse: 15.4872 - mae: 1.6035 - val_loss: 10.8141 - val_mse: 10.8141 - val_mae: 1.6409 - lr: 5.0175e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 6s - loss: 15.1594 - mse: 15.1594 - mae: 1.6016 - val_loss: 11.0432 - val_mse: 11.0432 - val_mae: 1.7067 - lr: 5.0175e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 6s - loss: 15.3165 - mse: 15.3165 - mae: 1.5963 - val_loss: 10.8574 - val_mse: 10.8574 - val_mae: 1.4926 - lr: 5.0175e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 6s - loss: 15.2305 - mse: 15.2305 - mae: 1.6019 - val_loss: 11.6413 - val_mse: 11.6413 - val_mae: 1.4275 - lr: 5.0175e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 6s - loss: 15.1697 - mse: 15.1697 - mae: 1.6100 - val_loss: 11.3952 - val_mse: 11.3952 - val_mae: 1.8917 - lr: 5.0175e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 6s - loss: 15.0614 - mse: 15.0614 - mae: 1.6030 - val_loss: 11.1348 - val_mse: 11.1348 - val_mae: 1.5088 - lr: 5.0175e-04 - 6s/epoch - 6ms/step\n",
            "Score for fold 4: loss of 11.134769439697266\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 6s - loss: 11.9249 - mse: 11.9249 - mae: 1.5942 - val_loss: 24.0779 - val_mse: 24.0779 - val_mae: 1.5476 - lr: 5.0175e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 6s - loss: 12.0057 - mse: 12.0057 - mae: 1.5956 - val_loss: 23.7170 - val_mse: 23.7170 - val_mae: 1.7062 - lr: 5.0175e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 6s - loss: 11.9336 - mse: 11.9336 - mae: 1.5991 - val_loss: 23.8300 - val_mse: 23.8300 - val_mae: 1.6329 - lr: 5.0175e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 6s - loss: 11.9443 - mse: 11.9443 - mae: 1.6044 - val_loss: 24.0607 - val_mse: 24.0607 - val_mae: 1.6117 - lr: 5.0175e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 6s - loss: 11.6882 - mse: 11.6882 - mae: 1.6101 - val_loss: 24.5739 - val_mse: 24.5739 - val_mae: 1.5737 - lr: 5.0175e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 6s - loss: 11.8387 - mse: 11.8387 - mae: 1.6220 - val_loss: 23.8869 - val_mse: 23.8869 - val_mae: 1.6916 - lr: 5.0175e-04 - 6s/epoch - 6ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 7s - loss: 11.8735 - mse: 11.8735 - mae: 1.6189 - val_loss: 26.3962 - val_mse: 26.3962 - val_mae: 2.3941 - lr: 5.0175e-04 - 7s/epoch - 7ms/step\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-10-16 09:18:43,041]\u001b[0m Finished trial#47 resulted in value: 15.13. Current best value is 14.024000000000001 with parameters: {'activation': 'relu', 'num_hidden_layer': 4, 'i': 9, 'learning_rate': 0.00018064149922822524}.\u001b[0m\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Score for fold 5: loss of 26.396228790283203\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 14s - loss: 15.6853 - mse: 15.6853 - mae: 1.6129 - val_loss: 14.3814 - val_mse: 14.3814 - val_mae: 1.6074 - lr: 1.1509e-04 - 14s/epoch - 14ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 14s - loss: 15.2343 - mse: 15.2343 - mae: 1.5848 - val_loss: 14.5889 - val_mse: 14.5889 - val_mae: 1.5176 - lr: 1.1509e-04 - 14s/epoch - 14ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 14s - loss: 15.2299 - mse: 15.2299 - mae: 1.5813 - val_loss: 14.4410 - val_mse: 14.4410 - val_mae: 1.6224 - lr: 1.1509e-04 - 14s/epoch - 14ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 14s - loss: 15.1717 - mse: 15.1717 - mae: 1.5817 - val_loss: 14.6073 - val_mse: 14.6073 - val_mae: 1.5535 - lr: 1.1509e-04 - 14s/epoch - 14ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 14s - loss: 15.0977 - mse: 15.0977 - mae: 1.5815 - val_loss: 14.3323 - val_mse: 14.3323 - val_mae: 1.6524 - lr: 1.1509e-04 - 14s/epoch - 14ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 14s - loss: 15.0643 - mse: 15.0643 - mae: 1.5797 - val_loss: 14.0485 - val_mse: 14.0485 - val_mae: 1.6220 - lr: 1.1509e-04 - 14s/epoch - 14ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 13s - loss: 15.0333 - mse: 15.0333 - mae: 1.5770 - val_loss: 14.2984 - val_mse: 14.2984 - val_mae: 1.5788 - lr: 1.1509e-04 - 13s/epoch - 13ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 13s - loss: 14.9835 - mse: 14.9835 - mae: 1.5746 - val_loss: 14.6584 - val_mse: 14.6584 - val_mae: 1.6227 - lr: 1.1509e-04 - 13s/epoch - 13ms/step\n",
            "Epoch 9/100\n",
            "1000/1000 - 13s - loss: 14.8809 - mse: 14.8809 - mae: 1.5665 - val_loss: 14.0352 - val_mse: 14.0352 - val_mae: 1.6336 - lr: 1.1509e-04 - 13s/epoch - 13ms/step\n",
            "Epoch 10/100\n",
            "1000/1000 - 13s - loss: 14.8731 - mse: 14.8731 - mae: 1.5676 - val_loss: 14.0489 - val_mse: 14.0489 - val_mae: 1.6306 - lr: 1.1509e-04 - 13s/epoch - 13ms/step\n",
            "Epoch 11/100\n",
            "1000/1000 - 14s - loss: 14.8471 - mse: 14.8471 - mae: 1.5660 - val_loss: 14.1657 - val_mse: 14.1657 - val_mae: 1.5817 - lr: 1.1509e-04 - 14s/epoch - 14ms/step\n",
            "Epoch 12/100\n",
            "1000/1000 - 13s - loss: 14.8054 - mse: 14.8054 - mae: 1.5631 - val_loss: 14.2181 - val_mse: 14.2181 - val_mae: 1.5438 - lr: 1.1509e-04 - 13s/epoch - 13ms/step\n",
            "Epoch 13/100\n",
            "1000/1000 - 13s - loss: 14.7067 - mse: 14.7067 - mae: 1.5550 - val_loss: 14.2985 - val_mse: 14.2985 - val_mae: 1.5823 - lr: 1.1509e-04 - 13s/epoch - 13ms/step\n",
            "Epoch 14/100\n",
            "1000/1000 - 13s - loss: 14.7015 - mse: 14.7015 - mae: 1.5599 - val_loss: 14.5704 - val_mse: 14.5704 - val_mae: 1.6449 - lr: 1.1509e-04 - 13s/epoch - 13ms/step\n",
            "Score for fold 1: loss of 14.570361137390137\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 13s - loss: 15.2917 - mse: 15.2917 - mae: 1.5692 - val_loss: 11.6522 - val_mse: 11.6522 - val_mae: 1.5085 - lr: 1.1509e-04 - 13s/epoch - 13ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 13s - loss: 15.1842 - mse: 15.1842 - mae: 1.5610 - val_loss: 11.5746 - val_mse: 11.5746 - val_mae: 1.5848 - lr: 1.1509e-04 - 13s/epoch - 13ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 14s - loss: 15.0676 - mse: 15.0676 - mae: 1.5608 - val_loss: 11.6699 - val_mse: 11.6699 - val_mae: 1.5505 - lr: 1.1509e-04 - 14s/epoch - 14ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 14s - loss: 15.0423 - mse: 15.0423 - mae: 1.5541 - val_loss: 11.7439 - val_mse: 11.7439 - val_mae: 1.5680 - lr: 1.1509e-04 - 14s/epoch - 14ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 13s - loss: 14.8284 - mse: 14.8284 - mae: 1.5595 - val_loss: 11.7406 - val_mse: 11.7406 - val_mae: 1.5957 - lr: 1.1509e-04 - 13s/epoch - 13ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 14s - loss: 14.8035 - mse: 14.8035 - mae: 1.5438 - val_loss: 11.7220 - val_mse: 11.7220 - val_mae: 1.6435 - lr: 1.1509e-04 - 14s/epoch - 14ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 13s - loss: 14.6458 - mse: 14.6458 - mae: 1.5464 - val_loss: 11.8167 - val_mse: 11.8167 - val_mae: 1.5256 - lr: 1.1509e-04 - 13s/epoch - 13ms/step\n",
            "Score for fold 2: loss of 11.816694259643555\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 13s - loss: 13.3161 - mse: 13.3161 - mae: 1.5597 - val_loss: 17.1615 - val_mse: 17.1615 - val_mae: 1.6125 - lr: 1.1509e-04 - 13s/epoch - 13ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 13s - loss: 13.1870 - mse: 13.1870 - mae: 1.5576 - val_loss: 17.4346 - val_mse: 17.4346 - val_mae: 1.4872 - lr: 1.1509e-04 - 13s/epoch - 13ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 13s - loss: 13.1036 - mse: 13.1036 - mae: 1.5492 - val_loss: 17.4911 - val_mse: 17.4911 - val_mae: 1.4963 - lr: 1.1509e-04 - 13s/epoch - 13ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 13s - loss: 12.9356 - mse: 12.9356 - mae: 1.5498 - val_loss: 17.5570 - val_mse: 17.5570 - val_mae: 1.4427 - lr: 1.1509e-04 - 13s/epoch - 13ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 13s - loss: 12.9248 - mse: 12.9248 - mae: 1.5424 - val_loss: 17.6630 - val_mse: 17.6630 - val_mae: 1.7756 - lr: 1.1509e-04 - 13s/epoch - 13ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 13s - loss: 12.8824 - mse: 12.8824 - mae: 1.5409 - val_loss: 17.7355 - val_mse: 17.7355 - val_mae: 1.7385 - lr: 1.1509e-04 - 13s/epoch - 13ms/step\n",
            "Score for fold 3: loss of 17.735504150390625\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 13s - loss: 14.5888 - mse: 14.5888 - mae: 1.5578 - val_loss: 10.7887 - val_mse: 10.7887 - val_mae: 1.4897 - lr: 1.1509e-04 - 13s/epoch - 13ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 14s - loss: 14.4443 - mse: 14.4443 - mae: 1.5533 - val_loss: 10.8387 - val_mse: 10.8387 - val_mae: 1.4756 - lr: 1.1509e-04 - 14s/epoch - 14ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 13s - loss: 14.2274 - mse: 14.2274 - mae: 1.5381 - val_loss: 10.6900 - val_mse: 10.6900 - val_mae: 1.6103 - lr: 1.1509e-04 - 13s/epoch - 13ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 13s - loss: 14.3099 - mse: 14.3099 - mae: 1.5387 - val_loss: 11.4096 - val_mse: 11.4096 - val_mae: 1.4465 - lr: 1.1509e-04 - 13s/epoch - 13ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 13s - loss: 14.1156 - mse: 14.1156 - mae: 1.5444 - val_loss: 11.2809 - val_mse: 11.2809 - val_mae: 1.6136 - lr: 1.1509e-04 - 13s/epoch - 13ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 14s - loss: 14.1565 - mse: 14.1565 - mae: 1.5511 - val_loss: 10.7907 - val_mse: 10.7907 - val_mae: 1.5466 - lr: 1.1509e-04 - 14s/epoch - 14ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 14s - loss: 14.0486 - mse: 14.0486 - mae: 1.5613 - val_loss: 10.9457 - val_mse: 10.9457 - val_mae: 1.4712 - lr: 1.1509e-04 - 14s/epoch - 14ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 13s - loss: 13.9145 - mse: 13.9145 - mae: 1.5482 - val_loss: 11.2199 - val_mse: 11.2199 - val_mae: 1.5027 - lr: 1.1509e-04 - 13s/epoch - 13ms/step\n",
            "Score for fold 4: loss of 11.219907760620117\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 13s - loss: 12.8388 - mse: 12.8388 - mae: 1.5811 - val_loss: 16.2085 - val_mse: 16.2085 - val_mae: 1.9350 - lr: 1.1509e-04 - 13s/epoch - 13ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 13s - loss: 12.7956 - mse: 12.7956 - mae: 1.5957 - val_loss: 16.5954 - val_mse: 16.5954 - val_mae: 1.5534 - lr: 1.1509e-04 - 13s/epoch - 13ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 13s - loss: 12.6646 - mse: 12.6646 - mae: 1.5778 - val_loss: 16.1163 - val_mse: 16.1163 - val_mae: 1.7610 - lr: 1.1509e-04 - 13s/epoch - 13ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 13s - loss: 12.5713 - mse: 12.5713 - mae: 1.5733 - val_loss: 16.6703 - val_mse: 16.6703 - val_mae: 1.7813 - lr: 1.1509e-04 - 13s/epoch - 13ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 13s - loss: 12.5952 - mse: 12.5952 - mae: 1.5987 - val_loss: 16.4376 - val_mse: 16.4376 - val_mae: 1.7021 - lr: 1.1509e-04 - 13s/epoch - 13ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 14s - loss: 12.3556 - mse: 12.3556 - mae: 1.5838 - val_loss: 16.2881 - val_mse: 16.2881 - val_mae: 1.5582 - lr: 1.1509e-04 - 14s/epoch - 14ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 13s - loss: 12.4802 - mse: 12.4802 - mae: 1.6084 - val_loss: 16.2971 - val_mse: 16.2971 - val_mae: 1.5191 - lr: 1.1509e-04 - 13s/epoch - 13ms/step\n",
            "Epoch 8/100\n",
            "1000/1000 - 13s - loss: 12.2892 - mse: 12.2892 - mae: 1.5982 - val_loss: 16.3519 - val_mse: 16.3519 - val_mae: 1.6506 - lr: 1.1509e-04 - 13s/epoch - 13ms/step\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-10-16 09:28:27,809]\u001b[0m Finished trial#48 resulted in value: 14.339999999999998. Current best value is 14.024000000000001 with parameters: {'activation': 'relu', 'num_hidden_layer': 4, 'i': 9, 'learning_rate': 0.00018064149922822524}.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Score for fold 5: loss of 16.35189437866211\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 13s - loss: 15.6273 - mse: 15.6273 - mae: 1.6136 - val_loss: 14.3676 - val_mse: 14.3676 - val_mae: 1.5873 - lr: 1.1676e-04 - 13s/epoch - 13ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 12s - loss: 15.2608 - mse: 15.2608 - mae: 1.5912 - val_loss: 14.5282 - val_mse: 14.5282 - val_mae: 1.5400 - lr: 1.1676e-04 - 12s/epoch - 12ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 13s - loss: 15.2119 - mse: 15.2119 - mae: 1.5899 - val_loss: 14.5358 - val_mse: 14.5358 - val_mae: 1.5504 - lr: 1.1676e-04 - 13s/epoch - 13ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 13s - loss: 15.1697 - mse: 15.1697 - mae: 1.5871 - val_loss: 14.7235 - val_mse: 14.7235 - val_mae: 1.5360 - lr: 1.1676e-04 - 13s/epoch - 13ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 13s - loss: 15.0745 - mse: 15.0745 - mae: 1.5871 - val_loss: 14.4831 - val_mse: 14.4831 - val_mae: 1.5661 - lr: 1.1676e-04 - 13s/epoch - 13ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 12s - loss: 15.0430 - mse: 15.0430 - mae: 1.5805 - val_loss: 14.4386 - val_mse: 14.4386 - val_mae: 1.5780 - lr: 1.1676e-04 - 12s/epoch - 12ms/step\n",
            "Score for fold 1: loss of 14.438610076904297\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 12s - loss: 13.9483 - mse: 13.9483 - mae: 1.5844 - val_loss: 18.6547 - val_mse: 18.6547 - val_mae: 1.5565 - lr: 1.1676e-04 - 12s/epoch - 12ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 13s - loss: 13.8432 - mse: 13.8432 - mae: 1.5802 - val_loss: 18.6394 - val_mse: 18.6394 - val_mae: 1.6136 - lr: 1.1676e-04 - 13s/epoch - 13ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 13s - loss: 13.7557 - mse: 13.7557 - mae: 1.5760 - val_loss: 18.8639 - val_mse: 18.8639 - val_mae: 1.5204 - lr: 1.1676e-04 - 13s/epoch - 13ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 12s - loss: 13.8098 - mse: 13.8098 - mae: 1.5786 - val_loss: 18.9050 - val_mse: 18.9050 - val_mae: 1.4885 - lr: 1.1676e-04 - 12s/epoch - 12ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 13s - loss: 13.7328 - mse: 13.7328 - mae: 1.5793 - val_loss: 18.7960 - val_mse: 18.7960 - val_mae: 1.5495 - lr: 1.1676e-04 - 13s/epoch - 13ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 13s - loss: 13.7228 - mse: 13.7228 - mae: 1.5740 - val_loss: 18.7353 - val_mse: 18.7353 - val_mae: 1.5800 - lr: 1.1676e-04 - 13s/epoch - 13ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 13s - loss: 13.5486 - mse: 13.5486 - mae: 1.5722 - val_loss: 18.6918 - val_mse: 18.6918 - val_mae: 1.6372 - lr: 1.1676e-04 - 13s/epoch - 13ms/step\n",
            "Score for fold 2: loss of 18.691837310791016\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 12s - loss: 14.4644 - mse: 14.4644 - mae: 1.5629 - val_loss: 15.1158 - val_mse: 15.1158 - val_mae: 1.6524 - lr: 1.1676e-04 - 12s/epoch - 12ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 12s - loss: 14.3907 - mse: 14.3907 - mae: 1.5633 - val_loss: 15.0230 - val_mse: 15.0230 - val_mae: 1.6157 - lr: 1.1676e-04 - 12s/epoch - 12ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 13s - loss: 14.3517 - mse: 14.3517 - mae: 1.5578 - val_loss: 15.1473 - val_mse: 15.1473 - val_mae: 1.6240 - lr: 1.1676e-04 - 13s/epoch - 13ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 13s - loss: 14.3524 - mse: 14.3524 - mae: 1.5481 - val_loss: 15.1497 - val_mse: 15.1497 - val_mae: 1.6361 - lr: 1.1676e-04 - 13s/epoch - 13ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 13s - loss: 14.1642 - mse: 14.1642 - mae: 1.5534 - val_loss: 15.1092 - val_mse: 15.1092 - val_mae: 1.5923 - lr: 1.1676e-04 - 13s/epoch - 13ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 12s - loss: 14.0295 - mse: 14.0295 - mae: 1.5569 - val_loss: 15.2019 - val_mse: 15.2019 - val_mae: 1.5751 - lr: 1.1676e-04 - 12s/epoch - 12ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 12s - loss: 14.0239 - mse: 14.0239 - mae: 1.5440 - val_loss: 15.2219 - val_mse: 15.2219 - val_mae: 1.6681 - lr: 1.1676e-04 - 12s/epoch - 12ms/step\n",
            "Score for fold 3: loss of 15.221918106079102\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 12s - loss: 13.9535 - mse: 13.9535 - mae: 1.5486 - val_loss: 15.4954 - val_mse: 15.4954 - val_mae: 1.5419 - lr: 1.1676e-04 - 12s/epoch - 12ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 13s - loss: 13.8943 - mse: 13.8943 - mae: 1.5497 - val_loss: 15.3267 - val_mse: 15.3267 - val_mae: 1.6527 - lr: 1.1676e-04 - 13s/epoch - 13ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 13s - loss: 13.7213 - mse: 13.7213 - mae: 1.5354 - val_loss: 15.8715 - val_mse: 15.8715 - val_mae: 1.7379 - lr: 1.1676e-04 - 13s/epoch - 13ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 12s - loss: 13.6737 - mse: 13.6737 - mae: 1.5466 - val_loss: 15.7804 - val_mse: 15.7804 - val_mae: 1.5956 - lr: 1.1676e-04 - 12s/epoch - 12ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 12s - loss: 13.5832 - mse: 13.5832 - mae: 1.5332 - val_loss: 15.6795 - val_mse: 15.6795 - val_mae: 1.5043 - lr: 1.1676e-04 - 12s/epoch - 12ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 12s - loss: 13.5931 - mse: 13.5931 - mae: 1.5405 - val_loss: 15.6090 - val_mse: 15.6090 - val_mae: 1.7347 - lr: 1.1676e-04 - 12s/epoch - 12ms/step\n",
            "Epoch 7/100\n",
            "1000/1000 - 13s - loss: 13.4667 - mse: 13.4667 - mae: 1.5363 - val_loss: 16.2345 - val_mse: 16.2345 - val_mae: 1.5584 - lr: 1.1676e-04 - 13s/epoch - 13ms/step\n",
            "Score for fold 4: loss of 16.234498977661133\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "1000/1000 - 12s - loss: 15.4390 - mse: 15.4390 - mae: 1.5899 - val_loss: 8.0926 - val_mse: 8.0926 - val_mae: 1.4961 - lr: 1.1676e-04 - 12s/epoch - 12ms/step\n",
            "Epoch 2/100\n",
            "1000/1000 - 12s - loss: 15.2421 - mse: 15.2421 - mae: 1.5759 - val_loss: 8.3940 - val_mse: 8.3940 - val_mae: 1.5107 - lr: 1.1676e-04 - 12s/epoch - 12ms/step\n",
            "Epoch 3/100\n",
            "1000/1000 - 12s - loss: 15.2925 - mse: 15.2925 - mae: 1.5864 - val_loss: 8.6304 - val_mse: 8.6304 - val_mae: 1.3843 - lr: 1.1676e-04 - 12s/epoch - 12ms/step\n",
            "Epoch 4/100\n",
            "1000/1000 - 13s - loss: 15.1066 - mse: 15.1066 - mae: 1.5900 - val_loss: 8.5320 - val_mse: 8.5320 - val_mae: 1.4859 - lr: 1.1676e-04 - 13s/epoch - 13ms/step\n",
            "Epoch 5/100\n",
            "1000/1000 - 13s - loss: 14.9292 - mse: 14.9292 - mae: 1.5932 - val_loss: 8.9382 - val_mse: 8.9382 - val_mae: 1.5652 - lr: 1.1676e-04 - 13s/epoch - 13ms/step\n",
            "Epoch 6/100\n",
            "1000/1000 - 12s - loss: 14.8817 - mse: 14.8817 - mae: 1.5846 - val_loss: 8.9466 - val_mse: 8.9466 - val_mae: 1.4178 - lr: 1.1676e-04 - 12s/epoch - 12ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-10-16 09:35:31,851]\u001b[0m Finished trial#49 resulted in value: 14.706. Current best value is 14.024000000000001 with parameters: {'activation': 'relu', 'num_hidden_layer': 4, 'i': 9, 'learning_rate': 0.00018064149922822524}.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Score for fold 5: loss of 8.946612358093262\n"
          ]
        }
      ],
      "source": [
        "model_list=[]\n",
        "history_list=[]\n",
        "study_name = 'NN_study_6'\n",
        "study = optuna.create_study(study_name=study_name, load_if_exists=True)\n",
        "func = lambda trial: objective(trial,training_shuffled5,labelsForTrain_shuffled5)\n",
        "study.optimize(func, n_trials=50,)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#{'activation': 'relu', 'num_hidden_layer': 4, 'i': 9, 'learning_rate': 0.00018064149922822524}..\n",
        "optimizer = Adam(learning_rate=0.00018064149922822524 ,clipnorm=1.0)\n",
        "model_6 = create_model(activation=\"relu\",num_hidden_layer=4,num_hidden_unit=512)\n",
        "\n",
        "es = EarlyStopping(monitor='mse', patience=5)\n",
        "reduce_lr = ReduceLROnPlateau(monitor='val_mse', factor=0.2,\n",
        "                              patience=20, min_lr=0.001)\n",
        "model_6.compile(loss='mse',optimizer=optimizer,metrics=['mse','mae'])\n",
        "history = model_6.fit(training,labelsForTrain,\n",
        "                batch_size=64,\n",
        "                epochs=20,\n",
        "                verbose=2,\n",
        "                validation_data=(valing,labelsForVal),\n",
        "                validation_batch_size=64,\n",
        "                callbacks=[es,reduce_lr])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wNz7DEVq6XI_",
        "outputId": "435c3943-364d-4c78-98a1-ff67c07d4ea8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "1250/1250 - 6s - loss: 15.4598 - mse: 15.4598 - mae: 1.6063 - val_loss: 13.7991 - val_mse: 13.7991 - val_mae: 1.5361 - lr: 1.8064e-04 - 6s/epoch - 5ms/step\n",
            "Epoch 2/20\n",
            "1250/1250 - 5s - loss: 15.0903 - mse: 15.0903 - mae: 1.5900 - val_loss: 13.6928 - val_mse: 13.6928 - val_mae: 1.5615 - lr: 1.8064e-04 - 5s/epoch - 4ms/step\n",
            "Epoch 3/20\n",
            "1250/1250 - 5s - loss: 15.0270 - mse: 15.0270 - mae: 1.5898 - val_loss: 13.3485 - val_mse: 13.3485 - val_mae: 1.6743 - lr: 1.8064e-04 - 5s/epoch - 4ms/step\n",
            "Epoch 4/20\n",
            "1250/1250 - 5s - loss: 14.9620 - mse: 14.9620 - mae: 1.5861 - val_loss: 13.4234 - val_mse: 13.4234 - val_mae: 1.5668 - lr: 1.8064e-04 - 5s/epoch - 4ms/step\n",
            "Epoch 5/20\n",
            "1250/1250 - 5s - loss: 14.9829 - mse: 14.9829 - mae: 1.5834 - val_loss: 13.3538 - val_mse: 13.3538 - val_mae: 1.5883 - lr: 1.8064e-04 - 5s/epoch - 4ms/step\n",
            "Epoch 6/20\n",
            "1250/1250 - 5s - loss: 14.8728 - mse: 14.8728 - mae: 1.5766 - val_loss: 13.2445 - val_mse: 13.2445 - val_mae: 1.6180 - lr: 1.8064e-04 - 5s/epoch - 4ms/step\n",
            "Epoch 7/20\n",
            "1250/1250 - 5s - loss: 14.8137 - mse: 14.8137 - mae: 1.5752 - val_loss: 13.3362 - val_mse: 13.3362 - val_mae: 1.6179 - lr: 1.8064e-04 - 5s/epoch - 4ms/step\n",
            "Epoch 8/20\n",
            "1250/1250 - 5s - loss: 14.7335 - mse: 14.7335 - mae: 1.5769 - val_loss: 13.5698 - val_mse: 13.5698 - val_mae: 1.5524 - lr: 1.8064e-04 - 5s/epoch - 4ms/step\n",
            "Epoch 9/20\n",
            "1250/1250 - 5s - loss: 14.8070 - mse: 14.8070 - mae: 1.5722 - val_loss: 13.4258 - val_mse: 13.4258 - val_mae: 1.6172 - lr: 1.8064e-04 - 5s/epoch - 4ms/step\n",
            "Epoch 10/20\n",
            "1250/1250 - 5s - loss: 14.7494 - mse: 14.7494 - mae: 1.5722 - val_loss: 13.4628 - val_mse: 13.4628 - val_mae: 1.6166 - lr: 1.8064e-04 - 5s/epoch - 4ms/step\n",
            "Epoch 11/20\n",
            "1250/1250 - 5s - loss: 14.7187 - mse: 14.7187 - mae: 1.5725 - val_loss: 13.2250 - val_mse: 13.2250 - val_mae: 1.6866 - lr: 1.8064e-04 - 5s/epoch - 4ms/step\n",
            "Epoch 12/20\n",
            "1250/1250 - 5s - loss: 14.6467 - mse: 14.6467 - mae: 1.5722 - val_loss: 13.2489 - val_mse: 13.2489 - val_mae: 1.5414 - lr: 1.8064e-04 - 5s/epoch - 4ms/step\n",
            "Epoch 13/20\n",
            "1250/1250 - 5s - loss: 14.4447 - mse: 14.4447 - mae: 1.5693 - val_loss: 13.3608 - val_mse: 13.3608 - val_mae: 1.5403 - lr: 1.8064e-04 - 5s/epoch - 4ms/step\n",
            "Epoch 14/20\n",
            "1250/1250 - 5s - loss: 14.4990 - mse: 14.4990 - mae: 1.5641 - val_loss: 13.2512 - val_mse: 13.2512 - val_mae: 1.6875 - lr: 1.8064e-04 - 5s/epoch - 4ms/step\n",
            "Epoch 15/20\n",
            "1250/1250 - 5s - loss: 14.4128 - mse: 14.4128 - mae: 1.5605 - val_loss: 13.2618 - val_mse: 13.2618 - val_mae: 1.6006 - lr: 1.8064e-04 - 5s/epoch - 4ms/step\n",
            "Epoch 16/20\n",
            "1250/1250 - 5s - loss: 14.3340 - mse: 14.3340 - mae: 1.5633 - val_loss: 13.2949 - val_mse: 13.2949 - val_mae: 1.6120 - lr: 1.8064e-04 - 5s/epoch - 4ms/step\n",
            "Epoch 17/20\n",
            "1250/1250 - 5s - loss: 14.2623 - mse: 14.2623 - mae: 1.5591 - val_loss: 13.3621 - val_mse: 13.3621 - val_mae: 1.6571 - lr: 1.8064e-04 - 5s/epoch - 4ms/step\n",
            "Epoch 18/20\n",
            "1250/1250 - 5s - loss: 14.1714 - mse: 14.1714 - mae: 1.5571 - val_loss: 13.3676 - val_mse: 13.3676 - val_mae: 1.5749 - lr: 1.8064e-04 - 5s/epoch - 4ms/step\n",
            "Epoch 19/20\n",
            "1250/1250 - 5s - loss: 14.0960 - mse: 14.0960 - mae: 1.5545 - val_loss: 13.4711 - val_mse: 13.4711 - val_mae: 1.5714 - lr: 1.8064e-04 - 5s/epoch - 4ms/step\n",
            "Epoch 20/20\n",
            "1250/1250 - 5s - loss: 14.1214 - mse: 14.1214 - mae: 1.5548 - val_loss: 13.3401 - val_mse: 13.3401 - val_mae: 1.7049 - lr: 1.8064e-04 - 5s/epoch - 4ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "results_model6 = model_6.evaluate(testing, labelsForTest, batch_size=32)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cZfW5Qer8vPF",
        "outputId": "0ab68f67-79ac-4133-da8b-55842d58a367"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "313/313 [==============================] - 1s 2ms/step - loss: 11.9252 - mse: 11.9252 - mae: 1.6769\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The models performance on validation datasets:"
      ],
      "metadata": {
        "id": "xynvcNcc80kF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "MSE_val=np.array([13.34,13.15,13.75,13.05,13.22,13.27])\n",
        "RMSE_val=np.sqrt(MSE_val)\n",
        "MAE_val=np.array([1.704,1.593,1.555,1.61,1.569,1.589])"
      ],
      "metadata": {
        "id": "auBWNvOY87OQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mean_RMSE_val=np.mean(RMSE_val)\n",
        "mean_MAE_val=np.mean(MAE_val)"
      ],
      "metadata": {
        "id": "R4BRo87p9sml"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(mean_RMSE_val)\n",
        "print(mean_MAE_val)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2xuTrDyj93hc",
        "outputId": "5481b551-d5f5-4a56-dcda-44ee6e36edac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3.646333312697695\n",
            "1.6033333333333333\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(np.std(RMSE_val))\n",
        "print(np.std(MAE_val))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-cJ7KG1WGg6z",
        "outputId": "e4c5091a-c6f9-4e0d-b2e6-dcab8e5f5ed2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.030332150891770937\n",
            "0.048313789151982506\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Combine the trainning dataset and validation dataset for training \"the best of best\" model"
      ],
      "metadata": {
        "id": "xbL-szW2CVS0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_val=pd.concat([train_df, val_df])"
      ],
      "metadata": {
        "id": "WlM_LkDuCdMh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_val_input,labelsFortrain_val=process_shuffle_dataset(train_val)"
      ],
      "metadata": {
        "id": "cb0W1elMG4Aa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_val_input"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "BGvn-JwKHIQv",
        "outputId": "e21aad14-8bb7-4955-fc23-d0f4d005c19a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "             0         1         2         3         4         5         6   \\\n",
              "0      0.902011  0.995920 -0.804756 -0.330617 -0.379517 -0.206158 -0.188756   \n",
              "1      0.902011  0.995920 -0.603161 -0.026909  0.119132 -1.204293 -0.111687   \n",
              "2      0.902011  0.995920  0.930712  0.589285  0.861888  2.393625  0.795618   \n",
              "3      0.253331  0.061782 -0.033437 -0.104152 -0.014398  2.512132  0.505354   \n",
              "4     -1.692709 -1.495115  3.542680  0.059113 -0.394122 -1.233808  0.157354   \n",
              "...         ...       ...       ...       ...       ...       ...       ...   \n",
              "89995 -0.395349 -0.664770 -0.883641 -0.330617 -0.358653 -0.920665 -0.188756   \n",
              "89996  0.902011  0.995920  0.781707  0.194289 -0.068644 -0.004835 -0.056484   \n",
              "89997  0.253331  0.061782  0.483698  0.066135  1.788247  0.171983 -0.123963   \n",
              "89998  0.253331  0.061782 -0.243797  0.417242  1.992714  2.826385 -0.065426   \n",
              "89999  0.253331  0.061782  1.517967 -0.132241  0.271438  0.244796  0.035134   \n",
              "\n",
              "             7         8         9   ...        25        26        27  \\\n",
              "0     -0.610631 -0.172660  1.410127  ...  1.010500  0.728954  1.191707   \n",
              "1      1.015724  0.012508  0.583623  ...  0.362933 -1.006542  0.347735   \n",
              "2      3.292620  0.218251  0.535006  ... -1.024711 -0.059908  1.191707   \n",
              "3      1.015724  0.005650 -0.084872  ... -1.487259  0.728954 -0.496237   \n",
              "4     -0.610631 -0.172660 -3.305804  ... -2.689883  0.886726  1.191707   \n",
              "...         ...       ...       ...  ...       ...       ...       ...   \n",
              "89995 -0.610631 -0.165802 -0.024100  ...  0.640461 -0.533225 -0.496237   \n",
              "89996  0.039911 -0.124653  0.413461  ... -0.562163  0.571181  1.191707   \n",
              "89997  0.690453  0.122238  0.401307  ... -0.284634 -0.217680  0.347735   \n",
              "89998  3.292620  0.499433  0.510697  ...  0.270423 -0.533225 -0.496237   \n",
              "89999  1.991536  0.060515 -0.558896  ... -0.747182  0.097864  2.879650   \n",
              "\n",
              "             28        29        30        31        32        33        34  \n",
              "0     -0.797708 -0.915846 -0.878536 -0.600577 -0.037368 -0.012793 -0.565006  \n",
              "1     -0.567887 -0.699348 -0.878536 -0.070421 -0.037368 -0.012793  0.222404  \n",
              "2      1.270684  0.491392  1.120411  0.636455 -0.037368 -0.012793 -0.171301  \n",
              "3      0.305434  0.058396 -0.434325 -0.423858 -0.037368 -0.012793 -0.565006  \n",
              "4      3.109255  1.249135  3.563567  3.817395 -0.037368 -0.012793 -0.565006  \n",
              "...         ...       ...       ...       ...       ...       ...       ...  \n",
              "89995 -0.935601 -0.591099 -0.767483 -0.600577 -0.037368 -0.012793 -0.565006  \n",
              "89996  0.489291  0.707890  1.342516  0.283017 -0.037368 -0.012793  0.222404  \n",
              "89997  1.086827  0.924388  0.343043 -0.423858 -0.037368 -0.012793 -0.171301  \n",
              "89998 -0.154209  0.383143  0.343043 -0.070421 -0.037368 -0.012793 -0.565006  \n",
              "89999  2.052077  2.439875  0.565148  0.636455 -0.037368 -0.012793 -0.565006  \n",
              "\n",
              "[90000 rows x 35 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-1f3a415a-ccd1-4def-84af-736ffc950214\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>...</th>\n",
              "      <th>25</th>\n",
              "      <th>26</th>\n",
              "      <th>27</th>\n",
              "      <th>28</th>\n",
              "      <th>29</th>\n",
              "      <th>30</th>\n",
              "      <th>31</th>\n",
              "      <th>32</th>\n",
              "      <th>33</th>\n",
              "      <th>34</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.902011</td>\n",
              "      <td>0.995920</td>\n",
              "      <td>-0.804756</td>\n",
              "      <td>-0.330617</td>\n",
              "      <td>-0.379517</td>\n",
              "      <td>-0.206158</td>\n",
              "      <td>-0.188756</td>\n",
              "      <td>-0.610631</td>\n",
              "      <td>-0.172660</td>\n",
              "      <td>1.410127</td>\n",
              "      <td>...</td>\n",
              "      <td>1.010500</td>\n",
              "      <td>0.728954</td>\n",
              "      <td>1.191707</td>\n",
              "      <td>-0.797708</td>\n",
              "      <td>-0.915846</td>\n",
              "      <td>-0.878536</td>\n",
              "      <td>-0.600577</td>\n",
              "      <td>-0.037368</td>\n",
              "      <td>-0.012793</td>\n",
              "      <td>-0.565006</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.902011</td>\n",
              "      <td>0.995920</td>\n",
              "      <td>-0.603161</td>\n",
              "      <td>-0.026909</td>\n",
              "      <td>0.119132</td>\n",
              "      <td>-1.204293</td>\n",
              "      <td>-0.111687</td>\n",
              "      <td>1.015724</td>\n",
              "      <td>0.012508</td>\n",
              "      <td>0.583623</td>\n",
              "      <td>...</td>\n",
              "      <td>0.362933</td>\n",
              "      <td>-1.006542</td>\n",
              "      <td>0.347735</td>\n",
              "      <td>-0.567887</td>\n",
              "      <td>-0.699348</td>\n",
              "      <td>-0.878536</td>\n",
              "      <td>-0.070421</td>\n",
              "      <td>-0.037368</td>\n",
              "      <td>-0.012793</td>\n",
              "      <td>0.222404</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.902011</td>\n",
              "      <td>0.995920</td>\n",
              "      <td>0.930712</td>\n",
              "      <td>0.589285</td>\n",
              "      <td>0.861888</td>\n",
              "      <td>2.393625</td>\n",
              "      <td>0.795618</td>\n",
              "      <td>3.292620</td>\n",
              "      <td>0.218251</td>\n",
              "      <td>0.535006</td>\n",
              "      <td>...</td>\n",
              "      <td>-1.024711</td>\n",
              "      <td>-0.059908</td>\n",
              "      <td>1.191707</td>\n",
              "      <td>1.270684</td>\n",
              "      <td>0.491392</td>\n",
              "      <td>1.120411</td>\n",
              "      <td>0.636455</td>\n",
              "      <td>-0.037368</td>\n",
              "      <td>-0.012793</td>\n",
              "      <td>-0.171301</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.253331</td>\n",
              "      <td>0.061782</td>\n",
              "      <td>-0.033437</td>\n",
              "      <td>-0.104152</td>\n",
              "      <td>-0.014398</td>\n",
              "      <td>2.512132</td>\n",
              "      <td>0.505354</td>\n",
              "      <td>1.015724</td>\n",
              "      <td>0.005650</td>\n",
              "      <td>-0.084872</td>\n",
              "      <td>...</td>\n",
              "      <td>-1.487259</td>\n",
              "      <td>0.728954</td>\n",
              "      <td>-0.496237</td>\n",
              "      <td>0.305434</td>\n",
              "      <td>0.058396</td>\n",
              "      <td>-0.434325</td>\n",
              "      <td>-0.423858</td>\n",
              "      <td>-0.037368</td>\n",
              "      <td>-0.012793</td>\n",
              "      <td>-0.565006</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>-1.692709</td>\n",
              "      <td>-1.495115</td>\n",
              "      <td>3.542680</td>\n",
              "      <td>0.059113</td>\n",
              "      <td>-0.394122</td>\n",
              "      <td>-1.233808</td>\n",
              "      <td>0.157354</td>\n",
              "      <td>-0.610631</td>\n",
              "      <td>-0.172660</td>\n",
              "      <td>-3.305804</td>\n",
              "      <td>...</td>\n",
              "      <td>-2.689883</td>\n",
              "      <td>0.886726</td>\n",
              "      <td>1.191707</td>\n",
              "      <td>3.109255</td>\n",
              "      <td>1.249135</td>\n",
              "      <td>3.563567</td>\n",
              "      <td>3.817395</td>\n",
              "      <td>-0.037368</td>\n",
              "      <td>-0.012793</td>\n",
              "      <td>-0.565006</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>89995</th>\n",
              "      <td>-0.395349</td>\n",
              "      <td>-0.664770</td>\n",
              "      <td>-0.883641</td>\n",
              "      <td>-0.330617</td>\n",
              "      <td>-0.358653</td>\n",
              "      <td>-0.920665</td>\n",
              "      <td>-0.188756</td>\n",
              "      <td>-0.610631</td>\n",
              "      <td>-0.165802</td>\n",
              "      <td>-0.024100</td>\n",
              "      <td>...</td>\n",
              "      <td>0.640461</td>\n",
              "      <td>-0.533225</td>\n",
              "      <td>-0.496237</td>\n",
              "      <td>-0.935601</td>\n",
              "      <td>-0.591099</td>\n",
              "      <td>-0.767483</td>\n",
              "      <td>-0.600577</td>\n",
              "      <td>-0.037368</td>\n",
              "      <td>-0.012793</td>\n",
              "      <td>-0.565006</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>89996</th>\n",
              "      <td>0.902011</td>\n",
              "      <td>0.995920</td>\n",
              "      <td>0.781707</td>\n",
              "      <td>0.194289</td>\n",
              "      <td>-0.068644</td>\n",
              "      <td>-0.004835</td>\n",
              "      <td>-0.056484</td>\n",
              "      <td>0.039911</td>\n",
              "      <td>-0.124653</td>\n",
              "      <td>0.413461</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.562163</td>\n",
              "      <td>0.571181</td>\n",
              "      <td>1.191707</td>\n",
              "      <td>0.489291</td>\n",
              "      <td>0.707890</td>\n",
              "      <td>1.342516</td>\n",
              "      <td>0.283017</td>\n",
              "      <td>-0.037368</td>\n",
              "      <td>-0.012793</td>\n",
              "      <td>0.222404</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>89997</th>\n",
              "      <td>0.253331</td>\n",
              "      <td>0.061782</td>\n",
              "      <td>0.483698</td>\n",
              "      <td>0.066135</td>\n",
              "      <td>1.788247</td>\n",
              "      <td>0.171983</td>\n",
              "      <td>-0.123963</td>\n",
              "      <td>0.690453</td>\n",
              "      <td>0.122238</td>\n",
              "      <td>0.401307</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.284634</td>\n",
              "      <td>-0.217680</td>\n",
              "      <td>0.347735</td>\n",
              "      <td>1.086827</td>\n",
              "      <td>0.924388</td>\n",
              "      <td>0.343043</td>\n",
              "      <td>-0.423858</td>\n",
              "      <td>-0.037368</td>\n",
              "      <td>-0.012793</td>\n",
              "      <td>-0.171301</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>89998</th>\n",
              "      <td>0.253331</td>\n",
              "      <td>0.061782</td>\n",
              "      <td>-0.243797</td>\n",
              "      <td>0.417242</td>\n",
              "      <td>1.992714</td>\n",
              "      <td>2.826385</td>\n",
              "      <td>-0.065426</td>\n",
              "      <td>3.292620</td>\n",
              "      <td>0.499433</td>\n",
              "      <td>0.510697</td>\n",
              "      <td>...</td>\n",
              "      <td>0.270423</td>\n",
              "      <td>-0.533225</td>\n",
              "      <td>-0.496237</td>\n",
              "      <td>-0.154209</td>\n",
              "      <td>0.383143</td>\n",
              "      <td>0.343043</td>\n",
              "      <td>-0.070421</td>\n",
              "      <td>-0.037368</td>\n",
              "      <td>-0.012793</td>\n",
              "      <td>-0.565006</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>89999</th>\n",
              "      <td>0.253331</td>\n",
              "      <td>0.061782</td>\n",
              "      <td>1.517967</td>\n",
              "      <td>-0.132241</td>\n",
              "      <td>0.271438</td>\n",
              "      <td>0.244796</td>\n",
              "      <td>0.035134</td>\n",
              "      <td>1.991536</td>\n",
              "      <td>0.060515</td>\n",
              "      <td>-0.558896</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.747182</td>\n",
              "      <td>0.097864</td>\n",
              "      <td>2.879650</td>\n",
              "      <td>2.052077</td>\n",
              "      <td>2.439875</td>\n",
              "      <td>0.565148</td>\n",
              "      <td>0.636455</td>\n",
              "      <td>-0.037368</td>\n",
              "      <td>-0.012793</td>\n",
              "      <td>-0.565006</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>90000 rows × 35 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-1f3a415a-ccd1-4def-84af-736ffc950214')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-1f3a415a-ccd1-4def-84af-736ffc950214 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-1f3a415a-ccd1-4def-84af-736ffc950214');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#{'activation': 'relu', 'num_hidden_layer': 4, 'i': 10, 'learning_rate': 0.00015658492778156685}.\n",
        "optimizer = Adam(learning_rate=0.00015658492778156685 ,clipnorm=1.0)\n",
        "best_model = create_model(activation=\"relu\",num_hidden_layer=4,num_hidden_unit=1024)\n",
        "\n",
        "es = EarlyStopping(monitor='mse', patience=5)\n",
        "reduce_lr = ReduceLROnPlateau(monitor='mse', factor=0.2,\n",
        "                              patience=20, min_lr=0.001)\n",
        "best_model.compile(loss='mse',optimizer=optimizer,metrics=['mse','mae'])\n",
        "history = best_model.fit(train_val_input,labelsFortrain_val,\n",
        "                batch_size=64,\n",
        "                epochs=20,\n",
        "                verbose=2,\n",
        "                callbacks=[es,reduce_lr])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XyM66ov2IUCO",
        "outputId": "5e4b75d5-d867-4e96-bc5a-85fa455e018e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "1407/1407 - 13s - loss: 12.7619 - mse: 12.7619 - mae: 1.5371 - lr: 1.5658e-04 - 13s/epoch - 9ms/step\n",
            "Epoch 2/20\n",
            "1407/1407 - 12s - loss: 11.9341 - mse: 11.9341 - mae: 1.4849 - lr: 1.5658e-04 - 12s/epoch - 8ms/step\n",
            "Epoch 3/20\n",
            "1407/1407 - 12s - loss: 11.7715 - mse: 11.7715 - mae: 1.4672 - lr: 1.5658e-04 - 12s/epoch - 8ms/step\n",
            "Epoch 4/20\n",
            "1407/1407 - 12s - loss: 11.8170 - mse: 11.8170 - mae: 1.4617 - lr: 1.5658e-04 - 12s/epoch - 8ms/step\n",
            "Epoch 5/20\n",
            "1407/1407 - 12s - loss: 11.5307 - mse: 11.5307 - mae: 1.4523 - lr: 1.5658e-04 - 12s/epoch - 8ms/step\n",
            "Epoch 6/20\n",
            "1407/1407 - 12s - loss: 11.3214 - mse: 11.3214 - mae: 1.4419 - lr: 1.5658e-04 - 12s/epoch - 8ms/step\n",
            "Epoch 7/20\n",
            "1407/1407 - 12s - loss: 11.2257 - mse: 11.2257 - mae: 1.4355 - lr: 1.5658e-04 - 12s/epoch - 8ms/step\n",
            "Epoch 8/20\n",
            "1407/1407 - 12s - loss: 11.2936 - mse: 11.2936 - mae: 1.4294 - lr: 1.5658e-04 - 12s/epoch - 8ms/step\n",
            "Epoch 9/20\n",
            "1407/1407 - 12s - loss: 10.7149 - mse: 10.7149 - mae: 1.4193 - lr: 1.5658e-04 - 12s/epoch - 8ms/step\n",
            "Epoch 10/20\n",
            "1407/1407 - 12s - loss: 10.8829 - mse: 10.8829 - mae: 1.4036 - lr: 1.5658e-04 - 12s/epoch - 8ms/step\n",
            "Epoch 11/20\n",
            "1407/1407 - 12s - loss: 10.3980 - mse: 10.3980 - mae: 1.3925 - lr: 1.5658e-04 - 12s/epoch - 8ms/step\n",
            "Epoch 12/20\n",
            "1407/1407 - 12s - loss: 10.0946 - mse: 10.0946 - mae: 1.3806 - lr: 1.5658e-04 - 12s/epoch - 8ms/step\n",
            "Epoch 13/20\n",
            "1407/1407 - 12s - loss: 9.9864 - mse: 9.9864 - mae: 1.3671 - lr: 1.5658e-04 - 12s/epoch - 8ms/step\n",
            "Epoch 14/20\n",
            "1407/1407 - 12s - loss: 9.6111 - mse: 9.6111 - mae: 1.3488 - lr: 1.5658e-04 - 12s/epoch - 8ms/step\n",
            "Epoch 15/20\n",
            "1407/1407 - 12s - loss: 9.4423 - mse: 9.4423 - mae: 1.3384 - lr: 1.5658e-04 - 12s/epoch - 8ms/step\n",
            "Epoch 16/20\n",
            "1407/1407 - 12s - loss: 9.2512 - mse: 9.2512 - mae: 1.3257 - lr: 1.5658e-04 - 12s/epoch - 8ms/step\n",
            "Epoch 17/20\n",
            "1407/1407 - 12s - loss: 9.0260 - mse: 9.0260 - mae: 1.3085 - lr: 1.5658e-04 - 12s/epoch - 8ms/step\n",
            "Epoch 18/20\n",
            "1407/1407 - 12s - loss: 8.8887 - mse: 8.8887 - mae: 1.2936 - lr: 1.5658e-04 - 12s/epoch - 8ms/step\n",
            "Epoch 19/20\n",
            "1407/1407 - 12s - loss: 8.2226 - mse: 8.2226 - mae: 1.2738 - lr: 1.5658e-04 - 12s/epoch - 8ms/step\n",
            "Epoch 20/20\n",
            "1407/1407 - 12s - loss: 8.1933 - mse: 8.1933 - mae: 1.2607 - lr: 1.5658e-04 - 12s/epoch - 8ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "results_best_model = best_model.evaluate(testing, labelsForTest, batch_size=32)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2x9tPx7KIs_s",
        "outputId": "7a5eb2a5-2494-4518-93b6-2ad494500b0b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "313/313 [==============================] - 1s 3ms/step - loss: 14.0053 - mse: 14.0053 - mae: 1.5175\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "collapsed_sections": [],
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}