from ctypes import Union
import numpy as np
import pandas as pd
import torch
import torch.utils.data as dt
import matplotlib.pyplot as plt
import pytorch_lightning as pl
import optuna
import sys
import os
import gc
import argparse

sys.path.append("..")

from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping
from pytorch_lightning.loggers import WandbLogger
from sklearn.model_selection import KFold
from sklearn.metrics import mean_absolute_error, mean_squared_error
from functools import partial
from copy import deepcopy
from collections import defaultdict
from tqdm import tqdm

from scripts.distilbert_reg import DistilBERTRegressor
from scripts.data_module import YelpDataModule, YelpPredictDataModule

from config import CONFIG

pl.seed_everything(seed=42)



def early_stopping_check(study, trial, early_stopping_rounds=2):
    current_trial_number = trial.number
    best_trial_number = study.best_trial.number
    should_stop = (current_trial_number - best_trial_number) >= early_stopping_rounds
    if should_stop:
        print("Early stopping detected!!")
        study.stop()



def objective(trial:optuna.Trial, train:pd.DataFrame, val:pd.DataFrame):

    gc.collect()
    torch.cuda.empty_cache()

    max_len = trial.suggest_int("max_len", 128, 512, step=128)
    lr = trial.suggest_float("lr", 1e-6, 1e-4)
    t_max = trial.suggest_int("T_max", 500, 1000, step=100)
    clip_val = trial.suggest_float("clip_val", 0, 0.5, step=0.1)
    weight_decay = trial.suggest_float("weight_decay", 0, 1e-6)
    n_heads = trial.suggest_int("n_heads", 2, 12)

    config = deepcopy(CONFIG)

    del config['wandb']
    del config['_wandb_kernel']

    config['max_len'] = max_len
    config['lr'] = lr
    config['scheduler']['T_max'] = t_max
    config['clip_val'] = clip_val
    config['weight_decay'] = weight_decay
    config['bert']['config']['n_heads'] = n_heads

    early_stopping = EarlyStopping(
        monitor="val_loss",
        mode="min",
        patience=2,
        verbose=True
    )

    dm = YelpDataModule(config, train, val=val)

    model = DistilBERTRegressor(config)

    trainer = pl.Trainer(
        devices=-1,
        accelerator="gpu",
        precision=16,
        max_epochs=config['n_epochs'],
        callbacks=[
            early_stopping
        ],
        gradient_clip_val=config['clip_val'],
        deterministic=True,
        strategy="ddp"
    )
    
    dm.setup()
    train_dl = dm.train_dataloader()
    val_dl = dm.val_dataloader()
    trainer.fit(model, train_dl, val_dl)
 
    # curr_loss = trainer.callback_metrics['val_loss'].item()
    score = trainer.test(model, val_dl)
    print(score)

    del trainer
    del model
    del train_dl
    del val_dl
    del dm

    return np.mean([score[i]['test_loss'] for i in range(len(score))])



def objective_cv(trial:optuna.Trial, data:pd.DataFrame) -> np.array:
    kfold = KFold(n_splits=CONFIG['nested']['inner'], shuffle=True, random_state=42)
    scores = list()

    print(f'Trial Number:{trial.number}')
    for fold, (train_idx, val_idx) in enumerate(kfold.split(data)):
        torch.cuda.empty_cache()
        gc.collect()

        print(f"<==== Fold - {fold}====>")
        train, val = data.iloc[train_idx], data.iloc[val_idx]
        val_loss = objective(trial, train, val)
        scores.append(val_loss)
    
    return np.mean(scores)


def score_model(train, test, hyp):

    gc.collect()
    torch.cuda.empty_cache()

    config = deepcopy(CONFIG)

    config['max_len'] = hyp['max_len']
    config['lr'] = hyp['lr']
    config['scheduler']['T_max'] = hyp['T_max']
    config['clip_val'] = hyp['clip_val']
    config['weight_decay'] = hyp['weight_decay']
    config['bert']['config']['n_heads'] = hyp['n_heads']

    dm = YelpDataModule(config, train, test=test)
    dm.setup()
    train_loader = dm.train_dataloader()
    test_loader = dm.test_dataloader()


    model = DistilBERTRegressor(config)

    trainer = pl.Trainer(
        devices=-1,
        accelerator="gpu",
        precision=16,
        max_epochs=15,
        gradient_clip_val=config['clip_val'],
        deterministic=True,
        strategy="ddp"
    )

    trainer.fit(model, train_loader)
    score = trainer.test(model, test_loader)

    print(score)

    del model
    del trainer
    del train_loader
    del test_loader
    del dm
    
    return np.mean([score[i]['test_loss'] for i in range(len(score))])



def nested_cross_val(data):

    kfold = KFold(n_splits=CONFIG['nested']['outer'], shuffle=True, random_state=42)

    scores = list()

    for fold, (train_idx, val_idx) in enumerate(kfold.split(data)):
        print(f"<------ Outer Fold - {fold} --------> ")

        torch.cuda.empty_cache()
        gc.collect()

        train, val = data.iloc[train_idx], data.iloc[val_idx]
        sampler = optuna.samplers.TPESampler(multivariate=True, seed=1234)
        study = optuna.create_study(
            study_name = f"dbert-fine-tuning-outer-fold{fold}",
            direction = "minimize",
            sampler = sampler, 
            pruner = optuna.pruners.PatientPruner(optuna.pruners.MedianPruner(), patience=5),
            load_if_exists=True,
            storage=f"sqlite:///study.db"
        )

        func = lambda trial: objective_cv(trial, train)

        study.optimize(
            func,
            n_trials=50,
            callbacks=[partial(early_stopping_check, early_stopping_rounds=5)]
        )

        print(f"Number of finished trial : {len(study.trials)}")
        print(f"Best Trial Value : {study.best_trial.value}")

        best_hyp = dict()
        for k, v in study.best_trial.params.items():
            print(f"{k}:{v}")
            best_hyp[k] = v

        val_score = score_model(train, val, best_hyp)
        print(f'Validation Score (Outer fold) - {val_score:.5f}')
        scores.append(val_score)

    print(f'Mean validation score : {np.mean(scores):.5f}')

    return scores




if __name__ == "__main__":

    df_train = pd.read_parquet(CONFIG["file_paths"]["train"])
    scores = nested_cross_val(df_train)
    print(scores)
