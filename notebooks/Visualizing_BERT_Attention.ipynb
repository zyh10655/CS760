{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1ff638f7-0ecb-4d05-9cb6-e522f56cfaa2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-02T05:21:47.994391Z",
     "iopub.status.busy": "2022-10-02T05:21:47.993774Z",
     "iopub.status.idle": "2022-10-02T05:22:00.792780Z",
     "shell.execute_reply": "2022-10-02T05:22:00.792173Z",
     "shell.execute_reply.started": "2022-10-02T05:21:47.994228Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "42"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.utils.data as dt\n",
    "import pytorch_lightning as pl\n",
    "import os\n",
    "import sys\n",
    "import gc\n",
    "\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from bertviz import model_view, head_view, neuron_view\n",
    "from copy import deepcopy\n",
    "\n",
    "from scripts.distilbert_reg import DistilBERTRegressor\n",
    "from scripts.data_module import YelpDataModule, YelpPredictDataModule\n",
    "\n",
    "from scripts.best_config import CONFIG\n",
    "\n",
    "pl.seed_everything(seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "49bca2b7-e8ec-41b9-98d3-ed99da8e08ed",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-02T05:22:00.793930Z",
     "iopub.status.busy": "2022-10-02T05:22:00.793659Z",
     "iopub.status.idle": "2022-10-02T05:22:00.799762Z",
     "shell.execute_reply": "2022-10-02T05:22:00.798651Z",
     "shell.execute_reply.started": "2022-10-02T05:22:00.793909Z"
    }
   },
   "outputs": [],
   "source": [
    "def merge_data(df1:pd.DataFrame, df2:pd.DataFrame, on:str, suffixes:tuple=None) -> pd.DataFrame:\n",
    "    \"\"\" Function to merge the dataframe \"\"\"\n",
    "  \n",
    "    if suffixes is None:\n",
    "        suffixes = ('_x', '_y')\n",
    "    df_merge = pd.merge(df1, df2, on=on, suffixes=suffixes)\n",
    "    df_merge = df_merge[['r_text', 'r_useful']]\n",
    "\n",
    "    return df_merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "aa403cf7-1e11-49e1-8464-99c1bba8a5e1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-02T05:22:00.803028Z",
     "iopub.status.busy": "2022-10-02T05:22:00.802582Z",
     "iopub.status.idle": "2022-10-02T05:22:05.243466Z",
     "shell.execute_reply": "2022-10-02T05:22:05.242754Z",
     "shell.execute_reply.started": "2022-10-02T05:22:00.802990Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               r_text  r_useful\n",
      "4   Always a good experience. Dr Ramsey has been m...         2\n",
      "10  I see that Steve's Prince of steaks has been i...         3\n"
     ]
    }
   ],
   "source": [
    "df_text = pd.read_parquet(\"../data/new_data/train_text.parquet.snappy\")\n",
    "df_main = pd.read_parquet(\"../data/new_data/train_main.parquet.snappy\")\n",
    "df = merge_data(df_text, df_main, \"r_id\", suffixes=(\"_text\", \"_main\"))\n",
    "sample_df = df.iloc[[4, 10]]\n",
    "print(sample_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6a7d8e55-6b1b-41ef-9b4d-c25a6403e00b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-02T05:22:05.244715Z",
     "iopub.status.busy": "2022-10-02T05:22:05.244374Z",
     "iopub.status.idle": "2022-10-02T05:22:05.251310Z",
     "shell.execute_reply": "2022-10-02T05:22:05.250676Z",
     "shell.execute_reply.started": "2022-10-02T05:22:05.244689Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4      35\n",
       "10    113\n",
       "Name: r_text, dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_df['r_text'].apply(lambda x: len(x.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e2b453d0-6768-4818-b80c-358eb2478900",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-02T05:22:05.252386Z",
     "iopub.status.busy": "2022-10-02T05:22:05.252119Z",
     "iopub.status.idle": "2022-10-02T05:22:08.716076Z",
     "shell.execute_reply": "2022-10-02T05:22:08.715372Z",
     "shell.execute_reply.started": "2022-10-02T05:22:05.252364Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_projector.bias', 'vocab_layer_norm.weight', 'vocab_transform.weight', 'vocab_projector.weight', 'vocab_transform.bias', 'vocab_layer_norm.bias']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DistilBertModel(\n",
      "  (embeddings): Embeddings(\n",
      "    (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
      "    (position_embeddings): Embedding(512, 768)\n",
      "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (transformer): Transformer(\n",
      "    (layer): ModuleList(\n",
      "      (0): TransformerBlock(\n",
      "        (attention): MultiHeadSelfAttention(\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "        )\n",
      "        (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "        (ffn): FFN(\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (activation): GELUActivation()\n",
      "        )\n",
      "        (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "      )\n",
      "      (1): TransformerBlock(\n",
      "        (attention): MultiHeadSelfAttention(\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "        )\n",
      "        (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "        (ffn): FFN(\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (activation): GELUActivation()\n",
      "        )\n",
      "        (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "      )\n",
      "      (2): TransformerBlock(\n",
      "        (attention): MultiHeadSelfAttention(\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "        )\n",
      "        (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "        (ffn): FFN(\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (activation): GELUActivation()\n",
      "        )\n",
      "        (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "      )\n",
      "      (3): TransformerBlock(\n",
      "        (attention): MultiHeadSelfAttention(\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "        )\n",
      "        (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "        (ffn): FFN(\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (activation): GELUActivation()\n",
      "        )\n",
      "        (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "      )\n",
      "      (4): TransformerBlock(\n",
      "        (attention): MultiHeadSelfAttention(\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "        )\n",
      "        (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "        (ffn): FFN(\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (activation): GELUActivation()\n",
      "        )\n",
      "        (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "      )\n",
      "      (5): TransformerBlock(\n",
      "        (attention): MultiHeadSelfAttention(\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "        )\n",
      "        (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "        (ffn): FFN(\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (activation): GELUActivation()\n",
      "        )\n",
      "        (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = DistilBERTRegressor(CONFIG)\n",
    "print(model.dbert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "280337bb-9461-4aac-ae70-7adb8e2c97ce",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-02T05:22:08.719327Z",
     "iopub.status.busy": "2022-10-02T05:22:08.718905Z",
     "iopub.status.idle": "2022-10-02T05:22:08.727037Z",
     "shell.execute_reply": "2022-10-02T05:22:08.724318Z",
     "shell.execute_reply.started": "2022-10-02T05:22:08.719300Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def sample_predict_data(data:pd.DataFrame):\n",
    "    dm = YelpPredictDataModule(data, CONFIG)\n",
    "    model = DistilBERTRegressor(CONFIG)\n",
    "    dm.setup()\n",
    "    trainer = pl.Trainer(\n",
    "        devices=1,\n",
    "        accelerator=\"gpu\",\n",
    "        precision=16,\n",
    "    )\n",
    "    \n",
    "    pred_dl = dm.predict_dataloader()\n",
    "    preds = trainer.predict(model=model, dataloaders=pred_dl, ckpt_path=\"../scripts/models/dbert.ckpt\")\n",
    "    preds = np.round(torch.cat(preds).flatten().numpy())\n",
    "    \n",
    "    dbert = model.load_from_checkpoint(\"../scripts/models/dbert.ckpt\", config=CONFIG).dbert\n",
    "    return preds, dbert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e5bdf98a-16f1-46d1-9c1c-77056512f836",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-02T05:22:08.728336Z",
     "iopub.status.busy": "2022-10-02T05:22:08.728097Z",
     "iopub.status.idle": "2022-10-02T05:22:26.557399Z",
     "shell.execute_reply": "2022-10-02T05:22:26.556760Z",
     "shell.execute_reply.started": "2022-10-02T05:22:08.728310Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_projector.bias', 'vocab_layer_norm.weight', 'vocab_transform.weight', 'vocab_projector.weight', 'vocab_transform.bias', 'vocab_layer_norm.bias']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Using 16bit native Automatic Mixed Precision (AMP)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Restoring states from the checkpoint path at ../scripts/models/dbert.ckpt\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "Loaded model weights from checkpoint at ../scripts/models/dbert.ckpt\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ab1bb6977c3495eaeeb2569b1186e5b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_projector.bias', 'vocab_layer_norm.weight', 'vocab_transform.weight', 'vocab_projector.weight', 'vocab_transform.bias', 'vocab_layer_norm.bias']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "# np.round()\n",
    "preds, model2 = sample_predict_data(sample_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7b05b9ff-0e34-40c3-99fd-1676a4ec9493",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-02T05:22:26.559185Z",
     "iopub.status.busy": "2022-10-02T05:22:26.558791Z",
     "iopub.status.idle": "2022-10-02T05:22:26.570557Z",
     "shell.execute_reply": "2022-10-02T05:22:26.568010Z",
     "shell.execute_reply.started": "2022-10-02T05:22:26.559134Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2., 2.], dtype=float16)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ee3736b3-ba88-4174-8668-d4fc3958cfde",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-02T05:22:26.574719Z",
     "iopub.status.busy": "2022-10-02T05:22:26.574131Z",
     "iopub.status.idle": "2022-10-02T05:22:26.580255Z",
     "shell.execute_reply": "2022-10-02T05:22:26.579406Z",
     "shell.execute_reply.started": "2022-10-02T05:22:26.574677Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Always a good experience. Dr Ramsey has been my doctor since he began practicing. He is easy to talk with and explains things so I understand. So glad to be a patient in this group.'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_df['r_text'].values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "73f6a97f-d14e-4770-a8c0-5c8976aabf8b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-02T05:22:52.697987Z",
     "iopub.status.busy": "2022-10-02T05:22:52.697798Z",
     "iopub.status.idle": "2022-10-02T05:22:52.765321Z",
     "shell.execute_reply": "2022-10-02T05:22:52.763840Z",
     "shell.execute_reply.started": "2022-10-02T05:22:52.697968Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', 'always', 'a', 'good', 'experience', '.', 'dr', 'ramsey', 'has', 'been', 'my', 'doctor', 'since', 'he', 'began', 'practicing', '.', 'he', 'is', 'easy', 'to', 'talk', 'with', 'and', 'explains', 'things', 'so', 'i', 'understand', '.', 'so', 'glad', 'to', 'be', 'a', 'patient', 'in', 'this', 'group', '.', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "sample_text = \"Always a good experience. Dr Ramsey has been my doctor since he began practicing. He is easy to talk with and explains things so I understand. So glad to be a patient in this group.\"\n",
    "tokenizer = CONFIG['bert']['tokenizer']\n",
    "inputs = tokenizer.encode(sample_text, return_tensors=\"pt\")\n",
    "outputs = model2(inputs)\n",
    "attention = outputs[-1]\n",
    "tokens = tokenizer.convert_ids_to_tokens(inputs[0])\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f64e31b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(\"../html_views/\"):\n",
    "    os.mkdir(\"../html_views/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "06450617-8f15-4ce1-9180-699c6c52e2e0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-02T05:23:06.053752Z",
     "iopub.status.busy": "2022-10-02T05:23:06.053285Z",
     "iopub.status.idle": "2022-10-02T05:23:06.185205Z",
     "shell.execute_reply": "2022-10-02T05:23:06.183471Z",
     "shell.execute_reply.started": "2022-10-02T05:23:06.053708Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "head_view_html = head_view(attention, tokens, html_action='return')\n",
    "with open(\"../html_views/head_view.html\", \"w\") as fp:\n",
    "    fp.write(head_view_html.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3ffaeccf-3aaa-4ad7-81bc-edac19be80fa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_view_html = model_view(attention, tokens, html_action='return')\n",
    "with open(\"../html_views/model_view.html\", \"w\") as fp:\n",
    "    fp.write(model_view_html.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "020aec75",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
